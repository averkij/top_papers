{
    "paper_title": "VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation",
    "authors": [
        "Lesly Miculicich",
        "Mihir Parmar",
        "Hamid Palangi",
        "Krishnamurthy Dj Dvijotham",
        "Mirko Montanari",
        "Tomas Pfister",
        "Long T. Le"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The deployment of autonomous AI agents in sensitive domains, such as healthcare, introduces critical risks to safety, security, and privacy. These agents may deviate from user objectives, violate data handling policies, or be compromised by adversarial attacks. Mitigating these dangers necessitates a mechanism to formally guarantee that an agent's actions adhere to predefined safety constraints, a challenge that existing systems do not fully address. We introduce VeriGuard, a novel framework that provides formal safety guarantees for LLM-based agents through a dual-stage architecture designed for robust and verifiable correctness. The initial offline stage involves a comprehensive validation process. It begins by clarifying user intent to establish precise safety specifications. VeriGuard then synthesizes a behavioral policy and subjects it to both testing and formal verification to prove its compliance with these specifications. This iterative process refines the policy until it is deemed correct. Subsequently, the second stage provides online action monitoring, where VeriGuard operates as a runtime monitor to validate each proposed agent action against the pre-verified policy before execution. This separation of the exhaustive offline validation from the lightweight online monitoring allows formal guarantees to be practically applied, providing a robust safeguard that substantially improves the trustworthiness of LLM agents."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 6 5 1 5 0 . 0 1 5 2 : r VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation Lesly Miculicich1, Mihir Parmar1, Hamid Palangi1, Krishnamurthy Dj Dvijotham2, Mirko Montanari3, Tomas Pfister1* and Long T. Le1* 1Google Cloud AI Research, 2Google DeepMind, 3Google Cloud AI The deployment of autonomous AI agents in sensitive domains, such as healthcare, introduces critical risks to safety, security, and privacy. These agents may deviate from user objectives, violate data handling policies, or be compromised by adversarial attacks. Mitigating these dangers necessitates mechanism to formally guarantee that an agents actions adhere to predefined safety constraints, challenge that existing systems do not fully address. We introduce Ve uar d, novel framework that provides formal safety guarantees for LLM-based agents through dual-stage architecture designed for robust and verifiable correctness. The initial offline stage involves comprehensive validation process. It begins by clarifying user intent to establish precise safety specifications. VeriGuard then synthesizes behavioral policy and subjects it to both testing and formal verification to prove its compliance with these specifications. This iterative process refines the policy until it is deemed correct. Subsequently, the second stage provides online action monitoring, where VeriGuard operates as runtime monitor to validate each proposed agent action against the pre-verified policy before execution. This separation of the exhaustive offline validation from the lightweight online monitoring allows formal guarantees to be practically applied, providing robust safeguard that substantially improves the trustworthiness of LLM agents. 1. Introduction The proliferation of Large Language Model (LLM) agents marks significant leap towards autonomous AI systems capable of executing complex, multi-step tasks (Xi et al., 2023; Yao et al., 2023). These agents, often empowered to interact with external tools, APIs, and file systems (Patil et al., 2024; Schick et al., 2023a), hold immense promise for automating digital workflows and solving real-world problems. However, this power introduces substantial and often unpredictable safety and security vulnerabilities. critical reliability gap has emerged: while LLM agents can generate solutions with unprecedented flexibility, the solution they produce often lacks assurances, making it susceptible to subtle errors, security flaws, and emergent behaviors that can lead to catastrophic failures. An agent tasked with data analysis could inadvertently exfiltrate sensitive information; one managing cloud infrastructure could execute destructive commands; another interacting with financial APIs could trigger erroneous, irreversible transactions. This problem is even more serious when there is adversary attack on the system, as shown in Zhang et al. (2025). Existing safety mechanismssuch as sandboxing, input/output filtering, and static rule-based guardrails (Inan et al., 2023; Rebedea et al., 2023) provide necessary but insufficient first line of defense. These approaches are fundamentally reactive or based on pattern matching; they struggle to cover the vast and dynamic state space of agent actions and can be bypassed by novel adversarial inputs or unforeseen edge cases (Wei et al., 2023; Xu et al., 2023). They lack deep, semantic understanding of the codes intent and consequences, treating the agents output as black box to be constrained. This leaves systems vulnerable to sophisticated exploits that static rule set cannot anticipate (Schulhoff et al., 2023). For LLM agents to be trusted in high-stakes, mission-critical Corresponding authors: Corresponding authors: Lesly Miculicich lmiculicich@google.com and Long T. Le: longtle@google.com * Joint last authors. VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation environments, more rigorous, provable approach to safety is required. In this work, we propose novel method to address this reliability gap, centered on the Ve ri - uar framework. Ve ri uar represents paradigm shift from reactive filtering to proactive, provable safety by deeply integrating policy specification generation and automated verification into the agents action-generation pipeline. VeriGuard fundamentally reshapes the code generation process to be correct-by-construction. This is achieved by prompting the LLM agent to generate not only the functional code for an action but also its corresponding verification that precisely define the codes expected behavior and safety properties. These paired artifacts are then immediately subjected to an automated verification engine. An iterative refinement loop forms the core of our framework: if verification fails, the verifier provides specific counterexample or logical inconsistency, which is fed back to the agent as concrete, actionable critique to guide the generation of corrected and verifiably safe version of the code (Pan et al., 2024; Zhao et al., 2025). More details are in 3. The primary contribution of this paper is the VeriGuard framework itself, which includes novel methodologies for the LLM-driven generation and refinement of verifiable code tailored to agent security and safety contexts. We further contribute comprehensive empirical validation of the frameworks effectiveness in preventing unsafe actions across variety of challenging domains. Finally, we present detailed analysis of the performance trade-offs inherent in this approach. 2. Related Work 2.1. LLM Agents and the Emergence of Autonomous Systems The development of Large Language Models (LLMs) has catalyzed the emergence of new class of autonomous systems known as LLM agents. LLM agents are designed to be proactive, goal-oriented entities capable of planning, reasoning, and interacting with their environment through the use of tools (Schick et al., 2023b). Early frameworks like ReAct demonstrated how to synergize reasoning and acting within LLMs, enabling them to solve complex tasks by generating both textual reasoning traces and executable actions (Yao et al., 2023). The agent can also execute more complex tasks like web browsing. This capability, however, is merely the entry point into broader spectrum of autonomous actions. Advanced agents are not just navigating websites but are becoming generalist problem-solvers on the web and beyond. This evolution is detailed in research and demonstrated in benchmarks like WebArena (Zhou et al., 2023) and Mind2Web (Gou et al., 2025), which test agents on their ability to perform multi-step, realistic tasks on live websites. This paradigm quickly evolved into more sophisticated agent architectures. Systems like AutoGPT (Gravitas, 2023) and BabyAGI showcased the potential for fully autonomous task completion, where agents could decompose high-level goals into smaller, executable steps, manage memory, and selfdirect their workflow. Further research has explored enhancing agent capabilities through mechanisms like self-reflection and verbal reinforcement learning, allowing them to learn from past mistakes and improve their performance over time (Shinn et al., 2023). The concept of \"Generative Agents\" pushed the boundaries even further by creating interactive simulacra of human behavior within sandbox environment, highlighting the potential for complex social and emergent behaviors (Park et al., 2023). comprehensive survey by (Wang et al., 2023) details the rapid advancements and architectural patterns in this burgeoning field. 2.2. LLM Safety, Alignment, and Guardrails significant body of research has focused on ensuring the safety and alignment of LLMs. primary line of defense involves creating guardrails to constrain agent behavior. These can range from simple 2 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation input/output filtering and prompt-based restrictions to more sophisticated techniques (Bai et al., 2022). Another critical area is the proactive discovery of vulnerabilities through red teaming, where humans or other AIs craft adversarial prompts to elicit unsafe or undesirable behaviors from the model (Ganguli et al., 2022). The insights from these attacks are then used to fine-tune the model for greater robustness. Despite these efforts, LLMs remain susceptible to wide array of \"jailbreaking\" techniques that can bypass safety filters (Wei et al., 2023). More recent work has focused on creating safety-tuned LLMs specifically for tool use, aiming to prevent harmful API calls or command executions (Jin et al., 2024). There are some previous work in Agent safeguard. GuardAgent, framework that uses an LLMbased guard agent to safeguard other LLM agents. GuardAgent operates as protective layer, using reasoning to detect and prevent unsafe behaviors (Xiang et al., 2025). Another work is ShieldAgent, guardrail agent designed to ensure that autonomous agents powered by large language models (LLMs) adhere to safety policies (Chen et al., 2025). However, these existing approaches are largely empirical and reactive. They rely on identifying and patching vulnerabilities as they are discovered, but they do not provide formal, provable guarantees of safety. clever adversary can often devise novel attack that circumvents existing guardrails. This highlights fundamental limitation: without formal specification of what constitutes safe behavior and method to verify compliance, safety remains an ongoing. VeriGuard distinguishes itself from this body of work by moving from an empirical to formal verification paradigm, aiming to prove the correctness of an agents actions before they are ever executed. 2.3. Formal Methods and Verifiable Code Generation Formal methods provide mathematically rigorous set of techniques for the specification, development, and verification of software and hardware systems. The advent of powerful LLMs has opened new frontier for bridging the gap between natural language specifications and formal, machine-checkable code. Recent research has begun to explore the potential for LLMs to automate or assist in the generation of not just code, but also its formal specification and verification artifacts. For example, (Li et al., 2024) demonstrate system where LLMs are used to generate verifiable computation, producing code along with the necessary components for verifier to check its correctness. Further studies have investigated the self-verification capabilities of LLMs (Ghaffarian et al., 2024). This line of work shows the promise of integrating LLMs into high-assurance software development pipelines. 3. Methodology Figure 1 describes the high-level ideas of VeriGuard, which operates in two main stages: (i) Policy Generation: VeriGuard takes inputs including the agents specification and high-level security request in natural language to synthesize an initial policy function and its corresponding formal constraints. To ensure the correctness and alignment of this policy, we employ rigorous, multi-step refinement feedback loop. This loop begins with validation phase to resolve any ambiguities in the users request, followed by an automated code testing phase that generates unit tests to verify functional correctness. The most critical phase uses formal verification to prove that the policy code adheres to its specified conditions, ensuring provably-sound safety contract. (ii) Policy Enforcement: The verified policy is integrated into the agentic system at key enforcement points, where it intercepts and evaluates agent-initiated actions before execution. When potential violation is detected, VeriGuard can employ one of several enforcement strategies, ranging from immediately terminating the agents task to blocking the specific unsafe action or engaging in collaborative re-planning dialogue with the agent. 3 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation Figure 1 Ve ri uar overview which includes Policy generation and Policy enforcement. The verified policy is integrated into the agent as runtime safeguard, intercepting and preventing harmful actions. 3.1. Task Definition In this section, we formalize the process of generating agent policies from high-level, natural language specifications. Policy Generation Given safety and security request in natural language, denoted as ð‘Ÿ, and agent specification, ð’®, the primary objective is to synthesize policy function, ð‘, written in structured programming language. Concurrently, set of verifiable constraints (i.e. pre and post-conditions), ð¶ = {ð‘1, ð‘2, . . . , ð‘ð‘›}, is derived. The system must guarantee that the generated policy ð‘ complies with all constraints in ð¶. This relationship is formally denoted as ð‘ = ð¶, signifying that ð‘ ð¶, the policy ð‘ satisfies ð‘. The user request ð‘Ÿ typically defines security or operational protocol in text format, while the agent specification ð’® provides schematic of the agent: task description, input/output (I/O) data structures, available context, environmental information, and any other available data. Policy Enforcement Given an agentic system and set of verified policies, the second objective is to integrate these policies as enforcement mechanisms. The goal is to optimize the systems performance by minimizing policy violations (i.e., reducing the attack surface) while maximizing the agents task-completion utility. 3.2. Framework To address the defined tasks, we propose framework, VeriGuard, which consists of an initial policy generator followed by an iterative refinement loop. This loop validates, tests, and formally verifies the policy code to ensure it accurately reflects the agent requirements and specifications. For the policy enforcement task, experiment with multiple integration strategies for deploying VeriGuard within 4 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation an agentic system. 3.2.1. Policy Generator The Policy Generator is the core component responsible for translating the agents specification and users intent into executable code and formal specifications. It has two sub-components: (1) policy code generation, and (2) constrains generation, both LLM-based. At the first pass, the policy generator takes the user request ð‘Ÿ, an agent specification ð’®, to produce preliminary policy function ð‘0, together with list of arguments the policy function requires ð’«0. Similarly, the constrains generator take same inputs to produce set of constraints ð¶0. This initial generation functions, ðº0 and ð»0, can be represented as: ðº0ð‘Ÿ, ð’® ð‘0, ð’«0 ð»0ð‘Ÿ, ð’® ð¶0 The arguments schema ð’«0 contains the name, description and type of each required input argument of the policy function. If the request entails multiple interdependent rules, the generator produces single, cohesive codebase that encapsulates all logic. The prompts for the initial generations are detailed in A.1. The Policy Generator operates within an iterative refinement loop where policy and constraints are gradually improved from the previous step (ð‘ð‘¡1, ð¶ð‘¡1): ðºð‘¡ð‘Ÿ, ð’®, ð‘…, ð´, ð‘’, ð‘ð‘¡1 ð‘ð‘¡, ð’«ð‘¡ ð»ð‘¡ð‘Ÿ, ð’®, ð‘…, ð´, ð‘ð‘¡1 ð¶ð‘¡ ð‘…, ð´, and ð‘’ are the set of requirements, assumptions and coding error messages. 3.2.2. Refinement Process We employ three-stage refinement process: validation, testing, and formal verification. Validation The Validators primary function is to resolve ambiguities and ensure the semantic alignment between the users natural language request and its formal representation (ð‘0, ð¶0). This process is bifurcated into an analysis phase and disambiguation phase. In the analysis phase, function ð‘‰ð‘Ž scrutinizes the initial artifacts to identify semantic ambiguities, logical inconsistencies, and implicit presuppositions. The output is set of queries, ð‘„, that encapsulate these issues: ð‘‰ð‘Žð‘0, ð¶0 ð‘„ In the disambiguation phase, function ð‘‰ð‘‘ processes the users feedback, ð‘ˆfeedback, to resolve the queries in ð‘„. This interactive process yields definitive set of explicit assumptions, ð´, and refined, unambiguous set of requirements ð‘… as: ð‘‰ð‘‘ð‘„, ð‘ˆfeedback ð´, ð‘… In an autonomous operational mode where user feedback is unavailable, an internal module, Î©, is invoked to resolve the queries by selecting the most contextually plausible interpretations. This generates set of default assumptions, ð´default, which are then used to produce the final requirements ð‘…. This autonomous path is modeled as: ð‘‰ð‘‘ð‘„, Î©ð‘„ ð´default, ð‘…. A.3 shows the implementations detail of this component. Code Testing This module automatically generates suite of test cases to perform empirical validation of the policy function. It takes the policy code ð‘, the user request ð‘Ÿ, and the agent specification ð’® as input. The objective is to ensure that the policy meets baseline of functional requirements and correctly handles typical and edge-case scenarios before proceeding to the more computationally VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation expensive formal verification stage. The output is set of test cases formatted for the PyTest framework. The policy code is refined iteratively until all generated tests pass, with failure reports and error messages ð‘’ serving as feedback for the refinement loop. The iteration stops when not more errors are found or at maximum ð‘ number. Details in A.4. Verification The final stage of refinement involves formal verification using program verifier. This component takes the logical constraints ð¶ and the policy code ð‘ as input. The constraints in ð¶ define formal contract, specifying the pre-conditions (ð¶pre ð¶) that must hold before the policys execution and the post-conditions (ð¶post ð¶) that must be guaranteed upon its completion. The verifiers task is to mathematically prove that the policy code ð‘ adheres to this contract. This relationship is formally expressed using Hoare triple: {ð¶pre} ð‘ {ð¶post}. If program ð‘ starts in state where pre-condition ð¶pre is true, its execution is guaranteed to terminate in state where post-condition ð¶post is true. If the code violates the contract, the verifier provides counterexample or error trace ð‘’, which is used as feedback to refine the policy or constraints. The refinement cycle continues until formal verification succeeds or at maximum ð‘ number. For this implementation, we utilize the Nagini verifier (Eilers and MÃ¼ller, 2018) as black box. As static verifier built on the Viper (Eilers et al., 2025) infrastructure, Nagini can handle more complex properties than other available Python verifiers. Pre-processing for Nagini is detailed in A.5. 3.3. Policy Enforcement Strategies Once policy is generated and verified, it is integrated into the agentic system at specific enforcement points that intercept agent-initiated actions (e.g., tool executions, database access, environmental interactions). Each agent can be governed by one or more policy functions. 3.3.1. Policy Function Arguments At runtime, the arguments for the policy function defined, in ð’«, must be populated from the agentic system data defined in ð‘†. We do not assume ð‘† is direct input to the policy, as this data could be unstructured, and require preprocessing or extraction. Moreover, implementing preprocessing step strictly via code can limit the systems flexibility. Thus, function ð‘“ : ð‘† ð‘ƒ is required to map the agent data to the policy arguments. For our experiments, we implement ð‘“ as flexible LLM-based component (A.2). The input of ð‘“ is the agent data in the format specified in ð‘† and the output is populated dictionary of arguments specified in ð‘ƒ . 3.3.2. Policy Function Integration We experimented with four distinct enforcement strategies upon detecting policy violation: (i) Task Termination: the most restrictive approach, which halts the agents entire high-level task and issues notification explaining the violation; (ii) Action Blocking: more targeted approach, where the specific action that violates the policy is blocked, but the agent is permitted to continue executing subsequent actions in its plan that do not violate policy; (iii) Tool Execution Halt: which stops the specific execution that caused the violation and returns no observation to the agent, forcing the agents reasoning process to halt and decide on new course of action based on the lack of feedback; and (iv) Collaborative Re-planning: the most interactive strategy, which informs the agent of the specific policy it has violated and requests that it generate new action plan that complies with the stated constraints. 6 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation 4. Experiments 4.1. Datasets Agent Security Bench (ASB) (Zhang et al., 2025) This dataset focuses on four types of attacks against agentic systems: direct and indirect prompt injection, memory poisoning, and plan-of-thought backdoors. The framework utilizes 10 ReAct agents across 10 different scenarios (e.g., finance, commerce, autonomous driving), incorporating 10 different tasks, 13 attack variations, and 400 attack tools. EICU-AC (Xiang et al., 2025) This benchmark evaluates an ICU question/answer agents access control over 10 patient databases. It defines specific user roles, each restricted to designated subsets of the databases and their internal information categories. The agent must deny any user question if its answer requires accessing data that is forbidden for the users given role. Mind2Web-SC (Xiang et al., 2025) Mind2Web-SC adapts the Mind2Web benchmark, which tasks web agents with solving over 2,000 complex online tasks using sequential actions. The SC (Safety Control) variant augments this framework by introducing safety rules that prohibit specific users from performing certain web activities, thereby testing the agents ability to enforce these policies. 4.2. Evaluation Metrics ASB : we simultaneously evaluate defense efficacy and operational interference using two primary metrics: the (i) Attack Success Rate (ASR): quantifies the quality of the defense. It is defined as the ratio of successful attacks to the total number of attack attempts. (ii) Task Success Rate (TSR): measures the operational interference or overhead imposed by the defense mechanism. It is calculated as the ratio of successfully completed original tasks to the total number of tasks attempted. EICU-AC and Mind2Web-SC: the task is defined exclusively as access control, where correctly stopping forbidden action is the desired outcome. This evaluation is treated as binary classification problem (Allow vs. Deny), so we use standard classification metrics: Accuracy, Precision, and Recall. 4.3. Baselines ASB : we experiment with four backbone LLMs: Gemini 2.5 Flash and Pro, GPT-4.1, and ClaudeSonnet-4. We establish two base boundaries: the upper-bound No Attack, derived from unattacked model performance, and No Defence, derived from undefended model performance.We compare against several baselines: Paraphrasing (Jain et al., 2023), which rewords the query to disrupt malicious special-character sequences and triggers (effective for DPI and PoT Backdoor attacks); Dynamic Prompt Rewriting (Zhang et al., 2025), which transforms the input to align with security objectives (proposed for DPI); and Delimiter (Mattern et al., 2023), which encapsulates the user query to ensure bounded execution (effective for IPI). We also implemented stronger Guardrail baseline that receives the same input as Ve ri uar but, instead of generating code function, directly asks an LLM to evaluate policy compliance. EICU-AC and Mind2Web-SC: we report the results of several state-of-the-art (SOTA) approaches. These include GuardAgent (Xiang et al., 2025), which translates natural language safety rules VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation Table 1 Experiment results of Veri Guard on ASB benchmark. Attack Success Rate (ASR ) Task Success Rate (TSR ). Defense Gemini-2.5-Flash No attack No defense Delimiter Paraphrasing DP Rewriting GuardRail VeriG uard Gemini-2.5-Pro No attack No defense GuardRail VeriG uard GPT-4.1 No attack No defense Delimiter Paraphrasing DP Rewriting GuardRail VeriG uard Claude-sonnet-4 No attack No defense Delimiter Paraphrasing DP Rewriting GuardRail VeriG uard DPI IPI MP PoT AVG ASR TSR ASR TSR ASR TSR ASR TSR ASR TSR 98.5 71.8 70.5 0.0 0.0 83.0 0.0 0.0 92.5 80.3 74.5 0.0 0. 31.3 39.8 66.8 0.0 0.0 57.5 0.5 24.0 30.0 24.5 50.5 76.0 3.5 48.8 55. 64.5 1.0 19.0 15.5 20.0 28.0 100.0 89.0 88.5 57.5 68.5 86.8 40.5 40.8 0.0 0. 62.3 0.0 0.0 60.0 64.3 0.0 0.0 63.8 60.8 0.0 0. 57.5 46.3 48.5 35.3 55.8 76.0 68.0 18.0 65.5 64.5 45.3 52.0 31.5 42. 15 0.0 0.0 11.0 0.0 0.0 2.8 0.0 0. 57.5 57.3 58.5 69.0 76.0 79.8 67.3 76.8 64.5 62.3 63.0 63. 100.0 97.0 24.0 100.0 82.0 98.3 46.0 91.5 0.0 0.0 75.5 81. 53.5 57.3 0.0 0.0 52.2 0.0 0.0 99.5 60.0 0.0 0. 80.5 73.3 0.0 0.0 74.3 64.3 67.3 66.3 77.7 78.0 75.5 72.0 71. 87.0 87.0 85.5 82.0 94.5 99.0 87.8 90.5 83.5 80.2 51.9 0.0 0. 52.1 0.0 0.0 63.7 0.0 0.0 49.9 0.0 0. 61.7 42.1 40.2 63.3 76.5 56.7 51.5 67.3 70.1 43.1 44.6 57. 99.8 89.0 68.3 85.1 into executable code via manually defined functions; AGrail (Luo et al., 2025), which implements mechanism to continually learn and adapt policies (as security checks) and uses an LLM for verification; LLaMA-Guard 3 Llama Team (2024), model trained to detect security issues; and AgentMonitor (Chan et al., 2024), guardrail method for multi-agent systems. We also include the Hard-coded Rules baseline in (Xiang et al., 2025). 8 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation Table 2 Performance comparison on the EICU-AC and Mind2Web-SC benchmarks"
        },
        {
            "title": "Methods",
            "content": "EICU-AC Mind2Web-SC Acc. Acc. Hardcoded Safety Rules (Xiang et al., 2025)* 81.0 77.5 95.1 LLaMA-Guard3 (Luo et al., 2025)* 56.0 93.0 48.7 AgentMonitor (GPT-4o) (Luo et al., 2025)* 79.2 72.5 82.3 GuardAgent (GPT-4) (Xiang et al., 2025)* 90.0 100.0 98.7 AGrail (GPT-4o) (Luo et al., 2025)* 99.0 98.4 97.8 91.3 100.0 100.0 100.0 95.1 Veri Guard (GPT-4o) 91.2 100.0 100.0 100.0 96.2 Veri Guard (GPT-4.1) 95.0 100.0 100.0 100.0 97.2 Veri Guard (Gemini-2.5-pro) 76.6 98.2 100.0 97.5 90.7 66.7 97.5 98.1 58.0 13.0 61.0 80.0 98.0 99.0 100.0 99. * Values obtained from the cited papers. 4.4. Results Table 1 summarizes our evaluation on the ASB dataset, conducted across three backbone LLMs to assess generalization. The table reports the ASR and TSR against several baselines, including No Defense scenario (providing lower bound for ASR) and No Attack scenario (an upper bound for TSR). The low ASR achieved by GuardRail indicates that simple violation detection is largely solved task for strong LLMs. The primary challenge, therefore, is not if violation occurs, but how to intervene precisely by blocking only the malicious component (e.g., specific tool) without degrading task utility. Paraphrasing and Delimiter defenses show high TSR whit Claude-Sonnet-4, Claude-Sonnet showed strong performance in this benchmark (Zhang et al., 2025), however the ASR remains high. Ve ri uar proves particularly effective at this, achieving near-zero ASR while simultaneously outperforming all other defenses in TSR, demonstrating superior trade-off between security and utility. Table 2 summarizes the performance evaluation on the EICU-AC and Mind2Web-SC datasets. To ensure fair comparison, we use GPT-4o as the backbone LLM, consistent with the SOTA model. We also report with Gemini-2.5-pro. Ve ri uar d, achieves perfect accuracy on the EICU-AC dataset and outperforms all baselines on recall in Mind2Web-SC. This is particularly noteworthy given that Ve ri uar is generic policy constructor, whereas strong baseline like GuardAgent employs predefined policy structure specifically tailored to these access control tasks. Furthermore, unlike GuardAgent, our method does not require any in-context learning to build its policies. On the other hand, Agrail shows better accuracy and precision showing that an external memory bank of policies can be beneficial. Future, work can enhance Ve ri uar with memory of previous judgments. While our method attains high accuracy, we argue that recall is more critical metric for security applications. On both datasets, Ve ri uar achieves high recall, signifying that it successfully identifies and blocks every policy violation. This capacity to prevent all illicit actions, even at the cost of decrease in precision, is crucial requirement for deploying secure agentic systems. 5. Analysis 5.1. Ablation Study of VeriGuard Components The results of our ablation study, presented in Figure 2, detail the cumulative impact of each VeriG uar component. The analysis was conducted on the Agent Security Benchmark (ASB), utilizing 9 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation Figure 2 (a) shows the ASR is systematically reduced to 0% across all evaluated attack types. (b) shows the TSR increases as defense layers are added. Gemini-2.5-Flash with default parameters. Figure 2a shows the defense is built in stages as initially the agent is highly vulnerable, with an average ASR of 53.5%. First, the Policy Generation step provides substantial impact, reducing the average ASR to 9.97%. Subsequently, the Validation plays critical role for complex attacks where the initial policy may be incomplete or non-executable; this is most evident against Memory Poisoning, where this step reduces the ASR by more than half (from 31.75% to 15%). Following this, the Validation component further enhances robustness, fully neutralizing all remaining threats and reducing the ASR to 0% across all attack vectors. Finally, the formal verification step ensures that the defense code rigorously follows all security constraints. Figure 2b demonstrates that these robust security gains do not incur performance trade-off. The TSR remains high and exhibits consistent increase (from 51.87% to 63.25% average), confirming Ve ri uar ds ability to secure the agent without compromising functional utility. 5.2. Evaluating Integration Methods: Security vs. Utility Each strategy offers different trade-off. Task Termination (TT) is the most stringent approach; it neutralizes threats by terminating any task when an attack is detected. This method is impractical for real-world scenarios because it results in complete task failure (0% TSR). Action Blocking (AB) is less severe strategy that blocks single malicious action but allows subsequent, non-malicious actions to proceed, forcing the agent to replan. Tool Execution Halt (TEH) offers more granular approach. single agent \"action\" can invoke multiple tool calls (some benign), so TEH blocks only the suspicious tool callnot the entire actionletting the agent continue its plan with \"no tool response\" error. In contrast, Collaborative Re-planning (CRP) is the least invasive method. Instead of blocking, Ve ri uar sends an alert to the agent, which allows it to formulate new, safer plan. While this significantly boosts the TSR, it doesnt guarantee security, as the agent can still perform unsafe actions (leading to an 11.9% average ASR). Therefore, hybrid CRP + TEH approach yields the optimal results. This combination leverages the high TSR of CRP with the fine-grained security of TEH, achieving both near-zero average ASR (0.1%) and the highest average TSR (63.6%). Table 3 presents the results from the ASB using Gemini-2.5-Flash. It evaluates the five integration strategies detailed in Section 3.3.2: Task Termination (TT), Action Blocking (AB), Tool Execution Halt (TEH), Collaborative Re-planning (CRP), and combination of CRP and TEH. 10 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation Table 3 ASR and TSR for VeriGuard integration methods including Task Termination (TT), Action Blocking (AB), Tool Execution Halt (TEH), Collaborative Re-planning (CRP). CRP + TEH combination achieves the optimal balance of security and utility."
        },
        {
            "title": "IPI",
            "content": "MP"
        },
        {
            "title": "AVG",
            "content": "ASR TSR ASR TSR ASR TSR ASR TSR ASR TSR 0.0 0.0 38.1 0.0 44.6 0.0 62.1 14.3 63.3 0.0 0.0 0.0 0.0 11.9 0.0 0.0 0.0 0.0 33.3 0.0 0.0 55.5 61.5 69.0 69.0 0.0 34.0 48.8 50.0 55.8 0.0 62.3 68 77.7 77. 0.0 0.5 0.3 51.5 50.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 TT AB TEH CRP CRP + TEH 5.3. Limitations It is pertinent to acknowledge some limitations of our current approach that define clear avenues for future research. primary limitation stems from the reliance on Large Language Model (LLM) to generate formal constraints from natural languagea process that is inherently non-deterministic and susceptible to error. Consequently, the soundness of the formal verification is contingent upon manual validation by the user to ensure the generated constraints accurately reflect their intent. Secondly, the systems capabilities are intrinsically bound by the underlying program verification tool, Nagini. As Nagini is an active research project, it may possess limited grammar for expressing certain complex properties. Furthermore, while extending the framework to other programming languages is possible, doing so represents non-trivial implementation challenge. Finally, our hybrid architecture, which integrates an LLM for argument interpretation with deterministic Python code for rule implementation, may be insufficient for identifying sophisticated attacks that require deeper capacity for logical reasoning and dynamic policy updates, which presents key direction for future investigation. 6. Conclusion In this work, we introduce Ve ri uar d, novel framework designed to substantially enhance the safety and reliability of Large Language Model (LLM) agents. By integrating verification module that formally checks agent-generated policies and actions against predefined safety specifications, Ve ri uar moves beyond reactive, pattern-matching safety measures to proactive, provablysound approach. Our experiments demonstrate that this interactive verification loop is highly effective at preventing wide range of unsafe operations, from prompt injections to unauthorized data access, while maintaining high degree of task success. The results on benchmarks such as ASB, EICU-AC, and Mind2Web-SC show that VeriGuard not only significantly reduces the attack success rate to near-zero but also offers flexible policy enforcement strategies that can be tailored to different operational needs. Ve ri uar provides robust and essential safeguard, paving the way for the trustworthy deployment of LLM agents in complex and high-stakes real-world environments. Building on the foundation of VeriGuard, several promising avenues for future research emerge. One key direction is the scalability and efficiency of the formal verification process. Another area for exploration is the autonomous generation of safety specifications themselves. 11 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation"
        },
        {
            "title": "References",
            "content": "Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. TranJohnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, L. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Grosse, S. Ringer, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei, N. Joseph, S. McCandlish, T. Brown, and J. Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022. C.-M. Chan, J. Yu, W. Chen, C. Jiang, X. Liu, W. Shi, Z. Liu, W. Xue, and Y. Guo. Agentmonitor: plug-and-play framework for predictive and secure multi-agent systems, 2024. URL https: //arxiv.org/abs/2408.14972. Z. Chen, M. Kang, and B. Li. Shieldagent: Shielding agents via verifiable safety policy reasoning. ICML, 2025. M. Eilers and P. MÃ¼ller. Nagini: static verifier for python. In International Conference on Computer Aided Verification, pages 596603. Springer, 2018. M. Eilers, M. Schwerhoff, A. J. Summers, and P. MÃ¼ller. Fifteen years of viper. In R. Piskac and Z. RakamariÄ‡, editors, Computer Aided Verification (CAV), pages 107123, Cham, 2025. Springer Nature Switzerland. doi: 10.1007/978-3-031-98668-0_5. URL https://link.springer.com/ chapter/10.1007/978-3-031-98668-0_5. D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N. Schiefer, K. Ndousse, A. Jones, S. Bowman, A. Chen, T. Conerly, N. DasSarma, D. Drain, N. Elhage, S. ElShowk, S. Fort, Z. Hatfield-Dodds, T. Henighan, D. Hernandez, T. Hume, J. Jacobson, S. Johnston, S. Kravec, C. Olsson, S. Ringer, E. Tran-Johnson, D. Amodei, T. Brown, N. Joseph, S. McCandlish, C. Olah, J. Kaplan, and J. Clark. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned, 2022. URL https://arxiv.org/abs/2209.07858. S. Ghaffarian, R. Raval, G. Bavota, and M. Izadi. Can llms verify their own code? case study in secure web development, 2024. B. Gou, Z. Huang, Y. Ning, Y. Gu, M. Lin, W. Qi, A. Kopanev, B. Yu, B. J. GutiÃ©rrez, Y. Shu, C. H. Song, J. Wu, S. Chen, H. N. Moussa, T. Zhang, J. Xie, Y. Li, T. Xue, Z. Liao, K. Zhang, B. Zheng, Z. Cai, V. Rozgic, M. Ziyadi, H. Sun, and Y. Su. Mind2web 2: Evaluating agentic search with agent-as-a-judge, 2025. S. Gravitas. Auto-gpt: Significant-Gravitas/Auto-GPT, 2023. An autonomous gpt-4 experiment. https://github.com/ H. Inan, K. Kandasamy, ganath. conversations, llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/. and S. Ranhuman-ai for https://ai.meta.com/research/publications/ S. Purohit, safeguard S. Rameshbabu, M. El-Khamy, input-output Llm-based URL guard: Llama 2023. N. Jain, A. Schwarzschild, Y. Wen, G. Somepalli, J. Kirchenbauer, P. yeh Chiang, M. Goldblum, A. Saha, J. Geiping, and T. Goldstein. Baseline defenses for adversarial attacks against aligned language models, 2023. URL https://arxiv.org/abs/2309.00614. Z. Jin, H. Zhang, Z. Zhou, J. Li, M. Gao, and E. Chen. Llm-safeguard: human-in-the-loop framework for tuning safety-guard of llm-based agents, 2024. 12 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation G. Li, Y. Zhang, Z. Chen, H. Wang, Z. Wang, S.-Q. Chen, Y.-F. Li, Z. Tang, M. ud K. Effendy, A.-T. T. Nguyen, X. Xie, M.-H. Tsai, and T.-C. Chen. Llm-based generation of verifiable computation, 2024. A. . M. Llama Team. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. W. Luo, S. Dai, X. Liu, S. Banerjee, H. Sun, M. Chen, and C. Xiao. AGrail: lifelong agent guardrail with effective and adaptive safety detection. In W. Che, J. Nabende, E. Shutova, and M. T. Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 81048139, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.399. URL https:// aclanthology.org/2025.acl-long.399/. J. Mattern, F. Mireshghallah, Z. Jin, B. Schoelkopf, M. Sachan, and T. Berg-Kirkpatrick. Membership inference attacks against language models via neighbourhood comparison. In A. Rogers, J. BoydGraber, and N. Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 1133011343, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.719. URL https://aclanthology.org/2023.findings-acl. 719/. L. Pan, M. Saxon, W. Xu, D. Nathani, X. Wang, and W. Y. Wang. Automatically correcting large language models: Surveying the landscape of diverse automated correction strategies. Transactions of the Association for Computational Linguistics, 12:484506, 2024. doi: 10.1162/tacl_a_00660. URL https://aclanthology.org/2024.tacl-1.27/. J. S. Park, J. C. OBrien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents: Interactive simulacra of human behavior, 2023. S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez. Gorilla: Large language model connected with massive apis. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 126544126565. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/ 2024/file/e4c61f578ff07830f5c37378dd3ecb0d-Paper-Conference.pdf. T. Rebedea, R. Dinu, M. N. Sreedhar, C. Parisien, and J. Cohen. NeMo guardrails: toolkit for controllable and safe LLM applications with programmable rails. In Y. Feng and E. Lefever, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 431445, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-demo.40. URL https://aclanthology.org/2023.emnlp-demo. 40. T. Schick, J. Dwivedi-Yu, R. DessÃ¬, R. Raileanu, M. Tsvigun, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023a. T. Schick, J. Dwivedi-Yu, R. DessÃ¬, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools, 2023b. S. V. Schulhoff, J. Pinto, A. Khan, L.-F. Bouchard, C. Si, J. L. Boyd-Graber, S. Anati, V. Tagliabue, A. L. Kost, and C. R. Carnahan. Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through global prompt hacking competition. In Empirical Methods in Natural Language Processing, 2023. N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. 13 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z.-Y. Chen, J. Tang, X. Chen, Y. Lin, W. X. Zhao, Z. Wei, and J.-R. Wen. survey on large language model based autonomous agents, 2023. A. Wei, N. Haghtalab, and J. Steinhardt. Jailbroken: How does llm safety training fail? NeurIPS, 2023. Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou, et al. The rise and potential of large language model based agents: survey. arXiv preprint arXiv:2309.07864, 2023. Z. Xiang, L. Zheng, Y. Li, J. Hong, Q. Li, H. Xie, J. Zhang, Z. Xiong, C. Xie, C. Yang, D. Song, and B. Li. Guardagent: Safeguard llm agents by guard agent via knowledge-enabled reasoning, 2025. X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, and M. Kankanhalli. An llm can fool itself: prompt-based adversarial attack, 2023. URL https://arxiv.org/abs/2310.13345. S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models, 2023. H. Zhang, J. Huang, K. Mei, Y. Yao, Z. Wang, C. Zhan, H. Wang, and Y. Zhang. Agent security bench (asb): Formalizing and benchmarking attacks and defenses in llm-based agents. In The Thirteenth International Conference on Learning Representations, 2025. Y. Zhao, S. Chen, J. Zhang, and Z. Li. Recode: Improving llm-based code repair with fine-grained retrieval-augmented generation, 2025. URL https://arxiv.org/abs/2509.02330. S. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, T. Ou, Y. Bisk, D. Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. 14 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation A. Experiment Details This section details the implementation VeriGuard with agent systems mentioned in Section 4. A.1. Prompts Used To Generate the Policy Prompt: Policy Code Generation You are an expert AI security agent. Your primary function is to generate Python security policy function based on high-level user request. ## CONTEXT You are part of system that moderates target agents actions at runtime. This system works as follows: 1. user gives natural language security request (e.g., The agent cannot send emails to non-company addresses\"). 2. You receive this request and generate specific Python policy function to enforce it. 3. At runtime, before the target agent acts, its inputs and outputs are intercepted. 4. Parameters are extracted from this runtime data using the read_parameters_with_LLM tool. 5. The extracted parameters are passed to your generated policy function, which decides whether to allow or deny the action. **Runtime Data:** - agent_information: str # The text prompt input given to the agent, the agent output, and any available context. **Parameter Extraction Tool:** The system uses the read_parameters_with_LLM tool to extract relevant arguments from the agent_information. - Signature**: list_of_arguments_to_read: dict[str, type]) -> dict - **Example**: Given the text Send an email to alex@example.com and Dona Jhonson with the subject Update\" and list_of_arguments_to_read={{recipients\": list, subject\": str}}, the tool would return {{recipients\": [alex@example.com\", dona@example.com\"], subject\": read_parameters_with_LLM(text: Update\"}}. **Tool str, ## YOUR TASK Given agents security request and the target agent_specification, you must generate two outputs: list of parameters to extract, and the Python code for the policy function. ## OUTPUT REQUIREMENTS Your response must contain exactly two parts in the following order: **1. Arguments to Extract** Python dictionary named list_of_arguments_to_read that contains all the string arguments needed from the runtime data to evaluate the policy. Follow this exact format: ``` python list _ of _ arguments _ to _ read = { { \" argument1 \" : type1 , \" argument2 \" : type2 , \" argument3 \" : type3 } } ``` **2. Policy Function Header** ``` python 15 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation def policy ( arguments : dict ) -> tuple [ bool , str ]: ``` **3. Constant variables** You can define global variables needed to execute the function. ``` python variable1 = \" value1 \" variable2 = [ \" value2 \" ] ``` **4. Policy Function Code** Python function that implements the users security policy. You can create auxiliary functions as well, but the policy function must be named exactly policy. The function must adhere to these rules: - It must be named exactly policy. - It must accept one argument: arguments: - It must return tuple: (bool, str). The bool is the decision: True if the action is **allowed**, False if it is **denied**. - It must include clear docstring explaining the policy. - It must not use any external libraries. Note: Do not use python string functions to get parameters or arguments. Use the read_parameters_with_LLM tool instead. Follow this exact format: ``` python def policy ( arguments : dict ) -> tuple [ bool , str ]: dict. \" \" \" brief , one - sentence description of what this policy enforces . \" \" \" # Your policy logic here # ... if condition _ is _ met : return True , \" Reason for allowing . \" return False , \" Reason for denial . \" else : ``` Target Agent Specification: {agent_specification} User Request {request} Requirements # If available {requirements} Assumptions # If available {assumptions} Prompt: Constraint Generation You are senior **Security Systems Architect** with deep understanding of **Natural Language Understanding (NLU)** and formal logic. Your expertise lies in translating complex, human-written security policies into clear, unambiguous, and verifiable logical rules. ## Objective 16 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation Your task is to analyze given natural language security policy, an agents description, and set of defined variables/constants to generate set of discrete, testable logical statements. These statements should represent the core requirements of the policy and can be used to programmatically verify the agents behavior. ## Instructions 1. **Analyze the Inputs:** Carefully read the provided security policy, the agents description, and the list of variables and constants. Identify key actors, actions, conditions, and the specific capabilities of the agent that are relevant to the policy. 2. **Generate Logical Statements:** For each policy rule, create one or more logical statements. * Each statement should be complete, verifiable condition. * Use clear **IF-THEN** or similar logical structure. * Focus on verifiable conditions (e.g., \"The agents state is idle\") and outcomes (e.g., \"The agent must not respond to external commands\"). * Ensure the statements cover both \"positive\" (allowed) and \"negative\" (denied) cases, taking into account the agents specific functions. 3. **Provide Rationale:** After the list, briefly explain the logical structure and why these statements effectively capture the policys intent and apply to the described agent. 4. **Output Format:** Verifiable Logical Statements: (Give the statements in json format as:) ``` json [ { { \" statement \" : ... } } , { { \" statement \" : ... } } ... ] ` ` ` Rationale: (A brief explanation of the logical framework and how it aligns with the policy, the agents capabilities, and the provided variables.) ## Input: Security Policy Specification: {security policy} Agent Specification: {agents_description} Variables and Constants: {variables_and_constants} Requirements # If available {requirements} Assumptions # If available {assumptions} ## Output: Verifiable Logical Statements: 17 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation A.2. Prompts Used To Enforce the Policy Prompt: Argument Extraction You are an expert Natural Language Understanding (NLU) engine. Your task is to extract structured information from given text based on specification and return it as single Python dictionary. ## YOUR TASK You will be provided with block of input text, Python function for context, and specification of the parameters to extract. You must carefully read the **entire** input text, find the values for each required parameter, and format them into dictionary. **Instructions:** - Synthesize information from all parts of the input text to fill the parameters. - Ensure the data types in the output dictionary match the specification (e.g., list for list[str]). - Your output at the end should include your reasoning and valid Python dictionary. ## EXAMPLE GIVEN THIS CONTEXT: Python Function: ``` python def send _ email ( arguments : dict ) -> str : \" \" \" Send an email to the specified recipients with the given subject . \" \" \" # send email logic here # ... return \" Email sent successfully . \" ``` Agent Information: Input: Send an email to alex@example.com and Dona Jhonson with the subject Update Output: Step 1: Using retreive_email(\"Dona Jhonson\") we get the email \"dona@example.com\" AND THIS PARAMETER SPECIFICATION: {{\"recipients\": \"list[str]\", \"subject\": \"str\"}} PRODUCE THIS OUTPUT: **Reasoning** From the \"Agent Input\", found one recipient, \"alex@example.com\", and the subject, \"Updates\". From \"Agent Output\" Step 1, found the email for \"Dona Jhonson\" is \"dng@example.com\". will combine these into the recipients list. **Python dictionary** ``` python { \" recipients \" : [ \" alex@example . com \" , \" dona@example . com \" ] , \" subject \" : \" Update \" } ``` ## YOUR TURN GIVEN THIS CONTEXT: 18 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation Python Function: {function} Input Text: {text} AND THIS PARAMETER SPECIFICATION: {parameters} PRODUCE THIS OUTPUT: A.3. Prompts Used for Validation Prompt: Validation Analysis You are an expert **Natural Language Understanding (NLU)** and **logic engine**. Your primary function to verify logical statements. ## YOUR TASK Given users security specification and statements, you must analyze the specification in detail and then check if the logical statements is valid or needs correction. 1. Check if the user specification has ambiguity, needs clarification, for example co-references. 2. Check pre-assumptions for the statements. Focus on the specification. 4. Find contra examples. 5. Find any logical error in the statements. Output: After your analysis list all the points that require clarification or correction. User Specification {user_specification} Logical Statements {statements} Prompt: Validation Disambiguation You are an expert in **System Requirements**, **Security Policy**, and **Logical Deduction**. Your primary function is to act as an arbiter to resolve ambiguities identified in system analysis. You must review users security goals, the agents capabilities, and the provided analysis to establish definitive, clear, and reasonable set of system requirements and assumptions. ## YOUR TASK You are given high-level user_specification, the technical agent_specification, and an analysis that identifies points of ambiguity, conflict, or missing details. Your task is to: 1. Carefully examine each point raised in the analysis. 2. Use the user_specification as the primary source of intent and the agent_specification as the context for technical constraints. 3. For each point of ambiguity, make clear and logical **decision** to finalize the requirement or assumption. 4. Compile these decisions, along with any original unambiguous requirements, into single, comprehensive list of detailed requirements. VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation ## OUTPUT FORMAT Your response must contain two parts: **Part 1: Decisions on Ambiguities** For each point from the analysis, provide your decision in the following structured format: 1. [Title of the Point/Ambiguity] - Decision: [State your clear and final decision on the requirement or assumption.] - Justification: [Briefly explain *why* this decision is the most reasonable, referencing the user/agent specifications as needed.] 2. [Title of the Next Point/Ambiguity] - Decision: [...] - Justification: [...] **Part 2: Finalized Detailed Requirements List** After addressing all ambiguities, compile complete and final list of all detailed requirements (combining the original, clear requirements with your new decisions). 1. [Detailed Requirement 1] 2. [Detailed Requirement 2] 3. [Detailed Requirement 3] ## INPUTS User Specification {user_specification} Agent Specification {agent_specification} Analysis of Ambiguities {analysis} A.4. Prompts Used for Code Testing Prompt: Test Case Generation You are an expert at writing Pytest functions. Your task is to generate complete and effective test cases for given Python function, adhering to best practices. ## YOUR TASK Generate Pytest functions within single Python code block. The tests should be comprehensive, covering wide range of scenarios including: - **Happy Path:** Standard, valid inputs. - **Edge Cases:** Boundary conditions (e.g., empty strings, zero, negative numbers). - **Error Handling:** Cases that should raise specific exceptions. Use the following format for your output: ``` python # your generated test code here ``` User Request: {user_request} Requirements VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation {requirements} Assumptions {assumptions} Python function to test: {function_to_test} Test cases: Prompt: Policy Code Correction You are an expert Python developer and debugger. Your task is to analyze Python function and its corresponding pytest error message, identify the bug, and provide the corrected code. Python Function to Correct {function_to_test} Pytest Error Message {error_message} ## Your Task Analyze the function and the error message to find the source of the error. Explain the bug clearly and concisely. Provide the complete, corrected Python function. **Response Format** Bug Explanation (Describe the bug and the reason for the error here.) Corrected Function ``` python # Your corrected Python code here . ``` A.5. Prompts Used for Verification Prompt: Code Generation for Verification You are an expert in **formal methods** and **software verification**, specializing in Python. Your primary skill is translating requirements into precise **Nagini preand post-condition contracts**. ## Objective: Your task is to augment given Python function with Nagini contracts (Requires and Ensures) based on set of logical statements. You must ensure the generated code is syntactically correct and accurately reflects the logic of the provided statements. ## Instructions 1. **Analyze the Inputs:** Carefully review the provided Python function and the list of requirements given as logical statements. 2. **Translate Policies to Nagini:** For each logical statement, formulate the equivalent Nagini Requires (pre-conditions) or Ensures (post-conditions). 21 VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation 3. **Adhere to Grammar:** Strictly follow the provided Nagini grammar and refer to the examples for correct syntax and structure. 4. **Integrate and Output:** Embed the generated Nagini contracts directly into the Python function. ## Inputs Python Function: {python_function_code} Requirements: {list_of_logical_statements} Nagini Grammar Reference: {grammar} Nagini Examples: {examples} ## Output Format Provide the complete Python code for the function, including the newly added Nagini decorators, inside single Python code block."
        }
    ],
    "affiliations": [
        "Google Cloud AI",
        "Google Cloud AI Research",
        "Google DeepMind"
    ]
}