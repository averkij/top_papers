{
    "paper_title": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction",
    "authors": [
        "Jiewen Chan",
        "Zhenjun Zhao",
        "Yu-Lun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/"
        },
        {
            "title": "Start",
            "content": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction Jiewen Chan1 Zhenjun Zhao2 Yu-Lun Liu1 1National Yang Ming Chiao Tung University 2University of Zaragoza 6 2 0 J 2 ] . [ 1 6 9 7 0 0 . 1 0 6 2 : r Figure 1. State-of-the-art video reconstruction quality on DAVIS [65] dataset. Our Adaptive Gabor representation achieves superior rendering quality (PSNR: 35.49 dB, SSIM: 0.9433) while preserving fine details and temporal consistency. (Left) Qualitative comparisons demonstrate sharper textures in challenging regions (car windows, drum surface) compared to CoDeF [62] and Splatter Video [76]. (Right) Our method (red point) significantly outperforms recent baselines across all metrics, achieving 6.86 dB PSNR improvement over the second-best method with reasonable training time (circle size indicates training duration: 30 mins to 24 hours)."
        },
        {
            "title": "Abstract",
            "content": "Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/ 1. Introduction Reconstructing dynamic 3D scenes from monocular videos is fundamental challenge in computer vision with wide applications in VR, AR, and film production. The key difficulty lies in jointly achieving temporal continuity and rich frequency representation: real-world scenes demand smooth motion over time while preserving high-frequency textures that define appearance. Existing approaches [6, 30, 41, 66, 75, 102] fall into two camps. Gaussian-based primitives provide fast, explicit modeling but suffer from strong low-pass filtering, which suppresses high-frequency detail. Introducing frequency modulation [87] (e.g., Gabor-like representations) can enhance texture fidelity but often destabilizes energy balance and rendering quality. Moreover, many methods lack explicit temporal constraints, leading to motion discontinuities and geometric tearing, especially under rapid motion or occlusions. To address these gaps, we propose AdaGaR (Adaptive Gabor Representation for Dynamic Scene Reconstruction), unified framework that jointly optimizes time and frequency in explicit dynamic representations. Our core idea is to separate and yet tightly couple two orthogonal aspects: (i) frequency adaptivity via learnable Adaptive Gabor Representation that balances highand low-frequency components while maintaining energy stability; and (ii) temporal continuity via Cubic Hermite Splines with Temporal Curvature Regularization, constraining motion trajectories for smooth evolution. An Adaptive Initialization further bootstraps stable, temporally coherent geometry at early training. We validate AdaGaR on Tap-Vid [65], achieving stateof-the-art video reconstruction and strong generalization to frame interpolation, depth consistency, video editing, and stereo view synthesis, as shown in Fig. 1. This work provides compact, end-to-end solution for modeling both time and frequency in explicit dynamic representations, with potential to guide future developments in frequency-aware dynamic modeling. Our main contributions are summarized as follows: We propose novel Adaptive Gabor Representation that extends traditional Gaussians to the frequency domain, Fig. 2, which is (i) frequency-adaptive, (ii) energystable, and (iii) capable of capturing high-frequency texture details while automatically adjusting between high and low-frequency components according to scene requirements; We introduce Temporal Curvature Regularization with Cubic Hermite Spline interpolation, which accurately and effectively ensures geometric and motion continuity in the temporal dimension, achieving smooth temporal evolution and avoiding interpolation artifacts; We present an Adaptive Initialization mechanism that combines depth estimation, point tracking, and foreground masks to establish stable and temporally consistent point cloud distributions, significantly improving training efficiency and final reconstruction quality. 2. Related Work Dynamic 3D Gaussian Splatting. 3D Gaussian Splatting (3DGS)[38] has inspired extensive research on dynamic scene extensions. Early work[59] used timedependent MLPs for deformation. Recent canonical space approaches [2, 19, 27, 32, 53, 58, 85, 96] employ deformation networks to handle compression and specular dynamics. Temporal modeling strategies include flow-guided methods [105], neural features [49], temporal slicing [18], spatial-temporal regularization [13, 45], and hash encoding [89]. Specialized applications target autonomous driving [71, 91, 104], sparse reconstruction [61], unconstrained capture [40, 72], acceleration [77, 101], and motion blur [86]. Most similar to our work, SplineGS [63] applies Cubic Hermite splines in multi-view settings. In contrast, we combine Cubic Hermite splines with Gabor-based primitives for monocular videos without camera pose estimation, introducing Temporal Curvature Regularization for physically plausible motion. Frequency-Adaptive Rendering. Traditional 3D Gaussian kernels act as low-pass filters, limiting high-frequency detail representation. Anti-aliasing methods for 3DGS include multi-scale filtering [92, 98], analytical integration [51], and opacity field derivation [99]. NeRF frequency-aware approaches employ cone-tracing [3], frequency regularization [88, 93], frequency decomposition [25], and structurenoise separation [67]. Gabor representations in neural rendering [1, 83, 87] build on procedural graphics foundations [21, 44]. Alternative primitives include exponential functions [23], surfels [31], and Beta kernels [54]. However, existing Gabor approaches target static scenes with fixed frequencies. Our Adaptive Gabor Representation extends to dynamic videos with learnable frequency weights and graceful degradation to standard Gaussians. Temporal Modeling and Spline Representations. Classical splines [20, 55] provide smooth temporal interpolation. Recent neural rendering incorporates splines through Hermite formulations [15], B-splines [63, 82], and timemodulated weights [22]. Coarse-fine decomposition methods [2, 90, 95] separate temporal scales. Flow-guided approaches [50, 60, 103, 105] leverage optical flow constraints. Alternative temporal models include Kalman filtering [100], neural trajectories [42], frame interpolation [33, 69], and robust dynamic fields [57]. Unlike implicit smoothness from architecture or training, we explicitly enforce smoothness through Temporal Curvature Regularization based on secondorder derivatives, ensuring physically plausible motion with geometric interpretability. Video Representations and Canonical Spaces. Canonical space methods enable temporally consistent processing through layered atlases [36], deformation fields [11, 62], and canonical volumes [80]. Implicit neural video representations [8, 9, 39, 43, 47, 70, 73] achieve compression through image-wise functions. Explicit representations employ 4D Gaussians [85], 2D feature streams [78], layer decomposition [74], learned quantization [46], hash encoding [10], and scene inpainting [84]. Video Gaussian Splatting methods target monocular [6, 28, 30, 41, 52, 66, 75, 102] and multi-view [12, 56] settings. Our approach operates in an orthographic camera coordinate system [76], eliminating pose estimation while maintaining explicit 3D structure through Gabor primitives for high-frequency preservation and versatile applications. 2 is modeled as set of dynamic Adaptive Gabor primitives in an orthographic camera coordinate system, where spatial texture and structure are encoded by the primitives and temporal evolution is interpolated with Cubic Hermite Splines to guarantee geometric and temporal consistency. Adaptive Gabor Representation extends Gaussian primitives with learnable frequency weights and energy compensation, enabling frequency-adaptive detail capture while maintaining energy stability. Coupled with temporal curvature regularization and multi-supervision losses, our approach delivers high visual quality and robust temporal consistency, with strong applicability to frame interpolation, depth consistency, video editing, and related tasks. 4.2. Adaptive Gabor Video Representation Camera Coordinate Space. Inspired by [80] and [76], we adopt an orthographic camera coordinate system that maps width, height, and depth to the X, , and axes, enabling direct orthogonal representation of the 3D video structure. This avoids costly camera pose estimation and motion disentangling, treating camera motion and object motion as single type of dynamic variation. The video is represented as collection of dynamic adaptive Gabor primitives, each encoding spatial position, temporal variation, and frequency response, rendered from fixed identity pose. Adaptive Gabor Representation. To introduce highfrequency details on the image plane, the Gabor function can be viewed as periodic extension of the Gaussian function. Its general 2D form can be defined as: GGabor(x) = exp (cid:18) (cid:19) 1 2 µ2 Σ cos(f + ϕ), (1) where = (x, y) denotes the image plane coordinates, = (fx, fy) is the center frequency vector, and ϕ represents the phase offset. This structure introduces sinusoidal modulation within the Gaussian envelope, enabling the distribution to simultaneously capture local directional textures and high-frequency detail variations. To model richer frequency components, multiple Gabor waves can be combined into weighted superposition: S(x) = (cid:88) i=1 ωi cos(fidi, + ϕi), (2) where ωi denotes the amplitude weight, fi R+ represents the frequency magnitude, and di R2 with di2 = 1 is the frequency direction unit vector. This structure generates spatially periodic texture variations, which produce richer textural details when combined with Gaussians. While the Gabor structure enhances detail representation, fixed-amplitude cosine modulation disrupts the energy stability of the Gaussian. To address this, we propose Adaptive Figure 2. Hierarchical frequency adaptation. Our primitives adaptively transition from Gaussian (topleft) to Gabor (bottom), enabling coarse-to-fine reconstruction. Each primitive learns its optimal frequency response via learnable weights ωi, achieving both geometric stability and texture detail in unified framework. Monocular Depth and Motion Estimation. Foundation models provide robust monocular priors. Depth estimation methods [5, 29, 37, 48, 64, 68, 79, 94] achieve zero-shot generalization through synthetic training, diffusion repurposing, and multi-dataset learning. Point tracking methods [14, 16, 17, 24, 34, 35, 81] enable dense correspondence through pseudo-labeling, self-supervision, and local correlation. Unlike prior work using these signals independently, our adaptive initialization jointly reasons about depth, motion, and segmentation for geometrically and temporally consistent initialization. 3. Preliminary: 3D Gaussian Splatting 3D Gaussian Splatting (3DGS) [38] represents 3D scene as collection of parameterized Gaussian primitives {Gk = 1, . . . , }. Each Gk has center µk R3, covariance Σk R33, opacity αk [0, 1], and color ck. The density is Gk(x) = exp (cid:0) 1 2 (x µk)Σ (x µk)(cid:1) , with Σk = RkSkS . Rendering projects Gaussians onto the image plane and accumulates color via front-to-back blending: C(x) = (cid:88) k=1 Tkαkck, Tk = (1 αj). (cid:89) j<k key limitation is that single Gaussian acts as lowpass filter, constraining high-frequency textured detail. To address this, we introduce Gabor kernels as periodic extensions of Gaussians to enhance spatial frequency representation. 4. Method 4.1. Overview We present AdaGaR, an explicit 3D video representation that preserves high-frequency appearance while ensuring temporally smooth motion. As illustrated in Fig. 3, The video 3 Figure 3. Method overview. Our approach represents dynamic videos as Adaptive Gabor primitives with temporally smooth motion. (Input) Multi-modal supervision from RGB, depth, tracking, and masks. (Optimization) Two core components: (1) Adaptive Motion: Cubic Hermite splines model primitive trajectories with control points µ(t), q(t) in orthographic camera space, ensuring C1 continuity. (2) Adaptive Gabor Representation: Learnable frequency weights ωk enable primitives to adaptively span from Gaussian (low-freq) to Gabor (high-freq), achieving hierarchical detail reconstruction. (Loss) Joint optimization via RGB, depth, flow supervision, and curvature regularization Lcurv. (Application) Supports frame interpolation, depth consistency, and video editing. Gabor, which automatically adjusts the intensity based on the wave energy and naturally degrades to Gaussian in extreme cases. We extend the original opacity expression to αGabor = G(x) S(x). In practice, we set the phase terms ϕi = 0, yielding: S(x) = (cid:88) i=1 ωi cos(fidi, x), (3) where we fix the frequency parameters fi {1, 2}, corresponding to two orthogonal base frequency waveforms. The amplitude weights ωi [0, 1] are the introduced learnable parameters for the Gabor structure, adjusting the energy weights of different frequency components. The direction unit vectors di are shared with the spatial orientation of the original Gaussian, ensuring consistency between frequency modulation and Gaussian shape orientation. To prevent overall intensity attenuation when (cid:80) ωi < 1, we introduce compensation term b: Sadap(x) = + 1 N (cid:88) i=1 (cid:32) = γ + (1 γ) 1 ωi cos(fidi, x), (4) 4.3. Temporally Dynamic Adaptive Gabor Cubic Hermite Spline Interpolation. We use Cubic Hermite Splines [7, 26] to interpolate the temporal evolution of dynamic primitives. Given temporal keyframes at times {t0, t1, . . . , tM 1} with corresponding control point positions {y0, y1, . . . , yM 1} R3, we define the time interval between adjacent keyframes as = tk+1 tk, and the slope as δk = (yk+1 yk)/k. To avoid unnecessary oscillations between keyframes, we introduce an auto-slope mechanism with monotone gate: mk = (cid:40) β δk1+δk 0, , if sign(δk1) = sign(δk), otherwise, (6) where β (0, 1] is smoothness coefficient controlling the flatness of the interpolation curve. This design prevents reverse oscillations at keyframes and ensures visually stable interpolation. The Hermite basis functions are defined as: H00(s) = 2s3 3s2 + 1, H10(s) = s3 2s2 + s, H01(s) = 2s3 + 3s2, H11(s) = s3 s2, (7) (cid:33) ωi , 1 (cid:88) i=1 (5) where = (t tk)/k [0, 1] is the normalized time within the interval [tk, tk+1]. The interpolated displacement at time is: where γ [0, 1] is fixed hyperparameter controlling the degradation smoothness, and the factor 1/N normalizes the weighted average of multiple waves to stable range. When ωi 0, we have 1, and the formulation naturally degrades to traditional Gaussian. (t) = H00(s)yk + H10(s)kmk + H01(s)yk+1 + H11(s)kmk+1. (8) To ensure consistent geometric continuity, the final position is obtained by adding the interpolated displacement to 4 (a) G(x),Gaussian, S(x), sinusoid part (b) The effect of different combinations of Gabor wave coefficients (ω0 and ω1) on spatial frequency textures. Figure 4. Adaptive Gabor formulation. (a) Smooth transition between Gaussian and Gabor kernels. Our method (rightmost column, Sours(x)) uses compensation term to maintain energy stability while transitioning from pure Gaussian (ω = 0, top) to frequencymodulated Gabor (ω = 1, bottom). Naive combination 1 + S(x) (third column) suffers from intensity artifacts. (b) Frequency weight combinations. Different (ω0, ω1) pairs generate diverse spatial patterns, from smooth (low ω) to high-frequency textures (high ω), enabling adaptive detail capture in different scene regions. base position: 4.4. Optimization µ(t) = µbase + (t). (9) Rotation Interpolation. We extend the same principle to temporal interpolation of rotations. For rotation parameters, we first interpolate in the so(3) Lie algebra space, then convert to unit quaternions via the exponential map: q(t) = normalize(cid:0)normalize(qbase)exp(q(t))(cid:1), (10) where denotes quaternion multiplication, and angle wrapping ensures rotation angles remain within (π, π]. Temporal Curvature Regularization. To enforce smooth temporal evolution, we introduce curvature penalty on the trajectory at each keyframe. For non-uniform keyframes, the second-order derivative is estimated as = 2(d+ ) hk1 + hk , (11) with hk1 = tk tk1, hk = tk+1 tk, d+ yk)/hk, ture loss is = (yk+1 = (yk yk1)/hk1, and = 3. The curvaLcurve = (cid:80)M 2 k=1 wky 2 2 k=1 wkD + ε (cid:80)M 2 , (12) where wk = 1 2 (hk1 + hk) and ε > 0 is small constant. This term enforces smoothness by penalizing the secondorder energy along time. 5 To maintain both realistic appearance and temporal stability in dynamic scenes, we employ multi-objective loss function that constrains appearance fidelity, motion consistency, depth geometry, and temporal smoothness. Rendering Reconstruction Loss. We combine L1 and SSIM to preserve both pixel-level accuracy and structural features: Lrgb(It, ˆIt) = (1 λssim)Lrgb ssim(It, ˆIt), (13) where It and ˆIt denote the ground-truth and predicted images at frame t, respectively. 1 (It, ˆIt) + λssimLrgb Optical Flow Consistency Loss. We leverage CoTracker [34] to provide cross-frame supervision. The projected positions of Adaptive Gabor primitives are aligned with 2D trajectories using visibility-weighted L1 loss: Lflow( ˆFt1,t2 , Ft1,t2) = (cid:80) wj (cid:80) (cid:13) (cid:13)ˆxj t2 xj (cid:13) t2 wj + ε (cid:13) (cid:13) (cid:13)1 , (14) t2 and ˆxj where xj t2 are the ground-truth and predicted pixel positions of the j-th tracked point at frame t2, and wj denotes its visibility weight. Depth Loss. We use monocular depth estimates from DPT [68] as geometric priors with scaleand shift-invariant alignment: Ldepth(Dt, ˆDt) = (cid:13) (cid:13) (cid:13)γ(Dt) γ( ˆDt) (cid:13) (cid:13) (cid:13) , (15) where γ(Dt) = (Dt ct(Dt))/Dt ct(Dt)1 with ct(Dt) = median(Dt). Total Loss. The overall optimization objective combines all components: Ltotal = λrgbLrgb + λflowLflow + λdepthLdepth + λcurvLcurv. (16) This multi-faceted supervision enables AdaGaR to achieve both high-fidelity rendering and temporally stable dynamic scene representation. 4.5. Adaptive Initialization We propose an adaptive initialization to initialize temporally coherent 3D point distribution early in training. It fuses multi-modal cues to generate dense, dynamic initial point cloud from the input video, forming the geometric basis for subsequent explicit representations. Unlike random sampling or single-frame methods, our approach adaptively adjusts sampling density according to scene motion and depth distribution, ensuring balanced foreground/background coverage. TemporalSpatial Adaptive Sampling. For each candidate point pi, the sampling probability is Π(pi) 1 τi + ϵ + λτ 1 ρi + ϵ , (17) where τi is the temporal support, ρi the local density, λτ [0, 1] balances temporal stability and spatial uniformity, and ϵ > 0. Grid-Based Uniform Coverage. To ensure global coverage, we partition the image into fixed grid = {Gu,v} and modulate per-cell sampling by Table 1. Quantitative results on Tap-Vid DAVIS [65]. Our method achieves state-of-the-art performance across all metrics, with 6.86 dB PSNR improvement over the previous best method [76], validating our frequency-adaptive primitives with smooth temporal modeling. Method PSNR SSIM LPIPS 4DGS [86] RoDynRF [57] Deformable Sprites [97] Omnimotion [80] CoDeF [62] Splatter Video [76] Ours 18.12 24.79 22.83 24.11 26.17 28.63 35. 0.5735 0.7230 0.6983 0.7145 0.8160 0.8373 0.9433 0.5130 0.3940 0.3014 0.3713 0.2905 0.2283 0.0723 Implementation Details. The training consists of two stages: 500-iteration warm-up and 10K iterations for main optimization, with control points updated every 100 iterations. Experiments run on an NVIDIA RTX 4090, 90 minutes per video sequence. Video Reconstruction. As shown in Tab. 1, our method outperforms the baselines across PSNR/SSIM/LPIPS on TapVid DAVIS [65]. Compared with MLP-based representations, ours yields sharper textures and coherent motion in Fig. 5. 5.2. Applications Depth Consistency. We achieve stable depth distributions over time, substantially reducing depth flicker and boundary misalignment, and outperforming per-frame optimizers, as shown in Fig. 6. Frame Interpolation. We generate smooth intermediate frames between keyframes using cubic Hermite splines with curvature regularization, preserving texture detail and avoiding boundary artifacts, as shown in Fig. 7. Π(pi Gu,v) = Π(pi) 1 + λgCu,v , with Cu,v the cells cumulative samples and λg > 0. (18) Video Editing. In canonical space, style transfers remain temporally coherent by acting on shared Adaptive Gabor primitives, reducing style drift and flicker, as shown in Fig. 8. Boundary-Aware Compensation. We further adjust for motion boundaries via Π(pi) = Π(pi Gu,v) (1 + λbMt(pi)) , (19) where Mt is the foreground mask and λb > 0. This scheme yields dense, temporally coherent initial point cloud and reduces early-stage flickering. 5. Experiment 5.1. Evaluation Stereo View Synthesis. Our explicit representation supports stereo synthesis from monocular input, with improved disparity consistency and plausible geometry, as shown in Fig. 9. 5.3. Ablation Study Adaptive Gabor Representation. We compare Adaptive Gabor Representation (AGR) to Gaussian and standard Gabor, using the same 1M primitives. As shown in Tab. 2, AGR improves high-frequency detail and energy stability, yielding the best PSNR/SSIM/LPIPS among the three configurations. Dataset and Metrics. We evaluate on Tap-Vid DAVIS [65], featuring diverse dynamic scenes and occlusions. Quantitative metrics include PSNR, SSIM, and LPIPS to assess pixel accuracy, structural fidelity, and perceptual quality. Spline Interpolation. We ablate curve interpolation on 50 frames in Tab. 3. B-Spline and Cubic Spline provide some temporal continuity but struggle with nonlinear motion. In contrast, the proposed Cubic Hermite Spline achieves the 6 Figure 5. Visual comparison on DAVIS dataset. Our method preserves finer details (fur, vehicle edges, wheel structures) and sharper motion boundaries compared to CoDeF [62] and Splatter Video [76]. Red boxes highlight key regions demonstrating our superior texture reconstruction and temporal consistency. Best viewed zoomed in. Table 2. Gabor primitive ablation. Our Adaptive Gabor with compensation term outperforms standard Gaussian, naive Gabor variants, validating that energy-aware formulation (Eq. (5)) is crucial for stable frequency modeling. Method PSNR SSIM LPIPS Gaussian Standard Gabor (b = 0) 1 + S(x) Adaptive Gabor (Ours) 36.66 36.65 36.50 37.43 0.9423 0.9543 0.9511 0. 0.0421 0.0345 0.0322 0.0242 Table 3. Spline method ablation. Our Cubic Hermite Spline with monotone gate outperforms B-Spline and significantly surpasses standard Cubic Spline, which suffers from trajectory oscillations. Explicit velocity control (Eq. (6)) is essential for smooth, artifactfree motion modeling. Methods PSNR SSIM LPIPS B-Spline Cubic Spline Cubic Hermite Spline (Ours) 36.68 32.42 38.98 0.9573 0.9073 0.9697 0.0368 0.0818 0.0259 Figure 6. Depth consistency across time. (Left) Our 3D primitive representation maintains consistent depth for static elements across frames. (Right) While per-frame estimation (Marigold [37]) shows temporal flickering (red boxes). Explicit 3D geometry with smooth motion modeling ensures temporal coherence essential for depthbased video applications. best performance across metrics, with smoother trajectories and preserved dynamic details. Curvature Regularization. We compare with/without the temporal curvature term Lcurv in Fig. 10. Without Lcurv, motion artifacts and tearing appear, while, with Lcurv, interpolation is smoother and more stable, confirming the necessity 7 Figure 7. Frame interpolation results. Our method generates temporally smooth intermediate frames between input keyframes and + 1 by querying Cubic Hermite splines at fractional timestamps. The interpolated sequence (1st through 4th frames) maintains consistent fur texture details and natural motion without ghosting artifacts. Red boxes show the preservation of high-frequency details throughout the interpolation. This demonstrates our methods ability to produce continuous motion with 1 smoothness via curvature-regularized spline trajectories. Please refer to the supplementary video for full temporal coherence. Figure 8. Temporally consistent video editing. (Top) Per-frame editing causes temporal flickering with inconsistent styles between frames. (Bottom) Our canonical space editing maintains temporal consistency by applying style transfer to shared Adaptive Gabor primitives, ensuring identical treatment of scene elements across time while preserving motion dynamics. Red boxes highlight key differences. Please see the supplementary video. Figure 9. Stereo view synthesis. Our 3D representation enables novel view synthesis for stereo visualization from monocular video. This demonstrates that Adaptive Gabor primitives in orthographic camera coordinate space capture accurate 3D geometry, enabling immersive applications. of explicit curvature control. Adaptive Initialization. We compare random initialization with our adaptive initialization. As shown in Fig. 11, the adaptive approach yields denser, temporally coherent initial geometry, reducing flicker and improving early reconstruction quality. 6. Conclusion We present AdaGaR, unified framework for temporal continuity and frequency adaptivity in dynamic scene modeling. By extending Gaussian primitives to Adaptive Gabor 8 Figure 10. Curvature regularization ablation. (Left) Without Lcurv, interpolated frames show motion artifacts from trajectory oscillations. (Right) Our method produces smooth, artifact-free interpolation by constraining second-order derivatives, validating the necessity of explicit curvature control for temporal consistency. Figure 11. Adaptive initialization ablation. (Left) Without motionaware initialization, primitives are poorly distributed, causing blurred details. (Right) Our adaptive initialization based on depth, tracking, and masks (Eqs. (17) to (19)) provides better initial geometry, yielding 6.78 dB improvement and sharp reconstruction. Representation and employing Cubic Hermite Splines with Temporal Curvature Regularization, our approach captures high-frequency details while ensuring geometric and motion continuity. Experiments demonstrate state-of-the-art performance on Tap-Vid DAVIS with strong generalization across frame interpolation, depth consistency, video editing, and stereo synthesis. Limitations. Despite superior performance, AdaGaR has limitations. The spline-based motion modeling assumes smooth trajectories, potentially causing misalignment under abrupt or highly nonlinear motion. Additionally, Adaptive Gabor Representation may exhibit oscillations in highfrequency regions due to energy constraints. Future work could introduce adaptive temporal control points and motionaware frequency modulation. Acknowledgements. This research was funded by the National Science and Technology Council, Taiwan, under Grants NSTC 112-2222-E-A49-004-MY2 and 113-2628E-A49-023-. The authors are grateful to Google, NVIDIA, and MediaTek Inc. for their generous donations. Yu-Lun Liu acknowledges the Yushan Young Fellow Program by the MOE in Taiwan."
        },
        {
            "title": "References",
            "content": "[1] Ahmad AlMughrabi, Ricardo Marques, and Petia Radeva. Momentsnerf: Leveraging orthogonal moments for few-shot neural rendering. arXiv preprint arXiv:2407.02668, 2024. 2 [2] Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, and Youngjung Uh. Per-gaussian embeddingbased deformation for deformable 3d gaussian splatting. In European Conference on Computer Vision, pages 321335. Springer, 2024. 2 [3] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased In Proceedings of the grid-based neural radiance fields. IEEE/CVF International Conference on Computer Vision, pages 1969719705, 2023. 2 [4] Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic arXiv preprint neurons for conditional computation. arXiv:1308.3432, 2013. 14 [5] Aleksei Bochkovskii, Ama AG Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv preprint arXiv:2410.02073, 2024. 3 [6] Minh-Quan Viet Bui, Jongmin Park, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, and Munchurl Kim. Mobgs: Motion deblurring dynamic 3d gaussian splatting for blurry monocular video. arXiv preprint arXiv:2504.15122, 2025. 1, 2 [7] AKB Chand and Viswanathan. Cubic hermite and cubic spline fractal interpolation functions. In AIP conference Proceedings, pages 14671470. American Institute of Physics, 2012. [8] Hao Chen, Bo He, Hanyu Wang, Yixuan Ren, Ser Nam Lim, and Abhinav Shrivastava. Nerv: Neural representations for videos. Advances in Neural Information Processing Systems, 34:2155721568, 2021. 2 [9] Hao Chen, Matthew Gwilliam, Ser-Nam Lim, and Abhinav Shrivastava. Hnerv: hybrid neural representation for videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10270 10279, 2023. 2 [10] Jie Chen, Zhangchi Hu, Peixi Wu, Huyue Zhu, Hebei Li, and Xiaoyan Sun. Dash: 4d hash encoding with self-supervised decomposition for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2634926359, 2025. 2 refined canonical image with integration of diffusion prior for video editing. Advances in Neural Information Processing Systems, 37:3609736120, 2024. 2 [12] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision, pages 370386. Springer, 2024. 2 [13] Hao-Jen Chien, Yi-Chuan Huang, Chung-Ho Wu, Wei-Lun Chao, and Yu-Lun Liu. Splannequin: Freezing monocular mannequin-challenge footage with dual-detection splatting. arXiv preprint arXiv:2512.05113, 2025. [14] Seokju Cho, Jiahui Huang, Jisu Nam, Honggyu An, Seungryong Kim, and Joon-Young Lee. Local all-pair correspondence for point tracking. In ECCV, pages 306325, 2024. 3 [15] Ilya Chugunov, David Shustin, Ruyu Yan, Chenyang Lei, and Felix Heide. Neural spline fields for burst image fusion and layer separation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2576325773, 2024. 2 [16] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. Tapir: Tracking any point with per-frame initialization and temporal refinement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10061 10072, 2023. 3 [17] Carl Doersch, Pauline Luc, Yi Yang, Dilara Gokay, Skanda Koppula, Ankush Gupta, Joseph Heyward, Ignacio Rocco, Ross Goroshin, Joao Carreira, et al. Bootstap: Bootstrapped training for tracking-any-point. In Proceedings of the Asian Conference on Computer Vision, pages 32573274, 2024. 3 [18] Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, and Baoquan Chen. 4d-rotor gaussian splatting: towards efficient novel view synthesis for dynamic scenes. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2 [19] Cheng-De Fan, Chen-Wei Chang, Yi-Ruei Liu, Jie-Ying Lee, Jiun-Long Huang, Yu-Chee Tseng, and Yu-Lun Liu. Spectromotion: Dynamic 3d reconstruction of specular scenes. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2132821338, 2025. 2 [20] Gerald Farin. Curves and surfaces for CAGD: practical guide. Morgan Kaufmann, 2002. 2 [21] Bruno Galerne, Ares Lagae, Sylvain Lefebvre, and George Drettakis. Gabor noise by example. ACM Transactions on Graphics (ToG), 31(4):19, 2012. 2 [22] Ivan Grega, William Whitney, and Vikram Deshpande. arXiv Neural rendering enables dynamic tomography. preprint arXiv:2410.20558, 2024. 2 [23] Abdullah Hamdi, Luke Melas-Kyriazi, Jinjie Mai, Guocheng Qian, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, and Andrea Vedaldi. Ges: Generalized exponential splatting In Proceedings of for efficient radiance field rendering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1981219822, 2024. 2 [11] Ting-Hsuan Chen, Jie Wen Chan, Hau-Shiang Shiu, ShihHan Yen, Changhan Yeh, and Yu-Lun Liu. Narcan: Natural [24] Adam Harley, Yang You, Xinglong Sun, Yang Zheng, Nikhil Raghuraman, Yunqi Gu, Sheldon Liang, Wen-Hsuan 9 Chu, Achal Dave, Suya You, et al. Alltracker: Efficient In Proceedings dense point tracking at high resolution. of the IEEE/CVF International Conference on Computer Vision, pages 52535262, 2025. 3 [25] Yisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, and Qixing Huang. Freditor: High-fidelity and transferable nerf editing by frequency decomposition. In European Conference on Computer Vision, pages 7391. Springer, 2024. 2 [26] Niels Hintzen, Gerjan Piet, and Thomas Brunel. Improved estimation of trawling tracks using cubic hermite spline interpolation of position registration data. Fisheries research, 101(1-2):108115, 2010. 4 [27] Cheng-Yuan Ho, He-Bi Yang, Jui-Chiu Chiang, Yu-Lun Liu, and Wen-Hsiao Peng. Ted-4dgs: Temporally activated and embedding-based deformation for 4dgs compression. arXiv preprint arXiv:2512.05446, 2025. [28] Hao-Yu Hou, Chia-Chi Hsu, Yu-Chen Huang, Mu-Yi Shen, Wei-Fang Sun, Cheng Sun, Chia-Che Chang, Yu-Lun Liu, and Chun-Yi Lee. 3d gaussian splatting with grouped uncertainty for unconstrained images. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. 2 [29] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 3 [30] Shoukang Hu, Tao Hu, and Ziwei Liu. Gauhuman: Articulated gaussian splatting from monocular human videos. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2041820431, 2024. 1, 2 [31] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 conference papers, pages 111, 2024. 2 [32] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 42204230, 2024. 2 [33] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Real-time intermediate flow estimation for video frame interpolation. In European Conference on Computer Vision, pages 624642. Springer, 2022. 2 [34] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In ECCV, pages 1835, 2024. 3, 5 [35] Nikita Karaev, Yuri Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudolabelling real videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 60136022, 2025. [36] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video editing. ACM Transactions on Graphics (TOG), 40(6):112, 2021. 2 [37] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 94929502, 2024. 3, 7 [38] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3 [39] Jina Kim, Jihoo Lee, and Je-Won Kang. Snerv: SpectraIn European preserving neural representation for video. Conference on Computer Vision, pages 332348. Springer, 2024. 2 [40] Mijeong Kim, Jongwoo Lim, and Bohyung Han. 4d gaussian splatting in the wild with uncertainty-aware regularization. Advances in Neural Information Processing Systems, 37: 129209129226, 2024. 2 [41] Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, and Anurag Ranjan. Hugs: Human gaussian In Proceedings of the IEEE/CVF conference on splats. computer vision and pattern recognition, pages 505515, 2024. 1, [42] Agelos Kratimenos, Jiahui Lei, and Kostas Daniilidis. Dynmf: Neural motion factorization for real-time dynamic view synthesis with 3d gaussian splatting. In European Conference on Computer Vision, pages 252269. Springer, 2024. 2 [43] Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, and David Bull. Nvrc: Neural video representation compression. Advances in Neural Information Processing Systems, 37: 132440132462, 2024. 2 [44] Ares Lagae, Sylvain Lefebvre, George Drettakis, and Philip Dutre. Procedural noise using sparse gabor convolution. ACM Transactions on Graphics (TOG), 28(3):110, 2009. 2 [45] Deqi Li, Shi-Sheng Huang, Zhiyuan Lu, Xinran Duan, and Hua Huang. St-4dgs: Spatial-temporally consistent 4d gaussian splatting for efficient dynamic scene rendering. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2 [46] Jiahao Li, Bin Li, and Yan Lu. Neural video compression with feature modulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2609926108, 2024. 2 [47] Zizhang Li, Mengmeng Wang, Huaijin Pi, Kechun Xu, Jianbiao Mei, and Yong Liu. E-nerv: Expedite neural video representation with disentangled spatial-temporal context. In European Conference on Computer Vision, pages 267 284. Springer, 2022. 2 [48] Zhenyu Li, Shariq Farooq Bhat, and Peter Wonka. Patchfusion: An end-to-end tile-based framework for highresolution monocular metric depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1001610025, 2024. 3 [49] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. 10 In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85088520, 2024. 2 [50] Feng Liang, Bichen Wu, Jialiang Wang, Licheng Yu, Kunpeng Li, Yinan Zhao, Ishan Misra, Jia-Bin Huang, Peizhao Zhang, Peter Vajda, et al. Flowvid: Taming imperfect optical flows for consistent video-to-video synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82078216, 2024. 2 [51] Zhihao Liang, Qi Zhang, Wenbo Hu, Lei Zhu, Ying Feng, and Kui Jia. Analytic-splatting: Anti-aliased 3d gaussian splatting via analytic integration. In European conference on computer vision, pages 281297. Springer, 2024. 2 [52] Chin-Yang Lin, Cheng Sun, Fu-En Yang, Min-Hung Chen, Yen-Yu Lin, and Yu-Lun Liu. Longsplat: Robust unposed 3d gaussian splatting for casual long videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2741227422, 2025. 2 [53] Youtian Lin, Zuozhuo Dai, Siyu Zhu, and Yao Yao. Gaussian-flow: 4d reconstruction with dynamic 3d gaussian particle. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21136 21145, 2024. 2 [54] Rong Liu, Dylan Sun, Meida Chen, Yue Wang, and Andrew Feng. Deformable beta splatting. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 111, 2025. 2 [55] Songrun Liu, Alec Jacobson, and Yotam Gingold. Skinning cubic bezier splines and catmull-clark subdivision surfaces. ACM Transactions on Graphics (TOG), 33(6):19, 2014. 2 [56] Tianqi Liu, Guangcong Wang, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, and Ziwei Liu. Mvsgaussian: Fast generalizable gaussian splatting reconstruction from multi-view stereo. In European Conference on Computer Vision, pages 3753. Springer, 2024. 2 [57] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1323, 2023. 2, [58] Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, and Yuchao Dai. 3d geometry-aware deformable gaussian splatting for dynamic view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 89008910, 2024. 2 [59] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In 2024 International Conference on 3D Vision (3DV), pages 800809. IEEE, 2024. 2 [60] Wan-Duo Kurt Ma, John Lewis, and Bastiaan Kleijn. Trailblazer: Trajectory control for diffusion-based video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 2 fields: Neural gaussian splats for sparse 3d and 4d reconIn European Conference on Computer Vision, struction. pages 313332. Springer, 2024. 2 [62] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen, and Yujun Shen. Codef: Content deformation fields for temporally consistent video processing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80898099, 2024. 1, 2, 6, 7 [63] Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, and Munchurl Kim. Splinegs: Robust motion-adaptive spline for real-time dynamic 3d gaussians from monocular video. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2686626875, 2025. 2 [64] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: Universal monocular metric depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1010610116, 2024. [65] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 1, 2, 6 [66] LIU Qingming, Yuan Liu, Jiepeng Wang, Xianqiang Lyu, Peng Wang, Wenping Wang, and Junhui Hou. Modgs: Dynamic gaussian splatting from casually-captured monocular videos with depth priors. In The Thirteenth International Conference on Learning Representations, 2025. 1, 2 [67] Zefan Qu, Ke Xu, Gerhard Petrus Hancke, and Rynson WH Lau. Lush-nerf: Lighting up and sharpening nerfs for lowlight scenes. arXiv preprint arXiv:2411.06757, 2024. 2 [68] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1217912188, 2021. 3, 5 [69] Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline Pantofaru, and Brian Curless. Film: Frame interpolation for large motion. In European Conference on Computer Vision, pages 250266. Springer, 2022. 2 [70] Jens Eirik Saethre, Roberto Azevedo, and Christopher Schroers. Combining frame and gop embeddings for neural video representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 92539263, 2024. 2 [71] Mu-Yi Shen, Chia-Chi Hsu, Hao-Yu Hou, Yu-Chen Huang, Wei-Fang Sun, Chia-Che Chang, Yu-Lun Liu, and Chun-Yi Lee. Driveenv-nerf: Exploration of nerf-based autonomous driving environment for real-world performance validation. arXiv preprint arXiv:2403.15791, 2024. 2 [72] Meng-Li Shih, Ying-Huan Chen, Yu-Lun Liu, and Brian Curless. Prior-enhanced gaussian splatting for dynamic scene reconstruction from casual video. In Proceedings of the SIGGRAPH Asia 2025 Conference Papers, pages 113, 2025. 2 [61] Marko Mihajlovic, Sergey Prokudin, Siyu Tang, Robert Maier, Federica Bogo, Tony Tung, and Edmond Boyer. Splat- [73] Seungjun Shin, Suji Kim, and Dokwan Oh. Efficient neural video representation with temporally coherent modulation. 11 In European Conference on Computer Vision, pages 179 195. Springer, 2024. 2 [74] Gaurav Shrivastava, Ser-Nam Lim, and Abhinav Shrivastava. Video decomposition prior: methodology to decompose videos into layers. arXiv preprint arXiv:2412.04930, 2024. 2 [75] Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, and Leonidas Guibas. Dynamic gaussian marbles for novel view synthesis of casual monocular videos. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 1, 2 [76] Yang-Tian Sun, Yihua Huang, Lin Ma, Xiaoyang Lyu, YanPei Cao, and Xiaojuan Qi. Splatter video: Video gaussian representation for versatile processing. Advances in Neural Information Processing Systems, 37:5040150425, 2024. 1, 2, 3, 6, [77] Allen Tu, Haiyang Ying, Alex Hanson, Yonghan Lee, Tom Goldstein, and Matthias Zwicker. Speedy deformable 3d gaussian splatting: Fast rendering and compression of dynamic scenes. arXiv preprint arXiv:2506.07917, 2025. 2 [78] Liao Wang, Kaixin Yao, Chengcheng Guo, Zhirui Zhang, Qiang Hu, Jingyi Yu, Lan Xu, and Minye Wu. Videorf: Rendering dynamic radiance fields as 2d feature video streams. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 470481, 2024. 2 [79] Ning-Hsu Albert Wang and Yu-Lun Liu. Depth anywhere: Enhancing 360 monocular depth estimation via perspective distillation and unlabeled data augmentation. Advances in Neural Information Processing Systems, 37:127739127764, 2024. 3 [80] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, and Noah Snavely. Tracking everything everywhere all at once. In ICCV, pages 1979519806, 2023. 2, 3, 6 [81] Yihan Wang, Lahav Lipson, and Jia Deng. Sea-raft: Simple, efficient, accurate raft for optical flow. In European Conference on Computer Vision, pages 3654. Springer, 2024. 3 [82] Yikai Wang, Xinzhou Wang, Zilong Chen, Zhengyi Wang, Fuchun Sun, and Jun Zhu. Vidu4d: Single generated video to high-fidelity 4d reconstruction with dynamic gaussian surfels. Advances in Neural Information Processing Systems, 37:131316131343, 2024. 2 [83] Haato Watanabe, Kenji Tojo, and Nobuyuki Umetani. 3d gabor splatting: Reconstruction of high-frequency surface texture using gabor noise. arXiv preprint arXiv:2504.11003, 2025. [84] Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Jie-Ying Lee, Bo-Hsu Ke, Chun-Wei Tuan Mu, Yi-Chuan Huang, Chin-Yang Lin, Min-Hung Chen, Yen-Yu Lin, et al. Aurafusion360: Augmented unseen region alignment for referencebased 360deg unbounded scene inpainting. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1636616376, 2025. 2 [85] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2031020320, 2024. 2 [86] Renlong Wu, Zhilu Zhang, Mingyang Chen, Zifei Yan, and Wangmeng Zuo. Deblur4dgs: 4d gaussian splatting from blurry monocular video. arXiv preprint arXiv:2412.06424, 2024. 2, 6 [87] Skylar Wurster, Ran Zhang, and Changxi Zheng. Gabor splatting for high-quality gigapixel image representations. In ACM SIGGRAPH 2024 Posters, pages 12. 2024. 1, 2 [88] Shuxiang Xie, Shuyi Zhou, Ken Sakurada, Ryoichi Ishikawa, Masaki Onishi, and Takeshi Oishi. 2 r: Frequency regularization in grid-based feature encoding neural radiance fields. In European Conference on Computer Vision, pages 186203. Springer, 2024. 2 [89] Jiawei Xu, Zexin Fan, Jian Yang, and Jin Xie. Grid4d: 4d decomposed hash encoding for high-fidelity dynamic gaussian splatting. Advances in Neural Information Processing Systems, 37:123787123811, 2024. [90] Hao Yan, Zhihui Ke, Xiaobo Zhou, Tie Qiu, Xidong Shi, and Dadong Jiang. Ds-nerv: Implicit neural video representation with decomposed static and dynamic codes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2301923029, 2024. 2 [91] Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, and Sida Peng. Street gaussians: Modeling dynamic urban scenes with gaussian splatting. In European Conference on Computer Vision, pages 156173. Springer, 2024. 2 [92] Zhiwen Yan, Weng Fei Low, Yu Chen, and Gim Hee Lee. Multi-scale 3d gaussian splatting for anti-aliased rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2092320931, 2024. 2 [93] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regularization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 82548263, 2023. 2 [94] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1037110381, 2024. 3 [95] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Real-time photorealistic dynamic scene representation and arXiv preprint rendering with 4d gaussian splatting. arXiv:2310.10642, 2023. 2 [96] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2033120341, 2024. [97] Vickie Ye, Zhengqi Li, Richard Tucker, Angjoo Kanazawa, and Noah Snavely. Deformable sprites for unsupervised In Proceedings of the IEEE/CVF video decomposition. Conference on Computer Vision and Pattern Recognition, pages 26572666, 2022. 6 12 [98] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1944719456, 2024. 2 [99] Zehao Yu, Torsten Sattler, and Andreas Geiger. Gaussian opacity fields: Efficient adaptive surface reconstruction in unbounded scenes. ACM Transactions on Graphics (ToG), 43(6):113, 2024. 2 [100] Yifan Zhan, Zhuoxiao Li, Muyao Niu, Zhihang Zhong, Shohei Nobuhara, Ko Nishino, and Yinqiang Zheng. Kfdnerf: Rethinking dynamic nerf with kalman filter. In European Conference on Computer Vision, pages 118. Springer, 2024. 2 [101] Yu-Ting Zhan, Cheng-Yuan Ho, Hebi Yang, Yi-Hsin Chen, Jui Chiu Chiang, Yu-Lun Liu, and Wen-Hsiao Peng. Cat-3dgs: context-adaptive triplane approach to ratedistortion-optimized 3dgs compression. arXiv preprint arXiv:2503.00357, 2025. [102] Tingyang Zhang, Qingzhe Gao, Weiyu Li, Libin Liu, and Baoquan Chen. Bags: Building animatable gaussian splatting from monocular video with diffusion priors. arXiv preprint arXiv:2403.11427, 2024. 1, 2 [103] Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, and Chen Change Loy. Upscale-a-video: Temporalconsistent diffusion model for real-world video superresolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2535 2545, 2024. 2 [104] Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, and Ming-Hsuan Yang. Drivinggaussian: Composite gaussian splatting for surrounding dynamic autonomous driving scenes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 21634 21643, 2024. 2 [105] Ruijie Zhu, Yanzhe Liang, Hanzhi Chang, Jiacheng Deng, Jiahao Lu, Wenfei Yang, Tianzhu Zhang, and Yongdong Zhang. Motiongs: Exploring explicit motion guidance for deformable 3d gaussian splatting. Advances in Neural Information Processing Systems, 37:101790101817, 2024."
        },
        {
            "title": "Contents",
            "content": "1. Introduction 2. Related Work 3. Preliminary: 3D Gaussian Splatting 4. Method . . . . . . . . . . . . . . . 4.1. Overview . 4.2. Adaptive Gabor Video Representation . . . . 4.3. Temporally Dynamic Adaptive Gabor . . . . . . . . 4.4. Optimization . . . . . . . 4.5. Adaptive Initialization . . . . . . . . . . . . . . . . . . . . . 5. Experiment . 5.1. Evaluation . . 5.2. Applications 5.3. Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6. Conclusion A. Activation for Gabor Coefficients A.1. Straight-Through Hard Sigmoid for Fre- . . . . . quency Weights . . . . . . . . . . B. Proof of Adaptive Degradation to Gaussian B.1. Mathematical Formulation . . B.2. Degradation to Gaussian . . . B.3. Implication for Opacity . . . . B.4. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. Additional Visual Comparisons and Results A. Activation for Gabor Coefficients 2 3 3 3 3 4 5 6 6 6 6 6 8 14 14 14 14 15 15 15 it. Instead, during the backward pass, we use the gradient of the sigmoid function as surrogate: ω = ˆω σ(ω)(1 σ(ω)), (21) where σ() is the standard sigmoid function. This provides smooth, bounded gradient signal. The combination of bounded forward pass and smooth backward pass achieves stable training: the forward pass prevents artifacts by constraining frequency weights, while the backward pass enables effective gradient-based optimization. This approach avoids exploding gradients that can arise from unbounded activations. B. Proof of Adaptive Degradation to Gaussian We prove that our Adaptive Gabor representation naturally degrades to traditional Gaussian when all frequency weights vanish, demonstrating its adaptive capability between Gaussian and Gabor modes. B.1. Mathematical Formulation Recall from Eq. (4) and Eq. (5) in the main paper, the adaptive modulation function is defined as: Sadap(x) = + 1 (cid:88) i=1 ωi cos(fidi, x), (22) where the compensation term is given by: (cid:32) = γ + (1 γ) 1 (cid:33) ωi , 1 (cid:88) i=1 (23) with γ [0, 1] as fixed hyperparameter controlling degradation smoothness, and 1/N normalizing the weighted average of multiple waves. A.1. Straight-Through Hard Sigmoid for FreB.2. Degradation to Gaussian quency Weights In Gabor primitives, the frequency coefficients ωi must satisfy two requirements: (1) values must be constrained within learnable range, and (2) gradients must flow back through the activation to enable end-to-end optimization. To achieve this, we employ Straight-Through Estimator (STE) [4] with hard sigmoid activation. During the forward pass, we apply hard sigmoid to clip ωi into the range [0, 1]: Consider the limiting case where all frequency weights approach zero: ωi 0 for all {1, . . . , }. In this case: (cid:88) i= ωi 0. (24) Substituting into the compensation term: γ + (1 γ) 1 (cid:18) 1 (cid:19) 0 = γ + (1 γ) 1 = 1. (25) ˆω = clip (cid:18) ω + 1 (cid:19) , 0, 1 . (20) And the modulation term becomes: This ensures that the Gabor kernels frequency modulation remains bounded, preventing unbounded growth that could destabilize energy balance. However, since the hard clipping operation is nondifferentiable, we cannot directly backpropagate through 1 (cid:88) i=1 Therefore: ωi cos(fidi, x) 0. (26) Sadap(x) 1 + 0 = 1. (27) 14 Figure 12. Visual comparison on DAVIS dataset. to 14. These figures demonstrate our methods superior performance in preserving high-frequency texture details and maintaining temporal consistency across challenging scenarios including fast motion, occlusions, and complex deformations. For interactive visualization of downstream application results, including frame interpolation, video editing, and stereo view synthesis, please refer to the supplementary HTML page (index.html). The interactive viewer allows frameby-frame inspection and video playback to better appreciate the temporal coherence and visual quality of our method. B.3. Implication for Opacity Since the Gabor-modulated opacity is defined as: αGabor(x) = G(x) Sadap(x), (28) when Sadap(x) = 1, we recover: αGabor(x) = G(x) 1 = G(x), (29) which is exactly the traditional Gaussian primitive without frequency modulation. B.4. Conclusion This proof demonstrates that our Adaptive Gabor representation gracefully degrades to standard Gaussian when frequency content is not needed (ωi 0), while smoothly transitioning to frequency-enhanced Gabor modes when high-frequency details are required (ωi > 0). This adaptive behavior is crucial for maintaining energy stability across diverse scene regions with varying frequency characteristics. C. Additional Visual Comparisons and Results For comprehensive visual comparisons with baseline methods across various dynamic scenes, please refer to Figs. 12 15 Figure 13. Visual comparison on DAVIS dataset. 16 Figure 14. Visual comparison on DAVIS dataset."
        }
    ],
    "affiliations": [
        "National Yang Ming Chiao Tung University",
        "University of Zaragoza"
    ]
}