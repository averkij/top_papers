{
    "paper_title": "KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models",
    "authors": [
        "Yongliang Wu",
        "Zonghui Li",
        "Xinting Hu",
        "Xinyu Ye",
        "Xianfang Zeng",
        "Gang Yu",
        "Wenbo Zhu",
        "Bernt Schiele",
        "Ming-Hsuan Yang",
        "Xu Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in multi-modal generative models have enabled significant progress in instruction-based image editing. However, while these models produce visually plausible outputs, their capacity for knowledge-based reasoning editing tasks remains under-explored. In this paper, we introduce KRIS-Bench (Knowledge-based Reasoning in Image-editing Systems Benchmark), a diagnostic benchmark designed to assess models through a cognitively informed lens. Drawing from educational theory, KRIS-Bench categorizes editing tasks across three foundational knowledge types: Factual, Conceptual, and Procedural. Based on this taxonomy, we design 22 representative tasks spanning 7 reasoning dimensions and release 1,267 high-quality annotated editing instances. To support fine-grained evaluation, we propose a comprehensive protocol that incorporates a novel Knowledge Plausibility metric, enhanced by knowledge hints and calibrated through human studies. Empirical results on 10 state-of-the-art models reveal significant gaps in reasoning performance, highlighting the need for knowledge-centric benchmarks to advance the development of intelligent image editing systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 7 0 7 6 1 . 5 0 5 2 : r KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models Yongliang Wu1,4 Zonghui Li1 Xinting Hu2 Xinyu Ye3 Xianfang Zeng4 Gang Yu4 Wenbo Zhu5 Bernt Schiele2 Ming-Hsuan Yang6 Xu Yang1 1 Southeast University 4 StepFun 2Max Planck Institute for Informatics 3 Shanghai Jiao Tong University 5 University of California, Berkeley 6 University of California, Merced Project Page: https://yongliang-wu.github.io/kris_bench_project_page/"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in multi-modal generative models have enabled significant progress in instruction-based image editing. However, while these models produce visually plausible outputs, their capacity for knowledge-based reasoning editing tasks remains under-explored. In this paper, We introduce KRIS-Bench (Knowledge-based Reasoning in Image-editing Systems Benchmark), diagnostic benchmark designed to assess models through cognitively informed lens. Drawing from educational theory, KRIS-Bench categorizes editing tasks across three foundational knowledge types: Factual, Conceptual, and Procedural. Based on this taxonomy, we design 22 representative tasks spanning 7 reasoning dimensions and release 1,267 high-quality annotated editing instances. To support fine-grained evaluation, we propose comprehensive protocol that incorporates novel Knowledge Plausibility metric, enhanced by knowledge hints and calibrated through human studies. Empirical results on 10 state-of-the-art models reveal significant gaps in reasoning performance, highlighting the need for knowledge-centric benchmarks to advance the development of intelligent image editing systems."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in multi-modal generative models have led to impressive performance in instructionbased image editing [13]. Given various textual prompts, these models can produce visually coherent and semantically aligned edits across tasks such as object manipulation [4, 5], style transformation [6, 7], and action simulation [8, 9]. However, while the editing quality of these model outputs has improved substantially, the reasoning processes underpinning such edits remain under-explored [10 13]. For example, as shown in Figure 1 (b), when given the instruction add piece of solid sodium to the water, the models generate visually plausible image in which the sodium appears submerged in the water. But it reveals lack of reasoning grounded in chemistry knowledge, as solid sodium will react violently with water, releasing large amount of heat that causes the water to boil. Successful reasoning may require perceptual recognition, spatial interpretation, social commonsense, science concepts, or procedural planning [14, 15]. The diversity of these knowledge types underscores the need for more fine-grained and cognitively informed evaluation frameworks that can systematically disentangle the reasoning capabilities required for different editing goals [1618]. Recently, several benchmarks have been proposed to evaluate the capabilities of image editing models [3, 4, 8, 1926]. RISEBench [26], most relevant to our work, introduces reasoning-aware image Work done during an internship at StepFun Corresponding author Project leader Preprint. Figure 1: (a) We present KRIS-Bench, benchmark for instruction-based image editing grounded in knowledge-based reasoning taxonomy. It covers 3 knowledge dimensions, 7 reasoning dimensions, and 22 editing tasks. Specific examples are shown in Figure 2. (b) Given an editing pair of (image, instruction) under specific reasoning dimension (i.e., Chemistry in Natural Science), we evaluate the output of image editing models with automated VLM tools over the proposed four complementary metrics, which are aligned with human scoring. editing evaluations across temporal, causal, spatial, and logical dimensions. However, its reasoning types remain coarse and do not provide formal structure for representing the underlying knowledge required by different tasks. Rather than simply evaluating image editing through task categories or action types, we benchmark it based on structured understanding of knowledge [14]. We view instruction-based image editing as cognitively grounded process that mirrors human learning. From this perspective, equipping image editing models with the ability to identify, internalize, and apply appropriate knowledge during editing resembles the process of educating student to perceive, reason about, and interact with the real world. Guided by this analogy, we draw inspiration from the revised taxonomy of educational objectives proposed by Anderson and Krathwohl [27], and define three foundational types of knowledge: Factual knowledge, Conceptual knowledge, and Procedural knowledge. This taxonomy supports systematic decomposition of the knowledge demands involved in the reasoning process of image editing, and provides principled foundation for our design of diagnostic benchmarks for image editing models [28]. Building on these knowledge types, we present KRIS-Bench (Knowledge-based Reasoning in Image-editing Systems Benchmark), diagnostic benchmark designed to systematically evaluate the reasoning capabilities of image editing models. KRIS-Bench adopts top-down design paradigm grounded in principles of cognitive education. It structures tasks according to three foundational knowledge types, each further decomposed into specific reasoning dimensions. For example, factual knowledge covers directly observable properties and does not involve abstract inference or contextual interpretation, thus supporting basic reasoning processes such as perceptual recognition [7], spatial relation understanding [29], and temporal prediction [30]. The taxonomy is visualized in Figure 1 (a), where 22 editing tasks are organized across 7 reasoning dimensions under the three knowledge types. To support reliable evaluation at scale, KRIS-Bench comprises 1,267 high-quality instances. Furthermore, we propose comprehensive evaluation protocol grounded in vision-language models (VLMs) [31, 32]. Beyond conventional metrics [26, 3335], we introduce new dimension, Knowledge Plausibility, which assesses whether the edited outputs align with real-world knowledge, as illustrated in Figure1 (b). To facilitate this evaluation, each knowledge-intensive test case is accompanied by manually curated knowledge hint designed to guide the VLMs reasoning. We conduct user study to validate the alignment between our evaluation protocol and human judgments, and demonstrate that the inclusion of knowledge hints significantly enhances the plausibility assessment by VLMs [3638]. Extensive experiments across 10 state-of-the-art models reveal persistent limitations in performing knowledge-grounded reasoning for image editing tasks. 2 The main contributions of this work are: We propose the first cognitively grounded taxonomy of knowledge types for instruction-based image editing. Drawing from educational theory, we systematically define Factual, Conceptual, and Procedural knowledge as the foundation for evaluating reasoning capabilities. We introduce KRIS-Bench, comprehensive benchmark consisting of 22 carefully designed tasks across 7 reasoning dimensions, supported by 1,267 expertly annotated editing instances. This significantly expands the scale and depth of reasoning evaluation in the image editing. We design comprehensive evaluation protocol that, for the first time, introduces the Knowledge Plausibility dimension to assess whether model-generated edits are consistent with real-world knowledge, with manually curated knowledge hints to support more reliable plausibility judgments. We conduct systematic experiments of 10 state-of-the-art image editing models, revealing substantial limitations across knowledge types, reasoning dimensions, and editing tasks."
        },
        {
            "title": "2 Related Work",
            "content": "Instruction-based Image Editing Methods. Instruction-based image editing [17, 39, 40] has progressed significantly through the use of diffusion models and instruction-following strategies. Some methods enable test-time controllability by altering the diffusion trajectory, including partial denoising from intermediate steps [41], attention-based control for localized edits [42], CLIP-guided manipulation with region-of-interest masks [43, 44], and latent inversion strategies that optimize noise embeddings to preserve fidelity [45]. Beyond test-time control, many approaches improve editing performance through model training or fine-tuning. Some enhance the architecture with taskaware conditioning, cross-modal attention, or instruction-parsing modules to support more complex edits [25, 46, 47]. Others scale up with large-scale instruction tuning on millions of image-text pairs to boost generalization and fidelity for open-ended prompts [4, 48]. further line of work incorporates human feedback via reward learning or reference-based alignment to better capture user intent [49, 50]. Closed-source systems such as GPT-Image-1 [51], Doubao [52], and Gemini 2.0 Flash Experimental [53] further push performance through large-scale multi-modal training and integrated reasoning. However, across both open and closed models, existing methods emphasize visual plausibility and instruction adherence, with limited attention to the knowledge and reasoning processes essential for cognitively grounded editing. Benchmarks for Instruction-based Image Editing. To effectively evaluate the capabilities of instruction-based image editing models [5459], growing number of datasets and benchmarks have been proposed. EditBech [19], TEdBench [21], and EditEval [3] focus on task-oriented evaluation, targeting canonical sub-tasks such as inpainting, attribute manipulation, or layout adjustment. To expand evaluation coverage, benchmarks like EMU-Edit[25], GEdit-Bench [8], and REALEDIT [35] collect diverse free-form user instructions, while I2EBench [22] scales across editing types and metrics. Complex-Edit [60] further introduces multi-step editing chains to model task complexity. Despite these advances, these work focus on task complexity or data scale, without explicitly modeling the reasoning processes or knowledge structures involved in instruction understanding. Recent works try to address this gap by incorporating reasoning-aware evaluation [24]. AURORA-BENCH [61] focuses on action-centric edits by leveraging curated triplets from videos and simulations, and SmartEdit [13] explores spatial and interaction-based reasoning within ambiguous editing scenarios. IntelligentBench [62] is designed to evaluate the ability of editing models in complex multimodal reasoning, but it does not provide detailed categorization of task types. RISEBench [26] categorizes tasks along temporal, causal, spatial, and logical dimensions. However, these reasoning axes remain coarse and are not grounded in formal cognitive or knowledge-based framework, limiting their capacity to capture the full scope of reasoning challenges in instruction-driven image editing."
        },
        {
            "title": "3 KRIS-Bench",
            "content": "In this section, we introduce KRIS-Bench, comprehensive benchmark designed to evaluate image editing models through the lens of knowledge-based reasoning. comparative analysis with prior reasoning-based image editing benchmarks is presented in Table 1. KRIS-Bench offers the most comprehensive coverage to date, featuring the largest size (1,276 samples across 22 tasks) with strong emphasis on reasoning capabilities across varying levels of complexity. For cases involving knowledge-based reasoning, we additionally provide knowledge hints to assist the evaluation process. 3 Table 1: Comparison of open-source reasoning-based image editing benchmarks. Dataset Publication Size Dimensions Tasks Complexity Knowledge Hints AURORA-Bench [61] NeurIPS 2024 CVPR 2024 SmartEdit [13] arXiv 2025.4 RISE [26] arXiv 2025.5 IntelligentBench [62] KRIS-Bench 400 219 360 350 1,267 2 4 7 8 7 16 22 Simple Medium Hard Medium Mixed 3.1 Taxonomy of Knowledge Types Our knowledge-based reasoning taxonomy in image editing models is inspired by the revised Bloom taxonomy of educational objectives [27]. We organize the knowledge required in image editing into three levels: Factual Knowledge, Conceptual Knowledge, and Procedural Knowledge.4 Unlike prior works that emphasize editing actions, our focus is on the types of knowledge model must internally represent and apply to perform reasoning-aware edit. This perspective is rooted in pedagogical theory, where different levels of knowledge serve as foundation for learning and problem solving. Factual Knowledge includes directly observable properties such as visual attributes (e.g., color, size), spatial relations (e.g., left/right, different viewpoint), and temporal cues (e.g., before/after states). This knowledge does not require abstract inference or contextual interpretation, serving as the basic prerequisite for more complex reasoning. Conceptual Knowledge represents higher-order form of understanding that connects perceptual information to generalizable principles from the physical, biological, or social world. Unlike factual recognition, conceptual knowledge enables models to anticipate plausible outcomes following realworld dynamics, knowledge, and rules. For example, the instruction Ripen the bananas by turning them yellow presumes an understanding of the natural ripening process. Procedural Knowledge refers to the ability of model to perform multi-step reasoning, task decomposition, and rule-based execution within image editing contexts. It involves not only understanding what change should occur, but also how to perform that change in procedure. Procedural knowledge is essential for instructions requiring multi-element coordination (e.g., multi-element referring generation) or complex logical reasoning (e.g., complete the Ravens progressive matrix) [63]. 3.2 Knowledge-Based Task Formulation Drawing from the three knowledge types, we define 7 associated reasoning dimensions that correspondingly span across 22 tasks. The tasks in KRIS-Bench are not mere isolated editing actions. Instead, they are crafted and organized based on their specific knowledge requirements derived from our taxonomy. Representative examples from each task are illustrated in Figure 2. Factual Knowledge. Tasks in this category evaluate fundamental visual and temporal understanding that does not require external knowledge or reasoning. The sub-dimensions encompass: Attribute Perception. Modifications to object count, color, size, part completion, and correction of abnormalities based on direct perception in the image. Spatial Perception. Movement of objects to target locations within the image and adjustment of viewpoints for the same object. Temporal Prediction. Prediction of previous, intermediate, or future frames based on surrounding frames for maintaining temporal consistency. Conceptual Knowledge. Tasks in this category necessitate understanding and applying real-world knowledge beyond perceptual cues. The sub-dimensions encompass: Social Science: Modifications involving commonsense reasoning (e.g., adjusting clock for daylight saving time) and edits based on cultural or religious contexts (e.g., substituting dish with mooncakes for festival). Natural Science: Modifications based on science principles, covering biology (e.g., fruit ripening), chemistry (e.g., color changes in pH indicators), geography (e.g., terrain alterations), mathematics 4We do not include Metacognitive Knowledge in Blooms taxonomy, as it involves self-monitoring and learning regulation, which current large models do not yet demonstrate within the one-turn image editing process. 4 Figure 2: Representative examples from the 22 knowledge-based reasoning image editing tasks in KRIS-Bench. Each task is designed to evaluate specific knowledge grounded in factual, conceptual, or procedural, covering diverse reasoning dimensions. (e.g., geometric transformations), medicine (e.g., blood pressure changes), and physics (e.g., changes based on physical laws). Procedural Knowledge. Tasks in this category involve executing structured reasoning processes and following multi-step instructions. The sub-dimensions include: Logical Reasoning: Modifications involving reasoning with symbolic structures and numerical relationships (e.g., solving puzzles or applying logical rules). Instruction Decomposition: Modifications requiring the execution of multiple sequential instructions (e.g., designing poster) and integrating visual elements from various sources into coherent scene (e.g., combining objects from different images). 3.3 Data Collection Most images in our benchmark were collected from the internet, with small portion generated using generative models [51] and collected from existing datasets [13, 6469]. For each image, one editing 5 Table 2: Performance of different models across different reasoning dimensions and metrics, including Visual Consistency (VC), Visual Quality (VQ), Instruction Following (IF), and Knowledge Plausibility (KP). Scores marked with indicate models unable to handle multi-image input tasks, with the corresponding task scores set to 0. The performance of open-source and closed-source models is separately marked with the best performance in bold, and the second best underlined. Reasoning Dimension Metric Attribute Perception Spatial Perception Temporal Prediction Average Social Science VC VQ IF Avg VC VQ IF Avg VC VQ IF Avg VC VQ IF KP Avg VC VQ IF KP Avg VC VQ IF KP Avg VC VQ IF Avg Average Overall Average Natural Science Average Logical Reasoning Instruction Decomposition Closed-Source Models Open-Source Models GPT-4o Gemini 2.0 Doubao OmniGen Emu2 BAGEL BAGEL-Think 35.75 49.50 28.50 37.92 24.00 50.00 10.75 28.25 19.25 26.25 20.00 21.83 33.11 37.25 46.00 22.50 16.75 30.63 31.00 47.00 18.25 12.50 27.19 28.02 15.00 26.75 4.25 1.75 11.94 28.75 46.50 32.25 35.83 23.89 28.85 66.75 67.00 40.50 58.08 53.50 71.25 38.75 54.50 0.00 0.00 0.00 0.00 47.71 75.75 75.50 34.25 25.25 52.69 65.75 76.00 38.25 28.00 52.00 52.17 74.75 84.25 23.25 16.25 49.63 30.75 29.00 32.75 30.83 40.23 47.76 74.75 75.00 49.50 66.42 77.25 81.25 44.75 67.75 0.00 0.00 0.00 0.00 55.77 76.50 77.75 46.00 38.25 59.63 68.00 80.25 49.00 40.25 59.38 59.44 71.25 83.00 29.25 21.25 51.19 32.25 25.25 24.50 27.33 39.26 53.36 47.75 75.25 31.50 51.50 41.50 77.75 18.25 48.83 12.50 37.50 16.50 22.17 45.40 32.75 72.75 22.00 11.25 34.69 35.00 75.50 25.00 18.25 38.44 37.54 23.50 66.25 7.25 2.25 24.81 31.00 64.75 39.25 45.00 34.91 39. 74.50 94.75 80.25 83.17 69.50 94.50 73.25 79.08 54.00 86.25 64.50 68.25 79.80 83.00 95.75 84.50 78.75 85.50 80.00 96.00 76.50 67.75 80.06 81.37 81.00 95.00 59.25 51.00 71.56 71.00 96.25 88.00 85.08 78.32 80.09 66.75 89.00 57.00 70.92 67.50 89.00 21.00 59.17 26.75 77.50 17.50 40.58 63.30 72.00 86.50 54.75 48.75 65.50 70.25 87.25 48.00 39.25 61.19 62.23 64.75 85.00 24.75 16.50 47.75 51.50 76.75 53.50 60.58 54.17 60.70 69.50 81.75 47.75 66.33 60.50 83.25 46.25 63.33 54.50 75.00 62.25 63.92 65.26 77.00 83.75 59.00 53.00 68.19 65.00 83.75 44.75 34.25 56.94 59.65 73.50 84.50 33.00 25.50 54.13 58.25 82.50 74.25 71.67 62.90 62.41 Step1X-Edit AnyEdit MagicBrush 54.75 67.50 20.75 47.67 55.75 72.00 7.75 45.17 0.00 0.00 0.00 0.00 39.26 62.00 66.75 15.00 10.50 38.56 61.75 77.75 18.25 14.00 42.94 41.88 55.50 72.75 10.25 7.75 36.56 29.75 39.25 11.75 26.92 31.74 38.55 63.00 70.25 33.25 55.50 64.25 83.00 8.00 51.75 0.00 0.00 0.00 0.00 45.52 63.25 72.50 25.50 17.50 44.69 71.25 78.00 27.50 19.50 49.06 48.01 58.75 72.25 20.25 12.25 40.88 25.75 26.50 16.00 22.75 31.82 43.29 53.50 76.25 32.00 53.92 38.00 69.25 11.50 39.58 0.00 0.00 0.00 0.00 41.84 54.00 70.00 27.25 20.50 42.94 47.00 72.75 19.00 13.50 38.06 39.24 37.25 75.50 5.25 2.00 30.00 20.75 39.25 9.25 23.08 26.54 37. InsPix2Pix 17.50 55.50 18.00 30.33 13.25 40.25 10.50 21.33 0.00 0.00 0.00 0.00 23.33 15.75 50.00 14.25 10.25 22.56 18.75 58.25 17.50 11.75 26.56 25.59 14.75 58.75 3.75 2.00 19.81 9.50 27.75 7.00 14.75 17.28 22.82 e n u F e n u c e l K r c instruction is created by trained annotators. To enhance instruction diversity and realism, we augment the original prompts using ChatGPT, paraphrasing and elaborating them under human supervision. The data was curated by three human annotators, two of whom have obtained Bachelors degrees, while the third is currently pursuing one. All annotations were subsequently reviewed by three experts with Ph.D. degrees. For tasks requiring domain expertise (e.g., physics-based or biomedical edits), additional domain-specific reviewers were consulted. More details can be found in the Appendix. 3.4 Evaluation Metrics image editing models on To comprehensively evaluate the performance of state-of-the-art KRIS-Bench, we propose four-dimensional evaluation protocol. In addition to the three widely adopted dimensions, namely Visual Consistency, Visual Quality, and Instruction Following [13, 24, 26, 70], we introduce novel fourth dimension called Knowledge Plausibility, which explicitly assesses whether the generated edits are consistent with real-world knowledge. To support this evaluation, we provide concise knowledge hint for test cases that require real-world knowledge. Each hint is brief description of the expected outcome based on humanities, scientific, or procedural understanding. For example, adding purple cabbage indicator to acidic water should result in red color change. These hints offer evaluators the necessary reference to determine whether the edited image reflects plausible and knowledge-consistent effects. Visual Consistency. This dimension evaluates whether the edited image faithfully preserves the parts of the original image that are not semantically or spatially related to the instruction. An effective editing model should localize changes precisely while leaving the rest of the scene unchanged. Visual Quality. This dimension evaluates the perceptual quality of the generated image, focusing on overall realism, natural appearance, and the absence of noticeable artifacts. It assesses whether the 6 Figure 3: Visualization results of (a) Color Change, (b) Position Movement, (c) Humanities, (d) Chemistry, and (e) Abstract Reasoning across different models and metrics. Each example is provided with scores across the four evaluation metrics as well as an overall average score. Note that the knowledge hint is provided solely for evaluation and has been shortened for better illustration. output maintains structural coherence and visual plausibility, without introducing distortions such as unnatural textures, broken geometry, or degraded fine details. Instruction Following. This dimension evaluates whether the model accurately and completely executes the user-provided instruction. It focuses purely on the literal fulfillment of the editing instruction, independent of perceptual quality or real-world plausibility. For instance, when given the instruction add wooden block into the tank, this dimension solely verifies if the edited image includes the additional wooden block in the tank, without regard to whether the block floats or sinks. Knowledge Plausibility. This dimension assesses whether the edits are consistent with real-world knowledge and domain-specific principles. It functions as higher-level criterion that evaluates the coherence of the output within plausible environment. For example, the addition of wooden block to tank that appears fully submerged indicates poor plausibility of the physics knowledge. Edits that fail to fulfill the instruction are automatically considered implausible under this dimension, as basic instruction compliance is prerequisite for meaningful knowledge reasoning. This metric is only available for tasks in Natural Sciences, Social Sciences, and Logical Reasoning. Each evaluation metric is rated from 1 to 5. We use GPT-4o (May 2025) as the evaluation model, with carefully crafted prompts tailored for each dimension to ensure precise and consistent assessment [71]."
        },
        {
            "title": "4 Experiments and Analysis",
            "content": "4.1 Evaluation Models & Settings We evaluate 10 state-of-the-art image editing models on KRIS-Bench to assess their reasoning capabilities. These models include three closed-source models: GPT-Image-1 [51](GPT-4o), Gemini 2.0 Flash Experimental [53](Gemini 2.0), and Doubao [52].5 Seven open-source models: OmniGen [72], 5Results obtained via OpenAI, API Google AI Studio, and Doubao App (all in April 2025). 7 Figure 4: Performance on KRIS-Bench across different editing tasks and four different metrics. Top: closed-source models. Bottom: open-source models. Emu2 [73], BAGEL [62], Step1X-Edit [8], AnyEdit [9], InstructPix2Pix [5](InsPix2Pix), and MagicBrush [4]. Note that open-source models, except OmniGen and Emu2, are limited to single-image inputs and thus cannot be evaluated on tasks requiring multiple input images. In such tasks, these models receive an evaluation score of one (lowest). Moreover, BAGEL is capable of performing image editing in reasoning mode, which we refer to as BAGEL-Think in our experiments. All generation and evaluation processes were conducted on H100 GPUs, using default hyperparameter settings to ensure fairness and reproducibility. 4.2 Results and Analysis Overall Performance. Table 2 reports evaluation results across various knowledge types, spanning seven dimensions with different metrics. All scores are normalized to 100-point scale to enable straightforward comparison. The results reveal that closed-source models substantially outperform open-source models on KRIS-Bench. BAGEL-Think achieves the best performance among opensource models and has begun to approach the performance level of closed-source models such as Gemini 2.0 and Doubao. Notably, we observe that introducing reasoning process into BAGEL (BAGEL-Think) yields marked improvement over the baseline BAGEL model without reasoning, highlighting the critical role of reasoning in KRIS-Bench. Among all models, GPT-4o achieves the highest overall scores across nearly all knowledge types and evaluation dimensions, except for slightly lagging behind Gemini 2.0 in visual consistency for temporal prediction. Analysis by Knowledge Types. Based on Table 2, nearly all models consistently perform the weakest on procedural knowledge, indicating significant challenges in multi-step reasoning and task decomposition for current editing models. Surprisingly, models do not consistently struggle more with conceptual knowledge than with factual knowledge, despite the former requiring higher level of abstraction and generalization. In particular, models such as GPT-4o, BAGEL, BAGEL-Think, Step1X-Edit, and AnyEdit perform slightly worse on factual knowledge tasks than on conceptual ones. This counterintuitive finding suggests that the current strong image generation models still lack robust grounding in perceptual and real-world facts, such as object counting and spatial positioning. Analysis by Reasoning Dimensions. Within each knowledge type, closer breakdown of reasoning dimensions reveals diverse performance patterns in Table 2. For factual knowledge, most models achieve relatively high accuracy in attribute-level perception tasks (Figures 3 (a)), but exhibit sharp drops in spatial reasoning (Figures 3 (b)). For conceptual knowledge, models generally perform better on tasks requiring commonsense or cultural knowledge, but struggle with tasks grounded in scientific principles where expert-domain reasoning is needed. As illustrated in Figure 3 (cd), although the models demonstrate strong performance on the humanities task by correctly identifying the panda as 8 Figure 5: Correlation between human and VLM scores across Visual Consistency (VC), Visual Quality (VQ), Instruction Following (IF), and Knowledge Plausibility (KP). We compare the prompts incorporating knowledge hints (Knowledge Prompts) with simple baseline (Simple Prompts). Chinas most iconic national treasure, they exhibit significant limitations in scientific reasoning, such as failing to accurately interpret chemical reactions and overlooking the fact that red cabbage turns red in acidic conditions. For procedural knowledge, closed-source models exhibit significantly stronger performance on instruction decomposition tasks, with GPT-4o achieving particularly notable results. In contrast, all models face considerable challenges in logical reasoning tasks involving symbolic manipulation or abstract pattern recognition. Interestingly, GPT-4o occasionally succeeds in solving such tasks (Figure 3 (e), the value on the right is twice that of the value on the left), highlighting its emerging capacity for logical reasoning. Analysis by Editing Tasks and Metrics. Figure 4 presents radar chart depicting model performance across various editing tasks and metrics. The results reveal substantial variation in performance across specific tasks, even within the same reasoning dimension. For example, under the Attribute Perception category, both Gemini 2.0 and Doubao perform noticeably worse on Count Change and Size Adjustment compared to Color Change in terms of instruction following. Furthermore, while all models attain relatively high scores in Visual Consistency and Visual Quality, their performance in Instruction Following and Knowledge Plausibility exposes significant shortcomings. Notably, scores for Knowledge Plausibility are consistently lower than those for Instruction Following, highlighting persistent challenges in integrating and applying external knowledge accurately during editing. Moreover, BAGEL-Think surpasses nearly all other open-source models on the Knowledge Plausibility metric across most tasks. Remarkably, it even outperforms closed-source models such as Gemini 2.0 and Doubao in Biology and Chemistry tasks. These comprehensive analyses reveal that despite recent advancements in instruction-based image editing, current models exhibit inherent limitations in knowledge-centric reasoning. The challenges extend beyond the completion of complex edits to encompass the comprehension and application of diverse forms of knowledge in coherent and grounded manner. By anchoring the evaluation on cognitively informed taxonomy, KRIS-Bench surpasses task-specific benchmarks to systematically evaluate how models internalize, manipulate, and operationalize knowledge. This paradigm shift offers new pathways for developing editing models that engage in reasoning processes more analogous to human cognition. In addition, the performance gains observed in BAGEL-Think through the integration of reasoning process on certain tasks suggest promising direction for tackling knowledge-based reasoning challenges. Additional experimental results are provided in the Appendix. 4.3 Assessment of Evaluation Protocol To evaluate the reliability of VLM scores, we conducted user study with 12 human experts on three closed-source models. We report the Pearson correlation coefficient (r) and Mean Absolute Error (MAE) between the expert ratings and the scores produced by the VLM, as shown in Figure 5. We compared our carefully designed prompts incorporating knowledge hints (Knowledge Prompts) with simple baseline (Simple Prompts). The results show that Knowledge Prompts yield stronger and lower MAE values, especially for the Knowledge Plausibility metric. This indicates that our knowledge-enhanced prompts provide more accurate evaluations for knowledge-based reasoning in image editing. All scoring prompts and details of the user study are provided in the Appendix."
        },
        {
            "title": "5 Conclusion",
            "content": "We present KRIS-Bench, cognitively grounded benchmark for evaluating the reasoning capabilities of image editing models through the lens of factual, conceptual, and procedural knowledge. Unlike prior task-based or content-driven approaches, KRIS-Bench offers principled, knowledge-centric framework supported by fine-grained tasks and human-calibrated evaluation. Our results highlight persistent gaps in current models ability to reason across diverse knowledge types. Limitations. While KRIS-Bench provides comprehensive knowledge-based reasoning image editing benchmark with broader task coverage and more diverse evaluation dimensions compared to prior benchmarks, it may still exhibit certain biases. These challenges include the relatively small scale of the benchmark size, the potential over-representation of particular knowledge types, and culturally specific assumptions inherent in task design."
        },
        {
            "title": "References",
            "content": "[1] Kaihang Pan, Wang Lin, Zhongqi Yue, Tenglong Ao, Liyu Jia, Wei Zhao, Juncheng Li, Siliang Tang, and Hanwang Zhang. Generative multimodal pretraining with discrete diffusion timestep tokens. arXiv preprint arXiv:2504.14666, 2025. 1 [2] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [3] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Liangliang Cao, and Shifeng Chen. Diffusion model-based image editing: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, page 127, 2025. ISSN 1939-3539. doi: 10.1109/tpami.2025.3541625. URL http://dx.doi.org/10.1109/TPAMI. 2025.3541625. 1, 3 [4] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. 1, 3, 8 [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 1, 8 [6] Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit Bermano. Hyperstyle: Stylegan inversion with hypernetworks for real image editing. In Proceedings of the IEEE/CVF conference on computer Vision and pattern recognition, pages 1851118521, 2022. [7] Jing Shi, Ning Xu, Haitian Zheng, Alex Smith, Jiebo Luo, and Chenliang Xu. Spaceedit: Learning unified editing space for open-domain image color editing. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 1973019739, 2022. 1, 2 [8] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 1, 3, 8 [9] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. arXiv preprint arXiv:2411.15738, 2024. 1, 8 [10] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. 1 [11] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. [12] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of text-to-image generation models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 30433054, 2023. [13] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, and Ying Shan. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 83628371, June 2024. 1, 3, 4, 5, 6, 18 [14] Yuwei Niu, Munan Ning, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Bin Zhu, and Li Yuan. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. 1, 2 [15] Yifan Gao, Zihang Lin, Chuanbin Liu, Min Zhou, Tiezheng Ge, Bo Zheng, and Hongtao Xie. Postermaker: Towards high-quality product poster generation with accurate text rendering. arXiv preprint arXiv:2504.06632, 2025. 1 [16] Mengxue Kang, Xinyu Zhang, Fei Wei, Shuang Xu, and Yuhe Liu. Enhancing image editing with chain-of-thought reasoning and multimodal large language models. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. 1 [17] Xincheng Shuai, Henghui Ding, Xingjun Ma, Rongcheng Tu, Yu-Gang Jiang, and Dacheng Tao. survey of multimodal-guided image editing with text-to-image diffusion models. arXiv preprint arXiv:2406.14555, 2024. [18] Zhan, Yu, Wu, Zhang, Lu, Liu, Kortylewski, Theobalt, and Xing. Multimodal image synthesis and editing: survey and taxonomy. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 1 [19] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J. Fleet, Radu Soricut, Jason Baldridge, Mohammad Norouzi, Peter Anderson, and William Chan. Imagen editor and editbench: Advancing and evaluating text-guided image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1835918369, June 2023. 1, 3 [20] Samyadeep Basu, Mehrdad Saberi, Shweta Bhardwaj, Atoosa Malemir Chegini, Daniela Massiceti, Maziar Sanjabi, Shell Xu Hu, and Soheil Feizi. Editval: Benchmarking diffusion based text-guided image editing methods, 2023. URL https://arxiv.org/abs/2310.02426. [21] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 60076017, June 2023. 3 [22] Yiwei Ma, Jiayi Ji, Ke Ye, Weihuang Lin, Yonghan Zheng, Qiang Zhou, Xiaoshuai Sun, Rongrong Ji, et al. I2ebench: comprehensive benchmark for instruction-based image editing. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. 3 [23] Yulin Pan, Xiangteng He, Chaojie Mao, Zhen Han, Zeyinzi Jiang, Jingfeng Zhang, and Yu Liu. Ice-bench: unified and comprehensive benchmark for image creating and editing, 2025. URL https://arxiv.org/abs/2503.14482. [24] Ying Jin, Pengyang Ling, Xiaoyi Dong, Pan Zhang, Jiaqi Wang, and Dahua Lin. Reasonpix2pix: Instruction reasoning dataset for advanced image editing, 2024. URL https://arxiv.org/ abs/2405.11190. 3, 6 [25] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 88718879, June 2024. 3 11 [26] Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Hao Li, Zicheng Zhang, Guangtao Zhai, Junchi Yan, Hua Yang, Xue Yang, and Haodong Duan. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025. 1, 2, 3, 4, 6 [27] David Krathwohl. revision of blooms taxonomy: An overview. Theory into practice, 41 (4):212218, 2002. 2, [28] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris Metaxas, and Jian Ren. Sine: Single image editing with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60276037, 2023. 2 [29] Pengfei Jiang, Mingbao Lin, and Fei Chao. Move and act: Enhanced object manipulation and background integrity for image editing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 40394047, 2025. 2 [30] Wei Chen, Lin Li, Yongqi Yang, Bin Wen, Fan Yang, Tingting Gao, Yu Wu, and Long Chen. Comm: coherent interleaved image-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2406.10462, 2024. 2 [31] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 2 [32] Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, and Linda Ruth Petzold. Gpt-4v (ision) as generalist evaluator for vision-language tasks. arXiv preprint arXiv:2311.01361, 2023. 2 [33] Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, and Jinwoo Shin. Collaborative score distillation for consistent visual editing. Advances in Neural Information Processing Systems, 36:7323273257, 2023. [34] Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, and Sanja Fidler. Editgan: High-precision semantic image editing. Advances in Neural Information Processing Systems, 34:1633116345, 2021. [35] Peter Sushko, Ayana Bharadwaj, Zhi Yang Lim, Vasily Ilin, Ben Caffee, Dongping Chen, Mohammadreza Salehi, Cheng-Yu Hsieh, and Ranjay Krishna. Realedit: Reddit edits as large-scale empirical dataset for image transformations. arXiv preprint arXiv:2502.03629, 2025. 2, 3 [36] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-asa-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning, 2024. 2 [37] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. [38] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 2 [39] Yiyan Xu, Jinghao Zhang, Alireza Salemi, Xinting Hu, Wenjie Wang, Fuli Feng, Hamed Zamani, Xiangnan He, and Tat-Seng Chua. Personalized Generation in Large Model Era: Survey. arXiv preprint arXiv:2503.02614, 2024. [40] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Liangliang Cao, and Shifeng Chen. Diffusion Model-Based Image Editing: Survey. arXiv preprint arXiv:2402.17525, 2024. 3 [41] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations, 2022. URL https://arxiv.org/abs/2108.01073. 3 12 [42] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control, 2022. URL https://arxiv. org/abs/2208.01626. 3 [43] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1820818218, June 2022. 3 [44] Xinting Hu, Haoran Wang, Jan Eric Lenssen, and Bernt Schiele. PersonaHOI: Effortlessly Improving Personalized Face with Human-Object Interaction Generation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [45] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion In Proceedings of the IEEE/CVF for editing real images using guided diffusion models. Conference on Computer Vision and Pattern Recognition (CVPR), pages 60386047, June 2023. 3 [46] Shufan Li, Harkanwar Singh, and Aditya Grover. Instructany2pix: Flexible visual editing via multimodal instruction following, 2024. URL https://arxiv.org/abs/2312.06738. 3 [47] Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka. Instructedit: Improving automatic masks for diffusion-based image editing with user instructions, 2023. URL https://arxiv. org/abs/2305.18047. 3 [48] Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 30583093. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/ paper/2024/file/05a30a0fc9e6bacdd3abd4ca8508a9e6-Paper-Datasets_and_ Benchmarks_Track.pdf. 3 [49] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, and Ran Xu. Hive: Harnessing human feedback for instructional visual editing, 2024. URL https://arxiv.org/abs/2303.09618. [50] Runze He, Kai Ma, Linjiang Huang, Shaofei Huang, Jialin Gao, Xiaoming Wei, Jiao Dai, Jizhong Han, and Si Liu. Freeedit: Mask-free reference-based image editing with multi-modal instruction, 2024. URL https://arxiv.org/abs/2409.18071. 3 [51] OpenAI. Gpt-image-1: Openais multimodal image generation model. https://platform. openai.com/docs/models/gpt-image-1, 2025. Accessed: 2025-05-08. 3, 5, 7, 18 [52] ByteDance. Doubao: Bytedances ai chat assistant. https://www.doubao.com/chat/, 2025. Accessed: 2025-05-08. 3, 7 Experiment with gemini 2.0 flash native im- [53] Kat Kampf and Nicole Brichtova. URL https://developers.googleblog.com/en/ age generation, March 2025. experiment-with-gemini-20-flash-native-image-generation/. Accessed: 202505-08. 3, [54] Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In The Thirteenth International Conference on Learning Representations, 2024. 3 [55] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. [56] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. 13 [57] Yongsheng Yu, Ziyun Zeng, Hang Hua, Jianlong Fu, and Jiebo Luo. Promptfix: You prompt and we fix the photo. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. [58] Jinbin Bai, Wei Chow, Ling Yang, Xiangtai Li, Juncheng Li, Hanwang Zhang, and Shuicheng Yan. Humanedit: high-quality human-rewarded dataset for instruction-based image editing. arXiv preprint arXiv:2412.04280, 2024. [59] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. 3 [60] Siwei Yang, Mude Hui, Bingchen Zhao, Yuyin Zhou, Nataniel Ruiz, and Cihang Xie. Complex-Edit: Cot-like instruction generation for complexity-controllable image editing benchmark, 2025. URL https://arxiv.org/abs/2504.13143. 3 [61] Benno Krojer, Dheeraj Vattikonda, Luis Lara, Varun Jampani, Eva Portelance, Chris Pal, and Siva Reddy. Learning action and reasoning-centric image editing from videos and simulation. Advances in Neural Information Processing Systems, 37:3803538078, 2024. 3, 4 [62] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 3, 4, 8 [63] Simeng Han, Tianyu Liu, Chuhan Li, Xuyuan Xiong, and Arman Cohan. Hybridmind: Meta selection of natural language and symbolic language for enhanced llm reasoning. arXiv e-prints, pages arXiv2409, 2024. 4 [64] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2112621136, 2022. 5, [65] Yizhe Zhang, Richard He Bai, Ruixiang ZHANG, Jiatao Gu, Shuangfei Zhai, Joshua Susskind, and Navdeep Jaitly. How far are we from intelligent visual deductive reasoning? In First Conference on Language Modeling. 18 [66] Damien Teney, Peng Wang, Jiewei Cao, Lingqiao Liu, Chunhua Shen, and Anton van den Hengel. V-prom: benchmark for visual reasoning using visual progressive matrices. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 1207112078, 2020. 18 [67] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. Viton-hd: High-resolution virtual try-on via misalignment-aware normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1413114140, 2021. 18 [68] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 10961104, 2016. 18 [69] Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang. Lvos: benchmark for long-term video object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1348013492, 2023. 5, 18 [70] Dana Cohen Hochberg, Oron Anschel, Alon Shoshan, Igor Kviatkovsky, Manoj Aggarwal, and Gerard Medioni. Towards quantitative evaluation metrics for image editing approaches. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 78927900, 2024. [71] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1226812290, 2024. 7 14 [72] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 7 [73] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024."
        },
        {
            "title": "A Detailed Tasks Explanation",
            "content": "Based on the previously defined knowledge categories, we further refine them into 7 capability dimensions, each capturing distinct aspect of visual reasoning. To systematically evaluate these dimensions, we design suite of 22 representative tasks that span wide range of perceptual, conceptual, and procedural challenges. In the following section, we comprehensively explain each tasks. A.1 Factual Knowledge Tasks in this category evaluate fundamental visual and temporal understanding that does not require external knowledge or abstract reasoning. These tasks rely on direct perception and low-level cognitive operations. We divide this category into three sub-dimensions: Attribute Perception, Spatial Perception, and Temporal Prediction. Attribute Perception: Count Change. Modify the number of specific objects in an image based on the instruction, testing the models ability to perceive and edit object quantities accurately. Color Change. Modify the color of specified object or region, evaluating the models ability to recognize and apply precise color transformations. Size Adjustment. Modify the size of target object to match reference, evaluating the models understanding of relative scale and spatial consistency. Part Completion. Fill in missing or occluded parts of objects using visual context, testing spatial reasoning and shape completion ability. Anomaly Correction. Detect and fix visually or logically implausible elementssuch as anatomical errors, structural anomalies, or impossible object configurations, to ensure real-world plausibility and visual coherence. Spatial Perception: Position Movement. Move objects to target locations within the image, requiring spatial understanding and coherent object placement relative to surrounding elements. Viewpoint Change. Translate between different viewpoints (e.g., front, side, top) of the same object, testing spatial imagination and 3D reasoning ability. Temporal Prediction: Reverse Prediction. Given several consecutive future frames, infer and reconstruct plausible earlier frame in the sequence. This task tests the models ability to reason backward over temporal dynamics while preserving consistency in motion and appearance. Intermediate Prediction. Predict missing intermediate frame given the surrounding frames in temporal sequence. This task requires understanding temporal continuity, motion interpolation, and visual coherence across multiple time steps. Forward Prediction. Predict the future frame based on several earlier frames in visual sequence. This evaluates the models ability to extrapolate motion and anticipate changes in the scene based on past observations. A.2 Conceptual Knowledge Tasks in this category require understanding and applying real-world knowledge beyond perceptual cues. They often involve reasoning grounded in external knowledge systems, such as cultural norms, scientific principles, or domain-specific rules. We divide this category into two sub-dimensions: Social Science and Natural Science. Social Science: 16 Practical Knowledge. Apply everyday commonsense reasoning to adjust objects or scenarios in plausible, real-world ways, e.g., modifying clock for daylight saving time or removing meat from vegetarian meal. Humanities. Edit images based on cultural, historical, or religious context. Tasks require understanding symbolic elements such as traditional foods, attire, landmarks, or artifacts. For example, replacing dish with mooncakes for the Mid-Autumn Festival. Natural Science: Biology. Apply biological principles to depict realistic life stages, behaviors, or environmental responses, e.g., fruit ripening, animal defense reactions, or plant seasonal changes. Chemistry. Modify images based on chemical properties, reactions, or material transformations. For example, show color changes from pH indicators or gas generation during acidbase reactions. Geography. Modify images by incorporating spatial, climatic, and geological concepts. This includes changes in terrain, celestial events, or weather-related effects such as snowfall, tides, or desertification. Mathematics. Perform modifications guided by mathematical concepts, including geometric properties, algebraic transformations, graph theory, and so on. Medicine. Apply medical understanding to visualize anatomical structure, physiological signals, pathological symptoms, or treatment-related conditions. Physics. Apply knowledge of physical laws and principles such as motion, force, thermodynamics, optics, and electromagnetism to guide image modifications. A.3 Procedural Knowledge Tasks in this category involve executing structured reasoning processes and following complex or multi-step instructions that go beyond simple visual matching. These tasks typically require planning, rule-following, and the integration of multiple operations into coherent output. We divide this category into two sub-dimensions: Logical Reasoning and Instruction Decomposition. Logical Reasoning: Abstract Reasoning. Reason about symbolic structures, numerical relationships, or highlevel conceptual patterns that go beyond literal visual interpretation, often requiring logical deduction, analogy, or transformation rules. Rule-based Reasoning. Apply explicit and well-defined rules to guide visual transformations, such as maze solving, game logic (e.g., Sudoku, Tic-Tac-Toe), or constraint satisfaction, requiring precise adherence to task constraints and rule consistency. Instruction Decomposition: Multi-instruction Execution. This category focuses on executing multiple sequential editing instructions in coherent manner. typical task involves designing posters or product visuals from given object, requiring identity preservation and edits such as background generation, text placement, and lighting adjustment. Multi-element Composition. This category focuses on integrating visual elements from multiple sources into coherent scene. Representative tasks include replacing clothing with provided reference or inserting objects from several images, requiring segmentation, spatial reasoning, and consistent visual blending."
        },
        {
            "title": "B Data Distribution",
            "content": "To support comprehensive evaluation of knowledge-based image editing, our benchmark comprises total of 1,267 instances spanning 22 task types. Each task is designed to reflect unique combination of knowledge requirements and reasoning dimensions. Figure 6 shows three views of the dataset: by knowledge type (left), by reasoning dimension (center), and by individual editing task (right). 17 Figure 6: Distribution of KRIS-Bench instances by knowledge type (left), reasoning dimension (center), and editing task (right). Knowledge Type Breakdown. Conceptual Knowledge has the most instances (518, 40.9%), followed by Factual Knowledge (449, 35.4%) and Procedural Knowledge (300, 23.7%). Reasoning Dimension Breakdown. Natural Science dominates with 393 instances (31.0%), followed by Attribute Perception (275, 21.7%). Logical Reasoning and Instruction Decomposition each contribute 150 (11.8%), with Social Science (125, 9.9%), Spatial Perception (100, 7.9%), and Temporal Prediction (74, 5.8%) trailing behind. Editing Task Breakdown. Among 22 unique tasks, nine have the highest count of 75 (5.9%), including Mathematics, Abstract Reasoning, and Multi-instruction Execution. Biology appears 68 times (5.4%), while perceptual tasks like Color Change and Size Adjustment each have 50 (3.9%)."
        },
        {
            "title": "C Data Source",
            "content": "Most images in our benchmark were collected from the internet under Creative Commons licenses to ensure eligibility for academic use. smaller portion was generated using generative models [51] or sourced from existing datasets [13, 6469]. For the Viewpoint Change task, we utilized 3D assets from the Amazon-Berkeley Objects (ABO) dataset [64] and Sketchfab (https://sketchfab.com/) to enable accurate evaluation with ground truth views. The Abstract Reasoning task includes atomic examples derived from prior works [65, 66] and extended through manual annotation. Some samples for the Multi-element Composition task were taken from virtual try-on datasets [67, 68]. For the Temporal Prediction dimension, we incorporated some clips from video object segmentation datasets [69] and searched through the internet, including freely available videos that permit academic use."
        },
        {
            "title": "D User Study Details",
            "content": "We recruited 12 human annotators, all with at least an undergraduate-level education, to conduct the user study, given that KRIS-Bench involves knowledge-based reasoning tasks. The study adhered to ethical standards, with compensation set above the local minimum wage. All annotators received at least one round of training and performed trial annotation session. Their results were then reviewed and discussed in pairs to ensure alignment with the evaluation criteria. Given the potential subjectivity in human scoring, we normalized the raw scores into three qualitative categories: Good, Fair, and Poor, which were subsequently mapped to numerical scores of 5, 3, and 1, respectively. For each sample, we collected ratings from at least two annotators, and the final score was computed as the average of the individual ratings. 18 Figure 7: Performance on KRIS-Bench across different editing tasks and four different metrics using Qwen2.5-VL-72B as scoring VLM. Top: closed-source models. Bottom: open-source models. Open-source VLM Evaluation To ensure transparency and reproducibility, we adopt the open-source vision-language model Qwen2.5-VL-72B as proxy judge to score the predictions of each evaluated model. The results are presented in Figure 7. As shown, the scoring trends across tasks align closely with those obtained using GPT-4o (May 2025) in Figure 4. Table 3 further summarizes the performance across different knowledge dimensions and evaluation metrics based on Qwen2.5-VL-72Bs assessments."
        },
        {
            "title": "F Computing Source Requirements",
            "content": "All experiments on open-source models were conducted on server equipped with dual Intel Xeon Platinum 8468 CPUs (192 threads), 960 GB RAM, and 8NVIDIA H100 80GB GPUs. Each model required approximately 2 hours to complete all 1,267 editing tasks. Closed-source models were accessed via official APIs or web platforms, where compute details are not user-controllable. No additional large-scale pretraining or auxiliary runs were performed beyond the reported experiments."
        },
        {
            "title": "G More Visualization Results",
            "content": "In this section, we present additional qualitative results. The results show that most models struggle with Count Change tasks and often fail to correct anomalies in the image (Figure 8, Figure 9). For the Part Completion task, many models are unable to infer missing components in the image unless explicitly instructed (e.g., complete the bottle cap) (Figure 10). In contrast, performance on the Color Change task is generally strong across all models (Figure 11). In the Spatial Perception dimension, GPT-4o consistently outperforms other models, especially in tasks involving Viewpoint Change and Position Movement (Figure 13, Figure 14). However, its performance on the Size Adjustment task is relatively weak, frequently failing to apply the correct edits (Figure 12). Regarding Temporal Prediction, both GPT-4o and Gemini 2.0 demonstrate certain degree of temporal reasoning with logically coherent outputs. In contrast, models such as Doubao, OminiGen, and Emu2 generally fail to generate reasonable predictions (Figure 15). We further present results on Conceptual Knowledge across multiple domains in Figures 16, 17, 18, 19, 20, 21, 22, and 23. Open-source models rarely succeed on these tasks, possibly due to the domain-specific nature of the content, which may fall outside their training distributions. 19 Table 3: Performance of different models across different reasoning dimensions and metrics, including Visual Consistency (VC), Visual Quality (VQ), Instruction Following (IF), and Knowledge Plausibility (KP). Scores marked with indicate models unable to handle multi-image input tasks, with the corresponding task scores set to 0. The performance of open-source and closed-source models is separately marked with the best performance in bold, and the second best underlined. In this table, we use Qwen2.5-VL-72B as scoring VLM. Reasoning Dimension Metric Spatial Perception Attribute Perception Temporal Prediction Average Social Science VC VQ IF Avg VC VQ IF Avg VC VQ IF Avg VC VQ IF KP Avg VC VQ IF KP Avg VC VQ IF KP Avg VC VQ IF Avg Average Overall Average Natural Science Average Logical Reasoning Instruction Decomposition Closed-Source Models Open-Source Models GPT-4o Gemini 2.0 Doubao OmniGen Emu2 BAGEL BAGEL-Think 54.25 59.00 34.50 49.25 46.50 64.50 13.75 41.58 24.75 48.25 24.75 32.58 44.79 49.50 51.25 22.50 16.50 34.94 47.00 58.25 18.25 12.50 34.00 34.23 24.25 55.50 5.00 1.50 21.56 39.50 67.25 34.75 47.17 34.37 38.00 92.00 78.00 55.25 75.08 94.00 82.25 49.50 75.25 0.00 0.00 0.00 0.00 62.75 88.75 79.75 48.00 42.75 64.81 80.25 77.75 46.25 42.75 61.75 62.49 82.75 87.75 23.00 15.25 52.19 43.25 31.50 25.25 33.33 42.76 57.91 89.25 72.00 47.50 69.58 77.00 79.25 47.50 67.83 0.00 0.00 0.00 0.00 57.73 87.00 76.75 34.75 29.25 56.94 81.50 78.00 33.75 27.50 55.19 55.61 89.75 88.00 20.75 13.25 52.94 39.25 31.75 27.75 32.92 42.93 53.36 90.25 87.50 63.00 80.25 87.00 91.50 36.50 71.67 24.75 74.25 26.75 41.92 72.02 80.75 86.50 56.75 51.50 68.88 82.25 85.75 45.75 41.25 63.75 64.99 81.00 87.50 27.75 21.25 54.38 73.00 79.25 62.25 71.50 62.94 67.00 77.50 74.50 41.25 64.42 77.25 81.75 28.25 62.42 12.75 54.50 13.75 27.00 57.81 59.00 70.00 18.50 12.50 40.00 66.50 75.75 21.50 16.00 44.94 43.75 46.25 81.00 8.00 4.00 34.81 50.25 70.50 36.25 52.33 43.57 48.69 91.75 94.00 82.50 89.42 92.00 96.00 73.25 87.08 69.50 88.75 75.25 77.83 86.99 88.75 91.75 79.75 78.25 84.63 87.00 91.00 69.25 67.25 78.63 80.08 89.25 96.25 48.25 43.75 69.38 81.25 96.75 85.50 87.83 78.61 82. 88.00 80.00 58.00 75.33 83.00 87.00 46.50 72.17 59.50 71.50 66.00 65.67 73.03 82.75 82.00 57.25 53.00 68.75 78.00 81.25 42.75 37.00 59.75 61.92 85.75 90.75 34.50 28.75 59.94 72.75 83.25 70.75 75.58 67.76 67.24 Step1X-Edit AnyEdit MagicBrush 73.50 77.50 40.00 63.58 71.25 81.00 24.50 58.92 0.00 0.00 0.00 0.00 52.06 81.50 81.50 23.50 20.00 51.63 77.50 83.00 22.75 19.75 50.75 50.96 68.25 84.50 12.00 11.00 43.94 40.50 43.00 10.75 31.42 37.68 48.21 82.50 72.50 40.00 65.00 76.50 81.75 23.75 60.67 0.00 0.00 0.00 0.00 53.32 78.50 77.75 28.75 22.75 51.94 82.75 79.00 28.00 21.00 52.69 52.51 78.75 81.25 20.00 14.00 48.50 30.25 32.25 15.25 25.92 37.21 49.17 74.75 81.50 47.50 67.92 61.25 83.50 25.25 56.67 0.00 0.00 0.00 0.00 54.22 63.25 79.00 33.75 30.25 51.56 60.50 83.00 22.25 18.00 45.94 47.30 62.50 88.75 11.25 8.25 42.69 30.25 39.75 9.50 26.50 34.60 46.74 InsPix2Pix 34.00 67.00 22.50 41.17 30.75 65.75 13.50 36.67 0.00 0.00 0.00 0.00 33.38 28.75 63.25 19.25 12.75 31.00 32.50 69.75 16.75 12.75 32.94 32.47 31.00 84.00 5.50 3.50 31.00 21.50 34.75 5.75 20.67 25.84 31.22 e n l c g w l p o d o a e P Interestingly, all three closed-source models exhibit some capability in Instruction Decomposition (Figure 24, Figure 25). However, they fall short in the Logical Reasoning dimension (Figure 26, Figure 27), highlighting significant limitations in current models logical reasoning abilities."
        },
        {
            "title": "H Evaluation Prompts",
            "content": "Figures 28, 29, and 30 illustrate the prompts used to evaluate Visual Consistency, Visual Quality, and Instruction Following, respectively. Specifically, for the reasoning dimension involving Knowledge Plausibility, we observed that evaluating Instruction Following and Knowledge Plausibility separately can introduce inconsistencies and lead to inaccurate model assessments. Thus, we jointly evaluate both aspects in single prompt, as shown in Figures 31 and 32. Considering that Temporal Prediction and Multi-element Composition involve multiple reference images, we designed customized prompts for evaluating Visual Consistency and Instruction Following, presented in Figures 33 and 34. For the Viewpoint Change task, where ground truth images are available, we provide an additional Instruction Following prompt that uses the ground truth image as reference, shown in Figure 35. As shown in Figure 36, we design dedicated prompt for the Anomaly Correction task by incorporating knowledge hint to facilitate accurate evaluation. 20 Figure 8: Visualization results of Count Change task. Figure 9: Visualization results of Anomaly Correction task. 21 Figure 10: Visualization results of Part Completion task. Figure 11: Visualization results of Color Change task. 22 Figure 12: Visualization results of Size Adjustment task. Figure 13: Visualization results of Viewpoint Change task. 23 Figure 14: Visualization results of Position Movement task. Figure 15: Visualization results of Temporal Prediction tasks. 24 Figure 16: Visualization results of Humanities task. Figure 17: Visualization results of Practical Knowledge task. 25 Figure 18: Visualization results of Biology task. Figure 19: Visualization results of Chemistry task. 26 Figure 20: Visualization results of Geography task. Figure 21: Visualization results of Medicine task. 27 Figure 22: Visualization results of Mathematics task. Figure 23: Visualization results of Physics task. 28 Figure 24: Visualization results of Multi-element Composition task. Figure 25: Visualization results of Multi-instruction Execution task. 29 Figure 26: Visualization results of Abstract Reasoning task. Figure 27: Visualization results of Rule-based Reasoning task. 30 Figure 28: Prompt used to evaluate Visual Consistency. 31 Figure 29: Prompt used to evaluate Visual Quality. Figure 30: Prompt used to evaluate Instruction Following. 33 Figure 31: Joint evaluation prompt for Instruction Following where the model is asked to assess both in unified manner to avoid evaluation misalignment. 34 Figure 32: Joint evaluation prompt for Knowledge Plausibility where the model is asked to assess both in unified manner to avoid evaluation misalignment. Figure 33: Customized prompt for Temporal Prediction dimension. 36 Figure 34: Prompt for evaluating Multi-element Composition task. 37 Figure 35: Instruction Following prompt for the Viewpoint Change task, where the evaluation leverages the ground truth image as visual reference. Figure 36: Prompt designed for the Anomaly Correction task, where knowledge hint is provided as an additional reference to guide the evaluation of whether the anomaly is correctly identified and resolved."
        }
    ],
    "affiliations": [
        "Max Planck Institute for Informatics",
        "Shanghai Jiao Tong University",
        "Southeast University",
        "StepFun",
        "University of California, Berkeley",
        "University of California, Merced"
    ]
}