{
    "paper_title": "RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT",
    "authors": [
        "Chuyu Zhao",
        "Hao Huang",
        "Jiashuo Guo",
        "Ziyu Shen",
        "Zhongwei Zhou",
        "Jie Liu",
        "Zekuan Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Semi-supervised learning has become a compelling approach for 3D tooth segmentation from CBCT scans, where labeled data is minimal. However, existing methods still face two persistent challenges: limited corrective supervision in structurally ambiguous or mislabeled regions during supervised training and performance degradation caused by unreliable pseudo-labels on unlabeled data. To address these problems, we propose Region-Aware Instructive Learning (RAIL), a dual-group dual-student, semi-supervised framework. Each group contains two student models guided by a shared teacher network. By alternating training between the two groups, RAIL promotes intergroup knowledge transfer and collaborative region-aware instruction while reducing overfitting to the characteristics of any single model. Specifically, RAIL introduces two instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller improves supervised learning by instructing predictions only within areas where student outputs diverge from both ground truth and the best student, thereby concentrating supervision on structurally ambiguous or mislabeled areas. In the unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces agreement in regions with high model certainty while reducing the effect of low-confidence predictions during training. This helps prevent our model from learning unstable patterns and improves the overall reliability of pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets show that RAIL surpasses state-of-the-art methods under limited annotation. Our code will be available at https://github.com/Tournesol-Saturday/RAIL."
        },
        {
            "title": "Start",
            "content": "RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT 5 2 0 2 6 ] . [ 1 8 3 5 3 0 . 5 0 5 2 : r Chuyu Zhao1*, Hao Huang1*, Jiashuo Guo1*, Ziyu Shen1*, Zhongwei Zhou3, Jie Liu1, Zekuan Yu2 1School of Computer Science & Technology, Beijing Jiaotong University, Beijing 100044, China 2Academy for Engineering and Technology, Fudan University, Shanghai 200433, China 3Department of Oral and Maxillofacial Surgery, General Hospital of Ningxia Medical University, Yinchuan 750004, China {22723077, 22722088, 22722087, 23722061}@bjtu.edu.cn, zzwjoel@hotmail.com"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Semi-supervised learning has become compelling approach for 3D tooth segmentation from CBCT scans, where labeled data is minimal. However, existing methods still face two persistent challenges: limited corrective supervision in structurally ambiguous or mislabeled regions during supervised training and performance degradation caused by unreliable pseudo-labels on unlabeled data. To address these problems, we propose Region-Aware Instructive Learning (RAIL), dual-group dual-student, semisupervised framework. Each group contains two student models guided by shared teacher network. By alternating training between the two groups, RAIL promotes intergroup knowledge transfer and collaborative region-aware instruction while reducing overfitting to the characteristics of any single model. Specifically, RAIL introduces two instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller improves supervised learning by instructing predictions only within areas where student outputs diverge from both ground truth and the best student, thereby concentrating supervision on structurally ambiguous or mislabeled areas. In the unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces agreement in regions with high model certainty while reducing the effect of low-confidence predictions during training. This helps prevent our model from learning unstable patterns and improves the overall reliability of pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets show that RAIL surpasses state-of-the-art methods under limited annotation. Our code will be available at https://github.com/Tournesol-Saturday/RAIL. *These authors contributed equally to this work. Corresponding author: jieliu@bjtu.edu.cn, yzk@fudan.edu.cn Semi-supervised learning (SSL) has become practical solution for 3D tooth segmentation in CBCT [11], [10], [14], [16], [7], [8], [15], [29], where the cost and effort of manual annotation remain prohibitive in clinical-scale datasets. Semi-supervised methods [5], [23], [27], [17], [19], [25], [12], [4], [1], [22] address annotation bottlenecks in medical imaging by exploiting small labeled subset together with abundant unlabeled data. Recent approaches in semi-supervised medical segmen- [22] employ primarily two key strategies: tation [12], pseudo-labeling and consistency regularization. Pseudo-labeling [5], [23], [27], [17], [19], [25], [7] enables the model to generate provisional annotations for unlabeled inputs, which are subsequently leveraged as training signals. However, pseudo-labeling is prone to challenges, especially when the generated pseudo-labels are incorrect or unreliable [8], [15]. Such inaccuracies can degrade the models performance, especially in regions with structural ambiguity caused by noisy data or complex anatomical features, where insufficient corrective supervision leads to inaccurate predictions. In contrast, consistency-regularization [24], [15] methods are designed to ensure that models predictions remain stable for the same input across different perturbations, such as noisy data or random transformations. Recently, multimodel frameworks [3], [19], [24], [11], [7], [21], [12], [22] have been extensively applied to ensure stable and reliable predictions in medical image segmentation. However, these methods can lead to model imbalance, where inconsistencies in predictions across models can cause overfitting and the amplification of errors [29]. To overcome such limitations, we propose the RegionAware Instuctive Learning (RAIL), dual-group, dualstudent Mean Teacher framework for semi-supervised 3D CBCT segmentation, where the two groups are alternatively involved in training and gradient updating, allowing for more balanced and effective model collaboration, leading to better generalization and reduced overfitting. Specifically, we introduce two key modules: the Disagreement-Focused Supervision (DFS) Controller, which processes the differences between the student network output, ground truth, and the best students output, guiding the model to focus on areas of structural ambiguity or incorrect labeling, and the Confidence-Aware Learning (CAL) Modulator that identifies regions of discrepancy between student network pseudo-labels and the best student pseudo-labels, ensuring the model places less emphasis on uncertain areas and reduces the impact of low-confidence predictions in unsupervised learning. Our major contributions are summarized as follows: We propose dual-group, dual-student Mean Teacher framework for semi-supervised 3D tooth segmentation from CBCT. We design Disagreement-Focused Supervision (DFS) Controller to target areas with structural ambiguity or incorrect labeling. We design Confidence-Aware Learning (CAL) Modulator to enhance pseudo-label reliability. Extensive experiments were conducted on four CBCT tooth segmentation datasets (FDDI+, FDDI-E, 3D CBCT Tooth, and CTooth) to evaluate the RAIL algorithm. The results demonstrate that RAIL achieves competitive performance under sparse supervision, outperforming prior methods with limited labeled data. 2. Related Work cal ambiguity, limiting their accuracy and generalizability. Deep learning has recently enhanced CBCT-based tooth analysis. Cui et al. [7] introduced ToothNet, an end-toend model for instance-level segmentation and classification, yielding superior accuracy compared to conventional approaches. Introduced an end-to-end artificial intelligence solution aimed at segmenting dental and alveolar structures from CBCT data, demonstrating resilience even in anatomIn related adically complex or artifact-prone scans. vancement, Jing et al. [15] proposed dual-phase semisupervised framework that incorporates an Adaptive Channel Interaction Module (ACIM) alongside an uncertaintyguided regularization mechanism. In 2024, Hao et al. developed the T-Mamba architecture, which integrates Tim block with DenseVNet to jointly leverage shared positional encodings and frequency-oriented representations. Zhong et al. [29] proposed lightweight segmentation architecture named PMFSNet, which integrates PMFS block to achieve an effective compromise between computational cost and segmentation precision in the context of dental imaging. However, deep learning methods often require extensive manual annotations and face challenges in handling limited annotated data. Accordingly, designing resilient architectures capable of integrating both global contextual cues and fine-grained local features remains essential, particularly in light of the scarcity of annotated samples and the anatomical intricacy inherent in dental structures. Our work addresses this by proposing Region-Aware Instructive Learning (RAIL), novel framework that integrates dualstudent models and employs region-aware instruction to improve segmentation under limited annotation. 2.1. Tooth segmentation in CBCT 2.2. Semi-supervised learning in segmentation Tooth boundary extraction from CBCT images remains persistent challenge due to anatomical complexity and imaging artifacts. Over the years, the field has witnessed methodological shiftfrom classical algorithmic solutions to modern deep learning paradigmsreflecting significant progress in both accuracy and automation. Early methods, such as level-set and graph-cut algorithms, have laid the foundation for tooth segmentation. For instance, Gao et al. [11] applied level-set models enhanced with prior knowledge of shape and intensity distributions. Building on this, Gan et al. [10] proposed hybrid strategy that integrates multiple energy functionals to enable more accurate contour evolution during segmentation. Similarly, Ji et al. [14] introduced specialized level-set approach tailored for the segmentation of anterior teeth in CBCT scans. In addition, Graph-cut techniques have also been widely adopted, with Keustermans et al. [16] incorporating statistical shape models to improve segmentation robustness. While effective under controlled conditions, these methods often depend on manual initialization and degrade under noise or anatomiSemi-supervised learning (SSL) offers an effective solution to annotation scarcity in medical image segmentation [5], [23], including Ultrasound Computed Tomography (USCT) [9]. common SSL strategy, pseudo-labeling [13], suffers from low-confidence predictions due to insufficient labeled data. To mitigate this, consistency regularization methods enforce prediction consistency under perturbations, thereby enhancing model robustness. Yu et al. [27] introduced self-ensembling strategy called UA-MT. The method leverages Monte Carlo dropout to estimate predictive uncertainty and reduce the influence of unreliable regions. Building on this idea, Li et al. [17] developed SASSNet, which incorporates structural priors into semi-supervised 3D segmentation framework to enhance anatomical fidelity. Further extending this line of work, Luo et al. [19] proposed DTC, dual-task architecture that concurrently predicts voxel-wise masks and geometric descriptors to preserve shape consistency during training. By enforcing consistency between these tasks, this framework significantly enhances segmentation accuracy while reducFig. 1. Pipeline of our Region-Aware Instructive Learning (RAIL) framework in Mean Teacher architecture. The total loss function for every student network in the training phase includes supervised losses Ls, LDF S, and unsupervised losses LU , LT , LCAL. ing the reliance on labeled data. Concurrently, Wu et al. [25] devised MC-Net, mutual consistency-based training strategy for segmenting the left atrium, where predictive alignment across multi-view inputs is enforced to enhance segmentation reliability. Building upon these early efforts, Gao et al. [12] introduced progressive mean teacher (PMT) framework that explores temporal consistency to improve segmentation accuracy over time. This approach, which builds on the Mean Teacher framework, employs exponential moving averages of model weights to guide the student network. In further advancement of consistency-driven learning, Chen et al.[4] advanced consistency learning by unifying three tasks in TTMC for improved 3D analysis. Bai et al.[1] later introduced BCP, data augmentation technique that mixes labeled and unlabeled volumes bidirectionally to boost diversity under semi-supervised settings. In addition, shape-driven and discrepancy-aware methods have emerged to counteract prediction noise and pseudo-label uncertainty. In this regard, Song and Wang [22] introduced student discrepancy-informed correction learning (SDCL) framework, which corrects pseudo-labels based on discrepancies between student models. Despite these advancements, SSL frameworks still face challenges in medical imaging tasks, particularly for handling variability in image quality, anatomical complexity, and the need for more reliable pseudo-labeling methods. To address these challenges, our work introduces two key contributions: Confidence-Aware Learning Modulator (CAL) that enhances pseudo-label reliability by focusing on high-confidence regions and minimizing the impact of lowconfidence areas, and Disagreement-Focused Supervision (DFS) Controller, which targets regions where model predictions diverge. These mechanisms improve pseudo-label reliability, model stability, and segmentation accuracy, particularly in anatomically complex or ambiguous areas, thus enhancing performance under limited annotations. 2.3. Multimodel Framework Multimodel architectures have emerged as pivotal strategy in semi-supervised learning (SSL), especially within the domain of medical image segmentation. These frameworks capitalize on the use of multiple networks or their variants to enforce output consistency, thereby enhancing both model robustness and generalization capability. For instance, Chen et al. [3] proposed Cross-Pseudo Supervision (CPS), where two networks iteratively exchange pseudo-labels to enable collaborative training. Moreover, the dual-task consistency framework, introduced by Luo et al. [19], enforces consistency across multiple tasks within single model, indicating the effectiveness of multitask learning in semi-supervised settings. Currently, the use of heterogeneous models has been further explored to refine output consistency. For instance, Wang et al. [24] presented Mutual Correction Framework (MCF), which employs heterogeneous models to constrain output consistency, thereby enhancing segmentation accuracy under semi-supervised settings. This approach underscores the benefits of model-level regularization in mul- [21] introtimodel frameworks. Additionally, Na et al. duced multiteacher framework, where multiple teachers are used to promote diverse learning in semi-supervised semantic segmentation. It highlights the importance of maintaining diversity among models to prevent overfitting and strengthen generalization. However, multimodel frameworks can suffer from inefficiencies due to overfitting or the propagation of incorrect predictions across models. Our work introduces dualgroup, dual-student framework where inter-group knowledge transfer is promoted, allowing for more balanced and effective model collaboration, leading to better generalization and reduced overfitting. 3. Methodology 3.1. The Overall Pipeline of RAIL 3.1.1. Problem Definition Our training medical image dataset = {Dl, Du} contains labeled images Dl and unlabeled images Du (N }N +M i)}N i, yl ), where Dl = {(xl i=1 and Du = {xu i=N . RW HD in Dl has label Each 3D volume image xl yl {0, 1}W HD, where 0 denotes the background class and 1 represents the foreground target. The model produces an output prediction denoted as ˆyi {0, 1}W HD, representing volumetric segmentation across spatial dimensions. Each time batch is fed into the network, it contains an equal proportional volume of labeled data (X L, L) and unlabeled data . Predictions generated by the network for labeled samples are represented as (cid:98)Y , while those corresponding to unlabeled inputs are denoted by (cid:101)Y . We have incorporated the Mean Teacher architecture within the semi-supervised learning framework. This integration aims to enhance the model by providing highquality pseudo-labels while also ensuring that the models structure facilitates continuous improvement in its representational power. This leads to enhanced performance while maintaining robust diversity. The teacher network mirrors the student network in structure but plays passive role in training. Parameter updates are performed via the Exponential Moving Average (EMA) mechanism, which enforces consistency regularization on the student network by leveraging its predictions. The update follows the EMA formulation given below: = αθ θ t1 + (1 α)θt (1) Here, θ represents the teacher models parameters at the current iteration, while θ t1 corresponds to its parameters from the previous step. The student models parameters at the current iteration are denoted by θt. The hyperparameter α serves as momentum-like smoothing factor that governs the update rate of the teacher network. 3.1.2. Dual-group Dual-student Mean Teacher Framework The training framework of RAIL is shown in Fig. 1. RAIL consists of two groups of Mean Teacher networks with the same framework, which are alternatively involved in training iterations. ti denotes the current training iteration and ti+1 denotes the next turn of the training iteration. Each group of frameworks contains two student networks and one teacher network, where Sv denotes the VNet student network, Sr denotes the ResVNet [24] student network, and denotes the VNet teacher network. Following established practices, we employ the VNet-based student model Sv to update the teacher network via the Exponential Moving Average (EMA) scheme. The training process of RAIL consists of three parts: (i) obtaining some fundamental supervised and unsupervised losses according to the PMT strategy; (ii) DisagreementFocused Supervision (DFS) Controller: generating DiffMask Mdif from the difference between student segmentation and the best student segmentation, and MisMask Mmis from the misalignment between student segmentation and ground truth, thus multiplying Mdif and Mmis to create DiffMisMask Mdif mis to guide the model in focusing supervision on structurally ambiguous or mislabeled voxels; (iii) Confidence-Aware Learning (CAL) Modulator: generating DivMask Mdiv from the divergence between the student pseudo-label and the best student pseudolabel to improve the overall stability and reliability of the pseudo-label. For convenience, the upper and lower corner notations of many symbols are simplified here. more detailed symbolic description is given in later explanations. 3.2. Progressive Mean Teacher Our framework is integrated with the state-of-the-art PMT method [12] to enhance performance. The supervised loss, , Sr i+1; labeled dataset Dl = {(xl Algorithm 1 Training with Confidence-Aware Learning Input: Student networks Sv teacher i)}N i, yl network Ti; i=1; unlabeled dataset Du = {xu boolean flag irst term = rue for initial phase; number of classes = 2. Output: Updated weights for Sv }N +M i=N ; i+1, Sr , Sv , Sr . 1: for each training iteration ti do 2: Sample batch (X L, L), from Dl and Du // Supervised training without CAL 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: if irst term then , Sr for {Sv } do"
        },
        {
            "title": "Update S by backpropagating LDF S",
            "content": "end for Update Ti EMA(Sv ) continue to next iteration // Semi-supervised training with pseudo-labels and CAL algorithm else // Compute supervised Dice loss on labeled data for all students , Sr i+1, Sr dice = DiceLoss(S(X L), L) i+1} do , Sv for {Sv LS end for // Identify the best student and get its pseudo-label div : pixels where Sv/r dice Sbest arg minS LS (cid:101)Y best = Sbest(X ) for {Sv , Sr } do Sv/r (cid:101)Y v/r (X ) // Compute Mv/r disagrees with Sbest Mv/r div = (cid:101)Y v/r (cid:101)Y best // uniform distribution tensor U(k) 1/K, = [1/2, ..., 1/2] Lv/r CAL = DKL( (cid:101)Y v/r U) Mdiv Update by LCAL + LDF i end for Update Ti EMA(Sv ) continue to next iteration end if 24: 25: end for denoted as Ls, is defined as follows: s = LCE( (cid:98)Y v/r Lv/r , L) + βLM SE( (cid:98)Y v/r , L) Mv/r dif (2) where β = 0.5. (cid:98)Y v/r represents the model outputs for labeled data of the two student networks in the current training phase, and denotes the ground truth. LU represents the unsupervised loss: = LM SE( (cid:101)Y v/r Lv/r , (cid:101)Y best) (3) Additionally, the consistency loss LT , derived from the Mean Teacher framework, is computed as the mean squared error between the pseudo-labels (cid:101)Y v/r produced by the student and teacher networks: and (cid:101)Y i = LM SE( (cid:101)Y v/r Lv/r , (cid:101)Y ) (4) 3.3. Disagreement-Focused Supervision In the training phase, we introduce the DisagreementFocused Supervision (DFS) Controller, which minimizes the Kullback-Leibler Divergence [2] between the student models predictions and the ground truth. This approach encourages the model to focus its learning on regions where predictions are correct and where structural clarity is achieved, thereby enhancing the efficacy of supervised learning. i+1, (cid:98)Y i+1 and (cid:98)Y First, the model outputs of the two student networks (cid:98)Y , (cid:98)Y in the current training phase, and the two student neti works (cid:98)Y i+1 in the next turn of the training phase, perform DICE loss calculation with ground truth L, respectively ( (cid:98)Y i+1 do not perform gradient updating). We choose the student network with the highest DICE loss as the current best student, whose corresponding label predictions and pseudo-labels are denoted as (cid:98)Y best and (cid:101)Y best, respectively. We then take (cid:98)Y v/r and (cid:98)Y best after argmax to get the difference set between their union and intersection, which is denoted as Mv/r dif : Mv/r dif = (cid:16) arg max (cid:98)Y v/r (cid:16) arg max (cid:98)Y v/r arg max (cid:98)Y best(cid:17) arg max (cid:98)Y best(cid:17) (5) where and represent VNet students and ResVNet students, respectively. Similarly, we take (cid:98)Y v/r and after argmax to get the difference set between their union and intersection, which is denoted as Mv/r mis: (cid:16) Mv/r mis = arg max (cid:98)Y v/r (cid:16) arg max (cid:98)Y v/r arg max L(cid:17) arg max L(cid:17) (6) Afterward, Mv/r dif mis = Mv/r mis. Ultimately, the loss function LDF is derived as the Kullback-Leibler divergence between the student networks output (cid:98)Y v/r and the ground truth L: dif Mv/r DF = LKL( (cid:98)Y v/r Lv/r , L) Mv/r dif mis (7) Table 1. Ablation results on FDDI+ dataset"
        },
        {
            "title": "Metrics",
            "content": "Labeled Unlabeled Ls + LU + LT LKL Mmis Mdiv Dice (%) Jaccard (%) 95HD (voxel) ASD (voxel) 11 86.06 87.67 87.65 88.32 88. 75.68 78.04 78.04 79.08 79.33 91.60 91.01 48.53 9.06 8.37 17.18 12.22 9.02 8.10 8.67 Table 2. Comparison results on FDDI+ dataset with 9% and 14% labeled data Method Scans used Labeled Unlabeled Metrics Dice Jaccard 95HD ASD V-Net[20](3DV2016) ResV-Net[24](CVPR2023) V-Net[20] (3DV2016) ResV-Net[24](CVPR2023) 7 7 11 11 0 0 0 0 UA-MT[27] (MICCAI2019) SASSNet[17] (MICCAI2020) DTC[19] (AAAI2021) MC-Net+[25] (MICCAI2021) BCP[1] (CVPR2023) TTMC[4] (CBM2024) PMT[12] (ECCV2024) SDCL[22] (MICCAI2024) RAIL (Ours) UA-MT[27] (MICCAI2019) SASSNet[17] (MICCAI2020) DTC[19] (AAAI2021) MC-Net+[25] (MICCAI2021) BCP[1] (CVPR2023) TTMC[4] (CBM2024) PMT[12] (ECCV2024) SDCL[22] (MICCAI2024) RAIL (Ours) 7(9%) 11(14%) 66 79.88 80.54 81.93 81.86 74.60 77.86 37.62 75.92 19.86 77.63 84.91 72.06 66.67 67.54 69.47 69.38 59.55 63.93 26.25 61.66 14.14 63.63 73.85 56. 55.30 53.03 47.24 50.64 125.44 145.55 98.37 136.86 122.67 133.6 9.85 167.24 10.10 9.00 5.12 6.58 41.87 50.24 35.23 40.13 68.83 35.64 3.57 60.11 89.554.64 81.217.36 6.743.11 3.200.37 75.39 77.32 38.37 81.44 39.32 85.10 87.37 66. 60.82 63.35 26.65 68.91 27.88 74.15 77.64 49.8 129.82 135.71 96.25 135.93 93.56 94.07 7.81 172.85 47.19 42.06 36.14 33.25 34.25 16.84 2.67 63.70 89.752.38 81.513.87 6.241.57 2.320.35 3.4. Confidence-Aware Learning In the context of unsupervised learning, we introduce the Confidence-Aware Learning (CAL) Modulator, which seeks to maximize the uncertainty in regions of divergence between the student networks pseudo-labels and those of the current best-performing student. This strategy mitigates the influence of low-confidence prediction regions during model training, thereby enhancing the stability and reliability of the generated pseudo-labels. The workflow of the Confidence-Aware Learning (CAL) Modulator can be summarized in Algorithm 1. Finally, we linearly combine Ls, LDF S, LU , LT , LCAL with specific weights to form the total loss function: As the training progresses, the values of λ1 and λ2 increase according to the iteration, reaching plateau after certain number of iterations. The PMT method utilizes two independent Gaussian warm-up functions to regulate the weights of the loss functions, λ1 and λ2, each governed by distinct parameters: λ1(ti) = λ2(ti) = tmax ) (cid:40)ˆλ1 e5(1 2ti ˆλ1, (cid:40)ˆλ2 e5(1 2ti ˆλ2, tmax )2 , , ti < tmax 2 ti tmax 2 ti < tmax 2 ti tmax 2 (9) Lv/r total = Lv/r + γLv/r DF + λ1(Lv/r + µLv/r CAL) + λ2Lv/r (8) where γ = 0.05 and µ = 0.1. Here, ti and tmax indicate the current and total training steps. The coefficients ˆλ1 and ˆλ2 are empirically initialized to 20.0 and 10.0. Table 3. Comparison results on FDDI-E dataset with 10% and 20% labeled data Method Scans used Labeled Unlabeled Metrics Dice Jaccard 95HD ASD 20 20 40 40 200 200 0 0 0 0 0 0 20(10%) 180 40(20%) 160 V-Net[20](3DV2016) ResV-Net[24](CVPR2023) V-Net[20](3DV2016) ResV-Net[24](CVPR2023) V-Net[20](3DV2016) ResV-Net[24](CVPR2023) UA-MT[27] (MICCAI2019) SASSNet[17] (MICCAI2020) DTC[19] (AAAI2021) MC-Net+[25] (MICCAI2021) BCP[1] (CVPR2023) TTMC[4] (CBM2024) PMT[12] (ECCV2024) SDCL[22] (MICCAI2024) RAIL (Ours) UA-MT[27] (MICCAI2019) SASSNet[17] (MICCAI2020) DTC[19] (AAAI2021) MC-Net+[25] (MICCAI2021) BCP[1] (CVPR2023) TTMC[4] (CBM2024) PMT[12] (ECCV2024) SDCL[22] (MICCAI2024) RAIL (Ours) 87.44 78.94 87.30 77.45 90.10 81.01 85.50 88.56 71.61 88.06 71.10 89.52 88.01 86.59 90.741.22 86.44 90.60 71.96 87.42 66.07 90.01 88.64 85.79 90.920.32 77.93 66.86 77.83 65.11 82.19 69.72 75.48 79.78 58.72 79.15 58.99 81.21 78.83 76.85 83.171.96 76.77 82.98 59.92 78.36 55.73 83.03 79.86 75.94 83.470. 25.77 66.10 32.47 71.86 21.21 65.80 62.61 49.07 43.18 52.63 42.16 41.00 11.23 60.22 5.2735.73 58.97 29.38 43.11 59.01 53.55 24.69 14.15 65.95 5.5823.80 5.46 23.90 7.85 27.30 3.87 25.00 23.82 11.55 1.18 13.25 4.19 8.40 2.27 17.86 1.087.32 20.50 7.32 2.47 20.16 14.91 5.71 2.92 22.75 1.056. Fig. 2. 2D segmentation visualization of different semi-supervised methods on FDDI+ (first line), FDDI-E (second line), 3D CBCT Tooth (third line) and CTooth (last line) dataset under 14%, 10%, 10% and 10% labeled, respectively. 4. Experiments 4.1. Datasets and Metrics Our method is evaluated on four datasets: FDDI+ [28], FDDI-E, 3D CBCT Tooth [8], and CTooth [6]. For each dataset, the training volumes are randomly cropped to size of 112 112 80 to serve as model input. To cope with limited GPU memory and sparse labels, 15 patches are extracted from each scan. The cropped volumes are normalized to reduce scanning-induced noise and artifacts before model input. At inference, predictions are generated using fixed-size sliding window with stride of 64 64 32. Fig. 3. 3D segmentation visualization of different semi-supervised methods on FDDI+ (first line), FDDI-E (second line), 3D CBCT Tooth (third line) and CTooth (last line) dataset under 14%, 10%, 10% and 10% labeled, respectively. 4.1.1. FDDI+ Dataset This study primarily utilizes the Fudan Dual-Modality Dental Imaging (FDDI) dataset [28], which consists of 66 CBCT scans. Additionally, we collect 14 supplementary scans to enhance our experimental analysis, termed as FDDI+ dataset. Informed consent is obtained from all patients, and all original DICOM images are anonymized to ensure privacy. Each scan, acquired using clinical-grade medical imaging instruments, comprises 400 axial slices with resolution of 800 800 with 1mm slice thickness. To comprehensively evaluate the proposed method, we utilize total of 80 3D CBCT scans and design two experimental settings. In the first setting, training includes 7 labeled (9%) and 70 unlabeled scans, while 3 scans are reserved for testing. The second setting adopts 11 labeled (14%) and 66 unlabeled scans for training, with 3 held out for evaluation. 4.1.2. FDDI-E Dataset The second dataset employed in this research is the FDDIE dataset, an extended version of the original FDDI dataset. The FDDI-E dataset contains 286 CBCT scans and the corresponding labels, and their dimensional size is 604604 412. During the experiments, we designed two experimental settings. In the first experimental configuration, 20 labeled volumes (10%) and 180 unlabeled volumes constitute the training set, while 30 labeled volumes are reserved for validation and 56 labeled volumes for testing. In the second configuration, the training set comprises 40 labeled volumes (20%) along with 160 unlabeled volumes, maintaining the same validation and test partitions of 30 and 56 labeled volumes, respectively. 4.1.3. 3D CBCT Tooth Dataset subset of the CBCT dataset from Cui et al. [8] is used, comprising 4,938 CBCT scans obtained from 15 medical centers across China, representing wide range of data distributions. Due to privacy and regulatory restrictions, only portion of this dataset is publicly available. For our experiments, we utilize 126 3D CBCT scans and implement two experimental configurations to assess the proposed method. In the first configuration, 7 labeled scans (5%) and 113 unlabeled scans are used for training, with 6 labeled scans reserved for testing. The second configuration uses 13 labeled (10%) and 107 unlabeled samples for training, with 6 labeled for evaluation. 4.1.4. CTooth Dataset The CTooth dataset [6] includes total of 131 scans, with 22 labeled and 109 unlabeled, providing comprehensive resource for segmentation tasks. To evaluate our method, we design two experimental settings using 122 scans for training and 7 for testing. In the first setting, there are 7 labeled (5%) and 115 unlabeled for training. In the second setting, there are 13 labeled (10%) and 109 unlabeled for training. 4.1.5. Metrics In line with previous works [1, 17], [19], [24], [26], [27], we evaluate model performance using four key metrics. These include regional sensitivity measures, such as the Dice simTable 4. Comparison results on 3D CBCT Tooth dataset with 5% and 10% labeled data Method Scans used Labeled Unlabeled Metrics Dice Jaccard 95HD ASD 7 7 13 13 120 120 0 0 0 0 0 0 7(5%) 13(10%) 107 V-Net[20](3DV2016) ResV-Net[24](CVPR2023) V-Net[20](3DV2016) ResV-Net[24](CVPR2023) V-Net[20](3DV2016) ResV-Net[24](CVPR2023) UA-MT[27] (MICCAI2019) SASSNet[17] (MICCAI2020) DTC[19] (AAAI2021) MC-Net+[25] (MICCAI2021) BCP[1] (CVPR2023) TTMC[4] (CBM2024) PMT[12] (ECCV2024) SDCL[22] (MICCAI2024) RAIL (Ours) UA-MT[27] (MICCAI2019) SASSNet[17] (MICCAI2020) DTC[19] (AAAI2021) MC-Net+[25] (MICCAI2021) BCP[1] (CVPR2023) TTMC[4] (CBM2024) PMT[12] (ECCV2024) SDCL[22] (MICCAI2024) RAIL (Ours) 89.39 79.34 93.51 92.61 94.55 92. 85.83 85.16 90.06 88.38 84.29 80.34 85.00 86.05 91.601.54 91.06 81.10 92.68 92.01 87.17 76.15 87.93 90.72 94.091.41 81.33 65.90 87.98 86.40 89.70 86.94 75.33 74.25 82.19 79.39 74.04 68.22 74.83 75.72 84.732.54 83.81 68.73 86.48 85.42 77.96 64.61 79.10 83.15 88.962.48 2.75 19.25 1.48 1.65 1.24 1. 25.70 33.45 4.96 16.91 17.43 16.00 3.57 30.27 2.032.93 16.55 28.15 2.04 3.14 9.67 43.15 2.75 14.29 1.310.73 0.85 5.17 0.52 0.60 0.40 0.75 5.68 6.91 1.76 3.30 0.64 0.69 1.45 6.01 0.621.14 3.57 7.82 1.49 2.00 0.52 0.55 0.82 3.42 0.441.05 ilarity coefficient (Dice) [27] and the Jaccard similarity coefficient (Jaccard) [19], as well as edge-sensitive metrics, including the 95% Hausdorff Distance (95HD) [26] and the Average Surface Distance (ASD) [1]. 4.2. Implementation Details All experiments were run on an NVIDIA RTX 4090 24GB using Ubuntu 20.04 and PyTorch 1.11.0. We employ PMT [12], Mean Teacher-based semi-supervised baseline. The final prediction aggregates outputs from four student models. Training uses SGD (momentum = 0.9, weight decay = 0.0004) with an initial learning rate of 0.01 and linear warm-up over the first 1,000 iterations. After reaching 4,000 iterations, it is progressively reduced to 1e-5 following cosine annealing schedule [18], with total of 8,000 training iterations. The batch size of 2 is employed, where each batch comprises single labeled sample alongside an unlabeled one. The hyperparameters are configured as α = 0.5, β = 0.05. 4.3. Ablation Study Table 1 presents an ablation analysis evaluating the individual and combined contributions of key components within our framework, based on Dice score improvements over the baseline. The results demonstrate that both the Disagreement-Focused Supervision (DFS) Controller and the Confidence-Aware Learning (CAL) Modulator in the RAIL architecture contribute positively to segmentation performance. the highest performance is achieved when both Mmis and Mdiv are jointly applied, underscoring their synergistic effect. Importantly, 4.4. Compare with Other Methods We conduct comprehensive comparison between our method and existing SOTA approaches across four datasets: FDDI+ dataset, FDDI-E dataset, 3D CBCT Tooth dataset, and CTooth dataset. PMT serves as the primary baseline for evaluation. In addition, we benchmark against several representative methods, including UA-MT [27], which introduces uncertaintyaware self-ensembling; SASSNet [17], which incorporates geometric shape constraints; and DTC [19], which exploits dual-task consistency for enhanced structural prediction. We also include MC-Net+ [25] with dual-decoder mutual consistency, TTMC [4], introducing triple-task mutual consistency framework, and BCP [1], which employs bidirectional Copy-Paste to align labeled and unlabeled data distributions. Additionally, SDCL [22] enhances semi-supervised segmentation by incorporating student discrepancy-informed correction learning. Table 5. Comparison results on CTooth dataset with 5% and 10% labeled data Method Scans used Labeled Unlabeled Metrics Dice Jaccard 95HD ASD V-Net[20](3DV2016) ResV-Net[24](CVPR2023) V-Net[20](3DV2016) ResV-Net[24](CVPR2023) 7 7 13 13 0 0 0 0 UA-MT[27] (MICCAI2019) SASSNet[17] (MICCAI2020) DTC[19] (AAAI2021) MC-Net+[25] (MICCAI2021) BCP[1] (CVPR2023) TTMC[4] (CBM2024) PMT[12] (ECCV2024) SDCL[22] (MICCAI2024) RAIL (Ours) UA-MT[27] (MICCAI2019) SASSNet[17] (MICCAI2020) DTC[19] (AAAI2021) MC-Net+[25] (MICCAI2021) BCP[1] (CVPR2023) TTMC[4] (CBM2024) PMT[12] (ECCV2024) SDCL[22] (MICCAI2024) RAIL (Ours) 7(5%) 13(10%) 109 88.09 87.06 88.34 87.48 86.04 81.38 84.90 87.31 81.01 79.50 88.09 85.13 89.361.27 86.57 85.63 85.03 84.72 80.12 80.86 86.74 88.43 89.030.6 78.98 77.31 79.44 77. 75.67 68.81 73.97 77.60 70.09 67.59 78.83 74.53 81.012.18 76.54 75.10 74.15 73.70 68.42 68.56 76.82 79.51 80.490.98 7.01 8.06 7.11 6.90 11.54 29.48 11.6 6.74 32.53 25.81 6.49 15.68 5.481.01 10.52 9.76 10.58 9.41 33.67 20.59 7.90 8.46 6.032.43 1.50 2.10 1.51 1. 3.70 7.30 3.81 2.29 1.84 3.04 1.58 5.07 1.450.13 3.52 3.48 3.32 3.45 2.18 4.88 2.44 3.41 1.581.83 For fair comparison, all methods are configured according to their official settings, with training capped at 8,000 iterations. BCP and SDCL are pre-trained for 2,000 iterations and fine-tuned for the remaining 6,000. 4.4.1. Comparison on FDDI+ Dataset We evaluate our model on FDDI+ under 9% and 14% label ratios. As shown in Table 2, it consistently outperforms PMT and recent strong baselines across all four metrics. With only 9% labeled data, our approach surpasses the strongest competing method by margins of 4.64% in Dice and 7.36% in Jaccard, while reducing 95HD and ASD by 3.11 and 0.37, respectively. Under the 14% setting, our model continues to deliver superior performance, yielding gains of 2.38% in Dice and 3.87% in Jaccard, along with reductions of 1.57 in 95HD and 0.35 in ASD. Remarkably, even with smaller fraction of labeled samples, our framework outperforms PMT trained on 14% labeled data, demonstrating enhanced label efficiency and generalization capability. To further illustrate the effectiveness of our approach, we provide 2D and 3D qualitative visualizations of segmentation results in Fig. 2 and Fig. 3, respectively. The visual comparisons emphasize the superior segmentation quality of our method across different proportions of labeled data, demonstrating its robustness in tackling the challenging FDDI+ dataset. 4.4.2. Comparison on FDDI-E Dataset We evaluate performance under 10% and 20% labeling protocols. As listed in Table 3, our method consistently yields superior results over PMT and recent semi-supervised techniques across all four metrics. When trained with 10% labeled data, our approach delivers performance gains of 1.22% in Dice and 1.96% in Jaccard, along with substantial reductions of 35.73 in 95HD and 7.32 in ASD. Under the 20% labeling scenario, our model continues to outperform the baseline, yielding an additional 0.32% improvement in Dice, 0.49% in Jaccard, decrease of 23.80 in 95HD, and 6.27 reduction in ASD. Fig. 2 and Fig. 3 illustrate representative 2D and 3D segmentation results on the FDDI-E dataset, offering clear visual perspective on model performance. As shown, our approach consistently delivers more accurate and refined segmentation across various labeling ratios. 4.4.3. Comparison on 3D CBCT Tooth Dataset Table 4 provides detailed comparison between our framework and previous leading methods, along with the full supervision bounds. The evaluation is conducted under two annotation ratios (5% and 10%), and visual results in both 2D and 3D (Fig. 2 and Fig. 3) further demonstrate the effectiveness of our framework in segmenting the 3D CBCT Tooth dataset under varying levels of annotation. Across both labeling scenarios, our model consistently Specifically, under the 5% labeled setting, outperforms existing methods across all four evaluation metrics. it achieves improvements of 1.54% in Dice and 2.54% in Jaccard, with corresponding reductions of 2.93 and 1.14 in 95HD and ASD, respectively. When the label ratio is increased to 10%, the model maintains its advantage, yielding gains of 1.41% in Dice and 2.49% in Jaccard, while decreasing 95HD and ASD by 0.73 and 1.05, respectively. Overall, the results underscore our models resilience across varying label proportions, consistently outperforming earlier methods on the densely annotated 3D CBCT Tooth dataset. 4.4.4. Comparison on CTooth Dataset We evaluate our model on the CTooth dataset under 5% and 10% labeling ratios. Table 5shows that our method surpasses PMT and other SOTA baselines in all four metrics. Under the 5% supervision setting, our model achieves gains of 1.27% in Dice and 2.18% in Jaccard, along with reductions of 1.01 in 95HD and 0.13 in ASD, when compared to the strongest competing method. At the 10% annotation level, further improvements are observed, including 1.54% and 2.48% increases in Dice and Jaccard, respectively, and decreases of 0.73 in 95HD and 1.05 in ASD. Interestingly, the results suggest that the inclusion of additional labeled data does not yield substantial performance gains on this dataset, likely due to the suboptimal annotation quality of the CTooth dataset. This observation is further supported by the fully supervised VNet model, which shows minimal improvement between the two settings. In contrast, our method consistently achieves SOTA performance across both label proportions, indicating its robustness in handling datasets with noisy annotations. These findings highlight the effectiveness of our approach, even in scenarios where annotation quality is limiting factor. To facilitate more intuitive understanding of model performance on the CTooth dataset, we present representative 2D and 3D qualitative results in Fig.2 and Fig.3, respectively. These visual comparisons further confirm the superiority of our method over existing approaches across varying annotation ratios in the context of the complex CTooth segmentation task. 5. Conclusion In this paper, we propose Region-Aware Instructive Learning (RAIL), novel dual-group, dual-student semisupervised framework designed for 3D tooth segmentation from CBCT scans. The RAIL model incorporates several innovative mechanisms, including dual-group, dual-student Mean Teacher framework, the DisagreementFocused Supervision (DFS) Controller, and the ConfidenceAware Learning (CAL) Modulator. The dual-group, dualstudent framework allows for alternating training between two student models, fostering inter-group knowledge transfer and reducing overfitting. The DFS Controller specifically targets regions with structural ambiguity or incorrect labeling, guiding the model to focus on challenging areas and significantly improving prediction accuracy in those regions. Meanwhile, the CAL Modulator adjusts the models attention to regions of low confidence, thus stabilizing the learning process by minimizing the impact of unreliable pseudo-labels, which enhances the models robustness. Additionally, we conduct extensive experiments with existing state-of-the-art semi-supervised methods, showing that RAIL consistently outperforms other approaches across several benchmark datasets, including FDDI+, FDDI-E, 3D CBCT Tooth, and CTooth. RAIL not only achieves superior segmentation accuracy but also exhibits greater robustness when trained with limited labeled data. In future work, we aim to enhance the efficiency explore its application to the RAIL framework, of additional modalities, and further improve its ability to generalize to other medical image segmentation tasks."
        },
        {
            "title": "References",
            "content": "[1] Yunhao Bai, Duowen Chen, Qingli Li, Wei Shen, and Yan Wang. Bidirectional copy-paste for semi-supervised medical image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1151411524, 2023. 1, 3, 6, 7, 8, 9, 10 [2] Soufiane Belharbi, Jerˆome Rony, Jose Dolz, Ismail Ben Ayed, Luke McCaffrey, and Eric Granger. Deep interpretable classification and weakly-supervised segmentation of histology images via max-min uncertainty. IEEE Transactions on Medical Imaging, 41(3):702714, 2021. 5 [3] Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong Wang. Semi-supervised semantic segmentation with cross the IEEE/CVF pseudo supervision. Conference on Computer Vision and Pattern Recognition (CVPR), pages 26132622, 2021. 1,"
        },
        {
            "title": "In Proceedings of",
            "content": "[4] Yantao Chen, Yong Ma, Xiaoguang Mei, Lin Zhang, Zhigang Fu, and Jiayi Ma. Triple-task mutual consistency for semi-supervised 3d medical image segmentation. Computers in Biology and Medicine, 175:108506, 2024. 1, 3, 6, 7, 9, 10 [5] Veronika Cheplygina, Marleen de Bruijne, and Josien P. W. Pluim. Not-so-supervised: survey of semi-supervised, multi-instance, and transfer learning in medical image analysis. Medical Image Analysis, 54:280296, 2019. 1, 2 [6] Weiwei Cui, Yaqi Wang, Qianni Zhang, Huiyu Zhou, Dan Song, Xingyong Zuo, Gangyong Jia, and Liaoyuan Zeng. Ctooth: fully annotated 3d dataset and benchmark for tooth volume segmentation on cone beam computed tomography images. In International Conference on Intelligent Robotics and Applications, pages 191200. Springer, 2022. 7, 8 [7] Zhiming Cui, Changjian Li, and Wenping Wang. Toothnet: Automatic tooth instance segmentation and identification from cone beam ct images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 1, 2 [8] Zhiming Cui, Yu Fang, Lanzhuju Mei, Bojun Zhang, Bo Yu, Jiameng Liu, Caiwen Jiang, Yuhang Sun, Lei Ma, Jiawei Huang, et al. fully automatic ai system for tooth and alveolar bone segmentation from cone-beam ct images. Nature communications, 13(1):2096, 2022. 1, 7, 8 [9] Neb Duric, Peter Littrup, Libero Poulo, Alex Babkin, Roman Pevzner, Eric Holsapple, Ogonna Rama, and Cheryl Glide. Detection of breast cancer with ultrasound tomography: First results with the computed ultrasound risk evaluation (cure) prototype. Medical Physics, 34(2):773785, 2007. 2 [10] Yangzhou Gan, Zeyang Xia, Jing Xiong, Qunfei Zhao, Ying Hu, and Jianwei Zhang. Toward accurate tooth segmentation from computed tomography images using hybrid level set model. Medical Physics, 42(1):1427, 2015. 1, 2 [11] Hui Gao and Oksam Chae. Individual tooth segmentation from ct images using level set method with shape and intensity prior. Pattern Recognition, 43(7):24062417, 2010. 1, 2 [12] Ning Gao, Sanping Zhou, Le Wang, and Nanning Zheng. Pmt: Progressive mean teacher via exploring temporal consistency for semi-supervised medical image segmentation. In European Conference on Computer Vision, pages 144160. Springer, 2024. 1, 3, 4, 6, 7, 9, 10 [13] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-supervised learning. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 50705079, 2019. 2 [14] Dong Xu Ji, Sim Heng Ong, and Kelvin Weng Chiong Foong. level-set based approach for anterior teeth segmentation in cone beam computed tomography images. Computers in Biology and Medicine, 50:116128, 2014. 1, 2 [15] Yixin Jing, Jie Liu, Weifan Liu, Zhicheng Yang, ZhongWei Zhou, and Zekuan Yu. Usct: Uncertainty-regularized symmetric consistency learning for semi-supervised teeth segmentation in cbct. Biomedical Signal Processing and Control, 91:106032, 2024. 1, 2 [16] Johannes Keustermans, Dirk Vandermeulen, and Paul Suetens. Integrating statistical shape models into graph cut framework for tooth segmentation. In Machine Learning in Medical Imaging, pages 242249, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg. 1, 2 [17] Shuailin Li, Chuyu Zhang, and Xuming He. Shape-aware semi-supervised 3d semantic segmentation for medical images. In Medical Image Computing and Computer Assisted InterventionMICCAI 2020: 23rd International Conference, Lima, Peru, October 48, 2020, Proceedings, Part 23, pages 552561. Springer, 2020. 1, 2, 6, 7, 8, 9, [18] Ilya Loshchilov and Frank Hutter. tic gradient descent with warm restarts. arXiv:1608.03983, 2016. 9 Sgdr: StochasarXiv preprint [19] Xiangde Luo, Jieneng Chen, Tao Song, and Guotai Wang. Semi-supervised medical image segmentation through dualtask consistency. In Proceedings of the AAAI conference on artificial intelligence, pages 88018809, 2021. 1, 2, 4, 6, 7, 8, 9, 10 [20] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In Fourth International Conference on 3D Vision, 3DV 2016, pages 565571. IEEE Computer Society, 2016. 6, 7, 9, [21] Jaemin Na, Jung-Woo Ha, Hyung Jin Chang, Dongyoon Han, and Wonjun Hwang. Switching temporary teachers In Advances for semi-supervised semantic segmentation. in Neural Information Processing Systems, pages 40367 40380. Curran Associates, Inc., 2023. 1, 4 [22] Bentao Song and Qingfeng Wang. SDCL: Students Discrepancy-Informed Correction Learning for Semisupervised Medical Image Segmentation . In proceedings of Medical Image Computing and Computer Assisted Intervention MICCAI 2024. Springer Nature Switzerland, 2024. 1, 3, 6, 7, 9, 10 [23] Nima Tajbakhsh, Latha Jeyaseelan, Qian Li, Jeffrey N. Chiang, Zhiwei Wu, and Xiaowei Ding. Embracing imperfect datasets: review of deep learning solutions for medical image segmentation. Medical Image Analysis, 63:101693, 2020. 1, 2 [24] Yuyin Wang, Biao Xiao, Xiang Bi, Wen Li, and Xinjian Gao. Mcf: Mutual correction framework for semisupervised medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1565115660, 2023. 1, 4, 6, 7, 8, 9, 10 [25] Yicheng Wu, Minfeng Xu, Zongyuan Ge, Jianfei Cai, and Lei Zhang. Semi-supervised left atrium segmentation with In Medical Image Computmutual consistency training. ing and Computer Assisted Intervention - MICCAI 2021: 24th international conference, Strasbourg, France, September 27October 1, 2021, proceedings, part II 24, pages 297 306. Springer, 2021. 1, 3, 6, 7, 9, [26] Z. Xu, Y. Wang, D. Lu, L. Yu, J. Yan, J. Luo, K. Ma, Y. Zheng, and R. K. Y. Tong. All-around real label supervision: Cyclic prototype consistency learning for semi-supervised medical image segmentation. IEEE Journal of Biomedical and Health Informatics, 26(7):31743184, 2022. 8, 9 [27] Lequan Yu, Shujun Wang, Xiaomeng Li, Chi-Wing Fu, and Pheng-Ann Heng. Uncertainty-aware self-ensembling model In Medfor semi-supervised 3d left atrium segmentation. ical image computing and computer assisted intervention MICCAI 2019: 22nd international conference, Shenzhen, China, October 1317, 2019, proceedings, part II 22, pages 605613. Springer, 2019. 1, 2, 6, 7, 8, 9, 10 [28] Zekuan Yu, Meijia Li, Jiacheng Yang, Zilong Chen, Huixian Zhang, Weifan Liu, Fang Kai Han, and Jie Liu. benchmark dual-modality dental imaging dataset and novel cognitively inspired pipeline for high-resolution dental point cloud synthesis. Cognitive Computation, 15(6):19221933, 2023. 7, 8 [29] Jiahui Zhong, Wenhong Tian, Yuanlun Xie, Zhijia Liu, Jie Ou, Taoran Tian, and Lei Zhang. Pmfsnet: Polarized multiscale feature self-attention network for lightweight medical image segmentation. Computer Methods and Programs in Biomedicine, 261:108611, 2025. 1,"
        }
    ],
    "affiliations": [
        "Academy for Engineering and Technology, Fudan University, Shanghai 200433, China",
        "Department of Oral and Maxillofacial Surgery, General Hospital of Ningxia Medical University, Yinchuan 750004, China",
        "School of Computer Science & Technology, Beijing Jiaotong University, Beijing 100044, China"
    ]
}