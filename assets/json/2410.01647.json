{
    "paper_title": "3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection",
    "authors": [
        "Yang Cao",
        "Yuanliang Jv",
        "Dan Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Neural Radiance Fields (NeRF) are widely used for novel-view synthesis and have been adapted for 3D Object Detection (3DOD), offering a promising approach to 3DOD through view-synthesis representation. However, NeRF faces inherent limitations: (i) limited representational capacity for 3DOD due to its implicit nature, and (ii) slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS) has emerged as an explicit 3D representation that addresses these limitations. Inspired by these advantages, this paper introduces 3DGS into 3DOD for the first time, identifying two main challenges: (i) Ambiguous spatial distribution of Gaussian blobs: 3DGS primarily relies on 2D pixel-level supervision, resulting in unclear 3D spatial distribution of Gaussian blobs and poor differentiation between objects and background, which hinders 3DOD; (ii) Excessive background blobs: 2D images often include numerous background pixels, leading to densely reconstructed 3DGS with many noisy Gaussian blobs representing the background, negatively affecting detection. To tackle the challenge (i), we leverage the fact that 3DGS reconstruction is derived from 2D images, and propose an elegant and efficient solution by incorporating 2D Boundary Guidance to significantly enhance the spatial distribution of Gaussian blobs, resulting in clearer differentiation between objects and their background. To address the challenge (ii), we propose a Box-Focused Sampling strategy using 2D boxes to generate object probability distribution in 3D spaces, allowing effective probabilistic sampling in 3D to retain more object blobs and reduce noisy background blobs. Benefiting from our designs, our 3DGS-DET significantly outperforms the SOTA NeRF-based method, NeRF-Det, achieving improvements of +6.6 on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet dataset, and impressive +31.5 on mAP@0.25 for the ARKITScenes dataset."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 ] . [ 1 7 4 6 1 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Under review",
            "content": "3DGS-DET: EMPOWER 3D GAUSSIAN SPLATTING WITH BOUNDARY GUIDANCE AND BOX-FOCUSED SAMPLING FOR 3D OBJECT DETECTION Yang Cao, Yuanliang Ju, Dan Xu Department of Computer Science and Engineering, HKUST Figure 1: Illustration of the proposed Boundary Guidance. By incorporating Boundary Guidance in the training of 3D Gaussian Splatting (3DGS), we significantly improve the spatial distribution of Gaussian blobs relating objects and the background. To better show this improved spatial distribution, we visualize only the positions of the Gaussian blobs, omitting other attributes for clarity. ABSTRACT Neural Radiance Fields (NeRF) are widely used for novel-view synthesis and have been adapted for 3D Object Detection (3DOD), offering promising approach to 3D object detection through view-synthesis representation. However, NeRF faces inherent limitations: (i) It has limited representational capacity for 3DOD due to its implicit nature, and (ii) it suffers from slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS) has emerged as an explicit 3D representation that addresses these limitations with faster rendering capabilities. Inspired by these advantages, this paper introduces 3DGS into 3DOD for the first time, identifying two main challenges: (i) Ambiguous spatial distribution of Gaussian blobs 3DGS primarily relies on 2D pixel-level supervision, resulting in unclear 3D spatial distribution of Gaussian blobs and poor differentiation between objects and background, which hinders 3DOD; (ii) Excessive background blobs 2D images often include numerous background pixels, leading to densely reconstructed 3DGS with many noisy Gaussian blobs representing the background, negatively affecting detection. To tackle the challenge (i), we leverage the fact that 3DGS reconstruction is derived from 2D images, and propose an elegant and efficient solution by incorporating 2D Boundary Guidance to significantly enhance the spatial distribution of Gaussian blobs, resulting in clearer differentiation between objects and their background (see Fig. 1). To address the challenge (ii), we propose Box-Focused Sampling strategy using 2D boxes to generate object probability distribution in 3D spaces, allowing effective probabilistic sampling in 3D to retain more object blobs and reduce noisy background blobs. Benefiting from the proposed Boundary Guidance and Box-Focused Sampling, our final method, 3DGS-DET, achieves significant improvements (+5.6 on mAP@0.25, +3.7 on mAP@0.5) over our basic pipeline version, without introducing any addi-"
        },
        {
            "title": "Under review",
            "content": "tional learnable parameters. Furthermore, 3DGS-DET significantly outperforms the state-of-the-art NeRF-based method, NeRF-Det, achieving improvements of +6.6 on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet dataset, and impressive +31.5 on mAP@0.25 for the ARKITScenes dataset. Codes and models are publicly available at: https://github.com/yangcaoai/3DGS-DET."
        },
        {
            "title": "INTRODUCTION",
            "content": "3D Object Detection (3DOD) (Qi et al., 2017a; 2019) is fundamental task in computer vision, providing foundations for wide realistic application scenarios such as autonomous driving, robotics, and industrial production, as accurate localization and classification of objects in 3D space are critical for these applications. Most existing 3DOD methods (Rukhovich et al., 2022b;a) explored using non-view-synthesis representations, including point clouds, RGBD, and multi-view images, to perform 3D object detection. However, these approaches mainly focus on the perception perspective and lack the capability for novel view synthesis. Neural Radiance Fields (NeRF) (Mildenhall et al., 2021) provide an effective manner for novel view synthesis and have been adapted for 3D Object Detection (3DOD) through view-synthesis representations (Xu et al., 2023; Hu et al., 2023). However, as view-synthesis representation for 3D object detection, NeRF has inherent limitations: 1) Its implicit nature restricts its representational capacity for 3DOD, and 2) it suffers from slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) has emerged as an explicit 3D representation that offers faster rendering, effectively addressing these limitations. Inspired by these strengths, our work is the first to introduce 3DGS into 3DOD. In this exploration, we identify two primary challenges: (i) Ambiguous spatial distribution of Gaussian blobs 3DGS primarily relies on 2D pixel-level supervision, resulting in unclear 3D spatial distribution of Gaussian blobs and insufficient differentiation between objects and background, which hinders effective 3DOD; (ii) Excessive background blobs 2D images often contain numerous background pixels, leading to densely populated 3DGS with many noisy Gaussian blobs representing the background, negatively impacting the detection of foreground 3D objects. To address the above-discussed challenges, we further empower 3DGS with two novel strategies for 3D object detection (i) 2D Boundary Guidance Strategy: Given the fact that 3DGS reconstruction is optimized from 2D images, we introduce novel strategy by incorporating 2D Boundary Guidance to achieve more suitable 3D spatial distribution of Gaussian blobs for detection. Specifically, we first perform object boundary detection on posed images, then overlay the boundaries onto the images, and finally train the 3DGS model. This proposed strategy can facilitate the learning of spatial Gaussian blob distribution that is more differentiable for the foreground objects and the background (see Fig. 1). (ii) Box-Focused Sampling Strategy: This strategy further leverages 2D boxes to establish 3D object probability spaces, enabling an object probabilistic sampling of Gaussian blobs to effectively preserve object blobs and prune background blobs. Specifically, we project the 2D boxes that cover objects in images into 3D spaces to form frustums. The 3D Gaussian blobs within the frustum have higher probability of being object blobs compared to those outside. Based on this strategy, we construct 3D object probability spaces and sample Gaussian blobs accordingly, finally preserving more object blobs and reducing noisy background blobs. In summary, the contributions of this work are fourfold: To the best of our knowledge, we are the first to integrate 3D Gaussian Splatting (3DGS) into 3D Object Detection (3DOD), representing novel contribution to the field. We propose 3DGS-DET, which empowers 3DGS with Boundary Guidance and Box-Focused Sampling for 3DOD. We design Boundary Guidance to optimize 3DGS with the guidance of object boundaries, which achieves significantly better spatial distribution of Gaussian blobs and clearer differentiation between objects and the background, thereby effectively enhancing 3D object detection. We propose Box-Focused Sampling, which establishes 3D object probability spaces, enabling higher sampling probability to be assigned to object-related 3D Gaussian blobs. This probabilistic sampling strategy preserves more object blobs and suppresses noisy background blobs, therefore producing further improved detection performance. With zero additional learnable parameters, Boundary Guidance and Box-Focused Sampling improve detection by 5.6 points on mAP@0.25 and 3.7 points on mAP@0.5 as demonstrated in our ablation study. Furthermore, our final approach, 3DGS-DET, significantly outperforms the state-of-the-art NeRF-based method, NeRF-Det, on both ScanNet (+6.6 on mAP@0.25, +8.1 on mAP@0.5) and ARKITScenes (+31.5 on mAP@0.25)."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "3D Gaussian Splatting (3DGS) is an effective explicit representation that models 3D scenes or objects using Gaussian blobs small, continuous Gaussian functions distributed across 3D space, enabling faster rendering. Recent works (Shen et al., 2024b; Liu et al., 2024b; Lee et al., 2024) have shown that 3DGS is highly suitable for dynamic scene modeling. Additionally, some studies (Lin et al., 2024; Zhang et al., 2024; Xiong et al., 2024; Wang & Xu, 2024; Liu et al., 2024a; Feng et al., 2024) also demonstrate its efficiency in processing large-scale 3D scene data. key focus of recent 3DGS research is integrating semantic understanding to enhance perception capabilities. Researchers (Zhou et al., 2024; Qin et al., 2024; Shi et al., 2024; Zuo et al., 2024) leverage advanced 2D foundational models, such as SAM (Kirillov et al., 2023) and CLIP (Radford et al., 2021), along with feature extraction methods like DINO (Zhang et al., 2022), to boost perception effectiveness. Unlike previous methods that often overlook specific challenges of 3D Object Detection (3DOD), our approach uniquely introduces Boundary Guidance and Box-Focused Sampling, marking the first exploration of 3DGS as representation for the 3D object detection task. Non-View-Synthesis Representation-Based 3D Object Detection. Traditional 3D detection tasks primarily utilize the following representations: (i) Point cloud-based methods (Yang et al., 2018; Ali et al., 2018; Shi et al., 2019; Qi et al., 2019; 2021; Wang et al., 2022b; Peng et al., 2022; Wang et al., 2022a; Rukhovich et al., 2022a; Cao et al., 2023; 2024) directly process unstructured 3D points captured by sensors like LiDAR or depth cameras. Techniques such as VoteNet (Qi et al., 2019) and CAGroup3D (Wang et al., 2022a) efficiently handle point clouds, capturing detailed geometries while facing challenges in computational efficiency due to their irregular structure. Some researches (Zhou & Tuzel, 2018; Ye et al., 2020; Deng et al., 2021; Mao et al., 2021; Noh et al., 2021; Chen et al., 2023b; Mahmoud et al., 2023) divide 3D space into uniform volumetric units, enabling 3D convolutional neural networks to process the data, although they encounter trade-offs between resolution and memory usage. (ii) Multi-view image-based methods (Wang et al., 2022c; Xiong et al., 2023; Wang et al., 2023; Chen et al., 2023a; Feng et al., 2023; Tu et al., 2023; Shen et al., 2024a) leverage 2D images from multiple perspectives to reconstruct 3D structures. (iii) RGBD based methods (Qi et al., 2018; 2020; Luo et al., 2020) enhance 3D object detection by combining 2D images cues, with 3D data to improve accuracy. However, these representations predominantly focus on perception and lack the capability for novel view synthesis. View-Synthesis Representation-Based 3D Object Detection. Neural Radiance Fields (NeRF) (Mildenhall et al., 2021) have become popular for novel-view-synthesis and have been adapted for 3D Object Detection (3DOD) (Hu et al., 2023; Xu et al., 2023). These adaptations present promising solutions for detecting 3D objects using view-synthesis representations. For instance, NeRF-RPN (Hu et al., 2023) employs voxel representations, integrating multi-scale 3D neural volumetric features to perform category-agnostic box localization rather than category-specific object detection. NeRF-Det (Xu et al., 2023) incorporates multi-view geometric constraints from the NeRF component into 3D detection. Notably, NeRF-RPN focuses on class-agnostic box detection, while NeRF-Det targets class-specific object detection. Our work follows the class-specific setting of NeRF-Det. However, NeRF faces significant challenges: its implicit nature limits its representational capacity for 3D object detection, and it suffers from slow rendering speeds. 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) has emerged as an explicit 3D representation, offering faster rendering and effectively addressing these limitations. Motivated by these advantages, our work introduces 3DGS into 3DOD for the first time, and presents novel designs to adapt 3DGS for detection, making significant differences from NeRF-based methods (Hu et al., 2023; Xu et al., 2023)."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "The pipeline of our 3DGS-DET is illustrated in the bottom row of Fig. 2. Initially, we train the 3D Gaussian Splatting (3DGS) on the input scenes using the proposed Boundary Guidance, which significantly enhances the spatial distribution of Gaussian blobs, resulting in clearer differentiation between objects and the background. Subsequently, we apply the proposed Box-Focused Sampling, which effectively preserves object-related blobs while suppressing noisy background blobs. The sampled 3DGS is then fed into the detection framework for training. In this section, we detail our method step by step. First, we introduce the preliminary concept of 3D Gaussian Splatting (3DGS) in Sec. 3.1. As the first to introduce 3DGS in 3D object detection, we establish the basic pipeline in Sec. 3.2, utilizing 3DGS for input and output detection predictions. We then present Boundary Guidance in Sec. 3.3. Finally, we describe the Box-Focused Sampling Strategy in Sec. 3.4."
        },
        {
            "title": "Under review",
            "content": "Figure 2: Pipeline overview (zooming in for clearer view). The top row illustrates our basic pipeline detailed in Sec. 3.2. The bottom row shows our 3DGS-DET pipeline with both Boundary Guidance (Sec. 3.3) and Box-Focused Sampling (Sec. 3.4) embedded. The Boundary Guidance can significantly improve the 3D spatial distribution of Gaussian blobs, and thus produce clearer differentiation between objects and the background. The Box-Focused Sampling effectively preserves more object-related blobs while suppressing noisy background blobs, compared to random sampling. These two proposed strategies together largely advance the 3D detection performance. 3.1 PRELIMINARY: 3D GAUSSIAN SPLATTING In our proposed method, 3DGS-DET, the input scene is represented using 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023), formulated as follows: = {(µi, Si, Ri, ci, αi)}N (1) where denotes the number of Gaussian blobs. Each blob is characterized by its 3D coordinate µi, scaling matrix Si, rotation matrix Ri, color features ci, and opacity αi. These attributes define the Gaussian through covariance matrix Σ = RSST RT , centered at µ: i=1 , G(x) = exp( 2 (xµ)T Σ1(xµ)) . (2) During rendering, opacity modulates the Gaussian. By projecting the covariance onto 2D plane (Zwicker et al., 2001), we derive the projected Gaussian, and utilize volume rendering (Max, 1995) to compute the image pixel colors: = (cid:88) k= αkck k1 (cid:89) (1 αj), j=1 (3) where is the number of sampling points along the ray. αi is determined by evaluating 2D Gaussian with covariance Σ, multiplied by the learned opacity (Yifan et al., 2019). The initial 3D coordinates of each Gaussian are based on Structure from Motion (SfM) points (Schonberger & Frahm, 2016). Gaussian attributes are refined to minimize the image reconstruction loss: Lrender = (1 λ)L1(I, ˆI) + λLD-SSIM(I, ˆI), (4) where ˆI represents the ground truth images. Additional details can be found in Kerbl et al. (2023). 3.2 PROPOSED BASIC PIPELINE OF 3DGS FOR 3D OBJECT DETECTION In this section, we build our basic pipeline by directly utilizing the original 3D Gaussian Splatting (3DGS) for 3D Object Detection (3DOD) without any further improvement. As depicted in the top row of Fig. 2, we train the 3DGS representation of the input scene using posed images, denoted as = {(µi, Si, Ri, ci, αi)}N i=1. Given that the number of Gaussian blobs is too large for them to be input into the detector, we perform random sampling to select subset of Gaussian blobs, denoted as ˆG = {(µi, Si, Ri, ci, αi)}M i=1, where < . We then concatenate the attributes of the Gaussian blobs along the channel dimension as follows: ˆGinput = Concat(µi, Si, Ri, ci, αi) {1, . . . , }. (5)"
        },
        {
            "title": "Under review",
            "content": "Figure 3: Illustration of the proposed Boundary Guidance and Box-Focused Sampling strategies. In the top row, Boundary Guidance is constructed by three steps, i.e., detecting boundaries on posed images, overlaying them to images, and training 3DGS model to achieve more distinct spatial distribution of Gaussian blobs for objects and the background. In the bottom row, Box-Focused Sampling is achieved by conducting object detection on posed images. The predicted 2D boxes are projected into the 3D domain to establish object probability spaces, allowing probabilistic sampling of Gaussians to preserve more object blobs and suppress noisy background blobs. This concatenated representation ˆGinput is then fed into the subsequent detection tool. Note that since 3DGS is an explicit 3D representation, ˆGinput can be utilized with any point-cloud-based detector by retraining the detector model on 3DGS representation. In our study, the research focus is on enhancing 3DGS for 3DOD in general, rather than designing specific detector. Therefore, we utilize the existing work (Rukhovich et al., 2022a) as the detection tool. The final detection predictions are obtained as follows: = F( ˆGinput) = (p, z, b) , where denotes the detector tool and represents the predictions, including classification probabilities p, centerness z, and bounding box regression parameters b. The training loss (Rukhovich et al., 2022a) is defined as: (6) Ldet = 1 Npos (cid:88) (cid:16) ˆx,ˆy,ˆz 1{p(ˆx,ˆy,ˆz)=0}Lreg(ˆb, b) + 1{p(ˆx,ˆy,ˆz)=0}Lcntr( ˆz, z) + Lcls( ˆp, p) (cid:17) , (7) where the number of matched positions Npos is given by (cid:80) 1{p(ˆx,ˆy,ˆz)=0}. Ground truth labels are indicated with hat symbol. The regression loss Lreg is based on Intersection over Union (IoU), the centerness loss Lcntr uses binary cross-entropy, and the classification loss Lcls employs focal loss. Further details on the detection tool can be found in Rukhovich et al. (2022a). Building upon this basic pipeline, we develop our method, 3DGS-DET, by introducing two novel designs to improve the 3DGS representation, as illustrated in the bottom row of Fig. 2. These designs are detailed in the following sections Sec. 3.3 and Sec. 3.4. ˆx,ˆy,ˆz 3.3 BOUNDARY GUIDANCE Given the fact that 3DGS reconstruction is derived from 2D images, we design the novel Boundary Guidance strategy by incorporating 2D Boundary Guidance to achieve more suitable 3D spatial distribution of Gaussian blobs for detection. In this section, we present our Boundary Guidance strategy in detail. As illustrated in the top row of Fig. 3, to provide the guidance priors for 3DGS reconstruction, we first generate category-specific boundaries for posed images: (8) bd represents the binary boundary map for category c. bd(x, y) = 1, the pixel at (x, y) belongs to the boundary for objects of category c. The set where Hbd is the boundary generator, and bc If bc Bbd = Hbd(I) = {bc C, bd}"
        },
        {
            "title": "Under review",
            "content": "includes all categories. In practice, the operations of Hbd are as follows: we use Grounded SAM (Ren et al., 2024) to generate category-specific masks. Then, the Suzuki-Abe algorithm (Suzuki et al., 1985) is employed to extract the boundaries of these masks, along with category information. The category-specific boundaries are then overlaid on the posed images in different colors: Ibd(x, y) = I(x, y) 1 (cid:32) (cid:33) bc bd(x, y) + (cid:88) cC (cid:88) cC bc bd(x, y) color(c), (9) where Ibd(x, y) is the pixel at position (x, y) of the final image with overlaid boundaries. I(x, y) is from the original image, bc bd(x, y) is the boundary map for category c, and color(c) is the color associated with category c. These Ibd images are used as ground truth to train the 3DGS representation Gbd by the following loss: Lrender = (1 λ)L1(I, Ibd) + λLD-SSIM(I, Ibd). (10) To effectively reduce Lrender during training, it is crucial to ensure the rendering quality of boundaries and the multi-view stability of boundaries. In this way, the Boundary Guidance lead 3DGS to incorporate boundary prior information into the 3D space. As shown in Fig. 2 (better viewed when zoomed in), 3DGS trained with Boundary Guidance demonstrates improved spatial distribution of Gaussian blobs compared to those trained without it, without introducing additional learnable parameters. 3.4 BOX-FOCUSED SAMPLING Considering that 2D images often include numerous background pixels, leading to densely reconstructed 3DGS with many noisy Gaussian blobs representing the background, negatively affecting detection. To reduce the excessive background blobs, in this section, we propose the Box-Focused Sampling strategy in detail. As depicted in the bottom row of Fig. 3, to provide priors for the following sampling, we utilize 2D object detector to identify object bounding boxes: Bbb = Hbb(I) = {(bbb, pC)}, (11) where Hbb is the box detector, and we select Grounding DINO (Liu et al., 2023) as the detector in our experiments. Here, bbb denotes the bounding box positions, and pC is the probability vector for the box belonging to each category in C. We define pmax = maxcC pc as the highest category probability for given bounding box, which helps to establish object probability spaces in later step. Then, we project the 2D boxes into 3D space: Fft = {K 1 (cid:35) (cid:34)xi yi (xi, yi) bbb, {zmin, zmax}}, (12) where Fft is the projected 3D frustum from bbb, and 1 is the inverse camera matrix used to map 2D bounding box corners (xi, yi) and depth values zmin and zmax into 3D space. Next, we establish object probability spaces using Fft and pmax. Specifically, for each bounding box, the maximum probability pmax models the likelihood of each Gaussian blob within the corresponding frustum being an object blob: pobj(gi gi Fft) = pmax, (13) where pobj(gi gi Fft) indicates the probability of each Gaussian blob gi within frustum Fft being an object blob. To integrate priors from different view frustums, we select the maximum probability as the aggregated probability: pagr(gi) = max vV pobj(gi gi ft ), (14) where pagr(gi) is the aggregated probability for Gaussian blob gi, and represents the set of all views. Gaussian blobs not belonging to any frustum are assigned small probability pbg, set to 0.01 in practice. In this way, we obtain the object probability spaces Pobj, where each Gaussian blob has an associated probability of being an object. We then perform independent probabilistic sampling based on Pobj to achieve Box-Focused Sampling, resulting in the sampled Gaussian set ˆGbs bd as: ˆGbs bd = {g Pobj(g)}. (15) In this way, it allows object blobs to be better preserved due to their higher probabilities, while most background points, having lower probabilities, are effectively reduced. Then, based on ˆGbs bd, we proceed with the training of the detector, as formulated by Equ. 5-Equ. 7 as described in Sec. 3.2. As shown in Fig. 2, 3DGS sampled via Box-Focused Sampling retains more object blobs and reduces background noise."
        },
        {
            "title": "Under review",
            "content": "Table 1: Comparison of mAP@0.25 across different methods on ScanNet. The first block includes methods using non-view-synthesis representations, such as point cloud, RGB-D, and multi-view images. The second block includes methods utilizing view-synthesis representations (NeRF-based and our 3DGS-based method). Our 3DGS-DET significantly outperforms the NeRF-based method NeRF-Det by 6.6 points. For other representations, 3DGS-DET surpasses all methods except for the point-cloud-based methods, FCAF3D and CAGroup3D, which have inherent advantages by directly using sensor-captured 3D data, specifically point clouds, as input. Methods Seg-Cluster (Wang et al., 2018) Mask R-CNN (He et al., 2017) SGPN (Wang et al., 2018) 3D-SIS (Hou et al., 2019) 3D-SIS (w/ RGB) (Hou et al., 2019) VoteNet (Qi et al., 2019) FCAF3D (Rukhovich et al., 2022a) CAGroup3D (Wang et al., 2022a) ImGeoNet (Tu et al., 2023) CN-RMA (Shen et al., 2024a) ImVoxelNet (Rukhovich et al., 2022b) NeRF-Det (Xu et al., 2023) 3DGS-DET (Our basic pipeline) 3DGS-DET (Our basic pipeline+BG) 3DGS-DET (Our basic pipeline+BG+BS) Methods Seg-Cluster (Wang et al., 2018) Mask R-CNN (He et al., 2017) SGPN (Wang et al., 2018) 3D-SIS (Hou et al., 2019) 3D-SIS (w/ RGB) (Hou et al., 2019) VoteNet (Qi et al., 2019) FCAF3D (Rukhovich et al., 2022a) CAGroup3D (Wang et al., 2022a) ImGeoNet (Tu et al., 2023) CN-RMA (Shen et al., 2024a) ImVoxelNet (Rukhovich et al., 2022b) NeRF-Det (Xu et al., 2023) 3DGS-DET (Our basic pipeline) 3DGS-DET (Our basic pipeline+BG) 3DGS-DET (Our basic pipeline+BG+BS) cab 11.8 15.7 20.7 12.8 19.8 36.3 57.2 60.4 40.6 42.3 30.9 37.6 39.6 38.9 44.1 desk 12.2 14.4 14.1 33.3 46.9 71.7 71.5 83.9 70.9 70.0 65.5 52.0 69.8 68.6 72.8 bed 13.5 15.4 31.5 63.1 69.7 87.9 87.0 93.0 84.1 80.0 84.0 84.9 82.5 83.5 82.7 curt 12.4 14.7 22.2 2.5 14.1 47.2 60.1 69.4 33.7 44.9 19.6 29.2 36.7 45.2 40.7 chair 18.9 16.4 31.6 66.0 66.2 88.7 95.0 95.3 74.8 79.4 77. 76.2 75.8 81.7 81.7 fridg 11.2 21.6 0.0 10.4 53.8 45.4 52.4 65.7 54.4 44.0 58.2 68.2 38.3 52.7 56.6 sofa 14.6 16.2 40.6 46.3 71.8 89.6 92.3 92.3 75.6 83.1 73.3 76.7 78.0 82.6 79.6 showr 18.0 18.5 0.0 12.2 36.0 57.1 83.9 73.0 47.5 55.2 32.8 49.3 55.3 45.0 71.9 tabl 13.8 14.9 31.9 26.9 36.1 58.8 70.3 69.9 59.9 55.2 56.7 57.5 53.6 54.4 56.0 toil 19.5 25.0 72.9 74.5 87.6 94.9 99.9 100.0 95.2 95.4 92. 97.1 93.5 98.3 98."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP door wind bkshf pic 11.1 12.5 16.6 8.0 30.6 47.3 61.1 67.9 40.4 44.0 35.1 36.4 36.1 36.2 35.4 sink 18.9 24.5 52.4 22.9 43.0 54.7 84.7 79.7 57.5 68.1 40.1 57.6 64.0 69.6 72.2 11.5 11.6 15.3 2.8 10.9 38.1 60.2 63.6 24.7 30.6 18.6 17.8 26.9 26.0 27.6 bath 16.4 24.5 0.0 58.7 84.3 92.1 86.6 87.0 81.5 86.1 77.6 83.6 80.8 84.3 88. 11.7 11.8 13.6 2.3 27.3 44.6 64.5 67.3 60.1 53.6 47.5 0.0 19.5 0.0 0.0 0.0 7.8 29.9 40.7 4.2 8.8 0.0 cntr 13.7 13.7 17.4 6.9 10.0 56.1 64.3 77.0 41.2 65.0 44.4 49.2 2.5 47.0 56.0 11.9 41.8 52.8 13.5 39.6 45.2 61.9 17.3 ofurn mAP@0.25 12.2 16.9 18.6 7.1 16.2 37.2 65.4 66.1 36.1 49.7 28. 35.9 37.5 48.0 46.7 13.4 17.1 22.2 25.4 40.2 58.7 71.5 75.12 54.6 58.6 49.0 53.3 54.3 56.7 59.9 (+6.6) Dataset: To thoroughly evaluate the performance of our proposed method in 3D detection tasks, we selected two representative datasets: ScanNet (Dai et al., 2017) and ARKitScene (Baruch et al., 2021). ScanNet is large-scale indoor scene dataset containing over 1,500 real-world 3D scanned scenes, encompassing various complex indoor environments such as residential spaces, offices, and classrooms. The ARKitScene dataset is constructed from RGB-D image sequences, offering detailed geometric information and precise object annotations. For each scene, maximum of 600 posed images are extracted. The category settings follow the standard 18 categories for ScanNet and 17 categories for ARKitScene. Metrics: We use mAP@0.25 and mAP@0.5 as the primary evaluation metrics. Mean Average Precision (mAP) is calculated at different IoU thresholds, providing comprehensive measure of the detection models performance across various categories. Implementation Details: For training 3DGS, we follow Kerbl et al. (2023) to initialize the 3D coordinates of Gaussian blobs using Structure from Motion (SfM) points. The training hyperparameters are the same as those in Kerbl et al. (2023). We employ pretrained GroundedSAM (Ren et al., 2024) and the Suzuki-Abe algorithm (Suzuki et al., 1985) as the boundary detector in Boundary Guidance. The pretrained GroundingDINO (Liu et al., 2023) is used as the box detector in the Box-Focused Sampling strategy. For the detection tool, we utilize the FCAF3D (Rukhovich et al., 2022a) architecture implemented in MMDetection3D (Contributors, 2020). The training hyperparameters are the same as those in FCAF3D. In our ablation study, to ensure fair comparison, all model versions are trained with the same hyperparameters, such as the same number of epochs, specifically 12 epochs. All the ablation experiments (Sec. 4.3) are conducted on ScanNet."
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "Quantitative Results. For the ScanNet dataset, we present the mAP@0.25 and mAP@0.5 performances of various methods in Tab. 1 of the main paper and Tab. 6 of the Appendix, respectively. Note that some methods did not report mAP@0.5 in previous studies, resulting in blank entries for these methods in Tab. 6 of the Appendix. In both Tab. 1 and Tab. 6 of the Appendix, the methods listed in the first block (Wang et al., 2018; He et al., 2017; Hou et al., 2019; Qi et al., 2019; Rukhovich et al., 2022a; Wang et al., 2022a; Tu et al., 2023; Shen et al., 2024a; Rukhovich et al., 2022b) are non-view-synthesis representation-based 3D detection methods. These methods utilize point clouds, RGB-D data, or multi-view images for 3D object detection. The second block consists of view-synthesis representation-based 3DOD methods, including NeRF-Det (Hu et al., 2023) and our proposed 3DGS-DET. NeRF-Det is the closest work to ours, leveraging Neural Radiance Fields (NeRF). Our approach variants are detailed as follows: 3DGS-DET (Our basic pipeline) represents the basic pipeline method established in Sec. 3.2. 3DGS-DET (Our basic pipeline+BG) incorporates the proposed Boundary Guidance as detailed in Sec. 3.3. 3DGS-DET (Our basic pipeline+BG+BS) is our full method, utilizing both Boundary Guidance and Box-Focused Sampling as described in Sec. 3.4. As illustrated in Tab. 1 and Tab. 6 of the Appendix, all versions of our methods significantly outperform NeRF-Det. Notably, our full method (Our basic pipeline+BG+BS) surpasses the state-of-the-art NeRF-based method, NeRFDet, by +6.6 on mAP@0.25 and +8.1 on mAP@0.5, showcasing the superiority of our approach. Table 2: Comparison of the whole-scene performance on the ARKITScenes validation set. Our 3DGS-DET significantly outperforms NeRF-Det by 31.5 points. Note that we follow the setup described in the NeRF-Det (Xu et al., 2023) supplementary materials: In our experiments, we utilize the subset of the dataset with low-resolution images, considering it is the closest work to ours. Other methods that do not use the same setting are not listed in this table. Methods ImVoxelNet (Rukhovich et al., 2022b) NeRF-Det (Xu et al., 2023) 3DGS-DET (Ours) Methods cab 32.2 36.1 45.2 fridg 34.3 40.7 84.4 shlf 4.2 4.9 33.3 stove 0.0 0.0 41.4 oven dshwshr frplce stool ImVoxelNet (Rukhovich et al., 2022b) NeRF-Det (Xu et al., 2023) 3DGS-DET (Ours) 9.9 14.0 74.3 4.1 7.4 6.0 10.2 10.9 56.4 0.4 0.2 26.3 bed 64.7 69.3 87. chr 5.2 4.0 70.3 sink wshr 20.5 24.4 75.5 tble 11.6 14.2 60. 15.8 17.3 67.6 TV 3.1 5.3 0.7 tolt 68.9 75.1 87.2 bthtb 80.4 84.6 90.8 sofa mAP@.25 35.6 44.0 81.8 23.6 26.7 58.2 (+31.5) Regarding the ARKitScene dataset, considering NeRF-Det is the closest work to ours, we follow the same setup described in the NeRF-Det (Xu et al., 2023) supplementary materials: In our experiments, we utilize the subset of the dataset with low-resolution images. Similarly, we adopt the same subset of the ARKitScenes dataset. Other methods that report performance on ARKitScene use the full dataset, so our 3DGS-DET is only compared with ImVoxelNet and NeRF-Det under the same conditions as described in NeRF-Det. The results in Tab. 2 demonstrate that 3DGS-DET performs better across most categories, achieving an mAP@0.25 of 58.2, which significantly outperforms NeRF-Det by +31.5, highlighting the superiority of our method. Qualitative results. We provide qualitative comparison with NeRF-Det in Fig. 4. As shown, our methods detect more objects in the scene with greater positional accuracy compared to NeRFDet (Xu et al., 2023), demonstrating the superiority of our approach. More qualitative comparisons can be found in Fig. 6 and Fig. 7 in the Appendix. 4.3 ABLATION STUDY 4.3.1 ANALYSIS ON THE EFFECT OF PROPOSED DESIGNS In this section, we demonstrate the effectiveness of our contributions by first presenting the performance of our proposed basic 3DGS detection pipeline and then incrementally incorporating our additional designs to analyze the resulting performance improvements. Our Proposed Basic 3DGS Detection Pipeline. As shown in Tab. 1, 3DGS-DET (Our basic pipeline) represents our proposed detection pipeline utilizing 3DGS, as described in Section 3.2. Benefiting from the advantages of 3DGS as an explicit scene representation, our basic pipeline surpasses NeRF-Det by 1 point (54.3 vs. 53.3), underscoring the significance of introducing 3DGS into 3DOD for the first time."
        },
        {
            "title": "Under review",
            "content": "Figure 4: Qualitative comparison. Our methods identify more 3D objects in the scene with better positional precision, highlighting the advantages of our approach over NeRF-Det (Xu et al., 2023). In this figure, the scene is represented using mesh to clearly show the boxes. Table 3: Ablation study on guidance from different priors. Table 4: Ablation study on different sampling methods. Different Priors mAP@0.25 mAP@0.5 Sampling Methods mAP@0.25 mAP@0. 2D Center Point 2D Mask 2D Boundary (ours) 54.4 54.9 56.7 33.9 34.2 36.9 Random Sampling Farthest Point Sampling Box-focused Sampling (ours) 56.7 57.4 59.9 36.9 37.6 37. Boundary Guidance. 3DGS-DET (Our basic pipeline+BG) incorporates the proposed Boundary Guidance as detailed in Sec. 3.3. Introducing Boundary Guidance into the basic pipeline results in significant improvement of 2.4 points (56.7 vs. 54.3), demonstrating the effectiveness of the proposed Boundary Guidance. To further explore the impact of Boundary Guidance on 3DGS representations, we present visual comparison of the spatial distribution of trained Gaussian blobs in Fig. 8 in the Appendix. As we can see, Gaussian blobs trained with Boundary Guidance demonstrate clearer spatial distribution and more distinct differentiation between objects and the background. We also present rendered images from different views by 3DGS trained with Boundary Guidance in Fig. 9 and Fig. 10 in the Appendix. As can be observed, the category-specific boundaries are clearly rendered and show multi-view stability, indicating that the 3D representation has effectively embedded the priors from Boundary Guidance. All these results clearly verify the effectiveness of the proposed Boundary Guidance for 3D detection with 3DGS. Box-Focused Sampling. Furthermore, we introduce Box-Focused Sampling detailed in Sec. 3.4, represented by 3DGS-DET (Our basic pipeline+BG+BS) in Tab. 1. This addition leads to further performance boost of 3.2 points (59.9 vs. 56.7), proving the effectiveness of Box-Focused Sampling. The visual comparison of sampled Gaussian blobs is shown in Fig. 11 in the Appendix. We can observe that the proposed Box-Focused Sampling significantly retains more object blobs and suppresses noisy background blobs. 4.3.2 ABLATION STUDY ON GUIDANCE FROM DIFFERENT PRIORS In this section, we analyze the impact of guidance from various priors. As described in Sec. 3.3, we utilize the objects boundary as the guidance prior. Here, we perform an ablation study considering the objects center point and mask as alternative priors. To obtain the center point, we detect the objects bounding box using GroundingDINO (Liu et al., 2023) and compute its center coordinates. The mask is generated with GroundedSAM (Ren et al., 2024). Note that all priors are category-specific, with each class associated with fixed color. These priors are overlaid on the posed images, as shown in Fig. 5, and then used to train the 3DGS for detection. Tab. 3 presents the detection performance for 3DGS trained with the different priors. As reported in Tab. 3, the 3DGSDET method using boundary guidance achieved 56.7% in mAP@0.25 and 36.9% in mAP@0.5, demonstrating significant superiority over the center point and mask priors. Lets explore the visualizations for further insights. In (a) and (c) of Fig. 5, we observe that the spatial distribution of Gaussian blobs with Point Guidance is less distinct compared to Boundary Guidance."
        },
        {
            "title": "Under review",
            "content": "Figure 5: Analysis of guidance from different priors: (a) Center Point Guidance, (b) Mask Guidance, and (c) Boundary Guidance. In (a) and (b), the spatial distribution of Gaussian blobs for objects like the chair, trash bin and sink is incomplete and ambiguous. Gaussian blobs trained with Boundary Guidance exhibit clearer spatial distribution. The reason behind this phenomenon is that the center point provides only positional guidance, lacking richer information like shape or size. The mask highlights shape and size but hides the objects surface, reducing texture and geometric information. Boundary Guidance offers positional cues and richer information, such as shape and size, while preserving texture and geometric details on the objects surface, leading to the best performance. This is because the center point provides only positional guidance, lacking richer information like shape or size, making it less effective compared to the boundary prior. For the mask prior, as shown in (b) and (c) of Fig. 5, the Gaussian blobs spatial distribution with Mask Guidance is more ambiguous than with the Boundary Guidance. Although the mask highlights shape and size information, it hides the objects surface, reducing texture and geometric information, thus being less effective than the boundary prior. Overall, Boundary Guidance offers positional cues and richer information such as shape and size while preserving texture and geometric details on the objects surface, leading to the best performance. 4.3.3 ANALYSIS ON DIFFERENT SAMPLING METHODS In this section, we compare two additional sampling methods with our Box-Focused Sampling: 1) Random Sampling and 2) Farthest Point Sampling (Qi et al., 2017b). The latter iteratively selects points farthest from those already chosen, ensuring even distribution for better scene coverage, focusing on global distribution rather than specific geometric features of objects. The results in Tab. 4 demonstrate that our Box-Focused Sampling achieves the highest performance, with mAP@0.25 and mAP@0.5 reaching 59.9% and 37.8%, respectively. This is because 3DGS often contain excessive background blobs. Our Box-Focused Sampling is specifically designed to preserve more object-related blobs while suppressing noisy background blobs. In contrast, other sampling methods primarily focus on global scenes without differentiation between objects and background blobs."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we introduce 3D Gaussian Splatting (3DGS) into 3D Object Detection (3DOD) for the first time. We propose 3DGS-DET, novel approach that leverages Boundary Guidance and Box-Focused Sampling to enhance 3DGS for 3DOD. Our method effectively addresses the inherent challenges of 3DGS in 3D object detection by improving spatial distribution and reducing background noise. By incorporating 2D Boundary Guidance, we achieve clearer differentiation between objects and background, while Box-Focused Sampling retains more object points and minimizes background noise. Our method demonstrates significant improvements, with gains of +5.6 on mAP@0.25 and +3.7 on mAP@0.5 over the basic pipeline. It also outperforms state-of-the-art NeRF-based methods, achieving +6.6 on mAP@0.25 and +8.1 on mAP@0.5 on the ScanNet dataset, and an impressive +31.5 on mAP@0.25 on the ARKITScenes dataset. These results underscore the effectiveness and superiority of our designs."
        },
        {
            "title": "REFERENCES",
            "content": "Waleed Ali, Sherif Abdelkarim, Mahmoud Zidan, Mohamed Zahran, and Ahmad El Sallab. Yolo3d: End-to-end real-time 3d oriented object bounding box detection from lidar point cloud. In Proceedings of the European conference on computer vision (ECCV) workshops, pp. 00, 2018. Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman. Arkitscenes - diverse real-world In NeurIPS, 2021. URL dataset for 3d indoor scene understanding using mobile rgb-d data. https://arxiv.org/pdf/2111.08897.pdf. Yang Cao, Yihan Zeng, Hang Xu, and Dan Xu. Coda: Collaborative novel box discovery and cross-modal alignment for open-vocabulary 3d object detection. In NeurIPS, 2023. Yang Cao, Yihan Zeng, Hang Xu, and Dan Xu. Collaborative novel object discovery and arXiv preprint box-guided cross-modal alignment for open-vocabulary 3d object detection. arXiv:2406.00830, 2024. Dian Chen, Jie Li, Vitor Guizilini, Rares Andrei Ambrus, and Adrien Gaidon. Viewpoint equivariance for multi-view 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 92139222, 2023a. Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and Jiaya Jia. Voxelnext: Fully sparse voxelnet for 3d object detection and tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2167421683, 2023b. MMDetection3D Contributors. MMDetection3D: OpenMMLab next-generation platform for general 3D object detection. https://github.com/open-mmlab/mmdetection3d, 2020. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3D reconstructions of indoor scenes. In CVPR, 2017. Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and Houqiang Li. Voxel r-cnn: Towards high performance voxel-based 3d object detection. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 12011209, 2021. Chengjian Feng, Zequn Jie, Yujie Zhong, Xiangxiang Chu, and Lin Ma. Aedet: Azimuth-invariant multi-view 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2158021588, 2023. Guofeng Feng, Siyan Chen, Rong Fu, Zimu Liao, Yi Wang, Tao Liu, Zhilin Pei, Hengjie Li, Xingcheng Zhang, and Bo Dai. Flashgs: Efficient 3d gaussian splatting for large-scale and highresolution rendering. arXiv preprint arXiv:2408.07967, 2024. Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask R-CNN. In ICCV, 2017. Ji Hou, Angela Dai, and Matthias Nießner. 3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans. In CVPR, 2019. Benran Hu, Junkai Huang, Yichen Liu, Yu-Wing Tai, and Chi-Keung Tang. Nerf-rpn: general framework for object detection in nerfs. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023. URL https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40154026, 2023. Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian splatting for static and dynamic radiance fields. arXiv preprint arXiv:2408.03822, 2024."
        },
        {
            "title": "Under review",
            "content": "Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, et al. Vastgaussian: Vast 3d gaussians for large scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 51665175, 2024. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. Wenkai Liu, Tao Guan, Bin Zhu, Lili Ju, Zikai Song, Dan Li, Yuesong Wang, and Wei Yang. Efficientgs: Streamlining gaussian splatting for large-scale high-resolution scene representation. arXiv preprint arXiv:2404.12777, 2024a. Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, and Zhaoxiang Zhang. Citygaussian: Real-time high-quality large-scale scene rendering with gaussians. arXiv preprint arXiv:2404.01133, 2024b. Qianhui Luo, Huifang Ma, Li Tang, Yue Wang, and Rong Xiong. 3d-ssd: Learning hierarchical features from rgb-d images for amodal 3d object detection. Neurocomputing, 378:364374, 2020. Anas Mahmoud, Jordan SK Hu, and Steven Waslander. Dense voxel fusion for 3d object detection. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 663 672, 2023. Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang, Hang Xu, and Chunjing Xu. Voxel transformer for 3d object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 31643173, 2021. Nelson Max. Optical models for direct volume rendering. TVCG, 1(2):99108, 1995. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. Jongyoun Noh, Sanghoon Lee, and Bumsub Ham. Hvpr: Hybrid voxel-point representation for single-stage 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1460514614, 2021. Liang Peng, Fei Liu, Zhengxu Yu, Senbo Yan, Dan Deng, Zheng Yang, Haifeng Liu, and Deng Cai. Lidar point cloud guided monocular 3d object detection. In European conference on computer vision, pp. 123139. Springer, 2022. Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. Pointnet: Deep learning on point sets for 3D classification and segmentation. CVPR, 2017a. Charles Qi, Li Yi, Hao Su, and Leonidas Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. NeurIPS, 2017b. Charles Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas Guibas. Frustum pointnets for 3d object detection from rgb-d data. In CVPR, 2018. Charles R. Qi, Or Litany, Kaiming He, and Leonidas J. Guibas. Deep hough voting for 3D object detection in point clouds. ICCV, 2019. Charles Qi, Xinlei Chen, Or Litany, and Leonidas Guibas. detection in point clouds with image votes. computer vision and pattern recognition, pp. 44044413, 2020. Imvotenet: Boosting 3d object In Proceedings of the IEEE/CVF conference on Charles Qi, Yin Zhou, Mahyar Najibi, Pei Sun, Khoa Vo, Boyang Deng, and Dragomir Anguelov. Offboard 3d object detection from point cloud sequences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 61346144, 2021. Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d language gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2005120060, 2024."
        },
        {
            "title": "Under review",
            "content": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 87488763. PMLR, 2021. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. Danila Rukhovich, Anna Vorontsova, and Anton Konushin. Fcaf3d: Fully convolutional anchor-free 3d object detection. In European Conference on Computer Vision, pp. 477493. Springer, 2022a. Danila Rukhovich, Anna Vorontsova, and Anton Konushin. Imvoxelnet: Image to voxels projection for monocular and multi-view general-purpose 3d object detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 23972406, 2022b. Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016. Guanlin Shen, Jingwei Huang, Zhihua Hu, and Bin Wang. Cn-rma: Combined network with ray In Proceedings marching aggregation for 3d indoor object detection from multi-view images. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 21326 21335, June 2024a. Licheng Shen, Ho Ngai Chow, Lingyun Wang, Tong Zhang, Mengqiu Wang, and Yuxing Han. Gaussian time machine: real-time rendering methodology for time-variant appearances. arXiv preprint arXiv:2405.13694, 2024b. Jin-Chuan Shi, Miao Wang, Hao-Bin Duan, and Shao-Hua Guan. Language embedded 3d gaussians for open-vocabulary scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 53335343, 2024. Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointrcnn: 3d object proposal generation and detection from point cloud. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 770779, 2019. Satoshi Suzuki et al. Topological structural analysis of digitized binary images by border following. Computer vision, graphics, and image processing, 30(1):3246, 1985. Tao Tu, Shun-Po Chuang, Yu-Lun Liu, Cheng Sun, Ke Zhang, Donna Roy, Cheng-Hao Kuo, and Min Sun. Imgeonet: Image-induced geometry-aware voxel representation for multi-view 3d object detection. In Proceedings of the IEEE international conference on computer vision, 2023. Haiyang Wang, Lihe Ding, Shaocong Dong, Shaoshuai Shi, Aoxue Li, Jianan Li, Zhenguo Li, and Liwei Wang. Cagroup3d: Class-aware grouping for 3d object detection on point clouds. Advances in Neural Information Processing Systems, 35:2997529988, 2022a. Shihao Wang, Yingfei Liu, Tiancai Wang, Ying Li, and Xiangyu Zhang. Exploring object-centric temporal modeling for efficient multi-view 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 36213631, 2023. Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity group proposal network for 3d point cloud instance segmentation. In CVPR, 2018. Yikai Wang, TengQi Ye, Lele Cao, Wenbing Huang, Fuchun Sun, Fengxiang He, and Dacheng Tao. Bridged transformer for vision and point cloud 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1211412123, 2022b. Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao, and Justin In ConSolomon. Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. ference on Robot Learning, pp. 180191. PMLR, 2022c. Zipeng Wang and Dan Xu. Pygs: Large-scale scene representation with pyramidal 3d gaussian splatting. arXiv preprint arXiv:2405.16829, 2024."
        },
        {
            "title": "Under review",
            "content": "Butian Xiong, Xiaoyu Ye, Tze Ho Elden Tse, Kai Han, Shuguang Cui, and Zhen Li. Sa-gs: Semantic-aware gaussian splatting for large scene reconstruction with geometry constrain. arXiv preprint arXiv:2405.16923, 2024. Kaixin Xiong, Shi Gong, Xiaoqing Ye, Xiao Tan, Ji Wan, Errui Ding, Jingdong Wang, and Xiang Bai. Cape: Camera view position embedding for multi-view 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2157021579, 2023. Chenfeng Xu, Bichen Wu, Ji Hou, Sam Tsai, Ruilong Li, Jialiang Wang, Wei Zhan, Zijian He, Peter Vajda, Kurt Keutzer, et al. Nerf-det: Learning geometry-aware volumetric representation for multi-view 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2332023330, 2023. Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-time 3d object detection from point clouds. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 7652 7660, 2018. Maosheng Ye, Shuangjie Xu, and Tongyi Cao. Hvnet: Hybrid voxel network for lidar based 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16311640, 2020. Wang Yifan, Felice Serena, Shihao Wu, Cengiz Oztireli, and Olga Sorkine-Hornung. Differentiable surface splatting for point-based geometry processing. ACM Transactions on Graphics (TOG), 38(6):114, 2019. Hanyue Zhang, Zhiliu Yang, Xinhe Zuo, Yuxin Tong, Ying Long, and Chen Liu. Garfield++: arXiv preprint Reinforced gaussian radiance fields for large-scale 3d scene reconstruction. arXiv:2409.12774, 2024. Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022. Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, and Achuta Kadambi. Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2167621685, 2024. Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud based 3d object detection. In CVPR, 2018. Xingxing Zuo, Pouya Samangouei, Yunwen Zhou, Yan Di, and Mingyang Li. Fmgs: Foundation model embedded 3d gaussian splatting for holistic 3d scene understanding. International Journal of Computer Vision, pp. 117, 2024. Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. In VIS, 2001."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 PERFORMANCE ON NERF-RPN SETTING Table 5: Performance on the NeRF-RPN setting, which targets class-agnostic box detection. Our method significantly outperforms NeRF-RPN in this setting. Methods mAP@0.25 mAP@0.5 NeRF-RPN (Hu et al., 2023) 3DGS-DET (ours) 55.5 75.6 (+20.1) 18.4 52.3 (+33.9) In this section, we adapt our 3DGS-DET to the NeRF-RPN Setting (Hu et al., 2023), which targets class-agnostic box detection. To achieve this, we labeled all the ground-truth boxes with single object category and trained 3DGS-DET accordingly. Additionally, NeRF-RPN uses different train/validation split compared to the official ScanNet dataset, with its validation set overlapping the official ScanNet training set. To address this, we excluded the overlapping parts between the NeRF-RPN test set and the ScanNet official training set from our training data. We then used the remaining scenes for training, and tested on the same validation set provided by NeRF-RPN. As shown in Tab. 5, 3DGS-DET achieved an mAP@0.25 of 75.6% and an mAP@0.5 of 52.3%, significantly outperforming NeRF-RPN (Hu et al., 2023)s 55.5% and 18.4%. This demonstrates the significant superiority of our method in the class-agnostic setting. A.2 FUTURE WORK As the first work to introduce 3DGS into 3DOD, our paper mainly focuses on the primary stage of this pipeline: empowering 3DGS for 3DOD. Diverse experiments demonstrate that our designs can lead to significant improvements. Beyond empowering the 3DGS representation, subsequent detector specifically designed for 3DGS could hold promise in the future. Besides, exploring joint training of 3DGS and the detector is also an interesting direction. We hope our exploration knowledge, open-source codes and data will inspire further research."
        },
        {
            "title": "Under review",
            "content": "Table 6: Comparison of mAP@0.5 across different methods on ScanNet. The first block presents methods that employ non-view-synthesis representations, including point clouds, RGB-D, and multi-view images. The second block lists methods using view-synthesis representations, such as NeRF-based and our 3DGS-based techniques. Our 3DGS-DET significantly surpasses the NeRFbased NeRF-Det by 8.1 points. Among other representations, 3DGS-DET outperforms all except the point-cloud-based methods FCAF3D and CAGroup3D, which benefit from directly using sensor-captured 3D data, specifically point clouds, as input. Note that some methods did not report mAP@0.5 in previous works, resulting in blank entries for these methods. door wind bkshf pic - - - 1.4 10.9 15.3 44.1 54.3 8.0 11.1 4.9 7.1 9.3 11.7 9.6 sink - - - 8.7 8.9 16.8 52.6 55.4 24.5 36.1 18.9 25.5 29.0 36.3 35.1 - - - 0.0 0.0 6.4 30.7 37.3 2.9 6.5 1.3 3.0 5.6 11.3 8.2 bath - - - 28.5 56.4 78.9 84.5 82.4 61.7 76.4 60.2 55.8 68.3 78.3 74. - - - 1.4 13.2 28.0 58.4 64.1 32.9 40.0 7.0 - - - 0.0 0.0 1.3 17.9 31.4 0.3 1.2 0.1 cntr - - - 0.0 0.0 9.5 31.3 41.1 7.9 24.9 0.9 11.6 1.6 31.3 2.0 2.3 28.7 19.0 1.7 24.4 31.8 20.9 4.2 ofurn mAP@0.5 - - - 2.6 6.9 11.7 57.1 58.8 17.4 31.5 10. 21.1 24.2 28.8 28.9 - - - 14.6 22.5 33.5 57.3 61.3 28.9 36.8 22.7 29.7 34.1 36.9 37.8 (+8.1) Methods Seg-Cluster (Wang et al., 2018) Mask R-CNN (He et al., 2017) SGPN (Wang et al., 2018) 3D-SIS (Hou et al., 2019) 3D-SIS (w/ RGB) (Hou et al., 2019) VoteNet (Qi et al., 2019) FCAF3D (Rukhovich et al., 2022a) CAGroup3D (Wang et al., 2022a) ImGeoNet (Tu et al., 2023) CN-RMA (Shen et al., 2024a) ImVoxelNet (Rukhovich et al., 2022b) NeRF-Det (Xu et al., 2023) 3DGS-DET (Our basic pipeline) 3DGS-DET (Our basic pipeline+BG) 3DGS-DET (Our basic pipeline+BG+BS) Methods Seg-Cluster (Wang et al., 2018) Mask R-CNN (He et al., 2017) SGPN (Wang et al., 2018) 3D-SIS (Hou et al., 2019) 3D-SIS (w/ RGB) (Hou et al., 2019) VoteNet (Qi et al., 2019) FCAF3D (Rukhovich et al., 2022a) CAGroup3D (Wang et al., 2022a) ImGeoNet (Tu et al., 2023) CN-RMA (Shen et al., 2024a) ImVoxelNet (Rukhovich et al., 2022b) NeRF-Det (Xu et al., 2023) 3DGS-DET (Our basic pipeline) 3DGS-DET (Our basic pipeline+BG) 3DGS-DET (Our basic pipeline+BG+BS) cab - - - 5.1 5.7 8.1 35.8 41.4 15.8 21.3 8.9 12.0 18.5 16.1 19.2 desk - - - 13.7 23.6 37.5 53.4 63.6 43.9 51.4 35. 46.0 53.5 47.4 52.4 bed - - - 42.2 50.3 76.1 81.5 82.8 74.8 69.2 67.1 68.4 73.5 77.0 73.8 curt - - - 0.0 2.6 11.6 44.2 44.4 4.3 19.6 0.6 5.8 18.1 27.2 22. chair - - - 50.1 52.6 67.2 89.8 90.8 46.5 52.4 35.0 47.8 44.6 51.6 52.7 fridg - - - 2.7 24.5 27.8 46.8 57.0 24.0 33.0 22.1 26.0 30.7 30.4 36.9 sofa - - - 31.8 55.4 68.8 85.0 85.6 45.7 63.5 33.1 58.3 61.9 62.4 65.2 showr - - - 3.0 0.8 10.0 64.2 49.3 2.0 6.6 4.5 1.6 3.4 8.3 15.7 tabl - - - 15.1 22.0 42.4 62.0 64.9 39.9 42.9 30. 42.8 42.2 44.7 46.2 toil - - - 56.8 71.8 86.5 91.6 98.2 68.8 73.3 67.7 69.0 77.0 87.0 82."
        },
        {
            "title": "Under review",
            "content": "Figure 6: More qualitative comparison. Our methods identify more objects in the scene with better positional precision, highlighting the advantages of our approach over NeRF-Det (Xu et al., 2023). In this figure, the scene is represented using mesh to clearly display the boxes. Note that, Black and white boxes indicate predictions with incorrect categories, while boxes of other colors represent predictions with the correct category."
        },
        {
            "title": "Under review",
            "content": "Figure 7: More qualitative comparison. Our methods identify more objects in the scene with better positional precision, highlighting the advantages of our approach over NeRF-Det (Xu et al., 2023). In this figure, the scene is represented using mesh to clearly display the boxes. Note that, Black and white boxes indicate predictions with incorrect categories, while boxes of other colors represent predictions with the correct category."
        },
        {
            "title": "Under review",
            "content": "Figure 8: Analysis on the effect of Boundary Guidance. Gaussian blobs trained with Boundary Guidance exhibit clearer spatial distribution and more distinct differentiation between objects and background. Note that we visualize only the positions of the Gaussian blobs to highlight their spatial distribution, omitting other attributes."
        },
        {
            "title": "Under review",
            "content": "Figure 9: Rendered images from different views by 3DGS trained with Boundary Guidance. The category-specific boundaries are well rendered and exhibit multi-view stability, demonstrating that the 3D representation has successfully embedded the priors provided by Boundary Guidance."
        },
        {
            "title": "Under review",
            "content": "Figure 10: Rendered images from different views by 3DGS trained with Boundary Guidance. The category-specific boundaries are well rendered and exhibit multi-view stability, demonstrating that the 3D representation has successfully embedded the priors provided by Boundary Guidance."
        },
        {
            "title": "Under review",
            "content": "Figure 11: Analysis on the effect of Box-Focused Sampling. Box-Focused Sampling significantly retains more object blobs and reduces noisy background blobs. Note that we visualize only the positions of the Gaussian blobs to highlight their spatial distribution, omitting other attributes."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Engineering, HKUST"
    ]
}