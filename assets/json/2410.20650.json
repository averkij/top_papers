{
    "paper_title": "NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks",
    "authors": [
        "Yongchang Hao",
        "Yanshuai Cao",
        "Lili Mou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The performance of neural networks improves when more parameters are used. However, the model sizes are constrained by the available on-device memory during training and inference. Although applying techniques like quantization can alleviate the constraint, they suffer from performance degradation. In this work, we introduce NeuZip, a new weight compression scheme based on the entropy of floating-point numbers in neural networks. With NeuZip, we are able to achieve memory-efficient training and inference without sacrificing performance. Notably, we significantly reduce the memory footprint of training a Llama-3 8B model from 31GB to less than 16GB, while keeping the training dynamics fully unchanged. In inference, our method can reduce memory usage by more than half while maintaining near-lossless performance. Our code is publicly available."
        },
        {
            "title": "Start",
            "content": "NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks Dept. Computing Science & Alberta Machine Intelligence Institute (Amii), University of Alberta Yongchang Hao Yanshuai Cao Lili Mou Borealis AI Canada CIFAR AI Chair 4 2 0 2 8 2 ] . [ 1 0 5 6 0 2 . 0 1 4 2 : r yongcha1@ualberta.ca yanshuai.cao@borealisai.com doublepower.mou@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "The performance of neural networks improves when more parameters are used. However, the model sizes are constrained by the available on-device memory during training and inference. Although applying techniques like quantization can alleviate the constraint, they suffer from performance degradation. In this work, we introduce NeuZip, new weight compression scheme based on the entropy of floating-point numbers in neural networks. With NeuZip, we are able to achieve memory-efficient training and inference without sacrificing performance. Notably, we significantly reduce the memory footprint of training Llama-3 8B model from 31GB to less than 16GB, while keeping the training dynamics fully unchanged. In inference, our method can reduce memory usage by more than half while maintaining near-lossless performance. Our code is publicly available."
        },
        {
            "title": "Introduction",
            "content": "Deep learning with neural networks has become the backbone of numerous artificial intelligence applications. The search for better-performing networks is longstanding topic in deep learning. Without modifying the design, scaling up the number of parameters (e.g., number of hidden dimensions or layers) has been demonstrated as an effective practice to boost the performance of neural networks of the same kind (Kaplan et al., 2020). This idea has been successfully applied to text, image, audio, and multi-modal tasks with wide range of model architectures (Yu et al., 2022; Radford et al., 2019; Brown et al., 2020). Recently, the number of parameters in the state-of-the-art models has become more than 100 billion or even trillion parameters. For example, one of the state-of-the-art language models in 2020, GPT-3, has 175B parameters (Brown et al., 2020), growing by nearly 100 times compared with the largest Transformer architecture in the 2017 paper (Vaswani et al., 2017). Despite the growth in the model size, the hardware capacity is not keeping up with the pace: the largest on-device memory of GPUs was 32GB in 2017, and is 80GB to this date in 2024, growing by only 2.5 times. The available hardware supply poses limitation on the trainable model size, bottlenecking the scaling capacity. Although this problem can be alleviated by using more GPUs and sharding the model in multiple devices (Rajbhandari et al., 2019), such practice introduces more communication overheads among GPUs, making large-scale distributed training less efficient. Therefore, saving the total memory usage is critical in scaling up neural networks. The peak memory usage is dominated by three relatively independent parts: the optimizer, the saved activations for back-propagation, and the model itself. For the optimizer, there are already memory-efficient optimizers achieving sublinear space complexity (Shazeer and Stern, 2018; Hao et al., 2024); for the activations, the memory can be saved by enabling activation checkpointing (Chen et al., 2016), which saves the storage by recomputing the forward activations during the back-propagation. For the model parameters, there has not been an effective method to save the memory while preserving the ability to train the model. Recently, Dettmers et al. (2023) proposed the quantized low-rank adaptation (QLoRA), which freezes the parameters using 4-bit data type for the backbone pre-trained model. While significantly saving the memProject done during Mitacs internship at Borealis AI. 1https://github.com/BorealisAI/neuzip ory for the model, it imposes constraint on the overall change of the model to be low-rank, limiting the model capacity. In this paper, we propose NeuZip, an algorithm to compress the neural networks while maintaining their full abilities. Specifically, each floating-point number is represented by three parts: the sign bit, the exponent bits, and the mantissa bits. Following the observation that weights are concentrated around zero (Kalamkar et al., 2019), we demonstrate that this corresponds to the low-entropy nature of the exponent bits. We hence compress the exponent bits using the asymmetric numeral system (ANS Duda (2013)), lossless compression algorithm that achieves high throughput on parallel computing devices like GPUs. Since the compression is lossless, the memory reduction comes without compromising any precision loss and enables full-parameter training. In addition to lossless compression for training, we also propose lossy variant of NeuZip for inference that further reduces the memory footprint. Specifically, we control the relative change of each parameter by storing only the top-k significant bits of the mantissa. We empirically show that lossy NeuZip lies at the Pareto frontier of the memoryperformance trade-off when compared with several state-of-the-art quantization baselines."
        },
        {
            "title": "2 Our Approach",
            "content": "The Shannon entropy (Shannon, 1948) is used to measure the stochasticity of random variable with the following definition: H(X) := Xp(X) [ log2 p(X)] (1) for random variable with probability p. lower entropy indicates less stochasticity of random variable. In fact, the entropy equals the minimum number of bits required, in expectation, to represent random variable, therefore corresponding to data compressibility. For the non-concentrating random variable with all possible values sharing an equal probability, the entropy of which reaches the maximum value log2 n, where is all possible values can take. On the other hand, for highly-concentrating (e.g., fully deterministic) random variables, the entropy can be as low as 0. 2.1 Low-Entropy Nature of Neural Network Parameters We argue that the parameters in neural network tend to have low entropy. First, parameters are typically initialized with Gaussian distribution for matrices (Glorot and Bengio, 2010; He et al., 2015). This encourages all weights to be centered around zero, effectively reducing the entropy (or randomness). In addition, regularization is also applied for better generalization ability. For example, the weight decay technique reduces the magnitudes of weights at every update iteration. Similarly in Bayesian inference, prior distributions (e.g., Gaussian and Laplace distributions) are often applied, imposing zero-concentrated preference over the parameters.Even without explicit regularization, stochastic gradient descent (SGD) or its variants are shown to have the implicit regularization effect on neural networks, meaning the model parameters are implicitly encouraged to have smaller magnitudes during training (Soudry et al., 2018; Vardi and Shamir, 2021). All the above effects and techniques lead to the following observation: Observation 2.1 Assuming neural network parameters are i.i.d. random variables, the entropy of the distribution is likely to be low. Specifically, each parameter is represented is represented by three components: the sign bit, the exponent bits, and the mantissa bits in the IEEE 754 standard (IEEE, 2019).2 Therefore, we conduct fine-grained analysis and investigate the distribution of each component of floating-point number in neural networks. As shown in Figure 1, the sign bit has high entropy as it is evenly distributed; hence, it is not compressible. For the exponent bits, there is clear pattern that they demonstrate low-entropy nature, carrying only less than 3 bits of information with 8 bits of capacity. For the mantissa bits, they store nearly 7-bit information with 7-bit capacity. In fact, we shown in Appendix that this is common in deep learning. This phenomenon suggests that by simply compressing the exponents, we are able to recover the overall optimal compression ratio. In this example, an ideal compression algorithm is able to achieve ratio as 2We use BF16 (Kalamkar et al., 2019) in this paper. 2 Figure 1: The histograms of different components of the parameters of LLama-3 8B model (Dubey et al., 2024). The x-axis is all possible binary values and the y-axis represent the frequency of each value. high as 1.501 (the sum of the three entropy values), only marginally below the overall compression ratio 1.505. 2.2 Lossless NeuZip: Compressing Exponents for Training Compressed representation. Based on observation, we see that the number of bits per exponent is largely inflated compared with the information entropy. However, previous research demonstrates that the dynamic range provided by the 8-bit exponents are critical for neural networks (Kalamkar et al., 2019). We therefore propose to compress the exponent bits in lossless manner based on the entropy. This practice mainly has three benefits: (1) it increases the throughput of compression as only part of the bits are processed by the compression; (2) it reduces the burden of maintaining the statistics of large set of symbols (e.g., 256 symbols for 8-bit exponents versus 65,536 symbols for 16-bit representations), enabling great efficiency of compression algorithms; (3) most importantly, it recovers most of the compressibility as shown in Figure 1. Multi-layer neural networks. The compression alone does not save any memory for maintaining single array. This is because, either compression or decompression, requires at least one buffer of the same size as the uncompressed array. In the scope of neural networks, the whole model is prohibitively large and it is infeasible to duplicate the memory. In NeuZip, however, we exploit the multi-layer structure of modern neural networks to avoid creating large buffer. Without loss of generality, we focus on the linear function as common building block in neural networks at layer l: xl Wl xl1 + bl, (2) where Wl Rmn is the weight matrix, bl Rm is the bias vector of layer l, and xl is the input of layer l. We propose to modify the compressed forward pass in the following form ˆW decompress(cl) xl ˆW xl1 + bl, (4) where cl is the compressed storage of the matrix Wl. In this way, we only need to store ci for each layer, enjoying low-memory usage. During each forward pass, weight matrices stay in the compressed form until the original data is needed, in which case it is decompressed into temporary space ˆW for computation. As result, the entire network is never fully decompressed at any point in time, making the overall forward pass memory efficient. The per-layer procedure is shown in Figure 2. (3) Note that although we alter the forward pass, the back-propagation for each linear layer is fully unaffected. This is because Wl = xl xl Wl = (cid:0)xl L(cid:1) l1. (5) Therefore, we are able to obtain the gradient as long as the activations are saved. Similarly, we can also propagate the gradient of inputs with xl1 = xl xl xl1 = (cid:0)xl L(cid:1) Wl, (6) 3 Output Gradient output Output Gradient output Output Gradient output Output Gradient output Weight Gradient weight Weight Gradient weight Weight Gradient weight Compressed Weight Gradient weight Input Gradient input Input Gradient input Input Gradient input Input Gradient input Layer-wise Global (a) Vanilla (b) AC (c) AC+LOMO (d) NeuZip Figure 2: Reverse-mode automatic differentiation (e.g., back-propagation) with different memory-saving techniques for linear layer. Blocks colored blue are loaded in memory temporarily for the calculation of this layer, whereas the blocks colored red are always in memory throughout training. where Wl can be constructed by decompression. It is worth noting that our NeuZip is compatible with activation checkpointing (Chen et al., 2016) by recomputing the activations, opening more opportunity for memory saving. For weight updates, we decompress the matrix into the original floating-point format and compress the updated matrix again. This procedure is done in layer-by-layer fashion, similar to LOMO (Lv et al., 2023). The overall training procedure is described in Appendix B. Compression algorithm. In our implementation, we choose to use the asymmetric numeral systems (ANS) (Duda, 2013) as our backbone compression algorithm because it can be easily parallelized and achieves high throughput with parallel execution, making it an ideal candidate on deep learning accelerators like GPUs. Specifically, ANS encodes sequence of symbols by treating them as base-n numbers. However, unlike the common numerical system that uses uniform base for each digit, ANS treats every single digit with different base 1/ ˆpi, where ˆp is the frequency of symbols. As result, it achieves near-optimal compression rate by suing around 1/ ˆpi bits for the ith symbol. 2.3 Lossy NeuZip: Additionally Truncating Mantissa for Inference In the algorithm above, we show that the training of neural networks can be completely unaffected by lossless compression. On the other hand, inference is known to be less sensitive to precision loss compared with training (Dettmers et al., 2022; Dettmers and Zettlemoyer, 2023). This enables further memory reduction of NeuZip by reducing the precision. In our study, we conduct pilot experiment that perturbs each weight with noise proportional to the weight magnitude. We observe that with small noise ratio there is little or no effect on the overall performance (Appendix C). Motivated by this, we propose variant of NeuZip that compresses mantissa in lossy way during inference. In its core, we simply round and truncate the mantissa to fewer bits. Specifically, we assume the original floating-point number has an exponent and mantissa m. After rounding, the mantissa is denoted by ˆm and the resulting floating-point number is denoted by ˆf . The rounding introduces an error expressed as: ˆf = (cid:12) (cid:12) (cid:12) (cid:12) 2e127 27 2e127 (cid:12) (cid:12) (cid:12) (cid:12) ˆm = 2e134 ˆm (7) where 127 interprets the exponent bits as an integer, which can be either positive, negative, or 0. In the fraction m/27, is the significand (an unsigned integer) and 7 is the precision. It is straightforward to see that the relative error is given by ˆf = 2e134 ˆm 2e134 = ˆm . (8) Suppose our rounding keeps most significant bits in ˆm, the earliest point where ˆm could differ from the original number is at the (k + 1)th bit. This means that the maximum possible relative change introduced by this rounding is 1/2k. Given that the mantissa bits are highly uniform as shown in Figure 1, such practice resembles the weight perturbation based on relative magnitudes, justifying the rounding trick applied to mantissas. In our implementation, we store the sign and mantissa bits together as signed integer to minimize the requests of memory write. Further, given that modern architectures are mostly byte (8-bit) addressable, we pack multiple such signed integers into single byte for memory efficiency. To align with an 8-bit byte, we let the precision after rounding to be {0, 1, 3}, ensuring that all the bits in byte are utilized efficiently. We illustrate the process in Figure 8b. Lastly, we enable block-wise normalization technique (Dettmers et al., 2023), where block is chunk of weights that are stored contiguously in memory. Such block-wise normalization makes sure that the weight with the largest magnitude in block will always be normalized to 1, invariant to mantissa rounding and truncation. The normalization coefficientwhich handles mantissa while ignoring the exponentis stored with 8 bits, and is used for de-normalization during the decompression of the weight. This strategy is based on the observation that larger weights play more important role in neural networks (Han et al., 2015)."
        },
        {
            "title": "3 Experiments",
            "content": "We empirically verify the effectiveness of NeuZip across different model architectures and datasets. Given the success of large language models, we mainly consider Transformer-based models for our experiments. We choose two designs of Transformer, decoder-only and encoderdecoder models, to show the generality of our method. All experiments are conducted on RTX A6000 GPUs where the uncompressed data type is BFloat16. 3.1 Lossless NeuZip for Pre-Training Settings. We choose decoder-only models to evaluate our method on the pre-training task. We select 3 models with different sizes to study the scaling effect, including GPT-Neo 2.7B (Black et al., 2021), Llama-3 8B (Dubey et al., 2024), and LLama-2 13B (Touvron et al., 2023). For fair comparison, all competing methods are initialized with the same random weights. For the task, we consider language modeling, which requires the model to predict the next token given the context. We use the Wikitext-2 dataset (Merity et al., 2016), where each data sample is fixed-length sequence from an article on Wikipedia. We set the length to 1024 following the common practice (Radford et al., 2019). For each experiment, we report the loss (negative log-likelihood) on unseen samples. To study memory saving, we report the peak memory usage for each run during the training process. The numbers are shown in gibibyte (GiB, 10243 bytes). We also report the speed by the number of iterations per second to demonstrate the time-efficiency of each method. We apply the vanilla SGD update to all runs for efficiency. The activation checkpointing technique (Chen et al., 2016) is enabled by default. It is worth noting that pre-training these large models to the optimal performance is extremely expensive (Rajbhandari et al., 2019). Given that our NeuZip training method is lossless, we only train the models for 1 epoch to showcase its effectiveness. We use the same hyperparameters for all runs. Results. We present the results in Table 1. We first test the vanilla training method, where only the activation checkpointing is applied (shown in Figure 2b). As shown, the vanilla training requires the highest amount of memory because it stores the uncompressed weights and gradients for all layers. We also test the LOMO technique (Lv et al., 2023), which promptly updates the weights in layer-by-layer fashion (shown in Figure 2c). This allows LOMO to reuse buffer to store the gradients for each layer. As result, LOMO approximately reduces the peak memory usage by the size of model. Finally, we apply our NeuZip on top of LOMO (shown in Figure 2d). For all models, NeuZip additionally reduces more than 20% percentage of memory compared with LOMO, accounting for total memory reduction of more than 50%. Notably, NeuZip reduces the peak memory of training Llama-2 13B model to less than 20GB, enabling training 13B model on consumer-grade GPUs without any precision loss. 5 Table 1: Pre-training decoder-only models on the language modeling task. The loss numbers are calculated on the validation set with the cross-entropy loss. Memory is reported in GiB (10243 B). Speed represents the number of iterations per second. The bold numbers represent the top results. GPT-Neo-XL 2.7B LLama-2 13B Llama-3 8B Name Loss Mem Speed Loss Mem Speed Loss Mem Speed Vanilla LOMO +NeuZip Lossless 8.81 8.81 8.81 11.22 6.97 5. 0.96 0.94 0.70 8.61 8.61 8.61 30.97 19.47 15.25 0.77 0.78 0.45 - 9.10 9.10 OOM 26.26 18. - 0.49 0.28 Table 2: Fine-tuning encoderdecoder models on the SQL generation task. The BLEU scores are calculated with SacreBLEU. Memory is reported in GiB (10243 B). Speed represents the number of iterations per second. The bold numbers represent the top results. T5 1B T5 3B T5 11B Name BLEU Mem Speed BLEU Mem Speed BLEU Mem Speed Vanilla LOMO + NeuZip Lossless QLoRA INT8 QLoRA FP4 QLoRA FP42 QLoRA NF4 QLoRA NF42 79.9 79.9 79.9 70.4 70.1 70.6 70.4 70.5 3.82 2.75 2. 5.84 3.63 3.61 3.63 3.61 3.69 3.68 2.02 1.11 1.70 1.63 1.83 1.64 85.1 85.1 85.1 72.1 72.1 72.0 71.2 71.2 11.32 7.07 5. 11.54 7.35 7.27 7.35 7.07 2.43 2.47 1.33 1.12 1.74 1.61 1.65 1.57 - 82.3 82.3 63.5 63.3 60.6 59.4 57.9 OOM 25.95 20. 33.36 22.73 22.38 22.73 22.38 - 0.69 0.46 0.37 0.58 0.57 0.57 0.57 3.2 Lossless NeuZip for Fine-Tuning Settings. benefit of using lossless compression comes from retaining the pre-trained weight without any information loss. We conduct fine-tuning experiment with encoderdecoder models to test the performance of our NeuZip on broader architectures. In particular, we choose three T5 models: T5 1B, T5 3B, and T5 11B (Raffel et al., 2020), where the pre-trained parameters are used for initialization. The T5 models are pre-trained on the C4 dataset (Lin et al., 2020), which is filtered to contain natural language only. To avoid data leaks from pre-training, we choose non-natural language generation dataset for fine-tuning. Specifically, we use public SQL generation dataset (Zhong et al., 2017; Yu et al., 2018) as the test bed. For each sample, the model is required to generate the SQL command from human question. For example, the question could be CREATE TABLE head (age INTEGER). is expected to generate How many heads of the departments are older than 56 ?. SELECT COUNT(*) FROM head WHERE age > 56. We feed the question and response into the encoder and decoder, respectively. The objective is to minimize the cross-entropy loss on the response. Similar to the pre-training experiments, we also sweep the learning rate from 103 to 3 101 for each run. After fine-tuning, we generate with the model on the validation set with greedy decoding. The generated SQL commands are then compared with the ground truths by SacreBLEU (Post, 2018), metric that evaluates the similarity between corpora based on precision scores. The model Results. The results are reported in Table 2. All baselines in the pre-training experiment (i.e., the vanilla training, LOMO, and NeuZip) are included in this table. Similar to the results in Section 3.1, they achieve the same BLEU scores for each model. Specifically, our NeuZip is able to train 11B model within 24GB. For fine-tuning, For example, it is possible to apply other memory-efficient training techniques. QLoRA (Dettmers et al., 2023) compresses the pre-trained model by using low-precision data types and train the LoRA modules only (Hu et al., 2022). In our comparison experiment, we choose the widely used quantization data types for QLoRA, including INT8 (Dettmers et al., 2022), FP4, and NF4 (Dettmers et al., 2023). We apply the LoRA modules (Hu et al., 2022) on all linear layers, where every LoRA rank is set to 6 Table 3: Evaluating lossy NeuZip on different models and tasks. PPL represents the perplexity values. Memory is reported in GiB. Speed represents the number of iterations per second. The bold numbers represent the top results, whereas the underlined numbers are the second-best ones. (a) Evaluating decoder-only models on the language modeling task. Here, the perplexities are adjusted to word level to compare across different tokenizations."
        },
        {
            "title": "Vanilla",
            "content": "Quant INT8 Quant FP4 Quant NF4 Quant FP42 Quant NF42 NeuZip 0-bit NeuZip 1-bit NeuZip 3-bit NeuZip 7-bit (lossless) Llama-3 8B Llama-2 13B Yi-1.5 34B"
        },
        {
            "title": "PPL Mem Speed",
            "content": "9.89 15.08 10.07 11.51 10.75 11.50 10.75 13.64 10.77 9.93 9.89 8.63 5.77 5.77 5.44 5.44 5.24 6.05 7.70 10. 5.07 3.54 3.45 3.38 3.41 3.34 3.44 3.38 3.38 3.39 10.87 24.36 10.97 11.38 11.15 11.38 11. 12.46 11.17 10.90 10.87 12.74 7.37 7.37 6.87 6.87 6.30 7.77 10.73 16.66 3.59 2.27 1.87 1.83 1.86 1.81 1.87 1.86 1.84 1. -"
        },
        {
            "title": "OOM",
            "content": "- 10.87 11.57 11.06 11.57 11.06 12.06 11.04 10.76 10.72 33.41 19.54 19.54 18.11 18.11 16.20 20.14 27.92 43.40 1.13 1.75 1.67 1.61 1. 0.94 0.93 0.93 0.94 (b) Evaluating encoderdecoder models on the language modeling task. Since all models use the same tokenizer, we reported perplexities at the token level for simplicity."
        },
        {
            "title": "Vanilla",
            "content": "Quant INT8 Quant NF4 Quant FP4 Quant FP42 Quant NF42 NeuZip 0-bit NeuZip 1-bit NeuZip 3-bit NeuZip 7-bit (lossless) T5 1B T5 3B T5 11B"
        },
        {
            "title": "PPL Mem Speed",
            "content": "2.614 2.615 2.632 2.646 2.646 2.632 2.731 2.641 2.614 2.614 1.37 1.28 1.08 1.08 1.05 1.05 0.40 0.48 0.66 0. 23.73 4.24 11.64 11.92 10.39 10.39 11.82 11.68 11.99 11.55 2.571 2.573 2.588 2.594 2.594 2.587 2.668 2.591 2.574 2. 5.31 4.94 4.12 4.12 4.03 4.03 1.41 1.78 2.42 3.73 19.86 4.28 11.82 11.99 9.72 9.96 8.70 8.61 8.60 8. 2.568 21.06 2.569 2.579 2.585 2.585 2.579 2.651 2.581 2.569 2.568 19.59 16.28 16.28 15.93 15.93 5.35 6.65 9.27 14. 6.20 2.58 4.48 4.59 4.52 4.39 3.24 3.21 3.19 3.23 8 to control the memory usage.3 As shown in the second half of Table 2, all quantization methods underperform NeuZip in terms of both generation quality and memory usage. In terms of time efficiency, some quantization methods are slower than others, but in general, they are in the same magnitude as our method. Overall, NeuZip achieves the least memory usage while maintaining the highest performance. The results strongly suggests the practicality of our NeuZip. 3.3 Lossy Compression for Inference As mentioned in Section 2.3, the inference process is less sensitive in precision loss, which provides an opportunity for compressing mantissa in lossy fashion during inference. We evaluate the performance of our lossy NeuZip in such scenarios. 3It should be noted that the down-projection matrices in each T5 feed-forward network are not quantized for stability, as otherwise the model performance is seriously jeopardized. See https://github.com/huggingface/ transformers/issues/20287 for more details. 7 Figure 3: The trade-off between memory and performance for different methods. Settings. Following the settings in previous sections, we test our approach with both decoder-only and encoderdecoder architectures. For the decoder-only models, we select the LLama-3 8B (Dubey et al., 2024), LLama-2 13B (Touvron et al., 2023), and Yi-1.5 34B (Young et al., 2024). For the encoderdecoder architecture, we use the T5 1B, 3B, and 11B models as in Section 3.2. Since all decoder-only models are trained for language modeling, we evaluate the performance with language modeling tasks. Specifically, we test all methods on the Wikitext-2 validation set (Merity et al., 2016) following Section 3.1, where each sequence consists of 1024 tokens. On the other hand, the encoderdecoder models (T5 series) contain multiple tasks in pre-training. Since they excel at zero-shot translation, we evaluate them on the WMT14 En-De translation task (Bojar et al., 2014), where each source sentence is prepended with translate from English to German: based on the pre-training format (Raffel et al., 2020). Following the standard evaluation pipeline for lossy compression (Frantar et al., 2023; Dettmers and Zettlemoyer, 2023), we evaluate all models with the perplexity metric, which is sensitive to how distorted the compressed model is. Results. The results for decoder-only and encoderdecoder models are shown in Tables 3a and 3b, respectively. We see that the vanilla (uncompressed BFloat16) models achieve the best perplexity scores in all experiments at cost of the excessive memory usage. For quantization methods, we choose the same INT8 (Dettmers et al., 2022), FP4, and NF4 (Dettmers et al., 2023) data types mentioned in Section 3.2. In general, quantization methods suffer from notable perplexity degradation. Although the INT8 variant (Dettmers et al., 2022) manages to better preserve the perplexity, it uses around 50% more memory compared with other quantization methods. For our lossy NeuZip, we set three different levels of precision: 0-bit, 1-bit, and 3-bit mantissa preserved. We choose these values because they are aligned in 8-bit byte arrays (discussed in Section 2.3). All these variants use block size of 512 for normalization. We additionally include the lossless NeuZip (7-bit mantissa) for full comparison. As shown in the table, our lossy NeuZip demonstrates spectrum of memory saving and performance preservation. The 0-bit NeuZip attains the best memory efficiency in all experiments, whereas the lossless 7-bit NeuZip obtains the best perplexity scores. Notably, the 3-bit NeuZip achieves nearly lossless performance in all experiments while using less than 50% memory compared with the uncompressed model. The results confirm the effectiveness of our method. 3.4 In-Depth Analyses The memoryperformance trade-off. In Section 3.3, we observe that the performance is generally decreased with less memory usage. We analyze this trade-off of our NeuZip as well as quantization methods in Figure 3. Note that the optimal methods are the ones on the Pareto frontier (Pareto, 2014), i.e., the more bottom-left, the better. In addition to measuring the perplexity, we also include preliminary study by evaluating the end-to-end performance on the MMLU dataset (Hendrycks et al., 2020) in Appendix E. As shown, three out of four NeuZip variants are on the Pareto frontier, with the remaining one staying fairly close to the frontier. On the other hand, there is only one quantization technique that lies on the 8 Table 4: The effect of block size. Block 32 Block 64 Block 128 Block Block 512 Name NeuZip 0-bit NeuZip 1-bit PPL Mem PPL Mem PPL Mem PPL Mem PPL Mem 33.5 6.341 41.4 - 6.694 35.7 OOM 4.611 6.853 4.662 7.104 4. 7.639 4.640 34.2 42.2 33.8 41.8 34.6 42.7 Pareto frontier. This result demonstrates that our NeuZip generally achieves better memoryperformance trade-off than quantization. The effect of block size in lossy compression. As introduced in Section 2, we apply normalization to lossy NeuZip to ensure the weight with the largest absolute value will not be affected by truncation. We show the effect of block size in this experiment with giant model, Llama-3 70B evaluated on the Wikitext-2 dataset. As seen in Table 4, smaller block size clearly leads to better performance at the cost of compromising memory efficiency due to the overhead of storing normalization coefficients. Therefore, the block-wise normalization provides more fine-grained trade-off between memory and performance by varying the block size. (a) Comparison of CPU-offloading, quantization, lossy NeuZip Figure 4: The throughput experiment. compression, and lossless NeuZip compression. (b) Comparison of GPU-reloading, de-quantization, lossy NeuZip decompression, and lossless NeuZip decompression. Throughputs of NeuZip. In addition to overall time efficiency presented in Tables 13, we analyze the throughput of matrix compression and decompression with our NeuZip, in comparison with the throughput of matrix quantization and de-quantization based on the NF4 data type (Dettmers et al., 2023) using the popular library bitsandbytes.4 We additionally include the CPU-offloading technique as baseline, which lowers the GPU memory pressure by transferring data to CPU and reloading them to GPU when needed. Figure 4 measures the throughput of matrix processing in GiB/s when we vary the matrix size from 105 to 108 bytes. We see that CPU-offloading is generally slow across different sizes of matrices. This is due to the bottleneck of CPUGPU communication through PCIe. For quantization, the bitsandbytes package has been highly optimized for GPU, and its throughput is one magnitude higher than the CPU-offloading technique when the matrix size is large. Profoundly, our NeuZip achieves the highest throughput for compression among all methods (Figure 4a), and high throughput for decompression similar to de-quantization (Figure 4b). The results suggest that our NeuZip, albeit causing overhead compared with uncompressed vanilla models, is still highly efficient in practice. 4Available at https://github.com/bitsandbytes-foundation/bitsandbytes"
        },
        {
            "title": "4 Related Work",
            "content": "Model compression. Previous work has explored different techniques to reduce the memory usage of neural networks, including knowledge distillation (Hinton et al., 2015) and pruning (Kwon et al., 2022). Most related to our work is the quantization technique, which represents each parameter with fewer bits; common approaches include k-means-based quantization (Han et al., 2016), linear quantization (Han et al., 2016), and mixed precision quantization (Dettmers et al., 2022, 2023). When training data are available, one may incorporate the quantization into the training process to improve performance (Xiao et al., 2023; Frantar et al., 2023). In this paper, our NeuZip compression is zero-shot method, and therefore, our experiments consider the widely used zero-shot quantization methods (Dettmers et al., 2022, 2023) for fair comparison. We leave the utilization of additional data of NeuZip to future work. Memory-efficient optimizers. The optimizer also occupies considerable amount of memory during training (Rajbhandari et al., 2019). To address this, memory-efficient optimizers (Shazeer and Stern, 2018; Zhao et al., 2024; Hao et al., 2024) are developed to reduce the memory footprint of training. Our NeuZip is orthogonal to these optimization techniques, as it can be seamlessly combined with any of these methods for further memory saving. In particular, the lossless NeuZip is expected to have exactly the same results with less memory. Parameter-efficient training. Another line of research saves memory by training subset of parameters (Houlsby et al., 2019; Zaken et al., 2022) so the optimizer only stores information about small set of trainable parameters. One notable example is the low-rank adaptation (LoRA (Hu et al., 2022)). However, such practice restricts the optimization space of parameters, and thus usually leads to significant performance degradation. Moreover, low-rank methods are unsuitable for pre-training. It is important to mention that memory-efficient optimizers and parameter-efficient training cannot reduce the memory cost during inference. By contrast, our NeuZip is suitable for both training and inference."
        },
        {
            "title": "5 Conclusion",
            "content": "Summary. In this work, we present NeuZip, novel compression scheme for neural networks that achieves memory-efficient training and inference. By analyzing the floating-point structures, we propose to compress the exponent in lossless way and to compress the mantissa in lossy way. The lossless variant of our NeuZip may be applied to both training and inference, while yielding exactly the same result as the uncompressed model. The lossy NeuZip provides additional memory saving for inference, achieving superior memoryperformance trade-off. Limitations and future work. Due to the hardware constraint, the largest model that we consider in this paper is 70B. We would like to verify our NeuZip on even larger models like GPT-3 (Brown et al., 2020) with more capable hardware. Another limitation of NeuZip is that the throughput is lower than the vanilla model. However, it has comparable speed with highly optimized quantization methods while achieving significantly better performance. By using NeuZip, we expect to create opportunities for researchers with academic budgets to explore and study large models."
        },
        {
            "title": "Acknowledgments",
            "content": "The research is supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC), Mitacs Accelerate project, the Amii Fellow Program, the Canada CIFAR AI Chair Program, an Alberta Innovates Program, and the Digital Research Alliance of Canada (alliancecan.ca)."
        },
        {
            "title": "References",
            "content": "S. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman. GPT-Neo: Large scale autoregressive language modeling with Mesh-Tensorflow, 2021. URL https://doi.org/10.5281/zenodo.5297715. O. Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn, J. Leveling, C. Monz, P. Pecina, M. Post, H. SaintAmand, R. Soricut, L. Specia, and A. s. Tamchyna. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 1258, 2014. URL http://www.aclweb.org/anthology/W/W14/W14-3302. 10 T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are fewshot learners. In NeurIPS, pages 18771901, 2020. URL https://papers.nips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. URL https://arxiv.org/abs/1604.06174. T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. ICML, 2023. URL https://proceedings.mlr.press/v202/dettmers23a.html. T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. GPT3.int8(): 8-bit matrix multiplication for Transformers at scale. In NeurIPS, 2022. URL https://openreview.net/forum?id=dXiGWqBoxaD. T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. In NeurIPS, 2023. URL https://openreview.net/forum?id=OUIFPHEgJU. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, A. Goyal, A. Hartshorn, A. Yang, A. Mitra, A. Sravankumar, and et al. The Llama 3 herd of models. arXiv preprint arXiv: 2407.21783, 2024. URL https://arxiv.org/abs/2407.21783. J. Duda. Asymmetric numeral systems: Entropy coding combining speed of Huffman coding with compression rate of arithmetic coding. arXiv preprint arXiv: 1311.2540, 2013. URL https://arxiv.org/abs/ 1311.2540. E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. OPTQ: Accurate quantization for generative pre-trained Transformers. In ICLR, 2023. URL https://openreview.net/forum?id=tcbBPnfwxS. X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, pages 249256, 2010. URL https://proceedings.mlr.press/v9/glorot10a.html. S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights and connections for efficient neural netIn NeurIPS, 2015. URL https://proceedings.neurips.cc/paper_files/paper/2015/file/ work. ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf. S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. In ICLR, 2016. URL http://arxiv.org/abs/1510.00149. Y. Hao, Y. Cao, and L. Mou. Flora: Low-rank adapters are secretly gradient compressors. In ICML, 2024. URL https://openreview.net/forum?id=uubBZKM99Y. K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. ICCV, pages 10261034, 2015. URL https://doi.org/10.1109/ICCV.2015.123. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv: 2009.03300, 2020. URL https://arxiv.org/ abs/2009.03300. G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in neural network. arXiv preprint arXiv: 1503.02531, 2015. URL https://arxiv.org/1503.02531. N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning for NLP. In ICML, pages 27902799, 2019. URL https: //proceedings.mlr.press/v97/houlsby19a.html. E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. IEEE. IEEE standard for floating-point arithmetic, 2019. URL https://doi.org/10.1109/IEEESTD.2019. 8766229. D. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee, S. Avancha, D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen, J. Yang, J. Park, A. Heinecke, E. Georganas, S. Srinivasan, A. Kundu, M. Smelyanskiy, B. Kaul, and P. Dubey. study of BFloat16 for deep learning training. arXiv preprint arXiv: 1905.12322, 2019. URL https://arxiv.org/abs/1905.12322. J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv: 2001.08361, 2020. URL https://arxiv.org/abs/2001.08361. 11 W. Kwon, S. Kim, M. W. Mahoney, framework 24101 https://proceedings.neurips.cc/paper_files/paper/2022/file/ J. Hassoun, K. Keutzer, for Transformers. In NeurIPS, pages and A. Gholami. fast post-training pruning 24116, URL 987bed997ab668f91c822a09bce3ea12-Abstract-Conference.html. 2022. Z. Lin, A. Madotto, and P. Fung. Exploring versatile generative language model via parameter-efficient In EMNLP Findings, pages 441459, 2020. URL https://aclanthology.org/2020. transfer learning. findings-emnlp.41. K. Lv, Y. Yang, T. Liu, Q. Gao, Q. Guo, and X. Qiu. Full parameter fine-tuning for large language models with limited resources. arXiv preprint arXiv: 2306.09782, 2023. URL https://arxiv.org/abs/2306.09782. S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. ICLR, 2016. URL https: //arxiv.org/abs/1609.07843. V. Pareto. Manual of Political Economy: Variorum Translation and Critical Edition. Oxhttps://global.oup.com/academic/product/ ford University manual-of-political-economy-9780198867661. Press UK, 2014. URL M. Post. call for clarity in reporting BLEU scores. In WMT, pages 186191, 2018. URL https: //aclanthology.org/W18-6319. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019. URL https://openai.com/research/better-language-models. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with unified text-to-text Transformer. JMLR, 21(1):54855551, 2020. URL http://jmlr.org/papers/v21/20-074.html. S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models. International Conference For High Performance Computing, Networking, Storage And Analysis, 2019. URL https://arxiv.org/abs/1910.02054. C. E. Shannon. mathematical theory of communication. The Bell System Technical Journal, 27(3):379423, 1948. URL https://doi.org/10.1002/j.1538-7305.1948.tb01338.x. N. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In ICML, pages 45964604, 2018. URL https://proceedings.mlr.press/v80/shazeer18a.html. D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient descent on separable data. JMLR, 19(70):157, 2018. URL http://jmlr.org/papers/v19/18-188.html. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv: 2307.09288, 2023. URL https://arxiv.org/abs/2307.09288. G. Vardi and O. Shamir. Implicit regularization in relu networks with the square loss. In COLT, pages 42244258, 2021. URL http://proceedings.mlr.press/v134/vardi21b.html. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In NIPS, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. SmoothQuant: Accurate and efficient posttraining quantization for large language models. In ICML, 2023. URL https://arxiv.org/abs/2211. 10438. A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang, K. Yu, P. Liu, Q. Liu, S. Yue, S. Yang, S. Yang, T. Yu, W. Xie, W. Huang, X. Hu, X. Ren, X. Niu, P. Nie, Y. Xu, Y. Liu, Y. Wang, Y. Cai, Z. Gu, Z. Liu, and Z. Dai. Yi: Open foundation models by 01.ai. arXiv preprint arXiv: 2403.04652, 2024. URL https://arxiv.org/abs/2403.04652. J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan, B. Hutchinson, W. Han, Z. Parekh, X. Li, H. Zhang, J. Baldridge, and Y. Wu. Scaling autoregressive models for contentrich text-to-image generation. TMLR, 2022. URL https://openreview.net/forum?id=AFDcYJKhND. T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao, S. Roman, Z. Zhang, and D. Radev. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In EMNLP, pages 39113921, 2018. URL https://aclanthology.org/D18-1425. E. B. Zaken, Y. Goldberg, and S. Ravfogel. BitFit: Simple parameter-efficient fine-tuning for Transformerbased masked language-models. In ACL, volume 2, pages 19, 2022. URL https://aclanthology.org/ 2022.acl-short.1. J. Zhao, Z. Zhang, B. Chen, Z. Wang, A. Anandkumar, and Y. Tian. GaLore: Memory-efficient LLM training by gradient low-rank projection. In ICML, 2024. URL https://arxiv.org/abs/2403.03507. V. Zhong, C. Xiong, and R. Socher. Seq2SQL: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017. URL https://arxiv.org/abs/1709.00103."
        },
        {
            "title": "A Inspecting the Entropy on More Models",
            "content": "Random initialization. When training from scratch, the parameters are randomly initialized. To verify the compressibility in this case, we check the parameter entropy of randomly initialized model with the same architecture as Lllama-3. The initialization methods follow the standard procedure provided in the Hugging Face library. The results show similar pattern to what the released Llama model has, suggesting the compressibility with NeuZip occurs even with random weights. Figure 5: The histograms of different floating-point components of the parameters of randomly initialized Llama-3 8B model. Diffusion. We also inspect the parameter entropies beyond Transformer models. In Figure 6, we check all four models in diffusion pipeline. We see that the low-entropy exponents not only occur in Transformer models but other architectures like convolution-based VAE and U-Net models. Figure 6: The histograms of the exponent bits in different components of Stable Diffusion 1.5 model. We omit the sign and mantissa bits for simplicity as we do not compress them based on their entropies. Both experiments show that the occurrence of low-entropy components is common phenomenon in deep learning."
        },
        {
            "title": "B The Algorithm for Training with Lossless NeuZip",
            "content": "In this section, we describe the forward-backward procedure of NeuZip. First, we compress all the linear layers in the original model and store the compressed information on-device. During the training iterations, we decompress the compressed weights in layer-by-layer manner for the forward pass. For the backward pass, the input is recalculated again following the forward pass like activation checkpointing (Chen et al., 2016). linear operation calculates the gradients for both the weight matrix and input. To do so, we need to decompress the weight again, which is used to calculate the gradient of input. After the gradient is calculated, we directly update the weight without storing it similar to LOMO (Lv et al., 2023). 14 Algorithm 1 Memory-efficient training with NeuZip Require: number of linear layers L, linear layer weights {Wi}L Require: data stream that yields training data for each iteration i=1. Initialization 1: for 1 . . . do 2: 3: 4: 5: end for sl, el, ml split(Wl) cl compression(el) store(sl, cl, ml) Training loop end for x0 for 1 . . . do ˆe decompression(cl) ˆW merge(sl, ˆel, ml) xl ˆW xl1 + bl save for backward(xl) 6: for in do 7: Model forward 8: 9: 10: 11: 12: 13: 14: 15: Model backward and update 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: end for ˆe decompression(cl) ˆW merge(sl, ˆel, ml) (x)x l1 ˆW ˆW optimizer( sl, el, ml split( ˆW ) cl compression(ei) store(sl, cl, ml) ˆW x L/xL for . . . 1 do end for ) Split each element in the matrix into three components Compress the exponents losslessly Store the compressed exponents cL on device Decompress the exponents using temporary space Concatenate into floating-point number matrix using temporary space Linear calculation Label the variable required for back-propagation Calculate the gradient w.r.t. the model output Decompress the exponents using temporary space Concatenate into floating-point number matrix using temporary space Calculate gradient by Equation (5) Update the weight on-the-fly Split each element in the matrix into three components again Compress the exponents losslessly again Replace the stored components for layer on device Calculate the gradient of input for next layers"
        },
        {
            "title": "C The Tolerance of Random Perturbation",
            "content": "In this experiment, we would like to explore the sensitivity of neural network weights to random perturbations. For each parameter, we have two types of magnitudes: absolute and relative magnitudes. The former one represents the actual numerical error, whereas the second one is calculated based on the original value. For example, when the original value is 1.5, an absolute magnitude of 0.125 means the perturbed range is [1.5 0.125, 1.5 + 0.125]. On the other hand, relative magnitude of 0.125 means the perturbed range is [1.5 (1 + 0.125), 1.5 (1 0.125]. We conduct such experiment with the perturbation grid in Figure 7. For each cell, we choose the maximum error between relative error and absolute value for perturbing. random value is sampled from the perturbed range uniformly as the perturbation. The weight value is then set to the random sample. As shown in Figure 7, we see clear pattern that the model tends to tolerate the relative change rather than the absolute change."
        },
        {
            "title": "D The Storage for Lossless and Lossy Compression",
            "content": "In this section, we describe the underlying storage layout for NeuZip in Figure 8. Essentially, each BFloat16 number is first split into an exponent and signed mantissa. We group all the exponents in the matrix and perform the lossless compression. The signed mantissa is optionally truncated, depending on the required precision. The signed mantissa is then stored separately in memory. 15 Figure 7: Evaluating the byte-level perplexity with perturbed LLama-3 8B model (Dubey et al., 2024) on Wikitext-2 (Merity et al., 2016). Each parameter is perturbed with controlled noises. Both the xand y-axes are log-scale with base 2. (a) Lossless compression scheme. (b) Lossy compression scheme (with 3 bits). Figure 8: The storage structures for NeuZip."
        },
        {
            "title": "E Evaluating on MMLU",
            "content": "We provide the results on MMLU (Hendrycks et al., 2020) in Figure 9. Here, the theoretical optimal point should be at the top left corner. 16 Figure 9: The memoryperformance trade-off on the MMLU dataset. Similar to the results in Section 3.4, all of our NeuZip variants are on the Pareto frontier, suggesting the optimal trade-off between memory and performance."
        }
    ],
    "affiliations": [
        "University of Alberta",
        "Borealis AI"
    ]
}