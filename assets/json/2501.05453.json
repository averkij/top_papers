{
    "paper_title": "An Empirical Study of Autoregressive Pre-training from Videos",
    "authors": [
        "Jathushan Rajasegaran",
        "Ilija Radosavovic",
        "Rahul Ravishankar",
        "Yossi Gandelsman",
        "Christoph Feichtenhofer",
        "Jitendra Malik"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We empirically study autoregressive pre-training from videos. To perform our study, we construct a series of autoregressive video models, called Toto. We treat videos as sequences of visual tokens and train transformer models to autoregressively predict future tokens. Our models are pre-trained on a diverse dataset of videos and images comprising over 1 trillion visual tokens. We explore different architectural, training, and inference design choices. We evaluate the learned visual representations on a range of downstream tasks including image recognition, video classification, object tracking, and robotics. Our results demonstrate that, despite minimal inductive biases, autoregressive pre-training leads to competitive performance across all benchmarks. Finally, we find that scaling our video models results in similar scaling curves to those seen in language models, albeit with a different rate. More details at https://brjathu.github.io/toto/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 3 5 4 5 0 . 1 0 5 2 : r An Empirical Study of Autoregressive Pre-training from Videos Jathushan Rajasegaran1,2, Ilija Radosavovic2, Rahul Ravishankar2, Yossi Gandelsman1,2, Christoph Feichtenhofer1, Jitendra Malik1,2 1Meta FAIR, 2UC Berkeley We empirically study autoregressive pre-training from videos. To perform our study, we construct series of autoregressive video models, called Toto. We treat videos as sequences of visual tokens and train transformer models to autoregressively predict future tokens. Our models are pre-trained on diverse dataset of videos and images comprising over 1 trillion visual tokens. We explore different architectural, training, and inference design choices. We evaluate the learned visual representations on range of downstream tasks including image recognition, video classification, object tracking, and robotics. Our results demonstrate that, despite minimal inductive biases, autoregressive pre-training leads to competitive performance across all benchmarks. Finally, we find that scaling our video models results in similar scaling curves to those seen in language models, albeit with different rate. Website: https://brjathu.github.io/toto"
        },
        {
            "title": "1 Introduction",
            "content": "In paper published in 1951, Shannon, having just published the foundational papers of information theory, proposed guessing game of next word prediction to estimate the entropy of English (Shannon, 1951). Nearly 70 years later, training high-capacity transformer network (Vaswani et al., 2017) on this task, provided the generative pre-training backbone for Large Language Models (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020). Less well known is the fact that in 1954, Fred Attneave (Attneave, 1954) proposed an analog of Shannons task for images. To quote We may divide the picture into arbitrarily small elements which we transmit to subject (S) in cumulative sequence, having them guess at the color of each successive element until they are correct. This method of analysis resembles the scanning process used in television and facsimile systems and accomplishes the like purpose of transforming two spatial dimensions into single sequence in time. While Attneave was concerned with images, in the context of 2024, we have to note that the Big Visual Data is in videos. While there are concerns that most of the text available on the Internet has already been used by the language models, in video we just started on the journey of Big Data exploitation. Despite the successes of autoregressive language and image models, their effectiveness for video modeling remains underexplored. In this paper, we empirically study autoregressive pre-training from videos. To perform our empirical study, we construct family of autoregressive video models which we call Toto. We treat videos as sequences of visual tokens and train causal transformer models on next-token prediction task. We use causal transformer model with LLaMa (Touvron et al., 2023) architecture. We use dVAE (Ramesh et al., 2021) to tokenize frames into discrete tokens. Treating videos as sequences of tokens enables us to jointly train on videos and images using unified format. We construct diverse dataset of videos and images comprising over 1 trillion visual tokens. Our models are first pre-trained on this data and then evaluated on downstream tasks. We extract visual representations using attention pooling from relevant layers of the model. We evaluate our models on various downstream tasks from image and video recognition, video forecasting, semi-supervised tracking, object permanence and robotics tasks in both simulation and real-world. We consider different design choices such as tokenizers including dVAE (Ramesh et al., 2021), VQGAN (Rombach et al., 2022) and continuous patch-normalized (He et al., 2022) tokens. We also consider different architectures such as LLaMA (Touvron et al., 2023), GPT2 (Radford et al., 2019) and Mamaba Gu & Dao (2023). Finally we study the compute optimal scaling behaviors of autoregressive video models. Figure 1 Overall Framework. Starting with images and video frames from collection of datasets, we tokenize each frame/image into discrete visual tokens independently. We pre-train the transformer by predicting the next visual tokens, with context length of 4K tokens of images or video frames. Once trained, we take the intermediate representations and evaluate them on various tasks. We find that, for tokenization autoregressive models based on discrete and continuous patch-normalized (He et al., 2022) tokens perform similarly on ImageNet classification task. For efficient pre-training, starting with lower resolution and fine-tuning at higher resolution gives better performance and RoPE (Su et al., 2024) helps with adopting to higher resolution. For measuring the representation quality in decoder-only models, due to skewed nature of the receptive field we use attention pooling over average pooling. We find that in decoder-only models, for all tasks and models sizes the middle layer gives the best performance. Finally, we study the scaling behaviors of autoregressive vision models, which scales with more compute but still at slower rate compared to large language models."
        },
        {
            "title": "2 Related work\nRepresentation Learning for Vision: Over the years self-supervised pre-training has proven to be effective in\nmany areas including language, vision, and robotics. Wu et al. (2018) and SimCLR (Chen et al., 2020b)\nshowed that instance discrimination training can learn strong discriminative features. MoCo (He et al., 2020)\nand DINO (Caron et al., 2021) showed the effectiveness of strong visual representations on various downstream\ntasks. Differently, BEiT (Bao et al., 2021) and MAE (He et al., 2022) used masked autoencoding for learning\nimage representations. ST-MAE (Feichtenhofer et al., 2022)and VideoMAE (Wang et al., 2023a) extended this\nmasked modeling approach to videos, by masking a large amount of tokens during pre-training and predict\nthe masked tokens with a light-weight decoder.",
            "content": "Autoregressive Modeling of Vision: Generative autoregressive pre-training learns to directly model the data distribution. In language models, generative pre-training has become the standard for training large models. For autoregressive pre-training in vision, rCNN (Ranzato et al., 2014), PixelCNN (Van den Oord et al., 2016) and PixelRNN (Van Den Oord et al., 2016) proposed generating pixels one by one using convolution and bidirectional LSTMs. With the introduction of the transformers (Vaswani et al., 2017), ImageTransformers (Parmar et al., 2018) showed generating pixels with causal local attention performs better than previous CNN and RNN-based methods. While all of these methods focused on the generation quality of the pixels, iGPT (Chen et al., 2020a) showed that generative pre-training is also good way to learn strong visual representations for recognition tasks. Henighan et al. (2020) showed scaling behaviors of autoregressive image and video models. AIM (El-Nouby et al., 2024) on the other hand uses patch embedding rather than any pre-trained models for tokenization, however, it trains on Data Filtering Networks (Fang et al., 2023) with clip filtered data. Compared to these works, we do not use any supervision during our pre-training and utilizes image and videos jointly. VisionMamba (Zhu et al., 2024) also showed how to utilize sequence models with bidirectional state-space modeling for supervised vision tasks. Weissenborn et al. (2019) showed autoregressive video generation for promotable video generations. Evaluation of Vision Representations: Most video pre-training models are evaluated on semantic tasks like ImageNet (Deng et al., 2009) and Kinetics (Kay et al., 2017). Additionally to the standard evaluation, we evaluate our models on semi-supervised tracking task on DAVIS (Pont-Tuset et al., 2017), action forecasting on Ego4D (Grauman et al., 2022), object permanence on CATER (Girdhar & Ramanan, 2019) and on robot manipulation tasksn simulation (Xiao et al., 2022) and in the real world Radosavovic et al. (2023)."
        },
        {
            "title": "3 Approach",
            "content": "We train casual transformer model to predict the next patch tokens in images and videos. This is akin to the next token prediction in large language models. From the vast collection of images and videos, every patch is tokenized into discrete token, and the transformer is trained to predict the next token, using raster scan ordering. We pre-train our models on over one trillion tokens. Finally, we evaluate the learned representations of these models on various downstream tasks including image classification, action classification, action anticipation, video tracking, object permanence, and robotic manipulation tasks. We also study the scaling behaviors of our models for compute optimal training."
        },
        {
            "title": "3.1 Pre-training",
            "content": "Given large collection of images and videos, we tokenize all of them into 1D sequence using raster scan ordering. This produces dataset of tokens, {xj n} where is the sample either 3, ..., xj from video or an image and is the number of tokens in an image or video. We model the density p(x) as : 2, xj 1, xj p(xj) = (cid:89) i= p(xj xj i1, xj i2, ..., xj 1, θ) (1) Here, θ is the model parameters, which can be optimized by minimizing the negative log-likelihood loss: Lpre-train = xj log p(xj). (2) Using this loss, we pre-train our models at different sizes on over one visual trillion tokens. These tokens are generated from images and video. Figure 2 shows the training loss of 3 differently sized models with 120M, 280m and 1.1b parameters."
        },
        {
            "title": "3.2 Architecture",
            "content": "Figure 2 Training Loss Curves: We show the training loss curves for base, large, and 1b models trained with tokens from dVAE (Ramesh et al., 2021) with vocabulary size of 8k and context length of 4k tokens (equivalent to 16 images or video frames). Our model is transformer (Vaswani et al., 2017) with causal attention. We apply recent advancements in language modeling such as pre-norm using RMSNorm (Zhang & Sennrich, 2019), SwiGLU activation (Shazeer, 2020), and RoPE positional embeddings (Su et al., 2024), following LLaMa (Touvron et al., 2023). For model with layers, we define to be the intermediate representations after layer l, 0 L. The intermediate representations after layer + 1, l+1, defined to be: (cid:98)H l+1 = + MHSA(RMS-norm(H l)) l+1 = (cid:98)H l+1 + MLP(RMS-norm( (cid:98)H l+1)), (3) (4) Where MHSA is multi-head self attention layer, MLP is multi-layer perceptron with SwiGLU activations. We train our models for the next token prediction task at different scales (base, large and 1b models). For more architecture details see Table 1. We train all these models with batch size of 1M tokens. We use AdamW (Loshchilov & Hutter, 2017) with maximum learning rate of 3e4, and β1 = 0.9, β2 = 0.95. We decay the learning rate with cosine schedule, after 2000 warm-up steps (Touvron et al., 2023). Model Params Dimension Heads Layers base large 1b 120m 280m 1.1b 768 1024 2048 12 16 16 12 16 22 Table 1 Model Architecture: We pre-train models at different scales, only on visual tokens from images and videos. 3 Datasets Instances Tokens ImageNet Kinetics-600 Ego4D HowTo100m 13.9M 0.53M 52.1K 1.172M 3.6B 41.3B 103B 2560B Hours - 1496 3750 92627 Table 2 Pre-training Dataset: We use both image datasets (Imagenet (Russakovsky et al., 2015)) and video datasets (Kinetics600 (Carreira et al., 2019), Ego4D (Grauman et al., 2022), HowTo100m (Miech et al., 2019)) with different mixing ratios during the pre-training of our models. The whole training data contains about 100,000 hours of videos."
        },
        {
            "title": "3.3 Dataset",
            "content": "To train our model, we compile large dataset from number of different sources. Table 2 shows the total number of images and videos used for training data, the total number of tokens, as well as the number of hours of videos in each dataset. Together these datasets contain over 100,000 hours of video data and about 2.5 trillion visual tokens. During training, each mini-batch is sampled at different ratios of datasets. Each batch approximately contains 20% of ImageNet images, 10% of Ego4D videos, 10% of Kinetics videos, and 60% of HowTo100m videos. Our full training utilized about 1 trillion tokens."
        },
        {
            "title": "3.4 Tokenization",
            "content": "We use dVAE tokenizer with vocabulary of 8k tokens, from Dall-E (Ramesh et al., 2021) as our tokenizer. Using an image-based tokenizer allows training on both images and videos and testing on respective downstream tasks. While VQGAN (Esser et al., 2020) tokenizers provide sharper images, these models were trained with perceptual loss (Larsen et al., 2016; Johnson et al., 2016), thus indirectly ingesting ImageNet label information via VGG-net (Simonyan & Zisserman, 2014). All raw pixel frames or images are tokenized into 256 discrete tokens. We take video and resize it such that its shortest size is pixels, and then take random crop of , and sample every 4 frames where is the number of frames. We use dVAE (Ramesh et al., 2021) with the vocabulary of 8k entries to tokenize every frame independently. For dVAE we set = 128, to get 16 16 discrete tokens. Once every frame is mapped into set of discrete tokens we have 256 tokens per each video. We pre-train all the models with = 16, thus all the models are per-trained for context length of 4096 tokens. When training with images and videos, 16 video frames are sampled to create 4k tokens. For images, we randomly sample 16 images and create sequence of 16 image frames to generate 4k tokens. Finally, we add start and end tokens for each sequence, for videos we use [1] as the start token, and for images we use [3] as the start token, and all sequences have an end token of [2]."
        },
        {
            "title": "3.5 Downstream Transfer",
            "content": "The idea of large pre-trained models is that they were trained at large compute scale, and then these models can be easily used for various downstream tasks without requiring task-specific design or lots of computing for transfer. The learned representations are general enough to transfer to various tasks. We evaluate our models on the intermediate features with linear and attention probing (Lee et al., 2019). For linear probing the model, we apply global average pooling (Lin et al., 2013) over the tokens from different layers to get the intermediate representation. We train linear layer on top of this representation on the downstream task. MAE (He et al., 2022) or DINO (Caron et al., 2021) have uniform structure when it comes to which token attends to which tokens, however in autoregressive sequence modeling later tokens attent to more tokens than the tokens at the beginning. Due to this skewed nature, equally weighting all the tokens affects the downstream performance. Attention pooling is an alternative to average pooling that allows to dynamically weight the tokens, ideally giving more weight to tokens that see more tokens. This requires learning Wk and Wv matrices and query token q. The query token cross-attends to the intermediate tokens and combines them into single vector. While this function is not linear anymore, it has been shown to learn better representations in recent works (El-Nouby et al., 2024)."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate our pre-trained models on various downstream tasks such as ImageNet classification, Kinetics action recognition, Ego4D action anticipation, Semi-Supervised tracking, and Robotic manipulation tasks. First, we discuss various design choices for pre-training and evaluation strategies for our method. All the models for studying the design choices are large models trained for 400 epochs on the ImageNet-1k dataset."
        },
        {
            "title": "4.1 Design Choices\nTokenizer: The are various options available for tokenizing an image or a video. We could use discrete\ntokenizers such as dVAE, and VQGAN, or simple patch-based continuous tokenization. To study the behavior\nof various tokenizers we pre-train a Toto-large model on ImageNet for 400 epochs. Using linear probing at an\noptimal intermediate layer, we evaluate the accuracy of the models on ImageNet classification task.",
            "content": "Table 3 shows linear probing accuracy when trained with various tokenizers. VQGAN (Esser et al., 2020) and dVAE (Ramesh et al., 2021) perform similarly with the same resolutions. However, VQGAN is contaminated with ImageNet label information via perceptual loss. In addition to that, as shown in Figure 3, dVAE tokens have full coverage compared to VQGAN tokens on their 1-gram distributions. Please see the supplementary material for more details. Regressing normalized-patch targets from patch embeddings performs slightly worse than classifying discrete tokens as targets. Additionally, discrete tokens as targets and patch embeddings as inputs perform poorly compared to other methods at the given input-output resolutions. Overall, Table 3 shows that various ways of tokenization have little effect on ImageNet linear probing accuracy. Figure 3 1-gram Distribution of Various Tokens: This Figure shows the distribution of 1-gram tokens of various tokenizers (dVAE (Ramesh et al., 2021), VQGAN-1k, VQGAN-16k (Esser et al., 2020)) on Imagenet validation set. Note that, dVAE has almost full convergence of the tokens while VQGAN has less than 50% coverage of the tokens. Tokens Vocabulary Input-Target VQGAN-VQGAN 16x16 VQGAN-VQGAN 16x16 32x32 dVAE-dVAE 16x16 dVAE-dVAE 16x16 patch-patch 16x16 patch-dVAE 16k 1k 8k 8k - 8k Top1 61.3 61.1 61.2 53.2 60.6 58. Table 3 ImageNet Linear Probing Accuracy with Various Tokenizers: We compare discrete (dVAE, VQGAN) and patch embedding as input and target for pre-training our models. ImageNet top-1 accuracies are computed by linear probing at the 9th layer of the large model. How to Probe: As discussed in Section 3.5 we probe the pre-trained models at the same layer with attention pooling and average pooling, followed by linear layer. Table 5 shows attention pooling performs 7.9% higher than average pooling on the ImageNet classification task. For attention pooling, we keep the embedding dimension the same as the intermediate feature dimensions. 5 Method dVAE/16 dVAE/32 dVAE/1632 dVAE/1632 Compute 1.42 1017 5.68 1017 2.13 1017 2.13 1017 Top1 53.2 61.2 63.2 64. Method Tokens Pooling Top1 dVAE 16x16 Average 53.2 dVAE 16x16 Attention 61.1 Table 4 Token Resolution: While the performance is lower for low-resolution model, when finetuned for next-patch prediction at higher resolution, its performance surpasses the full-resolution pre-trained model. Base values of the RoPE is 50,000. Table 5 Attention vs Average Pooling: When probed at the same layers, attention pooling performs much better than average pooling of intermediate tokens. Resolution: When training with dVAE tokens, 256x256 image results in 1024 tokens, this is four times more number of tokens compared to patch embeddings or VQGAN tokens. If we reduce the number of tokens to 256, then the effective image resolution becomes 128x128. Table 4 shows clear drop in performance when pre-training the model at 128x128 resolution. However, due to the use of relative positional embeddings (RoPE (Su et al., 2024)), we can easily finetune the 128x128 (or 16x16 token equivalent) model for higher resolution. Surprisingly, this does better than pre-training at 256x256 resolution and requires only one epoch of finetuning. Not only does this improve the performance, but the pre-training also becomes cheaper compared to full-resolution pre-training. Additionally, Fine-tuning with higher base values of the RoPE embeddings (50,000) leads to better accuracy. Architecture: We train various language models from GPT2 (Radford et al., 2019) with absolute sine-cosine positional embeddings, and non-transformer based model Mamba (Gu & Dao, 2023) only using dVAE tokens. We mimicked the GPT2 architecture and do architecture comparisons. We compare these models with LLaMA (Touvron et al., 2023). We evaluate linear probing performance at each layer of these models and report the best performance in Table 6. Probing Layer: When probing the pre-trained models, especially the decoder-only model best performance is observed at the middle layers. This behavior is first observed in iGPT (Chen et al., 2020a). Figure 4 shows the peak performance on recognition occurs at about 50% of the depth of the model. This behavior holds across all model sizes. While in MAE (He et al., 2022) and BEiT (Bao et al., 2021) encoder-decoder models, due to the uneven nature of the encoder and decoder, the best features are observed at the top of the encoder layers. However, on decoder-only models with uniformly distributed layers, the last layers perform worse on recognition tasks, mainly because these layers are trained to reconstruct the input. More probing results with various tokenizers, resolutions, and probing methods are shown in the supplementary material. Model GPT2 Radford et al. (2019) Mamba Gu & Dao (2023) LLaMA Touvron et al. (2023) Params Top1 280 48.5 290 40.7 280 53.2 Table 6 Architecture: We compare sequence modeling architectures LLaMA Touvron et al. (2023), GPT2 Radford et al. (2019), and non-transformer models, Mamba Gu & Dao (2023) on ImageNet linear probing task. Figure 4 Probing at Different Layers: We show the attentionprobing performance at each layer of our three models. Peak performance is observed at around 50% depth of the models."
        },
        {
            "title": "4.2 Image Recognition",
            "content": "To measure the representation quality of our pre-trained models, we evaluate our models on ImageNet-1k (Deng et al., 2009) classification. We apply probe at each layer of the model, with attention pooling, and choose the optimal layer with the highest classification accuracy. We fine-tune the pre-trained models further by applying self-supervised next token prediction loss in Eq 2, together with cross-entropy loss applied for probing layers (with stop-gradients). We train the probing layers for 90 epochs, with learning rate of 6e5. We also use layer decay of 0.9 to reduce the learning rate at the early layers of the model. During this stage, all the models are fine tuned with 32 32 token resolution, on the self-supervised loss, and increase the base value of the RoPE (Su et al., 2024) embeddings from 10,000 to 50,000 support larger resolution. Table 7 shows the ImageNet top-1 accuracy of our base, large and 1b models. First, there is clear difference in terms of classification performance when it comes to discriminative models versus generative models. Instance discriminative models such as SimCLR (Chen et al., 2020b), and DINO (Caron et al., 2021) are trained to separate samples from each other and they are designed to perform well on discriminative tasks. On the other hand, generative models are just trying to model the data distribution. While achieving comparable performance to other generative models on image recognition, among autoregressive generative models, our model achieved the highest top-1 accuracy. The scaling of data, and the use of tokens instead of pixels, allows our one billion parameter model to achieve similar performance compared to iGPT (Chen et al., 2020a) 7 billion models. Method Arch #θ Top Discriminative Approaches SimCLR (Chen et al., 2020b) RN50x2 RN50x2 BYOL (Grill et al., 2020) RN50x2 SwAV (Caron et al., 2020) ViT-B/8 DINO (Caron et al., 2021) ViT-g/14 DINOv2 (Oquab et al., 2023) Generative Approaches ViT-L/14 BEiT-L (Bao et al., 2021) ViT-1B/14 AIM (El-Nouby et al., 2024) ViT-H/14 MAE (He et al., 2022) iGPT-L (Chen et al., 2020a) GPT-2 iGPT-XL (Chen et al., 2020a) GPT-2 Toto-base Toto-large Toto-1b LLaMA LLaMA LLaMA 94 94 94 86 307 1200 632 1386 6801 120 280 1100 74.2 77.4 73.5 80.1 86.4 62.2 80.6 80.9 65.2 72.0 64.7 71.1 75.3 Table 7 ImageNet Results: We compare discriminative and generative models on ImageNet (Deng et al., 2009) recognition task. While achieving comparable performance among generative models, our models model achieves the highest accuracy on autoregressive modeling. models are evaluated with linear probing."
        },
        {
            "title": "4.3 Action Recognition",
            "content": "We use Kinetics-400 (K400) (Kay et al., 2017) for evaluating our models on action recognition tasks. Similar to ImageNet evaluation, we apply probe at each layer of the model, with attention pooling, and choose the optimal layer with the highest action classification accuracy. We also fine-tune the pre-trained models on self-supervised next-patch prediction task while training the probing layers with classification loss. All our video models are trained with 16 frames, thus with context length of 4096 tokens per video. When evaluating videos, we follow the protocol in SlowFast (Feichtenhofer et al., 2019). Unlike ImageNet where we evaluate the models at 256x256 resolution, on videos we only evaluate our models at 128x128 resolution, to keep the number of tokens in similar budget. 7 Table 8 shows the Kinetics-400 top-1 accuracy of our base, large and 1b models. Similar to ImageNet results in Table 7, we see that discriminately trained models perform better than generative models. Our models achieve comparable performance among generative models, and first to show competitive performance on action recognition with autoregressive generative modeling. All the models are trained and evaluated with 16 frames with stride of 4 frames. Method Arch Discriminative Approaches I-JEPA (Assran et al., 2023) OpenCLIP (Cherti et al., 2023) DINOv2 (Oquab et al., 2023) InternVideo (Wang et al., 2022) ViT-H/16 ViT-G/14 ViT-g/14 -"
        },
        {
            "title": "Generative Approaches",
            "content": "Hiera-H/14 Hiera (Ryali et al., 2023) MVD (Wang et al., 2023b) ViT-H/14 VideoMAE (Wang et al., 2023a) ViT-L/14 Toto-base Toto-large Toto-1b LLaMA LLaMA LLaMA Top1 74.5 83.3 84.4 73.7 77.0 79.4 79.8 59.3 65.3 74.4 Table 8 K400 Results: We compare discriminative and generative models on Kinetics-400 (Kay et al., 2017) action recognition task. While achieving comparable performance among generative models, our models are the first to show the competitive performance on K400 with autoregressive pre-training, and shows scaling with large model sizes."
        },
        {
            "title": "4.4 Action Forecasting",
            "content": "While the Kinetics dataset captures internet-style exocentric videos, Ego4D (Grauman et al., 2022) videos capture day-to-day life egocentric videos. general vision model should be able to reason about both exo and ego-centric videos. Task-wise, Kinetics requires the model to reason about the action using full context (e.g. the model has seen the action), while the Ego4D short-term action anticipation v1 task requires models to predict future actions from past context. We use our models as the backbone for the pyramid network used in StillFast (Ragusa et al., 2023) extract tokens at 5 layers and fuse them with the pyramid network. We fully fine-tuned our model with self-supervised next-patch loss along with task-related losses, and we observed having self-supervision loss improves overall performance. Table 9 shows the performance of our large model on the Ego4D short-term action anticipation task. This task requires predicting the object to be interacted with (noun) and the type of interaction (verb) as well as time to contact (ttc) from the last seen frame to an estimated time between object-hand contact. As shown in Table 9, these tasks are difficult with maximum overall mean-average precision of 2.70. Noun N+V N+TTC Overall Method 17.55 1.56 FRCNN+Rnd (Grauman et al., 2022) 17.55 5.19 FRCNN+SF (Grauman et al., 2022) 14.05 6.03 Hiera-large (Ryali et al., 2023) 16.20 7.47 StillFast (Ragusa et al., 2023) 15.16 6.72 VideoMAE-large (Wang et al., 2023a) MAE-ST-large (Feichtenhofer et al., 2022) 13.71 6.63 0.34 2.07 2.12 2.48 2.55 2.60 3.21 5.37 4.53 4.94 5.26 4.94 Toto-large 15.20 6. 5.41 2.70 Table 9 Ego4D Results: Our model achieves comparable mean-average precision compared to previous work. We compare our method with, FRCNN+Rnd (Grauman et al., 2022), FRCNN+SF (Grauman et al., 2022), Hiera (Ryali et al., 2023), StillFast (Ragusa et al., 2023), VideoMAE (Wang et al., 2023a), and MAE-ST (Feichtenhofer et al., 2022). 8 Figure 5 Semi-Supervised Tracking: We follow the protocol in STC (Jabri et al., 2020), start with the GT segmentation mask, and propagate the labels using the features computed by Toto-large. The mask was propagated up to 60 frames without losing much information. (a) Franka Pick (b) Kuka Pick (c) Franka Cabinet (d) Kuka Cabinet Figure 6 Robot Manipulation with Reinforcement Learning: We compare MAE-base (Radosavovic et al., 2022) with Toto-base pre-trained models in simulation following Xiao et al. (2022). We evaluate each model the mean success rate over training steps. Toto was able to learn these tasks faster than MAE, across two robots and two tasks."
        },
        {
            "title": "4.5 Video Tracking",
            "content": "We study our pre-trained models on label propagation using the protocols in (Jabri et al., 2020) on DAVIS dataset (Pont-Tuset et al., 2017). Compared to previous tasks such as classification, and forecasting, this evaluation does not require finetuning or probing of the features. Following Jabri et al. (2020), we use the features from the last frames to find the nearest neighbor patch in the current frame, and then propagate the masks from the previous frames to the current frame.Comparison with Dino (Caron et al., 2021) and MAE (He et al., 2022) is shown in Table 10 and qualitative results are shown in Figure 5. Method (Res/Patch) DINO-base (224/8) DINO-base (224/16) MAE-base (224/16) Toto-base (256/8) Toto-large (256/8) Toto-1b (256/8) Toto-large (512/8) J&F 54.3 33.1 31.5 42.0 44.8 46.1 62.4 52.5 36.2 34. 41.2 44.4 45.8 59.2 56.1 30.1 28.9 43.1 45.1 46.4 65.6 Table 10 DAVIS Tracking: We report J, F, and J&F scores at the peak layers of each model. We achieves comparable performance as DINO and at large resolution (512), it outperforms all methods. 9 Figure 7 Real-world Deployment: We show an example episode of our policy performing the cube picking task on Franka robot in the real world. We use Toto-base to run the robot at real time, despite being small model, Toto was able to achieve about 63% success rate in real world setting."
        },
        {
            "title": "4.6 Robotics",
            "content": "In this section, we study the effectiveness of our pre-trained representations for robotic manipulation. We consider tasks in both simulation and in the real world. Real world experiments needs to run at real time, there for we only use Toto-base models, in both setting. Despite being small model, Toto-base can achieve better performance in simulation and on-par performance to state-of-the-art robot models in real world experiments. Simulation Experiments: Following the protocols in MVP (Xiao et al., 2022), we use our visual pre-trained models to embed pixel observations. The model is frozen and we only take tokens at an intermediate layer, apply average pooling, and learn the linear layer on top to embed pixel observations. These observations are used to train DAgger policies for 4 different tasks: Franka-pick 6a, Kuka-pick 6b, Franka-cabinet 6c, and Kuka-cabinet tasks 6d. Figure 6 shows the mean success rate over training steps. Compared to the MVP baseline, our model was able to learn these tasks faster with better sample efficiency across robots and tasks. For fair comparisons, we use the best MAE model from MVP (Radosavovic et al., 2022) which is trained on ImageNet (Deng et al., 2009), Ego4D (Grauman et al., 2022) and 100DOH (Shan et al., 2020) datasets. Real-world Experiments: Next, we evaluate our pretrained representations in the real world. We follow the setup from (Radosavovic et al., 2022). We extract vision features using pre-trained vision encoder and train controller on top of frozen representations using behavior cloning. Specifically, we consider cube picking tasks using 7 DoF Franka robot, shown in Figure 7. We use the demonstrations provided by (Radosavovic In Table 11 we compare our model to et al., 2023). vision encoder from (Radosavovic et al., 2022). We report the success rate over 16 trials with variations in object position and orientation. Our model performs favorably to vision encoder pre-trained for robotics."
        },
        {
            "title": "4.7 Object Permanence",
            "content": "To quantitatively measure the performance of how well the model understands object permanence, we evaluate our models on CATER localization task (Girdhar & Ramanan, 2019). Here, ball is moving in the scene, and the task is to find its location in the 6 by 6 grid. We fine tune our Toto-large model on this task at temporal resolutions 16, and 32 frames. In both cases, our pre-trained models were better at localizing the target compared to models trained specifically for this task, such as V3D (Zhang, 2022), TFC-V3D (Zhang, 2022). Table ?? shows the performance on the CATER snitch localization task, and Toto-large achieve 62.8% and 70.9% performance with 16 and 32 frames respectively. Model MVP Toto-base # Traj 240 240 Success 75% 63% Table 11 Robotics, Real-world Experiments: We compare MVP (Radosavovic et al., 2022) and Toto on Franka cube-picking task in the real world. Features from both models are pre-trained, frozen, and passed into learning module trained with behavior cloning using the same demonstrations. We see that our approach performs comparably to the state-of-the-art vision backbone for robotics, despite not being designed with the robotic application in mind. Method V3D Model ResNet TFC V3D ResNet Toto-large LLaMa 16 55. 54.6 62.8 32 69.7 70.2 72. Table 12 Object Permanence: CATER (Girdhar & Ramanan, 2019) object localization task, where the object is hidden under or obstructed by other objects. The model is trained to predict its coarse location. Our model performs better than previous methods on snitch localization task at 16, 32 temporal resolutions. 10 Figure 8 Probing Across Layers, Models, and Tasks: We study the behavior of our models across multiple layers and tasks. For image classification, action recognition, and object tracking, all the models behave similarly and peak around 50% of the model depth. This behavior is observed across all model sizes. Robot tasks show similar behaviour, where the middle layers perform good at picking the objects, but last layers also perform good as middle layers. These plots suggests, in decoder-only model, first half of the model starts to behave like an encoder, and compress the information, and then rest of the model, projects the compressed semantic features back to input space."
        },
        {
            "title": "4.8 Probing Across Layers",
            "content": "As shown in Figure 4 for the ImageNet classification task, different layers of the model contribute to the task differently for the image classification task; this behavior is also observed in iGPT (Chen et al., 2020a). To study this behavior across multiple tasks, we train probing layers for action recognition, object tracking, and robot manipulation. Figure 8 shows probing performance across layers, model size, and tasks. It shows that action recognition follows similar trend to ImageNet classification, having peak performance at the middle of the model stacks. While Object tracking also shares similar trend with image classification and action recognition, object manipulation shows an interesting trend of the last layers performing well as middle layers from picking objects. Compared to the first three tasks, robot manipulation has generative nature as task and can benefit from generative pre-training. In encoder models (Caron et al., 2021) or encoder-decoder models (He et al., 2022; Bao et al., 2021) the last layer of the encoder has more semantic features. This may suggest that, in decoder-only model, first half of the model starts to behave like an encoder, and compress the information, and then rest of the model, projects the compressed semantic features back to input space."
        },
        {
            "title": "4.9 Compute Optimal Scaling",
            "content": "We study the scaling behaviors of Toto using µParameterization (Yang et al., 2022). First we train various models, a1-a6, with linearly increasing hidden size and number of layers  (Table 15)  . All models use the VQGAN tokenizer (Esser et al., 2020). We then optimize the learning rate for these models, with µ-Parameterization (Yang et al., 2022). Figure 11 shows optimal learning rate of 27 for all of the model widths. Once we find the optimal learning rate, we train a1-a6 models on our data mixture  (Table 2)  . Figure 9 shows the loss vs training compute of Toto models. This shows clear power law relationship between the compute and validation loss. Based on these experiments Toto shows power law of L(C) = 7.32 0.0378. For comparison, the GPT-3 power law relationship (Brown, 2020) is L(C) = 2.57 0.048. While these are not comparable directly, the scaling coefficients indicate how much change in loss to expect for extra added compute. This suggests that the visual next token prediction models, such as Toto, scale but at slower rate than language models. Figure 9 Scaling Toto: We train multiple variants of Toto, with increasing hidden size and depth, with optimal learning rates. We plot the validation loss vs the compute spent on training in MACs. This shows clear scaling behavior with optimal compute."
        },
        {
            "title": "5 Limitations",
            "content": "Our study suggests several important limitations and opportunities for future work. significant limitation stems from the use of internet videos, which, unlike carefully curated datasets, introduces challenges related to data quality and diversity. This variance in data quality can impact model performance, especially when compared to models trained on more curated datasets. Another limitation is the use of tokenizer, this makes the learning not end-to-end, and the representation and generation quality is bounded by the quality of the tokenizer, and with quantized vectors, the quality is very much limited, this needs further explorations to build universal visual tokenizer. Another fundamental limitation is training on videos for next token prediction task. The added redundancy in video frames, can hurt quality of the learned representations. See Appendix A.1 for more discussion on this topic. Additionally, our exploration of various design choices are based on ImageNet classification. While it does transfer to most of the tasks we considered in this paper, it may not be the optimal configuration for many other tasks. Furthermore, we have not yet fully assessed our methods effectiveness in dealing with dense prediction tasks, fine-grained recognition, or comprehending complex temporal dynamics over extended time frames. These areas represent key opportunities for further research, aiming to broaden the fruitfulness of autoregressive pre-trained models."
        },
        {
            "title": "6 Conclusion",
            "content": "We empirically studied autoregressive pre-training from images and videos. We curated large video dataset and conducted large-scale evaluation across range of diverse tasks, including image recognition, video classification, video forecasting, object tracking, object permanence, and robotic manipulation. We performed extensive ablation studies to understand different design choices and compared auto regressive pre-training from videos to strong baselines across different tasks. We found that, despite minimal inductive biases, our approach achieves competitive performance across all tasks. Finally, we studied the scaling behavior of visual next token prediction models, and showed it scales with compute, but at slower rate than text based next token prediction models."
        },
        {
            "title": "7 Acknowledgments",
            "content": "We thank Andrea Madotto, Po-Yao (Bernie) Huang, and Shiry Ginosar for helpful discussions. Were grateful to Ronghang Hu and Xinlei Chen for their help with TPU setup and code bases. We also thank Baifeng Shi for helping us with robots evaluations. We thank Valentin Gabeur and Neerja Thakkar for their valuable feedback on the paper."
        },
        {
            "title": "References",
            "content": "Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1561915629, 2023. Fred Attneave. Some informational aspects of visual perception. Psychological review, 1954. Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:99129924, 2020. 12 Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987, 2019. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pp. 16911703. PMLR, 2020a. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 15971607. PMLR, 2020b. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 28182829, 2023. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. 2019. Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal Shankar, Joshua Susskind, and Armand Joulin. Scalable pre-training of large autoregressive image models. arXiv preprint arXiv:2401.08541, 2024. Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis. 2021 ieee. In CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1286812878, 2020. Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 62026211, 2019. Christoph Feichtenhofer, Yanghao Li, Kaiming He, et al. Masked autoencoders as spatiotemporal learners. Advances in neural information processing systems, 35:3594635958, 2022. Rohit Girdhar and Deva Ramanan. Cater: diagnostic dataset for compositional actions and temporal reasoning. arXiv preprint arXiv:1910.04744, 2019. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1899519012, 2022. Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:2127121284, 2020. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 97299738, 2020. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020. Allan Jabri, Andrew Owens, and Alexei Efros. Space-time correspondence as contrastive random walk. Advances in neural information processing systems, 33:1954519560, 2020. Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pp. 694711. Springer, 2016. Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using learned similarity metric. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 15581566, New York, New York, USA, 2022 Jun 2016. PMLR. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: framework for attention-based permutation-invariant neural networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 37443753. PMLR, 0915 Jun 2019. Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 26302640, 2019. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International conference on machine learning, pp. 40554064. PMLR, 2018. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. 2019. Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world robot learning with masked visual pre-training. In Conference on Robot Learning, 2022. Ilija Radosavovic, Baifeng Shi, Letian Fu, Ken Goldberg, Trevor Darrell, and Jitendra Malik. Robot learning with sensorimotor pre-training. In Conference on Robot Learning, 2023. Francesco Ragusa, Giovanni Maria Farinella, and Antonino Furnari. Stillfast: An end-to-end approach for short-term object interaction anticipation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 36353644, 2023. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 88218831. PMLR, 2021. MarcAurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan Collobert, and Sumit Chopra. Video (language) modeling: baseline for generative models of natural videos. arXiv preprint arXiv:1412.6604, 2014. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. 14 Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, et al. Hiera: hierarchical vision transformer without the bells-andwhistles. arXiv preprint arXiv:2306.00989, 2023. Dandan Shan, Jiaqi Geng, Michelle Shu, and David Fouhey. Understanding human hands in contact at internet scale. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 98699878, 2020. Claude Shannon. Prediction and entropy of printed english. Bell system technical journal, 1951. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. Aäron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International conference on machine learning, pp. 17471756. PMLR, 2016. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1454914560, 2023a. Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Lu Yuan, and Yu-Gang Jiang. Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning. In CVPR, 2023b. Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. Dirk Weissenborn, Oscar Täckström, and Jakob Uszkoreit. Scaling autoregressive video models. arXiv preprint arXiv:1906.02634, 2019. Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 37333742, 2018. Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022. Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer, 2022. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. Shiwen Zhang. Tfcnet: Temporal fully connected networks for static unbiased temporal reasoning. arXiv preprint arXiv:2203.05928, 2022. Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Video Tokens for Pre-Training The next patch prediction for visual pre-training is equivalent to the next token prediction in large language models. However, most languages have clear sequential nature, therefore there is clear definition for the next word. This also makes the next word prediction task relatively harder, since the model requires learning to extrapolate the data. On the other hand, images and videos, especially over the spatial dimensions lack sequential nature. We follow the previous works (Chen et al., 2020a; Van Den Oord et al., 2016) to make the images and videos into 1D sequence by scanning the patches in raster order. While this ordering allows for example to learn to predict the bottom half of the image from the top part of the image, in many places, the tokens can be predicted by interpolating rather than extrapolating. On the time axis, yes, there is clear sequential nature, however, video frames compared to text tokens are more redundant, making the next frame prediction task much easier. Figure 10 shows average validation loss over 4096 token, in kinetics 400 dataset (Kay et al., 2017), on Toto-large model. This shows there is high loss of the first frame, but the subsequent frames have relatively lower loss compared to the first frame. This is because, even with reasonably lower sampling rate, frames following the first frame has some redundancy, and hinders the learning, since these tokens are relatively easy to predict. This also could be attributed by emergence of induction heads (Olsson et al., 2022). While we focused on learning from unfiltered internet scale video with minimal inductive bias, to learn efficiently from videos, need further research in this direction. Figure 10 Average Validation Loss Over Tokens: We show the average loss per token for kinetics validation set. It clearly shows the redundancy in videos, as the first frame has higher prediction loss, and rest of the frames on average has lower loss than the first frame. A.2 Prefix attention During fine-tuning, we experimented with causal and full attention. On ImageNet, our base model achieved full attn: 82.6% vs causal attn: 82.2%. Even though our models are not pre-trained with prefix attention, still able to utilize full attn at fine-tuning. This is an unrealized benefit of training with videos, (a middle token in say, 8th frame wont see the rest half of the 8th frame, but have seen all the tokens from 7th frame, which are similar because of video, hence approximating full attention at pre-training) A.3 Full fine-tuning We fine-tuned our models on ImageNet, and performance is close to SOTA, compared to linear probing (where we only use causal attention). But during the fine-tuning, we use full attention. DINO MoCo v3 82.8 83.2 BEiT MAE 83.6 83.2 Toto 82. Table 13 Full Fine Tuning Performance: Comparison of different methods performance on ImageNet-1K. 16 A.4 iGPT vs Toto on ImagenNet Table 7 shows ImageNet evaluation performance. However, iGPT (Chen et al., 2020a) models are evaluated only using linear probing. To have fair comparison, between iGPT and Toto, we reevaluated our models using linear probing. Both models have causal attention and are trained on auto-regressive objectives. On the same model sizes, about 1 billion parameters, our achieve 66.2% while the similar iGPT models ImageNet performance is 65.2%. This fair evaluation suggests the modifications made on Toto have clear benefits over iGPT. Method iGPT-L (Chen et al., 2020a) Toto-1b Arch GPT-2 LLaMA #θ 1386 1100 Top1 65.2 66. Table 14 ImageNet Linear Probing Results: Toto performs better than similar size iGPT models. A.5 µ-Parameterization To study the scaling behaviours of Toto using µ-Parameterization (Yang et al., 2022). First we train various models a1-a6 (in Table 15), with hidden sizes (64-1536) and number of layers (12-48), increasing linearly and we used VQGAN tokenizer (Esser et al., 2020). Then we tune the learning rate for these models, with fixed depth using µ-Parameterization (Yang et al., 2022). Figure 11 shows optimal learning rate of 27 for all the model widths. Once we find the optimal learning rate, we train a1-a6 models on the mixture of image and video data, as mentioned in Table 2. Model Params Dimension Heads Layers a1 a2 a4 a5 a6 14.8M 77.2M 215M 256 512 768 458M 1024 1.2B 1536 1.9B 1792 16 16 16 16 16 12 16 24 28 32 Table 15 Toto Varients: We scale Toto models by increasing hidden dimension and number of layers linearly while keeping number of heads constant following (Yang et al., 2022; Touvron et al., 2023). Figure 11 µ-Parameterization Learning Rate: We show that µ-Parameterization (Yang et al., 2022), we can train all width Toto models, with an single optimal learning rate of 27. A.6 n-gram distribution In this section, we compare the 2-gram and 3-gram distribution of dVAE (Ramesh et al., 2021), VQGAN (Esser et al., 2020) image tokeizers. We compute 2-gram and 3-gram distributions on the discrete tokens of 10000 ImageNet validation images. Figure 12 and Figure 13 show the distributions of these tokenizers respectively. On 2-gram distribution, dVAE (Ramesh et al., 2021) has more discrete combination of tokens compared to both VQGAN-1K and VQGAN-16k tokenizers. Figure 12 2-gram Distribution of Various Tokens: We compute the 2-gram distribution on 10000 images from the ImageNet validation set. Compared to VQGAN 1k and 16k vocabulary tokenizers, the dVAE tokenizer has larger set of token combinations. Figure 13 3-gram Distribution of Various Tokens: We compute the 3-gram distribution on 10000 images from the ImageNet validation set. All the tokenizers has similar almost flat distribution when it comes to 3-gram tokens. A.7 Attention probing variants on K400 We also evaluate our models and baselines on the Kinetics 400 dataset using variant of attention probing. In the main paper, we use attention probing, with only learning Wk, Wv matrices, and single learnable query vector. We also test with cross attention with MLP layers as the attention classifier, to give more capacity to the learnable head. Table 16 show the performance on the attention classifier with an additional MLP head. This helps to improve performance across over all models. Method Hiera (Ryali et al., 2023) Hiera (Ryali et al., 2023) VideoMAE (Wang et al., 2023a) VideoMAE (Wang et al., 2023a) Toto-base Toto-large Toto-1b Arch Hiera-L/14 Hiera-H/14 ViT-B/14 ViT-L/14 LLaMA LLaMA LLaMA Top1 74.2 75.2 65.4 74.8 61.2 65.8 74.8 Table 16 K400 Results: We evaluate our models using cross attention and MLP layer as the classification head. Overall using high-capacity head improves the performance across all models. 18 A.8 Generation samples long video generation: we can generate up to 64 frames, first raw: periodic motion, second raw: object permanence (light stand). prompting (pre-trained model): shows 3D rotation prompting (finetuned model): small 1000-step fine-tuning leads to promptable model for various vision tasks. A.9 Additional Layer-wise Probing Results We probe the multiple variants of our models at each layer for the best ImageNet performance. First, we test the models on linear probing, on both sizes of 128 and 256 resolution. Figure 14 presents the probing curves of the models trained with attention probing at 128 resolution. Across all models, the performance has similar behavior to the pre-trained models, with peak performance around the middle of the depth of the model. Figure 14 Training Loss Curves: We show the training loss curves for multiple variants of our models."
        }
    ],
    "affiliations": [
        "Meta FAIR",
        "UC Berkeley"
    ]
}