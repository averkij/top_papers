{
    "paper_title": "Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning",
    "authors": [
        "Ethan Mendes",
        "Jungsoo Park",
        "Alan Ritter"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization."
        },
        {
            "title": "Start",
            "content": "Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning Ethan Mendes 1 Jungsoo Park 1 Alan Ritter 1 6 2 0 2 2 ] . [ 1 5 0 4 2 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Improving the reasoning capabilities of large language models (LLMs) typically relies either on the models ability to sample correct solution to be reinforced or the existence of stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out-of-distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 1025% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2 to 4, and enable out-of-domain generalization. 1. Introduction Large reasoning models (LRMs) (Yang et al., 2025; Guo et al., 2025; OpenAI, 2025; DeepMind, 2025) have recently demonstrated an impressive ability to solve extremely challenging tasks, from competition mathematics problems to graduate student exam questions (Rein et al., 2024). These models acquire this reasoning ability primarily through stage of reinforcement learning with verifiable rewards (RLVR) (Shao et al., 2024), an online RL procedure where *Equal contribution Atlanta, Georgia. <emendes3@gatech.edu>. 1Georgia Institute of Technology, Ethan Mendes Correspondence to: Preprint. February 3, 2026. 1 models are rewarded based on the correctness of their final answer (ˆy = y) on large datasets of reasoning questions. However, during RLVR, problem contributes to learning only if the model can sample rollout that produces the correct answer. Paradoxically, on the most difficult problems, from which we would like them to learn the most, models often fail to extract any training signal as the rewards, advantages, and gradients are 0 (Nan et al., 2025; Chen et al., 2025a;b). To bypass this exploration bottleneck, models could instead learn from high-quality human expert reasoning traces, such as those seen in recent evaluation benchmarks (Glazer et al., 2024; Phan et al., 2025). However, directly fine-tuning post-trained model on inputoutput pairs = {(xi, si)}n i=1 to imitate expert solutions leads to severe reasoning performance deterioration (Yang et al., 2026) . This degradation appears to arise because expert human targets are fundamentally out-of-distribution (OOD) from the models own reasoning process learned during post-training. Specifically, expert solutions are typically didactic; they prioritize human readability by omitting granular intermediate steps that, while implied for human reader, are essential for the model to reason successfully. Additionally, human solutions lack explicit search dynamics, such as backtracking and self-correction, that characterize some long chain-of-thought (CoT) LRM generations (Marjanovic et al., 2025) after training with RLVR. Consequently, standard behavioral cloning forces the model to shortcut its internal reasoning process acquired during post-training, collapsing performance. In this paper, we propose Distribution Aligned Imitation Learning (DAIL), novel post-training method that enables student model to learn directly from high-quality expert solutions to complex problems (see Figure 1). Because collecting this data requires domain experts and could cost upwards of $1,000 per sample (Chiou, 2025), is practically small. In this regime, we aim to maximally leverage each high-cost instance for generalizable reasoning improvement. DAIL bridges the mentioned distribution gap between expert solutions and students own reasoning process by transforming highly OOD expert solutions into learnable training examples. Specifically, we propose mixed policy rollout process where the student generates reasoning traces in tandem with privileged student, frozen instance of the model conditioned with the ground-truth solution. This Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning Figure 1. Overview of DAIL. Starting from small set of expert solutions, we generate in-distribution transformed solutions for training via mixed policy decoding: the student model uses the expert solution as reference to produce detailed reasoning trace. This process mitigates didactic shortcuts, e.g., the expert solution skips the proof of why AOM is acute (in red), while the transformed solution explicitly details this reasoning (in green). Using this dataset of transformed solutions, DAIL applies contrastive learning on paired full vs. partial solutions to discourage imitating rationalization shortcuts in transformed solutions, e.g., the model ignoring the irrational part of derived result to force the generation of the correct answer (in orange). process expands the often non-comprehensive expert solution into detailed, in-distribution reasoning chain and fills in didactic shortcuts in the human-written solution, such as skipped steps and implied calculations. However, the cost of this distributional alignment is that generated traces may contain rationalization shortcuts or missing or insufficient justifications that lead to an intermediate result in the experts solution. As standard negative log-likelihood (NLL) loss enforces indiscriminate token-level imitation, it compels the student to internalize these harmful shortcuts. We find this naive mimicry ultimately degrades the students generalizable reasoning ability. Instead, we propose contrastive objective designed to prevent the student model from learning superficial reasoning. This objective penalizes mimicking negative reference conditioned solely on isolated intermediate results, enabling robust reasoning improvements. Building on findings (Zhou et al., 2023; Muennighoff et al., 2025) that fine-tuning on even limited number of highquality examples can yield substantial performance gains, we demonstrate that applying DAIL on dataset of fewer than 1000 expert human solutions leads to significant improvements across two settings. For Qwen2.5-Instruct, non-reasoning, post-trained model, we utilize dataset of competition mathematics problems that the model itself failed to solve, even with repeated sampling (i.e., for high k, pass@k = 0). To train Qwen3 (think), an LRM, we collect novel dataset of Olympiad proofs authored by current International Math Olympiad coach, which we publicly release as new resource.1 This latter setting highlights that DAIL enables learning on non-verifiable proof problems, 1We were given permission to use and release this dataset. which is not possible with standard RLVR without generative reward model. Across both settings, we find substantial improvements in pass@k on challenging mathematics reasoning benchmarks such as AIME 2024/2025 and Beyond AIME (Seed et al., 2025), including 15% gain for Qwen3 (think) on IMO-AnswerBench (Luong et al., 2025). To our knowledge, this is the first demonstration of improving the reasoning ability of long-CoT reasoning while learning directly from expert solutions. We also find that DAIL generalizes well to the out-of-domain GPQA-diamond benchmark (Rein et al., 2024), while also promoting efficient reasoning, as it matches or exceeds the performance of untrained models with 2 to 4 fewer tokens. 2. Distribution Aligned Imitation Learning In this section, we outline DAIL, an offline learning method that improves student model Mθ on dataset = {(xi, si)}n i=1 consisting of high-quality expert solutions to complex, potentially non-verifiable, problems x. Directly training on such data via standard behavior cloning (BC) fails due to fundamental distribution mismatch between expert human reasoning and the students reasoning. DAIL addresses this via two-stage process. First, we bridge this distribution gap by synthesizing expanded reasoning traces that are in-distribution for Mθ yet grounded in the expert solutions (2.1). This approach resolves didactic shortcuts (see the left side of Figure 1) in expert traces (e.g., solving with the quadratic formula, we obtain...) that omit the granular reasoning steps required by the student to reason. Second, we optimize contrastive objective (2.2) to mitigate the side-effects of this in-distribution expansion. 2 Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning 2.1. Generating In-Distribution Learnable Reasoning For any pair (x, s), we define the privileged student MPS() = Mθref(x, s). Here, θref denotes the frozen initial weights of the student model prior to optimization. The goal of this generation step is to use the privileged student to generate version of each expert solution that is in-distribution for the student Mθ(x), resulting in new synthetic dataset Dsyn = {(xi, ri)}n i=1. Direct Sampling. simple way to generate Dsyn is to sample solution trajectories directly from the privileged student, i.e., ri MPS(r<i) = Mθref (x, s, r<i). We find that this approach is surprisingly effective for non-reasoning instruction models like Qwen2.5-Instruct, if we carefully prompt the privileged student during this generation process to fill in reasoning gaps in s, and solve the problem as if it were solving it without guidance (see the full prompt in Table 9). Mixed Policy Rollouts. However, generating high-quality reasoning traces via direct sampling on LRMs like Qwen3 (think) is challenging due to the reflective nature of these models. Specifically, traces generated with access to the expert solution via direct sampling often contain references to this solution despite explicit prompting. Additionally, these traces exhibit fewer self-correction steps because the strong guidance of the expert solution allows the generation process to bypass the models natural verification mechanisms, resulting in less authentic reasoning. To mitigate these effects, we propose mixed policy rollout approach. Inspired by speculative decoding (Leviathan et al., 2023) and interactive imitation (Ross et al., 2011), our approach has the student and the privileged student work in tandem to generate reasoning traces, with the student deferring to the privileged student only when necessary. Specifically, the ith token of generation is formed by first sampling from the student, i.e., Mθ(x, r<i), and verifying it with the privileged student, i.e., confirming MPS(tr<i) τ , for fixed hyperparameter 0 τ 1. If this verification is successful, ri := t, and otherwise, we sample from the privileged student: ri MPS(r<i). Unlike work on improving traditional distillation (Xu et al., 2024), which jointly generates with the student and larger teacher model to ensure the teacher can provide valuable guidance to the student, mixed policy rollouts aim to ensure the resulting traces preserve the students natural reasoning flow and authentic artifacts, while utilizing the privileged student mainly to anchor generation to the expert solution. 2.2. Learning from Expert-Grounded Traces After generating the synthetic dataset Dsyn, the natural next step is to optimize the student model Mθ using these expanded reasoning traces. naive approach is i.e., minimize LNLL(θ) = to use BC via NLL loss, E(x,r)Dsyn[log Mθ(rx)]. However, NLL is ill-suited for training on Dsyn due to the presence of rationalization shortcuts in the expanded reasoning traces. Unlike didactic shortcuts, which are logical omissions made by human experts for brevity, rationalization shortcuts are deficient logical bridges to an intermediate result contained in the expert solution. Specifically, because the model generating the trace has access to the expert solution in its context, it often forces mathematical derivation to reach the known result rather than strictly deriving it (see the right side of Figure 1). Standard NLL indiscriminately forces the student to imitate every token in the trace with equal weight. This applies regardless of whether token represents valid logical progression or spurious shortcut. Consequently, models trained via NLL tend to memorize these flawed heuristics, leading to poor generalization when the expert solution is unavailable at inference time. Mitigating Learning Rationalization Shortcuts. To mitigate this behavior, we introduce contrastive objective designed to penalize the imitation of shortcuts, thereby improving the models generalization to more complex, outof-distribution tasks. This objective explicitly addresses the known limitations of BC on suboptimal datasets, where models tend to indiscriminately imitate both effective reasoning and spurious shortcuts (Kumar et al., 2022). Concretely, we construct negative reference model that preferentially produces rationalized shortcut-laden reasoning. Like the privileged student, the negative reference shares the students parameters but is conditioned differently. Specifically, this model is defined as MNR() = Mθref (x, s), where is partial solution to obtained by deleting intermediate reasoning components of and retaining only coarse-grained solution waypoints. We construct automatically using regular expression (see Appendix B.1). For the math domain, consists of list of key numerical and symbolic results from the expert solution. Conditioning on these waypoints shifts the models probability distribution to favor bypassing step-by-step logical progression and jumping directly between intermediate results, effectively characterizing the shortcut behavior we aim to penalize. Using this negative reference model, we can optimize the following contrastive loss function: L(θ) = E(x,r)Dsyn (cid:34) (cid:88) (cid:16) t=1 DKL (cid:0)Mθ(x, r<t) MPS(r<t)(cid:1) γDKL (cid:0)Mθ(x, r<t) MNR(r<t)(cid:1)(cid:17) (cid:35) 3 Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning where DKL is the token-level KL divergence, 0 γ 1 is hyperparameter. The objective effectively reduces the likelihood of tokens that have high probability under the negative reference relative to the full-information privileged student. While maximizing divergence via the negative term is theoretically unbounded, we find that because the student is initialized with the same weights as the privileged student and negative reference and strictly anchored by the positive distillation term, training remains stable (see Appendix B.5). Training Efficiency. The above objective provides an offpolicy, offline training framework that offers two primary efficiency advantages: (1) Asynchronous Data Generation: Unlike online RLVR approaches (Shao et al., 2024; Guo et al., 2025) that require interleaved generation and optimization, the in-distribution generation process is entirely decoupled from the training loop. This allows for massive parallelization of the dataset construction across distributed clusters before optimization. (2) Efficient Memory Management: Since Mθref and Mθ share identical base parameters at the start of training, we can represent the student updates using LoRA adapter (Hu et al., 2022). By selectively enabling the adapter, we can compute the forward passes for the privileged student and negative reference using the same frozen model θref. Thus, we only need to store single copy of the model weights in memory, significantly reducing hardware requirements compared to traditional distillation approaches. 3. Results 3.1. Experimental Setup Curated Training Datasets. As mentioned briefly in 1, we curate two datasets of reasoning problems for training. The first dataset e1-verifiable consists of 417 problems and solutions from historical American Invitational Mathematics Examinations (AIME) from the years 1985 to 2023. To measure how well DAIL can enable learning from difficult problems, we only retain problems that were not solved by the target Qwen2.5-7B-Instruct model in 32 attempts. Since official solutions to past exams are difficult to find online, we utilize community-generated solutions collected from the AoPS Wiki 2 by Yue et al. (2024). This process yields final set of 417 problems that constitute e1-verifiable. Training on this dataset allows for direct comparison between DAIL and RLVR methods. We also curate e1-proof, which consists of 669 nonverifiable Olympiad proof problems from the International 2https://artofproblemsolving.com/wiki/ind Mathematics Olympiad and other regional contests. Since these are open-ended proof problems, it is not possible to train on them using conventional correctness-based RLVR approaches. We sourced expert solutions to these problems from USA IMO Coach Evan Chens website3 with permission.4 Training Configuration. We train Qwen2.5-7BInstruct on e1-verifiable and Qwen3-8B (think) on e1-proof. As mentioned in 2.1, for Qwen3-8B (think), we use mixed policy rollouts. We investigate the effect of mixed policy rollouts and other factors during generation and learning in 3.4. For more details about training, please see Appendix B.5 Evaluation Datasets. To quantify how DAIL scales to problems more challenging than the training distribution, we evaluate performance on three mathematical reasoning benchmarks as well as an out-of-domain science reasoning benchmark to measure generalization: AIME 2024 / 2025: collection of 60 problems from the two most recent editions of the American Invitational Mathematics Examination (AIME). All answers are integers within the range [0, 999]. BeyondAIME (Seed et al., 2025): verifiable benchmark of 100 problems designed to mirror the difficulty of the final five (hardest) problems of standard AIME exams. Unlike standard AIME, the integer answers are unbounded. IMO-AnswerBench (Luong et al., 2025): curated dataset of 400 Olympiad-level problems across algebra, combinatorics, geometry, and number theory. These problems are manually rewritten by experts to be verifiable and resistant to memorization. GPQA-Diamond (Rein et al., 2024): collection of 198 graduate-level biology, physics, and chemistry multiple choice questions written by domain experts. Evaluation Metrics. We use the pass@k metric, which quantifies the probability of obtaining correct sample when sampling times. This metric can be computed via the following unbiased estimator (Chen, 2021): pass@k = ExDeval 1 (cid:34) (cid:35) (cid:1) (cid:0)nc (cid:1) (cid:0)n where is the number of samples generated by the model for each problem and is the number of correct 3https://web.evanchen.cc/problems.html 4Our extracted training dataset is available at https://hu ggingface.co/datasets/emendes3/e1-proof. 5Our code is available at https://github.com/ethan ex.php?title=AIME_Problems_and_Solutions m88/DAIL Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning Figure 2. Pass@k performance comparison of DAIL on Qwen2.5-7B-Instruct with e1-verifiable compared to RLVR methods on IMO-Answer, Beyond AIME, and AIME 2024 / 2025 benchmarks. DAIL exhibits consistent performance improvements over the base instruction model, while applying RLVR methods results in pass@k reductions due to the difficulty of training dataset problems. as hint; these rationalizations are subsequently used for SFT. Finally, to ensure improvements are not solely due to inference parameters, we evaluate performance across temperature range of τ {0.6, 0.8, 1.0, 1.2}, as variations in temperature are known to influence pass@k performance (Du et al., 2025; Wu et al., 2025). Unless specified, we otherwise default to temperature of 0.6. Correctness-based RLVR: We further benchmark against correctness-based RLVR methods trained on answerverifiable tasks. Specifically, we compare against Group Relative Policy Optimization (GRPO) (Shao et al., 2024) and NuRL (Chen et al., 2025a). The latter addresses the challenge of reward sparsity on hard problems by providing high-level hints derived from the ground truth when standard rollouts fail to yield correct answer. Finally, we also compare to realistic RLVR setup by training on the > 40K example DeepScaleR dataset (Luo et al., 2025) with GRPO. 3.2. DAIL Expands Math Reasoning Ability In this section, we focus on performance improvements of DAIL when applied to Qwen-2.5-7B-Instruct by training on e1-verifiable. This verifiable problem setting allows us to rigorously evaluate and contextualize DAILs improvements compared to baseline methods that may require reward. DAIL enables robust generalization where RLVR fails. In Figure 2, we compare DAIL against the Qwen2.5-7BInstruct baseline and standard RLVR approaches. DAIL consistently outperforms the base instruct model across all benchmarks, with the performance gap widening significantly at larger values. Moreover, DAIL enables consistent improvements on BeyondAIME and IMOAnswerBench, which contain more difficult problems than e1-verifiable. Since e1-verifiable problems were chosen such that they were not solved by the base instruct model in 32 atFigure 3. Baselines. Comparing pass@k performance of DAIL to temperature, STaR rationalization, and direct SFT on expert solutions baselines. To better visualize the relative performance between baselines, the results are plotted for 8. See Figure 15 for results at lower values. answers generated by the model for specific problem. We set = 128 and report results for {1, 2, 4, . . . , 128}. For difficult mathematics questions like those in BeyondAIME and IMO-AnswerBench, greedy accuracy (pass@1) is inherently low ( 5%). Thus, the practical utility of these models depends on their ability to uncover correct solutions via repeated sampling. Given the demonstrated tradeoff between these metrics (Yue et al., 2025; Tang et al., 2025), we seek significant gains in search potential (pass@k for 1) that do not come at prohibitive cost to greedy stability (pass@1). Baselines. We evaluate the performance of DAIL against comprehensive set of baseline methods: Naive Baselines: We first compare DAIL to standard imitation learning via Supervised Fine-Tuning (SFT) on expert human solutions. We also compare to STaR Rationalization (Zelikman et al., 2022), which induces the model to generate plausible solutions using the numerical answer 5 Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning Figure 4. Test-Time Efficiency. Performance on mathematics benchmarks under various token reasoning limits. To ensure gains are not simply due to non-response, models are given 2048 tokens beyond these limits to produce their final answer. Compared to Qwen3-8B (think), the model trained with DAIL on e1-proof yields improved performance across token budgets. tempts, if these problems were truly unsolvable, GRPO should not update model parameters as rewards, advantages, and policy-gradient loss would be zero across training problems. However, practically, the model may sample few correct rollouts on which it can take gradient steps. But, instead of generalizing, the model overfits to these rare, stochastic successes, leading to the observed degradation in general reasoning ability as evidenced by the downward shift in the pass@k curve. We confirm this behavior in our analysis in 3.4. Most notably, GRPO on the > 40K sample DeepScaleR dataset shows only marginal gains in pass@1 over the base instruct model on only subset of the evaluated benchmarks. Moreover, this model shows degradation in pass@k at larger values, which is consistent with findings in prior work (Yue et al., 2025). Therefore, simply scaling verifiable math data fails to enable Olympiad-level reasoning. While the hint-based NuRL method has shown promise in large-scale training regime, requiring 1k GPU hours for convergence (Nan et al., 2025), we find it ineffective for the targeted fine-tuning task that we study in this paper. In fact, NuRL + GRPO even consistently underperforms GRPO, despite training to convergence (see Appendix B.4), which we attribute to learned reliance on hints during RL due to the high frequency of difficult problems in the training data. In-distribution expert-grounded traces are required for improved reasoning. Figure 3 compares DAIL against direct SFT on expert traces and STaR rationalization, both of which degrade performance relative to the untrained model. The failure of direct SFT suggests that expert data alone is insufficient for improvement, as it is fundamentally OOD and underscores the need for in-distribution expansion to effectively update the policy. Conversely, the failure of STaR rationalization reveals that in-distribution self-generated expansion is insufficient when the problem difficulty exceeds the models baseline capabilities. Specifically, while rationalization is effective on simpler datasets like CommonTable 1. Out-of-domain performance of DAIL. Evaluated performance on GPQA-Diamond with the best result in each setting bolded. DAIL matches or outperforms the untrained model across most inference settings on this out-of-domain reasoning task. Qwen2.5 512 2048 4096 Qwen3 pass@1 Base DAIL pass@128 Base DAIL 34.1 35. 85.9 84.3 46.2 46.9 48.9 49.8 51.4 52.1 55.1 54.7 93.9 96. 95.5 96.9 93.4 96.5 93.4 96.0 senseQA (Talmor et al., 2019) and GSM8K (Cobbe et al., 2021), it struggles on the complex benchmarks we evaluate, as the model lacks the capacity to self-generate valid reasoning chains even when conditioned on the ground truth answer. This confirms that hard reasoning tasks require the specific combination of in-distribution generation grounded in external expert guidance that we propose through DAIL. 3.3. Inference Scaling and Generalization In this section, we measure the impact of DAIL with the non-verifiable e1-proof dataset on long-CoT LRMs. DAIL improves efficiency reasoning. Figure 4 compares the performance of Qwen3-8B (think) with DAIL when varying reasoning token budgets. Specifically, we employ budget forcing (Muennighoff et al., 2025) by appending an end think token (</think>) after the token budget has been reached and then allowing the model an additional 2048 tokens to process the reasoning and state final answer. We find that DAIL yields consistent improvements across benchmarks and token budgets, with the most substantial improvements under lower token budgets on more challenging benchmarks. Specifically across benchmarks, we find 6 Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning Figure 5. Contrastive loss consistently outperforms NLL. Comparison of the performance of Qwen2.5-7B-Instruct trained with DAILs contrastive objective and standard NLL. Contrastive loss outperforms NLL across generation settings and metrics. that DAIL can achieve roughly the same performance as the untrained model with 2 to 4 fewer tokens. This result indicates that applying DAIL induces models to reason more efficiently. We attribute such efficiency to the high information density and direct problem-solving paths inherent in expert traces, which teach the model to avoid the redundant or circular reasoning patterns that typically consume the budget in standard LRMs (Chen et al., 2024; Sui et al., 2025; Cuadron et al., 2025). Out-of-domain generalization. We also explore how applying DAIL on mathematics-centric dataset affects performance in other domains via an evaluation on GPQADiamond, presented in Table 1. Across most models and inference settings, DAIL maintains performance comparable to or better than the baseline. In fact, DAIL frequently outperforms the baseline, most notably achieving consistent pass@128 gains against Qwen3. These results suggest that the in-domain improvements yielded by DAIL do not come at the cost of catastrophic forgetting or domain overfitting. Rather, DAIL generally preserves or improves upon the models fundamental reasoning capabilities. 3.4. Ablations and Analysis Contrastive learning improves performance over NLL. Figure 5 compares the performance of Qwen2.5-7B-Instruct optimized with NLL loss versus our contrastive objective aggregated across the three mathematics benchmarks. As described in 2.2, unlike DAILs contrastive objective, NLL has no mechanism to prevent learning any shortcuts that may be present in the generated reasoning traces. This difference manifests in consistent gains in pass@1 and pass@128, both with direct sampling and mixed policy rollouts. However, the extent of improvement depends on the generation method used. Specifically, the contrastive objective yields significantly larger pass@1 gains over NLL when trained on directly sampled reasoning traces rather than mixed policy Figure 6. Baseline methods learn to imitate training data shortcuts. Evaluation on e1-verifiable. While NLL baselines learn to mimic shortcuts and RLVR baselines overfit to stochastic successes, both achieve high pass@k on seen data but degrade on unseen benchmarks (see Figures 2 and 15). DAILs lower training performance yet superior test generalization confirms that our contrastive loss effectively penalizes non-robust reasoning patterns. rollouts. This finding suggests that the contrastive objective enables learning from reasoning traces that may contain significant shortcuts and be more off-policy. These differences in improvements between generation methods mostlyevens out at pass@128, where the difference is only roughly 1%. For further loss ablations, please see Appendix B.6. DAIL learns effective generalizable reasoning strategies. To confirm that our method mitigates shortcut learning, we analyze performance on the e1-verifiable training dataset in Figure 6. This analysis reveals significant generalization gap in baseline methods. Specifically, applying NLL to either expert solutions or in-distribution traces achieves the highest training performance. However, this high training performance coincides with degraded performance on test benchmarks, confirming that these models effectively learn to imitate didactic and rationalization shortcuts in the expert traces. Similarly, RLVR methods exhibit high training accuracy despite having no exposure to expert solutions. This result further supports the conclusion mentioned in 3.2 that these methods overfit to isolated stochastic successes rather than learning robust reasoning. In contrast, DAIL yields lower training set performance but exhibits superior out-of-distribution generalization, validating that our contrastive objective successfully prevents the model from relying on non-robust reasoning patterns. Mixed policy rollouts enhance performance on reasoning models. Figure 7 compares the aggregate performance of models trained with DAIL, using either direct sampling or mixed policy rollouts to create the in-distribution training set (see 2.1). Unlike Qwen3-8B (thinking), the nonreasoning Qwen2.5-7B-Instruct (which is not RLVR-tuned) exhibits small performance drop relative to generation directly with the privileged student. This discrepancy likely 7 Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning problems (Zelikman et al., 2022; Yu et al., 2023; Mukherjee et al., 2023). Some methods iteratively self-train on filtered reasoning traces (Zelikman et al., 2022), while others expand coverage by generating new math questions with paired solutions (Yu et al., 2023). Another line rewrites solutions into richer explanation traces using strong (often closed-source) teachers, as in Orca (Mukherjee et al., 2023), and large math corpora further standardize or augment CoTstyle solutions at scale (Li et al., 2024; Toshniwal et al., 2024). While prior work has proposed self-distillation (Yang et al., 2024), which uses the same model to rewrite dataset solutions similar to our setting, it primarily targets simpler datasets. Unlike our approach, these methods do not address the challenge of learning from extremely difficult reasoning problems, where didactic and rationalization shortcuts are significant obstacles requiring specific interventions. Reinforcement Learning with Verifiable Rewards. To improve base LLMs without distillation from larger models, recent work leverages outcome-based reinforcement learning with verifiable rewards (RLVR) (Shao et al., 2024; Guo et al., 2025; Hu et al., 2025). RLVR encourages the model to place higher probability on solution trajectories that achieve correct outcomes (Wen et al., 2025), and performance on key metrics often improves steadily over training (Mai et al., 2025). However, these approaches do not directly leverage methods to enable models to learn from extremely difficult problems. Concurrent works (Nan et al., 2025; Chen et al., 2025a;b; Zhang et al., 2025; Qu et al., 2025) attempt to mitigate this zero-advantage issue, where on-policy training stalls on overly difficult problems, by injecting hints as prefixes during rollouts. Another concurrent work (Yang et al., 2026) addresses the credit assignment problem by using reference solutions to propose local interventions to correct specific intermediate errors in model-generated traces. While these works focus on repairing incorrect student rollouts, DAIL focuses on expanding compressed expert solutions. We argue that for extremely difficult problems where model may fail to generate even partially correct reasoning pathway, expanding expert solutions into constructive traces is necessary precursor to effective learning. 5. Conclusion We propose DAIL, framework designed to unlock the value of expert human solutions for problems where standard RLVR fails. By converting didactic expert solutions into in-distribution, constructive reasoning traces, DAIL enables improving reasoning capability while training on realistically small set of high-quality expert solutions. Through novel contrastive objective, we also suppress learning from reasoning shortcuts. Our results show that DAIL not only improves pass@k performance on challenging, non-verifiable mathematics problems but also yields Figure 7. Effect of mixed policy rollouts. Comparison of pass@128 improvement for Qwen2.5-7B-Instruct and Qwen38B (thinking). For the non-reasoning model (left), mixed policy rollouts slightly underperform compared to direct sampling. Conversely, reasoning models (right) benefit from mixed policy rollouts, suggesting that more in-distribution samples better support more deliberate reasoning processes. arises because the prompt provided to the privileged student is effectively sufficient at limiting shortcuts for standard instruction-tuned models, and our contrastive objective is quite effective at mitigating shortcuts as discussed above. However, for long-CoT reasoning models like Qwen3-8B (think), the reflective nature of the model causes direct privileged student generations to frequently reference the prompt or expert solution, harming performance when the traces are subsequently used for fine-tuning. In this setting, mixed policy generation provides crucial regularizing effect by limiting the presence of these artifacts. 4. Related Works Reasoning Distillation. Reasoning distillation improves smaller student model by training it to imitate stronger teacher models outputs (Sanh et al., 2019; Lin et al., 2020; Agarwal et al., 2024; Xu et al., 2024). Recent variants include on-policy distillation (Agarwal et al., 2024), where the teacher supervises the student on the students own trajectories, which has been used to transfer long CoT behaviors (Lu & Lab, 2025). Because token-level teacher supervision can be expensive for long trajectories, many practical approaches instead distill from realized teacher traces via standard likelihood-based training (SFT) (Guha et al., 2025; Bespoke-Labs, 2025; NovaSky-Team, 2025; Bakouch et al., 2025; Muennighoff et al., 2025; Guo et al., 2025; Zhao et al., 2026). key limitation of distillation is that it presumes access to stronger teacher, so it is not applicable at the frontier where such teacher does not exist. Synthetic Data Generation for Reasoning To scale reasoning supervision without additional human annotation, prior work generates synthetic reasoning data by bootstrapping model-produced rationales and/or synthesizing new 8 Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning models that reason more efficiently and generalize to outof-domain tasks. Ultimately, DAIL can help extend the capabilities of frontier models by enabling them to learn from high-value, non-verifiable data in domains where reward signals are sparse or undefined."
        },
        {
            "title": "Impact Statement",
            "content": "In this paper, we studied difficult problems in the mathematics and science reasoning domains. However, DAIL could also be applied to generally enable post-trained models, especially LRMs, to learn well from supervised-training datasets. Currently, there is no good way to fine-tune LRMs like Qwen3 on dataset input-output pairs other than by augmenting the dataset with offline-generated reasoning on mathematics problems, such that the model learns from D, while still retaining its reasoning ability.6 Needless to say, this is not cost-effective strategy and incurs unnecessary training costs. Instead, it would be useful to apply method like DAIL to expand the outputs in into in-distribution samples for the model before fine-tuning. We believe that this sort of application could be used for improved model safety, e.g., getting the model to reason about refusals, privacy policies, etc., in human-like manner. We hope that future work can further explore these potential applications and use cases."
        },
        {
            "title": "References",
            "content": "Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Garea, S. R., Geist, M., and Bachem, O. On-policy distillation of language models: Learning from self-generated mistakes. In The Twelfth International Conference on Learning Representations, 2024. Bakouch, E., von Werra, L., Tunstall, L., and contributors, H. F. Open-r1: fully open reproduction of deepseek-r1. https://huggingface.co/blog/open-r1, January 28 2025. Bespoke-Labs. Bespoke-stratos: The unreasonable effectiveness of reasoning distillation, 2025. URL https: //www.bespokelabs.ai/blog/bespoke-str atos-the-unreasonable-effectiveness-o f-reasoning-distillation. Chen, J. C.-Y., Peng, B. X., Choubey, P. K., Huang, K.-H., Zhang, J., Bansal, M., and Wu, C.-S. Nudging the boundaries of llm reasoning. arXiv preprint arXiv:2509.25666, 2025a. Chen, M. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 6Unsloth Qwen3 Guide Chen, P., Li, X., Li, Z., Chen, X., and Lin, T. Spectral policy optimization: Coloring your incorrect reasoning in grpo. In 2nd AI for Math Workshop@ ICML 2025, 2025b. Chen, X., Xu, J., Liang, T., He, Z., Pang, J., Yu, D., Song, L., Liu, Q., Zhou, M., Zhang, Z., et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. Chiou, L. Inside the secret meeting where mathematicians struggled to outsmart ai. Scientific American, June 2025. URL https://www.scientificamerican.c om/article/inside-the-secret-meeting -where-mathematicians-struggled-to-o utsmart-ai/. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Cuadron, A., Li, D., Ma, W., Wang, X., Wang, Y., Zhuang, S., Liu, S., Schroeder, L. G., Xia, T., Mao, H., et al. The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks. arXiv preprint arXiv:2502.08235, 2025. DeepMind, G. Gemini 3: State-of-the-art multimodal reasoning and agentic intelligence. Google Blog, 2025. URL https://blog.google/products/gemini/ gemini-3/. Released November 18, 2025. Accessed: February 3, 2026. Du, W., Yang, Y., and Welleck, S. Optimizing temperature for language models with multi-sample inference. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net /forum?id=rmWpE3FrHW. Glazer, E., Erdil, E., Besiroglu, T., Chicharro, D., Chen, E., Gunning, A., Olsson, C. F., Denain, J.-S., Ho, A., Santos, E. d. O., et al. Frontiermath: benchmark for evaluating advanced mathematical reasoning in ai. arXiv preprint arXiv:2411.04872, 2024. Guha, E., Marten, R., Keh, S., Raoof, N., Smyrnis, G., Bansal, H., Nezhurina, M., Mercat, J., Vu, T., Sprague, Z., et al. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 9 Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning Hu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H.-Y. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Kumar, A., Hong, J., Singh, A., and Levine, S. When should we prefer offline reinforcement learning over behavioral cloning? arXiv preprint arXiv:2204.05618, 2022. Leviathan, Y., Kalman, M., and Matias, Y. Fast inference In Interfrom transformers via speculative decoding. national Conference on Machine Learning, pp. 19274 19286. PMLR, 2023. Li, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S., Rasul, K., Yu, L., Jiang, A. Q., Shen, Z., et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. Lin, A., Wohlwend, J., Chen, H., and Lei, T. Autoregressive knowledge distillation through imitation learning. arXiv preprint arXiv:2009.07253, 2020. Lu, K. and Lab, T. M. On-policy distillation. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml .20251026. https://thinkingmachines.ai/blog/on-policydistillation. Luo, M., Tan, S., Wong, J., Shi, X., Tang, W., Roongta, M., Cai, C., Luo, J., Zhang, T., Li, E., Popa, R. A., and Stoica, I. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://pretty-radio-b75.n otion.site/DeepScaleR-Surpassing-O1-P review-with-a-1-5B-Model-by-Scaling -RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. Luong, M.-T., Hwang, D., Nguyen, H. H., Ghiasi, G., Chervonyi, Y., Seo, I., Kim, J., Bingham, G., Lee, J., Mishra, S., et al. Towards robust mathematical reasoning. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 3540635430, 2025. Mai, X., Xu, H., Li, Z.-Z., Wang, W., Hu, J., Zhang, Y., Zhang, W., et al. Agent rl scaling law: Agent rl with spontaneous code execution for mathematical problem solving. arXiv preprint arXiv:2505.07773, 2025. Marjanovic, S. V., Patel, A., Adlakha, V., Aghajohari, M., BehnamGhader, P., Bhatia, M., Khandelwal, A., Kraft, A., Krojer, B., L`u, X. H., et al. Deepseek-r1 thoughtology: Lets think about llm reasoning. arXiv preprint arXiv:2504.07128, 2025. Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand`es, E., and Hashimoto, T. B. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 20286 20332, 2025. Mukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi, H., and Awadallah, A. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023. Nan, G., Chen, S., Huang, J., Lu, M., Wang, D., Xie, C., Xiong, W., Zeng, X., Zhou, Q., Li, Y., et al. Ngrpo: Negative-enhanced group relative policy optimization. arXiv preprint arXiv:2509.18851, 2025. NovaSky-Team. Sky-t1: Train your own o1 preview model within $450. https://novasky-ai.github.io/posts/sky-t1, 2025. Accessed: 2025-01-09. NVIDIA. Nvidia h100 tensor core gpu architecture. Whitepaper, 2022. URL https://resources.nv idia.com/en-us-hopper-architecture/nv idia-h100-tensor-c. OpenAI. Introducing openai o3 and o4-mini, April 2025. URL https://openai.com/index/introdu cing-o3-and-o4-mini/. Phan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., Zhang, C. B. C., Shaaban, M., Ling, J., Shi, S., et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Qu, Y., Setlur, A., Smith, V., Salakhutdinov, R., and Kumar, A. How to explore to scale rl training of llms on hard problems? urlhttps://blog.ml.cmu.edu/2025/11/26/how-toexplore-to-scale-rl-training-of-llms-on-hard-problems, 2025. CMU MLD Blog. Reimers, N. and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Ross, S., Gordon, G., and Bagnell, D. reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627635. JMLR Workshop and Conference Proceedings, 2011. Sanh, V., Debut, L., Chaumond, J., and Wolf, T. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. 10 Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning Seed, B., Chen, J., Fan, T., Liu, X., Liu, L., Lin, Z., Wang, M., Wang, C., Wei, X., Xu, W., et al. Seed1. 5-thinking: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. Yang, Z., Pang, T., Feng, H., Wang, H., Chen, W., Zhu, M., and Liu, Q. Self-distillation bridges distribution arXiv preprint gap in language model fine-tuning. arXiv:2402.13669, 2024. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J. T., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Yue, A. S., Madaan, L., Moskovitz, T., Strouse, D., and Singh, A. K. Harp: challenging human-annotated math reasoning benchmark. arXiv preprint arXiv:2412.08819, 2024. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Zelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Zhang, X., Huang, Z., Li, Y., Ni, C., Chen, J., and Oymak, S. Bread: Branched rollouts from expert anchors bridge sft & rl for reasoning. arXiv preprint arXiv:2506.17211, 2025. Zhao, S., Xie, Z., Liu, M., Huang, J., Pang, G., Self-distilled reaurlhttps://siyanChen, F., soner: On-policy self-distillation. zhao.github.io/blog/2026/opsd/, 2026. and Grover, A. Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021, 2023. Sui, Y., Chuang, Y.-N., Wang, G., Zhang, J., Zhang, T., Yuan, J., Liu, H., Wen, A., Zhong, S., Zou, N., et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. Talmor, A., Herzig, J., Lourie, N., and Berant, J. Commonsenseqa: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41494158, 2019. Tang, Y., Zheng, K., Synnaeve, G., and Munos, R. Optimizing language models for inference time objectives using reinforcement learning. In Forty-second International Conference on Machine Learning, 2025. URL https: //openreview.net/forum?id=ZVWJO5YTz4. Toshniwal, S., Moshkov, I., Narenthiran, S., Gitman, D., Jia, F., and Gitman, I. Openmathinstruct-1: 1.8 million math instruction tuning dataset. Advances in Neural Information Processing Systems, 37:3473734774, 2024. Wen, X., Liu, Z., Zheng, S., Ye, S., Wu, Z., Wang, Y., Xu, Z., Liang, X., Li, J., Miao, Z., et al. Reinforcement learning with verifiable rewards implicitly incentivizes correct reasoning in base llms. arXiv preprint arXiv:2506.14245, 2025. Wu, Y., Mirhoseini, A., and Tambe, T. On the role of temperature sampling in test-time scaling. arXiv preprint arXiv:2510.02611, 2025. Xu, W., Han, R., Wang, Z., Le, L. T., Madeka, D., Li, L., Wang, W. Y., Agarwal, R., Lee, C.-Y., and Pfister, T. Speculative knowledge distillation: Bridging the teacherstudent gap through interleaved sampling. arXiv preprint arXiv:2410.11325, 2024. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yang, M. Y., Bai, H., Wu, I., Yang, G., Setlur, A., and Int: Self-proposed interventions enable arXiv preprint Kumar, A. credit assignment in llm reasoning. arXiv:2601.14209, 2026. 11 Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning A. Dataset Curation In this section, we explain the curation process of e1-verifiable and e1-proof in more detail, expanding upon 3.1. Curating e1-verifiable. We filter the full set of 903 AIME problems from the years 1985 - 2023.7 by retaining problems that were not able to be solved by Qwen2.5-7B-Instruct in 32 attempts. This process yields 417 problems. Since the AoPS forum usually contains multiple solutions for each AIME problem, we concatenate these solutions together to form s, which is provided in context to the privileged student model. We include any figures included in these solutions that are drawn in the Asymptote programming language. Curating e1-proof. We collected total of 683 Olympiad problems and expert solutions for e1-proof. Like with e1-verifiable, we include any figures with the expert proofs. Deduplication. Since IMO-AnswerBench consists of rewritten versions of existing olympiad problems specifically designed to mitigate data memorization effects, and given that the benchmark was incorporated into our evaluation suite after the training of Qwen2.5-7B-Instruct with DAIL, we did not initially perform deduplication on the e1-verifiable split. Post-hoc analysis revealed that 15 problems in IMO-AnswerBench appear to share underlying sources with instances in e1-verifiable. However, evaluating the model while excluding these overlapping problems yields no significant change in performance (see Figure 14), suggesting that the results remain robust to potential leakage. For the e1-proof split, we implemented proactive deduplication pipeline using SentenceBERT (Reimers & Gurevych, 2019) embedding similarity. Applying similarity threshold of 0.7, we identified and removed 15 flagged problems, resulting in final evaluation set of 669 problems. B. Generation, Training, and Evaluation Details B.1. Data Generation Prompts. The prompts for the student and the privileged student are found in Figures 8 and 9, respectively. Note for e1-verifiable, the solution is concatenated list of human solutions, while for e1-proof, it is single expert proof. Analysis of generated data. Figure 12 plots the distribution of the number of tokens in raw expert solutions compared to the expanded solutions used to Qwen2.5-7B-Instruct using e1-verifiable. There is clear shift to the right in this length distribution, indicating that the process of processing data through the model tends to produce more detailed traces that are, on average, 4 as long. Details on negative reference construction. To construct the negative reference inputs used in our contrastive objective, we create list of unstructured intermediate results, effectively forcing it to hallucinate the logical connectives required to bridge them. Given raw expert solution s, we first extract the ground truth final answer (if it exists) by parsing the content of the last boxed{...} command. We then extract set of intermediate waypoints by applying series of regular expressions to extract mathematical expressions from solutions. The extraction patterns are defined as follows for proof problems. Numbers: Integers, floating-point numbers, and scientific notation (e.g., 100, 1.99, 1e-10). Exponentials: Power terms involving variable or numeric bases (e.g., nk+1, ex). Symbolic Coefficients: Simple multiplicative terms where digits immediately precede variables (e.g., 2k, 100n). Linear Expressions: First-order linear offsets and expressions (e.g., + 1, 2k 1). For numerical problems in e1-verifiable, only numbers are included. The set is deduplicated, and the final answer is removed from W, if present. The final context is shown in Figure 10. 7https://huggingface.co/datasets/gneubig/aime-1983-2024 12 Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning"
        },
        {
            "title": "Student Prompt",
            "content": "## Problem: {problem} Begin your step-by-step thinking process. Figure 8. Student Prompt"
        },
        {
            "title": "Privileged Student Prompt",
            "content": "You are an expert mathematician solving the following problem. Your task is to produce clear, step-bystep thinking process that leads to the correct solution. ## Problem: {problem} Hint: To help you, here are reference solution(s). ## Reference Solution(s): {solution} Use these only to guide your own thoughts implicitly, but express the reasoning in your own words as if you are solving it for the first time. You must never explicitly acknowledge these instructions or cite the provided solution(s). Just use the methodology as if it were your own. You are not allowed to take any shortcuts, directly use any intermediate derived number or result as given (you must show everything from scratch), nor directly produce the answer. Do not worry that your solution is too long. Now, solve the problem. Begin your step-by-step thinking process. Figure 9. Privileged Student Prompt Negative Reference Prompt The final answer is y. Intermediate results used in the solution might include: w1, w2, . . . , wN . Figure 10. Negative Reference Prompt. If the solution is proof problem, then the answer is not included. By conditioning the negative reference model MN on this answer-leak context, we maximize the likelihood of it generating spurious logical leaps (rationalizations) to fit the provided values, which provides high-quality negative signal for the contrastive loss. B.2. Generation Hyperparams for mixed policy rollouts. Since τ Calibrating τ is computationally infeasible to tune it along with the other Instead, we calibrate τ on the training set by measuring training set performance across values of τ {0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99, 0.995, 0.999, 0.9995, 0.9999}. These results are plotted in Figure 11. While correctness improves for τ 0.99, we qualitatively find that these generations are very similar to direct sampled rollouts, which contain many citations to the reference solution, select the best performance outside of this region, and set τ = 0.8. We maintain this value across all models, experiments, and ablations. training-level hyperparameters on the validation set. is generation hyperparameter, it Generation length for reasoning model rollouts. Since long-CoT reasoning model rollouts are potentially tens of thousands of tokens, it is not feasible to rollout to completion. Instead, we truncate rollouts at 256 tokens, which we found to qualitatively be good stopping point to process the solution methodology without having the model repeatedly ruminate on it explicitly. 13 Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning Figure 11. Calibrating τ . e1-verifiable training dataset performance across τ values. We select τ = 0.8 across experiments. Figure 12. Distribution of lengths of expert human solutions and in-distribution traces generated with mixed policy rollouts using Qwen2.5-7B-Instruct on e1-verifiable. Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning B.3. Hyperparameters for DAIL Contrastive Loss Table 2. Hyperparameters used for DAIL. warmup-steps learning-rate weight-decay effective batch-size lora-r lora-alpha 5 2e4 0.01 32 32 For all DAIL runs, we use the default hyperparameters in Table 2. We also tuned γ, the hyperparameter controlling the effect of the negative reference on the contrastive objective by sweeping across values γ {0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0} and measuring performance on small 100 sample validation set created by selecting the next most challenging AIME problems based on Qwen2.5-7B-Instruct correctness after those in e1-verifiable. We run validation in the mixed policy rollout setting on Qwen2.5-7B-Instruct using pass@128 as the selection metric, but compare pass@64, pass@32, etc., if there is tie. Through this process, we find the optimal value for γ is 0.1. We use this same parameter value across settings and models. Since there are many settings and ablations run with Qwen2.5-7B-Instruct, we do not tune epochs across models, but instead use constant 5 epochs across runs. However, from prior reasoning distillation work showing that epochs may yield large changes in performance, we tune epochs for the reasoning model experiments using the same validation set and selection metrics. We find 3 epochs to be optimal in this case. B.4. Hyperparameters for GRPO and NuRL For all GRPO and NuRL runs, we use the default hyperparameters in Table 3. For training on e1-verifiable, we train until convergence, i.e., for 15 epochs with GRPO and 15 additional epochs on this checkpoint with NuRL following the papers implementation details (Chen et al., 2025a). For GRPO on DeepScaleR, we train for single epoch due to the larger size of the dataset. Table 3. Hyperparameters used for RL training. learning-rate 106 batch-size rollout rollout temp / top-p 32 0.6 / 0.95 B.5. Training Compute We run training on two separate compute clusters. DAIL experiments were run on SLURM cluster consisting of A40 nodes. All training runs used 4 A40 GPUs. RL training runs were run on separate supercomputing SLURM cluster with H100 nodes, with each run using 4 H100 GPUs. We note that DAIL delivers orders-of-magnitude improvements in training efficiency due to its offline formulation (2.2). In contrast to the 277.6 and 8.7 H100 hours required per epoch for GRPO training on e1-verifiable and DeepScaleR, respectively, DAIL incurs marginal cost of only 0.1 H100 hours per epoch once the synthetic dataset Dsyn is generated. Note this estimate uses conservative 3 speed-up conversion factor between A40 and H100 hours based on benchmarked performance differences between the Ampere and Hopper architectures (NVIDIA, 2022). Finally, the training curve for DAIL is presented in Figure 13. B.6. Other Evaluation Details and Ablations Naive baselines. Figure 15 presents the version of Figure 3, i.e., the aggregated performance of training Qwen2.5-7BInstruct on e1-verifiable across benchmarks, for all values of k. Effect of contrastive objective. Table 4 presents the performance of various ablations of DAILs contrastive objective on BeyondAIME when using mixed policy rollouts. We find that the contrastive objective outperforms optimizing only (cid:0)Mθ(x, r<t) MPS(r<t)(cid:1). Additionally, minimizing the divergence between the student and the negative reference, DKL (cid:0)Mθ(x, r<t) MNR(r<t)(cid:1) degrades performance compared to the untrained model at = 128. i.e., optimizing DKL 15 Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning These results match the motivation for this objective and negative reference outlined in 2.2. Figure 13. Loss curve for our contrastive loss. We find training is stable, with spikes in loss only occurring at epoch boundaries. Figure 14. IMO-AnswerBench results with problems with the same source de-duplicated. There is little difference between these results and those in Figure 2. Table 4. Ablation of contrastive objective on BeyondAIME. Results are in the mixed policy rollouts setting. Contrastive loss outperforms only minimizing KL between the student and privileged student. Loss pass@128 (cid:0)Mθ(x, r<t) MPS(r<t)(cid:1) Contrastive (Ours) DKL Qwen2.5-7B-Instruct DKL (cid:0)Mθ(x, r<t) MNR(r<t)(cid:1) 46.0 44.0 41.0 38.1 Figure 15. version of Figure 3 for all values. These results include the aggregated performance of Qwen2.57B-Instruct when training on e1-verifiable. Figure 16. Test-time efficiency on smaller, less capable models. We find that for these less capable models, improvements are more mixed. This finding indicates we need baseline level of reasoning performance to apply DAIL. Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning Competent models are necessary for consistent improvement with DAIL. Figure 16 presents test-time efficiency analysis with weaker models that are weaker than Qwen3-8B (think), namely Qwen3-4B (think), Deepseek-R1-Distill-7B (think). For these weaker models, we find that DAIL has mixed results when training on e1-proof. This is likely due to two reasons (1) these models, especially Deepseek-R1-Distill-7B, are not strong instruction following models and have only seen reasoning data in the same format. Therefore, generation using the prompts we have defined is difficult. (2) e1-proof is very challenging dataset, and these models cannot generate even partially valid traces consistently, i.e., their traces are laden with shortcuts, which cannot be effectively mitigated. Tuning γ for these particular models, rather than using the 0.1 value tuned on Qwen3-8B (think) likely will help performance."
        }
    ],
    "affiliations": [
        "Georgia Institute of Technology"
    ]
}