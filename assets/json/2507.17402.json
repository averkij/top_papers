{
    "paper_title": "HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning",
    "authors": [
        "Li Jun",
        "Wang Jinpeng",
        "Tan Chaolei",
        "Lian Niu",
        "Chen Long",
        "Zhang Min",
        "Wang Yaowei",
        "Xia Shu-Tao",
        "Chen Bin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of matching untrimmed videos with text queries describing only partial content. Existing methods suffer from geometric distortion in Euclidean space that sometimes misrepresents the intrinsic hierarchical structure of videos and overlooks certain hierarchical semantics, ultimately leading to suboptimal temporal modeling. To address this issue, we propose the first hyperbolic modeling framework for PRVR, namely HLFormer, which leverages hyperbolic space learning to compensate for the suboptimal hierarchical modeling capabilities of Euclidean space. Specifically, HLFormer integrates the Lorentz Attention Block and Euclidean Attention Block to encode video embeddings in hybrid spaces, using the Mean-Guided Adaptive Interaction Module to dynamically fuse features. Additionally, we introduce a Partial Order Preservation Loss to enforce \"text < video\" hierarchy through Lorentzian cone constraints. This approach further enhances cross-modal matching by reinforcing partial relevance between video content and text queries. Extensive experiments show that HLFormer outperforms state-of-the-art methods. Code is released at https://github.com/lijun2005/ICCV25-HLFormer."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 2 0 4 7 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning",
            "content": "Jun Li1*, Jinpeng Wang2, Chaolei Tan4, Niu Lian1, Long Chen4, Min Zhang1, Yaowei Wang3, Shu-Tao Xia2,3, Bin Chen1 1Harbin Institute of Technology, Shenzhen 2Tsinghua Shenzhen International Graduate School, Tsinghua University 3Research Center of Artificial Intelligence, Peng Cheng Laboratory 4The Hong Kong University of Science and Technology 220110924@stu.hit.edu.cn (cid:66) wjp20@mails.tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of matching untrimmed videos with text queries describing only partial content. Existing methods suffer from geometric distortion in Euclidean space that sometimes misrepresents the intrinsic hierarchical structure of videos and overlooks certain hierarchical semantics, ultimately leading to suboptimal temporal modeling. To address this issue, we propose the first hyperbolic modeling framework for PRVR, namely HLFormer, which leverages hyperbolic space learning to compensate for the suboptimal hierarchical modeling capabilities of Euclidean space. Specifically, HLFormer integrates the Lorentz Attention Block and Euclidean Attention Block to encode video embeddings in hybrid spaces, using the Mean-Guided Adaptive Interaction Module to dynamically fuse features. Additionally, we introduce Partial Order Preservation Loss to enforce text video hierarchy through Lorentzian cone constraints. This approach further enhances cross-modal matching by reinforcing partial relevance between video content and text queries. Extensive experiments show that HLFormer outperforms state-of-the-art methods. Code is released at https://github.com/lijun2005/ICCV25-HLFormer. 1. Introduction Text-to-video retrieval (T2VR) [5, 1113, 15, 18, 35, 38, 44] is fundamental module in many search applications and popular topic in multi-modal learning. While most T2VR models are developed for short clips or pre-trimmed video segments, they may face challenges where user queries describe only partial content in the video. This practical issue in real-world usage promotes more challenging setting of *These authors contributed equally to this work. Corresponding author. Figure 1. (a) Modeling the semantic hierarchy in untrimmed videos helps Partially Relevant Video Retrieval (PRVR). (b) Euclidean space is less effective in modeling semantic hierarchy due to the flat geometry. Data points with distant hierarchical relation may be close. (c) Hyperbolic space allows larger cardinals when approaching the edge, which is preferable to preserve the hierarchy. partially relevant video retrieval (PRVR) [14], which aims to match each text query with the best untrimmed video. Due to unlabeled moment timestamps, PRVR requires solid abilities on (i) identifying key moments in videos for extracting informative features and (ii) learning robust crossmodal representations to match text queries and videos precisely. Prior arts have developed preliminary solutions on both aspects, while challenges remain. For (i), MS-SL [14] exhaustively enumerated consecutive frame combinations through multi-scale sliding windows, which inevitably engaged redundancy, noise, and high computational complexity in extracting moment features. GMMFormer [60, 61] improved efficiency by leveraging Gaussian neighborhood priors to traverse each timestamp and discover potential key moments. However, it may still be hard to distinguish adjacent or semantically similar candidate moments. Though DL-DKD [16] neatly benefited from the pretrained CLIP [50] to enhance text-frame alignment, the temporal generalizability is bounded by the text-image teacher model. For (ii), most existing solutions inherited similar ideas from classic T2VR, e.g., ranking and contrastive learning, at holistic level, but important characteristics of PRVR, e.g., partial relevance and semantic entailment, are still under-explored. In this paper, we take hierarchical perspective to review the task, in the belief that videos naturally exhibit semantic hierarchy. As illustrated in Fig. 1(a), an untrimmed video can be regarded as progression from frames to informative segments (e.g., Dunk), extended moments, and ultimately, the whole. Leveraging this intrinsic property is expected to benefit long video understanding. In particular for PRVR, the hierarchical prior provides positive guidance to arrange the moment features. Meanwhile, the supervisory signals from query-video matching can activate moment extraction more precisely through implicit bottom-up modeling. Exploring hierarchical features is never trivial. Unfortunately, existing PRVR approaches relying on Euclidean space are less effective in modeling the desired patterns in the flat geometry. We present Fig. 1(b) to exemplify this: two embeddings with distant hierarchical relation may be spatially close to each other, as marked by the red arrows. Biased representation will increase the difficulty in disentangling informative moments from background, which limits the robustness in cross-modal matching considering partial relevance. Inspired by the emerging success of hyperbolic learning [10, 17, 30, 32, 46], which takes advantage of exponentially expanding metric in non-Euclidean space to better capture hierarchical structure (Fig. 1(c)), we introduce HLFormer, sincere exploration of hyperbolic learning to enhance PRVR. On temporal modeling, we carefully design dual-branch strategy to capture informative moment features comprehensively. Specifically, for the hyperbolic branch, we develop Lorentz Attention Block (LAB) with the hyperbolic selfattention mechanism. With the implicit hierarchical prior through end-to-end matching optimization, LAB learns to activate informative moment features relevant to queries and distinguish them from noisy background in the hyperbolic space, compensating for the limitations of Euclidean attention in capturing hierarchical semantics. We integrate dual-branch moment features with Mean-Guided Adaptive Interaction Module (MAIM), which is lightweight but effective. On cross-modal matching, drawing on the intrinsic text video hierarchy in PRVR where textual queries are subordinate to their paired videos, we introduce Partial Order Preservation (POP) loss that geometrically confines text embeddings within hyperbolic cone anchored by corresponding video representations in an auxiliary Lorentzian manifold. This hierarchical metric alignment ensures semantic consistency between localized text semantics and their parent video structure while preserving partial relevance. Empirical evaluations on three benchmark datasets: ActivityNet Captions [29], Charades-STA [23], and TVR [31] establish HLFormers state-of-the-art performance. Ablation studies confirm the necessity of hyperbolic geometry for hierarchical representation and the critical role of explicitly relational constraints in Partial Order Preservation Loss. Meanwhile, visual evidences further reveal that hyperbolic learning can enhance discriminative representation while maintaining video-text entailment, sharpening moment distinction and improving query alignment. The primary contributions can be summarized as follows: We propose to enhance PRVR with hyperbolic learning, including Lorentz attention block with hierarchical priors to enhance the moment feature extraction, which collaborates with Euclidean attention and hybrid-space fusion. We design partial order preservation loss that geometrically enforces the text video hierarchy through hyperbolic cone constraints, strengthening partial relevance. Extensive experiments on three benchmarks validate HLFormers superiority, with analyses confirming the efficacy of hyperbolic modeling and geometric constraints. 2. Related Works 2.1. Partially Relevant Video Retrieval With the growth of video content [19, 36, 62], video retrieval has become key research area. Given text query, Text-toVideo Retrieval (T2VR) [5, 11, 15, 18, 35, 37, 38, 44, 58, 59] focuses on retrieving fully relevant videos from pre-trimmed short clips. Video Corpus Moment Retrieval (VCMR) [7, 31, 52, 53] aims to localize specific moments within videos from large corpus. Partially Relevant Video Retrieval (PRVR) [8, 9, 14, 16, 27, 60, 61, 64], more recent task introduced by Dong et al. [14], aims to retrieve partially relevant videos from large, untrimmed long video collections. Unlike T2VR, PRVR must address the challenge of partial relevance, where the query pertains to only specific moment of the video. Though the first stage of VCMR is similar to PRVR, VCMR requires moment-level annotations, limiting scalability. Existing methods enhance PRVR retrieval from various perspectives. MS-SL [14] defines the PRVR task as Multiinstance Learning, providing strong baseline with explicit redundant clip embeddings. GMMFormer [60, 61] and PEAN [27] propose implicit clip modeling to improve efficiency. DL-DKD [16] achieves great results through dynamic distillation of CLIP [50]. BGM-Net [64] exploits an instance-level matching scheme for pairing queries and videos. However, these methods predominantly rely on Euclidean space, which sometimes distort the hierarchical structures in untrimmed long videos. Consequently, they fail to fully exploit video hierarchy priors. To overcome this issue, we propose HLFormer to enhances PRVR by implicitly capturing hierarchical structures through hyperbolic learning. 2.2. Hyperbolic Learning Hyperbolic learning has attracted significant attention for its effectiveness in modeling hierarchical structures in realworld datasets. Early studies in computer vision tasks explored hyperbolic image embeddings from image-label pairs [28, 46], while subsequent progress extended hyperbolic optimization to multi-modal learning. MERU [10] and HyCoCLIP [48] notably surpassed Euclidean counterparts like CLIP [50] via hyperbolic space adaptation. Applications span semantic segmentation [1, 4], recognition tasks (skin [65], action [40]), meta-learning [17], and detection frameworks (violence [32, 49], anomalies [34]). Recent advances in fully hyperbolic neural networks [6, 22, 25, 33, 56] further underscore their potential. Motivated by them, we present the first study to explore the potential of hyperbolic learning for PRVR. Unlike other methods such as DSRL [32] and HOVER [51], our approach utilizes hyperbolic space to compensate for the limitations of Euclidean space in capturing the hierarchical structure of untrimmed long videos. Furthermore, we introduce the Partial Order Preservation Loss to explicitly capture the partial relevance between video and text in hyperbolic space, improving retrieval performance. 3. Method 3.1. Preliminaries Hyperbolic Space Hyperbolic spaces are Riemannian manifolds with constant negative curvature K, contrasting with the zero-curvature (flat) geometry of Euclidean spaces. Among several isometrically equivalent hyperbolic models, we adopt the Lorentz model [47] for its numerical stability and computational efficiency, with set to -1 by default. Formally, an n-dimensional Lorentz Lorentz Model model is the Riemannian manifold Ln = (Ln, gx). gx = diag(1, 1, , 1) is the Riemannian metric tensor. Each point in Ln has the form = [x0, xs] Rn+1, x0 = (cid:112)xs2 + 1 R. Following Chen et al. [6], we denote x0 as time axis and xs as spatial axes. Ln is given by: Ln := {x Rn+1 x, xL = 1, x0 > 0}, (1) and the Lorentzian inner product given by: x, yL := x0y0 + ys. (2) Here Ln is the upper sheet of hyperboloid in (n+1) dimensional Minkowski space with the origin = (1, 0, , 0). The tangent space at Ln is EuTangent Space clidean space that is orthogonal to it, defined as: TxLn := {y Rn+1 y, xL = 0}. (3) Where TxLn is Euclidean subspace of Rn+1. In particular, the tangent space at the origin is denoted as ToLn. Logarithmic and Exponential Maps The mutual mapping between the hyperbolic space Ln and the Euclidean subspace TxLn can be realized by logarithmic and exponential maps. The exponential map expx(z) can map any tangent vector TxLn to Ln, written as: expx(z) = cosh(zL)x + sinh(zL) zL , (4) where zL = (cid:112)z, zL and the logarithmic map logx(y) plays an opposite role to map Ln to TxLn as follows: logx(y) = arcosh(x, yL) (cid:112)(x, yL)2 1 (y + (x, yL)x). (5) Lorentzian centroid The weighted centroid with respect to the squared Lorentzian distance, which solves minµLn (cid:80)m L(xi, µ), with xi Ln and νi 0, (cid:80)m i=1 νid2 i=1 νi > 0, is denoted as: (cid:80)m (cid:80)m µ = i=1 νixi i=1 νixiL . (6) 3.2. Problem Formulation and Overview Partially Relevant Video Retrieval (PRVR) aims to retrieve videos containing moment semantically relevant to given text query, from large corpus of untrimmed videos. In the PRVR database, each video has multiple moments and is associated with multiple text descriptions, with each text description corresponding to specific moment of the related video. Critically, the temporal boundaries of these moments (i.e., start and end time points) are not annotated. In this paper, we introduce HLFormer, the first hyperbolic modeling approach designed for PRVR. The proposed framework encompasses three key components: text query representation encoding, video representation encoding, and similarity computation, as illustrated in Fig. 2 (a). Text Representation Given text query of Nq words, we first use pre-trained RoBERTa [39] model to extract word-level features, which are then projected into lowerdimensional space via fully connected (FC) layer. standard Transformer [57] layer is applied to obtain sequence of d-dimensional contextualized feature vectors, = {qi}Nq i=1 RNqd. Finally, we utilize simple attention mechanism to get the sentence embedding Rd: = Nq (cid:88) i=1 aq qi, aq = softmax(wQ), (7) where R1d is trainable vector, and aq R1Nq represents the attention vector. Video Representation Given an untrimmed video, we first extract embedding features using pre-trained 2D or 3D CNN. Then we utilize the gaze branch and glance branch Figure 2. Overview of HLFormer. (a) The sentence embedding is obtained via the query branch, while the gaze and glance branches encode the video, producing frame-level embedding Vf and clip-level embedding Vc and forming the video representation Vv. learns query diversity through Ldiv and computes similarity scores Sf and Sc, while preserving partial order relations with Vv using Lpop. (b) HLFormer block combines parallel Lorentz and Euclidean attention blocks for multi-space encoding, with Mean Guided Adaptive Interaction Module for dynamic aggregation. (c) Partial Order Preservation Loss ensures the text query embedding lies within the cone defined by the video embedding v. The loss is zero if is inside the cone. to capture frame-level and clip-level multi-granularity video representations, respectively. In the gaze branch, we densely sample Mf frames, denoted as RMf D, where is the frame feature dimension. The sampled frames are processed through fully connected (FC) layer to reduce the dimensionality to d, followed by the HLFormer block to obtain frame embeddings Vf = {fi}Mf i=1 RMf d, capturing semantically rich frame-level information for fine-grained relevance assessment to the query. The glance branch down-samples the input along the temporal dimension to aggregate frames into clips. Following MS-SL [14], fixed number Mc of clips is sparsely sampled by mean pooling over consecutive frames. fully connected layer is applied to the pooled clip features, followed by the HLFormer block, generating clip embeddings Vc = {ci}Mc i=1 RMcd. These embeddings capture adaptive clip-level information, enabling the model to perceive relevant moments at coarser granularity. Similarity Computation To compute the similarity between text-video pair (T , V), we first measure the abovementioned embeddings q, Vf and Vc. Then, we employ cosine similarity along with max operation to calculate the frame-level and clip-level similarity scores: Sf (T , V) = max{cos(q, f1), ..., cos(q, fMf )}, Sc(T , V) = max{cos(q, c1), ..., cos(q, cMc )}. (8) Next, we compute the overall text-video pair similarity: S(T , V) = αf Sf (T , V) + αcSc(T , V), (9) where αf , αc [0, 1] are hyper-parameters satisfying αf + αc = 1. Finally, we retrieve and rank partially relevant videos based on the computed similarity scores. 3.3. HLFormer Block The HLFormer Block constitutes the core of our method. As shown in Fig. 2 (b), it comprises three key modules: (i) Euclidean Attention Block, capturing fine-grained visual features in Euclidean space; (ii) Lorentz Attention Block, projecting video embeddings into hyperbolic Lorentz space for capturing the hierarchical structures of video; (iii) MeanGuided Adaptive Interaction Module, dynamically fusing hybrid-space features. We describe the details below. Euclidean Attention Block Given feature embeddings RM d, where is the feature dimension, the Euclidean Attention Block utilizes Euclidean Gaussian Attention [61] to capture multi-scale visual features, expressed as: GA(x) = softmax (cid:18) Mg σ (cid:19) xW q(xW k) dh xW v, (10) σ2 σ is the Gaussian matrix with elements Mg where Mg σ(i, j) = 2π (ji)2 1 , and σ2 denotes the variance. By varying σ, feature interactions at different scales are modeled, generating video features with multiple receptive fields. q, k, are linear projections, while dh is the latent attention dimension, denotes element-wise product. Finally, We replace the self-attention in Transformer block with Euclidean Gaussian attention to form the Euclidean Attention Block. Lorentz Attention Block Given extracted Euclidean in RM d, we first project it to video embeddings xE RM via linear layer and apply scaling. Let := [1, 0, . . . , 0] be the origin on the Lorentz manifold, satisfying o, [0, xE in ] can be interpreted as vector in the tangent space at o. The Lorentz embedding is then obtained via the exponential map Eq. (4): (cid:0)(cid:2)0, βxE in ]L = 0. Thus, [0, xE (cid:3)(cid:1) Ln, RM (n+1), xL (11) in = expo in W1 where W1 denotes the linear layer, β is learnable scaling factor to prevent numerical overflow."
        },
        {
            "title": "Having obtained the Lorentz embedding xL",
            "content": "in , which inherently exhibits prominent hierarchical structure due to the hyperbolic space properties, we next design Lorentz linear transformation and Lorentz self-attention module to capture and fully leverage the hierarchical priors. Inspired by prior studies [6, 33], we redefine the Lorentz (cid:21) , where Rn+1 linear layer to learn matrix = (cid:20)p is weight parameter and Rm(n+1) ensures that Ln, fx(M )x Lm. Specifically, the transformation matrix fx(M ) is expressed as: fx(M ) = fx (cid:21)(cid:19) (cid:18)(cid:20)p = (cid:34) (cid:35) W x2+1 px . (12) Adding other components including normalization, the final definition of the Lorentz Linear layer becomes: (cid:105) (cid:104) ϕ(W x,p)2+1 = HL(x) = , (13) ϕ(W x,p) with operation function: ϕ (W x, p) = λ (cid:0)px + b(cid:1) (x) + (W (x) + b) , (14) where and are bias terms, λ > 0 regulates the scaling range. denotes the activation function. Based on the Lorentz Linear Layer, we propose Lorentz self-attention module that integrates Gaussian constraints into feature interactions, enabling multiscale and hierarchical video embeddings in hyperbolic space. Specifically, given in Ln, RM (n+1), we hyperbolic video embedding xL first obtain the attention query Q, key K, and value using Eq. (13), all in the shape of RM (n+1). We calculate attention scores based on Eq. (6) and apply Gaussian maσ RM for element-wise multiplication with trix Mg the score matrix to obtain multi-scale receptive field. The out = {µ1, . . . , µQ} RM (n+1): output is defined as xL exp( L(qi,kj )Mg σ(i,j) ) Sij = µi = (n+1) L(qi,kk)Mg σ(i,k) (n+1) (cid:80)K k=1 exp( d2 (cid:80)K j=1 Sijvj k=1 SikvkL , (cid:12) (cid:12) (cid:12) (cid:12)(cid:80)K , ) (15)"
        },
        {
            "title": "After computing xL",
            "content": "the squared Lorentzian distance d2 L(a, b) = 2 2a, bL. out, we apply the logarithmic map Eq. (5), while discarding the time axis, to obtain the Euclidean space embedding xE mid. Then, the output xE out is obtained through Linear Layer followed by rescaling: mid = drop time axis(logo(xL xE out)) RM n, xE out = xE midW2 β RM d, (16) where W2 Rnd, β is the scale factor in Eq. (11). Finally, We replace the self-attention in Transformer block with Lorentz attention to form the Lorentz Attention Block. Mean-Guided Adaptive Interaction Module We arange NL Lorentz and NE Euclidean Attention Blocks in parallel to construct NO Gaussian Attention Blocks for multi-scale hybrid-space video embeddings. To integrate these features, we introduce Mean-Guided Adaptive Interaction Module, which utilizes globally pooled features to compute dynamic aggregation weights. Specifically, we first obtain the global query φ R1d and compute aggregation weights via Cross Attention Block consisting of cross-attention layer (CA) followed by fully connected layer (FC): φ = Mean(xσ1 , xσ2, .., xσNo wi = FC(CA(φ, xσi, xσi)), = 1, 2, ..., No, ), wi,j = ewi,j /τ k=1 ewk,j /τ (cid:80)No , = 1, ..., M, (17) xj = No(cid:88) i=1 wi,jxσi,j, = 1, ..., M, xMAIM = Concat(x1, x2, ..., xM ), where xσi RM denotes the output of the i-th Gaussian block and corresponds to the number of time points (i.e., clips or frames). wi RM represents the aggregation weights for the i-th Gaussian block, and τ is the temperature factor. xj Rd denotes the aggregated feature at time point j, while xMAIM is the final output. 3.4. Learning Objectives Given the partial relevance in PRVR, where each video fully entails its corresponding text, partial order relationship is established, with the text-query semantically subsumed by the video: text video. Inspired by MERU [10], we propose the Partial Order Preservation Loss to enforce this relationship in Hyperbolic Space. Given Vf and Vc from Sec. 3.2, simple attention module similar to Eq. (7) is applied, followed by mean pooling to get the unified video representation Vv. The video and text representations are then mapped to Lorentz space via the exponential map, yielding v, Ln, as shown in Fig. 2(c). We define an entailment cone for each v, which is characterized by the half-aperture: HA(v) = arcsin (cid:18) 2c vs (cid:19) . (18) = 0.1 is used to define the boundary conditions near the origin. We measure the exterior angle EA(v, t) = π Ovt to penalize cases where falls outside the entailment cone: EA(v, t) = arccos t0 + v0v, tL (cid:113) vs (v, tL)2 1 . (19) The Loss for single video-text pair is given by: Lpop(v, t) = max(0, EA(v, t) HA(v)). (20) Besides, following MS-SL [14], we use the standard similarity retrieval loss to train the model, denoted as Lsim. Meanwhile, the query diversity [61] Ldiv is used to improve retrieval performance. The aggregate loss is defined as: comprises 3,072-dimensional visual features obtained by concatenating frame-level ResNet152 features [24] and segment-level I3D features [2]. For ActivityNet Captions and Charades-STA, we only utilize I3D features as provided by Zhang et al. [66] and Mun et al. [45], respectively. For sentence representations, we adopt the 768-dimensional RoBERTa features supplied by Lei et al. [31] for TVR. On ActivityNet Captions and Charades-STA, we employ 1,024dimensional RoBERTa features extracted using MS-SL[14]. Model Configurations The HLFormer block consists of 8 Gaussian blocks (NO = 8), 4 Lorentz Attention blocks (NL = 4), with Gaussian variances ranging from 21 to 2NL1 and , and 4 Euclidean Attention blocks (NE = 4), with Gaussian variances ranging from 21 to 2NE 1 and . The latent dimension = 384 with 4 attention heads. Training Configurations We employ the Adam optimizer with mini-batch size of 128 and set the number of epochs to 100. The model is implemented using PyTorch and trained on one Nvidia RTX 3080 Ti GPU. We adopt learning rate adjustment schedule similar to MS-SL. Lagg = Lsim + λ1Ldiv + λ2Lpop, (21) 4.3. Comparison with State-of-the arts λ1 and λ2 are hyper-parameters that balance learning losses. 4. Experiments 4.1. Experimental Setup Datasets We conduct experiments on three benchmark datasets: (i) ActivityNet Captions [29], which comprises approximately 20K YouTube videos with an average duration of 118 seconds. Each video contains an average of 3.7 annotated moments with corresponding textual descriptions. (ii) TV show Retrieval (TVR) [31], consisting of 21.8K videos sourced from six TV shows. Each video is associated with five natural language descriptions covering different moments. (iii) Charades-STA [23], which includes 6,670 videos annotated with 16,128 sentence descriptions. On average, each video contains approximately 2.4 moments with corresponding textual queries. We adopt the same data split as used in prior studies[14, 61]. It is important to note that the moment annotations are unavailable in the PRVR task. Metrics Following previous works [14, 61], we adopt rank-based evaluation metrics, specifically R@K (K = 1, 5, 10, 100). The metric R@K represents the proportion of queries for which the correct item appears within the top positions of the ranking list. All results are reported as percentages (%), where higher values indicate superior retrieval performance. To facilitate an overall comparison, we also report the Sum of all Recalls (SumR). 4.2. Implementation Details Data Processing For video representations on TVR, we employ the feature set provided by Lei et al. [31], which Baselines We select six representative PRVR baselines for comparison: MS-SL [14], PEAN [27], LH [20], BGMNet [64], GMMFormer [61], and DL-DKD [16]. We also compare HLFormer with methods for T2VR and VCMR. For T2VR, we select six T2VR models: CE [38], HGR [5], DE++ [13], RIVRL [15], CLIP4Clip [41], Cap4Video [63], For VCMR, we consider four models: XML [31], ReLoCLNet [67], CONQUER [26] and JSG[7]. Retrieval Performance Tab. 1 presents the retrieval performance of various models on three large-scale video datasets. As observed, T2VR models, designed to capture overall video-text relevance, underperform for PRVR. VCMR models, which focus on moment retrieval, achieve better results. PRVR methods perform best as they are specifically designed for this task. Attributed to hyperbolic space learning and effective utilization of video hierarchical structure priors, HLFormer consistently surpasses all baselines. It outperforms DL-DKD by 4.9% and 4.3% in SumR on ActivityNet Captions and TVR, respectively, and exceeds PEAN by 5.4% on Charades-STA. 4.4. Model Analyses Efficacy of Temporal Modeling Design We perform ablation studies to examine the effect of the attention block number No and the attention mechanism ratio NL/NE, with results shown in Fig. 3. Model performance improves as No increases, then stabilizes or declines when No 8. Even with only two attention blocks, HLFormer surpasses most competing methods. Furthermore, using solely Euclidean or Lorentz attention blocks results in suboptimal performance, whereas the hybrid attention block achieves the best results. Model T2VR HGR [5] RIVRL [15] DE++ [13] CE [38] CLIP4Clip [41] Cap4Video [63] VCMR ReLoCLNet [67] XML [31] CONQUER [26] JSG [7] PRVR MS-SL [14] PEAN [27] LH [20] BGM-Net [64] GMMFormer [61] DL-DKD [16] HLFormer (ours) ActivityNet Captions Charades-STA TVR R@1 R@5 R@10 R@100 SumR R@1 R@5 R@10 R@100 SumR R@1 R@5 R@10 R@100 SumR 4.0 5.2 5.3 5.5 5.9 6.3 5.7 5.3 6.5 6.8 7.1 7.4 7.4 7.2 8.3 8.0 8.7 15.0 18.0 18.4 19.1 19.3 20.4 18.9 19.4 20.4 22.7 22.5 23.0 23.5 23.8 24.9 25.0 27. 24.8 28.2 29.2 29.9 30.4 30.9 30.0 30.6 31.8 34.8 34.7 35.5 35.8 36.0 36.7 37.5 40.1 63.2 66.4 68.0 71.1 71.6 72.6 72.0 73.1 74.3 76.1 75.8 75.9 75.8 76.9 76.1 77.1 79. 107.0 117.8 121.0 125.6 127.3 130.2 126.6 128.4 133.1 140.5 140.1 141.8 142.4 143.9 146.0 147.6 154.9 1.2 1.6 1.7 1.3 1.8 1.9 1.2 1.6 1.8 2.4 1.8 2.7 2.1 1.9 2.1 - 2. 3.8 5.6 5.6 4.5 6.5 6.7 5.4 6.0 6.3 7.7 7.1 8.1 7.5 7.4 7.8 - 8.5 7.3 9.4 9.6 7.3 10.9 11.3 10.0 10.1 10.3 12.8 11.8 13.5 12.9 12.2 12.5 - 13. 33.4 37.7 37.1 36.0 44.2 45.0 45.6 46.9 47.5 49.8 47.7 50.3 50.1 50.1 50.6 - 54.0 45.7 54.3 54.1 49.1 63.4 65.0 62.3 64.6 66.0 72.7 68.4 74.7 72.7 71.6 72.9 - 78. 1.7 9.4 8.8 3.7 9.9 10.3 10.0 10.7 11.0 - 13.5 13.5 13.2 14.1 13.9 14.4 15.7 4.9 23.4 21.9 12.8 24.3 26.4 26.5 28.1 28.9 - 32.1 32.8 33.2 34.7 33.3 34.9 37. 8.3 32.2 30.2 20.1 34.3 36.8 37.3 38.1 39.6 - 43.4 44.1 44.4 45.9 44.5 45.8 48.5 35.2 70.6 67.4 64.5 72.5 74.0 81.3 80.3 81.3 - 83.4 83.9 85.5 85.2 84.9 84.9 86. 50.1 135.6 128.3 101.1 141.0 147.5 155.1 157.1 160.8 - 172.4 174.2 176.3 179.9 176.6 179.9 187.7 Table 1. Retrieval performance of HLFormer and other faithfull methods on ActivityNet Captions, Charades-STA and TVR. State-of-the-art performance is highlighted in bold. - indicates that the corresponding results are unavailable. ID Model ActivityNet Captions Charades-STA TVR R@1 R@5 R@10 R@100 SumR R@1 R@5 R@10 R@100 SumR R@1 R@5 R@10 R@100 SumR (0) HLFormer (full) 8.7 27.1 40. 79.0 154.9 2.6 8.5 13.7 54. 78.7 15.7 37.1 48.5 86.4 187. Efficacy of Multi-scale Branches (1) w/o gaze branch (2) w/o glance branch 7.6 6.4 Efficacy of Different Loss Terms (3) Lsim Only (4) w/o Ldiv (5) w/o Lpop 7.7 8.5 8.6 24.4 21.7 25.0 26.6 26. Efficacy of various Aggregation Strategies (6) w/ MP (7) w/ CL 25.7 26.8 8.5 8.7 36.7 33.6 38.1 39.6 39.7 38.2 39. 77.3 75.4 78.3 78.8 78.8 77.8 78.6 146.1 137.2 149.1 153.5 154.0 150.2 153. 1.8 1.6 2.0 2.0 2.2 2.0 2.0 8.0 7.7 8.1 7.8 8.4 8.0 8. 13.9 13.1 13.2 13.6 14.0 13.2 13.9 50.8 48.4 52.0 53.0 53.0 52.1 52. 74.5 70.8 75.3 76.4 77.6 75.3 76.1 13.9 11.4 15.1 15.7 15.6 15.2 15. 34.0 30.5 36.2 36.4 36.8 36.5 36.9 45.2 41.8 47.8 48.4 48.4 47.4 48. 85.3 82.4 86.0 86.0 86.0 86.0 86.0 178.3 166.1 185.2 186.5 186.8 185.1 186. Table 2. Ablation Study of HLFormer. The best scores are marked in bold. This may be attributed to the differences in representational focus: Euclidean space emphasizes fine-grained local feature learning and sometimes overlooks global hierarchical structures, while hyperbolic space prioritizes global hierarchical relationships at the expense of local details. Moreover, hyperbolic space tends to be more sensitive to noise and numerically unstable. By integrating hybrid spaces, HLFormer achieves mutual compensation, enhancing representation learning and facilitating video semantic understanding. Efficacy of Hyperbolic Learning Hyperbolic learning demonstrates significant advantages in capturing the hierarchical structure of videos. As illustrated in Fig. 4(a), embeddings learned solely in Euclidean space exhibit indistinct cluster boundaries, with red and green points at the periphery closely interspersed. In contrast, Fig. 4(b) demonstrates that incorporating Lorentz attention facilitates the learning of more discriminative representations, while refining moment cluster boundaries, increasing inter-moment separation, and compacting intra-moment frame distributions, revealing more pronounced hierarchical structure. Efficacy of Multi-scale Branches To evaluate the effectiveness of the multi-scale branches, we conduct comparative experiments by removing either the glance clip-level branch or the gaze frame-level branch. As shown in Tab. 2, the absence of any branch leads to noticeable performance degradation. These results not only validate the efficacy of the coarse-to-fine multi-granularity retrieval mechanism but also highlight the complementary nature of the two branches. Efficacies of Different Loss Terms To analyze the effectiveness of three loss terms (i.e. Lsim, Ldiv and Lpop) of HLFormer, we construct several HLFormer variants: (i) Lsim Only: train the model with merely Lsim. (ii) w/o Ldiv: We train the model without query diverse learning. (iii) w/o Lpop: HLFormer removes the partial order preservation task. (a) ActivityNet Captions (b) Charades-STA (c) TVR Figure 3. The influence of different attention blocks, with default settings marked in bold. (a) w/o Lorentz Attention (b) w/ Lorentz Attention Figure 4. The UMAP [42] visualization displays the learned frame embeddings from video in TVR. Data points of the same color correspond to the same moment. As shown in Tab. 2, the worst performance occurs when only Lsim is used. Comparing Variant (5) with Variant (3), adding Ldiv increases the SumR, which can validate its necessity. Similarly, comparing Variant (4) with Variant (3) and Fig. 5, integrating Lpop not only boosts retrieval accuracy but also ensures that the text query remains semantically embedded within the corresponding video, preserving partial relevance. Efficacy of Aggregation Strategy We compare three aggregation strategies: (i) w/ MP: mean pooling for static fusion. (ii) w/ CL: feature concatenation with linear layers . (iii) MAIM (default): mean-guided adaptive interaction module. As shown in Tab. 2, MP performs the worst due to its fixed static fusion, which limits semantic interaction. CL improves upon MP by leveraging linear layers for dynamic feature fusion. MAIM achieves the best performance by learning adaptive aggregation weights and dynamically selecting hyperbolic information under global guidance. (a) w/o Lpop (b) w/ Lpop Figure 5. Visualization of the learned hyperbolic space. The closer to the origin, the higher semantic hierarchy and coarser granularity. Visualization of Hyperbolic Space Inspired by HyCoCLIP [48], we visualize the learned hyperbolic space by sampling 3K embeddings from the TVR training set. We analyze their norm distribution via histogram and reduce dimensionality using HoroPCA [3], as shown in Fig. 5. Glance branch embeddings are positioned closer to the origin than text query embeddings, indicating that clip-level video representations subsume textual queries. This phenomenon can be attributed to Lpop, which enforces the partial order relationship between video and text representations. In contrast, without Lpop, embeddings exhibit uncorrelated distributions. Moreover, text queries, being coarser in semantics, lie closer to the origin than fine-grained gaze-level embeddings, reflecting clear hierarchical structure. 5. Conclusions In this paper, we propose HLFormer, novel hyperbolic modeling framework tailored for PRVR. By leveraging the intrinsic geometric properties of hyperbolic space, HLFormer effectively captures the hierarchical and multi-granular structure of untrimmed videos, thereby enhancing video-text retrieval accuracy. Furthermore, to ensure partial relevance between paired videos and text, partial order preservation loss is introduced to enforce their semantic entailment. Extensive experiments indicate that HLFormer consistently outperforms state-of-the-art methods. Our study offers new perspective for PRVR with hyperbolic learning, which we hope will inspire further research in this direction. Acknowledgments. We sincerely thank the anonymous reviewers and chairs for their efforts and constructive suggestions, which have greatly helped us improve the manuscript. This work is supported in part by the National Natural Science Foundation of China under grant 624B2088, 62171248, 62301189, the PCNL KEY project (PCL2023AS6-1), and Shenzhen Science and Technology Program under Grant KJZD20240903103702004, JCYJ20220818101012025, GXWD20220811172936001. Long Chen was supported by the Hong Kong SAR RGC Early Career Scheme (26208924), the National Natural Science Foundation of China Young Scholar Fund (62402408), Huawei Gift Fund, and the HKUST Sports Science and Technology Research Grant (SSTRG24EG04)."
        },
        {
            "title": "References",
            "content": "[1] Mina Ghadimi Atigh, Julian Schoep, Erman Acar, Nanne Van Noord, and Pascal Mettes. Hyperbolic image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44534462, 2022. 3 [2] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 62996308, 2017. 6 [3] Ines Chami, Albert Gu, Dat Nguyen, and Christopher Re. Horopca: Hyperbolic dimensionality reduction via horospherical projections, 2021. 8 [4] Bike Chen, Wei Peng, Xiaofeng Cao, and Juha Roning. HyIEEE perbolic uncertainty aware semantic segmentation. Transactions on Intelligent Transportation Systems, 25(2): 12751290, 2023. 3 [5] Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. Fine-grained video-text retrieval with hierarchical graph reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1063810647, 2020. 1, 2, 6, 7 [6] Weize Chen, Xu Han, Yankai Lin, Hexu Zhao, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Fully hyperbolic neural networks. arXiv preprint arXiv:2105.14686, 2021. 3, 5 [7] Zhiguo Chen, Xun Jiang, Xing Xu, Zuo Cao, Yijun Mo, and Heng Tao Shen. Joint searching and grounding: MultiIn Proceedings of the granularity video content retrieval. 31st ACM International Conference on Multimedia, pages 975983, 2023. 2, 6, [8] Dingxin Cheng, Shuhan Kong, Bin Jiang, and Qiang Guo. Transferable dual multi-granularity semantic excavating for partially relevant video retrieval. Image and Vision Computing, 149:105168, 2024. 2 [9] Cheol-Ho Cho, WonJun Moon, Woojin Jun, MinSeok Jung, and Jae-Pil Heo. Ambiguity-restrained text-video representation learning for partially relevant video retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 25002508, 2025. 2 Text Representations. In Proceedings of the International Conference on Machine Learning, 2023. 2, 3, 5 [11] Jianfeng Dong, Xirong Li, and Cees GM Snoek. Predicting visual features from text for image and video caption retrieval. IEEE Transactions on Multimedia, 20(12):33773388, 2018. 1, 2 [12] Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, Yuan He, Gang Yang, and Xun Wang. Dual encoding for zeroexample video retrieval. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 93469355, 2019. [13] Jianfeng Dong, Xirong Li, Chaoxi Xu, Xun Yang, Gang Yang, Xun Wang, and Meng Wang. Dual encoding for video retrieval by text. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(8):40654080, 2021. 1, 6, 7, 12 [14] Jianfeng Dong, Xianke Chen, Minsong Zhang, Xun Yang, Shujie Chen, Xirong Li, and Xun Wang. Partially relevant video retrieval. In Proceedings of the 30th ACM International Conference on Multimedia, pages 246257, 2022. 1, 2, 4, 6, 7, [15] Jianfeng Dong, Yabing Wang, Xianke Chen, Xiaoye Qu, Xirong Li, Yuan He, and Xun Wang. Reading-strategy inspired visual representation learning for text-to-video retrieval. IEEE Transactions on Circuits and Systems for Video Technology, 32(8):56805694, 2022. 1, 2, 6, 7 [16] Jianfeng Dong, Minsong Zhang, Zheng Zhang, Xianke Chen, Daizong Liu, Xiaoye Qu, Xun Wang, and Baolong Liu. Dual learning with dynamic knowledge distillation for partially relevant video retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11302 11312, 2023. 2, 6, 7 [17] Aleksandr Ermolov, Leyla Mirvakhabova, Valentin Khrulkov, Nicu Sebe, and Ivan Oseledets. Hyperbolic vision transformers: Combining improvements in metric learning. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 73997409, 2022. 2, 3 [18] Fartash Faghri, David Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improving visual-semantic embeddings with hard negatives. arXiv preprint arXiv:1707.05612, 2017. 1, 2 [19] Hao Fang, Changle Zhou, Jiawei Kong, Kuofeng Gao, Bin Chen, Tao Liang, Guojun Ma, and Shu-Tao Xia. Grounding language with vision: conditional mutual information calibrated decoding strategy for reducing hallucinations in lvlms. arXiv preprint arXiv:2505.19678, 2025. 2 [20] Sheng Fang, Tiantian Dang, Shuhui Wang, and Qingming Huang. Linguistic hallucination for text-based video retrieval. IEEE Transactions on Circuits and Systems for Video Technology, 34(10):96929705, 2024. 6, 7 [21] Octavian Ganea, Gary Becigneul, and Thomas Hofmann. Hyperbolic entailment cones for learning hierarchical embeddings. In Proceedings of the 35th International Conference on Machine Learning, pages 16461655. PMLR, 2018. [22] Octavian Ganea, Gary Becigneul, and Thomas Hofmann. Hyperbolic neural networks. Advances in neural information processing systems, 31, 2018. 3 [10] Karan Desai, Maximilian Nickel, Tanmay Rajpurohit, Justin Johnson, and Ramakrishna Vedantam. Hyperbolic Image- [23] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pages 52675275, 2017. 2, 6 [24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 6 [25] Neil He, Menglin Yang, and Rex Ying. Lorentzian residual neural networks. arXiv preprint arXiv:2412.14695, 2024. 3 [26] Zhijian Hou, Chong-Wah Ngo, and Wing Kwong Chan. Conquer: Contextual query-aware ranking for video corpus moment retrieval. In Proceedings of the 29th ACM International Conference on Multimedia, pages 39003908, 2021. 6, 7 [27] Xun Jiang, Zhiguo Chen, Xing Xu, Fumin Shen, Zuo Cao, and Xunliang Cai. Progressive event alignment network for partial relevant video retrieval. In 2023 IEEE International Conference on Multimedia and Expo (ICME), pages 1973 1978. IEEE, 2023. 2, 6, [28] Valentin Khrulkov, Leyla Mirvakhabova, Evgeniya Ustinova, Ivan Oseledets, and Victor Lempitsky. Hyperbolic image embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3 [29] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706715, 2017. 2, 6 [30] Marc Law, Renjie Liao, Jake Snell, and Richard Zemel. Lorentzian distance learning for hyperbolic representations. In Proceedings of the 36th International Conference on Machine Learning, pages 36723681. PMLR, 2019. 2 [31] Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal. Tvr: large-scale dataset for video-subtitle moment retrieval. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXI 16, pages 447463. Springer, 2020. 2, 6, 7 [32] Jiaxu Leng, Zhanjie Wu, Mingpi Tan, Yiran Liu, Ji Gan, Haosheng Chen, and Xinbo Gao. Beyond euclidean: Dualspace representation learning for weakly supervised video violence detection. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2, 3 [33] Keegan Lensink, Bas Peters, and Eldad Haber. Fully hyperbolic convolutional neural networks. Research in the Mathematical Sciences, 9(4):60, 2022. 3, 5 [34] Huimin Li, Zhentao Chen, Yunhao Xu, and Junlin Hu. Hyperbolic anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1751117520, 2024. 3 [35] Xirong Li, Chaoxi Xu, Gang Yang, Zhineng Chen, and Jianfeng Dong. W2vv++ fully deep learning for ad-hoc video search. In Proceedings of the 27th ACM international conference on multimedia, pages 17861794, 2019. 1, [36] Haitong Liu, Kuofeng Gao, Yang Bai, Jinmin Li, Jinxiao Shan, Tao Dai, and Shu-Tao Xia. Protecting your video content: Disrupting automated video-based llm annotations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2405624065, 2025. 2 [37] Peidong Liu, Dongliang Liao, Jinpeng Wang, Yangxin Wu, Gongfu Li, Shu-Tao Xia, and Jin Xu. Multi-task ranking In Companion with user behaviors for text-video search. Proceedings of the Web Conference 2022, pages 126130, 2022. 2 [38] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019. 1, 2, 6, 7 [39] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 3 [40] Teng Long, Pascal Mettes, Heng Tao Shen, and Cees GM Snoek. Searching for actions on the hyperbole. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11411150, 2020. [41] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. Neurocomputing, 508:293304, 2022. 6, 7 [42] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. 8 [43] Guanghao Meng, Sunan He, Jinpeng Wang, Tao Dai, Letian Zhang, Jieming Zhu, Qing Li, Gang Wang, Rui Zhang, and Yong Jiang. Evdclip: Improving vision-language retrieval with entity visual descriptions from large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 61266134, 2025. 12 [44] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2630 2640, 2019. 1, 2 [45] Jonghwan Mun, Minsu Cho, and Bohyung Han. Local-global video-text interactions for temporal grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1081010819, 2020. 6 [46] Maximillian Nickel and Douwe Kiela. Poincare embeddings In Advances in for learning hierarchical representations. Neural Information Processing Systems. Curran Associates, Inc., 2017. 2, [47] Maximillian Nickel and Douwe Kiela. Learning continuous hierarchies in the Lorentz model of hyperbolic geometry. In Proceedings of the 35th International Conference on Machine Learning, pages 37793788. PMLR, 2018. 3 [48] Avik Pal, Max van Spengler, Guido Maria DAmely di Melendugno, Alessandro Flaborea, Fabio Galasso, and Pascal Mettes. Compositional entailment learning for hyperbolic vision-language models. In The Thirteenth International Conference on Learning Representations, 2025. 3, 8 [49] Xiaogang Peng, Hao Wen, Yikai Luo, Xiao Zhou, Keyang Yu, Yigang Wang, and Zizhao Wu. Learning weakly supervised audio-visual violence detection in hyperbolic space, 2023. 3 [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Mang Ye. An empirical study of federated prompt learning for vision language model. arXiv preprint arXiv:2505.23024, 2025. 2 [63] Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, and Wanli Ouyang. Cap4video: What can auxiliary captions do for text-video retrieval? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1070410713, 2023. 6, 7 [64] Shukang Yin, Sirui Zhao, Hao Wang, Tong Xu, and Enhong Chen. Exploiting instance-level relationships in weakly supervised text-to-video retrieval. ACM Trans. Multim. Comput. Commun. Appl., 20(10):316:1316:21, 2024. 2, 6, [65] Zhen Yu, Toan Nguyen, Yaniv Gal, Lie Ju, Shekhar Chandra, Lei Zhang, Paul Bonnington, Victoria Mar, Zhiyong Wang, and Zongyuan Ge. Skin lesion recognition with classIn Internahierarchy regularized hyperbolic embeddings. tional conference on medical image computing and computerassisted intervention, pages 594603. Springer, 2022. 3 [66] Bowen Zhang, Hexiang Hu, Joonseok Lee, Ming Zhao, Sheide Chammas, Vihan Jain, Eugene Ie, and Fei Sha. hierarchical multi-modal encoder for moment localization in video corpus. arXiv preprint arXiv:2011.09046, 2020. 6 [67] Hao Zhang, Aixin Sun, Wei Jing, Guoshun Nan, Liangli Zhen, Joey Tianyi Zhou, and Rick Siow Mong Goh. Video corpus moment retrieval with contrastive learning. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 685695, 2021. 6, 7, 12 [68] Minyi Zhao, Jinpeng Wang, Dongliang Liao, Yiru Wang, Huanzhong Duan, and Shuigeng Zhou. Keyword-based diverse image retrieval by semantics-aware contrastive learning and transformer. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 12621272, 2023. 12 Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 3 [51] Ruiqi Shi, Jun Wen, Wei Ji, Menglin Yang, Difei Gao, and Roger Zimmermann. HOVER: Hyperbolic video-text retrieval, 2024. 3 [52] Xue Song, Jingjing Chen, Zuxuan Wu, and Yu-Gang Jiang. Spatial-temporal graphs for cross-modal text2video retrieval. IEEE Transactions on Multimedia, 24:29142923, 2021. 2 [53] Chaolei Tan, Jianhuang Lai, Wei-Shi Zheng, and Jian-Fang Hu. Siamese learning with joint alignment and regression for weakly-supervised video paragraph grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1356913580, 2024. [54] Haomiao Tang, Jinpeng Wang, Yuang Peng, GuangHao Meng, Ruisheng Luo, Bin Chen, Long Chen, Yaowei Wang, and ShuTao Xia. Modeling uncertainty in composed image retrieval In Proceedings of the 63rd via probabilistic embeddings. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12101222, 2025. 12 [55] Yuanmin Tang, Jing Yu, Keke Gai, Jiamin Zhuang, Gang Xiong, Gaopeng Gou, and Qi Wu. Missing target-relevant information prediction with world model for accurate zero-shot composed image retrieval. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 24785 24795, 2025. 12 [56] Max Van Spengler, Erwin Berkhout, and Pascal Mettes. Poincare resnet. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 54195428, 2023. 3 [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3 [58] Jinpeng Wang, Bin Chen, Dongliang Liao, Ziyun Zeng, Gongfu Li, Shu-Tao Xia, and Jin Xu. Hybrid contrastive quantization for efficient cross-view video retrieval. In Proceedings of the ACM Web Conference 2022, pages 30203030, 2022. 2 [59] Jinpeng Wang, Ziyun Zeng, Bin Chen, Yuting Wang, Dongliang Liao, Gongfu Li, Yiru Wang, and Shu-Tao Xia. Hugs bring double benefits: Unsupervised cross-modal hashing with multi-granularity aligned transformers. International Journal of Computer Vision, 132(8):27652797, 2024. 2 [60] Yuting Wang, Jinpeng Wang, Bin Chen, Tao Dai, Ruisheng Luo, and Shu-Tao Xia. Gmmformer v2: An uncertainty-aware framework for partially relevant video retrieval, 2024. 1, 2, [61] Yuting Wang, Jinpeng Wang, Bin Chen, Ziyun Zeng, and Shu-Tao Xia. Gmmformer: Gaussian-mixture-model based transformer for efficient partially relevant video retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024. 1, 2, 4, 6, 7, 12 [62] Zhihao Wang, Wenke Huang, Tian Chen, Zekun Shi, Guancheng Wan, Yu Qiao, Bin Yang, Jian Wang, Bing Li, and A. Appendix A.1. Derivation of Lpop In this section, we formally derive the components of the partial order preservation loss employed in our approach. Half-Aperture We begin with the definition of the halfaperture for the Poincare ball, as introduced by Ganea et al. [21]. Given point xP on the Poincare ball, the cone halfaperture is formulated as: HAP (xP ) = sin (cid:18) 1 xP 2 xP (cid:19) . (22) Since the Poincare ball model and the Lorentz hyperboloid model are isometric, any point xP in the Poincare ball can be mapped to corresponding point xL in the hyperboloid model via the following differentiable transformation: xL = 2xP 1 xP 2 . (23) To ensure model invariance, the half-aperture should remain unchanged across hyperbolic representations, i.e., HAL(xL) = HAP (xP ). Substituting Eq. (23) into Eq. (22), we derive: HAL(xL) = sin . (24) (cid:18) 2c (cid:19) xL Exterior Angle Consider three points: the origin o, the video embedding v, and the text embedding t. These points form hyperbolic triangle whose sides are defined by the geodesic distances = d2 L(o, v), and = d2 L(v, t). The hyperbolic law of cosines provides means to compute the angles of this triangle. The exterior angle is given by: L(o, t), = d2 EA(v, t) = π ovt = π cos1 (cid:20) cosh(z) cosh(y) cosh(x) sinh(z) sinh(y) (cid:21) . (25) We define g(s) = cosh(s) and employ the hyperbolic identity sinh(s) = cosh2(s) 1: (cid:113) EA(v, t) = cos1 (cid:34) g(x) g(z)g(y) (cid:112)g(z)2 1(cid:112)g(y)2 1 (cid:35) . (26) We now compute g(x), g(y), and g(z). Given that g(z) = cosh(d2 L(v, t)) and utilizing the definition d2 L(v, t) = cosh1(v, tL), we obtain: g(z) = cosh (cid:0)d2 L(v, t)(cid:1) = cosh (cid:0)cosh1(v, tL)(cid:1) = v, tL. Similarly, we derive g(x) = o, tL and g(y) = o, vL. The Lorentzian inner product with the origin simplifies as follows: o, vL = v0, and o, tL = t0. (28) Thus, we obtain g(x) = t0 and g(y) = v0. Substituting these values into Eq. (26), we derive the refined expression: EA(v, t) = cos t0 + v0v, tL (cid:113) (cid:112)v2 0 (v, tL)2 1 . Finally, utilizing the relation between x0 and vs, we simplify the denominator to obtain the final expression for the exterior angle: EA(v, t) = cos1 t0 + v0v, tL (cid:113) (v, tL)2 1 vs . A.2. Training Objectives Following existing works [14, 61], we adopt triplet loss [13, 54] Ltrip and InfoNCE loss [43, 55, 67, 68] Lnce, query diverse loss [60, 61] Ldiv. text-video pair is considered positive if the video contains moment relevant to the text; otherwise, it is regarded as negative. Given positive textvideo pair (T , V), the triplet ranking loss over the mini-batch is formulated as: Ltrip = 1 (cid:88) (T ,V)B {max(0, + S(T , V) S(T , V)) +max(0, + S(T , ) S(T , V))}, (29) where is margin constant. and indicate negative text for and negative video for , respectively. The similarity score S(, ) is obtained by Equation (9) . The infoNCE loss is computed as: Lnce = 1 (cid:88) {log( (T ,V)B S(T , V) S(T , V) + (cid:80) NT ) S(T , V) +log( S(T , V) S(T , V) + (cid:80) NV )}, (30) S(T , ) where NT and NV represent the negative texts and videos of and within the mini-batch B, respectively. Finally , Lsim is defined as: Lsim = Ltrip clip + Ltrip clip + λf Lnce rame + λcLnce where rame and clip mark the objectives for the gaze frame-level branch and the glance clip-level branch, respectively. λc and λf are hyper-parameters to balance the contributions of InfoNCE objectives. rame, (31) (27) Given collection of text queries in the mini-batch B, the query diverse loss is defined as: (cid:88) Ldiv = 1 qi,qj 1qi,qj log(1 + eα(cos(qi,qj )+δ)) (32) where δ > 0 denotes the margin, α > 0 is scaling factor, and 1qi,qj {0, 1} represents an indicator function, 1qi,qj = 1 when qi and qj correspond to the same video. blocks. As illustrated in Fig. 6, larger σ generally leads to superior performance due to its broader receptive field, which enables better modeling of temporal dependencies within videos. However, excessively large σ results in overly dispersed attention, weakening the enhancement of semantic information from adjacent frames or clips, thereby leading to suboptimal performance. In contrast, HLFormer employs multiple σ values to achieve multi-scale flexible of video semantics, not only attaining improved performance but also mitigating the need for extensive hyper-parameter tuning. Name CPU GPU RAM OS Configuration Intel Xeon Platinum 8269CY CPU @ 2.50GHz (26 cores) single NVIDIA GeForce GTX 3080 Ti (12GB) 64GB Ubuntu 20.04 LTS CUDA Version 11. GPU Driver Version 535.183.01 Language Python 3.11.8 Dependencies torch 2.0.1 torchvision 0.15.2 numpy 1.26.4 Table 3. Computing infrastructure for our experiments. Params ActivityNet Captions TVR Charades-STA learning rate αf αc α δ τ λc λf λ1 λ2 2.5e-4 0.3 0.7 32 0.2 0.2 6e-1 2e-2 4e-2 3e-3 1e3e-4 0.3 0.7 32 0.15 0.1 9e-2 5e-2 4e-2 8e-5 1e-3 2e-4 0.3 0.7 32 0.2 0.2 6e-1 2e-2 4e-2 3e-3 1e-3 Table 4. Hyper-parameter settings. Figure 6. The impact of the Gaussian variance σ on Charades-STA. B. Experiments B.1. Details of Experimental Setup The computing inDetails of Training Configurations frastructure is in Table 3. All random seeds are set to 0. Hyper-parameter Notably, we directly inherent most hyper-parameter settings from GMMFormer. In detail, we use Mc = 32 for downsampling and set the maximum frame number Mf = 128. If the number of frames exceeds Mf , we uniformly downsample it to Mf . For sentences, we set the maximum length of query words to Nq = 64 for ActivityNet Captions and Nq = 30 for TVR and Charades-STA. Any words beyond the maximum length will be discarded. The Lorentz latent dimension = 127. You can find other detailed hyper-parameter settings in Tab. 4. B.2. Additional Results on Model Analyses Impact of the Gaussian Variance σ We investigate the impact of the Gaussian variance σ on experimental results by employing uniform σ across all Gaussian attention"
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology, Shenzhen",
        "Research Center of Artificial Intelligence, Peng Cheng Laboratory",
        "The Hong Kong University of Science and Technology",
        "Tsinghua Shenzhen International Graduate School, Tsinghua University"
    ]
}