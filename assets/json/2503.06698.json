{
    "paper_title": "What's in a Latent? Leveraging Diffusion Latent Space for Domain Generalization",
    "authors": [
        "Xavier Thomas",
        "Deepti Ghadiyaram"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Domain Generalization aims to develop models that can generalize to novel and unseen data distributions. In this work, we study how model architectures and pre-training objectives impact feature richness and propose a method to effectively leverage them for domain generalization. Specifically, given a pre-trained feature space, we first discover latent domain structures, referred to as pseudo-domains, that capture domain-specific variations in an unsupervised manner. Next, we augment existing classifiers with these complementary pseudo-domain representations making them more amenable to diverse unseen test domains. We analyze how different pre-training feature spaces differ in the domain-specific variances they capture. Our empirical studies reveal that features from diffusion models excel at separating domains in the absence of explicit domain labels and capture nuanced domain-specific information. On 5 datasets, we show that our very simple framework improves generalization to unseen domains by a maximum test accuracy improvement of over 4% compared to the standard baseline Empirical Risk Minimization (ERM). Crucially, our method outperforms most algorithms that access domain labels during training."
        },
        {
            "title": "Start",
            "content": "Whats in Latent? Leveraging Diffusion Latent Space for Domain Generalization Xavier Thomas1 Deepti Ghadiyaram12* 2Runway 1Boston University {xthomas, dghadiya}@bu.edu 5 2 0 2 M 9 ] . [ 1 8 9 6 6 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Domain Generalization aims to develop models that can generalize to novel and unseen data distributions. In this work, we study how model architectures and pre-training objectives impact feature richness and propose method to effectively leverage them for domain generalization. Specifically, given pre-trained feature space, we first discover latent domain structures, referred to as pseudo-domains, that capture domain-specific variations in an unsupervised manner. Next, we augment existing classifiers with these complementary pseudo-domain representations making them more amenable to diverse unseen test domains. We analyze how different pre-training feature spaces differ in the domain-specific variances they capture. Our empirical studies reveal that features from diffusion models excel at separating domains in the absence of explicit domain labels and capture nuanced domain-specific information. On 5 datasets, we show that our very simple framework improves generalization to unseen domains by maximum test accuracy improvement of over 4% compared to the standard baseline Empirical Risk Minimization (ERM). Crucially, our method outperforms most algorithms that access domain labels during training. 1. Introduction It is now common practice to use models pre-trained on billion-scale data [18, 20, 42, 47, 49, 53, 56] as defacto backbones for diverse downstream tasks [39, 65]. In order to make these large-scale models foundational, and offer rich feature representations, variety of powerful pre-training strategies have been designed. Some of these objectives aim to eliminate the need for clean labeled data [7, 8, 12, 19, 75], some reap the benefits from rich text representations by aligning them with corresponding visual signals [28, 53], while others force models to build more meaningful understanding of scenes by learning to predict *Corresponding author. Figure 1. T-SNE visualization of the latent space from different pretraining objectives: CLIP [53], DiT [49], MAE [20], ResNet-50 [18] on the domain generalization benchmark VLCS [15]. VLCS is curated from 4 different datasets, thus dataset-specific biases like spatial composition and object size variations serve as different domains. Note how the diffusion features separate domains effectively, suggesting that latent domain structures can be captured without explicit supervision. Best viewed in color. large hidden regions of images [20]. Despite such tremendous progress, what exactly is captured in the underlying latent landscape remains an open question. This question becomes more challenging in diffusion models mainly due to their iterative global denoising objective. This work aims to understand the feature landscape learnt from different pre-training models and objectives in the context of domain generalization. Robust generalization to unseen domains has been long-standing goal in machine learning research [5, 44], particularly in scenarios where collecting domain-specific data is infeasible or expensive. In such cases, models must learn to generalize without relying on explicit domain labels even during training [36]. It has been established that most sophisticated models struggle when the test data distribution differs from that of training data [55, 61, 63] even in subtle ways, e.g., same visual 1 scene but captured using different cameras, same patient but different brand imaging devices, same object but captured in different color schemes and so on. We posit that the first step to make fundamental progress towards designing foundational models is to examine and interpret how current state-of-the-art models structure visual information and uncover their strengths and limitations. For instance, how are object, scene, and domainspecific variations internally encoded in latent space? Do domain-specific traits manifest in distinct regions of the latent space or are they engulfed along with lowto mid-level scene and object level information? We study these questions in great detail in this work. Specific to the task of domain generalization, we analyze how different pre-training objectives and architectures influence the granularity of visual information captured in their feature space. Our key insight is that certain internal states of diffusion models effectively capture abstract information such as photographic styles, camera angles, and so on. Building on this insight, we first develop an unsupervised method for discovering latent domain structures. Next, we alter standard domain generalization classification [67] pipeline with one key difference: we augment the classifiers representations with the discovered latent domain representations. We show through extensive empirical analysis that this simple tweak to the standard pipeline assists in training model that generalizes well to unseen domains [4]. While most prior works focus on leveraging single feature space to design universal model [10, 16, 40, 60, 62], we take an alterative approach and compliment existing classifiers features with domain rich features and show that this auxiliary guidance makes the overall feature space more robust to unseen domains. Our framework dubbed GUIDE: Generalization using Inferred Domains from Latent Embeddings, offers simple and effective method to guide given feature space to adapt better to unseen domains. We summarize our key contributions below: We propose method of unsupervised pseudodomain discovery from frozen pre-trained feature spaces and use them to improve any models ability to generalize to diverse domains, making it particularly useful in scenarios where domain labels used during training are unavailable or noisy (Sec. 3.3). We analyze different pre-training objectives and architectures and investigate how they influence the structure of the feature latent landscape of both diffusion and conventional vision models (Sec. 4.3). We shine light on the ability of diffusion models to capture domain-specific information, such as photographic and artistic styles, texture variations, and demonstrate their effectiveness to domain generalization (Sec. 4.4). We obtain an average test accuracy improvement of +2.6% on 5 datasets, notably beating ERM [67] by +4.3% on the TerraIncognita dataset [3]. 2. Related Work Diffusion features for representation learning: Diffusion models [24, 58] have significantly advanced image and video generation, prompting extensive exploration of their intermediate representations and their utility for diverse downstream tasks such as detection [11], segmentation [2, 72], classification [32], semantic correspondence [41], depth estimation [71, 78], and visual reasoning [70], showcasing their utility in both discriminative and generative domains. Recent studies [30, 41, 69] demonstrate that features extracted across layers and timesteps encode rich semantic information, ranging from coarse patterns to fine-grained details. In this work, we analyze how the latent space of diffusion models captures class and domain-specific information and leverage these representations for the task of domain generalization. Domain generalization: First formalized in [5], domain generalization is the challenging task of designing models capable of generalizing to unseen test domains. Various methods have been proposed to address this by learning domain-agnostic representations [27, 45], data or latent augmentation methods [25, 37, 40, 59], and metalearning [1, 6]. Despite numerous advancements, most methods still under perform Empirical Risk Minimization (ERM) when evaluated rigorously [17], making it very strong baseline. Teterwak et al. [62] builds stronger baseline by incorporating improved training strategies. Matsuura and Harada [43] learn domain-invariant feature extractor by clustering samples into latent domains using style statistics from early convolutional layers, then applying adversarial learning to reduce domain distinctions. Bui et al. [6] uses meta-learning and explicit domain labels to disentangle domain-invariant and domain-specific features, ensuring that the latter remains useful when adapting to new domains. The classifier then integrates both feature types for improved generalization. Dubey et al. [14], Thomas et al. [64] explore techniques to incorporate pseudo-domain information into classifiers to make them generalizable to unseen domains. Our work differs from these prior arts in several crucial ways: we leverage pre-trained models instead of learning separate domain prototype network as in [14], utilize more domain-rich feature space compared to [64], and do not rely on domain labels as in [6, 14]. Diffusion models for domain generalization. Prior works [21, 22, 26, 76] use text-to-image diffusion models as data augmentation tool by generating diverse synthetic samples with variations that help models generalize better to unseen domains. However, these techniques rely on finetuning the diffusion model, expensive data augmentation steps, or access to the test data. By contrast, to the best of our knowledge, we are the first to investigate using frozen pre-trained diffusion features in an unsupervised manner for domain generalization. 3. Approach First, we introduce the preliminaries of diffusion models (Sec. 3.1) and domain generalization (Sec. 3.2). Then, we present our two-step framework where we first learn pseudo-domain representations in an unsupervised manner and use them to adapt classifier to unseen domains (Sec. 3.3). We stress that we do not have domain label information during both training and test phases. 3.1. Preliminaries on Diffusion Models Diffusion models [24, 58] are probabilistic generative models designed to learn the data distribution through an iterative denoising process. In the forward diffusion process, an image is incrementally corrupted with noise (ϵ) over timesteps, resulting in sequence of increasingly noisy images {xt}T t=1. In the reverse process of iterative denoising, model θ, predicts the added noise ϵθ(xt, t) at each timestep t. Latent Diffusion Models [56] (LDM) extend this framework by operating on latent representation of the image instead of directly in its high-dimensional pixel space. This latent representation is obtained by mapping the image into lower-dimensional space using variational autoencoder [31] with an encoder and decoder D. The diffusion process models the distribution of these lower-dimensional latent embeddings, enabling more efficient computation. The training objective is: LLDM = EE(x),t,ϵN (0,1)ϵ ϵθ(zt, t)2 3.2. Domain Generalization Let and be random variables denoting input and target labels respectively, and Φ feature extractor. In supervised learning, predictor is learnt to map feature representations of inputs X, i.e., Φ(x) to labels , such that generalizes to unseen test samples. We denote this as (Φ(x)) y. Domain generalization is an extension of supervised learning, where training data from multiple domains is available and the goal is to learn predictor that performs well on samples from an unseen test domain [5]. As in conventional domain generalization framework, each domain is characterized by probability distribution Pd defined over and . The training dataset is cond }dtr structed by sampling dtr domains, denoted as {P tr d=1, and collecting nd labeled points from each domain, forming the dataset (cid:83)dtr )}nd i=1. The unseen test domain distribution is denoted as te , from which nT unlabeled points {xdte i=1 are sampled during evaluation. One popular approach for domain generalization is to learn universal classifier on all training samples [67] that d=1{(xd }nT , yd Figure 2. Training Pipeline. The green-shaded region represents the clustering and transformation step. Green solid arrows indicate gradient flow, while red arrows represent non-gradient operations. The feature extractor Ψ first clusters samples to compute the pseudo-domain centroids. The transformation function then transforms these centroids to the latent space of Φ, producing transformed pseudo-domain centroids, which are concatenated with the features from Φ, and sent to the classifier. is agnostic to the underlying domains. However, this algorithm makes strong assumption that all training samples are drawn from single, unified distribution and minimizes the average risk across them. Though simple and effective, this may not guarantee good performance, especially when the test domain lies further from the assumed unified distribution or when the training domains themselves have very high variance [14]. To address this, motivated by findings in [6, 14, 43, 64] which leverage domain-specific representations, we complement input features with these representations. We hypothesize that augmenting input features with rich, complementary information about (pseudo) domains would make the overall feature space more robust to diverse domain variations. Control experiment using ground truth domain labels: To validate the above hypothesis, we conduct the following control experiment. We assume access to ground truth domain labels, cluster diffusion features explicitly into each domain, and compute cluster centroids. Next, we augment the input features (Φ(x)) by concatenating them with the cluster centroids and train classifier on them. On popular domain generalization benchmark OfficeHome [68], we achieve boost of 3% over the strongest baseline. We acknowledge that the number of pseudo-domains we learn per dataset in GUIDE (Sec. 3.3) is different from the true domains present in each dataset. Yet, this controlled setup highlights that augmenting feature space with domainspecific representations from seen domains yields an overall generalizable feature space for unseen domains. Though the standard domain generalization framework assumes access to domain labels during training, in certain applications, this information may be unavailable or incorrect. Thus, we design robust algorithm to learn this complementary pseudo domain information, described next. 3.3. Adaptive Domain Generalization Learning pseudo-domain representations: In the absence of true domain labels, we adopt an unsupervised method Dataset PACS [33] VLCS [15] TerraIncognita [3] (TI) OfficeHome [68] (OH) DomainNet [50] (DN) Synth-Artists Synth-Photography Domain Shift Type Style and texture variations Dataset-specific biases Environmental and background shifts Low-level style differences Style, texture, and complexity differences Synthetic artistic styles Synthetic photographic styles Example Monochromatic Sketch, color-rich Art Painting Spatial composition and object size variations (e.g., objects in Caltech are centered) Location-specific foliage and terrain variations clipart has bold edges, real with softer, natural edges Quickdraw has simplistic and often incomplete outlines, Sketch features more refined strokes with shading Variations in artist techniques, color schemes, and brushwork Changes in lighting, contrast, focus Table 1. Overview of domain shifts in each dataset, including low-level and global photographic style variations, environmental, and dataset-specific biases. Example images for each dataset in suppl. material. }dtr called Kernel Mean Embeddings (KME) [5, 44] to capture key statistical properties of domain. KMEs offer an efficient way to summarize and represent probability distribution into single, representative feature vector. In our case, given the probability distributions of the training domains {P tr d=1, we use the feature extractor Ψ to compute feature representations for samples drawn from each tr . Then, we apply K-Means++ clustering and obtain clusters as way to capture the underlying domain structures. Given we lack information about the true number or nature of the underlying domains during training in our setup, we refer to these clusters as pseudo domains. The centroid of each cluster (cid:98)Ψk, for is used as the compact representation of each pseudo-domain. Finally, we assign each training sample to its nearest cluster, such that its pseudo-domain feature representation is (cid:98)Ψx = (cid:98)Ψk, the centroid of the corresponding pseudo domain. We study the impact of different feature extractors Ψ in Sec. 4.1 and 4.3. We show how clustering smooths out any noise or samplespecific variations and creates more stable (pseudo) domain representations in Sec. 4.4. Leveraging pseudo-domain representations: We take inspiration from ERM [67] and learn single universal classifier on all training domains, with one key difference: we augment each input feature vector with its corresponding pseudo-domain representation. Specifically, we first apply transformation function on the pseudo-domain representations to bring the latent manifold of Ψ closer to Φ to mitigate feature domain drift, i.e., : Ψ (cid:55) Φ. Then, we concatenate the input feature vector Φ(x) with its corresponding pseudo-domain representation ( (cid:98)Ψk) during training, to learn domain-adaptive classifier (as introduced in Dubey et al. [14]). At test time, we first process the input through Ψ, then assign it to the nearest cluster centroid learned during training, and finally apply before passing through the classifier. We stress that in our setup, we do not assume access to domain information during training and make no assumptions about the test domains. 4. Experiments We outline the implementation details and training setup for GUIDE in Sec 4.1, followed by detailed analysis of the capability of different feature extractors (Ψ) in capturModel Source Feature Dimension ResNet-50 CLIP (ViT-L/14) DINOv2 (ViT-L/14) MAE (ViT-L/14) SD-2.1 DiT-XL/2Global Average Pooling (GAP) at layer 49 CLS token Mean over patch tokens Mean over patch tokens Mean over channels of up t1 layer [30] Mean over tokens of block 14 [30] 2048 1024 1024 1024 1280 1152 Table 2. Feature extraction details from each model. SD-2.1 features are conditioned on an empty text prompt. ing domain-specific information to augment class-specific features (Φ) in Sec 4.3. We empirically show how our approach leads to more domain generalizable classifier on unseen test domains and the role of clustering in Sec. 4.4. 4.1. Implementation Details Datasets: We conduct our experiments on 7 datasets, summarized in Table 1. Five of these datasets (PACS, VLCS, TerraIncognita, OfficeHome, DomainNet) are part of the DomainBed [17] test bed. We present details of SynthArtists, and Synth-Photography in Sec. 4.5. Training Setup: We use the default hyper parameter settings from DomainBed [17]: batch size of 32 per domain, learning rate of 5 105, number of steps as 5001, no dropout in the backbone model, and weight decay of 0 on 1 A6000 GPU. We report test accuracies using the leaveone-domain-out cross-validation methodology [17], and average the results obtained over 3 trial seeds. Choice of Φ: We use ResNet-50 [18], initialized with AugMix [23] pre-trained weights as in DomainBed [17]. Choice of Ψ: We study the feature spaces from several vision encoders with varied pre-training objectives: crossentropy loss-based ResNet [18], contrastive loss-based CLIP [53], distillation-based loss in DINOv2 [47], and reconstruction of masked patches loss-based MAE [20]. We further study two diffusion model architectures: the convolutional UNet-based [57] Stable Diffusion 2.1 (SD-2.1) [56] and transformer-based DiT-XL-2-512 (DiT) [49]. Though the underlying pre-training objective is the same for diffusion models, we aim to study the influence of the underlying diffusion architecture on the learnt feature landscape. We provide details on the layers from which the features are extracted in Table. 2. Choice of and cluster refinement schedule: To adapt the pseudo-domain representations as Φ evolves during 4 training, we define (Sec 3.3) as radial basis function (RBF) kernel ridge regressor (more in suppl. material). RBF kernels are well-known for their ability to model nonlinear, distance-based relationships and have been effectively used to align second-order statistics between source and target distributions [77]. In our approach, maps the centroid (cid:98)Ψk of given pseudo-domain to the mean of the features Φ(x) of the samples belonging to cluster k. We employ logarithmic schedule [64] to periodically apply on (cid:98)Ψk, starting with frequent updates and progressively reducing their frequency and thus the overall computational overhead. We note that clustering is done only once on the static Ψ-feature space, but the refinement follows logarithmic schedule. Number of Pseudo-Domains: For GUIDE, the number of clusters (K) is the sole hyper-parameter. We follow simple heuristic from Thomas et al. [64] to determine this: = max(cid:0){1, 3, 5} nc, 200(cid:1), where nc represents the number of classes in the dataset. The upper-bound of 200 clusters helps prevent over-clustering. The number of clusters that yields the best test accuracy for each domain is used to report the scores in Table 6. Evaluation of domain separability: With motive to measure expressivity [14] of the underlying pseudo-domain representations, we measure normalized mutual information In our setup, let (NMI) as done in prior works [43, 64]. and be random variables that denote pseudo domain labels and ground truth domain (or class) labels. NMI is defined as: NMI(U, ) = 2 I(U, ) H(U ) + H(V ) , where I(U, ) is the mutual information between and and H(U ), H(V ) their respective entropies. NMI measures how well the discovered clusters match the ground truth domain or class labels. In our setup, feature space that yields clusters having high domain-NMI score is an ideal candidate to complement existing class-specific features. 4.2. Underlying Domains in Each Dataset We begin by summarizing the types of domain shifts present in the datasets we study (described in Table 1). PACS [33] image dataset captures 7 object categories and 4 domains: real-world photos, art paintings, cartoons, and sketches. Thus, the domains have stark visual distinctions driven by both global and local changes such as shapes, colors, and edges. VLCS [15] is curated from different datasets, making dataset-specific biases such as spatial composition and object size variations as different domains. OfficeHome [68] similar to PACS also has images belonging to four domains: artistic, clip-art, product catalog, and realworld images. Thus, while there is some overlap in the underlying structural characteristics of the objects across Dataset DiT SD-2.1 MAE CLIP DINOv2 RN50 PACS VLCS TerraInc OfficeHome DomainNet Synth-Artists Synth-Photography 0.85 0.58 0.22 0.25 0.54 0.89 0. 0.82 0.26 0.55 0.28 0.51 0.86 0.43 0.71 0.20 0.21 0.10 0.52 0.75 0.31 0.54 0.01 0.01 0.12 0.32 0.25 0.17 0.55 0.05 0.01 0.38 0.47 0.34 0.23 0.59 0.22 0.25 0.08 0.46 0.63 0.33 Table 3. Comparison of domain NMI scores across datasets. The highest domain NMI score depends both on the type of pre-training feature space and the underlying domain shifts in the dataset as noted in Sec 4.3. We note that inherent domain label noise can impact domain NMI scores. Thus, NMI is more valuable when used as relative measure rather than an absolute indicator of domain separability. Dataset DiT SD-2.1 MAE CLIP DINOv2 RN50 PACS VLCS TerraInc OfficeHome DomainNet 0.08 0.12 0.32 0.16 0.16 0.08 0.15 0.35 0.22 0.20 0.11 0.17 0.32 0.28 0. 0.05 0.01 0.01 0.10 0.13 0.15 0.11 0.16 0.23 0.19 0.29 0.39 0.30 0.59 0.36 Table 4. Comparison of class NMI scores across datasets. In order to choose auxiliary features for domain separation, feature space that yields lower class NMI score along with high domain NMI is desirable, i.e. the latent space should favor grouping domains over object classes. Note that Synth-Artists and Synth-Photography datasets are omitted here as they do not have predefined class labels. domains, the domain shifts primarily involve style differences such as variations in texture, color, and outlines. TerraIncognita [3] consists of images taken from different camera trap locations, and each camera serves as domain. Thus, the domain shifts are driven by physical environmental aspects such as variations in foliage density, terrain patterns, and spatial patterns of vegetation. DomainNet [50] is composed of six domains such as quick-draw, infographic, real images, and so on, and exhibits broader range of domain shifts than PACS, spanning both coarse and finegrained variations. For example, the quickdraw domain consists of simple, rough sketches, while sketch has more detailed drawings with shading and varied strokes, showing style differences. By contrast, real domain captures fully detailed images, indicating shifts of varied granularities between different domains. 4.3. Effect of the Choice of Ψ on Domain Separation Next, we study how different pre-training objectives affect the separation of domain-specific signals using domain NMI () (introduced in Sec. 4.1), which measures how well domains are separated in the latent space. We acknowledge that all models are of varied architectural complexities, trained on very different datasets, thereby making it nonviable to concretely isolate the cause of performance discrepancies in domain separation. Nevertheless, we believe our below analysis is valuable to understand the semantic information captured by different pre-training objectives. 5 ResNet-50 [18] (RN50) is pre-trained on ImageNet [13] using discriminatory cross-entropy classification loss. Consequently, the feature space evolves to aid object discrimination, making samples from the same class cluster together across domains. This is evident in the relatively low domain NMI scores  (Table 3)  and high class NMI across all datasets  (Table 3)  , e.g., class NMI of 0.29 compared to 0.08 by DiT features on the PACS dataset. CLIP [53] is pre-trained on internet-scale, noisy imagetext pairs using contrastive loss that aligns images with their textual descriptions in joint embedding space. This objective prioritizes high-level semantic similarity, making CLIPs feature space more representative of global semantics and overall context of the image instead of objectspecific details. Consequently, images of the same object may not form tight clusters if their captions emphasize different contextual attributes (e.g. dog on beach vs. golden retriever indoors). Thus, CLIP, though rich in broader contextual semantics, yields low class and domain NMI scores across all datasets in (Tables 3 and 4). DINOv2 [47] is self-supervised vision transformer trained by aligning representations between student and teacher network, across global and local crops of an image. This encourages the model to capture primarily low-level features, while also capturing global relationships to some extent [29, 47, 65]. By enforcing consistency across augmentations, DINOv2 preserves low-level features that remain invariant to these transformations. Thus DINO-v2 features are particularly effective for datasets like OfficeHome (domain NMI of 0.38 in Table 3), where domain shifts are driven by low-level style differences such as bold outlines in the clipart domain vs softer, natural edges in the real domain (example images in suppl. material). By contrast, DINOv2 performs poorly on VLCS (domain NMI of 0.05 in Table 3), likely due to its over-reliance on lowlevel features, making it less effective at capturing highlevel dataset-specific biases in VLCS, such as differences in spatial composition and object size variations. Masked Autoencoders [20] (MAEs) are pre-trained using masking objective, where the model learns to reconstruct locally masked patches of an image. We conjecture that by reconstructing small, local patch details, MAEs pretraining objective may introduce strong locality bias, and fail to capture global image context, as studied in [38, 79]. We hypothesize that this lack of global understanding limits the capability of MAEs to offer complimentary domainspecific representations. This is evident in their relatively high class-NMI scores (as seen in Table 4) and low domainNMI scores (as seen in Table 3) across most datasets. However, MAEs achieve relatively high domain NMI scores on PACS (0.71) and DomainNet (0.52) leveraging the visual information from local details such as textures, shading, and brushstrokes. We note similar trend with DINOv2 which Figure 3. T-SNE visualization of how pseudo-domains are clustered together in the latent space of DiT for PACS. Note how the sketch domain forms distinct clusters, with light and dark pencil strokes mapped to separate regions in the latent space. Best viewed in color. also captures rich local features. This may explain why both models perform better in separating domains driven by low-level visual variations  (Table 3)  . However, MAEs perform poorly on TerraIncognita despite its reliance on local features. Unlike PACS, we think that the domain shifts in TerraIncognita require an understanding of both local and global spatial understanding (e.g., vegetation density, terrain patterns), potentially leading to lower domain NMI. Conclusion: This indepth analysis indicates that comprehending different pre-training objectives is essential to maximize profit from their latents for domain separation."
        },
        {
            "title": "Diffusion models for domain separation",
            "content": "Next, we focus exclusively on diffusion architectures and closely study the impact of some of their architectural design choices on domain separation. As discussed in Sec. 3.1, during diffusion model pre-training, noise added to an image is iteratively removed using pixel reconstruction loss. Recent studies [48, 52] have indicated that this makes the model first capture broad structural patterns before encoding finer details. We hypothesize that this implicit hierarchical feature learning indirectly induced by the denoising objective enables diffusion models to encode global structures and fine-grained variations, assisting faithful image reconstruction. Moreover, since the generative objective is entirely agnostic to class labels, we posit that there is no incentive to group features based on classdiscriminative signals. Perhaps this lack of class-driven objective allows domain-specific variations to emerge more prominently in the latent space. This is reflected in Table 3 where we observe that diffusion features achieve high domain NMI scores across most datasets compared to their non-diffusion counterparts. Figures 4, and 3 further reinforces this observation and illustrates how different clusters (pseudo-domains) in the diffusion latent space capture domain-specific class-agnostic variations. Within the family of diffusion feature space, we now in6 (a) Animal portraits (b) Oil paintings (c) Similar color schemes Figure 4. Pseudo-domains captured in the diffusion latent space of DiT on PACS. The clusters group images based on nuanced style-specific variances rather than class-specific variances. spect if transformer based DiT and U-Net based SD-2.1 behave differently for the task of domain separation. We acknowledge that both models are trained on very different datasets which makes this analysis more challenging. DiT [49]: Following the analysis in Kim et al. [30], we extract features from the 14th (out of 28) block of the transformer architecture of the DiT model, at timestep t=50 (more in suppl. material). As noted in [30], by attending to the entire image, DiTs self-attention mechanism effectively captures global context, making it capable at distinguishing high-level semantics and stylistic differences (e.g, pencil sketches vs paintings). This proves advantageous on datasets like PACS [33] which comprises domains with varied global context (detailed in Sec. 4.2), where DiT achieves the highest domain NMI of 0.85  (Table 3)  . SD-2.1 [56]. We extract features from the second upsampling layer of the U-Net (denoted as up ft:1) in SD-2.1 at timestep t=50 (more in suppl. material). As noted in [30], these features are rich in fine-grained visual information, with convolutional-based U-Net [57] of SD-2.1 [56] capturing local spatial information [30, 66]. As result, we observe that SD-2.1 and DiT exhibit complementary strengths. This is particularly evident on the TerraIncognita dataset, where SD-2.1 achieves the highest domain NMI of 0.55  (Table 3)  . This is likely due to SD-2.1s ability to capture fine-grained spatial features such as foliage density and terrain patterns, which define the domain shifts for this dataset (as described in Sec 4.2). By contrast, we find that DiT struggles with domain separation on TerraIncognita, achieving lower domain NMI of 0.22. On the other hand, on VLCS [15] where each domain represents dataset-specific biases described in Sec. 4.2, we note that DiT (domain NMI: 0.58) outperforms SD-2.1 (domain NMI: 0.26). This highlights DiTs strength to capture global context. Interestingly, SD-2.1s bottleneck layer achieves higher domain NMI of 0.45 compared to up ft:1s score of 0.26. This aligns with the findings from Kim et al. [30] that UNets bottleneck layer captures coarser, more global features, compared to up ft:1. From Table 3, we observe that OfficeHome [68] proves 7 Dataset DiT SD-2.1 RN50 CLIP DINOv2 MAE ERM VLCS PACS OH TI Avg 78.5 87.1 68.4 48.2 70.6 77.0 86.9 68.6 51.3 71.0 76.3 84.8 65.7 49. 69.1 76.8 84.7 64.6 47.4 68.4 77.3 84.9 68.3 48.4 69.7 76.4 84.6 65.2 50. 69.1 76.6 83.8 67.2 47.0 68.7 Table 5. Domain generalization performance on PACS and TerraIncognita (TI). The pseudo-domain representations obtained from the latent space of diffusion models provide the highest gains in accuracy, while those from CLIP yield minimal accuracy gains. to be challenging for both DiT (domain NMI: 0.25) and SD-2.1 (domain NMI: 0.28). Upon inspection, we found that samples from the real domain visually look similar to those from both product and art in the feature spaces (suppl. material for visual examples), potentially contributing to low domain separation. On DomainNet [50], we observe moderate domain NMI scores for all pre-training objectives (except for CLIP, as discussed in Sec 4.3), with DiT achieving the highest score of 0.54, in Table 3. We attribute this to the diverse nature of domain shifts in DomainNet, which include both highand low-level variations (described in Sec. 4.2). We believe that this variability makes it challenging for models to fully leverage their distinct strengths, as no single model seems to effectively capture all domain-specific characteristics. Conclusion: This analysis reveals that for the same pretraining objective (diffusion denoising), the underlying architecture and the specific layer for feature extraction plays crucial role in shaping the latent space, thereby performance on the downstream tasks. 4.4. Domain Generalization Performance In this section, we compare GUIDE against prior domain generalization methods and examine the impact of different feature extractors (Ψ) in capturing domain-specific information to enhance classification performance. Choice of Ψ on domain generalization: Building on our findings in Sec. 4.3, we test the utility of different feature spaces for domain separation and generalization against ERM [67], strong baseline that has been shown by Gulrajani and Lopez-Paz [17] to outperform many domain generalization algorithms. We evaluate on the DomainBed test suite, which comprises PACS [33], VLCS [15], OfficeHome [68], TerraIncognita [3], and DomainNet [50]. From Table 5, we note that diffusion features consistently outperform their non-diffusion counterparts on all datasets. Notably, DiT and SD-2.1 achieve highest accuracy while the rest show only marginal gains over ERM. CLIP seems to yield minimal gains on average on this task limiting its ability to be used as is. GUIDE-DiT yields an average accuracy improvement of 1.9% over ERM and performs best on VLCS (+1.9%) and PACS (+3.3%). On the other Algorithm VLCS PACS OH TI DN Avg Dataset ERM / ERM++ GUIDE / GUIDE++ (DiT) GUIDE / GUIDE++ (SD-2.1) uses multilayer features uses domain labels - - ERM [67] MLDG [34] MMD [35] CORAL [60] SagNet [46] DANN [16] Fishr [54] MIRO [10] Mixup [73, 74] LatentDR (SA) [40] LatentDR (Pool) [40] DA-ERM ([14]) AdaClust ([64]) GUIDE-DiT (ours) GUIDE-SD-2.1 (ours) GUIDE-BEST (ours) 76.6 77.2 77.5 78.8 77.8 78.6 77.8 79.0 77.4 78.7 78. 78.0 78.9 78.5 77.0 78.5 83.8 84.9 84.7 86.2 86.3 83.6 85.5 85.4 84.6 85.8 86.3 84.1 87.0 87.1 86.9 87.1 67.2 47. 44.1 66.8 66.3 68.7 68.1 65.9 67.8 70.5 68.1 69.0 68.4 67.9 67.7 68.4 68.6 68.6 47.7 42.2 47.6 48.6 46.7 47.4 50.4 47.9 49.9 49.5 47.3 48.1 48.2 51.3 51.3 41.2 23.4 41.5 40.3 38.3 41.7 44.3 39.2 45.1 43. 43.6 43.6 45.8 45.9 45.9 63.7 63.6 58.8 64.5 64.2 62.6 64.0 65.9 63.4 65.7 65.2 64.1 64.9 65.6 65.9 66.3 Table 6. Comparison of GUIDE with other domain generalization algorithms on 5 datasets: utilizing the DomainBed test bed. The methods are categorized based on (1) whether they operate across multiple intermediate layers in the network and (2) whether they require explicit ground truth domain labels during training. The highest-performing method that does not rely on either is underlined. The overall best-performing method is in bold. Methods in cyan corresponds to domain-adaptive classifiers (described in Sec. 3.3). Among those methods we find GUIDE achieves the highest performance. GUIDE-BEST reports the best performance among the two diffusion latent spaces (DiT and SD-2.1) for easy reading. hand, GUIDE-SD-2.1 outperforms on TerraIncognita, beating ERM by +4.3%. These results are inline with the analysis and domain NMI scores in Table 3. Comparison with prior art: In Table 6, we compare GUIDE with other state-of-the-art domain generalization algorithms1 and note that GUIDE-BEST achieves the highest average performance of 66.3% without using domain labels at any point. Compared to all methods, GUIDE-BEST shows the largest improvements on the PACS, TerraIncognita, and DomainNet datasets. The significant gains on DomainNet, dataset with over 500, 000 images across 325 classes and 6 domains, highlights GUIDEs ability to scale to larger datasets. Among the domain-adaptive classifier frameworks (bottom rows), GUIDE-BEST outperforms DA-ERM [14] by +2.2% and AdaClust [64] by +1.4%. Notably, the reported scores for most algorithms are obtained after extensive hyperparameter searches, whereas GUIDE achieves these gains with the default setting of DomainBed without using features from multiple layers or ground truth domain labels. Overall, results in Tables 5 and 6 validate our hypothesis that augmenting feature space with rich domain-specific information on seen domains results in an overall generalizable feature space for unseen domains. Effect of enhanced training strategies: We follow the ERM++ [62] implementation from DomainBed [17] which improves ERM by better utilization of training data, model parameter selection, and weight-space regularization techniques. From Table 7, ERM++ improves over standard 1We compare against algorithms reported in [14, 40, 64]. PACS TI 83.8 / 88.0 47.0 / 50.7 87.1 / 89.2 48.2 / 52.7 86.9 / 88.6 51.3 / 53.6 Table 7. ERM++ [62] training strategies on GUIDE boost performance. Figure 5. Example images from Synth-Artists and SynthPhotography, generated using Stable Diffusion XL [51]. Synth-Artists includes artistic styles such as Van Gogh and Kinkade, while the SynthPhotography captures photography effects like Tilt-Shift and Bokeh. ERM by +4.2% on PACS and +3.7% on TerraIncognita. Applying the same strategies to GUIDE, we achieve even greater improvements, with GUIDE++ outperforming ERM by +5.4% on PACS and +6.6% on TerraIncognita. These results show that GUIDE could benefit from any training optimizations proposed over ERM, such as SWAD [9]. Is clustering necessary? With motive to understand the role of clustering of features from Ψ before feature concatenation, we conduct an empirical analysis comparing GUIDE with and without pseudo-domain clustering. To this end, we directly append the raw features Ψ(x) to Φ(x). This results in moderate gain of +1.3 over ERM, whereas clustering improves performance by +3.3 on the PACS dataset. We believe that clustering helps smooth out any noise or sample-specific variations and creates more stable (pseudo) domain representations. Clustering also offers more interpretability to inspect what domain-specific variations are captured in the latent space  (Fig. 4)  . 4.5. Pseudo-domains for Style Discovery Next, we evaluate different pre-training objectives on the task of photographic and artistic style separation. Automatic style identification is valuable for curating and inspecting large-scale datasets, image retrieval, and several such applications. To study this, we first construct two datasets with controlled domain shifts using Stable Diffusion XL [51] (dataset construction details in suppl. material): (i) Synth-Photography features photographic styles such as macro, tilt-shift, bokeh, symmetry, and zoom blur. Thus, the domain shifts are primarily driven by variations in focus, sharpness, edge details, and depth contrasts. (ii) Synth-Artists, captures styles of Van Gogh, Kinkade, Warhol, Rembrandt, and Dali, making the domain shifts 8 more high-level such as brush stroke patterns and color palettes. We show few example images in Fig 5. On Synth-Artists, we observe that DiT achieves better domain separation, with domain NMI score of 0.89  (Table 3)  . By contrast, on Synth-Photography, SD-2.1 performs better, achieving domain NMI score of 0.43 compared to DiTs score of 0.35  (Table 3)  . This finding aligns with our analysis from Sec. 4.3 that DiT seem more apt for global variations and SD-2.1 for finer-grained spatially detailed variations. 5. Discussion and Future Work In this work, we introduce GUIDE, simple yet effective framework that improves generalization to unseen domains in the absence of domain labels during both train and test times. GUIDE learns pseudo-domain representations from pre-trained diffusion models and leverages them for domain generalization. Future work includes exploring ways to combine multiple models and build generalizable latent space that works out of the box for diverse tasks."
        },
        {
            "title": "References",
            "content": "[1] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. Metareg: Towards domain generalization using metaregularization. Advances in Neural Information Processing Systems, 2018. 2 [2] Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko. Label-efficient semantic segmentation with diffusion models. arXiv preprint arXiv:2112.03126, 2021. 2 [3] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the European conference on computer vision (ECCV), 2018. 2, 4, 5, 7, 23 [4] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. theory of learning from different domains. Machine learning, 2010. 2 [5] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to new unlabeled sample. Advances in Neural Information Processing Systems, 2011. 1, 2, 3, [6] Manh-Ha Bui, Toan Tran, Anh Tran, and Dinh Phung. Exploiting domain-specific features to enhance domain generalization. Advances in Neural Information Processing Systems, 2021. 2, 3 [7] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In Proceedings of the European conference on computer vision (ECCV), 2018. 1 [8] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 2020. 1 [9] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. Advances in Neural Information Processing Systems, 2021. 8 [10] Junbum Cha, Kyungjae Lee, Sungrae Park, and Sanghyuk Chun. Domain generalization by mutual-information regIn Proceedings of the ularization with pre-trained models. European conference on computer vision (ECCV), 2022. 2, 8 [11] Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Diffusiondet: Diffusion model for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1983019843, 2023. [12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, 2020. 1 [13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image In Proceedings of the IEEE/CVF conference on database. computer vision and pattern recognition, 2009. 6 [14] Abhimanyu Dubey, Vignesh Ramanathan, Alex Pentland, and Dhruv Mahajan. Adaptive methods for real-world domain generalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021. 2, 3, 4, 5, 8 [15] Chen Fang, Ye Xu, and Daniel Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In Proceedings of the IEEE/CVF international conference on computer vision, 2013. 1, 4, 5, 7, 19 [16] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of machine learning research, 2016. 2, [17] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International Conference on Learning Representations, 2021. 2, 4, 7, 8 [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2016. 1, 4, 6 [19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020. 1 [20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022. 1, 4, 6 [21] Sobhan Hemati, Mahdi Beitollahi, Amir Hossein Estiri, Bassel Al Omari, Soufiane Lamghari, Yasser Khalil, Xi Chen, and Guojun Zhang. Beyond loss functions: Exploring datacentric approaches with diffusion model for domain generalization. Transactions on Machine Learning Research, 2022. 2 [22] Sobhan Hemati, Mahdi Beitollahi, Amir Hossein Estiri, Bassel Al Omari, Xi Chen, and Guojun Zhang. Cross domain generative augmentation: Domain generalization with latent diffusion models. arXiv preprint arXiv:2312.05387, 2023. 2 [23] Dan Hendrycks, Norman Mu, Ekin Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: simple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019. 4 [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 2020. 2, [25] Minui Hong, Jinwoo Choi, and Gunhee Kim. Stylemix: Separating content and style for enhanced data augmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021. 2 [26] Yuyang Huang, Yabo Chen, Yuchen Liu, Xiaopeng Zhang, Wenrui Dai, Hongkai Xiong, and Qi Tian. Domainfusion: Generalizing to unseen domains with latent diffusion models. In Proceedings of the European conference on computer vision (ECCV), 2025. 2 [27] Zeyi Huang, Haohan Wang, Eric Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In Proceedings of the European conference on computer vision (ECCV), 2020. 2 10 [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representaIn International tion learning with noisy text supervision. conference on machine learning, 2021. 1 [29] Dongsheng Jiang, Yuchen Liu, Songlin Liu, Jine Zhao, Hao Zhang, Zhen Gao, Xiaopeng Zhang, Jin Li, and Hongkai Xiong. From clip to dino: Visual encoders shout arXiv preprint in multi-modal large language models. arXiv:2310.08825, 2023. [30] Dahye Kim, Xavier Thomas, and Deepti Ghadiyaram. Revelio: Interpreting and leveraging semantic information in diffusion models. arXiv preprint arXiv:2411.16725, 2024. 2, 4, 7, 15 [31] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. 3 [32] Alexander Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly zero-shot classifier. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22062217, 2023. 2 [33] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, 2017. 4, 5, 7, 17 [34] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for domain generalization. In Proceedings of the AAAI conference on artificial intelligence, 2018. 8 [35] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2018. 8 [36] Jingjing Li, Zhiqi Yu, Zhekai Du, Lei Zhu, and Heng Tao Shen. comprehensive survey on source-free domain adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 1 [37] Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy Hospedales. simple feature augmentation for doIn Proceedings of the IEEE internamain generalization. tional conference on computer vision, 2021. 2 [38] Feng Liang, Yangguang Li, and Diana Marculescu. Supmae: Supervised masked autoencoders are efficient vision learners. arXiv preprint arXiv:2205.14540, 2022. 6 [39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [40] Ran Liu, Sahil Khose, Jingyun Xiao, Lakshmi Sathidevi, Keerthan Ramnath, Zsolt Kira, and Eva Dyer. Latentdr: Improving model generalization through sample-aware laIn Proceedings of the tent degradation and restoration. IEEE Winter Conference on Applications of Computer Vision, 2024. 2, 8 [41] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. Advances in Neural Information Processing Systems, 2024. 2 [42] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European conference on computer vision (ECCV), 2018. 1 [43] Toshihiko Matsuura and Tatsuya Harada. Domain generalization using mixture of multiple latent domains. In Proceedings of the AAAI conference on artificial intelligence, 2020. 2, 3, 5 [44] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Scholkopf, et al. Kernel mean embedding of distributions: review and beyond. Foundations and Trends in Machine Learning, 2017. 1, 4 [45] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021. 2 [46] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021. 8 [47] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 1, 4, [48] Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, and Youngjung Uh. Understanding the latent space of diffusion models through the lens of riemannian geometry. Advances in Neural Information Processing Systems, 2023. 6 [49] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 1, 4, 7 [50] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE international conference on computer vision, 2019. 4, 5, 7, 25 [51] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 8, 27 [52] Yurui Qian, Qi Cai, Yingwei Pan, Yehao Li, Ting Yao, Qibin Sun, and Tao Mei. Boosting diffusion models with moving In Proceedings of average sampling in frequency domain. the IEEE/CVF conference on computer vision and pattern recognition, 2024. 6 [53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 1, 4, [54] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Invariant gradient variances for out-of-distribution In International conference on machine Fishr: generalization. learning, 2022. 8 11 [69] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-toimage generation. arXiv preprint arXiv:2303.09522, 2023. 2 [70] Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu, and Xinlong Wang. Diffusion feedback helps clip see better. arXiv preprint arXiv:2407.20171, 2024. 2 [71] Weijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao, Yefei He, Hong Zhou, Mike Zheng Shou, and Chunhua Shen. Datasetdm: Synthesizing data with perception annotations using diffusion models. In Advances in Neural Information Processing Systems, 2023. [72] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panopIn tic segmentation with text-to-image diffusion models. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023. 2 [73] Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and Wenjun Zhang. Adversarial domain adaptation with domain mixup. In Proceedings of the AAAI conference on artificial intelligence, 2020. 8 [74] Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. arXiv preprint arXiv:2001.00677, 2020. 8 [75] Xueting Yan, Ishan Misra, Abhinav Gupta, Deepti GhadiImproving genyaram, and Dhruv Mahajan. Clusterfit: In Proceedings of eralization of visual representations. the IEEE/CVF conference on computer vision and pattern recognition, 2020. 1 [76] Runpeng Yu, Songhua Liu, Xingyi Yang, and Xinchao Wang. Distribution shift inversion for out-of-distribution prediction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023. 2 [77] Yun Zhang, Nianbin Wang, Shaobin Cai, and Lei Song. Unsupervised domain adaptation by mapped correlation alignment. IEEE Access, 2018. [78] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing text-to-image diffusion In Proceedings of the IEEE models for visual perception. international conference on computer vision, 2023. 2 [79] Xie Zhenda, Geng Zigang, Hu Jingcheng, Zhang Zheng, Hu Han, and Cao Yue. Revealing the dark secrets of masked image modeling. arXiv preprint arXiv:2205.13543, 2022. 6 [55] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imIn International conference on machine learning, agenet? 2019. 1 [56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, 2022. 1, 3, 4, 7 [57] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention, 2015. 4, 7 [58] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning. PMLR, 2015. 2, 3 [59] Nathan Somavarapu, Chih-Yao Ma, and Zsolt Kira. Frustratingly simple domain generalization via image stylization. arXiv preprint arXiv:2006.11207, 2020. [60] Baochen Sun and Kate Saenko. Deep coral: Correlation In Proceedings of alignment for deep domain adaptation. the European conference on computer vision (ECCV), 2016. 2, 8 [61] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. Advances in Neural Information Processing Systems, 2020. 1 [62] Piotr Teterwak, Kuniaki Saito, Theodoros Tsiligkaridis, Kate Saenko, and Bryan Plummer. Erm++: An improved baseline for domain generalization. arXiv preprint arXiv.2304.01973, 2023. 2, 8 [63] Piotr Teterwak, Kuniaki Saito, Theodoros Tsiligkaridis, Bryan Plummer, and Kate Saenko. Is large-scale pretraining the secret to good domain generalization? arXiv preprint arXiv:2412.02856, 2024. 1 [64] Xavier Thomas, Dhruv Mahajan, Alex Pentland, and Abhimanyu Dubey. Adaptive methods for aggregated domain generalization. arXiv preprint arXiv:2112.04766, 2021. 2, 3, 5, 8 [65] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024. 1, [66] N. Tumanyan, M. Geyer, S. Bagon, and T. Dekel. Plug-andplay diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023. 7 [67] Vladimir Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks, 1999. 2, 3, 4, 7, 8 [68] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network In Proceedings of for unsupervised domain adaptation. the IEEE/CVF conference on computer vision and pattern recognition, 2017. 3, 4, 5, 7, 21 12 Supplementary Material: Whats in Latent? Leveraging Diffusion Latent Space for Domain Generalization A. Transformation Function Transformation (T ) ERM Direct Concatenation (No Transformation) Cluster-Based Replacement Linear Regression RBF Kernel Ridge Regression Acc 83.8 84.3 84.6 85.7 87. Table 8. Effect of on Test Accuracy for PACS, using GUIDE-DiT. We find that the RBF step (Sec 4.1) aids in classification performance on unseen domains. Effect of the choice of : As noted in Sec. 3.3, we apply transformation function : Ψ (cid:55) Φ to bring the latent manifold of Ψ closer to Φ and mitigate feature domain drift. To understand the role of , we explore the following alternatives to it: (a) Direct concatenation, i.e., appending pseudo-domain representations (from Ψ) to the features (from Φ) without any transformation. While this introduces domain-specific information, lack of alignment between the two feature spaces led to minimal improvement of +0.5% over ERM. (b) Cluster-based replacement, where pseudo-domains identified in the Ψ space are used to compute cluster centroids using features from Φ space, i.e. cluster samples are averaged in Φ space. This provides slightly better alignment yielding an accuracy gain of +0.8% over the baseline. (c) Linear regression, where linear mapping is learned between the pseudo-domain centroids and the centroids obtained in (b). This helps in bridging differences between Ψ and Φ better, leading to larger improvement of +1.4%. (d) RBF kernel ridge regression, where the linear regressor in (c) is replaced with an RBF kernel (Sec 4.1). We note that this achieves the highest accuracy gains of +3.3%, highlighting its effectiveness of bridging feature domain drift while incorporating pseudo-domain information into the classifier. These results underscore the necessity of well-chosen transformation to fully leverage the pseudo-domain information. B. Domain Predictability Dataset DiT SD-2.1 MAE CLIP DINOv2 RN PACS VLCS TerraInc OfficeHome DomainNet Synth-Artists Synth-Photography 98.89 96.08 99.97 89.16 88.55 100 83.33 98.95 92.72 99.94 86.43 89.58 99.00 87.50 98.69 94.03 99.91 82.55 87.50 97.00 86.67 98.29 83.87 99.83 83.41 87.61 92.00 73.33 98.89 81.86 99.87 78.28 87.24 90.00 78. 97.85 88.48 99.79 77.52 87.21 97.00 77.50 Table 9. Comparison of Domain Predictability Scores Across Datasets. Diffusion models consistently outperform other models in domain predictability scores, highlighting the effectiveness of encoding domain-specific information in their latent space. Domain Predictability: To complement NMI, we evaluate domain predictability and predict domain labels from latent feature representations. Specifically, we use single-layer MLP classifier, trained on an 80-20 train-test split. We report the mean test accuracy over 3 such random splits. While NMI measures alignment and variance across samples belonging to domain, domain predictability directly assesses latent representations ability to learn to classify domain information. We observe in Table. 9 that diffusion models attain the highest domain predictability scores, highlighting their effectiveness in encoding domain-specific information. 13 C. Label Noise and Domain Inconsistencies Figure 6. Examples of inconsistent or confusing domain labels. Given that most datasets in this study are web-scraped, we expect there to be label noise and domain inconsistencies which may impact the NMI scores. These examples from the PACS dataset and SD-2.1 feature space illustrate cases where domain assignments may be unclear or conflicting. The color of the border on the images denotes the ground truth domain label. 14 D. Effect of Text-Conditioning in SD-2.1 for Domain Separation Dataset Domain NMI Domain Predictability Empty Prompt Prompt Empty Prompt Prompt PACS OfficeHome 0.82 0.22 0.85 0.24 98.95 86.43 99.51 92.91 Table 10. Domain NMI and predictability scores for empty vs text conditioned prompts for SD-2.1 on PACS and OfficeHome. For text conditioning we used the prompt: photo of an object in the style of {domain}. Similar to the findings of Kim et al. [30], text conditioning appears to activate more relevant features. E. Effect of Layer and Timestep in Diffusion Models for Domain Separation (DiT vs SD-2.1) on PACS, and VLCS Following Kim et al. [30], we choose lower noise level at timestep (t=50), with motivation to capture rich fine-grained visual information. We use t=50 for both DiT (at block 14) and SD-2.1 (at up ft:1) for both class and domain NMI scores (in Tables 3, and 4), and to obtain the classification accuracies in Table 6. In Fig. 7, we observe that t=50 provides the highest domain NMI score for PACS using DiT. We also note that on VLCS, the bottleneck layer outperforms the domain NMI score obtained from up ft:1 in Fig. 8, likely due its focus on coarse-grained features as noted in [30]. Figure 7. Domain NMI comparison across layers and timesteps for PACS. Top: Domain NMI scores for SD-2.1 layers (best: up ft:1) and DiT blocks (best: block:14). Bottom: Domain NMI scores across various denoising timesteps for SD-2.1 and DiT on PACS. Figure 8. Domain NMI comparison across layers for VLCS. The Bottleneck Layer of Stable Diffusion (SD-2.1) which capture more coarse-grained features aids in separating high-level domain shifts in VLCS. However, DiTs superior capability to capture global context via self-attention outperforms the domain NMI scores at bottleneck and up ft:1. F. GUIDE Pseudo-code Algorithm 1 Training Pseudocode with RBF Kernel Ridge Regression Input: Training data Dtr, transform schedule Ttransform, K: #clusters Output: Fimage(.; ω), FMLP(.; W), mapping Initialize: Compute feature representations Ψ, Φ, initialize model parameters ω0, W. {ψk}, {Dk} CLUSTERING(Ψ, K) for = 1 to do if Ttransform then (cid:80) Dk xDk Φ(x) (ψx) For each k: (cid:98)Φk = 1 Compute pairwise distances ψi ψj 2, = γ 1/(2 median(pairwise distances)2) {using median heuristic} Fit via RBF Kernel Ridge Regression using { (cid:98)ψk} (cid:55) { (cid:98)Φk} and γ ψ end if for batch (x, ψx, y) in Dtr do Φ(x) Fimage(x; ωt) ψ (cid:1); Wt ˆy FMLP Update ωt+1, Wt+1 via SGD STEP on = CROSSENTROPY(ˆy, y) (cid:0)CONCAT(cid:0)Φ(x), ψ (ψx) (cid:1) end for end for Return Fimage(.; ωT ), FMLP(.; WT ), and Inference Input: Test data Dtest, transformation function , and centroids { (cid:98)ψk}K Output: Predicted labels ˆy k=1 for Dtest do (ψx) {Apply same RBF transform as in training} ψx NEARESTCENTROID(cid:0)Ψ, x(cid:1) {Find closest cluster in Ψ-space} ψ Φ(x) Fimage(x; ωT ) ˆy FMLP (cid:0)CONCAT(Φ(x), ψ x); WT (cid:1) end for Return ˆy 16 G. Domain Shift Examples and Domain Separation in Feature Spaces In this section, we provide: Example images, i.e. class samples across domains for each dataset. Class vs Domain NMI scores for each feature extractor (Ψ) studied in this work, on each dataset. Feature space visualizations for each feature extractor (Ψ) studied in this work, on the PACS, VLCS, OfficeHome, and TerraInognita datasets. G.1. PACS [33] Figure 9. Class examples across domains in the PACS dataset. Each column represents domain, and each row corresponds to class. Domains art painting, cartoon, photo, sketch Classes dog, elephant, giraffe, guitar, horse, house, person Table 11. 4 domains and 7 classes of the PACS dataset. 17 Figure 10. Class vs Domain NMI scores for PACS. Note how RN50 has the highest class NMI and diffusion models have low class NMI scores. Diffusion models also has the highest domain NMI scores, thereby capturing domain-specific class invariant structures. Figure 11. T-SNE visualization of domain separation for PACS. Each point represents sample, colored by its domain. Notice how well separated the domains are when diffusion features are used compared to other models. 18 G.2. VLCS [15] Figure 12. Class examples across domains in the VLCS dataset. Each column represents domain, and each row corresponds to class. Domains Caltech101, LabelMe, SUN09, VOC2007 Classes bird, car, chair, dog, person Table 12. 4 domains and 5 classes of the VLCS dataset. 19 Figure 13. Class vs Domain NMI scores for VLCS. Note how RN50 has the highest class NMI score, and diffusion models have low class NMI scores. DiT has much higher domain NMI score than SD-2.1, resulting from its stronger capability in capturing high-level dataset-specific biases, as discussed in Sec. 4.3. Figure 14. T-SNE visualization of domain separation for VLCS. Each point represents sample, colored by its domain. Note how the DiT feature space best separate the domains. 20 G.3. OfficeHome [68] Figure 15. Class examples across domains in the OfficeHome dataset. Each column represents domain, and each row corresponds to class. Domains Art, Clipart, Product, Real World Classes Alarm Clock, Backpack, Batteries, Bed, Bike, Bottle, Bucket, Calculator, Calendar, Candles, Chair, Clipboards, Computer, Couch, Curtains, Desk Lamp, Drill, Eraser, Exit Sign, Fan, File Cabinet, Flipflops, Flowers, Folder, Fork, Glasses, Hammer, Helmet, Kettle, Keyboard, Knives, Lamp Shade, Laptop, Marker, Monitor, Mop, Mouse, Mug, Notebook, Oven, Pan, Paper Clip, Pen, Pencil, Post-it Notes, Printer, Push Pin, Radio, Refrigerator, Ruler, Scissors, Screwdriver, Shelf, Sink, Sneakers, Soda, Speaker, Spoon, TV, Table, Telephone, ToothBrush, Toys, Trash Can, Webcam. Table 13. 4 domains and 65 Classes of the OfficeHome dataset. 21 Figure 16. Class vs Domain NMI scores for OfficeHome. Note how RN50 has the highest class NMI score and DINOv2 has the highest domain NMI score, resulting form its stronger ability in capturing low-level style shifts, as discussed in Sec. 4.3. DiT and SD-2.1 have moderate domain NMI scores, with DiT having lower class NMI score. Figure 17. T-SNE visualization of domain separation for OfficeHome. Each point represents sample, colored by its domain. All models struggle to separate the domains in this dataset. The real domain has considerable overlap with the other domains. 22 G.4. TerraIncognita [3] Figure 18. Class examples across domains in the TerraIncognita dataset. Each column represents domain, and each row corresponds to class. Domains Location 100, Location 38, Location 43, Location 46 Classes bird, bobcat, cat, coyote, dog, empty, opossum, rabbit, raccoon, squirrel Table 14. 4 domains and 10 classes of the TerraIncognita dataset. 23 Figure 19. Class vs Domain NMI scores for TerraIncognita. Most models have high class NMI score. SD-2.1 has the highest domain NMI score, resulting from its stronger capability in capturing spatial information, as discussed in Sec. 4.3. Figure 20. T-SNE visualization of domain separation for TerraIncognita. Each point represents sample, colored by its domain. Note how the SD-2.1 feature space best groups samples from the same domain closer together, and separate from other domains. 24 G.5. DomainNet [50] Figure 21. Class examples across domains in the DomainNet dataset. Each column represents domain, and each row corresponds to class. Figure 22. Class vs Domain NMI scores for DomainNet. Note how RN50 has the highest class NMI and diffusion models, and MAE have the highest domain NMI scores, with DiT having lower class NMI score. All models except CLIP exhibit moderate domain NMI score, likely due to the varied domain shifts inherent in the dataset, as discussed in Sec. 4.3. 25 Domains clipart, infograph, painting, quickdraw, real, sketch Classes The Eiffel Tower, The Great Wall of China, The Mona Lisa, aircraft carrier, airplane, alarm clock, ambulance, angel, animal migration, ant, anvil, apple, arm, asparagus, axe, backpack, banana, bandage, barn, baseball, baseball bat, basket, basketball, bat, bathtub, beach, bear, beard, bed, bee, belt, bench, bicycle, binoculars, bird, birthday cake, blackberry, blueberry, book, boomerang, bottlecap, bowtie, bracelet, brain, bread, bridge, broccoli, broom, bucket, bulldozer, bus, bush, butterfly, cactus, cake, calculator, calendar, camel, camera, camouflage, campfire, candle, cannon, canoe, car, carrot, castle, cat, ceiling fan, cell phone, cello, chair, chandelier, church, circle, clarinet, clock, cloud, coffee cup, compass, computer, cookie, cooler, couch, cow, crab, crayon, crocodile, crown, cruise ship, cup, diamond, dishwasher, diving board, dog, dolphin, donut, door, dragon, dresser, drill, drums, duck, dumbbell, ear, elbow, elephant, envelope, eraser, eye, eyeglasses, face, fan, feather, fence, finger, fire hydrant, fireplace, firetruck, fish, flamingo, flashlight, flip flops, floor lamp, flower, flying saucer, foot, fork, frog, frying pan, garden, garden hose, giraffe, goatee, golf club, grapes, grass, guitar, hamburger, hammer, hand, harp, hat, headphones, hedgehog, helicopter, helmet, hexagon, hockey puck, hockey stick, horse, hospital, hot air balloon, hot dog, hot tub, hourglass, house, house plant, hurricane, ice cream, jacket, jail, kangaroo, key, keyboard, knee, knife, ladder, lantern, laptop, leaf, leg, light bulb, lighter, lighthouse, lightning, line, lion, lipstick, lobster, lollipop, mailbox, map, marker, matches, megaphone, mermaid, microphone, microwave, monkey, moon, mosquito, motorbike, mountain, mouse, moustache, mouth, mug, mushroom, nail, necklace, nose, ocean, octagon, octopus, onion, oven, owl, paint can, paintbrush, palm tree, panda, pants, paper clip, parachute, parrot, passport, peanut, pear, peas, pencil, penguin, piano, pickup truck, picture frame, pig, pillow, pineapple, pizza, pliers, police car, pond, pool, popsicle, postcard, potato, power outlet, purse, rabbit, raccoon, radio, rain, rainbow, rake, remote control, rhinoceros, rifle, river, roller coaster, rollerskates, sailboat, sandwich, saw, saxophone, school bus, scissors, scorpion, screwdriver, sea turtle, see saw, shark, sheep, shoe, shorts, shovel, sink, skateboard, skull, skyscraper, sleeping bag, smiley face, snail, snake, snorkel, snowflake, snowman, soccer ball, sock, speedboat, spider, spoon, spreadsheet, square, squiggle, squirrel, stairs, star, steak, stereo, stethoscope, stitches, stop sign, stove, strawberry, streetlight, string bean, submarine, suitcase, sun, swan, sweater, swing set, sword, syringe, t-shirt, table, teapot, teddy-bear, telephone, television, tennis racquet, tent, tiger, toaster, toe, toilet, tooth, toothbrush, toothpaste, tornado, tractor, traffic light, train, tree, triangle, trombone, truck, trumpet, umbrella, underwear, van, vase, violin, washing machine, watermelon, waterslide, whale, wheel, windmill, wine bottle, wine glass, wristwatch, yoga, zebra, zigzag Table 15. 6 domains and 325 classes of the DomainNet dataset. 26 H. Synth-Photography and Synth-Artists Custom Datasets Figure 23. Synth-Photography examples generated using Stable Diffusion XL [51], each column is photography effect which forms the domain. Figure 24. Synth-Artists examples generated using Stable Diffusion XL [51], each column is an artistic style which forms the domain. We generate the Synth-Photography and Synth-Artists datasets in Sec. 4.5 using Stable Diffusion XL [51]. For Synthphotography  (Fig. 23)  we use the prompt Generate an image in the style of {effect} photography; where effect can be Macro, Tilt-Shift, Bokeh, Symmetry, and Zoom Blur. Similarly, for Synth-Artists  (Fig. 24)  we use the prompt Generate an image in the style of {artist}; where artist can be Vincent Van Gogh, Thomas Kinkade, Andy Warhol, Rembrandt, and Salvador Dali."
        }
    ],
    "affiliations": [
        "Boston University",
        "Runway"
    ]
}