{
    "paper_title": "Large Language Models Implicitly Learn to See and Hear Just By Reading",
    "authors": [
        "Prateek Verma",
        "Mert Pilanci"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents a fascinating find: By training an auto-regressive LLM model on text tokens, the text model inherently develops internally an ability to understand images and audio, thereby developing the ability to see and hear just by reading. Popular audio and visual LLM models fine-tune text LLM models to give text output conditioned on images and audio embeddings. On the other hand, our architecture takes in patches of images, audio waveforms or tokens as input. It gives us the embeddings or category labels typical of a classification pipeline. We show the generality of text weights in aiding audio classification for datasets FSD-50K and GTZAN. Further, we show this working for image classification on CIFAR-10 and Fashion-MNIST, as well on image patches. This pushes the notion of text-LLMs learning powerful internal circuits that can be utilized by activating necessary connections for various applications rather than training models from scratch every single time."
        },
        {
            "title": "Start",
            "content": "Department of Electrical Engineering Stanford University Stanford, CA, USA 5 2 0 2 0 2 ] . [ 1 1 9 0 7 1 . 5 0 5 2 : r Fig. 1: Overview of our method. We replace ViT/Audio-Transformer with text-LLM enabling them to see and hear using learned circuits just by reading text. AbstractThis paper presents fascinating find: By training an autoregressive LLM model on text tokens, the text model inherently develops internally an ability to understand images and audio, thereby developing the ability to see and hear just by reading. Popular audio and visual LLM models fine-tune text LLM models to give text output conditioned on images and audio embeddings. On the other hand, our architecture takes in patches of images, audio waveforms or tokens as input. It gives us the embeddings or category labels typical of classification pipeline. We show the generality of text weights in aiding audio classification for datasets FSD-50K and GTZAN. Further, we show this working for image classification on CIFAR-10 and Fashion-MNIST, as well on image patches. This pushes the notion of text-LLMs learning powerful internal circuits that can be utilized by activating necessary connections for various applications rather than training models from scratch every single time. 1. INTRODUCTION Large Language Models (LLM) have had profound impact on AI by pushing the frontier of problems and things that computers can do that were unimaginable even 3-4 years back, e.g. winning medal in International Math Olympiad [1]. This has led to almost every single approach in variety of domains such as natural language processing [2], acoustic tokens [3], raw audio [4], vision [5], robotics [6] converging on GPT like pipeline. This approach is: the modality of interest is tokenized, and GPT architecture is trained to predict the next token. If necessary, the modality of interest is reconstructed from the tokens predicted by the GPT-based LLM in the specific domain of interest. Post-training methods such as test time scaling [7], [8] are further pushing the performance of LLMs significantly. The primary motivation for our work is three papers, the first being the Audio Spectrogram Transformer (AST) proposed by Glass et al. [9]. Their work showed that Vision Transformer (ViT) weights [10] could be utilized to understand the contents of the audio spectrogram. The vision transformer only saw the images from the Imagenet dataset during training, yet the weights were generalized to understand audio in 2-D spectrogram-like representation. There are strong similarities between images and spectrograms, as experienced researchers can interpret spectrogram visually. Further, image-like representations, e.g. spectrograms, have also historically shared similar neural architectures to images, e.g. CNN [11]. The second work that led to this is AudioPALM [12], which Google proposed in 2023. strong association exists between spoken language and text, with text being compressed representation that removes prosodic, energy, and other attributes like emotion, prosody, style, etc. If the goal is to build foundation model for speech tokens, one way would be to directly train an autoregressive model with Transformer from scratch for the following speech token, similar to text token. However, because there is commonality between text and audio, AudioPALM architecture mapped the acoustic tokens to Byte Pair Encoded text tokens and vice-versa, keeping the backbone of the text LLM frozen. This allowed the model to map speech tokens to the text LLM embeddings and vice-versa. Further, third paper showed how frozen Transformers could act as universal compute engines (UCE) [13] for various inputs. This work preceded the discovery of fine-tuning techniques such as LORA. In this work, we go one step further. We show that by finetuning the existing LLMs, we can further improve or surpass the performance of frozen architecture and be comparable to architectures trained from scratch. This generalizes to audio understanding, too, which has never been shown before. The pre-training of GPT-style model with billions of text tokens has no connection whatsoever with images and audio, as given the previous context it only learns how to predict the next token. However, to do this accurately, the GPT model learns complex dependencies via the attention mechanism. We show that the idea proposed for unified modality of speech and text in AudioPALM is generic enough for domains uncorrelated with that of text. By activating the internal circuits by finetuning, we can replace standard classification pipeline with pre-trained text LLM, which outperforms frozen LLMs proposed work on UCE. Another fascinating paper deserves to be mentioned here, which shows the powerful structure of internal circuits developed inside an LLM by finetuning. For entity tracking, [14] demonstrated that pre-trained text LLMs and subsequently fine-tuned on structured domains such as math give substantial gains on specialized NLP tasks, outperforming original LLM model both seeing the same language data. The contributions of the work are as follows: i) Encoder-free learning: We show that the weights of text-LLM trained on next token prediction can be utilized for understanding the contents of an image or audio at patch level without utilizing visual or audio encoders. This work uses text-LLM to act as an encoder, which takes in an image/audio chunk and gives an embedding typical of classification pipeline. ii) We outperform the performance of frozen GPT weight architecture as proposed in AudioPALM or UCE by utilizing PEFT techniques [15] such as LORA [16] as intuitively finetuning should improve iii) By scaling the base LLM parameters and keeping everything else the same, we improve the classification accuracy for all the data sets proposed in this paper. Since we did not bias the work on particular modality or domain, the ideas proposed in this work are generic and can improve any classification pipeline. 2. DATASET For the sake of our work in this paper, we show the proposed method, i.e. utilizing text LLM weights pre-trained on next language token prediction on two classification datasets from two modalities: visual recognition and audio classification. We take two popular yet academic scale benchmarks, CIFAR-10 [17] and Fashion-MNIST [18] for images. CIFAR-10 is widely cited image classification dataset comprising 10 image categories with 50k images for training and 10k for testing, suitable for academic scale experiments. Further, we also run our benchmarks on Fashion-MNIST, 10-category classification task on grayscale images. For audio, we use FSD-50K [19], popular audio classification benchmark, which, unlike AudioSet [20], has the audio given along with multi-class labels for every audio. Another reason for choosing FSD-50K is the training; testing pipelines are fixed with clear instructions on handling audio, the augmentations that should be carried out and the context length the model should be trained on, making the comparisons fair. Finally, in order to showcase the diversity of our method and its ability to handle variety of audio, we model only the coarse acoustic tokens, with the goal being to classify the contents of the audio in its tokenized representation with stark similarities to that of AudioPALM and similar approaches on GTZAN dataset [21]. It classifies 1s chunk of audio into one of the possible 10 genres of music that use coarse representation. 3. METHODOLOGY 3.1. Difference with AudioLLM We categorize AudioLLMs into two broad architectural classes. Though analogous formulations exist for vision, we focus solely Fig. 2: Two popular AudioLLM architectures are shown here with figures taken from respective papers LTU-AS [32] and Qwen-Audio [33] for illustration. Red highlights the block that we replaced in this work. We propose learning an audio encoder, typically classifier/pre-trained model that takes in an audio representation and gives an embedding vector using pre-trained text-LLM. on audio for brevity. The first class comprises purely decoder-based models that operate on audio token sequences produced by tokenizers like EnCodec [22] or SoundStream [23] mirroring autoregressive language models (e.g., GPT-2 [2], LLaMA [24]), with training objectives based on next-token prediction over acoustic tokens. Google pioneered this formulation in their work on discrete representation learning [25] and later extended to Jukebox [26], VALL-E [27] for music and speech generation and audio understanding [28]. Such models, lacking an encoder, represent canonical form of decoderonly audio LLMs. The second class of AudioLLMs maps audio to text, typically following an encoder-decoder paradigm. Here, audio inputs are encoded into embeddings or acoustic tokens, which condition finetuned pre-trained text LLM (e.g., GPT, LLaMA) for autoregressive text generation. These models are designed for tasks such as audio captioning and audio-based question answering, predicting the next text token based on an audio prompt. Architecturally, they mirror vision language models like Flamingo [29] and LLaVA [30]; NVIDIAs AudioFlamingo [31] extends this paradigm to audio with no modifications. Unlike decoder-only audio LLMs, these are not foundational models but task-specific adaptations of text LLMs. Our architecture diverges from both families of AudioLLMs by repurposing pre-trained text-only LLM weights for non-text modalities. As illustrated in Fig. 2 (highlighted in red), we fine-tune these weights to act as audio or image classifiers/embedding extractors, enabling representation learning beyond text. Unlike models such as LTU-AS [32] or QwenAudio [33], which condition LLMs on audio encoder embeddings, we directly adapt the embedding pipeline by fine-tuning the LLM itself. Similarly, in models like Audio Transformer [34]  (Fig. 1)  , embeddings are projected to class labels via MLP heads. We hypothesize that the core Transformer [35] componentsfeed-forward layers and attention matricestrained on next-token prediction over text are modalityagnostic. Empirically, when minimally fine-tuned, we show that these weights outperform equivalent models trained from scratch. Further performance scales with the size of the GPT backbone, giving analogy to already existing scaling laws [36] in text-only decoder-based LLMs. 3.2. Training Recipe Given an input data modality of interest, RHW , we divide into non-overlapping patches of size . The number of patches: = 2 . (1) Each patch is flattened into vector, forming matrix of patch tokens: Xp RN (P 2C). (2) Linear projection is applied to flattened patch to get patch embeddings: = XpWe + be, We R(P 2C)D, RN D. (3) To retain spatial information, we add positional embeddings: Xe = + P, RN D. (4) The embedding Xe is then passed to the first layer of the Transformer. This computation block remains the same for images and audio with different dimensions of C, H, . For ViT for R, G, and colour images, is 3 for colour datasets such as CIFAR-10. For audio waveform transformers that take as input 1-D waveforms, is 1. For grayscale images like Fashion-MNIST and Audio Transformers/Audio Spectogram Transformer, is 1. Extra care has to be taken to select patch size for audio to consider the quasi-stationary property of waveforms to allow the linear projection to learn sound and good representations. Let the patch embedding with positional encodings be denoted by Xe R(N +1)D. Transformer encoder consists of stack of identical layers having multi-head self-attention followed by feed-forward network with residual connections and layer normalization. For the ℓ-th layer (ℓ = 1, . . . , L), we compute: (ℓ1)(cid:17)(cid:17) (cid:16) (cid:101)X (ℓ)(cid:17)(cid:17) (cid:101)X (ℓ) = MSA (ℓ) = MLP + (ℓ1), + (cid:101)X (ℓ), LN LN (5) (6) (cid:16) (cid:16) (cid:16) (cid:18) (cid:19) Let MSA denote multi-head self-attention. Then, MSA(X) = Concat(h1, . . . , hH )W O, where each head is computed as hi = QiK dk softmax Vi, with Qi = XW , Ki = XW RDdk and RHdkD Vi = XW are learnable parameters, and is the number of attention heads. The feed-forward network is two-layer MLP with GELU activation: . Here, , , , and MLP(x) = GELU(xW1 + b1)W2 + b2. (7) The initial input is (0) = Xe, and the output of the final encoder layer is (L). What we have described remains the same for AST, Audio Transformers, and ViT, with the only difference being the learnable parameters and the input and output labels. An important point to note, however, is that the learnable parameters along with MLP weights W1, b1, W2, b2 for layer are the same for LLM architecture except LLM Transformer stack having causal attention masks. This paper explores the question: Can we reuse the same set of learnable parameters for the next token prediction for problems that have no connection to that of text or output text tokens, e.g. using text-GPT weights to image and audio classifier? Intuitively, it has worked for domains that bore some connections. AudioPALM utilized similar spaces of speech and text to build foundational architectures of speech from text by mapping text tokens to speech tokens. AST used ViT weights to build spectrogram inputbased audio classifiers, as 2-D mel spectrogram representations can be interpreted visually, e.g. trained researchers/academics can look at logmagnitude mel spectrogram and interpret it if it is speech/music/siren. We take this further by using weights of GPT architecture trained on internet scale data, thereby learning universal dependencies/functions to make the next token prediction accurately. These weights are then used to make image and audio classifiers. To achieve this, we follow similar approach to AudioPALM/AST but with that of GPT-2 weights. We remove the pipeline that maps GPT-2 text tokens to convert it to embedding Xe. Instead similar to ViT or Audio Transformer we take the input image or waveform representation RHW , and patchify it, learn to linearly map to get Xe by learning the matrices We and be to get En = XpWne + bne. These new embedding En are then added with the same positional embedding that the original based LLM model was trained with, e.g. GPT-2 in this uses learned positional embeddings rather than fixed ones, making learning the right Wne crucial. The weights of the first layer of the Transformer stack of GPT-2 LLM expect embeddings with the learned positional embeddings from text to be fed. Thus, we do not learn the positional embeddings from scratch. Rather, we reuse the learned positional embeddings and allow the matrix Wne, bne that maps image/audio patches to get En to make sure that it maps the new modality of interest to the space the stack of text decoder layers expect. Thus, we re-learn new embedding matrix similar to AudioPALM but on continuous representations such as image pixels or audio waveforms. Further, both AudioPALM/AST keep the weights of text-LLM or ViT frozen. We allow parameter-efficient fine-tuning like LoRA to tweak the weights. For the query, key, and value projections: WQ = αQ AQBQ, LoRA WK = αK AK BK , LoRA WV = αV AV BV , LoRA = WQ + WQ, = WK + WK , = WV + WV , (8) (9) (10) where and are low-rank trainable matrices with Rdoutr and Rrdin , and α is scaling factor. We choose and α for the whole paper to be 8. We do not carry out grid-search to choose the optimal α and rank for specific dataset and size of the architecture. This will only increase the performance further from already reported strong results. The only tweak left is to take the last token from the last layer of the output of the text-pre-trained GPT-2 backbone and learn an MLP layer to classify the outputs from the embedding learned from the GPT. Intuitively, thus, we are just mapping the modality of interest to be classified by mapping it to the embedding space the weights of the GPT want and then undoing it from the last layer of the GPT output. This generic pipeline can replace any classification pipeline of interest. We would show that it can replace it for vision (viT) or audio (Audio Transformer) that learn embedding weights + Transformer weights from scratch. The number of learnable parameters is now the added PEFT and input and output projection matrices. We show that keeping the backbone architecture similar to trained LLM achieves almost comparable performance with very small trainable parameters. This work also would push us to learn better front-end architectures rather than just going after scale for architectures in terms of parameters and data such as Whisper [37], which utilized sub-optimal front-end like mel-spectrogram [38], [39]. We can envision for audio using front ends such as Leaf-Net [40] or even making them content adaptive [41] to significantly boost the performance further rather than just increasing Transformer backbone size like One-Peace [42] which is out of reach of most academic labs. 4. RESULTS AND DISCUSSION To showcase our method, we evaluate our architecture on two typical audio and image datasets. The goal would be to compare our method with finetuned pre-existing architecture such as ViT. We also compare it against architectures with similar parameters trained from scratch. Finally, we compare the method with the LLM backbone frozen vs tunable, showing that PEFT techniques on text-only LLM outperform other methods giving competitive results on audio/images. 4.1. Results on Images We apply similar recipe to ViT for evaluating our approach. All the architectures share the same topology i.e. how patch embedding and position encoding layer takes image patches and maps them to Model Variant Fashion -GPT(S) Fashion -GPT(M) Fashion -ViT [45] Backbone GPT - 87M GPT - 340M ViT-Finetuned #train params 0.6M 1.4M 86M Accuracy 91.6% 92.4% 95.5% Table 2: Different model variants on the Fashion-MNIST dataset. All models train patch embeddings from scratch, with only difference being in the Transformer stack, with using i) pretrained GPT-2 weights from next text token prediction followed by LORA ii) Fine-tuned ViT weight trained on images as they continue to improve the performance as we increase the model size and the amount of data [36]. However, the interesting thing is that we continue to see better performance despite keeping the front end and the linear classifier topology the same, both images and audio. Perhaps this is due to much richer set of functions and connections available in the larger models that get activated during fine-tuning. For the case of audio genres, the performance increased close to 2%. We can see that for the continuous audio representations for the FSD-50K dataset; we continued to see gains in the performance further when we explored billion parameter LLM architectures with gains close to almost 4% absolute accuracy improvement. This is non-trivial gain showing how our architecture activates the circuits necessary for understanding audio already existing in pre-trained text LLM. We can see this in Figure 3, where not only does the model achieve better performance, but it also converges faster. Model Variant GTZAN Scratch GTZAN LORA GTZAN LORA Backbone #train params 2.3M GPT(S) - 87M 1.2M GPT(S) - 340M 1.9M Accuracy 63.1% 66.7% 68.9% Table 3: Results on 1s ENCODEC tokens for various configurations. All models train embeddings matrix from scratch(AudioPALM), with only difference being in the Transformer stack, with using i) pretrained GPT-2 weights from next text token prediction followed by LORA ii) training Transformers from scratch 4.4. Results on Audio Model Variant FSD Small FSD Large FSD XLarge FSD-Scratch [34] Backbone GPT - 87M (LORA) GPT - 710M(LORA) GPT - 1.5B(LORA) Audio Transformer #train params 1.2M 1.9M 2.4M 1.9M Accuracy 41.4% 42.7% 44.1% 40.4% Table 4: Results on audio waveform for various configurations. All models train patch embeddings from scratch, with only difference being in the Transformer stack, with either using i) pretrained GPT-2 weights from next text token prediction followed by LORA ii) Training Audio Transformer from scratch. We present the result on two audio datasets, GTZAN and FSD-50K, with coarsest ENCODEC token input representation for GTZAN and continuous audio waveform for FSD-50K. For GTZAN, the goal was to map the acoustic tokens to the embedding matrix similar to how it was done for Audio-PALM [12], with the only difference being the Transformer decoder backbone was tunable using LORA for classification. For continuous audio representation for experiments for FSD-50K, we utilise the front end inspired by [34], [46] for all the experiments reported in Table 4. It consists of 4 sets of convolutional filters, each with 64 filters with kernel lengths of 16, 32, 64, and 128 on 1s audio with 25ms as patch length. Each convolutional filter output is then collapsed to single scalar by taking the maximum output of the conv filter, followed by relu non-linearity. This gives us 256-dimensional output for every audio waveform patch. This is then projected to the dimension of the first layer of the Transformer backbone we use. The output is taken from the last decoder layer of the final audio waveform patch, and the representation is used using linear head to classify the contents of the audio using Huber loss similar to [34]. Therefore, similar to the image experiments, we only change the Transformer backbone, i.e., it is either trained from scratch or text-GPT is fine-tuned using LORA. We see from Figure 1 that Fig. 3: Results for FSD-50K showing performance improvement with scale. All of the four models share the same learnable front end that patches the waveform and learns embedding for the first layer of Transformer stack. Audio Transformer trains everything from scratch whereas others use GPT-2 weights pretrained on next text token prediction tuned LORA for various model sizes. the embedding that the first layer of the Transformer wants. This is to ensure that we can compare them with each other in fair manner. The only difference between our proposed method and ViT/others is what happens afterwards. For the case of keeping the LLM frozen, we do not fine-tune the architecture and use the textLLM weights as is. We also compare them with the PEFT technique LoRA, which fine-tunes only tiny fraction of the parameters. Finally, we compare this with architecture trained from scratch for CIFAR-10 and fully fine-tuned ViT for Fashion MNSIT. We can see that for both Fashion MNIST and CIFAR-10, we achieve comparable results for architectures trained from scratch and ViT architectures that we fine-tuned on the downstream dataset. Model Variant CIFAR10-GPT(S) LORA CIFAR10-GPT(M) LORA CIFAR10-GPT(S) [43] CIFAR10-ViT [44] #train params Backbone GPT - 87M 0.64M GPT - 340M 1.4M GPT -87M Scratch Frozen 86M Accuracy 76.6 % 78.4 % 72.1% 80.1% Table 1: Performance of different model variants on the CIFAR-10 dataset. All models train patch embeddings from scratch, with only difference being in the Transformer stack, with either using i) pretrained GPT-2 weights from next text token prediction followed by LORA ii) Keeping text pretrained GPT-2 weights frozen iii) Training ViT from scratch with random initialization. 4.2. Frozen Backbone vs LoRA based finetuning Similar to AudioPalm [12] and UCE [13], we compare the performance of our model for one of the modalities images keeping the base LLM parameter frozen for the case of CIFAR-10. The model outperforms the frozen-based setup significantly, as seen in Table 1. Intuitively again, allowing few model parameters to be fine-tuned should help better generalize, as seen across several custom tasks in the post-training step [16]. Therefore, we only compare LORA-tuned architectures for the subsequent experiments for scale, different audio representations, and different training setups, i.e. models trained from scratch. 4.3. Effect of Scale We see consistent increase in the accuracy/classification performance as we increase the parameters of backbone text LLM from 87M to close to billion. This aligns with the scaling laws exhibited by LLM it surpasses the models trained from scratch with similar number of trainable parameters. Further, as we increase the size of the backbone text LLM, we see an increase in performance. 5. CONCLUSION AND FUTURE WORK We show that the weights of text-based LLM can be reused for downstream classification tasks in image, audio and music domains, with no similarity between text tokens and the input patch representation of pixels or waveform. Inspired by AudioPalm, we learn lightweight embedding matrices that map non-textual inputs into the space of the first layer of text LLM. Unlike prior VLLM/ALLM approaches that feed embeddings into frozen text model, we demonstrate that the LLMs internal circuitry alone can suffice for direct classification, obviating the need for separate encoder. Our approach achieves competitive performance using only small fraction of the parameters required by standard architectures. Scaling up the backbone LLM further boosts accuracy, suggesting broad potential impact across modalities by leveraging circuitry inside text-pre-trained LLMs instead of training new classifiers from scratch every time."
        },
        {
            "title": "REFERENCES",
            "content": "[1] T. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong, Solving olympiad geometry without human demonstrations, Nature, vol. 625, no. 7995, pp. 476482, 2024. [2] T. B. Brown, B. Mann et al., Language models are few-shot learners, 2020. [3] Z. Borsos, R. Marinier et al., Audiolm: language modeling approach to audio generation, 2023. [4] P. Verma and C. Chafe, generative model for raw audio using transformer architectures, in 2021 24th International Conference on Digital Audio Effects (DAFx). IEEE, 2021, pp. 230237. [5] W. Yan, Y. Zhang, P. Abbeel, and A. Srinivas, Videogpt: Video generation using vq-vae and transformers, 2021. [6] A. Brohan, N. Brown et al., Rt-2: Vision-language-action models transfer web knowledge to robotic control, 2023. [7] N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. Cand`es, and T. Hashimoto, s1: Simple test-time scaling, arXiv preprint arXiv:2501.19393, 2025. [8] B. Brown, J. Juravsky, R. Ehrlich, R. Clark, Q. V. Le, C. Re, and A. Mirhoseini, Large language monkeys: Scaling inference compute with repeated sampling, arXiv preprint arXiv:2407.21787, 2024. [9] Y. Gong, Y.-A. Chung, and J. Glass, AST: Audio Spectrogram Transformer, in Proc. Interspeech 2021, 2021, pp. 571575. [10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [11] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold et al., Cnn architectures for large-scale audio classification, in 2017 ieee international conference on acoustics, speech and signal processing (icassp). IEEE, 2017, pp. 131135. [12] P. K. Rubenstein, C. Asawaroengchai et al., Audiopalm: large language model that can speak and listen, arXiv preprint arXiv:2306.12925, 2023. [13] K. Lu, A. Grover, P. Abbeel, and I. Mordatch, Frozen pretrained transformers as universal computation engines, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 7, 2022, pp. 76897697. [14] N. Prakash, T. R. Shaham, T. Haklay, Y. Belinkov, and D. Bau, Finetuning enhances existing mechanisms: case study on entity tracking, 2024. [15] Z. Han, C. Gao, J. Liu, J. Zhang, and S. Q. Zhang, Parameter-efficient fine-tuning for large models: comprehensive survey, arXiv preprint arXiv:2403.14608, 2024. [16] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, Lora: Low-rank adaptation of large language models, 2021. features from tiny images, University of Toronto, Tech. Rep., 2009. [Online]. Available: https://www.cs.toronto.edu/kriz/cifar.html [17] A. Krizhevsky, Learning multiple layers of [18] H. Xiao, K. Rasul, and R. Vollgraf, Fashion-mnist: novel image dataset for benchmarking machine learning algorithms, arXiv preprint arXiv:1708.07747, 2017. [19] E. Fonseca, X. Favory, J. Pons, F. Font, and X. Serra, FSD50K: an open dataset of human-labeled sound events, arXiv preprint arXiv:2010.00475, 2020. [20] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, Audio set: An ontology and humanlabeled dataset for audio events, in 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2017, pp. 776780. [21] G. Tzanetakis, G. Essl, and P. Cook, Automatic musical genre [Online]. Available: http: classification of audio signals, 2001. //ismir2001.ismir.net/pdf/tzanetakis.pdf [22] A. Defossez, J. Copet, G. Synnaeve, and Y. Adi, High fidelity neural audio compression, arXiv preprint arXiv:2210.13438, 2022. [23] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi, Soundstream: An end-to-end neural audio codec, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 495507, 2021. [24] H. Touvron, T. Lavril et al., Llama: Open and efficient foundation language models, 2023. [25] A. Van Den Oord, O. Vinyals et al., Neural discrete representation learning, Advances in neural information processing systems, vol. 30, 2017. [26] P. Dhariwal, H. Jun, A. Payne, J. W. Kim, A. Radford, and I. Sutskever, Jukebox: generative model for music, arXiv preprint arXiv:2005.00341, 2020. [27] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li et al., Neural codec language models are zero-shot text to speech synthesizers, arXiv preprint arXiv:2301.02111, 2023. [28] P. Verma and J. Smith, framework for contrastive and generative learning of audio representations, arXiv preprint arXiv:2010.11459, 2020. [29] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds et al., Flamingo: visual language model for few-shot learning, Advances in neural information processing systems, vol. 35, pp. 23 71623 736, 2022. [30] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, Advances in neural information processing systems, vol. 36, pp. 34 89234 916, 2023. [31] Z. Kong, A. Goel, R. Badlani, W. Ping, R. Valle, and B. Catanzaro, Audio flamingo: novel audio language model with few-shot learning and dialogue abilities, arXiv preprint arXiv:2402.01831, 2024. [32] Y. Gong, A. H. Liu, H. Luo, L. Karlinsky, and J. Glass, Joint audio and speech understanding, in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2023, pp. 18. [33] Y. Chu, J. Xu, X. Zhou, Q. Yang, S. Zhang, Z. Yan, C. Zhou, and J. Zhou, Qwen-audio: Advancing universal audio understanding via unified largescale audio-language models, arXiv preprint arXiv:2311.07919, 2023. [34] P. Verma and J. Berger, Audio transformers: Transformer architectures for large scale audio understanding. adieu convolutions, arXiv preprint arXiv:2105.00335, 2021. [35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. [36] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, Scaling laws for neural language models, 2020. [37] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, Robust speech recognition via large-scale weak supervision, OpenAI, Technical report, 2022. [Online]. Available: https://cdn.openai.com/papers/whisper.pdf [38] M. Ravanelli and Y. Bengio, Speaker recognition from raw waveform with sincnet, in 2018 IEEE spoken language technology workshop (SLT). IEEE, 2018, pp. 10211028. [39] P. Verma and R. W. Schafer, Frequency estimation from waveforms using multi-layered neural networks, in Proceedings of Interspeech 2016, 2016, pp. 21602164. [40] N. Zeghidour, O. Teboul, F. de Chaumont Quitry, and M. Tagliasacchi, Leaf: learnable frontend for audio classification, in International Conference on Learning Representations (ICLR), 2021. [41] P. Verma and C. Chafe, content adaptive learnable time-frequency representation for audio signal processing, in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 15. [42] P. Wang, S. Wang, J. Lin, S. Bai, X. Zhou, J. Zhou, X. Wang, and C. Zhou, One-peace: Exploring one general representation model toward unlimited modalities, arXiv preprint arXiv:2305.11172, 2023. [43] K. Lu, A. Grover, P. Abbeel, and I. Mordatch, Pretrained transformers as universal computation engines, 2021. [44] S. Chh, PyTorch from Scratch: Vision Transformer (ViT), https://github. com/s-chh/PyTorch-Scratch-Vision-Transformer-ViT, 2021, accessed: 2025-04-24. [45] S. Bbouzidi, G. Hcini, I. Jdey, and F. Drira, Convolutional neural networks and vision transformers for fashion mnist classification: literature review, arXiv preprint arXiv:2406.03478, 2024. [46] T. N. Sainath, R. J. Weiss, A. W. Senior, K. W. Wilson, and O. Vinyals, Learning the speech front-end with raw waveform cldnns. in Interspeech. Dresden, Germany, 2015, pp. 15."
        }
    ],
    "affiliations": [
        "Department of Electrical Engineering Stanford University Stanford, CA, USA"
    ]
}