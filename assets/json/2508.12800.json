{
    "paper_title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward",
    "authors": [
        "Yong Deng",
        "Guoqing Wang",
        "Zhenzhe Ying",
        "Xiaofeng Wu",
        "Jinzhen Lin",
        "Wenwen Xiong",
        "Yuqin Dai",
        "Shuo Yang",
        "Zhanwei Zhang",
        "Qiwen Wang",
        "Yang Qin",
        "Changhua Meng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) exhibit remarkable problem-solving abilities, but struggle with complex tasks due to static internal knowledge. Retrieval-Augmented Generation (RAG) enhances access to external information, yet remains limited in multi-hop reasoning and strategic search due to rigid workflows. Recent advancements in agentic deep research empower LLMs to autonomously reason, search, and synthesize information. However, current approaches relying on outcome-based reinforcement learning (RL) face critical issues such as conflicting gradients and reward sparsity, limiting performance gains and training efficiency. To address these, we first propose Atomic Thought, a novel LLM thinking paradigm that decomposes reasoning into fine-grained functional units. These units are supervised by Reasoning Reward Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained guidance. Building on this, we propose Atom-Searcher, a novel RL framework for agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher uses a curriculum-inspired reward schedule, prioritizing process-level ATR early and transitioning to outcome rewards, accelerating convergence on effective reasoning paths. Experiments on seven benchmarks show consistent improvements over the state-of-the-art. Key advantages include: (1) Atom-Searcher scales computation at test-time. (2) Atomic Thought provides supervision anchors for RRMs, bridging deep research tasks and RRMs. (3) Atom-Searcher exhibits more interpretable, human-like reasoning patterns."
        },
        {
            "title": "Start",
            "content": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward Yong Deng, Guoqing Wang, Zhenzhe Ying, Xiaofeng Wu, Jinzhen Lin, Wenwen Xiong, Yuqin Dai, Shuo Yang, Zhanwei Zhang, Qiwen Wang, Yang Qin, Changhua Meng Ant Group Core Contributors Large language models (LLMs) exhibit remarkable problem-solving abilities, but struggle with complex tasks due to static internal knowledge. Retrieval-Augmented Generation (RAG) enhances access to external information, yet remains limited in multi-hop reasoning and strategic search due to rigid workflows. Recent advancements in agentic deep research empower LLMs to autonomously reason, search, and synthesize information. However, current approaches relying on outcome-based reinforcement learning (RL) face critical issues such as conflicting gradients and reward sparsity, limiting performance gains and training efficiency. To address these, we first propose Atomic Thought, novel LLM thinking paradigm that decomposes reasoning into fine-grained functional units. These units are supervised by Reasoning Reward Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained guidance. Building on this, we propose Atom-Searcher, novel RL framework for agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher uses curriculum-inspired reward schedule, prioritizing process-level ATR early and transitioning to outcome rewards, accelerating convergence on effective reasoning paths. Experiments on seven benchmarks show consistent improvements over the state-of-the-art. Key advantages include: (1) Atom-Searcher scales computation at test-time. (2) Atomic Thought provides supervision anchors for RRMs, bridging deep research tasks and RRMs. (3) Atom-Searcher exhibits more interpretable, human-like reasoning patterns. Code: https://github.com/antgroup/Research-Venus 5 2 0 2 9 ] . [ 2 0 0 8 2 1 . 8 0 5 2 : r Figure 1 Atom-Searcher achieves SOTA performance on both in-domain and out-of-domain benchmarks."
        },
        {
            "title": "1 Introduction",
            "content": "Although large language models (LLMs) demonstrate impressive language understanding and logical reasoning abilities Yang et al. (2025a); Guo et al. (2025); Hurst et al. (2024), their capacity to solve complex problems ultimately hits ceiling due to the static nature of their internal knowledge representation Wang et al. (2024a); Jin et al. (2024). Retrieval-Augmented Generation (RAG) Lewis et al. (2020) offers solution by equipping LLMs with external information sources, enhancing the relevance, accuracy, and timeliness of their responses Gao et al. (2023); Fan et al. (2024). However, RAGs static workflows, making them ineffective at handling real-world questions that require sophisticated multi-hop reasoning and strategic search planning Singh et al. (2025), as they often fail to construct correct search paths for complex problems Yao et al. (2023). To mitigate these limitations, new search paradigm, termed Agentic Deep Research system, has been proposed, which enables autonomous reasoning, ondemand searching, and iterative information synthesis. Demonstrations from recent deep research systems by OpenAI OpenAI (2025) and Google Google (2024) reveal several key advantages of this paradigm: 1) Comprehensive Understanding: Effectively handles complex, multi-step queries that challenge traditional methods Wei et al. (2022); 2) Enhanced Synthesis: Integrates diverse and even conflicting sources into coherent, informative outputs Cheng et al. (2025); 3) Reduced User Effort: Automates tedious search processes, easing users cognitive and manual burden Sami et al. (2024). Figure 2 Atomic Thought paradigm automatically decomposes each <think> into finer-grained functional units <atom-think> during the rollout. Early implementations of agentic deep research relied on prompt engineering Song et al. (2024); Kim et al. (2024) and supervised fine-tuning (SFT) Zhang et al. (2024). Yet, prompt-based methods rely heavily on LLMs instruction-following and long-context capabilities, whereas SFT tends to generalize poorly across domains Chu et al. (2025). More recently, post-training LLMs via reinforcement learning with outcome-based rewards (outcome-based RL) has yielded notable gains in reasoning performance Guo et al. (2025); OpenAI (2024). Building on this insight, recent advances Dai et al. (2025); Yang et al. (2025b,c) (e.g. Search-R1 Jin et al. (2025) and DeepResearcher Zheng et al. (2025)) treat the search tool as part of the environment and apply outcome-based RL to enable end-to-end optimization of the entire workflow, resulting in more performant and generalizable agentic deep research systems. Although outcome-based RL has shown promise, it remains insufficient in fully advancing agentic deep research, for the following reasons: 1) Gradients Conflicts: In the outcome-based RL paradigm, an incorrect final answer results in the entire trajectory being penalized Lightman et al. (2023), even when intermediate reasoning process or research strategies are effective. This coarse-grained reward design introduces potential gradient conflicts between intermediate reasoning steps and final answers, which hinders the model from discovering better reasoning capabilities and research strategies, thereby limiting its generalization ability. 2) Reward sparsity: Outcome-based RL relies solely on the final answer to generate rewards Du et al. (2024), resulting in each training sample providing only sparse feedback. This severely limits the efficiency of policy optimization, as it increases the reliance on larger training datasets and prolonged training schedules. 2 To address these challenges, we begin by introducing Atomic Thought, novel LLM thinking paradigm that decomposes reasoning into fine-grained functional units, called Atomic Thoughts, guiding LLMs to engage in clearer and more in-depth reasoning, as illustrated in Figure 2. For example, reasoning operations like <Reflection> and <Verification> serve as Atomic Thoughts. Their interactions constitute the functional backbone of the reasoning process. To promote generalization, we avoid manual decomposition of Atomic Thoughts and instead encourage the model to autonomously induce them from reasoning processes. Building on this definition, we employ Reasoning Reward Model (RRM) to score the generated Atomic thoughts and construct fine-grained Atomic Thought Reward (ATR). The ATR serves as an auxiliary signal to calibrate the outcome reward, thereby mitigating gradient conflicts during policy optimization. To aggregate the ATR and outcome reward, we design an curriculum-inspired strategy. During the early stages of training, the model is in solution path exploration phase: while it may struggle to produce fully correct final answers, it can more easily develop partially correct reasoning traces. Relying solely on outcome rewards at this stage may induce severe gradient conflicts, thus requiring stronger calibration. As training advances, the alignment between reasoning and answers improves, reducing gradient conflicts and necessitating weaker calibration to avoid introducing excessive noise. Accordingly, we employ linearly decaying weighting scheme, wherein the contribution of the ATR is gradually reduced as training proceeds. In addition, the hybrid reward incorporates process-level signals into the outcome-based reward, alleviating the problem of reward sparsity. Building on the above components, we propose Atom-Searcher, novel RL framework for agentic deep research, aimed at advancing the performance frontier of agentic deep research models. We conducted experiments on seven benchmarks covering both in-domain and out-of-domain tasks, demonstrating that Atom-Searcher achieves significant performance gains compared to the state-of-the-art (SOTA) baseline. Furthermore, we designed experiments to highlight the following advantages of Atom-Searcher: (1) Atom-Searcher effectively scales computation during test-time. (2) Atomic Thoughts provide supervision anchors for RRMs, effectively bridging deep research tasks and RRMs. (3) Atom-Searcher exhibits more interpretable, human-like reasoning patterns In summary, our main contributions are as follows: We first introduce Atomic Thought, novel LLM thinking paradigm that decomposes reasoning into fine-grained functional units, effectively guiding LLMs to engage in clearer and more in-depth reasoning. Building on Atomic Thought, we design fine-grained Atomic Thought Reward and construct curriculum-inspired aggregation strategy to integrate ATR with the outcome reward. This reward modeling alleviates gradient conflicts and reward sparsity during policy optimization. Building on Atomic Thought paradigm, ATR and the proposed reward aggregation strategy, we introduce Atom-Searcher, novel RL framework for agentic deep research, aimed at advancing the performance frontier of agentic deep research. We demonstrated that Atom-Searcher achieves significant performance improvements over the SOTA baseline on seven benchmarks covering both in-domain and out-of-domain tasks. Additionally, we designed experiments to highlight range of impressive advantages of AtomSearcher. Figure 3 Overview of Atom-Searcher. Within the Atom-Searcher framework, we: 1) construct an atomic thought dataset and apply supervised fine-tuning (SFT) to the policy LLMserving as the agentic deep research modelto incentivize its capability for generating atomic thoughts; 2) formulate fine-grained atomic thought rewards using reasoning reward model, aligned with the atomic structure of the reasoning process, and integrate them with rule-based outcome rewards to optimize the SFT-initialized policy LLM via reinforcement learning."
        },
        {
            "title": "2 Atom-Searcher",
            "content": "We propose novel framework for enhancing agentic deep research models. As illustrated in Figure 3, the framework consists of two phases. In phase1, we construct an atomic thought instruction dataset and perform SFT on the policy model to incentivize its ability to generate atomic thoughts. In phase2, we leverage Reasoning Reward Model to derive fine-grained rewards based on the generated atomic thoughts, and integrate them with existing rule-based outcome rewards. The resulting hybrid reward is then used to further train the SFT-initialized policy LLM via RL. 2.1 Preliminary Atomic Thought. Understanding the fundamental units of thought is critical for simulating intelligence, optimizing decision-making, and extracting actionable knowledge in both cognitive science and computational reasoning Anderson et al. (1997); Ho and Griffiths (2022). Drawing inspiration from philosophical conceptions of thought and the structured decomposition of actions in domains such as football, we propose principled framework for defining the atomic thought within the reasoning processes of LLMs. LLM atomic thought is the minimal, functionally coherent unit of reasoning, irreducible in form, yet integral to the models reasoning trajectory. The interactions among atomic thoughts collectively form functionally complete reasoning or behavior process. Take football as an example: when learning the kicking motion of skilled player, we need to analyze the atomic units that compose this complex behavior, such as step adjustment, leg swing, and point of contact with the ball. Similarly, when shifting to LLMs, assessing the quality of their reasoning requires analyzing the atomic thoughts that compose their thought process. Therefore, in RL settings, designing fine-grained rewards at the atomic thought level can provide valuable intermediate supervision signals for guiding the reasoning trajectory. 4 In implementation, we encapsulate the LLMs reasoning process within <atom-thinki> tag and structure the atomic thoughts as subtags within it, as illustrated in Figure 2. Importantly, the model is not constrained to follow manually defined atomic thoughts. Instead, we incentivize the model to autonomously generate atomic thoughts, enabling it to learn how to decompose reasoning into task-specific atomic thoughts across different scenarios. Atom-Searcher Trajectory. In an agentic deep research trajectory, the model iteratively performs reasoning and search invocations based on the user question and accumulated observations, as illustrated in Figure 2) Reasoning: Following the setup of DeepSeek-R1 Guo et al. (2025), we constrain Atom-Searcher to perform reasoning before taking any other action. Each segment of reasoning is encapsulated between the tags <think> and </think>. Notably, Atom-Searcher further decomposes the reasoning within the <think> tag into sequence of atomic thoughts, each of which is encapsulated between the tags <atom-think> and </atom-think> (e.g., <Reflection> and </Reflection>). 2) Search: After reasoning, Atom-Searcher may choose to invoke the web search tool by generating JSON-formatted request with the tool name (web_search) and the search queries as arguments. The request is encapsulated between the tags <tool_call> and </tool_call>. 3) Search Response: When the system detects the tokens <tool_call> and </tool_call>, search invocation is triggered. The retrieved results are then wrapped between the tags <tool_response> and </tool_response> and appended to the current trajectory. 4) Answer: Once Atom-Searcher determines that sufficient information has been gathered, it generates the final response enclosed between the tags <answer> and <answer>. This serves as the final answer returned to the user. Problem Formulation. We model the process of completing the agentic deep research tasks as finite-horizon Markov Decision Process (MDP), denoted by (S, A, R, ). Given user instruction I, the agent is required to complete the corresponding task. The state is defined as the retrieved content along with the history of previous actions. The action space includes three types of actions: 1) aG(Generate Atomic Thought); 2) aS(Invoke Search; and) 3) aA(Answer). At the t-th step, conditioned on the state st, the agent takes an action at following the LLM policy πθ, which can be expressed as: at = πθ(I, st) (1) The agent then receives reward rt and the state is updated to st+1. We formalize this process as follows. (st, at) = st+1 = (st, at) (cid:40) concat(st; at, dt) concat(st; at) rt = R(st, at) if at = aS otherwise (2) (3) (4) where and denote the deterministic state transition function and deterministic reward function provided by the environment, respectively; concat(; ) denotes the concatenation operation; dt represents the retrieved external information; and rt denotes the immediate reward at time step t. In the finite-horizon setting, the trajectory terminates either upon task completion or once the maximum number of interactions is reached. Finally, based on the sampled trajectories, we optimize the policy πθ using the Group Relative Policy Optimization (GRPO) algorithm Shao et al. (2024). 5 2.2 Incentivizing LLMs to Generate Atomic Thoughts To enable LLMs to learn how to reasonably decompose their reasoning processes into atomic thoughts, we construct high-quality atomic thought dataset Datom consisting of 1,000 annotated examples and perform supervised fine-tuning to impart prior knowledge of atomic thought structures to the model. The details are as follows. The construction of Datom involves two phases: 1) synthesizing atomic action prompts: Firstly, we carefully design 10 distinct seed system prompt templates, each containing two atomic thought examples. Each example consists of 3 to 10 common atomic thoughts (e.g., <plan>, <reflection>, etc.). Secondly, we leverage powerful teacher model (e.g., Qwen2.5-72B Hui et al. (2024)) to generate approximately 1,000 system prompts based on the seed prompt templates, each containing distinct combination of atomic thoughts. Finally, we combine each system prompt with different questions and callable search tools (e.g., web_search) to obtain 1,000 prompts. 2) sampling high-quality reasoning trajectories: Based on these 1,000 prompts, we use Qwen2.5-72B to sample complete reasoning trajectories. To ensure the quality of the generated trajectories, we employ majority voting strategy during the sampling process. Data samples in Datom follow the reasoning trajectory illustrated in Figure 2. We perform SFT of πθ on Datom to obtain πθ, which is endowed with prior knowledge of atomic thoughts. 2.3 Reward Modeling The introduction of atomic thoughts offers promising perspective for designing fine-grained reward signals to guide agentic deep research models toward developing more intelligent and efficient research strategies. We first construct ATR using reasoning reward model, and then integrate them with the outcome-level reward through training-dynamics-aware, linearly decaying aggregation strategy. Constructing fine-grained atomic thought reward. With the rapid progress of foundation model capabilities and the rise of test-time scaling techniques Snell et al. (2024), Reasoning Reward Models (RRMs) Liu et al. (2025), which leverage large reasoning models (e.g., DeepSeek-R1 Guo et al. (2025)) to generate rewards, have become promising solution. RRMs are particularly effective in settings that require fine-grained supervision, adaptive reasoning, and open-ended tasks without ground truth, making them well aligned with the characteristics of atomic thoughts. Therefore, we use the RRM to score the atomic thoughts generated by the policy model, resulting in the ATR. This process can be formulated as follows:: atom, r2 r1 atom, ..., rn Ratom = (r1 atom = RRM (Iscore, y) atom, ..., rn atom, r2 where, Iscore denotes the scoring prompt, as illustrated in Figure 6; refers to the generated trajectory; ri atom represents the score of the i-th atomic thought, and () denotes the aggregation function that combines individual atomic scores. The choice of () is not fixedit can be simple average or more sophisticated weighting strategy. Ratom denotes the ATR of trajectory y. atom) (6) (5) Dynamic, Curriculum-Based Approach to Reward Aggregation. key limitation of outcomebased reward is their coarse credit assignment: it attribute the correctness of intermediate reasoning solely to the final answer, often rewarding or penalizing steps regardless of their actual contribution. This misalignment introduces gradient conflicts during optimization. To address this, we aggregate ATR with the outcome reward, using ATR as an auxiliary signal to calibrate the final reward, thereby mitigating gradient conflicts and improving test-time performance. However, using static weighting 6 coefficient for reward aggregation fails to align with training dynamics. Specifically, early in training, the modelstill limited in its deep research capabilitystruggles to generate fully correct answers but is more likely to explore useful atomic thoughts that contribute toward correct solution. If training relies solely on outcome-based rewards at this stage, these beneficial atomic thoughts may be unjustly penalized due to the incorrect final answer; conversely, harmful atomic thoughts may also be mistakenly reinforced, resulting in severe gradient conflict and necessitating strong calibration from ATR. As training progresses and the models deep research ability improves, its reasoning trajectories become increasingly aligned with correct answers. Consequently, gradient conflicts diminish, and excessive calibration from ATR may introduce unnecessary noise, potentially harming final accuracy. To accommodate this, we adopt training-dynamics-aware weighting scheme that linearly reduces the contribution of ATR as training progresses, formulated mathematically as follows: α = 0.5 (1 TM AX ) = (cid:40) αRatom + (1 α)Rf 1 if format is correct if format is incorrect Rf 1 = 2 IN + RN (7) (8) (9) where, denotes the current training step, and TM AX denotes the maximum number of training steps. denotes the final reward used for RL training, and Rf 1 represents the outcome-based reward computed from the F1 score. The coefficient α [0, 1] is hyperparameter that balances the influence of ATR and the outcome reward during training. denotes the word count of the predicted answer, RN denotes the word count of the reference answer, and IN denotes the word count of their intersection. 2.4 RL Training Framework Policy Optimization. In this work, we adopt the GRPO algorithm Shao et al. (2024) to optimize the SFT policy πθ using the hybrid reward that aggregates final answer correctness and reasoning quality. GRPO improves the current policy πθ by leveraging reference policy πθ and set of rollouts generated by previous policy πθ . The objective is extended and formulated as follows: ref old r1, r2, ..., rG = R(y1, y2, ..., yG) Ai = ri mean(r1, r2, ..., rG) std(r1, r2, ..., rG) (cid:32) (cid:88) (cid:34) min 1 i= πθ(yix) (yix) πθ old Ai, JGRP O(θ) = xD, {yi}G i=1πθ (x) old (cid:32) clip πθ(yix) (yix) πθ old (cid:33) (cid:33) , 1 ϵ, 1 + ϵ Ai β DKL (cid:0)πθ πθ ref (cid:1) (10) (11) (cid:35) (12) where denotes an input sampled from the experience distribution D, yi represents trajectory , is the number of trajectories sampled per training example, ri is the reward of yi, generated by πθ Ai is the advantage of yi, DKL denotes the unbiased estimate of KL divergence Shao et al. (2024), and β is tunable hyperparameter. In addition, to mitigate entropy collapse during policy optimization, we adopt sliding-window-based entropy regulation mechanism, as detailed in Appendix A.1. old 7 Loss Masking. In the original GRPO framework, loss is computed over all tokens in the trajectory. However, in Atom-Searcher, trajectories include retrieval results that are externally fetched by the environment rather than generated by the policy itself. To prevent biasing the policy update toward non-trainable, static content, we apply loss masking to exclude these retrieved segments from the optimization objective. Specifically, in the computation of Equation 12, only tokens corresponding to the models reasoning (i.e., text-based thinking) and search queries are included, while tokens originating from retrieval results are masked out."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Implementation Details We use Qwen2.5-7B-Instruct Qwen et al. (2025) as the backbone models. The training is conducted using the verl framework Sheng et al. (2024). At each training step, we sample 32 prompts and generate 16 rollouts per prompt. Each rollout consists of up to 10 tool calls, followed by final answer step. The training is performed with mini-batch size of 512, meaning that one rollout stage corresponds to single backpropagation step. By default, we use Qwen3-30B-A3B Yang et al. (2025a) as the reasoning reward model in Atom-Searcher. 3.2 Benchmarks To comprehensively assess model performance in both in-domain (ID) and out-of-domain (OOD) scenarios, we construct diverse evaluation benchmark spanning wide range of open-domain QA tasks. For ID evaluation, we include the development sets of NQ Kwiatkowski et al. (2019), TQ Joshi et al. (2017), HotpotQA Yang et al. (2018), and 2Wiki Ho et al. (2020). To evaluate OOD generalization, we incorporate three datasets that differ substantially in question format and information distribution: MuSiQue Trivedi et al. (2022), Bamboogle Press et al. (2022), and PopQA Mallen et al. (2022). These datasets are chosen to challenge the models ability to generalize beyond its training distribution. To ensure fair comparison and balanced evaluation, we randomly sample 512 examples from the development sets of NQ, TQ, HotpotQA, 2Wiki, MuSiQue, and PopQA, along with all 125 examples from the Bamboogle development set. This evaluation setup enables rigorous assessment of model robustness across diverse topics and reasoning demands. 3.3 Baselines To evaluate the effectiveness of Atom-Searcher, we compare it against the following baseline methods: CoT: This baseline performs Chain-of-Thought (CoT) reasoning to generate answers without access to any external reference context. Cot+RAG: This baseline integrates CoT reasoning with retrieved reference context to guide the answer generation process. Search-o1: This baseline performs multi-step reasoning by generating search queries or intermediate answers. For each query, the model receives only snippet retrieved by retriever, rather than the full document content. 8 Search-o1-Web: Unlike Search-o1, this setting allows the model to interact with the open web by issuing real-time queries through APIs and browsing webpages via URLs. This capability supports more dynamic and comprehensive information acquisition, laying the groundwork for deep research. Search-r1: This is reinforcement learning approach for question answering that utilizes retriever to search Wikipedia during both training and inference. It includes two variantsSearchr1-base and Search-r1-instructwhich are initialized from either the base model or the instructtuned model, respectively. R1-Seaecher: This is two-stage, outcome-driven RL baseline that equips LLMs with autonomous search: the model learns to invoke external search tools and incorporate retrieved evidence on the fly to improve reasoning. DeepResearcher: This is an end-to-end trained LLM agent for deep research tasks, leveraging reinforcement learning in real-world web environments. It interacts with the open web via real-time search and browsing, enabling dynamic information acquisition. Table 1 Performance comparison of Atom-Searcher and baselines on in-domain and out-of-domain benchmarks, evaluated by F1 score; the best and second-best results are marked in bold and underlined, respectively. Type Method Prompt Based Training Based CoT CoT+RAG Search-o1 Search-o1-Web Search-r1-base Search-r1-Instruct R1-Searcher DeepResearcher Atom-Searcher NQ 19.8 42.0 34.5 32.4 45.4 33.1 35.4 39.6 44.0 In-domain Out-of-domain TQ 45.6 68.9 52.6 58.9 71.9 44.7 73.1 78.4 81.8 HotpotQA 2Wiki Musique Bamboogle PopQA 24.4 37.1 31.6 33.0 55.9 45.7 44.8 52.8 57.3 26.4 24.4 28.6 30.9 44.6 43.4 59.4 59.7 66.9 8.5 10.0 16.8 14. 26.7 26.5 22.8 27.1 27.6 22.1 25.4 35.8 46.6 56.5 45.0 64.8 71.0 70.7 17.0 46.9 36.9 38.3 43.2 43.0 42.7 48.5 50.3 Table 2 Ablation study of Atom-Searcher on seven QA benchmarks. We analyze the contribution of each component (RRM and Atom Thought). The bold indicates the best performance, and underline indicates the second-best performance. Method Base + RRM Atom-Searcher NQ 39.6 40.1 44.0 In-domain Out-of-domain TQ 78.4 78.2 81.8 HotpotQA 2Wiki Musique Bamboogle PopQA 52.8 53.5 57.3 59.7 60.0 66.9 27.1 25.7 27.6 71.0 70.5 70.7 48.5 48.8 50. 3.4 Main Result Our main result, presented in Table 1, show that Atom-Searcher achieves significant performance gains over both prompt-based and training-based baselines on in-domain and out-of-domain benchmarks. 9 3.4.1 Atom-Searcher outperforms baselines on in-domain benchmarks In the in-domain results, Atom-Searcher achieved the best performance on the TQ, HotpotQA and 2Wiki benchmarks, showing significant improvements over the second-best results, with increases of 4.3%, 2.5% and 12.1%, respectively. On average, Atom-Searcher outperformed the SOTA baseline (DeepResearcher) by 8.5% across the four in-domain benchmarks. Notably, while Search-r1-base achieved optimal performance on NQ, it was trained and evaluated using local RAG system with direct access to the relevant Wikipedia corpus. In contrast, Atom-Searcher navigates the entire Internet to find relevant information, presenting more realistic and challenging scenario, despite both models ultimately sourcing answers from Wikipedia. 3.4.2 Atom-Searcher demonstrates optimal out-of-domain generalization In the out-of-domain results, Atom-Searcher achieved the best performance on the Musique and PopQA benchmarks, improving over the second-best performance by 1.8% and 3.7%, respectively. On Bamboogle, it achieved second-best performance, but was only 0.4% lower than the optimal result. On average, Atom-Searcher outperformed the SOTA baseline (DeepResearcher) by 2.5% across the three out-of-domain benchmarks. This demonstrates that Atom-Searcher effectively generalizes the skills learned during RL to unseen scenarios. 3.5 Atom-Searcher Effectively Scales Computation at Test Time Method Table 3 Test-time token generation statistics for Atom-Searcher vs DeepResearcher. To analyze whether Atom-Searcher can effectively scale computation at test time, we compared the average number of tokens generated during the testing phase between Atom-Searcher and the SOTA baseline DeepResearcher. As shown in Table 3, Atom-Searcher generates 3.2 times more tokens in the average response length (avg.# response tokens) compared to DeepResearcher. In terms of the average length of single think process within the response (avg.# think tokens), Atom-Searcher generates 2.6 times more tokens. Additionally, Atom-Searcher performs 1.24 times more tool calls per response (avg.# tool calls) than DeepResearcher. This demonstrates that the Atom-Searcher architecture effectively achieves Test-Time Scaling without the introduction of additional incentives for generating more tokens, highlighting its stronger exploration and discovery capabilities when handling complex and challenging deep research tasks. avg.# response tokens 176 avg.# think tokens 55 143 DeepResearcher Atom-Searcher avg.# tool calls 2.13 2.65 3.6 Ablation Study We conduct an ablation study to evaluate the impact of the Atomic Thought and the fine-grained rewards generated by RRM on Atom-Searcher. To assess their contributions, we compare AtomSearcher with two alternative frameworks: (1) Base refers to the DeepResearcher Zheng et al. (2025) setting, indicating Atom-Searcher w/o Atomic Thought & fine-grained rewards generated by RRM. (2) +RRM refers to the incorporation of fine-grained rewards generated by RRM (with the same implementation details as Atom-Searcher) on top of the Base setting, indicating AtomSearcher w/o Atomic Thought. As shown in Table 2, the results across seven benchmarks, including both in-domain and out-of-domain, indicate that +RRM does not yield significant performance improvement over Base. This suggests that directly using RRM for fine-grained supervision provides minimal benefits. However, Atom-Searcher significantly outperforms +RRM, achieving an average performance improvement of 6.1% across four in-domain benchmarks and 2.5% across three out-of-domain benchmarks, demonstrating the contribution of Atomic Thought. The above results raise an interesting question: why does direct supervision using RRM have minimal effect on the reasoning process, while its effectiveness significantly improves after decomposing the reasoning process into Atom Thoughts? We speculate that this is because Atom Thoughts provide supervision anchors for RRM, helping it focus on the effective functional modules in the reasoning process, thereby generating meaningful fine-grained reward signals (ATR in Atom-Searcher). 3.7 Case Study Figure 4 analyzes the behavioral differences between Atom-Searcher and the SOTA baseline DeepResearcher in completing deep research task. It demonstrates the following advantages of Atom-Searcher: (1) Atom-Searcher employs Atomic Thoughts in its reasoning, which leads to more human-like cognitive behaviors, such as problem analysis, solution hypotheses, error prediction, and next-step planning, making its reasoning process deeper and clearer. (2) Atom-Searcher triggers more search calls, allowing it to obtain richer external information to ensure the correctness of the answer. These advantages indicate that Atom-Searcher has great potential in more complex deep research tasks. Additionally, we analyzed the token frequency statistics for Atom-Searcher and DeepResearcher during the testing phase. The word cloud, shown in Figure 5, illustrates the most frequently occurring tokens. The top-5 most frequent tokens in Atom-Searcher are <observation>, <action>, hypothesis, risk, and <risk_analysis>, whereas the top-5 most frequent tokens in DeepResearcher are I, search, need, find, and from. This disparity suggests that, compared to DeepResearcher, Atom-Searcher better aligns with human-like efficient cognitive patterns when performing deep research tasks, with stronger focus on in-depth problem analysis, hypothesis evaluation, risk assessment, and strategic planning."
        },
        {
            "title": "4 Related Work",
            "content": "4.1 Prompt and SFT-based Agentic Deep Research Early prompt-based paradigms rely on human-authored workflows to specify the interaction between LLMs and external knowledge sources. Wang et al. (2024b). For example, OpenResearcher Zheng et al. (2024), AirRAG Feng et al. (2025), IterDRAG Yue et al. (2024), Plan*RAG Verma et al. (2025), Search-o1 Li et al. (2025a), and Open Deep Search Alzubi et al. (2025) have advanced search capabilities via carefully designed workflows. However, their reliance on human-engineered prompts and interaction patterns imposes rigid behavior constraints, limiting adaptability. These limitations motivate shift toward SFT-based approaches that support more flexible and adaptive search strategies Yu et al. (2024); Wang et al. (2024c). For example, CoRAG Wang et al. (2024c) employs Monte Carlo Tree Search (MCTS) to dynamically select document blocks under budget constraints. However, it suffers from high computational overhead and limited generalization to unseen scenarios due to its reliance on supervised signals."
        },
        {
            "title": "4.2 RL-based Agentic Deep Research",
            "content": "As LLMs have achieved remarkable breakthroughs in reasoning through outcome-based RL Guo et al. (2025); Team et al. (2025), this paradigm is emerging as promising direction for enhancing agentic deep research via end-to-end optimization, attracting growing interest and active exploration 11 Figure 4 The case study demonstrates comparison of the reasoning behavior between Atom-Searcher (below) and the SOTA baseline DeepResearcher (above). from the research community. Recent works, such as ReSearch Chen et al. (2025), Search-R1 Jin et al. (2025), R1-Searcher Song et al. (2025), DeepResearcher Zheng et al. (2025), WebRL Qi et al. (2024), WebThinker Li et al. (2025b), ZeroSearch Sun et al. (2025) and WebAgent-RL Wei et al. (2025) have extended outcome-supervised reinforcement learning to the agentic deep research 12 Figure 5 Word cloud: Token frequency statistics of the responses during the testing phase for Atom-Searcher (a) and DeepResearcher (b). setting, enabling LLMs to autonomously leverage search engines for complex reasoning tasks. Although enhancing agentic deep research with outcome-supervised reinforcement learning has led to performance gains, the coarse-grained reward signals provide limited guidance for learning efficient and intelligent search strategies, often resulting in suboptimal search calls. To overcome this, we propose an atomic thought-aware fine-grained reward to guide the model toward more efficient and intelligent search behaviors, while mitigating the training inefficiency caused by reward sparsity. Building on this, we further introduce novel agentic deep research framework, Atom-Searcher, that integrates this reward formulation into reinforcement learning paradigm."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we first introduce Atomic Thought, novel LLM thinking paradigm designed to guide LLMs in clearer and more in-depth reasoning. We then supervise Atomic Thoughts using Reasoning Reward Model to generate fine-grained Atomic Thought Reward and aggregate them with outcome reward through training-dynamics-aware strategy. Based on this, we propose AtomSearcher, novel RL framework for agentic deep research, which advances the performance frontier of agentic deep research models by addressing the conflicting gradients and reward sparsity issues present in existing outcome-based deep research frameworks. Experimental results demonstrate the outstanding performance of Atom-Searcher and range of impressive advantages."
        },
        {
            "title": "References",
            "content": "Salaheddin Alzubi, Creston Brooks, Purva Chiniya, Edoardo Contente, Chiara von Gerlach, Lucas Irwin, Yihan Jiang, Arda Kaz, Windsor Nguyen, Sewoong Oh, et al. Open deep search: Democratizing search with open-source reasoning agents. arXiv preprint arXiv:2503.20201, 2025. John Anderson, Michael Matessa, and Christian Lebiere. Act-r: theory of higher level cognition and its relation to visual attention. HumanComputer Interaction, 12(4):439462, 1997. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Pan, Wen Zhang, Huajun Chen, Fan Yang, et al. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470, 2025. Mingyue Cheng, Yucong Luo, Jie Ouyang, Qi Liu, Huijie Liu, Li Li, Shuo Yu, Bohou Zhang, Jiawei Cao, Jie Ma, et al. survey on knowledge-oriented retrieval-augmented generation. arXiv preprint arXiv:2503.10677, 2025. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Yuqin Dai, Shuo Yang, Guoqing Wang, Yong Deng, Zhanwei Zhang, Jun Yin, Pengyu Zeng, et al. Careful queries, credible results: Teaching rag models advanced web search tools with reinforcement learning, 2025. https://arxiv.org/abs/2508.07956. 13 Yuqing Du, Alexander Havrilla, Sainbayar Sukhbaatar, Pieter Abbeel, and Roberta Raileanu. study on improving reasoning in language models. In Cant Believe Its Not Better Workshop: Failure Modes in the Age of Foundation Models, 2024. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. survey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 64916501, 2024. Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Jingyi Song, and Hao Wang. Airrag: Activating intrinsic reasoning for retrieval augmented generation via tree-based search. arXiv preprint arXiv:2501.10053, 2025. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2(1), 2023. Google. Gemini deep research. Technical report, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Mark Ho and Thomas Griffiths. Cognitive science as source of forward and inverse models of human decisions for robotics and control. Annual Review of Control, Robotics, and Autonomous Systems, 5(1):3353, 2022. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan Arik. Long-context llms meet rag: Overcoming challenges for long inputs in rag. arXiv preprint arXiv:2410.05983, 2024. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and Jinwoo Shin. Sure: Summarizing retrievals using answer candidates for open-domain qa of llms. arXiv preprint arXiv:2404.13081, 2024. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025a. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776, 2025b. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511, 7, 2022. OpenAI. Learning to reason with llms. Technical report, 2024. OpenAI. Deep research system card. Technical report, 2025. 14 Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022. Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. arXiv preprint arXiv:2411.02337, 2024. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. https://arxiv.org/abs/2412.15115. Abdul Malik Sami, Zeeshan Rasheed, Kai-Kristian Kemell, Muhammad Waseem, Terhi Kilamo, Mika Saari, Anh Nguyen Duc, Kari Systä, and Pekka Abrahamsson. System for systematic literature review using multiple ai agents: Concept and an empirical evaluation. arXiv preprint arXiv:2403.08399, 2024. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei. Agentic retrieval-augmented generation: survey on agentic rag. arXiv preprint arXiv:2501.09136, 2025. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. Maojia Song, Shang Hong Sim, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, and Soujanya Poria. Measuring and enhancing trustworthiness of llms in rag through grounded attributions and learning to refuse. arXiv preprint arXiv:2409.11242, 2024. Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. Zerosearch: Incentivize the search capability of llms without searching. arXiv preprint arXiv:2505.04588, 2025. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. Prakhar Verma, Sukruta Prakash Midigeshi, Gaurav Sinha, Arno Solin, Nagarajan Natarajan, and Amit Sharma. Plan rag: Efficient test-time planning for retrieval augmented generation. In Workshop on Reasoning and Planning for Large Language Models, 2025. Shuting Wang, Jiejun Tan, Zhicheng Dou, and Ji-Rong Wen. Omnieval: An omnidirectional and automatic rag evaluation benchmark in financial domain. arXiv preprint arXiv:2412.13018, 2024a. Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, et al. Searching for best practices in retrieval-augmented generation. arXiv preprint arXiv:2407.01219, 2024b. Ziting Wang, Haitao Yuan, Wei Dong, Gao Cong, and Feifei Li. Corag: cost-constrained retrieval optimization system for retrieval-augmented generation. arXiv preprint arXiv:2411.00744, 2024c. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, et al. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning. arXiv preprint arXiv:2505.16421, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Shuo Yang, Yuqin Dai, Guoqing Wang, Xinran Zheng, Jinfeng Xu, Jinze Li, Zhenzhe Ying, Weiqiang Wang, and Edith C. H. Ngai. Realfactbench: benchmark for evaluating large language models in real-world fact-checking, 2025b. Shuo Yang, Zijian Yu, Zhenzhe Ying, Yuqin Dai, Guoqing Wang, Jun Lan, Jinfeng Xu, Jinze Li, and Edith C. H. Ngai. Rama: Retrieval-augmented multi-agent framework for misinformation detection in multimodal fact-checking, 2025c. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Tian Yu, Shaolei Zhang, and Yang Feng. Auto-rag: Autonomous retrieval-augmented generation for large language models. arXiv preprint arXiv:2411.19443, 2024. Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Bendersky. Inference scaling for long-context retrieval augmented generation. arXiv preprint arXiv:2410.04343, 2024. Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat. When scaling meets llm finetuning: The effect of data, model and finetuning method. arXiv preprint arXiv:2402.17193, 2024. Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, et al. Openresearcher: Unleashing ai for accelerated scientific research. arXiv preprint arXiv:2408.06941, 2024. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025."
        },
        {
            "title": "A Training Details",
            "content": "A.1 Sliding-Window-based Entropy Regulation Mechanism major obstacle in scaling reinforcement learning for LLMs is the occurrence of entropy collapse Yu et al. (2025), characterized by rapid sharp drop in policy entropy at the early training stage, which results in an overconfident policy and severely impairs exploration. To mitigate policy entropy collapse, we introduce Sliding-Window-based dynamic Entropy Regulation Mechanism (SWERM) applied at the granularity of training steps. Before introducing SWERM, we first define policy entropy as the average token-level entropy of the policy model πθ over the current batch B, which can be formulated as follows: H(πθ, B) = EB,πθ [log πθ(yty<t)] = 1 (cid:88) xB 1 y (cid:88) t=1 Eytπθ [log πθ(yty<t, x)] (13) where represents an input sampled from B, yt denotes the token generated at time step t, and y<t denotes the prefix sequence consisting of the first 1 tokens. In SWERM, sliding window of size is employed to track the average policy entropy over the latest training steps, and is defined as: HT = 1 (cid:88) i=T k+1 Hi (14) where denotes the current training step, Hi denotes the policy entropy at training step and HT is the average entropy computed over the sliding window at step . To monitor the stability of entropy reduction during training, we quantify the drop in from step 1 to step as follows: HT = HT 1 HT (15) HT serves as an effective indicator for measuring the smoothness of entropy drop. When HT > τ (where τ is threshold hyperparameter), it indicates collapse in HT , which significantly pulls down 16 Figure 6 Prompt for RRM to assess the Atomic Thoughts. HT . In contrast, HT τ suggests that the policy entropy is dropping smoothly. Accordingly, to mitigate entropy collapse, we increase the policy temperature and resample the outputs on the current batch whenever HT > τ is detected. Prompts Employed in Atom-Searcher Figure 7 Prompt for RRM to assess the Thought Process."
        }
    ],
    "affiliations": [
        "Ant Group"
    ]
}