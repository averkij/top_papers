{
    "paper_title": "DragMesh: Interactive 3D Generation Made Easy",
    "authors": [
        "Tianshan Zhang",
        "Zeyu Zhang",
        "Hao Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh."
        },
        {
            "title": "Start",
            "content": "DragMesh: Interactive 3D Generation Made Easy Tianshan Zhang Zeyu Zhang Hao Tang School of Computer Science, Peking University Equal contribution. Project lead. Corresponding authors: bjdxtanghao@gmail.com. 5 2 0 2 6 ] . [ 1 4 2 4 6 0 . 2 1 5 2 : r Figure 1. Results by DragMesh: Our method translates intuitive drag-and-drop actions into accurate joint motion. It correctly infers and generates the motion of rotational joints (e.g., microwave, bucket, door) and translational joints (e.g., desk drawer, oven rack). For each object group, the images from left to right typically depict the initial state, an intermediate generated motion, and the final articulated state. Code: https://github.com/AIGeeksGroup/DragMesh Website: https://aigeeksgroup.github.io/DragMesh"
        },
        {
            "title": "Abstract",
            "content": "While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains fundamental challenge. Current methods for articulated motion lie at crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, robust framework for real-time interactive 3D articulation built around lightweight motion generation core. Our core contribution is novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAEs non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering practical step toward generative 3D intelligence. 1. Introduction The pursuit of generative 3D intelligencesystems capable of autonomously constructing, understanding, and manipulating the physical worldhas long been central ambition of computer vision and graphics. While 2D diffusion and large-scale vision-language models have made striking progress in image synthesis and scene understanding, extending this generative capability into the 3D domain remains fundamentally more challenging. Unlike pixels, 3D structures obey physical constraints, exhibit compositional part relationships, and evolve through spatially consistent motion. complete 3D generative system must, therefore, not only represent what the world looks like but also how it moves and responds to interaction. Recent advances such as implicit neural fields (NeRFs), 3D Gaussian Splatting, and mesh-based transformers have largely solved static reconstruction: we can now generate photorealistic geometry and texture at scale [12, 14, 20, 24]. However, these methods remain fundamentally passivethey describe the world as it is, not how it behaves. Static generative priors lack notion of causal articulation or kinematic structure, making them unsuitable for realworld interactions, robotics, or design tasks that require physically grounded motion. natural next step for 3D generation is interactive articulationlearning how objects with movable parts respond to user or agent input. This direction bridges static synthesis and embodied reasoning, enabling models that understand both form and function [3, 10, 11, 37]. Yet, current solutions lie at two extremes: optimization-based methods that are physically consistent but too slow for real-time use, and large generative models that produce plausible motions but violate basic kinematic constraints. The field thus faces core dilemma between fidelity, physicality, and efficiency. Pioneering works such as DragApart [15] initiated the study of interactive generation, demonstrating that user drags could drive object deformation [12]. However, they operated only on projected geometry, lacking true 3D structure and physical grounding. Subsequent 3D Gaussian Splattingbased approaches like ArtGS [21] introduced explicit volumetric representations, achieving physically plausible effects but at the cost of scalabilityeach object requires separate model. Across these paradigms, recurring limitation persists: interactive fidelity remains bound by computational cost and model specificity. This dilemma arises because existing generalizable models attempt to solve two fundamentally different problems with one monolithic, heavyweight architecture: 1) Kinematic Reasoning (a one-time prediction of what can move) and 2) Motion Generation (a real-time synthesis of how it moves). This motivates our decoupled, probabilistic formulation, built around lightweight generation core that can generalize across diverse 3D meshes while enabling real-time interaction. Our paper presents three key contributions: Dual Quaternion VAE for Efficient Motion Learning. We introduce DualQuaternionVAE, compact and efficient architecture that learns articulated motions via geometrically principled dual quaternion encoding by transforming only articulated parts with FiLM-based conditioning, our single-pass generation ensures both kinematic consistency and computational efficiency. Decoupled Kinematic Reasoning and Motion Generation Framework. We construct decoupled framework that separates kinematic reasoning from generative motion synthesis. Our framework first separates semantic intent reasoning (determining the joint type) from geometric regression (which our Kinematics Prediction Network (KPP-Net) performs to find the axis and origin). The full predicted priors are then fed to the Dual-Quaternion VAE (DQ-VAE) to generate articulated motion. Efficiency and Generalization Trade-off. We compare our model with state-of-the-art methods, demonstrating that existing generalizable methods are computationally expensive, requiring 5-10 times more parameters and GFlops of computation. Conversely, other lightweight methods lack generalization ability and require individual training for each object. Our contribution is that DragMesh implements an optimally balanced framework, whose core generation module achieves robust generalization to new objects with low computational overhead. In summary, DragMesh bridges the gap between largescale generative modeling and real-time interactive articulation, offering practical step toward generative 3D intelligence. 2. Related Work 2.1. 3D Generation The evolution of 3D generation has been driven by persistent tension between representation fidelity and downstream editability. Early voxel-based methods [4, 30, 47] and implicit representations like DeepSDF [28] and NeRF [24] prioritized visual quality, achieving photorealistic reconstruction through dense volumetric encoding. However, their monolithic, continuous formulations produced \"black-box\" outputs that were inherently resistant to structural editing or part-level manipulation. Transformer-based reconstruction models (LRM [13], TripoSR [35]) and diffusion pipelines (TripoSG [17]) have since scaled these approaches to unprecedented fidelity; yet, they perpetuate the same fundamental limitation: they generate holistic, unstructured geometries that lack explicit part decomposition. To unlock editability, recent work has revisited explicit mesh representations [5, 6, 16, 18, 33, 36, 42, 46]. Unlike neural fields, meshes expose vertex-face topology directly, enabling fine-grained vertex editing, skeletal rigging, and seamless integration with physics engines and rendering pipelines. This explicit structure is not merely convenienceit is essential for downstream applications in animation, simulation, and interactive design. Our DragMesh builds directly upon these structured mesh and part-level generative foundations by taking their decomposable meshes as inputs and enabling real-time, physically consistent manipulation through lightweight dual-quaternion generative model. 2.2. Interactive Manipulation The paradigm of point-based interactive editing, pioneered in 2D by methods [26, 29, 32, 39, 44], cannot be directly applied to 3D scenarios that demand multi-view consistency and adherence to physical constraints, as these methods fundamentally manipulate pixels rather than underlying geometry. To address this, common paradigm is the \"2D-3D2D\" framework, where 2D object is first \"lifted\" to 3D representation, edited in 3D space where physical constraints can be applied, and then reprojected back into the original [2, 27, 41]. The existence of this complex workflow demonstrates the inherent limitations of direct 2D manipulation for 3D-consistent tasks. Manipulating objects with movable parts, such as opening drawer or folding laptop, presents level of complexity far beyond the scope of standard 3D generation models. To address this, some approaches have proposed using Transformer-based methods to autoregressively generate interactive components. Methods like MeshArt [8], ArtFormer [34], FreeArt3D [1], Articulate-Anything [31], and DIPO [38] focus specifically on generating objects with explicit parts and joints. DragAPart [15] learns part-level motion priors to predict plausible deformations from user drags. Even advanced representations like 3D Gaussian Splatting (3DGS) require specialized extensions, such as ArtGS [22], Part2GS [43] and RoboSimGS [45], which integrate physical modeling and part-aware representations to impose constraints and handle articulated motion. However, diffusion-based models, while capable of generating highfidelity 3D results, create an \"Interaction-Fidelity Chasm.\" Their iterative sampling process, which can take several minutes, forces users to choose between near-instant feedback with low-quality, non-generative methods (like direct mesh deformation) or high-fidelity generative results at the cost of significant latency [9, 10]. This latency significantly disrupts the seamless, iterative creative workflow that Figure 2. Bubble size indicates parameter count (core module only), color denotes generalization capability (Purple: generalizable; Orange: per-object training required). Our method achieves generalization with significantly lower computational cost than existing generalizable approaches. is standard in 2D design software, where low-latency, realtime adjustments are expected. 3. Methodology 3.1. Overview Our training framework (illustrated in Figure 3) decouples motion generation from kinematic reasoning. This design provides kinematic consistency by using dual quaternions and enables efficient, real-time inference by transforming only articulated parts. Dual Quaternion VAE (DQ-VAE): conditional generative model (Sec. 3.3) that outputs physically consistent motion trajectories. It is trained to produce dual quaternion sequences representing the articulation. Kinematics Prediction Pipeline: To enable annotationfree use, this pipeline first uses VLM to determine the semantic interaction type. Then, our proposed Kinematics Prediction Network (KPP-Net, Sec 3.5) regresses the precise geometric parameters. At inference, the parameters predicted by this pipeline are fed into the pre-trained DQ-VAE, enabling generative articulation on novel meshes. 3.2. Motion Representation and Pre-processing Traditional representations suffer from inherent drawbacks: Euler angles exhibit gimbal lock, axis-angle lacks translation encoding, and transformation matrices require 12 parameters per frame. Dual quaternions (DQ) provide compact (8-parameter), continuous, and singularity-free parameterization for rigid body transformations, naturally satisfying screw motion theory. Formally, given joint with unit axis R3 and origin R3, we generate ground-truth motion sequences of length = 16 by uniformly sampling angles θt [0, θmax] Figure 3. The DragMesh pipeline fuses point cloud, joint, and drag inputs through VAE and Transformer architecture to predict physically-corrected dual quaternion. (for revolute joints) or distances dt (for prismatic joints), where denotes quaternion multiplication, and the dual quaternion at time is: q(t) = [cos(θt/2), sin(θt/2)], q(t) = 1 [0, ttrans] 2 q(t) (1) where ttrans = o+q(t) by rotating around point o. For prismatic joint: o encodes the translation induced = [1, 0, 0, 0], q(t) q(t) = 1 2 [0, dta] (2) To establish canonical input space, we normalize each mesh by its axis-aligned bounding box center and largest dimension s. All spatial quantities (vertices, drag point pdrag, joint origin o) are transformed via ( c)/s, while vectors (drag vector vdrag, the translational component of qgt ) are scaled by 1/s. The joint axis remains unit vector. 3.3. Network Architecture In this section, we formalize our probabilistic articulation framework. Given an input mesh and user drag signal, our goal is to learn distribution p(qr, qdx, d) over dual quaternions representing rigid-body transformations, where denotes geometric features and denotes interaction cues. We achieve this through variational autoencoder that jointly encodes geometric and kinematic priors. 3.3.1. Multi-Modal Encoders We process the three input modalities using specialized encoders: 1. Joint Condition Encoder. This module encodes the fundamental kinematic constraints. It processes the discrete Joint Type (revolute/prismatic) through an Embedding layer, and the continuous Joint Axis and Joint Origin through separate MLPs. These are fused into 512-dimensional joint feature vector, fjoint. 2. Point Cloud Encoder. We sample 4096 points from the normalized mesh, concatenating the 3D coordinates (XYZ) with binary Part Mask (indicating which part moves) to form 4D input. PointNet-style encoder (PointCloudEncoder) processes this 4D point cloud to extract per-point features fpoints RN 1024. 3. Motion Intent Encoder. This module interprets the users drag interaction. It dynamically computes feature fmotion based on the joint type. For revolute joints, it combines features from the Rotation Direction and the Average Trajectory Velocity (processed by separate MLPs). For prismatic joints, it uses features derived from the Drag Chord using MultiScaleDragEncoder. An Amplitude feature is added to all types. Finally, separate Drag Trajectory feature, encoded via small Transformer, is concatenated with fmotion to capture the paths temporal shape regardless of joint type. 3.3.2. Feature Fusion and Conditional VAE Instead of simple concatenation, we fuse features in structured manner to preserve local and global information. To capture geometry relevant to the interaction, we sample local features from fpoints using k-Nearest Neighbors (kNN) around the joint origin and drag point. The outputs are processed by MLPs and fused. Critically, this module is modulated by fjoint via FiLM layer, allowing the joint type to influence which local features are salient. This produces context-aware local feature flocal. learned gating mechanism combines the outputs from all preceding modules. This MLP takes the concatenation of [flocal, fjoint, fmotion] and produces single fused motioncontext feature, ffused. To parameterize the latent space, the VAE encoder receives carefully constructed input: the concatenation of the fused feature and the raw joint feature, [ffused, fjoint]. This structure ensures the VAE is always strongly conditioned on the primary joint constraints, even after fusion. The encoder then outputs the latent parameters µ and log σ2. fcombined = Concat(ffused, fjoint), µ, log σ2 = MLPenc(fcombined), ϵ (0, I) = µ + σ ϵ, (3) 3.3.3. Conditioned Transformer Decoder and Physics Correction Our decoder must translate the latent code into temporally coherent motion sequence RT 8 while adhering to joint constraints. naive MLP decoder is insufficient, as it fails to model inter-frame correlations, leading to unrealistic, drifting motion. We also found standard autoregressive models unsuitable, as they suffer from error accumulation, where early errors cascade, and latent space collapse, where the decoder learns to ignore z, thus hindering the VAEs representation learning. We therefore employ non-autoregressive Transformer Encoder, which processes all = 16 frames in parallel. This design captures global sequence coherence without the fragility of autoregressive models and ensures every frame is robustly conditioned on z. To strongly enforce the kinematics, the decoder input sequence is formed by concatenating the positional encoding, the repeated latent code z, and multiple, scaled injections of the joint feature fjoint. This strong conditioning is maintained throughout the networks depth using FiLM (Feature-wise Linear Modulation), where fjoint predicts affine parameters (γ, β) to modulate each Transformer layers output. The Transformers output is first passed to an MLP to predict base sequence, Qbase. This is then refined by Physics Correction module, which predicts residual Qresidual such that the final output is Qfinal = Qbase + Qresidual. To provide maximum context for this correction, its input includes Qbase, the joint type embedding, and both raw and scaled versions of the joint feature fjoint and the joint axis a, ensuring the final motion strictly respects the physical constraints. 3.4. VAE Training Objectives Our objective is to train the VAE to generate motions that are not only geometrically accurate but also physically plausible and consistent with the specified joint constraints. We found that naive baseline objective, consisting only of geometric loss (Lcd) and direct quaternion reconstruction loss (Lqr, Lqd), was insufficient. With only these losses, the VAE struggles to learn meaningful and well-structured latent representation. The network fails to respect the underlying kinematics, often producing implausible motions, such as drifting or arbitrary rotations, rather than the intended motion around the specified joint axis. We found it essential to introduce set of explicit physical constraint losses. To resolve this, we found it essential to introduce set of explicit physical constraint losses. These constraints serve as strong inductive bias, forcing the network to learn the correct motion laws. Our final loss is weighted sum of four key components: = λmeshLmesh + λquat(Lqr + Lqd) + λcdLcd + λaxisLaxis + λqd0Lqd0 + λqr1Lqr1 + λklLKL (4) Reconstruction and Geometric Losses. We supervise the output at two complementary levels. Lmesh (L2 vertex loss) and Lcd (Chamfer Distance) ensure fine-grained geometric accuracy for the moving part. Concurrently, Lqr and Lqd provide direct supervision on the motion parameters themselves, using an L2 loss on the extracted 3D translation vector and dot-product-based distance for rotation. For all reconstruction losses, we apply 2x weight to the first frame (t = 0) to enforce an accurate starting pose. Physical Constraint Losses. This is the core component that enforces kinematic plausibility. Laxis dynamically enforces the joints degrees of freedom: for revolute joints, it penalizes misalignment between the predicted rotation axis and the ground-truth axis a; for prismatic joints, it penalizes any translation component perpendicular to a. Furthermore, we add two zero-motion penalties: Lqd0 penalizes any translation for revolute joints, while Lqr1 penalizes any rotation for prismatic joints. VAE Regularization with Free Bits. Finally, LKL regularizes the latent space. To prevent the \"posterior collapse\" common in VAEs (where the decoder ignores the latent code), we employ Free Bits strategy. We define minimum information threshold δ and only penalize the KL divergence for samples that fall below this threshold: LKL ="
        },
        {
            "title": "1\nB",
            "content": "(cid:88) max(0, DKL(q(z)p(z)) δ) (5) This encourages the VAE to encode meaningful information before being forced to match the prior, which resolves the training difficulties we initially observed. 3.5. Kinematics Prediction Network (KPP-Net) To enable annotation-free articulation at inference, we train separate network, KPP-Net, to predict kinematic parameters from geometry and user interaction. Unlike the DualQuaternion VAE, which models motion generation, KPPNet focuses on estimating the underlying joint configurationspecifically the axis and originfrom an articulated mesh, its part segmentation, and drag interaction. KPP-Net follows point-based design, composed of Transformer-based Point Encoder. This architecture uses two parallel encoders to extract features: Global Encoder processes 10D input (XYZ + part mask + repeated drag point/vector), while Local Encoder processes the partmasked mesh to focus on local geometry. The resulting global and local features are concatenated and fed into two independent regression heads, which predict the joint axis and origin respectively: [ˆa, ˆo] = fKPP(P, M, dp, dv) (6) where RN 3 denotes sampled points, the binary part mask, and dp, dv the drag interaction vectors. KPP-Net is trained independently using ground-truth URDF parameters with composite loss function combining weighted axis and origin losses: LKPP = λaxisLaxis + λoriginLorigin (7) Laxis is the geodesic loss, arccos(clamp(ˆa agt, 0.0, 1.0 ϵ)), which measures the angle between the predicted and ground-truth axes. Lorigin is the Smooth L1 Loss, which robustly regresses the origin coordinates. This formulation encourages the predicted joint axis to align with the true kinematic direction while accurately localizing the joint origin. 3.6. Inference Pipeline During inference, DragMesh operates in fully annotationfree setting, taking as input only raw mesh and userdefined drag interaction (pdrag, vdrag). The goal is to generate physically consistent articulated motion without any URDF annotations. The complete process proceeds in three stages. Part Segmentation. We segment the raw mesh into movable components using an off-the-shelf part segmentation model (e.g., P3-SAM [23]). This yields discrete part mask M. The users drag point pdrag is used to identify the target part Mmov (with mask = 1) from the static subset Msta (with mask = 0). This step provides the structural decomposition required for kinematic reasoning. Intent Reasoning and Kinematic Prediction. We found that while our KPP-Net (Sec. 3.5) excels at geometric regression, it struggles to reliably predict the motion intent (i.e., revolute vs. prismatic) when the input only contains mesh information. Relying solely on global point cloud data is insufficient to resolve this inherent semantic ambiguity.To address this, we decouple the task. First, we use VLM (e.g., GPT-4o) to provide the necessary semantic context by processing information about the interacting parts. The VLM robustly determines the high-level interaction intent, outputting the categorical joint type jtype. This predicted jtype is then fed into our pre-trained KPP-Net. We leverage KPP-Nets strength in geometric regression, using only its axis and origin heads to predict the precise kinematic parameters ˆa and ˆo. This combined approach provides the full kinematic constraints ˆc = (jtype, ˆa, ˆo) required by the VAE. Articulated Motion Generation. Finally, the full set of predicted kinematic parameters ˆc, along with the user drag features fdrag (e.g., dv and an interpolated trajectory) and mesh features, are fed into the trained Dual Quaternion VAE. The decoder produces temporal sequence of dual quaternions {(q(t) t=1.To realize the animation, we apply these transformations selectively to the movable vertices Mmov: (cid:40) , q(t) )}T DQApply(cid:0)(q(t) vi, , q(t) ), vi, ˆo(cid:1), if Mi = 1, if Mi = 0. (8) v(t) = where DQApply denotes the rigid transformation applied relative to the predicted origin ˆo. This completes the pipeline, enabling DragMesh to animate unseen meshes from raw inputs by combining LLM-based intent reasoning with KPP-Nets geometric prediction. 4. Experiments 4.1. Datasets and Evaluation Metrics Datasets. There exist multiple datasets containing articulated objects and joint annotation [9, 19, 25, 40], reflecting the growing interest in modeling 3D object articulation. We chose GAPartNet [10], which provides high-quality interaction models with part-level geometry, joint structures defined by URDF, and physically valid mobility parameters. It covers wide range of real-world categories and provides rich metadata and diverse articulated models. Figure 4. The DragMesh Annotation-Free Inference Pipeline. Given raw mesh and drag, segmentation model identifies the movable part while VLM predicts the joint type. Our KPP-Net then regresses the precise axis and origin, enabling the final DragMesh model to generate the Dual Quaternion animation. Metrics. We evaluate: (1) Geometric accuracy via Chamfer Distance (CD) [7] and Vertex L2 Error (Lmesh); (2) Physical constraints via Axis Error (milliradians) and Origin Error (millimeters) for joint parameters, plus zeromotion penalties for invalid revolute translations and prismatic rotations; (3) Latent quality via KL divergence to detect posterior collapse. 4.2. Comparative Study Our goal is to provide horizontal comparison of various interactive 3D content generation methods [9, 15, 21]. We uniformly adopt the Objaverse dataset as our benchmark. We observe that existing methods exhibit significant differences in their data requirements and preprocessing pipelines. For instance, some methods may require specific part annotations, watertight meshes, or are restricted to certain object categories. Therefore, forcing single, fixed data subset for training all models would fail to fairly evaluate the optimal performance of each method. Consequently, our adopted strategy is as follows: We adhere to the official implementation of each competing method, preparing and filtering specific data subset for each from our pre-screened Objaverse interactive corpus. We will detail the specific data processing steps taken for each method in the appendix. Given the differences in input data and training setups, our comparison will primarily focus on three aspects: the complexity of data processing, computational overhead (performance consumption), and the qualitative fidelity of the final generated results. Detailed data is available in the supplementary material 7. 4.3. Ablation Study We validate our design choices through comprehensive ablations on model architecture  (Table 1)  , loss functions  (Table 2)  , and the KPP geometry predictor  (Table 3)  . Architecture Components. Table 1 reveals critical failure mode in the baseline (M2): despite low physics error, the KL value indicates that the model did not learn the correct information, generating only minimal motion (<5). Adding encoder fusion and FiLM conditioning (M3-M5) enables expressive motion but initially destabilizes physical accuracy. Our full model (M7) resolves this tension through Physics Correction (M6) and tailored loss terms, achieving optimal reconstruction, physical plausibility, and motion expressiveness. Loss Components. Table 2 demonstrates clear synergy between our physics losses and free-bits KL. Physics losses Table 1. Additive Ablation of Model Architecture. Each row progressively adds one component to the previous configuration. ID Model Variant (Progressive Build) M1 Base: VAE + MLP Decoder M2 M1 + Transformer Decoder (Base) M3 M2 + Encoder Fusion (LFS + FFG) M4 M3 + LFS FiLM Encoder M5 M4 + Transformer FiLM M6 M5 + Physics Correction (PCM) M7 M6 + Physical Loss Terms (Ours) Reconstruction Error CD (103) Physical Constraint Error VAE Axis Error (103) QR1 (Pris) KL (Raw) ... 93.5 184.4 78.773 63.561 71.814 61.485 ... 0.28 0.12 16.74 0.45593 0. 0.265 ... 0 0.0017 1.457 0.0004 0 2.68 2.578 18.88 29.85 36.96 64.55 0 26.0515 Table 2. Loss Function Ablation. Baseline uses only reconstruction and geometry losses with standard KL. Model Physical Error Recon. & VAE 50% improvement. Note that rows 4-6 all use the complete feature set (mesh, mask, and drag), isolating the impact of architectural design on kinematic regression accuracy. Axis Rev. Trans. CD KL 5. Efficiency (1) Baseline (2) + Physics (3) + Free Bits (4) Ours (Full) 1.301 0.0006 0.680 0.0004 0.130 0.014 0.080 0. 0.0577 0.0653 0.0679 0.051 67.35 34.56 21.49 12.90 Table 3. KPP Geometry Predictor Ablation. Progressive improvements. Configuration Axis (mrad) Origin (mm) Baseline (PointNet) + Mask + Drag + Attention Encoder + Decoupled Heads Ours (Full) 450.0 350.0 300 150.0 80.0 45.0 15.0 12.0 >10.0 6.0 3.5 1.8 alone (2) drastically reduce constraint violations but degrade reconstruction. Free bits alone (3) improve VAE stability but fail to enforce physical correctness. Combining both (4) yields the best results across all metrics, indicating that the free bits provide the capacity for complex motions while physics losses guide geometrically accurate and physically plausible solutions. KPP Geometry Predictor. Table 3 reveals that architectural improvements dominate performance gains. Starting from PointNet baseline with only mesh geometry, progressively adding mask and drag features (rows 1-3) yields modest improvements, with the drag feature even degrading origin prediction. The critical breakthroughs come from architectural changes: replacing PointNet with dual-stream attention encoder (row 4) achieves 2 error reduction, and decoupling the prediction heads (row 5) provides another Figure 2 illustrates the trade-offs between our method and state-of-the-art (SOTA) methods in terms of parameter count and GFLOPs in the core generation module. Clearly, existing generalizable models (purple bubbles) incur 5 to 10 times higher computational costs than ours due to their attempt to solve all problems with single, large end-to-end model. Other lightweight methods (orange bubbles) sacrifice generalization ability, requiring separate training for each object. This comparison focuses on the core generation architecture, which is standard and fair evaluation method in the field because it isolates the actual overhead of the model in real-time interactive loops. Therefore, this comparison fairly excludes the individual upstream, oneoff costs of all models (including ours), such as generic part segmentation preprocessing. Similarly, the overhead of (one-time) VLM calls used for semantic reasoning in our framework was intentionally moved out of this real-time loop by our decoupling design, thus boosting efficiency. 6. Conclusion In this paper, we propose DragMesh, lightweight framework designed to bridge the gap between static 3D generation and real-time physics interaction. Current methods are either too slow or lack physical consistency. We address this dilemma with decoupled framework that separates semantic reasoning from geometric generation. We first leverage an LLM for semantic reasoning to determine the joint type. This intent is then fed into our kinematic prediction network (KPP-Net), which performs geometric reasoning to regress the precise joint axes and origin. Finally, novel, lightweight core built around thisthe FiLM-conditional Dual Quaternion VAEgenerates physically consistent and temporally smooth animation conditioned on these complete kinematic parameters."
        },
        {
            "title": "References",
            "content": "[1] Chuhao Chen, Isabella Liu, Xinyue Wei, Hao Su, and Minghua Liu. Freeart3d: Training-free articulated object In SIGGRAPH Asia 2025 generation using 3d diffusion. Conference Papers, 2025. 3 [2] Honghua Chen, Yushi Lan, Yongwei Chen, Yifan Zhou, and Xingang Pan. Mvdrag3d: Drag-based creative 3d editing via multi-view generation-reconstruction priors. arXiv preprint arXiv:2410.16272, 2024. 3 [3] Minghao Chen, Roman Shapovalov, Iro Laina, Tom Monnier, Jianyuan Wang, David Novotny, and Andrea Vedaldi. Partgen: Part-level 3d generation and reconstruction with multi-view diffusion models. arXiv preprint arXiv:2412.18608, 2024. 2 [4] Qimin Chen, Zhiqin Chen, Hang Zhou, and Hao Zhang. Shaddr: interactive example-based geometry and texture generation via 3d shape detailization and differentiable rendering. In SIGGRAPH Asia 2023 Conference Papers, pages 111, 2023. 2 [5] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, et al. Meshanything: Artist-created mesh generation with autoregressive transformers. arXiv preprint arXiv:2406.10163, 2024. [6] Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, and Guosheng Lin. Meshanything v2: Artist-created mesh generation with adjacent mesh tokenization. arXiv preprint arXiv:2408.02555, 2024. 3 [7] Haoqiang Fan, Hao Su, and Leonidas Guibas. point set generation network for 3d object reconstruction from single image. In CVPR, pages 605613, 2017. 7 [8] Daoyi Gao, Yawar Siddiqui, Lei Li, and Angela Dai. Meshart: Generating articulated meshes with structure-guided transformers. In CVPR, 2025. 3 [9] Mingju Gao, Yike Pan, Huan-ang Gao, Zongzheng Zhang, Wenyi Li, Hao Dong, Hao Tang, Li Yi, and Hao Zhao. Partrm: Modeling part-level dynamics with large cross-state reconstruction model. In CVPR, pages 70047014, 2025. 3, 6, 7 [10] Haoran Geng, Helin Xu, Chengyang Zhao, Chao Xu, Li Yi, Siyuan Huang, and He Wang. Gapartnet: Crosscategory domain-generalizable object perception and manipulation via generalizable and actionable parts. arXiv preprint arXiv:2211.05272, 2022. 2, 3, 6 [11] Junfu Guo, Yu Xin, Gaoyi Liu, Kai Xu, Ligang Liu, and Ruizhen Hu. Articulatedgs: Self-supervised digital twin modeling of articulated objects using 3d gaussian splatting. In CVPR, pages 2714427153, 2025. 2 [12] Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, Wanli Ouyang, Yan-Pei Cao, and Yangguang Li. Sparseflex: High-resolution and arbitrary-topology 3d shape modeling. arXiv preprint arXiv:2503.21732, 2025. [13] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 2 [14] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., pages 1391, 2023. 2 [15] Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi. Dragapart: Learning part-level motion prior for articulated objects. In ECCV, pages 165183, 2024. 2, 3, 7 [16] Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, CraftsXuelin Chen, Ping Tan, and Xiaoxiao Long. man3d: High-fidelity mesh generation with 3d native genarXiv preprint eration and interactive geometry refiner. arXiv:2405.14979, 2024. 3 [17] Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. Triposg: High-fidelity 3d shape arXiv synthesis using large-scale rectified flow models. preprint arXiv:2502.06608, 2025. 2 [18] Stefan Lionar, Jiabin Liang, and Gim Hee Lee. Treemeshgpt: Artistic mesh generation with autoregressive tree sequencing. In CVPR, pages 2660826617, 2025. [19] Liu Liu, Wenqiang Xu, Haoyuan Fu, Sucheng Qian, Yang Han, and Cewu Lu. Akb-48: real-world articulated object knowledge base, 2022. 6 [20] Minghua Liu, Chong Zeng, Xinyue Wei, Ruoxi Shi, Linghao Chen, Chao Xu, Mengqi Zhang, Zhaoning Wang, Xiaoshuai Zhang, Isabella Liu, Hongzhi Wu, and Hao Su. Meshformer: High-quality mesh generation with 3d-guided reconstruction model. arXiv preprint arXiv:2408.10198, 2024. 2 [21] Yu Liu, Baoxiong Jia, Ruijie Lu, Junfeng Ni, Song-Chun Zhu, and Siyuan Huang. Artgs: Building interactable replicas of complex articulated objects via gaussian splatting. arXiv preprint arXiv:2502.19459, 2025. 2, 7 [22] Yu Liu, Baoxiong Jia, Ruijie Lu, Junfeng Ni, Song-Chun Zhu, and Siyuan Huang. Building interactable replicas of complex articulated objects via gaussian splatting. In ICLR, 2025. 3 [23] Changfeng Ma, Yang Li, Xinhao Yan, Jiachen Xu, Yunhan Yang, Chunshi Wang, Zibo Zhao, Yanwen Guo, Zhuo Chen, and Chunchao Guo. P3-sam: Native 3d part segmentation. arXiv preprint arXiv:2509.06784, 2025. 6 [24] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. [25] Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas J. Guibas, and Hao Su. PartNet: largescale benchmark for fine-grained and hierarchical part-level 3D object understanding. In CVPR, 2019. 6 [26] Xingang Pan, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag your gan: Interactive point-based manipulation on the generative image manifold. In ACM SIGGRAPH, 2023. 3 [27] Karran Pandey, Paul Guerrero, Matheus Gadelha, Yannick Hold-Geoffroy, Karan Singh, and Niloy Mitra. Diffusion handles enabling 3d edits for diffusion models by lifting activations to 3d. In CVPR, pages 76957704, 2024. 3 [41] Yuhuan Xie, Aoxuan Pan, Ming-Xian Lin, Wei Huang, YiHua Huang, and Xiaojuan Qi. 2d instance editing in 3d space. arXiv preprint arXiv:2507.05819, 2025. 3 [42] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 3 [43] Tianjiao Yu, Vedant Shah, Muntasir Wahed, Ying Shen, Kiet Nguyen, and Ismini Lourentzou. Part2gs: Part-aware modeling of articulated objects using 3d gaussian splatting. arXiv preprint arXiv:2506.17212, 2025. [44] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In CVPR, pages 38363847, 2023. 3 [45] Haoyu Zhao, Cheng Zeng, Linghao Zhuang, Yaxi Zhao, Shengke Xue, Hao Wang, Xingyue Zhao, Zhongyu Li, Kehan Li, Siteng Huang, et al. High-fidelity simulated data generation for real-world zero-shot robotic manipuarXiv preprint lation learning with gaussian splatting. arXiv:2510.10637, 2025. 3 [46] Ruowen Zhao, Junliang Ye, Zhengyi Wang, Guangce Liu, Yiwen Chen, Yikai Wang, and Jun Zhu. Deepmesh: Autoregressive artist-mesh creation with reinforcement learning. In ICCV, 2025. 3 [47] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel diffusion. In ICCV, pages 58265835, 2021. 2 [28] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In CVPR, 2019. 2 [29] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, MingChang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. [30] Kebin Peng, Rifatul Islam, John Quarles, and Kevin Desai. Tmvnet: Using transformers for multi-view voxel-based 3d reconstruction. In CVPR Workshops, 2022. 2 [31] Xiaowen Qiu, Jincheng Yang, Yian Wang, Zhehuan Chen, Yufei Wang, Tsun-Hsuan Wang, Zhou Xian, and Chuang Gan. Articulate anymesh: Open-vocabulary 3d articulated objects modeling. arXiv preprint arXiv:2502.02590, 2025. 3 [32] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive In CVPR, pages 88398849, point-based image editing. 2024. 3 [33] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes In CVPR, pages 19615 with decoder-only transformers. 19625, 2024. 3 [34] Jiayi Su, Youhe Feng, Zheng Li, Jinhua Song, Yangfan He, Botao Ren, and Botian Xu. Artformer: Controllable genIn CVPR, pages eration of diverse 3d articulated objects. 18941904, 2025. 3 [35] Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from single image. arXiv preprint arXiv:2403.02151, 2024. 2 [36] Zhengyi Wang, Jonathan Lorraine, Yikai Wang, Hang Su, Jun Zhu, Sanja Fidler, and Xiaohui Zeng. Llama-mesh: Unifying 3d mesh generation with language models. arXiv preprint arXiv:2411.09595, 2024. [37] Di Wu, Liu Liu, Zhou Linli, Anran Huang, Liangtu Song, Qiaojun Yu, Qi Wu, and Cewu Lu. Reartgs: Reconstructing and generating articulated objects via 3d gaussian splatting with geometric and motion constraints. arXiv preprint arXiv:2503.06677, 2025. 2 [38] Ruiqi Wu, Xinjie Wang, Liu Liu, Chunle Guo, Jiaxiong Qiu, Chongyi Li, Lichao Huang, Zhizhong Su, and Ming-Ming Cheng. Dipo: Dual-state images controlled articulated obarXiv preprint ject generation powered by diverse data. arXiv:2505.20460, 2025. 3 [39] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In ECCV, pages 331348, 2024. 3 [40] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, and Hao Su. SAPIEN: simulated part-based interactive environment. In CVPR, 2020. 6 DragMesh: Interactive 3D Generation Made Easy"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Implementation Details For our comparative study 4.2, we evaluated DragMesh against several baselines, including MeshArt, PartRM. We used the official, publicly available code for all methods. To enable fair qualitative comparison, we performed custom data preparation work tailored to each baseline method. This custom processing was necessary to adapt our data to meet the specific input requirements of each model, enabling them to achieve minimal level of interactive functionality on our dataset. critical step in constructing our interaction dataset was generating corresponding drag points and drag vectors for each articulated part. Our generation pipeline is as follows: First, we parsed the kinematic data from our curated set of interactive models and converted it into JSON format. Subsequently, we randomly sampled points Pstart on the surface of each movable part to serve as the drag points. To generate the corresponding drag vector Vdrag, we calculated the points motion using the trajectory defined in the JSON file. We identified key challenge: for large-angle rotations (e.g., exceeding 180), defining Vdrag by simply subtracting the start point from the end point of the full trajectory introduces ambiguity and fails to yield unique, correct direction. To ensure the rigor of our data and the absolute correctness of the motion, we instead adopted strategy based on instantaneous motion. We strictly define the \"end point\" Pend as the position of the start point Pstart at the immediately subsequent timestep (t + 1). Thus, the drag vector is Vdrag = Pend Pstart, which guarantees that Vdrag accurately reflects the points local direction of motion. Finally, we recognized that the magnitude of this instantaneous vector might be too short to simulate complete user interaction. We employed two strategies to generate the final vector: (1) accumulating the displacement over several subsequent timesteps, contingent on the points trajectory remaining relatively linear (i.e., without significant deviation), or (2) directly applying fixed scaling factor to the original instantaneous vector Vdrag to extend it to standard length. PartRm The data requirements of the PartRM baseline model are significantly incompatible with our workflow. The model relies on set of discrete 3D states (e.g., state 0 to state 5) as input, while our Objaverse corpus consists of GLB models and continuous Trajectory JSON trajectories. Therefore, generating effective qualitative comparisons for PartRM requires customized data adaptation pipeline and modifications to its core code. We implemented data generation pipeline to bridge PartRMs discrete state requirements with our continuous trajectory data: First, we take state 0 (the stationary state) as input, parse the Trajectory JSON, and automatically render all six discrete states (state 0 to state 5) required by PartRM, each containing 12 views. Next, we implemented the logic for automatically solving 2D drag vectors from the 3D trajectory data and generated the propagated drags index file necessary for the PartRM evaluation process. In addition, we modified its data loader to support the use of our newly generated propagated drags as interactive guides when mesh is missing. When Zero123++ image is detected as missing, the loader automatically falls back to using the original Blender-rendered image, allowing the process to continue. It is worth noting that due to the significant differences between PartRMs input requirements and our data source, even after adaptation, various issues such as missing 2D drag vectors due to insufficient information still occur. ArtGS In our data preprocessing pipeline, we utilize Blender scripts to render multi-view images and their corresponding depth maps for each model in the dataset. During initial experiments, we observed that some default camera poses were positioned too low or too close, resulting in heavily blurred or partially occluded images that failed to capture effective geometric information. To address this, we implemented targeted adjustment to our rendering strategy, moving these specific camera perspectives to farther position. However, this adjustment introduced downstream challenge: during the 3DGS backpropagation process, these distanced views would sometimes encounter \"no visible Gaussians\" issue due to sparse points, leading to abnormal gradient calculations. To mitigate this, we adopted trade-off training strategy: during training iterations, when the model detects these views that cannot be effectively backpropagated due to camera positioning, the system automatically skips the corresponding depth loss and uses only the RGB loss for photometric supervision. While this adaptive loss mechanism ensured effective gradient flow, allowing the model to complete training and avoid pipeline crash, we must also note that the quality of the views generated using only this RGB supervision was exceptionally poor, with significant loss of geometric detail. Figure 5. Qualitative Comparison. Our method (Ours) generates plausible interactions across all categories. Blank spaces for baselines (e.g., ArtGS, PartRm, DragApart) represent generation failures, where results are omitted due to unrecognizable outlines. DragAPart The DragAPart baseline model operates primarily in 2D image space, making its data preparation pipeline distinct. We first rendered the 3D GLB files from Objaverse into static 2D RGB image, which serves as the models conditional input. To generate the required 2D drags tensor, we initially attempted to project our 3D kinematic data. We parsed our Trajectory JSON files, calculated 3D motion vector, and then normalized this 3D information for the 2D plane. However, we found that for model with no 3D perception, this 3D-to-2D normalization process introduced significant information deviation and larger errors. Consequently, we abandoned this approach and adopted direct 2D-native strategy. We chose to pick points directly on the 2D object (the RGB image), selecting start and end points based on our own intended interaction. This manually-defined 2D vector was then fed into the model as the drags tensor to process the single RGB image. time. We bypassed the VLM used for motion type classification. During training, we employed heuristic annotation strategy: we forced the application of translation and rotation strategies to each model, and then intuitively selected and manually labeled the object with unique, correct, and reasonable interaction type. 8. Visualization Results In our qualitative comparison 5, we must also note that certain baseline methods fail completely when processing specific interactive data. For these cases, we mark the result as blank, as the generated effect was extremely poor (e.g., the outline information was entirely unrecognizable) and provided no basis for meaningful comparison. 9. Limitations and Future Work 9.1. Limitations Ours During the training data preparation phase, we made two key modifications to the data to meet the specific architectural requirements of the DragMesh model. After acquiring the drag points and trajectory information, we first forced each object to perform single-joint interactions, as our model can only handle the movement of one joint at While the DragMesh framework achieves real-time, generalizable articulation, our approach currently presents several limitations which guide future research. 1. The models kinematic scope is restricted to singlejoint interactions involving only two fundamental motion types: simple translation and rotation around fixed noisy joint parameter predictions. The goal is to explore how to imbue models with robust physical common sense while maintaining compact and efficient architecture, enabling truly real-time, physically plausible intelligent agents. axis, lacking the capacity for more complex features such as screw motion or multi-joint chains. 2. Achieving robust kinematic prediction is highly sensitive to the initial geometric input. Our pipeline requires strict adherence to the expected data representation; otherwise, errors in prerequisite information, such as incorrect joint axis prediction, can arise, resulting in physically implausible mesh transformations. 3. Our annotation-free inference pipelines ability to clasprismatic) sify the motion type (e.g., revolute vs. currently relies on the assistance of external VisionLanguage Models (VLMs) for semantic reasoning. While integrating VLMs represents the current mainstream approach for handling semantic intent, we acknowledge that their inherent error rate and external dependency impact the robustness of our prediction pipeline. Removing this reliance to create fully selfcontained, robust geometric model remains core challenge for future work. 9.2. Future work Interactive generation is progressively gaining traction in the field. In essence, it can be viewed as key component of World Model, that is, system processing the behavior of scene at the next timestep, given specific condition. We believe core trend for future world models, regardless of the scenario, will be primary focus on lightweight generation. This trend is particularly critical for realizing digital Human-Object Interaction (HOI) and for robotics applications in the sim2real domain. These frontier domains not only demand deeper understanding of physical laws (such as kinematics and dynamics) but also impose extremely low-latency requirements on response speed. Therefore, future work must focus on finding new equilibrium between the depth of physical understanding and the speed of lightweight generation. For DragMesh, this specifically means: 1. VLM Dependency: Our current inference pipeline relies on an external VLM for semantic intent classification. primary future task is to explore methods for \"internalizing\" this reasoning capability, perhaps through more powerful geometric encoders or by leveraging end-toend interaction data, to achieve truly lightweight and self-contained framework. 2. Expanding Kinematic Scope: The model is currently limited to single-joint translation and rotation. The next step is to extend this to more complex motions, such as screw motion and multi-joint kinematic chains. 3. Enhancing System Robustness: Future work must address the gap between training (which uses ground-truth data) and inference (which relies on upstream predictions ). This involves making the model less sensitive to potential errors from upstream part segmentation or"
        }
    ],
    "affiliations": [
        "School of Computer Science, Peking University"
    ]
}