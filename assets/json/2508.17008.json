{
    "paper_title": "EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks",
    "authors": [
        "Yan Cathy Hua",
        "Paul Denny",
        "Jörg Wicker",
        "Katerina Taskova"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Every year, most educational institutions seek and receive an enormous volume of text feedback from students on courses, teaching, and overall experience. Yet, turning this raw feedback into useful insights is far from straightforward. It has been a long-standing challenge to adopt automatic opinion mining solutions for such education review text data due to the content complexity and low-granularity reporting requirements. Aspect-based Sentiment Analysis (ABSA) offers a promising solution with its rich, sub-sentence-level opinion mining capabilities. However, existing ABSA research and resources are very heavily focused on the commercial domain. In education, they are scarce and hard to develop due to limited public datasets and strict data protection. A high-quality, annotated dataset is urgently needed to advance research in this under-resourced area. In this work, we present EduRABSA (Education Review ABSA), the first public, annotated ABSA education review dataset that covers three review subject types (course, teaching staff, university) in the English language and all main ABSA tasks, including the under-explored implicit aspect and implicit opinion extraction. We also share ASQE-DPT (Data Processing Tool), an offline, lightweight, installation-free manual data annotation tool that generates labelled datasets for comprehensive ABSA tasks from a single-task annotation. Together, these resources contribute to the ABSA community and education domain by removing the dataset barrier, supporting research transparency and reproducibility, and enabling the creation and sharing of further resources. The dataset, annotation tool, and scripts and statistics for dataset processing and sampling are available at https://github.com/yhua219/edurabsa_dataset_and_annotation_tool."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 8 0 0 7 1 . 8 0 5 2 : r EDURABSA: AN EDUCATION REVIEW DATASET FOR ASPECT-BASED SENTIMENT ANALYSIS TASKS Yan Cathy Hua Paul Denny Jörg Wicker Katerina Taskova School of Computer Science, University of Auckland, New Zealand"
        },
        {
            "title": "ABSTRACT",
            "content": ""
        },
        {
            "title": "Introduction",
            "content": "for reporting low-granularity Every year, most educational institutions seek and receive an enormous volume of text feedback from students on courses, teaching, and overall experience. Yet, turning this raw feedback into useful insights is far from straightforward. It has been long-standing challenge to adopt automatic such education opinion mining solutions review text data due to the content complexity and requirements. Aspect-based Sentiment Analysis (ABSA) offers promising solution with its rich, sub-sentence-level opinion mining capabilities. However, existing ABSA research and resources are very heavily focused on the commercial domain. In education, they are scarce and hard to develop due to limited public datasets and strict data protection. high-quality, annotated dataset is urgently needed to advance research in this under-resourced area. In this work, we present EDURABSA (EDUCATION REVIEW ABSA), the first public, annotated ABSA education review dataset that covers three review subject types (course, teaching staff, university) in the English language and all main ABSA tasks, including the under-explored implicit aspect and implicit opinion extraction. We also share ASQE-DPT (DATA PROCESSING TOOL), an offline, lightweight, installation-free manual data annotation tool that generates labelled datasets for comprehensive ABSA tasks from single-task annotation. Together, these resources contribute to the ABSA community and education domain by removing the dataset barrier, supporting research transparency and reproducibility, and enabling the creation and sharing of further resources. The dataset, annotation tool, and scripts and statistics for dataset processing and sampling are available at https://github.com/yhua219/edurabsa_ dataset_and_annotation_tool. Keywords dataset, annotation tool, ABSA, aspect-based sentiment analysis, education domain, student feedback ASPECT-BASED Sentiment Analysis (ABSA) is type of fine-grained sentiment analysis that identifies opinions and their target entities or their attributes (aspects) at the sub-sentence level, and classifies them into categories and/or sentiment polarities [1, 2]. An ABSA problem comprises one or more subtasks that output various combinations of these components as shown in Table 1. This low granularity and the ability to turn varied raw expressions into standardised category labels make ABSA an ideal choice and sentiment for mining targeted insights from nuanced opinionated text It has been applied to mine online reviews and social media posts across domains, from product/services to public policies, news, and healthcare [1]. However, distinguishing different and relevant opinions and aspects within sentence relies heavily on the textual context and domain knowledge. This makes ABSA particularly domain-dependent and thus resource-sensitive[1, 2]. [2, 3]. Despite the rapid growth of ABSA research in the last decade, the availability of public dataset resources has remained heavily concentrated in the commercial product and service review domain, while important domains such as healthcare and education are disproportionately under-represented [1]. In recent systematic review of ABSA studies published between 2008 and 2023 [1], education review emerged as the third most common domain, yet it was represented by only 12 primary studies (2.31%) out of the 519 reviewed. The situation has not substantially improved we replicated the search for peer-reviewed literature from 2024 to mid-2025 and identified only three additional studies in this domain. Together, these 15 education review ABSA studies highlight shortage of public datasets in this domain: as shown in Table 3, 12 of them created datasets from scratch, nine went through manual annotation, and only two offer the labelled datasets upon request. The lack of public ABSA datasets reflects primary challenge with education reviews: such data often comes from course and teaching evaluations or student surveys EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks Table 1: Example of ABSA components and subtask outputs Example text Its expensive but the pizza is good. Aspect (implicit) 1 pizza ABSA Component Relation group 1 Relation group ABSA Tasks Aspect Extraction (AE) Opinion Extraction (OE) Opinion expensive good Output [null, pizza] [expensive, good] Category Sentiment price food neutral positive Aspect Category Detection (ACD) [<null, price>, <pizza, food>] Aspect Sentiment Classification (ASC) [<null, neutral>, <pizza, positive>] Aspect-Opinion Pair Extraction (AOPE) Aspect Sentiment Triplet Extraction (ASTE) 2 Aspect-Sentiment Quadruplet Extraction (ASQE) 3 [<null, expensive>, <pizza, good>] [<null, expensive, neutral>, <pizza, good, positive>] [<null, expensive, price, neutral>, <pizza, good, food, positive>] 1 This is an implicit aspect that is absent from the text (marked null) but its category and relations can be inferred from the context. 2 Despite the task name, each ASTE triplet contains an aspect term, an opinion term, and sentiment label. 3 Despite the task name, each ASQE quadruplet contains an aspect term, an opinion term, category label, and sentiment label. protected by institutional data rules, and thus cannot be publicly shared. Some institutions also restrict third-party tools or servers allowed to access this data, limiting the data-handling choice and capability. This shortage of public datasets has slowed the education sectors adoption of ABSA solution well suited to persistent challenge: making sense of the vast Institutions collect volume of student review text. this feedback regularly, but struggle to extract timely, actionable insights to inform reporting and intervention. Student length, reviews vary widely in formality, and complexity. single review or sentence can touch on multiple subtopics, making review-level analysis difficult. However, institutions often wish to analyse student feedback on specific aspects (e.g. course organisation, assessments, teaching quality) and aggregate or slice them across levels and dimensions (e.g. by courses, departments, programmes) and join with student or course attributes. This requires solution that can identify target aspects and related opinions within review entries, and also standardise them for further aggregation and analysis [4, 5]. These objectives are exactly what ABSA offers through its subtasks [1, 2]: parsing through text to extract relevant opinion and aspect expressions within each sentence (AE, OE, AOPE), and assigning them pre-defined categoryand/or sentiment-labels (ACD, ASC, ASTE, ASQE). The ABSA output is at the sub-sentence level and linked to each review entry, and the category and sentiment labels can be easily aggregated and analysed. In reality, few educational institutions have the resources to develop their own ABSA datasets, models, or evaluation benchmarks for assessing the quality of 2 external ABSA tools when allowed by the data policy. As result, student review texts are often under-analysed and under-reported due to the huge volume and the lack of an appropriate automated solution [6, 7]. Given the domain-dependence and the dominance of supervised training methods among existing ABSA studies [1], having public ABSA-annotated student review dataset is thus essential for enabling the creation of more public resources and models in this domain. 1.1 Our Work and Contributions address shortage the ABSA dataset We in the education review domain by presenting EDURABSA (EDUCATION REVIEW ABSA), collection of 6,500 entries of real student reviews in the English language on courses, teaching staff, and university. The dataset was manually annotated and covers all main ABSA tasks, including challenging and less studied ones. We also share ASQE-DPT, an offline, installation-free, It supports lightweight ABSA data annotation tool. manual annotation of all main ABSA tasks, and can automatically parse ASQE and ASTE annotations into other task dataset files. This tool is simple to use and is suitable for all ABSA domains, particularly low-resource for processing protected data or environments. for Our resources make the following contributions and impact: 1. Uniqueness and Necessity: To our knowledge, EduRABSA is the first public dataset of student teaching reviews on all three subjects (course, EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks Figure 1: The overall workflow of creating the EduRABSA dataset staff, and university), including annotations for comprehensive ABSA tasks. As Table 2 shows, it has 7.8 times more reviews and 4.5 times more sentences than the three SemEval 2015 ABSA benchmarks datasets combined [8]. It fills long-standing resource gap in the ABSA community, enables research advancement in this under-studied domain, and contributes to the collaboration, transparency, and reproducibility of education review ABSA research by removing the data barrier. 2. Dataset Usefulness: and The EduRABSA dataset their all ABSA components annotates relationships, and thus can support model training and evaluation of all main ABSA tasks. This includes AE and OE for both explicit and implicit aspects and opinions, ACD, ASC, and the composite tasks AOPE, ASTE, ASQE, and the overall review-level sentiment classification (SA). Its task range supports model training and evaluation with singleand multi-task approaches, under the pipeline and end-to-end frameworks [2, 9, 1, 10]. As the majority of existing ABSA approaches are supervised [1], it can also be used for data augmentation and enables more diverse ABSA approaches such as semi-supervised learning and contrastive learning (e.g. [11, 12]). The dataset is machine-learning ready and is presented in multiple formats, including that for the popular PyABSA [13] platform. 3. Source Data Authenticity: The dataset was sourced from public-domain licensed, unannotated student review text. The reviews have low risk of being machine-generated as the majority (6,000 out of 6,500) of the original entries were published before the prevalence of Generative Large Language Models (LLMs). 4. Robustness: The dataset supports the development of robust models by incorporating realistic complexity and challenging ABSA tasks. It was designed based on our observations of real-world student reviews to reflect comparable range of length, language style, and content complexity, featuring multiple aspects and sentiments in most sentences, as illustrated in Table 3 2. The annotation captures the prevalent yet largely overlooked implicit opinion expression, which will enable better model generalisation [12, 14, 1]. 5. Reproducibility : To ensure reproducibility and encourage collaboration and further development, we also share the sampling script, sample distribution analysis code, annotation protocol and notes, dataset format conversion script, and raw annotation file. 6. Enabling Further Resource Creation: The ASQE-DPT tool is highly mobile and lowers the barrier of data annotation with its installation-free, is useful single-file, browser-based design. tool for low-resource settings and safe offline choice for protected data. It also supports annotation checking and modification, and thus contributes to the improvement and adaptation of existing ABSA datasets. For example, the raw annotation files we share can be directly uploaded to this tool for further quality improvement or category changes. It annotation dataset, and scripts mentioned above Our data at dataset_and_annotation_tool. source available https://github.com/yhua219/edurabsa_ tool, and the are"
        },
        {
            "title": "2 Related Work",
            "content": "Table 3 summarises the ABSA tasks and dataset details of the 15 education review ABSA studies published since 2008 from the two systematic searches In terms of ABSA mentioned in the previous section. task coverage, over 92.3% (N=12) formulated the problem as chaining multiple tasks (i.e. AE and/or ACD, ASC) sequentially, either through pipeline of task-specific modules (N=11) or in an end-to-end (E2E) unified system (N=1). However, as the ABSA components are tightly intertwined and form context for each other, this multi-task approach is prone to error propagation, context isolation, and representational bottlenecks between interrelated components [16, 9, 2, 17]. To overcome these issues, an increasing number of recent E2E ABSA studies have adopted composite tasks EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks Table 2: Dataset summary Number of text entries and extracted quadruplets in the EduRABSA dataset. For reference, the number of reviews and sentences in the SemEval 2015 ABSA dataset is: Laptop (450, 2500), Restaurant (350, 2000), Hotels (30, 266) [15]. Review Type Entries Sentences Quadruplets Positive Neutral Negative1 course review 5,093 teacher review 9,102 university review 1,380 15,575 TOTAL 1 The last three columns show the number of quadruplets per each sentiment category. 10,816 14,530 1,691 27, 8,689 11,576 1,011 21,276 3,000 3,000 500 6,500 1,624 1,066 72 2,762 4,099 4,362 239 8,700 such as AOPE, ASTE, and ASQE that can better capture the inter-task relations via shared context and learning [1, 2, 9]. It is therefore important to include composite tasks in the education review ABSA datasets to enable this unified approach. With respect to dataset sources, many of these studies used Twitter posts and Coursera course reviews for their public accessibility. However, these reviews are often much simpler and shorter than the average course evaluations and general student survey open-ended comments from our experience. This could lead to downstream model robustness and evaluation validity issues that were raised on the benchmark SemEval datasets [18, 19]. For public datasets, it is thus crucial to capture sufficient complexity and diversity to reflect real-life use cases. In addition, from the perspective of creating useful datasets for the entire education review ABSA domain, we identified two gaps. First, none of these studies covered the OE task to extract the opinion expressions, while education review reporting commonly needs to examine certain raw text to understand the details. Second, robust and realistic dataset should cover two less-studied ABSA subtasks that are prevalent in opinionated text, especially education reviews: Implicit Aspect Extraction (IAE) and Implicit Opinion Extraction (IOE) [20, 21, 12, 14, 22]. An implicit aspect is absent from the text but can be inferred from the context [21], such as the inferred aspect price in The restaurant was expensive. An implicit opinion1 is an expression that does not carry words with obvious sentiment tendencies, but rather expresses the sentiment implicitly with objective statements, and expresses itself through factual, metaphorical, or ironic expressions. [23, p. 1]. For example: Explicit opinion: The waiter was rude. (Explicit negative opinion word rude) Implicit opinion: The waiter poured water on my hand and walked away. (No opinion word, but 1Some studies use the term implicit sentiment, where we choose implicit opinion as it shows the direct relationship with explicit opinion and the (explicit) OE task. has clear negative sentiment through implicit opinion expression poured water on my hand and walked away) [12] (2021) [12] reported that around 27% and Li et al. 30% of review entries in the most widely used ABSA benchmark datasets [1] SemEval 2014 Restaurant and Laptop [15] contain implicit opinions. In our observation, implicit opinions are very common in student reviews, such as Didnt learn single thing., Anything learned was self-taught., Would take again. (from [24, 25]). In addition, teacher reviews often have sentences with only descriptions of staff that have clear contextualised sentiment behaviour orientation (e.g. Never answers e-mails and cannot answer questions. (from [25])). However, implicit opinion extraction has not been widely covered by existing ABSA studies and datasets [12, 22, 1]. We designed our dataset aiming to address the gaps identified above, while capturing the common domain categories and tasks suggested by the previous studies. As detailed in the next section, our dataset covers almost all single and composite ABSA tasks, and includes the extraction of both implicit aspects and opinions to facilitate the development of future solutions and tools with this capability."
        },
        {
            "title": "3 Dataset Details",
            "content": "The Education Review ABSA (EduRABSA) dataset consists of 6,500 entries of public tertiary student review in the English language released in 2020-2023 text on courses (course review, N=3,000), teaching staff (teacher review, N=3,000), and university (university Each text entry was manually review, N=500). relevant aspect, annotated to extract list of all opinion, category, sentiment quadruplets and assign an overall sentiment label for the entire entry. The aspect and opinion terms were extracted as verbatim consecutive words. The category labels follow the entity-attribute two-level structure from SemEval 2015-2016 [8, 37] and include Course, Staff, and University (the entities) and their main attributes. The quadruplet-level and review-level sentiment categories 4 EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks Table 3: Task and Dataset Summary of Education Review Domain ABSA Studies from 2008-2025 * Note 1: These studies were from two systematic literature searches. Items #1-12 were all the in-domain studies from systematic review of 519 ABSA studies from 2008-2023 [1]. Items #13-15 were from the 502 results of Search 2 (details below) for 2024-2025 ABSA publications from three main databases (*), among which three 2024 in-domain ABSA studies were excluded due to no information on ABSA method (N=2) and experiment (N=1). * Note 2: For Search 2, we searched ACM Digital Library, IEEE Xplore, and Springer Link with 2024-2025 publication year filter and the following keywords: (aspect-based sentiment analysis OR aspect based sentiment analysis) AND (student OR course OR teaching OR teacher OR education OR MOOC). # Title Domain ABSA tasks Dataset Public Datasets Used Dataset Language Sentiment Labels1 Aspect Categories 1 Sivakumar & Reddy (2017) [26] Student Tweets & posts 2 Sindhu et al. (2019) [6] Teacher review 3 Chauhan et al. (2019) [27] Student Tweets 4 Soe & Soe (2019) [5] 5 Kastrati et al. (2020) [28] Course, Teacher, Facility MOOC review AE, ASC No ACD, ASC No AE, ACD, ASC No AE, ASC No ACD, ASC No 6 Kastrati et al. (2020) [29] MOOC review ACD, ASC No 7 Alassaf & Qamar (2020) [30] Student Tweets AE, ASC No Twitter tweets and online posts (Size unclear, original, unlabelled) Students feedback of Sukkur IBA University (N = 5000+, original, manually labelled) Student tweets (N = 1000, original, unlabelled) UCST student feedback (size unclear, original); Manually created domain aspect ontology Coursera course review (N = 104999, original, partially manually labelled) Coursera course review (N = 21940, original, manually labelled) Tweets related to the institute (N = 8023, original, manually labelled) 8 Wehbe et al. (2021) [4] Student Tweets ACD Yes (unlabelled) UAE Twitter Dataset (Task dataset, = 171873, original, unlabelled); COVID-19 Tweets (manually labelled) English P, Nu, Ng English P, Nu, Ng (N = 7) Teaching, Placement, Facilities, Sports, Organizing events, Fees, Transport (N = 6) Teaching pedagogy, Behavior, Knowledge, Assessment, Experience, General Thai P, Nu, Ng, N/A, Cluster-based English P, Ng N/A English P, Nu, Ng English P, Nu, Ng Arabic P, Non-Ng (N = 9) Course (Content, Structure, General), Instructor (Knowledge, Skill, Experience, Interaction), Assessment, Technology (N = 5) Instructor, Content, Structure, Design, General (N = 9) Teaching, Environment, Electronic services, Staff affairs, Academic affairs, Activities, Student affairs, Higher education, Miscellaneous English, Arabic N/A (N = 5) Educational rights, Financial security, Job security, Safety, Death 9 Hussain et al. (2022) [31] Course, Teacher, University review ACD, ASC No Students reviews from NCBA&E University (N = 5767, original, unlabelled) English P, Nu, Ng 10 Ren et al. (2022) [7] Teacher review ASC On request Chinese Education Bureau teaching evaluation (N = 4483, original, manually labelled) Chinese P, Ng 11 Almatrafi & Johri (2022) [32] 12 Edalati et al. (2022) [33] 13 Dissanayake & Fernando (2025) [34] MOOC review MOOC review course review AE; ASC N/A ACD; ASC Review level categorisation No No 14 Gul et al. (2025) [35] Course & teacher review ACD, ASC On request The Stanford MOOCPosts dataset (used = 906, manually labelled for ABSA) Coursera Course Review (N = 21940, from [29]) Udemy course review (N = 10K, original, manually labelled); 100K Coursera Course Review (N = 100K, unlabelled) 3 Student review data from National Textile University (N = 11747, original, manually labelled) English P, Nu, Ng English P, Nu, Ng English P, Ng (N = 11) Teacher (Subject matter knowledge, Experience, Behavior, General), Course (Objective & goals, Course content, Assessment, General), University (Environment, Policy, General) (N = 9) Teacher quality, Teacher image, Teaching method, Teaching content, Teaching ability, Teaching attitude, Teaching effect, Teacher-student relationship, Classroom atmosphere (N = 5) Assessments, General (Course), Instructional staff, Material, Others (N = 5) Instructor, Content, Structure, Design, General (N = 7) Instructor, Content, Assignment, Interactions, Tech issues, Timeliness, General English P, Ng (N = 10) Assessment, Behavior, Experience, Knowledge, Teaching skills, Teacher general, Course structure, Course material, Lab/Practical, Course general 15 Marir et al. (2025) [36] course review ACD, ASC Yes (unlabelled) Udemy Courses dataset (used = 104712, unlabelled) 4 English P, Nu, Ng (N = 4) Course, Instructor, Assessment, Technology 1 P: Positive; Nu: Neutral; Ng: Negative; C: Conflict. 2 For obtaining aspect categories. Unlabelled version from https://www.kaggle.com/datasets/gpreda/covid19-tweets. 3 Unlabelled original from https://www.kaggle.com/datasets/septa97/100k-courseras-course-reviews-dataset. 4 Unlabelled original from https://www.kaggle.com/datasets/hossaingh/udemy-courses. 5 EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks Neutral, and Negative. The are Positive, quadruplet extraction captured both explicit and implicit aspectand opinion-terms [23, 12], as well as the multi-aspect multi-sentiment relations on the same opinion term [19]. Table 2 shows the key dataset statistics. Figure 2 visualises the number of extracted quadruplets and aspect sentiment labels across the three review types. We also provide few examples of review text and annotations in the EduRABSA dataset in Table A.1 in the Appendix. The mixture of mentions of course and staff attributes in either course or teacher review is fairly common in this domain. Figure 2: Counts and percentages of review entries (N) and quadruplets (Q) across review types and aspect sentiment categories in the EduRABSA dataset The sections below provide details of the data sources, sampling strategy, and annotation method. The overall process is illustrated in Figure 1. We represented review text length using token count2 binned in intervals of 20. The rating scores were grouped into low / medium / high categories as follows: For Course Reviews, we used the Boolean rating liked the course\", with 0 mapped to low and 1 mapped to high. The overall ratings for Teacher and University reviews both range from 1.0 to 5.0, with values in the range [1.0, 2.5] mapped to low, (2.4, 4] to medium, and (4.0, 5.0] to high. Figures C.2 through C.4 in Appendix show the source and sample dataset distribution comparison. 3.3 Sentiment and Aspect Categories We determined the sentiment polarity and aspect category labels before data annotation. We chose the labels Positive, Neutral, and Negative for sentiment consistency with most ABSA studies we reviewed. For the aspect category labels, we used each review type as the main category, and developed the sub-categories based on pilot annotation of 200 review entries, as well as the common categories reported in the past education review ABSA studies detailed in Table 3. The aspect categories are listed below, while their definitions with some examples are provided in Table 6. Course (Content, Learning activity, Assessment, Workload, Difficulty, Course materials, Technology & tools, Overall); Staff (Teaching, Knowledge & skills, Helpfulness, Attitude, Personal traits, Overall); University (Cost, Opportunities, Programme, Campus & facilities, Culture & diversity, Information services, Social engagement & activities, Overall) 3.1 Source Review Data 3.4 Data Annotation To create the EduRABSA dataset, we sampled 6,500 pieces of review text from three open-source, public-domain licensed tertiary student reviews datasets listed in Table 4. The course and teacher review datasets were released before the prevalence of generative LLMs (e.g. ChatGPT [38], released in November 2022) and are therefore likely to be free from LLM-generated content. 3.2 Dataset Sampling Our and sampling strategy focused on realistic representative sample distributions per review type, so that the output dataset can be used for multiple purposes, such as drawing rebalanced subset for model training/testing, conducting fine-grained insight analysis, and providing distributional statistics for data augmentation and synthesis. We applied stratified sampling to each of the three data sources (i.e. review types) based on two criteria: 1) review text length, and 2) review rating score. We manually annotated the sampled dataset using the ASQE-DPT tool (introduced in Section 4), using the following three-stage process (illustrated in Figure 1). 1. Protocol development with pilot annotation We developed an initial protocol following that of the SemEval 2014-2016 ABSA datasets [15, 8, 37] due to their wide usage and impact on the ABSA community [1]. We then carried out pilot annotation on random sample of 200 text entries to test the protocol and identify confusions. The pilot sample size was chosen for both feasibility and sufficient size to show diversity across review types. After the pilot, we discussed the identified ambiguities and refined the protocol accordingly. We also finalised the list of aspect categories to align with common tertiary student review use cases (e.g. [35, 5, 26]) and the aspect categories used in prior education domain ABSA studies listed in Table 3. This process resulted in the following six annotation rules: 2We used the NLTK [40] word_tokenize library: https:// www.nltk.org/api/nltk.tokenize.word_tokenize.html 6 EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks Table 4: Information of the unannotated source data of the Edu Reviews ABSDA dataset Review Type Dataset Name Course review Course Reviews University of Waterloo [24] Teacher review Big Data Set from RateMyProfessor.com for Professors Teaching Evaluation [25] Publish Year October 2022 Licence CC0: Public Domain Total Entries Sampled (N=6,500) 14,810 3,000 March 2020 CC BY 4.0 19, 3,000 University review University of Exeter Reviews [39] June 2023 CC0: Public Domain 500 R1. For each review text, first identify the opinion terms relevant to the main categories, then extract the target aspect terms of each opinion term. This is to avoid extracting aspect terms without opinion, such as list of learning activities mentioned in factual statement. R2. Extract both explicit and implicit opinion terms. Implicit opinions are determined by whether, in the education context, an expression without opinion words conveys clear sentiment relevant to the given categories. R3. Extract both aspect terms and opinion terms as verbatim consecutive words. Aspect terms with conjunction or disjunction should be extracted as the maximum phrase following the SemEval approach to avoid ambiguity [8], e.g. this professor instead of professor as we found the review text often mentions multiple staff or courses. In addition, the extraction boundary of both aspect and opinion terms should cover enough content for readability and clarity. R4. Each category label is combination of one main category (i.e. entity) and one of its sub-category (i.e. entity attribute) from Table 6. E.g. Course - Assessment or Staff - Attitude. Only assign one category label to each aspect-opinion pair. The categorisation should consider both aspect and opinion terms as context to avoid ambiguity between main categories, or from implicit aspect terms. R5. If within the sentence the direct opinion target is pronoun (e.g. this, he/she), refer to the adjacent sentence for the direct aspect term such as course name, staff name, or staff reference (e.g. this If none is available, use pronoun professor). instead. if the In the case of implicit aspects (e.g. entire course review text is Its waste of time!), use null to represent the implicit aspect term (course). R6. Use neutral for very weak or mildly positive / negative sentiment [8]. For review-level overall sentiment, neutral also represents mixed sentiment at the quadruplet level, e.g. fairly useless and hard course. The concepts and labs are pretty cool though, which makes this course bearable. 2. annotation The annotation was implemented by the first author, who is an experienced Manual in tertiary student evaluations and domain expert surveys. During the annotation process, we encountered number of additional edge case aspects that were challenging to categorise largely due to the highly contextualised nature of teacher behaviour and course aspects. These cases were discussed and resolved with the other authors, one of whom is an education subject expert. The annotator then updated the annotation rules and previous relevant annotations accordingly. We share some of these edge cases and our decisions in Appendix B. 3. Final review After annotating all 6,500 entries, the annotator manually reviewed all annotations against the protocol and notes and made necessary revisions to ensure consistency."
        },
        {
            "title": "4 ASQE-DPT: A slim offline annotation",
            "content": "tool Figure 3: Screenshot of the ASQE-DPT Annotation Tool text is manual ASQE-DPT3 review and ABSA annotation tool we adapted from the PyABSA Data Preparation Tool (DPT)4. It inherited the original DPTs architecture, which is self-contained HTML file with JavaScript and Vue components that can be used offline and directly on browser without installation. It follows the original DPTs simple interface that displays review 3Available at https://github.com/yhua219/edurabsa_dataset_ and_annotation_tool 4 https://github.com/yangheng95/ABSADatasets/tree/v2.0/DPT 7 EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks Table 5: Test performance of models trained on EduRABASA subset for ASC/ASQE task Dataset EduRABSA EduRABSA EduRABSA EduRABSA EduRABSA EduRABSA EduRABSA EduRABSA EduRABSA SemEval Rest15 SemEval Rest16 Task ASC ASQE ASQE ASQE ASQE ASQE ASQE ASQE ASQE ASQE ASQE Model Precision Recall F1-score FAST_LSA_T_V2 (Accuracy) 0. GPT-4.1_fewshot GPT-4.1_zeroshot Llama3.3-70B_fewshot Llama3.3-70B_zeroshot Llama3-8B_fewshot Llama3-8B_zeroshot T5_base Tk-Instruct T5_base T5_base 0.25 0.17 0.19 0.13 0.14 0.03 0.32 0. 0.43 0.57 0.28 0.20 0.21 0.15 0.15 0. 0.32 0.26 0.42 0.57 0.58 0.26 0.18 0. 0.14 0.14 0.03 0.32 0.26 0.42 0.57 text in an interactive table form and only relies on mouse clicks for data annotation. However, it significantly differs from the original DPT in terms of functionality and the types of ABSA subtasks covered. The original DPT only supports three tasks: AE and ASC for ABSA and review-level sentiment analysis (SA). It takes user-uploaded CSV file of review text, displays it as table, and allows users to assign sentiment labels to either the selected consecutive words or an entire review entry. The sentiment labels on words are displayed as highlight colours. It has Save button that generates three consecutive output files, two for the annotation tasks and one for the DPT process file to record progress. ASQE-DPT extends the original tool by adding new features: 1. comprehensive and more challenging ABSA tasks: Mouse-clicking annotation functions for OE and AOPE (Allows multi-aspect and multi-opinion pairing, and visualises linked pairs upon mouse hover over either of the annotated member terms), Implicit AE (Automatically marks an unpaired aspect (and unpaired opinion) as null), ACD (Supports single and two-level user-defined aspect categories. The pre-defined categories can be updated in the HTML source file, and are displayed in pop-over drop-down upon clicking on labelled opinion or aspect term), ASTE and ASQE (The AOPE, ACD, and ASC annotations on the same aspect-opinion pair are automatically combined into ASTE and ASQE annotations). Clear option to undo annotation. 2. Auto-conversion of ASQE annotation into different task files. The tool outputs zip file containing five annotation files and one progress file that can be re-uploaded to review or continue annotation. The annotation files for all the tasks follow the corresponding format of the PyABSA platform [13] for easy downstream model training. 3. Checking record and improved UI. To support annotation review and record keeping, the new interface includes checkbox column and dynamic filename feature that records the number of rows marked as checked."
        },
        {
            "title": "5 Example Application",
            "content": "for two tasks. the new dataset to train We use subsample of models For ASC, we trained FAST_LSA_T_V2 from PyABSA [13] using the example settings provided by the platform5. For ASQE, we trained TK-INSTRUCT[41] using the example script from PyABSA6, and T5_BASE using the code and The two parameters from Li et al. ASQE models were trained for 5 epochs. The training, validation, and test dataset sizes between EduRABSA and SemEval Restaurant 15/16 were all matched to 834 instances for training, 200 for validation, and 300 In addition, we also applied the same for testing. ASQE test set examples with pre-trained GPT-4.1, LLAMA3.3-70B, and LLAMA3-8B with zero-shot and 4-shot prompts provided in our code 7. (2023) [42]. https://github.com/yangheng95/PyABSA/blob/v2/ examples-v2/aspect_polarity_classification/Aspect_Sentiment_ Classification.ipynb 6 https://github.com/yangheng95/PyABSA/blob/v2/ examples-v2/aspect_opinion_sentiment_category_extraction/ multitask_train.py https://github.com/yhua219/edurabsa_dataset_and_ annotation_tool 8 EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks further leverage the dataset, such as semi-supervised or contrastive learning with in-training data augmentation. 3) Extending the dataset into more languages (e.g. [43] and categories or ABSA tasks. 4) Explore evaluation metrics for ASTE and ASQE tasks, particularly on more complex and longer texts that contain more units to extract, where classic exact string-matching becomes uninformative. We also hope that more research effort and resources could go to the public sectors and low-resource domains, so that the advancement of machine learning and natural language processing can benefit more areas and populations. The results for all pre-trained LLMs and T5_BASE were evaluated using the latters evaluation method. The two PyABSA models were evaluated using methods provided as part of their scripts. Our focus is on dataset comparison instead of the best training parameters. Overall, model performance on ASQE is lower than ASC, as the latter is classification task on given aspect, and the exact-string-matching evaluation method can be too harsh on ASQE for long aspect/opinion terms. As we have expected, all models performed worse on the EduRABSA data than the SemEval Restaurant datasets, proving our dataset to be more challenging in terms of length and the multi-aspect, multi-sentiment nature. On the other hand, the SemEval datasets do not cover implicit opinions, and were criticised for being overly simplified for model robustness [19, 18]."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "ABSA is promising solution for automated text mining of rich, fine-grained insights. The current public dataset domain skewness hinders its advancement and and application in non-commercial domains, particularly in the education review domain, which faces data sensitivity restrictions that greatly limit solution options, and research transparency and reproducibility Our work fills the gap by developing and [1]. sharing publicly sourced, comprehensively annotated, realistically challenging ABSA dataset for course, teacher, and university reviews. The dataset aspect categories cover common education reporting use cases and the comprehensive, reflected by past studies; challenging ABSA task types enable more robust model training and evaluation beyond this domain. The ASQE-DPT annotation tool can further enable easy dataset creation and adaptation, particularly for low-resource areas and sensitive domains. Limitations: Despite our effort and domain expertise, the dataset annotation was conducted by single annotator and thus lacks reliability statistics. As with any manual annotation, it also reflects intrinsically imperfect decisions on aspect and opinion boundaries and categories, particularly with the highly contextualised education reviews. The source reviews are only in the English language, and may not reflect concepts and events in later times, such as the topic of generative AI in education settings. The ASQE-DPT tool is currently targeted at PC monitor views and only supports mouse-based operations. Future work: By making both the dataset and the annotation tool open-source, we welcome the ABSA community to further explore and improve these resources. Future work could consider the following: 1) Developing more rigorous annotation rules that can support automatic annotation and quality evaluation by rule-based systems or generative LLMs. 2) Exploring more diverse ABSA approaches in this domain that 9 EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks Table 6: Main Categories (entities) and Sub-Categories (attributes) in EduRABSA Dataset. The category label consists of main category and one of its sub-categories connected by hyphen, e.g. Course - Assessment or Staff - Attitude Main Category Sub-category Example Course Content Course content, subject, topics Learning activity Lectures, tutorials, workshops, lab sessions, field trips, group work, in-class activities Assessment Workload Difficulty Assignments, homework, tests, exams, lab reports, quizzes Time and effort taken to complete the course activities or assessments Difficulty level of course content (Assessment difficulty falls under the Assessment sub-category) Course materials Lecture slides, handouts, coursebooks, textbooks, reading materials, class notes, lecture recordings, other teaching materials Technology & tools Any technology, software and device used in the course Overall General opinion about the overall course, e.g. great course, learned so much, do not recommend Staff Teaching Teaching method, behaviour, effectiveness, including things like going over materials after class, giving lot of assignments, marks harshly, and passionate about what he/she teaches Knowledge & skills Staff knowledge and skills Helpfulness Attitude Personal traits Staff behaviours or attitudes that are perceived as being helpful/unhelpful, e.g. answers emails/questions, willing to help, ready to help Staff attitude towards the course and/or students, e.g. wants everyone to do well, fair, approachable Staff individual attributes, including physical features, intelligence, personality, accents, and general demeanour such as humorous, very kind/sweet without specific context Overall General opinion about the staff, e.g. hes/shes the best, do not take University Cost Tuition and living cost at the university Opportunities Programme Direct mention of opportunities Programme aspects mentioned in the university review context, e.g. course options, programme quality Campus & facilities The environment and hardware of the university, including the relevant but non-university facilities such as ease of commute Culture & diversity The universitys culture and diversity in student and staff communities Information & Services The provision and quality of information and services, e.g. counselling, help desk, campus cafe, gym. Social engagement & activities Campus social atmosphere, clubs/societies and activities, perceived social engagement with staff and peers Overall Overall opinion about the university, e.g. the best uni, waste of time 10 EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks"
        },
        {
            "title": "APPENDICES",
            "content": "Table A.1: Examples of the EduRABSA dataset review text and annotation (ASQE quadruplets (Quad.), and review-level sentiment label). Each row across columns shows piece of review text and its review-level sentiment label; and the rows underneath list each extracted quadruplet containing one aspect term, one opinion term, their category label, and quadruplet-level sentiment label. # Aspect Term Opinion Term Category Label Sentiment Label 1. Course review Quad.1-1 Quad.1-2 Quad.1-3 Quad.1-4 Quad.1-5 2. Course review fairly useless and hard course. The concepts and labs are pretty cool though, which makes this course bearable. course course concepts labs course fairly useless hard cool cool bearable Course - Overall Course - Difficulty Course - Content Course - Learning activity Course - Overall This class was challenging because the instructor was not able to express the materials clearly. Not to mention, he does not speak fluent English. In addition, it is even harder to understand the course materials, due to lack of samples and again fluent expression of the materials. Moreover, this class will require much practice and studying. Quad.2-1 Quad.2-2 This class instructor Quad.2-3 instructor course materials course materials this class Quad.2-4 Quad.2-5 Quad.2-6 3. Teacher review Quad.3-1 Quad.3-2 Quad.3-3 teacher teacher teacher Quad.3teacher challenging not able to express the materials clearly does not speak fluent English even harder to understand lack of samples require much practice and studying Course - Difficulty Staff - Teaching Staff - Personal traits Course - Course materials Course - Course materials Course - Workload Awesome makes class entertaining will stay late to help you understand the material Take her for math Staff - Overall Staff - Teaching Staff - Helpfulness Staff - Overall Took her for math 85 and math 90. Awesome teacher. She makes class entertaining and will stay late to help you understand the material. Take her for math! 4. Teacher Review Quad.4-1 Quad.4-2 Quad.4He gives unclear instructions and unclear due dates. If you didnt do project exactly how he wants it, hell chew you out and degrade your art for 10-15 minutes in front of the class (yes this happens on multiple occasions). Hes late about 75% of the time (once even forgetting he had class go teach and another bc he was looking at art). He He He gives unclear instructions and unclear due dates late about 75% of the time chew you out and degrade your art for 10-15 minutes in front of the class Staff - Teaching Staff - Teaching Staff - Attitude 5. University review The University is great at sorting out career opportunities and the campus is great with very good facilities Quad.5-1 Quad.5-2 Quad.5-3 sorting out career opportunities campus facilities great at great very good University - Opportunities University - Campus & facilities University - Campus & facilities 11 Neutral Negative Negative Positive Positive Neutral Negative Negative Negative Negative Negative Negative Negative Positive Positive Positive Positive Positive Negative Negative Negative Negative Positive Positive Positive Positive EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks"
        },
        {
            "title": "B Annotation Notes on Edge Cases",
            "content": "Classification Decisions of Edge Cases 01 This course is lecture-only (statement, not opinion)"
        },
        {
            "title": "02 Staff assessment design, marking criteria",
            "content": "Staff-Teaching"
        },
        {
            "title": "04 Assessment difficulty and others",
            "content": "Course-Workload Course-Assessment"
        },
        {
            "title": "05 Staff attitude and interaction with students",
            "content": "Staff-Attitude"
        },
        {
            "title": "06 Staff cool, passionate, friendly, approachable",
            "content": "Staff-Attitude"
        },
        {
            "title": "07 Staff appearance, accent, fun, humor, smart",
            "content": "Staff-Personal traits"
        },
        {
            "title": "08 Peer interaction, group work, lectures, labs",
            "content": "Course-learning activity 09 University campus University-Campus & facilities 10 Course being fun, engaging, dry, useful Course-Overall 12 (Course review) learned lot, had good time Course-Overall bird course = easy course Course-Difficulty 13 Textbook, handbook, slides etc. Course-Course materials 14 15 great teacher, best professor (on role) Staff-Overall shes the best, love her, Take him Staff-Overall 16 Staff answer emails/questions Staff-Helpfulness 17 Readings Course-Course material 18 University nice people University-Social engagement & activities 19 (University review) nice/helpful staff Staff-Attitude 20 Her class/His lecture (Depends on focus: Course-Learning activity 21 Willing to help/ready to help 22 Passionate about what he/she teaches 23 Wants everyone to do well or Staff-Teaching) Staff-Helpfulness Staff-Teaching Staff-Attitude 24 Sweetheart, sweet (without context) Staff-Personal Trait 25 You have to read and study/do the work Course-Workload 26 If you . . . you will get A/you will be fine Course-Difficulty or Course-Overall (aspect: 27 like going to class as he made it enjoyable! 28 First half is easy, then it gets difficult 29 Staff being fair course) Course-Learning (aspect: course) Course-Content Staff-Attitude 30 Staff go over materials with you after class Staff-Teaching 31 Gives lot of homework/gives extra credit Staff-Teaching activity; Staff-teaching 12 EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks Detailed Explanations Case 06 Staff-Attitude vs. Staff-Personal Trait Staff-attitude captures interpersonal qualities, social approach, and manner of engagement, which cool directly addresses. Staff-personal trait refers more to inherent characteristics like personality type, which is more stable and less situational than attitude. Case 26 If you ... you will get A/you will be fine This type of sentence is about the course. More particularly, describing the aspect of course, such as overall outcome or difficulty. The specific subcategory under Course depends on the particular context and focus. Case 28 Course-Content vs. Course-Difficulty Sentences similar to First half is easy, then it gets difficult is classified as Course-Content because: 1. The comment describes how the subject matter develops over the courses duration 2. Difficulty here refers to property of the content The phrase reveals how the course material transforms, becomes more complex, or introduces more advanced conceptswhich is essentially description of course content, with difficulty being an inherent attribute of that content. Case 30 Staff-Helpfulness vs. Staff-Teaching The sentence He will go over materials with you after class if you need it is classified as Staff-Helpfulness rather than Staff-Teaching. The focus is on the availability of additional support and the staff members willingness to assist the student after class, which is form of help. Staff-Teaching would typically involve direct actions or instructions related to delivering course content during the class. 13 EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks The Education Review ABSA (EduRABSA) Dataset Statistics Figure C.1: Percentage of review text entries (x-axis) by rating category (y-axis) for source and sampled datasets (left and right column in each pair) across review types. Note: For Course Reviews, we used the Boolean rating \"liked the course\", with 0 mapped to low and 1 mapped to high. The overall ratings for Teacher and University reviews both range from 1.0 to 5.0, with values in the range [1.0, 2.5] mapped to low, (2.4, 4] to medium, and (4.0, 5.0] to high. 14 EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks Figure C.2: Percentage of review text entries (x-axis) by entry token count group (y-axis) for source and sampled datasets (top and bottom bars in each pair) across review types. For both source and sampled datasets, Course, Teacher, and University reviews have 86%, 99%, and 94% entries within 80 tokens, and 91%, 100%, and 96% entries within 100 tokens, respectively. EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks Figure C.3: Summary of review text entry length (token count; y-axis) distribution by review rating category (x-axis) for source and sampled datasets (left and right components in each pair) across review types. Boxes show the middle 50% of data (IQR; 25th-75th percentiles) with labelled medians. Whiskers extend to data within 1.5IQR; outliers appear as individual points. (Note: The Course Review source file has two entries of 635 and 856 tokens respectively and were excluded from sampling and this graph). 16 EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks Figure C.4: Percentage of review text entries (x-axis) by rating category (segments within bar) per entry token count group (y-axis) for source and sampled datasets (top and bottom bars in each pair) across review types. (Note: There were 221 original and 45 sampled Course Review entries without rating score.) EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks"
        },
        {
            "title": "References",
            "content": "[1] Y.C. Hua, P. Denny, J. Wicker, et al. systematic review of aspect-based sentiment analysis: Domains, methods, and trends. Artificial Intelligence Review, 57:296, 2024. [2] Wenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, and Wai Lam. survey on aspect-based sentiment analysis: Tasks, methods, and challenges. IEEE Trans. on Knowl. and Data Eng., 35(11):1101911038, dec 2022. [3] Shailendra Satyarthi and Sanjiv Sharma. Identification of effective deep learning approaches for classifying In 2023 IEEE International Conference on Paradigm Shift in sentiments at aspect level in different domain. Information Technologies with Innovative Applications in Global Scenario (ICPSITIAGS), pages 496508, 2023. [4] Dana Wehbe, Ahmed Alhammadi, Hajar Almaskari, Kholoud Alsereidi, and Heba Ismail. Uae e-learning sentiment analysis framework. In The 7th Annual International Conference on Arab Women in Computing in Conjunction with the 2nd Forum of Women in Research, ArabWIC 2021, New York, NY, USA, 2021. Association for Computing Machinery. [5] Nilar Soe and Paing Thwe Soe. Domain oriented aspect detection for student feedback system. In 2019 International Conference on Advanced Information Technologies (ICAIT), pages 9095, 2019. [6] Irum Sindhu, Sher Muhammad Daudpota, Kamal Badar, Maheen Bakhtyar, Junaid Baber, and Mohammad Nurunnabi. Aspect-based opinion mining on students feedback for faculty teaching performance evaluation. IEEE Access, 7:108729108741, 2019. [7] Ping Ren, Liu Yang, and Fang Luo. Automatic scoring of student feedback for teaching evaluation based on aspect-level sentiment analysis. Education and Information Technologies, 28(1):797814, July 2022. [8] Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. In Preslav Nakov, Torsten Zesch, Daniel Cer, and SemEval-2015 task 12: Aspect based sentiment analysis. David Jurgens, editors, Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 486495, Denver, Colorado, June 2015. Association for Computational Linguistics. [9] You Li, Yongdong Lin, Yuming Lin, Liang Chang, and Huibing Zhang. span-sharing joint extraction framework for harvesting aspect sentiment triplets. Knowledge-Based Systems, 242:108366, 2022. [10] Siva Uday Sampreeth Chebolu, Franck Dernoncourt, Nedim Lipka, and Thamar Solorio. review of datasets for aspect-based sentiment analysis. In Jong C. Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu Purwarianti, and Adila Alfa Krisnadhi, editors, Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 611628, Nusa Dua, Bali, November 2023. Association for Computational Linguistics. [11] Zhuoming Zheng, Yi Cai, and Liuwu Li. weakly-supervised aspect detection. 33:506517, 2025. knowledge-enhanced contrastive learning network for IEEE Transactions on Audio, Speech and Language Processing, [12] Zhengyan Li, Yicheng Zou, Chong Zhang, Qi Zhang, and Zhongyu Wei. Learning Implicit Sentiment in Aspect-based Sentiment Analysis with Supervised Contrastive Pre-Training. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 246256, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. [13] Heng Yang, Chen Zhang, and Ke Li. Pyabsa: modularized framework for reproducible aspect-based sentiment analysis. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 23, page 51175122, New York, NY, USA, 2023. Association for Computing Machinery. [14] Xinlong Wang, Xu Li, Yinghui Yin, and Yang Li. Implicit aspect-based generative model for sentiment analysis based on prompt learning. In 2024 5th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE), pages 9497, 2024. [15] Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. SemEval-2014 task 4: Aspect based sentiment analysis. In Preslav Nakov and Torsten Zesch, editors, Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 2735, Dublin, Ireland, August 2014. Association for Computational Linguistics. [16] Jia Li, Yuyuan Zhao, Zhi Jin, Ge Li, Tao Shen, Zhengwei Tao, and Chongyang Tao. Sk2: Integrating implicit sentiment knowledge and explicit syntax knowledge for aspect-based sentiment analysis. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, CIKM 22, page 11141123, New York, NY, USA, 2022. Association for Computing Machinery. 18 EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks [17] Zhou Lei, Yawei Zhang, and Shengbo Chen. dual-template prompted mutual learning generative model for implicit aspect-based sentiment analysis. Applied Sciences, 14(19), 2024. [18] Hao Fei, Tat-Seng Chua, Chenliang Li, Donghong Ji, Meishan Zhang, and Yafeng Ren. On the robustness of aspect-based sentiment analysis: Rethinking model, data, and training. ACM Transactions on Information Systems, 41(2):132, 2023. [19] Qingnan Jiang, Lei Chen, Ruifeng Xu, Xiang Ao, and Min Yang. challenge dataset and effective models In Proceedings of the 2019 Conference on Empirical Methods in for aspect-based sentiment analysis. Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 62796284, Hong Kong, China, 2019. Association for Computational Linguistics. [20] Vishal Singhi, Charulata Chauhan, and Piyush Kumar Soni. Exploring progress in aspect-based sentiment analysis: An in-depth survey. In 2024 IEEE 9th International Conference for Convergence in Technology (I2CT), pages 110, 2024. [21] Piyush Kumar Soni and Radhakrishna Rambola. survey on implicit aspect detection for sentiment analysis: Terminology, issues, and scope. IEEE Access, 10:6393263957, 2022. [22] Xiancai Xu, Jia-Dong Zhang, Lei Xiong, and Zhishang Liu. iACOS: Advancing implicit sentiment extraction with informative and adaptive negative examples. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 42834293, Mexico City, Mexico, June 2024. Association for Computational Linguistics. [23] Yuxia Zhao, Mahpirat Mamat, Alimjan Aysa, and Kurban Ubul. dynamic graph structural framework for implicit sentiment identification based on complementary semantic and structural information. Scientific Reports, 14(1):16563, July 2024. [24] Anthony Susevski. Course reviews university of waterloo. https://www.kaggle.com/datasets/ anthonysusevski/course-reviews-university-of-waterloo, 2022. Accessed: 2024-09-08. [25] Jibo He. Big data set from ratemyprofessor.com for professors teaching evaluation. Mendeley Data, V2, 2020. Accessed: 2024-09-08. [26] M. Sivakumar and U. Srinivasulu Reddy. Aspect based sentiment analysis of students opinion using machine learning techniques. In 2017 International Conference on Inventive Computing and Informatics (ICICI), pages 726731, 2017. [27] Ganpat Singh Chauhan, Preksha Agrawal, and Yogesh Kumar Meena. Aspect-based sentiment analysis of students feedback to improve teachinglearning process. In Suresh Chandra Satapathy and Amit Joshi, editors, Information and Communication Technology for Intelligent Systems, pages 259266, Singapore, 2019. Springer Singapore. [28] Zenun Kastrati, Ali Shariq Imran, and Arianit Kurti. Weakly supervised framework for aspect-based sentiment analysis on students reviews of moocs. IEEE Access, 8:106799106810, 2020. [29] Zenun Kastrati, Blend Arifaj, Arianit Lubishtani, Fitim Gashi, and Engjëll Nishliu. Aspect-based opinion mining of students reviews on online courses. In Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence, ICCAI 20, page 510514, New York, NY, USA, 2020. Association for Computing Machinery. [30] Manar Alassaf and Ali Mustafa Qamar. Aspect-based sentiment analysis of arabic tweets in the education sector using hybrid feature selection method. In 2020 14th International Conference on Innovations in Information Technology (IIT), pages 178185, Al Ain, United Arab Emirates, 2020. IEEE. [31] Shabir Hussain, Muhammad Ayoub, Ghulam Jilani, Yang Yu, Akmal Khan, Junaid Abdul Wahid, Muhammad Farhan Ali Butt, Guangqin Yang, Dietmar P.F. Moller, and Hou Weiyan. Aspect2labels: novelistic decision support system for higher educational institutions by using multi-layer topic modelling approach. Expert Systems with Applications, 209:118119, 2022. [32] Omaima Almatrafi and Aditya Johri. Improving moocs using information from discussion forums: An opinion summarization and suggestion mining approach. IEEE Access, 10:1556515573, 2022. [33] Maryam Edalati, Ali Shariq Imran, Zenun Kastrati, and Sher Muhammad Daudpota. The potential of machine learning algorithms for sentiment classification of students feedback on mooc. In Kohei Arai, editor, Intelligent Systems and Applications, pages 1122, Cham, 2022. Springer International Publishing. [34] A.D.M.B.B. Dissanayake and Pumudu A. Fernando. Aspect-based sentiment analysis of student reviews on mooc platforms. In 2025 5th International Conference on Advanced Research in Computing (ICARC), pages 16, 2025. EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks [35] Shahla Gul, Muhammad Asif, Fazal-E Amin, Kashif Saleem, and Muhammad Imran. Advancing aspect-based IEEE sentiment analysis in course evaluation: multi-task learning framework with selective paraphrasing. Access, 13:77647779, 2025. [36] Naila Marir, Akila Sarirete, and Tayeb Brahimi. Centroid-based aspect sentiment analysis for mooc learner reviews. In 2025 22nd International Learning and Technology Conference (L&T), volume 22, pages 712, 2025. [37] Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Mohammad AL-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orphée De Clercq, Véronique Hoste, Marianna Apidianaki, Xavier Tannier, Natalia Loukachevitch, Evgeniy Kotelnikov, Nuria Bel, Salud María Jiménez-Zafra, and Gülsen Eryigit. SemEval-2016 task 5: Aspect based sentiment analysis. In Steven Bethard, Marine Carpuat, Daniel Cer, David Jurgens, Preslav Nakov, and Torsten Zesch, editors, Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 1930, San Diego, California, June 2016. Association for Computational Linguistics. [38] OpenAI. Chatgpt (mar 14 version) [large language model]. https://chat.openai.com/chat, 2023. [39] Rohit Pawar. https://www.kaggle.com/datasets/rohitpawar1/"
        },
        {
            "title": "University of exeter",
            "content": "reviews. university-of-exeter-reviews, 2023. Accessed: 2024-09-08. [40] Steven Bird, Edward Loper, and Ewan Klein. Natural Language Processing with Python. OReilly Media, Inc., 2009. [41] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 50855109, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. [42] Zhijun Lil, Zhenyu Yang, Xiaoyang Li, and Yiwen Li. Two-stage aspect sentiment quadruple prediction based on mrc and text generation. In 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pages 21182125, 2023. [43] Chengyan Wu, Bolei Ma, Yihong Liu, Zheyu Zhang, Ningyuan Deng, Yanshu Li, Baolan Chen, Yi Zhang, Yun Xue, and Barbara Plank. M-absa: multilingual dataset for aspect-based sentiment analysis, 2025."
        }
    ],
    "affiliations": [
        "School of Computer Science, University of Auckland, New Zealand"
    ]
}