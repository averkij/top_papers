{
    "paper_title": "Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation",
    "authors": [
        "Qiyue Gao",
        "Xinyu Pi",
        "Kevin Liu",
        "Junrong Chen",
        "Ruolan Yang",
        "Xinqi Huang",
        "Xinyu Fang",
        "Lu Sun",
        "Gautham Kishore",
        "Bo Ai",
        "Stone Tao",
        "Mengyang Liu",
        "Jiaxi Yang",
        "Chao-Jung Lai",
        "Chuanyang Jin",
        "Jiannan Xiang",
        "Benhao Huang",
        "Zeming Chen",
        "David Danks",
        "Hao Su",
        "Tianmin Shu",
        "Ziqiao Ma",
        "Lianhui Qin",
        "Zhiting Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Internal world models (WMs) enable agents to understand the world's state and predict transitions, serving as the basis for advanced deliberative reasoning. Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and Gemini, exhibit potential as general-purpose WMs. While the latest studies have evaluated and shown limitations in specific capabilities such as visual understanding, a systematic evaluation of VLMs' fundamental WM abilities remains absent. Drawing on comparative psychology and cognitive science, we propose a two-stage framework that assesses Perception (visual, spatial, temporal, quantitative, and motion) and Prediction (mechanistic simulation, transitive inference, compositional inference) to provide an atomic evaluation of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse simulated environments with controlled counterfactual simulations. Through 660 experiments on 15 latest commercial and open-source VLMs, we find that these models exhibit striking limitations in basic world modeling abilities. For instance, almost all models perform at near-random accuracy when distinguishing motion trajectories. Additionally, they lack disentangled understanding -- e.g., some models tend to believe blue objects move faster than green ones. More rich results and analyses reveal significant gaps between VLMs and human-level world modeling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 6 7 8 1 2 . 6 0 5 2 : r Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation Qiyue Gao, Xinyu Pi, Kevin Liu, Junrong Chen, Ruolan Yang Xinqi Huang, Xinyu Fang, Lu Sun, Gautham Kishore, Bo Ai, Stone Tao, Mengyang Liu Jiaxi Yang, Chao-Jung Lai, Chuanyang Jin, Jiannan Xiang, Benhao Huang, Zeming Chen David Danks, Hao Su, Tianmin Shu, Ziqiao Ma, Lianhui Qin, Zhiting Hu Maitrix.org UC San Diego"
        },
        {
            "title": "JHU Cornell Tech EPFL UMich",
            "content": "https://wm-abench.maitrix.org/ *equal contribution Figure 1: WM-ABench overview. Grounded in comparative psychology and cognitive science, our framework delineates the functioning of world models into two distinct stages: (1) perception stage, encompassing visual, spatial, temporal, quantitative, and motion perceptions, and (2) prediction stage, involving mechanistic simulation, transitive inference, and compositional inference. Large-scale evaluation reveals VLMs limitations as WMs."
        },
        {
            "title": "Abstract",
            "content": "(WMs) Internal world models enable agents to understand the worlds state and predict transitions, serving as the basis for advanced deliberative reasoning. Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and Gemini, exhibit potential as general-purpose WMs. While the latest studies have evaluated and shown limitations in specific capabilities such as visual understanding, systematic evaluation of VLMs fundamental WM abilities remains absent. Drawing on comparative psychology and cognitive science, we propose two-stage framework that assesses perception (visual, spatial, temporal, quantitative, and motion) and simulation, transitive inference, compositional inference) to provide an atomic evaluation of VLMs as WMs. Guided by this framework, we introduce WM-ABench, large-scale benchmark (mechanistic prediction fine-grained comprising 23 evaluation simulated across 6 diverse dimensions environments with controlled counterfactual simulations. Through 660 experiments on 15 latest commercial and open-source VLMs, we find that these models exhibit striking limitations in basic world modeling abilities. For instance, almost all models perform at near-random accuracy when distinguishing motion trajectories. Additionally, they lack disentangled some models tend to believe blue objects move faster than green ones. More rich results and analyses reveal significant gaps between VLMs and human-level world modeling. understandinge.g.,"
        },
        {
            "title": "Introduction",
            "content": "World models (WMs) in agents provide internal representations of the external world (JohnsonLaird, 1983; Wooldridge and Jennings, 1995). They simulate how the current state transforms to 1 the next (Kappes and Morewedge, 2016; Russell and Norvig, 2016), enabling agents to perform deliberative planning by predicting probable future states and choosing the most favorable one (Ha and Schmidhuber, 2018b; LeCun, 2022; Hu and Shu, 2023; Ai et al., 2024; Tian et al., 2025). Different environments operate under distinct the mechanisms and dynamicsfor example, transition mechanics of autonomous vehicles, surgical robots, and rover-based spacecraft vary significantly. Prior work handled this diversity by building environment-specific world models, but they failed to adapt across various real applications. Recent large-scale Vision-Language Models (VLMs; Google, 2023; OpenAI et al., 2024; Anthropic, 2024; Li et al., 2024b; OpenAI, 2025b, inter alia) enhance generalist LLMs with visual semantics and encapsulate extensive knowledge of world dynamics, making them promising potential world models for general domains. Unlike video generative world models which directly generate the next states, VLMs can reason in their latent representation space and forecast via language. However, their language grounding and world simulation capabilities may still be insufficient in various aspects. For example, existing benchmarks have revealed their vulnerability in visual or spatiotemporal perception (Goyal et al., 2020; Shangguan et al., 2024; Fu et al., 2024b; Zhang et al., 2025) and future state prediction driven by intuitive physics (Bakhtin et al., 2019; Yi et al., 2020a; Bear et al., 2022). These limitations underscore the need for more systematic evaluation. robust world model must integrate multiple fundamental abilities in perception and prediction, yet previous studies that assess these aspects in isolation provide only partial view. To address this, we propose an atomic evaluation framework, systematically testing the essential aspects (and their interactions) of VLMs internal world model from first principles. With theories and evidence from comparative psychology and cognitive science (Spelke, 2000; Knill and Pouget, 2004; Olmstead and Kuhlmeier, 2015), we first present systematic conceptual framework to formalize the functioning of world models (Figure 1). We decompose the process into two stages: (1) perception stage, which involves visual, spatial, temporal, quantitative, and motion perceptions (Baillargeon et al., 1985; MerleauPonty, 2004; Coren et al., 2004; Hoffmann et al., 2011); and (2) prediction stage, which involves mechanistic simulation, transitive inference, and compositional inference (Hegarty, 2004; Barsalou, 2008; Prystawski et al., 2023). Following this framework, we create WM-ABench, the World Model Atomic Benchmark that covers 23 fine-grained dimensions (Figure 2) of world modeling and over 100,000 instances curated from 6 different simulators (Dosovitskiy et al., 2017a; Gan et al., 2021; Szot et al., 2021a; Tao et al., 2024; Bear et al., 2022; Gu et al., 2023). By systematically manipulating environmental factors and simulating counterfactual actions, we generate incorrect states for models to differentiate from the correct ones to ensure controlled studies. We also measure human performance to verify the fairness and solvability of our problems. We conduct 660 experiments on 15 state-of-theart VLMs and find that, while they excel in certain aspects of visual and quantitative perception, they are surprisingly limited in many other dimensions (Figure 1). time, and motion; Specifically, VLMs exhibit (1) weak perception (2) insufficient of space, knowledge of physical causality in intuitive physics and agentic actions; (3) limited transitive and compositional reasoning capabilities, showing near-random accuracy. Our further analyses reveal lack of independent and robust world representations e.g., mistakenly associating color with speed. These findings reveal significant gaps between current VLMs and human-level world modeling, shedding light on the need for deeper understanding, grounding, and reasoning over the perceptual world and its transition mechanisms before VLMs can truly serve as generalist world models. in VLMs,"
        },
        {
            "title": "2 The Dual-Stage Conceptual Framework",
            "content": "We view world modeling as two-stage process of perception and prediction (Knill and Pouget, 2004; Ha and Schmidhuber, 2018b). In the first stage, agents form internal representations of the current state by sensing and encoding environmental stimuli. In the second stage, agents use these internal representations to extrapolate future states, refining their model whenever new ground-truth observations arrive. This dual-stage framework explains (1) how raw sensory signals are converted into compact world representations; and (2) how these representations then guide forward simulations. The two stages of WMs are core functions in agents advanced planning and 2 decision-making. We brief our formulation here and leave the in-depth discussion in Appendix A."
        },
        {
            "title": "2.1 Perception Stage",
            "content": "Perception involves extracting and organizing information from multi-sensory cues essential (Merleau-Ponty, 2004; Coren et al., 2004). It is not just bottom-up signal processing but also top-down inference grounded in prior knowledge. For instance, many intelligent animals (e.g., crows, dolphins, chimpanzees) grasp object permanence by maintaining accurate representations of hidden or partially occluded objects (Baillargeon et al., 1985; Hoffmann et al., 2011). While real-world spans multitudinous modalities perception temperature, humidity, audition, (time, vision, proprioception, etc.), we only focus on limited number of perceptual dimensions. From an epistemic perspective, we design these dimensions to maximize the information captured about the external world while minimizing representational dimensions, ensuring they remain mutually orthogonal. From the practical perspective, we only cover the dimensions that modern VLMs can access. In this work, we consider 5 perceptual dimensions in our framework: space, time, motion, quantity, and vision. To make models perceptual competency empirically testable along each dimension, we further break the major 5 perceptual dimensions into sub-dimensions: Space and Time. All entities in spacetime must be located at some position and occupy some room or period, i.e., extension. Any pair of entities to exist in spacetime necessarily have spatiotemporal relations (e.g., front, left, before, after). Thus, we break spacetime down into position, extension, and relations (Reichenbach, 2012). Motion. At every moment, motion can be described as vector with speed and direction. The integration of motion along time forms trajectory. Thus we consider direction, speed, and trajectory (Johansson, 1975). Quantity. Axiomatically, quantities can either be discrete or continuous. For any pair of quantities, there could be quantitative relations (e.g., more, less, n-times) (Kaufman et al., 1949; Hurst and Piantadosi, 2024). We consider discrete, continuous, and relations (Kadosh and Dowker, 2015) quantities in this study. Vision. In contrast with the previous four dimensions, the vision channel is extremely broad including orientation, and hard to enumerate, density, edge, color, shape, to name few (Marr, 2010). We consider salient features like color, shape, and material (i.e., texture and reflexivity)."
        },
        {
            "title": "2.2 Prediction Stage",
            "content": "Simulation. Agents Once current-state representations are established, the agent must predict how future states evolve in response to both natural dynamics and possible actions. We distinguish 3 primary sub-dimensions: Mechanistic should understand the causality of intuitive physical dynamics (e.g., motion, collisions) and intentional actions to simulate the next state (Hegarty, 2004; Barsalou, 2008). For example, predicting how one ball bounces off wall or another ball depends on basic principles like momentum or elasticity. Transitive Inference. Multi-step forecasts are often needed for tasks requiring long-horizon planning (Prystawski et al., 2023). Rather than only predicting the immediate next state, robust world models should extrapolate further into the future by chaining intermediate predictions. Compositional Inference. Real-world scenarios usually involve multiple interacting objects and agents (e.g., two incoming balls hitting third one from different directions). Agents must merge known mechanisms to predict novel outcomes, even if that specific combination has not been observed (Xu and Denison, 2009; Eckert et al., 2021; Gweon et al., 2010). This requires compositional reasoning, where partial preconditions (e.g., hit from the left plus hit from the right) merge into an overall post-condition (e.g., moves straight up)."
        },
        {
            "title": "3 The WM-ABench Benchmark",
            "content": "Following the above taxonomy, we design and implement corresponding tasks for benchmarking VLMs as WMs. Figure 2 presents visual overview of our complete benchmark tasks, offering readers high-level understanding of the framework. Controlled Experiment and Causal Analysis. To ensure controlled evaluation, we exhaustively iterate over all dimensions, keep all other dimensions fixed as independent variables, and allow only one to vary at each data point. This methodology, which holds all variables constant except the independent ones across data points, allows us to draw causal conclusions (e.g., changing color causes the model to misperceive size). Rather than claiming generality or complete 3 Figure 2: Overview of WM-ABench tasks. The Perception stage (top) covers Spatial, Temporal, Visual, Quantity, and Motion dimensions, each shown with example questions and outputs. The Prediction stage (bottom) includes Mechanistic Simulation, which covers Intuitive Physics (e.g., drop), Agent Navigation (e.g., turn left), and Agent Manipulation (e.g., push), plus Transitivity and Compositionality tasks that build on these transitions."
        },
        {
            "title": "Model",
            "content": "SR SE-V SE-O SP TP TE Mani. TDW Mani. TDW Mani. TDW Mani. TDW Mani. TDW Mani. TDW"
        },
        {
            "title": "Time",
            "content": "Open-source Models NVILA QWen2-VL-72b QWen2.5-VL-72b InternVL2.5-78b Llama 3.2 vision-90b LLaVA-OneVision Closed-source Models GPT-4o-mini GPT-4o Claude 3.5 Sonnet Gemini-1.5-flash Gemini-1.5-pro GPT-4.5* OpenAI o3* Claude 3.7 Sonnet* Gemini-2.5-pro* 53.7 40.4 55.0 74.4 84.9 92.9 35.4 26.9 35.5 59.1 32.7 9.0 42.2 72.6 42.6 57.0 87.6 93.6 58.9 47.1 46.5 75.5 36.4 53.4 57.8 70.6 49.5 57.7 97.5 93.5 36.1 45.5 48.4 52.9 35.0 53.5 44.2 65.8 51.3 73.5 98.4 93.0 31.3 30.9 43.5 63.6 43.9 59.7 50.7 47.5 50.0 43.8 78.6 25.2 25.0 24.6 33.4 16.8 38.0 37.2 71.0 61.8 56.6 53.1 96.5 90.9 24.8 22.7 38.5 31.1 32.2 25.8 37.6 37.7 61.3 69.1 98.2 35.3 37.0 38.1 31.7 47.1 29.0 40.6 55.1 58.7 66.1 70.4 96.6 78.9 47.4 47.5 37.5 45.7 35.0 25.0 54.6 59.9 64.2 58.0 73.2 72.6 41.7 31.9 42.0 49.1 32.2 43.7 54.5 59.1 50.0 50.0 80.6 79.6 34.0 21.8 37.5 50.9 33.4 56.1 49.1 65.0 64.0 68.5 83.6 92.9 44.7 43.4 42.8 60.1 77.0 42.2 72.0 79.0 55.0 52.0 95.0 92.0 57.0 38.0 47.0 66.0 55.0 67.0 88.0 88.0 88.0 99.0 100.0 97.0 62.0 56.0 60.0 72.0 53.0 60.0 56.0 72.0 60.0 76.0 82.0 63.0 70.0 60.0 35.0 38.4 22.0 49.0 97.0 90.0 84.0 78.0 100.0 99.0 48.0 62.0 54.0 65.0 35.0 37."
        },
        {
            "title": "Random\nHuman",
            "content": "25.0 25.0 50.0 50.0 50.0 50.0 25.0 25.0 33.3 33.3 33.3 33.3 90.0 100.0 98.0 100.0 100.0 86.0 92.0 92.0 80.0 82.0 86.0 90."
        },
        {
            "title": "Model",
            "content": "V-C V-S V-M MS MD MI MT DQ"
        },
        {
            "title": "Quantity\nCQ",
            "content": "RQ Mani. TDW TDW TDW Mani. TDW Mani. TDW Mani. TDW Mani. TDW Mani. TDW TDW Mani. TDW Open-source Models NVILA QWen2-VL-72b QWen2.5-VL-72b InternVL2.5-78b Llama 3.2 vision-90b LLaVA-OneVision Closed-source Models GPT-4o-mini GPT-4o Claude 3.5 Sonnet Gemini-1.5-flash Gemini-1.5-pro GPT-4.5* OpenAI o3* Claude 3.7 Sonnet* Gemini-2.5-pro*"
        },
        {
            "title": "Random\nHuman",
            "content": "94.6 97.2 94.7 95.4 95.2 95.2 88.0 98.2 56.3 88.6 99.0 63.7 89.4 91.5 51.4 89.3 95.7 53.9 87.0 99.6 50.0 88.8 98.4 71.2 87.8 99.7 50.0 98.1 88.0 98.7 55.6 98.6 62.4 96.2 50.0 42.1 88.0 98.7 50.0 98.3 99.1 86.8 89.2 50.0 100.0 87.0 99.0 51.0 100.0 87.0 99.0 54.0 27.3 76.0 98.0 53.0 100.0 87.0 99.0 55.0 25.0 25.0 50.0 25.0 100.0 88.0 100.0 84.0 34.2 33.3 47.6 23.5 27.4 45.2 24.1 24.2 35.8 36.2 36.1 25.0 27.0 32.0 42. 33.3 84.0 45.1 58.8 74.7 55.3 42.4 40.5 36.7 45.3 69.2 43.3 58.8 57.0 63.0 61.0 68.0 52.6 85.4 69.8 88.5 51.4 54.1 53.5 85.4 71.9 92.1 36.1 53.1 33.3 71.1 59.7 72.2 23.7 33. 53.9 37.8 32.5 34.7 69.4 39.6 43.9 83.8 55.7 25.2 76.2 49.4 30.0 66.1 56.0 75.0 79.0 92.0 89.0 100.0 83.0 34.0 85.7 43.8 46.0 98.0 51.0 25.0 25.0 33.3 90.0 100.0 98.0 25.0 96.0 35.3 84.4 66.8 85.6 23.8 25.5 51.4 38.8 52.6 34.4 35.2 88.0 89.0 46.0 60.0 25.0 98. 24.4 29.5 28.9 31.3 23.8 26.1 26.6 29.9 24.8 24.3 24.2 28.0 48.0 22.0 36.0 14.8 21.5 27.7 25.6 25.9 14.5 17.1 28.9 41.7 14.5 33.8 61.0 70.0 25.0 51.0 57.8 75.7 73.1 76.4 66.5 67.5 31.3 57.0 76.3 76.1 84.9 61.0 85.0 72.0 81. 81.4 63.9 83.1 75.5 84.7 88.0 81.4 76.7 70.8 72.0 76.9 81.1 60.1 73.9 61.3 87.3 73.9 43.3 81.8 80.6 83.1 79.8 87.0 89.0 90.0 96.0 69.0 49.0 92.0 89.0 73.5 75.5 75.9 72.1 70.6 74.0 60.1 64.6 65.0 69.2 73.0 76.0 93.0 69.0 92.0 58.0 74.2 71.3 69.2 67.4 76.8 54.5 57.8 49.6 66.2 69.1 70.0 92.0 49.0 93. 25.0 25.0 50.0 25.0 25.0 76.0 100.0 98.0 100.0 98.0 33.3 33.3 98.0 100.0 Table 1: Results on the Perception tasks in our WM-ABench, reported as accuracies (%). Models are evaluated in two simulators ManiSkill and ThreeDWorld). Cell shades indicate different performance levels (dark red indicates proficient performance; dark blue indicates performance close to random, and the lighter intermediate shades represent levels in between). Random and Human provide reference baselines. * denotes models evaluated on the subset (100 instances) of each dataset. coverage (Raji et al., 2021; Saxon et al., 2024), this benchmark aims to provide precise, atomic diagnosis of models perception and prediction, establishing clear checklist for VLMs as world models. Fighting Shortcuts and Spurious Correlations. Pre-trained models such as VLMs are known to rely on shortcuts and spurious correlations (Ye et al., 2024; Steinmann et al., 2024). To test whether VLMs can truly simulate and extrapolate into the next states, rather than relying on some spurious correlations, we generate hard negative options in our benchmark. We consider 2 methodologies to generate counterfactual states: counterfactual action and counterfactual previous states. Consider ground truth transition triplet (S t+1). For counterfactual action-based option generation, we fix the ground-truth previous , a, , a, state and perturb the action, and the transition becomes (S t+1). For counterfactual statebased option generation, we keep the ground-truth action, and perturb the previous state, so that the transition becomes (S False options generated under these methodologies often exhibit high visual similarity to the ground truth state, requiring models to possess genuine understanding of world dynamics to distinguish and eliminate counterfactual states. t, a, t+1). Data Collection. To rigorously evaluate models, large and diverse dataset of test cases is essential. While manual data collection is possible, it is costly and often impractical for obtaining images where only single factor varies for controlled studies. Therefore, we utilize compute-scalable simulation frameworks to synthetically generate substantial number of test cases, minimizing human labor 5 Model Mechanistic Simulation Compositionality Car. Hab. Phys. Phys. Phys. Mani. Mani. Mani. Car. Hab. Mani. Mani. TDW Mani. Mani. nav nav coll slide drop drop lift push nav nav pu-pi pi-ro coll push lift Transitivity Open-source Models NVILA Qwen2-VL-72b Qwen2.5-VL-72b InternVL2.5-78b Llama-3.2-90b LLaVA-OneVision Closed-source Models GPT-4o GPT-4o-mini Claude 3.5 Sonnet Gemini-1.5-flash Gemini-1.5-pro GPT-4.5* OpenAI o3* Claude 3.7 Sonnet* Gemini-2.5-pro* 31.3 16.7 28.4 44.6 44.7 48.9 62.0 65.3 30.4 40.3 59.2 95.3 76.6 70.9 36.5 51.9 55.0 88.9 57.9 54.6 24.9 41.3 51.3 73.7 24.7 33.3 24.0 25.4 24.7 23.7 19.0 57.9 25.8 20.5 19.4 26.1 61.6 69.6 39.2 37.6 49.4 83.9 45.8 34.0 25.8 32.9 49.9 77.7 36.3 39.6 15.0 22.1 36.7 86.3 36.8 41.1 27.3 30.2 42.2 75.7 44.3 57.6 47.5 37.6 60.1 79.4 71.0 34.0 55.0 72.0 56.0 86.0 85.0 87.0 60.0 78.0 79.0 80.0 59.0 51.0 38.5 49.0 67.7 96.0 81.0 77.0 53.0 68.0 73.0 87.0 33.8 91.9 85.5 60.3 26.3 30.8 71.2 68.2 57.7 65.8 76.8 59.0 83.0 56.4 76.0 26.4 32.4 4.2 29.1 20.0 50.1 43.8 55.7 31.9 61.1 34.7 24.4 28.1 51.1 31.9 35.6 23.1 23.9 33.6 25.6 25.1 22.6 26.7 25.4 26.4 26.6 25.5 31.2 24.3 51.3 30.9 21.6 34.8 34.7 40.1 47.0 20.8 25.9 24.1 22.4 22.4 25. 30.3 42.6 43.2 42.4 30.9 41.6 24.4 32.9 28.0 35.3 27.5 36.3 18.4 40.7 39.0 42.7 33.5 48.1 43.3 46.2 40.0 47.0 34.0 44.0 50.0 61.0 51.0 42.0 39.4 44.0 26.0 34.3 44.0 66.0 67.0 42.0 45.5 22.8 33.8 39.4 37.2 40.8 37.5 39.0 41.9 37.0 40.2 30.8 35.9 34.1 36.8 35.0 32.0 41.0 48.0 33.0 37.0 48.4 41.5 29.0 54.0 38.0 47.0 31.8 41.8 35.2 27.8 25.3 28.2 35.6 30.8 51.0 46.2 49.6 39.0 37.0 49.5 57.0 Random Human 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 98.0 98.0 100.0 98.0 86.0 100.0 100.0 100.0 78.0 90.0 80. 25.0 25.0 25.0 25.0 25.0 82.0 84.0 88.0 100.0 Table 2: Results of the Prediction tasks in WM Benchmark (%). Our mechanistic simulation tasks cover three categories: intuitive physics (e.g., drop, slide collision), agent manipulation (e.g., lift, push), and navigation (e.g., turn left, move forward). * denotes models evaluated on the subset (100 instances) of each dataset. while ensuring precise control over variations. To avoid bias towards one single environment, we use wide variety of simulation frameworks to create data, including ThreeDWorld (TDW; Gan et al., 2021), ManiSkill (Tao et al., 2024; Gu et al., 2023), Habitat 2.0 (Szot et al., 2021a), Physion (Bear et al., 2022), and Carla (Dosovitskiy et al., 2017a). Our diverse set of simulators allows the benchmark to incorporate various world dynamics that align with human intuition while leveraging broad range of assets to address diverse questions."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "a evaluate on state-of-the-art VLMs Evaluated Models. We range WM-ABench, of including closed-source models: Gemini-1.5-pro, Gemini-1.5-flash (Google et al., 2024), GPT-4o (gpt-4o-2024-08-06), GPT-4o-mini (OpenAI et al., 2024), Claude 3.5 Sonnet (Anthropic, 2024), and open-source models: Qwen2-VL (72B) (Bai et al., 2023), Qwen2.5-VL (72B) (Team, 2024), InternVL2.5 (78B) (Chen et al., 2024), LLaVA-OneVision (72B) (Li et al., 2024a), NVILA (15B) (Liu et al., 2024c), and Llama 3.2-Vision (90B) (Meta AI, 2024). We used the same system prompt (see Appendix E.2) and greedy decoding for all questions to maintain consistent output formatting. We evaluate model performance by comparing the parsed labels from model outputs to the ground-truth labels, and skip the instances where model outputs failed to be parsed by our template. Human Evaluation. For each subtask, we randomly sample 50 questions and employ Amazon Mechanic Turk for human evaluations. We asked 3 evaluators for each question and finalized the labels via majority voting. The inter-rater agreement is measured by Fleiss kappa on 50 samples for each task. All tasks are above moderate agreement (Fleiss > 0.4). Detailed instructions are available in Appendix E.1)."
        },
        {
            "title": "4.2 Main Results on Perception Tasks",
            "content": "Table 1 presents the results for perception tasks. In fact, closed-source VLMs do not exhibit an overwhelming advantage over open-source models in terms of perceptual capabilities. Across the 5 dimensions, Qwen2-VL achieves the highest overall performance with an average accuracy of 67.7%. All models still fall behind human-level perception, which is near-perfect or substantially more accurate across all the perception tasks. Recommendation 1: Improve 3D Perception. Spatial tasks require grounding and reasoning over 3D semantics, including constructing robust scene representations from limited views. Even the most advanced models achieve less than 60% accuracy in spatial positioning tasks. These results suggest that current VLMs struggle to form robust internal 3D representations, in line with previous findings (El Banani et al., 2024; Zhang et al., 2025). We recommend future VLMs transition from relying solely on 2D semantics to incorporating 3D priors or explicit 3D representations. Recommendation 2: Improve Temporal and Motion Understanding. We found that models struggle with coherent temporal representations across consecutive frames, as reflected in their low performance on temporal extension (TE). In contrast, they perform much better on tasks relying on subset of frames, such as temporal positioning (TP). Similarly, while models perform relatively well on motion detection (MD), they exhibit near-random performance on motion trajectory (MT), which demands thorough understanding of consecutive states. These results suggest that current VLMs still struggle to perceive and form dynamic scene representations. We recommend future VLMs incorporate temporal and motion priors by leveraging the rich visual dynamics in videos (Song et al., 2024; Ko et al., 2024)."
        },
        {
            "title": "4.3 Main Results on Prediction Tasks",
            "content": "Table 2 presents the results for prediction tasks, which generally pose greater challenges for VLMs than perception tasks. Qwen2-VL achieves the highest average accuracy of 47.5%. Similar to perception tasks, open-source VLMs perform on par with closed-source ones, and all VLMs fall behind human performance by large margin. Recommendation 3: Improve Cause-and-Effect Understanding. Our prediction tasks cover intuitive physics (e.g., drop, collision) and agent-initiated actions like navigation (e.g., turn left, move forward), and manipulation (e.g., lift, push). We find that VLMs struggle with predicting transitions and the post-condition of physical manipulation actions. For instance, on the ManiSkill simulator, Qwen2-VL achieves 95.3% accuracy in predicting the outcomes of dropping and 91.4% in lifting. However, its performance on pushing objects in the same environment is close to random, and its accuracy in predicting dropping outcomes on Physion decreases to 59.2%. These results suggest that current VLMs still struggle to reliably predict the cause and effect of physical processes and actions (Gao et al., 2018). Recommendation 4: Improve Transitive and Compositional World Modeling. Our results also reveal that VLMs do poorly in transitive and compositional inference when reasoning over sequential or concurrent actions. In transitive inference tasks, models consistently underperform, with even the best achieving only 43.8% on the multi-step navigation task in Habitat, which is far below the human accuracy of 90.0%. Similarly, in compositional inference tasks, the gap remains substantial: the best models reach only 40.2% on the collision prediction task in TDW and 51.3% on the manipulation task in ManiSkill, compared to the human performance of 84.0% and 88.0%."
        },
        {
            "title": "4.4 Performance of Frontier VLMs",
            "content": "frontier VLMs We extend our evaluation to include the most recent (Kavukcuoglu, 2025; OpenAI, 2025b,a; Anthropic, 2025), including visual reasoning models such as OpenAI o3. Due to computational constraints and API limitations, we evaluate these frontier models on subset of 100 instances per task. Frontier Models Reach Human Parity in Static Perception. Frontier VLMs (e.g., Gemini-2.5Pro, o3, GPT-4.5) demonstrate clear improvements over previous state-of-the-art models across static perception tasks. On the Size Comparison (SEO) task in ThreeDWorld, frontier models achieve 92-100% accuracy, up from GPT-4os 88%, even exceeding the 93% human baseline. Similarly, Relative Position (SR) performance improves from 71-73% (LlaVA-OneVision, QWen2-VL) to 9097% (Gemini-2.5-Pro). Notable Gains in Mechanistic Simulation but Gaps Remain. We also observe substantial improvements from these models in mechanistic simulation tasks. In Carla Navigation task, accuracy rises from 60% (GPT-4o) to 81-85% for Gemini-2.5-Pro and OpenAI o3, though still below 98% human performance. Intuitive physics tasks (Physion Collision, Slide) advance from 47-60% to 60-79%. Persistent Challenges in Spatial, Temporal, and Compositional Inference. Depite these performance gains, certain spatial and temporal perception tasks remain challenging. Multi-view spatial reasoning (SP) for these frontier models does not exceed 70%, well under the 90%+ human level. Temporal Extension (TE) shows minimal improvement, with only 7.3% gains from GPT-4.5. Advanced prediction tasks also shows little progress, with the highest ManiSkill Lift performance (57%, Gemini-2.5-Pro) significantly lagging human capabilities."
        },
        {
            "title": "5.1 VLMs fail to represent different world\nattributes robustly and independently",
            "content": "World models disentangled representations of key perceptual dimensions, like color or spatial position, for flexible compositional should learn Figure 3: Model performance with respect to increasing stimulus differences, as discussed in Section 5.2. Here, Object Num., Size, Speed, and Time represent the differences in the number of objects, object sizes, object speeds, and object movement durations, respectively. GPT-4o Gemini-1.5 Qwen2-VL InternVL 2.5 collide slide drop -1.09 1.67 3.55 -0.52 4.02 1.50 -0.62 1.36 1.94 0.36 1.15 2.95 (cid:12) (cid:12) (cid:12) i= (cid:80)N ppi Table 3: Accuracy differences (()%) between filtered (correct state perception) and unfiltered inputs across physical transition tasks. reasoning and categorical distinctions.1 To assess entanglement, we perturb one dimension while keeping others constant and define the standardized Relative Entanglement (s-RE) as the normalized performance deviation between (cid:12) different perturbations: s-RE = 1 (cid:12) (cid:12), where pi is model performance under the ith perturbation and is the average performance over all perturbations. Essentially, s-RE calculates the average relative deviation from the mean performance, reflecting model sensitivity to changes in that particular dimension. We control 5 perceptual dimensions (color, shape, size, position, material) and present the entanglement heatmap in Figure 4. Our results show that VLMs representations of orthogonal world attributes have significant entanglement. Color and Shape are major sources of entanglement in multiple tasks. For example, color entangles with the Discrete Quantity (DQ) task, where s-RE ranges from 5% (Gemini-1.5 Pro) to 17% (Qwen-2.5 VL). Other dimensions like size and absolute position also exhibit relatively modest entanglement effects."
        },
        {
            "title": "5.2 VLMs are sensitive to stimulus differences,",
            "content": "but not fine details As shown in Figure 3, we find that VLM performance is positively correlated with the such as physical differentiability of stimuli, 1Human perception also relies on interdependent encoding of visual properties, e.g., object attributes are represented as statistical summaries rather than in isolation (Whitney and Yamanashi Leib, 2018). However, unlike VLMs, human cognition can differentiate these summaries into discrete symbolic states, as evident in our evaluations, enabling precise world representation despite potential interdependencies. 8 differences in size, speed, or moving duration. This aligns with the previously reported myopia in model perception (Rahmanzadehgervi et al., 2024). Our observation suggests that VLMs can, to some extent, effectively ground language in corresponding physical attributes. This is evidenced by their strong performance in scenarios with large stimulus differences, which would otherwise be inexplicable. On the other hand, VLMs perception capabilities are strongly influenced by the magnitude of the stimulus, irrespective of specific physical attributes. This suggests that while they perform well when distinctions they struggle with fine-grained, high-resolution perception, highlighting significant gap in modeling subtle physical variations. are pronounced,"
        },
        {
            "title": "5.3 VLMs struggle with intuitive physics even",
            "content": "under accurate state perception Accurate next-state prediction relies on two key factors: correctly representing the current state and possessing sufficient mechanistic knowledge for transition simulation. Our analysis reveals that models often make mistakes in perception, which may contribute to their underperformance in prediction tasks, stemming from limited world modeling capabilities or accumulated errors resulting from perception failures. To disentangle perception from prediction, we present further the mechanistic simulation from analysis of intuitive physics (collide, slide, drop). We retrieve instances where all models correctly answer all relevant perception questions, ensuring an accurate state representation (details provided in Appendix D.2). As Table 3 shows, performance on predicting the next state of slide and drop increases only marginally or even decreases for collide, indicating that limited perception capability is not the only cause. Rather, models lack the foundational physical knowledge to simulate"
        },
        {
            "title": "Model",
            "content": "SP TP TE V-C V-S MT Lift Drop Open-source Models QWen2-VL-72b 32.1 76.7 63.3 100.0 100.0 30.0 QWen2.5-VL-72b 35.3 76.7 76.7 100.0 100.0 23.3 30.0 90.0 63.3 100.0 100.0 40.0 InternVL2.5-78b Closed-source Models Claude 3.5 Sonnet 26.7 70.0 60.0 100.0 100.0 26.7 43.3 83.3 60.0 100.0 100.0 30.0 Gemini-1.5-flash 40.0 53.3 80.0 63.3 100.0 97.7 Gemini-1.5-pro 50.0 90.0 60.0 100.0 97.7 GPT-4o 26.7 33.3 83.3 50.0 100.0 100.0 30.0 GPT-4o-mini"
        },
        {
            "title": "Random",
            "content": "25.0 33.0 33.0 25.0 25.0 25.0 53.3 76.7 30.0 40.0 36.7 53.3 60.0 60.0 25.0 80.0 73.3 50. 66.7 50.0 76.7 80.0 53.3 25.0 Table 4: Results across 8 dimensions adapted from realworld datasets into WM-ABench design. We recast from real datasets and curate approximately 50 data points for each selected dimension. object interactions accurately. 5.4 WM-ABench from Real-World Data Transferring WM-ABenchs controlled settings to real-world scenarios is challenging due to the need for precise attribute manipulations (e.g., altering object colors) and counterfactual state generation (e.g., applying forces to the target object). To draw some insights on whether our simulation findings might generalize to real-world data, we repurpose existing datasets (Walke et al., 2023; Liu et al., 2024a; Lai et al., 2014; Ammirato et al., 2018; Yu et al., 2023; Shangguan et al., 2025; Hwang et al., 2025) and recast them into our evaluation format. For each dimension, we curated approximately 50 data points and evaluated several top-performing models. Results in Table 4 demonstrate consistent alignment between real-world and simulated performance patterns. For instance, models exhibit poor performance on Spatial Positioning (SP) and Motion Trajectory (MT), mirroring simulation trends. Conversely, they achieve nearperfect accuracy on Color and Shape recognition Additionally, Temporal Positioning tasks. performance (7090%) consistently exceeds Temporal Extension (5076%), confirming our earlier findings. These trends suggest that bias from our use of simulation data is likely to be minimal, and our findings tend to be generalizable."
        },
        {
            "title": "6 Related Work",
            "content": "World Models. World models (WMs) predict how the current state transitions to the next, based on prior states and actions (Tolman, 1948; Battaglia et al., 2013). Traditionally, people train framelevel video-generative models specializing in some narrow domains. For instance, in robotics, WMs enable model-based reinforcement learning and trajectory prediction (Yang et al., 2023; Zhou et al., 2024); in autonomous driving (Wang et al., 2023; Hu et al., 2023), they facilitate path planning; and in gaming (Hafner et al., 2019; Bruce et al., 2024; Ha and Schmidhuber, 2018a,c), they power interactive simulations. Meanwhile, recent work (Brooks et al., 2024; Kang et al., 2024) has explored whether video-generation models can serve as world simulators that go beyond mere pixel-level synthesis. We instead investigate whether VisionLanguage Models can capture world dynamics from large-scale training data, enabling them to function as generalist world models. low-level Benchmarks for Vision Language Models. Previous VLM benchmarks typically take reductionist approach, measuring wide range of perceptual capabilities while giving limited attention to how these perceptual dimensions interact and influence one another. For instance, there are works focusing on visual semantics perception, e.g., object categories, attributes, actions, agent-object interactions, emotions (Liu et al., 2024a; Li et al., 2024c; Liu et al., 2023), visual whereas others emphasize perception, e.g., basic attributes, line segments, optical flow (Johnson et al., 2016; Fu et al., 2024b; Shiono et al., 2025), spatiotemporal and motion, e.g., geometry, event ordering, trajectories (Goyal et al., 2020; Mirzaee et al., 2021; Shangguan et al., 2024), or next-state prediction (often limited to intuitive physics) (Bear et al., 2022; Yi et al., 2020b). Compared to these efforts, our framework decomposes world modeling into atomic dimensions, offering precise diagnosis of models perception and prediction capabilities while establishing clear checklist for VLMs as world models. Due to the page limit, we leave the comprehensive comparison between WM-ABench and existing benchmarks in Table 6 and Appendix B.2."
        },
        {
            "title": "7 Conclusion",
            "content": "Our study provides the first atomic evaluation of VLMs internal world modeling abilities with cognitively inspired framework. While VLMs excel in scenarios with pronounced differences, they struggle with 3D and dynamic perception, fail to differentiate subtle physical distinctions, and exhibit failures in understanding world transitions of transitive and compositional scenarios."
        },
        {
            "title": "References",
            "content": "While simulators enable compute-scalable and cheap generation of dataset questions and answers, most simulators are difficult to tune or are incapable of photo-realistic image generation. As result, we may be evaluating VLMs on somewhat out-ofdistribution data as the images do not look realistic and the majority of the image data used to train VLMs likely come from real-world videos/images. While ray tracing is used in some of the ManiSkillgenerated problems, better photo-realism can be achieved if higher quality assets are used and lighting is tuned better. Ethics Statement Human Study. We only involve human subjects to obtain the human performance on our benchmark. The institutions Institutional Review Board (IRB) considered this project exempt from ongoing review. All human evaluations in our study are conducted in Amazon Mechanical Turk (MTurk), with participants fairly compensated according to ethical guidelines. Further details on the evaluation process are provided in the Appendix E.1). Societal Impact. Our benchmark aims to provide an atomic evaluation of VLMs fundamental WM abilities. Since our experiments are conducted in simulated environments rather than using realworld data, our work does not raise ethical concerns related to privacy, bias, or societal impact. Licenses. We strictly adhere to the protocols governing the academic use of all VLMs. Our benchmark is designed to evaluate the performance of VLMs across various dimensions, focusing primarily on inference rather than the training of large-scale models. As result, it is unlikely to have significant environmental impact. AI Assistants In Research. Furthermore, regarding the use of AI tools, we only used ChatGPT for grammar checking and made minor refinements to our visualization code with its assistance, which is entirely unrelated to the scientific content or findings of our work."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Angela Shen, Kai Kim, Reventh Sharma, and Prathish Murugan for their contribution to early data collection, and Yi Gu, Yuheng Zha for assistance with setting up simulation environments. Bo Ai, Stephen Tian, Haochen Shi, Yixuan Wang, Cheston Tan, Yunzhu Li, and Jiajun Wu. 2024. Robopack: Learning tactile-informed dynamics In Robotics: Science models for dense packing. and Systems (RSS). Phil Ammirato, Alexander C. Berg, and Jana Košecká. In 2018 2018. Active vision dataset benchmark. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 212721273. Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3948. Anthropic. 2025. Claude 3 & 7 sonnet system card. https://assets.anthropic.com/m/785e23186 9ea8b3b/original/claude-3-7-sonnet-syste m-card.pdf. Accessed May 31, 2025. AI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966. Renée Baillargeon, Elizabeth Spelke, and Stanley Wasserman. 1985. Object permanence in five-monthold infants. Cognition, 20(3):191208. Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick. 2019. Phyre: new benchmark for physical reasoning. Preprint, arXiv:1908.05656. Fabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, and Christian Wolf. 2020. Cophy: Counterfactual learning of physical dynamics. Preprint, arXiv:1909.12000. Lawrence Barsalou. 2008. Grounded cognition. Annu. Rev. Psychol., 59(1):617645. Peter Battaglia, Jessica Hamrick, and Joshua Simulation as an engine of Tenenbaum. 2013. physical scene understanding. PNAS. Daniel M. Bear, Elias Wang, Damian Mrowca, Felix J. Binder, Hsiao-Yu Fish Tung, R. T. Pramod, Cameron Holdaway, Sirui Tao, Kevin Smith, FanYun Sun, Li Fei-Fei, Nancy Kanwisher, Joshua B. Tenenbaum, Daniel L. K. Yamins, and Judith E. Fan. 2022. Physion: Evaluating physical prediction from vision in humans and machines. Preprint, arXiv:2106.08261. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. 2024. Video generation 10 models as world simulators. OpenAI, https://op enai.com/research/video-generation-model s-as-world-simulators. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. 2024. Genie: Generative interactive environments. arXiv preprint arXiv:2402.15391. Peter E. Bryant and Tom Trabasso. 1971. Transitive inferences and memory in young children. Nature, 232:456458. Haoran Chen, Jianmin Li, Simone Frintrop, and Xiaolin Hu. 2022. The msr-video to text dataset with clean annotations. Computer Vision and Image Understanding, 225:103581. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. 2024. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271. An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. 2024. SpatialRGPT: Grounded spatial reasoning in vision-language models. In The Thirtyeighth Annual Conference on Neural Information Processing Systems. Stanley Coren, Lawrence Ward, and James Enns. 2004. Sensation and perception. John Wiley & Sons Hoboken, NJ. Carl Craver, James Tabery, and Phyllis Illari. 2024. In Edward N. Zalta and Mechanisms in Science. Uri Nodelman, editors, The Stanford Encyclopedia of Philosophy, Fall 2024 edition. Metaphysics Research Lab, Stanford University. Arnaud Delorme, Guillaume Rousselet, Marc J-M Macé, and Michele Fabre-Thorpe. 2004. Interaction of top-down and bottom-up processing in the fast visual analysis of natural scenes. Cognitive Brain Research, 19(2):103113. Stephanie Denison, Pallavi Trikutam, and Fei Xu. 2014. Probability versus representativeness in infancy: Can infants use naïve physics to adjust population base rates in probabilistic inference? Developmental psychology, 50(8):2009. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. 2017a. CARLA: An open urban driving simulator. In Proceedings of the 1st Annual Conference on Robot Learning, pages 116. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. 2017b. CARLA: An open urban driving simulator. In Proceedings of the 1st Annual Conference on Robot Learning, pages 116. Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. 2023. Faith and fate: Limits of transformers on compositionality. Preprint, arXiv:2305.18654. Johanna Eckert, Hannes Rakoczy, Shona Duguid, Esther Herrmann, and Josep Call. 2021. The ape lottery: Chimpanzees fail to consider spatial information when drawing statistical inferences. Animal Behavior and Cognition. Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. 2024. Probing the 3d awareness In Proceedings of of visual foundation models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2179521806. Sagi Eppel. 2024. Do large language vision models understand 3d shapes? Preprint, arXiv:2412.10908. Chris Frith and Raymond Dolan. 1997. Brain mechanisms associated with top-down processes in perception. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 352(1358):12211230. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. 2024a. Mme: comprehensive evaluation benchmark for multimodal large language models. Preprint, arXiv:2306.13394. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, and Ranjay Krishna. 2024b. Blink: Multimodal large language models can see but not perceive. Preprint, arXiv:2404.12390. Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, Kuno Kim, Elias Wang, Michael Lingelbach, Aidan Curtis, Kevin T. Feigelis, Daniel Bear, Dan Gutfreund, David D. Cox, Antonio Torralba, James J. DiCarlo, Josh Tenenbaum, Josh H. McDermott, and Dan Yamins. 2021. Threedworld: platform for interactive multimodal physical simulation. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. Qiaozi Gao, Shaohua Yang, Joyce Chai, and Lucy Vanderwende. 2018. What action causes this? towards naive physical action-effect prediction. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 934945. 11 Andreas Geiger, Philip Lenz, and Raquel Urtasun. 2012. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pages 33543361. IEEE. Rohit Girdhar and Deva Ramanan. 2020. Cater: diagnostic dataset for compositional actions and temporal reasoning. Preprint, arXiv:1910.04744. Bruce Goldstein. 1989. Sensation and perception. Wadsworth/Thomson Learning. Google. 2023. Gemini: family of highly capable multimodal models. Technical report, Google. Google, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Preprint, arXiv:2403.05530. Ankit Goyal, Kaiyu Yang, Dawei Yang, and Jia Deng. 2020. Rel3d: minimally contrastive benchmark for grounding spatial relations in 3d. In Advances in Neural Information Processing Systems, volume 33, pages 1051410525. Curran Associates, Inc. Yash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2018. Making the in vqa matter: Elevating the role of image understanding in visual question International Journal of Computer answering. Vision, 127(4):398414. Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, Xiaodi Yuan, Pengwei Xie, Zhiao Huang, Rui Chen, and Hao Su. 2023. Maniskill2: unified benchmark for generalizable manipulation skills. In International Conference on Learning Representations. Xianda Guo, Ruijun Zhang, Yiqun Duan, Yuhang He, Chenming Zhang, Shuai Liu, and Long Chen. 2024. Drivemllm: benchmark for spatial understanding with multimodal in autonomous driving. Preprint, arXiv:2411.13112. language models large Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. 2019. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603. Mary Hegarty. 2004. Mechanical reasoning by mental simulation. Trends in cognitive sciences, 8(6):280 285. Almut Hoffmann, Vanessa Rüttler, and Andreas Nieder. 2011. Ontogeny of object permanence and object tracking in the carrion crow, corvus corone. Animal behaviour, 82(2):359367. Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. 2023. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080. Zhiting Hu and Tianmin Shu. 2023. Language models, agent models, and world models: The law for machine reasoning and planning. Preprint, arXiv:2312.05230. Michelle Hurst and Steven Piantadosi. 2024. Continuous and discrete proportion elicit different cognitive strategies. Cognition, 252:105918. Minyoung Hwang, Joey Hejna, Dorsa Sadigh, and Yonatan Bisk. 2025. Motif: Motion instruction fine-tuning. IEEE Robotics and Automation Letters, 10(3):22872294. Gunnar Johansson. 1975. Visual motion perception. Scientific American, 232(6):7689. Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. 2016. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. Preprint, arXiv:1612.06890. P.N. Johnson-Laird. 1983. Mental Models: Towards Cognitive Science of Language, Inference, and Consciousness. Cognitive science series. Harvard University Press. C. Kadosh and A. Dowker. 2015. The Oxford Handbook of Numerical Cognition. Oxford handbooks. Oxford University Press. Hyowon Gweon, Joshua B. Tenenbaum, and Laura E. Infants consider both the sample Schulz. 2010. and the sampling process in inductive generalization. Proceedings of the National Academy of Sciences, 107(20):90669071. Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. 2024. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385. David Ha and Jürgen Schmidhuber. 2018a. Recurrent world models facilitate policy evolution. Advances in neural information processing systems, 31. David Ha and Jürgen Schmidhuber. 2018b. World models. arXiv preprint arXiv:1803.10122. David Ha and Jürgen Schmidhuber. 2018c. World models. arXiv preprint arXiv:1803.10122. 12 Immanuel Kant, John Miller Dow Meiklejohn, Thomas Kingsmill Abbott, and James Creed Meredith. 1934. Critique of pure reason. JM Dent London. Heather Barry Kappes and Carey Morewedge. substitute for Social and Personality Psychology 2016. experience. Compass, 10(7):405420. simulation as"
        },
        {
            "title": "Mental",
            "content": "Edna Kaufman, Miles Lord, Thomas Whelan Reese, The and John Volkmann. 1949. discrimination of visual number. The American journal of psychology, 62(4):498525. Koray Kavukcuoglu. 2025. Gemini 2.5: Our most intelligent ai model. Google Blog. Accessed May 31, 2025. Bruce Kendall, Cheryl Briggs, William Murdoch, Peter Turchin, Stephen Ellner, Edward McCauley, Roger Nisbet, and Simon Wood. 1999. Why do populations cycle? synthesis of statistical and mechanistic modeling approaches. Ecology, 80(6):17891805. Mukul Khanna, Yongsen Mao, Hanxiao Jiang, Sanjay Haresh, Brennan Shacklett, Dhruv Batra, Alexander Clegg, Eric Undersander, Angel X. Chang, and Manolis Savva. 2023. Habitat synthetic scenes dataset (hssd-200): An analysis of 3d scene scale and realism tradeoffs for objectgoal navigation. Preprint, arXiv:2306.11290. David Knill and Alexandre Pouget. 2004. The bayesian brain: the role of uncertainty in neural coding and computation. TRENDS in Neurosciences, 27(12):712719. Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, and Joshua B. Tenenbaum. 2024. Learning from actionless videos through dense to act In The Twelfth International correspondences. Conference on Learning Representations. Alexander Kuhnle Shapeworld - multimodal arXiv:1704.04517. and Ann Copestake. 2017. new test methodology for Preprint, language understanding. Kevin Lai, Liefeng Bo, and Dieter Fox. 2014. Unsupervised feature learning for 3d scene labeling. In 2014 IEEE International Conference on Robotics and Automation (ICRA), pages 30503057. Yann LeCun. 2022. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024a. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Seed-bench: llms with generative Yixiao Ge, and Ying Shan. 2023. Benchmarking multimodal comprehension. Preprint, arXiv:2307.16125. Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. 2024b. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895. 13 Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. 2024c. Mvbench: comprehensive multimodal video understanding benchmark. Preprint, arXiv:2311.17005. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. 2024d. Mvbench: comprehensive multi-modal video In Proceedings of the understanding benchmark. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2219522206. Fangyu Liu, Guy Emerson, and Nigel Collier. 2023. the Visual spatial reasoning. Association for Computational Linguistics, 11:635 651."
        },
        {
            "title": "Transactions of",
            "content": "Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. 2024a. Mmbench: Is your multi-modal model an all-around player? Preprint, arXiv:2307.06281. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. 2024b. Mmbench: Is your multi-modal model an all-around player? In Computer Vision ECCV 2024: 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part VI, page 216233, Berlin, Heidelberg. Springer-Verlag. Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, AnChieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, and Yao Lu. 2024c. Nvila: Efficient frontier visual language models. Preprint, arXiv:2412.04468. Francesco Mannella and Giovanni Pezzulo. 2024. Transitive inference as probabilistic preference learning. Psychonomic Bulletin & Review. David Marr. 2010. computational Vision: investigation into the human representation and processing of visual information. MIT press. Brendan McGonigle and Margaret Chalmers. 1977. Are monkeys logical? Nature, 267(5613):694696. Andrea Mechelli, Cathy Price, Karl Friston, and Alumit Ishai. 2004. Where bottom-up meets topdown: neuronal interactions during perception and imagery. Cerebral cortex, 14(11):12561265. Maurice Merleau-Ponty. 2004. The world of perception. Routledge. Meta AI. 2024. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. https: //ai.meta.com/blog/llama-3-2-connect-2 024-vision-edge-mobile-devices/. [Online; accessed 15-February-2025]. Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, and Parisa Kordjamshidi. 2021. SPARTQA: textual question answering benchmark for spatial reasoning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 45824598, Online. Association for Computational Linguistics. Stephen Mitroff and Brian Scholl. 2004. Seeing the disappearance of unseen objects. Perception, 33(10):12671273. Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. 2024. Robocasa: Largescale simulation of everyday tasks for generalist robots. In Robotics: Science and Systems (RSS). Mary Olmstead and Valerie Kuhlmeier. 2015. Comparative cognition. Cambridge University Press. OpenAI. 2025a. Gpt-4-5 system card. http://cdn.op enai.com/gpt-4-5-system-card-2272025.pdf. Accessed May 31, 2025. OpenAI. 2025b. Openai o3 and o4-mini system card. https://cdn.openai.com/pdf/2221c875-02d c-4789-800b-e7758f3722c1/o3-and-o4-min i-system-card.pdf. Accessed May 31, 2025. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel BernadettShapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. Gpt4 technical report. Preprint, arXiv:2303.08774. Letitia Parcalabescu, Michele Cafagna, Lilitta Muradjan, Anette Frank, Iacer Calixto, and Albert 14 Gatt. 2022. VALSE: task-independent benchmark for vision and language models centered on linguistic In Proceedings of the 60th Annual phenomena. the Association for Computational Meeting of Long Papers), pages Linguistics (Volume 1: Ireland. Association for 82538280, Dublin, Computational Linguistics. Maitreya Patel, Tejas Gokhale, Chitta Baral, and Yezhou Yang. 2022. Cripp-vqa: Counterfactual reasoning about implicit physical properties via video question answering. Preprint, arXiv:2211.03779. Xinyu Pi, Mingyuan Wu, Jize Jiang, Haozhen Zheng, Beitong Tian, Chengxiang Zhai, Klara Nahrstedt, and Zhiting Hu. 2024. Uouo: Uncontextualized uncommon objects for measuring knowledge horizons of vision language models. arXiv preprint arXiv:2407.18391. Ben Prystawski, Michael Y. Li, and Noah D. Goodman. 2023. Why think step by step? reasoning emerges from the locality of experience. Preprint, arXiv:2304.03843. Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adrià Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and João Carreira. 2023. Perception test: diagnostic benchmark for multimodal video models. Preprint, arXiv:2305.13786."
        },
        {
            "title": "Pooyan",
            "content": "Rahmanzadehgervi, Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. 2024. Vision language models are blind. Preprint, arXiv:2407.06581."
        },
        {
            "title": "Logan",
            "content": "Inioluwa Deborah Raji, Emily Denton, Emily M. Bender, Alex Hanna, and Amandalynne Paullada. 2021. AI and the everything in the whole wide In Thirty-fifth Conference on world benchmark. Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Rajesh PN Rao and Dana Ballard. 1999. Predictive coding in the visual cortex: functional interpretation of some extra-classical receptive-field effects. Nature neuroscience, 2(1):7987. H. Reichenbach. 2012. The Philosophy of Space and Time. Dover Books on Physics. Dover Publications. Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, Véronique Izard, and Emmanuel Dupoux. 2020. Intphys: framework and benchmark for visual intuitive physics reasoning. Preprint, arXiv:1803.07616. Stuart Russell and Peter Norvig. 2016. Artificial intelligence: modern approach. Pearson. 15 Michael Saxon, Ari Holtzman, Peter West, William Yang Wang, and Naomi Saphra. 2024. Benchmarks as microscopes: call for model metrology. In Proceedings of the First Conference on Language Modeling. Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, and Arman Cohan. 2024. Tomato: Assessing visual temporal reasoning capabilities in multimodal foundation models. Preprint, arXiv:2410.23266. Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, and Arman Cohan. 2025. TOMATO: Assessing visual temporal reasoning capabilities in multimodal foundation models. In The Thirteenth International Conference on Learning Representations. Daiki Shiono, Ana Brassard, Yukiko Ishizuki, and Jun Suzuki. 2025. Evaluating model alignment with human perception: study on shitsukan in LLMs and LVLMs. In Proceedings of the 31st International Conference on Computational Linguistics, pages 1142811444, Abu Dhabi, UAE. Association for Computational Linguistics. Smith, Hamrick, Adam Sanborn, Battaglia, Gerstenberg, Ullman, and Tenenbaum. 2024. Bayesian models of cognition : reverse engineering the mind. MIT Press, Boston, MA. Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. 2024. Moviechat: From dense token to sparse memory In Proceedings of for long video understanding. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232. Elizabeth Spelke. 2000. Core knowledge. American psychologist, 55(11):1233. David Steinmann, Felix Divo, Maurice Kraus, Antonia Wüst, Lukas Struppek, Felix Friedrich, and Kristian Kersting. 2024. Navigating shortcuts, From spurious correlations, and confounders: origins via detection to mitigation. arXiv preprint arXiv:2412.05152. Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. 2017. corpus of natural language for the 55th visual reasoning. Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 217223, Vancouver, Canada. Association for Computational Linguistics."
        },
        {
            "title": "In Proceedings of",
            "content": "Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. 2019. corpus for reasoning about natural language grounded in In Proceedings of the 57th Annual photographs. the Association for Computational Meeting of Linguistics, pages 64186428, Florence, Italy. Association for Computational Linguistics. Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. 2021a. Habitat 2.0: Training home assistants to rearrange In Advances in Neural Information their habitat. Processing Systems (NeurIPS). Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. 2021b. Habitat 2.0: Training home assistants to rearrange In Advances in Neural Information their habitat. Processing Systems (NeurIPS). Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. 2021c. Habitat 2.0: Training home assistants to rearrange In Advances in Neural Information their habitat. Processing Systems (NeurIPS). Stone Tao, Fanbo Xiang, Arth Shukla, Yuzhe Qin, Xander Hinrichsen, Xiaodi Yuan, Chen Bao, Xinsong Lin, Yulin Liu, Tse kai Chan, Yuan Gao, Xuanlin Li, Tongzhou Mu, Nan Xiao, Arnav Gurha, Zhiao Huang, Roberto Calandra, Rui Chen, Shan Luo, and Hao Su. 2024. Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai. arXiv preprint arXiv:2410.00425. Qwen Team. 2024. Qwen2.5: party of foundation models. Erno Téglás, Edward Vul, Vittorio Girotto, Michel Gonzalez, Joshua Tenenbaum, and Luca Bonatti. 2011. Pure reasoning in 12-month-old infants as probabilistic inference. science, 332(6033):1054 1059. Susan Thompson and Elissa Newport. 2007. Statistical learning of syntax: The role of transitional probability. Language learning and development, 3(1):142. Tongxuan Tian, Haoyang Li, Bo Ai, Xiaodi Yuan, Zhiao Huang, and Hao Su. 2025. Diffusion dynamics models with generative state estimation for cloth manipulation. arXiv preprint arXiv:2503.11999. Edward Tolman. 1948. Cognitive maps in rats and men. Psychological review, 55(4):189. Hsiao-Yu Tung, Mingyu Ding, Zhenfang Chen, Daniel Bear, Chuang Gan, Joshua B. Tenenbaum, Daniel LK Yamins, Judith Fan, and Kevin A. Smith. 2023. Physion++: Evaluating physical scene understanding that requires online inference of different physical properties. Preprint, arXiv:2306.15668. Martijn Van Otterlo and Marco Wiering. 2012. Reinforcement learning and markov decision processes. In Reinforcement learning: State-of-theart, pages 342. Springer. Homer Rich Walke, Kevin Black, Tony Z. Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, Abraham Lee, Kuan Fang, Chelsea Finn, and Sergey Levine. 2023. Bridgedata v2: dataset for robot learning at scale. In 7th Annual Conference on Robot Learning. Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. 2023. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. arXiv preprint arXiv:2311.17918. Ziyue Wang, Chi Chen, Fuwen Luo, Yurui Dong, Yuanchi Zhang, Yuzhuang Xu, Xiaolong Wang, Peng Li, and Yang Liu. 2024. Actiview: Evaluating active perception ability for multimodal large language models. Preprint, arXiv:2410.04659. Michael Weisberg. 2013. Simulation and Similarity: Using Models to Understand the World. Oxford University Press. David Whitney and Allison Yamanashi Leib. 2018. Ensemble perception. Annual review of psychology, 69(1):105129. Susan Wood, Kathleen Moriarty, Beatrice Gardner, and Allen Gardner. 1980. Object permanence in child and chimpanzee. Anim. Learn. Behav., 8(1):3 9. Michael Wooldridge and Nicholas Jennings. 1995. The Intelligent agents: Theory and practice. knowledge engineering review, 10(2):115152. Barlow C. Wright and Jennifer Smailes. 2015. Factors and processes in childrens transitive deductions. Journal of Cognitive Psychology, 27(8):967978. PMID: 26635950. Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. 2024. Star: benchmark for situated reasoning in real-world videos. Preprint, arXiv:2405.09711. Penghao Wu and Saining Xie. 2023. V*: Guided visual search as core mechanism in multimodal llms. Preprint, arXiv:2312.14135. Junbin Xiao, Xindi Shang, Angela Yao, and TatSeng Chua. 2021. Next-qa:next phase of questionanswering to explaining temporal actions. Preprint, arXiv:2105.08276. 16 Fei Xu and Stephanie Denison. 2009. Statistical inference and sensitivity to sampling in 11-month-old infants. Cognition, 112(1):97104. Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. 2024. Thinking in space: How multimodal large language models Preprint, see, arXiv:2412.14171. remember, and recall spaces. Kaiyu Yang, Olga Russakovsky, and Jia Deng. 2019. Spatialsense: An adversarially crowdsourced benchmark for spatial relation recognition. Preprint, arXiv:1908.02660. Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. 2023. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114. Wenqian Ye, Guangtao Zheng, Yunsheng Ma, Xu Cao, Bolin Lai, James Matthew Rehg, and Aidong Zhang. 2024. MM-spubench: Towards better understanding of spurious biases in multimodal LLMs. In Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models. Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B. Clevrer: Collision events Tenenbaum. 2020a. for video representation and reasoning. Preprint, arXiv:1910.01442. Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B. Tenenbaum. 2020b. Clevrer: Collision events for video representation and reasoning. Preprint, arXiv:1910.01442. Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Tianyou Liang, Guanying Chen, Shuguang Cui, and Xiaoguang Han. 2023. Mvimgnet: large-scale dataset of multi-view images. In CVPR. Zheyuan Zhang, Fengyuan Hu, Jayjun Lee, Freda Shi, Parisa Kordjamshidi, Joyce Chai, and Ziqiao Ma. 2025. Do vision-language models represent space and how? evaluating spatial frame of reference under ambiguities. In The Thirteenth International Conference on Learning Representations. Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin. 2023. Vl-checklist: Evaluating pre-trained visionlanguage models with objects, attributes and relations. Preprint, arXiv:2207.00221. Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, DitYan Yeung, and Chuang Gan. 2024. Robodreamer: Learning compositional world models for robot imagination. Preprint, arXiv:2404.12377."
        },
        {
            "title": "A Conceptual Framework Explained",
            "content": "A.1 The Dual-Stage Model In general, world models (WMs) predict the future states of the world (based on observations of the past and current states) and the next action to be taken.2 Formally, WM θ recursively models the transition distribution: Pθ(St S1, a1, . . . , St1, at1). With theoretical consistency with previous research (Knill and Pouget, 2004; Ha and Schmidhuber, 2018b; Smith et al., 2024), we decouple the world modeling process into two In the first stage, an agent encodes stages. environmental stimuli from sensory signals into internal representations of the external world. In the second stage, the agent performs an extrapolation into possible future states. As time progresses and future states come into embodiment, the agent acquires ground truth data and updates its WM based on the divergence between prediction and reality. Unlike model-free RL, where the agent optimizes its action policy to maximize utility, world modeling is primarily supervised learning problem, optimized through error reduction. Given this dual-stage framework, world modeling can fail for two different reasons: Perceptual limitations that lead to problematic representations of external states. For example, an agent may encode different colors identically or conflate object size with speed. Prediction limitations that arise from insufficient mechanistic knowledge and lead to inaccurate simulations. For example, an agent may fail to follow momentum conservation or struggle with complex multi-object collisions. Our framework addresses these challenges by considering both perception competency and prediction competency. We discuss how we further decompose the two stages to make this benchmark empirically possible. A.2 Perception Process Accurate prediction of future states relies on constructing precise representations of the current environment, process known as perception (Goldstein, 1989). Perceptions have multitudinous dimensions: force, motion, temperature, sound, 2We regard the corner case of inaction as an element of the action space magnetics, space, time, quantity, and other agents, to name few (Merleau-Ponty, 2004; Coren et al., 2004). Agents detect physical signals from external stimuli through their physiological sensors, converting multimodal raw inputs into physioelectronic signals. These signals then undergo complex bottom-up processing and functional transformations until they become recognizable to higher cognitive faculties involved in semantics and reasoning. Subsequently, top-down processing refines and hypothesizes representations of the world state. This bidirectional interaction continues iteratively until convergence. Such top-down processing enables more reliable and robust representations of the world. (Rao and Ballard, 1999; Frith and Dolan, 1997; Delorme et al., 2004; Mechelli et al., 2004) For example, animals such as crows, dolphins, and chimpanzees exhibit an understanding of object permanence, allowing them to maintain stable representations of objects even after they become obscured or disappear (Baillargeon et al., 1985; Hoffmann et al., 2011; Wood et al., 1980; Mitroff and Scholl, 2004). Due to the sensory capabilities of current VLMs, we keep spatial, temporal, quantitative, visual, and motion perceptions in our framework only to accommodate their status quo. A.3 Prediction Process To formalize how next-state prediction is processed, we draw on the compositional generalization framework from Dziri et al. (2023) and consider next-state predictions as topologically sorted computational graph. Given world environment, each state can be hierarchically decomposed into structured sequence of nodes, where node intuitively represents an object as vector of attributes (e.g., speed, mass, direction, color). The underlying rationale is as follows: 1. Any valid program can be expressed as an equivalent topologically sorted computational graph based on dependency structures; 2. All world simulators function as programs; 3. Therefore, all world simulators have equivalent graphs. The topological order of the graph is determined by time and world dynamics. Within this framework, future state prediction involves computing values for future nodes based on historical nodes, using the current time step as cutoff. We then identify three necessary and collectively sufficient conditions. 18 Atomic Mechanistic Simulation. Mechanism simulation refers to the process of predicting systems future states via representing and modeling the dynamic interactions among its (Weisberg, 2013; Hegarty, component parts 2004; Barsalou, 2008). This is to be contrasted with statistical predictions, which typically shortcuts and bypass rely on correlational explicit simulations of component behaviors over time (Kendall et al., 1999). Mechanistic knowledge about natural laws and object-specific properties, functions, behavioral patterns, and interactive dynamics are the fuel of mechanistic simulation (Craver et al., 2024), and are mostly learned in posterior manner (Kant et al., 1934). Here we especially emphasize the atomicity of mechanistic simulation, i.e., single object motion or minimally viable object interactions. More complicated world dynamics are left to In terms of the compositional inference part. computational graphs, extrapolation based on atomic mechanistic knowledge corresponds to predicting the immediate next state based on the current observations and intended action. Formally, the inference process can be expressed as predicting St (St St1, at1, . . . , Sth, ath), where denotes the window size of historical states. All of St1 . . . Sth are observed. Transitive Prediction. WM that only predicts the immediate next state is hardly useful for complex planning in long-horizon tasks. Given long hypothetical action sequence generated from any policy, competent WM should accurately predict The the corresponding future state. statistically less biased way is to perform step-by-step extrapolation into distant future states (Prystawski et al., 2023). Known as transitive this ability is exhibited by many inference, including rats, monkeys, intelligent animals, and human infants (Wright and Smailes, 2015; Thompson and Newport, 2007; Mannella and Pezzulo, 2024; Bryant and Trabasso, 1971; McGonigle and Chalmers, 1977). Formally, the inference process can be expressed as: ( (cid:100)St+q, (cid:92)St+q1, . . . , (cid:98)St) Pθ( (cid:100)St+q, (cid:92)St+q1, . . . , (cid:98)St at, . . . , at+q1, at+q, St1, at1, . . . , Sth, ath), and the most natural chain of thoughts (CoT) paradigm can be expressed recursively with: (cid:100)St+i Pθ( (cid:100)St+i at+i, (cid:92)St+i1, at+i1, . . . , (cid:98)St, at, St1, at1, . . . , Sth, ath). All (cid:98)Si are inferred, not observed. In terms of computational graphs, the transitive extrapolation through time takes the form of forward chain. Compositional Prediction. Previous research in cognitive psychology provides strong evidence that humans and intelligent animals can adjust their statistical expectations of outcome distributions when domain-specific mechanisms (e.g., agentic sampling intuitive physics, preferences, procedures) conflict with the base rates of the population (Xu and Denison, 2009; Eckert et al., 2021; Denison et al., 2014; Gweon et al., 2010; Téglás et al., 2011). Extending from sampling processes to general next-state predictions, we identify another advanced inference mechanism: integrating two or more known conflicting or synergistic mechanisms into unified effect. or We provide motivating example to demonstrate what we mean by compositional inference: Consider 2D plane with three balls, A, B, and C, each of equal weight. Observation 1: Ball strikes Ball from the lower left at specific speed and angle, causing to move upper right. Observation 2: Ball strikes Ball from the vertically symmetric lower right at the same speed, causing to move upper left. Now, suppose the agent has never observed scenario where single ball is simultaneously struck by two others. However, with basic physical intuition, the agent should infer that the leftward and rightward motion components cancel each other out, leading to prediction that Ball will move vertically upward. Formally expressing compositional inference under the standard Markov Decision Process will be tricky (Van Otterlo and Wiering, 2012), because the monolithic state denotation in the MDP formalism fails to capture crucial fact that complex states can be decomposed into multiple atomic states. To bridge the gap, we define conceptual-level composition function (Marr, 2010) St = Compose(cid:2)S(1) (cid:3) to denote the whole relationship between component states and complex states. Then compositional inference can , . . . S(n) be expressed as: St Compose(cid:2)S(1) , . . . S(n) ) Pθ(S(i) t1, at1, . . . S(i) (cid:3). S(i) where S(i) th, ath The compositional extrapolation through time takes the form of collider with two or more parent node. A.4 Final Remarks In this work and the conceptual framework section, we adopt specific interpretation of world model, which we refer to as mechanistic world model. For example, statistical model (e.g., multi-linear regression, XGBoost) used to predict clients risk of loan default based on features like yearly income, number of children, ethnicity, and medical history is undeniably form of future-state prediction. However, such model does not faithfully simulate world dynamics, as it lacks any representation of temporal progression. While it may provide useful predictive insights, it does not qualify as mechanistic model because it blackboxes causal mechanisms and interactive kinetic dynamics. Thus, it is important to recognize that mechanistic simulation is not the only approach to extrapolating future states."
        },
        {
            "title": "B Benchmark Details and Comparisons",
            "content": "B.1 Benchmark Statistics Table 5 provides structured overview of our benchmark tasks across different perceptual and predictive dimensions. B.1.1 Perception Spatial Perception 1. Spatial Relation (SR): Given the front and top view of two objects on table, let the model infer the relative position of one object with respect to the other. This task evaluates whether the model can accurately discern spatial relationships based on visual cues. 2. Spatial Vacancy (SE-V): Given an object and structure with hollow space in the middle, let the model infer whether the object can fit into the hollow space. Thus testing the models understanding of spatial constraints. 3. Spatial Occupancy (SE-O): Given two objects with different sizes, let the model infer which object Thus testing the models understanding of object size. is larger. 4. Spatial Positioning (SP): Given the front and side view of set of object arrangements, let the model infer the top view of the objects. These multi-view tasks emphasize the models ability to synthesize distinct viewpoints into coherent three-dimensional representation of object arrangements. Temporal Perception 1. Temporal positioning (TP): Compare two episodes of object motions from different views, and let the model differentiate which motion started first. 2. Temporal extension (TE): Compare two episodes of object motions from different perspectives, and let the model differentiate which motion lasts longer. Collectively, these tasks investigate the aptitude of model temporal representations, estimate durations, and infer the correct order of events. to maintain consistent Visual Perception 1. Color (V-C): Differentiating whether two objects have the same color, or identifying which color the object is. 2. Shape (V-S): Determine the objects shape; another task involves differentiating whether two objects have the same shape. 3. Material (V-M): Differentiating whether the two objects have the same material or the model determine the material of the given object. By isolating these fundamental visual features, the tasks provide targeted evaluations of how effectively model can parse and differentiate basic object attributes, separate from any contextual or motion-based confounding factors. Motion Perception 1. Motion Identification (MI): Given an episodes of motions of an object and set of static objects, let the model decide which object is moving. This setup tests the models capacity to identify the moving action of objects. 2. Motion Speed (MS): Given episodes of motions let the model decide which of two objects, object moves faster. This setup tests the models capacity to track position changes over time and estimate relative velocity. 3. Motion Direction (MD): Given one moving object and set of static objects, let the model determine which static object the moving object is heading towards. This setup tests the models 20 capacity to track position changes over time and estimate relative moving direction. 4. Motion Trajectory (MT): Given an episode of motions of two objects, let the model decide the moving trajectory of specified object, assessing its aptitude for higher-level spatiotemporal pattern recognition and object-specific path tracking across multiple frames. Quantitative Perception 1. Discrete Quantity (DQ): Given the top view of objects on the table, let the model count the objects. This setup evaluates the model capacity for discrete numerical estimation. 2. Continuous Quantity (CQ): Given the top view of two objects with the same color theme, let the model determine which object has darker shade. 3. Relative Quantity (RQ): Given the top view of objects with different colors, determine which color group has more objects. This setup probes counting skills, numerical reasoning, and perceptual comparisons in visual context. B.1.2 Prediction Mechanistic Knowledge 1. Intuitive Physics (M-IP): Given sequence of images showing consecutive states of the environment in which two objects move towards each other, let the model choose the most probable prediction of the next state. This setup evaluates the model capacity in physical reasoning. 2. Agent Navigation (M-Nav): Given an image of the start state, let the model choose what is most likely to be the final state after the robot/vehicle moves in certain direction. This setup evaluates the model capacity in predictive reasoning. 3. Agent Manipulation (M-Man): Given an image of the start state, let the model choose what is most likely to be the final state after the robot arm does certain movements toward the object. This setup evaluates the model capacity in predictive reasoning for robot manipulation. Transitivity 1. Agent Navigation (T-Nav): Given an image of the start state, let the model choose what is most likely to be the final state after the robot/vehicle moves through multiple directions in sequence. This setup evaluates the model capacity in predictive reasoning for autonomous navigation. 2. Agent Manipulation (T-Man): Given an image of the start state, let the model choose what is most likely to be the final state after the robot arm performs two actions in sequence. This setup evaluates the model capacity in multi-step predictive reasoning for robotic manipulation. Compositionality 1. Multi-Object Intuitive Physics (C-IP): Given several images of two balls colliding with third object at the same time, let the model predict the state after the collision occurred. This task evaluates the models ability to perform compositional inferences about physical causality and object behavior. 2. Multi-Agent Manipulation (C-Man): Given an image of the start state, let the model choose what is most likely to be the final state after two robot arms do certain actions on one object simultaneously. This setup evaluates the model capacity in concurrent action predictive reasoning for robotic manipulation. B.2 Relevant Benchmarks We summarize and compare WM-ABench to existing benchmarks in Table 6, and provide more descriptive details below. social functions, interactions, agent-object Visual semantics perception benchmark. This line of work primarily tests multimodal models static (image) and dynamic (video) recognition competency of object class, object existence, object components, object properties, postures, actions, activities, image relations, emotions, quality, image style, scene, OCR, layouts. Such competency requires models to have diverse schematic knowledge and common sense about the anthropocentric world, and strong pattern recognition capability (i.e., recognize unobserved instances of known categories). However, the foundational perceptual and cognitive functions as enumerated in our work are out of coverage. Representative works include MMBench(Liu et al., 2024a), MvBench (Li et al., 2024c), VSR(Liu et al., 2023), VQA v2 (Goyal et al., 2018), UOUO (Pi et al., 2024), MME (Fu et al., 2024a), STAR (Wu et al., 2024), Perception Test (Patraucean et al., 2023), MSRVTT-QA (Chen et al., 2022), NExT-QA (Xiao et al., 2021). in this line Visual perception Benchmark. This line of work primarily focuses on testing models 21 Dimension Spatial Perception Temporal Perception Visual Perception Motion Perception"
        },
        {
            "title": "Quantitative Perception",
            "content": "Subdimension Spatial Relation (SR) Spatial Vacancy (SE-V) Spatial Occupancy (SE-O) Spatial Positioning (SP) Temporal Positioning (TP) Temporal Extension (TE) Color (V-C) Shape (V-S) Material (V-M) Motion Identification (MI) Motion Speed (MS) Motion Direction (MD) Motion Trajectory (MT) Discrete Quantity (DQ) Continuous Quantity (CQ) Relative Quantity (RQ) Intuitive Physics (M-IP) Mechanistic Knowledge (Prediction) Agent Navigation (M-Nav) Agent Manipulation (M-Man) Agent Navigation (T-Nav) Agent Manipulation (T-Man) Transitivity (Prediction) Compositionality (Prediction) Total Cases 1600 (ManiSkill) 8640 (ThreeDWorld) 2880 (ManiSkill) 324 (ThreeDWorld) 1152 (ManiSkill) 941 (ThreeDWorld) 6400 (ManiSkill) 2000 (ThreeDWorld) 2000 (ManiSkill) 5646 (ThreeDWorld) 2000 (ManiSkill) 7680 (ThreeDWorld) 648 (ManiSkill) 1440 (ThreeDWorld) 1000 (ThreeDWorld) 1728 (ThreeDWorld) 2688 (ManiSkill) 1913 (ThreeDWorld) 2304 (ManiSkill) 2304 (ThreeDWorld) 2688 (ManiSkill) 1913 (ThreeDWorld) 6912 (ManiSkill) 671 (ThreeDWorld) 2520 (ManiSkill) 1134 (ThreeDWorld) 756 (ThreeDWorld) 2880 (ManiSkill) 1701 (ThreeDWorld) 2160 (Physion: Slide) 3456 (Physion: Drop) 1728(Physion: Collide) 1575 (Carla) 1728 (Habitat-Lab) 2304 (ManiSkill: Drop) 2304 (ManiSkill: Lift) 2304 (ManiSkill: Push) 700 (Carla) 1956 (Habitat-lab) # Images per Case 2 (ManiSkill) 2 (ThreeDWorld) 3 (ManiSkill) 1 (ThreeDWorld) 1 (ManiSkill) 1 (ThreeDWorld) 2 (ManiSkill) 2 (ThreeDWorld) 5 (ManiSkill) 6 (ThreeDWorld) 5 (ManiSkill) 6 (ThreeDWorld) 3 (ManiSkill) 3 (ThreeDWorld) 3 (ThreeDWorld) 1 (ThreeDWorld) 6 (ManiSkill) 4 (ThreeDWorld) 4 (ManiSkill) 8 (ThreeDWorld) 6 (ManiSkill) 6 (ThreeDWorld) 7 (ManiSkill) 8 (ThreeDWorld) 1 (ManiSkill) 1 (ThreeDWorld) 1 (ThreeDWorld) 1 (ManiSkill) 1 (ThreeDWorld) 3 (Physion: Slide) 3 (Physion: Drop) 3 (Physion: Collide) 1 (Carla) 1 (Habitat-Lab) 1 (ManiSkill: Drop) 1 (ManiSkill: Lift) 1 (ManiSkill: Push) 1 (Carla) 1 (Habitat-lab) 1728 (ManiSkill: Push&Pick) 576 (Maniskill: Pick&Rotate) 1 (ManiSkill: Push&Pick Pick&Rotate) Multi-Object Intuitive Physics (C-IP) 1296 (ThreeDWorld) Multi-Agent Manipulation (C-Man) 2304 (ManiSkill: Push) 2304 (ManiSkill: Lift) 3 (ThreeDWorld) 1 (ManiSkill: Push) 1 (ManiSkill: Lift) Table 5: Benchmark Task Breakdown of texture), line segments, competency semantic-scarce low-level, visual perceptions, such as elementary visual attributes recognition (e.g. color, material, shape, size, lighting, optical flow, insection, segmentation, overlapping area, corresponding points across different perspectives. CLEVR (Johnson et al., 2016), BLINK (Fu et al., 2024b), BlindTest (Rahmanzadehgervi et al., 2024), V* (Wu and Xie, 2023), KITTI (Geiger et al., 2012), ActiView (Wang et al., 2024), Shitsukan-eval (Shiono et al., 2025). and motion Spatiotemporal perception Benchmark. This spatial perception tasks focus on evaluating models ability to understand spatial relationships, configurations, and arrangements Spatial within static and dynamic contexts. perception tasks require recognizing geometric and topological relationships between objects, including proximity, alignment, containment, positions, intersection, orientation, and distances, as well as multiperspective alignment and integration. Rel3D adjacency, relative"
        },
        {
            "title": "Mechanistic Transitivity Compositionality",
            "content": "SP MV SO SE TP TE DQ CQ RQ AR-S AR-M AR-C MD DI SC MT IP Nav Mani Nav Mani IP"
        },
        {
            "title": "Mani",
            "content": "BLINK (2024b) VALSE (2022) CLEVR (2016) NLVR (2017) NLVR2 (2019) SpatialRGPT (2024) VSI-Bench (2024) VL-CheckList (2023) CLEVRER (2020a) ShapeWorld (2017) MVBench (2024d) MMBench (2024b) MME (2024a) VQA(v2) (2018) NExT-QA (2021) ActiView (2024) SEED-Bench (2023) Perception Test (2023) BlindTest (2024) SHAPES (2016) TOMATO (2024) STAR (2024) V* (2023)"
        },
        {
            "title": "Ours",
            "content": "Table 6: Comparison between different benchmarks. (Goyal et al., 2020), SPARTQA (Mirzaee et al., 2021), SpatialSense (Yang et al., 2019), 3D-ShapeTest (Eppel, 2024), DriveMLLM (Guo et al., 2024), VSI-Bench (Yang et al., 2024). Temporal and motion perception go handSince perception of time typically in-hand. relies on changes and motions, temporal and motion perception tasks lack clear boundaries. Temporal perceptions typically involve: action count, attribute change, action sequence and procedure understanding, event order, scene transition, character order, and action antonym. Motion perception typically involves: direction (e.g. left, clockwise, up, outward), speed, and trajectory. Representative works include: TOMATO (Shangguan et al., 2024), CATER (Girdhar and Ramanan, 2020). Next state prediction benchmark. Next-state prediction is different from the perception, visual-semantic inference (typically about static, state-intrinsic specifications, such as categories, properties, functions, relations, emotions, and intentions), and verbal-logical reasoning (e.g. comparison, logic operations) tasks introduced above. Emphasizing extrapolating objective world states, next-state prediction requires grounded knowledge (in contrast with verbal knowledge) of world mechanics and dynamics. That is, how the environment changes and transits. The agentcentric next-state prediction would additionally emphasize interactive dynamics. Representative works includes: Physion action-transition and (Bear et al., 2022), Physion++ (Tung et al., 2023), Phyre (Bakhtin et al., 2019), CLEVRER (Yi et al., 2020b), IntPhys (Riochet et al., 2020), CoPhy (Baradel et al., 2020), CRIPP-VQA(Patel et al., 2022), SEED-Bench (Li et al., 2023)."
        },
        {
            "title": "C Simulator Setup",
            "content": "We describe how we set up each simulator for generating test cases for the proposed world model benchmark. C.1 ThreeDWorld We use the ThreeDWorld simulator (Gan et al., 2021) to generate images for the perception tasks, with intuitive physics (e.g., collisions) modeled using the Physion framework (Bear et al., 2022). We first select curated set of pre-packaged scenes, objects, and materials from ThreeDWorld. For most questions, we spawn selected objects like cubes and spheres onto the floor or table within one of the selected scenes, then randomly varying their color, size, position, and material to ensure diversity across samples. We then render images of the scene from multiple viewing angles, including top-down, front, and side views, to ensure diverse perspectives for question generation. For tasks requiring sequence of images to demonstrate object movement, we first render an initial image capturing the objects in their original spawned positions. We then teleport the objects to new locations, rendering an image after each movement until the desired movement is complete. After generating all images for given scene, we clear the 23 scene by removing all objects. We then repeat the process in newly selected scene, beginning with the random selection of object color, size, position, and material to ensure diversity across scenes. C.2 ManiSkill We use both ManiSkill framework version 2 (Gu et al., 2023) and 3 (Tao et al., 2024)) and we do the following to generate the images used in the dataset. For most questions, we spawn objects such as cubes and spheres onto table in the ReplicaCAD apartment scene (Szot et al., 2021b) and render an image of the scene. For tasks requiring before-and-after images, we first render the scene, then teleport objects to new locations, and render again. For static tasks, following similar approach to ThreeDWorld, we place several cubes and spheres on table within the provided background. To ensure sample diversity, these objects vary randomly in color, size, and position. We then render images from multiple viewpoints (e.g., top-down, front, and side angles) to provide diverse perspectives for subsequent question generation For tasks requiring sequence of images to capture object movements (similar to ThreeDWorld), we first render an initial image depicting the objects in their original positions. We then relocate the objects, rendering an image after each repositioning, until the intended motion is complete. For agent manipulation questions, we use the RoboCasa dataset (Nasiriany et al., 2024). One scene features two Franka Panda arm robots, while another includes only one. Using ManiSkill, we randomize several dimensions in our test cases, including object geometry, object color, apartment layout, and visual style. Teleoperation tools collect demonstrations of both successful and failed trajectories for each object geometry, with physics simulation enabled. Other randomized dimensions are synthetically generated. For the question-answer pairs, we render the first and last frames of each demonstration C.3 Physion We use various setups (e.g. collide, slide, drop) provided by Physion (Bear et al., 2022) and instantiate various objects and run the physics simulation to simulate various physical effects such as dropping or rolling, testing mechanistic state transition knowledge. An image is captured before the simulation starts and frames are iteratively being captured after simulation has run. C.4 Carla Using Carla (Dosovitskiy et al., 2017b) simulator we do the following to generate the images used in the dataset. We first select curated set of pre-packaged towns, weather, and car agents from Carla. For the majority questions we spawn car agent at random position onto one of the selected scenes, instantiated with random weather from the selected weathers. We refined the control mechanisms of the car agent to enhance realism, ensuring that actions such as moving forward and turning exhibit natural and physically plausible behavior that aligns with their corresponding natural language descriptions. We then provide commands instructing the car to move in specified direction or make turn at an intersection. The simulator subsequently renders and captures sequence of images depicting the cars actions, which are used to construct our dataset. C.5 Habitat We use Habitat 2 (Szot et al., 2021c) to render the HSSD (Khanna et al., 2023) dataset, which includes large number of simulated indoor scenes, to create navigational transition action-state pairs. We use discrete actions that enable the agent in the simulation to move around and change viewing directions. Pre-condition and post-condition images are generated for the dataset."
        },
        {
            "title": "D Addendum to Results",
            "content": "D.1 Entanglement in Perception Tasks (s-RE) scores. Figure 4 provides heatmap showing Relative Entanglement These scores represent the average performance deviation (s-RE) across subset of the highest-performing models (GPT-4o, Gemini-1.5 Pro, Qwen2-VL, Qwen2.5-VL, and InternVL-2.5). D.2 Evaluation via Perceptual Queries We devised three targeted perceptual queries to rigorously assess the models understanding of dynamic scene attributes: Does the scene contain moving object? What is the observed color of the moving object? What is the observed shape of the moving object? These queries are intended to verify that the models accurately detect and interpret the moving objecta critical prerequisite for successful task performance. Notably, this stringent evaluation protocol resulted in the exclusion of approximately Figure 4: Heatmap showing s-RE scores that quantify the relative impact of different physical dimensions on perception task performance. Higher values (darker red) indicate stronger entanglement between physical dimension and task performance, while lower values (lighter colors) suggest weaker relationships. 30% of the test instances, thereby underscoring the robustness and effectiveness of our filtering approach."
        },
        {
            "title": "E Evaluation and Reproducibility",
            "content": "E.1 Human Evaluation We recruited Mechanical Turk Masters on Amazon Mechanical Turk. Annotators were required to have 98% HIT approval rate, at least 100 approved HITs, and reside in the United States. Each problem was evaluated by three annotators, with the final label determined by majority vote (ties were resolved by randomly selecting an answer). Workers were paid $1 per HIT (10 examples per HIT, where each example took about 20 to 30 seconds). For each task, we provide brief task instructions. Here is an example: You will be provided with six images, each representing evenly spaced frames from video. Two moving objects are visible in the frames. Your task is to determine which object started moving first, $object_name1 or $object_name2? E.2 VLM Evaluation This allows seamless retrieval of the corresponding To datasets for varied evaluation scenarios. streamline prompt creation and ensure consistency, we maintain prompt template files containing different formats for wide range of question types. Below is the system prompt that we use to regularize the output format from the models:"
        },
        {
            "title": "If it",
            "content": "You are helpful assistant. You will be given question to answer. is multichoice question, return the index of your choice 1,2,3,4 or A,B,C,D depending on the question, and then followed by any explanation necessary. is yes/no question, clearly answer yes or no at first, and then follow with your explanation If you are asked to choose the if needed. images, please note that the last four images of all the given images are your choices. And your answer should be 1 or 2 or 3 or 4 pointing to these last four images."
        },
        {
            "title": "If it",
            "content": "E.3 Computational Resource We use H100 GPUs to run the experiments, with an estimated total runtime of 200 GPU hours for model inference. We develop general prompt framework to support both open-source and closed-source models under unified design. The system is built around general evaluator class capable of loading and executing multiple model types. Our data management strategy involves categorizing datasets and assigning each task unique identifier. E.4 License and Research Artifacts Simulators. Regarding the licenses or terms for the use and distribution of artifacts, our paper utilizes the following simulation environments: ThreeDWorld (Gan et al., 2021), ManiSkill (Tao et al., 2024), Physion (Bear et al., 2022), Carla (Dosovitskiy et al., 2017b), and Habitat 2 (Szot et al., 2021c). Detailed documentation on these artifacts is provided in Appendix C. Their corresponding licenses are listed in table 7."
        },
        {
            "title": "URL License",
            "content": "ThreeDWorld (TDW) Link BSD-2-Clause ManiSkill Physion Carla Habitat 2 Link Apache v2.0 Link MIT license Link MIT license Link MIT license Table 7: License information for the simulators used. Having reviewed the rights and terms of these licenses, we confirm that we have fully complied with their requirements. Our work will be released under the MIT License, ensuring no legal issues arise. As discussed in the ethics statement of section 7, our benchmark is designed to provide fundamental evaluation of the core world modeling abilities in VLMs, which generally do not involve social aspects or social reasoning. We stipulate that our work should be used strictly for academic purposes. Since our data is collected from simulators, it is fully anonymized, does not contain personally identifiable information, and does not require additional measures to verify the absence of sensitive information relevant to individuals."
        }
    ],
    "affiliations": [
        "Maitrix.org",
        "UC San Diego"
    ]
}