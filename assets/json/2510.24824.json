{
    "paper_title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
    "authors": [
        "Bohong Wu",
        "Mengzhao Chen",
        "Xiang Luo",
        "Shen Yan",
        "Qifan Yu",
        "Fan Xia",
        "Tianqi Zhang",
        "Hongrui Zhan",
        "Zheng Zhong",
        "Xun Zhou",
        "Siyuan Qiao",
        "Xingyan Bin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are powerful but often too slow and costly for real-world use during inference. Looped transformers save on parameters by reusing the same weights for multiple computational steps, or \"loops.\" However, this approach has a major flaw: the loops run one after another, causing inference latency and memory requirements to increase with each added loop. This makes them impractical for fast applications. To solve this problem, we introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that delivers the performance benefits of a deep, looped model but with the low latency of a standard, non-looped model. PLT works using two key techniques. First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by computing different loops for different tokens at the same time, all within a single pass. Second, to prevent memory costs from growing, we use an Efficient Representation Enhancement strategy. This method shares the memory (KV cache) from the first loop with all other loops. It then uses a Gated Sliding-Window Attention (G-SWA) to combine this shared global information with local information, maintaining high accuracy. Our experiments show that PLT achieves the high accuracy of a traditional looped model but with almost no extra latency or memory cost compared to a standard transformer."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 4 2 8 4 2 . 0 1 5 2 : r Parallel Loop Transformer for Efficient Test-Time Computation Scaling"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are powerful but often too slow and costly for real-world use during inference. Looped transformers save on parameters by reusing the same weights for multiple computational steps, or \"loops.\" However, this approach has major flaw: the loops run one after another, causing inference latency and memory requirements to increase with each added loop. This makes them impractical for fast applications. To solve this problem, we introduce the Parallel Loop Transformer (PLT). PLT is new architecture that delivers the performance benefits of deep, looped model but with the low latency of standard, non-looped model. PLT works using two key techniques. First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by computing different loops for different tokens at the same time, all within single pass. Second, to prevent memory costs from growing, we use an Efficient Representation Enhancement strategy. This method shares the memory (KV cache) from the first loop with all other loops. It then uses Gated Sliding-Window Attention (G-SWA) to combine this shared global information with local information, maintaining high accuracy. Our experiments show that PLT achieves the high accuracy of traditional looped model but with almost no extra latency or memory cost compared to standard transformer. Date: October 30, 2025 Correspondence: Bohong Wu at bohongwu@bytedance.com, Xingyan Bin at binxingyan@bytedance.com"
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across wide range of tasks [21, 26, 28, 29, 38, 39], yet their practical deployment is often constrained by substantial computational costs during inference [4, 5, 24, 45, 49]. As models scale, the latency and memory bandwidth required for token generation become significant bottlenecks. This challenge has spurred research into architectures that can achieve high performance while maintaining inference efficiency. One promising direction is the use of looped transformers, such as the Universal Transformer [11], which reuses the same set of parameters across multiple computational steps or \"loops.\" This weight-sharing mechanism makes them highly parameter-efficient, enabling them to achieve greater effective depth and stronger performance on complex reasoning tasks without increasing the models storage footprint [13, 16, 30, 32, 35, 37, 43, 46]. However, this parameter efficiency comes at severe cost: in their vanilla implementation, the loops are executed in strictly sequential manner as shown in Figure 1a. Such sequential dependency means that per-token compute, wall-clock latency, and KV-cache size all scale linearly, O(L), with the number of loops (L). This scaling bottleneck renders vanilla looped transformers impractical for latency-sensitive 1 (a) Inference pipeline in vanilla loop transformer (b) Inference pipeline in proposed PLT . Figure 1 Illustration of the computation flow. (a) Vanilla loop transformer, where each loop in each token should be computed in serial manner. (b) Parallel loop transformer (PLT), where transformer loops within the same blue dashed box can be computed in parallel. applications, effectively negating their primary advantages. To resolve this tension between effective depth and inference speed, we introduce the Parallel Loop Transformer (PLT). PLT is novel architecture designed to unlock the performance benefits of deep, looped computation while maintaining the approximate inference latency of standard, non-looped transformer. As shown in Figure 1b, the core principle of PLT is to break the sequential loop dependency by parallelizing the computation of different loops across different tokens. Our approach consists of two key components. First, we introduce Cross-Loop Parallelism (CLP), technique that reformulates the training and inference pipelines. During decoding, CLP executes the l-th loop for the current token ti concurrently with the (l + 1)-th loop for the previous token ti1 and so on, all within single forward pass. This overlapping computation effectively collapses the sequential steps into one, decoupling the models effective depth from its wall-clock latency. Second, to address the O(L) growth in KV cache, we propose an Efficient Representation Enhancement strategy. This strategy shares the KV cache from the first loop with all subsequent loops, reducing the KV cache memory footprint back to the level of Loop times as 1. To preserve high accuracy, we augment this global shared representation with local context mechanism, using gated sliding-window attention (SWA) in non-first loops. Our contributions are as follows: We propose the Parallel Loop Transformer (PLT), an architecture that, to our knowledge, is the first to successfully parallelize the computation of looped transformers to achieve scalable test-time computation with negligible latency overhead. We introduce Cross-Loop Parallelism (CLP) and an Efficient Representation Enhancement technique (KV-cache sharing with gated SWA) to overcome the critical latency and memory bottlenecks of traditional looped models. We demonstrate through extensive experiments on both in-house and open-source models that PLT significantly outperforms vanilla transformer baselines in accuracy while adding minimal latency. We show that PLT is far more efficient than vanilla looped transformers, and can even enable shallower, more efficient PLT model (e.g., 1.7B activated parameters) to achieve superior performance and lower latency than much larger vanilla model (e.g., 2.5B activated parameters)."
        },
        {
            "title": "2 Method",
            "content": "2 Figure 2 Training and inference pipeline of PLT with loop count L=3. Training (Left): Same Colored boxes trace how input tokens traverse the loops to predict their targets (e.g., token T1 passes three loops to predict T4, consistent with Figure 1b). Training is parallel along the token dimension and serial along the loop dimension. Inference (Right): Parallelized forward pass of PLT when decoding T4 and T5 in Loop Transformer with L=3. Because there are no horizontal (same-step, cross-loop) activation dependencies during training, computations within the same step (each row; see the blue dashed box) run in parallel during decoding."
        },
        {
            "title": "2.1 Preliminaries: Vanilla Loop Transformer\nWe consider a vanilla Loop Transformer [11] with L loops. Let T = (t1, t2, . . . , tn) be a token sequence,\nE = (e1, e2, . . . , en) is the token sequence after embedding. For token index i ∈ {1, . . . , n} and loop index\nl ∈ {1, . . . , L}, let h(l)\nthe initial state.\ni\nThe computation flow for i-th token in vanilla loop transformer is as follow:",
            "content": "denote the hidden state at position after forward loops, with h(0) h(0) = ti, = (l)(cid:16) h(l) h(l1) (cid:17) , = 1, . . . , L, (1) where (l) denotes the l-th forward loop. Finally, h(L) token. feeds into the classifier head to predict the (i + 1)-th In vanilla loop transformer, loops run strictly sequentially, so Challenge in vanilla loop transformer. per-token compute, wall-clock latency, and KV-cache size scale as O(L) with the number of loops as shown in Table 1. While weight sharing makes it parameter-efficient (it achieves greater effective depth with fewer stored weights), it does not reduce latency. Therefore, it mainly helps under equal-parameter comparisons; under equal-latency budgetstypical in practical inferencethe vanilla loop transformer offers no inherent advantage and can be worse due to longer decode paths, higher memory-bandwidth pressure, and larger KV caches."
        },
        {
            "title": "2.2 Parallel Loop Transformer\nMotivation. Vanilla loop transformers are parameter-efficient but rely on strictly serial computation per token,\nwhich increases the forward times and the KV-cache footprint. We aim to keep the parameter savings while\nshortening the forward times by enabling parallelism across loops and tokens without breaking causality.",
            "content": "As shown in Figure 1b, we convert the vanilla sequential loop transformer into parallel loop transformer that executes loops in parallel. The design has two key components: (i) cross-loop parallelism (Sec. 2.2.1) for parallel loop inference; (ii) efficient representation enhancement (Sec. 2.2.2) to improve accuracy with light time and KV cache overhead."
        },
        {
            "title": "2.2.1 Cross-Loop Parallelism",
            "content": "3 Algorithm 1 Decoding of PLT with loop times as 3 Require: Loop times = 3 Transformer block function ; Input sequence = (t1, . . . , tn); Maximum new tokens . n2 (T ) , h2 , h3 k2) 1: Kshare, Vshare, h1 2: logits ClassifierHead(h3 3: tk+1 argmax(logits) 4: for = + 1 to + 1 do ei = Embedding(ti) 5: B0, B1, B2 ei, ei + h1 6: i1, h3 Kshare, Vshare, h1 7: logits ClassifierHead(h3 8: ti+1 argmax(logits) 9: 10: end for , h2 i1, ei + h2 i2 (B, Kshare, Vshare) i2) Prefilling Predict the next token Algorithm 2 Training for PLT Require: Loop count Transformer block function Classification head ClassifierHead() Input token sequence = (t1, . . . , tn) 1: Embedding(T ) 2: Kshare, Vshare, H(1) (E) 3: for = 2 to do 4: 5: H(i1) concat(0, H(i1)[: 1]) + H(i1) H(i) (B; Kshare, Vshare) 6: 7: end for 8: logits ClassifierHead(cid:0)H(L)(cid:1) 9: loss = CrossEntropy(logits, ) 10: return loss Algorithm 3 Efficient representation enhancement for non-first loop in PLT Require: Input hidden state QKV linear layer fqkv, Output linear layer fo Gate linear layer fgate shared Key-Value cache from first Loop (Kshare, Vshare) window size of sliding window attention (SWA) 1: Q, K, = fqkv(H) 2: yglobal = Attn(Q, Kshare, Vshare) 3: ylocal = SWA(Q, K, V, w) 4: = Sigmoid(cid:0)fgate(Q)) 5: = ylocal + (1g) yglobal. 6: = fo(y) 7: return We introduce Cross-Loop Parallelism (CLP): overlapping later-loop computation on earlier tokens with earlier-loop computation on later tokens. As shown in Figure 1b, taking decoding token t4 with 3 loops as example, first loop on token t3, second loop on token t2 and third loop on token t1 are executed simultaneously, and similar cross-loop parallelism are executed by the following tokens. The detailed inference and training pipeline of this method are proposed as follows: Training. The training flow of PLT is illustrated in Figure 2 and Algorithm 2. In the first loop, PLT matches vanilla Transformer: it feeds the token embeddings H(0) = = (e1, e2, . . . , en) into the model and obtains the last-layer hidden states H(1) = (h(1) ). Before the second loop, PLT shifts H(1) to the right by one position, from (h(1) n1), and then adds back the original embeddings: = + shift(H(1)). This shift removes direct dependence between the states with the same index across consecutive loops, which enables parallel processing during decoding; we refer to this as cross-loop parallelism. PLT repeats this process for = 2, . . . , L: at each loop, it right-shifts the previous loops states by one position, adds the embeddings, and applies the transformer while reusing the shared Kshare and Vshare from the first loop. 1 , h(1) ) to (0, h(1) 1 , . . . , h(1) 1 , . . . , h(1) 2 , . . . , h(1) 4 Table 1 Complexity comparison of the vanilla Transformer, looped Transformer, and the proposed PLT. The vanilla Transformer serves as the baseline with parameter count , per-token compute cost C, and single-token decoding latency under memory-bound setting. The attention KV cache scales as O(nd), where is the sequence length and is the embedding dimension. Here, denotes the loop count and the sliding-window size used by SWA (sliding-window attention)."
        },
        {
            "title": "Loop Times",
            "content": "Param. Compute."
        },
        {
            "title": "Decoding latency",
            "content": "(1)Vanilla transformer (2)Vanilla loop transformer (3)Loop+CLP (4)Loop+CLP +KV share (5)Loop+CLP +KV share+G-SWA"
        },
        {
            "title": "C\nLC\nLC\nLC\nLC",
            "content": "O(nd) O(Lnd) O(Lnd) O(nd) O(nd + (L 1)wd) Lt Inference. As shown in Figure 2 (right), cross-loop parallelism in PLT processes tokens with single forward pass. This parallel design leverages the memory-bound nature [44] of LLM decoding: adding parallel test-time computation FLOPs improves accuracy, while the extra decoding latency is negligible. Algorithm 1 illustrates the decoding flow of PLT for L=3. At decoding step i, we construct displaced micro-batch B={B0, B1, B2}, where B0 is the first loop of token i, B1 is the second loop of token i1, and B2 is the third loop of token i2. We then predict token (i+1) from the final loop state h3 . Compared with vanilla loop transformer (which applies sequential passes per token), PLT performs the same logical compute with one parallel pass per token. Compared with standard non-loop decoder, it adds FLOPs that improve accuracy, yet the latency increase is negligible because decoding is memory-bound [44]. i"
        },
        {
            "title": "2.2.2 Efficient Representation Enhancement",
            "content": "Cross-loop parallelism addresses the inference-time scaling with loop count in vanilla loop transformer, but it still incurs an L-fold KV-cache memory cost, which limits long-context use. We introduce efficient representation enhancement with two components: (i) first-loop KV-cache sharing to provide single global representation, and (ii) gated sliding-window attention to strengthen local context. Details follow. KV-cache sharing from the first loop. In standard loop design, each loop maintains its own KV cache, so memory grows linearly with L. To reduce KV memory, we share the first loops KV cache with all later loops. As shown in Algorithm 3, non-first loops keep their private queries but perform global attention on Kshare and Vshare from the first loop. Thus, only one global KV cache needs to be stored. This design preserves global information for non-first loops and removes the L-dependent KV-cache growth. Gated sliding-window attention (G-SWA) in non-first loops. To further enhance local information on top of the shared global representation, non-first loops apply sliding-window attention over their private Q, K, and with window size w. In our experiments, we set w=64, and it does not increase with the overall sequence length. As shown in Lines 35 of Algorithm 3, we then fuse the outputs of SWA (local) ylocal and full attention on the shared KV (global) yglobal using sigmoid gate: Sigmoid(cid:0)fgate(Q)(cid:1), ylocal + (1g) yglobal. (2) The gate linear layer fgate is head-wise with scalar output per head, so the added parameters and computation are negligible. In addition, due to SWA, non-first loops only cache the most recent KV entries during decoding. Overall, gated sliding-window attention makes the proposed PLT more KV-cache efficient than vanilla loop transformer, since non-first loops store only recent KV entries, while accuracy is maintained by combining both global and local information."
        },
        {
            "title": "2.3 Inference Efficiency Analysis",
            "content": "As shown in Table 1, we analyze per-token inference in memory-bound setting. The vanilla Transformer (baseline) has parameter count , per-token compute C, KV cache O(nd), and decoding latency t. Introducing loops increases compute to LC and, if each loop keeps its own cache, increases memory to O(Lnd); because the loops run sequentially, latency grows to Lt. With the proposed cross-loop parallelism, the loop iterations 5 Table 2 Performance evaluation of in-house Seed-MoE and PLT with the same number of activated parameters. Lat. and KV cache represents decoding latency (ms) and KV cache memory overhead under batch size as 4. MMLU CEval M.PRO AGI. BBH DROP GSM. H.Eval MBPP TQA Avg. (1)Seed-MoE 680M/13B (2)+loop- (3)+loop-2 +CLP (4)+loop-2 +CLP+KV share (5)+loop-2 +CLP+KV share+G-SWA(PLT -2) (6)+loop-3 +CLP+KV share+G-SWA(PLT -3) 54.0 59. 59.7 55.7 59.6 62.5 53.0 61. 60.6 57.2 58.9 61.4 22.5 26. 28.6 22.1 27.0 27.1 31.5 37. 36.6 33.4 34.9 35.7 36.8 44. 38.9 35.3 41.6 40.3 30.1 36. 36.8 33.5 36.4 41.7 22.6 29. 34.6 27.3 33.6 39.3 25.0 25. 25.6 23.8 26.8 23.8 34.1 35. 38.1 34.4 37.8 34.4 37.4 41. 36.3 39.1 40.0 41.3 34.7 39. 39.6 36.2 39.7 40.8 Lat.1 kv cache 4.8 9.4 5.9 4.8 4.9 5. 280M 560M 560M 280M 284M 287M run concurrently, so latency is t, compute remains LC, and the KV cache remains O(Lnd). KV-cache sharing variant reuses the cache of the first loop and does not store separate caches for the non-first loops, which reduces the KV cache to O(nd) at the cost of small accuracy drop. Finally, rather than using full-context attention or disabling attention in the non-first loops, we apply sliding-window attention of size in those loops, which yields total KV cache of O(cid:0)nd + (L 1)wd(cid:1). Since typically L, the extra KV cache and compute overhead is small. Overall, PLT maintains near-baseline decoding latency and avoids the L-fold growth of the KV cache."
        },
        {
            "title": "3 Experiments",
            "content": "We organize our experiments in two parts. Section 3.1 provides comprehensive comparison of PLT against vanilla Transformer baseline under identical parameter settings, demonstrating the contribution of each component introduced in PLT and examining scalability with respect to the number of parallelized loops. Section 3.2 evaluates inference efficiency at matched accuracy, showing that PLT achieves performance comparable to the vanilla Transformer with fewer parameters while delivering significantly improved inference efficiency."
        },
        {
            "title": "3.1.1 Settings",
            "content": "Training recipe. In this section, we compare Seed-MoE models with vanilla looped Transformer and variants of PLT . We add each componentcross-loop parallelism, KV-cache sharing, and gated sliding-window attention (G-SWA)to the vanilla looped Transformer one by one to assess the effect of each component. For PLT-related hyperparameters, we set the G-SWA window size to 64 and vary the PLT loop count {2, 3} to study scalability. We train 680M/13B MoE (680M activated parameters with 13B total parameters) models on 150B high-quality tokens. Additional details are withheld due to confidentiality; more detailed open-source training configuration appears in Section A.2. Accuracy evaluation recipe. We evaluate accuracy on the following open-source benchmarks: MMLU [17], CEval [20], AGIEval (AGI.) [48], MMLU-Pro (M. Pro) [40], BBH [34], DROP [12], MBPP [1], HumanEval (H.Eval) [3], MATH [18], and GSM8k [9]. Inference efficiency evaluation recipe. We use FP8 self-attention [33] and W4A8 quantization of linear layers [19] to simulate real serving. To evaluate efficiency under both lowand high-throughput scenarios, we set the prefill context length to 5000 and vary the inference batch size in [4, 8, 16, 32, 64], which shifts decoding from low-throughput to high-throughput scenarios. We report per-token decoding latency, averaged over five independent runs that each decode 256 tokens on one single GPU."
        },
        {
            "title": "3.1.2 Results and Analysis",
            "content": "Table 2 reports the performance of vanilla transformers, loop transformers, and variants of PLT . We summarize and explain the main findings below. 6 Table 3 Inference efficiency evaluation of PLT variants across different batch size. (1)Seed-MoE 680M/13B (2)+loop-2 (3)+loop-2 +CLP (4)+loop-2 +CLP+KV share (5)+loop-2 +CLP+KV share+G-SWA(PLT -2) 4.8(1.00) 9.4(1.96) 5.9(1.23) 4.8(1.00) 4.9(1.02) 5.6(1.00) 11.1(1.98) 6.9(1.23) 5.6(1.00) 5.7(1.02) 6.7(1.00) 13.2(1.97) 8.5(1.27) 6.8(1.01) 6.9(1.03) 8.1(1.00) 16.1(1.99) 10.9(1.35) 8.3(1.02) 8.6(1.06) 10.9(1.00) 21.4(1.96) 16.4(1.50) 11.1(1.02) 11.3(1.04) bs=4 bs=8 bs= bs=32 bs=64 Observation 1: Cross-loop parallelism (CLP) preserves accuracy and reduces latency. loop transformer with two loops (row (2)) raises average accuracy by +5.0 points over the vanilla model (34.739.7), but also increases latency and memory (4.89.4 ms, +96%; 280M560M, +100%). Adding CLP at the same loop count (row (3)) keeps the accuracy nearly unchanged (39.739.6, 0.1), while it cuts latency from 9.4 ms to 5.9 ms (37%) by parallelizing loop computation. Compared to the vanilla transformer, CLP keeps most of the accuracy gain (+4.9 points) with only modest latency factor (4.85.9 ms, 1.23) and no extra KV-cache versus the naive loop (still 560M). Overall, CLP removes the sequential latency cost of looping while keeping the accuracy benefit. Observation 2: Efficient representation enhancement reduces KV-cache with minimal accuracy loss. Efficient representation enhancement includes two parts: KV-cache sharing and gated sliding-window attention (GSWA). KV-cache sharing (row (4)) removes the extra KV-cache footprint from looping (560M280M, 50%) and also reduces latency (5.94.8 ms, 19%) because of less KV-cache loading time during decoding, but also lowers average accuracy benefit by 3.4 points (39.636.2) because each loop no longer holds its own dedicated KV. Furthermore, Adding G-SWA (row (5)) restores per-loop specificity using local window, raising accuracy from 36.2 to 39.7 (+3.5) while adding only 1.4% KV-cache overhead (280M284M) and nearly no latency penalty (4.84.9 ms). Therefore, KV-cache sharing solves the memory blow-up; G-SWA recovers accuracy at negligible cost. Observation 3: PLT (PLT) delivers loop-level accuracy with near-vanilla efficiency and scales well. With two loops, PLT -2 (row (5)) matches the accuracy of the naive loop transformer (39.7) but keeps efficiency close to the vanilla model (latency 4.84.9 ms, +2%; KV 280M284M, +1.4%). Scaling to PLT -3 (row (6)) further improves average accuracy to 40.8 (+1.1 over PLT -2) with only small latency increase (4.84.9 ms, +2%) and minor KV change (284M287M, +1.1%). PLT decouples latency and memory from the loop count, so it preserves the accuracy gains of looping while keeping inference overhead close to vanilla, and it scales smoothly with more loops."
        },
        {
            "title": "3.1.3 Inference Efficiency",
            "content": "Table 3 shows that PLT (PLT ) improves latency in both low-throughput and high-throughput regimes while outperforming the vanilla loop transformer. In the low-throughput small-batch setting (bs {4, 8}), adding KV sharing on top of CLP (row (3)(4)) reduces latency from 5.9 4.8 ms (19%) at bs = 4 and 6.9 5.6 ms (19%) at bs = 8, and PLT -2 (row (5)) stays near vanilla (row (1)) at 1.02 (4.8 4.9 ms; 5.6 5.7 ms) while G-SWA recovers accuracy with negligible latency change. In the high-throughput large-batch setting (bs {32, 64}), CLPs concurrency is the main driver: relative to the vanilla loop transformer (row (2)), PLT -2 cuts per-token latency by 47% at bs = 32 (16.1 8.6 ms) and 47% at bs = 64 (21.4 11.3 ms), with residual overhead vs. vanilla limited to 1.06 (8.1 8.6 ms) and 1.04 (10.9 11.3 ms), respectively. Overall, PLT -2 reduces latency by 47% compared to naive looping across batch sizes and remains within 46% of vanilla for practical serving (bs 32)."
        },
        {
            "title": "3.2 Latency Comparisons under Same Accuracy\nWe scale the P LT model size to match the accuracy of the vanilla Seed-MoE baseline while improving inference\nefficiency.",
            "content": "Training recipe. The baseline is an in-house 2.5B/60B Seed-MoE model trained on 1T tokens. For LT , we use shallower model by setting the number of layers to two-thirds of the baseline, yielding 1.7B/40B MoE 7 Table 4 Performance comparison of in-house Seed-MoE 2.5B and PLT-2 1.7B. MMLU CEval M.PRO AGI. BBH MATH TQA DROP MBPP H.Eval Avg. Seed-MoE (2.5B/60B) PLT -2 (1.7B/40B) 75.1 77.3 78.2 80. 47.4 45.8 60.6 58.9 70.9 66.7 48.6 43.5 66.3 71.6 63.9 65. 60.8 64.0 49.4 52.4 62.1 62.6 configuration. Accuracy and inference efficiency evaluation Recipe Same as Sec.3.1. Results. As shown in Table 4, the 1.7B/40B MoE model with LT and two loops achieves an average accuracy of 62.6, outperforming the vanilla 2.5B/60B MoE model by 0.5 points. Figure 3 illustrates decoding latency across batch sizes from 4 to 64. The 1.7B/40B MoE model with LT and two loops delivers about 30% lower latency than In addition, the vanilla 2.5B/60B MoE model. its KV cache is roughly two-thirds of the baseline due to the reduced depth. Overall, LT improves the scalability of Transformers, achieving similar accuracy with fewer parameters, lower KV-cache memory, and lower inference latency."
        },
        {
            "title": "4.1 Latent Reasoning",
            "content": "While Chain-of-Thought (CoT) [41] improves reasoning through explicit intermediate step generation, researchers have also explored latent reasoning paradigms that internalize or extend such multistep computation within the model itself either vertically (looped Transformers) or horizontally (latent CoT). Figure 3 Batch size vs latency on Seed-MoE (2.5B/60B) and PLT-2 (1.7B/40B). Looped Transformers vertical latent reasoning The initial studies by Dehghani et al. [11] and Lan et al. [25] introduced parameter sharing and recurrent adoption across layers, laying the foundation of looped Transformers. Saunshi et al. [32] highlighted the strong capabilities of looped Transformers in complex reasoning tasks. Meanwhile, Fan et al. [13], Yang et al. [43] demonstrated its parameter efficiency in datafitting settings, and superior length generalization on RASP-L tasks. More recent works such as Chen et al. [7] and Geiping et al. [14] allocate greater computational depth to complex tokens through looping, enabling models to scale up inference-time reasoning capacity without increasing parameter count. Latent CoT horizontal latent reasoning Previous works [15, 46] enhance the models reasoning capability by inserting special discrete, non-semantic tokens into the sequence, thereby allocating additional computational steps to refine its intermediate representations. In contrast, recent studies by [16, 35] compress reasoning steps into continuous latent thoughts, achieving more efficient and expressive internal reasoning. These latent CoT methods demonstrate notable improvements across reasoning benchmarks such as GSM8K [9], NaturalQuestions [22], and CommonsenseQA [36], etc. Both looped Transformers and latent CoT reasoning suffer from inferior inference efficiency due to their inherently sequential computation whether across loops or by tokens, while our proposed PLT innovatively overlaps the computation of different loops in different tokens, leading to extreme inference efficiency."
        },
        {
            "title": "4.2 Parallelized Computation",
            "content": "The parallelized computation during inference in Large Language Models is relatively new research area in recent years. We highlight three representative works including PHD [42], ParScale [6], and StagFormer [10] PHD PHD [42] presents that utilizing parallel computation can leads to scalable performance improvement via parallelizing the forward of repeated tokens. To maintain minimum memory access overhead while achieve better performance, PHD introduces both KV cache sharing and chunk-wise sliding window attention. The drawback of PHD lies in the token repetition methodology, which is an inefficient method of utilizing parallel computation, since the hidden representations in the former transformer layers are very similar. Under high-throughput serving scenarios, the improved performance of PHD can not compensate for the loss of throughput due to increased decoding computation. ParScale Latter, ParScale [6] presents that using sequence repetition with prefix tuning [27] can also leads to scalable performance improvement. The drawback of ParScale lies in its inefficiency by introducing KV cache when inference streams are activated, leading to overhead both in KV cache footprint and inference latency, especially in the high-throughput serving scenarios. StagFormer StagFormer [10] proposes time-staggered decoding mechanism that parallelizes Transformer layer execution along the depth axis by splitting layers into multiple stacks, where the upper stack attends to the lower stacks hidden states from the previous time step via cross-attention. This achieves partial layer-level parallelism but still suffers from incomplete parallelism in attention computation and memory access, leading to limited efficiency gains. Specifically, The separate-weight variant doubles hardware usage but achieves less than 50% throughput improvement, while the weight-sharing variant, though lighter in parameters, incurs extra KV-cache cost and additional cross-attention overhead compared to our loop2+CLP design, resulting in strictly worse inference efficiency. With completed parallelism and sharing strategy over the looped transformer, our proposed PLT further presents more impressive inference efficiency by reducing the KV cache footprint by over 50% with no performance degradation. In this paper, our presented PLT improves the utilization of parallel computation to an unprecedented level. By discovering loop transformers are naturally suitable for parallel computation utilization, we propose cross-loop parallelism to improve looped transformer, maintaining the performance of loop transformers while achieving especially better inference efficiency."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we present Parallelized Looped Transformer (PLT), which proposes Cross-Loop Parallelism (CLP) that overlaps the computation and the memory-access of latter loops in previous tokens and previous loops in latter tokens, and gated sliding window attention (gated-SWA) to achieve parallelism in the access of KV cache with no performance degradation. PLT presents impressive performance improvement with negligible latency overhead compared with the vanilla Transformer under memory-access bottle-neck, and clearly better inference efficiency with similar or even better performance compared with vanilla looped transformers with the same inference computation budget, showing the potential of parallel computation utilization."
        },
        {
            "title": "6 Contributions and Acknowledgments",
            "content": "Names are Listed in the alphabetic order."
        },
        {
            "title": "Core Contributors",
            "content": "Mengzhao Chen, Xiang Luo, Bohong Wu, Shen Yan"
        },
        {
            "title": "Infrastructure",
            "content": "Xiang Luo, Fan Xia, Tianqi Zhang, Hongrui Zhan, Zheng Zhong"
        },
        {
            "title": "Model Architecture",
            "content": "Mengzhao Chen, Bohong Wu, Shen Yan, Qifan Yu, Xun Zhou"
        },
        {
            "title": "Supervision",
            "content": "Siyuan Qiao, Xingyan Bin"
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Jianqiao Lu, Yunshui Li, Yao Luo, Jin Ma, Yiyuan Ma, Yutao Zeng, Chaoyi Zhang, Zhijian Zhuo as well as other colleagues at ByteDance for their support for this project."
        },
        {
            "title": "References",
            "content": "[1] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [2] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. [3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [4] Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, and Ping Luo. Efficientqat: Efficient quantization-aware training for large language models. arXiv preprint arXiv:2407.11062, 2024. [5] Mengzhao Chen, Chaoyi Zhang, Jing Liu, Yutao Zeng, Zeyue Xue, Zhiheng Liu, Yunshui Li, Jin Ma, Jie Huang, Xun Zhou, et al. Scaling law for quantization-aware training. arXiv preprint arXiv:2505.14302, 2025. [6] Mouxiang Chen, Binyuan Hui, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Jianling Sun, Junyang Lin, and Zhongxin Liu. Parallel scaling law for language models. arXiv preprint arXiv:2505.10475, 2025. [7] Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, and Haifeng Wang. Inner thinking transformer: Leveraging dynamic depth scaling to foster adaptive internal thinking. arXiv preprint arXiv:2502.13842, 2025. [8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [10] Dylan Cutler, Arun Kandoor, Nishanth Dikkala, Nikunj Saunshi, Xin Wang, and Rina Panigrahy. Stagformer: Time staggering transformer decoding for runninglayers in parallel. arXiv preprint arXiv:2501.15665, 2025. [11] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= HyzdRiR9Y7. [12] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 23682378, 2019. [13] Ying Fan, Yilun Du, Kannan Ramchandran, and Kangwook Lee. Looped transformers for length generalization. arXiv preprint arXiv:2409.15647, 2024. [14] Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025. [15] Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. In The Twelfth International Conference on Learning Representations. [16] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. [17] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [18] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [19] Huanqi Hu, Bowen Xiao, Shixuan Sun, Jianian Yin, Zhexi Zhang, Xiang Luo, Chengquan Jiang, Weiqi Xu, Xiaoying Jia, Xin Liu, et al. Liquidgemm: Hardware-efficient w4a8 gemm kernel for high-performance llm serving. arXiv preprint arXiv:2509.01229, 2025. [20] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36:6299163010, 2023. [21] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. CoRR, abs/2310.06825, 2023. doi: 10.48550/ARXIV.2310.06825. URL https: //doi.org/10.48550/arXiv.2310.06825. [22] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. [23] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Jason Flinn, Margo I. Seltzer, Peter Druschel, Antoine Kaufmann, and Jonathan Mace, editors, Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, pages 611626. ACM, 2023. doi: 10.1145/3600006.3613165. URL https://doi.org/10.1145/3600006.3613165. [24] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [25] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=H1eA7AEtvS. [26] Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, et al. Minimax-01: Scaling foundation models with lightning attention. arXiv preprint arXiv:2501.08313, 2025. [27] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 45824597, 2021. [28] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. [29] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [30] Amirkeivan Mohtashami, Matteo Pagliardini, and Martin Jaggi. Cotformer: More tokens with attention make up for less depth. In Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023), 2023. [31] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. [32] Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, and Sashank Reddi. Reasoning with latent thoughts: On the power of looped transformers. arXiv preprint arXiv:2502.17416, 2025. [33] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37:6865868685, 2024. 12 [34] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1300313051, 2023. [35] Jihoon Tack, Jack Lanchantin, Jane Yu, Andrew Cohen, Ilia Kulikov, Janice Lan, Shibo Hao, Yuandong Tian, Jason Weston, and Xian Li. Llm pretraining with continuous concepts. arXiv preprint arXiv:2502.08524, 2025. [36] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. [37] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference. In International Conference on Machine Learning, pages 4790147911. PMLR, 2024. [38] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [39] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [40] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. [41] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [42] Bohong Wu, Shen Yan, Sijun Zhang, Jianqiao Lu, Yutao Zeng, Ya Wang, and Xun Zhou. Efficient pretraining length scaling. arXiv preprint arXiv:2504.14992, 2025. [43] Liu Yang, Kangwook Lee, Robert Nowak, and Dimitris Papailiopoulos. Looped transformers are better at learning learning algorithms. arXiv preprint arXiv:2311.12424, 2023. [44] Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, et al. Llm inference unveiled: Survey and roofline model insights. arXiv preprint arXiv:2402.16363, 2024. [45] Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, et al. Llm inference unveiled: Survey and roofline model insights. CoRR, 2024. [46] Eric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. Quiet-star: Language models can teach themselves to think before speaking. In First Conference on Language Modeling, 2024. [47] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [48] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 22992314, 2024. [49] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et al. survey on efficient inference for large language models. CoRR, 2024. [50] Zhijian Zhuo, Yutao Zeng, Ya Wang, Sijun Zhang, Jian Yang, Xiaoqing Li, Xun Zhou, and Jinwen Ma. Hybridnorm: Towards stable and efficient transformer training via hybrid normalization. CoRR, 2025. 13 (a) Dense latency. (b) MoE latency. (c) Dense throughput. (d) MoE throughput. Figure 4 Inference efficiency analysis including latency and throughput for vanilla transformer, PLT and looped transformer over 1 billion activated parameters. We use FP8 quantization during inference based on VLLM [23]. Table 5 Performance evaluation of 1.2B dense models and 1B/7B MoE models across the baseline, vanilla looped transformer with = 2 and our proposed PLT with = 2. Evaluated benchmarks include: MMLU, Hellaswag (Hella.), ARC-Challenge (ARC-C), ARC-Easy (ARC-E), PIQA, Winogrande (Wino.), and Commonsense QA (Comm.)."
        },
        {
            "title": "Loss MMLU",
            "content": "Hella. ARC-C ARC-E PIQA Wino. Comm. Avg. Vanilla Dense + loop-2 + PLT-2 Vanilla MoE + loop-2 + PLT-2 2.577 2.532 2.537 2.342 2.302 2.280 35.5 36.3 36.8 37.3 38.7 38. 62.5 66.1 65.4 67.2 70.3 71.0 38.1 40.5 41.5 40.5 44.1 43.1 71.4 73.9 72.3 72.1 75.1 77. 74.9 75.6 76.5 76.3 77.0 78.7 60.6 64.6 61.4 62.7 64.1 63.5 44.7 46.4 47.9 48.2 48.2 48. 55.4 57.6 57.4 57.8 59.6 60."
        },
        {
            "title": "A Experiments opensource",
            "content": "We arrange our opensource experiments in two-fold. We present the open source Experiments on both the dense model series and MoE model series, with over 1 billion activated Parameters in Section A.2. We present similar ablation study in Section A.3 on the dense models, as complementary of the our in-house Seed-MoE experiments. A.1 Evaluation Datasets We used the following open-source datasets in our evaluation, including MMLU [17], HellaSwag [47], ARC [8], PIQA [2], Winogrande [31], CommonsenseQA [36]. A.2 Main Experiments A.2.1 Training and Evaluation Settings Our open-source implementation is based on OLMo and OLMoE. We compare our method with the vanilla transformer and vanilla looped transformer with the same activated parameters. Dense Models Recipe For dense models, we set the number of transformer layers to 16. We set the hidden dimension to 2048. For MLP modules in these models, we set the MLP hidden dimension to 16384. We use FFN output norm. For Attention modules in these models, we use GQA with 32 query heads and 8 key/value heads. We use query/key/value layernorm at the same time, while no attention output norm. For LM head modules, we use weight tying that shares the parameter with the embedding layer. Table 6 Component analysis of PLT . Compared with vanilla looped transformer, we introduce two extra components for the consideration of both inference speed and performance. Lat. is the abbreviation for latency(ms). Loss MMLU Hella. ARC-C ARC-E PIQA Avg. Lat.@bs=1 Lat.@bs=16 29.2 2.880 (1) Vanilla Dense 2.856 28.7 (2) +loop-2 (3) +loop-2 +CLP 2.840 30.0 29.6 2.858 (4) +loop-2 +CLP +KV share 29.7 (5) +loop-2 +CLP +KV share+GSW 2.844 45.7 47.1 48.3 46.5 47.4 26.4 26.1 30.8 28.4 30.1 61.8 60.9 59.5 60.2 62. 69.7 46.6 70.6 46.7 69.7 47.8 69.8 46.9 69.6 47.8 1.68 2.66 1.73 1.73 1.73 2.49 3.92 3.23 2.69 2.70 For training dynamics, we train the model for 400B tokens in total, with global training batch size set to 1024. We use the cosine learning rate schedule, using 3e-4 as the peak learning rate and 2000 steps warmup. Note that compared with the original setting of public available opensource model OLMo-1B, we have made slight modifications on the norm settings due to the consideration of training stability [50]. MoE Models Recipe For MoE models, we set the number of transformer layers to 16. We set the hidden dimension to 2048. For MLP modules in these models, we use SwiGLU experts and the 8 in 64 recipe. For Attention modules in these models, we use MHA with 16 attention heads in total. For LM head modules, we also use weight tying that shares the parameter with the embedding layer. For training dynamics, we train the model for 400B tokens in total, with global training batch size also set to 1024. We use the cosine learning rate schedule, using 4e-4 as the peak learning rate and 2500 steps warmup. Efficiency Evaluation Recipe Based on VLLM [23], we analyze the efficiency including both the latency and throughput, of all these baseline models. We vary the prefilling length in [1024, 2048] and the serving batch size in [1, 2, 4, 8, 16, 32, 64, 128, 256], which gradually shifts from memory-access bottleneck to compute bottleneck. For serving, we use FP8 quantization which is closer to the real industrial serving scenarios. The latency metric is averaged across 5 independent runs of decoding 256 tokens. We conduct the efficiency analysis experiments on one single GPU. A.2.2 Analysis Table 5 presents the performance of variants on Dense Models and MoE models. The observations are similar with our in-house experiments that PLT-2 achieves very similar performance with vanilla looped transformers. Figure 4 further presents the efficiency analysis, where we also obtain similar observations that PLT-2 presents obviously better efficiency than vanilla looped transformers. A.3 Ablation study A.3.1 Training/Evaluation Settings Dense Models Recipe The training settings in this section mainly follows the setting in Section A.2.1. Except for the following two modifications. We change GQA with 16 query heads and 4 key value heads, and set hidden dimension to 1536 and MLP hidden size to 8192. We change the training token numbers to 100B, with 3e-4 as the peak learning rate and the cosine learning rate scheduler, warm-up 2000 steps. MoE Models Recipe The training settings in this section mainly follows the setting in Section A.2.1. Different recipes are also presented as follows. We change the layer numbers to 12 and hidden dimension to 1536. 15 For training dynamics, we also set the number of training tokens to 100B, with 3e-4 as the peak learning rate and the cosine learning rate scheduler, warm-up 2000 steps. Evaluation Settings The evaluation settings mainly follows Section A.2.1, except for we use one H800 GPU for evaluation. A.3.2 Analysis Table A.3.1 presents our ablation study on OLMo2, where the observation are similar with our in-house experiments."
        }
    ],
    "affiliations": [
        "ByteDance"
    ]
}