{
    "paper_title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning",
    "authors": [
        "Alan Dao",
        "Dinh Bach Vu",
        "Bui Quang Huy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of large language models (LLMs) for 3D Cartesian space navigation. AlphaSpace employs a semantics-based tokenization strategy, encoding height information through specialized semantic tokens, and integrates primarily symbolic synthetic reasoning data. This approach enables LLMs to accurately manipulate objects by positioning them at specific [x, y, z] coordinates. Experimental results demonstrate that AlphaSpace significantly outperforms existing models on manipulation subtasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet."
        },
        {
            "title": "Start",
            "content": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning Alan Dao (Gia Tuan Dao)1, Dinh Bach Vu1, Bui Quang Huy Menlo Research alan@menlo.ai, bach@menlo.ai, yuuki@menlo.ai 1Equal contribution. February 20, 2025 5 2 0 2 4 ] . [ 1 9 6 7 8 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This paper presents AlphaSpace, novel methodology designed to enhance the spatial reasoning capabilities of language models for robotic manipulation in 3D Cartesian space. AlphaSpace employs hierarchical semanticsbased tokenization strategy that encodes spatial information at both coarse and fine-grained levels. Our approach represents objects with their attributes, positions, and height information through structured tokens, enabling precise spatial reasoning without relying on traditional vision-based embeddings. This approach enables LLMs to accurately manipulate objects by positioning them at specific [x, y, z] coordinates. Experimental results demonstrate that AlphaSpace significantly outperforms existing models on manipulation subtasks, achieving total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet."
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including advancements in spatial reasoning Li et al. [2024], Wu et al. [2024]. significant step in this direction is the AlphaMaze methodology Dao and Vu [2025] , which introduced novel approach to enhance LLMs visual spatial intelligence, specifically for maze navigation. AlphaMaze employs two-stage training framework, using Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO), to enable LLMs to interpret tokenized maze representations and predict step-by-step movement commands. While effective for navigating mazes, this approach presents challenges in larger and more complex environments. This paper introduces AlphaSpace, an innovative methodology designed to address this challenge and substantially improve the spatial reasoning capabilities of LLMs. AlphaSpace builds upon the foundational principles of AlphaMaze by leveraging semantics tokenization strategy with enhanced semantic tokens to support height (z-coordinate) information. AlphaSpace incorporates synthetic reasoning data, primarily symbolic in nature, to enable the model to move objects to specific given [x, y, z] coordinates Chen et al. [2024b,a], Mecattaf Figure 1: Put black cube onto green cube et al. [2025]. This includes adding local position information to help reconstruct larger spaces and spawning multiple containers for placing tasks Vig [2023], Yamada et al. [2024], Daxberger et al. [2025]. By augmenting the tokenization method with height information and integrating symbolic reasoning data, AlphaSpace enables LLMs to understand and manipulate objects within 3D Cartesian space Wu et al. [2025], Sharma [2023], Xiong et al. [2025]. This enhancement allows the model to go beyond the 2D limitations and operate in more complex, three-dimensional environment Chandhok [2024]. Our experiments on embodied manipulation subtasks demonstrate that AlphaSpace achieves 66.67% total accuracy, significantly outperforming GPT4o (37.5%) and Claude 3.5 Sonnet (29.17%). 1.1 Motivation The motivation behind AlphaSpace stems from the current state of robotics models and benchmarks, which primarily rely on either generally pretrained vision-language 1 models (VLMs) Bai et al. [2025], Yao et al. [2024], Hu et al. [2024] or research approaches that use visionlanguage-action (VLAs) Kim et al. [2025, 2024] to predict robotic arm joint angles. However, both of these methods demand extensive training and can be computationally expensive, even during inference. While generally pretrained vision-language models (VLMs) have demonstrated impressive capabilities in interpreting visual scenes, they struggle with fine-grained spatial reasoning and object manipulation in 3D environments. These models often rely on implicit visual features rather than explicitly structured spatial representations, leading to inconsistencies when performing precise object placement or navigation tasks. Additionally, their performance is constrained by the lack of dedicated spatial priors, making it difficult for them to generalize across diverse manipulation settings. Similarly, approaches that directly predict robotic arm joint angles from visual inputs also faces substantial hurdles. Their reliance on end-to-end learning pipelines often limits adaptability, as minor variations in task conditions can degrade performance. Furthermore, the high-dimensional control space of robotic arms makes it difficult to scale these methods efficiently to more complex tasks. In contrast, the semantics tokenization strategy has demonstrated that it can provide decoder models with sufficient information to reason about coordinates and spatial attributes in 2D spaces Dao and Vu [2025]. Many other studies further highlight the ability of decoderbased models to perform spatial reasoning through structured tokenization Wu et al. [2024], Yamada et al. [2024]. By encoding spatial semantics explicitly, these approaches enable models to achieve more structured and interpretable spatial understanding. Building on these insights, AlphaSpace introduces an advanced tokenization method that extends spatial reasoning beyond two dimensions into full 3D Cartesian space. By integrating symbolic reasoning data and enriched semantic tokens, AlphaSpace enables more efficient and precise manipulation of objects in three-dimensional environments. Unlike VLAs or VLMs, which heavily depend on the quality of modality encoderswhether for vision Zhai et al. [2023] or actions Pertsch et al. [2025]and require intensive computation, AlphaSpace offers lightweight yet effective approach to spatial reasoning. This structured representation significantly enhances the models ability to generalize across tasks, making it particularly well-suited for robotics, object manipulation, and large-scale spatial navigation. 1.2 Contributions Our key contributions are as follows: Evidence that Decoder-Only Models Can Perform 3D Spatial Reasoning Using Semantic Tokens Demonstrating that decoder-only architecture, without reliance on explicit 3D geometric encoders or vision modules, can effectively reason in 3D Cartesian space using an enhanced semantic tokenization 2 approach. This extends previous 2D spatial reasoning capabilities to support height (z-coordinates) and object manipulation. Enhanced Semantic Tokenization for 3D Spatial Reasoning extending the semantics tokenization approach to support 3D Cartesian space, allowing models to reason about height (z-coordinates) in addition to 2D positional attributes. Symbolic Reasoning for Object Manipulation Incorporating synthetic reasoning data to enable LLMs to manipulate objects in structured manner, facilitating precise object placement and spatial transformations. Empirical Validation on Embodied Manipulation Tasks Demonstrating AlphaSpaces effectiveness through experiments on embodied spatial reasoning tasks, where it significantly outperforms existing models such as GPT-4o and Claude 3.5 Sonnet in 3D object manipulation accuracy."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Advancements in Vision-Language-Action Models Recent developments in VLA models have significantly enhanced robotic manipulation capabilities. Kim et al. [2024] introduced OpenVLA, open-source 7-billionparameter open-source VLA model trained on 970,000 robot episodes from the Open X-Embodiment dataset. OpenVLA supports controlling multiple robots out-ofthe-box and can be quickly adapted to new robot setups via parameter-efficient fine-tuning, setting new state of the art for generalist robot manipulation policies. 2.2 General-Purpose Robot Foundation Models The π0 (pi-zero) model, proposed by Black et al. [2024], represents significant advancement in general-purpose robot foundation models. Built on flow matching architecture atop pre-trained vision-language model, π0 is trained on diverse datasets from multiple dexterous robot platforms, including single-arm robots, dual-arm robots, and mobile manipulators. The model demonstrates capabilities in performing zero-shot tasks, following language instructions, and acquiring new skills via fine-tuning, covering wide array of tasks such as laundry folding, table cleaning, and assembling boxes. 2.3 Integrating Spatial Intelligence into AI and Robotics Several notable developments have contributed to integrating spatial intelligence into AI and robotics. Deepintroduced Gemini Robotics and Gemini Mind [2025] Robotics-ER, models that leverage the reasoning abilities of LLMs to assist robots in performing complex tasks, marking significant shift in using AI to improve adaptability and performance in various environments. These advancements collectively contribute to the enhancement of spatial reasoning in LLMs, providing valuable insights and methodologies that align with the objectives of AlphaSpace in improving 3D spatial understanding and manipulation capabilities."
        },
        {
            "title": "3 Methodology",
            "content": "To develop AlphaSpace, we designed training pipeline inspired by AlphaMaze to enhance manipulation capabilities within tabletop environment, specifically targeting the EmbodiedBench benchmark. The methodology consists of two-stage synthetic dataset generation process, followed by model training utilizing structured tokenization and reasoning-based learning. 3.1 Synthetic Dataset Generation Our synthetic dataset consists of two connected components: object configurations and corresponding action plans. Object Configurations The environment consists of planar tabletop workspace discretized into 100 100 grid. To facilitate hierarchical reasoning, this grid is further subdivided into coarser 25 25 grid, where each coarse cell encompasses 4 4 subgrid of the finer resolution. An objects location is specified by tuple (rg, cg, rl, cl), where (rg, cg) represent the row and column indices in the coarse grid (0 rg, cg < 25), and (rl, cl) represent the row and column indices within the fine grid of the corresponding coarse cell (0 rl, cl < 4). Objects are represented by tuple (color, shape), where color is selected from set of 19 distinct colors (red, blue, green, purple, etc.), and shape is one of cube, cylinder, triangular prism, star, moon, or container. During object placement, we enforce constraints to ensure nonoverlapping objects (minimum 4-unit Euclidean distance between centers), uniform spatial distribution of objects, and variable object heights (uniformly sampled from [1, 30]). The dataset encompasses three primary task categories: (1) placement tasks, where the robot must pick up an object and place it into container (e.g., Pick up the red cube and place it into the blue container); (2) stacking tasks, where one object must be positioned on top of another (e.g., Stack the red cube on top of the blue cylinder); and (3) movement tasks, where objects are relocated to specific coordinates (e.g., Move the green star to [76, 65]). Scene complexity ranges from 4-7 objects per environment, with each scene containing the necessary objects for task completion plus additional objects for environmental complexity. Action Plan Generation For each object configuration, we generate corresponding action plan representing sequence of robot actions required to achieve the specified manipulation goal. Each action is encoded as 7-dimensional vector consisting of: global position in the 2525 grid (rg, cg), local position in the 4 4 subgrid (rl, cl), Z-axis height (vertical distance from gripper to table surface), roll orientation (range: 0120, each unit representing 3 degrees), pitch orientation (range: 0-120), yaw orientation (range: 0-120), and gripper state (0 for closed, 1 for open). typical action sequence progresses through seven steps: (1) approaching above the source object with open gripper, (2) lowering to the objects base, (3) closing the gripper to grasp the object, (4) lifting to safe height, (5) moving above the target location, (6) lowering to the placement position, and (7) opening the gripper to release the object. Each sequence is accompanied by explicit reasoning annotations that detail the spatial problemincluding object localization (identifisolving strategy, cation of relevant objects positions and properties) and action planning (step-by-step breakdown of the manipulation process). These reasoning annotations provide valuable supervision signals for training models to understand the connection between perception, spatial reasoning, and action planning. Our final dataset consists of approximately 260,000 synthetic samples, with 100,000 placement tasks, 120,000 stacking tasks, and 40,000 movement tasks. For subset of the data, we employed object uniqueness constraints to prevent the model from relying on spurious correlations between object types and actions. 3.2 Model Training Pipeline Following dataset generation, we trained the model using decoder-only architecture with an enhanced semantic tokenization strategy. By integrating enhanced spatial tokenization and structured reasoning data through Supervised Fine-Tuning (SFT) on the synthetic reasoning dataset, AlphaSpace effectively learns spatial relationships and object manipulation tasks, outperforming baseline models in EmbodiedBench evaluations. 3.3 Evaluation We use EmbodiedBench Yang et al. [2025], widely used benchmarking framework for evaluating the performance of vision-language models (VLMs) on various vision and spatial reasoning tasks. Specifically, we focus on the EB-Manipulation subset, which assesses VLMs on spatially related pick-and-place tasks. Within this benchmark, we use set of 12 pick tasks and 12 place tasks, selected from the spatial tasks category of EB-Manipulation. Hence, the benchmark used in this paper is subset of EB-Manipulation, specifically focusing on spatial pick-and-place tasks, and comprises total of 24 actions. 3 Additionally, we re-ran the benchmark for Claude-3.5Sonnet and GPT-4o, the two leading models on the current benchmark leaderboard."
        },
        {
            "title": "4 Experiments and Results",
            "content": "To evaluate the effectiveness of AlphaSpace in manipulation tasks, we conducted experiments on the EmbodiedBench benchmark, specifically focusing on the Manipulation Subtask. 4.1 Training Details We trained our spatial reasoning model using supervised fine-tuning on the synthetic dataset described in the previous section. Our model is based on DeepSeek-R1-distilQwen-1.5B. The training process used learning rate of 1.0 104 with cosine decay schedule and warm-up ratio of 0.1. Training was conducted on 8 NVIDIA H200 GPUs, with batch size of 16 samples per device and maximum context length of 4096 tokens. 4.2 Evaluation Results We evaluated AlphaSpace against state-of-the-art models, including GPT-4o and Claude 3.5 Sonnet. The results on the EmbodiedBench Manipulation Subtask are presented in Table 1 and visualized in Figure 2. Model AlphaSpace (Ours) GPT-4o Claude 3.5 Sonnet Picking 10/12 6/12 5/12 Stacking Total (%) 66.67% 37.5% 29.17% 6/12 3/12 2/12 Table 1: Evaluation Results on EmbodiedBench Manipulation Subtask As shown in the results, AlphaSpace significantly outperforms both GPT-4o and Claude 3.5 Sonnet across the picking and stacking tasks. Our model achieves 66.67% overall accuracy, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet. This performance gap demonstrates the effectiveness of our approach in spatial reasoning for robotic manipulation tasks. The significant improvement over commercial state-ofthe-art models highlights the advantages of our specialized training approach, which combines synthetic data generation with explicit reasoning supervision. In particular, AlphaSpace demonstrates stronger performance on stacking tasks (50% success rate) compared to picking tasks (83.3% success rate), suggesting that compositional manipulation tasks remain challenging even for our specialized model. These results indicate that our approach provides promising direction for embodied AI systems that require sophisticated spatial reasoning capabilities. Total Accuracy 66.67 37.5 29.17 ) % ( r A o 80 60 20 0 AlphaSpace GPT-4o Claude 3.5 Sonnet Figure 2: Performance Comparison on EmbodiedBench Manipulation Subtask"
        },
        {
            "title": "5 Discussion",
            "content": "The experimental results demonstrate that AlphaSpace significantly improves the spatial reasoning and manipulation capabilities of LLMs in 3D Cartesian space. This enhancement is primarily attributed to the novel semantics-based tokenization strategy and the integration of symbolic reasoning data. In this section, we analyze the implications of these results, discuss the strengths and limitations of the approach, and outline potential future directions for research. 5.1 Strengths One of the key advantages of AlphaSpace is its ability to encode height information through semantic tokens, allowing LLMs to reason about three-dimensional spatial structures without relying on traditional vision-based embeddings. Unlike VLMs and VLA models, which extract spatial information from visual encoders such as SigLIP, AlphaSpace directly represents spatial semantics through structured tokenization. This removes the need for intermediate visual interpretation and allows for more precise and computationally efficient reasoning about object relationships. As result, AlphaSpace serves as lightweight yet effective alternative for robotic manipulation tasks. Furthermore, AlphaSpaces reliance on structured symbolic reasoning data enhances its generalizability. The models ability to infer correct object placements and transformations in novel spatial arrangements suggests that it is learning more structured form of spatial reasoning compared to end-to-end visual learning approaches. This is evident in its performance on embodied manipulation tasks, where it achieved 66.67% accuracysignificantly outperforming state-of-the-art models like GPT-4o and Claude 3.5 Sonnet. 4 5.2 Limitations Despite its strong performance, AlphaSpace has several limitations. First, the models reliance on tokenized spatial representations means it may struggle with highly dynamic environments where real-time sensory feedback is crucial. Unlike VLMs, which continuously process visual input, AlphaSpace depends on pre-tokenized spatial descriptions, making it less adaptive to rapidly changing scenarios. Another limitation is the assumption of structured environment. The experimental setup ensures controlled object placements and clear spatial references, but realworld applications often involve occlusions, complex object geometries, and unpredictable external forces. Extending AlphaSpace to handle such uncertainties would require either hybrid approach that integrates limited vision-based feedback or an expansion of the tokenization framework to incorporate uncertainty modeling. Additionally, while AlphaSpace demonstrates strong performance in tabletop object manipulation tasks, its scalability to more complex robotic scenarios, such as multi-step assembly or dynamic obstacle avoidance, remains an open question. Future work should explore ways to extend its reasoning capabilities beyond static manipulation tasks. Furthermore, the model does not leverage recent advancements in reinforcement learning for large language models (LLMs), particularly the methods introduced by DeepSeek Shao et al. [2024], DeepSeek-AI et al. [2025]. Despite these techniques being proven effective in improving the accuracy of decoder models for solving spatial reasoning tasks with visual components Dao and Vu [2025], they have not been incorporated into the current approach. The last limitation stems from the premature conclusion of the project due to change in direction. Originally, the study aimed to cover four tasks to fully evaluate the Spatial section of EB-Manipulation, but only two tasks were completed. This restricted the scope of evaluation, potentially leaving gaps in understanding AlphaSpaces full capabilities. Future iterations should ensure comprehensive task coverage to provide more complete assessment of its effectiveness. 5.3 Future Directions computational costs associated with full VLM-based architectures. Finally, expanding the tokenization approach to support dynamic spatial transformations, such as rotating or deforming objects, would be crucial next step. Current tokenization strategies primarily capture static positions, but real-world robotic tasks often involve complex spatial operations that require continuous adjustment."
        },
        {
            "title": "6 Conclusion",
            "content": "AlphaSpace introduces novel semantic tokenization strategy and symbolic reasoning approach to enhance 3D spatial reasoning in LLMs. By encoding height information and leveraging structured synthetic reasoning data, AlphaSpace enables accurate object manipulation in Cartesian space. We demonstrate significant performance improvement over existing models with our experimental results, highlighting the effectiveness of our approach. This work paves the way for more structured, efficient spatial reasoning in robotics and AI, reducing reliance on vision-based encoders and high-dimensional control models. Future work will explore real-world deployment and integration with multimodal systems."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. π0: vision-language-action flow model for general robot control, 2024. URL https://arxiv.org/ abs/2410.24164. To further enhance AlphaSpace, several research directions can be explored. One promising avenue is the integration of reinforcement learning-based fine-tuning to enable real-time decision-making. While AlphaSpace currently relies on fixed set of symbolic reasoning data, incorporating reinforcement learning could help it adapt to unexpected scenarios more effectively. Another area of interest is hybrid modeling Noh et al. [2021], where AlphaSpace could be combined with lightweight vision modules to bridge the gap between symbolic reasoning and real-world perception. By integrating minimal visual priors, AlphaSpace could potentially enhance its adaptability without incurring the high Shivam Chandhok. Scenegpt: language model for 3d scene understanding, 2024. URL https://arxiv.org/ abs/2408.06926. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing visionlanguage models with spatial reasoning capabilities, 2024a. URL https://arxiv.org/abs/2401.12168. Wanyi Chen, Meng-Wen Su, Nafisa Mehjabin, and Mary L. Cummings. Can llms plan paths in the real world?, 2024b. URL https://arxiv.org/abs/2411. 17912. 5 Alan Dao and Dinh Bach Vu. Alphamaze: Enhancing large language models spatial intelligence via grpo, 2025. URL https://arxiv.org/abs/2502.14669. Erik Daxberger, Nina Wenzel, David Griffiths, Haiming Gang, Justin Lazarow, Gefen Kohavi, Kai Kang, Marcin Eichner, Yinfei Yang, Afshin Dehghan, and Peter Grasch. Mm-spatial: Exploring 3d spatial understanding in multimodal llms, 2025. URL https: //arxiv.org/abs/2503.13111. Google DeepMind. Gemini physical world, robotics 2025. brings URL the into ai https://deepmind.google/discover/blog/ gemini-robotics-brings-ai-into-the-physical-world/. Accessed: 2025-03-23. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Siyuan Hu, Mingyu Ouyang, Difei Gao, and Mike Zheng Shou. The dawn of gui agent: preliminary case study with claude 3.5 computer use, 2024. URL https:// arxiv.org/abs/2411.10323. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model, 2024. URL https:// arxiv.org/abs/2406.09246. Moo Jin Kim, Chelsea Finn, and Percy Liang. Finetuning vision-language-action models: Optimizing speed and success, 2025. URL https://arxiv.org/ abs/2502.19645. Fangjun Li, David C. Hogg, and Anthony G. Cohn. Advancing spatial reasoning in large language models: An in-depth evaluation and enhancement using the stepgame benchmark, 2024. URL https://arxiv. org/abs/2401.03991. Matteo G. Mecattaf, Ben Slater, Marko Teˇsic, Jonathan Prunty, Konstantinos Voudouris, and Lucy G. Cheke. little less conversation, little more action, please: Investigating the physical common-sense of llms in 3d embodied environment, 2025. URL https://arxiv. org/abs/2410.23242. Jongyoun Noh, Sanghoon Lee, and Bumsub Ham. Hvpr: Hybrid voxel-point representation for single-stage 3d object detection, 2021. URL https://arxiv.org/ abs/2104.00902. Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models, 2025. URL https://arxiv.org/abs/2501.09747. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. Manasi Sharma. Exploring and improving the spatial reasoning abilities of large language models, 2023. URL https://arxiv.org/abs/2312.01054. Jesse Vig. Language models and spatial reasoning: Whats good, what is still terrible, and what is improving, 2023. URL https://medium.com/data-science/ language-models-and-spatial-reasoning-whats-good-what-is-still-terrible-and-what-is-improving-175d2099eb4c. Accessed: Mar. 23, 2025. Shengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Towards semantic equivalence of tokenization in multimodal llm, 2025. URL https://arxiv.org/abs/2406.05127. Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei. Minds eye of LLMs: Visualization-of-thought elicits spatial reasoning in large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=CEJ1mYPgWw. Haomiao Xiong, Yunzhi Zhuge, Jiawen Zhu, Lu Zhang, and Huchuan Lu. 3ur-llm: An end-to-end multimodal large language model for 3d scene understanding, 2025. URL https://arxiv.org/abs/2501.07819. Yutaro Yamada, Yihan Bao, Andrew K. Lampinen, Jungo Kasai, and Ilker Yildirim. Evaluating spatial understanding of large language models, 2024. URL https://arxiv.org/abs/2310.14540. Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, and Tong Zhang. Embodiedbench: Comprehensive benchmarking multilarge language models for vision-driven emmodal bodied agents, 2025. URL https://arxiv.org/abs/ 2502.09560. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: gpt-4v level mllm on your phone, 2024. URL https://arxiv.org/ abs/2408.01800. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pretraining, 2023. URL https://arxiv.org/abs/2303. 15343."
        }
    ],
    "affiliations": [
        "Menlo Research"
    ]
}