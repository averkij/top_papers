{
    "paper_title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
    "authors": [
        "Bulent Soykan",
        "Sean Mondesire",
        "Ghaith Rabadi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESO's effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: https://github.com/bulentsoykan/TESO."
        },
        {
            "title": "Start",
            "content": "TESO: TABU-ENHANCED SIMULATION OPTIMIZATION FOR NOISY BLACK-BOX PROBLEMS 5 2 0 2 0 3 ] . [ 1 7 0 0 4 2 . 2 1 5 2 : r Bulent Soykan Institute for Simulation and Training University of Central Florida Orlando, FL, USA Bulent.Soykan@ucf.edu Sean Mondesire, Ghaith Rabadi School of Modeling, Simulation, and Training University of Central Florida Orlando, FL, USA {Sean.Mondesire, Ghaith.Rabadi}@ucf.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages short-term Tabu List to prevent cycling and encourage diversification, and long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESOs effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: github.com/bulentsoykan/TESO. Keywords Simulation optimization Metaheuristics Optimization"
        },
        {
            "title": "Introduction",
            "content": "Simulation Optimization (SO) is now widely recognized as powerful methodology for decision-making across diverse spectrum of fields, including engineering design [1], supply chain management [2], healthcare [3] and increasingly, the tuning of hyperparameters in complex machine learning models [4]. The prevalence of SO stems from its power to identify optimal parameter settings or system configurations by directly interacting with simulation models, which often represent the only viable way to capture the intricacies and stochastic nature of real-world systems when analytical models are intractable [5]. However, the practical application of SO is frequently confronted by significant challenges. Many high-fidelity simulations are computationally expensive, demanding substantial time or resources for each evaluation. These simulations are often treated as black-box functions, meaning their internal analytical structure is either unknown or too complex to be exploited directly for optimization, thus preventing the direct calculation of derivatives [6]. Also, the outputs derived from stochastic simulations are inherently noisy due to the underlying random components within the model. This noise complicates the assessment of candidate solutions true performance, making it difficult to reliably distinguish genuine improvements from mere statistical fluctuations based on limited number of simulation runs [7]. In addition, the search landscapes encountered in SO problems are frequently highly complex. They can be high-dimensional, contain numerous local optima (i.e., be multimodal), and lack readily available or meaningful gradient information. This complexity renders standard gradient-based optimization techniques ineffective and necessitates the use of algorithms capable of global search without relying on derivative information [8]. variety of approaches have been developed to tackle the challenges inherent in SO. Among the most prominent are Surrogate-Based Optimization (SBO) methods, which aim to mitigate the high computational cost of simulations by constructing cheaper-to-evaluate approximation models, or surrogates [9]. Gaussian Processes (GPs), often employed within Bayesian Optimization (BO) framework, are frequently used for this purpose due to their ability to capture uncertainty in the approximation [10]. However, constructing and tuning effective surrogate models, especially for TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems high-dimensional or highly complex functions, can be computationally intensive itself and sensitive to the choice of kernel functions and hyperparameters [11]. Another significant class of methods, particularly relevant when comparing finite number of system designs, falls under the umbrella of Ranking and Selection (R&S) procedures [12]. These statistical methods focus on efficiently allocating simulation budget to identify the best among discrete set of alternatives with certain level of statistical confidence [13]. While powerful in their domain, R&S procedures are primarily suited for problems with discrete or relatively small number of candidate solutions and are less directly applicable to the continuous or high-dimensional search spaces often encountered in optimization. Metaheuristics represent third major category, offering general-purpose stochastic search strategies applicable to complex black-box problems. Common examples used in SO include Genetic Algorithms (GA) [14], Simulated Annealing (SA) [15], and Particle Swarm Optimization (PSO) [16]. These methods often excel at global exploration but can suffer from premature convergence to local optima, particularly in the presence of significant simulation noise [17]. Also, their performance can be highly sensitive to the choice of algorithmic hyperparameters (e.g., population size, mutation rates, cooling schedules), often requiring substantial tuning effort, which can itself become an optimization problem [5]. Distinct from these, Tabu Search (TS) is metaheuristic optimization strategy characterized by its intelligent use of memory structures [18]. Central to TS is the concept of Tabu List, which records recently visited solutions or moves, temporarily forbidding them to prevent the search from cycling and to encourage exploration of unvisited areas of the solution space. This memory-based mechanism is specifically designed to help search processes escape the confines of local optima, common pitfall for many other heuristic methods. The integration of such memory-based strategies offers promising direction for enhancing the robustness and exploratory capabilities of SO algorithms. Addressing the limitations of existing approaches and benefiting from the strengths of memory-based heuristics, this paper introduces Tabu-Enhanced Simulation Optimization (TESO), novel algorithm specifically designed for complex and noisy SO problems. The core idea behind TESO is to dynamically balance the fundamental trade-off between exploration, which involves discovering potentially new and promising regions of the vast solution space, and exploitation, which focuses on carefully refining solutions within regions already identified as high-performing. This research is motivated by the central research question: Can the integration of TS principles, Elite Memory structures, and adaptive perturbation strategies within unified SO framework effectively balance exploration and exploitation to reliably converge towards high-quality solutions, particularly when dealing with noisy, potentially multimodal, and computationally expensive simulation environments? While TS and elite set strategies are known in deterministic optimization, their application to stochastic SO presents unique challenges. Standard TS assumes deterministic objective evaluations, making concepts like best move and aspiration straightforward. In SO, these must be redefined to operate on noisy estimates. TESOs novelty is not in inventing these memory structures, but in adapting and integrating them into cohesive framework for the SO context. Specifically, it uses dual-memory approach which is explicitly designed to balance the search process under the uncertainty inherent to simulation. To address the research questions, this paper makes the following primary contributions to the field of SO: (i) The proposal of TESO, novel SO algorithm that synergistically integrates Tabu Search principles (tabu list, aspiration criterion) with Elite Memory and adaptive noise control, specifically tailored for noisy, complex black-box simulation environments. (ii) An empirical demonstration of TESOs effective balancing of exploration and exploitation. We show how tabu-driven diversification and random sampling interact with elite-guided intensification and adaptive perturbation to navigate challenging stochastic landscapes, leading to improved solution quality and reliability. The remainder of this paper is structured as follows. Section 2 provides detailed review of relevant background concepts. Section 3 presents the methodology of the proposed TESO algorithm, elaborating on its components. Section 4 details the numerical experiments conducted to evaluate TESOs performance. Finally, Section 5 concludes the paper by summarizing the key findings, acknowledging limitations, and suggesting potential directions for future research in this area."
        },
        {
            "title": "2 Background and Related Work",
            "content": "2.1 Simulation Optimization (SO) SO refers to the process of finding optimal input parameters or decision variables, denoted by vector x, for system whose performance is evaluated using computer simulation model [19]. The goal is typically to minimize or maximize an objective function, J(x), which represents the expected value of some performance measure obtained from the 2 TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems simulation output, (x, ω) [20]. Formally, the problem can often be expressed as: min xX J(x) = Eω[f (x, ω)] (1) or equivalently for maximization, where represents the vector of decision variables, is the feasible set of decisions, (x, ω) is the performance measure output from single simulation run given input and realization ω of the random elements within the simulation, and Eω[] denotes the expectation taken over the probability space governing the stochastic components ω. key characteristic of many SO problems is that the simulation model acts as black box, meaning an explicit analytical expression for J(x) or its gradients is often unavailable [21]. The application of optimization techniques within this simulation context presents several distinct and significant challenges. First and foremost is the need for handling stochastic responses [9]. Since the simulation output (x, ω) for any given is random variable due to the inherent randomness ω within the simulation model, single simulation run provides only noisy estimate of the true expected performance J(x). Relying on single, noisy observations can lead to incorrect assessments of solution quality and poor optimization decisions. Accurately estimating J(x) typically requires multiple simulation replications for each candidate solution x, which directly impacts the overall computational effort. This inherent noise complicates tasks such as estimating gradients (if applicable), comparing candidate solutions reliably, and determining convergence. Also, SO problems are often constrained by computational budget limitations [22]. High-fidelity simulations representing complex real-world systems can be computationally intensive, requiring significant time or resources for each execution. Consequently, the total number of simulation runs (across all candidate solutions and all replications per solution) that can be performed is often severely limited. This budget constraint forces careful allocation of computational effort and exacerbates the challenge posed by stochastic responses, as obtaining highly precise estimates of J(x) for many different values may be infeasible. In addition, SO algorithms must effectively balance the fundamental exploration versus exploitation dilemma [4]. Given the limited budget and noisy feedback, the algorithm must decide whether to spend resources investigating new, unexplored regions of the decision space (exploration) in the hope of discovering globally superior solutions, or to focus resources on refining solutions within regions already known to yield good performance (exploitation). Over-emphasizing exploration can waste valuable computational budget on evaluating unpromising areas, while overemphasizing exploitation risks premature convergence to locally optimal solution that may be significantly inferior to the true global optimum. Achieving an effective balance between these two competing objectives is critical for the success of any SO algorithm, particularly when dealing with the complex, potentially multimodal landscapes frequently encountered in simulation-based problems [23, 24]. TESO is designed specifically to address these intertwined challenges. 2.2 Metaheuristics in Simulation Optimization Given the black-box nature and the frequent absence of readily available gradient information that characterize many SO problems, metaheuristic algorithms offer compelling and widely adopted set of tools [4]. primary strength of these population-based or trajectory-based methods lies in their inherent ability to perform global search across the decision space, rather than being confined to local improvements like gradient-based methods [25]. They operate directly on candidate solutions and their observed (potentially noisy) objective function values, bypassing the need for derivative information or strong assumptions about the objective functions structure (e.g., convexity). This makes them particularly suitable for solving the complex, potentially multimodal, and rugged landscapes often characteristic of simulation models [26]. However, the effective application of metaheuristics in SO is not without its difficulties. significant challenge lies in parameter tuning; algorithms like GA (requiring settings for mutation and crossover rates, population size, selection mechanisms), SA (requiring an effective cooling schedule), and PSO (requiring inertia weights, cognitive and social parameters) possess numerous control parameters. The optimal settings for these parameters can be highly problem-dependent and non-trivial to determine priori, often requiring extensive preliminary experimentation or specific expertise [5]. Also, most metaheuristics are stochastic in nature and often lack formal convergence guarantees to the global optimum, especially within finite computational budget typically available in SO. While some theoretical convergence results exist under specific assumptions (e.g., for SA under infinitely slow cooling), practical guarantees of finding the true optimum in finite number of steps are rare. Despite their global search capabilities, these algorithms can still suffer from premature convergence, becoming trapped in sub-optimal regions of the search space. The inherent stochasticity of simulation outputs adds another layer of complexity; noise can obscure the true underlying landscape, making fitness or objective evaluations unreliable and potentially misleading the search direction, which can exacerbate issues like premature convergence or difficulty in 3 TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems discerning truly superior solutions. Despite these weaknesses, metaheuristics remain popular and often effective approach for SO due to their flexibility, general applicability to challenging black-box problems, and potential for finding high-quality solutions when gradient-based or analytical methods are infeasible. TESO aims to retain the exploratory strengths while mitigating some weaknesses, particularly premature convergence, through its specific memory structures. 2.3 Memory-Based Heuristics: Tabu Search and Elite Memory TS is prominent metaheuristic that exemplifies the strategic use of adaptive memory, particularly for escaping local optima in complex search spaces [18]. Its core mechanism is the Tabu List, short-term memory that records recent moves or solution attributes, temporarily forbidding their reversal for set duration (Tabu Tenure). This prevents cycling and promotes diversification by forcing the search into new areas. TS iteratively explores solution neighborhoods, selecting the best permissible move (non-tabu, or tabu but meeting an aspiration criterion, such as finding new global best) even if it results in temporary worsening of the objective function. This non-improving move capability, combined with longer-term strategies for exploitation and exploration, has made TS highly effective [27]. The broader concept of leveraging memory extends beyond TS, with many advanced heuristics incorporating mechanisms to learn from search history. common and powerful form is the use of long-term memory to store collection of elite solutionsthe best candidates found so far. This elite set serves as basis for exploitation. Other forms of memory might track historical attribute frequencies or operator performance to adaptively bias the search. By incorporating memory, whether the short-term restrictive memory of Tabu List or the long-term guiding memory of an elite set, heuristics can achieve more effective balance between exploration and exploitation, leading to faster convergence and increased robustness, particularly in the noisy and complex landscapes characteristic of SO [28]."
        },
        {
            "title": "3 TESO: Algorithm Methodology",
            "content": "3.1 Framework Overview TESO is positioned as direct search metaheuristic specifically adapted for noisy, black-box SO. It improves upon standard metaheuristics by integrating adaptive memory structures inspired by Tabu Search (TS) to better navigate complex, uncertain landscapes. TESO accommodates simulation noise by operating on mean estimates derived from multiple replications. Key adaptations include: Tabu List storing recent candidate representations to prevent reevaluation and encourage diversification despite noise; stochastic aspiration criterion allowing tabu candidates to be accepted if their estimated performance surpasses the current best; and an Elite Memory storing top-performing candidates based on their mean estimates, which provides robust starting points for intensification via perturbation. When contrasted with standard TS, TESO differs significantly in its neighborhood exploration and evaluation. Standard TS often relies on explicitly defined, discrete neighborhood structures (e.g., adjacent swaps, bit flips). TESO, on the other hand, employs an implicit neighborhood structure defined by adaptive random perturbation around elite solutions. The search step is probabilistic rather than deterministic evaluation of all neighbors. Also, TESO evaluates candidates using multiple simulation replications to handle noise, whereas standard TS typically assumes deterministic function evaluations. This distinction is critical. In deterministic setting, move is unambiguously improving or not. In TESO, all comparisons are based on mean estimates from multiple replications, which are themselves random variables. Therefore, the aspiration criterion and improvement checks (detailed later) are inherently stochastic, key adaptation for the SO domain. Compared to other metaheuristics like GA, SA, or PSO commonly used in SO, TESOs explicit use of both short-term restrictive memory (Tabu List) and long-term guiding memory (Elite Memory) is key differentiator. While GAs maintain diversity through populations and PSO uses particle/global bests (a form of Elite Memory), they typically lack the systematic short-term avoidance mechanism provided by Tabu List. Basic SA is generally memoryless. Also, TESO should be clearly distinguished from SBO and BO approaches. SBO/BO methods construct an explicit statistical model (the surrogate, e.g., Gaussian Process) of the underlying simulation response. They use this surrogate, along with an acquisition function (like Expected Improvement or Upper Confidence Bound), to intelligently select the next point(s) to evaluate with the expensive simulation, aiming to improve the surrogate model and find the optimum efficiently. TESO, in contrast, is direct search method. While it uses memory, it does not build an explicit global model of the objective function; it operates directly on the (replicated) outputs of the simulation model itself to guide its search trajectory. 4 TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems Figure 1: Flow Diagram of the TESO Algorithm, illustrating the iterative cycle including candidate generation, tabu/aspiration filtering, evaluation, and memory updates. 3.2 Algorithm Methodology TESO algorithm (Algorithm 1) begins by initializing essential components (line 2): the Tabu List (T ) and Elite Memory (E) are cleared, the best objective value found so far (fbest) is set to an appropriate initial value (e.g., infinity for minimization), the adaptive noise level (η) is set to its starting value (ηinit), and iteration counters (t, t) are initialized. The core logic resides within loop iterating up to the total trial budget (line 4). Each iteration commences with Candidate Generation (lines 5-9). During the initial ninit iterations or with small probability pdiv thereafter, the algorithm performs diversification by generating candidate x(t) randomly (line 6). Otherwise, it engages in intensification: an elite solution xe is selected from E, and new candidate x(t) is created by perturbing xe using the current noise level η (line 8). fallback to random generation occurs if is empty. unique, hashable representation h(t) of the generated candidate is then created (line 9). Since our test problem uses continuous decision variable, this representation can be created by discretizing the variable into bins or by applying hash function to its string representation. Before proceeding to potentially costly evaluation, the candidate undergoes Tabu Check and Aspiration step (lines 11-14). If the candidates representation h(t) is found in the Tabu List and it does not satisfy the predefined aspiration criterion (i.e., it is not expected to significantly improve upon fbest), the candidate is discarded, and the algorithm proceeds directly to the next iteration via the continue statement (line 13). If the candidate x(t) is not tabu or satisfies the aspiration criterion, it is subjected to Stochastic Evaluation (line 16). This involves running nrep independent simulation replications using x(t) as input. The mean performance µ(t) and its standard deviation σ(t) are calculated from these replications, providing statistically robust estimate of the candidates objective value. Note that while the standard deviationσ(t) is calculated, it is not directly used in the decision logic of the current TESO implementation. It is recorded for analysis and could be used in more advanced versions of the algorithm, for instance, to dynamically adjust the number of replications or to inform risk-based aspiration criterion. Following evaluation, the algorithm performs Updates to the Best Solution and Memory Structures (lines 18-22). It checks if the evaluated mean µ(t) constitutes an improvement over the current fbest. If an improvement is found, fbest and the corresponding best solution xbest are updated, and the no-improvement counter is reset (line 20). If no improvement occurred (and the algorithm is past the initialization phase), is incremented (line 21). Regardless of improvement, the candidates representation h(t) is added to the Tabu List , and the candidate-performance pair (x(t), µ(t)) is added to the Elite Memory E, maintaining their respective capacity limits (line 22). Within the loop, Adaptive Noise Control and Termination Check occur (lines 24-25). The noise level η used for perturbation is updated according to predefined schedule (e.g., decaying over iterations). The algorithm then checks if the termination criteria are met: either the total trial budget is exhausted (implicit in the For loop) or the number of iterations without improvement has reached the maximum allowed tmax. If the latter occurs, the loop terminates early via the break statement (line 25). Once the loop finishes, the algorithm returns the best solution xbest and its corresponding estimated objective value fbest (line 27). Figure 1 provides visual summary of the TESO methodology detailed in Algorithm 1. The diagram illustrates the flow starting from initialization, proceeding through the main iterative loop. Key stages depicted include the adaptive 5 TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems else x(t) RandCand(X ) if ninit or Random() < pdiv then end if h(t) Represent(x(t)) xe SelectElite(E); x(t) Perturb(xe or RandCand(), η) if h(t) and not AspirCritMet(x(t), fbest, direction) then Algorithm 1 TESO Algorithm Require: (x, ω), , , ninit, nrep, ηinit ηfinal, CT , CE , pdiv, tmax, direction. Ensure: Best solution xbest, Best objective fbest. 1: Init , E, fbest, η ηinit, 0, xbest null. 2: for = 1 to do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: end for 27: Return: xbest, fbest. else if > ninit then t + 1 end if Add h(t) to ; Add (x(t), µ(t)) to η UpdateNoise(t, T, ηinit, ηfinal) if tmax then break end if if IsImprovement(µ(t), fbest, direction) then fbest µ(t); xbest x(t); (µ(t), σ(t)) Evaluate(x(t), f, nrep) continue end if Initialize memory, best value, noise, counters Generate Candidate x(t) Diversify Intensify Check Tabu & Aspiration Skip tabu, non-aspirated Evaluate Candidate Run nrep sims, get mean/std Update Best & Memory Update memories Adapt Noise & Check Termination Stop if no improvement candidate generation (switching between diversification via random sampling and intensification via elite perturbation), the critical filtering step based on the Tabu List (T ) and aspiration criteria, the stochastic evaluation using multiple replications, and the subsequent updates to the best solution, both memory structures (T , E), and control parameters like noise level (η) before checking termination conditions."
        },
        {
            "title": "4 Numerical Experiments",
            "content": "4.1 Experimental Objectives The numerical experiments herein aim to empirically validate the proposed TESO algorithms effectiveness for noisy SO problems. Our goals are fourfold: first, to demonstrate TESOs capability to converge reliably to high-quality solutions within typical budget constraints despite stochastic evaluations; second, to quantify the impact of its core memory components (Tabu List, Elite Memory) by comparing the full algorithm against ablation variants lacking one of these structures; third, to benchmark TESOs final solution quality, convergence speed, and reliability against baseline methods; and fourth, to analyze how TESOs mechanisms dynamically balance exploration and exploitation during the search. These experiments collectively seek to establish TESO as an effective framework for challenging SO problems. 4.2 Test Problem: M/M/k Queue Optimization To evaluate the performance of the TESO algorithm on multiple-server M/M/k queueing system. This system is characterized by Poisson arrivals (M), exponentially distributed service times (M) shared across identical parallel servers (k), and typically assumes an infinite buffer capacity operating under First-In, First-Out (FIFO) discipline. The key parameters governing its behavior are the mean arrival rate, λ, the number of servers, k, and the mean service rate, µ, for each individual server. For stability, the system requires the total service capacity to exceed the arrival rate, i.e., µ > λ. In this optimization context, the decision variable, x, remains the service rate µ provided by each of the identical servers. We assume this rate can be controlled within predefined feasible range [µmin, µmax], where µmin > λ must hold for stability. The objective function, J(µ), aims to minimize combination of operational performance TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems (customer waiting time) and the cost associated with providing service capacity. Specifically, we seek to minimize the sum of the expected steady-state average sojourn time (W ) experienced by customers (time spent waiting in queue plus time in service) and cost penalty proportional to the square of the individual server service rate, aggregated across all servers. The quadratic cost reflects the increasing marginal expense of providing faster service per server. Therefore, the objective function to be minimized is: J(µ) = E[W (k, µ, ω)] + µ2 (2) where E[W (k, µ, ω)] is the true expected average sojourn time when servers operate at rate µ, and is positive cost coefficient applied per server. For our experiments, we set the system parameters to create non-trivial scenario: Arrival Rate: λ = 2.5 customers per unit time, Number of Servers: = 3, Cost Coefficient: = 0.5, Feasible Service Rate Range: The stability condition is 3µ > 2.5, or µ > 0.833.... We define the feasible range for the individual server service rate as µ [1.0, 4.0]. This M/M/3 objective function retains the key characteristics relevant for testing TESO: Stochastic / Noisy Evaluation: While complex analytical formulas exist for M/M/k steady-state probabilities (Erlang formula), directly calculating the mean sojourn time E[W (k, µ, ω)] can be cumbersome or impractical within an optimization loop. It is typically estimated by running discrete-event simulation of the M/M/3 system and averaging observed sojourn times. Random arrival and service processes (ω) ensure each simulation yields noisy estimate ˆW (k, µ, ω), making the evaluated objective ˆJ(µ) = 1 nrep ˆW (k, µ, ωj) + µ2 inherently stochastic. (cid:80)nrep j=1 Black-Box Nature (from Optimizers Perspective): TESO treats the simulation providing ˆW (k, µ, ω) as black box, using only the input µ and the noisy output ˆJ(µ). Complex Landscape: Although the underlying trade-off between reducing waiting time (higher µ) and increasing cost (higher µ) likely results in generally convex-like shape for J(µ). Also, the noise present in the simulation-based evaluation creates rugged landscape for the optimizer, posing significant challenge for convergence and requiring robust handling of noisy feedback. It is important to note that this M/M/3 problem is expected to be unimodal. While it effectively tests the algorithms ability to handle noise and balance exploration and exploitation in simple setting, it does not challenge their capacity to escape distinct local optima. The primary benefit of the Tabu List observed here will relate to preventing cycling in noisy regions rather than overcoming multimodality 4.3 Benchmark Algorithms for Comparison We compare TESOs results against several benchmark algorithms to evaluate the performance and understand the contributions of the different components within the proposed TESO framework. These benchmarks are chosen to represent both baseline level of performance and to facilitate an ablation study isolating the effects of TESOs core memory structures. The algorithms used for comparison are: Pure Random Sampling (PRS): This serves as the most basic benchmark. In PRS, candidate solutions x(t) are generated purely by sampling uniformly at random from the feasible decision space for the entire duration of the optimization run (T trials). No memory or adaptive search strategy is employed. PRS provides baseline against which the added value of any intelligent search mechanism, including TESOs, can be measured. Its performance indicates the difficulty of finding good solutions simply by chance within the given budget. TESO without Tabu List (TESO-noTabu): This variant is identical to the full TESO algorithm (Algorithm 1) except that the Tabu List mechanism is disabled. Comparing full TESO against TESO-noTabu allows us to directly assess the impact and benefit of the short-term tabu memory on preventing cycling, encouraging diversification, and potentially improving the final solution quality or convergence speed. TESO without Elite Memory (TESO-noElite): This variant mirrors the full TESO algorithm but removes the influence of the Elite Memory on candidate generation during the intensification phase. Instead of selecting an elite candidate xe from and perturbing it, this variant might, for example, always perturb the single current best-known solution xbest or simply rely more heavily on the diversification mode (random generation). The comparison between full TESO and TESO-noElite aims to quantify the contribution of the long-term Elite Memory in effectively guiding the intensification process and exploiting promising regions identified during the search. We have intentionally omitted comparisons to other common metaheuristics like SA or GA in this study. fair comparison would require extensive hyperparameter tuning for each method, which itself constitutes difficult optimization problem and could obscure the specific contributions of TESOs memory components. Our ablation study, TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems therefore, provides more controlled analysis of the direct impact of the Tabu List and Elite Memory, which is the primary focus of this paper. 4.4 Experimental Design To conduct fair and comprehensive evaluation of TESO, we define consistent experimental setup for optimizing the M/M/3 queue problem (k = 3, λ = 2.5, = 0.5, µ [1.0, 4.0]). Each algorithm (TESO and benchmarks) is run for total budget of = 300 candidate evaluations, with the first ninit = 20 dedicated to random initialization. Candidate solutions are evaluated using nrep = 30 independent simulation replications to estimate the mean objective and its standard deviation. For statistical robustness, all results are averaged over Nmacro = 30 independent macroreplications. TESO utilizes specific parameter settings: linearly decaying noise schedule for perturbations (ηinit = 0.2 to ηf inal = 0.01), Tabu List capacity CT = 15, an Elite Memory capacity CE = 10, diversification probability pdiv = 0.2, and termination if no improvement occurs in tmax = 50 iterations. The benchmark algorithms adapt this setup: PRS omits memory and adaptive search; TESO-noTabu disables the Tabu List (CT = 0); and TESO-noElite modifies intensification to not rely on the elite set E. All algorithms operate on the same queue problem defined in Section 4.2. Performance is assessed using several key metrics across the 30 macro-replications. We report the average and standard deviation of the Best Mean Objective Value Found (f best) upon termination to gauge final solution quality and reliability. The stability of performance near the end of the search is measured by the Average Objective Value over the Last 50 Trials. Convergence Plots illustrate the progression of the average best-found solution over the trial budget, visually comparing speed and consistency (often including standard error bands). Finally, we record the average Computational Time per macro-replication as practical measure of efficiency. These metrics collectively allow for thorough comparison of the effectiveness, reliability, and efficiency of the algorithms. 4.5 Results and Discussion The performance of the proposed TESO algorithm and the selected benchmarks are evaluated based on the queue optimization problem described in Section 4.2. The results, averaged over Nmacro = 30 independent macro-replications, are presented numerically in Table 1 and visually through the convergence plots in Figure 2. Comparison against Benchmarks As expected, PRS demonstrates the weakest performance, achieving the highest (worst) final best mean objective value (approximately 4.11) with the largest standard deviation (0.20), indicating low solution quality and poor reliability. The convergence plot for PRS shows slow improvement, confirming its inefficiency as search strategy. In contrast, all TESO variants significantly outperform PRS. The full TESO algorithm achieves the best overall performance, converging to the lowest final best mean objective value (2.53), which is very close to the assumed true optimum (2.52). Furthermore, TESO exhibits the lowest standard deviation for the final best value (0.07) across the macro-replications, highlighting its superior reliability and consistency in finding high-quality solutions compared to all benchmarks. While TESO incurs slightly higher average computational time compared to its ablation variants, this is justifiable given the significant improvement in solution quality and reliability. The convergence plot (Figure 2) confirms TESOs faster convergence towards better solutions compared to the other methods. Impact of Memory Components (Ablation Study) Comparing the full TESO algorithm with its variants allows us to analyze the specific contributions of the Tabu List and Elite Memory. Effect of Tabu List: Comparing TESO with TESO-noElite (which has Tabu but no Elite guidance for intensification) against PRS shows the baseline benefit of using memory. Comparing TESO (full) with TESO-noTabu reveals the advantage conferred by the tabu mechanism. TESO-noTabu achieves respectable final objective value (2.72), substantially better than PRS, demonstrating the effectiveness of elite-guided intensification. However, it performs worse than the full TESO (2.53) and exhibits higher variability (std dev 0.16 vs 0.07). This suggests that without the Tabu List, the search, while guided by elite solutions, may indeed spend unnecessary effort revisiting recently explored areas or get temporarily stuck near elite candidates, hindering diversification and slowing convergence to the very best solutions. The Tabu List effectively mitigates this. Effect of Elite Memory: Comparing TESO with TESO-noTabu (which has Elite but no Tabu) against PRS shows the benefit of intensification. Comparing TESO (full) with TESO-noElite highlights the role of guided intensification. TESO-noElite performs better than PRS but worse than both TESO-noTabu and the full TESO (final mean obj. 2.89, std dev 0.21). This indicates that while the Tabu List helps diversification, the lack of targeted intensification based on pool of elite solutions (instead potentially perturbing only the single best or relying more on random jumps) makes the exploitation phase less efficient. The Elite Memory is crucial for effectively focusing the search and refining TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems Figure 2: Convergence Plot for Queue Optimization high-potential regions identified during exploration. The results strongly suggest that both the short-term tabu memory and the long-term Elite Memory play vital and complementary roles in TESOs performance. The Tabu List enhances exploration and prevents stagnation, while the Elite Memory effectively guides exploitation, leading to synergistic effect in the full TESO algorithm that outperforms either component used in isolation (within the TESO framework). Exploration and Exploitation Balance The convergence plots provide insights into the exploration-exploitation dynamics. PRS represents pure exploration with no learning. TESO-noElite, lacking strong guidance for exploitation, shows slower convergence after the initial phase compared to methods using Elite Memory. TESO-noTabu shows faster initial convergence due to elite guidance but potentially plateaus earlier or at slightly higher level than full TESO, possibly due to insufficient diversification caused by revisiting areas near elite solutions. The full TESO algorithm exhibits desirable pattern: period of exploration (similar slope to others initially, possibly slightly slower than TESO-noTabu if elite guidance is very strong early on), followed by phase of rapid improvement (steeper slope) as intensification guided by Elite Memory takes over, and finally converging reliably to the best solution region with low variance. This behavior suggests that the combination of tabu diversification, elite intensification, and adaptive noise successfully manages the exploration-exploitation trade-off for this noisy SO problem. Table 1: Performance Comparison of TESO and Benchmark Algorithms. Algorithm PRS TESO-noElite TESO-noTabu TESO Final Best Mean Obj. Final Best Std Dev Avg Obj. (Mean Std Dev) Avg Comp. Time (s) 4.11 2.89 2.72 2. 0.23 0.21 0.16 0.07 4.110.20 2.890.18 2.720.13 2.530.06 53.34 101.43 113.52 132."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "This paper introduced TESO, novel metaheuristic framework tailored for optimizing noisy, expensive black-box simulation models. TESOs methodology combines adaptive random search with short-term Tabu List to prevent cycling and long-term Elite Memory to guide intensification around promising solutions, employing an aspiration criterion and adaptive noise control. Our key experimental results on queue optimization problem demonstrated TESOs effectiveness, showing consistent convergence to high-quality solutions with better reliability (lower variance) compared to PRS and ablation variants lacking either memory component. The study confirmed the synergistic benefit of integrating both tabu restrictions and elite guidance, leading to successful balance between exploration 9 TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems and exploitation. The primary contributions include this novel integration of TS principles and Elite Memory for stochastic optimization, the empirical demonstration of improved performance, and structured approach to managing the exploration-exploitation trade-off. However, we acknowledge limitations such as validation on single, unimodal problem class, potential sensitivity to TESOs internal parameters, and untested scalability in very high-dimensional settings. Furthermore, while we criticize other metaheuristics for their sensitivity to hyperparameters, TESO is not immune. Its performance depends on parameters and the noise decay schedule. full sensitivity analysis was beyond the scope of this initial study but is critical direction for future work. Finally, the algorithms scalability in very high-dimensional settings remains untested. Future work will focus on three key areas. First, we will test TESOs generalizability on wider range of benchmark problems with higher dimensions and multimodality. Second, we aim to enhance the algorithm by developing adaptive mechanisms for its internal parameters and exploring hybridization with surrogate models. Finally, pursuing theoretical analysis of its convergence properties would provide important formal grounding."
        },
        {
            "title": "References",
            "content": "[1] Geng Deng. Simulation-based optimization. PhD thesis, University of WisconsinMadison, 2007. [2] June Young Jung, Gary Blau, Joseph Pekny, Gintaras Reklaitis, and David Eversdyk. simulation based optimization approach to supply chain management under demand uncertainty. Computers & chemical engineering, 28(10):20872106, 2004. [3] Tarun Mohan Lal, Thomas Roh, and Todd Huschka. Simulation based optimization: Applications in healthcare. In 2015 winter simulation conference (WSC), pages 12611271. IEEE, 2015. [4] Satyajith Amaran, Nikolaos Sahinidis, Bikram Sharda, and Scott Bury. Simulation optimization: review of algorithms and applications. Annals of Operations Research, 240:351380, 2016. [5] Nazanin Nezami and Hadis Anahideh. Hyperparameter adaptive search for surrogate optimization: self-adjusting approach. In 2023 Winter Simulation Conference (WSC), pages 34843495, 2023. [6] Eylem Tekin and Ihsan Sabuncuoglu. Simulation optimization: comprehensive review on theory and applications. IIE Transactions, 36(11):10671081, 2004. [7] M.C. Fu. tutorial review of techniques for simulation optimization. In Proceedings of Winter Simulation Conference, pages 149156, 1994. [8] Gonçalo Figueira and Bernardo Almada-Lobo. Hybrid simulationoptimization methods: taxonomy and discussion. Simulation modelling practice and theory, 46:118134, 2014. [9] Jeff Hong and Xiaowei Zhang. Surrogate-based simulation optimization. In Tutorials in Operations Research: Emerging Optimization Methods and Modeling Techniques with Applications, pages 287311. INFORMS, 2021. [10] Michael Pearce, Matthias Poloczek, and Juergen Branke. Bayesian simulation optimization with common random numbers. In 2019 Winter Simulation Conference (WSC), pages 34923503, 2019. [11] Weiwei Fan, Jeff Hong, Guangxin Jiang, and Jun Luo. Review of large-scale simulation optimization. arXiv preprint arXiv:2403.15669, 2024. [12] Justin Boesel, Barry Nelson, and Seong-Hee Kim. Using ranking and selection to clean up after simulation optimization. Operations Research, 51(5):814825, 2003. [13] Jeff Hong, Weiwei Fan, and Jun Luo. Review on ranking and selection: new perspective. Frontiers of Engineering Management, 8(3):321343, 2021. [14] Rafael de Carvalho Miranda, José Arnaldo Barra Montevechi, and Alexandre Ferreira de Pinho. Development of an adaptive genetic algorithm for simulation optimization. Acta Scientiarum. Technology, 37(3):321328, 2015. [15] Jorge Haddock and John Mittenthal. Simulation optimization using simulated annealing. Computers & industrial engineering, 22(4):387395, 1992. [16] RJ Kuo and CY Yang. Simulation optimization using particle swarm optimization algorithm with application to assembly line design. Applied Soft Computing, 11(1):605613, 2011. [17] Benaissa Brahim, Masakazu Kobayashi, Musaddiq Al Ali, Tawfiq Khatir, and Mohamed El Amine Elaissaoui Elmeliani. Metaheuristic optimization algorithms: An overview. HCMCOU Journal of ScienceAdvances in Computational Structures, 2024. [18] Fred Glover. Tabu search: tutorial. Interfaces, 20(4):7494, 1990. [19] Sigrún Andradóttir. Simulation optimization. Handbook of simulation, pages 307334, 1998. 10 TESO: Tabu-Enhanced Simulation Optimization for Noisy Black-Box Problems [20] Abhijit Gosavi et al. Simulation-based optimization, volume 62. Springer, 2015. [21] Hao Cao, Jian-Qiang Hu, and Teng Lian. Blackbox simulation optimization. Journal of the Operations Research Society of China, pages 127, 2024. [22] Chun-Hung Chen and Loo Hay Lee. Stochastic simulation optimization: an optimal computing budget allocation, volume 1. World scientific, 2011. [23] Emmanuel Nsiye, Tori Wright, Brian Tse, and Sean Mondesire. micro-discrete event simulation environment for production scheduling in manufacturing digital twins. In MODSIM World, Norfolk, Virginia, May 2024. [24] Bulent Soykan and Ghaith Rabadi. Optimizing job shop scheduling problem through deep reinforcement learning In Proceedings of the 2024 Winter Simulation Conference (WSC24), pages and discrete event simulation. 26072618, 2024. [25] Bulent Soykan and Ghaith Rabadi. simulation-based optimization approach for multi-objective runway operations scheduling. SIMULATION: Transactions of the Society for Modeling and Simulation International, 98(11):9911012, 2022. [26] Benjamin Thengvall, Shane Hall, and Michael Deskevich. Measuring the effectiveness and efficiency of simulation optimization metaheuristic algorithms. Journal of Heuristics, 31(1):121, 2025. [27] Bulent Soykan and Ghaith Rabadi. tabu search algorithm for the multiple runway aircraft scheduling problem. In Ghaith Rabadi, editor, Heuristics, Metaheuristics and Approximate Methods in Planning and Scheduling, International Series in Operations Research & Management Science, pages 165186. Springer International Publishing, Cham, 2016. [28] Chunlong Yu, Nadia Lahrichi, and Andrea Matta. Optimal budget allocation policy for tabu search in stochastic simulation optimization. Computers & Operations Research, 150:106046, 2023."
        }
    ],
    "affiliations": [
        "University of Central Florida"
    ]
}