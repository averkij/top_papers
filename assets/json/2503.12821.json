{
    "paper_title": "From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration",
    "authors": [
        "Mingyang Song",
        "Xiaoye Qu",
        "Jiawei Zhou",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Vision-Language Models (LVLMs) have achieved significant progress in combining visual comprehension with language generation. Despite this success, the training data of LVLMs still suffers from Long-Tail (LT) problems, where the data distribution is highly imbalanced. Previous works have mainly focused on traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as recognition and classification. Nevertheless, the exploration of LVLM (e.g. LLaVA) and more general tasks (e.g. Visual Question Answering and Visual Reasoning) remains under-explored. In this paper, we first conduct an in-depth analysis of the LT issues in LVLMs and identify two core causes: the overrepresentation of head concepts and the underrepresentation of tail concepts. Based on the above observation, we propose an $\\textbf{A}$daptive $\\textbf{D}$ata $\\textbf{R}$efinement Framework ($\\textbf{ADR}$), which consists of two stages: $\\textbf{D}$ata $\\textbf{R}$ebalancing ($\\textbf{DR}$) and $\\textbf{D}$ata $\\textbf{S}$ynthesis ($\\textbf{DS}$). In the DR stage, we adaptively rebalance the redundant data based on entity distributions, while in the DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and scarce images to supplement underrepresented portions. Through comprehensive evaluations across eleven benchmarks, our proposed ADR effectively mitigates the long-tail problem in the training data, improving the average performance of LLaVA 1.5 relatively by 4.36%, without increasing the training data volume."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 2 1 2 8 2 1 . 3 0 5 2 : r From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration Mingyang Song1,2, Xiaoye Qu2, Jiawei Zhou3, Yu Cheng4 1Fudan University, 2Shanghai Artificial Intelligence Laboratory 3Stony Brook University, 4The Chinese University of Hong Kong mysong23@m.fudan.edu.cn; quxiaoye@pjlab.org.cn; jiawei.zhou.1@stonybrook.edu; chengyu@cse.cuhk.edu.hk;"
        },
        {
            "title": "Abstract",
            "content": "Large Vision-Language Models (LVLMs) have achieved significant progress in combining visual comprehension with language generation. Despite this success, the training data of LVLMs still suffers from Long-Tail (LT) problems, where the data distribution is highly imbalanced. Previous works have mainly focused on traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as recognition and classification. Nevertheless, the exploration of LVLM (e.g. LLaVA) and more general tasks (e.g. Visual Question Answering and Visual Reasoning) remains under-explored. In this paper, we first conduct an in-depth analysis of the LT issues in LVLMs and identify two core causes: the overrepresentation of head concepts and the underrepresentation of tail concepts. Based on the above observation, we propose an Adaptive Data Refinement Framework (ADR), which consists of two stages: Data Rebalancing (DR) and Data Synthesis (DS). In the DR stage, we adaptively rebalance the redundant data based on entity distributions, while in the DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and scarce images to supplement underrepresented portions. Through comprehensive evaluations across eleven benchmarks, our proposed ADR effectively mitigates the long-tail problem in the training data, improving the average performance of LLaVA 1.5 relatively by 4.36%, without increasing the training data volume. 1. Introduction Large Vision-Language Models (LVLMs) have become pivotal at the intersection of computer vision and natural language processing, facilitating wide range of applications. Recent advancements in LVLMs [1, 3, 6, 9, 12, 13, 26 28, 4042, 60, 67, 71] have significantly advanced generalpurpose foundation models, elevating them to unprecedented Corresponding authors (a) Performance over LLaVA 1.5. (b) Tail 30% Acc. on VQAV2. Figure 1. Performance before and after addressing the LT problem. Our method surpasses the baseline over all benchmarks and also effectively improves the performance of tail 30% concepts. levels. However, the training data of LVLMs are suffering from the problem of Long-Tail (LT) [37], which refers to the fact that the training datasets present highly imbalanced distributions, and they have large number of tail classes. As widely existing phenomenon in real-world distribution, some recent research [17, 58, 63, 68] found that balancing the LT data can bring positive effects. To achieve it, several works [21, 31, 45, 54, 70] attempt to filter the redundant data or re-design the network structure. However, these studies primarily focus on traditional models (e.g., CLIP [43]) or tasks (e.g., image classification). In contrast, the LT problem in LVLMs presents unique challenges due to their cross-modal nature, the involvement of multiple aspects, and the distinctive co-occurrence phenomenon, which still remains under-explored. To address the above issue, in this paper, we delve into the distribution of the training data, analyzing it from four perspectives based on both language and visual features, and introduce an Adaptive Data Refinement Framework (ADR). Our framework can be easily integrated into training data of any open-source LVLMs such as LLaVA [27], ShareGPT-4V Figure 2. The overview of our Adaptive Data Refinement Framework (ADR). (a) In the Analyzing Stage, we first extract tokens, objects, co-occurrences, and interrogations from the training instances, then construct corresponding distribution using reverse-indexed mapping. (b) In the Data Rebalancing stage, we analyze the optimizing direction and adaptively rebalance the redundant data based on the entity distribution identified in the Analyzing stage. (c) Finally, in the Data Synthesis stage, we utilize DDPM and the latent representations of scarce image instances to synthesize the underrepresented data. [7], ALLaVA [5], or Mini-GPT4V [71]. Specifically, ADR addresses the long-tail (LT) problem by both filtering redundant data and synthesis scarce data through three key stages: Analyzing Stage, Data Rebalancing Stage (DR) and Data Synthesis Stage (DS). Firstly, the Analyzing stage assesses the severity of LT problem and builds key entity distributions from four different perspectives, including tokens, objects, co-occurrences, and interrogations. Then, the DR stage rebalances the overrepresented head portion of the data by filtering out low-quality or redundant instances based on entity distributions, thereby mitigating overfitting to redundant head data. Finally, the DS stage leverages the latent representations of scarce images to adaptively synthesize the underrepresented tail data, significantly improving the performance on tail-end concepts of the model. To comprehensively evaluate our framework, we adopt diverse general-purpose benchmarks, hallucination metrics, and GPT-4 evaluations to validate the effectiveness. The results demonstrate that our proposed ADR framework significantly improves performance over baseline data and methods. As shown in Figure 1(a), across all 11 benchmarks, ADR consistently achieves better results, improving the average performance of LLaVA 1.5 relatively by 4.36%, without increasing the training data volume or introducing additional training stages. Furthermore, the performance on the tail data also observes significant improvement as depicted in Figure 1(b). To sum up, our contributions are threefold: We present the first in-depth analysis of the unique LT challenges in LVLMs from four key aspects across both vision and language modalities, including tokens, objects, co-occurrences, and interrogations. Our analysis reveals the significant LT problem within LVLMs training data. We introduce comprehensive Adaptive Data Refinement (ADR) framework to effectively mitigate the LT problem without increasing data volume or requiring additional training. ADR is both model-agnostic and data-agnostic, making it easily transferable to any open-source dataset. Comprehensive evaluation over general-purpose, hallucination, and GPT assessments proves the superiority of our framework. Moreover, we present the potential of our framework to serve as general toolkit as it can be easily transferred to any open-source data. 2. Related Work 2.1. Large Vision-Language Models Recently, Large Vision-Language Models (LVLMs) have attracted significant attention. Besides the powerful business models such as GPT-4V, GPT-4o, Gemini, and Claude [2, 35, 36, 52], many open-source LVLMs emerge [3, 12, 27, 71]. With the aid of strong large language models such as LLaMA [53] or Vicuna [10], and powerful vision encoder such as CLIP [43], they manage to align visual comprehension with remarkable language generation capabilities. However, all of the aforementioned LVLMs still face significant LT issues regardless of their structure or training phases. Therefore, this paper focuses on addressing the LT problems to facilitate the practical application of LVLMs. 2.2. Data Development of LVLMs The instruction-tuning data for LVLMs typically includes carefully crafted instructions designed to enhance the general instruction-following capabilities or improve downstream task performance of LVLMs. These instructions are often (a) MME-Tok (b) LLaVA-Tok (c) MME-Obj (d) LLaVA-Obj Figure 3. Long-tail distribution in instruction-tuning and benchmark datasets: (a) Token-level distribution in MME [16]. (b) Token-level distribution in InstructMix665K [28]. (c) Object-level distribution in MME [16]. (d) Object-level distribution in InstructMix665K [28]. generated by large language models (LLMs) like GPT-4 [28] or LVLMs such as GPT-4V [5, 4951, 57]. Notably, ShareGPT4V [7] is initially developed from 100K highquality captions collected from GPT-4V, which are later expanded to 1.2M using captioning model. Additionally, various data augmentation techniques are employed during LVLM development, such as random cropping and flipping for vision encoders [61] and projectors [24, 59], as well as wordand sentence-level augmentation for instruction tuning [4]. However, these augmentation methods often overlook the inherent distribution of the training data, leading to an inability to balance the data distribution effectively. Besides data acquisition and augmentation, there is also significant research on data filtering. For instance, Paul et al. [38] introduces two popular importance scores for effective data pruning. TIVE [31] leverages gradient-based importance scores to design data filtering pipeline. COINCIDE [21] examines the distributional differences between training sets and benchmark data, using smaller model as reference to select visual instruction tuning data for more efficient fine-tuning of the target LVLM. 2.3. Long Tail Analysis of VLMs Some recent studies [45, 48, 54, 64, 65, 69, 70] seek to mitigate imbalanced predictions of VLMs by training on additional data from downstream tasks. For example, MetaCLIP [56] analyzes the long tail problem of CLIP pre-training data and uses sub-string matching and inverted indexing to balance the pre-training dataset. GENSYNTH [47] generates balanced contrast sets through image editing to mitigate spurious correlations in vision-language models. REAL [37] analyzes the long tail problem of popular image recognition datasets and designs tail concept replacement method during the inference stage, significantly improving the recognition accuracy of VLMs. However, the long-tail problem within generative LVLMs is still under-explored. 3. Analysis 3.1. Preliminary . In this paper, we focus primarily on the LT problem in the instruction-tuning process of LVLMs. In this paper, we priTable 1. Relative data volume of tail data after reverse indexing. Tok, Obj, Co, and Int represent Token, Object, Cooccurrence, and Interrogation, respectively. %E denotes the percentage of tail entities, while %DI indicates the percentage of tail data instances. Data Level thres % % DI LLaVA [27] Avg. Tok Obj Co Int - 120 304 24 4895 - 98.7 98.0 92.7 99.6 97.25 10.0 10.0 25.0 10.0 13. Table 2. Relative training data volume for the tail 50% of failed cases. Tok, Obj, and Co refer to Token, Object, and Cooccurrence, respectively. %E denotes the percentage of tail entities, while %DI represents the percentage of tail data instances. Data Level thres % % DI MME [16] POPE [25] Avg. Tok Obj Co Tok Obj Co - 156 153 7448 120 90 3496 - 99.98 99.83 99. 99.98 99.71 99.54 99.68 75.23 51.33 69.62 80.50 59.76 75.45 68.65 marily investigate the LT problem in the instruction-tuning process of LVLMs. During this process, the training data is typically structured as = (I, C), where represents the image and denotes the corresponding conversation. 3.2. Entity Distribution Construction Specifically, we conduct the whole analysis procedure by constructing the frequency distribution of entities Qe from these four perspectives among the whole training set. The overall framework is as shown in Figure 2. Here we describe these four perspectives as below: Token entities are set of meaningful nouns that are extracted from the text within data instances across the whole training set. et = {nn Noun for (I, C) in D}. Technically, we employ Part of Speech (POS) parser to extract all nouns from each data instance within the training set, identifying them as token entities. (a) Token (b) Object (c) Co-occurrence Figure 4. Error accumulation curve of POPE and MME based on the training data distribution. It reveals that tail entities contribute to the majority of failure cases. (a) Token-level word distribution in MME [16] and POPE [25]. (b) Object-level word distribution in MME and POPE. (c) Co-occurrence-level word distribution in MME and POPE. the objects that Object truly exist entities represent eo = {oo in the image within data instances. for (I, C) in D}. We initially employ LLMs to extract all potential objects from the textual records of each data instance within the training set. The full prompts used to extract object information are detailed in the Appendix D.1. Subsequently, we input the image along with all token entities and LM-extracted objects into visual grounding model, i.e., GroundingDINO [29] to identify visual objects for each data instance, termed as object entities. Finally, we compute the frequency distribution of all object entities across the entire training set. Co-occurrence entities refer to pairs of objects that appear together in the same image within data instance. Formally, ec = {(o1, o2)o1 o2 for (I, C) in D}. Using the extracted object entities, we can construct co-occurrence graph G(V, E), where the vertex set consists of object entities and the edge set corresponds to the set of cooccurrence pairs ec. Interrogation entities are the questioning methods used in the text within data instances. ew = {qq for (I, C) in D}. Where is the full question method set. We employ LLMs to extract all methods of posing questions from the data instances, defining them as interrogation entities. We extract all four kinds of entities from LLaVA [27]s instruction-tuning dataset. 3.3. Reverse Indexing To study the severity of LT issues within the training data and build connection between entities and data instances, we build reverse indexing dictionary mapping from the entities in four different perspectives backward to the data instances. Subsequently, we use the number of data instances corresponding to each entity as frequency to build the reversed distribution Qr of four perspectives, which can be formulated as Qr = {e1 : Ne1 , e2 : Ne2, . . . , en : Nen } where ei means entity item and Nei means the number of corresponding data instances of ei. Taking LLaVA 1.5 [27]s instruction tuning data as an example, we count the number of data instance matches for each entity and build reversed distribution based on the mapping data. The thresholds of the tail data and relative data volume are shown in Table 1. Surprisingly, among four perspectives, an average of 97.25% entries account for only 13.75% data instances on average, which can partially illustrate the scarcity of tail data and severity of the long-tail problem existing in training data of LVLMs. 3.4. Significance of Long-Tail Problem Mitigation Tail data accounts for more failed cases. We first evaluate LLaVA 1.5 on two popular benchmarks, POPE and MME, and analyze the failed cases, as incorrect responses generated by LVLMs often reveal the models weaknesses. Subsequently, we rank the failed cases according to the entity distribution of LLaVAs instruction-tuning data and extract the bottom (tail) 50% of these cases. Next, we measure the number of entities and data instances corresponding to these errors. The results, presented in Table 2, reveal that the tail 50% of failed cases cover over 99% of entities and account for an average of 68.65% of training instances. Additionally, we present cumulative error curve with entity distribution on the horizontal axis, ordered from most to least frequent. As shown in Figure 4, it can be observed that tail entities account for the majority of failure cases. Moreover, the distribution location of failed cases is positioned further towards the tail of the distribution compared to correct answers. The detailed results of location analysis are shown in Appendix B.2. Distribution varies between train and test data. Besides, the distribution of train and test data is also different. In statistics-based deep learning, it is assumed that the training data maintains the same distribution as the evaluation data. Based on the intuition that larger bias between distributions can result in performance loss, we examine the differences between the entity distributions of the training and evaluation data. We select the training data of LLaVA 1.5 [27], as well as the evaluation data from POPE [25] and MME [16]. The resulting co-distribution is presented in Figure 3. Notably, clear difference between the distributions of the evaluation and training data can be observed. Therefore, addressing LT issues in the training data is crucial for LVLMs to enhance their understanding of underrepresented concepts and improve overall performance. Next, we feed the original head conversation into language model (LM) and prompt the LLMs to rewrite the conversation using the selected tail synonyms. Full prompts to instruct LLMs can be found in Appendix D.2. It is important to note that certain stop words would not be replaced. 4. Approach 4.1. Data Rebalancing Stage To mitigate the long-tail problem in the LVLM, our adaptive data refinement framework starts by alleviating the redundancy problem existing in training data. Concretely, we achieve this by flattening the exponential distribution and decimating the duplicated entities. 4.1.1. Probability Dictionary Construction DR stage starts with settling down the resampling ratios of redundant data instances. First, we use the entity distribution construction method mentioned in Section 3.2 to construct the distribution dictionary Qe for each entity within all selected perspectives C. Subsequently, we construct the reverse indexed dictionary and use the number of data instances mapped with entities Ne as frequency to build reversed distribution Qr, which is used for calculating the sampling ratio ps. threshold τ is used to distinguish the head and tail data. τ is an entitys position in Qr while entities before τ count for small ratio of all entities but are mapped to massive data instances. We set τ as indicated in Table 1, consistent with the values used in the analysis stage. For an entity of perspective x, we set the probability of sampling by Pex = τx/Nex . After constructing the probability dictionary of each entity e, we start from Qe to sample the selected data. Since each data instance contains several entities in Qe, we sample every entity among one instance. So we introduce new hyperparameter np, which means the data instance with the total number of sampled entities over np is selected. We conduct this procedure over the full dataset, and the final selected core set is denoted as D. The detailed method is demonstrated in Appendix C. 4.2. Data Synthesis Stage Despite the presence of redundancy in the head entities, the issue of scarcity in the tail entities still persists. To alleviate the issue of scarcity, we design the data synthesis methods from the perspective of vision and language. Figure 2 displays the full data adjusting framework. 4.2.1. Language Data Synthesis The core idea of our DS stage is to replace head concepts with tail ones. First, we use WordNet [15] to extract all synonyms of token entities and construct mapping system. For each head instance, we extract the linguistic entities, search for their synonyms in the mapper, and filter out the head ones. 4.2.2. Diffusion-Based Visual Data Synthesis In addition to the language synthesis method, we propose more comprehensive multiple-perspective data synthesis approach. Editing tail images into different styles without altering the key entities can address the scarcity of objects and co-occurring objects. Meanwhile, rewriting process can resolve the scarcity of tokens and interrogation methods. In our analysis of the LT problem, we determine whether data instance is selected using Pe and np. However, in certain cases, the probability Pe may exceed 1. The absolute value of Pe also provides some insight into the scarcity of data instance. This value can be used to decide how many instances to synthesize. Notably, 76% of the synthetic data has Pe value of less than 5, so we utilize this method to determine the synthetic quantity for each data instance. The synthetic quantity for each data instance = (I, C) can be calculated as: = max e,x Pex; Nd,aug = 0 (cid:4)(cid:112)P 2 (cid:5) if < 1, if 1 < 5, if 5 , (1) Given tail instance dt = (It, Ct), our objective is to generate an image similar to and produce corresponding instruction data, specifically in the form of conversations. To achieve the goal of subtly altering an images style while retaining its key information (i.e. objects or their cooccurrence), we propose two methods, both built upon diffusion-based model [11, 19, 44]. We leverage ControlNet [66] as our DDPM model, which can generate new image based on an existing image and natural language prompt t. The generating procedure of ControlNet is represented as It,syn = G(It, pdef) where pdef denotes the default prompt used to preserve the primary information of the input image. Through detailed comparison of generation quality, we found that compared to traditional DDPM, using ControlNet is more effective in preserving image details and key objects. With the generated image It,syn, we can obtain descriptive conversation paired with the image to serve as the instruction tuning data instance. We utilize an off-the-shelf vision captioner to generate captions Capt,syn for It,syn. Subsequently, we extend the captions into conversations, as required by visual instruction tuning. During this stage, we use LLMs to expand the captions into full conversations, with the prompts for expansion provided in Appendix D.2. This process enables us to effectively synthesize scarce data Table 3. Comparison with models trained with different methods on different benchmarks. IT represents the number of training instances used during instruction tuning. +DR denotes the results after the data rebalancing stage, while +DS represents the results following the data synthesis stage. Benchmark names are abbreviated due to space limits. *: ShareGPT4Vs instruction tuning stage refers to the 2nd stage (3 in total). The best results are indicated in bold. Method IT* VQAOK SEED2 QB2 MMS MMEP SQAI MMMU VQAT GQA MMB VQAv LLaVA 1.5 +DR +DR +DS 665.0K 581.0K 665.0K ShareGPT4V 1246.0K 1168.0K 1246.0K +DR +DR +DS 53.2 55.3 57.4 54.0 56.7 57.9 48.7 57.2 57.4 59.6 59.6 59.9 47.3 46.8 49.6 44.2 44.9 45. 33.5 33.8 35.5 34.7 35.0 35.5 1510.7 1470.6 1512.8 1560.4 1542.3 1564.9 69.3 69.5 70.4 68.9 68.6 69.4 35.3 34.8 36.7 35.1 35.7 36.1 46.0 46.0 47.2 50.2 50.9 50.9 61.9 62.8 62.9 63.3 63.9 63. 64.3 65.5 65.0 68.0 67.9 68.8 76.6 76.9 76.9 78.6 78.7 78.7 Table 4. Performance comparison across existing data balancing methods. The best results are indicated in bold, and the secondbest results are underlined. IT represents the number of training instances used during instruction tuning. Method IT GQA SEED SEEDv POPE MMB Baseline EL2N Perplexity CLIP Score COINCIDE Ours-DR Ours 665K 581K 581K 581K 133K 581K 665K 62.0 62.5 62.3 62.5 59.8 62.8 62.9 61.0 53.6 53.4 53.0 - 61.0 61.3 57.2 47.4 47.4 47.0 - 57.2 57. 87.2 87.2 86.8 87.0 86.1 87.2 87.4 65.5 65.2 63.7 64.5 63.1 65.5 65.0 instances, helping to alleviate the LT problem from all four perspectives. Finally, we augment the rebalanced dataset with synthetic data to restore its scale to match that of the LLaVA base model. Given the generated image It,syn, we derive descriptive conversation paired with the image, forming an instructiontuning data instance. Specifically, we employ an offthe-shelf vision captioner to produce the relative captions Capt,syn for It,syn. These captions are then expanded into conversations, as visual instruction tuning requires. For this step, we utilize LLMs to transform the captions into comprehensive conversations, following the prompts detailed in Appendix D.2. This methodology allows us to effectively synthesize scarce data instances, mitigating the LT problem from all four perspectives. Finally, we augment the rebalanced dataset with synthetic data to match the original data scale of the LLaVA base model. 5. Experiments 5.1. Baseline Models In this paper, we use LLaVA 1.5 and ShareGPT4V as baselines. We apply ADR to their instruction tuning data and train our model on the adjusted data to verify its effectiveness. Below is an overview of the two models: LLaVA 1.5 [27, 28] represents novel end-to-end trained large multimodal model that combines vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities. ShareGPT4V [7] uses the adjusted training data obtained by GPT-4 and post-trained ShareCapioner and improves the performance of existing VLMs. Though they focus on data adjustment either, the long-tail problem remains, that is, our method is orthogonal to ShareGPT4V. 5.2. Benchmarks We utilize comprehensive set of widely recognized benchmarks, spanning broad range of academic VQA tasks and recent benchmarks designed to test the extensive abilities of LVLMs. The VQA series benchmarks [18, 33, 46] represent traditional, comprehensive VQA tasks. GQA [20] evaluates multiple reasoning skills and spatial understanding, which presents greater challenge. The MME Benchmark [16] assesses the comprehensive capabilities of LVLMs through series of carefully crafted questions spanning 14 distinct sub-tasks. MMBench and MMBench-CN [30] are designed to assess the models vision-based reasoning and perceptual abilities in both English and Chinese. MMSTAR [8] and SEED [23] evaluate the models comprehensive ability from different aspects. ScienceQA [32] evaluates LVLMs on multimodal, multiple-choice science questions, while MMMU [62] tests LVLMs across multiple disciplines, requiring college-level subject knowledge and sophisticated reasoning. Finally, Q-Bench [55] focuses on assessing lowlevel perception. All benchmarks we used and their abbreviations can be found in Appendix A.1. 5.3. Results For Comprehensive Evaluation The results on the eleven selected benchmarks are shown in Table 3. By balancing the training data while retaining 87% of its original scale, our method outperforms the baseline on most benchmarks. After the DS stage, we restore the same data scale as LLaVA 1.5. Without incorporating additional data, our method achieves an average absolute improvement of 2.28 points and relative improvement of 4.36%. Notably, on challenging benchmarks such as MMStar, SEED Bench2, OK-VQA, and Qbench-2, it surpasses the baseline with an Figure 5. Ablation study on data rebalancing combinations. T, O, C, and refer to Token, Object, Co-occurrence, and Interrogation respectively. The values displayed in the graph represent average scores across variety of comprehensive benchmarks. The blue dashed line indicates the baseline performance of LLaVA 1.5. Figure 6. Ablation study on data rebalancing synthesis methods. The meaning of abbrs occurred here is explained in Section 6.2. The values displayed in the graph represent average scores across variety of comprehensive benchmarks. The blue dashed line indicates the baseline performance of LLaVA 1.5. Table 5. Tail concept prediction accuracy (%) on ScienceQA-IMG [32] dataset. Tail@k% (simplified as @k), head@k% (simplified as H@k), and overall accuracy are reported. +DR denotes the results after data rebalancing, while +DS represents the results following the data synthesis stage. Bold numbers represent the best results across all methods. IT represents the number of training instances used during instruction tuning. Methods IT ScienceQA @5 @10 @15 @20 H@80 Overall LLaVA 1.5 +DR +DR +DS 665.0K 67.9 581.0K 69.2 665.0K 70.1 70.0 69.7 70.5 67.9 67.8 68. 68.5 68.5 69.0 74.6 76.2 78.6 69.3 69.5 70.2 average absolute improvement of 4.3 points and relative improvement of 9.15%. Moreover, we display the comparison between our method and popular data-balancing methods such as EL2N [38], Perplexity [34], CLIP Score, and COINCIDE [21]1 in Table 4. Our method consistently outperforms the baselines across the majority of benchmarks, which cover wide range of comprehensive tasks. Additionally, our approach focuses on mitigating LT issues and is orthogonal to most existing data balancing and augmentation methods. This means it can be applied alongside those techniques to achieve even better performance across various benchmarks. The detailed experimental results are provided in Appendix A. 5.4. Performance on Tail Instances In addition to the main results on comprehensive benchmarks, we assess the models performance on tail concepts to validate the effectiveness of improving tail concept performance. We selected ScienceQA [32] and VQAV2 [18] to evaluate performance on tail data. First, we applied the same method described in Section 3.2 to extract entities 1COINCIDE did not release code, we compare results from their paper. from the selected benchmark data and build their reverse indexed distribution. Subsequently, for each data instance, we calculated the average distribution position across each perspective and determined whether the data instance falls into the tail category by 1(average(Li) > τR). Here, 1 represents an indicator function. We set different thresholds to make sure to get different ratios of tail data. We then extract the tail data instances of different rations and evaluate the performance accordingly. The complete results for tail performance are presented in Table 5 and Figure 1(b). As shown in the table, our method effectively improves tail performance without compromising the head or overall performance, demonstrating the efficacy of our approach. 6. Ablation Study 6.1. Different Combinations of Perspectives To determine the most effective rebalancing and synthesis method, we train the model using data processed with different combinations of perspectives and subsequently evaluate the target model. The results are presented in Figure 5. We assess these combinations based on average performance across different benchmarks, the number of top-ranked results, and performance stability. While several combinations achieved the highest average performance, the full combination of all perspectives proved to be the most stable, as it ranked first in both the number of top performances and stability. Detailed results can be found in the Appendix A.2. 6.2. Synthesis Methods In addition to the perspective combination selection during the Data Rebalancing stage, we present our ablation study on different synthesis methods. Synthesis is performed on the same rebalanced data checkpoint across all perspectives to determine which method is the most effective. Figure 7. Qualitative comparison between the baseline model (LLaVA 1.5) and our proposed method (LLaVA w/ ADR) on tail example. LLaVA w/ ADR can handle tail questions while LLaVA 1.5 fails to answer. While LLaVA 1.5 fails to answer tail questions, LLaVA w/ ADR successfully addresses them. We select six synthesis methods to compare, divided into language synthesis methods (Sec.4.2.1) and vision synthesis methods (Sec.4.2.2). The following methods are tested: All: Tail instances are selected from all four perspectives, with full visual data synthesis (Sec.4.2.2) applied. Image Only (IO): Tail instances are selected from all four perspectives, applying visual data synthesis (Sec.4.2.2), but the conversation text remains unchanged. Token Rewrite (TR): Full language data synthesis (Sec.4.2.1) methods are applied. TW Rewrite (TWR): Tail instances are selected based on Token and Interrogation perspectives, and the conversations are rewritten using language model (LM). PlainAug SimpAdd (PA SA): Tail data are selected from all four perspectives, and simple resampling is applied. PainAug NewCap (PA NC): Tail data are selected from all four perspectives, followed by re-captioning, with the new captions incorporated into conversations using the same method with Sec.4.2.2. We use these synthesis methods to restore the data to 665K and test which checkpoint yields the best performance using the same volume of training data. We assess these methods based on average performance across different comprehensive benchmarks. The results are displayed in Figure 6, while detailed results can be found in the Appendix A.2. 7. Qualitive Results 7.1. Comparison on Tail Examples We compare our method (ADR) and baseline on several tail cases, the results are demonstrated in Figure 7, Where the baseline model fails to provide the correct answer, while our method (ADR) successfully handles the case. This highlights the ability of ADR to generalize better on challenging, less frequent instances, showcasing its robustness in scenarios where baseline models often struggle. Figure 8. Comparison between the original instruction-tuning (IT) data and our synthesized IT data. Tail concepts in the original data are highlighted using red boxes and fonts, whereas synthesized tail concepts are marked with green boxes and yellow fonts. 7.2. Data Synthesis Results Figure 8 provides direct example of our synthesized data. The synthesis process starts by generating an image that closely mirrors those containing tail concepts, which are typically underrepresented in conventional datasets. This step ensures that the visual features of these infrequent concepts are well captured. Subsequently, we generate corresponding textual data, such as descriptive captions or questions, carefully designed to complement the visual content and enhance the contextual understanding of the tail concepts. As illustrated in Figure 8, our synthesis approach not only produces high-quality visual content but also enriches the associated textual representation. This dual enhancement across both visual and textual modalities effectively addresses the data imbalance, significantly improving the representation of tail concepts. Consequently, our method effectively enhances model generalization and performance in underrepresented scenarios. 8. Conclusion In this paper, we study the long-tail problem existing in the instruction tuning data of LVLM from four perspectives and make the first attempt to mitigate it. Our analysis reveals that unbalanced data can result in performance gap between the head data and the tail data, therefore harming the models performance. Based on this insight, we develop an Adaptive Data Refinement Framework (ADR), which first balances the redundant head data and then adjusts the unbalanced tail distribution. Experimental results demonstrate that our method improves the tail performance and overall performance without harming the head part performance. In the future, we plan to extend our method to the pretraining (PT) stage of LVLMs. 9. Acknowledgement This work was supported by the Shanghai Artificial Intelligence Laboratory. We sincerely appreciate their support and resources, which contributed to the successful completion of this research."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 1 [2] Anthropic. Introducing the next generation of claude. https://www.anthropic.com/news/claude-3family, 2024. Accessed: 2024-09-26. 2 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 1, 2 [4] Delong Chen, Jianfeng Liu, Wenliang Dai, and Baoyuan In Wang. Visual instruction tuning with polite flamingo. Proceedings of the AAAI Conference on Artificial Intelligence, pages 1774517753, 2024. 3 [5] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4vsynthesized data for lite vision-language model. arXiv preprint arXiv:2402.11684, 2024. 2, [6] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. 1 [7] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 2, 3, 6, 14 [8] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large visionlanguage models? arXiv preprint arXiv:2403.20330, 2024. 6, 12 [9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 1 [10] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 2 [11] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(9):1085010869, 2023. 5 [12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024. 1, 2 [13] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. 1 [14] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 13, 14 [15] Christiane Fellbaum. Wordnet: An electronic lexical database. MIT Press google schola, 2:678686, 1998. 5 [16] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 3, 4, 6, 12, [17] Yu Fu, Liuyu Xiang, Yumna Zahid, Guiguang Ding, Tao Mei, Qiang Shen, and Jungong Han. Long-tailed visual recognition with deep models: methodological survey and evaluation. Neurocomputing, 509:290309, 2022. 1 [18] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913, 2017. 6, 7, 12 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 5 [20] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 6, 12 [21] Jaewoo Lee, Boyang Li, and Sung Ju Hwang. Concept-skill transferability-based data selection for large vision-language models. arXiv preprint arXiv:2406.10995, 2024. 1, 3, 7 [22] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench-2: Benchmarking multimodal large language models. arXiv preprint arXiv:2311.17092, 2023. 12 [23] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 6, 12 [24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. 3 examples early in training. Advances in neural information processing systems, 34:2059620607, 2021. 3, 7 [25] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 3, 4, 12 [26] Daizong Liu, Mingyu Yang, Xiaoye Qu, Pan Zhou, Yu Cheng, and Wei Hu. survey of attacks on large vision-language models: Resources, advances, and future trends. arXiv preprint arXiv:2407.07403, 2024. 1 [27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 1, 2, 3, 4, [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 3, 6, 17 [29] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded pre-training for open-set object detection, 2023. 4, 13 [30] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 6, 12 [31] Zikang Liu, Kun Zhou, Wayne Xin Zhao, Dawei Gao, Yaliang Li, and Ji-Rong Wen. Less is more: Data value estimation for visual instruction tuning. arXiv preprint arXiv:2403.09559, 2024. 1, 3 [32] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 2022. 6, 7, 12 [33] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. 6, 12 [34] Max Marion, Ahmet Üstün, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker. When less is more: Investigating data pruning for pretraining llms at scale. arXiv preprint arXiv:2309.04564, 2023. [35] OpenAI. Gpt-4v(ision) system card. https://cdn. openai.com/ papers/GPTV _System_Card.pdf, 2023. Accessed: 2024-09-26. 2 [36] OpenAI. Gpt-4o system card. https://cdn.openai. com/gpt4osystemcard.pdf, 2024. Accessed: 2024-09-26. 2 [37] Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva Ramanan, James Caverlee, and Shu Kong. The neglected tails in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1298812997, 2024. 1, 3 [38] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on data diet: Finding important [39] Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher Manning. Stanza: python natural language processing toolkit for many human languages. arXiv preprint arXiv:2003.07082, 2020. 13 [40] Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, and Jianfeng Dong. Alleviating hallucination in large vision-language models with active retrieval augmentation. arXiv preprint arXiv:2408.00555, 2024. [41] Xiaoye Qu, Mingyang Song, Wei Wei, Jianfeng Dong, and Yu Cheng. Mitigating multilingual hallucination in large vision-language models. arXiv preprint arXiv:2408.00550, 2024. [42] Xiaoye Qu, Jiashuo Sun, Wei Wei, and Yu Cheng. Look, compare, decide: Alleviating hallucination in large visionlanguage models via multi-view multi-path reasoning. arXiv preprint arXiv:2408.17150, 2024. 1 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 2 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 5 [45] Jie-Jing Shao, Jiang-Xin Shi, Xiao-Wen Yang, Lan-Zhe Guo, and Yu-Feng Li. Investigating the limitation of clip models: The worst-performing categories. arXiv preprint arXiv:2310.03324, 2023. 1, 3 [46] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 83178326, 2019. 6, [47] Brandon Smith, Miguel Farinha, Siobhan Mackenzie Hall, Hannah Rose Kirk, Aleksandar Shtedritski, and Max Bain. Balancing the picture: Debiasing vision-language datasets with synthetic contrast sets, 2023. 3 [48] Zhaochen Su, Juntao Li, Jun Zhang, Tong Zhu, Xiaoye Qu, Pan Zhou, Yan Bowen, Yu Cheng, et al. Living in the moment: Can large language models grasp co-temporal reasoning? arXiv preprint arXiv:2406.09072, 2024. 3 [49] Zhaochen Su, Jun Zhang, Xiaoye Qu, Tong Zhu, Yanshu Li, Jiashuo Sun, Juntao Li, Min Zhang, and Yu Cheng. Conflictbank: benchmark for evaluating the influence of knowledge conflicts in llm. arXiv preprint arXiv:2408.12076, 2024. 3 [50] Zhaochen Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao Li, Min Zhang, and Yu Cheng. Timo: Towards better temporal reasoning for language models. arXiv preprint arXiv:2406.14192, 2024. [51] Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi Liu, Hao Feng, Yang Li, Siqi Wang, Lei Liao, et al. Textsquare: Scaling up text-centric visual instruction tuning. arXiv preprint arXiv:2404.12803, 2024. 3 [64] Jihai Zhang, Xiang Lan, Xiaoye Qu, Yu Cheng, Mengling Feng, and Bryan Hooi. Learning the unlearned: Mitigating In European feature suppression in contrastive learning. Conference on Computer Vision, pages 3552. Springer, 2024. [65] Jihai Zhang, Xiaoye Qu, Tong Zhu, and Yu Cheng. Clip-moe: Towards building mixture of experts for clip with diversified multiplet upcycling. arXiv preprint arXiv:2409.19291, 2024. 3 [66] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 5, 14 [67] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, et al. Internlm-xcomposer: visionlanguage large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. 1 [68] Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and IEEE Jiashi Feng. Deep long-tailed learning: survey. Transactions on Pattern Analysis and Machine Intelligence, 45(9):1079510816, 2023. 1 [69] Qihao Zhao, Yalun Dai, Hao Li, Wei Hu, Fan Zhang, and Jun Liu. Ltgc: Long-tail recognition via leveraging llms-driven generated content. arXiv preprint arXiv:2403.05854, 2024. 3 [70] Beier Zhu, Kaihua Tang, Qianru Sun, and Hanwang Zhang. Generalized logit adjustment: Calibrating fine-tuned models by removing label bias in foundation models. Advances in Neural Information Processing Systems, 36, 2024. 1, 3 [71] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1, 2 [52] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2 [53] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [54] Xudong Wang, Zhirong Wu, Long Lian, and Stella Yu. Debiased learning from naturally imbalanced pseudo-labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1464714657, 2022. 1, 3 [55] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, and Weisi Lin. Q-bench: benchmark for general-purpose foundation models on low-level vision. In ICLR, 2024. 6, 12 [56] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023. 3 [57] An Yan, Zhengyuan Yang, Junda Wu, Wanrong Zhu, Jianwei Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Julian McAuley, Jianfeng Gao, et al. List items one by one: new data source and learning paradigm for multimodal llms. arXiv preprint arXiv:2404.16375, 2024. 3 [58] Lu Yang, He Jiang, Qing Song, and Jun Guo. survey on long-tailed visual recognition. International Journal of Computer Vision, 130(7):18371872, 2022. 1 [59] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, et al. mplug-docowl: Modularized multimodal large language model for document understanding. arXiv preprint arXiv:2307.02499, 2023. [60] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with multimodality, 2023. 1 [61] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplugowl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1304013051, 2024. 3 [62] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. 6, 12 [63] Chongsheng Zhang, George Almpanidis, Gaojuan Fan, Binquan Deng, Yanbo Zhang, Ji Liu, Aouaidjia Kamel, Paolo Soda, and João Gama. systematic review on long-tailed learning. arXiv preprint arXiv:2408.00483, 2024. 1 A. Details of Experiments A.1. Benchmarks All benchmarks we used and their abbreviations are introduced as follows. VQAv2: The Visual Question Answering v2 dataset [18] consists of 265,016 images, each with 5.4 questions on average, requiring vision, language, and commonsense understanding, with 10 ground-truth and 3 plausible but incorrect answers for evaluation. VQAT: The TextVQA dataset [46] includes 45,336 questions over 28,408 OpenImages images, requiring models to read and reason about text within images for answers. VQAOK: The Open-Ended Knowledge Visual Question Answering dataset [33] includes over 14,000 questions that require integrating visual content with external knowledge, such as Wikipedia, for final accurate answers. GQA: GQA [20] is large-scale dataset comprising over 22 million questions generated from scene graphs of 113,000 images. It is specifically designed to assess models on visual reasoning and compositional question answering, with focus on reducing language biases. SQAI: ScienceQA-IMG [32] is multimodal dataset comprising 21,208 science questions, each accompanied by corresponding images and explanations. It is designed to evaluate models capabilities in answering science-related questions through multimodal reasoning. POPE: The Polling-based Object Probing benchmark [25] evaluates vision-language models ability to detect hallucination by prompting them with classification questions regarding the presence of specific objects in an image. SEED: The SEED Bench [23] is large-scale benchmark with 19,000 multiple-choice questions across 12 dimensions, designed for efficient evaluation of LVLMs without human intervention. SEED2: SEED Bench v2 [22] is comprehensive benchmark with 24,000 multiple-choice questions across 27 dimensions, comprehensively evaluating text and image generation capabilities of LVLMs. MMMU: The Massive Multi-discipline Multimodal Understanding benchmark [62] is designed to evaluate multimodal models on complex, college-level tasks that require subject-specific knowledge and advanced reasoning. MMEP: The Multimodal Evaluation Benchmark [16] assesses LVLMs perception and cognition through 14 subtasks, including object recognition and reasoning. This paper focuses on its perception subset. MMBCN: MMBench [30] is benchmark with 3,000 multiple-choice questions across 20 dimensions, assessing vision-language models perceptual and cognitive abilities. CN denotes its Chinese validation set. MMB: The English validation subset of MMBench [30]; MMS: MMStar [8] is benchmark with 1,500 samples, assessing six core capabilities across 18 axes to evaluate LVLMs visual comprehension in complex scenarios. QB2: Q-Bench 2 [55] is benchmark for evaluating multimodal models on low-level vision tasks, focusing on visual perception, description, and quality assessment with datasets like LLVisionQA and LLDescribe. A.2. Detailed Results of Ablation Study We conducted an ablation study on different balancing combinations and synthesis methods. In the ablation study of different rebalancing combinations, we conduct the DR stage using different combinations of four perspectives, i.e., one or more from (Token, Object, Co-occurrence, and Interrogation) to validate the effectiveness of different perspectives. The detailed results of the balancing ablation experiment are presented in Table 7. Although some checkpoints achieved similar average results, we found that combining all perspectives yields the best performance in terms of both the number of top results and performance stability. Additionally, we conducted an ablation study on different synthesis methods. The results of the augmentation and synthesis experiments are presented in Table 8. Obviously, synthesizing from ALL perspectives (as outlined in Section 4.2.2) yields the best performance. A.3. Detailed Results of Main Experiment Beyond the main experiments, we conduct pure data augmentation on the original instruction-tuning dataset of LLaVA 1.5, focusing solely on the DS stage applied to the original training data. The resulting augmented data is used to instruction-tune LLaVA, which is then evaluated on various benchmarks. As shown in Table 6, our ADR framework consistently surpasses most pure augmentation checkpoints on the majority of benchmarks, with few exceptions, such as MMMU, MMB, and VQAv2. A.4. Qualitive Results We present the full qualitative results in Figure 9. LLaVA 1.5 often fails to provide accurate responses when addressing tail questions. However, with the integration of our ADR framework, the model demonstrates significant improvement in recognizing and handling tail concepts. Additionally, we showcase more examples of our synthesized data in Figure 10. This synthesis process enriches the tail data with additional instances, effectively boosting the models generalization and performance in underrepresented scenarios. B. Details of Analyzing Stage B.1. Examples of Entities Different kinds of entities are extracted from four perspectives: Token, Object, Co-occurrence, and Interrogations. The top 20 frequently-shown entities from instruction-tuning data of LLaVA 1.5 are displayed in Figure 11. (a) bear resting peacefully beside rock wall. (b) cell phone displaying cartoon princess on its screen. (a) train traveling along railway near church. (c) dump truck. Figure 9. Qualitative comparison between the baseline model (LLaVA 1.5) and our proposed method (LLaVA w/ ADR) on few tail examples. While LLaVA 1.5 fails to answer tail questions, LLaVA w/ ADR successfully addresses them. B.2. Implement Details of Analyzing Stage In this work, we construct the entity distribution using both the pretraining and instruction-tuning datasets from LLaVA 1.5, specifically LCS558K and Instructmix665K. To compare the differences between training and test data further, we also incorporate portions of the distributions from POPE and MME within the same figure. The complete results are presented in Figure 12. As illustrated, all pretraining, instruction-tuning, and evaluation datasets exhibit LT issues. However, the frequency distributions of training and evaluation data differ significantly. In the Analyzing stage, token entities are extracted using Stanza1 [39] as the POS parser. For object entities, we initially use LLaMA 3 70B Instruct2 [14] to detect potential object-related vocabulary, followed by GroundingDINO3 [29] to extract actual objects from the image. For co-occurrence distribution construction, we use Neo4j4 to 1stanza: link 2meta-llama/Meta-Llama-3-70B-Instruct: link 3IDEA-Research/grounding-dino-base: link 4Enterprise version 5.19.0: link (b) bench by the lake, with forest on the opposite shore. (c) furniture arrangement complemented by variety of planters. Figure 10. Comparison between the original instruction-tuning (IT) data and our synthesized IT data. Tail concepts in the original data are highlighted using red boxes and fonts, whereas synthesized tail concepts are marked with green boxes and yellow fonts. create an undirected graph. To construct interrogation entity distributions, we utilize LLaMA 3 70B Instruct2 [14] to extract interrogation words. B.3. Analysis of Failed Cases We experiment to observe the distribution location of failed cases. We first extract all entities within the failed cases Algorithm 1 Pseudo Code for Data Resampling 1 # D: raw training set; 2 # C: target perspectives list 3 # tau: the threshold for entities; 4 # D_bal: the rebalanced data, a.k.a. D*; 5 # n_p, alpha: hyperparameters 6 D_bal=[] 7 for pers in C: # build prob dict 8 9 entity_dist = entity_distribution_construction (D,pers) prob_dict[pers] = {ent:tau[pers]/ entry_dist[ent] for ent in entry_dict.keys()} 10 for instance in D: # data rebalancing 11 13 14 15 16 17 pass_cnt = 0 for pers in C: for entity in instance[entity ][pers]: if random.random() < prob_dict[pers][entity]: pass_cnt += 1 break if pass_cnt > n_p and random.random () < alpha: D_bal.append(instance) using the provided tail tokens. The corresponding prompts are shown in Figure 15. Additionally, we rewrite conversations containing tail tokens or interrogation entities (TWR in the ablation study or Section 6.2). As this task closely resembles standard rephrasing tasks with similar prompts, we will not elaborate on it further here. and calculate the max, min, and average location of these entities in the pertaining distribution. Also, we calculate the distribution locations of the correct cases as well to compare. The results are shown in Table 9. As shown in the table, it is easy to discover that the failed cases are positioned further behind the correct ones in the distribution. C. Details of our ADR Approach C.1. Data Rebalancing Method The algorithm for our data rebalancing method is detailed in Algorithm 1. Initially, we calculate the sampling probability for each entity using the reverse distribution Qr and threshold τ . Entities with higher frequencies are assigned lower sampling probabilities, reducing the likelihood of overrepresented entities being selected. We then iterate over the entire dataset, leveraging these probabilities to filter out overrepresented instances. For each data instance d, we assess all four perspectives via random sampling. If an entity within perspective is sampled, the perspective is marked as pass. Instances with number of passed perspectives greater than np are retained; otherwise, they are discarded. C.2. Implement Details of Data Synthesis Stage During the Data Synthesis (DS) stage, we use ControlNet5 [66] to generate images that closely resemble those containing tail concepts. To produce high-quality captions for the generated images, we employ ShareCaptioner6 [7]. Finally, we leverage LLaMA 3 70B Instruct [14] to expand the captions into detailed conversations. D. Prompts D.1. Object Information Extraction In this section, we release all of our prompts for guiding LLMs to do specific tasks. Firstly during the analyzing stage, we utilize the LLMs to extract object information from the text within data instances at the very first step during object entity extraction. This part of the prompt we used to guide LLMs is illustrated in Figure 13. D.2. Conversation Rewrite We leverage LLaMA3 70B Instruct [14] to rewrite our conversations. During the Data Synthesis (DS) Stage, synthetic data and captions are generated using diffusion models and captioning models. Once the image and its corresponding caption are obtained, we employ the LM to transform the caption into conversation. The prompt used to guide the LM is shown in Figure 14. Moreover, during the language data synthesis process in the DS stage, we also utilize LLMs to rewrite conversations 5lllyasviel/ControlNet: link 6Lin-Chen/ShareCaptioner: link Table 6. Comparison of models trained with different approaches across multiple benchmarks. IT represents the number of training instances used during instruction tuning. +DR signifies performance after the data rebalancing stage, and +DS indicates performance after the data synthesis stage, with the number following DS denoting the augmentation volume from the DS stage. Benchmark names are abbreviated due to space constraints. The best results are indicated in bold. Method IT* VQAOK SEED2 QB2 MMS MMEP SQAI MMMU VQAT GQA MMB VQAv2 665.0K LLaVA 1.5 581.0K +DR 665.0K +DR +DS 690.0K +DS 25K +DS 50K 715.0K +DS 100K 765.0K 53.2 55.3 57.4 56.2 57.3 54.5 48.7 57.2 57.4 47.5 47.3 47.2 47.3 46.8 49.6 47.9 47.7 46. 33.5 33.8 35.5 34.5 35.2 34.6 1510.7 1470.6 1512.8 1486.0 1472.5 1502.7 69.3 69.5 70.4 68.7 69.9 69.7 35.3 34.8 36.7 36.0 36.9 36.8 46.0 46.0 47.2 47.1 47.0 46.1 61.9 62.8 62.9 62.8 62.7 62. 64.3 65.5 65.0 66.3 66.3 64.5 76.6 76.9 76.9 77.2 77.1 76.6 Table 7. Full results of ablation study on different combinations of perspectives. T, O, C, and refer to Token, Object, Co-occurrence, and Interrogation respectively. The best results are indicated in bold, and the second-best results are underlined."
        },
        {
            "title": "T O C W IT",
            "content": "VQAv2 VQAT VQAOK GQA SQA SQAI REF REF+ FLIK POPE SEED Avg. baseline 665.0K 76.6 488.1K 76.5 197.9K 74.6 242.4K 75.2 176.3K 73.9 534.2K 76.7 553.4K 75.7 521.5K 75.7 276.9K 75.4 318.3K 75.7 349.9K 76.8 375.9K 76.2 575.5K 76.8 559.3K 76.7 561.5K 76.8 581.7K 76.9 46.0 46.6 44.0 43.3 43.0 47.1 44.5 44.5 44.6 44.6 46.8 45.3 46.7 46.9 47.2 46.0 53.2 55.3 50.4 47.3 46.3 55.6 52.8 52.8 46.8 50.9 54.4 54.4 56.7 52.5 50.0 55.3 61.9 62.3 61.3 61.3 60.7 62.8 62.0 62.0 61.7 61.8 62.5 62.8 62.4 62.3 62.3 62. 70.4 70.8 69.9 70.0 69.5 71.4 70.8 70.8 69.0 71.5 71.5 70.7 71.2 71.6 71.7 71.4 69.3 69.2 67.9 68.5 66.7 68.1 68.4 68.4 66.4 69.0 68.8 67.6 68.8 69.2 69.9 69.5 29.4 28.5 30.8 31.4 32.3 30.3 30.4 30.4 30.6 29.9 29.9 29.7 30.1 30.8 28.8 30.2 28.5 28.1 29.7 29.8 31.7 29.1 29.2 29.2 29.4 29.0 29.2 28.8 29.1 30.0 28.1 29.7 74.9 73.8 74.1 76.2 71.9 75.4 75.1 75.1 74.2 74.9 75.7 74.3 75.9 76.6 75.6 76.2 86.9 86.7 86.3 86.8 85.6 86.9 86.4 86.4 87.1 86.8 86.8 86.8 87.2 87.4 86.6 87. 60.6 60.2 59.3 59.0 57.4 60.9 59.9 59.9 59.3 59.6 61.5 60.1 61.2 61.0 60.6 61.0 59.8 59.8 59.0 59.0 58.1 60.4 59.6 59.6 58.6 59.4 60.4 59.7 60.6 60.5 59.8 60.6 Table 8. Full results of ablation study on different augmentation methods. Methods are introduced in Sec. 6.2. The best results are indicated in bold, and the second-best results are underlined. Method IT VQAv2 VQAT VQAOK GQA SQA SQAI REF REF+ FLIK POPE SEED Avg. ALL Image Only Token Rewrite TW Rewrite 665.0K 76.9 665.0K 76.9 665.0K 76.9 665.0K 76.9 PlainAug SimpAdd 665.3K 76.8 PlainAug NewCap 665.3K 76.8 47.2 46.5 46.1 46.9 46.2 46.7 57.4 57.2 49.2 54.9 56.0 54.6 62.9 62.5 62.4 62.5 63.0 62.1 72.0 68.8 70.6 68.9 71.7 68. 70.4 68.4 68.6 68.7 69.3 69.4 30.5 30.6 32.3 31.0 29.3 31.1 29.9 30.2 31.3 30.3 28.5 30.7 76.2 75.9 0.6 77.5 74.1 77.3 86.9 87.3 87.4 87.5 86.6 87.7 61.3 53.8 54.1 53.7 61.7 54. 61.1 59.8 52.7 59.9 60.3 59.9 Figure 11. Top 20 most frequent entities in the instruction-tuning dataset of LLaVA 1.5. Table 9. Distribution locations of entities in correct and incorrect answers for POPE and MME, generated by LLaVA 1.5. Tok, Obj, and Co refer to Token, Object, and Co-occurrence, respectively, while and represent wrong and correct answers, respectively. The gray rows ( ) indicate the relative displacement of incorrect concepts in the distribution compared to correct concepts. Methods Tok-C Tok-W Obj-C Obj-W Co-C Co-W Tok-C Tok-W Obj-C Obj-W Co-C Co-W MME POPE Max Min 1 Mean 1035 10377 +639 1 +0 1068 +33 2708 842 3222 +514 131 +71 1035 +193 247315 12732 71123 257107 +9792 20741 +8009 79104 + 2242 1 313 2772 +30 1 +0 340 +27 1085 319 1100 +15 21 +4 336 +17 130043 926 27457 141722 +11679 1033 +107 30989 + (a) MME: Tok (b) LCS558K: Tok (c) InstructMix665K: Tok (d) MME: Obj (e) LCS558K: Obj (f) InstructMix665K: Obj (g) InstructMix665K: Co-occurrence (h) InstructMix665K: Interrogation Figure 12. Long-tail distribution in instruction-tuning and benchmark datasets. Some plots feature multiple curves, with the x-axis standardized according to the dataset mentioned in the title. Distributions from various datasets are overlaid on the same graph to emphasize the differences between them. (a) Token-level word distribution in MME [16]. (b) Token-level word distribution in LCS558K [28]. (c) Token-level word distribution in InstructMix665K [28]. (d) Object-level word distribution in MME [16]. (e) Object-level word distribution in LCS558K [28]. (f) Object-level word distribution in InstructMix665K [28]. (g) Co-occurrence distribution in InstructMix665K [28]. (h) Interrogation distribution in InstructMix665K [28]. Figure 13. Complete prompts used to guide the language model in extracting object information. Figure 14. Complete prompts used to guide the language model in converting captions into conversation instructions. Figure 15. Complete prompts used to guide the language model in rewrite conversation instructions using given tokens."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory",
        "Stony Brook University",
        "The Chinese University of Hong Kong"
    ]
}