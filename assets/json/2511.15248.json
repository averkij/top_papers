{
    "paper_title": "EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control",
    "authors": [
        "Kai Yang",
        "Xin Xu",
        "Yangkun Chen",
        "Weijie Liu",
        "Jiafei Lyu",
        "Zichuan Lin",
        "Deheng Ye",
        "Saiyong Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 8 4 2 5 1 . 1 1 5 2 : r EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control 2025-11-20 Kai Yang1, Xin Xu1,2, Yangkun Chen1, Weijie Liu1, Jiafei Lyu1, Zichuan Lin1, Deheng Ye1, Saiyong Yang1, 1Tencent Hunyuan 2The Hong Kong University of Science and Technology (cid:66) {kasperyang, stevesyang}@tencent.com"
        },
        {
            "title": "Abstract",
            "content": "Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs. Github Repo [GitHub Page]"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated exceptional performance across various natural language processing tasks, excelling in comprehension, generation, and reasoning (Achiam et al., 2023; Team et al., 2024; Liu et al., 2024). In verifiable-reward domains, such as mathematics and programming, reinforcement learning with verifiable rewards (RLVR) (Guo et al., 2025) has proven highly effective. RLVR addresses the issue of reward hacking, common in reward models, by directly comparing model outputs to ground-truth answers. In this context, reinforcement learning algorithm capable of sustaining long-term training is essential for continuous model improvement. Notably, output diversity, quantified by entropy, plays critical role in both the effectiveness and stability of the training process (Cui et al., 2025). Entropy serves as key measure in reinforcement learning (RL), quantifying agent exploration and preventing premature convergence to local optima. Maintaining an appropriate level of entropy is vital: too low entropy causes premature convergence (Arora et al., 2023; Li et al., 2025; Zawistowski, 2024), while too high entropy leads to noisy outputs and instability. Foundational RL research has successfully integrated entropy into optimization algorithms to balance exploration and exploitation in classical environments (Haarnoja et al., 2018; Zhao et al., 2019; Eysenbach & Levine, 2021; Shi et al., 2019). However, RL application to LLMs is challenged by the fact that initial models are pre-trained and start with low entropy. Low regularization coefficients fail to preserve exploration, while high coefficients degrade performance and increase sensitivity to hyperparameters (He et al., 2025; Cui et al., 2025). Current entropy regulation methods can be divided into two main approaches: (1) masking or down-weighting tokens to enhance diversity (Cui et al., 2025; Du et al., 2025; Zhu et al., 2025), and (2) embedding entropy bonuses into the reward or loss functions (Zhang et al., 2024; Li et al., 2025; He et al., 2025). While these methods slow the decline in entropy, they struggle to maintain entropy near desired target without sacrificing model performance. Mask-based techniques lose token gradients, while weight and bonus methods suffer from sensitivity to hyperparameters and added complexity, hindering scalability (He et al., 2025). Furthermore, most methods focus on off-policy settings, even though on-policy training has demonstrated higher performance potential. However, early-stage uncontrollable entropy fluctuations during large dataset training remain challenge, and as result, on-policy entropy control has been less explored. Empirical research shows that positive-sample training reduces entropy (negative samples increase it) and that policy entropy is influenced by the covariance between action log-probabilities and the advantage function (Zhu Corresponding Authors. et al., 2025; Cui et al., 2025). This insight frames entropy regulation as balance between these opposing effects. In this work, we first prove theoretically that positive samples decrease entropy (negative samples increase it) and then introduce EntroPIC. EntroPIC uses Proportional (P) and Proportional-Integral (PI) control to dynamically adjust the weight of positive and negative samples. By using target entropy and tracking both historical and current deviations, it stabilizes training entropy, offering convergence guarantees (see Figure 4). As shown in Figure 1, EntroPIC stabilizes entropy and gradient updates by adjusting the weight of high-probability tokens. Dynamic tuning of coefficients enables targeted entropy control without requiring global adjustments. This method stabilizes entropy for both on-policy and off-policy training, and its effectiveness has been validated through successful training on over 1M prompts, demonstrating its suitability for large-scale LLM training in industrial settings. Figure 1: Overview of EntroPIC. The method calculates correction factor based on historical and current entropy values relative to the target entropy and adjusts the weights of high-probability positive and negative samples to achieve entropy control."
        },
        {
            "title": "2 Related Work",
            "content": "Reinforcement Learning with Large Language Models. Recent advances in aligning large language models (LLMs) with human preferences have heavily relied on reinforcement learning techniques, particularly Reinforcement Learning from Human Feedback (RLHF) (Achiam et al., 2023; Ouyang et al., 2022; Korbak et al., 2023; Hurst et al., 2024). These methods typically involve training reward model based on human or AI feedback, followed by policy optimization using algorithms like PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024). Recently, RLVR has made significant progress in fields like mathematics, coding, science, and complex instructions. RLVR eliminates the need for reward model and is nearly immune to overfitting, offering breakthrough in performance (Lambert et al., 2024; openai, 2024; Zhang et al., 2025a; Guo et al., 2025; Team et al., 2025; Zhang et al., 2025b). With RLVR, the performance ceiling is no longer limited by the quality of the reward model but instead depends largely on the control of entropy (Cui et al., 2025; Yu et al., 2025). Our research demonstrates that maintaining an appropriate entropy level enhances model diversity and boosts its performance potential. Entropy in Policy Optimization. In reinforcement learning, exploration is essential for achieving the upper limit of model performance (Sutton et al., 1998). Many works related to exploration have achieved success in gaming and robotics scenarios (Burda et al., 2018; Yang et al., 2024; Lobel et al., 2023). One major category of methods focuses on enhancing exploration by increasing entropy. Entropy regularization has long been recognized as critical component in policy gradient methods to promote exploration (Williams & Peng, 1991; Mnih et al., 2016; Haarnoja et al., 2017; Kim et al., 2023). Maintaining entropy at an appropriate level prevents premature convergence to deterministic and potentially suboptimal policies (Haarnoja et al., 2018; Schulman et al., 2017). Recent studies have further investigated entropy dynamics in the context of LLM alignment, proposing methods such as modifying entropy bonuses (Zhang et al., 2024; Li et al., 2025; Tan & Pan, 2025), importance sampling weights (Vanlioglu, 2025; Yin et al., 2024; Cui et al., 2025; Su et al., 2025), or entropy regularization terms (Adamczyk et al., 2025; He et al., 2025). While these methods can slow the decline in entropy, they still struggle to maintain entropy at high, stable level during long-term training."
        },
        {
            "title": "3 Preliminary",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR). RLVR is framework for training LLMs on tasks with objectively verifiable outcomes, such as mathematics and programming. Unlike RLHF, RLVR directly computes rewards by comparing model outputs to ground-truth solutions, eliminating the need for reward models and mitigating reward hacking. For given prompt x, an LLM policy πθ generates token sequence = {y1, , yT}. The goal is to maximize the expected reward: max θ J(θ) := xD,yπθ (x)[r(y)], where is the training distribution and r(y) is the reward for output y. To optimize this objective, policy gradient methods estimate the gradient of J(θ) as: θ J(θ) = xD,yπθ (x) θ log πθ(yty<t)At . (cid:35) (cid:34) t=1 Here, At is the advantage at step t, reflecting the value of selecting token yt given the context y<t. In Group Relative Policy Optimization (GRPO) (Shao et al., 2024), representative on-policy method, advantages are normalized within groups of sampled responses to reduce variance. For each prompt x, responses {y1, , yK} are sampled, and the advantage for response yk is: Ak = r(yk) mean(r(y1:K)) std(r(y1:K)) + ϵ , where ϵ is small constant. This group-normalized advantage is used for gradient updates, ensuring robustness to reward scale variations. For constrained policy updates, Proximal Policy Optimization (PPO) (Schulman et al., 2017) uses clipped surrogate objective: L(θ) = Eytπθold (cid:20) min (cid:18) πθ(yty<t) πθold (yty<t) At, clip (cid:18) πθ(yty<t) πθold (yty<t) , 1 ϵ, 1 + ϵ (cid:19) (cid:19)(cid:21) At . This clipping term limits excessive policy updates, balancing optimization progress with training stability. Policy Entropy in Large Language Models. Policy entropy measures the uncertainty of token selection in LLM generation, balancing exploration and exploitation. For policy πθ on training data D, the average token-level entropy is defined as: H(πθ, D) = xD,yπθ (x) log πθ(yty<t) . (cid:34) 1 y t=1 (cid:35) This quantifies the randomness of token predictions. Maintaining appropriate entropy is critical: low entropy causes mode collapse (deterministic, suboptimal outputs), while high entropy leads to unstable training and noisy generation. Notably, LLM-RL starts with pretrained policies with low entropy, making fixed-coefficient entropy regularization ineffective due to hyperparameter sensitivity. Our analysis shows that entropy dynamics are influenced by the covariance between log πθ(yty<t) and the advantage At: positive samples (high At) reduce entropy, while negative samples (low At) increase it. Proportional-Integral Control. PI control is feedback mechanism that adjusts control inputs based on error signals to track target setpoint. In continuous time, the control law is: u(t) = Kp e(t) + Ki (cid:90) 0 e(τ)dτ, where e(t) = r(t) y(t) is the error between the target setpoint r(t) and the measured variable y(t), with Kp and Ki being the proportional and integral gains, respectively. The proportional term provides an immediate response to current errors, while the integral term accumulates historical errors to eliminate steady-state bias. In discrete-time training (e.g., LLM-RL), the integral term is approximated by summation. For the n-th step, the discrete PI control law becomes: u(n) = Kp e(n) + Ki e(k), n1 k=1 where e(n) = y(n) is the error in step n, and the summation replaces the integral. In our work, PI control is adapted to entropy regulation: the target setpoint is the desired entropy, y(n) is the measured entropy at step n, and e(n) dynamically adjusts the sample weight coefficients."
        },
        {
            "title": "4 Adjust Entropy via PI Control",
            "content": "4.1 The Impact of Positive and Negative Samples on Entropy To effectively control entropy during RL training, it is essential to understand the impact of positive and negative samples on training entropy. The NSR method (Zhu et al., 2025) experimentally demonstrates that positive samples tend to decrease entropy, while negative samples tend to increase it. We first provide theoretical explanation for these observations, showing that using only positive or negative samples during training with the RLVR binary reward distribution results in decrease or increase in entropy, respectively. Corollary 4.1. When the reward values for positive and negative samples follow binary distribution, and the expected advantage equals zero, training exclusively with positive/negative samples using policy gradient method will result in decrease/increase of the policys entropy. The complete proof is provided in Appendix A.1. This corollary illustrates that the use of positive samples results in decrease in entropy, while negative samples cause an increase. Based on these findings, we will modify the loss weights for positive and negative samples to dynamically regulate entropy during training. 4.2 Adjusting Weights to Control Entropy Building on the previous discussion, training with mixture of positive and negative samples can lead to an imbalance, resulting in either excessively high or low entropy. The policy gradient loss function for sequence modeling is defined as follows: L(θ) = Eaπθ [A(s, a) log πθ(as)]. To stabilize entropy, we propose the EntroPIC method, which uses PI control to dynamically adjust the loss coefficients for positive and negative samples based on the discrepancy between the current entropy value and the target entropy. The modified loss function is: (cid:104) L(θ) = Eaπθ I(A(s, a) > 0)(1 + α)A(s, a) log πθ(as) (cid:105) I(A(s, a) < 0)(1 α)A(s, a) log πθ(as) , (1) (2) (3) where is the indicator function and α is balancing coefficient that adjusts the influence of positive and negative samples. α is determined by the PI control formula: αt = Kp(Ht Ht tar) + Ki t1 k= (Hk Hk tar). (4) Here, Ht is the entropy at time step t, Ht tar is the target entropy, and Kp and Ki are the proportional and integral control coefficients. When α = 0, the loss function is equivalent to the original policy gradient loss. If α = 1, the loss focuses only on positive samples, reducing entropy; if α = 1, only negative samples are updated, increasing entropy. This mechanism enables dynamic regulation of policy entropy during training. Theorem 4.2. When training with an on-policy method and using P-control or PI-control (i.e., when Kp > 0 and Ki 0), the error et between the policys entropy and the target entropy Ht tar converges to zero: lim ek 0. The proof can be found in Appendix A.2. This theorem shows that using P-control or PI-control ensures reliable entropy regulation toward the target entropy for on-policy methods. For off-policy training, the situation changes as the sampling and training policies are different. The policy gradient loss is: (cid:2)min (cid:0)ρ(as) A(s, a), clip(ρ(as), 1 ϵlow, 1 + ϵhigh)(cid:1) A(s, a)(cid:3) , (5) is the importance sampling ratio. Our entropy control method also adjusts the sample L(θ) = where ρ(as) = πθ (as) µ(as) weights, resulting in the modified loss: aµ(s) L(θ) = Eaµ [I(A(s, a) > 0)(1 + α)ρ(as) A(s, a) + I(A(s, a) < 0)(1 α)ρ(as) A(s, a)] . Theorem 4.3. When training with an off-policy method using P-control with the above loss (i.e., Kp > 0 and Ki = 0), steady-state error in entropy will occur. Only PI-control (i.e., Kp > 0 and Ki > 0) ensures that the error et between the policys entropy and the target entropy Ht ek 0. tar converges to zero: (6) lim 4 Figure 2: Investigating how masking positive/negative and high/low-probability tokens affects entropy during training. Left: Entropy variation after masking different token groups (P: high-prob. positive, P: low-prob. positive, N: high-prob. negative, N: low-prob. negative). Right: Comparison of entropy changes between highand low-probability tokens. The proof is found in Appendix A.3. From Theorems 4.2 and 4.3, we conclude that for on-policy training, P-control is sufficient to achieve convergence to the target entropy, while for off-policy training, PI-control is necessary. Our method is versatile, with no restrictions on the initial or target entropy, and can adjust entropy in both directions, which most existing methods cannot. This approach guarantees convergence to the target entropy. 4.3 Simplify Loss by Controlling High-Probability Samples In Section 4.2, we introduced entropy control by adjusting the weights of all positive and negative samples. However, modifying low-probability samples can lead to substantial entropy changes and affect stability (Cui et al., 2025). We now explore whether we can achieve similar entropy control by adjusting the weights of only small subset of tokens, specifically high-probability samples. Corollary 4.4. When the control coefficient α is applied only to high-probability samples, i.e., tokens satisfying πθ(yty<t) > τ for fixed threshold τ, the convergence properties of Theorems 4.2 and 4.3 still hold under the same conditions. The proof can be viewed as Appendix A.4. As shown in Appendix A.2, entropy changes are positively correlated with token probabilities. Therefore, controlling the weights of high-probability tokens should yield similar results with minimal disruption to the original training process. To test this hypothesis, we conducted ablation experiments where we masked high-probability and low-probability positive and negative samples, observing entropy changes during training, as shown in Figure 2. The results confirm that modifying the weights of high-probability samples produces more significant entropy changes than adjusting low-probability tokens. Modifying only high-probability tokens offers several advantages over adjusting low-probability ones: 1) Easier Identification: High-probability tokens are easily identifiedtokens with probabilities greater than 95% are considered high-probability. Low-probability tokens are more challenging to define due to dependencies on sampling strategies like temperature, top-p, and top-k. 2) Enhanced Exploration: Reducing the weight of high-probability positive samples encourages exploration (Figure 3), while reducing the weight of low-probability negative samples can avoid suppressing rare events, which could degrade model performance. In conclusion, EntroPIC focuses on adjusting the weights of high-probability tokens to achieve optimal entropy control. The simplified loss function, which uses only high-probability tokens for regulation, is: L(θ) = (1 + α) A(s, a) log πθ(as) (1 α) A(s, a) log πθ(as) πθ (as)>τ A(s,a)> A(s, a) log πθ(as) πθ (as)<τ = LGRPO(θ) α πθ (as)>τ A(s, a) log πθ(as) πθ (as)>τ A(s,a)< For off-policy PPO-like losses, the entropy-controlled loss becomes: L(θ) = Lorigin(θ) α A(s, a) πθ (as)>τ πθ(as) µ(as) . (7) (8) 5 Figure 3: Schematic of entropy control via weight modulation. Modulating weights for high-probability tokens results in better performance than for low-probability ones. In both on-policy and off-policy scenarios, the final modified loss is: L(θ) = Lorigin(θ) α A(s, a) πθ(as) sg(π(as)) , (9) πθ (as)>τ where sg denotes the stop-gradient operation, and π(as) is the sampling policy. Although this loss differs slightly from the on-policy version, their gradients are identical."
        },
        {
            "title": "5 Experiment",
            "content": "5.1 Settings We evaluate the performance of Qwen3-8B-Base models (Yang et al., 2025) on mathematical tasks to validate our method. Initially, we fine-tune the model with supervised fine-tuning to endow it with basic mathematical capabilities. For RL training, we use several datasets: DAPO-MATH-17K (Yu et al., 2025), OpenReasonerZero (Hu et al., 2025), and DeepScaleR (Luo et al., 2025). The test datasets during training include OMNI-MATH (Gao et al., 2024), AIME2024, AIME2025 (of Problem Solving, 2025), AMC (Lightman et al., 2023), MATH (Lightman et al., 2023), and OlympiadBench (He et al., 2024). The average accuracy across these evaluation datasets is reported. We use the GRPO method as the baseline and compare EntroPIC with other entropy control methods such as Clip cov, KL cov (Cui et al., 2025), DMMPT (Du et al., 2025), NSR (Zhu et al., 2025), and AEC (He et al., 2025). For entropy control, we apply the default PI-control setup with coefficients Kp = 1 , Ki = 0.01 and high-probability threshold τ = 0.95. In each rollout step, 8 responses per prompt are sampled for batch of 512 prompts using temperature of 0.6. The target entropy for EntroPIC is set to 0.1, while the configurations for other methods adhere to the default settings in their respective papers. More details can be found in Appendix B. 5.2 Results 5.2.1 Entropy Control Experiments To verify that the proposed method is consistent with the theoretical analysis and whether it can control the entropy at the target value, we performed simple entropy control experiment to evaluate the EntroPIC method. We examine both on-policy and off-policy reinforcement learning setups to validate whether the PI control mechanism can effectively stabilize entropy throughout training. As shown in Figure 4, EntroPIC successfully stabilizes entropy near the desired target in on-policy training. The left plot depicts the convergence of entropy over time, while the right plot shows how the adaptive coefficient α dynamically adjusts to maintain stability. In contrast, training without entropy control exhibits continual entropy decay. Similarly, in off-policy settings (Figure 5), we observe that only the full PI control can maintain entropy around the target value, whereas the P-only control fails to achieve stable regulation. These findings empirically confirm our theoretical claim that positive and negative sample weighting, governed by PI feedback, effectively controls entropy dynamics. 6 Figure 4: On-policy training results. Left: entropy stabilization process. Right: variation of adaptive coefficient α. Figure 5: Off-policy training results. Only PI control successfully stabilizes entropy at the target value. 5.2.2 Large-scale Training Results Following the above verification, we conduct large-scale experiments to assess EntroPIC under various configurations, including on-policy, off-policy, and plug-and-play integration scenarios. We compare our method with GRPO and other baselines in terms of entropy stability, convergence speed across multiple benchmarks. On-policy Results. We compare the EntroPIC method with various entropy control approaches in Figure 6. The baseline demonstrates significant drop in entropy after 1,000 training steps, leading to subsequent decline in performance metrics. Methods like Clip cov, KL cov, and NSR, which are effective in off-policy training, result in entropy increase and failed control during on-policy training, causing instability. While AEC can maintain entropy above the target value, only EntroPIC successfully stabilizes entropy at the target throughout the training process. Figure 6: Left: Entropy change curves of different methods during training. Middle: Accuracy on the training set. Right: Accuracy on the validation set. EntroPIC stabilizes entropy at the target value throughout training, ensuring steady growth on both training and validation sets, ultimately achieving long-term stable performance. In terms of accuracy, other methods show slow improvements after 1,000 steps, while EntroPIC demonstrates consistent performance growth, achieving long-term stability and the best final results. We further evaluate the best-performing checkpoints from methods that behaved normally during on-policy training on the evaluation set. Detailed results are provided in Table 1. EntroPIC outperforms other methods with 3.5% improvement in average pass rate (avg@N) and 3.8% improvement in pass rate (pass@N) over GRPO. Off-policy Results. In the off-policy setting, we validate the effectiveness of the EntroPIC method by comparing it with the GRPO approach. Additionally, we conduct comparative experiment using P-control-only method. As shown in Table 2, EntroPIC with PI control leads to better model performance. Comparing the on-policy and off-policy metrics, we observe that on-policy training can achieve higher performance ceiling. Therefore, focusing on entropy-stabilized on-policy training is crucial for unlocking higher final performance. 7 Table 1: Model Performance Comparison (%). The numbers in parentheses indicate pass@N results. The EntroPIC method outperforms all other methods in both avg@N and pass@N metrics. Models Math AIME25 avg@N pass@N avg@N pass@N avg@N pass@N avg@N pass@N avg@N pass@N avg@N pass@N avg@N pass@N Olympic Bench Omni-math AIME24 Overall AMC Initial Model GRPO NSR AEC EntroPIC 86.1 91.2 91.5 92.5 92. 97.0 97.4 96.4 97.8 97.2 58.4 75.1 74.1 77.6 80.1 81.6 88.0 89.2 89.2 91.6 23.4 34.3 34.7 37.1 42.3 60.0 70.0 63.3 73.3 76.7 23.0 31.0 30.0 31.6 34. 53.0 53.3 46.7 60.0 66.7 49.9 59.1 58.5 60.9 60.0 68.7 72.7 71.3 72.5 71.3 32.0 40.7 39.7 42.0 42.7 49.3 57.6 56.2 58.5 58.4 45.5 55.2 54.8 56.9 58. 68.3 73.2 70.5 75.2 77.0 Table 2: Results of off-policy training. Models Math AIME25 avg@N pass@N avg@N pass@N avg@N pass@N avg@N pass@N avg@N pass@N avg@N pass@N avg@N pass@N Olympic Bench Omni-math AIME24 Overall AMC GRPO EntroPIC(P) EntroPIC(PI) 88.7 89.8 91. 93.6 96.4 97.0 64.0 67.8 75.3 87.9 90.4 90.4 28.9 34.8 34.7 63.3 66.7 70.0 25.5 27.5 27. 50.0 53.3 53.3 53.2 56.4 58.8 69.0 71.1 71.9 35.3 36.6 40.0 52.4 54.9 56.8 49.3 52.2 54. 69.4 72.2 73.2 5.2.3 Further Analysis on EntroPIC Plug-and-play Experiment. We further investigate the plug-and-play capability of EntroPIC. To verify if it can be applied after entropy starts declining during late training, we introduce EntroPIC to the GRPO model before its entropy drops significantly in the later stages. As shown in Figure 7, EntroPIC stabilizes entropy at the target value, and both training and testing metrics improve, boosting model performance. Figure 7: Plug-and-play experiment. EntroPIC stabilizes entropy during the late training phase, improving model performance. Results on Different Temperature. We further explore EntroPICs performance under different temperature settings. By increasing the sampling temperature to 1.0 and controlling the models initial entropy at approximately 0.3, we apply the method with target entropy of 0.3. As shown in and Table 3, EntroPIC achieve better performance across all evaluation datasets. Table 3: Results under High Temperature Setting. Models Math AIME25 avg@N pass@N avg@N pass@N avg@N pass@N avg@N pass@N avg@N pass@N avg@N pass@N avg@N pass@N Olympic Bench Omni-math AIME Overall AMC GRPO EntroPIC 91.3 92.7 97.4 98.0 72.4 78. 92.8 94.0 34.3 39.8 66.7 76.7 26.7 32.1 43.3 50.0 57.7 60. 70.1 72.5 39.2 41.2 55.8 57.5 53.6 57.8 71.0 74."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper addresses the challenge of entropy control in large models, particularly the instability caused by fluctuations in entropy during on-policy training. We propose the EntroPIC method, which adjusts the update weights of high-probability samples to stabilize entropy at predefined target value throughout the training process. The convergence of EntroPIC in entropy control is validated both theoretically and empirically through toy experiments, demonstrating its ability to maintain stability. Experimental results further show that EntroPIC not only achieves more consistent entropy regulation but also facilitates higher performance ceilings compared to existing methods. This approach proves effective in range of settings, including on-policy training, off-policy training, non-thinking models, thinking models, and plug-and-play scenarios, even under varying temperature conditions. 8 Limitations: While our EntroPIC method addresses key issues such as training collapse due to uncontrolled entropy increase and reduced exploration capacity resulting from entropy decline in prolonged training, it is not universally beneficial. In training scenarios where on-policy training alone is sufficient to maintain entropy stability, EntroPIC may not provide significant improvements. Additionally, since the target entropy needs to be manually set, its value must be carefully adjusted to fit the specific requirements of the model and task, which can be limitation in automated or highly dynamic settings."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Jacob Adamczyk, Volodymyr Makarenko, Stas Tiomkin, and Rahul Kulkarni. Average-reward reinforcement learning with entropy regularization. arXiv preprint arXiv:2501.09080, 2025. Kushal Arora, Timothy ODonnell, Doina Precup, Jason Weston, and Jackie CK Cheung. The stable entropy hypothesis and entropy-aware decoding: An analysis and algorithm for robust natural language generation. arXiv preprint arXiv:2302.06784, 2023. Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Dong Du, Shulin Liu, Tao Yang, Shaohua Chen, and Yang Li. Ulorl: An ultra-long output reinforcement learning approach for advancing large language models reasoning abilities. arXiv preprint arXiv:2507.19766, 2025. Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257, 2021. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: universal olympiad level mathematic benchmark for large language models, 2024. URL https://arxiv.org/abs/2410.07985. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International conference on machine learning, pp. 13521361. PMLR, 2017. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pp. 18611870. Pmlr, 2018. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, et al. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasonerzero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Dongyoung Kim, Jinwoo Shin, Pieter Abbeel, and Younggyo Seo. Accelerating reinforcement learning with valueconditional state entropy exploration. Advances in Neural Information Processing Systems, 36:3181131830, 2023. Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason Phang, Samuel Bowman, and Ethan Perez. Pretraining language models with human preferences. In International Conference on Machine Learning, pp. 1750617533. PMLR, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pp. 611626, 2023. 10 Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Xianzhi Li, Ethan Callanan, Xiaodan Zhu, Mathieu Sibue, Antony Papadimitriou, Mahmoud Mahfouz, Zhiqiang Ma, and Xiaomo Liu. Entropy-aware branching for improved mathematical reasoning. arXiv preprint arXiv:2503.21961, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Sam Lobel, Akhil Bagaria, and George Konidaris. Flipping coins to estimate pseudocounts for exploration in reinforcement learning. In International Conference on Machine Learning, pp. 2259422613. PMLR, 2023. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 19281937. PmLR, 2016. Art of Problem Solving. Aime problems and solutions. wiki/index.php?title=AIME_Problems_and_Solutions&oldid=254825, 2025. https://artofproblemsolving.com/wiki/index.php?title=AIME_Problems_and_ Solutions&oldid=254825. Accessed: 2025-09-02. https://artofproblemsolving.com/ URL openai. Learning to reason with llms. https://openai.com/index/learning-to-reason-with-llms/, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Wenjie Shi, Shiji Song, and Cheng Wu. Soft policy gradient method for maximum entropy deep reinforcement learning. arXiv preprint arXiv:1909.03198, 2019. Zhenpeng Su, Leiyu Pan, Minxuan Lv, Yuntao Li, Wenping Hu, Fuzheng Zhang, Kun Gai, and Guorui Zhou. Ce-gppo: Controlling entropy via gradient-preserving clipping policy optimization in reinforcement learning. arXiv preprint arXiv:2509.20712, 2025. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Hongze Tan and Jianfei Pan. Gtpo and grpo-s: Token and sequence-level reward shaping with policy entropy. arXiv preprint arXiv:2508.04349, 2025. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Abdullah Vanlioglu. Entropy-guided sequence weighting for efficient exploration in rl-based llm fine-tuning. arXiv preprint arXiv:2503.22456, 2025. Ronald Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241268, 1991. 11 Xin Xu, Cliveb AI, Kai Yang, Tianhao Chen, Yang Wang, Saiyong Yang, and Can Yang. Thinking-free policy initialization makes distilled reasoning models more effective and efficient reasoners. arXiv preprint arXiv:2509.26226, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Kai Yang, Jian Tao, Jiafei Lyu, and Xiu Li. Exploration and anti-exploration with distributional random network distillation. arXiv preprint arXiv:2401.09750, 2024. Mingjia Yin, Chuhan Wu, Yufei Wang, Hao Wang, Wei Guo, Yasheng Wang, Yong Liu, Ruiming Tang, Defu Lian, and Enhong Chen. Entropy law: The story behind data compression and llm performance. arXiv preprint arXiv:2407.06645, 2024. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Krystian Zawistowski. Unused information in token probability distribution of generative llm: improving llm reading comprehension through calculation of expected values. arXiv preprint arXiv:2406.10267, 2024. Hanning Zhang, Pengcheng Wang, Shizhe Diao, Yong Lin, Rui Pan, Hanze Dong, Dylan Zhang, Pavlo Molchanov, and Tong Zhang. Entropy-regularized process reward model. arXiv preprint arXiv:2412.11006, 2024. Junjie Zhang, Guozheng Ma, Shunyu Liu, Haoyu Wang, Jiaxing Huang, Ting-En Lin, Fei Huang, Yongbin Li, and Dacheng Tao. simple motivation can enhance reinforcement finetuning of large reasoning models. arXiv preprint arXiv:2506.18485, 2025a. Junjie Zhang, Rushuai Yang, Shunyu Liu, Ting-En Lin, Fei Huang, Yi Chen, Yongbin Li, and Dacheng Tao. Supervised optimism correction: Be confident when llms are sure. arXiv preprint arXiv:2504.07527, 2025b. Rui Zhao, Xudong Sun, and Volker Tresp. Maximum entropy-regularized multi-goal reinforcement learning. In International Conference on Machine Learning, pp. 75537562. PMLR, 2019. Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The surprising effectiveness of negative reinforcement in llm reasoning. arXiv preprint arXiv:2506.01347, 2025."
        },
        {
            "title": "A Proof",
            "content": "A.1 Proof of Corollary 4.1 According to Theorem 4.2 in (Cui et al., 2025), we have (cid:16) πk+1 θ (cid:17) (cid:17) (cid:16) πk θ = η Covaπk θ (s) (cid:16) log πk θ(a s), πk θ(a s) A(s, a) (cid:17) . Based on the assumption, for the binary distribution, since all positive and negative samples have the same reward and the expectation of A(s, a) is 0, all positive samples have A(s, a) > 0 while all negative samples have A(s, a) < 0, and the magnitude of the value is independent of a. Denoting the advantage of positive samples as Apos(s) > 0 and substituting it into the above formula, we get: (cid:16) πk+1 θ (cid:17) (cid:17) (cid:16) πk θ = η Covaπk θ (s) (cid:16) log πk θ(a s), πk log πk (cid:16) θ(a s) Apos(s) θ(a s), πk θ(a s) (cid:17) (cid:17) = η Apos(s) Covaπk 0. θ (s) Therefore, positive samples will definitely lead to decrease in entropy; similarly, negative samples will definitely lead to an increase in entropy. A.2 Proof of Theorem 4.2 First, we define coefficient function based on the advantage value: c(A(s, a)) = (cid:26)1 + α if A(s, a) > 0, 1 α if A(s, a) < 0. With this definition, we derive the gradient of the loss function θs,a L(θ) and the parameter update θk step: s,a step by θs,a L(θ) = θs,a = (cid:16) aπθ (s) (cid:2)c(A(s, a)) A(s, a) log πθ(as)(cid:3)(cid:17) aπθ (s) (cid:2)c(A(s, a)) A(s, a) θs,a log πθ(as)(cid:3) log πθ(as) θs,a πθ(as) c(A(s, a)) A(s, a) πθ(as) c(A(s, a)) A(s, a) (cid:0)I(a = a) πθ(as)(cid:1) = aA = aA = [πθ(as) c(A(s, a)) A(s, a) (1 πθ(as)) + a=a πθ(as) c(A(s, a)) A(s, a) (πθ(as)) (cid:35) πθ(as) c(A(s, a)) A(s, a) c(A(s, a)) A(s, a) (cid:34) = πθ(as) aA s,a = η θs,a L(θ) (cid:32) θk = η πθ(as) aA (cid:35) (cid:33) πθ(as) c(A(s, a)) A(s, a) + c(A(s, a)) A(s, a) . 13 (cid:32) = η aA η aA (cid:32) aA (cid:32) η + η = η aA η aA η aA + η aA Next, we calculate the the entropy change direction (cid:68) θH (cid:16) θk (cid:17) , θk(cid:69) and simplify it: (cid:16) θk (cid:17) , θk(cid:69) = aπθ (s) [log πθ(a s) η πθ(a s) πθ(a s) c(A(s, a)) A(s, a) + c(A(s, a)) A(s, a) aA aπθ (s) [log πθ(a s)] aπθ (s) (cid:2)η πθ(a s) (cid:33)(cid:35) (cid:33)(cid:35) πθ(a s) c(A(s, a)) A(s, a) + c(A(s, a)) A(s, a) (cid:68) θH (cid:32) + (cid:32) aA = η aA πθ(a s) log πθ(a s) πθ(a s) + η (cid:32) aA πθ(a s) log πθ(a s) (cid:33) aA πθ(a s) πθ(a s) (cid:33) πθ(a s) c(A(s, a)) A(s, a) + c(A(s, a)) A(s, a) aA θ (a s) log πθ(a s) π2 aA πθ(a s) c(A(s, a)) A(s, a) π2 θ (a s) log πθ(a s) c(A(s, a)) A(s, a) (cid:32) aA πθ(a s) c(A(s, a)) A(s, a) c(A(s, a)) A(s, a) (cid:33) πθ(a s) log πθ(a s) (cid:33) aA πθ(a s) c(A(s, a)) A(s, a) aA θ (a s) π2 (cid:33) πθ(a s) log πθ(a s) aA aA πθ(a s) c(A(s, a)) A(s, a) aA θ (a s) c(A(s, a)) A(s, a) π2 π2 θ (a s) log πθ(a s) π2 θ (a s) log πθ(a s) c(A(s, a)) A(s, a) πθ(a s) c(A(s, a)) A(s, a) aA θ (a s) π2 θ (a s) c(A(s, a)) A(s, a) π2 aA πθ(a s) log πθ(a s), where we use the identity: log πθ(as) πθ(as) log πθ(as) = πθ(as) (cid:0)log πθ(a s) log πθ(a s)(cid:1) . First, rewrite Term (III) by renaming dummy variables (consistent with Term (I)): η aA πθ(a s) c(A(s, a)) A(s, a) aA θ (a s) π2 aA πθ(a s) log πθ(a s). (V) Combine Term (I) and Term (V): η aA = η aA πθ(as)c(A(s, a))A(s, a) aA πθ(as)c(A(s, a))A(s, a) aA θ (as) π2 aA θ (as) π2 aA πθ(as) (cid:2)log πθ(as) log πθ(as)(cid:3) πθ(as) (cid:2)log πθ(as) log πθ(as)(cid:3) . Similarly, combine Term (II) and Term (IV): η aA = η aA θ (as)c(A(s, a))A(s, a) π2 πθ(as) (cid:2)log πθ(as) log πθ(as)(cid:3) aA θ (as)c(A(s, a))A(s, a) π2 aA πθ(as) (cid:2)log πθ(as) log πθ(as)(cid:3) . 14 Adding the two combined results and simplifying further, we obtain: (cid:68) (cid:16) (cid:17) θH θk , θk(cid:69) πθ(a s)c(A(s, a))A(s, a) θ (a s) π = η (cid:34) (cid:35) log πθ(a s) πθ(a s) log πθ(a s) π2 θ (a s)c(A(s, a))A(s, a) (cid:34) log πθ(a s) (cid:0)a s(cid:1) πθ(a s) log πθ(a s) (cid:35) c(A(s, a)) A(s, a) πθ(a s) πθ (cid:0)a s(cid:1)(cid:1) (cid:0)log πθ(a s) log πθ (cid:0)a s(cid:1)(cid:1) Apos(s) π2 θ (a s)πθ(a s) (cid:0)log πθ(a s) log πθ(a s)(cid:1) Aneg(s) π2 θ (a s)πθ(a s) (cid:0)log πθ(a s) log πθ(a s)(cid:1) η = η aA aA (cid:0)πθ(a s) πθ = η (1 + α) a,aApos η (1 α) a,aAneg = η(1 + α)Apos(s)Spos η(1 α)Aneg(s)Sneg, where Apos(s) (resp. Aneg(s)) denotes the set of actions with positive (resp. negative) advantage, Apos(s) > 0 (resp. Aneg(s) < 0) is the constant advantage value for positive (resp. negative) actions, and: Spos = θ (a s)πθ(a a,aApos π2 s) (log πθ(a s) log πθ(a s)) 0. Since the sum of advantages equals zero, we let Aneg = hApos (where > 0 is positive constant). Substituting this into the above equation gives: s) (log πθ(a s) log πθ(a s)) 0, Sneg = a,aAneg π2 θ (a s)πθ(a (cid:68) θH (cid:16) θk (cid:17) , θk(cid:69) = η Apos(s) [(1 + α)Spos h(1 α)Sneg] . When the entropy stabilizes, the error ek = 0 and α = 0, so: (cid:68) (cid:17) (cid:16) θH θk , θk(cid:69) = η Apos(s)(Spos hSneg) = 0. Control. Let the entropy error be ek = H(θks) Htar (where Htar is the target entropy). For proportional control (P control), we set α = Kpek (where Kp > 0 is the proportional gain). Substituting α into the inner product formula gives: , θk(cid:69) θk = C0 + C1 ek, θH (cid:68) (cid:17) (cid:16) where: C0 = η Apos 2 0 (all terms are positive except the negative sign). (Spos hSneg) = 0 (from entropy stabilization condition), C1 = η AposKp(Spos + hSneg) < The error at the next step satisfies: ek+1 = ek + H(θks) = ek + C1ek = (1 + C1)ek < ek. since 0 < 1 + C1 < 1. Thus, the error converges to zero: lim ek = lim 1 + C1ke0 0. PI Control. For PI control, we set α = Kpek + Ki Ik, where: ek = H(θks) Htar (error term), Ik = (integral of error), Kp > 0 (proportional gain) and Ki > 0 (integral gain). From the previous derivation, the entropy change can be written as: (cid:68) (cid:17) (cid:16) θH θk , θk(cid:69) = C(Kpek + Ki Ik), t=1 et where = η Apos(Spos + hSneg) > 0. The error evolution satisfies H(θks) = (cid:68) θH (cid:16) θk (cid:17) , θk(cid:69) , so we have: ek+1 = ek + H(θks) = ek C(Kpek + Ki Ik). 15 With the integral definition Ik+1 = Ik + ek+1, we get the two-dimensional linear recurrence system: (cid:21) (cid:20)ek+1 Ik+ = (cid:20)1 CKp CKi 1 CKi 1 CKp (cid:21) (cid:20)ek Ik (cid:21) = (cid:20)a 1 (cid:21) (cid:20)ek Ik (cid:21) , where we define simplified coefficients: - = 1 CKp (proportional-related coefficient), - = CKi (integralrelated coefficient). To analyze the stability of this recurrence system, we construct Lyapunov function: Vk = k + 1 I2 , where 0 < < 1 (to be proven later) ensures the coefficient is positive. Calculating Vk+1 using the recurrence relation: Vk+1 = (aek bIk)2 + [aek + (1 b)Ik]2 . 1 We now compute the difference Vk+1 Vk correctly: Vk+1 Vk = (aek bIk)2 + 1 [aek + (1 b)Ik]2 (cid:18) e2 + 1 I2 (cid:19) = a2e2 2abek Ik + b2 k + = a2e2 2abek Ik + b2 I2 + (cid:19) (cid:18) a2b 1 a2 + 1 + I2 b 1 a2b 1 (cid:18) = e2 = e2 (cid:16) a2e2 + 2a(1 b)ek Ik + (1 b)2 I2 (cid:17) e2 1 I2 e2 + 2abek Ik + b(1 b)I e2 1 I2 b2 + b(1 b) (cid:19) 1 (cid:18) a2(1 b) + a2b (1 b) 1 (cid:19) + I2 (cid:18) b2(1 b) + b(1 b)2 1 (cid:19) = = 1 1 1 1 (cid:16) (cid:16) (cid:104) (cid:104) e2 e2 a2 (1 b) a2 (1 b) (cid:17) (cid:17) (cid:16) + I2 b2 b3 + 2b2 + b3 (cid:17)(cid:105) b2 I2 (cid:105) . For stability, we need Vk+1 < Vk, which requires both terms in the brackets to be negative: 1. a2 (1 b) < 0 = a2 < 1 Substituting back = 1 CKp and = CKi: (1 CKp)2 < 1 CKi = C2K2 2CKp + CKi < 0. 2. b2 < 0, which is always true since > 0 Additionally, the denominator requires 1 > 0 = < 1 = CKi < 1. These conditions are easily satisfied with small learning rate η (making small) and moderate gains Kp, Ki. Under these conditions: Rearranging using Ik+1 = Ik + ek+1: e2 k+1 + 1 k+1 < e2 I2 + 1 I2 . k+1 e2 e2 < (cid:16) 1 I2 k+1 (cid:17) = 1 ek+1(2Ik + ek+1). Proof by Contradiction (Convergence of ek) Assumption: There exists an infinite subsequence {kj} with ekj+1 ekj Since Vk is monotonically decreasing and bounded below by 0, it converges to 0. Thus: - ek is bounded: ek V0 - Ik is bounded: Ik (cid:113) . 1b V0 Case 1: ek+1 and Ik have the same sign. Here ek+1 Ik > 0, so 2Ik + ek+1 has the same sign as ek+1. This gives: ek+1(2Ik + ek+1) > 0 = 1 ek+1(2Ik + ek+1) < 0. 16 Substituting into our inequality: e2 k+1 k < 0 = ek+1 < ek, contradicting the assumption. Case 2: ek+1 and Ik have opposite signs. For the assumption ek+1 ek to hold, we need: ek+1(2Ik + ek+1) < 0 (). From (): (1) If ek+1 > 0, then 2Ik + ek+1 < 0 = Ik < ek+1/2. (2) If ek+1 < 0, then 2Ik + ek+1 > 0 = Ik > ek+1/2. In both subcases, Ik > ek+1/2, so: Ik+1 = Ik + ek+1 Ik ek+1/2 < Ik. Thus Ik is monotonically decreasing along {kj}. Since Vk and Ik is decreasing/bounded, Ik I. For the recurrence system: (cid:21) (cid:20)e = (cid:20)a 1 (cid:21) (cid:20)e (cid:21) . The only solution is = 0, = 0 under our stability conditions, contradicting the assumption. Thus limk ek = 0, meaning the entropy converges to the target value. A.3 Proof of Theorem 4.3 In the off-policy setting with PPO-like loss (without clipping), the objective function is defined as: (cid:2)min (cid:0)ρ(as) A(s, a), clip(ρ(as), 1 ϵlow, 1 + ϵhigh)(cid:1) A(s, a)(cid:3) , L(θ) = Eaµ where ρ(as) = πθ (as) µ(as) is the importance sampling ratio. For simplicity, we define ρclip(as) = 1 ϵlow ρ(as) 1 + ϵhigh if ρ(as) < 1 ϵlow, if 1 ϵlow ρ(as) 1 + ϵhigh, if ρ(as) > 1 + ϵhigh. The gradient of this loss with respect to parameters θs,a is: θs,a L(θ) = θs,a (cid:16) aµ(s) (cid:2)c(A(s, a)) A(s, a) ρclip(as)(cid:3)(cid:17) (cid:16) aµ(s) aµ(s) = θs,a + θs,a + θs,a = (cid:16) (cid:16) E (cid:2)c(A(s, a)) A(s, a) (1 ϵlow))I(ρ(as) < 1 ϵlow)(cid:3)(cid:17) (cid:2)c(A(s, a)) A(s, a) ρ(as)I(1 ϵlow < ρ(as) < 1 + ϵhigh)(cid:3)(cid:17) (cid:2)c(A(s, a)) A(s, a) (1 + ϵlow))I(ρ(as) > 1 + ϵlow)(cid:3)(cid:17) aµ(s) (cid:2)I(1 ϵlow < ρ(as) < 1 + ϵhigh) ρ(as) c(A(s, a)) A(s, a) θs,a log πθ(as)(cid:3) aµ(s) I(1 ϵlow < ρ(as) < 1 + ϵhigh) πθ(as) c(A(s, a)) A(s, a) log πθ(as) θs,a I(1 ϵlow < ρ(as) < 1 + ϵhigh) πθ(as) c(A(s, a)) A(s, a) (cid:0)I(a = a) πθ(as)(cid:1) = aA = aA = [πθ(as) c(A(s, a)) A(s, a) (1 πθ(as)) + a=a I(1 ϵlow < ρ(as) < 1 + ϵhigh) πθ(as) c(A(s, a)) A(s, a) (πθ(as)) (cid:35) (cid:35) I(1 ϵlow < ρ(as) < 1 + ϵhigh) πθ(as) c(A(s, a)) A(s, a) c(A(s, a)) A(s, a) . = πθ(as) (cid:34) aA The parameter update is then: θk s,a = ηθs,a L(θ) (cid:34) = ηπθ(as) c(A(s, a)) A(s, a) aA I(1 ϵlow < ρ(as) < 1 + ϵhigh) πθ(as) c(A(s, a)) A(s, a) . (cid:35) 17 The directional change of entropy along the update direction is given by the inner product: (cid:68) θH (cid:16) θk (cid:17) , θk(cid:69) = + (cid:104) log πθ(a s) θk s,a aπθ (s) aπθ (s) [log πθ(a s)] (cid:105) aπθ (s) (cid:104) θk s,a (cid:105) . Substituting the expression for θk case but with off-policy corrections), we obtain: s,a and simplifying (following similar algebraic manipulations as in the on-policy (cid:68) θH (cid:16) θk (cid:17) , θk(cid:69) = η(1 + α)AposSpos η(1 α)AnegSneg + ηδ C, where: Apos and Aneg denote sets of actions with positive and negative advantages, respectively, Apos(s) > 0 and Aneg(s) < 0 are constant advantage values for positive and negative actions, Spos = aApos Sneg = aAneg δ = shift, aA π2 aA π2 (cid:2)I(1 ϵlow < ρ(as) < 1 + ϵhigh) ρ(as) A(s, a)(cid:3) is the bias term due to distribution θ (as)µ(as)ρ(as) (log πθ(as) log πθ(as)) 0, θ (as)µ(as)ρ(as) (log πθ(as) log πθ(as)) 0, aµ(s) = aA π2 = Aneg(s)/Apos(s) > 0. θ (as) log πθ(as) (a πθ(as) log πθ(as)) π2 θ (as), Control Analysis. For proportional control with α = Kpek (where ek = H(θks) Htar), the entropy change becomes: (cid:68) θH (cid:16) θk (cid:17) , θk(cid:69) = η Apos (cid:2)(1 + Kpek)Spos h(1 Kpek)Sneg (cid:3) + ηδ C. At steady state (ek ess), we require 0 = η Apos (cid:68) = 0, which gives: θH, θk(cid:69) (cid:2)(1 + Kpess)Spos h(1 Kpess)Sneg δ AposKp(Spos + hSneg) . ess = (cid:3) + ηδ C, This shows that steady-state error ess = 0 exists under control due to the bias term δ = 0 caused by the distribution shift (µ = π). PI Control Analysis. For proportional-integral control with α = Kpek + Ki Ik (where Ik = change becomes: t=1 et), the entropy H(θks) = (cid:68) θH (cid:16) θk (cid:17) , θk(cid:69) = C0(Kpek + Ki Ik) + D, where C0 = η Apos(Spos + hSneg) > 0 and = ηδ C. The error dynamics are: In matrix form: ek+1 = ek + H(θks) = ek C0(Kpek + Ki Ik) + D, Ik+1 = Ik + ek+1 = Ik + ek C0(Kpek + Ki Ik) + D. (cid:21) (cid:20)ek+1 Ik+1 = (cid:20)1 C0Kp C0Ki 1 C0Ki 1 C0Kp (cid:21) (cid:20)ek Ik (cid:21) (cid:21) (cid:20)D . + Steady-State Analysis. At equilibrium (e, I), we have: = C0(Kpe + Ki I) + D, = + e. Solving gives: = 0, = C0Ki . Thus, PI control achieves zero steady-state error (e = 0) when Ki = 0. Stability Analysis. Define the error vector ek = (cid:21) (cid:20)ek Ik . The error dynamics become: ek+1 = Aek, where = (cid:20)1 C0Kp C0Ki 1 C0Ki 1 C0Kp (cid:21) . The characteristic equation of is: det(λI A) = λ2 [2 C0(Kp + Ki)]λ + (1 C0Kp). For stability, the eigenvalues must lie within the unit circle. This is achieved when: 1. det(A) = 1 C0Kp < 1, 2. tr(A) = 2 C0(Kp + Ki) < 1 + det(A) = 2 C0Kp. These conditions are satisfied for sufficiently small learning rate η (making C0 small) and moderate gains Kp, Ki > 0. Lyapunov Stability. Choose positive definite matrix satisfying the discrete Lyapunov equation: where 0. Define the Lyapunov function Vk = eT Pek. Then: AT PA = Q, Vk = Vk+1 Vk = eT (AT PA P)ek = eT Qek 0, with equality only at ek = 0. By Lyapunov stability theory, limk ek = 0, implying limk ek = = 0. Therefore, while control in the off-policy setting exhibits steady-state error due to distribution shift, PI control achieves asymptotic convergence to the target entropy with zero steady-state error. A.4 Proof of Corollary 4.4 We consider the loss modified by applying the PI-weight only to high-probability tokens: A(s, a) log πθ(as) (1 α) L(θ) = (1 + α) A(s, a) log πθ(as) A(s, a) log πθ(as). πθ (as)>τ A(s,a)>0 πθ (as)>τ A(s,a)<0 πθ (as)τ Equivalently, L(θ) = LGRPO(θ) α A(s, a) log πθ(as), πθ (as)>τ where τ is the high-probability threshold. For simplicity, we define the high-probability indicator gθ(as) := I(πθ(as) > τ). Replace the coefficient function c(A(s, a)) used in the previous proof by its high-probability version: c(A(s, a), θ) = (1 + α) c(A(s, a)), (1 α) c(A(s, a)), c(A(s, a)), if A(s, a) > 0 and πθ(as) > τ, if A(s, a) < 0 and πθ(as) > τ, otherwise. Only actions with probability above threshold τ are reweighted by 1 α. The gradient and parameter update derivations in the on-policy case follow the same algebra as in Appendix A.2, but with every occurrence of c(A(s, a)) replaced by c(A(s, a), θ). After identical manipulations one obtains the directional change of entropy along the update: θH(θk s), θk = η (1 + α) Apos Sτ where now the S-terms are restricted to the high-probability subset: pos η (1 α) Aneg Sτ neg, pos = Sτ a,aApos neg = Sτ a,aAneg θ (a s) πθ(a s) gθ(a s) gθ(a s) ( log πθ(a s) log πθ(a s)) 0, π2 θ (a s) πθ(a s) gθ(a s) gθ(a s) ( log πθ(a s) log πθ(a s)) 0. π 19 Here Apos and Aneg are the action sets with positive / negative advantage under the binary reward assumption; Apos > 0, Aneg < 0. Write Aneg = hApos with > 0. Then θH, θk = η Apos (1 + α)Sτ pos h(1 α)Sτ neg (cid:104) (cid:105) . For the on-policy case, the entropy error ek = H(θks) Htar evolves as θH, θk = η Apos (1 + Kpek)Sτ With control (Kp > 0, Ki = 0) or PI control (Ki > 0), the convergence argument follows identically to the unmasked case, replacing Spos, Sneg by their masked counterparts. Both and PI controllers ensure limk ek = 0 under standard small-gain conditions, with convergence typically slower when Sτ pos h(1 Kpek)Sτ neg < Spos + hSneg. pos + hSτ neg (cid:105) (cid:104) . In the off-policy setting, importance weighting introduces masked bias term δτ due to distribution mismatch. Under control, the steady-state entropy error is δτCτ pos + hSτ,µ AposKp(Sτ,µ neg) while adding an integral term (Ki > 0) removes the bias, yielding eτ gain Cτ 0 is stable (eigenvalues within the unit circle). eτ ss = = 0, = 0 provided the discrete-time system with"
        },
        {
            "title": "B Experiment Details",
            "content": "B.1 Training Details We provide the detailed experimental setup in Section 5. To enhance the models ability to generate well-formatted answers, we performed lightweight supervised fine-tuning on the Qwen3-8B-Base model using approximately 10k non-thinking data samples to equip it with the capability to follow specific output formats. We build upon the VeRL codebase *. We collected data from the DAPOMATH dataset (Yu et al., 2025), the OpenReasonerZero dataset (Hu et al., 2025), and the DeepScaleR dataset (Luo et al., 2025). We filtered out true/false questions, multiple-choice questions, and proof problems from these datasets. Additionally, we performed 8 preliminary samplings using the model and excluded samples where all responses were either entirely correct or entirely incorrect to facilitate more efficient training. Dynamic sampling was not used during the training process. We used the return of Math-Verify as the reward signal for scoring, and the baseline algorithm was trained using the GRPO method. For entropy control, we apply the default PI-control setup with coefficients Kp = 1 , Ki = 0.01 and high-probability threshold τ = 0.95. In each rollout step, we sample 8 responses per prompt for batch of 512 prompts using temperature of 0.6, with top-k set to -1 and top-p set to 1. The maximum input length is 2048, and the maximum output length is 8192. We performed validation every 20 steps and saved the model every 100 steps. Our experiments were conducted on NVIDIA H20 96GB GPUs, utilizing 64 GPUs per experiment. Training the model for 2,000 steps involved approximately 1 million prompts and 8 million samples, taking about 300 hours to complete. For off-policy training, we use 4 steps to train on 512 promptsspecifically, 128 prompts are used for training in each step. The upper and lower bounds of the clip ratio are both set to 0.2, which is consistent with the standard GRPO method. B.2 Evaluation Details During evaluation, we refer to the configuration in (Guo et al., 2025) and (Xu et al., 2025). The rollout temperature is set to 0.6 for all test sets, with top-k set to -1 and top-p set to 0.95. All experiments used learning rate of 1e-6 with no decay. The KL coefficient and the entropy loss coefficient were both set to 0. The maximum output length is set to 8k for short sequences and 64k for long sequences. All evaluation codes are adapted from the DeepscaleR (Luo et al., 2025) codebase, where vLLM (Kwon et al., 2023) is leveraged to accelerate inference. In the final evaluation phase, we perform evaluations 8 times each on the Math, Olympic Bench, and Omnimath datasets, while the AIME24, AIME25, and AMC datasets (which show larger fluctuations) are evaluated 32 times each. We report the avg@N and pass@N metrics for each dataset respectively."
        },
        {
            "title": "C Code",
            "content": "The implementation of this entropy control strategy requires minimal code changes, as shown in Figure 8. *https://github.com/volcengine/verl https://github.com/huggingface/Math-Verify 20 Figure 8: The code change for using EntroPIC method."
        },
        {
            "title": "D More Results",
            "content": "D.1 Train Curve Due to page limit, we present only representative results in Sections 5. An analysis of the off-policy and temperature=1.0 training curves (Figures 9 and 10) is provided in this section. It can be observed that under off-policy conditions, GRPO rapidly experiences entropy decay, diminishing answer diversity and halting further performance improvement. Although P-controller offers entropy stabilization, it is incapable of precise target adherence. This capability is uniquely achieved by the PI-based EntroPIC method, which maintains entropy at the target value and facilitates sustained performance gains. parallel phenomenon is noted in the temperature=1.0 experiment: GRPOs early progress is later undermined by entropy collapse, whereas EntroPICs controlled exploration ensures consistent advancement and ultimate superiority. Figure 9: Off-policy training experiment. The entropy of the GRPO method drops significantly, while the EntroPIC method stabilizes entropy at the target value, continuously improving model performance. Figure 10: Temperature setting of 1.0. EntroPIC demonstrates better stability and higher performance than GRPO. 21 D.2 Control Coefficient α To further illustrate the working dynamics of EntroPIC, we analyze the control coefficient α that adaptively adjusts the strength of entropy regulation. Figure 11 shows α variations under multiple training scenarios in Section 5, including on-policy, off-policy, high-temperature, and plug-and-play configurations. We observe that α responds smoothly to entropy deviations: when entropy decreases, α increases to encourage exploration, and vice versa. EntroPIC ensures stable convergence and long-term entropy control across diverse training settings. Figure 11: Control α of EntroPIC method in training stage. D.3 Reflection Ability During training, we observed that models trained with EntroPIC frequently produced outputs containing reflective phrases such as wait, alternatively, and lets reconsider. This indicates that the model attempts to verify its answers through alternative reasoning paths, often generating multiple valid solutions clear manifestation of the high-entropy policys exploratory characteristics. In contrast, models trained with GRPO in the later stages exhibit much lower entropy, producing single-path, linear reasoning with almost no signs of self-correction or exploratory turns. The trend of reflective-word frequency is shown in Figure 12. Figure 12: Frequency of reflective words during training."
        },
        {
            "title": "E Case Study",
            "content": "In this section, we present concrete example to illustrate the difference between high-entropy exploration in EntroPIC models and low-entropy, deterministic reasoning in GRPO models. The goal is to show how the training method affects the style of reasoning and solution pathways. Prompt Let (x) = (x 18)(x 72)(x 98)(x k) . There exist exactly three positive real values of such that has minimum at exactly two real values of x. Find the sum of these three values of k."
        },
        {
            "title": "EntroPIC Response",
            "content": "Alright, lets tackle this problem step by step. We have: (x) = (x 18)(x 72)(x 98)(x k) The goal is to find three positive values such that has exactly two real minima. Finding Critical Points (x) = 0 Let N(x) = (x 18)(x 72)(x 98)(x k). The critical points are where (x) = 0. Using the quotient rule (x) = N(x)xN(x) , we set the numerator to zero: N(x) = N(x) Checking Roots of N(x) If is root of N(x) (i.e., {18, 72, 98, k}), then N(x) = 0. The condition N(x) = N(x) becomes N(x) = 0. Since = 0, we require N(x) = 0. Wait, so = 18 is critical point of if and only if N(18) = 0. Lets compute N(x) and check: Set = 18: N(x) = dx [(x 18)(x 72)(x 98)(x k)] N(18) = (18 72)(18 98)(18 k) N(18) = 0 = = 18. Similarly, = 72 is critical point of if = 72, and = 98 if = 98. Alternative Approach: Rewriting (x) Alternatively, let g(x) = (x 18)(x 72)(x 98). (x) = g(x) (cid:18) 1 (cid:19) The critical points satisfy (x) = 0: g(x) (cid:18) 1 (cid:19) + g(x) x2 = 0 This rearranges to: g(x)x(x k) + g(x)k = 0 Finding explicit solutions is non-trivial, but perhaps we can look for symmetry. Exploring Specific Cases and Symmetry The condition exactly two real values of where has minimum implies the quartic N(x) N(x) = 0 must have specific root structure. This usually means that two critical points must coincide, leading to double root in the derivative and simplifying the analysis. Alternatively, observe that 18, 72, 98 have specific structure. The three values of are known to be related to 18, 72, 98 themselves or symmetric counterparts that force the necessary double critical point. Testing = 72: If = 72, N(x) has repeated root. This is potential candidate. The analysis shows that for exactly two minima, the critical point equation must possess double root for certain k. Correct Insight: Valid Values Through systematic polynomial analysis (not shown here due to complexity), the three positive real values of that ensure exactly two distinct real minima are: k1 = 46, These values are consistent with symmetry properties for this type of function: k3 = k2 = 72, = 72 is the middle root. = 46 and = 122 are symmetric counterparts (e.g., related to the average/midpoint of 18 2 = 58, and 72 (72 46) = 46, 72 + (122 72) = 122, but these are and 98, which is 18+98 simplified relations). The sum of these three values of is: 46 + 72 + 122 = 240"
        },
        {
            "title": "GRPO Response",
            "content": "Alright, have this problem: Problem Statement: Let (x) = (x 18)(x 72)(x 98)(x k) . There exist exactly three positive real values of such that has minimum at exactly two real values of x. Find the sum of these three values of k. Step 1: Critical Points Let N(x) = (x 18)(x 72)(x 98)(x k). Derivative: Set numerator to zero: (x) = N(x) N(x) x2 . N(x) = N(x) Step 2: Roots of N(x) If is root of N(x) (i.e., {18, 72, 98, k}), then N(x) = 0. Equation reduces to N(x) = 0 = N(x) = 0. - = 18 = = 18 - = 72 = = 72 - = 98 = = 98 These are candidates but may not satisfy exactly two minima. Step 3: Alternative Form Let g(x) = (x 18)(x 72)(x 98): (x) = g(x) (cid:18) 1 (cid:19) Critical points satisfy: g(x) (cid:18) 1 (cid:19) + g(x) x2 = 0 g(x)x(x k) + g(x)k = 0 Step 4: Behavior and Extrema The condition exactly two minima implies the derivative must have double root. Observing the roots 18, 72, 98, the polynomial structure suggests the valid are related to these numbers. Step 5: Valid Values Through analysis: - = 18 aligns with left root. - = 72 aligns with middle root. - = 98 aligns with right root. k1 = 18, k2 = 72, k3 = 98 Step 6: Sum of 18 + 72 + 98 ="
        }
    ],
    "affiliations": [
        "Tencent Hunyuan",
        "The Hong Kong University of Science and Technology"
    ]
}