{
    "paper_title": "FAN: Fourier Analysis Networks",
    "authors": [
        "Yihong Dong",
        "Ge Li",
        "Yongding Tao",
        "Xue Jiang",
        "Kechi Zhang",
        "Jia Li",
        "Jing Su",
        "Jun Zhang",
        "Jingjing Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the remarkable success achieved by neural networks, particularly those represented by MLP and Transformer, we reveal that they exhibit potential flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize the periodic data rather than genuinely understanding the underlying principles of periodicity. However, periodicity is a crucial trait in various forms of reasoning and generalization, underpinning predictability across natural and engineered systems through recurring patterns in observations. In this paper, we propose FAN, a novel network architecture based on Fourier Analysis, which empowers the ability to efficiently model and reason about periodic phenomena. By introducing Fourier Series, the periodicity is naturally integrated into the structure and computational processes of the neural network, thus achieving a more accurate expression and prediction of periodic patterns. As a promising substitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in various models with fewer parameters and FLOPs. Through extensive experiments, we demonstrate the effectiveness of FAN in modeling and reasoning about periodic functions, and the superiority and generalizability of FAN across a range of real-world tasks, including symbolic formula representation, time series forecasting, and language modeling."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 ] . [ 2 5 7 6 2 0 . 0 1 4 2 : r FAN: Fourier Analysis Networks Yihong Dong1, Ge Li1, Yongding Tao1, Xue Jiang1, Kechi Zhang1, Jia Li 1, Jing Su2, Jun Zhang2, Jingjing Xu2 1School of Computer Science, Peking University 2ByteDance dongyh@stu.pku.edu.cn, lige@pku.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Despite the remarkable success achieved by neural networks, particularly those represented by MLP and Transformer, we reveal that they exhibit potential flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize the periodic data rather than genuinely understanding the underlying principles of periodicity. However, periodicity is crucial trait in various forms of reasoning and generalization, underpinning predictability across natural and engineered systems through recurring patterns in observations. In this paper, we propose FAN, novel network architecture based on Fourier Analysis, which empowers the ability to efficiently model and reason about periodic phenomena. By introducing Fourier Series, the periodicity is naturally integrated into the structure and computational processes of the neural network, thus achieving more accurate expression and prediction of periodic patterns. As promising substitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in various models with fewer parameters and FLOPs. Through extensive experiments, we demonstrate the effectiveness of FAN in modeling and reasoning about periodic functions, and the superiority and generalizability of FAN across range of real-world tasks, including symbolic formula representation, time series forecasting, and language modeling."
        },
        {
            "title": "Introduction",
            "content": "The flourishing of modern machine learning and artificial intelligence is inextricably linked to the revolutionary advancements in the foundational architecture of neural networks. For instance, multilayer perceptron (MLP) (Rosenblatt, 1958; Haykin, 1998) plays pivotal role in laying the groundwork for current deep learning models, with its expressive power guaranteed by the universal approximation theorem (Hornik et al., 1989). Recent claims about the impressive performance of large models on various tasks are typically supported by Transformer architecture (Vaswani et al., 2017; Touvron et al., 2023; OpenAI, 2023). In this context, the communitys enthusiasm for research on neural networks has never diminished. Some emerged neural networks demonstrate notable capabilities in specific fields (Gu & Dao, 2023; Liu et al., 2024), sparking widespread discussion within the community. Beneath the surface of apparent prosperity, we uncover critical issue that remains in existing neural networks: they struggle to model the periodicity from data. We showcase this issue through an empirical study as illustrated in Figure 1. The results indicate that existing neural networks, including MLP (Rosenblatt, 1958), KAN (Liu et al., 2024), and Transformer (Vaswani et al., 2017), face difficulties in fitting periodic functions, even on simple sine function. Although they demonstrate Equal Contribution This work was supported by cooperation project between Peking University and ByteDance Company. During this time, Yihong was also an intern at ByteDance. The code is available at https://github.com/YihongDong/FAN Preprint. Under review. Figure 1: The performance of different neural networks within and outside the domain of their training data for the sine function, where is scalar variable. proficiency in interpolation within the domain of training data, they tend to falter when faced with extrapolation challenges of test data, especially in the out-of-domain (OOD) scenarios. Therefore, their generalization capacity is primarily dictated by the scale and diversity of the training data, rather than by the learned principles of periodicity to perform reasoning. We argue that periodicity is an essential characteristic in various forms of reasoning and generalization, as it provides basis for predictability in many natural and engineered systems by leveraging recurring patterns in observations. In this paper, we investigate key research problem: How to enable neural networks to model periodicity? One core reason existing neural networks fail to model periodicity is that they heavily rely on data-driven optimization without explicit mechanisms to understand the underlying principles in the data. To this end, we propose Fourier Analysis Network (FAN), novel neural network framework based on Fourier Analysis. By leveraging the power of Fourier Series, we explicitly encode periodic patterns within the neural network, offering way to model the general principles from the data. FAN holds great potential as substitute to traditional MLP, which not only exhibits exceptional capabilities in periodicity modeling but also demonstrates competitive or superior effects on general tasks. To verify the effectiveness of FAN, we conduct extensive experiments from two main aspects: periodicity modeling and application of real-world tasks. 1) For periodicity modeling, FAN achieves significant improvements in fitting both basic and complex periodic functions, compared to existing neural networks (including MLP, KAN, and Transformer), particularly in OOD scenarios. 2) FAN demonstrates superior performance in real-world tasks, including symbolic formula representation, time series forecasting, and language modeling. The experimental results indicate that FAN outperform baselines (including MLP, KAN, and Transformer) for symbolic formula representation task, and Transformer with FAN surpasses the competing models (including Transformer, LSTM (Hochreiter & Schmidhuber, 1997), and Mamba (Gu & Dao, 2023)), for time series forecasting and language modeling tasks. As promising substitute to MLP, FAN improves the models generalization performance meanwhile reducing the number of parameters and floating point of operations (FLOPs) employed. We believe FAN is promising to be an important part of the fundamental model backbone."
        },
        {
            "title": "2 Preliminary Knowledge",
            "content": "Fourier Analysis (Stein & Weiss, 1971; Duoandikoetxea, 2024) is mathematical framework that decomposes functions into their constituent frequencies, revealing the underlying periodic structures within complex functions. At the heart of this analysis lies Fourier Series (Tolstov, 2012), which expresses periodic function as an infinite sum of sine and cosine terms. Mathematically, for function (x), its Fourier Series expansion can be represented as: (x) = a0 + (cid:88) (cid:18) n=1 an cos (cid:19) (cid:18) 2πnx + bn sin (cid:18) 2πnx (cid:19)(cid:19) , (1) where is the period of the function, and the coefficients an and bn are determined by integrating the function over one period: 2 an ="
        },
        {
            "title": "1\nT",
            "content": "(cid:90) 0 (x) cos (cid:19) (cid:18) 2πnx dx, bn ="
        },
        {
            "title": "1\nT",
            "content": "(cid:90) 0 (x) sin (cid:19) (cid:18) 2πnx dx. (2) The power of Fourier Series lies in its ability to represent wide variety of functions, including nonperiodic functions through periodic extensions, enabling the extraction of frequency components. Building on this mathematical foundation, FAN aims to embed the periodic characteristics directly into network architecture, enhancing generalization capabilities and performance on various tasks, particularly in scenarios requiring the identification of patterns and regularities."
        },
        {
            "title": "3 Fourier Analysis Network (FAN)",
            "content": "In this section, we first construct simple neural network modeled by the formula of Fourier Series, and then on this basis, we design FAN and provide its details. Finally, we discuss the difference between the FAN layer and the MLP layer. Consider task involving input-output pairs {xi, yi}, with the objective of identifying function (x) : Rdx Rdy that approximates the relationship such that yi (xi) for all xi, where dx and dy denote the dimensions of and y, respectively. To build simple neural network fS(x) that represents Fourier Series expansion of the function, specifically F{f (x)}, as described in Eq. (1), we can express fS(x) as follows: fS(x) a0 + (cid:88) (cid:18) an cos (cid:19) (cid:18) 2πnx + bn sin (cid:18) 2πnx (cid:19)(cid:19) , (I) = a0 + n=1 (cid:88) n=1 (cid:0)wc cos (cid:0)win x(cid:1) + ws sin (cid:0)win x(cid:1)(cid:1) , (II) = + [wc 1, ws + [ws 2, , wc 1, wc 2, , ws n] cos([win 1 win 1 win 2 win 2 win ]x) n] sin([win ]x) (3) = + Wc cos(Winx) + Ws sin(Winx), (III) = + Wout[cos(Winx) sin(Winx)], where Rdy , Win RN dx , and Wout Rdy2N are learnable parameters, (I) follows that the computation of an and bn computed via Eq. (2) is definite integral, (II) and (III) follows the equivalence of the matrix operations, [] and [, ] denotes the concatenation along the first and second dimension, respectively. To fully leverage the advantages of deep learning, we can stack the aforementioned network fS(x) to form deep network fD(x), where the i-th layer, denoted as li(x), retains the same structural design as fS(x). Therefore, fD(x) can be formulated as: fD(x) = lL lL1 l1 x, (4) where l1 denotes the application of the left function l1 to the right input x, that is l1(x). However, we discover that the direct stacking of fS(x) results in the primary parameters of the model fD(x) focusing on learning the angular frequency (ωn = 2πn ), thereby neglecting the learning of the Fourier coefficients (an and bn), as follows: fD(x) = lL(lL1 lL2 l1 x) = BL + out[cos(W in (l1:L1 x) sin(W in (l1:L1 x))] (5) where l1:L1 is defined as lL1 lL2 l1 x, in (l1:L1 x) is used to approximate the angular frequencies, and out is used to approximate the Fourier coefficients. Therefore, the capacity of fD(x) to fit the Fourier coefficients is independent of the depth of fD(x), which is an undesirable outcome. 3 Figure 2: Illustrations of MLP layer Φ(x) vs. FAN layer ϕ(x). To this end, we design FAN based on the following principles: 1) the capacity of FAN to represent the Fourier coefficients should be positively related to its depth; 2) the output of any hidden layer can be employed to model periodicity using Fourier Series through the subsequent layers. The first one enhances the expressive power of FAN for periodicity modeling by leveraging its depth, while the second one ensures that the features of FANs intermediate layers are available to perform periodicity modeling. Suppose we decouple fS(x) as follows: where fS(x) = fout fin x, fin(x) = [cos(Winx) sin(Winx)], fout(x) = + Woutx. (6) (7) (8) To satisfy both principles, the inputs of the intermediate layers in FAN necessitate to employ fin and fout simultaneously, rather than applying them sequentially. Finally, FAN is designed on this basis, with the FAN layer ϕ(x) defined as below: ϕ(x) [cos(Wpx) sin(Wpx)σ(B + px)], (9) where Wp Rdxdp , Rdxd , and Rd are learnable parameters (with the hyperparameters dp and indicating the first dimension of Wp and p, respectively), the layer output ϕ(x) R2dp+d , and σ denotes the activation function, which can further enhance its expressive power for periodicity modeling. The entire FAN is defined as the stacking of the FAN layer ϕ(x): FAN(x) = ϕL ϕL1 ϕ1 x, where ϕl(x) = (cid:26) [cos(W px) sin(W px)σ(Bl + px)], , BL + Lx, (10) (11) if < L, if = L, The illustrations of the MLP layer Φ(x) vs. the FAN layer ϕ(x) are shown in Figure 2. Note that the FAN layer ϕ(x) computed via Eq. (9) can seamlessly replace the MLP layer Φ(x) computed via Eq. (12) in various models with fewer parameters and FLOPs. The number of parameters and FLOPs of the FAN layer compared to the MLP layer are presented in Table 1. Table 1: Comparison of MLP layer and FAN layer, where dp is hyperparameter of FAN layer and defaults to 1 4 doutput in this paper, dinput and doutput denote the input and output dimensions of the neural network layer, respectively. In our evaluation, the FLOPs for any arithmetic operations are considered as 1, and for Boolean operations as 0."
        },
        {
            "title": "Formula\nNum of Params",
            "content": "Φ(x) = σ(Bm + Wmx) (dinput doutput) + doutput"
        },
        {
            "title": "FLOPs",
            "content": "2 (dinput doutput) +FLOPsnon-linear doutput"
        },
        {
            "title": "4 Experiments",
            "content": "ϕ(x) = [cos(Wpx) sin(Wpx)σ(B + px)] ) ((dinput doutput) + doutput) (1 dp doutput (1 dp doutput +FLOPsnon-linear doutput ) 2 (dinput doutput) In this section, we first introduce the baselines and implementation details of our experiments. Next, we verify the effectiveness of FAN in modeling and reasoning about periodic functions (Section 4.1). Finally, we demonstrate the superiority and generalizability of FAN across range of real-world tasks, including symbolic formula representation (Section 4.2), time series forecasting (Section 4.3), and language modeling (Section 4.4). Baselines. In our experiments, we mainly compare FAN with the following baselines. 1) MLP (Rosenblatt, 1958): the most classic model, which is widely used in the backbone of various models. 2) Transformer (Vaswani et al., 2017): prevalent model known for its self-attention mechanism, which achieves outstanding performance on various tasks. 3) KAN (Liu et al., 2024): an emerged model specialized for symbolic formula representation, which uses the b-spline functions instead of fixed activation functions. 4) LSTM (Hochreiter & Schmidhuber, 1997): well-known recurrent neural network (RNN) that can capture long-term dependencies on sequential data. 5) Mamba (Gu & Dao, 2023): an emerged selective state space model (SSM) that achieves competitive performance on some tasks with sequential inputs. Moreover, we also include the following variants of FAN into our comparisons: I) FAN (Gated): variant of FAN that adds gates to control the tendency of the layer, with the formula defined as ϕg(x) = [g cos(Wpx)g sin(Wpx)(1 g) σ(B + px)], where is learnable parameter. II) Transformer with FAN and Transformer with FAN (Gated): we replace each MLP layer in Transformer with the FAN layer computed via Eq. (9) and the layer of FAN (Gated), respectively. Implementation Details. We conduct our experiments on single GPU of Tesla A100-PCIe-40G. Unless otherwise specified, we use the following hyperparameters in the experiments. The model architecture consists of 3-12 layers, the activation function σ is set to GELU (Hendrycks & Gimpel, 2016), and the dimension of the projection matrix Wp is set to dp = 1 4 dh, where dh denotes the dimension of the hidden layers. We employ the AdamW optimizer (Loshchilov & Hutter, 2019) for the models training process. More experimental details and comprehensive setups of each task can be found in Appendix C. 4.1 Periodicity Modeling Setup. In periodic modeling tasks, we select periodic functions with practical significance and compare the models performance in learning the underlying principles of periodicity. Specifically, we generate data from periodic functions over large domain, using portion of this domain as training data and the entire domain as test data, i.e., part of test data would be out of the domain of training data. In this task, we compare FAN and its variant, FAN (Gated), with MLP, KAN, and Transformer. The input of each task is scalar. Results. Figure 3 illustrates the performance of FAN and other baselines in periodicity modeling. The results indicate that existing neural networks, including MLP, KAN, and Transformers, exhibit notable deficiencies in their ability to model periodicity. Although they attempt to fit these periodic functions, their ability limits their performance in modeling large domain of periodicity. In contrast, FAN significantly outperforms the baselines in all these tasks of periodicity modeling. 5 Figure 3: The performance of FAN in periodicity modeling compared to MLP, KAN, and Transformer, where the green line represents the test data within the domain of the training data, while the blue line represents the test data outside the domain of the training data. Moreover, FAN performs exceptionally well on test data both within and outside the domain of the training data, indicating that it is genuinely modeling periodicity rather than merely memorizing the training data. We also analyze the training process of different models on the tasks of learning complex periodic functions, as illustrated in Figure 4, which leads to the following findings. 1) FAN far exceeds the other baselines in both convergence speed and final effects. 2) In comparison to FAN, FAN (Gated) often achieves faster convergence, but the final performance remains comparable. 3) Although the baselines show stabilization or gradual reductions in training loss as the number of epochs increases, their modelingy may have diverged considerably from the distribution of the test data, resulting in sharp increase in test loss. This phenomenon further demonstrates the shortcomings of these models in capturing periodicity. 4.2 Symbolic Formula Representation Setup. Symbolic formula representation is common task in both mathematics and physics. We follow the experiments conducted in KANs paper (Liu et al., 2024), adhering to the same tasks, data, hyperparameters, and baselines. In addition to the original baselines, we also include Transformer for comparison in this task. Results. Figure 5 demonstrates the performance of different models applied to four common functions in mathematics and physics. From Figure 5, we can observe that while KAN remains competitive with FAN when the number of parameters is small, its performance declines significantly as the number of parameters increases. In contrast, as the number of parameters grows, FAN consistently Figure 4: Comparison of training and test losses for different models on the tasks of learning complex periodic functions. outperforms the other baselines, including MLP, KAN, and Transformer, in fitting these functions, despite many of these functions being only partially periodic or entirely non-periodic. These results indicate that although FAN enhances its ability to model periodicity, it does not compromise its capacity to fit non-periodic functions. Figure 5: Comparisons of FAN with the baselines, including MLP, KAN, and Transformer, across varying numbers of parameters on symbolic formula representation tasks. 4.3 Time Series Forecasting Setup. Time series forecasting plays critical role in various real-world applications. In our experiments, we employ four public datasets of this task to assess the model performance on time series forecasting, including Weather (Wu et al., 2021a), Exchange (Lai et al., 2018), Traffic (Wu et al., 2021a), and ETTh (Zhou et al., 2021) datasets. For each dataset, we input 96 previous time steps and forecast the subsequent time steps of {96, 192, 336, 720}. In this task, we choose the sequence models as baselines, including LSTM, Mamba, Transformer, Transformer with FAN , and Transformer with FAN (Gated). Results. As presented in Table 2, we compare the performance of Transformer with FAN and other sequence models for time series forecasting tasks on four public datasets. In most cases, Transformer with FAN and its gated version achieves the best performance on these tasks, compared to LSTM, Mamba, and the standard Transformer. The improvements of Transformer with FAN and FAN (Gated) over the standard Transformer are notable, with the average relative improvements ranging from 14.3% to 15.0% for MSE and from 7.6% to 7.9% for MAE. These results suggest that incorporating explicit periodic pattern encoding within neural networks improves time series forecasting performance in real-world applications. 7 Table 2: Performance of different sequence models on time series forecasting tasks, where Input Length = 96, the bold values indicate the lowest value on each row, and the improve means the relative improvements of using FAN and FAN (Gated) based on Transformer. Dataset Output Length LSTM (12.51 M) Mamba (12.69 M) Transformer (12.12 M) Transformer with FAN (11.06 M) Gated Default MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE 96 192 336 96 192 336 720 96 192 336 720 96 192 336 720 Weather Exchange Traffic ETTh Average (Improve) 1.069 1.090 0.992 1.391 0.938 1.241 1.645 1.949 0.659 0.668 0.644 0. 0.999 1.059 1.147 1.206 0.742 0.778 0.727 0.892 0.794 0.899 1.048 1.170 0.359 0.360 0.342 0.351 0.738 0.759 0.820 0.847 0.552 0.700 0.841 1. 0.908 1.328 1.512 2.350 0.666 0.671 0.665 0.662 0.860 0.849 1.005 0.994 0.519 0.595 0.667 0.803 0.748 0.925 0.992 1.271 0.377 0.381 0.374 0. 0.697 0.700 0.745 0.758 0.413 0.582 0.751 0.967 0.777 1.099 1.614 2.163 0.656 0.672 0.673 0.701 1.139 1.373 1.261 1.056 0.438 0.540 0.626 0. 0.681 0.800 1.029 1.204 0.357 0.363 0.360 0.380 0.853 0.932 0.924 0.819 0.292 0.535 0.637 0.845 0.685 0.998 1.511 1.658 0.647 0.649 0.665 0. 0.842 0.885 0.980 1.002 0.380 0.550 0.602 0.706 0.644 0.757 0.961 1.104 0.355 0.353 0.358 0.369 0.736 0.748 0.770 0.798 0.313 0.472 0.719 0. 0.657 0.968 1.266 2.063 0.643 0.657 0.656 0.673 0.873 0.914 0.999 1.031 0.431 0.525 0.581 0.670 0.623 0.741 0.905 1.205 0.347 0.354 0.353 0. 0.707 0.741 0.793 0.818 1.083 0.726 1.002 0.668 0. 0.689 0.845 15.0% 0.637 7.6% 0.852 14.3% 0.635 7.9% 4.4 Language Modeling Setup. Language modeling is fundamental task in natural language processing. In this experiment, we conduct language modeling using the SST-2 (Socher et al., 2013) dataset and evaluate the models performance on its test set, as well as on the related datasets such as IMDB (Maas et al., 2011), Sentiment140 (Sahni et al., 2017), and Amazon Reviews (Linden et al., 2003). These four classic datasets all belong to the field of sentiment analysis. In this task, the comparisons are between Transformer with FAN and FAN (Gated), along with other sequence models, including LSTM, Mamba, and Transformer. Table 3: Performance of different sequence models on language modeling tasks, where the models are trained on the training set of SST-2 and evaluated on the other datasets, the bold value indicates the best performance on each column, the bold italic indicates the best performance other than Transformer with FAN and FAN (Gated), and the improvements represent our relative improvements of using FAN based on Transformer. Model Num of Params Loss LSTM Mamba Transformer w/ FAN (Gated) w/ FAN 120.14M 0.4760 129.73M 0.4335 109.48M 0.4297 0.4250 95.33M 0.4094 95.32M SST-2 (test) IMDB Sentiment140 Amazon Reviews Acc 0.8060 0.7959 0.8119 0.8039 0.8154 Loss 0.6449 0.6863 0.5649 0.5817 0.5225 Acc 0.6438 0.6203 0.6994 0.7012 0.7398 Loss 0.8026 0.7871 0.8891 0.7941 0.8257 Acc 0.5979 0.5874 0.5779 0.6194 0.6093 Loss 0.5791 0.6163 0.5563 0.4835 0.4748 Acc 0.7152 0.6719 0.7155 0.7689 0.7763 Improvements 14.16M 4.72% 0.43% 7.51% 5.78% 7.13% 5.43% 14.65% 8.50% Results. We report the performance comparison between different sequence models across four sentiment analysis datasets, as shown in Table 3. We can find that our proposed Transformer with FAN demonstrates significantly superior performance compared to the standard Transformer and other baselines, such as LSTM and Mamba, especially for zero-shot cross-domain performance on IMDB, Sentiment140, and Amazon Reviewers datasets. Transformer with FAN achieves the relative improvements up to 14.65% and 8.50% in terms of Loss and Accuracy respectively, while reducing the number of parameters by about 14.16M. The result indicates the potential of periodicity modeling to enhance both effectiveness and generalization on cross-domain language modeling and sentiment analysis tasks."
        },
        {
            "title": "5 Related Work",
            "content": "In this section, we outline the two most relevant directions and associated papers of this work. Learning Periodicity with Neural Networks. Periodic functions are one of the most basic functions of importance to human society and natural science (Newton, 1687; Osborn & Sensier, 2002; Kwasnicki, 2008; De Groot & Franses, 2012; Zhang et al., 2017). However, commonly used neural networks, such as MLPs and transformers, struggle to extrapolate periodic functions beyond the scope of the training data. This limitation arises from the lack of inherent periodicity in their inductive biases. Some previous works (Silvescu, 1999; Liu, 2013; Parascandolo et al., 2016; Uteuliyeva et al., 2020) proposed merely using standard periodic functions themselves or their linear combinations as activation functions, which only work well on some shallow and simple models. On this basis, work (Liu et al., 2020) introduced the Snake function, i.e., + sin2(x), as the activation function. However, we observed that it can fit periodic functions to certain extent, but its effect is limited, as demonstrated in Appendix D. Therefore, although some previous studies have attempted to integrate the periodic information into neural networks, their actual performance and range of applications remain heavily constrained. Fourier-based Neural Network. Previous studies have explored Fourier-based neural networks to enhance the computational tasks (Zuo & Cai, 2005; Tan, 2006; Zuo & Cai, 2008; Zuo et al., 2008; Li et al., 2021; Chen et al., 2022). Fourier Neural Networks (Silvescu, 1999; Ngom & Marin, 2021) are shallow feedforward networks that employ cosine activation functions to map inputs to their Fourier decompositions. Work (Lee et al., 2021) directly utilized the Fourier Series constructed by shallow neural network for generating periodic signals. In addition, work (Jiang et al., 2022) introduces Fourier Series at the end of models to embed periodic components within the network. These approaches generally possess similar principle as Eq. (3), using neural network to simulate the formula of Fourier Series. However, this leads to the same problem as in Eq. (5), i.e., they are hard to serve as building blocks for deep neural networks, which limits these approaches capabilities. In this paper, we design FAN to address these challenges, which performs exceptionally well on periodicity modeling and range of real-world tasks."
        },
        {
            "title": "6 Discussion",
            "content": "In this section, we mainly discuss the expressive power and application scope of FAN as follows. First, FAN theoretically possesses the same expressive power as MLP as it also adheres to the universal approximation theorem, which ensures its capacity for functional approximation. Moreover, FAN introduces an important enhancement by explicitly incorporating periodicity, feature absent in traditional MLPs. Through this design, FAN not only retains the capabilities of MLP but also enhances its ability to capture periodic characteristics in data. Therefore, FAN can be seen as strong alternative to MLP. Second, beyond tasks that explicitly require periodicity modeling, FAN also has utility in broader range of applications. This has been evidenced by our experiments on real-world tasks, such as symbolic formula representation, time series forecasting, and language modeling, where FAN outperforms MLP and other baselines. In fact, many machine learning tasks may harbor hidden forms of periodicity, even without explicit requirements to include periodicity, such as mathematical operations and logic reasoning. If the neural network lacks the ability to model periodic components, it could impair its learning efficiency. From deeper perspective, periodicity is not just data feature but reflects form of structural knowledgeone that allows for the transfer and reuse of abstract rules and principles across different contexts."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we have proposed Fourier Analysis Network (FAN), novel neural network architecture for tackling the problem of periodicity modeling, which utilizes Fourier Series to facilitate capturing the underlying principles within data and reasoning. Experimental results demonstrate that FAN can successfully fit variety of both basic and complex periodic functions, whereas other 9 approaches failed. Moreover, FAN and its combination with Transformer also exhibit superior performance in multiple real-world tasks, including symbolic formula representation, time series forecasting, and language modeling tasks, outperforming existing neural networks such as MLP, KAN, Transformer, LSTM, and Mamba. These promising results, especially the stronger performance and the fewer parameters and FLOPs compared to MLP, suggest its potential to become key component of foundational models. In the future, we aim to further increase the scale of FAN and expand its scope of application, reinforcing its role as versatile and powerful building block in the machine learning landscape."
        },
        {
            "title": "8 Acknowledgement",
            "content": "We would like to thank Lecheng Wang and Xuanming Zhang for their participation in discussions related to this work."
        },
        {
            "title": "References",
            "content": "Hanlong Chen, Luzhe Huang, Tairan Liu, and Aydogan Ozcan. Fourier imager network (FIN): deep neural network for hologram reconstruction with superior external generalization. Light: Science & Applications, 2022. Bert De Groot and Philip Hans Franses. Common socio-economic cycle periods. Technological Forecasting and Social Change, 79(1):5968, 2012. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL http: //arxiv.org/abs/1810.04805. Javier Duoandikoetxea. Fourier analysis, volume 29. American Mathematical Society, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. CoRR, abs/2312.00752, 2023. Simon Haykin. Neural networks: comprehensive foundation. Prentice Hall PTR, 1998. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735 1780, 1997. Kurt Hornik, Maxwell B. Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359366, 1989. Song Jiang, Tahin Syed, Xuan Zhu, Joshua Levy, Boris Aronchik, and Yizhou Sun. Bridging selfattention and time series decomposition for periodic forecasting. In CIKM, pp. 32023211. ACM, 2022. Witold Kwasnicki. Kitchin, juglar and kuznetz business cycles revisited. Wroclaw: Institute of Economic Sciences, 2008. Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling longand short-term temporal patterns with deep neural networks. In SIGIR, pp. 95104. ACM, 2018. Jiyoung Lee, Wonjae Kim, Daehoon Gwak, and Edward Choi. Conditional generation of periodic signals with fourier-based decoder. CoRR, abs/2110.12365, 2021. Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew M. Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. In ICLR. OpenReview.net, 2021. Greg Linden, Brent Smith, and Jeremy York. Amazon.com recommendations: Item-to-item collaborative filtering. IEEE Internet Comput., 7(1):7680, 2003. 10 Shuang Liu. Fourier neural network for machine learning. In ICMLC, pp. 285290. IEEE, 2013. Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljacic, Thomas Y. Hou, and Max Tegmark. KAN: kolmogorov-arnold networks. CoRR, abs/2404.19756, 2024. Ziyin Liu, Tilman Hartwig, and Masahito Ueda. Neural networks fail to learn periodic functions and how to fix it. In NeurIPS, 2020. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR (Poster). OpenReview.net, 2019. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In ACL, pp. 142150. The Association for Computer Linguistics, 2011. Isaac Newton. Philosophiae naturalis principia mathematica. William Dawson & Sons Ltd., London, 1687. Marieme Ngom and Oana Marin. Fourier neural networks as function approximators and differential equation solvers. Stat. Anal. Data Min., 14(6):647661, 2021. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. Denise R. Osborn and Marianne Sensier. The prediction of business cycle phases: Financial variables and international linkages. National Institute Economic Review, 182(1):96105, 2002. doi: 10.1177/002795010218200110. URL https://doi.org/10.1177/002795010218200110. Giambattista Parascandolo, Heikki Huttunen, and Tuomas Virtanen. Taming the waves: sine as activation function in deep neural networks. 2016. Frank Rosenblatt. The perceptron: probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386, 1958. Tapan Sahni, Chinmay Chandak, Naveen Reddy Chedeti, and Manish Singh. Efficient twitter sentiment classification using subjective distant supervision. In COMSNETS, pp. 548553. IEEE, 2017. Adrian Silvescu. Fourier neural networks. In IJCNN, pp. 488491. IEEE, 1999. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. Recursive deep models for semantic compositionality over sentiment treebank. In EMNLP, pp. 16311642. ACL, 2013. Elias Stein and Guido Weiss. Introduction to Fourier analysis on Euclidean spaces, volume 1. Princeton university press, 1971. HS Tan. Fourier neural networks and generalized single hidden layer networks in aircraft engine fault diagnostics. 2006. Georgi Tolstov. Fourier series. Courier Corporation, 2012. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. Malika Uteuliyeva, Abylay Zhumekenov, Rustem Takhanov, Zhenisbek Assylbekov, Alejandro J. Castro, and Olzhas Kabdolov. Fourier neural networks: comparative study. Intell. Data Anal., 24(5):11071120, 2020. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 59986008, 2017. Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in neural information processing systems, 34:2241922430, 2021a. Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in neural information processing systems, 34:2241922430, 2021b. Liheng Zhang, Charu Aggarwal, and Guo-Jun Qi. Stock price prediction via discovering multifrequency trading patterns. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 17, pp. 21412149, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450348874. doi: 10.1145/3097983. 3098117. URL https://doi.org/10.1145/3097983.3098117. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In AAAI, pp. 1110611115. AAAI Press, 2021. Wei Zuo and Lilong Cai. Tracking control of nonlinear systems using fourier neural network. In Proceedings, 2005 IEEE/ASME International Conference on Advanced Intelligent Mechatronics., pp. 670675. IEEE, 2005. Wei Zuo and Lilong Cai. Adaptive-fourier-neural-network-based control for class of uncertain nonlinear systems. IEEE transactions on neural networks, 19(10):16891701, 2008. Wei Zuo, Yang Zhu, and Lilong Cai. Fourier-neural-network-based learning control for class of nonlinear systems with flexible components. IEEE transactions on neural networks, 20(1): 139151, 2008."
        },
        {
            "title": "A MLP",
            "content": "The MLP layer Φ(x) is defined as: Φ(x) = σ(Bm + Wmx), (12) where Bm Rdm and Rdxdm are learnable parameters with the hyperparameter dm indicating the first dimension of Wm, σ denotes the activation function, and MLP can be defined as the stacking of the MLP layer Φ(x): MLP(x) = ΦL ΦL1 Φ1 x, where Φl(x) ="
        },
        {
            "title": "B Additional Experiments",
            "content": "(cid:26) σ(Bl + BL + Lx, mx), if < L, if = L, (13) (14) Table 4: Experiments on image recognition tasks, where Accuracy* means the best Accuracy, Accuracy means the Accuracy at the last epoch, and OOD Accuracy means the Accuracy on other paired datasets. The bold values indicate the highest value between CNN and CNN w/ FAN under the same setting, and the improve means the relative improvements of using FAN based on CNN. Fashion-MNIST-C represents the Fashion-MNIST-corrupted dataset. Dataset MNIST MNIST-M Accuracy* OOD Accuracy* Accuracy OOD Accuracy CNN w/ FAN CNN w/ FAN CNN w/ FAN CNN w/ FAN Fashion-MNIST 94.15 Fashion-MNIST-C 88.61 99.63 94.52 99.67 94.23 94.47 88. 28.85 82.85 49.82 91.45 30.3 83.55 51.88 91.59 99.55 94.29 94.05 88. 99.64 94.22 94.21 88.59 22.12 80.07 48.08 91.41 21.64 81.44 50.3 91. Figure 6: Additional Experiments on Periodicity Modeling Tasks. 13 Table 5: Additional Experiments on time series forecasting tasks with Instance normalization (?), where Input Length = 96, the bold values indicate the lowest value on each row, and the improve means the relative improvements of using FAN and FAN (Gated) based on Transformer. We find that applying instance normalization before the architecture can effectively improve the performance, therefore we report the results of this setup here."
        },
        {
            "title": "Output\nLength",
            "content": "Transformer (12.12 M) Transformer with FAN (11.06 M)"
        },
        {
            "title": "Default",
            "content": "MSE MAE MSE MAE MSE MAE 96 192 336 720 96 192 336 720 96 192 336 720 96 192 336 720 96 192"
        },
        {
            "title": "Exchange",
            "content": "Traffic ETTh1 ETTh2 Average (Improve) 0.1772 0.2438 0.3077 0.4253 0.1433 0.2563 0.5273 1. 0.6160 0.6329 0.6369 0.6555 0.5339 0.5633 0.7576 0.7411 0.3881 0.5766 0.5782 0.5841 0.2301 0.2844 0.3267 0.3982 0.2653 0.3552 0.5218 0.9273 0.3449 0.3479 0.3485 0. 0.4910 0.5209 0.5813 0.6177 0.4097 0.4999 0.5100 0.5230 0.1864 0.2445 0.3156 0.3909 0.1157 0.2539 0.4329 1.5783 0.6030 0.6239 0.6416 0.6645 0.5503 0.5906 0.6640 0. 0.4082 0.4695 0.5556 0.5070 0.2352 0.2834 0.3320 0.3782 0.2452 0.3611 0.4891 0.9303 0.3334 0.3404 0.3487 0.3574 0.5216 0.5346 0.5636 0.6066 0.4292 0.4514 0.5012 0. 0.1756 0.2327 0.3118 0.4113 0.1436 0.2651 0.5092 1.0599 0.6109 0.6258 0.6200 0.6412 0.5378 0.5968 0.7525 0.7328 0.3833 0.5039 0.5417 0.5272 0.2247 0.2760 0.3291 0. 0.2666 0.3757 0.5326 0.7657 0.3319 0.3370 0.3380 0.3525 0.4983 0.5265 0.5933 0.6142 0.4149 0.4640 0.4940 0.4951 0.554 0. 0.526 5.1% 0.436 1.9% 0.509 8.2% 0.430 3.2%"
        },
        {
            "title": "C Experimental Details",
            "content": "C.1 Setup of Periodicity Modeling In periodicity modeling tasks, FAN, MLP, and KAN each consist of three layers with comparable FLOPs, while the Transformer model comprises twelve layers. For consistency, we set the hidden layer dimension (dh) to 2048 for FAN, MLP, and Transformer. In the case of KAN, we follow its original paper (Liu et al., 2024), where the spline order (K) and the number of spline intervals (G) are set to 3 and 50, respectively. We apply learning rate of 1 105 for training all models. We ensured that the data density of each period in tasks was consistent, meaning that each cycle contained fixed quantity of 10,000 training data points. C.2 Setup of Symbolic Formula Representation In symbolic formula representation tasks, we used the create dataset function from the official KAN repository to generate the datasets. Each dataset contains 3000 training samples and 1000 test samples, with all input variables randomly sampled from the range [-1, 1]. We followed the training settings from the original KAN paper, training all methods using LBFGS for 1800 steps. For KAN, we increased the number of grid points to scale up the parameter size, covering = {3, 5, 10, 20, 50, 100, 200, 500, 1000}. For other methods, we scaled up the parameter size by increasing the number of layers and the dimensions of hidden layers. 14 C.3 Setup of Time Series Forecasting In time series forecasting task, we implement our model based on the codebase by (Wu et al., 2021b). Each model comprises 2 encoder layers and 1 decoder layer. We fix the hidden size for both the Transformer and our model at 512, with the feedforward dimension set to 2048 (four times the hidden size). The parameter sizes detailed in the main text correspond to the Exchange dataset; variations in the number of variables across different datasets influence the linear layers in the model. We adjust the hidden sizes of the other models to align with the Transformer parameters for fairness. C.4 Setup of Language Modeling In language modeling task, we employ the BERT tokenizer (Devlin et al., 2018) and an embedding layer with dimensionality of 768, except for Mamba, which adheres to its default settings as specified in the original paper (Gu & Dao, 2023). The architecture features 4, 24, and 12 layers with hidden sizes of 1800, 768, and 768 for LSTM, Mamba, and Transformers, respectively. To mitigate training stagnation in deeper LSTM models, we reduce the number of layers while increasing the hidden size to balance the parameters. Importantly, Mambas layer count is twice that of similarly sized Transformer, as each layer consists of two Mamba blocks (Multihead attention block + MLP block). C.5 Setup of Image Recognition In image recognition tasks, we employ simple CNN generated by ChatGPT as the baseline model, which consists of four Convolutional Layers and two MLP Layers. We replace MLP with FAN in CNN, i.e. CNN with FAN, as the counterpart, ensuring that they have similar parameters. For each task, we use stochastic gradient descent with momentum (SGDM) as the optimizer, the learning rate is set to 0.01, and the training process runs for 100 epochs."
        },
        {
            "title": "D Comparison of FAN and Snake Activation Function",
            "content": "Figure 7: Comparisons of FAN with MLP (Snake) (Liu et al., 2020) in fitting periodic functions."
        }
    ],
    "affiliations": [
        "ByteDance",
        "School of Computer Science, Peking University"
    ]
}