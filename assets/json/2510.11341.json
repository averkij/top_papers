{
    "paper_title": "InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models",
    "authors": [
        "Haomin Wang",
        "Jinhui Yin",
        "Qi Wei",
        "Wenguang Zeng",
        "Lixin Gu",
        "Shenglong Ye",
        "Zhangwei Gao",
        "Yaohui Wang",
        "Yanting Zhang",
        "Yuanqi Li",
        "Yanwen Guo",
        "Wenhai Wang",
        "Kai Chen",
        "Yu Qiao",
        "Hongjie Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "General SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity. In response, we leverage the strong transfer and generalization capabilities of multimodal large language models (MLLMs) to achieve unified modeling for SVG understanding, editing, and generation. We present the InternSVG family, an integrated data-benchmark-model suite. At its core is SAgoge, the largest and most comprehensive multimodal dataset for SVG tasks, encompassing both static graphics and dynamic animations. It covers icons, long-sequence illustrations, scientific diagrams, and dynamic animations, supporting tasks of varied difficulty levels and providing deeper hierarchies with richer attributes compared to previous datasets. Based on this resource, we introduce SArena, a companion benchmark with comprehensive task definitions and standardized evaluation that aligns with the domains and difficulty spectrum covered by SAgoge. Building on these foundations, we propose InternSVG, a unified MLLM for SVG understanding, editing, and generation with SVG-specific special tokens, subword-based embedding initialization, and a two-stage training strategy that progresses from short static SVGs to long-sequence illustrations and complex animations. This unified formulation induces positive transfer and improves overall performance. Experiments on SArena and prior benchmark confirm that InternSVG achieves substantial gains and consistently outperforms leading open and proprietary counterparts."
        },
        {
            "title": "Start",
            "content": "INTERNSVG: TOWARDS UNIFIED SVG TASKS WITH MULTIMODAL LARGE LANGUAGE MODELS Haomin Wang1,2 Jinhui Yin3,2 Qi Wei3,2 Wenguang Zeng4 Lixin Gu2 Shenglong Ye2 Zhangwei Gao1,2 Yaohui Wang2 Yanting Zhang4 Yuanqi Li3 Yanwen Guo3 Wenhai Wang5 Kai Chen2 Yu Qiao2 Hongjie Zhang2 1 Shanghai Jiao Tong University 4 Donghua University Project Page: https://hmwang2002.github.io/release/internsvg 5 The Chinese University of Hong Kong 2 Shanghai AI Laboratory 3 Nanjing University 5 2 0 O 3 1 ] . [ 1 1 4 3 1 1 . 0 1 5 2 : r Figure 1: Overview of our InternSVG family. SAgoge provides large-scale and diverse SVG samples across multiple domains. SArena enables comprehensive assessment of existing MLLMs on SVG tasks. InternSVG supports unified modeling for SVG understanding, editing, and generation."
        },
        {
            "title": "ABSTRACT",
            "content": "General SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity. In response, we leverage the strong transfer and generalization capabilities of multimodal large language models (MLLMs) to achieve unified modeling for SVG understanding, editing, and generation. We present the InternSVG family, an integrated databenchmarkmodel suite. At its core is SAgoge, the largest and most comprehensive multimodal dataset for SVG tasks, encompassing both static graphics and dynamic animations. It covers icons, long-sequence illustrations, scientific diagrams, and dynamic animations, supporting tasks of varied difficulty levels and providing deeper hierarchies with richer attributes compared to previous datasets. Based on this resource, we introduce SArena, companion benchmark with comprehensive task definitions and standardized evaluation that aligns with the domains and difficulty spectrum covered by SAgoge. Building on these foundations, we propose InternSVG, unified MLLM for SVG understanding, editing, and generation with SVG-specific special tokens, subword-based embedding initialization, and two-stage training strategy that progresses from short static SVGs to long-sequence illustrations and complex animations. This unified formulation induces positive transfer and improves overall performance. Experiments on SArena and prior benchmark confirm that InternSVG achieves substantial gains and consistently outperforms leading open and proprietary counterparts. Equal Contribution. Corresponding Author: nju.zhanghongjie@gmail.com."
        },
        {
            "title": "INTRODUCTION",
            "content": "Scalable Vector Graphics (SVG) is an XML-based standard for 2D graphics that offers compact storage, fine-grained editability, and resolution-independent rendering across displays. It can function either as stand-alone format or as an embedded component within other documents, while also supporting interactivity and animation. Additionally, it interoperates smoothly with CSS, the DOM, and JavaScript in web contexts. These properties have led to broad adoption in web design, scientific visualization, and computer-aided design. Nevertheless, enabling machines to understand, generate, and edit SVGs remains challenging due to the methodological complexity involved, the scarcity of large-scale and high-quality training corpora, and the requirement for models that generalize across tasks rather than specializing in isolation. Existing resources and methods face three major limitations. First, current datasets provide limited support for unified modeling of SVG tasks. Most datasets and benchmarks target single task, such as semantic understanding in SGP-Bench (Qiu et al., 2024), instruction-based editing in SVGEditBench (Nishina & Matsui, 2024), or Text-to-SVG and Image-to-SVG generation in ColorSVG100K (Chen & Pan, 2025), SVG-Stack (Rodriguez et al., 2025), and MMSVG (Yang et al., 2025b). This narrow scope leads to fragmented supervision and evaluation, which in turn limits transfer across understanding, editing, and generation. Second, existing datasets are limited in both scale and diversity. UniSVG (Li et al., 2025) focuses on unified SVG understanding and generation, but it contains only about 525k samples. SVGenius (Chen et al., 2025) provides comprehensive benchmark spanning understanding, editing, and generation, while it contains only about 2,400 queries, making it suitable for evaluation rather than large-scale training. More broadly, current datasets concentrate on common static images such as icons and illustrations, while paying insufficient attention to SVGs with specific applications in professional domains. Third, existing methods commonly lack transferability and generalization. Optimization-based and differentiable rasterization pipelines scale poorly and lack semantic reasoning (Li et al., 2020; Ma et al., 2022). Although some approaches (Jain et al., 2023; Xing et al., 2024) employ diffusion models to improve visual fidelity, the resulting SVGs often suffer from limited editability and weak detail-aware control. Recently, LLM-based methods (Xing et al., 2025; Rodriguez et al., 2025; Yang et al., 2025b) have achieved significant progress in Text-to-SVG and Image-to-SVG generation, but they struggle to generalize to long sequences and complex SVG content, making it difficult to ensure generation quality. Moreover, these methods largely overlook tasks related to SVG understanding and editing. Name Datasets these overcome limitations, we Understanding Editing Generation Multi-Dom #Samples ColorSVG-100K SVG-Stack MMSVG SVGX UniSVG DeepSVG SAgoge (Ours) Table 1: Comparison of SVG datasets and benchmarks. proTo pose the InternSVG family, an integrated databenchmarkmodel suite for unified SVG modeling. Specifically, we introduce SAgoge, unified multimodal dataset that jointly supports SVG understanding, editing, and generation, covering both static graphics and dynamic animations. SAgoge combines Internet-sourced and synthetic SVG data across icons, illustrations, chemical structural formulas, and animations, totaling about 16 million training samples. Comparison of SAgoge and other datasets is shown in Table 1. To the best of our knowledge, SAgoge is the largest multimodal SVG dataset to date, with the broadest task coverage and the most comprehensive range of difficulty. It includes two understanding tasks, ten editing tasks, and four generation tasks, covering semantic understanding, icon editing and generation, long-sequence illustration generation, animation generation, and the generation of scientific diagrams. To enable rigorous and comparable assessment, we propose SArena, companion benchmark that standardizes tasks and metrics for understanding, editing, and generation, with sub-benchmarks covering icons, illustrations, chemical structural formulas, and animations. Together, SAgoge and SArena provide the scale, diversity, full task coverage, and standardized measurement needed to move beyond fragmented evaluations and enable systematic investigation of general SVG modeling in unified setting. SGP-Bench SVGEditBench VGBench SVGenius SArena (Ours) 100k 2.2M 2.0M 1.0M 525k 100k 16M 4.3k 1.3k 10.1k 2.3k 31k Benchmarks Building on SAgoge and the accompanying SArena for systematic evaluation, we introduce InternSVG, unified MLLM for SVG understanding, editing, and generation. InternSVG augments pretrained visionlanguage backbone with SVG-specific tokenization, introducing compact special 2 tokens for tags, attributes, and coordinates to reduce sequence length while retaining geometric and hierarchical structure. These tokens are initialized with subword-based strategy that anchors them in the pretrained embedding space, stabilizing early training and accelerating convergence. Training adopts two-stage strategy that progresses from short static SVGs to longer illustrations and complex animations. Through extensive experiments, we demonstrate that unified modeling can effectively improve performance across understanding, editing, and generation tasks. Comprehensive evaluations further show that our InternSVG surpasses both open-source and proprietary models on SArena and previous benchmarks. For example, on the SArena-Icon benchmark, InternSVG surpasses Claude-Sonnet-4, the strongest proprietary baseline on SVG tasks, by about 11% higher acc in understanding tasks, 34% higher PSNR in editing tasks, 56% lower FID in Text-to-SVG tasks, and 22% higher SSIM in Image-to-SVG tasks. In summary, our contributions are below: (1) We construct SAgoge, the largest and most comprehensive multimodal SVG dataset to date, encompassing static graphics and animations with over 16 million training samples. To enable rigorous and comparable evaluation, we further establish SArena, companion benchmark that standardizes tasks and metrics across SVG understanding, editing, and generation. (2) We propose InternSVG, unified MLLM for SVG understanding, editing, and generation. It introduces SVG-specific tokenization with subword-initialized special tokens and adopts two-stage training strategy to support effective cross-task generalization. (3) We conduct extensive experiments to demonstrate the benefits of unified modeling. The results on SArena and prior benchmarks show that our InternSVG outperforms traditional approaches as well as general-purpose open-source and proprietary models."
        },
        {
            "title": "2.1 SVG DATASETS AND BENCHMARKS",
            "content": "Most existing SVG datasets and benchmarks are limited in task coverage or data type and remain too small for effective model training, leading to fragmented evaluations and limited insights into generalization across tasks and complexity. SGP-Bench (Qiu et al., 2024) evaluates semantic comprehension and consistency in symbolic graphics programs. SVGEditBench (Nishina & Matsui, 2024) and its extension V2 (Nishina & Matsui, 2025) focus narrowly on instruction-based SVG editing measured by low-level syntactic metrics. On the generative side, SVG-Stack (Rodriguez et al., 2025), SVGX (Xing et al., 2025), MMSVG (Yang et al., 2025b), and ColorSVG-100K (Chen & Pan, 2025) address Text-to-SVG and Image-to-SVG generation, while VGBench Zou et al. (2024) and UniSVG (Li et al., 2025) jointly evaluate understanding and generation. DeepSVG (Carlier et al., 2020) introduces dataset of 100K SVG icons and explores generation, interpolation, and latentspace animation of static and limited animated graphics, but lacks rich editing instructions and image-conditioned generation. SVGenius (Chen et al., 2025) introduces comprehensive benchmark covering understanding, editing, and generation with systematic complexity levels and multidimensional metrics, but it includes only about 2,400 queries, making it sufficient for evaluation yet inadequate for training. In contrast, our SAgoge is substantially larger and more diverse, encompassing both static graphics and SVG animations. It unifies SVG understanding, editing, and generation, and with approximately 16M task samples, its scale and diversity enable robust model training and comprehensive evaluation across the full spectrum of SVG tasks, which effectively address the coverage and scalability limitations of prior datasets."
        },
        {
            "title": "2.2 SVG MODELING METHODS",
            "content": "Early research on SVG modeling treated vector graphics as sequences of geometric primitives and relied on specialized generative architectures trained on limited domains (Ha & Eck, 2017; Lopes et al., 2019; Frans et al., 2022; Vinker et al., 2022; Hu et al., 2024). Approaches such as DeepSVG (Carlier et al., 2020) used hierarchical VAEs with Transformer decoders to generate icons, while optimization-based methods like DiffVG (Li et al., 2020) and LIVE (Ma et al., 2022) applied differentiable rasterization to align SVG primitives with target images iteratively. Although these techniques captured low-level structure, they lacked semantic understanding, struggled with 3 complex compositions, and were computationally costly, often producing redundant or unsimplified paths. More recent works have explored diffusion-based pipelines, such as VectorFusion (Jain et al., 2023) and SVGDreamer (Xing et al., 2024), which refine vector renderings via Text-to-Image diffusion combined with SVG constraints. These approaches improve visual fidelity but remain limited to generation and offer only restricted controllability and task diversity. The emergence of large language models (LLMs) (Achiam et al., 2023; Yang et al., 2025a; OpenAI, 2025) has shifted SVG modeling toward code-centric paradigms that leverage the textual nature of SVG. Methods such as StarVector (Rodriguez et al., 2025), OmniSVG (Yang et al., 2025b), LLM4SVG (Xing et al., 2025), and SVGBuilder (Chen & Pan, 2025) integrate visual encoders with text decoders to generate SVGs from textual or visual inputs, achieving notable advances in Text-to-SVG and Image-to-SVG synthesis. However, these approaches are fragmented across isolated tasks, with limited mechanisms to handle the increasing complexity of SVG content. Different from prior approaches, InternSVG is unified MLLM for SVG understanding, editing, and generation. It is capable of understanding both the semantic and structural information of SVG code, editing graphics based on user instructions, and generating SVGs from textual or visual prompts."
        },
        {
            "title": "3 DATASET AND BENCHMARK",
            "content": "Figure 2: Overview of the dataset construction pipeline. Raw SVGs are gathered from the web and custom synthesis pipeline, then normalized to 128 128 canvas and simplified to shorten code. The rendered images or videos, processed SVG code, and handcrafted prompts are fed to an MLLM to synthesize high-quality training samples for understanding, editing, and generation."
        },
        {
            "title": "3.1 SAGOGE DATASET",
            "content": "We introduce SAgoge, large-scale and comprehensive dataset for SVG tasks with more than 16 million training samples spanning icons, illustrations, chemical structures, and animations. The construction pipeline is illustrated in Figure 2."
        },
        {
            "title": "3.1.1 TASK DEFINITION",
            "content": "SAgoge is comprehensive resource that supports unified modeling of SVG understanding, editing, and generation. We define fine-grained task suite spanning diverse vector-graphic categories to enable consistent training and evaluation. Understanding comprises SVG description and multiplechoice question answering. Editing for icons contains ten subtasks organized by difficulty, including eight low-level operations (color editing, stroke addition, translation, scaling, rotation, flipping, transparency adjustment, and cropping) and two high-level tasks (semantic color editing and style transfer). Generation encompasses Text-to-SVG and Image-to-SVG for icons, illustrations, and chemical structures, as well as Text-to-SANI and Video-to-SANI for animations. Appendix C.3.3 presents examples of tasks in SAgoge."
        },
        {
            "title": "3.1.2 DATASET CREATION PIPELINE",
            "content": "Data Source. SAgoge integrates Internet-sourced and synthetic SVGs. Icons and subset of illustrations and animations are collected from public repositories such as Iconfont, SVGRepo, and OpenClipart. Chemical structures are generated by converting PubChem SDF files to SVG with the Open Babel toolkit. Due to the scarcity of open-source SVGs for illustrations and animations, we built dedicated data synthesis pipeline to expand coverage in these domains. Implementation details are provided in Appendix C.3 and data statistics of SAgoge are summarized in Table 17. Normalize and Simplify. To improve training efficiency and reduce complexity, all SVGs are normalized to 128 128 viewBox. Because collected files are often verbose and LLMs operate under limited context lengths, we further simplify the code by removing nonessential metadata, comments, and redundant declarations while preserving the core geometric primitives, semantic information, and hierarchical group structures. This normalization and simplification shorten token sequences, improve consistency, and make the corpus better suited for scalable training and evaluation. Data annotation. We render SVGs to raster images and videos and, under carefully designed prompts, employ MLLMs to produce high-quality training annotations. Specifically, GPT-4o (Hurst et al., 2024), Qwen2.5 VL (Bai et al., 2025) and InternVL3 (Zhu et al., 2025) are employed for image annotation, and Gemini 2.0 Flash (Google DeepMind, 2024) for video annotation. Prompt templates for dataset construction are provided in Appendix C.4.1."
        },
        {
            "title": "3.2 SARENA BENCHMARK",
            "content": "To enable systematic evaluation across SVG understanding, editing, and generation, we introduce SArena, benchmark that aligns with the domains and difficulty spectrum covered by SAgoge and provides standardized tasks and metrics. SArena includes 4 sub-benchmarks, i.e., icons, illustrations, chemical structures, and animation. To ensure reliability and fairness of evaluation, all SVG samples used in SArena are carefully curated through combination of automated preprocessing and manual screening, with low-quality, corrupted, and semantically ambiguous files removed. Data statistics of SArena are shown in Appendix C.2."
        },
        {
            "title": "3.2.1 EVALUATION METRICS",
            "content": "Understanding. We evaluate code-level comprehension with four-option multiple-choice QA. The model receives only the SVG code and must infer the answer from its semantics and structure, and performance is reported as accuracy. Editing. The model edits the original SVG code to follow textual instructions. Performance is evaluated on rendered outputs using DINO score (Oquab et al., 2023), Structural Similarity Index (SSIM) (Wang et al., 2004), Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018), and Peak Signal-to-Noise Ratio (PSNR). Text-to-SVG. The model synthesizes SVGs from textual instructions. Quality is evaluated with FID (Theis et al., 2015) and FID-CLIP (Wu et al., 2023) for distributional and semantic fidelity, CLIP-T2I (Radford et al., 2021; Hessel et al., 2021) for textSVG alignment and CLIP-I2I for visual similarity. Image-to-SVG. The model receives reference image and textual instructions and generates the corresponding SVG. Evaluation mirrors the editing task and uses DINO score, SSIM, LPIPS, and PSNR to measure visual similarity between the rendered SVG and the ground-truth image. Text-to-SANI. The model generates SVG animations from textual instructions. Evaluation uses FVD (Unterthiner et al., 2018) for distributional consistency and ViCLIP-based CLIPScores (Wang et al., 2023), with CLIP-T2V quantifying textanimation alignment and CLIP-V2V measuring visual similarity to reference videos. Video-to-SANI. In this task the model generates SVG animations conditioned on an input video and textual instructions, enabling cross-modal animation generation. Evaluation follows the Image-toSVG protocol and uses DINO, SSIM, LPIPS, and PSNR computed between rendered SVG frames and reference frames on sampled timestamps, with scores averaged to produce the final result. 5 Figure 3: (a) Overall architecuture of InternSVG. (b) Distribution of the number of tokens per SVG before and after adding customized special tokens in the tokenizer. (c) Comparison of training loss curves between subword-based embedding initialization and random initialization. In addition to the task-specific metrics, we also account for cases where the model generates syntactically incorrect or incomplete SVGs that cannot be rendered. Any unrenderable SVG is replaced with pure black image or video as penalty during metric computation, ensuring that the evaluation fairly reflects both the quality of valid outputs and the models robustness in generation."
        },
        {
            "title": "4.1 MODEL ARCHITECTURE",
            "content": "As illustrated in Figure 3, InternSVG follows the ViTMLPLLM paradigm (Liu et al., 2023), using InternViT-300M (Chen et al., 2024) as the vision encoder and Qwen2.5-7B (Qwen Team, 2025) as the language model. We further design SVG-specific special tokens and introduce tailored embedding initialization strategy to incorporate SVG content effectively. Special tokens. We design 55 tag tokens and 42 attribute tokens based on SVG grammar. Tag tokens cover core structural elements such as svg, path, and circle, as well as animation elements like animate, animateMotion, and animateTransform. Attribute tokens include common geometric attributes (e.g., viewBox, cx, cy, d) and animation-related attributes (e.g., dur, from, to, repeatCount). To represent numerical values, we add integer tokens from -128 to 128 and 100 decimal tokens from .0 to .99. This fine-grained number representation allows accurate modeling of geometry while keeping the tokenization compact. As shown in Figure 3(b), these special tokens substantially reduce sequence length compared with the original tokenizer, improving representation efficiency and alleviating computational burden. Embedding initialization. Instead of randomly initializing the SVG-specific special tokens, we adopt subword-based strategy to ensure semantic coherence and stable training. Each token is decomposed into subwords using the pretrained tokenizer, and the embeddings of these subwords are averaged to form the token embedding. Formally, let new special token tnew be decomposed into subwords {s1, s2, . . . , sn}, with corresponding embeddings {es1, es2 , . . . , esn }. The initialized embedding etnew is computed as: etnew = 1 (cid:88) i=1 esi (1) This strategy preserves the semantic prior from the original vocabulary and accelerates adaptation to SVG tokens. As shown in Figure 3(c), it reduces initial loss and accelerates convergence, demonstrating its effectiveness in stabilizing early training and enhancing overall efficiency."
        },
        {
            "title": "4.2 TWO-STAGE TRAINING STRATEGY",
            "content": "The design of our two-stage training strategy is motivated by the inherent imbalance in SVG corpora. Icons are easy to collect in large quantities and have simpler structures, while illustrations are more difficult to obtain, less abundant, and usually longer and more complex. This imbalance in scale and complexity makes unified training challenging. To address this issue, we adopt curriculum-style approach that progresses from simple to complex samples. Initially, training is conducted only on the Icon and Chemistry datasets, which contain shorter and simpler SVGs. This stage covers SVG description, all editing tasks, and two generative tasks, namely Text-to-SVG and Image-to-SVG, and allows the model to build basic representation and generation abilities. Once the model has reached preliminary convergence, training is expanded to all datasets and tasks, including longer and more diverse SVGs. To ensure balanced learning, the Icon and Chemistry data are resampled to match the proportions of the other datasets. This progressive strategy enables the model to acquire SVG knowledge gradually and stably while reducing imbalance caused by heterogeneous dataset scales and complexities. Table 2: Comparison of SVG understanding, editing, and generation on SArena-Icon. We use uppercase bold initials to denote submetrics with Overall, Color, Geometry, Quantity, and Semantics. Understanding S DINO SSIM LPIPS PSNR FID FID-C CLIP-T2I CLIP-I2I Tokens DINO SSIM LPIPS PSNR Tokens Editing Text-to-SVG Image-to-SVG Model IconShop VectorFusion SVGDreamer DiffVG LIVE VTracer InternVL3-8B InternVL3-78B GPT-4o Qwen2.5-VL-7B 52.8 69.3 50.4 34.9 56.4 0.909 0.921 59.5 79.1 59.3 38.2 61.3 0.966 Llama-4-Maverick 64.7 87.5 62.0 47.2 62.3 0.961 Qwen2.5-VL-72B 63.4 82.4 65.1 44.6 61.6 0.958 65.3 86.4 71.0 48.8 54.9 0.968 71.0 88.2 78.5 47.5 69.6 0.942 Gemini-2.5-Flash 73.0 90.1 81.9 53.0 67.2 0.979 77.1 91.5 82.4 53.8 80.6 Claude-4-Sonnet Starvector 8B LLM4SVG 7B OmniSVG 3B 85.1 93.0 85.8 61.9 99.7 0.989 InternSVG 8B +8.0 +1.5 +2.8 +8.1 +19.1 +0.010 0.728 0.761 0.870 0.849 0.848 0.887 0.815 0.915 0.952 +0.037 Traditional SVG methods Large language models 17.919 17.394 33.312 32.288 16.594 26.612 25.402 29.615 46.944 41.006 40.533 55.255 54.200 57.595 77.331 15.454 0.192 14.303 0.170 6.526 0.109 9.875 0.124 10.596 0.116 6.763 0.088 5.208 0.113 4.291 0.071 8.611 11.318 0.036 1.876 +0.035 +19.736 +6.216 +2.415 24.781 23.061 14.931 15.948 17.580 15.178 16.720 15.840 21.939 28.292 8.715 20.894 22.992 20.329 21.538 21.897 23.570 22.946 22.805 24.617 24.658 25.421 19.458 21.679 23.916 -1. 70.922 70.308 69.975 71.384 71.450 75.816 73.681 73.123 77.742 78.218 80.579 70.726 74.831 80.911 +0.332 1.5k 33k 132k 249 269 265 275 252 246 451 444 705 1.7k 1.0k 0.869 0.973 0.966 0.781 0.812 0.863 0.837 0.850 0.874 0.876 0.915 0.871 0.748 0.894 0.949 +0. 0.927 0.986 0.875 0.506 0.557 0.596 0.584 0.584 0.616 0.587 0.665 0.623 0.472 0.756 0.811 +0.055 0.097 0.024 0.054 0.378 0.361 0.329 0.346 0.339 0.316 0.316 0.276 0.206 0.409 0.186 0.127 +0.059 23.614 35.419 21.748 6.534 7.220 8.027 7.834 7.802 8.435 8.324 9.855 13.595 5.375 12.669 18.226 +4. 17k 18k 4.4k 281 256 255 372 234 231 533 541 951 485 2.4k 1.3k Table 3: Comparison of SVG generation results on SArena-Illustration. Model FID FID-C CLIP-T2I CLIP-I2I Tokens DINO SSIM LPIPS PSNR Tokens Text-to-SVG Image-to-SVG VectorFusion SvgDreamer DiffVG LIVE VTracer 28.198 32.410 37.903 Qwen2.5-VL-7B 36.736 InternVL3-8B Llama-4-Maverick 30.835 Qwen2.5-VL-72B 29.521 30.457 28.124 28.865 27.294 48.704 42.756 22.397 +4.897 InternVL3-78B GPT-4o Gemini-2.5-Flash Claude-Sonnet-4 LLM4SVG 7B OmniSVG 3B InternSVG 8B 15.661 23.427 28.455 25.682 14.831 18.407 19.195 14.150 8.894 7.640 29.568 22.885 5.141 +2.499 22.638 21.026 18.069 18.493 21.872 20.923 20.577 23.637 24.800 23.094 15.468 16.861 21.116 -3.684 Traditional SVG methods 71.220 69.775 33k 132k Large language models 756 493 551 527 454 473 1.2k 1.0k 1.2k 4.5k 8.1k 61.928 61.964 67.366 65.349 64.826 70.696 74.796 74.525 62.933 64.815 74.662 -0.134 7 0.870 0.963 0.965 0.739 0.772 0.839 0.808 0.830 0.850 0.829 0.901 0.713 0.797 0.924 +0. 0.886 0.948 0.879 0.513 0.569 0.644 0.628 0.638 0.663 0.516 0.670 0.494 0.656 0.716 +0.046 0.138 0.078 0.093 0.413 0.397 0.340 0.363 0.348 0.327 0.359 0.305 0.413 0.330 0.188 +0.117 21.605 29.055 21.754 7.732 8.542 10.469 9.900 9.985 10.723 9.091 11.731 6.221 10.433 14.644 +2. 17k 18k 10k 1.2k 716 608 886 514 484 1.8k 1.3k 476 6.7k 7.7k Table 4: Comparison of SVG generation results on SArena-Chemistry and SArena-Animation. Model Chemistry: Text-to-SVG Chemistry: Image-to-SVG Animation: Text-to-SANI Animation: Video-to-SANI FID FID-C CLIP-I2I DINO SSIM LPIPS PSNR FVD CLIP-T2V CLIP-V2V DINO SSIM LPIPS PSNR 11.758 56.248 Qwen2.5-VL-7B 9.883 33.613 InternVL3-8B 12.858 Llama-4-Maverick 26.844 11.931 Qwen2.5-VL-72B 32.307 11.336 29.216 12.260 24.505 12.015 27.708 13.189 21.252 14.168 9.974 +0.979 +11.278 +14.363 InternVL3-78B GPT-4o Gemini-2.5-Flash Claude-Sonnet-4 Starvector 8B InternSVG 8B 296.113 345.181 234.577 265.631 250.478 238.052 224.954 271.387 207.935 +17.019 51.814 56.856 69.643 63.931 65.969 76.599 75.897 78.308 93.931 +15. 7.501 13.840 14.977 12.106 15.375 14.673 15.539 17.554 17.419 17.722 +0.178 50.649 43.856 67.615 59.454 60.896 70.608 66.554 74.179 73.162 -1.017 19.118 17.017 22.304 20.376 20.263 22.808 22.239 24.070 22.572 -1.498 0.468 0.783 0.798 0.647 0.813 0.791 0.817 0.871 0.841 0.873 +0.002 0.787 0.780 0.841 0.834 0.828 0.860 0.847 0.867 0.876 +0.009 0.273 0.286 0.246 0.261 0.264 0.250 0.257 0.240 0.237 +0. 0.769 0.865 0.908 0.846 0.911 0.920 0.934 0.957 0.977 0.994 +0.017 0.274 0.203 0.173 0.215 0.177 0.174 0.155 0.132 0.147 0.138 -0.006 0.716 0.612 0.754 0.721 0.704 0.743 0.701 0.760 0.754 -0.006 73.698 61.675 31.924 44.540 40.080 19.297 21.777 15.240 0."
        },
        {
            "title": "5.1 EXPERIMENTAL SETUP",
            "content": "InternSVG builds on pretrained InternVL3-8B model and adopts two-stage training strategy. In stage one, we train on the Icon and Chemistry datasets that contain shorter and simpler SVGs. Icon provides 1.0M samples for understanding, 1.0M for editing, and 5.6M for generation, and Chemistry provides 2.0M generation samples. In stage two, we expand to all domains and tasks, using 350K understanding, 500K editing, and 1.1M generation samples from Icon, 1.0M generation samples from Illustration, 1.0M from Chemistry, and 120K from Animation. Optimization uses AdamW with learning rate of 2 104 in stage one and 1 104 in stage two, running on 96 NVIDIA A800 GPUs with per-device batch size of 1."
        },
        {
            "title": "5.2 QUANTITATIVE EVALUATIONS",
            "content": "To validate the effectiveness of our SAgoge dataset and InternSVG, we evaluate multiple models on the SArena benchmark. The evaluation covers open-source MLLMs, proprietary systems, traditional SVG generation methods, and LLM-based approaches, including LLM4SVG (Xing et al., 2025), Starvector (Rodriguez et al., 2025), and OmniSVG (Yang et al., 2025b). The complete list of evaluated models with their parameter scales is given in Appendix B.1. As shown in Table 2, we systematically evaluate various models on SArena-Icon across understanding, editing, and generation tasks. Our InternSVG outperforms the second-best model by 8 points in Overall accuracy. In editing, InternSVG achieves the best scores across DINO, SSIM, LPIPS, and PSNR, yielding higher visual fidelity and geometric consistency. For Text-to-SVG, it achieves the best FID, FID-C, and CLIP-I2I among all methods, while its CLIP-T2I is comparable to that of Claude-Sonnet-4. In Image-to-SVG, optimization-based methods remain strong on similarity, yet InternSVG achieves comparable visual similarity while producing much more compact code at about 1.3k tokens per SVG, roughly one fourteenth of LIVEs token count, and it unifies understanding, editing, and generation within single model. As shown in Table 3, on SArena-Illustration, our method achieves the best FID and FID-C in Textto-SVG, with CLIPScores comparable to Gemini-2.5-Flash, the best-performing proprietary system, and outperforms both general MLLMs and LLM-based methods in Image-to-SVG. On SArenaChemistry, where common or IUPAC names are used as prompts, existing MLLMs perform poorly on Text-to-SVG due to limited training on scientific graphics, while InternSVG achieves the best results on both tasks with large margin in Text-to-SVG  (Table 4)  . On SArena-Animation, InternSVG follows protocol of sampling 8 frames from video input and surpasses all open-source MLLMs, reaching performance close to proprietary models on both Text-to-SANI and Video-to-SANI. Complete evaluation results are provided in Appendix B.2. Additional results on prior benchmarks are reported in Appendix B.3."
        },
        {
            "title": "5.3 QUALITATIVE VISUALIZATION",
            "content": "Figure 4 presents qualitative examples of SVGs generated by InternSVG. The results show that our InternSVG can produce diverse and visually appealing graphics with clear structures and semantically rich content. More qualitative visualization results of InternSVG and comparisons with other models can be found in Appendix B.5. 8 Figure 4: Visualization of SVG samples generated by InternSVG."
        },
        {
            "title": "5.4 ABLATION STUDIES",
            "content": "Table 5: Ablation results on Generation (G), Understanding (U), and Editing (E) under different task combinations. Tasks G+U G+E G+U+E Text-to-SVG Image-to-SVG Editing Overall FID FID-C CLIP-T2I CLIP-I2I DINO SSIM LPIPS PSNR DINO SSIM LPIPS PSNR 62.9 75.1 75.4 15.548 12.881 13.842 12.387 7.848 6.750 6.810 6.611 21.073 21.070 21.099 21. 73.016 73.310 73.279 73.630 0.807 0.813 0.808 0.814 0.547 0.563 0.553 0.568 0.387 0.382 0.383 0.379 6.860 7.042 6.897 7.095 0.943 0.969 0. 0.811 0.881 0.906 0.125 0.090 0.075 42.131 49.503 54.618 Benefits of Unified SVG Modeling. To validate the effectiveness of unified SVG modeling in improving model performance, we sample 100K SVGs from the Icon dataset and construct multitask training set. The set includes 100K samples for multiple-choice QA and 100K samples for SVG description in the understanding task, 100K samples for semantic color editing in the editing task, and 100K samples each for text-to-SVG and image-to-SVG in the generation task. As shown in Table 5, we compare single-task, two-task, and three-task training configurations. The results show that multi-task joint training leads to consistent performance improvements and achieves the best results across the evaluation metrics of all tasks. Overall, three-task joint training yields significant gains in most metrics, which confirms that unified SVG modeling facilitates cross-task knowledge transfer, enables the learning of richer structural and semantic representations, and improves the generalization ability of the model. Table 6: Ablation results on the effects of the two-stage training strategy. Illustration: Text-to-SVG Illustration: Image-to-SVG Animation: Text-to-SANI Animation: Video-to-SANI Strategy One-stage 68.644 Two-stage 22.397 FID FID-C CLIP-T2I CLIP-I2I DINO SSIM LPIPS PSNR FVD CLIP-T2V CLIP-V2V DINO SSIM LPIPS PSNR 13.420 14. 237.524 207.935 69.993 73.162 22.291 22.572 10.345 14.644 19.217 21.116 66.400 74. 25.671 5.141 0.245 0.237 0.735 0.754 0.867 0.876 0.278 0.188 0.503 0. 0.830 0.924 Comparison of One-stage and Two-stage Training. To verify the effectiveness of the two-stage training strategy, we merge the data from both stages into single-stage training scheme as baseline. As demonstrated in Table 6, the two-stage model achieves substantial advantages on both illustration and animation generation. Especially for the long-sequenced illustration generation task, the two-stage training strategy exhibits remarkable improvements, with FID-C reduced from 25.67 to 5.14 and DINO increased from 0.830 to 0.924. More comparisons can be found in Appendix B.4."
        },
        {
            "title": "6 CONCLUSIONS",
            "content": "In this work, we introduce the InternSVG family, unified data-benchmark-model suite for scalable vector graphics. It integrates the large-scale and diverse SAgoge, the comprehensive benchmark SArena, and the unified MLLM InternSVG. SAgoge encompasses broad spectrum of SVG tasks 9 with varied difficulty levels, ranging from static icons and scientific diagrams to long-sequence illustrations and dynamic animations. SArena provides standardized evaluation framework that assesses both mainstream MLLMs and traditional SVG generation approaches, enabling consistent comparison across tasks and domains. Through extensive experiments, we demonstrate that this unified formulation enhances SVG understanding, editing, and generation, yielding consistent improvements across domains and tasks. Our study highlights the importance of unified SVG modeling, and we hope it can serve as an insight for future research on vector-graphic reasoning and multimodal intelligence."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www-cdn.anthropic. com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_ 3.pdf, 2024. Anthropic. Introducing claude 4: Claude sonnet 4 and claude opus 4. https://www. anthropic.com/news/claude-4, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Alexandre Carlier, Martin Danelljan, Alexandre Alahi, and Radu Timofte. Deepsvg: hierarchical generative network for vector graphics animation. Advances in Neural Information Processing Systems, 33:1635116361, 2020. Siqi Chen, Xinyu Dong, Haolei Xu, Xingyu Wu, Fei Tang, Hang Zhang, Yuchen Yan, Linjuan Wu, Wenqi Zhang, Guiyang Hou, et al. Svgenius: Benchmarking llms in svg understanding, editing and generation. arXiv preprint arXiv:2506.03139, 2025. Zehao Chen and Rong Pan. Svgbuilder: Component-based colored svg generation with text-guided autoregressive transformers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 23582366, 2025. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Kevin Frans, Lisa Soros, and Olaf Witkowski. Clipdraw: Exploring text-to-drawing synthesis through language-image encoders. Advances in Neural Information Processing Systems, 35: 52075218, 2022. Google DeepMind. era. tic google-gemini-ai-update-december-2024/, 2024. Introducing gemini 2.0: agenhttps://blog.google/technology/google-deepmind/ our new ai model the for 10 David Ha and Douglas Eck. neural representation of sketch drawings. arXiv preprint arXiv:1704.03477, 2017. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006, 2025. Teng Hu, Ran Yi, Baihong Qian, Jiangning Zhang, Paul Rosin, and Yu-Kun Lai. Supersvg: Superpixel-based scalable vector graphics synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2489224901, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Ajay Jain, Amber Xie, and Pieter Abbeel. Vectorfusion: Text-to-svg by abstracting pixel-based diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19111920, 2023. Jinke Li, Jiarui Yu, Chenxing Wei, Hande Dong, Qiang Lin, Liangjing Yang, Zhicai Wang, and Yanbin Hao. Unisvg: unified dataset for vector graphic understanding and generation with multimodal large language models. arXiv preprint arXiv:2508.07766, 2025. Tzu-Mao Li, Michal Lukáˇc, Gharbi Michaël, and Jonathan Ragan-Kelley. Differentiable vector graphics rasterization for editing and learning. ACM Trans. Graph. (Proc. SIGGRAPH Asia), 39 (6):193:1193:15, 2020. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Raphael Gontijo Lopes, David Ha, Douglas Eck, and Jonathon Shlens. learned representation for scalable vector graphics. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 79307939, 2019. Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev, Nikita Orlov, Yun Fu, and Humphrey Shi. Towards layer-wise image vectorization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1631416323, 2022. Meta AI. Llama 4 maverick 17b-128e (model card). https://huggingface.co/ meta-llama/Llama-4-Maverick-17B-128E, 2025a. Released Apr 5, 2025; MoE with 17B activated / 400B total; native multimodality; knowledge cutoff Aug 2024. Meta AI. Llama 4 scout 17b-16e (model card). https://huggingface.co/meta-llama/ Llama-4-Scout-17B-16E, 2025b. Model release date: Apr 5, 2025; MoE 17B active / 109B total; 10M context. Kunato Nishina and Yusuke Matsui. Svgeditbench: benchmark dataset for quantitative assessment of llms svg editing capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 81428147, 2024. Kunato Nishina and Yusuke Matsui. Svgeditbench v2: benchmark for instruction-based svg editing. arXiv preprint arXiv:2502.19453, 2025. OpenAI. Gpt-5 system card. https://cdn.openai.com/gpt-5-system-card.pdf, 2025. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Xiao, Katherine Collins, Joshua Tenenbaum, Adrian Weller, Michael Black, and Bernhard Schölkopf. Can large language models understand symbolic graphics programs? arXiv preprint arXiv:2408.08313, 2024. Qwen Team. Qwen2.5 technical report. https://arxiv.org/abs/2412.15115, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Juan Rodriguez, Abhay Puri, Shubham Agarwal, Issam Laradji, Pau Rodriguez, Sai Rajeswar, David Vazquez, Christopher Pal, and Marco Pedersoli. Starvector: Generating scalable vector graphics code from images and text. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1617516186, 2025. StepFun Team. Step3: Cost-effective multimodal intelligence. https://stepfun.ai/ research/step3, 2025. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025a. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025b. Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, et al. Kwai keye-vl technical report. arXiv preprint arXiv:2507.01949, 2025c. Lucas Theis, Aäron van den Oord, and Matthias Bethge. note on the evaluation of generative models. arXiv preprint arXiv:1511.01844, 2015. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. Yael Vinker, Ehsan Pajouheshgar, Jessica Bo, Roman Christian Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, and Ariel Shamir. Clipasso: Semantically-aware object sketching. ACM Transactions on Graphics (TOG), 41(4):111, 2022. Vision Cortex. Vtracer. https://www.visioncortex.org/vtracer-docs, 2023. Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600 612, 2004. Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liao. Iconshop: Text-guided vector icon synthesis with autoregressive transformers. ACM Transactions on Graphics (TOG), 42(6):114, 2023. xAI. Grok 3 Beta The Age of Reasoning Agents. https://x.ai/news/grok-3, 2025. Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, and Qian Yu. Svgdreamer: Text In Proceedings of the IEEE/CVF Conference on guided svg generation with diffusion model. Computer Vision and Pattern Recognition, pp. 45464555, 2024. Ximing Xing, Juncheng Hu, Guotao Liang, Jing Zhang, Dong Xu, and Qian Yu. Empowering llms to understand and generate complex vector graphics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1948719497, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025a. Yiying Yang, Wei Cheng, Sijin Chen, Xianfang Zeng, Fukun Yin, Jiaxu Zhang, Liao Wang, Gang Yu, Xingjun Ma, and Yu-Gang Jiang. Omnisvg: unified scalable vector graphics generation model. arXiv preprint arXiv:2504.06263, 2025b. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. Bocheng Zou, Mu Cai, Jianrui Zhang, and Yong Jae Lee. Vgbench: comprehensive benchmark of vector graphics understanding and generation for large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 36473659, 2024."
        },
        {
            "title": "A IMPLEMENTATION DETAILS OF INTERNSVG",
            "content": "In this section, we provide the detailed implementation of the SVG-specific tokens used in InternSVG. All SVGs undergo normalization to canonical 128 128 coordinate system. This standardization reduces spurious coordinate drift and decreases computational burden for learning absolute magnitudes, thereby mitigating coordinate hallucination and enhancing cross-task consistency. We extend the base vocabulary with SVG-specific tokens that compress lengthy character sequences. As demonstrated in Table 7, the extended vocabulary incorporates 55 tag tokens covering structural and animation elements, including opening and closing forms such as <svg, </svg>, <path, <animate, and <animateTransform. Additionally, we include 42 attribute tokens encompassing geometry, styling, and animation controls with embedded quotes, such as viewBox=\", width=\", x=\", and fill=\". For compact numerical representation, we introduce 247 integer tokens spanning 128 to 128, plus 100 two-decimal and 10 one-decimal fractional tokens. This fine-grained numeric inventory enables precise geometric specification while maintaining short sequences, yielding substantial reductions in sequence length and computational requirements. New token parameters undergo initialization, preserving the base tokenizers semantic knowledge. Each added tokens embedding equals the average of its constituent subword embeddings when segmented by the original tokenizer. This procedure applies uniformly across all token types. Following vocabulary expansion, training proceeds end-to-end without parameter freezing. The subwordderived initialization stabilizes optimization, accelerates convergence, and aligns new symbols with adjacent embedding regions, improving grammatical decoding and numerical fidelity across all tasks."
        },
        {
            "title": "B RESULTS OF SARENA",
            "content": "B.1 EVALUATED MODELS Proprietary Models. We include several leading proprietary MLLMs in our evaluation, namely GPT-4o (Hurst et al., 2024), Gemini-2.5-Flash (Comanici et al., 2025), Claude-Sonnet-3.7 (Anthropic, 2024), Claude-Sonnet-4 (Anthropic, 2025), and Grok 3 (xAI, 2025). Open-Source Models. We further benchmark diverse set of open-source MLLMs to provide comprehensive comparison, including Llama-3.1-8B/70B/405B, Llama-3.2-11B/90B (Dubey et al., 2024), Llama-4-Scout (Meta AI, 2025b), Llama-4-Maverick (Meta AI, 2025a), Qwen2.5-VL7B/32B/72B (Bai et al., 2025), Keye-VL-8B (Team et al., 2025c), GLM-4.1V-9B (Hong et al., 2025), GLM-4.5V (Zeng et al., 2025), Gemma-3-12B/27B (Team et al., 2025a), Kimi-VL-A3B (Team et al., 2025b), Step3-321B (StepFun Team, 2025), and InternVL3 series (Zhu et al., 2025). Traditional SVG Generation Methods. We also evaluate range of traditional SVG generation methods, including Vectorfusion (Jain et al., 2023), IconShop (Wu et al., 2023), SVGDreamer (Xing et al., 2024), DiffVG (Li et al., 2020), LIVE (Ma et al., 2022), and VTracer (Vision Cortex, 2023). LLM-based SVG Generation Methods. To thoroughly validate the effectiveness of InternSVG, we compare it against representative LLM-based SVG generation approaches on the SArena benchmark. Specifically, we include LLM4SVG (Xing et al., 2025), StarVector (Rodriguez et al., 2025), and OmniSVG (Yang et al., 2025b), which exemplify current efforts in leveraging large language models for SVG generation. B.2 COMPLETE EVALUATION RESULTS ON SARENA Table 8, 9, 10, 11, 12, 13, and 14 present the complete evaluation results on the SArena benchmark across all four domains and tasks. As shown in Tables 8, 9, and 10, InternSVG achieves substantial improvements on both Understanding and Editing tasks. For the Understanding task, our model obtains the best results across all metrics, surpassing the second-best Claude-4-Sonnet by 8 points in Overall accuracy. Especially 14 Table 7: SVG-specific token vocabulary in InternSVG. (a) Tag tokens Tokens <svg, </svg>, <defs, </defs>, <use, </use> ,/> <g, </g> <path, </path>, <rect, </rect>, <circle, </circle>, <ellipse, </ellipse>, <line, </line>, <polyline, </polyline>, <polygon, </polygon> <text, </text>, <tspan, </tspan>, <textPath, </textPath> <linearGradient, </linearGradient>, <radialGradient, </radialGradient>, <stop, </stop> <clipPath, </clipPath>, <mask, </mask> <filter, </filter>, <feGaussianBlur, </feGaussianBlur>, <feColorMatrix, </feColorMatrix>, <feComposite, </feComposite>, <feBlend, </feBlend> <animate, </animate>, <animateMotion, </animateMotion>, <animateTransform, </animateTransform> (b) Attribute tokens Tokens width=\", height=\", viewBox=\", x=\", y=\", x1=\", y1=\", x2=\", y2=\", cx=\", cy=\", r=\", rx=\", ry=\", d=\", points=\" fill=\", stroke=\", stroke-width=\", stroke-linecap=\", stroke-linejoin=\", stroke-miterlimit=\", fill-rule=\", opacity=\" transform=\" font-size=\", font-family=\", text-anchor=\" gradientUnits=\", gradientTransform=\", offset=\", stop-color=\" begin=\", dur=\", repeatCount=\", from=\", to=\", rotate=\", path=\" id=\", class=\", clip-path=\" Category Root Grouping Shapes Text Gradients Clipping Filters Animation Category Geometry Styling Transform Text Gradients Animation Identifiers Table 8: Comparison of SVG understanding and editing performance on SArena-Icon. Model Qwen2.5-VL-7B InternVL3-8B Gemma-3-27B Qwen2.5-VL-32B Llama-4-Scout Llama-4-Maverick Qwen2.5-VL-72B InternVL3-78B GPT-4o Gemini-2.5-Flash Claude-Sonnet-4 InternSVG 8B Understanding Editing Overall Color Geometry Quantity Semantic DINO SSIM LPIPS PSNR Tokens 52.8 59.5 59.5 65.5 57.5 64.7 63.4 65.3 71.0 73.0 77.1 85.1 69.3 79.1 82.2 82.8 82.4 87.5 82.4 86.4 88.2 90.1 91.5 93. 50.4 59.3 67.6 65.5 57.0 62.0 65.1 71.0 78.5 81.9 82.4 85.8 34.9 38.2 43.6 47.7 41.6 47.2 44.6 48.8 47.5 53.0 53.8 61.9 56.4 61.3 44.7 66.1 49.0 62.3 61.6 54.9 69.6 67.2 80.6 99.7 0.909 0.921 0.942 0.933 0.949 0.966 0.961 0.958 0.968 0.942 0.979 0.989 0.728 0.761 0.815 0.782 0.825 0.870 0.849 0.848 0.887 0.815 0.915 0.952 0.192 0.170 0.113 0.148 0.138 0.109 0.124 0.116 0.088 0.113 0.071 0. 25.402 29.615 54.200 37.737 34.070 46.944 41.006 40.533 55.255 54.200 57.595 77.331 1.0k 1.2k 1.3k 1.0k 1.3k 1.3k 1.2k 1.2k 1.2k 1.3k 1.3k 1.4k on the Semantic subtask, the score reaches 99.7, indicating that InternSVG demonstrates strong capability in comprehending the semantics encoded in SVG code. The simple-level editing tasks primarily assess proficiency in SVG syntax, including the use of nested groups with the transform attribute to carry out translation, rotation, and flipping operations. After training on SAgoge, our 15 InternSVG effectively acquires these syntactic skills and achieves perfect scores in several subtasks. On the hard-level editing tasks, InternSVG attains the best performance in semantic-level color editing, consistent with the Understanding results and further confirming its strong semantic comprehension. However, in the more challenging style transfer task, the model still lags slightly behind Claude-Sonnet-4, leaving room for improvement. Table 9: Comparison of SVG editing performance across 8 simple subtasks on SArena-Icon. Model Low-level Color Editing Cropping Flipping Rotation DINO SSIM LPIPS PSNR DINO SSIM LPIPS PSNR DINO SSIM LPIPS PSNR DINO SSIM LPIPS PSNR 47.833 0.958 Qwen2.5-VL-7B 48.211 0.963 InternVL3-8B 21.118 0.999 InternVL3.5-8B 85.314 1.000 Gemma-3-27B 18.751 InternVL3.5-30B 0.999 90.586 Qwen2.5-VL-32B 0.967 80.043 0.969 Llama-4-Scout 88.142 Llama-4-Maverick 0.998 82.266 Qwen2.5-VL-72B 0.995 92.534 0.995 39.965 InternVL3.5-241B 0.983 94.845 0.995 85.314 1.000 96.676 1.000 99.692 1. 73.123 75.568 88.473 99.057 91.706 88.400 87.067 94.874 97.542 96.985 91.262 98.406 99.057 100.000 100.000 10.087 10.271 11.376 14.116 10.902 9.062 9.134 9.404 9.174 9.599 11.763 9.556 14.116 9.626 100.000 9.683 23.198 13.358 96.554 23.892 35.634 21.027 76.565 52.671 32.765 30.961 87.340 96.554 73.786 98.672 0.061 0.055 0.007 0.000 0.005 0.044 0.049 0.006 0.008 0.008 0.021 0.007 9.761 0.000 0.000 0.313 0.259 0.241 0.008 0.195 0.154 0.206 0.074 0.090 0.129 0.165 0.017 0.008 0.055 0.005 0.636 0.704 0.704 0.982 0.769 0.807 0.755 0.914 0.874 0.833 0.754 0.976 0.982 0.943 0. 0.892 0.903 0.992 1.000 0.995 0.914 0.925 0.996 0.986 0.987 0.956 0.987 1.000 1.000 1.000 0.919 0.979 0.886 0.991 0.869 0.986 0.974 0.989 0.992 0.994 0.901 0.995 0.991 0.999 1.000 0.852 0.842 0.905 0.995 0.916 0.919 0.901 0.955 0.948 0.936 0.896 0.994 0.995 0.944 0.996 0.870 0.884 0.881 0.885 0.889 0.903 0.879 0.903 0.909 0.909 0.904 0.913 0.885 0.928 1.000 0.270 0.673 0.257 0.705 0.195 0.761 0.297 0.619 0.235 0.732 0.306 0.657 0.283 0.652 0.301 0.677 0.307 0.668 0.299 0.682 0.225 0.763 0.300 0.688 0.297 0.619 0.291 0.696 1.000 0.000 Adding Stroke 0.152 0.803 0.157 0.818 0.246 0.697 0.041 0.945 0.262 0.708 0.024 0.959 0.051 0.926 0.024 0.967 0.045 0.949 0.017 0.974 0.188 0.783 0.010 0.986 0.041 0.945 0.006 0.994 1.000 0.000 Transparency GPT-4o Gemini-2.5-Flash Claude-Sonnet-4 InternSVG 8B InternVL3-78B Translation Scaling DINO SSIM LPIPS PSNR DINO SSIM LPIPS PSNR DINO SSIM LPIPS PSNR DINO SSIM LPIPS PSNR 50.893 0.902 Qwen2.5-VL-7B 67.912 0.923 InternVL3-8B 59.713 0.932 InternVL3.5-8B 63.444 0.943 Gemma-3-27B 63.038 InternVL3.5-30B 0.930 80.879 Qwen2.5-VL-32B 0.917 66.797 0.925 Llama-4-Scout 94.987 Llama-4-Maverick 0.927 Qwen2.5-VL-72B 0.901 72.101 68.573 0.931 64.399 InternVL3.5-241B 0.919 85.619 0.947 63.444 0.943 97.535 0.953 99.968 0.999 13.257 27.231 12.508 82.705 32.944 31.632 18.387 31.710 18.695 46.221 25.850 72.016 82.705 87.758 100. 25.767 35.333 20.350 40.216 27.933 33.796 38.360 52.249 44.055 37.317 27.335 48.913 40.216 51.913 99.488 12.466 12.403 16.638 67.280 14.118 19.639 18.068 23.361 11.492 12.792 11.857 45.845 67.280 50.330 98.655 0.073 0.026 0.024 0.141 0.024 0.029 0.028 0.006 0.010 0.015 0.059 0.014 0.141 0.000 0.000 0.262 0.231 0.234 0.100 0.236 0.236 0.226 0.194 0.267 0.238 0.245 0.163 0.100 0.138 0.000 0.180 0.150 0.162 0.116 0.135 0.139 0.104 0.073 0.105 0.145 0.136 0.093 0.116 0.055 0.000 0.295 0.222 0.276 0.045 0.222 0.191 0.251 0.226 0.256 0.134 0.160 0.060 0.045 0.002 0. 0.634 0.708 0.660 0.896 0.746 0.748 0.686 0.741 0.704 0.831 0.750 0.928 0.896 0.997 1.000 0.889 0.954 0.967 0.687 0.968 0.949 0.957 0.991 0.992 0.984 0.882 0.977 0.687 1.000 1.000 0.653 0.684 0.710 0.846 0.693 0.673 0.705 0.776 0.678 0.695 0.661 0.811 0.846 0.833 1.000 0.728 0.791 0.721 0.857 0.769 0.739 0.840 0.886 0.875 0.790 0.762 0.864 0.857 0.907 1.000 0.908 0.916 0.917 0.962 0.947 0.934 0.926 0.956 0.951 0.957 0.928 0.982 0.962 0.999 1.000 0.917 0.933 0.936 0.968 0.949 0.932 0.960 0.970 0.965 0.947 0.948 0.966 0.968 0.982 1. 0.966 0.982 0.989 0.883 0.992 0.980 0.983 0.996 0.995 0.992 0.956 0.990 0.883 0.999 1.000 GPT-4o Gemini-2.5-Flash Claude-Sonnet-4 InternSVG 8B InternVL3-78B Model Table 10: Comparison of SVG editing performance across 2 hard subtasks on SArena-Icon. Semantic-level Color Editing"
        },
        {
            "title": "Model",
            "content": "Qwen2.5-VL-7B InternVL3-8B Gemma-3-27B DINO SSIM LPIPS PSNR DINO SSIM LPIPS PSNR 11.940 0.919 13.457 0.903 12.174 0.981 14.283 Qwen2.5-VL-32B 0.926 15.417 0.964 Llama-4-Scout 16.765 Llama-4-Maverick 0.975 16.771 Qwen2.5-VL-72B 0.975 13.429 0.955 18.173 0.972 12.174 0.981 18.374 0.991 0.996 18.100 InternVL3-78B GPT-4o Gemini-2.5-Flash Claude-Sonnet-4 InternSVG 8B 23.902 22.071 53.068 28.290 27.852 41.222 42.759 27.033 54.651 53.068 56.741 69.875 0.193 0.158 0.210 0.162 0.119 0.105 0.113 0.175 0.117 0.210 0.097 0.139 0.658 0.728 0.591 0.723 0.848 0.855 0.836 0.705 0.819 0.591 0.867 0. 0.889 0.917 0.869 0.910 0.963 0.969 0.957 0.912 0.952 0.869 0.976 0.952 0.166 0.184 0.072 0.158 0.120 0.099 0.100 0.105 0.073 0.072 0.050 0.041 0.768 0.728 0.920 0.769 0.860 0.891 0.888 0.857 0.912 0.920 0.944 0.959 The complete results for both Text-to-SVG and Image-to-SVG tasks on icons and illustrations are reported in Table 11 and Table 12. InternSVG consistently outperforms prior SVG generation approaches across all major metrics. Compared with large-scale open-source models such as Llama4-Maverick and GLM-4.5V, it achieves lower FID, FID-C, and higher CLIP-I2I scores, with only slight gap on CLIP-T2I. For Image-to-SVG, InternSVG achieves the best performance across all metrics, highlighting its strength in visual fidelity and semantic consistency. 16 Table 11: Comparison of SVG generation performance on SArena-Icon (Text-to-SVG and Imageto-SVG). Model FID FID-C CLIP-T2I CLIP-I2I Tokens DINO SSIM LPIPS PSNR Tokens Text-to-SVG Image-to-SVG Traditional SVG methods IconShop VectorFusion SVGDreamer DiffVG LIVE VTracer 32.288 16.594 26.612 Llama-3.1-8B Keye-VL-8B GLM-4.1V-9B InternVL3-8B Llama-3.2-11B Gemma-3-12B InternVL3-14B Kimi-VL-A3B Gemma-3-27B 19.428 Qwen2.5-VL-7B 24.781 21.961 22.684 23.061 28.156 17.137 18.996 30.807 15.145 Qwen2.5-VL-32B 20.043 18.014 InternVL3-38B 21.967 Grok-3 Llama-3.1-70B 18.032 Llama-3.1-405B 16.794 24.990 15.178 Gemini-2.5-Flash 16.720 Claude-Sonnet-3.7 14.383 15.840 Claude-Sonnet-4 19.309 Llama-3.2-90B 17.908 Llama-4-Scout Llama-4-Maverick 14.931 16.641 20.061 Qwen2.5-VL-72B 15.948 17.580 21.939 28.292 8.715 InternVL3-78B Starvector 8B LLM4SVG 7B OmniSVG 3B InternSVG 8B DeepSeek-V3 GPT-4o GLM-4.5V Step3-321B 17.919 17.394 33.312 11.247 15.454 14.393 10.447 14.303 14.345 10.409 13.224 16.996 9.303 10.393 11.042 8.694 8.300 8.390 8.803 6.763 5.208 3.499 4.291 8.550 9.382 6.526 5.093 9.706 9.875 10.596 8.611 11.318 1.876 20.894 22.992 20.329 21.863 21.538 21.557 22.562 21.897 21.711 22.023 22.066 21.439 22.526 22.783 22.795 24.122 22.747 22.822 23.790 24.617 24.658 25.294 25.421 22.841 22.849 23.570 24.450 23.053 22.946 22.805 19.458 21.679 23.916 70.922 70.308 69.975 1.5k 33k 132k Large language models 71.859 71.384 71.167 73.197 71.450 71.485 71.622 71.493 70.536 73.277 73.228 73.077 76.797 73.876 73.920 76.470 77.742 78.218 80.786 80.579 74.006 73.563 75.816 78.349 74.184 73.681 73.123 70.726 74.831 80.911 280 249 227 269 269 261 290 227 228 249 317 251 346 255 236 251 246 451 417 444 249 256 265 372 308 275 252 705 1.7k 1.0k 0.869 0.973 0.966 0.781 0.801 0.820 0.812 0.759 0.821 0.825 0.798 0.826 0.836 0.829 0.874 0.876 0.909 0.915 0.757 0.844 0.863 0.872 0.834 0.837 0.850 0.871 0.748 0.894 0. 0.927 0.986 0.875 0.506 0.531 0.539 0.557 0.467 0.576 0.562 0.562 0.595 0.562 0.549 0.616 0.587 0.647 0.665 0.437 0.582 0.596 0.627 0.555 0.584 0.584 0.623 0.472 0.756 0.811 0.097 0.024 0.054 0.378 0.368 0.345 0.361 0.389 0.352 0.359 0.362 0.354 0.357 0.351 0.316 0.316 0.290 0.276 0.377 0.346 0.329 0.315 0.340 0.346 0.339 0.206 0.409 0.186 0.127 23.614 35.419 21.748 6.534 6.939 7.329 7.220 5.908 7.632 7.343 7.179 7.833 7.503 7.305 8.435 8.324 9.259 9.855 5.777 7.736 8.027 8.666 7.516 7.834 7.802 13.595 5.375 12.669 18. 17k 18k 4.4k 281 286 289 256 216 360 216 245 267 309 230 231 533 389 541 192 246 255 322 301 372 234 951 485 2.4k 1.3k Table 13 reports the complete evaluation results on SArena-Chemistry for both Text-to-SVG and Image-to-SVG tasks. In this benchmark, common names and IUPAC names of chemical compounds are used as instructions to guide models to generate structural formulas. The results indicate that most existing MLLMs perform poorly in Text-to-SVG generation due to their limited training exposure to scientific graphics. For example, Qwen2.5-VL-72B obtains FID scores around 32 and relatively low CLIP-I2I scores. In contrast, InternSVG achieves the lowest FID (9.974) and FID-C (0.877) and the highest CLIP-I2I (93.931), surpassing all baselines by large margin. On Imageto-SVG, InternSVG also outperforms both open-source and proprietary models, achieving the best results across DINO (0.949), SSIM (0.873), and PSNR (17.722). These results highlight the strong capability of InternSVG in handling domain-specific scientific graphics. For animation generation, InternSVG achieves strong performance on both Text-to-SANI and Videoto-SANI tasks according to Table 14. In the Text-to-SANI task, it attains the lowest FVD of 99.474 and ranks second on CLIP-T2V and CLIP-V2V, only slightly behind Claude-Sonnet-4. In the Video-to-SANI task, InternSVG demonstrates performance comparable to or even surpassing that 17 Table 12: Comparison of SVG generation performance on SArena-Illustration (Text-to-SVG and Image-to-SVG). Model FID FID-C CLIP-T2I CLIP-I2I Tokens DINO SSIM LPIPS PSNR Tokens Text-to-SVG Image-to-SVG Traditional SVG methods VectorFusion SVGDreamer DiffVG LIVE VTracer 16.594 26.612 Qwen2.5-VL-7B 37.903 InternVL3-8B 36.736 InternVL3.5-8B 70.837 InternVL3.5-14B 65.967 Gemma-3-27B 27.838 InternVL3.5-30B 68.438 Qwen2.5-VL-32B 32.115 InternVL3.5-38B 42.172 35.489 Llama-4-Scout Llama-4-Maverick 30.835 Qwen2.5-VL-72B 29.521 30.457 InternVL3.5-241B 43.339 28.124 Gemini-2.5-Flash 28.865 27.294 Claude-Sonnet-4 Starvector 8B 48.704 LLM4SVG 7B 42.756 OmniSVG 3B 22.397 InternSVG 8B InternVL3-78B GPT-4o 17.394 33.312 28.455 25.682 35.776 34.912 13.766 33.285 17.804 21.556 18.647 14.831 18.407 19.195 23.061 14.150 8.894 7.640 29.568 22.885 5. 22.992 20.329 18.069 18.493 18.095 18.131 21.486 18.354 19.773 18.221 20.299 21.872 20.923 20.577 18.191 23.637 24.800 23.094 15.468 16.861 21.116 70.308 69.975 33k 132k Large language models 61.928 61.964 63.357 63.496 67.255 63.910 64.555 65.511 64.182 67.366 65.349 64.826 65.689 70.696 74.796 74.525 62.933 64.815 74. 756 493 3.6k 3.5k 613 3.8k 779 4.3k 524 551 527 454 2.9k 473 1.2k 1.0k 1.2k 4.5k 8.1k 0.870 0.963 0.965 0.739 0.772 0.721 0.722 0.824 0.739 0.816 0.755 0.807 0.839 0.808 0.830 0.792 0.850 0.829 0.901 0.650 0.713 0.797 0.924 0.886 0.948 0.879 0.513 0.569 0.306 0.296 0.617 0.331 0.591 0.393 0.599 0.644 0.628 0.638 0.480 0.663 0.516 0.670 0.070 0.494 0.656 0.716 0.138 0.078 0. 0.413 0.397 0.410 0.414 0.379 0.404 0.382 0.400 0.360 0.340 0.363 0.348 0.378 0.327 0.359 0.305 0.447 0.413 0.330 0.188 21.605 29.055 21.754 7.732 8.542 5.283 5.130 9.920 5.778 9.297 6.540 9.549 10.469 9.900 9.985 8.093 10.723 9.091 11.731 1.990 6.221 10.433 14.644 17k 18k 10k 1.2k 716 2.5k 2.8k 764 3.0k 828 3.8k 574 608 886 514 3.1k 484 1.8k 1.3k 2.6k 476 6.7k 7.7k Table 13: Comparison of SVG generation performance on SArena-Chemistry (Text-to-SVG and Image-to-SVG). Text-to-SVG Image-to-SVG Model Llama-4-Scout InternVL3-8B Gemma-3-27B Qwen2.5-VL-7B 56.248 33.613 29.937 Qwen2.5-VL-32B 53.047 33.781 Llama-4-Maverick 26.844 Qwen2.5-VL-72B 32.307 29.216 24.505 Gemini-2.5-Flash 27.708 21.252 Claude-Sonnet-4 Starvector 8B 9.974 InternSVG 8B FID FID-C CLIP-I2I Tokens DINO SSIM LPIPS PSNR Tokens 0.769 0.865 0.887 0.821 0.866 0.908 0.846 0.911 0.920 0.934 0.957 0.977 0.994 7.501 13.840 14.959 10.005 12.984 14.977 12.106 15.375 14.673 15.539 17.554 17.419 17.722 51.814 56.856 60.776 58.428 62.522 69.643 63.931 65.969 76.599 75.897 78.308 93.931 73.698 61.675 49.967 56.431 46.584 31.924 44.540 40.080 19.297 21.777 15.240 0.877 0.468 0.783 0.823 0.570 0.734 0.798 0.647 0.813 0.791 0.817 0.871 0.841 0.873 0.274 0.203 0.190 0.225 0.205 0.173 0.215 0.177 0.174 0.155 0.132 0.147 0. 907 910 776 1.2k 849 747 620 698 640 1.4k 1.2k 981 996 805 683 900 624 687 716 545 533 1.1k 956 1.2k 931 InternVL3-78B GPT-4o of Claude-Sonnet-4. Its SSIM score is only lower by 0.006, while it achieves higher results on all other metrics. 18 Table 14: Comparison of SVG generation performance on SArena-Animation (Text-to-SANI and Video-to-SANI). Model Text-to-SANI Video-to-SANI FVD CLIP-T2V CLIP-V2V Tokens DINO SSIM LPIPS PSNR Tokens Llama-4-Scout InternVL3-8B Gemma-3-27B Qwen2.5-VL-7B 214.379 310.066 159.119 Qwen2.5-VL-32B 128.299 167.932 Llama-4-Maverick 141.470 Qwen2.5-VL-72B 151.682 169.159 286.352 Gemini-2.5-Flash 151.983 169.484 Claude-Sonnet-4 99.474 InternSVG 8B InternVL3-78B GPT-4o 19.118 17.017 21.105 20.535 21.014 22.304 20.376 20.263 22.808 22.239 24.070 22.572 50.649 43.856 59.309 59.188 62.929 67.615 59.454 60.896 70.608 66.554 74.179 73.162 296 433 533 537 505 563 433 409 404 986 907 812 0.787 0.780 0.824 0.823 0.831 0.841 0.834 0.828 0.860 0.847 0.867 0. 0.716 0.612 0.733 0.696 0.742 0.754 0.721 0.704 0.743 0.701 0.760 0.754 0.273 0.286 0.265 0.273 0.259 0.246 0.261 0.264 0.250 0.257 0.240 0.237 11.758 9.883 12.290 11.417 12.427 12.858 11.931 11.336 12.260 12.015 13.189 14.168 423 415 516 505 426 447 402 385 400 917 866 888 Table 15: Comparison of SVG understanding performance on SGP-Bench (Qiu et al., 2024). Model Gemma-1.1-2B InternLM2.5-7B Keye-VL-8B GLM-4.1V-9B InternVL3-8B Gemma-3-12B DeepSeek-Coder-V2-16B InternVL3-14B Kimi-VL-A3B-2506 Gemma-3-27B Qwen2.5-VL-32B InternVL3-38B GPT-4o Gemini-2.5-Flash Claude-Sonnet-4 GLM-4.5V Qwen2.5-VL-72B InternVL3-78B Step3-321B-A38B InternSVG 8B Semantics Count Color Shape Reasoning Overall 32.1 27.3 41.4 41.6 33.7 24.8 30.9 38.2 31.1 36.7 40.0 40.8 45.9 53.8 55.9 47.3 40.2 41.0 35.9 54.6 33.3 31.7 47.5 55.6 46.5 30.8 37.9 52.9 41.5 51.4 55.7 58.7 56.8 57.8 67.6 63.7 55.1 59.1 54.0 70.7 25.0 59.8 71.4 79.1 69.8 47.2 63.7 74.4 67.0 76.3 76.3 82.2 87.3 88.1 89.5 87.3 80.1 84.0 82.8 85.5 35.6 51.5 54.9 61.5 59.1 25.7 54.8 54.1 47.4 62.1 61.2 63.6 75.2 75.6 79.0 72.3 62.0 65.2 63.2 82.4 28.7 28.2 40.6 40.0 36.1 22.8 26.8 41.7 32.4 39.4 43.9 43.9 50.4 55.5 58.9 55.8 41.1 47.0 38.6 57. 31.7 42.1 52.2 57.1 50.6 30.5 45.1 52.9 44.9 54.7 56.5 59.1 64.8 67.6 71.5 66.1 57.1 60.3 56.5 72.3 B.3 RESULTS ON SGP-BENCH To further validate the effectiveness of SAgoge in enhancing model capabilities for SVG modeling, we conduct comparative experiments on SGP-Bench, benchmark specifically designed to evaluate semantic and structural understanding of symbolic graphic programs (e.g., SVGs). As shown in Table 15, our InternSVG achieves an overall accuracy of 72.3%, substantially outperforming existing open-source models. In particular, it surpasses the second-best open-source MLLM, GLM-4.5V, by 6.2 percentage points. When compared with proprietary systems, InternSVG also demonstrates advantages, achieving 1.3 percentage points higher overall accuracy than Claude-Sonnet-4, while also leading on the Count and Shape subtasks. The strong performance on SGP-Bench demonstrates the effectiveness of SAgoge in enhancing SVG modeling. 19 Table 16: Comparison of one-stage and two-stage training strategy across SArena-Icon, SArenaChemistry, SArena-Illustration, and SArena-Animation. (a) Icon & Chemistry: Editing, Text-to-SVG and Image-to-SVG. Icon: Editing Icon: Text-to-SVG Icon: Image-to-SVG Chemistry: Text-to-SVG Chemistry: Image-to-SVG DINO SSIM LPIPS PSNR FID FID-C CLIP-T2I CLIP-I2I DINO SSIM LPIPS PSNR FID FID-C CLIP-I2I DINO SSIM LPIPS PSNR 17.688 17. 92.181 93.931 14.605 9.974 10.854 8.715 65.481 77.331 24.206 23.916 81.615 80. 18.284 18.226 0.950 0.949 0.124 0.127 2.245 1.876 1.199 0.877 0.069 0. 0.904 0.952 0.972 0.989 0.810 0.811 0.139 0.138 0.993 0.994 0.871 0. Model One-stage Two-stage (b) Illustration & Animation: Illustration (Text-to-SVG / Image-to-SVG) and Animation (Text-to-SANI / Video-to-SANI). Model Illustration: Text-to-SVG Illustration: Image-to-SVG Animation: Text-to-SANI Animation: Video-to-SANI One-stage 68.644 Two-stage 22.397 FID FID-C CLIP-T2I CLIP-I2I DINO SSIM LPIPS PSNR FVD CLIP-T2V CLIP-V2V DINO SSIM LPIPS PSNR 13.420 14.168 237.524 207.935 66.400 74. 25.671 5.141 22.291 22.572 19.217 21.116 10.345 14.644 69.993 73.162 0.503 0. 0.830 0.924 0.245 0.237 0.735 0.754 0.867 0.876 0.278 0.188 B.4 TWO-STAGE VS. SINGLE-STAGE TRAINING In this section, we provide comprehensive analysis of the one-stage and two-stage training strategies. As shown in Table 16, for Icon generation, the one-stage model performs relatively well because Icon samples dominate the training data in the single-stage setting. Nevertheless, the two-stage model achieves comparable results and remains superior on FID, FID-C, and SSIM. For Icon editing as well as Chemistry, Illustration, and Animation generation, the two-stage model consistently outperforms the one-stage baseline. The experimental results further validate that the proposed two-stage training strategy is effective in alleviating data imbalance, and the progressive training scheme from easier to more challenging tasks leads to substantial improvements in SVG modeling performance. B.5 ADDITIONAL QUALITATIVE VISUALIZATION In this section, we present additional qualitative visualizations of InternSVG on SVG understanding, editing, and generation tasks. We further compare the generated SVGs with those produced by baseline methods to assess visual quality. As illustrated in Figure 5 and 7, for the Text-to-SVG generation tasks on icons and illustrations, VectorFusion and SVGDreamer produce visually appealing results but exhibit poor instructionfollowing ability and often include large number of irrelevant elements, limiting their practical applicability. Existing general-purpose MLLMs typically suffer from structural instability and incomplete semantic representation in SVG generation, making it difficult to balance visual quality and semantic alignment. In contrast, Starvector, LLM4SVG, and OmniSVG frequently fail to generate valid outputs or produce results that are entirely inconsistent with the textual description, highlighting the limited generalization ability of current LLM-based approaches for SVG generation. By comparison, InternSVG consistently generates high-quality and semantically aligned SVGs across diverse scenarios, demonstrating the advantages of its unified modeling paradigm for understanding, editing, and generation. Figure 6 and 8 compare the performance of InternSVG with other approaches on the Image-to-SVG task. The results demonstrate that InternSVG not only substantially outperforms general-purpose MLLMs and existing LLM-based SVG generation models, but also exceeds LIVE in performance while requiring significantly fewer tokens. In terms of detail preservation, InternSVG demonstrates superior capability compared with existing approaches. As shown in Figure 9 and 10, for the generation of chemical structural formulas, existing generalpurpose MLLMs generally lack relevant training data and therefore fail to produce correct chemical structures in either task. In addition, compared with Starvector, our method demonstrates superior control over fine-grained details of chemical bonds. Figure 11 and 12 present comparison between InternSVG and general-purpose MLLMs on SArena-Animation. Existing approaches show limited ability in handling vector animations, often struggling with visual quality and motion control. In contrast, InternSVG demonstrates superior capability, producing animations with higher fidelity and more consistent temporal dynamics. Figure 13, 14, and 15 present visualizations of InternSVG outputs on SVG understanding and editing tasks. 20 Figure 5: Qualitative comparison of Text-to-SVG performance between baselines and InternSVG on SArena-Icon. Red cross icons denote cases where the model failed to generate valid SVG output. Figure 6: Qualitative comparison of Image-to-SVG performance between baselines and InternSVG on SArena-Icon. Red cross icons denote cases where the model failed to generate valid SVG output. 21 Figure 7: Qualitative comparison of Text-to-SVG performance between baselines and InternSVG on SArena-Illustration. Red cross icons denote cases where the model failed to generate valid SVG output. 22 Figure 8: Qualitative comparison of Image-to-SVG performance between baselines and InternSVG on SArena-Illustration. Red cross icons denote cases where the model failed to generate valid SVG output. 23 Figure 9: Qualitative comparison of Text-to-SVG performance between baselines and InternSVG on SArena-Chemistry. Figure 10: Qualitative comparison of Image-to-SVG performance between baselines and InternSVG on SArena-Chemistry. Figure 11: Qualitative comparison of Text-to-SANI performance between baselines and InternSVG on SArena-Animation. 25 Figure 12: Qualitative comparison of Video-to-SANI performance between baselines and InternSVG on SArena-Animation. 26 Figure 13: Visualization of InternSVG outputs on SVG understanding. Figure 14: Visualization of InternSVG outputs on SVG editing. 28 Figure 15: Visualization of InternSVG outputs on SVG editing."
        },
        {
            "title": "C DETAILS OF SAGOGE AND SARENA",
            "content": "C.1 STATISTICS OF SAGOGE Table 17 presents the statistics of SAgoge in its four domains. The Icon subset contains 2.8M SVGs and 11M samples, with an average sequence length of 846 tokens, which reflects relatively simple structures. The Illustration subset is smaller in scale, with 600k SVGs and 1.6M samples, but it has much longer sequences with an average of 8.7k tokens that indicate high structural complexity. The Animation subset consists of 61K SVGs and 122K samples, and it provides dynamic elements that capture temporal changes. The Chemistry subset provides 1.7M SVGs and 3.4M samples, and its average length of 1.7k tokens shows the rich geometric and semantic details of chemical diagrams. Table 17: Data statistics of SAgoge. #SVGs denotes the number of vector graphic files, #Samples refers to the number of training samples, and Avg. Tokens indicates the average token length of the SVG code."
        },
        {
            "title": "Dataset",
            "content": "#SVGs #Samples Avg. Tokens"
        },
        {
            "title": "Icon\nIllustration\nAnimation\nChemistry",
            "content": "2.8M 600K 61K 1.7M 11M 1.6M 122K 3.4M 846 8673 847 1752 C.2 STATISTICS OF SARENA Table 18 shows the data statistics of SArena. SArena encompasses multiple domains (Icon, Illustration, Animation, Chemistry) and diverse set of tasks, including multiple-choice QA, editing, text-to-SVG, image-to-SVG, and video-to-SVG. Here, Avg. Tokens is the average token length of the training sample. Table 18: Data statistics of SArena."
        },
        {
            "title": "Tasks",
            "content": "#Samples Avg. Tokens"
        },
        {
            "title": "Chemistry",
            "content": "Mutiple-choice QA Editing Text-to-SVG Image-to-SVG Text-to-SVG Image-to-SVG Text-to-SANI Video-to-SANI Text-to-SVG Image-to-SVG 6012 2000 6013 6013 2001 504 504 3003 3003 1197 3175 1118 1169 8181 8291 1677 1719 1010 C.3 DATA SYNTHESIS PIPELINE C.3."
        },
        {
            "title": "ILLUSTRATION DATA SYNTHESIS PIPELINE",
            "content": "As high-quality open-source SVG resources for illustrations are scarce, we establish dedicated synthesis pipeline to expand our dataset. The overall process is depicted in Figure 16. We begin by employing GPT-4o to automatically generate diverse textual prompts covering wide range of objects, styles, and scene descriptions. These prompts are crafted to encourage the generation of vector-like visual features, ensuring compatibility with subsequent vectorization. The 30 Figure 16: Illustration data synthesis pipeline. Textual prompts generated by GPT-4o are processed by FLUX-based model and converted to SVGs via VTracer. prompts are then processed by FLUX-based generative model equipped with vector-style LoRA adapter1. This step ensures that the rendered images are not only semantically consistent with the prompts but also stylistically aligned with the characteristics of SVG, such as simplified shapes, flat colors, and clean edges. Finally, we apply VTracer to convert the generated images into SVG format, which performs vectorization by detecting paths, curves, and fills. This automated workflow bridges the gap between raster generation and vector representation, enabling us to systematically produce large-scale, diverse, and high-quality dataset of SVG illustrations. C.3.2 ANIMATION DATA SYNTHESIS PIPELINE High-quality open-source SVG animations are extremely scarce, making it infeasible to rely solely on existing resources to achieve the desired scale and diversity. To overcome this limitation, we exploit the powerful code-generation capability of Claude-Sonnet-4 to synthesize SVG animations compliant with the SMIL standard. We design generation protocols with explicit constraints, such as fixed canvas size and mandatory animation primitives, which ensure structural consistency, renderability, and suitability for large-scale training. This pipeline enables systematic expansion of the animation dataset and produces SVG animations with richer semantic content and greater geometric diversity. 1https://huggingface.co/renderartist/simplevectorflux 31 C.3.3 EXAMPLES OF SAGOGE Figure 17: Examples of multiple-choice QA tasks. In this task, we only provide the SVG code, requiring the model to answer the question directly based on the semantic and structural information contained in the SVG code. Figure 18: Examples of SVG description tasks. 33 Figure 19: Examples of SVG description tasks. 34 Figure 20: Examples of SVG editing tasks. Figure 21: Examples of SVG editing tasks. 36 Figure 22: Examples of Text-to-SVG tasks. Line 12 show icon generation, Line 34 show illustration generation, and Line 56 show chemical structural formula generation. 37 Figure 23: Examples of Image-to-SVG tasks. Line 1 shows icon generation, Line 2 shows illustration generation, and Line 3 shows chemical structural formula generation. Figure 24: Examples of Text-to-SANI tasks. 39 Figure 25: Examples of Video-to-SANI tasks. 40 Figure 26: Examples of Video-to-SANI tasks. C.4 TEXT PROMPT TEMPLATE C.4.1 TEMPLATE FOR DATASET CONSTRUCTION We randomly sample from the following prompts to generate the SVG description data. \"Describe in detail the semantic or geometric features of the object shown in the image.\", \"Detail the semantic or geometric features of the object in the image.\", \"Offer detailed description of the geometric or semantic features of the object in the image.\", \"Give comprehensive description of the semantic or geometric properties of the object depicted in the image.\", \"Provide an in-depth description of the semantic or geometric aspects of the object shown in the image.\", \"Explain in detail the semantic or geometric characteristics of the object displayed in the image.\", \"Could you detail the geometric or semantic features of the object in the image?\", \"I need detailed description of the geometric or semantic attributes of the object in the image.\", \"Please describe the semantic or geometric features of the object in the image comprehensively.\", \"Provide thorough description of the geometric or semantic properties of the object in this image.\", \"Can you elaborate on the semantic or geometric features of the object in the image?\", \"What are the geometric or semantic features of the object in the image ?\", \"Describe precisely the semantic or geometric characteristics of the object shown in the image.\", \"Provide detailed analysis of the geometric or semantic features of the object in this image.\", \"Elaborate on the semantic and geometric characteristics of the object shown in the image.\" For another form of SVG understanding, we design multiple-choice QA tasks, which are constructed using the following prompts. Given one image rendered from an SVG, write exactly four multiple-choice Q&A items about the depicted object. Q1-Q3: ask about visible semantic or geometric properties (e.g., color/ shape/count/position/size/symmetry/orientation). Q4: ask about the overall identity/meaning of the whole object. Each item has options A-D, one unambiguous correct answer, and three plausible distractors. Use only information visible in the image; avoid \"All/None of the above\". Output (JSON array): [ {\"q\": \"...?\", \"choices\": {\"A\": \"...\", \"B\": \"...\", \"C\": \"...\", \"D\": \"...\"}, \"answer\": \"B\"}, {\"q\": \"...?\", \"choices\": {\"A\": \"...\", \"B\": \"...\", \"C\": \"...\", \"D\": \"...\"}, \"answer\": \"D\"}, {\"q\": \"...?\", \"choices\": {\"A\": \"...\", \"B\": \"...\", \"C\": \"...\", \"D\": \"...\"}, \"answer\": \"A\"}, {\"q\": \"What is the object as whole?\", \"choices\": {\"A\": \"...\", \"B\": \"...\", \"C\": \"...\", \"D\": \"...\"}, \"answer\": \"C\"} ] For the editing tasks, we design specific prompts for each of the 10 subtasks to construct the dataset. The following code illustrates the prompts used for color editing and style transfer tasks. # color editing: \"\"\" 42 You are given two images: an original image and an edited version where certain colors have been changed. You are also given the exact color transformation information in the format: Change color [original_hex] to [new_hex]. Your task is to generate TWO textual editing instructions: 1. **simple instruction** (direct color change, using hex code). 2. **complex instruction** (semantic editing, describing what part of the object changed, e.g. \"the left arrow color\"). ### Example: Input: Change color #00abff to #D8BFD8 Output: Change #00abff to thistle (#D8BFD8) Change the left arrow color to thistle Now generate the two lines of instructions for the following input: {description} Important: You must output exactly two lines - no explanations, no formatting, no extra words. Only the two instructions. \"\"\" # Style transfer The first image shows {caption_before} and the second shows different {caption_after}. Describe how the first image should be edited to look like the second image. Do not just say \"Change to match the second image/emoji,\" but specify the expected result. Also, make the instructions as clear and as short as possible. For example, if plane is landing towards the runway in the first image and taking off in the second, you could say \"Make the plane take off\". For other simple editing tasks, we use the following prompts. # adding stroke \"Add an outline to this SVG shape.\", \"Apply stroke effects to the SVG graphic.\", \"Add border to the shape.\", \"Please add an outline stroke to this graphic.\", \"Add border lines to the SVG shape.\", \"Give the graphic border outline.\", \"Please add stroke to the SVG element.\", \"Add an outer outline to this shape.\", \"Add boundary lines to the graphic.\", \"Please add border effects to the SVG graphic.\", \"Add outline lines to this graphic.\", \"Add stroke outline to the SVG shape.\", \"Please add outer border lines to the graphic.\", \"Add border stroke to this SVG.\", \"Add outline border to the shape.\" # translation \"Translate this SVG graphic.\", \"Move the SVG graphic to new position.\", \"Please translate this graphic.\", \"Move the graphic in the specified direction.\", \"Please move the SVG shape.\", \"Adjust the position of the graphic.\", \"Please translate this shape.\", \"Move the position of the SVG element.\", \"Please adjust the graphics position.\", \"Move the SVG graphic in certain direction.\", \"Please apply translation transform to the graphic.\", \"Move this SVG shape.\", \"Please translate the graphic to new position.\", 43 \"Apply position movement to the SVG.\", \"Please translate this SVG element.\" # scaling \"Scale this SVG graphic.\", \"Adjust the size of the SVG graphic.\", \"Please scale this graphic.\", \"Enlarge or shrink the graphic.\", \"Please scale the SVG shape.\", \"Adjust the size of the graphic.\", \"Please scale this shape.\", \"Apply size adjustment to the SVG element.\", \"Please adjust the graphic size.\", \"Scale the SVG graphic proportionally.\", \"Please apply scaling transform to the graphic.\", \"Adjust the size of this SVG shape.\", \"Please scale the graphic proportionally.\", \"Apply size adjustment to the SVG.\", \"Please scale this SVG element.\" # rotation \"Rotate this SVG graphic.\", \"Rotate the SVG graphic around its center.\", \"Please rotate this graphic.\", \"Rotate the graphic by specified angle.\", \"Please rotate the SVG shape.\", \"Rotate the graphic around the center point.\", \"Please rotate this shape.\", \"Apply angle adjustment to the SVG element.\", \"Please rotate the graphic.\", \"Rotate the SVG graphic clockwise/counterclockwise.\", \"Please apply rotation transform to the graphic.\", \"Rotate this SVG shape around its center.\", \"Please rotate the graphic by certain angle.\", \"Apply rotation operation to the SVG.\", \"Please rotate this SVG element.\" # flipping \"Flip this SVG graphic.\", \"Flip the SVG graphic.\", \"Please flip this graphic.\", \"Flip the graphic vertically or horizontally.\", \"Please flip the SVG shape.\", \"Flip the direction of the graphic.\", \"Please flip this shape.\", \"Apply mirror flip to the SVG element.\", \"Please flip the graphic.\", \"Apply mirror processing to the SVG graphic.\", \"Please apply flip transform to the graphic.\", \"Flip this SVG shape.\", \"Please apply mirror flip to the graphic.\", \"Apply flip operation to the SVG.\", \"Please flip this SVG element.\" # transparency \"Adjust the opacity of this SVG graphic.\", \"Set the transparency of the SVG graphic.\", \"Please adjust the transparency of the graphic.\", \"Set the graphic to semi-transparent.\", \"Please modify the opacity of the SVG shape.\", \"Adjust the opacity of the graphic.\", \"Please set the transparency of this shape.\", \"Apply opacity adjustment to the SVG element.\", \"Please adjust the graphics opacity.\", \"Set the SVG graphic to transparent effect.\", 44 \"Please apply opacity transform to the graphic.\", \"Adjust the opacity of this SVG shape.\", \"Please modify the graphics transparency.\", \"Apply opacity operation to the SVG.\", \"Please adjust the opacity of this SVG element.\" # cropping \"Crop this SVG graphic.\", \"Crop the SVG graphic to show part of it.\", \"Please crop this graphic.\", \"Crop the graphic to specific region.\", \"Please crop the SVG shape.\", \"Crop the graphic to half size.\", \"Please crop this shape.\", \"Apply cropping to the SVG element.\", \"Please crop the graphic.\", \"Crop the SVG graphic to show only part.\", \"Please apply crop transform to the graphic.\", \"Crop this SVG shape.\", \"Please crop the graphic to specified area.\", \"Apply crop operation to the SVG.\", \"Please crop this SVG element.\" For the Icon generation task, we employ the following prompt to guide GPT-4o or InternVL3-78B in producing the required instructions. You are an expert in image analysis and description. You will be given an image of graphic object. Your task is to analyze the image and generate **concise and precise caption** that describes the semantic meaning and structural information shown in the image. Focus on: - Semantic meaning and object identification - Shape and geometric structure - Color information - Key visual features (e.g., symmetry, number of elements, patterns) - Spatial relationships between elements Avoid: - Symbolism, history, or cultural context - Subjective interpretations - Lengthy explanations or unnecessary details Generate clear, descriptive caption that captures the essential visual and semantic information of the image. Output format: concise caption such as: - red five-pointed star with black outline - blue symmetrical hexagon with white interior - Three overlapping circles in green, blue, and red For the illustration generation task based on Flux-synthesized images, which often contain richer elements and backgrounds, we require MLLM to generate more detailed instructions. You are an expert in vector-art image analysis and description. You will be given vector illustration, clipart, or doodle. Task: Produce **detailed, objective caption (30-80 words, 1-2 sentences)** that precisely describes the visible content and the vector graphic structure. 45 Must include, in consistent order: 1) Main subject(s): category, count, pose/action, relative sizes. 2) Appearance details: sex, salient parts (clothing, accessories, facial/ hair features if present), simple shapes used. 3) Geometry & line work: primitives (circles/rectangles/paths), curvature , corner roundness, symmetry/repetition, perspective (front/side/ isometric), stroke style (thin/medium/thick; solid/dashed; round/ square caps and joins). 4) Color & fill: dominant palette names, fill types (flat/gradient/ pattern/transparent). 5) Composition & layout: spatial relations (left/right/center/overlap/ occlusion), alignment, and layering depth if evident. 6) Text: if there is any text in the image, transcribe visible words/ letters and note font feel (block/script/handwritten), case. Constraints: - Be factual and visual only. No symbolism, history, brand identification , or guesses about identity or emotions. - Avoid opinions and storytelling; avoid tool/author speculation. - Prefer concrete nouns, numbers, and measurable attributes. - Remember to only output 30-80 words, 2-3 sentences. - Do not wrap the output in quotation marks or triple backticks (). Output format: e.g: - The illustration shows one woman in side view bending forward to place an arrow onto circular target symbol beside the large block letters \"SEO.\" She wears yellow short-sleeve top, purple pants, and green shoes, with shoulder-length black hair. The vector uses clean curves, rounded edges, flat fills in purple, yellow, green, and skin tones, and medium-weight solid strokes. The composition places the letters on the left, the target in the center, and the woman leaning in from the right, with light purple clouds above. - The illustration depicts single serving of chocolate mousse in an orange cup, topped with swirl of cream. The mousse is centrally placed on beige plate. The vector art uses smooth, rounded shapes and flat fills in orange, brown, beige, and cream, with no visible strokes. The composition is symmetrical, with the cup and plate aligned centrally, creating balanced and cohesive layout. - The illustration features single woman on the right, gesturing toward key and lock. She has long dark hair and wears yellow top with green trim and white patterns. The vector employs smooth, rounded shapes and flat fills in yellow, green, purple, and black, with medium solid strokes. The key and lock appear in speech bubbles on the left, using simple geometric forms like circles and squares, arranged asymmetrically. C.4.2 QUESTION TEMPLATE For the SVG description task, we randomly select one from the following five prompts as the question input. \"{svg}nDescribe in detail the semantic and geometric features of the SVG code.\", \"Here is an SVG code snippet:n{svg}nBased solely on this code, explain what the SVG code represents and describe its semantic meaning and geometric characteristics in detail.\", \"The SVG source is given below:n{svg}nIdentify the main object depicted , and list its geometric shapes (e.g., rectangles, circles, lines) as well as semantic cues (e.g., what the object is, its context, implied function).\", \"Below is the full SVG code:n{svg}nDescribe both the visual geometry (shapes, symmetry, layout) and the conceptual meaning (what it represents 46 , symbolic features, color implications) of the resulting image.\", \"SVG Code:n{svg}nWhat object does this render? Provide detailed breakdown of its semantic purpose and all notable geometric features (e.g., angles, curves, proportions).\" For multiple-choice QA tasks, we adopt the following template as the question input. \"As an SVG specialist, you have deep knowledge of vector graphics and their properties. Given an SVG code snippet, analyze the code to answer the following multiple-choice question about the visual elements it represents. Provide only the final answer in the format A., B., C., or D. (e.g., A.), without any additional explanation or text. {SVG}\" For the SVG editing task, we design set of instruction templates that guide models to modify the original SVG code with minimal changes while preserving its structural integrity. \"You are precision SVG editing engine. Your task is to modify the provided SVG code based on the users instruction. Follow these rules strictly: 1. Make only the minimal necessary changes to the SVG to satisfy the instruction. 2. Preserve the original code structure, IDs, and classes whenever possible. 3. Your output MUST be the complete, raw SVG code and nothing else. Do not include explanations, comments, or markdown fences (like svg). SVG Code: {svg} Instruction: {instruction}\", \"You are an automated SVG modification utility. Your function is to alter the given SVG code according to the instruction. Adhere to these directives: apply only the absolute minimum changes required. Do not alter existing IDs, classes, or the overall structure. The output must be the raw, modified SVG code exclusively. Suppress all explanatory text, commentary, and markdown formatting. SVG Code: {svg} Instruction: {instruction}\", \"You are specialized SVG editing assistant. need you to edit the SVG code below based on my instruction. Please adhere to the following strict requirements: 1. Implement the most efficient and minimal change possible. 2. Do not refactor or reformat the code; preserve its original structure and attributes. 3. Your entire response must be the edited SVG code and nothing more. Source SVG: {svg} Editing Instruction: {instruction}\", \"Act as an expert SVG code manipulator. Your objective is to edit the following SVG to match the users request. - **Modification Principle:** Edit only what is necessary to fulfill the instruction. - **Structural Integrity:** Maintain the original SVGs structure, including all IDs and classes. - **Output Format:** Respond with only the final, raw SVG source code. No extra text, comments, or markdown. SVG Code: {svg} Instruction: {instruction}\", \"As professional SVG graphic editor, your job is to apply the requested change to the SVG code. Core Directives: 47 1. Change only what the instruction requires. 2. Keep the original codes formatting and structure intact. 3. Output the full, edited SVG code. SVG to Edit: {svg} User Request: {instruction} REMINDER: Your response must not contain any text besides the SVG code itself.\", \"Edit the SVG based on the instruction. Apply minimal changes, preserve the structure, and output only the raw SVG code. SVG: {svg} Instruction: {instruction}\", For the Text-to-SVG generation task, we design multiple instruction templates for the dialogue data, as illustrated below. These templates guide the model to produce SVG code that fulfills the given instruction, thereby enhancing the diversity of dialogue formulations. \"You are an expert in SVG generation. After receiving an instruction, output an SVG that satisfies the instruction. Instruction: {instruction}\", \"You are skilled SVG designer. Based on the instruction below, create an SVG that fulfills the given instruction.n Instruction: {instruction }\", \"You are professional in vector-based image generation. Please produce an SVG that matches the following instruction. Instruction: {instruction}\", \"You are an expert in SVG generation. Generate SVG code that reflects the instruction. Instruction: {instruction}\", \"As an expert in SVG generation, you will be given an instruction. Please generate an SVG that satisfies the instruction. Instruction: {instruction}\", \"You are specialist in creating scalable vector graphics. Please design an SVG that meets the requirements specified in the instruction. Instruction: {instruction}\", \"You are an SVG development expert. Your task is to create an SVG image that accurately represents the given instruction. Instruction: {instruction}\", \"You are master of SVG generation. Please generate an SVG file that fulfills the following instruction requirements. Instruction: {instruction}\", \"You are professional SVG graphic designer. Create an SVG that perfectly matches the provided instruction. Instruction: {instruction}\", \"You are an expert in vector graphics and SVG coding. Please produce an SVG that satisfies the instruction below. Instruction: {instruction}\" For the Image-to-SVG generation task, we also design set of dialogue templates. In these templates, the model receives both an input image and textual instruction, and is required to generate SVG code that fulfills the given instruction. \"You are an expert in SVG generation. After receiving an image and an instruction, output an SVG that satisfies the instruction. Image: <image> Instruction: {instruction}\", \"You are skilled SVG designer. Based on the image and instruction below, create an SVG that fulfills the given instruction. Image: <image> Instruction: {instruction}\", \"You are professional in vector-based image generation. Please produce an SVG that matches the following image and instruction. Image: <image> Instruction: {instruction}\", 48 \"You are an expert in SVG generation. Generate SVG code that reflects the image and instruction. Image: <image> Instruction: {instruction}\", \"As an expert in SVG generation, you will be given an image and an instruction. Please generate an SVG that satisfies the instruction. Image: <image> Instruction: {instruction}\", \"You are specialist in creating scalable vector graphics. Please design an SVG that meets the requirements specified in the instruction. Image: <image> Instruction: {instruction}\", \"You are an SVG development expert. Your task is to create an SVG image that accurately represents the given image and instruction. Image: <image> Instruction: {instruction}\", \"You are master of SVG generation. Please generate an SVG file that fulfills the following image and instruction requirements. Image: <image> Instruction: {instruction}\", \"You are professional SVG graphic designer. Create an SVG that perfectly matches the provided image and instruction. Image: <image> Instruction: {instruction}\", For the Text-to-SANI animation generation task, we design multiple dialogue templates to guide the model in generating SVG animations from textual instructions. \"You are an expert in SVG animation creation. Given an instruction, generate an SVG animation that follows it. Instruction: {instruction}\", \"You are skilled SVG animation designer. Based on the instruction below, produce an SVG animation that meets the requirement. Instruction: {instruction}\", \"You are professional in vector animation. Please generate an SVG animation according to the following instruction. Instruction: {instruction}\", \"You are an expert in SVG animation coding. Create SVG animation code that fulfills the instruction. Instruction: {instruction}\", \"As an expert in SVG animation, you will be given an instruction. Please output an SVG animation that matches it. Instruction: {instruction}\", For the Video-to-SANI animation generation task, we design diverse set of dialogue templates to enable the model to generate SVG animations conditioned on both video input and textual instruction. \"You are an expert in SVG animation creation. Given video and an instruction, generate an SVG animation that follows it. Video: <video> Instruction: {instruction}\", \"You are skilled SVG animation designer. Based on the video and instruction below, produce an SVG animation that meets the requirement. Video: <video> Instruction: {instruction}\", \"You are professional in vector animation. Please generate an SVG animation according to the following video and instruction. Video: <video> Instruction: {instruction}\", \"You are an expert in vector animation. Given the video and the instruction, create an SVG animation that matches the request. Video: <video> 49 Instruction: {instruction}\", \"You are specialist in SVG animation design. Use the video and instruction provided to generate an SVG animation that aligns with them. Video: <video> Instruction: {instruction}\", \"As professional SVG animation designer, analyze the video and instruction, then generate an SVG animation that fulfills the task. Video: <video> Instruction: {instruction}\", \"You are an expert in animated vector graphics. Create an SVG animation that reflects both the video and the given instruction. Video: <video> Instruction: {instruction}\", \"You are skilled SVG animation creator. From the following video and instruction, produce an SVG animation that conveys the intended action. Video: <video> Instruction: {instruction}\", \"You are professional vector animation generator. Given the video and instruction, output SVG animation code that implements it. Video: <video> Instruction: {instruction}\", \"As an SVG animation expert, your task is to transform the video and instruction into corresponding SVG animation. Video: <video> Instruction: {instruction}\","
        }
    ],
    "affiliations": [
        "Donghua University",
        "Nanjing University",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong"
    ]
}