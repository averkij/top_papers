{
    "paper_title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
    "authors": [
        "Zhenyang Cai",
        "Jiaming Zhang",
        "Junjie Zhao",
        "Ziyi Zeng",
        "Yanchao Li",
        "Jingyi Liang",
        "Junying Chen",
        "Yunjin Yang",
        "Jiajun You",
        "Shuzhi Deng",
        "Tongfei Wang",
        "Wanting Chen",
        "Chunxiu Hao",
        "Ruiqi Xie",
        "Zhenwei Wen",
        "Xiangyi Feng",
        "Zou Ting",
        "Jin Zou Lin",
        "Jianquan Li",
        "Guangjun Yu",
        "Liangyi Chen",
        "Junwen Wang",
        "Shan Jiang",
        "Benyou Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 8 5 5 1 1 . 2 1 5 2 : r DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry Zhenyang Cai2, Jiaming Zhang1, Junjie Zhao5,6, Ziyi Zeng2, Yanchao Li2, Jingyi Liang2 Junying Chen2, Yunjin Yang2, Jiajun You2,4, Shuzhi Deng1, Tongfei Wang1 Wanting Chen1, Chunxiu Hao1, Ruiqi Xie1, Zhenwei Wen5, Xiangyi Feng4 Zou Ting1, Jin Zou Lin1, Jianquan Li4, Guangjun Yu2,7 Liangyi Chen3, Junwen Wang5, Shan Jiang1, Benyou Wang2,7,8,9 1 Shenzhen Stomatology Hospital (Pingshan) of Southern Medical University 2 The Chinese University of Hong Kong, Shenzhen 3 State Key Laboratory of Membrane Biology, Beijing Key Laboratory of Cardiometabolic Molecular Medicine, Institute of Molecular Medicine, National Biomedical Imaging Center, School of Future Technology, Peking University 4 Freedom AI 5 Division of Applied Oral Sciences & Community Dental Care Faculty of Dentistry, The University of Hong Kong 6 Beijing Institute of Collaborative Innovation 7 National Health Data Institute, Shenzhen 8 Shenzhen Loop Area Institute 9 Shenzhen Institute of Big Data"
        },
        {
            "title": "Abstract",
            "content": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLMs visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs."
        },
        {
            "title": "Introduction",
            "content": "Dental healthcare is an essential area of public health, yet the workload of dental professionals continues to increase each year [1, 2, 3, 4]. To support both clinicians and patients, multimodal large language models (MLLMs) [5, 6, 7, 8] capable of interactive communication through dialogue have recently attracted significant attention, offering new possibilities for intelligent dental care. However, despite their promising performance in general medical applications [9, 10, 11, 12, 13, 14, 15], current MLLMs still face notable limitations when dealing with more specialized medical imaging problems, such as images in dentistry. Equal contribution. Corresponding authors. After extensive observation, we found that although current MLLMs can sometimes identify relevant features in dental images, they often fail to extract the necessary information and reason over it to produce correct answers. This exposes two challenges when applying MLLMs to dentistry. First, current MLLMs lack sufficient visual understanding of dental images. This limitation prevents them from effectively using their knowledge to perform reliable diagnostic reasoning. Second, although recent MLLMs exhibit strong complex reasoning capabilities that have led to substantial gains in complex image understanding tasks [14, 16, 17, 18, 19], our quantitative results show that such reasoning provides only marginal improvements in dentistry. This suggests that there remains significant room for advancing complex reasoning specifically tailored to dental diagnosis. To address these challenges, we injected dental knowledge into the MLLM and strengthened its ability for complex reasoning. Specifically, we collected large number of dental images with textual descriptions or labels from online sources and combined them with professionally annotated images from dental hospitals. Together, these resources form dataset, which includes over 120k dental images with detailed descriptions and additional QA pairs for specific downstream tasks. The descriptions highlight diagnostically relevant cues in the image, helping the model connect visual features with its textual understanding of the conditions, while the QA data further enhances its downstream task performance. After the enhancement of multimodal understanding, reinforcement learning (RL) stage with the Group Relative Policy Optimization (GRPO) [20] algorithm was applied, guiding the model to use the added knowledge to explore more explanatory solutions for dental questions. This process resulted in DentalGPT, the first specialized dental MLLM equipped with complex reasoning capabilities. Then, comprehensive evaluation was conducted to assess DentalGPTs ability in dental image analysis. First, MMOral Bench and dentistry-focused subset of medical VQA benchmarks were curated to evaluate the models generalization ability. Second, professionally annotated intraoral and panoramic images were used to assess its competence in identifying specific dental diseases. The results show that DentalGPT, despite having only 7 billion parameters, surpasses existing MLLMs in dental image understanding and question answering, highlighting its efficiency and domain specialization. Beyond overall performance, the impact of the two training stages was examined through in-depth analysis, revealing several key findings. The training data exhibited higher knowledge density and greater professional quality compared with GPT-5distilled data, indicating their advantage for domain learning. Based on this stronger data foundation, the Multimodal Understanding Enhancement stage was shown to substantially enrich the models dental knowledge and improve its performance across diverse image analysis tasks. These gains were further amplified during reinforcement learning, resulting in higher disease classification accuracy and more professional identification of diagnostically relevant visual cues. Together, these results demonstrate that high-quality dental data, combined with staged training, plays critical role in shaping DentalGPTs specialized capabilities. In conclusion, our contributions are: 1) Introduce DentalGPT, specialized dental MLLM equipped with advanced dental image analysis and complex reasoning capabilities to interpret fine visual details and associate them with related conditions. 2) Curate large-scale VQA dataset containing the largest collection of dental images to date with detailed descriptions of diagnostically relevant visual features. 3) Demonstrate that DentalGPT achieves strong performance on multiple disease classification and VQA tasks in dentistry, surpassing many larger MLLMs."
        },
        {
            "title": "2 On Incentivizing Multimodal Complex Reasoning in Dentistry",
            "content": "2.1 Motivations for Multimodal Complex Reasoning in Dentistry Dentistry is key medical field that relies on clinicians analyzing patients imaging data and communicating with them for diagnosis, yet even leading MLLMs with strong general multimodal abilities still fall far behind professional dentists in multimodal diagnosis. Case Study To investigate this phenomenon, we conducted case study on dental multimodal task using one leading commercial MLLM as well as both the Instruct and Thinking mode of state-of-the-art open-source MLLM (Figure 1). After analyzing the model outputs, we found that although they could identify the relevant visual features to be counted, they still produced incorrect answers. Notably, by examining the reasoning trajectory of Qwen3-VL-235B-A22B-Thinking, we 2 observed that it repeatedly reflected on and revised its own intermediate conclusions. Through this type of complex reasoning pattern, the model gradually approached the correct answer step by step. Although it still failed to produce the correct final prediction, this behavior highlights the potential of applying complex reasoning strategies to achieve more accurate multimodal diagnosis in dentistry. Figure 1: Examples of top-tier general-purpose MLLMs analyzing dental image task. Red indicates incorrect analysis, green indicates correct analysis, and orange highlights reflective turns in the complex reasoning process. Quantitative Experiment Furthermore, to quantitatively assess model performance on dental tasks and examine whether complex reasoning provides additional benefits, We evaluated leading MLLMs 2 with and without the complex reasoning mode using the existing MMOral-OPG-Bench [21]. The results in Figure 2 show that both models achieve higher scores in their complex reasoning mode, further demonstrating the potential of complex reasoning in dentistry. Figure 2: Accuracy (%) of MLLMs with and without the complex reasoning mode on the MMOralOPG-Bench. 2Complex reasoning mode: GPT5-2025-08-07, Gemini-2.5-Pro-Thinking, Claude-Sonnet-4-5-20250929Thinking, Qwen3-VL-235B-A22B-Thinking. Without complex reasoning mode: GPT5-chat-2025-08-07, Gemini-2.5-Pro-NoThinking, Claude-Sonnet-4-5-20250929, Qwen3-VL-235B-A22B-Instruct. 3 2.2 Challenges for Multimodal Complex Reasoning in Dentistry Prior works [14, 16, 17, 18, 19] have shown that complex reasoning can significantly improve MLLM performance in mathematics, medicine and other complex image understanding tasks. However, the results in Section 2.1 indicate that the benefits of complex reasoning for dentistry-related multimodal tasks remain limited. Thus, after closer analysis of the quantitative results, we conclude the following challenges: Challenge I: Limited Multimodal Understanding in Dentistry MLLMs in Section 2.1 perform poorly on MMOral-OPG-Bench (None of them exceed 60% accuracy). The foundation of complex visual reasoning is models ability to accurately interpret dental images, and straightforward way to enhance this ability is to inject visual domain knowledge through large-scale data [5]. However, existing dental visionlanguage datasets remain very limited; for instance, only about 0.3 percent of PubMedVision [13] images involve teeth. To address this gap and support greater multimodal understanding in dentistry, we introduce multimodal dataset with 120k dental images. Challenge II: Limited Complex Reasoning in Dentistry MLLMs employing complex reasoning refine and verify their intermediate conclusions, allowing them to gradually approach the correct answer. As shown in Section 2.1, current MLLMs achieve only limited gains on dentistry-related tasks with the complex reasoning. However, prior studies [14, 18, 19] about medical MLLMs have shown that task-specific reinforcement learning can strengthen these reasoning abilities and improve downstream performance. This motivates us to explore domain-specific reinforcement learning to further enhance complex reasoning and improve performance in dentistry."
        },
        {
            "title": "3 DentalGPT: From Multimodal Understanding to Complex Reasoning",
            "content": "To overcome the limited multimodal understanding and basic reasoning patterns of existing MLLMs in dental imaging, DentalGPT is proposed as dentistry-specific MLLM built through 2-stage training process (Figure 3). Stage focuses on multimodal understanding enhancement to strengthen the MLLMs understanding of dental images, while Stage II leverages this enhanced ability through reinforcement learning to improve complex reasoning. Figure 3: The 2-stage process of building DentalGPT. Multimodal Understanding Enhancement stage uses large dataset to align the models medical knowledge with its multimodal understanding and prepare it for downstream tasks; Reinforcement Learning then strengthens complex reasoning ability. 4 3.1 Stage I: Enhancement of Multimodal Understanding To address the limited multimodal understanding of existing MLLMs in dentistry, this stage enriches the model with high-quality dental knowledge using large-scale, professionally curated imagetext data. GPT-5 was used to generate detailed descriptions, forming specialized multimodal dataset that strengthens fine-grained visual understanding and basic downstream task performance. Data Composition At this stage, the collected images and their corresponding labels are organized into several types of training data. First, Image Captioning data is used to train the model to clearly and comprehensively describe dental images, helping align textual representations with real-world dental concepts. Second, Instruction Tuning data consists of large number of questionanswer samples, enabling the model to understand user intent and respond appropriately. Third, Complex Reasoning data includes multi-step and reflective thinking examples, serving as foundation for subsequent reinforcement learning to enhance multimodal complex reasoning capabilities. Finally, general-domain data [5, 22, 23] is incorporated to maintain the models ability to understand both images and text beyond dental scenarios, preventing overfitting to dental-specific tasks. Training Settings The model was then trained on this dataset for two epochs with batch size of 256 and learning rate of 2 105. All parameters were fully updated during training, with the first 5% of steps allocated for learning-rate warmup. 3.2 Stage II: Reinforcement Learning for Complex Reasoning After gaining new knowledge, the MLLM must learn to apply it for improved complex reasoning in multimodal diagnosis. Recent works such as DeepSeek-R1 [24] and GPT-o1 [25] show that reinforcement learning can encourage long chain-of-thought generation and enhance reasoning quality. Following this paradigm, we adopt GRPO to optimize the reasoning process of DentalGPT. Data Composition To achieve this goal, we selected set of dental images that were not used during the Stage to construct new dataset. Based on the original labels and their label sets, we generated multiple-choice questions with correct answers, enabling rule-based correctness checking, which is crucial for reward computation in GRPO. Training Strategy During this stage, we employ the GRPO algorithm to enhance the models reasoning ability on dental multiple-choice tasks. GRPO evaluates relative advantages within sampled groups of responses, enabling efficient optimization without requiring value network. Sampling Action Groups. For each input state = (I, Q), where is the dental image and the question, fixed prompt is appended to require the model to generate reasoning inside <think> tags and produce the final answer within <answer> tags. GRPO then samples group of candidate responses: ai πθ(a I, Q), This promotes diverse reasoning paths and prevents premature convergence. Reward Design. Each sampled answer ai is evaluated with composite reward: = 1, . . . , G. R(ai) = 0.1 Rformat(ai) + 0.9 Racc(ai). Format Reward: Ensures adherence to the required template: Rformat(ai) = (cid:26)1, 0, if formatted correctly, otherwise. Accuracy Reward: For multiple-choice questions: Racc(ai) = (cid:26)1, 0, if the prediction is correct, otherwise. Policy Update. Rewards within the group are normalized to produce relative advantages: Ai = R(ai) mean(R(a1), . . . , R(aG)) std(R(a1), . . . , R(aG)) . GRPO then updates the policy to reinforce high-advantage actions while constraining deviations from the reference policy via KL regularization. 5 Training Settings During GRPO optimization, the model was optimized using grouped rollouts with sampling size of 10 responses per prompt. Training was conducted with rollout batch size of 256 and learning rate of 1 106. The optimization ran for 5 epochs, and the maximum response length was capped at 8192 tokens to accommodate long CoT reasoning. This configuration ensured stable exploration within each action group while maintaining sufficient capacity for detailed reasoning outputs."
        },
        {
            "title": "4 Data Engineering",
            "content": "This section elaborates on the data foundation supporting DentalGPT, including the sources of the training data and the quality control processes used to ensure high-quality dental images; additionally, we analyze the overall data quality to ensure that its completeness, knowledge depth, and safety remain at leading level. 4.1 Data Collection Before improving an MLLMs understanding and reasoning on dental images, it is necessary to collect sufficiently large set of training samples (examples are shown in Figure 4), which provides the foundation for the model to effectively use its knowledge during image interpretation [5]. Figure 4: Annotation examples from different dental image collections. 4.1.1 Existing Annotated Data To efficiently obtain sufficiently large number of dentistry-specific multimodal datasets, we first sourced datasets from variety of open-source platforms. By leveraging certified, high-quality dental image datasets and existing literature, we aim to enrich the models understanding of relevant radiographic data. PMC-Dental-Caption-47k PubMed Central (PMC) is publicly accessible biomedical repository that hosts vast collection of peer-reviewed medical publications. It is considered reliable and widely used data source in previous research. From PMC, we filtered large number of dental images and retained the associated captions and labels provided within the original papers. This rich textual context is expected to provide valuable information for enhancing visual understanding and facilitating the integration of dental domain knowledge into our model. Opensource-Dental-Classification-49k To further leverage image datasets that have previously been used to train classification models, we collected wide range of dental-related classification datasets and consolidated them into larger corpus of dental images with corresponding labels. Specifically, each image is associated with one or more disease labels; for multi-class or multi-label datasets, we also retain the negative labels so that all available, clinically validated annotations can be fully exploited. This unified resource supports MLLMs in better aligning common dental disease categories with key feature identification. 6 Opensource-Dental-Detection-31k We also collected number of datasets previously utilized for dental lesion localization tasks, in which each image is annotated with one or more lesion instances together with their spatial coordinates. Although our model is not explicitly trained to predict bounding boxes, such annotations provide MLLMs with implicit spatial cues and lesion counts, thereby supporting the models capability to understand spatial relationships and quantify dental abnormalities. 4.1.2 Newly Annotated Data Building upon the open-source datasets described above, we observed that although they contain considerable number of annotations, the diagnostic focus is predominantly centered on few common dental conditions. From clinical perspective, however, there exist additional critical abnormalities and visual manifestations that warrant greater attention yet are underrepresented in existing data sources. To address this gap, we expanded the diagnostic label set and further curated new subset of dental images, which were annotated by certified dental experts with an emphasis on clinically significant conditions and indicative visual features. Expert-Annotated Dataset We collected some dental images from internet sources, hospital imaging archives, and publicly available datasets. After removing duplicates and low-resolution images, we curated candidate dataset that was subsequently annotated by professional dental clinicians. To ensure annotation quality, strict cross-validation mechanism was applied with different levels of control over data variation. For the training set, annotations from dentists with cross-validation agreement rate below 85% were filtered out, while the remaining labels were retained for constructing caption-style training data. For the test set, more rigorous protocol was adopted: each sample was annotated by at least two dentists, and only those with consistent diagnostic results were preserved. This dataset equips the model with specialized clinical knowledge and broader ability to recognize dental diseases and clinically relevant signs. 4.2 Data Curation To enrich the models ability in image understanding, instruction following, and task-specific dental reasoning, we further curated and refined all collected data using GPT-5. This curation step enables the model to learn not only visual concepts but also how to interpret clinical instructions and produce structured diagnostic outputs. The overall process is outlined as follows: Image Captioning This component focuses on enhancing the models ability to capture diagnostically relevant visual details in dental images. GPT-5 was instructed to describe all observable features that may aid diagnosis while referencing the original descriptions and labels, and to avoid any diagnostic assumptions. These observation-based captions were then paired with predefined questions and answers to form caption-based VQA subset, improving the models interpretation of dental images and reducing visual information gaps. Instruction Tuning To enhance performance on different downstream tasks, GPT-5 was prompted to generate questions based on the collected images and their descriptions, simulating real diagnostic scenarios. Additionally, several vision models were used to annotate portion of the datasets, and only high-confidence annotations were retained. GPT-5 then refined these annotations and transformed them into structured questionanswer pairs, further improving the models ability to handle dental question answering. Complex Reasoning To support the subsequent reinforcement learning stage, complex chain-ofthought data were constructed by GPT-5 following the HuatuoGPT-o1 [26] methodology. Fixed reasoning templates were added to activate the models basic multi-step reasoning ability, enabling it to analyze before answering. After these steps were completed, GPT-5-mini was used to perform secondary verification of all data. Entries that diverged from the original image descriptions or labels were removed, further ensuring data accuracy and producing the training dataset. 7 4.3 Quality Assessment As described in the methodology, the training dataset of DentalGPT was generated by GPT-5 while referencing existing image labels or descriptions to minimize hallucinations and ensure professional domain knowledge injection. To validate the effectiveness of this approach, five evaluation dimensions were defined, and randomly sampled set of 3,000 entries was assessed and compared against data obtained through direct GPT-5 distillation. To ensure fairness, all comparisons were evaluated using Gemini-2.5-Pro as the judge. Evaluation Setup Specifically, the dataset was evaluated across the following five dimensions: 1) Description Completeness : Whether all observable visual details in the image are thoroughly described, with particular attention to features that may contribute to dental diagnosis. 2) Terminology Consistency : Whether professional dental terminology is used correctly and consistently throughout the description. 3) Content Safety : Whether the content adheres to medical ethics and safety standards, avoiding sensitive, discriminatory, misleading, or inappropriate statements. 4) TextImage Consistency : Whether the textual description is well written and accurately aligned with the corresponding image content. 5) Knowledge Depth : Whether the description demonstrates an appropriate level of dental knowledge. Gemini-2.5-Pro was asked to score each dimension on scale of 1 to 5, and the final dataset quality was reported using the average score across all evaluated samples. Figure 5: Gemini-2.5-Pros multi-dimensional evaluation of GPT-5distilled data and the training dataset of DentalGPT (scores range from 0 to 5). For each dataset, the color blocks from left to right represent Description Completeness , Terminology Consistency , Content Safety , TextImage Consistency , and Knowledge Depth . Results Analysis Results are shown in Figure 5, our training dataset demonstrates clear advantages over the directly distilled version across multiple evaluation metrics. It can be observed that the data generated with label references shows the most significant improvements in terminology consistency and knowledge depth, and also achieves notable gains in the completeness of visual detail descriptions. Interestingly, Gemini-2.5-Pro assigns perfect score for safety to both datasets, indicating that GPT-5generated data perform exceptionally well in medical safety, avoiding harmful diagnostic suggestions and providing timely guidance to reduce potential risks. In conclusion, the results indicate that our dataset, by leveraging annotations from public datasets and descriptions from academic literature, provides more comprehensive and more professional knowledge injection for the model. Such foundation ensures substantial improvement in the performance of DentalGPT."
        },
        {
            "title": "5 Benchmark Design and Construction",
            "content": "To comprehensively evaluate the model, existing benchmarks containing dental images were first used to assess its performance. Additionally, large set of dental images was collected and annotated 8 with disease labels by professional dentists, ensuring clinical validity and allowing further assessment aligned with expert consensus. 5.1 Existing Benchmarks We also conducted comprehensive evaluations on several open-source medical VQA benchmarks to assess the models performance in more scenarios. (1) MMOral-OPG-Bench MMOral [21] contains multiple panoramic dental images with highquality expert annotations and spans five clinically grounded dimensions, offering thorough assessment of an MLLMs panoramic X-ray understanding. We use its open-ended test split to more directly and intuitively evaluate model performance. (2) DentalBench-Mixed was further constructed by strictly filtering tooth-related images from existing medical VQA benchmarks. Specifically, the data were sourced from PMC-VQA [27], OmniMedVQA [28], and MedXpertQA-MM [29], which are widely used for evaluating the capabilities of medical MLLMs. The dental-relevant portions of these benchmarks were extracted to form the DentalBench-Mixed dataset, enabling targeted assessment of models on dental image understanding. 5.2 Expert-annotated Benchmarks Annotation Workflow To maximize data diversity and ensure clinically reliable model outputs, we collected dental images from multiple sources and invited professional dentists to perform expert annotations. Based on advice from dentists, we defined set of commonly observed dental diseases and clinically relevant signs that are either diagnostically important themselves or serve as auxiliary evidence in clinical reasoning. These labels were then used to assess whether the model could correctly identify key visual cues in dental images. Figure 6: Cross-validation workflow used for benchmark labeling. To ensure annotation reliability, rigorous cross-validation workflow (shown in Figure 6) was implemented. Each image was independently annotated by at least two dentists, who were asked to select all clinically relevant labels present in the image. Dentists were also allowed to mark an image as uncertain when image quality or lighting conditions made visual assessment unreliable, thereby reducing the risk of forced or ambiguous judgments. After annotation, labels with low agreement rate (below 85%) were removed, as they indicate visual patterns that cannot be consistently identified from the images alone. Furthermore, any labels with disagreement between annotators were filtered out, and only those with full consensus were retained to guarantee high diagnostic reliability. After the expert annotation process, only high-quality and clinically reliable labels were retained to form the final annotated dataset. This curated dataset serves as the foundation for constructing our expert-annotated benchmarks and enables rigorous evaluation of the models multimodal diagnostic capability. Benchmark Composition Based on the annotations workflow described above, we construct three benchmarks to comprehensively evaluate the multimodal diagnosis performance of DentalGPT. These 9 subsets cover both clinical and in-the-wild imaging conditions and span key dental diseases and clinically relevant signs that dentists frequently observe in real practice. Each benchmark targets different imaging modalities or usage scenarios, enabling thorough assessment of the models reliability, generalization ability, and robustness across diverse dental settings. Figure 7: Examples of Expert-annotated Benchmarks (1) Intraoral-Classification-I: This benchmark is sourced from the existing dental image classification dataset AlphaDent [30], which contains intraoral photographs collected by multiple clinical dentists under standardized lighting and imaging conditions. The dataset provides high-quality clinical views and includes expert annotations for ten conditions: tooth discoloration, abnormal gingival coloration, gingival recession, dental caries, tooth pigmentation, tooth defect or loss, tooth loss, dental calculus, abnormal tooth morphology, and abnormal gingival morphology. (2) Intraoral-Classification-II: This benchmark is composed of intraoral images collected from internet searches. Most images were taken directly by patients, resulting in diverse lighting conditions and shooting angles. After removing duplicates with the training set, professional dentists annotated seven conditions: tooth pigmentation, abnormal gingival coloration, dental calculus, tooth loss, dental caries, abnormal gingival morphology, and gingival recession. This dataset reflects the models generalization ability to non-clinical, in-the-wild photographs. (3) Panorama-Classification: This benchmark consists of real clinical panoramic radiographs (X-ray images) collected from hospitals. Unlike intraoral photos captured by conventional cameras, panoramic images reveal structural and pathological features invisible in standard photographs. Professional dentists annotated six categories: periodontal disease, root canal treatment, tooth defect or loss, jawbone lesion, periapical lesion, and impacted tooth. Each benchmark supports multi-label classification across both clinical and in-the-wild scenarios, enabling comprehensive evaluation of multimodal diagnostic ability. To avoid biased evaluation, we further applied strict data balancing strategies: the label distributions and the ratios of positive and negative samples for each category were aligned across subsets. This ensures that accuracy reliably reflect model performance rather than being influenced by label frequency or overrepresented diseases."
        },
        {
            "title": "6 Experiment",
            "content": "6.1 Experimental Setup Training Settings DentalGPT is developed on top of the Qwen2.5-VL-7B-Instruct. During both stages of our training pipeline, all parameters of the model are fully updated to ensure comprehensive adaptation to domain knowledge of dentistry and complex reasoning. The complete hyperparameter settings and implementation details for each stage are provided in the respective subsections of Section 3. All training experiments are performed using cluster equipped with 8 NVIDIA H200 GPUs. 10 Model Open-source MLLMs Deepseek-VL2 [31] Mistral-Large-2512 [32] Phi-4-Multimodal-Instruct [33] Ernie-4.5-VL-424B-A47B [34] Qwen3-VL-235B-A22B-Instruct [16] Gemma-3-27B-it [35] GLM-4.5v [17] Qwen3-VL-235B-A22B-Thinking [16] LLaMA-4-Maverick [36] Proprietary MLLMs Claude-Sonnet-4.5 [37] Claude-Sonnet-4.5-Thinking [37] Grok-4.1-Fast Gemini-2.5-Pro-Thinking [38] GPT-4.1 [39] GPT-5 [40] DentalGPT and Its Backbone Qwen2.5-VL-7B-Instruct [6] DentalGPT MMOral OPG-Bench DentalBench Mixed Intraoral Classification-I Intraoral Classification-II Panorama Classification Avg. 39.1 41.9 38.5 45.0 40.3 42.2 45.7 40.6 51.4 47.0 50.3 47.1 45.7 47.2 47. 27.0 60.0 22.6 48.2 44.4 51.4 51.6 43.0 51.4 51.6 53.9 50.4 53.9 52.2 57.4 51.7 54.3 46.1 54.4 51.1 50.7 52.2 58.1 50.7 51.5 54.8 56.7 61.1 51.9 55.2 57.0 57.0 60.4 59. 48.8 64.1 59.4 58.0 63.3 65.1 58.0 61.4 64.7 65.7 67.1 59.4 66.7 65.2 65.2 70.5 71.0 61.8 72.9 55.1 44.2 61.5 44.9 55.8 59.6 54.5 60.3 59.0 50.0 55.8 62.2 64.1 61.5 63. 50.0 84.0 45.5 48.6 52.0 52.9 51.3 51.5 54.2 55.0 58.5 51.7 56.4 56.7 57.9 58.3 59.2 46.7 67.1 Table 1: Accuracy (%) of MLLMs on Dental-related VQA Benchmarks. Bold indicates the best score; underlines marks the second-best. indicates that the model has activated complex reasoning. Evaluation Settings We conduct evaluations on the curated Dentistry-Specific Benchmark (Section 5), which assesses models across dental disease classification, lesion recognition, and common dental consultation scenarios. All models are required to provide responses to the given tasks. For models marked with an asterisk (), complex reasoning mode is enabled, while other models are instructed to directly output the correct option without additional reasoning steps. 6.2 Results and Analysis As shown in Table 1, DentalGPT delivers clear and consistent performance gains over both comparable and substantially larger MLLMs across all expert-annotated datasets. It exhibits substantial improvements over its backbone, Qwen2.5-VL-7B-Instruct, underscoring the effectiveness of the proposed data construction pipeline and domain-aligned training strategy. Despite its compact 7B parameter scale, DentalGPT also surpasses many general-purpose models with over 100B parameters across nearly all benchmarks, demonstrating that domain-specialized modeling can achieve expertlevel capability at fraction of the computational cost and offering promising path toward efficient, field-specific MLLMs. Beyond expert-annotated datasets, DentalGPT maintains strong performance on the high-quality MMOral-OPG-Bench and the DentalBench-Mixed subset derived from general medical VQA benchmarks, outperforming most competing models and showing robust generalization across diverse dental tasks. Together, these results establish DentalGPT as leading multimodal foundation model for dental image understanding. 7 In-depth Analysis After evaluating the overall diagnostic performance, we further conducted fine-grained analyses to examine how each training stage enhances different capabilities of DentalGPT, providing clearer understanding of the respective roles of alignment and reinforcement learning in shaping its final diagnostic behavior. 7.1 Effect of Multimodal Understanding Enhancement (on Stage I) Enhancement of multimodal understanding enables the MLLM to map its understanding of dental images into the textual semantic space, allowing it to effectively leverage existing knowledge for accurate interpretation of dentistry-specific multimodal tasks. To further assess how the strength of 11 domain-specific alignment influences both visual comprehension and complex multimodal reasoning, we conducted an ablation study by varying the amount of Stage-I alignment data used during training. Experimental Setup Specifically, we conducted three controlled experiments by incorporating 0%, 30%, and 100% of the Stage-I dataset to assess its impact. The effectiveness of different alignment levels was evaluated by analyzing reward improvements during the subsequent RL stage for complex reasoning. To ensure fairness, duplicate images between the RL data and alignment data were strictly removed, and all complex reasoning samples were excluded from the Stage-I dataset. Consistent with the DentalGPT training setup, Qwen2.5-VL-7B was used as the backbone. Each model underwent 30 RL training steps, and accuracy-based reward changes on the validation set were monitored to assess multimodal reasoning performance. Figure 8: Accuracy reward (%) of MLLM during RL training under different scales of the Stage dataset. Results Analysis As shown in Figure 8, the control group trained with 0% Stage-I dataset, which receives no dental-domain knowledge, exhibits only marginal reward gains during the subsequent RL stage. In contrast, increasing the proportion of domain-specific training data consistently raises the performance ceiling throughout RL training. These results demonstrate that the Stage-I Training provides essential knowledge and visual grounding, thereby improving downstream reasoning capability in dentistry and enabling deeper image interpretation. In summary, incorporating the enhancement of multimodal understanding in dentistry effectively elevates the upper bound of performance attainable through multimodal complex reasoning. 7.2 Effect of RL (on Stage II) This section investigates how the reinforcement learning (RL) stage further shapes the models capabilities. After applying RL training on 10k multiple-choice dental questions, the performance of the model was reevaluated across the same set of benchmarks. As shown in Table 2, reinforcement learning brings consistent improvements across all tasks, demonstrating that it further enhances the models ability to execute downstream dental tasks. These gains confirm that RL effectively strengthens both the accuracy and reliability of the models reasoning in dental image understanding. 7.3 Case Study We further analyze the outputs produced by DentalGPT at different training stages, as well as the original backbone model. As illustrated in Figure 9, the backbone model struggles with this query: although it describes relevant visual characteristics in the image, it fails to correctly identify any teeth with fillings. After the enhancement of multimodal understanding, the model without complex reasoning is able to perform basic image analysis and detect most teeth, missing only one less noticeable case. This demonstrates the importance of alignment, which enables the model to learn how to interpret dental images and forms the foundation for further improving complex reasoning. Benchmarks MMOral-OPG-Bench DentalBench-Mixed Intraoral-Classification-I Intraoral-Classification-II Panorama-Classification Total Qwen2.5-VL Backbone Qwen2.5-VL + Stage w/o Stage II DentalGPT w/ Stage & Stage II 27.0 46.1 48.8 61.8 50.0 46.7 56.8 51.7 61.5 67.6 78.4 63.2 60.0 54.4 64.1 72.9 84.0 67. Table 2: Accuracy (%) comparison between Qwen2.5-VL, Qwen2.5-VL with Stage training, and DentalGPT (with both Stage and Stage II training) across dentistry-specific benchmarks. Finally, after all the training stages, DentalGPT with complex reasoning enabled conducts deeper multimodal analysis. Although intermediate thoughts may include incorrect counts, the iterative checking and reflection eventually lead to the correct answer. This again highlights the necessity of reinforcement learning for enhancing accurate multimodal reasoning in dentistry. Figure 9: Examples of DentalGPT and its backbones analyzing multimodal task in dentistry. Red indicates incorrect analysis, green indicates correct analysis, and orange highlights reflective turns in the complex reasoning process."
        },
        {
            "title": "8 Related Work",
            "content": "Medical fields involve large amount of imaging data, making them one of the most practical application areas for MLLMs. General medical MLLMs [13, 41, 42, 43, 44] have demonstrated the ability to perform basic medical question answering and conduct preliminary visual analysis of medical images. Their aim is to generalize across diverse clinical scenarios and contribute to domain adaptation through large-scale multimodal medical data [27, 45, 46, 47]. To better support complex imaging scenarios, subsequent research has focused on adapting MLLMs to specific medical modalities. Some models incorporate richer modality support beyond 2D images, extending to 3D medical scans [11, 48, 49] and biomedical signal analysis [50, 51, 52, 53]. Additionally, for tasks requiring high-resolution understanding such as reading detailed case images or zoom-level reasoning, several models [54, 55] introduce dedicated processing pipelines or tailored training strategies to address fine-grained clinical perception. In dentistry, specialty-centered medical domain, there has also been notable progress in multimodal diagnosis. DentVLM [56] aggregates large-scale hospital report data to build powerful imagetext understanding models tailored for dental applications. OralGPT [21, 57] further advances this direction by evaluating multimodal benchmarks in dentistry and supporting various dental imaging modalities. These works demonstrate the growing recognition of dentistry as valuable and distinct multimodal AI research scenario."
        },
        {
            "title": "9 Conclusion",
            "content": "This work introduces DentalGPT, specialized MLLM designed to address the challenges of multimodal diagnosis in dentistry. By constructing the largest annotated dental image dataset to date and integrating high-quality domain knowledge through staged enhancement of multimodal understanding and reinforcement learning pipeline, the model gains the ability to capture fine-grained visual cues and perform more reliable disease-related reasoning. Extensive evaluations on intraoral, panoramic, and dental VQA benchmarks demonstrate that DentalGPT achieves strong performance despite its compact 7B parameter size, surpassing many state-of-the-art general-purpose MLLMs. These findings highlight the critical role of domain-specific data and training strategies in advancing dental AI, and underscore the potential of DentalGPT as foundational model for future research and applications in automated dental imaging and intelligent oral healthcare."
        },
        {
            "title": "Acknowledgment",
            "content": "This work was supported by Shenzhen Medical Research Fund (B2503005), Major Frontier Exploration Program (Grant No. C10120250085) from the Shenzhen Medical Academy of Research and Translation (SMART), the Shenzhen Science and Technology Program (JCYJ20220818103001002), NSFC grant 72495131, Shenzhen Doctoral Startup Funding (RCBS20221008093330065), Tianyuan Fund for Mathematics of National Natural Science Foundation of China (NSFC) (12326608), Shenzhen Science and Technology Program (Shenzhen Key Laboratory Grant No. ZDSYS20230626091302006), the 1+1+1 CUHK-CUHK(SZ)-GDSTC Joint Collaboration Fund, Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence (2023B1212010001), and Shenzhen Stability Science Program 2023."
        },
        {
            "title": "References",
            "content": "[1] Centers for Disease Control and Prevention. 2024 oral health surveillance report: Selected findings, 2024. Accessed: 2025-04-28. [2] Xiaochen Jiang, Zhiguo Ding, Yanlei Su, Fei Wang, Weifeng Wang, Ziyang Wang, Xueling Qiu, Chenxi Sun, Fan Sun, Lu Tang, et al. Dentists views on the role orientation of dental hygienists in china: qualitative content analysis. BMC Oral Health, 24(1):1563, 2024. [3] Peixin Zheng, Xiaoting Qiu, Lingxiao Zhang, Peizhang Liu, Zeyi Peng, and Zhijian Huang. Comparative analysis of oral disorder burden in china and globally from 1990 to 2021 based on gbd data. Scientific Reports, 15(1):10061, 2025. 14 [4] Richard Lilford, Benjamin Daniels, Barbara McPake, Zulfiqar Bhutta, Robert Mash, Frances Griffiths, Akinyinka Omigbodun, Elzo Pereira Pinto, Radhika Jain, Gershim Asiki, et al. Supplyside and demand-side factors affecting allopathic primary care service delivery in low-income and middle-income country cities. The Lancet Global Health, 13(5):e942e953, 2025. [5] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [6] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [7] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [9] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. [10] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. arXiv preprint arXiv:2306.00890, 2023. [11] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology. arXiv preprint arXiv:2308.02463, 2023. [12] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang. Pmcllama: toward building open-source language models for medicine. Journal of the American Medical Informatics Association, page ocae045, 2024. [13] Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, and Benyou Wang. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale, 2024. [14] Yanzhou Su, Tianbin Li, Jiyao Liu, Chenglong Ma, Junzhi Ning, Cheng Tang, Sibo Ju, Jin Ye, Pengcheng Chen, Ming Hu, et al. Gmai-vl-r1: Harnessing reinforcement learning for multimodal medical reasoning. arXiv preprint arXiv:2504.01886, 2025. [15] Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, et al. Lingshu: generalist foundation model for unified multimodal medical understanding and reasoning. arXiv preprint arXiv:2506.07044, 2025. [16] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, et al. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. [17] Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, 15 Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, and Jie Tang. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. [18] Sheng Zhang, Qianchu Liu, Guanghui Qin, Tristan Naumann, and Hoifung Poon. Med-rlvr: Emerging medical reasoning from 3b base model via reinforcement learning. arXiv preprint arXiv:2502.19655, 2025. [19] Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, and Daniel Rueckert. Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 337347. Springer, 2025. [20] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [21] Jing Hao, Yuxuan Fan, Yanpeng Sun, Kaixin Guo, Lizhuo Lin, Jinrong Yang, Qi Yong Ai, Lun Wong, Hao Tang, and Kuo Feng Hung. Towards better dental ai: multimodal benchmark and instruction dataset for panoramic x-ray analysis. arXiv preprint arXiv:2509.09254, 2025. [22] Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, Huajie Tan, Chunyuan Li, Jing Yang, Jie Yu, Xiyao Wang, Bin Qin, Yumeng Wang, Zizhen Yan, Ziyong Feng, Ziwei Liu, Bo Li, and Jiankang Deng. Llava-onevision-1.5: Fully open framework for democratized multimodal training. In arXiv, 2025. [23] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024. [24] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [25] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [26] Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. Huatuogpt-o1, towards medical complex reasoning with llms. arXiv preprint arXiv:2412.18925, 2024. [27] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023. [28] Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, and Ping Luo. Omnimedvqa: new large-scale comprehensive evaluation benchmark for medical lvlm. arXiv preprint arXiv:2402.09181, 2024. [29] Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, and Bowen Zhou. Medxpertqa: Benchmarking expert-level medical reasoning and understanding. arXiv preprint arXiv:2501.18362, 2025. [30] Evgeniy I. Sosnin, Yuriy L. Vasilev, Roman A. Solovyev, Aleksandr L. Stempkovskiy, Dmitry V. Telpukhov, Artem A. Vasilev, Aleksandr A. Amerikanov, and Aleksandr Y. Romanov. Alphadent: dataset for automated tooth pathology detection, 2025. 16 [31] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts visionlanguage models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. [32] Mistral AI. Mistral large instruct 2407. https://huggingface.co/mistralai/ Mistral-Large-Instruct-2407, 2024. Accessed: 2025-12-06. [33] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. [34] Baidu-ERNIE-Team. Ernie 4.5 technical report. https://ernie.baidu.com/blog/ publication/ERNIE_Technical_Report.pdf, 2025. [35] Gemma Team. Gemma 3. arXiv preprint arXiv:2503.19786, 2025. [36] Meta AI. Llama 4 maverick. https://www.llama.com/models/llama-4/, 2025. Accessed: 2025-12-06. [37] Anthropic. https://assets.anthropic.com/m/ 64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf, 2025. Accessed: 2025-11-30. Claude opus 4-5 system card. [38] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [39] OpenAI. Gpt-4.1. https://openai.com/index/gpt-4-1/, 2024. Accessed: 2024-11-30. [40] OpenAI. Gpt-5 system card, 2025. Available at https://cdn.openai.com/ gpt-5-system-card.pdf. [41] Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, and Beng Chin Ooi. Healthgpt: medical large vision-language model for unifying comprehension and generation via heterogeneous knowledge adaptation, 2025. [42] LASA Team, Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, Yu Sun, Junao Shen, Chaojun Wang, Jie Tan, Deli Zhao, Tingyang Xu, Hao Zhang, and Yu Rong. Lingshu: generalist foundation model for unified multimodal medical understanding and reasoning, 2025. [43] Chieh-Ju Chao, Imon Banerjee, Reza Arsanjani, Chadi Ayoub, Andrew Tseng, Garvan C. Kane, Jae Oh, Li Fei-Fei, Ehsan Adeli, and Curtis Langlotz. Echogpt: large language model for echocardiography report summarization. medRxiv, 2024. [44] Yuechun Yu, Han Ying, Haoan Jin, Wenjian Jiang, Dong Xian, Binghao Wang, Zhou Yang, and Mengyue Wu. Medkgeval: knowledge graph-based multi-turn evaluation framework for open-ended patient interactions with clinical llms, 2025. [45] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: semanticallylabeled knowledge-enhanced dataset for medical visual question answering. In 2021 IEEE 18th international symposium on biomedical imaging (ISBI), pages 16501654. IEEE, 2021. [46] Yunfei Xie, Ce Zhou, Lang Gao, Juncheng Wu, Xianhang Li, Hong-Yu Zhou, Sheng Liu, Lei Xing, James Zou, Cihang Xie, et al. Medtrinity-25m: large-scale multimodal dataset with multigranular annotations for medicine. arXiv preprint arXiv:2408.02900, 2024. [47] Zhenyang Cai, Junying Chen, Rongsheng Wang, Weihong Wang, Yonglin Deng, Dingjie Song, Yize Chen, Zixu Zhang, and Benyou Wang. Exploring compositional generalization of multimodal llms for medical imaging. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1305713079, 2025. 17 [48] Jing Wu, Yuli Wang, Zhusi Zhong, Weihua Liao, Natalia Trayanova, Zhicheng Jiao, and Harrison Bai. Vision-language foundation model for 3d medical imaging. npj Artificial Intelligence, 1(1):17, 2025. [49] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Hui Hui, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data. Nature Communications, 16(1):7866, 2025. [50] Yubao Zhao, Tian Zhang, Xu Wang, Puyu Han, Tong Chen, Linlin Huang, Youzhu Jin, and Jiaju Kang. Ecg-chat: large ecg-language model for cardiac disease diagnosis. arXiv preprint arXiv:2408.08849, 2024. [51] Ruoqi Liu, Yuelin Bai, Xiang Yue, and Ping Zhang. Teach multimodal llms to comprehend electrocardiographic images. arXiv preprint arXiv:2410.19008, 2024. [52] Junying Chen, Zhenyang Cai, Zhiheng Liu, Yunjin Yang, Rongsheng Wang, Qingying Xiao, Xiangyi Feng, Zhan Su, Jing Guo, Xiang Wan, et al. Shizhengpt: Towards multimodal llms for traditional chinese medicine. arXiv preprint arXiv:2508.14706, 2025. [53] Ziyi Zeng, Zhenyang Cai, Yixi Cai, Xidong Wang, Junying Chen, Rongsheng Wang, Yipeng Liu, Siqi Cai, Benyou Wang, Zhiguo Zhang, et al. Wavemind: Towards conversational eeg foundation model aligned to textual and visual modalities. arXiv preprint arXiv:2510.00032, 2025. [54] Jingyun Chen, Linghan Cai, Zhikang Wang, Yi Huang, Songhan Jiang, Shenjin Huang, Hongpeng Wang, and Yongbing Zhang. Pathagent: Toward interpretable analysis of wholeslide pathology images via large language model-based agentic reasoning. arXiv preprint arXiv:2511.17052, 2025. [55] Sheng Wang, Ruiming Wu, Charles Herndon, Yihang Liu, Shunsuke Koga, Jeanne Shen, and Zhi Huang. Pathology-cot: Learning visual chain-of-thought agent from expert whole slide image diagnosis behavior. arXiv preprint arXiv:2510.04587, 2025. [56] Zijie Meng, Jin Hao, Xiwei Dai, Yang Feng, Jiaxiang Liu, Bin Feng, Huikai Wu, Xiaotang Gai, Hengchuan Zhu, Tianxiang Hu, et al. Dentvlm: multimodal vision-language model for comprehensive dental diagnosis and enhanced clinical practice. arXiv preprint arXiv:2509.23344, 2025. [57] Jing Hao, Yuci Liang, Lizhuo Lin, Yuxuan Fan, Wenkai Zhou, Kaixin Guo, Zanting Ye, Yanpeng Sun, Xinyu Zhang, Yanqi Yang, et al. Oralgpt-omni: versatile dental multimodal large language model. arXiv preprint arXiv:2511.22055, 2025."
        }
    ],
    "affiliations": [
        "Beijing Institute of Collaborative Innovation",
        "Division of Applied Oral Sciences & Community Dental Care Faculty of Dentistry, The University of Hong Kong",
        "Freedom AI",
        "National Health Data Institute, Shenzhen",
        "Shenzhen Institute of Big Data",
        "Shenzhen Loop Area Institute",
        "Shenzhen Stomatology Hospital (Pingshan) of Southern Medical University",
        "State Key Laboratory of Membrane Biology, Beijing Key Laboratory of Cardiometabolic Molecular Medicine, Institute of Molecular Medicine, National Biomedical Imaging Center, School of Future Technology, Peking University",
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}