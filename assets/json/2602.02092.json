{
    "paper_title": "FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space",
    "authors": [
        "FSVideo Team",
        "Qingyu Chen",
        "Zhiyuan Fang",
        "Haibin Huang",
        "Xinwei Huang",
        "Tong Jin",
        "Minxuan Lin",
        "Bo Liu",
        "Celong Liu",
        "Chongyang Ma",
        "Xing Mei",
        "Xiaohui Shen",
        "Yaojie Shen",
        "Fuwen Tan",
        "Angtian Wang",
        "Xiao Yang",
        "Yiding Yang",
        "Jiamin Yuan",
        "Lingxi Zhang",
        "Yuxin Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space ($64\\times64\\times4$ spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report."
        },
        {
            "title": "Start",
            "content": "FSVideo: Fast Speed Video Diffusion Model in Highly-Compressed Latent Space"
        },
        {
            "title": "FSVideo Team",
            "content": "Intelligent Creation, ByteDance"
        },
        {
            "title": "Abstract",
            "content": "We introduce FSVideo, fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) new video autoencoder with highly-compressed latent space (64 64 4 spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) diffusion transformer (DIT) architecture with new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) multi-resolution generation strategy via few-step DIT upsampler to increase video fidelity. Our final model, which contains 14B DIT base model and 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report. Date: February 3, 2026 Correspondence: Xiao Yang at xiaoyang_seravee@outlook.com Project Page: https://kingofprank.github.io/fsvideo/ 6 2 0 2 2 ] . [ 1 2 9 0 2 0 . 2 0 6 2 : r Figure 1 Videos generated via FSVideos Image-to-Video framework while being 42.3 faster than Wan2.1-I2V-14B720P. For every row, first frame is the input image from VBench [29] testset, and the following frames are generated. Zoom in to see details."
        },
        {
            "title": "Introduction",
            "content": "Video generation models have become major focus in both academia and industry, due to their strong generation ability and wide range of application potential. Although recent models such as Sora 2 [53], Veo 1 Figure 2 Overall framework of FSVideo image-to-video pipeline. The input image is sent to the encoder to get its VAE latent code, which is then used as the condition of the base module DIT for the first diffusion process. Then, the diffused latent is sent to the upsample module, where it first passes latent convolution neural net (CNN) upscaler, and is then combined with the input images latent as the condition to the upsampler DIT for another diffusion process. Finally, the upsampled latent is sent to the decoder to generate output video. The decoder contains attention layers conditioned on input images encoder feature map to enhance video quality (see Section 2.2.3). 3 [18], Kling [33], Wan [75] and SeedDance [22] have shown impressive generation capability, the high inference cost of these large video models often results in long waiting times and high GPU cost, making scaling to broader audience challenge. Methods that increase video model generation speed, especially for models based on diffusion and flow-matching formulation, have been very popular research topic [63]. Many works have focused on training-free acceleration methods, such as efficient solvers [48, 88], cache-based methods [41, 49], low-resolution sampling [72] and sparse attention operation [13, 79]. However, the speed-up achieved via these training-free methods is often limited and may result in generation quality degradation. In different direction, training-based acceleration methods have also been extensively explored. Some methods are proposed to reduce the model size [78], or replace attention operation with lightweight alternatives [6, 9, 28], but these methods usually suffer from loss of generation quality due to reduced model capacity. Step-distillationbased methods can get greater speed-up while keeping the quality of the generation. The majority of these methods focus on the distillation of inference steps [39, 51, 83, 84, 87], reducing the number of inference steps to single digit number or even 1, offering more than an order of magnitude of speed-ups. However, in industry usage, it is more common to use inference step that is larger than 1 (e.g., 4 to 8 step), since the generation quality drastically degrades when reaching very low inference steps due to the high estimation error of the latent ODE/SDE path. Thus, when the undistilled model uses reasonable amount of inference steps 1 and an efficient solver, the speed-up achieved from step distillation without sacrificing generation quality can be limited. This, combined with the high amount of computation per model forward, makes video diffusion model computation intensive. In this paper, we propose FSVideo, video diffusion transformer model designed for fast image-to-video generation. The key idea of FSVideo is to reduce the amount of compute per model forward. To achieve this, we propose video autoencoder (AE) with spatial-temporal downsampling of 64 64 4, thus reducing the overall video generation time. Some previous work [27, 55] uses video autoencoder with up to 32 spatial compression ratio, but they may suffer from poor reconstruction and generation quality. There is concurrent work [11] that proposes deep compression video autoencoder with up to 64 64 spatial compression ratio. However, generative quality evaluation is done mainly using the autoencoder with 32 32 spatial compression ratio. In addition, [11] mainly focuses on fine-tuning pretrained video diffusion transformer (DIT) to the new VAE latent space, while in this work we explore training DIT from scratch in highly compressed latent space. Of course, our proposed new techniques can also be applied by fine-tuning an existing video DIT. Our main contributions in FSVideo are: Video autoencoder with highly compressed latent space: we introduce FSAE, new asymmetric video autoencoder with 64 64 4 spatial-temporal compression with 128 latent channels, resulting in 1For example, Wan 2.1 by default uses UniPC solver [88] with 50 inference steps in its open source repository https: //github.com/Wan-Video/Wan2.1 total of 384 information reduction. This autoencoder achieves competitive reconstruction performance while having strong generation capability. Improved diffusion transformer with layer memory: we propose new diffusion transformer (DIT) architecture with layer memory mechanic to provide more freedom to the DIT information flow during the diffusion process. This results in better utilization of the DIT model capacity and better generation performance, with negligible overhead. Latent video upsampler for increased video fidelity: we propose upsampler module, consists of Convolutional latent upsampler and DIT refiner, to upscale base DITs latent output, drastically increasing video fidelity while ensuring minimum inference time increase via lightweight refiner DIT step distillation. Benefiting from our design, FSVideo is able to generate videos with competitive quality while being an order of magnitude faster compared to other open-source models of similar amount of parameters (e.g. 42.3 faster than Wan2.1-14B). The reminder of the report are organized as follows. Section 2.1 discusses the overall FSVideo framework, then introduces the new Video AE (Section 2.2), base DIT (Section 2.3) and upsampler (Section 2.4) in detail. Section 3 presents implementation details of the training framework, as well as the experimental results of both VAE reconstruction and video generation. Section 4 discusses the potential extension and applications of our method."
        },
        {
            "title": "2.1 Framework Overview",
            "content": "The general framework of FSVideo is shown in Figure 2. Note that video generation model can have various input types, such as text-only (text-to-video), text with first frame (image-to-video), and text with first and last frame (first-last-iamge-to-video), etc. We formulate FSVideo as an image-to-video model for the following reasons: 1. Ease of training: We aim to train diffusion transformers in heavily compressed latent space with high number of latent channels (128 in our case). Diffusion training in such latent space has been shown to be hard task [10, 12]. By constraining the training task to be only on image-to-video task, which receives ample information of the video appearance via the input image being the videos first frame, we let the diffusion training focus more on modeling video movement, reducing the training difficulty and advocating better model performance. Moreover, there are tricks specifically suitable for image-to-video that further boost generation quality, such as in Section 2.2.3 and Section 2.4.2. 2. Resource constraints: The development of FSVideo is challenged by limited resources in GPU compute, training data, and labeling workforce. Thus, it is hard to gather sufficient numbers of high-quality video data with good aesthetics for text-to-video. Image-to-video is more data-friendly, and video aesthetics can be guaranteed by using high-quality text-to-image model to generate the first frame. 3. Application consideration: Many applications, such as photo reenactment and visual effects, often rely on users providing an image in the real world as the first frame of the video, which fits naturally in the image-to-video setting. For cases where input image is not required, we can always fall back to the text-to-image-to-video pipeline stated above."
        },
        {
            "title": "2.2 Video Autoencoder",
            "content": "Here we introduce our new FSAE model. We first present the model architecture and training design, then discuss methods to balance generation and reconstruction quality for the autoencoders latent space, and finally talk about various ways to improve the decoder quality, where we introduce two variants of FSAE: FSAE-Standard, which leans on reconstruction quality, and its lightweight variant FSAE-Lite, which mildly sacrifices quality for great speed and memory-consumption improvement. 3 Figure 3 Overall framework of FSAE."
        },
        {
            "title": "2.2.1 Overall Design And Training Strategy",
            "content": "First we define the notation of the total compression rate of an autoencoder. Given video R3T HW , the encoder encodes it into latent Rcthw, and decodes the latent via the decoder to recover the original video. The spatial compression fh and fw is calculated by h/H and w/W , and the temporal compression is ft = (t rt)/(T rt) where rt is set to 1 if using causal convolution else 0. The total compression ratio is formulated as follows: Total_Compression = fh fw ft 3 (1) Model structure. Figure 3 shows our FSAE framework. Note that, different from LTX-Video, we do not apply patchify to the input video as preprocessing step, and all downsample/upsample operations are done inside the autoencoder. This shows [71] better reconstruction quality and less artifacts. Our video autoencoder follows DC-AE [10], which is deep-compressed autoencoder for image generation. We start from the dc-ae-f32c32-sana-1.0 version of DC-AE, where both the encoder and the decoder consist of 3 convolution blocks and 3 transformer blocks, achieving 32 32 spatial compression ratio. We first add an additional set of transformer blocks to both the encoder and the decoder, bringing the spatial compression ratio to 64 64, and increase the latent channel to 128. Then we expand the 2D convolution kernels of the autoencoder to causal 3D convolutions, which enables long video generation and joint training between video and image [34, 75]. We then change the downsample/upsample operations [64] to include temporal dimension, enabling time-spatial-to-channel and channel-to-time-spatial transition. Finally, we change the downsample/upsample operation of the outer two blocks of encoder/decoder to include temporal dimension reduction, making the autoencoder performing 4 temporal compression, achieving our compression target of 64 64 4. We also make some minor adjustments to the model architecture, such as making the attention layers in the autoencoder operate only on height and width dimension, as well as changing all norms to pixel norm. These changes ensure that the latent computation is not sensitive to time and spatial dimension changes and allow us to apply various memory reduction inference techniques, such as temporal splitting and 3D tiling. Training Strategy. We apply multi-stage training strategy to the autoencoder. During the first stage, we train on 256 256 resolution2, using both images and 17-frame videos. We use both L1 and LPIPS loss [30] in this stage. In the second stage, we add GAN loss to improve video quality. We use 3D multiscale discriminator and use non-saturating logistic loss [26] with R1 regularization [52]. In this stage, we also increase the image/video resolution to 512 512, and increase the video duration to 61 frames. In the final stage, we further scale the spatial and temporal resolutions up to 121-frame 1024 1024 videos. Each stage is trained for approximately 200, 000 iterations. The final training loss Lae is written as Lae = L1 +0.1Llpips +0.1LGAN . 2Here 256 256 indicates the average resolution, with varying aspect ratios. Training data is grouped via an aspect ratio bucketing strategy, with image/videos of similar aspect ratios cropped and resized to the same resolution bucket. The same applies in later sections. 4 Table 1 FSAEs Intrinsic dimension comparisons with different regularization methods. Lower number = lower latent space complexity. Method No regularization Downscale regularization Upscale regularization Video VF Intrinsic Dimension using Gride [19] with 2/4/8/16/32/64 nearest neighbors Video [87.83, 182.16, 11.22, 16.94, 24.32, 29.55] [30.5, 37.48, 31.99, 25.86, 25.34, 24.05] [46.63, 57.16, 28.66, 28.11, 30.8, 29.63] [24.44, 29.60, 26.93, 21.11, 22.15, 22.04] Image [33.15, 62.24, 102.51, 12.07, 15.45, 21.13] [25.73, 38.20, 41.52, 28.75, 26.66, 26.59] [34.92, 44.30, 44.17, 26.39, 29.02, 27.97] [21.46, 24.52, 24.91, 18.07, 20.48, 20.13] Training autoencoder on high-resolution videos can be resource-challenging, which is especially true for our case, since we do not apply patchify to the input video, drastically increasing the feature map size. Thus, in stage 3 of our autoencoder training, we apply several tactics to reduce memory consumption. First, we apply mixed-resolution training strategy by training the autoencoder with both low-resolution, long-duration videos (e.g. 256 256 171) and high-resolution, short-duration ones (e.g. 704 704 9). This strategy leads to better generalization to high-resolution and long-duration reconstruction [60]. Second, when training on very high spatial resolutions (e.g. 1024 1024), we randomly extract small 3D patch from the full feature map from the third-to-last block of the decoder, and only forward this small patch to the remaining 3 decoder blocks. We then only use the output of this small patch for training loss computation and gradient back-propagation.3 This is similar to applying 3D binary window to the output video, and only calculate loss inside that window, but our approach can greatly reduce training memory while achieve the same effect. Finally, we apply temporal slicing for LPIPS calculation to reduce peak memory usage. Together, these techniques help drastically reduce the memory usage of training during the final stage. 2.2.2 Latent Space Sematic Alignment Since our final goal is to use the video autoencoder for DIT training, it is important to ensure that the autoencoder has good generation capability. Several methods [35, 65, 82] have been proposed to improve the semantic alignment of the latent space for the image autoencoder, but no discussions have been done in the video space yet. Here we propose Video Vision Foundation model alignment Loss (Video VF Loss), by extending VA-VAE [82]s paradigm to videos, as described below. Given video , we obtain the latent Rcthw through the encoder, and use Dinov2 [54] to extract frame-by-frame feature of the video to obtain Rcthw, and we want to align with to improve the generation ability of the video autoencoder. natural question is how to align two feature maps with different channel, spatial, and temporal dimensions. To solve this, we propose series of operations to match the feature dimensions. First, we map the channel dimension of to via learnable linear layer Rcc. Then, we interpolate on the spatial dimension and to match and w. Finally, since has smaller dimension due to temporal compression, we apply average pooling of kernel size 4 to in dimension4. After these steps,we obtain aligned feature pair (z, ) with the same shape h w. After the features are aligned, we then expand the feature matching loss of [82] to video domain as Video Marginal Cosine Similarity Loss Lvmcos Lvmcos = 1 w (cid:88) (cid:88) (cid:88) i=1 j=1 k=1 (cid:18) ReLU 1 m1 zijk fijk zijk fijk (cid:19) . and Video Marginal Distance Matrix Similarity Loss Lvmdms Lvmdms = 1 (t w) thw (cid:88) p,q ReLU (cid:18)(cid:12) (cid:12) (cid:12) (cid:12) zp zq zpzq fp fq fpfq (cid:12) (cid:12) (cid:12) (cid:12) (cid:19) m2 . (2) (3) 3Due to fixed padding of convolution layers in the spatial dimension, we will get different results between patch-based generation and full frame generation. Thus we only calculate loss using the patch area with correct boundary conditions. 4The first video frames feature in does not perform this pooling operation due to the video AEs causal nature. 5 The autoencoder training loss with the Video VF Loss is then written as Lae and Lvvf : Ltotal = Lae + Lvvf = Lae + α (Lvmcos + Lvmdms) . (4) with m1 = 0.5, m2 = 0.25, α = 0.5. We integrate this loss by finetuning the autoencoder with Ltotal. To check our methods impact on the complexity of the latent manifold, we calculate the intrinsic dimension of the latent space. Intrinsic dimension evaluates the minimum number of variables needed to represent distribution [7]. Hence, lower intrinsic dimension indicates lower latent space complexity. We use Gride [19] algorithm with up to 64 neighbors from the Dadapy [25] package for intrinsic dimension calculation. For comparison, we also implement other regularization techniques proposed in [35, 65] and calculate their intrinsic dimension. The result is shown in Table 1. It can be seen that our proposed Video VF loss achieves the overall lowest intrinsic dimension, while being drastically better than the autoencoder without any regularization. This shows that our Video VF loss can greatly reduce the latent space complexity, therefore improving the generation ability of the autoencoder. 2.2.3 Decoder Improvement Due to the high compression ratio of our video autoencoder, we still see two problems after applying the training discussed above: artifacts in the reconstructed videos and high computation requirement during the decoding stage. Therefore, we extend our autoencoder to an asymmetric structure, where we freeze the encoder, make architecture changes to the decoder, and only finetune the decoder. Below we discuss the steps we take for video quality enhancement and decoding performance optimization on the decoder during this stage. Video Quality Enhancement. To improve video quality and reduce artifacts, we implement the following architecture changes to the video decoder: 1. Change the convolution layers in the decoder to non-causal. We find that causal convolution introduces noticeable frame flickering, due to its unidirectional temporal dependency, especially in the decoder part. Switching these layers to non-causal convolutions solves this problem. 2. Inject first-frame features from encoder to decoder. Since we only focus on image-to-video generation, we can utilize the input image as an additional condition to the decoder to further increase the reconstruction quality. Past work like Reducio-VAE [71] explored similar ideas, but Reducio-VAE uses an extra 2D VAE to encode mid-frame feature, while we directly reuse the FSAE encoder to encode first-frame feature, which removes the need for an additional encoder. This is enabled due to the causal property of the encoder, which handles first frame independently from following frames. As shown in Figure 3, we extract first-frame features from the last 5 blocks of the encoder, and insert the feature into the corresponding decoder blocks via cross-attention. 3. Noise-injection [27] to each convolution block. This helps with high-frequency detail generation. We use standard Gaussian noise with fixed weight of 0.05. We apply these optimizations as an additional finetuning step to the FSAE model, and achieve our official video autoencoder, which we call FSAE-Standard. This version has the best reconstruction quality, but may suffer from long generation time and high memory consumption. Therefore, we design another version of FSAE with less compute at the cost of slightly degraded performance, which we discuss below. Performance Optimization. Starting from FSAE-Standard, we inspect the memory consumption, and find the main performance bottlenecks to be the last two blocks, which are closer to the RGB space and therefore having the largest feature map. Thus, we reduce the number of channels in these blocks, decreasing the maximum memory usage. We also reconsider the choice of convolution in the decoder. As stated in Video Quality Enhancement section, causal convolution may introduce temporal flickering, while noncausal convolution has high memory cost since we can no longer perform temporal slicing like we can with causal ones. Therefore, we take the middle ground 6 Table 2 Quantitative comparisons with other SOTA methods and our FS Video Autoencoder. Method Hunyuan VAE [34] Wan-2.1 VAE [75] CogVideoX-1.5 VAE [81] Step-Video VAE [50] VidTok [69] Cosmos-CV [1] LTX-Video [27] VidTok [69] Cosmos-CV [1] VidTok [69] FSAE-Standard FSAE-Lite Downsample Factor Total Compression 8 8 4 8 8 4 8 8 4 16 16 8 8 8 4 8 8 8 32 32 8 8 8 4 16 16 8 16 16 4 64 64 4 64 64 4 1:48 1:48 1:48 1:96 1:96 1:96 1:192 1:192 1:384 1: 1:384 1:384 Inter-4K SSIM PSNR LPIPS FVD 73.50 0.891 79.29 0.880 109.11 0.880 125.45 0.847 159.88 0.835 0.047 0.049 0.072 0.082 0.081 32.56 31.73 31.44 30.20 29.98 WebVid-10M SSIM PSNR LPIPS FVD 63.72 0.923 71.83 0.913 80.98 0.920 96.75 0.894 124.46 0.887 33.99 33.12 33.53 31.86 31. 0.031 0.032 0.043 0.050 0.048 0.805 0.787 0.783 0.724 0.645 0.806 0.788 29.16 28.40 28.20 26.72 24.19 28.96 28.48 0.184 0.153 0.119 0.271 0. 0.107 0.151 342.99 370.86 266.07 704.08 680.41 256.62 342.66 0.864 0.868 0.843 0.786 0.713 0.872 0.861 30.76 30.89 29.85 28.09 25. 30.91 30.42 0.113 0.077 0.077 0.180 0.138 0.058 0.075 211.33 232.02 217.77 518.86 625.32 203.19 240.03 and use group-causal convolution [77] in the decoder. Depending on different temporal compression ratio in each decoder block, we use group size of 1, 2 and 4 for the group-causal layers, respectively. Besides, different from [77], we use replication padding after each frame group, making finetuning much easier. We finetune FSAE-Standard with these changes to get the FSAE-Lite version of the autoencoder, which has 1.75-2 less memory consumption and less inference time. Figure 4 Video reconstruction comparison between LTX-Videos autoencoder and FSAE. The first three rows represent the reconstruction results of different AE, and the last row is the ground truth. The blue box tracks the clothing textures temporal consistency, where LTX-Video exhibits inconsistent inter-frame flickering, and FSAE consistently maintains the dotted texture. The red box compares the reconstruction quality to video details: FSAE-Lite and LTX-Video achieve comparable reconstruction results, whereas FSAE-Standard outperforms LTX-Video. 7 Qualitative and Quantitative Measurement We show the comparison between FSAE and other state-of-the-art video autoencoder methods in Table 2. We evaluate all methods using 256 256 17 videos using SSIM [8], PSNR, LPIPS [30] and FVD [74]. The evaluation set includes 1000 unseen videos from Inter-4K [68] and 1000 unseen validation set videos from WebVid-10M [5]. As shown in the table, FSAE achieves better reconstruction accuracy compared to other autoencoders with high compression rate, such as LTX-Video and VidTok [69], and can even outperform some AEs with low compression rate, such as Cosmos-CV [1]. We also visualize the video reconstruction result in Figure 4, showing FSAEs competitive visual reconstruction ability."
        },
        {
            "title": "2.3 Video Diffusion Transformer",
            "content": "Since we train our DIT in highly compressed latent space, we want to utilize mature DIT architecture to ensure stable training. At the same time, we also want to explore ways to better utilize DITs representation capability. Based on this mindset, we choose Wan2.1-14B-I2V [75]s DIT structure as the baseline and add modifications on it. Here we very briefly discuss Wan2.1-14B-I2Vs DIT structure and the changes we make to it. The DIT consists of patchify module via 3D convolution, series of transformer layers, and an unpatchify In the patchify module, unlike WAN, we use module. kernel size of (1, 1, 1) since we have already performed heavy compression in the autoencoder. This results in sequence of latent tokens with shape (B, T, D), where denotes the batch size, the latent embedding dimension and = (1 + F/4) H/64 W/64 the sequence length. The improved transformer layer with layer memory is shown in Figure 5, where the transformer layer contains both self-attention and cross-attention modules conditioned on both UmT5 [16]s text embedding and CLIP [57] embedding of the input image. To improve inter-layer information flow and strengthen DITs representation capacity, we introduce new Layer Memory Self-Attention mechanism (see Section 2.3.1) that allows each layer to adaptively attend to partial attention features from all preceding layers. This design facilitates hierarchical information reuse and enhances temporal coherence across depth. 2.3.1 Layer Memory Self-Attention Mechanism Figure 5 Transformer layer of FSVideo. In language modeling research, various methods [21, 24, 93] have been proposed to combat representation collapse, problem in transformers where features of adjacent layers are excessively similar, limiting the transformers representation capacity, especially for very deep networks [44]. However, similar attempts are rarely explored in image and video generation tasks, with U-DIT [73] being the closest work. In U-DIT, the DIT is formulated into U-net style structure, with encoding layers downsampling the tokens and decoding layers upsampling them, and skip connections are added symmetrically at each downsampling/upsampling transition. We want to explore method that is applicable to general DIT structure, thus we propose to integrate the Layer Memory mechanism into DITs. Unlike the standard design where each layer attends only to the preceding hidden states, Layer Memory enables every self-attention block to access learnable mixture of all prior layer representations, thereby forming differentiable memory across DIT depth. In conventional L-layer decoder-only DIT, the l-th self-attention layer (1 L) updates its representation as: Xl = SelfAttention(Xl1), where Xl1 RBTD. The multihead attention mechanism projects Xl1 into queries, keys, and values: Q, K, Rnd, with the sequence length and the hidden dimension, and performs attention operation as: Attention(Q, K, V) = softmax (cid:19) (cid:18) QK V. (5) Layer memory modifies key-value generation while retaining the standard query projection. For the l-th layer, queries are obtained from Xl1, but keys and values are derived from ˆXl1, learned fusion of all previous hidden representations X0, . . . , Xl1, where X0 denotes the input embeddings. This extension allows attention computation to exploit the deeper contextual structure accumulated through the network hierarchy. Inspired by LIMe [24], we introduce an inter-layer dynamic router to adaptively Inter-Layer Dynamic Router. weight prior representations, as shown in Figure 5. Each layer 2 maintains router consisting of learnable linear layer that outputs context-dependent weighting matrix Rl Rl: Rl(Xl1,t) = Routerl(Xl1,t), Routerl : Rnd Rnl. (6) Here Xl1,t is Xl1 modulated by the time embedding of DIT. By doing so, we get time-aware dynamic router, suitable for diffusion learning. The aggregated feature map is then computed as: ˆXl1 = softmax(Rl(Xl1,t)) X0:l1. This adaptive fusion encourages the model to selectively emphasize more informative layers, improving depth-wise consistency and feature reuse. (7) Memory-Aware Self-Attention. The self-attention computation at layer becomes: Ql = Xl1WQ, Kl = ˆXl1WK, Vl = ˆXl1WV, (8) where WQ, WK, WV Rdd are learnable projections. The query still originates from the previous layers output, while the keys and values are synthesized through layer-adaptive aggregation. Notably, our Layer Memory design preserves the original DIT architecture and introduces only minimal additional trainable parameters, facilitating straightforward model adaptation. The modification remains lightweight and fully compatible with efficient implementations such as FlashAttention [17]. Figure 6 Weight heatmap of the dynamic router. For each representation index in each layer, it shows the maximum value of dynamic router weights across all tokens in diffusion latent sequence. Layer index starts at 2 because there is no dynamic router for index 0 (input embeddings) and index 1 (only 1 previous layer). Router Visualization and Analysis. The router heatmap in Figure 6 illustrates how each layer aggregates prior representations for keyvalue construction. clear diagonal of high weights shows that each layer mainly attends to its immediate predecessor, reflecting the strong sequential dependency typical of standard DITs. In addition, we see common high weight value for the representation index 1 across all layers. This comes from the router weight for the first latent token, which is mapped to the first frame of the video. This is reasonable, as in image-to-video, the first frame is given as DIT input via channel concatenation [75], thus DIT does not need heavy processing of this token in deeper layers, and can borrow information directly from very early layers. Finally, scattered non-zero weights in the lower-left region indicate that deeper layers selectively reference much earlier representations. There is noticeable cluster of relatively high weights connecting later layers (e.g., layer 13-30) with much earlier representations, indicating that deeper layers are learning to selectively draw low-level, high-frequency features. The presence of these scattered nonzero weights confirms that the Inter-Layer Dynamic Router is successfully enabling differentiable memory across depth, allowing the model to selectively bypass intermediate layers to retrieve relevant context directly from any preceding layer when needed. 9 Figure 7 Training analysis of the layer memory mechanism. (a) Training Loss Curve (From Scratch): Comparison of the training loss between the baseline model (w/o Layer Memory) and the proposed model (with Layer Memory) on Wan2.1-14B-I2V-720P, demonstrating Layer Memorys ability to achieve consistently lower loss and faster initial convergence. (b) Resume Training Loss Curve (Fine-tuning): Loss curve showing the fine-tuning the original Wan2.1-14B-I2V-720P. The integration of Layer Memory facilitates rapid convergence and sustains stable performance advantage compared to the baseline. Zoom in to see details. Training Convergence Analysis. The integration of Layer Memory significantly enhances the optimization and convergence of the DIT architecture. As shown in Figure 7(a), when training models from scratch, the variant equipped with Layer Memory consistently achieves lower loss value throughout the entire training process compared to the baseline. This result indicates that the mechanism, by establishing differentiable memory across depth via the inter-layer router, not only yields slightly accelerated initial convergence rate but also effectively aids the model in finding superior minimum on the complex loss surface. Furthermore, the Layer Memory mechanism demonstrates its ability to seamlessly integrate into and improve existing DiT-structured models. Figure 7(b) illustrates the fine-tuning results when Layer Memory is added to pretrained WAN2.1[75] model. The mechanism facilitates rapid convergence within 100 steps and achieves stable performance gain of up to 4.7% compared to the baseline after just 1,000 fine-tuning steps. This empirical evidence strongly supports the hypothesis that the enhanced inter-layer information flow and adaptive feature reuse improve the models capacity to learn more efficient and robust set of representations. We can also integrate the Layer Memory mechanism into MMDIT [20] architecture for potentially larger performance gain, since MMDIT has one joint attention across different modalities, hinting at larger impact of layer memory to each transformer layer. This is left for future work. 2.3.2 Training Strategy Pre-training Stage. We train the DIT using the flow matching framework [20, 40] with logit-normal time sampling schedule. We use Pseudo-Huber loss as the training loss due to its robustness to outliers and lower gradient variance [37, 66], making the training more stable. The pretraining data are captioned by Qwen2.5-VL-7B [4], and using fixed video frame length of 121 frames with 24 FPS. The full pre-training stage is separated into 3 stages. In stage 1, we only use 256 256 images to learn text-visual alignment. In stage 2, we switch to 256 256 video data to learn video movement and low resolution visual appearance, and finally in stage 3 we use 512 512 video data to reach our target resolution. The training step ratio of these 3 stages is approximately 1 : 2 : 2. Post-training Stage. The DIT post-training was separated into the supervised fine-tuning (SFT) stage and the reinforcement learning (RL) stage. For the SFT stage, we finetune our DIT with 300k high-quality samples filtered on aesthetic scores and motion amplitude. This stage increases DITs generated video quality, but the DIT still suffers from motion integrity problems. Therefore, we apply RL to further improve the DIT, discussed below. 10 For the RL stage, we use the Reward Feedback Learning (ReFL) framework [80] for its high effectiveness to improve video generation quality [22]. Due to lack of human labeling resources, we opt to use opensource reward models (RM) for our RL training, instead of training RMs from scratch. Specifically, we use VideoAlign [43] as the video reward model and MPS [85], which scores individual video frames, as the frame reward model, and combine the loss of these two models as the reward loss. To reduce the high memory pressure from ReFL training, for each generated video, we only send first 61 frames worth of latent to FSAEs decoder and VideoAlign model, and only 10 frames with the lowest MPS scoring to MPS. This significantly reduces the training memory consumption. We perform multi-round finetuning between the DIT and VideoAlign RM with small amount of new labeling data generated from DIT each round, while keeping MPS fixed. This approach ensures stable DIT performance improvement without excessive reward hacking. During the RL process, we find two tricks that are essential for our training. The first trick is the domain adaptation of the video reward model in the first round of RM training. To do this, we first generate videos using our SFT model, as well as various public models(e.g. [34, 75, 81]) using the same input prompt and first video frame. Then we combine these generations with ground-truth video to form candidate pool for each input image plus prompt, and randomly select two candidates as labeling pairs, and finetune VideoAlign on the labeled results. This ensures smooth domain adaptation from the pre-trained VideoAlign model to our models data manifold. Experiments show that doing so ensures smooth reward loss increase and model improvement, while not doing this adaptation leads to training divergence. The second trick is the integration of the input image in the reward model input. Here, we finetune the VideoAlign model so that it takes the first frame of the video as an additional model condition when evaluating video quality. Furthermore, in the VLM prompts, we placed emphasis on maintaining consistency between the first frame and the subsequent frames in aspects of color, ID, and details. This aligns with our image-to-video training and significantly improves video consistency."
        },
        {
            "title": "2.4 Video Upsampler",
            "content": "2.4.1 Latent Upsampler After base DIT training, the generated video still suffers from low video detail problems due to the high spatial compression of FSAE. Thus, we propose an additional upsample step to increase video fidelity. The upsample step consists of two modules: (i) convolutional latent upsampler, and (ii) high resolution DIT refiner. Specifically, the latent upsampler module is responsible for upsampling low-resolution latents to the high-resolution feature space, producing preliminary draft latent. The high-resolution refiner then conditions on this draft latent to generate the full video sequence. This process preserves the overall structure of the low-resolution input while refining visual details and restoring high-frequency features, resulting in high-quality video output. Figure 8 Overall framework of latent upsampler. For the upsampler module, some work [56, 86] performs spatial interpolation in RGB space, resulting in extra VAE encoding-decoding operations. Instead, we choose to train video latent upsampler to upsample low-resolution latents by factor of 2. Inspired by LTX-Video [27], we design convolution-based upsampler starting with projection layer, followed by pixel-shuffle for upsampling, and then 16 residual blocks. Setting pixel-shuffle before residual blocks results in better upsample quality at the cost of slightly slower speed, which is negligible in the whole FSVideo framework. We show the latent upsampler training framework in Figure 8. The original video Vhigh is treated as the high-resolution ground truth. We perform bicubic downsample on Vhigh to get Vlow. These two videos inputs are encoded separately by the encoder to obtain Zhigh and Zlow. The low-resolution latent Zlow is fed into our Figure 9 Enhanced training strategies for the high-resolution video refiner. The figure illustrates three input conditioning channels (noise, image conditions, masks) and our proposed training design. Top: WAN Baseline. Only the first frame is conditioned. Middle: FSVideo refiner training. Incorporates low-resolution video latents and three alignment mechanisms: Deviation-Based Latent Estimation, Dynamic Masking (right), and Random Frame Replacement/Shuffle (bottom right) to improve restoration and temporal robustness. Bottom: FSVideo refiner inference. The refiner performs Video-to-Video generation using the latent upsampler output as its primary condition. video latent upsampler to generate the upsampled result ˆZhigh. We then compute the latent L1 loss Llatentl1 between ˆZhigh and Zhigh, and Ll1 and Llpips between ˆZhighs decoder output and Vhigh. The final loss is Lupsampler = α1Llatentl1 + α2Ll1 + α3Llpips. (9) During training, we set α1 = 0.1, α2 = 0.1, and gradually increase α3 from 0.1 to 1. We also gradually increase the size of the training video from 256p to 1024p and apply the mixed resolution strategy of the VAE training to reduce the memory burden, enabling the latent upsampler to generate latents for 121 frame 1024 1024 videos. 2.4.2 High-resolution Refiner We design our video refiner model based on FSVideos base DIT, and one question is how to integrate the latent output from the previous latent upsampler to the refiner. As shown in Figure 9, the common image-to-video DITs, e.g., Wan, extend the input channels to incorporate three conditioning components: noise, mask, and condition. The mask of the first frame is set to 1 and all subsequent frames to 0. Similarly, the condition latent of the first frame corresponds to the real image, while the remaining frames are set to zero. The model performs denoising generation based on the noise component, while the combination of mask and condition provides visual guidance. Our refiner follows this general architecture, but is different in the condition setting. Unlike typical video super-resolution or image-to-video methods, our approach operates in video-to-video fashion, where the high-resolution first-frame image and subsequent low-resolution frames jointly serve as conditional inputs. This design introduces additional challenges in conditioning alignment and model adaptation: (1) The refiner is required to ensure faithful adherence to the high-resolution latent while enhancing the low-resolution latent. (2) Since the low-resolution inputs are generated by the base DIT and latent upsampler, they may contain artifacts. The refiner is required to perform strong restoration, rather than simply upscaling the inputs. (3) As the refiner simultaneously performs detail restoration, it also needs to preserve its text-to-video generation capability. To address these challenges, we introduce an series of enhanced training methods, including dynamic masking, deviation estimation, and conditional dropout. Dynamic Masking Scheme. When the task shifts from image-to-video to video-to-video, straightforward solution is to inject low-resolution latent (upscaled by the latent upsampler, but still of low quality) as condition latents and set all masks to 1. However, when both first-frame preservation and subsequent-frame refinement are required, setting all mask values to 1 and composing the condition latents from both the first-frame and low-resolution latents leads to noticeable visual quality inconsistency across frames. This is 12 because the model struggles to distinguish between genuine latents and low-resolution latents under uniform masking. To mitigate this confusion, we introduce an dynamic masking strategy that adaptively differentiates between real and generated latents. Specifically, given low-resolution first frame latent and its upsampled output ˆz from the latent upsampler, we compute their difference ˆz to estimate the upsampling error for the current sample. This error is then normalized to predefined range narrower than [0, 1], and assigned as the mask value for the low-resolution latent regions, while the first frames mask value remains 1. In this way, we change the mask value from 0/1 binary to confidence score between the ground truth and generated latents, guiding the model to refine visual fidelity where necessary. We also introduce regularization strategy by randomly replacing subset of video frames with high-resolution real frames and setting the corresponding masks to 1, therefore reinforcing the mask cue in all frames. Deviation-Based Latent Estimation. To further encourage the refiner model to improve on low-resolution latents, we introduce deviation-based estimation mechanism consistent with the flow-matching formulation of diffusion models. Recall that in flow matching, the latent state at noise level σ is defined as deterministic interpolation between the clean latent z0 and Gaussian noise ϵ: where σ [0, 1] controls the noise intensity. The model is trained to predict the velocity field zσ = (1 σ)z0 + σϵ, vσ = dzσ dσ = ϵ z0, (10) (11) which describes the instantaneous flow direction from the data distribution toward the noise distribution. During refiner training, we generated the predicted low-resolution velocity ˆvσ from the base DIT for random σ. The predicted low-resolution clean latent can then be written as ˆz0 = zσ σˆvσ. (12) However, using ˆz0 directly as low-resolution condition often leads the refiner to overfit the input, neglecting fine-grained restoration. To avoid this, we define deliberately perturbed latent z0 as z0 = ϵ ˆvσ = ˆz0 + (ϵ ˆϵ) = ˆz0 z0 σ + z0, (13) where ˆϵ = ˆvσ + ˆz0 is the predicted noise. When σ is close to 1, z0 is closer to ˆz0, which is predicted by the base DIT with latent input of high noise level, meaning ˆz0 is now naturally inaccurate. When σ is close to 0, z0 receives an error offset ˆz0 z0 amplified by 1/σ. Thus, z0 maintains controlled offset from the true latent across all timesteps, ensuring that the conditional input remains imperfect. Consequently, during training, the refiner learns to correct the artifacts rather than simply replicating the low-resolution input, improving its capacity for structural and textural refinement. During inference, we do not apply this latent deviation technique, and feed the low-resolution latent produced by the first-stage generative model directly into the denoising process as latent condition, together with the dynamic mask. Condition Dropout and Frame-Shuffle Strategies. To preserve the text-to-video generation capability of the model, we introduce condition dropout strategy, where during training the model randomly omits the low-resolution latents. We also incorporate frame-shuffle strategy that, for the low-resolution latent condition, with 50% probability, we randomly permute local adjacent frames, two nonadjacent frames, or the entire clip in 6:3:1 ratio, thus synthetically emulating various temporal degradations. 2.4.3 Refiner Training Strategy For high-resolution training, we used the same DiT pre-training strategy (Section 2.3.2) and initialized the model by inheriting parameters from the base FSVideo DIT, but this time we train on high-resolution 13 Figure 10 GSB evaluation against other models. Table 3 Image-to-Video Generation Results on VBench 7201280 Method Video Autoencoder #Params (B) HunyuanVideo-I2V [34] Step-Video-TI2V [50] Wan2.1-I2V-14B-720P [75] Pusa-V1.0 [45] DC-VideoGen-Wan-2.1-14B [11] FSVideo - - Wan-2.1-VAE-f8t4c16 Wan-2.1-VAE-f8t4c16 DC-AE-V-f32t4c32 FSAE-f64t4c128 13 30 14 14 14 14+14 Total Score 86.82% 88.36% 86.86% 87.32% 87.73% 88.12% Score I2V Score Quality Score 95.10% 95.50% 92.90% 94.84% 94.08% 95.39% 78.54% 81.22% 80.82% 79.80% 81.39% 80.85% 1024 1024 video data. During training, we use base FSVideo DIT to generate low-resolution latent, passing it to the latent upsampler, and combine the upsampler output with ground-truth latent of the high-resolution first video frame as the input conditions to the refiner. To achieve better efficiency for refiner RL training, we perform light-weight distillation to reduce the refiners inference cost to 8 network forward evaluation (NFE). We first do CFG distillation, then perform step distillation through progressive distillation [59] to 32 steps, followed by SiDA [92] to 8 steps. This approach reduces inference time by 87% while maintaining high visual quality, and reducing the NFE to 8 instead of an even lower value gives enough flexibility of the model for RL training. In RL training, we utilize GRPO [42] since it does not need gradient back-propagation through the FSAE decoder, requiring much less GPU memory compared with ReFL. Specifically, we adopt the MixGRPO sliding window strategy [38] to reduce search space, which allows for faster convergence, and we use the fine-tuned VideoAlign [43] during the base DIT RL training stage as the reward model."
        },
        {
            "title": "3 Experiments",
            "content": "3."
        },
        {
            "title": "Implementation Details",
            "content": "All model trainings are implemented using PyTorch 2 [3] using Fully Sharded Data Parallel [89] and gradient checkpointing [14]. For DIT training, we also apply context parallelization to further reduce GPU memory consumption. We use AdamW [32, 47] with betas = [0.9, 0.95] as the optimizer, while changing the learning rate depending on the task."
        },
        {
            "title": "3.2 Evaluation\nGeneration Quality. To evaluate FSVideo against other leading I2V models, we perform image-to-video\nevaluation on the VBench 2.0 [91] I2V benchmark. All experiments adhere strictly to the official pipeline\nprovided by the VBench team, which evaluates models across two core dimensions: I2V Score and Quality\nScore. Each dimension is further composed of specific sub-evaluation criteria, such as subject consistency,\naesthetic quality, motion smoothness, etc. Table 3 presents the Vbench results at 720×1280 resolution, with\nscores from other open source models reported on the official Vbench website. It is clear that FSVideo\nachieves a competitive score compared to other open-source competitors, only slightly below Step-Video-TI2V,",
            "content": "14 Table 4 DIT inference speed comparison of 5 seconds 720 1280 24 fps video generation. Number of H100 GPUs Latency (in seconds, BFloat16 precision) Wan2.1-I2V-14B-720P 60 NFE* FSVideo 68 NFE (60 base DIT + 8 refiner) 1 2 Out of memory 822.1s 76.6s (with parameter offloading) 19.4s 1 (if no GPU memory constraint) 1607.5s** 27.4s Speedup 42.3 58.7 * For Wan and FSVideo base DIT, 2 NFE = 1 diffusion step with classifer-free guidance; for FSVideo refiner, 1 NFE = 1 diffusion step due to CFG distillation. ** Estimated using 5 seconds 16 fps video generation speed, which takes 1076.1 seconds. while having the highest video compression rate. Specifically, FSVideo has the best total score compared to other models based on Wan 2.1 DIT [11, 45, 75], while having higher compression rate than other deep compression methods, such as DC-VideoGen. We also performed series of human evaluations of FSVideo against other popular open-source video generation models, and show the result in Figure 10. FSVideo drastically outperforms HunyuanVideo and LTX-Video, and is on par with Wan 2.1 14B, while having much higher compression rate, resulting in much faster inference speed (see the Inference Speed part below). Compared to Wan 2.2 14B5, which is the highest-ranked open source image-to-video model in LMArena [46] and Artificial Analysis Arena [2] as of Oct. 2025, FSVideo is less preferred. But given that FSVideo is undertrained given the limited training data and compute. We believe FSVideos performance can improve even more given more, higher-quality training data, and longer training time. Inference Speed. We evaluate inference speed on H100 GPUs with FlashAttention 3 [61] installed. For speed evaluation, we mainly look at the DIT inference speed, which is the majority part of the inference computation. Since FSVideo contains two 14B DITs, making it hard to fit both DITs into single 80G H100, we calculate the inference speed for two cases: 1.) single GPU case where we perform parameter offloading for FSVideos two DITs, and 2.) duo-GPU case where we can utilize FSDP and context parallelization to reduce memory usage, avoiding parameter offloading. For comparison, we calculate the speed of Wan2.1-I2V-14B-720P model, and also utilize FSDP and context parallelization for the duo-GPU case for fair comparison. Evaluations are done for the generation of 5s, 720 1280, 24 fps video using BFloat16 precision. Table 4 shows the evaluation result. We can see that for single-GPU scenario, Wan cannot generate 5 second 24fps even with parameter offloading, while FSVideo succeed with 76.6s. For duo-GPU case, FSVideo achieves speedup of 42.3 ,which is much faster than Wan2.1 14B given similar amount of NFE. If we manage to avoid the GPU memory constraint, such as using FP8 quantization, we will achieve an estimated speed up of at least 58.7 over Wan2.1 14B. Moreover, we can introduce other speed-up methods such as caching or aggressive step distillation. Since these methods reduce the number of NFEs while our model reduces the computation per NFE, we can achieve multiplicative speed improvement, further strengthening FSVideos speed advantage."
        },
        {
            "title": "4 Conclusion",
            "content": "We introduce FSVideo, an image-to-video framework for fast video generation. At its core is new video autoencoder with high compression ratio for token reduction, new DIT architecture with layer memory self-attention to improve dit feature information reuse, and multiscale generation strategy with carefully designed latent refiner to increase video fidelity. Compared with other video generation models of similar parameter size, FSVideo is able to generate competitive videos while being order of magnitude faster, and 5Wan 2.2 is actually 28B model, considering its MOE design where the diffusion process is separated into two 14B models by different noise level. 15 its inference speed can be further enhanced through post-training distillation methods such as step distillation and model distillation. Future work may explore: 1.) better video encoding strategy to support higher resolution generation; 2.) new DIT design to further reduce generation time and increase generation quality without significantly increasing model size; 3.) improving training methods for better prompt coherence and larger video movement, 4.) extending to multimodal generation such as video+sound generation or video editing, and 5.) extending to longer videos or multiscene videos. Finally, we would like to have quick discussion on the future of efficient models. Past works have shown that model speed-up techniques by reducing model capacity, either by distillation of the model parameters or by designing lightweight modules, usually come with trade-off between speed and quality [70] that ultimately produces unsatisfactory results. We need the model capacity to be big enough to model complex training-data distributions, especially for large generative models. We believe that promising direction is to reduce the token amount per model inference and increase token efficiency, which applies to both training and inference. This can be done by looking in the following directions: 1.) more compact and meaningful token representation space, such as [10, 90] for DIT and [15, 76] for language models; 2.) better model and training design to increase token usage efficiency, such as [21, 24, 93] by adding more token passages in transformers, [36, 62] for adaptive token routing, [58] for higher attention computation to improve token efficiency, and to some extent, few-step distillation and training methods [23, 67], since they enforce each token to have high representation capacity for few-step inference; and 3.) better training optimizer, for example Muon [31]. We believe video generation can be both high-quality and cost-effective, and we hope our work will help pave this path in the long run."
        },
        {
            "title": "5 Contributors",
            "content": "Contributor names are alphabetically listed by last name and then first name. Names with an asterisk (*) are people who have left the company. Core contributors: Xinwei Huang, Minxuan Lin, Yaojie Shen, Xiao Yang*, Yuxin Zhang Contributors: Qingyu Chen*, Zhiyuan Fang, Haibin Huang*, Tong Jin, Bo Liu, Celong Liu*, Chongyang Ma, Xing Mei*, Xiaohui Shen, Fuwen Tan, Angtian Wang, Yiding Yang, Jiamin Yuan, Lingxi Zhang"
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [2] Artificial Analysis. URL artificialanalysis.ai. [3] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pages 929947, 2024. [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. [6] Philipp Becker, Abhinav Mehrotra, Ruchika Chavhan, Malcolm Chadwick, Luca Morreale, Mehdi Noroozi, Alberto Gil Ramos, and Sourav Bhattacharya. Edit: Efficient diffusion transformers with linear compressed attention. arXiv preprint arXiv:2503.16726, 2025. [7] R. Bennett. The intrinsic dimensionality of signal collections. IEEE Transactions on Information Theory, 15(5): 517525, 1969. doi: 10.1109/TIT.1969.1054365. [8] Alan Bovik, Hamid Sheikh, Eero Simoncelli, and Wang. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [9] Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, et al. Sana-video: Efficient video generation with block linear diffusion transformer. arXiv preprint arXiv:2509.24695, 2025. [10] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. [11] Junyu Chen, Wenkun He, Yuchao Gu, Yuyang Zhao, Jincheng Yu, Junsong Chen, Dongyun Zou, Yujun Lin, Zhekai Zhang, Muyang Li, et al. Dc-videogen: Efficient video generation with deep compression video autoencoder. arXiv preprint arXiv:2509.25182, 2025. [12] Junyu Chen, Dongyun Zou, Wenkun He, Junsong Chen, Enze Xie, Song Han, and Han Cai. Dc-ae 1.5: Accelerating diffusion model convergence with structured latent space. arXiv preprint arXiv:2508.00413, 2025. [13] Pengtao Chen, Xianfang Zeng, Maosen Zhao, Peng Ye, Mingzhu Shen, Wei Cheng, Gang Yu, and Tao Chen. Sparse-vdit: Unleashing the power of sparse attention to accelerate video diffusion transformers. arXiv preprint arXiv:2506.03065, 2025. [14] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. [15] Jiale Cheng, Yusen Liu, Xinyu Zhang, Yulin Fei, Wenyi Hong, Ruiliang Lyu, Weihan Wang, Zhe Su, Xiaotao Gu, Xiao Liu, et al. Glyph: Scaling context windows via visual-text compression. arXiv e-prints, pages arXiv2510, 2025. [16] Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, and Noah Constant. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. In The Eleventh International Conference on Learning Representations, 2023. [17] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. [18] Google Deepmind. Veo 3, 2025. URL https://deepmind.google/models/veo/. [19] Francesco Denti, Diego Doimo, Alessandro Laio, and Antonietta Mira. The generalized ratios intrinsic dimension estimator. Scientific Reports, 12(1):20005, 2022. 17 [20] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [21] Yanwen Fang, Yuxi Cai, Jintai Chen, Jingyu Zhao, Guangjian Tian, and Guodong Li. Cross-layer retrospective retrieving via layer attention. arXiv preprint arXiv:2302.03985, 2023. [22] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [23] Zhengyang Geng, Mingyang Deng, Xingjian Bai, Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. [24] Gleb Gerasimov, Yaroslav Aksenov, Nikita Balagansky, Viacheslav Sinii, and Daniil Gavrilov. You do not fully utilize transformers representation capacity. arXiv preprint arXiv:2502.09245, 2025. [25] Aldo Glielmo, Iuri Macocco, Diego Doimo, Matteo Carli, Claudio Zeni, Romina Wild, Maria dErrico, Alex Rodriguez, and Alessandro Laio. Dadapy: Distance-based analysis of data-manifolds in python. Patterns, 3(10), 2022. [26] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [27] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [28] Jiancheng Huang, Gengwei Zhang, Zequn Jie, Siyu Jiao, Yinlong Qian, Ling Chen, Yunchao Wei, and Lin Ma. M4v: Multi-modal mamba for text-to-video generation. arXiv preprint arXiv:2506.10915, 2025. [29] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [30] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European conference on computer vision, pages 694711. Springer, 2016. [31] Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. URL https://kellerjordan.github.io/posts/muon/. [32] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [33] Kuaishou KlingAI. Kling, 2025. URL klingai.com. [34] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [35] Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. EQ-VAE: Equivariance regularized latent space for improved generative image modeling. In Forty-second International Conference on Machine Learning, 2025. [36] Felix Krause, Timy Phan, Ming Gui, Stefan Andreas Baumann, Vincent Tao Hu, and Björn Ommer. Tread: Token routing for efficient architecture-agnostic diffusion training. arXiv preprint arXiv:2501.04765, 2025. [37] Sangyun Lee, Zinan Lin, and Giulia Fanti. Improving the training of rectified flows. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 6308263109. Curran Associates, Inc., 2024. [38] Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025. [39] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. In Forty-second International Conference on Machine Learning, 2025. 18 [40] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. [41] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 73537363, June 2025. [42] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. [43] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. [44] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difficulty of training transformers. In EMNLP 2020 - 2020 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference, EMNLP 2020 - 2020 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference, pages 57475763. Association for Computational Linguistics (ACL), 2020. doi: 10.18653/v1/2020.emnlp-main.463. [45] Yaofang Liu, Yumeng Ren, Aitor Artola, Yuxuan Hu, Xiaodong Cun, Xiaotong Zhao, Alan Zhao, Raymond Chan, Suiyun Zhang, Rui Liu, et al. Pusa v1. 0: Surpassing wan-i2v with $500 training cost by vectorized timestep adaptation. arXiv preprint arXiv:2507.16116, 2025. [46] LMArena. URL lmarena.ai. [47] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. [48] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan LI, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 57755787. Curran Associates, Inc., 2022. [49] Zhengyao Lv, Chenyang Si, Junhao Song, Zhenyu Yang, Yu Qiao, Ziwei Liu, and Kwan-Yee K. Wong. Fastercache: Training-free video diffusion model acceleration with high quality. In The Thirteenth International Conference on Learning Representations, 2025. [50] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. [51] Xiaofeng Mao, Zhengkai Jiang, Fu-Yun Wang, Jiangning Zhang, Hao Chen, Mingmin Chi, Yabiao Wang, and Wenhan Luo. Osv: One step is enough for high-quality image to video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1258512594, 2025. [52] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge? In International conference on machine learning, pages 34813490. PMLR, 2018. [53] OpenAI. Sora 2, 2025. URL https://openai.com/index/sora-2/. [54] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [55] Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, et al. Open-sora 2.0: Training commercial-level video generation model in $200 k. arXiv preprint arXiv:2503.09642, 2025. [56] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [58] Aurko Roy, Timothy Chou, Sai Surya Duvvuri, Sijia Chen, Jiecao Yu, Xiaodong Wang, Manzil Zaheer, and Rohan Anil. Fast and simplex: 2-simplicial attention in triton. arXiv preprint arXiv:2507.02754, 2025. [59] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. [60] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. [61] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [62] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [63] Hui Shen, Jingxuan Zhang, Boning Xiong, Rui Hu, Shoufa Chen, Zhongwei Wan, Xin Wang, Yu Zhang, Zixuan Gong, Guangyin Bao, Chaofan Tao, Yongfeng Huang, Ye Yuan, and Mi Zhang. Efficient diffusion models: survey. Transactions on Machine Learning Research, 2025. ISSN 2835-8856. [64] Wenzhe Shi, Jose Caballero, Ferenc Huszár, Johannes Totz, Andrew Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 18741883, 2016. [65] Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, and Aliaksandr Siarohin. Improving the diffusability of autoencoders. In Forty-second International Conference on Machine Learning, 2025. [66] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. In The Twelfth International Conference on Learning Representations, 2024. [67] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023. [68] Alexandros Stergiou and Ronald Poppe. Adapool: Exponential adaptive pooling for information-retaining downsampling. IEEE Transactions on Image Processing, 32:251266, 2022. [69] Anni Tang, Tianyu He, Junliang Guo, Xinle Cheng, Li Song, and Jiang Bian. Vidtok: versatile and open-source video tokenizer. arXiv preprint arXiv:2412.13061, 2024. [70] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: survey. ACM Comput. Surv., 55(6), December 2022. ISSN 0360-0300. doi: 10.1145/3530811. [71] Rui Tian, Qi Dai, Jianmin Bao, Kai Qiu, Yifan Yang, Chong Luo, Zuxuan Wu, and Yu-Gang Jiang. Reducio! generating 1k video within 16 seconds using extremely compressed motion latents. arXiv preprint arXiv:2411.13552, 2024. [72] Ye Tian, Xin Xia, Yuxi Ren, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Yunhai Tong, Ling Yang, and Bin Cui. Training-free diffusion acceleration with bottleneck sampling. arXiv preprint arXiv:2503.18940, 2025. [73] Yuchuan Tian, Zhijun Tu, Hanting Chen, Jie Hu, Chao Xu, and Yunhe Wang. U-dits: Downsample tokens in u-shaped diffusion transformers. Advances in Neural Information Processing Systems, 37:5199452013, 2024. [74] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. [75] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [76] Haoran Wei, Yaofeng Sun, and Yukun Li. Deepseek-ocr: Contexts optical compression. arXiv preprint arXiv:2510.18234, 2025. 20 [77] Pingyu Wu, Kai Zhu, Yu Liu, Liming Zhao, Wei Zhai, Yang Cao, and Zheng-Jun Zha. Improved video vae for latent video diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1812418133, 2025. [78] Yushu Wu, Yanyu Li, Anil Kag, Ivan Skorokhodov, Willi Menapace, Ke Ma, Arpit Sahni, Ju Hu, Aliaksandr Siarohin, Dhritiman Sagar, et al. Taming diffusion transformer for real-time mobile video generation. arXiv preprint arXiv:2507.13343, 2025. [79] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, Jianfei Chen, Ion Stoica, Kurt Keutzer, and Song Han. Sparse video-gen: Accelerating video diffusion transformers with spatial-temporal sparsity. In Forty-second International Conference on Machine Learning, 2025. [80] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. [81] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan.Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025. [82] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1570315712, 2025. [83] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2296322974, 2025. [84] Yuanhao Zhai, Kevin Lin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Chung-Ching Lin, David Doermann, Junsong Yuan, and Lijuan Wang. Motion consistency model: Accelerating video diffusion with disentangled motion-appearance distillation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [85] Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Learning multi-dimensional human preference for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80188027, 2024. [86] Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, and Bingyue Peng. Waver: Wave your way to lifelike video generation. arXiv preprint arXiv:2508.15761, 2025. [87] Zhixing Zhang, Yanyu Li, Yushu Wu, Anil Kag, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Dimitris Metaxas, Sergey Tulyakov, et al. Sf-v: Single forward video generation model. Advances in Neural Information Processing Systems, 37:103599103618, 2024. [88] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictor-corrector framework for fast sampling of diffusion models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 4984249869. Curran Associates, Inc., 2023. [89] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: Experiences on scaling fully sharded data parallel. Proceedings of the VLDB Endowment, 16(12):38483860, 2023. [90] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. [91] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Lulu Gu, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, et al. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. [92] Mingyuan Zhou, Huangjie Zheng, Yi Gu, Zhendong Wang, and Hai Huang. Adversarial score identity distillation: Rapidly surpassing the teacher in one step. In The Thirteenth International Conference on Learning Representations, 2025. 21 [93] Defa Zhu, Hongzhi Huang, Zihao Huang, Yutao Zeng, Yunyao Mao, Banggu Wu, Qiyang Min, and Xun Zhou. Hyper-connections. arXiv preprint arXiv:2409.19606, 2024."
        }
    ],
    "affiliations": [
        "ByteDance"
    ]
}