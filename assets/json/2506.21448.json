{
    "paper_title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing",
    "authors": [
        "Huadai Liu",
        "Jialei Wang",
        "Kaicheng Luo",
        "Wen Wang",
        "Qian Chen",
        "Zhou Zhao",
        "Wei Xue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While end-to-end video-to-audio generation has greatly improved, producing high-fidelity audio that authentically captures the nuances of visual content remains challenging. Like professionals in the creative industries, such generation requires sophisticated reasoning about items such as visual dynamics, acoustic environments, and temporal relationships. We present ThinkSound, a novel framework that leverages Chain-of-Thought (CoT) reasoning to enable stepwise, interactive audio generation and editing for videos. Our approach decomposes the process into three complementary stages: foundational foley generation that creates semantically coherent soundscapes, interactive object-centric refinement through precise user interactions, and targeted editing guided by natural language instructions. At each stage, a multimodal large language model generates contextually aligned CoT reasoning that guides a unified audio foundation model. Furthermore, we introduce AudioCoT, a comprehensive dataset with structured reasoning annotations that establishes connections between visual content, textual descriptions, and sound synthesis. Experiments demonstrate that ThinkSound achieves state-of-the-art performance in video-to-audio generation across both audio metrics and CoT metrics and excels in out-of-distribution Movie Gen Audio benchmark. The demo page is available at https://ThinkSound-Project.github.io."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . e [ 2 8 4 4 1 2 . 6 0 5 2 : r ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing Huadai Liu1,2, Jialei Wang3, Kaicheng Luo3, Wen Wang1 Qian Chen1, Zhou Zhao3, Wei Xue2 1Tongyi Lab, Alibaba Group 2Hong Kong University of Science and Technology (HKUST) 3Zhejiang University"
        },
        {
            "title": "Abstract",
            "content": "While end-to-end video-to-audio generation has greatly improved, producing highfidelity audio that authentically captures the nuances of visual content remains challenging. Like professionals in the creative industries, such generation requires sophisticated reasoning about items such as visual dynamics, acoustic environments, and temporal relationships. We present ThinkSound, novel framework that leverages Chain-of-Thought (CoT) reasoning to enable stepwise, interactive audio generation and editing for videos. Our approach decomposes the process into three complementary stages: foundational foley generation that creates semantically coherent soundscapes, interactive object-centric refinement through precise user interactions, and targeted editing guided by natural language instructions. At each stage, multimodal large language model generates contextually aligned CoT reasoning that guides unified audio foundation model. Furthermore, we introduce AudioCoT, comprehensive dataset with structured reasoning annotations that establishes connections between visual content, textual descriptions, and sound synthesis. Experiments demonstrate that ThinkSound achieves state-of-the-art performance in video-to-audio generation across both audio metrics and CoT metrics and excels in out-of-distribution Movie Gen Audio benchmark. The project page is available at https://ThinkSound-Project.github.io."
        },
        {
            "title": "Introduction",
            "content": "Generating realistic sound for video demands more than recognizing objects; it requires reasoning about complex visual dynamics and context determining when an owl is chirping versus flapping its wings, identifying the subtle sway of tree branches, and synchronizing multiple sound events within scene, as illustrated in Figure 1. Current end-to-end video-to-audio (V2A) generation systems (Luo et al., 2024; Xing et al., 2024; Zhang et al., 2024), while having largely improved, often struggle with this compositional complexity and contextual nuance. They may produce generic sounds or fail to synchronize precisely with subtle visual cues, thus limiting fidelity and user control. Preprint. Under review. Figure 1: ThinkSound with CoT: (1) CoT-driven foley synthesis capturing semantic and temporal details, (2) interactive object-centric refinement for user control, and (3) targeted audio editing. Recent breakthroughs in Multimodal Large Language Models (MLLMs) (Lu et al., 2024; Liu et al., 2023b; Wang et al., 2024b) offer powerful capabilities in understanding and reasoning across multiple modalities. Concurrently, Chain-of-Thought (CoT) prompting (Wei et al., 2022; Zhang et al., 2023; Wang et al., 2025) has proven effective at eliciting structured, step-by-step reasoning from large models. These advancements create great potential to fundamentally rethink V2A by decomposing complex sound design into explicit reasoning steps and actionable synthesis instructions. The necessity for CoT in V2A becomes evident when examining the process of sound designers. They employ multi-stage approach: analyzing visual content, then reasoning about acoustic properties, and finally synthesizing and refining sounds. End-to-end approaches compress the process into single black-box transformation, losing the nuanced reasoning required for audio generation. Some approaches have attempted to adopt MLLMs for multi-stage V2A. SonicVisionLM (Xie et al., 2024) converts video to textual captions using MLLMs and employs separate model for text-toaudio (T2A) generation, which inevitably loses critical visual details and motion dynamics essential for realistic sound synthesis. More recently, DeepSound-V1 (Liang et al., 2025), while introducing MLLM-generated CoT for V2A, fragments the process into three separate tasks (audio generation, vocal removal, and silence detection) using specialized models rather than unified framework. This fragmentation fails to fully leverage the deep understanding and reasoning capabilities that MLLMs could bring to comprehensive audio design. To overcome these limitations and unlock the full reasoning potential of MLLMs for V2A, we present ThinkSound that harnesses CoT reasoning to enable stepwise, interactive generation and editing for V2A. ThinkSound decomposes audio generation into three intuitive and user-centric stages: (1) foundational foley generation to synthesize semantic and temporal matching soundscapes, (2) interactive object-centric refinement via user clicks, and (3) targeted audio editing guided by highlevel natural language instructions. At each stage, an MLLM produces semantically and temporally aligned CoT instructions that guide unified foundation model for audio generation, ensuring that the generated audio remains coherent, contextually grounded, and high quality throughout the workflow. Moreover, to support the training of ThinkSound and advance research in this area, we introduce AudioCoT, comprehensive large-scale dataset with structured CoT reasoning annotations. Technically, three key innovations are proposed: a) We fine-tune MLLMs on AudioCoT to generate structured, audio-specific reasoning chains that explicitly capture temporal dependencies, acoustic properties, and the decomposition of complex audio events. b) We design unified audio foundation model based on flow matching that supports all three stages with the capabilities of synthesizing highfidelity audio from arbitrary combinations of input modalities. This foundation model directly benefits from the detailed CoT reasoning provided by MLLMs, which effectively decomposes complex audio scenes into manageable components, enabling focused sound event generation while maintaining global coherence. c) We introduce novel click-based interface that enables users to target specific visual objects for audio refinement, with CoT reasoning translating visual attention into contextually appropriate sound synthesis. The main contributions are summarized as follows: novel three-stage interactive framework for V2A that progressively builds soundscapes through initial generation, object-centric refinement, and targeted editing, all unified by CoT reasoning from MLLMs. unified multimodal foundation model capable of high-quality audio synthesis from arbitrary combination of video, text, and audio inputs, leveraging CoT instructions to decompose complex scenes into manageable sound components. AudioCoT, large-scale multimodal dataset with audio-specific CoT reasoning annotations that bridges visual content, textual descriptions, and sound synthesis. Experimental results demonstrate that ThinkSound achieves the state-of-the-art performance across objective metrics and subjective metrics, highlighting the effectiveness of our reasoning-guided generation."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Video-to-Audio Generation V2A (Cheng et al., 2024a; Wang et al., 2024c; Xu et al., 2024; Liu et al., 2025) focuses on synthesizing audio that aligns seamlessly with the visual content of video clip. Earlier work (Luo et al., 2024; 2 Xing et al., 2024; Zhang et al., 2024; Viertola et al., 2024) try to generate audio samples based on silent video only using latent diffusion models (Rombach et al., 2022) or language models (Floridi & Chiriatti, 2020). For example, Diff-Foley (Luo et al., 2024) employs an audio-visual contrastive feature and latent diffusion to predict the spectrogram latent, while FoleyGen uses autoregressive techniques. However, recent research (Chen et al., 2024; Mo et al., 2024; You et al., 2025) pay more attention to generating audios using multimodal control including videos, audios, and texts. For example, MMAudio (Polyak et al., 2024) uses flow matching conditioned on multi-modal inputs, including videos and texts. MultiFoley (Wu et al., 2024) further uses audio context as an additional input. Despite these advancements, existing work struggles to reason beyond simple object recognition and fails to conduct step-by-step interactive process with users for audio generation. In this work, we propose ThinkSound, V2A model with CoT reasoning in MLLMs, which supports step-by-step and interactive audio generation and editing. 2.2 Large Language Models and Reasoning LLMs (Liu et al., 2024a; Guo et al., 2025; Hurst et al., 2024) have demonstrated remarkable reasoning capabilities through CoT prompting, enabling complex problem decomposition via intermediate reasoning steps. This paradigm, pioneered by (Wei et al., 2022), has been extended to MLLMs (Alayrac et al., 2022; Yang et al., 2023; Chu et al., 2024) that integrate visual, audio, and textual understanding through cross-modal alignment. Recent works (Rubenstein et al., 2023; Li et al., 2023; Wu et al., 2024) have explored MLLMs potential in multimodal reasoning, particularly in visual-audio-textual grounding and cross-modal causal reasoning. Despite these, MLLMs remain under-explored for audio generation. While models like SonicVisionLM (Xie et al., 2024) and DeepSound-V1 (Liang et al., 2025) incorporate V2A, they lack mechanisms to decompose user intent into semantic and temporal reasoning steps. Our ThinkSound proposes novel framework for audio generation and editing that decomposes the complex task into foundation foley generation, interactive object-centric refinement, and targeted audio editing (Wang et al., 2023; Liu et al., 2024d). 2.3 Flow Matching Flow Matching has emerged as powerful alternative to diffusion models for high-quality audio synthesis (Evans et al., 2024; Liu et al., 2024b, 2023a), directly learning continuous normalizing flows between distributions by training time-dependent vector field. Recent works Lipman et al. (2022); Liu et al. (2022) establish its theoretical foundations, while Liu et al. (2024c) demonstrated its advantages for audio generation with improved quality and faster sampling. Polyak et al. (2024) further advance this approach by applying conditional flow matching to multimodal audio generation, showing its effectiveness in maintaining temporal coherence across complex audio signals. Our work extends these advances through CoT-guided generation and novel multi-guidance strategy that integrates control signals from multiple modalities. Our modality-agnostic training approach with classifier-free guidance dropout allows the model to generate high-quality audio from flexible input combinations, addressing both control precision and data scarcity challenges that have limited previous approaches."
        },
        {
            "title": "3 AudioCoT Dataset for CoT-Guided Generation and Editing",
            "content": "3.1 Multimodal Data Sources The AudioCoT dataset comprises both video-audio and audio-text pairs. For video-audio data, we utilize VGGSound (Chen et al., 2020) and curated, non-speech subset of AudioSet (Gemmeke et al., 2017) to ensure broad coverage of real-world audiovisual events. For audio-text data, we aggregate pairs from AudioSet-SL (Hershey et al., 2021), Freesound (Fonseca et al., 2017), AudioCaps (Kim et al., 2019), and BBC Sound Effects 1, resulting in diverse and representative corpus for training multimodal models. Further details on data processing and statistics are provided in the Supplementary Materialss Section A.1. 1https://sound-effects.bbcrewind.co.uk/ 3 Figure 2: Overview of our AudioCoT dataset construction pipeline. 3.2 Automated CoT Generation Pipeline Stage 1: Foundational Foley CoT Generation. To construct the foley generation CoT data, we employ multi-stage processing pipeline. For video-audio pairs, we utilize VideoLLaMA2 (Cheng et al., 2024b) to extract both temporal and semantic information from the videos through different prompting strategies, while for audio-text pairs (which lack video data), we employ more streamlined approach without VideoLLaMA2. In both cases, we generate audio descriptions using Qwen2-Audio (Chu et al., 2024), combining them with existing video captions or text annotations. The collected information is then integrated using GPT-4.1-nano2 to synthesize comprehensive CoT reasoning chains that capture the complex relationships between content conditions and the corresponding audio elements, ensuring both data types contribute to comprehensive understanding of audio generation reasoning. Stage 2: Interactive Object-Centric CoT Generation. To facilitate object-focused audio generation, we develop Region of Interest (ROI) extraction framework leveraging Grounded SAM2 (Ren et al., 2024; Ravi et al., 2024). Using audio captions as prompts, we identify and generate bounding boxes for potential sound-emitting objects. These coordinates are tracked temporally across video frames, while VideoLLaMA2 provides detailed semantic descriptions for each ROI segment. For complex audio manipulations (extraction/removal), we construct hierarchical reasoning structure where the CoT of the target video is merged with another videos CoT to establish global context. This composite representation is then integrated with ROI-specific generation information and processed by GPT-4.1-nano to formulate coherent manipulation rationales (detailed in Supplementary Materials A.2). Stage 3: Instruction-Based Audio Editing CoT Generation. For instruction-guided audio editing, we analyze and integrate CoT information from Stage 1 based on four primary operations: extension, inpainting, addition, and removal. These operations address scenarios from extending sequences to eliminating unwanted segments. GPT-4.1-nano processes this integrated information to generate instruction-specific CoT reasoning chains while we perform corresponding audio operations, creating (Instruction-CoT, input audio, output audio) triplets for model training and evaluation."
        },
        {
            "title": "4 ThinkSound",
            "content": "4.1 Overview As depicted in Figure 3, ThinkSound introduces novel step-by-step, interactive framework for audio generation and editing guided by CoT reasoning. Our approach decomposes the complex V2A 2https://openai.com/index/gpt-4-1/ 4 Figure 3: Overview of the ThinkSound architecture. Left: our Multimodal LLM framework, where fine-tuned VideoLLaMA 2 model generates CoT reasoning for audio generation and editing. Right: our enhanced Multimodal Transformer architecture, which features an MM-DiT backbone with dedicated pathways for processing multimodal inputs and CoT-driven conditioning to enable highfidelity, contextually grounded audio generation. task into three intuitive stages: (1) foundation foley generation that creates semantic and temporal matching soundscape, (2) interactive region-based refinement through user clicks, and (3) targeted audio editing based on high-level instructions. At each stage, an MLLM generates CoT reasoning that guides unified audio foundation model to produce and refine the soundtrack. 4.2 CoT Reasoning with Multimodal LLM To enable stepwise, context-aware audio generation, we leverage VideoLLaMA2 (Cheng et al., 2024b) as the core multimodal reasoning engine. VideoLLaMA2 is selected for its state-of-the-art capability in fusing video, text, and audio modalities, and its advanced spatial-temporal modelling is essential for capturing the nuanced interplay between visual events and their corresponding acoustic manifestations. We further adapt VideoLLaMA2 to the audio reasoning domain by fine-tuning it on our AudioCoT dataset, which contains rich, annotated reasoning chains tailored for audio-visual tasks. This finetuning process is designed to equip the model with three core competencies: (1) audio-centric understandingthe ability to infer acoustic properties, model sound propagation, and reason about audio-visual correspondences, including temporal and causal relationships among audio events (e.g., footsteps occur before the door opens, then conversation ensues); (2) structured CoT decompositionthe capacity to break down complex audio generation or editing tasks into explicit, actionable steps; and (3) multimodal instruction followingrobustly interpreting and executing diverse generation or editing instructions across modalities. As illustrated in Figure 3, the finetuning objective is the standard cross-entropy loss for next-token prediction. Through this targeted adaptation, VideoLLaMA2 is transformed into specialized audio reasoning module, capable of producing contextually precise CoT instructions that drive each stage of the ThinkSound pipeline. 4.3 CoT-Guided Unified Audio Foundation Model The core of ThinkSound is our unified audio foundation model that seamlessly translates CoT reasoning into high-quality audio, whose detail is shown in the right part of Fig. 3. We encode audio into latent representations using pre-trained VAE (Pinheiro Cinelli et al., 2021) and train our model with conditional flow matching (Lipman et al., 2022), where the velocity field prediction is conditioned on multimodal context including visual content, CoT reasoning, text captions, and audio context. To support any combination of input modalities, we adopt the integration of classifierfree guidance dropout during training. By randomly dropping different modality combinations with probability pdrop, we enable the model to handle arbitrary input configurations at inference timeessential for our interactive framework. We also incorporate strategic audio context masking to support advanced editing operations such as inpainting and extension. For text processing, we employ dual-pathway encoding strategy: MetaCLIP (Xu et al., 2023) encodes visual captions to provide scene-level context, while T5-v1-xl (Raffel et al., 2020) processes 5 structured CoT reasoning to capture detailed temporal and causal relationships. These complementary representations are effectively combined, with MetaCLIPs features serving as global conditioning signals while T5s outputs enable fine-grained reasoning-driven control. Our enhanced MM-DiT architecture builds on recent advances in multimodal generative modeling (Esser et al., 2024; Labs, 2024; Cheng et al., 2024a) with three key components: (1) we implement hybrid transformer backbone that alternates between modality-specific and shared processing. Multistream transformer blocks maintain separate parameters for each modality while sharing attention mechanisms, allowing efficient processing of diverse inputs without sacrificing cross-modal learning. (2) we design an adaptive fusion module that upsamples video features and fuses them with audio latents via gated mechanism (Cho et al., 2014). This not only highlights salient visual cues and suppresses irrelevant information, but also ensures that video information is directly involved in subsequent single-stream transformer blocks. By integrating video into the audio latent space, the model can better capture subtle visual details and their nuanced effects on the soundscape, enabling richer cross-modal reasoning than using audio latents alone. (3) we implement global conditioning by mean-pooling CLIP features from both the caption and video, and, following MMAudio (Cheng et al., 2024a), incorporating sync features to improve audio-visual temporal alignment. The resulting global condition is added to the timestep embedding and injected via adaptive layer normalization layers (AdaLN) (Peebles & Xie, 2023) into both multi-stream and single-stream blocks. 4.4 Step-by-Step CoT-Guided Audio Generation and Editing By enabling flexible combinations of input modalities along with CoT, ThinkSound supports decomposing audio generation into three intuitive stages shown in Fig. 1. This three-stage pipeline enables progressively refined, highly customized audio generation through an intuitive interactive workflow, with CoT reasoning bridging user intent and audio synthesis at each step. Stage 1: CoT-Guided Foley Generation. In the first stage, our framework analyzes the entire video to identify acoustic elements and their relationships. The fine-tuned MLLM generates detailed CoT reasoning that explicitly identifies primary sound events, ambient elements, acoustic properties, and their temporal dependencies - determining when objects make sounds and how these sounds interact. This structured reasoning guides the audio foundation model to synthesize high-fidelity audio that precisely matches both the semantic content and temporal dynamics of the visual scene. By decomposing complex audio scenes into explicit sound components through CoT reasoning, the model generates diverse and coherent soundscape that captures subtle visual cues and motion dynamics essential for realistic audio synthesis. Stage 2: Interactive Object-Focused Audio Generation. Stage 2 introduces an interactive framework that enables users to refine the initial soundscape by focusing on specific visual elements. Through simple click-based interface, users can select objects of interest for audio enhancement. Unlike the holistic approach in Stage 1, this object-centric refinement leverages the segmented ROI to guide targeted audio synthesis. The fine-tuned MLLM analyzes the selected ROI and generates specialized CoT reasoning focused on the objects acoustic properties within the global context. This structured reasoning conditions the audio foundation model to synthesize object-specific sounds, seamlessly integrating them with the initial soundtrack produced in Stage 1. Notably, in this stage, the foundation model incorporates the existing audio context as an additional conditioning signal. Stage 3: Instruction-based Audio Editing. In the final stage, users can provide high-level editing instructions to refine audio quality or modify specific elements. The MLLM translates these natural language instructions into precise audio processing operations through CoT reasoning, considering both visual content and current audio state. The foundation model, conditioned on both this reasoning and the existing audio context, applies targeted modifications while maintaining overall coherence. This natural language-driven approach enables non-technical users to perform sophisticated audio manipulation including add sounds, remove sound, audio inpainting, and audio extension."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experiment Setup Evaluation Metrics We conduct comprehensive evaluations using both objective and subjective metrics to assess audio quality, text-audio alignment, and video-audio synchronization. For objective 6 Table 1: Comparison of our ThinkSound foundation model with existing video-to-audio baselines on the VGGSound test set. indicates lower is better, indicates higher is better. For MOS, we show the mean and variance of the MOS scores. indicates that the method does not use text for inference. Method GT See&Hear V-AURA FoleyCrafter Frieren V2A-Mapper MMAudio ThinkSound w/o CoT Reasoning FD - 118.95 46.99 39.15 74.96 48.10 43.26 34.56 39. Objective Metrics Subjective Metrics Efficiency KLPaSST KLPaNNs DeSync CLAPcap CLAPCoT MOS-Q MOS-A 4.560.19 0.28 2.870.99 0.32 3.201.17 0.23 2.630.88 0.41 2.951.09 0.37 3.161.04 0.38 3.970.82 0.31 4.370.21 2.751.08 3.421.03 3.081.21 3.271.11 3.311.02 3.840.89 0.45 0.35 0.37 0.34 0.34 0.32 0. - 2.30 1.83 1.89 2.64 2.34 1.40 - 2.26 2.23 2.06 2.55 2.50 1.65 0.55 1.20 0.65 1.21 1.00 1.23 0.44 1.52 1.59 1. 1.40 0.46 0.48 0.33 0.29 0. 0.41 4.020.73 4.180.79 3.910.83 4.040.75 Params Time(s) - 415M 695M 1.20B 159M 229M 1.03B 1.30B 1.30B - 19.42 14.00 3.84 - - 3.01 1.07 0. evaluation, we compute the Fréchet Distance (FD) (Kilgour et al., 2018; Copet et al., 2024) in the OpenL3 feature space (Cramer et al., 2019; Evans et al., 2024) to measure distribution-level similarity, which we extend to evaluate stereo audio. We also use the Kullback-Leibler (KL) Divergence (Copet et al., 2024) based on predictions from PaSST model (KLPaSST) and PaNNs (KLP aN s) to evaluate label-level consistency. Temporal alignment between audio and video is assessed using DeSync predicted by the Synchformer model (Iashin et al., 2024), while semantic alignment with text is measured using the CLAP score (Wu* et al., 2023; Chen et al., 2022), including both CLAPcap (caption) and CLAPCoT (CoT). For subjective evaluation, we collect human ratings using the Mean Opinion Score (MOS) to evaluate perceived audio quality (MOS-Q) and alignment with video and CoT (MOS-A). Further details are provided in the Supplementary Materials C. Implementation Details For VAE training, we initialize our VAE using the VAE model weights trained on stereo data at 44.1kHz sample rate provided by Stability AI 3. We employ mixed precision training with batch size of 144 across 24 A800 GPUs for 500,000 steps. Subsequently, following Evans et al. (2024), we freeze the VAE encoder and train the VAE decoder with latent mask ratio of 0.1 for an additional 500,000 steps. We use AdamW (Loshchilov & Hutter, 2019) as the optimizer, setting the generator learning rate to 3e-5 and the discriminator learning rate to 6e-5. In the foundation model training phase, we utilize exponential moving average and automatic mixed precision for 100,000 steps on 8 A100 GPUs, with an effective batch size of 256. We adopt cfg dropout of 0.2 for each modality with learning rate of 1e-4. During the task-specific fine-tuning stage, we similarly apply exponential moving average and automatic mixed precision for 50,000 steps on 8 A100 GPUs, maintaining an effective batch size of 256. AdamW remains our optimizer of choice, with learning rate set at 1e-4. We attach the benchmark details of VGGSound, Movie Gen Audio Bench, and AudioCoT test set for stages 2 and 3 into Supplementary Materials A.3. 5.2 Main Results Video-to-Audio Generation Results We compare our foundation model against existing videoto-audio baselines including Seeing and Hearing (Xing et al., 2024), V-AURA (Viertola et al., 2024), FoleyCrafter (Zhang et al., 2024), Frieren (Wang et al., 2024d), V2A-Mapper (Wang et al., 2024a), and MMAudio (Cheng et al., 2024a). From Table 1, we observe three key findings: (1) GT audio exhibits low CLAPcap scores (0.28), revealing that original VGGSound captions inadequately capture the semantic content and temporal relationships needed for highfidelity audio generation. (2) ThinkSound outperforms all baselines across most objective metrics and all subjective metrics. Compared to the strongest baseline (MMAudio), our model achieves substantial improvements in audio quality (KLPaSST: 1.52 vs. 1.65) and semantic alignment (CLAPCoT: 0.46 vs. 0.40), while maintaining comparable temporal synchronization. Subjective evaluations further confirm these imTable 2: Out-of-distribution evaluation on provements (MOS-Q: 4.02 vs. 3.84, MOS-A: MovieGen Audio Bench. This benchmark does 4.18 vs. 3.97). (3) Removing CoT reasoning not provide the GT audios, so we cannot compare notably degrades both audio quality and alignFD and KL. ment metrics, especially for CLAPCoT decreasing from 0.46 to 0.41, confirming that CoT provides crucial information about sound events, their temporal relationships, and acoustic characteristics. CLAPCoT DeSync MOS-Q MOS-A 3.950.87 3.621.03 3.980.77 3.700. MMAudio MovieGen 0.77 1.00 0.45 0.47 Method 4.110.74 3.870.82 ThinkSound 0.51 0.76 3https://github.com/Stability-AI/stable-audio-tools 7 Furthermore, we conduct an out-of-distribution evaluation on the MovieGen Audio Bench (Polyak et al., 2024) to assess the generalization capability of our model. The results in Table 2 show that our ThinkSound model still achieves the best CLAPCoT score of 0.51 while DeSync is on par with the best baseline. For subjective evaluation, ThinkSound achieves the best performance in both alignment and fidelity metrics. This demonstrates that ThinkSound exhibits strong generalization capability across different scenarios. Table 3: Object-focused generation performance. Method Object-Focused Audio Generation and CoTGuided Audio Editing For object-focused generation, we compare two approaches: (1) MMAudio which does not have the ROI design, and (2) ThinkSound w/o CoT reasoning. Results in Table 3 show that our interactive approach with CoT reasoning achieves significantly better results, demonstrating superior object-specific sound quality and integration with the foundation audio. ThinkSound 43.27 45.28 w/o CoT FD KLPaSST CLAP MOS-Q MOS-A 3.890.52 3.910.53 3.770.64 3.810.59 3.610.63 3.640.69 MMAudio 0.48 0.43 1.32 1.34 44.46 0.41 1.38 Table 4: Audio editing results on AudioCoT test set (MOS-A: alignment between audio and text; DDPM: DDPM-Friendly). For text-guided audio editing, we compare ThinkSound with AudioLDM-2 (Liu et al., 2024e) and Edit Friendly DDPM (HubermanSpiegelglas et al., 2024), adapting these models to our experimental setup. As shown in Table 4, ThinkSound outperforms the baselines across all objective and subjective metrics. Specifically, ThinkSound achieves the lowest FD (34.78) and KLPaSST (1.45), and the highest CLAP score (0.51), indicating better audio fidelity and semantic relevance. In human evaluation, ThinkSound also gets the best MOS-Q and MOS-A. Removing CoT reasoning clearly drops all metrics, showing the importance of CoT-guided reasoning for text-based audio editing. AudioLDM-2 61.28 55.56 DDPM ThinkSound w/o CoT 34.78 45.78 1.45 1.58 0.51 0. 1.94 1.75 0.35 0.39 Method FD KLPaSST CLAP MOS-Q MOS-A 3.280.59 3.480.82 3.340.28 3.670.56 3.920.82 3.850.82 3.530.45 3.520. 5.3 Ablation Studies To better understand the contribution of each component in ThinkSound and to validate the effectiveness of our design choices, we conduct comprehensive ablation studies on the VGGSound test set. Mainly focus on: 1) text encoding strategies and 2) multi-modal integration mechanisms. For more ablation and exploratory results, we attach them to Supplementary Materials D. Text Encoding Strategies. We evaluate different text encoding strategies with or without CoT reasoning and the results are compiled in Table 5. The results show that (1) CoT reasoning substantially improves audio fidelity, with the FD score improving from 39.84 to 37.65 when comparing CLIP-only to T5-based CoT approaches. (2) The integration of contrastive features from CLIP with contextual features from T5 further enhances performance, reducing KL divergence metrics from 1.54 to 1.52 (KLPaSST) and from 1.35 to 1.32 (KLPaNNs). CLIP 39.84 T5 (CoT) 37.65 CLIP+T5 34.56 1.59 1. 1.52 Table 5: Comparison of text encoder fusion strategies. The input of the CLIP text encoder is the caption. The CLAP score means the value of CLAPCoT. Method FD KLPaSST KLPaNNs DeSync CLAP 1.40 1.35 1.32 0.48 0. 0.46 0.41 0.44 0.46 Multi-Modal Integration Mechanisms. We investigate different multi-modal integration mechanisms for video and audio before feeding into the single-stream transformer and the results are displayed in Table 6. We find that (1) the element-wise addition of video and audio features performs better than audio-only input with weak-supervision global conditioning, especially for synchronization metric DeSync Table 6: Comparison of different multi-modal integration mechanisms between video and audio features. Integration FD KLPaSST KLPaNNs DeSync CLAP 37.13 audio only linear video 38.96 gated video 34.56 1.58 1.58 1.52 1.37 1.38 1. 0.50 0.46 0.46 0.43 0.45 0.46 8 Figure 4: Qualitative Comparison: Left: Spectrograms for car door movement sequence (closed opened closed), showing ThinkSounds precise alignment of each door sound versus the baselines premature opening effect. Right: Spectrograms for grassy-field pheasant scene (ambient bird calls wing-flap chirp ambient calls), illustrating ThinkSounds accurate detection and timing of the transient chirp compared to the baselines omission or delay. decreasing from 0.50 to 0.46. (2) The gated fusion mechanism outperforms simple element-wise addition and audio-only input across all metrics. 5.4 Case Study In our qualitative analysis, we compare spectrograms of audio generated by ThinkSound and those produced by baselines, as illustrated in Figure 4. We make the following observations: (1) As demonstrated in case 1, car door sequenceclosedopenedclosedis accurately reflected by ThinkSound, whereas the baseline models erroneously introduce an extra opening sound at the start. This highlights ThinkSounds ability, via CoT prompting, to track temporal and causal event order in structured scenes. (2) In case 2, pheasant moving in grassy fieldaccompanied by ambient bird calls, suddenly flapping its wings and emitting sharp chirp before returning to background noiseis faithfully reproduced by ThinkSound. The baselines, however, often miss or delay this brief chirp. These cases underscore ThinkSounds enhanced temporal reasoning and sensitivity to subtle visual cues, resulting in more precise and context-aware audio synthesis."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper presented ThinkSound, novel CoT reasoning framework for audio generation and editing that decomposed complex audio generation into three interpretable and user-centric stages. Our evaluations showed that ThinkSound outperformed state-of-the-art methods, producing contextually appropriate and temporally precise soundscapes. The frameworks interactive nature enabled users to refine generated audio through intuitive pointing and natural language instructions, addressing the gap between creative intent and automated generation. While challenges remained in capturing nuanced acoustic properties, our AudioCoT dataset and training strategy established foundation for future research in intelligent audio generation with applications across film, gaming, and social media. Future work we will explore incorporating physical acoustic modeling and developing more sophisticated reasoning capabilities for complex multi-object sound interactions."
        },
        {
            "title": "References",
            "content": "Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. Chen, H., Xie, W., Vedaldi, A., and Zisserman, A. Vggsound: large-scale audio-visual dataset. arXiv preprint arXiv:2004.14368, 2020. Chen, K., Du, X., Zhu, B., Ma, Z., Berg-Kirkpatrick, T., and Dubnov, S. Hts-at: hierarchical token-semantic audio transformer for sound classification and detection. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, 2022. Chen, Z., Seetharaman, P., Russell, B., Nieto, O., Bourgin, D., Owens, A., and Salamon, J. Videoguided foley sound generation with multimodal controls. arXiv preprint arXiv:2411.17698, 2024. Cheng, H. K., Ishii, M., Hayakawa, A., Shibuya, T., Schwing, A., and Mitsufuji, Y. Taming multimodal joint training for high-quality video-to-audio synthesis. arXiv preprint arXiv:2412.15322, 2024a. Cheng, Z., Leng, S., Zhang, H., Xin, Y., Li, X., Chen, G., Zhu, Y., Zhang, W., Luo, Z., Zhao, D., et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024b. Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. Chu, Y., Xu, J., Yang, Q., Wei, H., Wei, X., Guo, Z., Leng, Y., Lv, Y., He, J., Lin, J., et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. Copet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y., and Défossez, A. Simple and controllable music generation. Advances in Neural Information Processing Systems, 36, 2024. Cramer, A. L., Wu, H.-H., Salamon, J., and Bello, J. P. Look, listen, and learn more: Design choices for deep audio embeddings. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 38523856. IEEE, 2019. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Evans, Z., Carr, C., Taylor, J., Hawley, S. H., and Pons, J. Fast timing-conditioned latent audio diffusion. arXiv preprint arXiv:2402.04825, 2024. Floridi, L. and Chiriatti, M. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 30:681694, 2020. Fonseca, E., Pons, J., Favory, X., Font, F., Bogdanov, D., Ferraro, A., Oramas, S., Porter, A., and Serra, X. Freesound datasets: platform for the creation of open audio datasets. In Cunningham, S. J., Duan, Z., Hu, X., and Turnbull, D. (eds.), Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017, Suzhou, China, October 23-27, 2017, pp. 486493, 2017. Gemmeke, J. F., Ellis, D. P., Freedman, D., Jansen, A., Lawrence, W., Moore, R. C., Plakal, M., and Ritter, M. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 776780. IEEE, 2017. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 10 Hershey, S., Ellis, D. P., Fonseca, E., Jansen, A., Liu, C., Moore, R. C., and Plakal, M. The benefit of temporally-strong labels in audio event classification. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 366370. IEEE, 2021. Huberman-Spiegelglas, I., Kulikov, V., and Michaeli, T. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1246912478, 2024. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Iashin, V., Xie, W., Rahtu, E., and Zisserman, A. Synchformer: Efficient synchronization from sparse cues. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 53255329. IEEE, 2024. Kilgour, K., Zuluaga, M., Roblek, D., and Sharifi, M. Frechet audio distance: metric for evaluating music enhancement algorithms. arXiv preprint arXiv:1812.08466, 2018. Kim, C. D., Kim, B., Lee, H., and Kim, G. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 119132, 2019. Kong, Q., Cao, Y., Iqbal, T., Wang, Y., Wang, W., and Plumbley, M. D. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:28802894, 2020. Koutini, K., Schlüter, J., Eghbal-Zadeh, H., and Widmer, G. Efficient training of audio transformers with patchout. arXiv preprint arXiv:2110.05069, 2021. Labs, B. F. Flux. https://github.com/black-forest-labs/flux, 2024. Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., and Qiao, Y. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. Liang, Y., Chen, Z., Ding, C., and Di, X. Deepsound-v1: Start to think step-by-step in the audio generation from videos, 2025. URL https://arxiv.org/abs/2503.22208. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Liu, H., Huang, R., Lin, X., Xu, W., Zheng, M., Chen, H., He, J., and Zhao, Z. Vit-tts: visual text-to-speech with scalable diffusion transformer. arXiv preprint arXiv:2305.12708, 2023a. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023b. Liu, H., Huang, R., Liu, Y., Cao, H., Wang, J., Cheng, X., Zheng, S., and Zhao, Z. Audiolcm: Efficient and high-quality text-to-audio generation with minimal inference steps. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 70087017, 2024b. Liu, H., Wang, J., Huang, R., Liu, Y., Lu, H., Xue, W., and Zhao, Z. Flashaudio: Rectified flows for fast and high-fidelity text-to-audio generation. arXiv preprint arXiv:2410.12266, 2024c. Liu, H., Wang, J., Huang, R., Liu, Y., Xu, J., and Zhao, Z. MEDIC: zero-shot music editing with disentangled inversion control. CoRR, abs/2407.13220, 2024d. doi: 10.48550/ARXIV.2407.13220. URL https://doi.org/10.48550/arXiv.2407.13220. Liu, H., Yuan, Y., Liu, X., Mei, X., Kong, Q., Tian, Q., Wang, Y., Wang, W., Wang, Y., and Plumbley, M. D. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024e. 11 Liu, H., Luo, T., Jiang, Q., Luo, K., Sun, P., Wan, J., Huang, R., Chen, Q., Wang, W., Li, X., et al. Omniaudio: Generating spatial audio from 360-degree video. arXiv preprint arXiv:2504.14906, 2025. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. Lu, H., Liu, W., Zhang, B., Wang, B., Dong, K., Liu, B., Sun, J., Ren, T., Li, Z., Yang, H., et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. Luo, S., Yan, C., Hu, C., and Zhao, H. Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models. Advances in Neural Information Processing Systems, 36, 2024. Mo, S., Shi, J., and Tian, Y. Text-to-audio generation synchronized with videos. arXiv preprint arXiv:2403.07938, 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Pinheiro Cinelli, L., Araújo Marins, M., Barros da Silva, E. A., and Lima Netto, S. Variational autoencoder. In Variational methods for machine learning with applications to deep networks, pp. 111149. Springer, 2021. Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C.- Y., Chuang, C.-Y., et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. URL http://jmlr.org/papers/v21/20-074.html. Ravi, N., Gabeur, V., Hu, Y.-T., Hu, R., Ryali, C., Ma, T., Khedr, H., Rädle, R., Rolland, C., Gustafson, L., Mintun, E., Pan, J., Alwala, K. V., Carion, N., Wu, C.-Y., Girshick, R., Dollár, P., and Feichtenhofer, C. Sam 2: Segment anything in images and videos, 2024. URL https: //arxiv.org/abs/2408.00714. Ren, T., Jiang, Q., Liu, S., Zeng, Z., Liu, W., Gao, H., Huang, H., Ma, Z., Jiang, X., Chen, Y., Xiong, Y., Zhang, H., Li, F., Tang, P., Yu, K., and Zhang, L. Grounding dino 1.5: Advance the \"edge\" of open-set object detection, 2024. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Rubenstein, P. K., Asawaroengchai, C., Nguyen, D. D., Bapna, A., Borsos, Z., Quitry, F. d. C., Chen, P., Badawy, D. E., Han, W., Kharitonov, E., et al. Audiopalm: large language model that can speak and listen. arXiv preprint arXiv:2306.12925, 2023. Viertola, I., Iashin, V., and Rahtu, E. Temporally aligned audio for video with autoregression. arXiv preprint arXiv:2409.13689, 2024. Wang, H., Ma, J., Pascual, S., Cartwright, R., and Cai, W. V2a-mapper: lightweight solution for vision-to-audio generation by connecting foundation models. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 1549215501, 2024a. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. 12 Wang, X., Wang, Y., Wu, Y., Song, R., Tan, X., Chen, Z., Xu, H., and Sui, G. Tiva: Timealigned video-to-audio generation. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 573582, 2024c. Wang, Y., Ju, Z., Tan, X., He, L., Wu, Z., Bian, J., et al. Audit: Audio editing by following instructions with latent diffusion models. Advances in Neural Information Processing Systems, 36: 7134071357, 2023. Wang, Y., Guo, W., Huang, R., Huang, J., Wang, Z., You, F., Li, R., and Zhao, Z. Frieren: Efficient video-to-audio generation network with rectified flow matching. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024d. URL https://openreview. net/forum?id=prXfM5X2Db. Wang, Y., Wu, S., Zhang, Y., Yan, S., Liu, Z., Luo, J., and Fei, H. Multimodal chain-of-thought reasoning: comprehensive survey. arXiv preprint arXiv:2503.12605, 2025. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Wu, S., Fei, H., Qu, L., Ji, W., and Chua, T.-S. Next-gpt: Any-to-any multimodal llm. In Forty-first International Conference on Machine Learning, 2024. Wu*, Y., Chen*, K., Zhang*, T., Hui*, Y., Berg-Kirkpatrick, T., and Dubnov, S. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, 2023. Xie, Z., Yu, S., He, Q., and Li, M. Sonicvisionlm: Playing sound with vision language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2686626875, 2024. Xing, Y., He, Y., Tian, Z., Wang, X., and Chen, Q. Seeing and hearing: Open-domain visual-audio generation with diffusion latent aligners. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 71517161. IEEE, 2024. doi: 10.1109/CVPR52733.2024.00683. URL https://doi.org/10.1109/CVPR52733. 2024.00683. Xu, H., Xie, S., Tan, X. E., Huang, P.-Y., Howes, R., Sharma, V., Li, S.-W., Ghosh, G., Zettlemoyer, L., and Feichtenhofer, C. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023. Xu, M., Li, C., Tu, X., Ren, Y., Chen, R., Gu, Y., Liang, W., and Yu, D. Video-to-audio generation with hidden alignment. arXiv preprint arXiv:2407.07464, 2024. Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and Wang, L. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1):1, 2023. You, Y., Wu, X., and Qu, T. Ta-v2a: Textually assisted video-to-audio generation. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2025. Zhang, Y., Gu, Y., Zeng, Y., Xing, Z., Wang, Y., Wu, Z., and Chen, K. Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds. arXiv preprint arXiv:2407.01494, 2024. Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and Smola, A. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023."
        },
        {
            "title": "A AudioCoT Dataset Details",
            "content": "A.1 Data Collection and Preprocessing To ensure high data quality and consistency, we employ comprehensive preprocessing pipeline. We begin by removing silent audio-video clips to retain only those with meaningful content. For the 13 AudioSet subset (Gemmeke et al., 2017), we further exclude segments containing human voices based on tag information, as our focus is on non-speech audio. All audio-video clips are then segmented into fixed-length intervals of 9.1 seconds, with any shorter clips discarded to maintain uniformity. To achieve balanced dataset, we maintain an approximately 1:1 ratio between music and sound effect samples, ensuring equal representation of both categories. A.2 Quality Control for Automated Data Pipeline To enhance the effectiveness of CoT reasoning in preserving audio characteristics while integrating visual context, we implemented comprehensive multi-stage quality control pipeline: Stage 1: Audio-Text Alignment Filtering We employ systematic approach to ensure highquality audio-CoT pairs. First, we calculate the CLAP score between each audio sample and its corresponding CoT description to quantify semantic alignment. For pairs exhibiting low CLAP scores (below 0.2), we regenerate the CoT using an enhanced prompt specifically designed to emphasize audio characteristics and features. After regeneration, we recalculate the CLAP score to assess improvement. Audio samples that continue to demonstrate poor alignment (persistent low CLAP scores) are excluded from the dataset to maintain quality standards. Stage 2: Object Tracking Consistency To ensure reliable audio-visual correspondence, we retain only those video sequences containing at least one Region of Interest (ROI) that remains consistently visible throughout the entire duration. Videos with objects that disappear from view or exhibit inconsistent tracking are filtered out. This criterion ensures that our dataset maintains high-quality visual references for audio generation tasks, providing consistent visual anchors for the audio generation process. Stage 3: Semantic Pairing of Audio Components For tasks requiring paired audio components, we utilize GPT-4.1-nano to analyze tag categories from VGGSound (Chen et al., 2020) based on two critical criteria. First, we ensure semantic distinctiveness, where tags must be sufficiently distinct to avoid confusion during audio extraction and removal tasks. Second, we verify contextual plausibility, ensuring that the co-occurrence of paired sounds is contextually reasonable within the same acoustic scene. This balanced approach ensures that our audio pairs are both semantically meaningful and practically useful for audio generation tasks. Human Verification Protocol To validate our automated filtering processes, we implement rigorous manual review at each pipeline stage. No less than 5% of the total data volume undergoes human inspection to ensure quality. This verification step helps validate the effectiveness of our automated filtering criteria and ensures the overall reliability and quality of our dataset. The human reviewers assess both the technical aspects of alignment and the perceptual quality of the audio-visual correspondence. When samples fail human verification, they are immediately removed from the dataset. Additionally, if the human rejection rate for any filtering criterion exceeds 5%, we recalibrate the corresponding automated filtering parameters and reprocess the entire batch to maintain dataset integrity. This feedback loop between automated filtering and human verification ensures continuous improvement of our quality control pipeline. Table 7: Overview of datasets used in our work. Dataset Modality Text Format Hours VGGSound (Chen et al., 2020) AudioSet (Gemmeke et al., 2017) AudioSet-SL (Hershey et al., 2021) Freesound (Fonseca et al., 2017) AudioCaps (Kim et al., 2019) BBC Sound Effects 4 Total Hours Audio-Video Audio-Video Audio-Text Audio-Text Audio-Text Audio-Text Caption Caption Caption Caption Caption Tags 453.6 287.5 262.6 1286.6 112.6 128.9 2531.8 A.3 Benchmark Construction We evaluate the performance of ThinkSound on three different tasks: video-to-audio generation, object-focused audio generation, and audio editing. For the video-to-audio generation task, we use the VGGSound test set as the in-distribution evaluation set while the MovieGen Audio Bench is the out-of-distribution evaluation set. For the VGGSound test set, we use the same quality filtering protocol as our training data preparation. Given that our primary focus is on video-to-sound/music generation, we construct three different difficulty levels based on the complexity of the audio-visual relationships. Specifically, we distinguish the difficulty levels based on multi-dimensional scoring system that evaluates: Semantic Consistency: The alignment between the audio and the visual content evaluated by Imagebind score (0.3+ for easy, 0.25-0.3 for medium, 0.2-0.25 for hard), and CLAP score between audio and CoT (0.4+ for easy, 0.3-0.4 for medium, 0.2-0.3 for hard). Temporal Synchronization: The degree of synchronization between visual events and corresponding sounds evaluated by DeSync score (0-0.3 for easy, 0.3-0.6 for medium, 0.6+ for hard). Acoustic Scene Complexity: The audio events numbers (one dominant sound for easy, 2-3 distinct sounds for medium, multiple overlapping sounds for hard) According to the above evaluation criteria, we input the scores and criteria into GPT-4.1-nano to generate the difficulty level for each sample. The final difficulty assignment follows tertile distribution: the lowest-scoring third is classified as \"easy,\" the middle third as \"medium,\" and the highest-scoring third as \"hard.\" This stratified approach ensures balanced representation across difficulty levels while maintaining meaningful distinctions in task complexity. For each difficulty level, we construct benchmark subset containing around 2000 samples. For stages 2 and 3, we maintain methodological consistency with our training protocols while adapting the evaluation criteria to each tasks specific requirements. For stage 2, we select samples with clearly identifiable visual objects that produce distinct sounds, while for stage 3, we focus on samples with different audio categories suitable for manipulation tasks. Each evaluation subset contains approximately 2,000 samples."
        },
        {
            "title": "B Model Configurations and Architecture",
            "content": "B.1 Model Configurations ThinkSound consists of two primary components: hierarchical variational autoencoder (VAE) for audio compression and reconstruction, and flow-matching multimodal transformers. Variational Autoencoder The encoder consists of five convolutional blocks with channel multipliers [1, 2, 4, 8, 16] and strides [2, 4, 4, 8, 8], projecting the stereo waveform into 128-dimensional latent space. The decoder mirrors this architecture with transposed convolutions to reconstruct 64-dimensional latent representations back into the time-domain waveform. Multimodal Diffusion Transformer ThinkSound employs an enhanced Multi-modal Diffusion Transformer (MM-DiT) with hidden size dimension of 1024. It comprises 14 multi-stream transformer layers and 7 single-stream transformer layers, with 16 attention heads. We further attach our different model scale parameters for reference. We use the large model by default. Table 8: Diffusion Transformer Configurations at Different Model Sizes Model Scale Large Medium Small Hidden Depth Attention Multistream Singlestream Size 1024 768 512 Heads Layers Layers 21 21 18 16 12 8 14 14 12 7 7 6 Total Parameters 1.3B 724M 533M 15 B.2 Model Architecture The architecture of multistream transformers is depicted in Figure 5. Figure 5: Multi-stream blocks: Fv is the video features, Ft is the text features, xt is the audio latents, and cg denotes the global condition."
        },
        {
            "title": "C Evaluation",
            "content": "C.1 Objective Metrics To comprehensively evaluate the generated audio, we adopt set of objective metrics targeting different aspects: perceptual quality, semantic consistency, temporal alignment, and cross-modal correspondence. Feature Distribution Alignment: We project both generated and reference audio into the OpenL3 embedding space Cramer et al. (2019); Evans et al. (2024) and compute the Fréchet Distance (FD) Kilgour et al. (2018); Copet et al. (2024) to assess the similarity between their distributional statistics. We chose OpenL3 because it accepts signals of up to 48kHz while VGGish operates at 16kHz, which is more suitable for our 44kHz audio. Following the previous work Evans et al. (2024), we extend the FD to evaluate stereo signals by projecting left and right channels separately and then averaging the results. Moreover, to evaluate whether the generated audio matches the reference in terms of its distribution, we compute the Kullback-Leibler (KL) DivergenceCopet et al. (2024) between class probability distributions predicted by the PaSST modelKoutini et al. (2021) and PaNNs model (Kong et al., 2020) as classifiers. Temporal Alignment: To evaluate the synchronization between generated audio and its corresponding video, we adopt the DeSync score predicted by the Synchformer model Iashin et al. (2024), following the practice of Cheng et al. (2024a). For each sample, we truncate the video to match the duration of the generated audio and compute the DeSync score using Synchformer, which operates with 4.8-second context window. Specifically, we extract both the first and last 4.8-second segments from each video-audio pair, calculate the DeSync score for each segment, and report the average as the final temporal alignment metric. Text-Audio Correspondence: To assess the semantic alignment between generated audio and textual prompts, we utilize the CLAP score Wu* et al. (2023); Chen et al. (2022), which measures similarity in shared audio-text embedding space. Specifically, we report CLAPcap for evaluating alignment with the original video captions and CLAPCoT for alignment with our constructed CoT descriptions. As discussed in Section 5.2, the original VGGSound captions are often low quality and yield lower CLAP scores, whereas our CoT annotations provide richer semantic detail and achieve higher alignment. Consequently, we primarily use the CoT-audio 16 alignment metric in our evaluations, except in Table 1 where caption-based alignment is also reported. C.2 Subjective Metrics Our subjective evaluation framework employs the Mean Opinion Score (MOS) methodology along two critical dimensions to comprehensively assess the generated audio: Audio Quality Assessment (MOS-Q) We evaluate the intrinsic perceptual quality of generated audio through rigorous assessment protocol where participants are instructed to focus on four specific aspects: Clarity: The absence of unwanted artifacts, distortions, or noise Naturalness: How realistic and non-synthetic the audio sounds Fidelity: The richness and accuracy of acoustic characteristics Overall impression: The holistic listening experience Each listener rates these qualities using standard 5-point Likert scale (1: Poor, 2: Fair, 3: Good, 4: Very Good, 5: Excellent). The final MOS-Q score for each audio sample represents the average rating across all evaluators, providing robust measure of perceived audio quality. Semantic and Temporal Alignment Assessment (MOS-A) To evaluate the cross-modal coherence between generated audio and visual content, we assess both semantic relevance and temporal synchronization (we also provide CoT text as the auxiliary information for semantic alignment): Semantic alignment: How well the audio content matches the objects, actions, and environment depicted in the video Temporal alignment: How accurately sound events correspond to visual events in time Participants judge the alignment according to three categories on the same 5-point scale: Fully aligned (4-5 points): Complete semantic correspondence with precise temporal synchronization Mostly aligned (2.5-3.9 points): Good semantic match with occasional minor temporal misalignments Partially aligned (1-2.4 points): Noticeable discrepancies in either semantic content or temporal synchronization Evaluation Protocol To ensure evaluation reliability and consistency, we implemented the following protocol: All assessments were conducted in controlled in-person sessions with standardized audio equipment 15 raters with normal hearing ability were recruited and briefed on the evaluation criteria Each rater evaluated random subset of 50 video-audio pairs from our test collection Samples were presented in randomized order to prevent ordering bias Reference examples of each quality level were provided before the evaluation sessions Raters were given sufficient time to carefully evaluate each sample"
        },
        {
            "title": "D Additional Quantitative Results",
            "content": "D.1 Details on Video-to-Audio Comparison For the results in Table 1, we reproduce the results of Seeing and Hearing (Xing et al., 2024), V-AURA (Viertola et al., 2024), FoleyCrafter (Zhang et al., 2024), and MMAudio (Cheng et al., 17 2024a) using the official code and pre-trained models. For the other baselines, we use the generated samples provided by the authors, i.e., Frieren, V2A-Mapper, and Movie Gen (Polyak et al., 2024). Furthermore, the CLAPcap scores of MMAudio, MovieGen, and ThinkSound are 0.43, 0.44, and 0.49, respectively. D.2 Impact of Model Size We compare three model size of ThinkSound: Large (1.3B), Medium (724M), and Small (533M). The Large model achieves the best performance across all metrics as shown in Table 9. These results demonstrate that model capacity significantly enhances audio quality and improves alignment with ground truth distribution. As model size decreases, performance degrades substantially, highlighting the necessity of adequate model capacity for effective audio generation. Table 9: Impact of model size results."
        },
        {
            "title": "Size",
            "content": "FD KLPaSST KLPaNNs DeSync CLAPCoT Small 43.26 Medium 37.62 34.56 Large 0.50 0.47 0.46 1.39 1.34 1.32 1.64 1.56 1.52 0.43 0.44 0.46 D.3 Performance across different difficulty levels To better validate the performance of our CoT-Guided generation, we also report the results in the video-to-audio generation of different difficulty levels. We illustrate the construction of different difficulty levels in Section A.3. The results are shown in Table 10, and we can conclude that (1) As expected, the performance of all models decreases as the difficulty level increases, and (2) Our CoT-Guided generation outperforms other baselines across all difficulty levels. Table 10: Performance across different difficulty levels. Difficulty Easy Medium Hard FD KLPaSST KLPaNNs DeSync CLAPCoT 31.32 35.45 48.78 0.52 0.49 0. 0.42 0.46 0.57 1.35 1.46 1.63 1.16 1.31 1.40 D.4 Performance Comparisons between coarse-grained and fine-grained CoT To further validate the effectiveness of our fine-grained CoT, we compare the performance of our model with the coarse-grained CoT. The results are shown in Table 11. We can conclude that our fine-grained CoT outperforms the coarse-grained CoT across all metrics. Table 11: Performance comparisons between coarse-grained and fine-grained CoT. Granularity Coarse Fine FD KLPaSST KLPaNNs DeSync CLAPCoT 42.72 34.56 0.52 0.46 1.58 1.52 0.34 0. 1.41 1."
        },
        {
            "title": "E Limitation and Future",
            "content": "While the current MLLMs are capable of strong understanding and reasoning of semantic information, they still have limitations in understanding the precise temporal and spatial information of video. For example, in the case of locating the exact timestamp of the sound event, MLLMs often fail to provide accurate results or provide wrong results. Whats more, the current open-source video-audio datasets for audio generation are limited in diversity and coverage, which may lack rare or culturally specific sound events. In the future, we will continue to explore more diverse and comprehensive datasets to improve the performance of our model. Furthermore, we will explore more effective methods to improve the temporal and spatial alignment of generated audio."
        },
        {
            "title": "F Potential Negative Societal Impacts",
            "content": "ThinkSound carries potential risks if misused. Malicious actors could exploit the system to generate fake audio for synthetic media, thereby contributing to the spread of misinformation. Moreover, if the training data underrepresents certain cultures or environments, the model may unintentionally amplify biasesfor instance, by reinforcing stereotypes or misassociating sounds with particular demographic groups. F.1 Ethical Considerations The dataset used in this research is strictly for academic and non-commercial purposes. We implemented several measures to ensure compliance with ethical standards, as follows. Data Transparency and Anonymization. We only provide ASR transcripts after rigorous text anonymization processes, visual features of video clips, our annotations, and links to the original videos, to ensure transparency regarding the data sources and their usage while maintaining anonymity. Authorization. Any personal data should be used only with express authorization, ensuring lawful and fair processing in accordance with applicable laws."
        },
        {
            "title": "G Safeguards",
            "content": "We used diverse training dataset covering wide range of acoustic scenes to minimize reinforcing stereotypes or incorrect associations between sounds and specific demographic groups. The model will be released in stages to better assess its impact and improve safeguards. However, once the model is openly released, we cannot control how others use it. Therefore, we provide clear usage guidelines to encourage responsible use and help mitigate potential misuse."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology (HKUST)",
        "Tongyi Lab, Alibaba Group",
        "Zhejiang University"
    ]
}