{
    "paper_title": "Identifying Sensitive Weights via Post-quantization Integral",
    "authors": [
        "Yuezhou Hu",
        "Weiyu Huang",
        "Zichen Liang",
        "Chang Chen",
        "Jintao Zhang",
        "Jun Zhu",
        "Jianfei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Serving Large Language Models (LLMs) is costly. However, post-training weight quantization can address this problem by both compressing their sizes for limited memory and saving bandwidth for acceleration. As not all weight dimensions are equally important, those methods typically rely on a sensitivity metric, which indicates the element-wise influence of weights on loss function and is used to preprocess original weights for better quantization. In this work, we conduct an empirical study on the accuracy of the sensitivity metric, and find that existing gradient and Hessian based metrics are very inaccurate: they underestimate quantization's impact on the loss function by orders of magnitude, mainly due to the small convergence radius of local 2nd order approximation, \\ie, gradient and Hessian term in Taylor's formula. To tackle this problem, we propose Post-quantization Integral (PQI), an accurate metric to estimate posterior sensitivity in a fine-grained manner. To leverage this accurate metric, we further propose ReQuant, a simple yet powerful framework that mainly consists of two Dense-and-Sparse detach components: self-adaptive outlier selection and step-wise significant weights detach. Results show that ReQuant boosts state-of-the-art post-training quantization methods, with a pronounced improvement of 2.66 perplexity gain on Llama 3.2 1B with QTIP."
        },
        {
            "title": "Start",
            "content": "Identifying Sensitive Weights via Post-quantization Integral Yuezhou Hu 1 Weiyu Huang 1 Zichen Liang 1 Chang Chen 1 Jintao Zhang 1 Jun Zhu 1 Jianfei Chen"
        },
        {
            "title": "Abstract",
            "content": "Serving Large Language Models (LLMs) is costly. However, post-training weight quantization can address this problem by both compressing their sizes for limited memory and saving bandwidth for acceleration. As not all weight dimensions are equally important, those methods typically rely on sensitivity metric, which indicates the element-wise influence of weights on loss function and is used to preprocess original weights for better quantization. In this work, we conduct an empirical study on the accuracy of the sensitivity metric, and find that existing gradient and Hessian based metrics are very inaccurate: they underestimate quantizations impact on the loss function by orders of magnitude, mainly due to the small convergence radius of local 2nd order approximation, i.e., gradient and Hessian term in Taylors formula. To tackle this problem, we propose Post-quantization Integral (PQI), an accurate metric to estimate posterior sensitivity in fine-grained manner. To leverage this accurate metric, we further propose ReQuant, simple yet powerful framework that mainly consists of two Dense-and-Sparse detach components: selfadaptive outlier selection and step-wise significant weights detach. Results show that ReQuant boosts state-of-the-art post-training quantization methods, with pronounced improvement of 2.66 perplexity gain on Llama 3.2 1B with QTIP. 5 2 0 2 8 2 ] . [ 1 1 0 9 1 0 . 3 0 5 2 : r 1. Introduction The past decade has witnessed the thriving of Large Language Models, which have exhibited their great potential in various domains, such as reasoning (Wei et al., 2022), code generation (Hui et al., 2024) and instruction following (Ouyang et al., 2022). However, as LLMs are becoming *Equal contribution 1Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University. Correspondence to: Jianfei Chen <jianfeic@tsinghua.edu.cn>. 1 larger, efficiently serving or even simply launching them becomes challenge, especially for those edge devices with limited memory. One possible solution to this is weight quantization, which converts high-precision LLM into low-precision counterpart. As the main bottleneck of LLM decoding is loading weights from memory to chip, weight quantization also proportionally accelerate decoding. Many previous methods have demonstrated the possibility to compress LLM into lower precision. Specifically, posttraining quantization (PTQ) methods (Frantar et al., 2023; Lin et al., 2024b; Xiao et al., 2023; Chee et al., 2023; Kim et al., 2024; Lin et al., 2024a; Tseng et al., 2024b; Zhang et al., 2025; 2024) are popular since they do not require the expensive training procedure. Although most PTQ methods are feasible with models such as OPT (Zhang et al., 2022) and Llama 2 (Touvron et al., 2023), the effectiveness of these methods on state-of-the-art models in preserving accuracy remains an open question. Empirical study (Kumar et al., 2024) reveals that the quantization difficulty depends on the ratio of dataset size and model size. As recent LLMs, such as Phi-4 (Abdin et al., 2024), are typically trained with more than ten trillion tokens, it is even challenging to compress them to 4-bit without accuracy degradation. Existing low-bit quantization methods for LLMs exploit the unequal importance of weight dimensions, necessitating the use of sensitivity metric to quantify their impact. Extensive experiments have demonstrated that certain dimensions are more critical than others, where even slight modifications to these weights can cause significant distortions in the models output (Lin et al., 2024b). Therefore, an accurate sensitivity metric for weight dimensions is essential for the effectiveness of quantization algorithm. Consider quantizing model with weights RD using calibration set D. PTQ method typically consists of two stages. In the first stage, sensitivity metric is calculated on the calibration set, which is typically vector indicating the element-wise1 importance: = S(M, D) RD. In the second stage, the sensitivity vector is used to quantize w: = Q(w, v), where is the quantization process, and RD is the low-precision weight vector. For example, 1For channel-wise sensitivity metrics such as AWQ (Lin et al., 2024b), the definition of sensitivity can also be extended to be element-wise by simple broadcasting along the other dimension. Identifying Sensitive Weights via Post-quantization Integral the simplest technique is to store the most sensitive elements in full precision to preserve accuracy. For another, scale can be applied to salient weights to diminish the relative quantization error. In this work, we empirically evaluate the accuracy of sensitivity metrics in predicting the change in the loss function caused by weight quantization. Unfortunately, none of the existing sensitivity metrics is sufficiently accurate, in the sense that they cannot predict the change of loss function caused by weight quantization. We identify two key reasons why previous methods fail to accurately estimate sensitivity: 1) Small convergence radius: The complicated loss landscape of LLM makes the local gradient and Hessian based approximation only valid in very small region near w. Specifically, the quantized weight falls outside the convergence radius. 2) Misalignment of Sensitivity between and w: While previous methods only calculate the sensitivity based on w, sensitivity results might change: previously sensitive weights may lose importance after quantization, and other non-sensitive weights may emerge as sensitive. To overcome these difficulties, we introduce PQI, novel sensitivity metric to estimate the influence of each quantized weight. First, PQI only leverages the local continuity of the model, which can be easily achieved. Additionally, both and are considered to calculate v, making it more accurate. Our contributions are summarized as follows: We propose Post-quantization Integral (PQI), an accurate sensitivity metric designed to compute the element-wise importance of quantized model. We introduce ReQuant, pipeline that utilizes PQI to enhance the quality of quantized model by Dense-andSparse (Li et al., 2023b; Kim et al., 2024) decomposition. When applied to Llama 3.2 1B model, ReQuant can reduce the perplexity up to 2.6 and enhance MATH (Hendrycks et al., 2021) few-shot by nearly 3%, compared to SqueezeLLM (Kim et al., 2024) and QTIP (Tseng et al., 2024b) baselines. 2. Related Work Post-training Quantization Early attempts to quantize transformers (Frantar & Alistarh, 2022; Frantar et al., 2023) utilize Hessian to calibrate quantized weights. Other studies focus on activation outliers (Dettmers et al., 2022a; Xiao et al., 2023; Lin et al., 2024b). However, all of these studies either suffer from suboptimal accuracy or low compression ratio problems. Another method is to resolve outliers via rotation (Ma et al., 2024; Shao et al., 2024a; Lin et al., 2024a; Ashkboos et al., 2024; Liu et al., 2024b). However, those methods require thorough search of the rotation matrices, which brings extra costs. Codebook-based algorithms (Tseng et al., 2024a; Kim et al., 2024; Tseng et al., 2024b) compress the model into pre-computed lookup table and its indices. Other lookup-free methods (Dettmers et al., 2023) can also be categorized as pre-established codebook. Constructing the codebook is non-trivial, since the distribution of the codebook needs to be aligned with weight distributions. Thus, most of the intuitive codebooks can only reach suboptimal performance. Other Quantization Methods Quantizaion Aware Training (QAT) methods dynamically quantize weights during training rather than after training (Liu et al., 2024a; Wang et al., 2024). However, this technique is still immature and requires considerable training cost. In contrast to accuracycentric studies, others focus on activation and KV cache quantization (Liu et al., 2023; Hooper et al., 2024; Lin* et al., 2024), which strikes balance between accuracy and fast inference. Since outliers in activations are equally or even more important than in weights, those methods typically adopt naive grouping-based quantization, which can maximize inference speed with higher accuracy loss than post-training quantization methods. Some other studies investigate data parallelism (Jia et al., 2024), adapters for quantized models (Dettmers et al., 2023; Li et al., 2024; Xia et al., 2024), and low-bit optimizers (Dettmers et al., 2022b; Li et al., 2023a), etc. These studies involve model pre-training and are orthogonal to our study. 3. Background In this section, we first present an overview of PTQ methods. Afterward, we revisit common sensitivity metrics in them. 3.1. Post-training Quantization We classify PTQ methods into two primary categories based on their quantization strategy. Grouping-based Quantization Let {wi}1id be group splits of w, and corresponding sensitivity {vi}1id. Each group is quantized with its own quantization step si: ˆwi = preprocess(wi, vi), wi = si round2int( ˆwi si ), si = max wi 2N 1 , where preprocess is basically transformation that makes sensitive elements more accurate to quantize, and round2int performs element-wise rounding. For example, in GPTQ (Frantar et al., 2023), the preprocessing step is performed by dynamically adjusting Hessian and quantized weights, and in AWQ, by multiplying channel-wise activations. Codebook-based Quantization Methods based on codebooks relie on pre-computed lookup table RK, which is generally vector of possible high-precision values 2 Identifying Sensitive Weights via Post-quantization Integral built from and v, and is obtained by rounding to the nearest possible entry in T: Hessian: LeCun et al. (1989) first propose to use Hessian to calculate the change in loss: = build(w, v) = round2nearest(w, T). in SqueezeLLM, is with K-means: For example, mint(t w)H(t w), with the clustering center vector serving as entries in T. RDD is the diagonal Hessian matrix and is the non-zero elements in it: = diag(H). In QTIP, the table is constructed based on Gaussian distribution. Note that some methods such as QLoRA (Dettmers et al., 2023) do not require sensitivity metric, and build the lookup table merely on weight distributions. 3.2. Sensitivity Metrics (cid:80) i=1 (w; x[i]), with = {x[i]}n The sensitivity is the metric used to signify the importance of each weight. Of note, we use to denote the loss function, to denote the optimization target: (w) = 1 i=1 denoting calibration set of samples. An ideal sensitivity metric should accurately predict the change of loss function ( w) (w), which we denote as . Theoretically, under ideal conditions with Kth-order continuously differentiable partial derivatives around w, can be expressed with Taylor series: = (cid:88) k=1 1 k! (cid:32) (cid:88) ( wi wi) i=1 (cid:33)k wi (w) + o( wK). By keeping the first two terms, can be reduced as: wF (w)( w) + ( w)H( w). (1) 1 2 Taking the special cases of Eq. (1) derives three metrics that are widely used: gradient, activation, and Hessian. It is worth noticing that the three metrics are calculated from the dense model before quantization. This means that we can only make use of the original weights to predict loss function change. Gradient: Shao et al. (2024b) propose to use gradient as sensitivity: = wF (w), where they choose to keep weights with the largest gradients in 16 bits. This approach retains the first-order term in Taylors formula: wF (w)( w). (2) Activation: Lin et al. (2024b) propose to use input activation of each linear layer as its sensitivity. Note that activation equals gradient only in simple cases (one linear layer with L1 norm loss). Since LLM consists of multiple layers and involves non-linearity, activation only represents special case. To take advantage of this, they propose to apply different channel-wise scales to weights, thus recovering accuracy. 3 1 2 ( w)H( w), (3) since the gradient is close to zero when the model is well pre-trained. Following this line of research, later methods (Frantar et al., 2023; Kim et al., 2024; Ding et al., 2024) reduce the Hessian matrix to diagonal or block diagonal. Typically, to leverage Hessian requires to minimize Eq. (3), for example, by gradient descending (Tseng et al., 2024b) or K-means (Kim et al., 2024). 4. Analysis of Existing Sensitivity Metrics The core of sensitivity is to convert element-wise quantization errors into estimation of , thus analyzing the importance of each dimension. The sensitivity should accurately predict , so we can specially choose the quantized weight to minimize . Unfortunately, our empirical study shows that Eq.(1) is very inaccurate for LLMs. 4.1. Precision of the approximation We run simple experiment to quantize the 16-layer Llama 3.2 1B (Grattafiori et al., 2024) model with the simple codebook-based quantization method SqueezeLLM. We quantize the model to 4-bit with no detached outliers. We investigate two settings: (1) only quantize one layer, while leaving all other layers in full precision; (2) quantize all layers. For each setting, we compute the actual as well as the first order and second order term on the right hand side of Eq. (1). Here, the Hessian matrix is computed by Fisher information approximation: 1 fw(w, x[i])(fw(w, x[i])). The result is listed in Table 1. We have several observations, which may be surprisingly different from the common belief: (cid:80)n Observation 1: Second-order approximation is not accurate The most important observation is that the secondorder approximation Eq. (1) deviates significantly from the actual change of loss . Note that is always positive, which aligns with the intuition that the loss should increase after quantization. However, the sum of firstand secondorder terms is much smaller than , by more than an order of magnitude, implying that the second-order approximation significantly underestimates the change of loss. The sum is even negative for sum layers, meaning that secondorder approximation incorrectly predicts the decrease in loss after quantization. direct consequence is that, neither the gradient-based approximation Eq. (2) nor the Hessian-based approximation Eq. (3) is accurate. Moreover, as the firstand secondorder terms are both nonzero, both of them cannot be omitted. Identifying Sensitive Weights via Post-quantization Integral Table 1: First-order, second-order term and actual in Equation (1). Table 2: Actual with different λ. Quantized Layer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 All First-order Second-order Actual 7.10E-04 -6.58E-05 -3.21E-04 -5.04E-04 -7.00E-04 -6.29E-04 -2.04E-04 6.82E-05 5.75E-05 2.86E-04 -6.43E-04 8.29E-04 6.14E-04 1.30E-03 -2.52E-04 3.47E-04 8.92E-04 -5.98E-06 -4.54E-06 -3.66E-06 -3.68E-06 -3.75E-06 -3.61E-06 -3.63E-06 -3.60E-06 -3.97E-06 -4.10E-06 -3.66E-06 -2.95E-06 -2.80E-06 -2.65E-06 -2.84E-06 -5.05E-06 -6.05E6.88E-03 4.45E-03 3.67E-03 3.82E-03 3.72E-03 4.27E-03 5.06E-03 5.59E-03 6.85E-03 7.78E-03 6.57E-03 6.81E-03 5.83E-03 6.57E-03 5.30E-03 9.79E-03 1.00E-01 Observation 2: Layers are interdependent Another observation is that, the for quantizing each layer separately does not sum to the for quantizing all layers at once. While the first-order terms should be summable, all the higher-order terms can reflect correlations between layers. For example, the non-diagonal elements in the Hessian. This implies that separately computing each layers sensitivity may not work. 4.2. Convergence Radius The reason behind these counterintuitive observations is the convergence radius of Taylors expansion. Eq. (1) requires the LLM to exhibit ideal mathematical properties: the existence of Hessian, and closeness between and w. The definition of Hessian can be extended to non-differentiable activation functions such as ReLU (Li & Yuan, 2017). However, the closeness between and is not guaranteed. Specifically, the Taylor series is meaningful only when falls within the convergence radius of w. However, LLMs have billions of parameters, leading to substantial total quantization errors and large w distance. To verify this, we gradually move towards by an interpolation = (1λ)w+λ w. When λ 0, we can narrow the distance w 0. We list = (w) (w) with different λ in Table 2. Results show as λ decreases, the second-order approximation becomes more accurate, and the Hessian term diminishes. When λ 102, the gradient term alone is quite accurate. λ 1E-1 5E-2 1E-2 5E-3 1E-3 First-order Second-order Actual 8.92E-5 4.46E-5 8.92E-6 4.46E-6 8.92E-7 -6.05E-7 -1.51E-7 -6.05E-9 -1.51E-9 -6.05E-11 1.00E-3 2.73E-4 1.81E-5 6.68E-6 9.54E5. Post-quantization Integral: An Accurate"
        },
        {
            "title": "Sensitivity Metric",
            "content": "In this section, we propose novel sensitivity metric, Postquantization Integral (PQI), which can accurately predict the change in loss in fine-grained way of quantizing each dimension. Most importantly, PQI works by decomposing the distant path from to into many small fragments, so each one can be approximated accurately by Taylor formula. Besides, both and are considered, so sensitivity is more precise. Formally, given that can be rewritten as the integral = ( w) (w) = (cid:90) (tF (t))dt (cid:20)(cid:90) = 0 ((1 t)w + w)dt (cid:21) ( w), (4) where be any trajectory2 from to w. we define PQI as vP QI = (cid:12) (cid:12) (cid:12) (cid:12) (cid:90) 1 0 (cid:12) (cid:12) ((1 t)w + w)dt (cid:12) (cid:12) , and we define: FP QI = QI w . (5) (6) Here, means element-wise absolute value. Note that in vP QI and FP QI , we omit the sign of each dimension, only focusing on its magnitude. Thus, FP QI stands for an approximate upper bound of . This is because when counting total , positive and negative entries in Eq. (4) will offset each other. However, this may lead to overfitting problems. To solve this, we need to take both positive and negative elements as an equal impact on the quantization results, and maximize generalization performance. Since we do not differentiate the sign when evaluating sensitivity, FP QI is slightly different from the original definition. In practice, We complete the numerical integration with the following rectangle approximation: vP QI = 1 (cid:88) i= (cid:12) (cid:12) (cid:12) (cid:12) (cid:18) + ( w) (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) , (7) 2Since is potential function, the result is actually not related to C. Identifying Sensitive Weights via Post-quantization Integral where is the number of intervals. Table 3 shows the taking 32 can control the error within 0.1%. PQI can be used as fine-grained metric for predicting the effect of quantizing each dimension to the loss. Quantizing an element wi to wi yields to increase by vP QI,i wi wi. Moreover, notice that the change in loss Eq. (6) is linear with w. Therefore, the effect of quantizing multiple dimensions is directly summable. Hence we can leverage PQI to estimate the effect of quantizing group, channel, layer, or block of weights by summing up vP QI,i wi wi for each element wi in the corresponding group. Numerical Experiments We repeat the experiment of Llama 3.2 1B model but use our proposed PQI and counting average FP QI for each layer. We choose = 32 and sample batch of 1000 sentences from WikiText-2 (Merity et al., 2017), while we keep the rest settings the same as Table 1. Results in Table 4 show that taking 32 intervals for integral yields perfect accuracy. Furthermore, we observe: 1) shallow layers have larger average FP QI , while deeper layers typically have smaller FP QI ; 2) in single layer, different sublayers exhibit different sensitivity. Specifically, proj has the largest average FP QI , denoting greatest impact on final results, followed by proj and proj. Table 3: Predicted with intervals we split. For reference, the actual (w) on this dataset should be 0.1024. where Q(w wo, v) is low-precision dense weight vector, while wo and ws are high-precision sparse outliers and significant weights, respectively. We start by explaining the two sparse components in our pipeline. Then, we present an overview of the whole workflow. 6.1. Self-adaptive Outlier Selection One of the biggest challenges in LLM quantization is outliers, and previous studies have proved that removing outliers helps to promote the models performance (Lin et al., 2024b; Kim et al., 2024; Shao et al., 2024a). However, the biggest problem we identify from previous studies is that they use the same ratio or clamping threshold for all layers in the model. Recall that in Section 5, we have shown that some specific layers have larger average FP QI , indicating larger impact on the final accuracy loss. This leads to the assumption that layers with larger FP QI should be allocated with more bits to quantize, correspondingly higher outlier ratio in our Dense-and-Sparse decomposition. Thus, utilizing the same ratio or threshold results in suboptimal final results. To strike balance among different layers, we devise ratio searching process based on PQI. The key to this method is to dynamically adjust each layers outlier ratio according to layer-wise FP QI . To tune the optimal proportion, we add temperature factor to the fractions, and run grid search on it: Intervals Predicted Error 4 8 16 1.042E-1 1.032E-1 1.028E-1 1.026E-1 1.72E-2 8.39E-4 3.90E-4 1.62E-4 The only consideration about PQI now is that it cannot be used to make predictions ahead of quantization, since is required in advance to construct the path from to w. Therefore, we cannot leverage PQI directly to determine the optimal that minimizes the loss increment. Our solution is to first obtain draft version of the quantized model with traditional sensitivity metric, and then refine the quantization with PQI, which we shall discuss next. 6. ReQuant: Sensitivity-aware Quantization"
        },
        {
            "title": "Pipeline",
            "content": "In this section, we take advantage of PQI by proposing ReQuant, quantization pipeline orthogonal to most of previous LLM quantization methods, and can be easily combined with the original quantization process. The core of this pipeline is Dense-and-Sparse detach: = Q(w wo, v) + wo + ws, (8) 5 outlier num(i) = dim(w) ro% (F (i) QI )t i(F (i) QI )t (cid:80) , (9) where ro% = dim(wo)/ dim(w) is the global outlier ratio. Superscripts (i) indicate parameters of the i-th layers. The relevant process is formulated in Algorithm 1. Algorithm 1 Outlier Ratio Search Input: layers l, global outlier ratio ro%, weight vector w, original sensitivity v, search step α, pre-quantized tbest = 0, lossmin = Calculate {F (i) for = 0; < 1; = + α do QI }1il via Equations (6) and (7) for = 1 to do Calculate outlier num(i) via Equation (9) Select the largest outlier num(i) elements from w(i) as w(i) = Q(w wo, v) + wo if ( w) < lossmin then tbest = t, lossmin = ( w) end if end for end for Calculate {outlier num(i)}1il via Equation (9) return {outlier num(i)}1il Identifying Sensitive Weights via Post-quantization Integral Table 4: Element-wise average FP QI of different layers and sublayers. Layer V Gate Up Down 1 5 8 11 4.53E-08 4.16E-08 3.83E-08 2.63E9.93E-08 6.66E-08 6.11E-08 4.53E-08 1.59E-07 1.07E-07 9.94E-08 7.88E-08 9.13E-08 7.37E-08 8.83E-08 4.67E-08 4.22E-08 2.57E-08 2.46E-08 2.90E-08 4.99E-08 4.14E-08 4.01E-08 3.78E-08 5.31E-08 4.37E-08 4.72E-08 4.61E6.2. Step-wise Significant Weights Detach 6.3. Quantization Pipeline Though outlier selection improves performance, it may not be enough only to clip outliers from weights. Intuitively, the element-wise importance of weights is implied by an element-wise product: vP QI w. As shown in Table 10, some significant weights are crucial to the final output, with top 5.25% significant weights covering over 30% of total FP QI . To further improve performance, we devise greedy search algorithm (Algorithm 2) to gradually detach rs% important weights via PQI. β% rs% is small fraction we detach in single pass. Here, rs% should be an integer multiple of β%. The reason why we choose to gradually detach is to pick out the most important weights for current w, as well as minimize the estimation error. In practice, we notice that in most circumstances, even if we pick them out all at once (i.e., β = rs), the final performance would not degrade much. As combination and conclusion of Sections 6.1 and 6.2, we now present an overview of our proposed workflow: 1) Pre-quantize = Q(w, v); 2) Run Algorithm 1 to get {outlier num(i)}1il; 3) Select outliers {w(i) 4) Re-quantize = Q(w wo, v) + wo; 5) Recover significant weights ws with Algorithm 2; 6) Complete quantization: = Q(w wo, v) + wo + ws. }1il from {w(i)}1il; It is worth noticing that while wo detach the outliers of w, ws actually detaches the outliers of vP QI w. Besides, the actual quantization happens in step 4. Step 1 is merely pre-quantization step to determine the outlier ratio ro. ws is acquired after quantization, aiming to recover the accuracy of important weights. The scheme of our proposed workflow is illustrated in Figure 1. Table 5: The proportion of significant weights we choose and how much they can cover in total FP QI . Proportion of Significant Weights FP QI percentage 0.15% 0.71% 5.25% 4.53% 11.29% 34.06% Algorithm 2 Significant Weight Search Input: global significant weight ratio rs%, weight vector w, Re-quantized weights w, search step β% ws = 0 for = 0; < rs/β; = + 1 do Calculate vP QI via Equation (7) Calculate element-wise FP QI by vP QI w Select the top β% elements with the largest FP QI from (w w) as s, ws = ws + w = + end for return ws Figure 1: ReQuant pipeline. 7. Experiment In this section, we apply ReQuant on Llama 3.2 1B and 3B small models. We pick out three representative groupingbased and codebook-based quantization methods: AWQ, SqueezeLLM, and QTIP, and evaluate ReQuant on them. 6 Identifying Sensitive Weights via Post-quantization Integral Table 6: QTIP results for Llama 3.2 1B Base/Instruct models. The entries share the same meaning as Table 7. Precision Method Calib Set full 2-bit 3-bit 4-bit Baseline - QTIP QTIP+ReQuant RedPajama RedPajama/WikiText-2/Tulu 3 QTIP QTIP+ReQuant RedPajamaRedPajama/WikiText-2/Tulu 3 QTIP QTIP+ReQuant RedPajama RedPajama/WikiText-2/Tulu 3 Sparsity rs ro - - 0 - 0 - - - 0.5 - 0.5 - 0.5 Bits 2.02 2.26 3.02 3.26 4.02 4.26 Mem Instruct (GB) Wiki2 MATH Base 2. 1.40 1.47 1.72 1.80 2.05 2.13 9.75 18.67 16.01 11.17 10. 10.12 10.06 29.30 0.78 2.68 18.78 20.06 26.38 27.36 7.1. Accuracy Results 7.2. Ablation Study For AWQ, following its original settings, we sample 100 sequences of 2048 length from Pile (Gao et al., 2020) dataset as its calibration set and for the ReQuant process. To reach comparable total bits, we choose 0.25% sparse ratio with group size of 256, while the baseline adopts 128 group size. Note that both increasing the group size and increasing sparsity improves the results, and primary results show that 256 grouping with 0.25% sparsity gives optimal results. For SqueezeLLM, we use sampled 100 sentences from WikiText-2 with 2048 sequence length as calibration set to calculate Hessian matrix and PQI. Since SqueezeLLM adopts the same Dense-and-Sparse decomposition format as ReQuant, we simply replace the counterpart with our detached weights. For QTIP, we use RedPajama (Weber et al., 2024) for its original calibration. We use Tulu 3 (Lambert et al., 2024) dataset for supervised fine-tuning models to calculate PQI, and use WikiText-2 dataset for base models. Perplexity Results for Pre-trained Models For Llama 3.2 1B and 3B models, we evaluate the model on WikiText2 test set with sequence length 2048. Results are presented in Tables 6 and 7. Results show significant improvement of ReQuant, highlighting 1.38 decrease on the 3-bit 1B model with AWQ and 0.56 decrease with SqueezeLLM. For 3B model, the improvement is minor since larger model are easier to quantize for the baselines. In this part, we explore the effectiveness of wo and ws separately. We evaluate our method with Llama 3.2 1B base model on WikiText-2 train and test set. Results are shown in Table 8. Outlier Selection To validate the effect of outlier selection, we compare 0.45% outlier results with the counterpart in SqueezeLLM. Results show that the uneven outlier ratio among different layers improves test perplexity from 10.62 to 10.52, while train perplexity from 11.15 to 11.02. Considering the model size and precision, we believe this improvement is significant. We also add rand baseline which randomly picks wo and ws. Comparison shows that ReQuant has the best performance to identify outliers. Significant Weight Detach The success of significant weight detach rely on accurately identifying important weights. To examine the influence of searching steps rs β on final results, we list results when rs/β = 1, 2, 4 and compare train and test perplexity. Results show that increasing greedy searching steps helps to improve performance, with rs/β = 2 improves test perplexity by 0.03. However, doubling searching steps also doubles the integral computation cost. Since more steps only provides minimum improvement, we choose rs/β = 2. 7.3. Inference Speed Few-shot Results for Instruction Following Models For instruction following models, we evaluate on the 4-shot MATH generation task. Notably, AWQ+ReQuant and SqueezeLLM+ReQuant show significant improvements: AWQ+ReQuant in 4-bit quantization achieves MATH score of 24.32, while SqueezeLLM+ReQuant achieves higher score of 24.74, demonstrating superior performance. Besides, ReQuant helps to restore severe loss in the models accuracy: SqueezeLLM baseline, which fails to present the desired answer format, can yield feasible outputs after applying ReQuant. Results are shown in Tables 6 and 7. The inference speed of Dense-and-Sparse decomposition is tested with Llama 3.2 1B and 3B instruction following models. For sparse matrix multiplication, we use cuSPARSE3, while for the dense quantized weight multiplication, we keep the methods original inference kernels. We report the latency of single linear layer and for the the whole model. We use the same settings for all experiments: an input sentence of 128 length, and 2048 generated tokens. For all experiments, we use RTX 3090 GPUs. Our Dense-and3https://docs.nvidia.com/cuda/cusparse 7 Identifying Sensitive Weights via Post-quantization Integral Table 7: WikiText-2 perplexity for base models and 4-shot MATH evaluation for instruction following models. Fail means failure to parse models output due to garbled characters. Llama 3.2 1B Base/Instruct Hyperparameters 3-bit 4-bit Method Baseline AWQ (g128) AWQ (g256)+ReQuant Calib Set - Pile Pile SqueezeLLM SqueezeLLM+ReQuant WikiText-2 WikiText-2 Sparsity ro - - 0.25 0.45 0. rs - - 0 0.05 0.05 Llama 3.2 3B Base/Instruct Hyperparameters Method Baseline AWQ (g128) AWQ (g256)+ReQuant Calib Set - Pile Pile SqueezeLLM SqueezeLLM+ReQuant WikiText-2 WikiText-2 Sparsity ro - - 0. 0.45 0.45 rs - - 0 0.05 0.05 Bits 16 3.25 3.25 3.25 3.25 Bits 16 3.25 3. 3.24 3.24 Mem (GB) Base Wiki2 Instruct MATH 2.30 0.86 0. 0.86 0.86 9.75 16.74 15.36 13.86 13.30 3-bit 29. fail fail 11.28 14.18 Mem (GB) Base Wiki2 Instruct MATH 5. 1.80 1.80 1.80 1.80 7.81 10.30 9.98 9.39 9.47 44. 29.64 35.08 33.80 35.34 Bits 16 4.25 4.25 4.25 4. Bits 16 4.25 4.24 4.24 4.24 Mem (GB) Base Wiki2 Instruct MATH 2.30 0.97 0.97 0.97 0.97 9.75 10.84 10. 10.51 10.43 4-bit 29.30 22.82 24.32 fail 24.74 Mem (GB) Base Wiki2 Instruct MATH 5.98 2.13 2.13 2.13 2.13 7. 8.22 8.20 8.12 8.14 44.92 42.88 42.20 43.06 42.24 Table 8: Ablation results on WikiText-2 perplexity. The rand line indicates that wo and ws are picked out randomly from the weights. Table 9: Inference speed of Dense-and-Sparse decomposition. Comment Train PPL Test PPL ro - 0.45 0.45 0.45 0 rs - 0.05 0 0 0.05 0 bfloat16 SqueezeLLM 0. 0.05 rand 0.45 0.45 0.45 0.05 0.05 0.05 β = 0.0125 β = 0.025 β = 0.05 10.20 10. 11.02 11.15 11.15 11.30 11.28 10.94 10.94 10.95 9.75 10.45 10.52 10.62 10.65 10.80 10. 10.42 10.42 10.45 Sparse decomposition meets the baseline inference speed of its equivalents (Kim et al., 2024) and is slightly (1.21.4x) slower than other inference frameworks, which is reasonable compared with similar studies (Li et al., 2023b; Kim et al., 2024); see Table 9. 8. Discussion and Conclusion In this study, we discuss the accuracy of sensitivity metrics in PTQ. We reveal the inaccuracy of the classic second order approximation of and point out the reason to be small convergence radius. To tackle this problem, we propose PQI as resolution, which calculates the posterior elementwise quantization impact. To leverage PQI, we propose 8 Model Precision Method 1B 1B 1B 1B 1B 1B 3B 3B 3B 3B 3B 3B 4-bit 4-bit 4-bit 4-bit 3-bit 3-bit 4-bit 4-bit 4-bit 4-bit 3-bit 3-bit AWQ AWQ+PQI SqueezeLLM SqueezeLLM+ReQuant SqueezeLLM SqueezeLLM+ReQuant AWQ AWQ+ReQuant SqueezeLLM SqueezeLLM+ReQuant SqueezeLLM SqueezeLLM+ReQuant Prefilling Decoding (ms) 13 18 86 29 85 86 31 230 68 229 229 (ms) 23768 35204 47151 45266 33657 59631 59174 56882 56372 56343 54640 Total (ms) 23781 35222 47237 33742 32654 59662 59205 57112 56440 56572 54869 ReQuant, workflow based on Dense-and-Sparse decomposition to boost the performance of low-precision model. Our method is validated on popular AWQ, SqueezeLLM, and QTIP, with frontier Llama 3.2 small language models. Future Work Our proposed workflow mainly targets on recovering the important weights after quantization. While one can do this process iteratively to detach the desired significant elements, it still requires to store sparse matrix, which consists of undesired bits, like the row and column indices. Meanwhile, the sparse matrix multiplication is bottleneck in inference. Thus, combining the PQI analysis into the quantization process is left as future work. For example, one can iteratively compute PQI and tune the weight distribution according to it, thus getting rid of the sparse weights. Identifying Sensitive Weights via Post-quantization Integral"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Abdin, M., Aneja, J., Behl, H., Bubeck, S., Eldan, R., Gunasekar, S., Harrison, M., Hewett, R. J., Javaheripi, M., Kauffmann, P., Lee, J. R., Lee, Y. T., Li, Y., Liu, W., Mendes, C. C. T., Nguyen, A., Price, E., de Rosa, G., Saarikivi, O., Salim, A., Shah, S., Wang, X., Ward, R., Wu, Y., Yu, D., Zhang, C., and Zhang, Y. Phi-4 technical report, 2024. URL https://arxiv.org/abs/ 2412.08905. Ashkboos, S., Mohtashami, A., Croci, M. L., Li, B., Cameron, P., Jaggi, M., Alistarh, D., Hoefler, T., and Hensman, J. Quarot: Outlier-free 4-bit inference in rotated llms, 2024. URL https://arxiv.org/abs/ 2404.00456. volume 36, pp. 1008810115. Curran Associates, Inc., 2023. URL https://proceedings.neurips. cc/paper_files/paper/2023/file/ 1feb87871436031bdc0f2beaa62a049b-Paper-Conference. pdf. Ding, X., Liu, X., Tu, Z., Zhang, Y., Li, W., Hu, J., Chen, H., Tang, Y., Xiong, Z., Yin, B., and Wang, Y. Cbq: Crossblock quantization for large language models, 2024. URL https://arxiv.org/abs/2312.07950. Frantar, E. and Alistarh, D. Optimal brain compression: framework for accurate post-training quantization and pruning. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 44754488. Curran Associates, Inc., 2022. URL https://proceedings.neurips. cc/paper_files/paper/2022/file/ 1caf09c9f4e6b0150b06a07e77f2710c-Paper-Conference. pdf. Chee, J., Cai, Y., Kuleshov, V., and De Sa, C. M. Quip: large language models with 2-bit quantization of guarantees. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 43964429. Curran Associates, Inc., 2023. URL https://proceedings.neurips. cc/paper_files/paper/2023/file/ 0df38cd13520747e1e64e5b123a78ef8-Paper-Conference. pdf. Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Gpt3.int8(): 8-bit matrix multiplication for transformers at scale. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 3031830332. Curran Associates, Inc., 2022a. URL https://proceedings.neurips. cc/paper_files/paper/2022/file/ c3ba4962c05c49636d4c6206a97e9c8a-Paper-Conference. pdf. Dettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L. 8-bit optimizers via block-wise quantization. In International Conference on Learning Representations, 2022b. URL https://openreview.net/forum? id=shpkpVXzo3h. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, 9 Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pretrained transformers, 2023. URL https://arxiv. org/abs/2210.17323. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. The pile: An 800gb dataset of diverse text for language modeling, 2020. URL https: //arxiv.org/abs/2101.00027. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Wyatt, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Guzman, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Thattai, G., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Prasad, K., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Malik, Identifying Sensitive Weights via Post-quantization Integral K., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh, M., Paluri, M., Kardas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova, M., Kambadur, M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov, N., Bogoychev, N., Chatterji, N., Zhang, N., Duchenne, O., elebi, O., Alrassy, P., Zhang, P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura, P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R., Cabral, R. S., Stojnic, R., Raileanu, R., Maheswari, R., Girdhar, R., Patel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S., Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V., Albiero, V., Petrovic, V., Chu, W., Xiong, W., Fu, W., Meers, W., Martinet, X., Wang, X., Wang, X., Tan, X. E., Xia, X., Xie, X., Jia, X., Wang, X., Goldschlag, Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert, Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Srivastava, A., Jain, A., Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A., Sharma, A., Boesenberg, A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A., Teo, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton, A., Ryan, A., Ramchandani, A., Dong, A., Franco, A., Goyal, A., Saraf, A., Chowdhury, A., Gabriel, A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi, B., Huang, B., Loyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker, C., Burton, C., Mejia, C., Liu, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai, C., Tindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery, E., Presani, E., Hahn, E., Wood, E., Le, E.-T., Brinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun, F., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F., Caggioni, F., Kanayet, F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Inan, H., Shojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H., Goldman, H., Zhan, H., Damlaj, I., Molybog, I., Tufanov, I., Leontiadis, I., Veliche, I.-E., Gat, I., Weissman, J., Geboski, J., Kohli, J., Lam, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings, J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J., Wu, K., U, K. H., Saxena, K., Khandelwal, K., Zand, K., Matosich, K., Veeraraghavan, K., Michelena, K., Li, K., Jagadeesh, K., Huang, K., Chawla, K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov, M., Lathi, M., Keneally, M., Liu, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M. J., Metanat, M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Mehta, N., Laptev, N. P., Dong, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P., Dollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R., Murthy, R., Nayani, R., Mitra, R., Parthasarathy, R., Li, R., Hogan, R., Battey, R., Wang, R., Howes, R., Rinott, R., Mehta, S., Siby, S., Bondu, S. J., Datta, S., Chugh, S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S., Mahajan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S., Feng, S., Lin, S., Zha, S. C., Patil, S., Shankar, S., Zhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe, S., Satterfield, S., Govindaprasad, S., Gupta, S., Deng, S., Cho, S., Virk, S., Subramanian, S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Koehler, T., Robinson, T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi, V., Montanez, V., Mohan, V., Kumar, V. S., Mangla, V., Ionescu, V., Poenaru, V., Mihailescu, V. T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X., Wu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., and Ma, Z. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021. URL https: //datasets-benchmarks-proceedings. neurips.cc/paper_ files/paper/2021/file/ be83ab3ecd0db773eb2dc1b0a17836a1-Paper-round2. Identifying Sensitive Weights via Post-quantization Integral pdf. Hooper, C. R. C., Kim, S., Mohammadzadeh, H., Mahoney, M. W., Shao, S., Keutzer, K., and Gholami, A. KVQuant: Towards 10 million context length LLM inference with In The Thirty-eighth Annual KV cache quantization. Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=0LXotew9Du. Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Lu, K., Dang, K., Fan, Y., Zhang, Y., Yang, A., Men, R., Huang, F., Zheng, B., Miao, Y., Quan, S., Feng, Y., Ren, X., Ren, X., Zhou, J., and Lin, J. Qwen2.5-coder technical report, 2024. URL https://arxiv.org/abs/2409.12186. Jia, J., Xie, C., Lu, H., Wang, D., Feng, H., Zhang, C., Sun, B., Lin, H., Zhang, Z., Liu, X., and Tao, D. SDP4bit: Toward 4-bit communication quantization in sharded data parallelism for LLM training. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/ forum?id=PEEqnXlSCk. Kim, S., Hooper, C. R. C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M. W., and Keutzer, K. SqueezeLLM: In Salakhutdinov, R., Dense-and-sparse quantization. Kolter, Z., Heller, K., Weller, A., Oliver, N., Scarlett, J., and Berkenkamp, F. (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 2390123923. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/ v235/kim24f.html. Kumar, T., Ankner, Z., Spector, B. F., Bordelon, B., Muennighoff, N., Paul, M., Pehlevan, C., Re, C., and Raghunathan, A. Scaling laws for precision, 2024. URL https://arxiv.org/abs/2411.04330. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S., Graf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi, H. Tulu 3: Pushing frontiers in open language model post-training. 2024. LeCun, Y., Denker, J., and Solla, S. Optimal brain damage. In Touretzky, D. (ed.), Advances in Neural Information Processing Systems, volume 2. Morgan-Kaufmann, 1989. URL https://proceedings.neurips. cc/paper_files/paper/1989/file/ 6c9882bbac1c7093bd25041881277658-Paper. pdf. 11 Li, B., Chen, J., and Zhu, J. Memory efficient optimizers with 4-bit states. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 1513615171. Curran Associates, Inc., 2023a. URL https://proceedings.neurips. cc/paper_files/paper/2023/file/ 3122aaa22b2fe83f9cead1a696f65ceb-Paper-Conference. pdf. Li, Y. and Yuan, Y. Convergence analysis of two-layer neural networks with relu activation. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ a96b65a721e561e1e3de768ac819ffbb-Paper. pdf. Li, Y., Yu, Y., Zhang, Q., Liang, C., He, P., Chen, W., and Zhao, T. Losparse: Structured compression of large language models based on low-rank and sparse approximation, 2023b. URL https://arxiv.org/abs/ 2306.11222. Li, Y., Yu, Y., Liang, C., Karampatziakis, N., He, P., Chen, W., and Zhao, T. Loftq: LoRA-fine-tuning-aware quantization for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=LzPWWPAdY4. Lin, H., Xu, H., Wu, Y., Cui, J., Zhang, Y., Mou, L., Song, L., Sun, Z., and Wei, Y. Duquant: Distributing outliers via dual transformation makes stronger quantized LLMs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024a. URL https: //openreview.net/forum?id=mp8u2Pcmqz. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq: Activation-aware weight quantization for on-device In Gibbons, P., llm compression and acceleration. Pekhimenko, G., and Sa, C. D. (eds.), Proceedings of Machine Learning and Systems, volume 6, pp. 87100, URL https://proceedings.mlsys. 2024b. org/paper_files/paper/2024/file/ 42a452cbafa9dd64e9ba4aa95cc1ef21-Paper-Conference. pdf. Lin*, Y., Tang*, H., Yang*, S., Zhang, Z., Xiao, G., Gan, C., and Han, S. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv:2405.04532, 2024. Identifying Sensitive Weights via Post-quantization Integral Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R., and Chandra, V. Llmqat: Data-free quantization aware training for large language models, 2023. URL https://arxiv.org/ abs/2305.17888. Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R., and Chandra, V. LLMQAT: Data-free quantization aware training for large lanIn Ku, L.-W., Martins, A., and Srikuguage models. mar, V. (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 467484, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl. 26. URL https://aclanthology.org/2024. findings-acl.26/. Liu, Z., Zhao, C., Fedorov, I., Soran, B., Choudhary, D., Krishnamoorthi, R., Chandra, V., Tian, Y., and Blankevoort, T. Spinquant: Llm quantization with learned rotations, 2024b. URL https://arxiv.org/abs/ 2405.16406. Ma, Y., Li, H., Zheng, X., Ling, F., Xiao, X., Wang, R., Wen, S., Chao, F., and Ji, R. Affinequant: Affine transformation quantization for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=of2rhALq8l. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. URL https:// openreview.net/forum?id=Byj72udxe. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022. URL https: //arxiv.org/abs/2203.02155. Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang, K., Gao, P., Qiao, Y., and Luo, P. Omniquant: Omnidirectionally calibrated quantization for large lanIn The Twelfth International Conferguage models. ence on Learning Representations, 2024a. URL https: //openreview.net/forum?id=8Wuvhh0LYW. Shao, Y., Liang, S., Ling, Z., Yan, M., Liu, H., Chen, S., Yan, Z., Zhang, C., Qin, H., Magno, M., Yang, Y., Lei, Z., Wang, Y., Guo, J., Shao, L., and Tang, H. Gwq: Gradient-aware weight quantization for large language models, 2024b. URL https://arxiv.org/abs/ 2411.00850. 12 Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and Sa, C. D. QuIP$#$: Even better LLM quantization with hadamard incoherence and lattice codebooks. In Fortyfirst International Conference on Machine Learning, 2024a. URL https://openreview.net/forum? id=9BrydUVcoe. Tseng, A., Sun, Q., Hou, D., and Sa, C. D. QTIP: Quantization with trellises and incoherence processing. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. URL https: //openreview.net/forum?id=7sdkLVuYCU. Wang, J., Zhou, H., Song, T., Mao, S., Ma, S., Wang, H., Xia, Y., and Wei, F. 1-bit ai infra: Part 1.1, fast and lossless bitnet b1.58 inference on cpus, 2024. URL https://arxiv.org/abs/2410.16144. Weber, M., Fu, D. Y., Anthony, Q., Oren, Y., Adams, S., Alexandrov, A., Lyu, X., Nguyen, H., Yao, X., Adams, V., Athiwaratkun, B., Chalamala, R., Chen, K., Ryabinin, M., Dao, T., Liang, P., Re, C., Rish, I., and Zhang, C. Redpajama: an open dataset for training large language models. NeurIPS Datasets and Benchmarks Track, 2024. Wei, J., Wang, X., Schuurmans, D., Bosma, M., ichter, b., Xia, F., Chi, E., Le, Q. V., and Zhou, D. Chain-ofthought prompting elicits reasoning in large language models. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2482424837. Curran Associates, Inc., 2022. URL https://proceedings.neurips. cc/paper_files/paper/2022/file/ 9d5609613524ecf4f15af0f7b31abca4-Paper-Conference. pdf. Xia, Y., Fu, F., Zhang, W., Jiang, J., and CUI, B. Efficient multi-task LLM quantization and serving for Identifying Sensitive Weights via Post-quantization Integral In The Thirty-eighth Annual multiple loRA adapters. Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=HfpV6u0kbX. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. SmoothQuant: Accurate and efficient post-training quantization for large language models. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 3808738099. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/ v202/xiao23c.html. Zhang, J., Huang, H., Zhang, P., Wei, J., Zhu, J., and Chen, J. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization, 2024. URL https://arxiv.org/abs/2411.10958. Zhang, J., Wei, J., Zhang, P., Zhu, J., and Chen, J. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In International Conference on Learning Representations (ICLR), 2025. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. URL https://arxiv.org/abs/2205. 01068. 13 Identifying Sensitive Weights via Post-quantization Integral A. Limitations In practice, we identify two limitations. First, the Dense-and-Sparse decomposition requires the sparse matrix to store its weights and index. While the weights require at least 16 bits per element, the row and column indices require at least 16 bits for each, which is non-negligible. Due to this reason, we only adopt minimum sparsity (0.5%). Besides, sparse matrix multiplication is slow, and may harm decoding speed during inference. B. Hyperparameters Table 10: Experimental hyperparameters. Setting Hyperparameter"
        },
        {
            "title": "AWQ",
            "content": "SqueezeLLM QTIP calib set (all) calib sequence length rs/β calib set (all) calib sequence length rs/β calib set (Hessian) calib set (ReQuant, base models) calib set (ReQuant, instruction following models) rs/β Value Pile 2048 32 100 2 WikiText-2 2048 32 100 2 RedPajama WikiText-2 Tulu 3 32 2 C. Experiments compute resources For reference, the estimated GPU hours of computing integral are listed Table 11. Table 11: GPU Hours of doing integral on A100. Calib Set GPU Hours Llama 3.2 1B WikiText Llama 3.2 3B WikiText Llama 3.2 1B Tulu 3 Llama 3.2 3B Tulu 3 100 100 2048 2048 0.5 1.5 1.5"
        }
    ],
    "affiliations": [
        "Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University"
    ]
}