{
    "paper_title": "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs",
    "authors": [
        "Artem Lykov",
        "Valerii Serpiva",
        "Muhammad Haris Khan",
        "Oleg Sautenkov",
        "Artyom Myshlyaev",
        "Grik Tadevosyan",
        "Yasheerah Yaqoot",
        "Dzmitry Tsetserukou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on a dataset comprising over 8,000 simulated flight trajectories across three key categories-Human Recognition, Symbol Understanding, and Reasoning-the model generates real-time 4D action commands based on first-person visual inputs and textual instructions. To further enhance performance in intricate scenarios, we propose CognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM) reasoning module to simplify task directives prior to high-frequency control. Experimental evaluations using our open-source benchmark, CognitiveDroneBench, reveal that while a racing-oriented model (RaceVLA) achieves an overall success rate of 31.3%, the base CognitiveDrone model reaches 59.6%, and CognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate improvements of up to 30% in critical cognitive tasks, underscoring the effectiveness of incorporating advanced reasoning capabilities into UAV control systems. Our contributions include the development of a state-of-the-art VLA model for UAV control and the introduction of the first dedicated benchmark for assessing cognitive tasks in drone operations. The complete repository is available at cognitivedrone.github.io"
        },
        {
            "title": "Start",
            "content": "CognitiveDrone: VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs Artem Lykov, Valerii Serpiva, Muhammad Haris Khan, Oleg Sautenkov, Artyom Myshlyaev, Grik Tadevosyan, Yasheerah Yaqoot, and Dzmitry Tsetserukou 5 2 0 2 3 ] . [ 1 8 7 3 1 0 . 3 0 5 2 : r Abstract This paper introduces CognitiveDrone, novel tailored for complex Vision-Language-Action (VLA) model Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on dataset comprising over 8,000 simulated flight trajectories across three key categoriesHuman Recognition, Symbol Understanding, and Reasoningthe model generates real-time 4D action commands based on first-person visual inputs and textual instructions. To further enhance performance in intricate scenarios, we propose CognitiveDrone-R1, which integrates an additional VisionLanguage Model (VLM) reasoning module to simplify task directives prior to high-frequency control. Experimental evaluations using our open-source benchmark, CognitiveDroneBench, reveal that while racing-oriented model (RaceVLA) achieves an overall success rate of 31.3%, the base CognitiveDrone model reaches 59.6%, and CognitiveDrone-R1 attains success rate of 77.2%. These results demonstrate improvements of up to 30% in critical cognitive tasks, underscoring the effectiveness of incorporating advanced reasoning capabilities into UAV control systems. Our contributions include the development of stateof-the-art VLA model for UAV control and the introduction of the first dedicated benchmark for assessing cognitive tasks in drone operations. The complete repository is available at https://cognitivedrone.github.io. I. INTRODUCTION In an era marked by rapid advancements in robotics and artificial intelligence, enabling robots to perform wide range of complex tasks in dynamically changing environments has emerged as critical challenge. Cognitive robotics strives not only to endow machines with precise control but also to equip them with high-level reasoning and decisionmaking capabilities that allow them to adapt to unpredictable real-world scenarios. Despite notable progress in various domains of robotics, one persistent challenge remains: how to objectively evaluate and compare cognitive robotic systems, particularly when they are expected to tackle multitude of intricate tasks. The scarcity of standardized, open-source benchmarks and datasets is especially evident in the realm of Unmanned Aerial Vehicles (UAVs), where existing evaluation frameworks are largely confined to racing or basic navigation tasks. This limitation not only hinders fair comparison among different cognitive UAV systems but also restricts the exThese authors contributed equally to this work. The authors are with the Intelligent Space Robotics Laboratory, Science Center {Artem.Lykov, Valerii.Serpiva, and haris.khan, Artyom.Myshlyaev, oleg.sautenkov, grik.tadevosyan, yasheerah.yaqoot, d.tsetserukou}@skoltech.ru for Digital Engineering, Technology. Skolkovo Institute of Fig. 1. CognitiveDrone is VLA system for UAVs that generates smooth 4D control commands from first-person visual inputs and natural language instructions. It combines 7B-parameter VLA model trained on an extensive open-source dataset of cognitive tasksincluding reasoning, human recognition, and symbol understandingwith 7B-parameter VLM reasoning module that refines task directives. The system is evaluated within CognitiveDroneBenchthe first evaluation benchmark for VLA systems tailored to cognitive UAVswhere the drone must navigate track with gates by selecting the appropriate gate through solving cognitive tasks. We have released the complete dataset, benchmark environment, model weights, and training/inference code as open source. ploration of more sophisticated cognitive functions such as reasoning, human recognition, and symbolic understanding. To address these challenges, we introduce CognitiveDrone novel VLA model designed for real-time cognitive task solving and reasoning in UAVs. In conjunction with the model, we propose CognitiveDroneBench, an open-source benchmark built upon Gazebo-based physical simulation environment that integrates drone racing track with cognitive checkpoints. At each stage of the track, the UAV is required to select specific gates by solving cognitive task, thus providing comprehensive performance evaluation that transcends traditional racing metrics. Furthermore, we augment our system with an auxiliary reasoning module based on the VLM model Qwen2.5VL, yielding the CognitiveDrone-R1 variant. This reasoning module, operating at lower frequency than the primary VLA component, is intended to enhance task comprehension and facilitate more robust decision-making. By seamlessly integrating these components, our work paves the way for more rigorous evaluations and innovative applications in cognitive UAV research. II. RELATED WORKS substantial body of research has focused on developing cognitive systems for robotics, where the integration of visual perception, language understanding, and action planning is paramount. For robotic manipulators, state-of-the-art models such as PaLM-E [1], RT-1 [2], RT-2 [3], and RT-X [4] have significantly advanced the field by incorporating VLA frameworks that enable these robots to handle complex manipulation tasks in dynamic environments. Complementary to these efforts, VLA-based systems like OpenVLA [5], MiniVLA [6], and Octo [7] have been developed for diverse robotic platforms, extending the application spectrum of cognitive robotics. The evaluation of such systems has been further supported by simulation benchmarks. For instance, LIBERO [8] provides simulation environment that standardizes the assessment of cognitive performance in robotic manipulators. In parallel, for robots aimed at human-robot collaboration and domestic assistance including humanoids such as Tesla Optimus, Agility Robotics [9], and FIGURE, as well as quadrupedal platforms like CognitiveDog [10] and DoggyBot [11] the open-source PARTNR benchmark [12] has been introduced to facilitate objective comparisons. Within the realm of cognitive UAVs, various transformerbased and VLA approaches have been explored. UAVs Meet LLMs [13] presents comprehensive overview of such methods, highlighting applications in drone navigation [14], [15], [16], [17], [18], flight control [19], [20], and mission planning [21], [22]. Additionally, models for drone swarm control, such as SwarmGPT [23] and FlockGPT [24], have been developed. SwarmGPT generates individual trajectories for each drone in swarm, while FlockGPT computes Signed Distance Function (SDF) to define the flight direction for each drone based on its position relative to the target surface. The RaceVLA model [25] notably introduced fourcomponent control signal (Vx, Vy, Vz, omega) for UAVs, derived from visual inputs and natural language commands. However, the evaluation of RaceVLA has been largely restricted to drone racing tasks, leaving its performance in more complex cognitive scenarios largely unexplored. In recent times, an alternative direction has emerged that integrates explicit reasoning modules within robotic systems. Initial proposals in language models, such as GPT-4o1 and DeepSeek-R1, as well as models such as Claude Sonnet 3.7, Gemini 2 Thinking, and Grok 3 Big Brain mode, have demonstrated the potential of decoupling reasoning from reactive control. This paradigm has also been adapted in robotics; for example, CognitiveOS [26] implements separate Fig. 2. CognitiveDrone system architecture. modules for action planning and execution, and the humanoid robot Helix [27] distinguishes between high-level reasoning component (System 2, powered by 7B parameter VLM pretrained on extensive internet data) and low-level fast control component (System 1, employing an 80M parameter visuomotor policy). Despite these significant contributions, the field of cognitive UAVs still suffers from lack of open-source datasets and standardized benchmarks capable of evaluating both control and high-level cognitive functions. Our work aims to fill this gap by introducing novel dataset that encompasses cognitive tasks across various domains (e.g., reasoning, human recognition, symbol understanding) and by providing comprehensive benchmark CognitiveDroneBench tailored to the unique challenges of cognitive UAV platforms. III. SYSTEM ARCHITECTURE A. CognitiveDrone System Architecture Figure 2 illustrates the overall architecture of the CognitiveDrone system. The primary goal of our system is to generate smooth 4D action commands for UAVs based on firstperson visual inputs and user instructions. To achieve this, we integrate VLA model adapted from the open-source OpenVLA model, which comprises 7 billion parameters. This model was trained on dataset of over 8,000 simulated flight episodes, enabling it to develop robust understanding of UAV flight physics and to effectively control the drones motion. OpenVLA is primarily optimized for accurately capturing the dynamics of drone flight. Consequently, it excels at generating high-frequency control commandsoperating at 10 Hzto produce smooth and continuous trajectories. However, its focus on flight physics means that it may struggle with more complex cognitive tasks, such as resolving ambiguities in task instructions or selecting the most appropriate action when faced with multifaceted challenges. To address this limitation, our architecture is extended with an additional VLM dedicated to high-level reasoning, forming the CognitiveDrone-R1 system. This reasoning module processes the task instructions and visual inputs to disambiguate and simplify the directives, effectively translating complex commands into clearly defined actions for the VLA model to execute. Owing to the inclusion of second 7B model for reasoning, the systems total memory requirement doubles to approximately 20 GB, compared to 10 GB when using only the single 7B VLA model. Additionally, due to the higher token count involved in reasoning tasks, the reasoning module operates at lower frequencyaround 2 Hzwhile the VLA model continues to generate control commands at 10 Hz. In summary, our system leverages the complementary strengths of two modules: the high-frequency OpenVLA module, which is specialized in the real-time generation of control commands based on deep flight physics understanding, and the slower, yet powerful, VLM reasoning module, which refines and clarifies task directives. Together, these components enable robust and cognitively capable UAV operation. Fig. 3. tasks adapted for UAVs. Examples of prepared dataset tasks for VLA to solve cognitive trajectory. As the camera moves along this trajectory, frames with size of 256x256 pixels and applied actions are recorded at each step. The camera simulation is handled using the Gazebo RealSense plugin, with all data collected via ROS topics. IV. DATASET COLLECTION AND TRAINING PIPELINE B. CognitiveDroneBench Simulation Architecture A. Dataset Collection Gazebo with ArduPilot was chosen for its simulation capabilities, allowing accurate replication of real-world drone dynamics, including flight control; additionally, the drone in simulation is controlled using velocity setpoints, ensuring consistency with real-world drones running ArduPilot firmware. The simulation environment is built using Gazebo with the ArduPilot Gazebo plugin and SITL ArduPilot. Each model (gates, labels) for dataset collection is described using an *.sdf file along with *.dae model. Tasks in the simulation are defined by JSON file, which includes task prompt, task options, the correct answer, and gate parameters such as size, shape, and color. The ROS-Gazebo plugin is used to dynamically spawn objects for drone tasks via Python scripts. During dataset collection, the initial position of the camera drone is spawned with randomized pose near initial point. path is then generated from the cameras initial position to the center of designated gate, following spline-based We collected dataset of 8,062 continuous trajectory samples to train the VLA model in simulated environment, grouped into three categories. Task examples are shown in Figure 3. In all samples, the UAV navigates to target gate with preliminary known answer, generating fourdimensional (4D) action command to control its motion. Human Recognition: The model is required to identify the individuals based on external characteristics specified within the textual prompt. Additionally, the UAV is tasked with navigating to designated gate associated with notable figure. Symbol Understanding: The model is required to differentiate between variety of symbols, including alphanumeric characters (e.g., numbers and letters), corporate logos, and pictorial representations of animals. Reasoning: In this category, the UAV must execute tasks necessitating logical deduction. Examples include navigating to gate displaying digit corresponding to the solution every 500 steps to monitor model performance. For detailed overview of the training progression, please refer to the learning curve in Figure 4. V. EVALUATION A. CognitiveDroneBench The evaluation of cognitive capabilities in UAVs poses unique challenges that are even more pronounced than those encountered in robotic manipulators. To address these challenges and to enable an objective comparison of VLA models for UAVs, we developed an open-source simulation benchmark named CognitiveDroneBench. As illustrated in Figure 1, the benchmark is implemented within highfidelity physical simulation environment that accurately replicates the dynamics and physics of UAV flight. In our benchmark, the drone is required to traverse race track composed of multiple sequential gates. At each stage of the track, the drone receives first-person view (FPV) image along with task-specific textual instruction. The core objective is to select the correct gate by solving an embedded cognitive task, thereby generating 4D action command that dictates its movement. The tasks are categorized into three distinct types: Human Recognition, Symbol Understanding, and Reasoning. For conventional VLA models, such as RaceVLA and the base version of CognitiveDrone, the decision-making process involves directly processing the FPV image and associated to determine the appropriate action. In contrast, prompt the CognitiveDrone-R1 variant incorporates an additional reasoning stage using VLM. This VLM processes the task instruction and visual data to disambiguate and simplify the directive, effectively reducing the complexity of the task before passing it to the VLA model for high-frequency control. Task performance is automatically validated: passing through the correct gate earns the drone 1 point. The final score in each category is obtained by normalizing the accumulated points by the maximum achievable score. This methodology provides an objective measure of the cognitive performance of the VLA models under realistic UAV operating conditions. B. Evaluation Methodology and Benchmarking Results comprehensive evaluation was conducted using the CognitiveDroneBench benchmark, with the full results presented in Figure 5. In this evaluation, the performance of three modelsRaceVLA, CognitiveDrone, and CognitiveDroneR1was assessed across three cognitive task categories: Human Recognition, Symbol Understanding, and Reasoning. In addition to the individual category scores, an overall Average metric was computed as the success rate across all samples in the benchmark. RaceVLA, optimized primarily for rapid drone racing, demonstrated strong capability in navigating the physical course due to its robust understanding of UAV flight dynamics. However, its performance in executing cognitive tasks was notably poor, with scores of 36.2% in Reasoning, 23.1% Fig. 4. Metrics Overview: (a) L1 loss indicates absolute prediction errors. (b) Action accuracy quantifies the percentage of correct predictions. (c) Cross-entropy loss measures performance on discretized action tokens. of mathematical problem or associating an object with specific attribute (e.g., interpreting the instruction Navigate to the gate with the sweet drink as selecting gate marked with soda logo). Virtual gates, corresponding to the dataset samples, were instantiated within the simulation. Throughout each flight, the UAVs velocity and the head profile (Vx, Vy, Vz, omega) was continuously logged, providing comprehensive record of its dynamic behavior. The dataset was divided into training and test subsets, ensuring an equitable distribution of samples across all three categories in both sets. The training subset was employed to optimize the VLA model parameters, while the test subset forms the foundation of the CognitiveDroneBench benchmark for performance evaluation. B. Model Training The VLA model was fine-tuned using our custom training dataset based on the OpenVLA architecture. The collected data was structured in accordance with the Reinforcement Learning Dataset (RLDS) format to ensure seamless compatibility with OpenVLA. This organization facilitated efficient management of actions, images, and task instructions, rendering the dataset suitable for both imitation learning and task-specific action prediction. Subsequently, the dataset was utilized to fine-tune an OpenVLA-7b model employing parameter-efficient Low-Rank Adaptation (LoRA) approach, applying rank-32 adapters to optimize memory usage while adjusting minimal set of trainable weights. The training configuration consisted of batch size of 64, learning rate of 5 104, and 4000 gradient steps, with image augmentation disabled. Training was conducted on four NVIDIA A100 GPUs, allowing for efficient large-scale optimization while maintaining real-time deployability on UAV hardware. Checkpoints were saved at regular intervals, with evaluations performed Fig. 5. Benchmark performance on CognitiveDroneBench for the RaceVLA, CognitiveDrone, and CognitiveDrone-R1 models. Shown are scores for Reasoning, Human Recognition, and Symbol Understanding tasks, as well as the overall average. in Human Recognition, and 34.6% in Symbol Understanding, resulting in an overall average success rate of 31.3%. These results indicate that while RaceVLA reliably guides the drone through the gates, its decision-making when selecting the correct gate based on task instructions is nearly random. In contrast, the CognitiveDrone model, designed specifically to tackle cognitive challenges, achieved markedly better results: 70.7% in Reasoning, 45.2% in Human Recognition, and 57.7% in Symbol Understanding, yielding an overall average success rate of 59.6%. This significant improvement, particularly in the Reasoning category, highlights the models enhanced ability to process and act upon complex task directives compared to RaceVLA. Moreover, the integration of dedicated VLM reasoning module in the CognitiveDrone-R1 system led to substantial performance gains. CognitiveDrone-R1 achieved success rate of 75.9% in Reasoning, 76.79% in Human Recognition, and 78.9% in Symbol Understanding, with an overall average success rate of 77.2%. Compared to the base CognitiveDrone model, the reasoning module improved the Reasoning score from 70.7% to 75.9% (an increase of about 6%), raised the Human Recognition score from 45.2% to 76.8% (an increase of roughly 31%), and enhanced the Symbol Understanding score from 57.7% to 78.9% (an increase of approximately 21%). Overall, the addition of the reasoning module resulted in an average success rate gain of around 17.6%. In summary, while RaceVLA is effective for basic navigation tasks due to its precise handling of UAV flight physics, it falls short in the nuanced cognitive decision-making required for accurate gate selection. The CognitiveDrone system, particularly when enhanced with an external reasoning module, demonstrates superior performance and robustness across all evaluated categories, underscoring the critical importance of integrating advanced reasoning capabilities into VLA models for UAVs. VI. CONCLUSION an on our with capabilities, In this work, we introduced the CognitiveDrone VLA model for UAV operations that require advanced cognitive enhanced along systemCognitiveDrone-R1that integrates an additional reasoning module to support the drone in solving complex tasks. Experimental open-source evaluations benchmark, CognitiveDroneBench, demonstrate significant performance improvements. While racing-oriented model like RaceVLA achieved an overall success rate of only 31.3%, increased this figure to 59.6%. Furthermore, the integration of the reasoning module in CognitiveDrone-R1 elevated the overall success rate to 77.2%. In particular, CognitiveDrone-R1 improved the reasoning capability by approximately 6% over the base model, and it boosted Human Recognition and Symbol Understanding performance by roughly 31% and 21%, respectively. the CognitiveDrone model These results underscore the critical importance of incorporating advanced reasoning capabilities into VLA models for UAVs, as doing so not only enhances performance in tasks requiring logical inference but also improves overall decision-making across diverse cognitive challenges. Moreover, the introduction of CognitiveDroneBench provides the first dedicated, open-source platform for objectively assessing the cognitive capabilities of VLA models in UAV applications. The complete codebase, dataset, and benchmark are publicly available for further research and development. REFERENCES [1] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Vanhoucke, K. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch, and P. Florence, Palm-e: An embodied multimodal language model, 2023. [Online]. Available: https://arxiv.org/abs/2303.03378 [2] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K.-H. Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich, Rt-1: Robotics transformer for real-world control at scale, 2023. [Online]. Available: https://arxiv.org/abs/2212.06817 [3] A. Brohan, N. Brown, Ichter, A. J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn, P. Florence, C. Fu, M. G. Arenas, K. Gopalakrishnan, K. Han, K. Hausman, Irpan, N. Joshi, R. Julian, A. Herzog, J. Hsu, B. D. Kalashnikov, Y. Kuang, I. Leal, L. Lee, T.-W. E. Lee, S. Levine, Y. Lu, H. Michalewski, I. Mordatch, K. Pertsch, K. Rao, K. Reymann, M. Ryoo, G. Salazar, P. Sanketi, P. Sermanet, J. Singh, A. Singh, R. Soricut, H. Tran, V. Vanhoucke, Q. Vuong, A. Wahid, S. Welker, P. Wohlhart, J. Wu, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich, Rt-2: Vision-language-action models transfer web knowledge to robotic control, 2023. [Online]. Available: https://arxiv.org/abs/2307.15818 [4] E. Collaboration et al., Open x-embodiment: Robotic learning datasets and rt-x models, 2024. [Online]. Available: https://arxiv.org/ abs/2310.08864 [5] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine, P. Liang, and C. Finn, Openvla: An open-source vision-language-action model, 2024. [Online]. Available: https://arxiv.org/abs/2406.09246 [6] S. Belkhale and D. Sadigh, Minivla: better vla with [Online]. Available: https://github.com/ footprint, 2024. smaller Stanford-ILIAD/openvla-mini [7] Octo Model Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, C. Xu, J. Luo, T. Kreiman, Y. Tan, P. Sanketi, Q. Vuong, T. Xiao, D. Sadigh, C. Finn, and S. Levine, Octo: An open-source generalist robot policy, in Proceedings of Robotics: Science and Systems, Delft, Netherlands, 2024. [8] B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone, Libero: Benchmarking knowledge transfer for lifelong robot learning, arXiv preprint arXiv:2306.03310, 2023. [9] Agility Robotics, 2025. [Online]. Available: https://agilityrobotics.com/ [10] A. Lykov, M. Litvinov, M. Konenkov, R. Prochii, N. Burtsev, A. A. Abdulkarim, A. Bazhenov, V. Berman, and D. Tsetserukou, Cognitivedog: Large multimodal model based system to translate vision and language into action of quadruped robot, in Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction, ser. HRI 24. New York, NY, USA: Association for Computing Machinery, 2024, p. 712716. [Online]. Available: https://doi.org/10.1145/3610978.3641080 [11] Q. Wu, Z. Fu, X. Cheng, X. Wang, and C. Finn, Helpful doggybot: Open-world object fetching using legged robots and vision-language models, 2024. [Online]. Available: https://arxiv.org/abs/2410.00231 [12] M. Chang, G. Chhablani, A. Clegg, M. D. Cote, R. Desai, M. Hlavac, V. Karashchuk, J. Krantz, R. Mottaghi, P. Parashar, S. Patki, I. Prasad, X. Puig, A. Rai, R. Ramrakhya, D. Tran, J. Truong, J. M. Turner, E. Undersander, and T.-Y. Yang, Partnr: benchmark for planning and reasoning in embodied multi-agent tasks, 2024. [Online]. Available: https://arxiv.org/abs/2411.00081 [13] Y. Tian, F. Lin, Y. Li, T. Zhang, Q. Zhang, X. Fu, J. Huang, X. Dai, Y. Wang, C. Tian, B. Li, Y. Lv, L. Kovacs, and F.-Y. Wang, Uavs meet llms: Overviews and perspectives toward agentic low-altitude mobility, 2025. [Online]. Available: https://arxiv.org/abs/2501.02341 [14] Y. Fan, W. Chen, T. Jiang, C. Zhou, Y. Zhang, and X. E. Wang, Aerial Vision-and-Dialog Navigation, arXiv preprint arXiv:2205.12219, 2023. [15] Y. Gao, Z. Wang, L. Jing, D. Wang, X. Li, and B. Zhao, Aerial Visionand-Language Navigation via Semantic-Topo-Metric Representation Guided LLM Reasoning, arXiv preprint arXiv:2410.08500, 2024. [16] W. Zhang, Y. Liu, X. Wang, X. Chen, C. Gao, and X. Chen, EmbodiedCity: Embodied Aerial Agent for City-level Visual Language Navigation Using Large Language Model, in 2024 23rd ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN), 2024, pp. 265266. [17] X. Wang, D. Yang, Z. Wang, H. Kwan, J. Chen, W. Wu, H. Li, Y. Liao, and S. Liu, Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology, arXiv preprint arXiv:2410.07087, 2024. [18] S. Liu, H. Zhang, Y. Qi, P. Wang, Y. Zhang, and Q. Wu, AerialVLN: Vision-and-Language Navigation for UAVs, arXiv preprint arXiv:2308.06735, 2023. [19] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, Chatgpt for robotics: Design principles and model abilities, 2023. [Online]. Available: https://arxiv.org/abs/2306.17582 [20] J. Zhong, M. Li, Y. Chen, Z. Wei, F. Yang, and H. Shen, safer vision-based autonomous planning system for quadrotor uavs with dynamic obstacle trajectory prediction and its application with llms, 2023. [Online]. Available: https://arxiv.org/abs/2311. [21] C. A. R. Hoare, VLP: Vision Language Planning for Autonomous Driving, arXiv preprint arXiv:2401.05577, 2024. [22] O. Sautenkov, Y. Yaqoot, A. Lykov, M. A. Mustafa, G. Tadevosyan, A. Akhmetkazy, M. A. Cabrera, M. Martynov, S. Karaf, and D. Tsetserukou, Uav-vla: Vision-language-action system for large scale aerial mission generation, 2025. [Online]. Available: https: //arxiv.org/abs/2501.05014 [23] A. Jiao, T. P. Patel, S. Khurana, A.-M. Korol, L. Brunke, V. K. Adajania, U. Culha, S. Zhou, and A. P. Schoellig, Swarm-gpt: Combining large language models with safe motion planning for robot choreography design, 2023. [Online]. Available: https://arxiv.org/abs/2312.01059 [24] A. Lykov, S. Karaf, M. Martynov, V. Serpiva, A. Fedoseev, M. Konenkov, and D. Tsetserukou, Flockgpt: Guiding uav flocking with linguistic orchestration, in 2024 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct), 2024, pp. 485488. [25] V. Serpiva, A. Lykov, A. Myshlyaev, M. H. Khan, A. A. Abdulkarim, O. Sautenkov, and D. Tsetserukou, Racevla: Vla-based racing drone navigation with human-like behaviour, https://racevla.github.io, 2025, accessed: 2025-03-01. [26] A. Lykov, M. Konenkov, K. F. Gbagbe, M. Litvinov, D. Davletshin, A. Fedoseev, M. A. Cabrera, R. Peter, and D. Tsetserukou, Cognitiveos: Large multimodal model based system to endow any type of robot with generative ai, 2024. [Online]. Available: https://arxiv.org/abs/2401.16205 [27] Figure AI, Helix: vision-language-action model for generalist humanoid control, February 2025, accessed: 26 February 2025. [Online]. Available: https://www.figure.ai/news/helix"
        }
    ],
    "affiliations": [
        "Intelligent Space Robotics Laboratory, Science Center for Digital Engineering, Technology. Skolkovo Institute"
    ]
}