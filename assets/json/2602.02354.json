{
    "paper_title": "Implicit neural representation of textures",
    "authors": [
        "Albert Kwok",
        "Zheyuan Hu",
        "Dounia Hammou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation."
        },
        {
            "title": "Start",
            "content": "Albert Kwok, Zheyuan Hu, Dounia Hammou Department of Computer Science and Technology, University of Cambridge, Cambridge, UK. {ak2441, zh369, dh706}@cam.ac.uk denotes equal contribution. 6 2 0 2 2 ] . [ 1 4 5 3 2 0 . 2 0 6 2 : r Figure 1. (left:) 3D scene rendered with our INR textures, showcasing high visual quality and straightforward deployment. (right:) three-channel mipmap pyramid represented by an MLP."
        },
        {
            "title": "Abstract",
            "content": "Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as new texture INR, which operates in continuous manner rather than discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INRspace generation. 1. Introduction Implicit Neural Representations (INRs) are effective by using neural networks to directly represent continuous, coordinate-based signals (such as images [16], shapes, audio, or physical fields) rather than discrete grids or explicit parametrizations [15]. Examples are multilayer perceptron (MLP) and convolutional neural network (CNN). Textures are one of the largest GPU memory usage and account for the majority of the energy consumed by processors [14]. Thus, INR presents memoryand powerefficient method, while maintaining high rendering quality. Furthermore, the compact form also helps with downstream tasks, including material generation and asset baking. ral models designed for graphics primitives will take as input the UV coordinates and output the corresponding RGB color. The optimized network weights will form compressed representation of textures. The contributions of our work are as follows, (1) implement four different INRs of textures, evaluating three of them in terms of performance, efficiency and memory usage; (2) integrate INRs into Mitsuba 3 [8]1, by extending the plugin that reconstructs from MLP weights during rendering, as shown in Figure 20b; (3) explore downstream tasks, including mipmap fitting and INR weight-space generation. 2. Related work Texture compression. Inspired by discrete cosine transform (DCT), JPEG [3] is an texture image compression method. Adaptive Scalable Texture Compression (ASTC) [12] provides lossy block-based compression over range of bit rates, i.e. 1 to 8 bits per pixel (bpp). In this work, we present neural network as an alternative method with good compression ratio and content-aware adaptation. INR design. Neural network, e.g. Multi-Layer Perceptron (MLP), is universal function approximator [6] for continuous functions. Given discrete samples of data, prior work [16] overfits the MLP, which becomes an implicit neural representation (INR) of images. Below is an overview of different design choices and our contributions. tiny multilayer perceptron (MLP) or other instant neu1Mitsuba 3 is customizable Python / C++ renderer. Periodic activation functions have been proposed in MLP [15], which have been proven to be an efficient signal representation. Positional encoding is crucial to leveraging input from low dimension to high dimension for efficient training. Examples are Fourier encoding [18] and multiresolution hash encoding [11]. The former has been applied in image INR [16] via MLP. In this work, we examine the effectiveness and potential limitations of periodic activation functions and explore both positional encoding schemes in the texture INR domain. 3. Method 3.1. Dataset analysis and sample selection Texture dataset. Describable Textures Dataset (DTD) [4] consists of 47 different categories from human perception and total of 5640 images. From which, we perform analysis and select the most diverse samples based on metrics. To filter out the most diverse images I(u, v) in the frequency domain, we utilize the Laplacian Variance (LAPV) [10], also known as Laplacian response, as measure of focus or sharpness. While different image complexity metrics exist, e.g. compression ratio, image entropy, and edge density, we use LAPV due to its correlation with perceived quality, i.e. FocusMeasure := LAPV(I) = Var(I(u, v)). The dataset Laplacian response histogram is shown in Figure 2, from which we sampled 25 images (Figure 18) at regular intervals. linear combination of values al1 Rdin from previous (l 1)-th layer. The l-th layer output is thus, al = fact(Wlal1 + bl), where weight matrix is Wl Rdindout and bias term bl Rdout.2 SIREN [15] is special type of MLP, with sinusoidal activation functions rather than ReLU, i.e. fact = sin(), which increases overfitting for images, video, sound, and their derivatives, even without any positional encoding. We observe that SIREN is sensitive to the learning rate η, leading to distinct converged results. Hence, smaller η was adopted. Its network weights are distinctly initialized via uniform distributions defined as below, First layer: (cid:32) Hidden layers: (cid:32) (cid:33) , , 1 1 din din (cid:112)6/din ω0 (cid:112)6/din ω0 , (cid:33) , where din is the input dimension of the layer and sine frequency scaling ω0 is the hyperparameter. Fourier encoding. To learn high-frequency functions effectively, we utilize the Fourier feature mapping [18] as positional encoding. By concatenating the input UV coordinates and their frequency features, the input to MLP is thus, Pos(v) = [v, sin(2πfiv)nf i=1, cos(2πfiv)nf i=1], where nf is the number of frequency bands and index loops over all frequencies. They are treated as hyperparameters, adjusted based on validation results. We also implement the extension of using multiresolution hash encoding [11]. However, due to the limited texture resolution, we believe that this would offer little added performance benefit. The assessment of this approach on higher-resolution images is deferred to future work. In conclusion, we implement four different MLP architectures, as shown in Figure 3, and evaluate three of them, i.e., (1) pure MLP with no positional encoding, (2) SIREN, and (3) an MLP with fourier positional encoding. In order to vary the size of the model to compare the performance against the bitrate, we varied the width and depth of the models. For each architecture, we tested the models with 1, 2, or 3 hidden layers of sizes 128, 256, and 512, (except for 3 layers of 512). All of the models were tested with the optimisers Adam and Rprop. Adam generally produced better results across the board. 2Here we assume no hidden layers exist. Otherwise, use dh instead of din when 2 or dout when = 1. Figure 2. Laplacian response histogram with its Cumulative Distribution Function (red curve). 3.2. INR architectures Multi-Layer Perceptron (MLP) [13], also known as Feed-forward Neural Network (FNN), consists of input, hidden and output layers with dimensions din, dh and dout. Each layer contains several neural nodes, which apply non-linear activation function fact (e.g. ReLU [1]) after the 2 (a) General MLP architecture without extra encoding. (b) General MLP architecture with positional encoding. Figure 3. MLP architectures used for INR texture, both without and with positional encoding (pos. enc.). 3.3. Mipmap fitting To model mipmaps, instead of directly integrating over MLPs : (u, v) R3, the models were given an additional normalized input parameter [0, 1], which denoted the level of detail. In this work, we tested models against 6 mipmap levels. The retraining on the augmented dataset removes the hallucinations at points the network hasnt explicitly seen during training. The mipmaps were generated by downscaling the base image by bilinear downsampling3. The generated images were then paired with their relevant u, v, and coordinates to produce the final dataset. In this way, each pixel sample is equally weighted, such that the higher resolution images have greater impact on the loss than the lower resolution images. 3.4. INR-space generation Inspired by both image-space and neural weight-space generation [21], we investigate generation on the new data modality of textures, i.e. INR. The goal is to generate new MLP weights, which is equivalent to novel textures after inference. To achieve this, we deploy generative methods, e.g. diffusion or VAE, on MLP weights4. They are termed hypernetworks [5], due to the nature of taking neural network weights as input. It reduces the input dimensionality and thus training cost. The evaluation focuses on the fidelity and coverage between synthetic and reference sets. Data augmentation. To avoid underfit for generation models, we adopt the RGB permutation (Figure 4) to increase the size of training dataset. As an extension, linear interpolation of two randomly selected textures could be adopted in future work. Diffusion model is modelled by the Markov chain along time steps [0, ] Z+. The forward process q(xtxt1 3We use resize() from scikit-image with default order of 1 in spline interpolation, i.e. bilinear downsampling. 4This feature was implemented at INR space generation branch. Figure 4. Texture RGB augmentation for generation. adds Gaussian noise to the input data x0, while backward process pθ(xt1xt) removes noise from the random Gaussian samples xT . In our work, separate transformer-based network predicts the pθ(xt1xt). For mathematical derivation, please refer to [17, 21]. 4. Evaluation Evaluation metrics. We evaluate the reconstructed texture quality of INR by measuring the pixel-wise errors (MAE, MSE, PSNR) and perceptual and structural similarity (SSIM, LPIPS, VMAF) against the ground truth gt. Define the coloured image as Iw,h,c : {1, ..., } {1, ..., H} [1, ..., C] [0, 1], where and are the width and height, and is the number of colour channels. MAE (Mean Absolute Error) measures the average absolute pixel differences between w,h and gt w,h, AE = 1 (cid:88) (cid:88) (cid:88) w= h=1 c=1 w,h,c gt w,h,c. MSE (Mean Squared Error) measures the average squared pixel differences between w,h and gt w,h, SE = 1 (cid:88) (cid:88) (cid:88) w=1 h=1 c=1 w,h,c gt w,h,c2. PSNR (Peak Signal-to-Noise Ratio) [7] is normalized fraction of the maximum signal power to that of the noise, where AXI [0, 1] is the maximum pixel value of images, SN = 10 log10 AXI SE [dB]. SSIM (Structural Similarity Index) [19], as perceptionbased metric, approximates local luminance muI , contrast σI and structure σI r,I gt via the image Is Y-channel mean, variance and covariance respectively, to accurately learn similar features. These results imply that the INRs may struggle to accurately capture details. This could be improved by adjusting the frequency parameters of the Fourier MLPs and the SIRENs. µI = , σI = ar(I ), σr,gt = Cov(I r, gt). Luminance, contrast, and structure are defined below, = = , 2µI µI gt + C1 µ2 + µ2 gt + C1 σI r,I gt + C3 σI σI gt + C3 , = 2σI σI gt + C2 + σ2 σ2 gt + C2 constants C1, C2, C3 R. , SSIM is defined by the product of the luminance, contrast and structure functions above, SSIM = lα cβ sγ, α, β, γ > 0. LPIPS (Learned Perceptual Image Patch Similarity) [20] extracts perceptional features from deep learning networks, where fl are the feature maps at layer with corresponding weights wl, LP IP = 1 (cid:88) (cid:88) w,h wl fl(I r) fl(I gt)2 2. VMAF (Video Multimethod Assessment Fusion) [9], originally developed by Netflix, applies SVM regression on image perception features VIF (visual information fidelity), ADM (Additive Detail Metric), and the motion feature. Here, we only applied the metric to images rather video, i.e. setting number of frames to be 1. For details, please refer to [2]. 4.1. Single texture performance evaluation Figure 5 shows the plots for the performance for the INRs against the bitrate of the model, for each model architecture. To reduce noise, this plot is bucketed by rounding the bitrate to the nearest 2 bits. This was done because the images in the dataset have differing sizes, and therefore the results have large variety of bitrates, which produces visual noise when plotted. For LPIPS, both the SIRENs and the Fourier MLPs performed very well, being very close to the maximum similarity to the original of 0, though the SIRENs had noticeable drop in performance for very high compression. On the other hand, the pure MLP clearly struggles to meet the performance of the other two. This shows that the INRs effectively captured key features of the image, effectively capturing the high-level structure of the image. The SSIM, VMAF, and PSNR of the INRs are more mixed, with noticeable difference to the original. Across both metrics, the best performing INRs are the Fourier MLPs, followed by the SIRENs. All of the models have broadly similar curves, implying that the models all struggle 4 4.2. Qualitative architecture evaluation The different architectures produced different kinds of artefacts. The simplest are those of the pure MLP, which consistently produced blurry results as expected: Fourier encoding and SIRENs were proposed to help MLPs learn higher frequency features. These blurry results sometimes had clear, straight lines through them, which if the image consisted mostly of straight lines allowed the pure MLP to achieve reasonable results. Some of these effects are shown in Figure 6. The Fourier encodings usually produced straight line artefacts, especially vertical and horizontal lines, reflective of the structure of the encoding. These artefacts were usually successfully trained out, but can be seen in early epochs. After training, these models still frequency produce grainy noise, which degrades its ability to learn large areas of flat colours. This noise is perceptually weak, whilst still having significant per pixel impacts. Some of these effects are shown in Figure 7. The SIRENs produced lumpy curve artefacts. These were harder to overcome, and can be still clearly seen affecting the results of several images. This may reflect why the SIRENs sometimes struggled to match performance with the other models: these artefacts can mean it struggles to learn straight lines, with noticeably poor performance on very geometric textures, as shown by Figure 8. 4.3. Comparison of Adam and Rprop Overall, Adam produced notably better and more consistent results than Rprop. The difference can be seen visually in the learning snapshots presented in Figures 9, 10, & 11. Across all three examples, the Rprop is clearly blurrier, with some of the images having additional artefacts from the learning process, such as the rectangular structures as result of the high frequencies of the Fourier encoding. It is worth noting that the given image for the pure MLPs represents an unusually large difference in performance between Adam and Rprop: for most of the images learned by the pure MLP neither optimiser made significant progress to accurately learning the image. Conclusion Adam has demonstrated itself to be the better optimiser across the board. Whilst it is possible that hyper parameter tuning could improve Rprops performance, we doubt that it will improve performance enough to consistently and significantly outperform Adam. (a) Performance measured by LPIPS. (b) Performance measured by VMAF. (c) Performance measured by SSIM. (d) Performance measured by PSNR. Figure 5. Texture INR performance against bitrate (bits per pixel). Figure 6. Some artefacts learned by the pure MLPs. Figure 7. Some artefacts learned by the fourier encoded MLPs. Figure 8. Some artefacts learned by the SIRENs. 4.4. Comparison to ASTC Figure 12 compares the performance of the best architecture (Fourier MLPs) against ASTC. As the graph clearly demonstrates, whilst ASTC achieves good compression ratios, it has clear sacrifice in quality, especially in LPIPS. The VMAF plot is representative of the results for SSIM and PSNR (which are in Figure 13): whilst the INR does perform consistently better, the result is noisy, and is less significant than the gain in LPIPS. Efficiency. Training occurred at around 0.5-2 iterations per second on device with an RTX 5080Ti, for total of 50 to 200 seconds for all 50 iterations. This makes it practical to use in non-real-time applications, though is slow enough 5 (a) 1Epochs Adam (b) 10Epochs Adam (c) 20Epochs Adam (d) 30Epochs Adam (e) 40Epochs Adam (f) 50Epochs Adam (g) 1Epochs Rprop (h) 10Epochs Rprop (i) 20Epochs Rprop (j) 30Epochs Rprop (k) 40Epochs Rprop (l) 50Epochs Rprop Figure 9. The texture learned by Fourier encoded MLP with 1282 hidden layers learning with Adam and Rprop. (a) 1Epochs Adam (b) 10Epochs Adam (c) 20Epochs Adam (d) 30Epochs Adam (e) 40Epochs Adam (f) 50Epochs Adam (g) 1Epochs Rprop (h) 10Epochs Rprop (i) 20Epochs Rprop (j) 30Epochs Rprop (k) 40Epochs Rprop (l) 50Epochs Rprop Figure 10. The texture learned by SIREN with 2563 hidden layers learning with Adam and Rprop. that extensive hyper-parameter search is still large time investment. Rendering time with MLP-based texture inference scales proportionally with the total number of samples per pixel (SPP) and the aggregated seeds. For spp = 1 and seed = 1, the teaser rendering time on an Apple M1 CPU is 4.7 seconds. 4.5. Mipmapped texture performance evaluation Figure 15 shows that the models have model mipmapped textures well, with similar results to textures without mipmapping. This demonstrates that adding mipmapping has little impact to the performance of the model. Surprisingly, the VMAF was significantly better than without mipmapping. This is visually confirmed by Figure 14. 4.6. INR-space generation evaluation We include the INR-space diffusion training loss in Figure 16, generative samples in Figure 17. We argue that the generative quality is yet to be perfect, despite model convergence. This might be limited by scarce training INR weights (around 600). We will explore with increase of training dataset and evaluate the fidelity and coverage between generative and reference set. 5. Conclusions In this work, we present how different INRs for textures can be built, with evaluation over their performance, efficiency, memory usage and complexity. In addition, we integrate INR in graphics pipeline, and different down-stream tasks. We have shown that perceptually, INRs can be very effective at image compression, though can struggle on exact details. Despite this, INRs can outperform classical compression methods across all metrics for similar bitrates. We have shown that whilst simple MLPs can achieve good results, Fourier encoding and SIRENs provide significant improvement. We have also shown the importance of hyperparameter tuning, which can significantly alter the performance for different image types. Author contributions. Please refer to Section 7. 6 (a) 1Epochs Adam (b) 10Epochs Adam (c) 20Epochs Adam (d) 30Epochs Adam (e) 40Epochs Adam (f) 50Epochs Adam (g) 1Epochs Rprop (h) 10Epochs Rprop (i) 20Epochs Rprop (j) 30Epochs Rprop (k) 40Epochs Rprop (l) 50Epochs Rprop Figure 11. The texture learned by simple MLP with 2563 hidden layers learning with Adam and Rprop. (a) Performance measured by LPIPS. (b) Performance measured by VMAF. Figure 12. ASTC and the best INR performance against bitrate (bits per pixel). gests that even higher compression ratios might be possible when compressing multiple similar images together, which could be used when compressing image libraries, or animations. The papers exploration of mipmapping could also be taken further. Effective compression for anisotropic filtering could allow the technique to use less GPU RAM. INRs could also provide novel opportunities in perfectly modelling the filtering, taking in the viewing angle and lod, and outputting the colour averaged over the pixel area. In addition to better representing filtering, INRs could also efficiently represent Spatially Varying BiDirectional Scattering Distribution Function (SVBSDF) [21], allow for more expressive materials to be efficiently stored in memory. 6. Future directions This paper has demonstrated that for reliable INR compression are consistent process is needed for selecting hyperparameters. Notably, due to time and computation constraints, we were unable to tune the frequency values used by the SIRENs and Fourier MLPs, which could had significant impacts on the performance. For INR compression to be fast and reliable, an efficient way to select these hyperparameters should be developed. Future work could explore the selection of hyper-parameters, either through traditional image processing techniques such as the Fourier Transform, or machine learning techniques such as training model to select the best hyper parameters. In addition to simply learning the images pixels, the model could be trained on greater variety of randomly sampled points, which may prevent grid-related artefacts when training. This paper has also demonstrated the potential of encoding multiple images into one image, in this case through mipmaps without significantly impacting quality. This sug7 (a) Performance measured by SSIM. (b) Performance measured by PSNR. Figure 13. ASTC and the best INR performance against bitrate (bits per pixel). Figure 14. Some learned mipmap pyramids, with the figure on the left being modelled by SIREN, and the figure on the right being modeeled by Fourier encoded MLP 8 (a) Performance measured by LPIPS. (b) Performance measured by VMAF. (c) Performance measured by SSIM. (d) Performance measured by PSNR. Figure 15. Mipmapped texture INR performance against bitrate (bits per pixel). Figure 16. INR-space Diffusion model training loss. Figure 17. Generated texture samples from INR-space diffusion. 9 neural representations with periodic activation functions. In Proceedings of the 34th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2020. Curran Associates Inc. 1, [16] Yannick Strumpler, Janis Postels, Ren Yang, Luc Van Gool, and Federico Tombari. Implicit Neural Representations for In Computer Vision ECCV 2022: Image Compression. 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XXVI, page 7491, Berlin, Heidelberg, 2022. Springer-Verlag. 1, 2 [17] Chong Su, Yingbin Fu, Zheyuan Hu, Jing Yang, Param Hanji, Shaojun Wang, Xuan Zhao, Cengiz Oztireli, and Fangcheng Zhong. Chord: Generation of collision-free, house-scale, and organized digital twins for 3d indoor scenes with controllable floor plans and optimal layouts, 2025. 3 [18] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In Proceedings of the 34th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2020. Curran Associates Inc. 2 [19] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4): 600612, 2004. 4 [20] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The Unreasonable Effectiveness of Deep Features as Perceptual Metric. In CVPR, 2018. 4 [21] Chenliang Zhou, Zheyuan Hu, Alejandro Sztrajman, Yancheng Cai, Yaru Liu, and Cengiz Oztireli. M3ashy: Multi-modal material synthesis via hyperdiffusion. In Proceedings of the 40th AAAI Conference on Artificial Intelligence, 2026. 3,"
        },
        {
            "title": "References",
            "content": "[1] Abien Fred Agarap. Deep Learning using Rectified Linear Units (ReLU), 2019. 2 [2] Kirill Aistov and Maxim Koroteev. VMAF reimplementation on PyTorch: Some experimental results. https://github.com/alvitrioliks/VMAF-torch, 2023. 4 [3] CCITT. T.81 digital compression and coding of continuous-tone still images requirements and guidelines. Technical report, International Telegraph and Telephone Consultative Committee, 1992. Accessed: 2025-11-11. 1 [4] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing Textures in the Wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014. [5] Ziya Erkoc, Fangchang Ma, Qi Shan, Matthias Nießner, and Angela Dai. HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1430014310. IEEE/CVF, 2023. 3 [6] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359366, 1989. 1 [7] Alain Hore and Djemel Ziou. Image Quality Metrics: PSNR vs. SSIM. In 2010 20th International Conference on Pattern Recognition, pages 23662369, 2010. 3 [8] Wenzel Jakob, Sebastien Speierer, Nicolas Roussel, Merlin Nimier-David, Delio Vicini, Tizian Zeltner, Baptiste Nicolet, Miguel Crespo, Vincent Leroy, and Ziyi Zhang. Mitsuba 3 renderer, 2022. https://mitsuba-renderer.org. 1 [9] Zhi Li, Anne Aaron, Ioannis Katsavounidis, Anush VMAF: The jourIn The Netflix Tech Blog, 2016. Moorthy, and Megha Manohara. ney continues. https://netflixtechblog.com/vmaf-the-journey-continues44b51ee9ed12. 4 [10] Farida Memon, Mukhtiar Ali Unar, and Sheeraz Memon. Image Quality Assessment for Performance Evaluation of Focus Measure Operators, 2016. 2 [11] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Trans. Graph., 41(4), 2022. 2 [12] J. Nystad, A. Lassen, A. Pomianowski, S. Ellis, and T. Olson. In Proceedings of Adaptive scalable texture compression. the Fourth ACM SIGGRAPH / Eurographics Conference on High-Performance Graphics, page 105114, Goslar, DEU, 2012. Eurographics Association. 1 [13] Marius-Constantin Popescu, Valentina E. Balas, Liliana Perescu-Popescu, and Nikos Mastorakis. Multilayer perceptron and neural networks. WSEAS Trans. Cir. and Sys., 8(7): 579588, 2009. 2 [14] B. V. N. Silpa, Anjul Patney, Tushar Krishna, Preeti Ranjan Panda, and G. S. Visweswaran. Texture filter memory power-efficient and scalable texture memory architecture for mobile graphics processors. In 2008 IEEE/ACM International Conference on Computer-Aided Design, pages 559 564, 2008. 1 [15] Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Acknowledgement We are deeply grateful to our supervisor, Dounia Hammou, for her insightful guidance and detailed suggestions throughout this project and in the preparation of this report. Special thanks to Professor Rafał Mantiuk for lecturing and organizing the Advanced Graphics and Image Processing module, which has been enjoyable along the way. Author contributions. Both authors jointly discussed and researched relevant prior work, assisted each other with debugging, and participated in reviewing and improving the overall codebase. - Albert proposed the idea of adopting SIREN, implemented the multi-resolution hash encoding, expanded the evaluation to cover the entire dataset, and added in evaluation with LPIPS and VMAF. He also proposed the idea of utilising traditional image metrics to select subset of the Describable Textures Dataset for evaluation. He authored the Evaluation section, including the performance-bitrate plot with confidence intervals, and as an extension, explored the use of INRs for mipmapping. - Zheyuan (Peter) performed the data analysis and selection, implemented the naıve MLP and its periodic-activated and Fourier-encoded variant, the code for calculating the MAE, MSE, PSNR, and SSIM, integrated with Mitsuba renderer, and evaluated models on individual images. He also chose the images to evaluate in the dataset. He authored the Method section and, as an extension, investigated INR-space generation. 8. Selected dataset As per 3, we visualize the selected 25 textures in Figure 18. 9. Methodology 10. Addition results 10.1. Evaluation of one texture sample For single data sample bubbly 0122, we visualize the training loss function, reconstructed texture, and residual error in Figure 19. The error metric is reported in Table 1. 10.2. Renderer: INR integration We demonstrate the three-colour-channel mipmap pyramid of the texture bubbly 0122 in Figure 20a. With integration with Mitsuba 3 renderer, we are able to render mate1 Figure 18. Sampled textures with their Laplacian response. Figure 19. Qualitative visualization comparison. Table 1. Evaluation Metrics Metric MAE MSE PSNR (dB) SSIM Value 8.0592 121.2885 27.29 0.9881 rials real-time from MLP weights. Figure 20b shows the renderer result from INR weights of bubbly 0122. tion, spectral error, and L2 norm of differential errors for derivative accuracy. Ablations vary activation types, initialization scales, network depth/width, and input encoding bandwidth to quantify their effects. Justification (max 150) The applicants combine strong foundation in machine learning and graphics with hands-on experience in neural representation research. Practical skills include the implementation of deep MLP architectures and automatic differentiation for derivative-based losses; proficiency in PyTorch enables rapid prototyping and reproducible experiments. The candidates also have proficiency in OpenGL, enabling the results to be tested with shaders. Prior projects have involved material reconstruction. The candidates demonstrate methodological rigour in experimental design and quantitative evaluation, ensuring that findings will be robust, interpretable, and of direct relevance to the INR community."
        },
        {
            "title": "Work list",
            "content": "The work list is included, incorporating suggestions provided by Dounia Hammou and Professor Rafał Mantiuk. * Focus on the benefits of implicit neural representation (a continuous function rather than discrete set of samples, resolution can be high). * Generate mipmap levels directly from the neural representation, rather than storing precomputed ones. Compare with other methods? * Extension: sampling from such representation - (anisotropic) texture filtering. * Extension: how to compress multiple similar textures into one neural representation, and report the compression efficiency-quality tradoff. * Try at least two different implicit representations. * Extension: build an impressive off-line rendering system - all can be done in PyTorch. (a) Input image mipmap. (b) Rendered result from realtime INR plugin. 11. Proposal Project 1: Implicit neural representation of textures. Description (max 250) The project area of Implicit Neural Representations (INRs) with periodic activation functions follows growing interest in using neural networks to directly represent continuous, coordinate-based signals (such as images [16], shapes, audio, or physical fields) rather than discrete grids or explicit parametrizations [15]. key insight is that standard networks (e.g., multilayer perceptrons with ReLU activations) suffer from so-called spectral bias (or frequency principle): they tend to fit lowfrequency components of the target and struggle with fine high-frequency variation. To solve this, [15] proposes replacing standard activations with sinusoidal (periodic) activations (so-called SIRENs) enables the network to represent signals and their derivatives with higher fidelity and expressivity. The project explores how choosing the architecture, activation function, and initialization for coordinate-based neural networks influences their spectral capacity, smoothness properties, and ability to encode derivatives and periodic activation functions represent powerful design choice in this context. Methodology (max 250) The approach in [15] combines architectural choices, input encodings, and training regimes to obtain accurate, stable implicit neural representations that capture high-frequency detail and well-behaved derivatives. SIREN-style multilayer perceptron will serve as the core model: fully connected layers with sinusoidal activations and the SIREN initialization to allow expressive periodic basis functions. As an alternative baseline, models with Fourier feature input mappings have been implemented to evaluate tradeoffs between input encoding and periodic activations. Optimization uses Adam with learning-rate scheduling, and regularization techniques (weight decay, gradient clipping) prevent instabilities caused by high-frequency modes. Evaluation metrics include PSNR/SSIM for reconstruc-"
        }
    ],
    "affiliations": [
        "University of Cambridge"
    ]
}