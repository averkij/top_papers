{
    "paper_title": "MIST: Mutual Information Via Supervised Training",
    "authors": [
        "German Gritsai",
        "Megan Richards",
        "Maxime Méloux",
        "Kyunghyun Cho",
        "Maxime Peyrard"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 5 4 9 8 1 . 1 1 5 2 : r MIST: MUTUAL INFORMATION ESTIMATION VIA SUPERVISED TRAINING German Gritsai, Megan Richards , Maxime Meloux, Kyunghyun Cho Maxime Peyrard Universite Grenoble Alpes, CNRS, Grenoble INP, LIG New York University Prescient Design, Genentech Equal contributions and shared first authorship {melouxm, peyrardm}@univ-grenoble-alpes.fr {mr7401, kyunghyun.cho}@nyu.edu {gritsai.gm}@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "We propose fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is function of the observed sample from two random variables, we parameterize this function with neural network (MIST) and train it end-to-end to predict MI values. Training is performed on large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return single point estimate. This research program departs from prior work by taking fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MIs invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions."
        },
        {
            "title": "INTRODUCTION",
            "content": "Mutual information (MI) is measure of nonlinear statistical dependence between two random variables, quantifying how much knowing one variable reduces the uncertainty about the other (Shannon, 1948; MacKay, 2003). For two continuous random variables and , the MI is written in terms of their joint distribution P(X,Y ) and marginal distributions PX and PY : I(X; ) = (cid:90) (cid:90) P(X,Y )(x, y) log (cid:18) P(X,Y )(x, y) PX (x) PY (y) (cid:19) dx dy. (1) Mutual information is one of the most widely applied information measures in data science, finding applications in feature selection (Peng et al., 2005; Kwak & Choi, 2002), causality (Butte & Kohane, 1999), representation learning (Chen et al., 2016; Higgins et al., 2018; Tishby & Zaslavsky, 2015; Zhao et al., 2018), reinforcement learning (Oord et al., 2018; Pathak et al., 2017), and generative modeling (Alemi et al., 2016; Alemi & Fischer, 2018; Huang et al., 2020). 1 Calculating MI requires access to the joint and marginal distributions, which are rarely known in practice. Consequently, most approaches infer MI from finite samples drawn from these distributions. Existing estimators fall into two main families. (1) Density estimators approximate the underlying densities, for example, using kernel density estimation, k-nearest neighbors, or variational autoencoders. MI can then be computed by substituting these estimates into one of MIs definitions (such as Equation 1). Prominent density estimators include KSG (Kraskov et al., 2004) and MINDE (Franzese et al., 2023). (2) Density relationship estimators instead estimate the relationship between the joint and marginal distributions, often using variational lower bounds derived from the DonskerVaradhan representation of the KullbackLeibler divergence (see Appendix A.1 for these definitions). Notable examples include MINE (Belghazi et al., 2018), NWJ (Nguyen et al., 2010b), and SMILE (Song & Ermon, 2019). In both families of approaches, neural methods have been developed to improve the estimators capacity to capture complex density functions or density relationships. Broadly, these neural methods have achieved better performance on moderately complex distributions and high-MI settings. However, both families of approaches struggle in challenging settings that are common in application, including high-dimension, limited samples, complex distributions, and high MI (Czyz et al., 2023; McAllester & Stratos, 2020). Prominent examples of these settings include applications in genomics (Klein et al., 2015), neuroscience (Ganguli & Sompolinsky, 2012), astronomy (Pandey & Sarkar, 2017), and machine learning (Goldfeld et al., 2018). While previous benchmarking efforts have expanded the set of distribution families considered, MI estimators have rarely been empirically evaluated under these challenging conditions individually, and even more rarely in combination. Most empirical evaluations have studied estimation for MI below 1, for fewer than 5 distributions, with minimum sample size above 10,000, and for higher dimensions limited to normal distributions. See Appendix A.3 for an overview of the empirical evaluation scale for existing methods. In this work, we propose fully data-driven, empirical approach to designing mutual information estimators. Instead of building estimators that compute MI by approximating density functions or their relation, we learn to predict MI directly from data. We parameterize mutual information estimator as neural network and train such network end-to-end on large meta-dataset of synthetic joint distributions with known ground-truth MI values. Our proposed model, MIST (Mutual Information estimation via Supervised Training), handles arbitrary sample sizes and dimensions, and features built-in uncertainty estimates. This meta-learning approach takes inspiration from amortized inference procedures such as simulation-based inference (SBI; Papamakarios et al., 2019; Cranmer et al., 2020), amortized Bayesian inference (ABI; Goncalves et al., 2020; Elsemuller et al., 2024; Radev et al., 2023; Avecilla et al., 2022; Gloeckler et al., 2024), and meta-statistical learning (Peyrard & Cho, 2025). Our work focuses on the setting for MI estimators that was underexplored in previous work. Empirically, we conduct large-scale evaluation that systematically explores the performance of our estimator in the most difficult regimes: with fewer samples (10-500), in higher dimensions (2-32), and across wider range of MI values (0-40). We find that our learned estimator outperforms existing methods across these conditions, especially in low-sample regimes. We provide an empirical study of the behavior of estimators under increasing dimensionality, finding that our models require roughly half as many samples on average to provide reliable MI estimates as the best baseline. The quantile-based intervals are well-calibrated and more reliable than bootstrap alternatives, while inference is orders of magnitude faster than existing neural methods. Finally, the learned estimators generalize to unseen distributions and sample sizes. Beyond empirical performance, this meta-statistical framework establishes new paradigm for constructing information-theoretic estimators. Because the estimator is fully differentiable and trainable, it can be embedded in larger learning pipelines, fine-tuned to specific data modalities, and systematically improved through curated meta-datasets. Leveraging MIs invariance under invertible transformations, synthetic training distributions can, in the future, be adapted to arbitrary domains and modalities via normalizing flows (Papamakarios et al., 2021), paving the way for generalpurpose, learned estimators of mutual information. We provide an open-source library1 for training and evaluating meta-learned MI estimators, and include detailed discussion of these opportunities and future work in Section 5. 1https://github.com/grgera/mist"
        },
        {
            "title": "2 RELATED WORK",
            "content": "Mutual information estimation is foundational challenge across data science tasks, for which wide variety of estimation methods have been developed. Although these approaches vary in their mathematical foundations and computational strategies, they can be grouped into two main families: density estimators and density ratio estimators. Density Estimators. Density estimators compute mutual information estimates by performing intermediate estimates of density functions. The most straightforward approach is to use MIs analytical definition, which requires estimates of the joint and marginal density functions. Histogram MI estimators approximate these density functions by partitioning the support of each variable into set of bins and counting the frequencies in each (Fraser & Swinney, 1986). Histogram estimators suffer from very poor scaling and are highly sensitive to the choice of bins, leading to research on optimizing the bin selection (Darbellay & Vajda, 1999). Kernel density estimators (KDE; Moon et al., 1995; Silverman, 2018) were later developed to help avoid this sensitivity by replacing fixed bins with kernel functions centered at each sample point. Kozachenko (1987) designed method for entropy estimation using neighborhood distances, an approach which was later refined with weighted variants (Berrett et al., 2019). The Kraskov-StogbauerGrassberger estimator (KSG; Kraskov et al., 2004) extended this k-NN framework to mutual information estimation using MIs expression in terms of conditional entropy (see Appendix A.1). KSG is one of the most broadly used MI estimators, especially for low-sample settings, and serves as primary benchmark comparison in this work. Several approaches have explored the use of deep generative models for density estimation. One line of work uses normalizing flows to reconstruct density functions directly. However, these methods suffer from poor estimates even for data with simple structure (Song & Ermon, 2019; Nguyen et al., 2010a). The Barber-Agakov bound (BA; Barber & Agakov, 2004) provides variational lower bound on MI by introducing variational approximation q(xy) to the true conditional p(xy). Several works have applied this bound using conditional generative models with different choices for the marginal distribution (Alemi et al., 2016; Chalk et al., 2016; Kolchinsky et al., 2019). Later work used VAEs to learn the joint and marginal density functions directly (Song & Ermon, 2019). Most recently, MINDE (Franzese et al., 2023) trains conditional or joint diffusion models to learn score functions, then applies the Girsanov theorem to compute KL-divergence as the expected difference between joint and marginal scores. VCE (Chen et al., 2025) is concurrent work to ours, and writes MI as the negative differential entropy of the vector copula density. Their approach uses flow models to estimate the marginal densities, then estimates the vector copula density using maximum likelihood estimation. Another line of density estimators has focused on building MI estimators for complex settings by learning methods to simplify the distributions before applying classical estimators or computing MI directly. MIENF (Butakov et al., 2024) learns pair of normalizing flows that apply MI-preserving transformations to the joint distribution, enabling closed-form MI calculation. Latent-MI (LMI; Gowri et al., 2024) trains network to compress data with minimal mutual information loss, then applies KSG to the low-dimensional embeddings. These methods have improved abilities to handle high-dimensional data, but have large sample and computational requirements, and rely on the existence of simple or low-dimensional structure in high-dimensional data. Density Ratio Estimators. Density ratio estimators compute mutual information by estimating the relationship between the joint density (X, ) and the product of marginal densities PX PY . Since mutual information can be equivalently expressed as the Kullback-Leibler divergence DKL(P (X, )PX PY ) (Donsker & Varadhan, 1975), many density ratio methods leverage variational bounds on KL divergence. Several methods have been developed by defining discriminative tasks that implicitly predict this density ratio. MINE (Belghazi et al., 2018) trains neural network critic to differentiate joint and marginal samples, optimizing the critic to maximize the Donsker-Varadhan (DV) lower bound. The Nguyen-Wainwright-Jordan estimator (NWJ; Nguyen et al., 2010b) trains critic model similarly, but uses an alternative variational bound which offers lower variance at the cost of looser bound. InfoNCE (Oord et al., 2018) trains classifier critic to identify true joint samples among negative samples drawn from the marginals. SMILE (Song & Ermon, 2019) combines the InfoNCE and NWJ bounds through clip operation, interpolating 3 between their respective bias-variance profiles to achieve more flexible trade-offs. The limitations of variational estimators are well-studied in McAllester & Stratos (2020), Song & Ermon (2019), and Poole et al. (2019), and include high variance estimates, high sample requirements, and poor estimates for high MI settings. We refer the reader to Poole et al. (2019) for detailed review of MI variational bounds. Evaluation of MI Estimators. Evaluating MI estimators is itself challenging: it requires distributions with known ground-truth MI and large sample sizes (required by most methods). Existing work has therefore primarily focused on theoretical guarantees of low bias, consistency with increasing samples, and the tightness of variational bounds (Belghazi et al., 2018; Nguyen et al., 2010b; Song & Ermon, 2019). Empirical evaluations have been limited in scope. Early methods, such as binning and kernel density estimators, were tested on simple time-series data like sine waves (Moon et al., 1995). Later neural methods began evaluation on smaller set of synthetic distributions: MINE and SMILE were evaluated on 34 distributions, including high-dimensional Gaussians up to 20D (Belghazi et al., 2018; Song & Ermon, 2019). The majority of methods have been evaluated exclusively on MI values below 1, often on Gaussian or near-Gaussian distributions (see Table 3). Methods that consider higher dimensionalities do so largely by scaling low-dimensional distributions (assuming low-dimensional latent structure), or test only on high-dimensional Gaussians (Gowri et al., 2024; Franzese et al., 2023). Recently, the BMI benchmark (Czyz et al., 2023) applied 20 invertible transformations to normal and Students t-distributions to create broader set of distribution families with unique challenges, such as sparsity and long tails. This evaluation on more complex distributions highlighted the limitations of many existing estimators, leading authors to argue that multivariate normal distributions provided overly optimistic view of estimator performance. BMI has been used by some of the most recent estimators, such as MINDE. We provide description of our datasets in Appendix B, and an overview of existing evaluation scopes in Appendix A.3. Machine Learning for Statistical Inference. Our work aligns with the broader research direction on learning inference procedures. relevant related direction concerns neural processes (Garnelo et al., 2018b;a; Kim et al., 2019; Gordon et al., 2020; Markou et al., 2022; Huang et al., 2023; Bruinsma et al., 2023), which can predict latent variables of interest from datasets (Chang et al., 2025) by leveraging transformers (Nguyen & Grover, 2022) and Deep Sets (Zaheer et al., 2017) to enforce permutation invariance (Bloem-Reddy & Teh, 2020). related approach, known as prior-fitted networks, has demonstrated that transformers can be effectively repurposed for Bayesian inference (Muller et al., 2022) and optimization tasks (Muller et al., 2023). Additionally, there is growing interest in using trained models to assist in statistical inference tasks (Angelopoulos et al., 2023) and optimization (Lueckmann et al., 2017; Liu et al., 2020; Simpson et al., 2021; Amos, 2023) like simulation-based inference (SBI; Papamakarios et al., 2019; Cranmer et al., 2020), amortized Bayesian inference (ABI; Goncalves et al., 2020; Elsemuller et al., 2024; Radev et al., 2023; Avecilla et al., 2022; Gloeckler et al., 2024) and the learning of frequentist statistical estimators (Peyrard & Cho, 2025)."
        },
        {
            "title": "3 LEARNING FRAMEWORK",
            "content": "The methods available today decompose MI estimation into intermediate tasks of density function or density ratio estimation. To estimate density function (or ratio) beyond very simple settings, these methods require large amounts of samples as part of inference. Even with access to high number of samples, density estimation itself is notoriously difficult and unsolved problem, particularly for high-dimensional or complex distributions (Meszaros et al., 2024). As result, existing MI estimators still fail in many realistic data settings. They scale poorly to high-dimensional data and complex distributions, are unstable for low-sample settings, and require expensive inference procedures. An additional consequence is that studying and improving MI estimators is challenging, with new methods often resulting from theoretical discoveries. Our work offers an alternative philosophy for designing mutual information estimators. Instead of building estimators that compute MI with pre-specified algorithm (e.g., approximating density functions or their ratios), we propose to predict MI directly from data. We take fully empirical 4 Figure 1: We propose fully data-driven, empirical approach to designing mutual information estimators. We design large empirical meta-dataset composed of samples from set of distributions with known MI (left), and train SetTransformer-based model to predict MI directly from sets of samples (right). perspective, asking: Can model learn generalizable algorithm for predicting mutual information directly from samples? To learn this algorithm, we parameterize mutual information estimator as neural network that we call MIST (Mutual Information estimation from Supervised Training). MIST takes as input set of paired samples {(xi, yi)}n i=1 drawn from joint distribution PXY , and outputs an estimate of I(X; ). The model is trained end-to-end on large meta-dataset of synthetic joint distributions with known ground-truth MI values. We have reflected our framework in Figure 1 and described the meta-dataset design, architecture, and training choices below. The Learning Task Let Γ denote family of joint data-generating distributions over Y. For any γ Γ, let Iγ denote the mutual information between and under γ. mutual information estimator observes finite dataset := {(xi, yi)}n i=1 drawn i.i.d. from γ, and aims to estimate Iγ from samples alone. We parametrize the estimator as function fθ : (cid:83) n=1(X Y)n R0, where θ are neural network weights. To train fθ, we assume access to meta-distribution PΓ over generative laws. First, distribution γ PΓ is sampled, then dataset is drawn from γ, together with its ground-truth mutual information Iγ. The training objective is to minimize the mean squared error: LMSE(θ) = (cid:104) (cid:0)fθ(D) Iγ (cid:1)2 (cid:105) , (1) where the expectation is taken over γ and γ PΓ. In the limit of sufficient model capacity and perfect optimization, fθ converges to the Bayes-optimal regression function: (D) = E[ Iγ ] . That is, the optimal estimator outputs the posterior expectation of the mutual information conditioned on the observed dataset. This shows the soundness of the approach, which is further clarified by the following decomposition of the MSE loss: LMSE(θ) = (cid:104)(cid:0)fθ(D) (D)(cid:1)2(cid:105) + E[Var(Iγ D)] , (2) The second term corresponds to the irreducible epistemic uncertainty due to the fact that, for finite n, the dataset does not uniquely identify the generative law γ. The irreducible error in MSE comes from the difficulty of the statistical inference task. Built-in Uncertainty Quantification Uncertainty quantification is desirable feature for MI estimators, allowing practitioners to assess reliability and construct confidence intervals. However, built-in uncertainty estimates remain largely absent from existing methodsnone of the baseline estimators in this work provide them. The main available approach is bootstrapping, which is prohibitively expensive in practice, as inference on each bootstrap sample requires refitting density or 5 density ratio model. Our meta-learning paradigm offers an opportunity to incorporate uncertainty estimation into model design by modifying the loss. Apart from MIST, trained to predict point estimate of MI using the mean squared error (MSE) loss, we also train quantile regression model (MISTQR) to predict the τ -quantile of the sampling distribution for any given τ [0, 1] using the pinball loss (Steinwart & Christmann, 2011), defined for target quantity and predicted quantity ˆq as: Lτ (q, ˆq) = max(τ (q ˆq), (1 τ )(ˆq q)). (3) By querying multiple values of τ , we can approximate the full sampling distribution of MI. Furthermore, these estimates are obtained as the result of forward pass and are therefore cheap to compute. We evaluate the calibration and reliability of these quantile-based intervals against bootstrap alternatives in Section 4.3 and include both model variants throughout our experiments. Meta-Dataset Training our meta-learned estimators in supervised manner requires distributions with known MI. We generate these distributions using BMI, library designed to create complex benchmarks for MI estimation. BMI exploits the invariance of mutual information to homeomorphisms, applying invertible transformations to simple base distributions to produce more complex and challenging distributions. We partition the base distribution families into separate train and test sets, that is, the meta-distribution used for training has different support than the one used for testing. For each base distribution, we apply BMIs suite of invertible transformations and generate variants across dimensions ranging from 2 to 32, yielding two distribution sets Γtrain and Γtest. From these distributions, we construct training meta-dataset Mtrain and two test meta-datasets differing only in scale: Mtest, and Mtest-extended. These test meta-datasets contain mix of distribution families found in Γtrain (subsequently referred to as in-meta-distribution or IMD) or unseen at training time (out-of-meta-distribution or OoMD). Since MI is function of two random variables, each meta-datapoint in our meta-datasets consists of paired samples {(xi, yi)}n i=1 drawn from joint distribution γ Γ, along with the known mutual information Iγ(X; ) as the label. The value of varies between 10 and 500 across each of our train and test meta-datasets, thus focusing on lowsample regimes. The resulting training meta-dataset Mtrain contains 16 base distribution families and 625,000 meta-datapoints. Mtest-extended contains 806,000 meta-datapoints from 13 distribution families, of which 6 are shared with Mtrain. Meanwhile, Mtest has reduced size of 2,340 metadatapoints, created to allow the testing of slower baselines that would take prohibitively long to run on the extended test set. The test sets are sampled to uniformly cover all sample sizes, dimensions, and base distribution families. The contents and generation of the meta-datasets are described in more detail in Appendix B. Model Architecture Building an estimator using our framework presents three main architectural challenges. First, the model must process datasets of varying size while remaining invariant to sample ordering. The Transformer architecture is naturally permutation-invariant and has been common choice to ingest datasets as inputs (Xu et al., 2024). The SetTransformer (Lee et al., 2019) architecture introduced the ISAB block to reduce the quadratic cost of attention by performing attention on fixed number of learned inducing points. Finally, the SetTransformer++ architecture (Zhang et al., 2022) further improves upon the original SetTransformer by introducing set normalization and additional residual connections that enable more stable training with deeper models. We therefore use SetTransformer++ as the basis of our architecture. Second, our estimator must process samples of varying dimensions. To address this, we introduce an additional attention block that operates over the dimension axis; this time, permutation invariance is not required. This rowwide attention also operates with learned fixed inducing point, which then serves as pooling mechanism into fixed row dimension. Third, unlike many supervised learning tasks, mutual information is an unbounded measure. This is long-standing challenge for MI estimators, which typically underestimate MI and fail in high-MI settings. We explored several normalization and multi-task prediction strategies (e.g., predicting normalized conditional entropies) to address this problem when designing our model, and provide our results in Appendix C.1. We found that the simplest solution of directly predicting MI produced the best estimators, provided the meta-dataset includes sufficient range of MI labels."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Empirically, we find that our learned estimators outperform existing methods across sample sizes and dataset dimensions, particularly in low-sample settings (4.1). We provide an empirical study of the behavior of estimators under increasing dimensionality, finding that on average, our models require roughly half the number of samples as the best baseline to provide reliable MI estimates (4.2). The quantile-based intervals are well-calibrated and more reliable than bootstrap alternatives (4.3), while inference is orders of magnitude faster than existing neural methods (4.4). Finally, the learned estimators generalize to unseen distributions and sample sizes (4.5), and benefits from the mix of dimensions and size of the training dataset (4.6)."
        },
        {
            "title": "4.1 COMPARISON WITH EXISTING ESTIMATORS",
            "content": "We begin by benchmarking our estimators against existing methods on diverse set of synthetic distributions. In Table 1, we present the average MSE loss of each estimator on Mtest, grouped by ranges of meta-datapoint size for clarity. We use the smaller test set of 2,340 meta-datapoints to evaluate all baselines. In-Meta-Distribution (IMD) [100, 300] [10, 100] [300, 500] Out-of-Meta-Distribution (OoMD) [100, 300] [10, 100] [300, 500] 4.6e3 CCA 3.0e1 KSG 6.3e3 MINE InfoNCE 1.4e2 3.0e19 DV 3.1e1 LMI 3.5e1 MINDE 3.1e0 MIST MISTQR 3.1e0 2.3e1 7.4e3 2.8e1 8.7e1 1.2e4 3.7e4 3.5e1 2.5e2 2.1e20 1.1e9 3.1e1 9.1e1 3.2e1 9.6e1 3.4e1 10.4 11.7 3.1e1 7.1e1 6.7e0 1.0e2 2.7e1 1.2e5 1.2e5 1.2e2 3.2e1 2.2e10 8.5e6 1.1e2 3.0e1 1.2e2 2.5e1 2.8e1 0.9 1.5 1.2e1 4.0e3 3.7e1 3.7e1 9.5e1 7.1e3 2.0e6 3.2e2 1.1e2 6.7e7 1.2e19 3.8e1 1.1e2 4.2e1 9.1e1 8.0e0 9.5e 0.3 0.9 1.8e1 6.8e3 3.5e1 7.7e1 1.5e4 4.1e4 4.2e1 2.1e3 6.3e19 8.7e6 3.6e1 8.0e1 3.8e1 8.5e1 14.1 7.3e0 9.9e0 17.1 7.3e1 1.2e1 8.6e1 4.3e1 2.3e5 3.3e4 9.7e1 5.0e1 5.1e7 6.1e6 9.1e1 4.6e1 9.4e1 3.6e1 18.5 8.8e0 1.0e1 24.6 6.2e1 1.1e2 4.0e5 1.2e2 5.8e7 1.2e2 1.0e2 22. 25.4 Table 1: Comparison of MI estimation baselines and our proposed methods across different distribution families (IMD and OoMD) and varying numbers of samples per meta-datapoint on the Mtest. Our estimators substantially outperform existing methods across sample size and distribution settings, including for distributions unseen during training. Shown are the averaged MSE loss values (mean stddev) of each estimator, grouped by the number of samples given to the estimator. The standard deviation is computed from 100 bootstrap resamples for each regime. In this challenging regime of low sample sizes, varying dimensionality, and non-Gaussian distributions, recent estimators struggle to outperform the classical KSG baseline. This aligns with recent findings demonstrating that evaluation on more complex settings reveals weaknesses in modern estimators (Czyz et al., 2023; Lee & Rhee, 2024). Our learned estimators substantially outperform all baselines. Relative to KSG (the next strongest method), our models achieve approximately 10 lower error on distributions seen during training (IMD), and 5 lower error on unseen distributions (OoMD). breakdown by distribution family is provided in Appendix C.3. For subsequent large-scale evaluation, we focus on KSG as the baseline, since it is both computationally feasible on the extended test set and remains the strongest baseline estimator in Table 1, particularly in low-sample regimes. Figure 2 further decomposes performance into MSE, bias, variance, and confidence interval coverage for our models (MIST, MISTQR) and for KSG. For all estimators except MISTQR, confidence intervals are obtained via bootstrap resampling of the input dataset; for MISTQR, we use the predicted sampling distribution from the quantile outputs. Each heatmap summarizes average performance as function of sample size (x-axis) and input dimensionality (y-axis). Our meta-learned estimators achieve substantial improvements in challenging regimes, with up to 100 lower loss for high-dimensional and low-sample settings. They are nearly unbiased in all Mtest-extended settings, only exhibiting positive bias for the high-dimensional, low-sample setting. 7 Figure 2: Heatmaps providing detailed analysis of the performance of the two proposed MI estimators and the KSG baseline. The data dimensions are grouped into three categories, and within each group, meta-datapoints are sorted by the number of samples. Shown are the average values of MSE, bias, variance, and confidence interval (CI cov.) width across Mtest-extended. CI coverage denotes the fraction of samples per group for which the true MI lies within the 95% bootstrap confidence interval. In contrast, KSG exhibits negative bias across all mediumand high-dimensional settings. Notably, our estimators avoid the well-documented underestimation problem that plagues existing methods in high dimensions. For variance, our estimators match KSGs low variance for sample sizes above 100. Finally, our estimators confidence intervals achieve approximately 2 better coverage than KSG across all settings. One of the most well-documented failure modes of existing MI estimators is their negative bias, in which estimators substantially underestimate predictions for distributions with large MI. In Figure 3, we plot the predictions of our estimators in terms of increasing MI. While KSGs estimates plateau quickly, our estimates closely follow the true MI values. 4.2 SCALING BEHAVIOR Traditional evaluations of MI estimators have been constrained to relatively simple distributions and high-sample regimes. While recent efforts such as the BMI benchmark have enabled more comprehensive studies on complex distributions, systematic empirical analysis of scaling behavior has remained limited, largely due to the computational expense of evaluating existing methods. Leveraging our larger test set Mtest-extended, we conduct detailed empirical study of estimator performance across wide range of dimensions and sample sizes. This enables us to characterize performance frontiers in terms of data requirements. This analysis addresses an important question for practitioners: How many samples are needed for reliable estimate? Figure 3: Predicted MI as function of the true MI on Mtest-extended. For each estimator, predictions are aggregated into bins of true MI values. Figure 4 shows the data requirement profiles of our estimators compared to the KSG estimator. The x-axis represents the dimensionality of the distribution. The y-axis indicates the sample size required to achieve target MSE thresholds of 0.03, 0.07, and 0.09, selected based on our prior heatmap analysis. Our estimators consistently attain the desired accuracy with substantially fewer 8 samples than KSG. In particular, for certain MSE targets, even sample size of approximately 500 is insufficient for KSG to achieve reliable performance. complete heatmap of these results is provided in Appendix C.5. Figure 4: The MIST and MISTQR models scale substantially better to higher dimensions, requiring roughly half as many samples as KSG to achieve an MSE below the selected thresholds. The + marker indicates that over 500 samples (nrow) are required to obtain accurate estimates in all higherdimensional settings. 4.3 CALIBRATION Uncertainty estimates are essential for MI estimation in practice, allowing practitioners to construct reliable confidence intervals and make informed decisions based on the uncertainty of the estimation. We evaluate uncertainty quantification for both variants of our estimator. For MIST, we compute confidence intervals via bootstrapping, which is feasible due to our methods fast inference. For the quantile regression model MISTQR, we obtain uncertainty estimates by querying the model at different values of τ , directly approximating the sampling distribution. To assess calibration, we compare predicted quantiles against empirical coveragethe proportion of true MI values that fall below each predicted quantile. Figure 5 plots this comparison for both inmeta-distribution and out-of-meta-distribution examples from Mtest. Both of our estimator variants substantially outperform KSG in calibration, even on unseen distribution families. This confirms the results reported in Figure 2 showing that confidence intervals build from MIST models often contains the true MI value (for around 80% of instances). The quantile-loss variant MISTQR achieves nearly perfect calibration on distributions seen in training and remains well-calibrated for unseen distributions. Crucially, we obtain these uncertainty estimates at minimal computational cost: computing confidence intervals requires only additional forward passes for either model. 4.4 INFERENCE EFFICIENCY Unlike traditional methods that require fitting density or density ratio models for each new dataset, our learned estimators perform inference through single forward pass. This makes our estimators dramatically more efficient at inference, as seen in Figure 6: 1.7 faster than the most efficient baseline and 480 faster than KSG, the best-performing traditional estimator. Training MIST and MISTQR each required 3 hours and 45 minutes using batch size of 256 on an Nvidia A100 GPU. Critically, this one-time training cost is amortized across all subsequent inference tasks. The combination of fast inference and efficient uncertainty quantification (as discussed in Section 4.3) makes our approach practical for large-scale applications where repeated MI estimation is required, such as hyperparameter tuning, feature selection, or iterative model training. We estimated the inference time of each baseline on the entirety of Mtest-extended and report the results in Appendix C.4. The high computational costs (> 1 week) of evaluating all baselines except for CCA and KSG on Mtest-extended, along with the poor performance measured for CCA on the Mtest for low sample sizes, further motivate our selection of KSG as the only baseline for other experiments in this work. 4.5 GENERALIZATION TO UNSEEN DISTRIBUTIONS, DIMENSIONS, AND SAMPLE SIZES 9 Figure 5: Calibration of the proposed models on Mtest,IMD (left) and Mtest,OoMD (right), computed directly from quantiles for MISTQR and through bootstrap resampling for MIST and KSG. We report the mean absolute error in the legend. The previous sections demonstrate that the learned algorithm generalizes effectively to unseen distribution families, which is evaluated at dimensions and sample sizes seen during training. We find that by learning to infer MI across diverse distributions and sample regimes, our estimators learn appropriate regularization strategies that transfer even to distribution families not encountered during training. The strength of these results motivates bolder question: if meta-learned MI algorithms can generalize to unseen distributions, can they also generalize to these distributions at dimensions and sample sizes beyond their training experience? We conduct controlled study by training models on restricted subsets of the training data, specifically training on meta-datapoints in Mtrain with dimensionality < 16 and sample size < 300. We evaluate performance on our full test meta-dataset Mtest-extended, which therefore includes distributions with higher dimensionalities and sample sizes and new distributions. Figure 7 presents two loss heatmaps, one for Mtest-extended,IMD and one for Mtest-extended,OoMD. Figure 6: Inference time for each method as function of the sample size. Results are averaged over 100 inferences (5 for MINDE), each performed over 10 meta-datapoints. We find that our estimators generalize well to new dimensions and sample sizes for distributions that are in the training meta-distribution (IMD heatmap). Similarly, we find that our models generalize well to new distributions when the data dimensionality is similar to that during training (OoMD heatmap, left half). However, they struggle to generalize to unseen distributions when the data also has higher dimensionality than any seen during training (OoMD heatmap, right half). 4.6 IMPACT OF VARIABLE INPUT DIMENSIONS natural question that arises from our approach is the impact of including multiple dimensions in model training. Of course, training on mix of dimensions allows the development of generalpurpose estimator that can achieve high performance across settings, as shown in the above exper10 Figure 7: Generalization performance (MSE) of models trained with limited dimensionality and sample size, evaluated and averaged on subsets of Mtest-extended. In each heatmap, we show the loss of MIST and MISTQR on subsets of varying dimensions (x-axis) and sample sizes (y-axis). The red dashed lines indicate the maximal dimensionality and sample size seen by the model during training: results in the bottom left quadrant were obtained on data with dimensions and sample sizes seen at training, while results in the top right quadrant use data with both larger dimensions and sample sizes than what was seen at training time. iments. However, there may be settings where practitioners know their data dimension apriori, and may consider producing training dataset of only that dimension. To study these questions, we trained separate (specialized) copies of MIST on subsets of Mtrain filtered to contain only metadatapoints with fixed input dimension (e.g., 4, 8, 16 or 32). This filtering procedure reduces the number of training data points substantially (by factor of 31). To provide comparison with the same number of samples, we also trained an additional (reduced data) MIST model on uniformly down-sampled version of Mtrain to match the training size of the individual dimension models. By comparing the base MIST (full training set) with the specialized models (only one dimension), we can measure the impact of scaling both the dimensions and dataset size. By comparing the reduced data model (downsampled train set) and the specialized models (only one dimension), we can isolate the impact of training on multiple dimensions, without increasing dataset scale. In Figure 8, we provide these comparisons for each input dimension in Mtest-extended. We find that scaling both the dimensionality and dataset size (base vs specialized) substantially improves model performance for high-dimensionality data (2-3 lower MSE for 16) but does not improve performance for low dimensions 8. We hypothesize that as the dimensionality increases, models are more likely to benefit from the regularization strategies learned from larger and more diverse training distribution. When controlling for the number of samples (reduced data vs specialized), we find that learning from mix of data dimensions has lower performance than specialized model trained on one dimension. We hypothesize that learning across dimensions requires larger number of samples and longer training times to learn the appropriate regularization strategies required for generalization. Overall, we find that learning from large and diverse set of distributions (which can be generated synthetically) is the strongest path to designing effective MIST estimators. Additional results in Appendix C.2 show that the same trend holds for MISTQR and in the OoMD setting, the key difference being that the reduced data models performance does not degrade as much in the OoMD setting. Furthermore, the increased meta-dataset size of the full model further improves its performance by factor of 5 (10 for dimension 4) compared to that of the reduced data model."
        },
        {
            "title": "5 DISCUSSION",
            "content": "This work rethinks the design of mutual information estimators. Rather than deriving estimators analytically from variational or density-based formulations, we treat estimation as supervised learning problem: the estimator is learned function, trained end-to-end on large synthetic meta-dataset 11 Figure 8: Average MSE obtained by variableand fixed-dimensionality versions of MIST on subsets of Mtest-extended,IMD with fixed dimensions (x-axis). Similar experiments were performed on MISTQR and OoMD data, with corresponding plots given in Appendix C.2. with known ground-truth MI values. This approach represents deliberate shift in paradigm, trading theoretical motivations for flexible and data-driven design. Trade-offs and limitations. By embracing an empirical learning framework, we inherit the usual risks of statistical learning. The learned estimators accuracy depends on the diversity and coverage of the meta-training distributions, and its generalization to distant out-of-distribution families cannot be guaranteed priori. Our estimators behavior is learned, rather than designed, and therefore does not carry the theoretical consistency guarantees of classical MI estimatorsnamely, that the estimate converges to the true MI as . We studied our estimators behavior on self-consistency tests from Song & Ermon (2019) in Appendix D, and find that while our estimators are approximately self-consistent under the independence test, they are not self-consistent and produce high variance prediction ratios for the data processing and additivity tests. We believe that learning-based MI framework offers opportunity for designing models, datasets, and losses to induce such desired behavior characteristics, but we believe it is important to recognize that our estimators do not carry these properties innately. In high-sample settings, our learned estimator cannot directly leverage all available samples during one inference pass and other estimators may be preferred if time and compute resources are available. Moreover, learned estimators are not inherently interpretable and may result in unforeseen failure modes. These limitations parallel the general challenges faced by machine learning models in other domains, highlighting the importance of careful validation and uncertainty quantification. Controllability and adaptability. key strength of this framework lies in its controllability. Our estimators behavior are largely determined by the synthetic datasets created; practitioners can customize, interrogate, and design datasets to improve estimator quality and reliability. Since mutual information is invariant under invertible transformations, our meta-dataset can be mapped to target modality using normalizing flows without altering its ground-truth MI. This property enables the generation of diverse synthetic meta-datasets that reflect the characteristics of arbitrary data domains, such as images, text, tabular data, or multimodal signals. By tailoring the training data through such transformations, one can design estimators specialized for particular classes of distributions or downstream applications. This mechanism opens the door to what may be viewed as foundational model for information-theoretic estimation, one that can be adapted via fine-tuning to specific tasks and modalities. Computational efficiency. Once trained, our learned estimators perform inference in single forward pass (or two to obtain confidence interval using the quantile regression model), making them several orders of magnitude faster than existing neural estimators. By amortizing the cost of inference during meta-training, our estimators achieve speedups of three to four orders of magnitude compared to neural MI estimators, and one order of magnitude compared to classical KSG imple12 mentations. The efficiency of our estimator offers new opportunities for MI estimation in settings where existing estimators are infeasible or intractable. Outlook. Beyond empirical gains, the proposed framework opens numerous directions for research in both information theory and statistical learning. First, it allows the inclusion of theoretical results as inductive biases, for instance, explicitly penalizing violations of the Data Processing Inequality. Second, because MI estimators are often used within optimization loops (e.g., to maximize or minimize information), it is possible to train models that focus on relative ordering rather than absolute accuracy, for example, by using ranking losses that emphasize correct ordering across batch of datasets. Taken together, these elements outline research program: the development of trainable, meta-learned estimators as flexible, efficient, and customizable alternatives to classical estimators."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was supported in part through the NYU IT High Performance Computing resources, services, and staff expertise. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-2039655 (MR). Any opinion, findings, and conclusions or recommendations expressed in this material are those of the authors(s) and do not necessarily reflect the views of the National Science Foundation. This work was supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) with grant funded by the Ministry of Science and ICT (MSIT) of the Republic of Korea in connection with the Global AI Frontier Lab International Collaborative Research. This work was also supported by the Samsung Advanced Institute of Technology (under the project Next Generation Deep Learning: From Pattern Recognition to AI) and the National Science Foundation (under NSF Award 1922658). This work was partially conducted within French research unit UMR 5217 and was supported by CNRS (grant ANR-22-CPJ2-0036-01) and by MIAI@Grenoble-Alpes (grant ANR-19-P3IA-0003). It was granted access to the HPC resources of IDRIS under the allocation 2025-AD011014834 made by GENCI."
        },
        {
            "title": "REFERENCES",
            "content": "Alexander Alemi and Ian Fischer. Gilbo: One metric to measure them all. Advances in Neural Information Processing Systems, 31, 2018. Alexander Alemi, Ian Fischer, Joshua Dillon, and Kevin Murphy. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016. Brandon Amos. Tutorial on amortized optimization, 2023. URL https://arxiv.org/abs/ 2202.00665. Anastasios N. Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I. Jordan, and Tijana Zrnic. Prediction-powered inference, 2023. URL https://arxiv.org/abs/2301.09633. Grace Avecilla, Julie Chuong, Fangfei Li, Gavin Sherlock, David Gresham, and Yoav Ram. Neural networks enable efficient and accurate simulation-based inference of evolutionary parameters from adaptation dynamics. PLoS biology, 20(5):e3001633, 2022. David Barber and Felix Agakov. The im algorithm: variational approach to information maximization. Advances in neural information processing systems, 16(320):201, 2004. Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International conference on machine learning, pp. 531540. PMLR, 2018. Thomas B. Berrett, Richard J. Samworth, and Ming Yuan. Efficient multivariate entropy estimation via $k$-nearest neighbour distances. The Annals of Statistics, 47(1):288318, February 2019. ISSN 0090-5364, 2168-8966. doi: 10.1214/18-AOS1688. URL https://projecteuclid. org/journals/annals-of-statistics/volume-47/issue-1/ Efficient-multivariate-entropy-estimation-via-k-nearest-neighbour-distances/ 10.1214/18-AOS1688.full. Benjamin Bloem-Reddy and Yee Whye Teh. Probabilistic symmetries and invariant neural networks. Journal of Machine Learning Research, 21(90):161, 2020. URL http://jmlr. org/papers/v21/19-322.html. Wessel P. Bruinsma, Stratis Markou, James Requeima, Andrew Y. K. Foong, Tom R. Andersson, Anna Vaughan, Anthony Buonomo, J. Scott Hosking, and Richard E. Turner. Autoregressive conditional neural processes. In ICLR. OpenReview.net, 2023. URL http://dblp. uni-trier.de/db/conf/iclr/iclr2023.html#BruinsmaMRFAVBH23. Ivan Butakov, Aleksandr Tolmachev, Sofia Malanchuk, Anna Neopryatnaya, and Alexey Frolov. Mutual information estimation via normalizing flows. Advances in Neural Information Processing Systems, 37:30273057, 2024. Atul Butte and Isaac Kohane. Mutual information relevance networks: functional genomic In Biocomputing 2000, pp. 418429. World clustering using pairwise entropy measurements. Scientific, 1999. Matthew Chalk, Olivier Marre, and Gasper Tkacik. Relevant sparse codes with variational information bottleneck. Advances in Neural Information Processing Systems, 29, 2016. Paul Chang, Nasrulloh Loka, Daolang Huang, Ulpu Remes, Samuel Kaski, and Luigi Acerbi. Amortized probabilistic conditioning for optimization, simulation and inference. 28th International Conference on Artificial Intelligence and Statistics (AISTATS 2025), 2025. Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. Advances in neural information processing systems, 29, 2016. Yanzhi Chen, Zijing Ou, Adrian Weller, and Michael Gutmann. Neural mutual information estimation with vector copulas. arXiv preprint arXiv:2510.20968, 2025. 14 Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference. Proceedings of the National Academy of Sciences, 117(48):3005530062, 2020. Paweł Czyz, Frederic Grabowski, Julia Vogt, Niko Beerenwinkel, and Alexander Marx. Beyond normal: On the evaluation of mutual information estimators. Advances in neural information processing systems, 36:1695716990, 2023. G.A. Darbellay and I. Vajda. Estimation of the information by an adaptive partitioning of the IEEE Transactions on Information Theory, 45(4):13151321, 1999. doi: observation space. 10.1109/18.761290. Monroe Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process expectations for large time, i. Communications on pure and applied mathematics, 28(1):147, 1975. Lasse Elsemuller, Hans Olischlager, Marvin Schmitt, Paul-Christian Burkner, Ullrich Koethe, and Stefan T. Radev. Sensitivity-aware amortized bayesian inference. Transactions on Machine ISSN 2835-8856. URL https://openreview.net/forum? Learning Research, 2024. id=Kxtpa9rvM0. Giulio Franzese, Mustapha Bounoua, and Pietro Michiardi. Minde: Mutual information neural diffusion estimation. arXiv preprint arXiv:2310.09031, 2023. Andrew Fraser and Harry Swinney. Independent coordinates for strange attractors from mutual information. Physical review A, 33(2):1134, 1986. Surya Ganguli and Haim Sompolinsky. Compressed sensing, sparsity, and dimensionality in neuronal information processing and data analysis. Annual review of neuroscience, 35(1):485508, 2012. Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In International conference on machine learning, pp. 17041713. PMLR, 2018a. Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo Rezende, SM Eslami, and Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018b. Manuel Gloeckler, Michael Deistler, Christian Dietrich Weilbach, Frank Wood, and Jakob H. Macke. All-in-one simulation-based inference. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 1573515766. PMLR, 2127 Jul 2024. URL https: //proceedings.mlr.press/v235/gloeckler24a.html. Ziv Goldfeld, Ewout van den Berg, Kristjan Greenewald, Igor Melnyk, Nam Nguyen, Brian Kingsbury, and Yury Polyanskiy. Estimating information flow in deep neural networks. arXiv preprint arXiv:1810.05728, 2018. Pedro Goncalves, Jan-Matthis Lueckmann, Michael Deistler, Marcel Nonnenmacher, Kaan Ocal, Giacomo Bassetto, Chaitanya Chintaluri, William Podlaski, Sara Haddad, Tim Vogels, et al. Training deep neural density estimators to identify mechanistic models of neural dynamics. Elife, 9:e56261, 2020. Jonathan Gordon, Wessel P. Bruinsma, Andrew Y. K. Foong, James Requeima, Yann Dubois, In International Conferand Richard E. Turner. Convolutional conditional neural processes. ence on Learning Representations, 2020. URL https://openreview.net/forum?id= Skey4eBYPS. Gokul Gowri, Xiaokang Lun, Allon Klein, and Peng Yin. Approximating mutual information of high-dimensional variables using learned representations. Advances in Neural Information Processing Systems, 37:132843132875, 2024. 15 Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and Alexander Lerchner. Towards definition of disentangled representations. arXiv preprint arXiv:1812.02230, 2018. Daolang Huang, Manuel Haussmann, Ulpu Remes, ST John, Gregoire Clarte, Kevin Sebastian Luck, Samuel Kaski, and Luigi Acerbi. Practical equivariances via relational conditional neural processes. Advances in Neural Information Processing Systems, 2023. Sicong Huang, Alireza Makhzani, Yanshuai Cao, and Roger Grosse. Evaluating lossy compression rates of deep generative models. In International Conference on Machine Learning, pp. 4444 4454. PMLR, 2020. Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol Vinyals, and Yee Whye Teh. Attentive neural processes. arXiv preprint arXiv:1901.05761, 2019. Allon Klein, Linas Mazutis, Ilke Akartuna, Naren Tallapragada, Adrian Veres, Victor Li, Leonid Peshkin, David Weitz, and Marc Kirschner. Droplet barcoding for single-cell transcriptomics applied to embryonic stem cells. Cell, 161(5):11871201, 2015. Artemy Kolchinsky, Brendan Tracey, and David Wolpert. Nonlinear information bottleneck. Entropy, 21(12):1181, 2019. Leonenko Kozachenko. Sample estimate of the entropy of random vector. Probl. Pered. Inform., 23:9, 1987. Alexander Kraskov, Harald Stogbauer, and Peter Grassberger. Estimating mutual information. Physical Review EStatistical, Nonlinear, and Soft Matter Physics, 69(6):066138, 2004. Nojun Kwak and Chong-Ho Choi. Input feature selection by mutual information based on parzen IEEE transactions on pattern analysis and machine intelligence, 24(12):16671671, window. 2002. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set In Protransformer: framework for attention-based permutation-invariant neural networks. ceedings of the 36th International Conference on Machine Learning, pp. 37443753, 2019. Kyungeun Lee and Wonjong Rhee. benchmark suite for evaluating neural mutual information estimators on unstructured datasets. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/ forum?id=xT5pmUju8W. Sulin Liu, Xingyuan Sun, Peter Ramadge, and Ryan Adams. Task-agnostic amortized inference of gaussian process hyperparameters. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 2144021452. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_ files/paper/2020/file/f52db9f7c0ae7017ee41f63c2a7353bc-Paper.pdf. Jan-Matthis Lueckmann, Pedro Goncalves, Giacomo Bassetto, Kaan Ocal, Marcel Nonnenmacher, and Jakob Macke. Flexible statistical inference for mechanistic models of neural dynamics. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/ paper/2017/file/addfa9b7e234254d26e9c7f2af1005cb-Paper.pdf. David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003. Stratis Markou, James Requeima, Wessel Bruinsma, Anna Vaughan, and Richard Turner. PracIn International Confertical conditional neural process via tractable dependent predictions. ence on Learning Representations, 2022. URL https://openreview.net/forum?id= 3pugbNqOh5m. David McAllester and Karl Stratos. Formal limitations on the measurement of mutual information. In International Conference on Artificial Intelligence and Statistics, pp. 875884. PMLR, 2020. 16 Anna Meszaros, Julian F. Schumann, Javier Alonso-Mora, Arkady Zgonnikov, and Jens Kober. Rome: robust multi-modal density estimator. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI 24, 2024. ISBN 978-1-956792-04-1. doi: 10.24963/ijcai.2024/525. URL https://doi.org/10.24963/ijcai.2024/525. Young-Il Moon, Balaji Rajagopalan, and Upmanu Lall. Estimation of mutual information using kernel density estimators. Phys. Rev. E, 52:23182321, Sep 1995. doi: 10.1103/PhysRevE.52. 2318. URL https://link.aps.org/doi/10.1103/PhysRevE.52.2318. Samuel Muller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers can do bayesian inference. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=KSugKcbNf9. Samuel Muller, Matthias Feurer, Noah Hollmann, and Frank Hutter. PFNs4BO: In-context learning for Bayesian optimization. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 2544425470. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/ v202/muller23a.html. Tung Nguyen and Aditya Grover. Transformer neural processes: Uncertainty-aware meta learning via sequence modeling. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 1656916594. PMLR, 1723 Jul 2022. URL https://proceedings.mlr.press/v162/nguyen22b.html. XuanLong Nguyen, Martin Wainwright, and Michael Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):58475861, 2010a. XuanLong Nguyen, Martin Wainwright, and Michael Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):58475861, 2010b. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Biswajit Pandey and Suman Sarkar. How much galaxy knows about its large-scale environment?: An information theoretic perspective. Monthly Notices of the Royal Astronomical Society: Letters, 467(1):L6L10, 2017. Sequential neural likelihood: Fast George Papamakarios, David Sterratt, and Iain Murray. In Kamalika Chaudhuri and Masashi likelihood-free inference with autoregressive flows. Sugiyama (eds.), Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pp. 837848. PMLR, 1618 Apr 2019. URL https://proceedings.mlr.press/v89/ papamakarios19a.html. George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57):164, 2021. Deepak Pathak, Pulkit Agrawal, Alexei Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning, pp. 27782787. PMLR, 2017. Hanchuan Peng, Fuhui Long, and Chris Ding. Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. IEEE Transactions on pattern analysis and machine intelligence, 27(8):12261238, 2005. Maxime Peyrard and Kyunghyun Cho. Meta-statistical learning: Supervised learning of statistical inference. arXiv preprint arXiv:2502.12088, 2025. 17 Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In International conference on machine learning, pp. 51715180. PMLR, 2019. Stefan Radev, Marvin Schmitt, Lukas Schumacher, Lasse Elsemuller, Valentin Pratz, Yannik Schalte, Ullrich Kothe, and Paul-Christian Burkner. Bayesflow: Amortized bayesian workflows with neural networks. arXiv preprint arXiv:2306.16015, 2023. Claude Shannon. mathematical theory of communication. The Bell system technical journal, 27(3):379423, 1948. Bernard Silverman. Density estimation for statistics and data analysis. Routledge, 2018. Fergus Simpson, Ian Davies, Vidhi Lalchand, Alessandro Vullo, Nicolas Durrande, and In M. Ranzato, Carl Edward Rasmussen. identification through transformers. A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 1048310495. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2021/ 2021. file/56c3b2c6ea3a83aaeeff35eeb45d700d-Paper.pdf."
        },
        {
            "title": "Kernel",
            "content": "Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information estimators. arXiv preprint arXiv:1910.06222, 2019. Ingo Steinwart and Andreas Christmann. Estimating conditional quantiles with the help of the pinball loss. Bernoulli, 17(1):211 225, 2011. doi: 10.3150/10-BEJ267. URL https://doi. org/10.3150/10-BEJ267. Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 ieee information theory workshop (itw), pp. 15. Ieee, 2015. Hengyuan Xu, Liyao Xiang, Hangyu Ye, Dixi Yao, Pengzhi Chu, and Baochun Li. PermutaIn 2024 IEEE/CVF Conference on tion Equivariance of Transformers and its Applications . Computer Vision and Pattern Recognition (CVPR), pp. 59875996, Los Alamitos, CA, USA, June 2024. IEEE Computer Society. doi: 10.1109/CVPR52733.2024.00572. URL https: //doi.ieeecomputersociety.org/10.1109/CVPR52733.2024.00572. and Alexander Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ SalakhutIn I. Guyon, U. Von Luxburg, Deep sets. (eds.), AdInc., URL https://proceedings.neurips.cc/paper_files/paper/2017/ Smola. dinov, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, vances in Neural Information Processing Systems, volume 30. Curran Associates, 2017. file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf. and R. Garnett Lily Zhang, Veronica Tozzo, John Higgins, and Rajesh Ranganath. Set norm and equivariant skip connections: Putting the deep in deep sets. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 2655926574. PMLR, 1723 Jul 2022. URL https://proceedings.mlr.press/ v162/zhang22ac.html. Shengjia Zhao, Jiaming Song, and Stefano Ermon. The information autoencoding family: lagrangian perspective on latent variable generative models. arXiv preprint arXiv:1806.06514, 2018."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DEFINITIONS OF MUTUAL INFORMATION Several equivalent expressions of mutual information are leveraged by existing estimators. For clarity, we provide simple overview of these expressions below. For two continuous random 18 variables and , the mutual information (MI) is defined in terms of the joint distribution P(X,Y ) and marginal distributions PX and PY : I(X; ) = (cid:90) (cid:90) P(X,Y )(x, y) log (cid:20) P(X,Y )(x, y) PX (x)PY (y) (cid:21) dx dy (4) Donsker-Varadhan commonly used expression of MI is in terms of the Kullback-Leibler (KL) divergence between the joint distribution and the product of marginals (Donsker & Varadhan, 1975): I(X; ) = DKL(P(X,Y )PX PY ) (5) This expression can be understood as measuring by how much the joint distribution deviates from the product of marginal distributions. If and are independent, P(X,Y ) = PX PY , producing KL-divergence (and MI estimate) of zero. Conditional Entropy Another common expression of MI is in terms of the conditional entropy: H() denotes conditional entropy, and H(X, ) is the joint entropy. I(X; ) = H(X) H(XY ) = H(Y ) H(Y X) = H(X) + H(Y ) H(X, ) (6) This expression of MI can be understood as measure of the reduction in uncertainty about one variable given the other. A.2 MI ESTIMATORS We provide an overview of prominent MI estimators for reference in Table 2, categorized by their prediction category, the MI definition considered, and the estimation task. Previous works such as Song & Ermon (2019) have described variational estimators as either discriminative or generative, with discriminative methods learning the ratio between joint and marginal densities directly and generative methods estimating the densities separately before computing the density ratio. We broaden this categorization into density and density ratio estimators. A.3 EMPIRICAL EVALUATION SCOPE OF MI ESTIMATORS In Table 3, we provide summary of how prominent estimators were empirically evaluated. We focus our summary on evaluations of the estimator directly, rather than the performance of an estimator in downstream tasks. Similarly, we only include the evaluations of estimators in the papers introducing the method, as we are most interested in how estimator quality is determined. Early methods for mutual information estimation, such as binning, were evaluated primarily using simulated time-series data. These commonly included sine and cosine functions, as well as chaotic dynamical systems such as the Lorenz and Rossler attractors (where the task is phase reconstruction, and no ground truth is available). In the initial histogram binning work (Fraser & Swinney, 1986), the Rossler attractor was used. For this work, we include it in the table as having one 1D distribution, which is non-Gaussian, with no ground truth MI reported (and therefore no maximum). The experiment calculated the mutual information over 65,536 points. KDE (Moon et al., 1995) was similarly evaluated with time-series data. KDE was evaluated on 1D synthetic sine waves, an autoregressive function of sine and Gaussian distribution, the Lorenz attractor, and the Rossler attractor. 400 samples were used for the sine function, and 500 samples were used for the autoregressive function. 4096 and 2048 samples were used for the Lorenz and Rossler attractors, respectively. Estimators are evaluated on the quality of the reconstruction, with no ground truth MI available. We include KDE in the table as having 4 distributions (sine, AR, Lorenz, Rossler), 4 of which are non-Gaussian, and all 1D. No ground truth MI values are present, and the samples included 400, 2K, 4K. KSG (Kraskov et al., 2004) performed evaluation on 2D correlated Gaussians with unit variance and zero mean, testing sample sizes of 125, 250, 500, 1K, 2K, 4K, 10K, and 20K. The MI was 0.830366. Authors performed error analysis with 10K samples, and performed experiments on the gammaexponential distribution, the ordered Weinman exponential, and the circle distribution. All of these 19 Method Category I(X; ) Binning Density = (cid:80) x,y p(x, y) log p(x,y) p(x)p(y) KSG Density = H(X) + H(Y ) H(X, ) KDE Density MIENF Density x,y p(x, y) log p(x,y) = (cid:80) = I(fX (X); fY (Y )) p(x)p(y) LMI Density I(Zx; Zy) GM Density = (cid:82) (cid:82) p(x, y) log p(x,y) p(x)p(y) dx dy MINDE Density = DKL(p(x, y)p(x)p(y)) VCE Density = H[c(uX , uY )] MINE NWJ Density ratio Density ratio InfoNCE Density SMILE MIST ratio Density ratio From Samples supθ EPXY [Tθ] log(EPX PY [eTθ ]) supθ EPXY [Tθ] EPX PY [eTθ 1] E(x,y)Pn (cid:20) log (cid:21) (cid:80)n fθ (x,y) j=1 fθ (x,yj ) 1 EP [Tθ] log EQ[clipτ (eTθ )] * (x, y) p(x), Estimation Task Estimate p(x, y), histograms Implicitly estimate H() by writing H() in terms of KNN distances Estimate p(x, y), p(x), p(y) via kernels p(y) via to Gaussian Learn flows fX , fY marginals Learn neural networks for embeddings Zx = gX (X), Zy = gY (Y ), then apply KSG Estimate p(x, y), p(x), p(y) via VAEs (or flows) Learn scores log p(x, y), log p(x) log p(y) Learn marginal ranks uX , uY with flows, estimate vector copula with MLE over mixture of Gaussian copulas Learn critic Tθ(x, y) to maximize bound, approx. log( p(x,y) Learn critic Tθ(x, y) bound, approx. log p(x,y) p(x)p(y) to maximize p(x)p(y) ) Learn classifier critic fθ(x, y) p(x,y) p(x)p(y) Learn classifier maximize bound. Learn optimization estimator critic fθ(x, y) to by direct Table 2: Summary of prominent methods for estimating mutual information, along with the MI definition used and their estimation tasks. *clipτ (z) [eτ , eτ ]. Method Binning KDE KSG MIENF LMI GM MINDE VCE MINE NWJ InfoNCE SMILE MIST # Data Type 1 Time-Series 4 Time-Series 4 Synthetic Synthetic 6 Synthetic,MNIST,PLM 3 2 Synthetic 20, 3 Synthetic 7,2,2 Synthetic,Images,Text 2 Synthetic - - - - 2 Synthetic 23 Synthetic # 1 4 3 5 2 1 17, 2 6,1,2 0 - - 1 19 Imax dmax - 1 - 1 0.83,1,1.2 3, 8 32, 1024 10 5K,784,1024 1 20 25, 9 64,16,16 2, 20 - - 20 32 10 1, 5 16,7,2.1 2, 40 - - 10 nmin 65K 400, 2K, 4K 125, 10K, 50K 10K 2K,5K 256K 100K 10K,10K,4K - - - 256K 10 Table 3: Summary of the empirical evaluation scope for prominent mutual information estimators. Comma-separated entries indicate values for different experimental settings (e.g., KDE has three experiments with minimum sample size considered 400, 2K, and 4K, respectively). Dashes indicate data that is not available or not specified. 20 had MI values below 1, and considered samples with 1K and 10K data points. For their higher dimension experiments, the estimator was evaluated on 3-D Gaussian distributions with sample size of 10K in one experiment, and on m-correlated uniform Gaussians up to dimension 8 with sample size of 50K. We include KSG in the table as being evaluated on synthetic 4 distributions (Gaussian, Weinman-exponential, gamma-exponential, and circle), 3 of which are non-Gaussian. The maximum dimension considered was 8, while the maximum MI values were 0.83, 1, and 1.2 for the 1-D Gaussians, 3-D Gaussians, and 8-D Gaussian experiments, respectively. The minimum sample size in these experiments was 125 for the 1-D Gaussian, 10K for other distributions, and 50K for larger dimensions. MIENF (Butakov et al., 2024) first evaluated their estimator on 2D correlated Gaussian distributions mapped into higher-dimensional images (16x16 and 32x32), evaluated on 10K samples, and with MI values between 0 and 10. In their second experiment, authors evaluated their method on five non-compressible distributions (uniform, smoothed uniform, student (dof = 4), arcsignh (student) student with dof = 1,2). For this experiment, authors varied the dimensions of these distribution from 2 to 32, using 10K samples for each prediction. The MI was set to 10 for all distributions and dimensions. Latent-MI (LMI; Gowri et al., 2024) was evaluated on multivariate Gaussian distributions under varying original data dimensionality and latent dimensionality learned before applying KSG. Their multivariate Gaussian experiment considered up to 10 latent dimensions and 5K original dimensions, with 2K samples. In second experiment, the authors studied the sample requirements of their estimators. For this experiment, authors used multivariate Gaussians from dimension 1 to 50, each with one correlated dimension, and an MI of 1. Authors varied the sample size up to 4K. In third experiment, MNIST and protein embeddings were resampled to align with Bernoulli vector with set correlation. For MNIST, the dimension was 784, and the sample size was 5K. For the protein embeddings, the dimension was 1024 and the sample size was 4402. In both cases, datasets were generated with MI values between 0 and 1. For GM and SMILE (Song & Ermon, 2019), evaluation was performed on two tasks. In the first task, authors evaluated their methods with 20-D Gaussians with correlation p, and in the second task, the authors applied cubic operation on these Gaussians. This set-up was introduced in (Belghazi et al., 2018). MI values were increased from 0 to 10 at step intervals during the critic training process. Training occurred over 20K steps, with MI increasing by 2 every 4K steps. The batch size was 64, and each batch was new random sample. For the sake of our table, we consider the number of samples used for estimation to be the number of samples used to fit the critic for given MI value. Since the estimator saw 4K steps of batch size 64 for each MI (2, 4, 6, 8, 10), we list the minimum samples seen as 4,000 64 = 256K. This work also includes consistency tests constructed by adapting MNIST images, but this setting does not carry ground-truth MI values (only the ideal ratio of estimates). MINDE (Franzese et al., 2023) used the BMI benchmark (Czyz et al., 2023) at larger sample sizes (100K rather than 10K). For our table, we consider the number of unique distribution types considered; for BMI, there are 20 unique distribution tasks after excluding tasks that are higherdimensional versions of another. The highest MI value used was 5, and the maximum dimension considered was 25. MINDE also performs image consistency experiments as in SMILE, but this setting does not have ground truth value. VCE Chen et al. (2025) evaluate estimators on five synthetic distributions with varying dependence. In this setting, MI ranges from 0 to 16, 10K samples are used, and dimensions range from 2 to 64. Then, the authors study increasing dimensionality on five distributions, with dimensions from 2 to 64 and MI values up to 14. 10K samples were used, and six distributions across the two synthetic experiments were not gaussian. Next, authors consider correlated rectangles and gaussian plates. These images have dimension 16x16 but are processed by an autoencoder prior to prediction to produce 16 dimensional embeddings. The max MI for this setting is 7 and 10K samples were used. Finally, they consider language embeddings. Data is similarly processed by an autoencoder to be 16 dimensions. The maximum MI is 2.1, and 400 samples were used. MINE (Belghazi et al., 2018) was first evaluated on 2-D and 20-D Gaussians with MI from 0 to 2 and 0 to 40, respectively. The number of samples used was not specified. In second experiment, 21 the authors applied deterministic nonlinear transformation and demonstrated that their estimates are invariant to it. NWJ (Nguyen et al., 2010b) introduced variational bound for estimating the KL-divergence between two arbitrary distributions, and their experiments focused on evaluating the quality of this bound and estimation on various distributions. The authors did not evaluate this approach on mutual information prediction directly, for which the distributions compared would be joint distribution and the product of its marginals. InfoNCE (Oord et al., 2018) was developed as method for representation learning, with evaluation including representation learning tasks but without any evaluation of MI prediction itself. DETAILS OF META-DATASET CREATION We construct meta-dataset of synthetic distributions with known MI using the BMI library (Czyz et al., 2023), which generates complex yet tractable distributions via invertible transformations applied to base families with analytical MI. Each meta-sample consists of joint samples from distribution paired with its ground-truth MI value, enabling supervised learning directly over distributions. To promote diversity and generalization, we partition distribution families into disjoint training Dtrain and testing Dtest groups and vary the sample dimensionality from 2 to 32. Since many existing estimators are prohibitively slow at this scale, we construct two evaluation corpora: smaller Mtest benchmark set for comparison with existing methods, and larger Mtest-extended set for assessing generalization to novel and higher-dimensional distributions. Complete details of meta-dataset sizes and included distribution families are provided in Table 4. The dataset includes both In-Meta-Distribution (IMD) and Out-of-Meta-Distribution (OoMD) families, differing in their base distributions and MI-preserving transformations. IN-META-DISTRIBUTION (IMD) The IMD test set comprises samples drawn from the following base distributions, each with its own structured variants and associated sampled hyperparameters: multi normal: Multivariate normal distributions over (X, ) with jointly Gaussian structure and identity marginal variances. Structured variants are parameterized as follows: dense: off diag U(0.0, 0.5) lvm: interacting {1, . . . , 5} alpha U(0.0, 1.0) lambd U(0.1, 3.0) beta U(0.0, 1.5) eta U(0.1, 5.0) (uniform pairwise correlation between all variables) (latent dimensionality) (anisotropy; relative scaling of vs. loadings) (latent signal strength) (additive noise scale in X) (intensity of nonlinear warping in X) multi student: Multivariate Students t-distributions over (X, ) sharing the same covariance (or scatter) structure as their Gaussian counterparts, but with heavier tails controlled by the degrees of freedom. Structured variants are parameterized as follows: df (1, 10) dense: off diag U(0.0, 0.5) (uniform pairwise correlation) (degrees of freedom; lower values correspond to heavier tails) sparse: interacting {1, . . . , 5} (latent dimensionality; governs low-rank structure) (latent signal strength; used as lambd and eta) (degrees of freedom) strength U(0.1, 5.0) df (1, 10) OUT-OF-META-DISTRIBUTION (OOMD) The OoMD test set comprises distribution families that are absent from the training set. It focuses on the following base distribution: multi additive noise: multivariate additive noise model in which, for each dimension = 1, . . . , d, Xj Uniform(0, 1), Nj Uniform(ϵ, ϵ), Yj = Xj + Nj, 22 with shared noise scale ϵ U(0.1, 2.0). This distribution is non-Gaussian with bounded support, and its mutual information equals the sum of per-dimension contributions. Sampled hyperparameter: epsilon U(0.1, 2.0) signal-to-noise ratio) (half-width of the uniform noise interval; controls the The true MI values for these distributions and their transformations are computed analytically or numerically, depending on tractability. Dataset Split"
        },
        {
            "title": "Mtrain",
            "content": "Size tended General) (Ex- / 624652 Mtest, IMD 372000 / 1080 Mtest, OoMD 434000 / 1260 Distribution Families multi normal-dense-[base, asinh, halfcube], multi normal-lvm-[base, wigglify], multi normal-sparse-[base, wigglify], multi normal-2pair-base, multi student-dense-[base, wigglify], multi student-sparse-[base, asinh, halfcube], multi student-2pair-[asinh, halfcube], multi additive noise-wigglify multi normal-dense-[base, halfcube], multi normal-lvm-wigglify, multi student-dense-base, multi student-sparse-[asinh, base] multi normal-dense-wigglify, multi normal-sparse-[asinh, halfcube], multi student-dense-halfcube, multi student-sparse-wigglify, multi additive noise-[base, halfcube] 4: Meta-Dataset Description. Table as multi <base dist>-<structure>-<transform>, where <base dist> denotes the base distribution type, <structure> specifies the parameterization structure, and <transform> indicates the nonlinear transformation applied to the data. For resourceconstrained evaluation, reduced test subset of 1k samples is used for both IMD and OoMD splits. The reduced subsets preserve the same distribution family composition as their full counterparts. distribution labeled family Each is"
        },
        {
            "title": "C RESULTS",
            "content": "C.1 EXPERIMENTS FOR NORMALIZED AND MULTI-TASK PREDICTIONS In high-dimensional settings, estimated MI values increased substantially, often reaching several dozen, as illustrated in Figure 9. Such wide numerical range can make direct regression with MSE or QCQR losses unstable, since large target values can lead to unstable gradients and slow convergence. To mitigate this issue, we applied normalization to the target variable, constraining its range and improving numerical stability. To stabilize regression over wide MI ranges, we evaluated two normalization schemes: ˆIlog = log(cid:0)I(X; ) + 1(cid:1), ˆIinv = I(X; ) I(X; ) + , (7) which reduce the impact of outliers and transform regression into constrained prediction problem. In addition to target normalization, we enriched the model by predicting auxiliary entropy coefficients corresponding to H(X), H(Y ), and H(X, ). Using the same monotonic mapping 23 (z) = z/(1 + z), the normalized forms were defined as: ˆHY,inv = f(cid:0)H(Y )(cid:1), ˆHX,inv = f(cid:0)H(X)(cid:1), ˆHXY,inv = f(cid:0)H(X, )(cid:1). (8) We investigated two formulations: (i) predicting the three entropy coefficients and reconstructing MI via Eq. 6, and (ii) predicting all four coefficients (cid:0) ˆIinv, ˆHX,inv, ˆHY,inv, ˆHXY,inv (cid:1) in the multi-task setting. For the four-coefficient configuration, physics-informed term was introduced to enforce consistency between two independent MI estimates. The first estimate is obtained from the directly predicted MI coefficient ˆI1 = 1(cid:0) ˆIinv (cid:1) + 1(cid:0) ˆHY,inv (cid:1) 1(cid:0) ˆHXY,inv (cid:1). To achieve alignment between these quantities, the overall training objective is expressed as (cid:1) and the entropy-derived target ˆI2 = 1(cid:0) ˆHX,inv Ltotal = 1 4 (cid:124) (cid:88) (cid:13) (cid:13) ˆQj yj (cid:13) 2 (cid:13) + λphys EB j{inv,X,Y,XY } (cid:123)(cid:122) Lbase (cid:125) (cid:124) (cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ˆI1 ˆI2 + ϵ (cid:123)(cid:122) Lphys 1 (cid:35) , (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:125) (9) where ˆQj { ˆIinv, ˆHX,inv, ˆHY,inv, ˆHXY,inv} denotes the predicted normalized coefficients, λphys = 0.3 balances regularization strength, ϵ > 0 prevents division by zero, and EB[] denotes expectation over training batch. We trained this model on fixed-dimension subset from Mtrain with all samples having dim = 9. The goal of this setup is to study whether predicting physically-related coefficients can help refine the models latent representations, incorporate additional structural knowledge, and improve prediction robustness. Prediction Method Mtest,IMD Mtest,OoMD Direct MI Log norm. ( ˆIlog) Inverse norm. ( ˆIinv) Entropy (3-coef) Entropy & ˆIinv (with Lphys) Entropy & ˆIinv (w/o Lphys) 5.4e2 2.0 1.2e 2.3 8.5e0 8.7e2 2.5e1 5.2e2 7.0e2 2.1 2.5 3.0 1. 2.4 1.4e1 1.9e0 3.5e0 1.5e0 1.8e0 2.4 3.3 4.0 2. 3.0 Figure 9: Distribution of true MI values within Mtrain. Figure 10: Mean MI estimates across prediction strategies on Mtest (dim = 9), comparing normalization methods and coefficient configurations. 3-coef uses the estimators ( ˆHX,inv, ˆHY,inv, ˆHXY,inv); the last two entries show predictions with ˆIinv, with and without the physics-informed loss Lphys. Values are (mean stddev) over three runs. Experiments on Mtest with dim = 9 are summarized in Table 10. The best results were obtained using two methods: direct MI prediction, and the four-coefficient formulation with the physicsinformed loss. The comparatively lower performance of the three-coefficient model can be attributed to error propagation, since each entropy term is predicted independently and their individual errors accumulate during MI reconstruction. In this work, we proceed with the direct MI prediction strategy for subsequent experiments. Nevertheless, the four-coefficient formulation remains promising direction for future research, as it provides richer structural information about the underlying distributions and could facilitate the derivation of both MI and entropy values within unified framework. C.2 DIMENSIONALITY ABLATION Figures 11 through 13 contain additional bar plots for the experiment described in the second half of Section 4.6. We give in Figures 14 through 16 high-level summary of the general performance of all various fixedand variable-dimensionality models for ease of comparison. 24 Figure 11: Average MSE obtained by variableand fixed-dimensionality versions of MIST on subsets of Mtest-extended,OoMD with fixed dimensions. Figure 12: Average MSE obtained by variableand fixed-dimensionality versions of MISTQR on subsets of Mtest-extended,IMD with fixed dimensions. Figure 13: Average MSE obtained by variableand fixed-dimensionality versions of MISTQR on subsets of Mtest-extended,OoMD with fixed dimensions. 25 Figure 14: Average MSE of the proposed models with variable input dimensionality versus dimension-specific variants. In this plot and the following ones, the test set was similar to the training one: variable-dimensionality models were evaluated on the full Mtest-extended, while other models were evaluated on subsets of it. Figure 15: Average bias magnitude of the proposed models with variable input dimensionality versus dimension-specific variants. 26 Figure 16: Average variance of the proposed models with variable input dimensionality versus dimension-specific variants. 1.8e3 1.3e2 9.1e2 1.2e3 1.5e3 8.8e2 1.1e2 1.1e3 2.7e2 6.7e2 7.7e2 1.1e2 2.0e3 CCA 3.2e2 1.5e1 4.7e2 1.6e1 7.7e1 7.6e1 4.6e2 5.7e1 7.5e1 5.2e1 2.5e1 4.6e1 4.1e1 KSG 2.3e5 7.8e4 4.3e3 2.8e3 1.8e3 2.1e3 6.7e4 6.9e4 5.4e2 3.5e3 1.2e3 1.8e3 1.8e3 MINE InfoNCE 1.5e1 3.9e1 1.4e1 2.0e1 1.1e2 1.3e2 1.3e1 8.0e1 1.1e2 3.2e2 3.2e1 7.1e1 6.9e1 1.4e17 2.1e18 6.2e16 3.9e18 6.5e18 2.3e19 1.5e17 1.8e18 2.3e16 1.6e18 6.0e18 6.0e18 1.6e18 DV 2.1e1 1.6e1 1.4e1 2.7e1 8.3e1 8.4e1 1.9e1 5.9e1 8.0e1 5.4e1 2.9e1 4.7e1 4.2e1 LMI MINDE 8.8e1 1.4e1 1.3e0 9.2e1 8.2e1 8.0e1 6.8e1 5.7e1 7.3e1 5.5e1 9.6e1 4.4e1 3.6e1 6.5e2 1.5e0 5.4e2 2.5e1 1.4e0 2.0e0 1.6e1 1.6e0 9.5e1 2.0e0 3.4e1 2.1e1 3.0e1 MIST MISTQR 3.4e2 1.5e0 2.6e2 1.4e1 1.5e0 1.6e0 9.7e2 1.4e0 6.9e1 1.7e0 2.7e1 3.0e1 3.6e ) ( r t @ g ) e ( r t ) e ( m i M @ c - H ) e ( r t @ g ) a ( - d @ s ) a ( r t @ c - H ) a ( r t @ s ) a ( - d @ g ) e ( - d @ c - H o i d @ u - H o i d ) e ( - d ) a ( - d S Table 5: Mean mutual information (MI) estimates across data distributions. Color indicates whether distributions are from the in-meta-distribution (blue) or out-of-meta-distribution (red) subsets from Mtest. We also highlight in green values that are within one order of magnitude of the best and mark the best values in bold. C.3 COMPARING WITH EXISTING ESTIMATORS For more detailed evaluation of existing baselines and our trained models MIST and MISTQR on Mtest, we also report the MSE loss values averaged separately over each distribution family in Table 5. 27 C."
        },
        {
            "title": "INFERENCE TIME",
            "content": "We report in Table 6 the inference time of each of the baselines, averaged across the data from Figure 6. Method MIST(QR) CCA KSG LMI InfoNCE NWJE DV MINE MINDE Time / sample (s) 0.000 55 0.000 90 0.021 0.76 2.01 2.15 2.20 2.75 122.7 Table 6: Average inference time per sample measured on Mtest-extended. C.5 SCALING BEHAVIOR We report in Table 7 the performance of the main model (5.5M) used in our experiments compared to scaled-down models. Model Params IMD OoMD (M) MSE Abs. Bias Variance MSE Abs. Bias Variance MIST MISTQR MISTmedium MISTQR-medium MISTsmall MISTQR-small 5.5 5.5 1.0 1.0 0.25 0.25 0.7508 0.7637 1.6223 2.0265 1.5498 1.8910 3.0255 0.6853 5.6388 1.1974 5.1608 1.1645 0.3401 0.2941 0.4427 0.5928 0.4851 0. 7.6679 9.2189 11.2461 10.5160 8.9834 11.3058 20.2033 2.9803 30.1180 3.1461 24.1663 3.2777 0.3743 0.3370 0.4765 0.6183 0.5342 0.5628 Table 7: Average performance metrics for models of different sizes. We give in Figure 17 fine-grained heatmap of the MSE performance obtained by MIST and MISTQR for various sample size groups and dimensions. SELF-CONSISTENCY TESTS D.1 SELF-CONSISTENCY OF QUANTILE REGRESSION We checked the consistency of MISTQRs quantile predictions in two ways: Monotonicity: For given fixed sample (x, y), we sample random uniform value of τ in (0, 1) and estimate ˆIτ = MISTQR(x, y, τ ). We repeat this process with values (τ1, . . . , τn) and check whether ( ˆIτ1 , . . . , ˆIτn ) is an increasing sequence. We perform this operation on each data point of Mtest using = 1000, and count the number of points for which monotonicity is not respected. Bound checking: For given fixed sample (x, y) with true MI Itrue, we compute ˆIlower = MISTQR(x, y, τ = 0) and ˆIupper = MISTQR(x, y, τ = 1). We perform this operation on each data point of Mtest, and count the number of points for which Itrue < ˆIlower (lower bound failure) or ˆIlower < Itrue (upper bound failure). The results, given in Table 8, show that monotonicity breaks occur relatively rarely on both IMD and OoMD sets (1.1% and 1.4%, respectively). Further analysis reveals that monotonicity failures Figure 17: Detailed visualization of the average MSE values for our two proposed models across different sample-size groups and all evaluated dimensionalities. The measurements were obtained on Mtest-extended,IMD. The results highlight the increased difficulty of estimating MI in small-sample regimes and the consistent improvement in performance as the sample size grows. 29 only occur on points with very low values of Itrue < 0.05. Bound failures happen at moderate rate on the IMD dataset, and significantly more often for lower bounds (9.6%) than for upper ones (4.3%). When moving to OoMD distributions, however, those failure rates increase to 15.8% for lower bounds and 27.5% for upper ones, showing that this task remains difficult for our model. IMD (1080 points) OoMD (1260 points)"
        },
        {
            "title": "Monotonicity breaks\nLower bound failures\nUpper bound failures",
            "content": "15 (1.4%) 104 (9.6%) 46 (4.3%) 12 (1.2%) 199 (15.8%) 346 (27.5%) Table 8: Consistency of the τ parameter of MISTQR on the IMD and OoMD subsets of Mtest D.2 SELF-CONSISTENCY OF THE LEARNT QUANTITY Figures 18 through 20 contain the results of three consistency tests applied to MIST and MISTQR: The independence test checks that if , then I(X; ) = 0. The data processing inequality checks that if X, and form Markov chain Z, then I(X; ) I(X; Z) (further processing of the data cannot add new information). The additivity over independent samples test checks that if (X1, X2) (Y1, Y2), then I(X1, Y1; X1, Y2) = I(X1; X2) + I(Y1, Y2) (a) MSE Model (b) QCQR Model, τ = 0.95 Figure 18: Independence Test: is an MNIST image, and Br contains the top rows of image A. For consistent estimator, the plotted MI ratio will be non-decreasing and approach 1 with an increasing number of shared rows. (a) MSE Model (b) QCQR Model, τ = 0.95 Figure 19: Data Processing Test: is an MNIST image, and Br contains the top rows of image A. This test compares I([A, A], [Br, Br3]) to I([A, Br]), where is an original MNIST image, Br is the first rows of A, and so Br3 is further masked version of image A. The test produces ratio which should be near to 1 for all values of r, which amounts to checking that adding further masked version of your image Br does not change the predicted MI. Images are encoded with PCA fit on 10K random samples, and error bars of 1σ reported, with 100 bootstrapped samples. (a) MSE Model (b) QCQR Model, τ = 0.95 Figure 20: Additivity Test: is an MNIST image, and Br contain the top rows of image A. The premise of this test is that adding second independent sample should double the mutual information estimate. The ratio compares I([A1, A2], [B1r, B2r]) to I([A, Br]), and should be near 2 for all values of r. Images are encoded with PCA fit on 10K random training samples. Error bars of 1σ are reported with 100 bootstrapped samples."
        }
    ],
    "affiliations": [
        "New York University",
        "Prescient Design, Genentech",
        "Universite Grenoble Alpes, CNRS, Grenoble INP, LIG"
    ]
}