{
    "paper_title": "Singular Value Few-shot Adaptation of Vision-Language Models",
    "authors": [
        "Taha Koleilat",
        "Hassan Rivaz",
        "Yiming Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present \\textbf{CLIP-SVD}, a novel \\textit{multi-modal} and \\textit{parameter-efficient} adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only \\textbf{0.04\\%} of the model's total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 0 4 7 3 0 . 9 0 5 2 : r Singular Value Few-shot Adaptation of Vision-Language Models Taha Koleilat1 Hassan Rivaz1 Yiming Xiao2 1 Department of Electrical & Computer Engineering, Concordia University, Montreal, Canada 2Department of Computer Science & Software Engineering, Concordia University, Montreal, Canada {taha.koleilat, hassan.rivaz, yiming.xiao}@concordia.ca"
        },
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present CLIP-SVD, novel multi-modal and parameter-efficient adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only 0.04% of the models total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD."
        },
        {
            "title": "Introduction",
            "content": "Vision-language models (VLMs), such as CLIP [57], have demonstrated remarkable versatility and generalization by aligning images and text through large-scale contrastive pretraining. These models enable powerful zero-shot and few-shot capabilities for various applications, but adapting them effectively to downstream tasks remains non-trivial. Full model fine-tuning is often computationally infeasible, while prompt learning strategies (e.g., CoOp [82] and CoCoOp [81]) may be limited by heavy dependency on handcrafted or learned text prompts. Adapter-based methods, such as CLIP-Adapter [19] and MaPLe [35], infuse additional modules to improve adaptation quality, but this increases model complexity, lowers inference efficiency, and sometimes degrades zero-shot performance by destabilizing pretrained representations [36]. Recent efforts in parameter-efficient fine-tuning (PEFT) aim to overcome these limitations. Among them, Singular Value Fine-Tuning (SVF) [63] has emerged as compelling strategy by modifying only the singular values of model weight matrices without changing the original model. While SVF has shown promise in CNNs and large language models (LLMs) [63, 49], it has not been fully explored in Transformer-based, multi-modal settings. Furthermore, despite the popularity of CLIP adaptation methods, very few have attempted to interpret the dynamics and effectiveness of model adaptation. Lastly, most CLIP adaptation techniques focus on natural domains alone, with few specialized in biomedical applications Coresponding author Table 1: Natural language-based interpretations for the top 3 Attention Heads associated with the highest normalized changes (sorted in descending order) in the Output-Value circuit after CLIP adaptation for different datasets. Here, L\" denotes layer while H\" denotes attention head. EuroSAT (Satellite Images) DTD (Texture Images) (L10.H0): Aerial Landscapes & Environments (L10.H10): Mood, Atmosphere & Highlights (L11.H0): Semantic Layout & Spatial Context (L8.H6) Refined Textural Details of Everyday Objects (L10.H2) Cultural & Textural Scenes (L9.H4) Natural Landscapes & Textures SUN397 (Scene Understanding) UCF101 (Action Recognition) (L11.H0): Semantic Layout & Spatial Context (L11.H2): Numbers, Symbols & Temporal Cues (L11.H3): Lifestyle & Tranquil Activities (L10.H5) Action, Emotion & Faces (L10.H6) Organic Flow & Movement (L10.H1) Human Experiences & Objects in Action BUSI (Breast Ultrasound) BTMRI (Brain MRI) (L11.H8): Converging Edges & Cluster Markers (L8.H6): Contour Irregularity & Internal Spread (L8.H3): Radiologic Artifacts & Diffuse Shapes (L8.H9) Scattered Highlights & Artifactual Spots (L8.H0) Focal Markers & Shape Cues (L9.H5) Streaks, Texture, & Soft Borders COVID-QU-Ex (Chest X-ray) CTKIDNEY (Kidney CT) (L11.H0): Signal Voids & Shifts (L9.H1): Ring-Like Structures & Localized Spread (L11.H3): Cross-Lobe Flow & Density Buildup (L8.H6): Contour Irregularity & Internal Spread (L8.H3): Radiologic Artifacts & Diffuse Shapes (L8.H10): Diffuse Zones & Overlapping Shapes [5, 39] due to distinctive visual features and complex clinical descriptions. This creates gap for universal strategy that can generalize effectively across natural and biomedical domains without high computational complexity or tailored adjustments (e.g., specialized prompt engineering [5, 39]). To address the aforementioned challenges, we present CLIP-SVD, novel parameter-efficient few-shot framework for unified CLIP adaptation across both natural and biomedical domains. While many methods [82] primarily focus on the text branch, recent ones [35] have shown the benefit of adapting image and text branches jointly, at the cost of heavy add-on modules. For example, the popular MaPLe requires additional trainable parameters for 2.85% of the CLIP model. In contrast, our approach leverages Singular Value Decomposition (SVD) to decompose the projection weights in CLIPs attention and feedforward layers into corresponding singular values and singular vectors, with only the first fine-tuned for both image and text encoders. We hypothesize that this allows the model to rescale the basis vectors for each downstream task with superior adaptation quality and generalizability. Furthermore, our combination of CLIPs multi-head attention and SVD-based weight adaptation invites an opportunity for natural language-based paradigm to localize, rank, and semantically describe the most significant dynamic shifts in the adapted model. Here, we probe the best text basis to map the semantic meaning of the attention heads [18] with the most significant updates through CLIP-SVD, as shown in Table 1 for 16-shot CLIP adaptation on distinct tasks. Compared with previous efforts [63] that rely on visual interpretation and/or model weight statistics, this approach offers more intuitive and granular insights into CLIP adaptation. Yet, related text corpus for analyzing biomedical data is still unavailable. Our study has four major contributions: First, we proposed an SVD-based few-shot adaptation framework for Transformer-based multi-modal settings (e.g., CLIP and BiomedCLIP) for the first time, requiring just 0.04% of the models total parameters, significantly lower than other multi-modal methods. Second, we performed comprehensive validation with 11 natural and 10 biomedical domain datasets, demonstrating CLIP-SVDs superior performance against the state-of-the-art (SOTA) methods in both accuracy and generalization. Third, with ranked weight changes associated with our method, we adopted natural language-facilitated approach to intuitively interpret the effectiveness and dynamics of task-specific CLIP adaptation. Lastly, to meet an urgent need for semantic interpretation of attention heads in CLIP for biomedical applications (e.g., analysis of CLIP-SVD), we built the first corpus of biomedical image descriptions."
        },
        {
            "title": "2.1 Parameter-efficient Fine-tuning",
            "content": "Fine-tuning large VLMs is often computationally prohibitive for domain-specific adaptation. Parameter-efficient fine-tuning addresses this by updating only small subset of parameters while keeping the backbone frozen [45]. Selective tuning methods like BitFit [74] adjust only bias terms 2 while pruning and sparsity techniques can further reduce trainable parameters [21, 25], though they often compromise robustness in zero-shot settings. Adapter-based tuning offers more robust alternative by inserting lightweight modules [59, 26, 32, 12, 46], but can introduce inference latency [54]. Popular prompt tuning [42, 44, 31] and Low-Rank Adaptation (LoRA) [27] provide other PEFT strategies, with LoRA inserting low-rank matrices to minimize overfitting [43, 2]. However, these methods typically rely on external, randomly initialized modules, risking destabilization and forgetting of original CLIP knowledge [78, 85]. Designing PEFT methods that enhance adaptation without compromising pre-trained strengths remains major goal. Recently, Singular Value Fine-Tuning (SVF) [63] has emerged as promising alternative. Initially applied to CNNs for segmentation with strong results, SVF modifies only the singular values of weight matrices while preserving their directions without introducing new modules. Later, SAM-PARSER [53] extended SVF to large vision Transformer models, but limited its application to the vision encoder, without fine-tuning query, key, and value (Q, K, V) matrices crucial for cross-modal tasks. Additionally, SVD-based methods have shown effectiveness in LLMs [49], but their potential for multi-modal VLM adaptation remains largely unexplored, offering an exciting direction for future research."
        },
        {
            "title": "2.2 Adapting Vision-Language Models",
            "content": "Vision-language models, such as CLIP [57] and ALIGN [30] have significantly advanced multi-modal learning by aligning image and text embeddings in shared space using self-supervised contrastive training. These models perform well on general-domain tasks like zero-shot classification and cross-modal retrieval, but their reliance on broad, non-specialized datasets limits their effectiveness in expert domains, such as healthcare, where nuanced visual cues and domain-specific semantics are critical. To address this, recent research has explored adapting VLMs to specialized settings using techniques like prompt learning, which offers lightweight alternative to full fine-tuning. Methods, such as CoOp [82] and CoCoOp [81] learn optimized prompts while keeping the VLM backbone frozen, with extensions like MaPLe [35] and PromptSRC [36] improving robustness through encoder tuning and self-regularization. Adapter-based strategies like CLIP-Adapter [19] and Tip-Adapter[77] modify the visual branch or incorporate support-set features to boost few-shot performance, although they may face optimization hurdles. Enhanced probing methods such as LP++ [28] further refine adaptation by balancing modality-specific features with adaptive learning dynamics. In the biomedical domain, adaptations of CLIP like BioViL [6], PubMedCLIP [16], and BiomedCLIP [79] leverage domain-specific corpora to improve relevance, with specific methods bridging general-purpose biomedical VLMs and specialized clinical tasks [37, 38, 62, 58]. Yet, these models still struggle with fine-grained clinical understanding [70, 80]. Prompt-learning methods, including XCoOp [5] and DCPL[9], extend CoOp-style tuning to medical applications, but often demand relatively large training sets. In comparison, BiomedCoOp [39] demonstrates that prompt tuning can preserve generalization across diverse medical tasks even in low-resource conditions. Despite this broad range of techniques, no method has yet achieved robust performance across both natural and biomedical domains."
        },
        {
            "title": "3.1 CLIP Preliminaries",
            "content": "CLIP consists of vision encoder Ev and text encoder Et that project images and text into shared embedding space. Given batch of images and distinct classes, image inputs Xv RB3HW are RGB images of height and width , and text inputs Xt RCL are tokenized sequences of length L, where each sequence serves as text prompt representing single class. The encoders generate modality-specific features: = Ev(Xv) RBD, = Et(Xt) RCD (1) where is the embedding dimension and is the number of candidate classes. Both and are L2-normalized onto the unit hypersphere. In zero-shot classification, CLIP matches an image to class descriptions (e.g., \"a photo of [CLASS]\"). The probability of assigning image embedding ˆV to class is: 3 exp( ˆV ˆT(k)/τ ) j=1 exp( ˆV ˆT(j)/τ ) where τ is learnable temperature parameter. The predicted class ˆk is: p(Y = ˆV, ˆT) = (cid:80)C ˆk = arg max p(Y = ˆV, ˆT) (2) (3) This formulation enables CLIP to generalize to unseen categories by leveraging the alignment between images and natural language descriptions."
        },
        {
            "title": "3.2 Singular Value Decomposition of Weight Matrices",
            "content": "SVD-Based Decomposition of Pre-trained Weights: The overall framework of CLIP-SVD is shown in Fig. 1. For our proposed CLIP-SVD technique, we decompose the weight matrices in the MultiHead Self-Attention (MHSA) and Multi-Layer Perceptron (MLP) blocks of each Transformer layer in CLIPs text and image encoders with SVD. Specifically, each weight matrix in the MHSA and MLP blocks can be factorized using SVD as follows: = SR (4) where Rdr is the left singular vector matrix, = diag(λ1, λ2, λ3, ..., λr) Rrr is diagonal matrix containing the singular values (λ1 λ2 λr 0) arranged in descending order, Rmr is the right singular vector matrix, and = min(d, m) is the rank of . Instead of fully modifying directly, we freeze the singular vectors and and fine-tune only the singular values λi. Note that we adapt the vector of full-rank singular values for our application. We further analyze the effect of different rank configurations in Appendix D. Multi-Head Self-Attention Computation: Each Transformer layer in CLIP-type models applies MHSA using the Query (Q), Key (K), Value (V), and Output (O) projection matrices: WQ, WK, WV RDd, WO RdD. Given an input RBLD, self-attention for the hth head is computed as: Qh = XWQh = X(UQh SQhR Qh Zh = softmax (cid:19) (cid:18) QhK ), Kh = XWKh = X(UKhSKhR Kh (cid:18) QhK (XUVh SVhR Vh (cid:19) ) (XWVh) = softmax (5) (6) (7) ) where is the batch size, is the sequence length, is the embedding dimension, and is the dimension of each attention head. For attention heads of Transformer layer: ZMHSA = Concat(Z1, . . . , ZG)(WO) = Concat(Z1, . . . , ZG)(UOSOR The output of the multi-head self-attention is then combined with the input via residual connection: = + ZMHSA. O). (9) (8) Feed-forward Network: Following the self-attention block, the updated representation is passed through feedforward MLP block ({Win, Wout}), which is also decomposed using SVD. The MLP applies two linear transformations with an activation function in between, and finally, residual connection is applied: = ReLU(X Win) = ReLU(X UinSinR in) = H(Wout) = H(UoutSoutR out) Xout = + . (10) (11) (12) With CLIP-SVD, each Transformer layer maintains its original representational capacity by rescaling the singular vectors, thus allowing robust adaptation and retention of pretrained knowledge. 4 Figure 1: The overall framework of CLIP-SVD. We decompose the Query, Key, Value, and Output projection weights (WQ, WK, WV and WO) of the MHSA blocks in both vision and text encoders Ev and Et, as well as the linear weights of the Feed-forward Networks (WM LP ) in all layers. We finetune only the singular values of the SVD decomposed weights. 3.3 Interpretation of CLIP-SVD with TextSpan To understand how our adaptation reshapes CLIPs internal representations, we utilize TextSpan [18], which aligns text descriptions to each attention head of layer in the ViT image encoder to reveal their semantic roles. This is achieved by decomposing the MHSA outputs xl,h into summation of contributions from image tokens through SVD: xl,h = (cid:88) i=0 xl,h , = αl,h xl,h l,h l,h zl1 = αl,h (U l,hSl,hV l,h)zl , (13) denotes the attention weight from the class token to token i, and zl1 where αl,h is the input token representation. TextSpan interprets the semantic roles of heads by projecting candidate text features Tcorpus into the span of xl,h: Tl,h = xl,h(xl,h xl,h)1xl,h Tcorpus, with l,h = xl,h Tl,h, (14) and identifies directions that maximize explained variance in l,h. Importantly, this procedure depends only on the span of the singular vectors l,h, not their magnitudes. In contrast, CLIP-SVD finetunes only the singular values Sl,h: xl,h = αl,h l,h Sl,hV l,hzl1 = αl,h sl,h vl,h , zl1 ul,h , (15) (cid:88) which preserves the Output-Value (OV) subspace span(U l,h) while reweighting its basis directions through sl,h . Thus, TextSpan and CLIP-SVD are complementary: the former reveals which semantic directions are encoded in span(U l,h), while the latter modulates how much each direction contributes after adaptation. To quantify these effects, we rank heads by the normalized change in their singular values Sl,h , summing absolute changes across both matrices. This provides direct measure of the functional shifts in heads that TextSpan already grounds in interpretable text semantics. and Sl,h"
        },
        {
            "title": "4.1 Benchmark evaluation settings",
            "content": "Few-Shot Learning: To assess the models performance under limited supervision, we conduct few-shot image classification experiments with varying numbers of labeled examples per class (K = 1, 2, 4, 8, and 16 shots) to assess the robustness of our method across both natural and biomedical 5 domains. This is critical for evaluating the methods ability to learn effectively from sparse data by obtaining task-specific knowledge while retaining general domain comprehension. Base-to-Novel Generalization: We evaluate the generalizability of CLIP-SVD in natural and biomedical domains, and follow zero-shot setting, where the datasets are split into base and novel classes for classification tasks. Here, the model is trained only on the base classes in few-shot setting and evaluated on both base and novel categories. Additionally, we compute the harmonic mean (HM) of both base and novel class prediction accuracies. Cross-dataset Evaluation: To validate the performance of our approach in cross-dataset transfer, we evaluate our ImageNet-trained model directly on other datasets in the natural domain. Consistent with previous methods, our model is trained on all 1000 ImageNet classes in few-shot manner. Due to the lack of similar ImageNet-like dataset and large domain shifts across datasets, we didnt perform cross-dataset evaluation for the biomedical domain. Datasets: For the natural domain, we follow [82, 81] and evaluate the performance of our method on 11 image classification datasets that cover wide range of recognition tasks. This includes two generic-objects datasets, ImageNet [15] and Caltech101 [17]; five fine-grained class-specific datasets, OxfordPets [52], StanfordCars [40], Flowers102 [51], Food101 [8], and FGVCAircraft [48]; scene recognition dataset SUN397 [69]; an action recognition dataset UCF101 [61]; texture dataset DTD [13] and satellite-image dataset EuroSAT [22]. For the biomedical domain, we follow [39] and evaluate the performance of our method on 10 diverse medical imaging datasets covering 9 different organs and 8 imaging modalities: Computerized Tomography (CTKidney [29]), Endoscopy (Kvasir [55]), Fundus Photography (RETINA [56, 41]), Histopathology (LC25000 [7], CHMNIST [33]), brain tumor Magnetic Resonance Imaging (BTMRI [50]), Optical Coherence Tomography (OCTMNIST [34]), breast ultrasound (BUSI [3]), and chest and knee X-Ray (COVID-QU-Ex [64], KneeXray [10]). Implementation Details We use few-shot training strategy in all experiments, with random sampling for each class. We use the ViT-B/16 CLIP and BiomedCLIP models for natural and biomedical domains, respectively. All models are trained using batch size of 32 and the AdamW optimizer[47] with weight decay of 0.01 on single NVIDIA A100 GPU (40GB RAM). For natural image datasets, we set the learning rate at 5 104 for few-shot classification, 6 104 for base-to-novel evaluations, and 5 104 for cross-dataset transfer tasks. In the biomedical domain, learning rates are optimized per dataset based on validation performance, depending on task complexity and data modality. We report classification accuracies averaged over three independent runs. Prompt templates and full hyperparameter configurations are detailed in Appendix and B, respectively."
        },
        {
            "title": "4.2 Few-shot Evaluation",
            "content": "Our method demonstrates superior performance in few-shot learning across both natural and biomedical domains, as shown in Tables 2 and 3. In the natural domain, CLIP-SVD achieves +1.00% improvement over the second-best method (CLIP-LoRA) in the 1-shot setting (73.20% vs. 72.20%). In the biomedical domain, it surpasses the second-best approach (BiomedCoOp) by +4.28% in the 8-shot setting (73.24% vs. 68.96%). These consistent gains highlight the robustness and effectiveness of our SVD-based tuning strategy across diverse domains."
        },
        {
            "title": "4.3 Base-to-Novel Generalization",
            "content": "Our method demonstrates strong base-to-novel generalization across both natural and biomedical domains, as shown in Tables 4 and 5. In the natural domain, CLIP-SVD improves over MaPLe by +1.67% on base accuracy, +1.06% on novel accuracy, and +1.58% on the harmonic mean, despite MaPLe being approximately 38 more computationally expensive. In the biomedical domain, CLIP-SVD achieves substantial gains over BiomedCoOp, improving base accuracy by +4.04%, novel accuracy by +0.41%, and the harmonic mean by +4.21%. These results highlight the robustness and scalability of our SVD-based tuning approach across domains. In addition, they also demonstrate the benefit of multi-modal tuning. The proposed CLIP-SVD enhances generalization without compromising the powerful representations learned during CLIPs pretraining. 6 Table 2: Evaluation against state-of-the-art techniques for natural domain: The average classification accuracy (%) obtained from 11 benchmarks derived from 3 sampled support sets for each dataset. The top-performing results are in bold, and the second-best are underlined."
        },
        {
            "title": "Method",
            "content": "K = 1 = 2 = 4 = 8 = 16 Zero-shot CLIP [57] CoOp [82] CoCoOp [81] ProGrad [84] KgCoOp [71] MaPLe [35] Linear Probing [57] LP++ [28] CLIP-Adapter [19] Tip-Adapter [77] Tip-Adapter-F [77] GDA [68] ProKeR [4] AdaLoRA [76] TCP [73] CLIP-LoRA [75] CLIP-SVD (Ours) 68.09 66.95 68.20 69.51 69.27 45.77 70.35 67.87 68.89 70.62 69.39 71.32 69.04 70.63 72.20 73.20 70.13 67.63 71.78 71.57 72.58 56.92 72.93 70.20 70.42 73.08 73.09 73.74 72.21 73.59 75.41 76.06 65.36 73.59 71.98 74.21 74.48 75.37 66.79 75.77 72.65 72.69 75.75 76.24 76.23 75.50 76.07 77. 78.18 76.45 72.92 77.93 75.82 78.89 73.43 77.94 76.92 74.41 78.51 79.71 79.84 78.13 78.39 80.10 80.55 79.01 75.02 79.20 77.26 81.79 78.39 80.32 79.86 76.44 81.15 81.70 82.01 80.95 80.98 82.89 82.97 Table 3: Evaluation against state-of-the-art techniques for biomedical domain: The average classification accuracy (%) obtained from 10 benchmarks derived from 3 sampled support sets for each dataset. The top-performing results are in bold, and the second-best are underlined."
        },
        {
            "title": "Method",
            "content": "K = 1 = 2 = 4 = 8 = 16 Zero-shot BiomedCLIP [79] CoOp [82] CoCoOp [81] ProGrad [83] KgCoOp [72] Linear Probing [57] LP++ [28] CLIP-Adapter [19] Tip-Adapter [77] Tip-Adapter-F [77] MaPLe [35] BiomedCoOp [39] CLIP-SVD (Ours) 52.59 50.88 53.67 54.31 48.91 49.27 45.53 50.35 52.55 37.99 56.87 56.35 55.71 53.91 56.42 55.79 55.82 55.88 44.70 53.50 54.17 40.89 59.32 62.63 42.38 61.35 57.63 62.10 60.92 62.12 61.30 45.30 58.33 62.30 44.09 64. 68.02 67.74 63.15 67.06 66.00 67.33 65.48 46.54 62.01 68.12 47.37 68.96 73.26 71.48 67.51 69.21 67.71 70.81 70.09 48.46 67.60 68.12 52.93 73.41 76."
        },
        {
            "title": "4.4 Cross-dataset Transfer",
            "content": "Table 6 shows that CLIP-SVD achieves the highest average accuracy of 66.99%, slightly outperforming MaPLes 66.30%. It obtains the best performance on several target datasets, including Aircraft (+1.29%), SUN397 (+0.73%), and UCF101 (+1.22%). These results suggest that CLIP-SVD offers strong cross-dataset transfer capabilities, confirming its potential for effective generalization."
        },
        {
            "title": "4.5 Ablation Experiments: Selective Model Component Fine-tuning",
            "content": "Effect of Tuning Different Weights: We ablated CLIP-SVDs components (WQ, WK, WV , WO, and WM LP ) under 4-shot setting in natural and biomedical domains (see Table 7). Without adaptation, the accuracy was 65.36% (natural) and 42.38% (biomedical). Adding WO alone substantially 7 Table 4: Base-to-novel generalization comparison measured by classification accuracy (%) between CLIP-SVD and SOTA methods on 11 natural domain datasets. IVLP ProGrad MaPLe CLIP-LoRA CLIP-SVD KgCoOp CoCoOp CoOp CLIP GDA Acc. TCP Base Novel HM 69.34 74.22 71.70 82.69 63.22 71.66 80.47 71.69 75.83 80.73 73.60 77. 82.48 70.75 76.16 82.28 75.14 78.55 84.21 71.79 77.51 83.96 74.53 78.72 84.13 75.36 79.51 84.10 74.80 79. 84.38 76.29 80.13 Table 5: Base-to-novel generalization comparison measured by classification accuracy (%) between CLIP-SVD and SOTA methods on 10 biomedical domain datasets. Acc. BiomedCLIP CoOp CoCoOp KgCoOp ProGrad MaPLe XCoOp BiomedCoOp GDA DCPL CLIP-LoRA CLIP-SVD Base Novel HM 49.27 67.17 55.23 76.71 65.34 68. 75.52 67.74 69.11 71.90 65.94 67.22 75.69 67.33 69.86 65.40 49.51 53.10 74.62 63.19 68.43 78.60 73.90 74. 57.70 64.66 60.98 73.70 69.35 71.46 70.56 59.84 64.76 82.64 74.31 78.25 Table 6: Cross-dataset natural image benchmark with classification accuracy (%) Source Target ImageNet Caltech"
        },
        {
            "title": "OxfordPet",
            "content": "StanfordCars Flowers102 Food"
        },
        {
            "title": "Aircraft",
            "content": "SUN"
        },
        {
            "title": "EuroSAT",
            "content": "UCF"
        },
        {
            "title": "Average",
            "content": "CLIP CoOp Co-CoOp KgCoOp ProGrad MaPLe 66.72 71.51 71.02 70.66 72.24 70.72 92.98 93.70 94.43 93.92 91.52 93.53 CLIP-SVD 72.15 93.68 89.13 89.14 90.14 89.83 89.64 90. 91.06 65.29 64.51 65.32 65.41 62.39 65.57 65.00 71.30 68.71 71.88 70.01 67.87 72.23 86.11 85.30 86.06 86.36 85.40 86.20 24.90 18.47 22.94 22.51 20.61 24. 62.59 44.56 47.84 64.15 41.92 46.39 67.36 45.73 45.37 66.16 46.35 46.04 62.47 39.42 43.46 67.01 46.49 48.06 66.83 66.55 68.21 68.50 64.29 68.69 65.15 63.88 65.74 65.51 62.71 66.30 72.45 86.21 26. 67.74 45.15 47.51 69.91 66.99 improved performance (75.45% and 62.27%), highlighting its importance. Including WM LP further boosted accuracy (77.78% and 66.40%), showing its role in fine-grained transformation. Adding WQ, WK, or WV individually on top of WO and WM LP led to near-identical results in the natural domain, but slight gains in biomedical, up to 67.40% with WV . Using all components yielded the best performance (natural: 78.18% and biomedical: 68.02%), confirming the benefit of full adaptation. tuning how different input modality Effect of Tuning Image and Text Encoders: trates Multi-modal tuning (text+image) consistently outperforms unimodal cases in both domains. In the natural domain, text-only and image-only achieve 75.78% and 74.31% accuracy, while their combination reaches 78.18%. In the biomedical domain, multi-modal tuning yields 68.02%, compared to 64.21% (text) and 65.94% (image), suggesting the complementary nature of visual and textual cues, especially valuable in biomedical settings with limited data and higher complexity. The right panel of Fig. 2 illusaccuracy. classification affect setups Table 7: Impact of each component of the proposed CLIP-SVD on the 4-shot accuracy (%) of natural and biomedical domain benchmarks WQ WK WV WO WM LP Natural Biomedical 65.36 76.69 75.45 77.78 77.15 77.83 77.88 78.12 78.12 78.11 78.18 42.38 65.27 62.27 66.40 65.35 67.50 66.74 66.99 67.09 67.40 68.02 Effect of Tuning Different Layers: The left panel of Fig. 2 shows how freezing different sets of Transformer layers that are matched in both text and image encoders during SVD-based adaptation affects few-shot accuracy in natural and biomedical domains. In the natural domain, accuracy stays relatively stable, dropping only slightly from 78.18% (all layers adapted) to 77.35% (first four layers frozen) and 75.36% (first and last four frozen), suggesting robust, distributed representations. In contrast, the biomedical domain is more sensitive: accuracy drops from 68.02% (all layers adapted) 8 Figure 2: 4-shot performance by freezing certain layers during finetuning (left) and by adapting text encoder and/or image encoder (right) for natural and biomedical domains. Table 8: Top 3 descriptions returned by TextSpan [18] applied to the attention head with the greatest adaptation-related change for different datasets. EuroSAT (L10.H0) BUSI (L11.H8) DTD (L8.H6) Aerial view of an agricultural field Image taken in the Namibian desert Picture taken in the Brazilian rainforest low-contrast region in clustered pattern double-density sign suggesting benignity solid-cystic component suggesting malignancy BTMRI (L8.H9) An area with decreased perfusion in the left hemisphere bright spot artifact in clustered pattern contrast-enhanced region on axial view UCF101 (L10.H5) Dynamic action Energetic children Playful winking facial expression SUN397 (L11.H0) Mysterious day scene Urban rooftop panorama zoomed out photo CTKIDNEY (L8.H6) An anatomical displacement spiculated margin zone of tissue infiltration Collage of textures Close-up of textured bark Mesmerizing kinetic sculpture COVID-QU-Ex (L11.H0) collapsed lung lobe low signal-to-noise ratio in the upper lobe lesion crossing compartments RETINA (L8.H4) An area with decreased perfusion vascular displacement vascular structure with sharp borders to 66.68% (last four frozen), 64.38% (first and last four), and 62.87% (top eight frozen), indicating that deeper layers are more critical for capturing domain-specific complexity."
        },
        {
            "title": "4.6 Natural Language-based Interpretation of CLIP-SVD",
            "content": "Natural language offers powerful but under-explored lens into the conceptual space of VLMs. To gain insights into the impact of CLIP-SVD, we investigate how textual descriptions align with CLIPs internal representations. Besides the natural domain, we present the first systematic, text-based analysis of biomedical VLM at the attention head level, focusing on how fine-tuning, such as in BiomedCLIP, reshapes semantic VLMs representations. To support this, similar to [18], we constructed new biomedical caption corpus of 300 clinically relevant text elements using GPT4 [1], describing features like contrast, shape, and texture. This enables interpretable alignment between vision and language representations and domain-targeted probing of attention heads. Our framework combines CLIP-SVD with TextSpan [18] to quantify semantic shifts in Output-Value circuits of ViT backbones, focusing on the last four layers [18]. By ranking the attention heads via singular value shift magnitude from CLIP-SVD and extracting aligned text spans, we gain an intuitive interpretation for the importance of different attention heads during model adaptation and their roles in the finetuned tasks (see Tables 1 and 8). These analyses highlight how finetuning steers VLMs toward task-relevant, domain-specific understanding. Additionally, the insights could allow debugging and further refinement (e.g., with prompt engineering) of task/domain-specific CLIP models."
        },
        {
            "title": "5 Limitations",
            "content": "We adopted natural language-based technique to understand the impact and insights of CLIP-SVD in model adaptation. For the related analyses in the biomedical domain, we constructed new corpus of biomedical image descriptions by leveraging the large language models. Although it is shown to 9 facilitate the interpretation, further validation is still required in broader applications, particularly with domain experts. We will investigate this in the near future."
        },
        {
            "title": "6 Conclusion",
            "content": "In conclusion, we introduced CLIP-SVD, novel parameter-efficient adaptation method for CLIP models that finetunes only the singular values of the weight matrices while preserving its pre-trained structure. Our approach enables effective few-shot learning with minimal computational overhead, achieving state-of-the-art results across both natural and biomedical domains. Through extensive ablation studies, we demonstrated the critical role of singular value adaptation in enhancing taskspecific feature extraction. Further analyses of the Output-Value circuit with natural-language-based approach revealed that adapting singular values steers attention heads toward more specialized roles, thus opening doors for further investigation of CLIPs characteristics in broader applications."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) and the Fonds de recherche du Québec Nature et technologies (B2X-363874)."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 9, 19 [2] Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 73197328, 2021. 3 [3] Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy. Dataset of breast ultrasound images. Data in brief, 28:104863, 2020. 6, 16, 18 [4] Yassir Bendou, Amine Ouasfi, Vincent Gripon, and Adnane Boukhayma. Proker: kernel perspective on few-shot adaptation of large vision-language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2509225102, 2025. 7 [5] Yequan Bie, Luyang Luo, Zhixuan Chen, and Hao Chen. Xcoop: Explainable prompt learning for computeraided diagnosis via concept-guided context optimization. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 773783. Springer, 2024. 2, [6] Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel Castro, Anton Schwaighofer, Stephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, et al. Making the most of text semantics to improve biomedical visionlanguage processing. In European conference on computer vision, pages 121. Springer, 2022. 3 [7] Andrew A. Borkowski, Marilyn M. Bui, L. Brannon Thomas, Catherine P. Wilson, Lauren A. DeLand, and Stephen M. Mastorides. Lung and colon cancer histopathological image dataset (lc25000), 2019. 6, 16, 18 [8] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101mining discriminative components with random forests. In ECCV, pages 446461. Springer, 2014. 6, 16 [9] Qinglong Cao, Zhengqin Xu, Yuntian Chen, Chao Ma, and Xiaokang Yang. Domain-controlled prompt learning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 936944, 2024. 3 [10] Pingjun Chen. Knee osteoarthritis severity grading dataset, 2018. 6, 16, [11] Peijie Chen, Qi Li, Saad Biaz, Trung Bui, and Anh Nguyen. gscorecam: What objects is clip looking at? In Proceedings of the Asian Conference on Computer Vision, pages 19591975, 2022. 21 [12] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. Advances in Neural Information Processing Systems, 35:1666416678, 2022. 3 [13] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, pages 36063613, 2014. 6, 16 [14] Noel Codella, Veronica Rotemberg, Philipp Tschandl, Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, et al. Skin lesion analysis toward melanoma detection 2018: challenge hosted by the international skin imaging collaboration (isic). arXiv preprint arXiv:1902.03368, 2019. [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, pages 248255. Ieee, 2009. 6, 16, 18 [16] Sedigheh Eslami, Gerard de Melo, and Christoph Meinel. Does clip benefit visual question answering in the medical domain as much as it does in the general domain?, 2021. 3 [17] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR Workshop, pages 178178. IEEE, 2004. 6, 16 [18] Yossi Gandelsman, Alexei Efros, and Jacob Steinhardt. Interpreting clips image representation via text-based decomposition. arXiv preprint arXiv:2310.05916, 2023. 2, 5, 9, 18, 19, 21, 22, 23, 24, 25 [19] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision, 132(2):581595, 2024. 1, 3, 7, 19 [20] Matthieu Guillaumin, Daniel Küttel, and Vittorio Ferrari. Imagenet auto-annotation with segmentation propagation. International Journal of Computer Vision, 110(3):328348, 2014. 21 [21] Demi Guo, Alexander Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 48844896, 2021. 3 [22] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. J-STARS, 12(7):22172226, 2019. 6, 16 [23] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: critical analysis of out-of-distribution generalization. In ICCV, pages 83408349, 2021. 16 [24] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In CVPR, pages 1526215271, 2021. 16 [25] Connor Holmes, Minjia Zhang, Yuxiong He, and Bo Wu. Nxmtransformer: semi-structured sparsification for natural language understanding via admm. Advances in neural information processing systems, 34: 18181830, 2021. 3 [26] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 27902799. PMLR, 2019. [27] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021. 3 [28] Yunshi Huang, Fereshteh Shakeri, Jose Dolz, Malik Boudiaf, Houda Bahig, and Ismail Ben Ayed. Lp++: surprisingly strong linear probe for few-shot clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2377323782, 2024. 3, 7 [29] Md Nazmul Islam, Mehedi Hasan, Md Kabir Hossain, Md Golam Rabiul Alam, Md Zia Uddin, and Ahmet Soylu. Vision transformer and explainable transfer learning models for auto detection of kidney cyst, stone and tumor from ct-radiography. Scientific Reports, 12(1):114, 2022. 6, 16, 18 [30] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 49044916. PMLR, 2021. 3 [31] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709727. Springer, 2022. 3 [32] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. In Advances in Neural Information Processing Systems, pages 10221035. Curran Associates, Inc., 2021. [33] Jakob Nikolas Kather, Cleo-Aron Weis, Francesco Bianconi, Susanne Melchers, Lothar Schad, Timo Gaiser, Alexander Marx, and Frank Gerrit Zöllner. Multi-class texture analysis in colorectal cancer histology. Scientific reports, 6(1):111, 2016. 6, 16, 18 [34] Daniel S. Kermany, Michael Goldbaum, et al. Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell, 172(5):1122 1131.e9, 2018. 6, 16, 18 [35] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1911319122, 2023. 1, 2, 3, 7, 19, 32, 33 12 [36] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1519015200, 2023. 1, [37] Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, and Yiming Xiao. Medclip-sam: Bridging text and image towards universal medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 643653. Springer, 2024. 3 [38] Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, and Yiming Xiao. Medclip-samv2: Towards universal text-driven medical image segmentation. Medical Image Analysis, page 103749, 2025. 3 [39] Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, and Yiming Xiao. Biomedcoop: Learning to prompt for biomedical vision-language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1476614776, 2025. 2, 3, 6, 7, 16, 33 [40] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV, pages 554561, 2013. 6, 16 [41] Thomas Köhler, Attila Budai, Martin Kraus, Jan Odstrcilik, Georg Michelson, and Joachim Hornegger. Automatic no-reference quality assessment for retinal fundus images using vessel segmentation, 2013. 6, 16, [42] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. 3 [43] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension of objective landscapes. In International Conference on Learning Representations, 2018. 3 [44] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582 4597, 2021. [45] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up: guide to parameterefficient fine-tuning. arXiv preprint arXiv:2303.15647, 2023. 2 [46] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: new baseline for efficient model tuning. Advances in Neural Information Processing Systems, 35:109123, 2022. 3 [47] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [48] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. 6, 16 [49] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. Advances in Neural Information Processing Systems, 37:121038 121072, 2024. 1, 3 [50] Msoud Nickparvar. Brain tumor mri dataset, 2021. 6, 16, 18 [51] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over large number of classes. In ICVGIP, pages 722729. IEEE, 2008. 6, 16 [52] Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR, pages 34983505. IEEE, 2012. 6, 16 [53] Zelin Peng, Zhengqin Xu, Zhilin Zeng, Xiaokang Yang, and Wei Shen. Sam-parser: Fine-tuning sam In Proceedings of the AAAI Conference on Artificial efficiently by parameter space reconstruction. Intelligence, pages 45154523, 2024. 3 [54] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 487503, 2021. 13 [55] Konstantin Pogorelov, Kristin Ranheim Randel, Carsten Griwodz, Sigrun Losada Eskeland, Thomas de Lange, Dag Johansen, Concetto Spampinato, Duc-Tien Dang-Nguyen, Mathias Lux, Peter Thelin Schmidt, Michael Riegler, and Pål Halvorsen. Kvasir: multi-class image dataset for computer aided gastrointestinal disease detection. In Proceedings of the 8th ACM on Multimedia Systems Conference, pages 164169, New York, NY, USA, 2017. ACM. 6, 16, 18 [56] Prasanna Porwal, Samiksha Pachade, Ravi Kamble, Manesh Kokare, Girish Deshmukh, Vivek Sahasrabuddhe, and Fabrice Meriaudeau. Indian diabetic retinopathy image dataset (idrid), 2018. 6, 16, 18 [57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 3, 7, 19, 32, 33 [58] Hamza Rasaee, Taha Koleilat, and Hassan Rivaz. Groundingdino-us-sam: Text-prompted multi-organ segmentation in ultrasound with lora-tuned vision-language models. arXiv preprint arXiv:2506.23903, 2025. [59] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in neural information processing systems, 30, 2017. 3 [60] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In ICML, pages 53895400. PMLR, 2019. 16 [61] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 6, [62] Pascal Spiegler, Taha Koleilat, Arash Harirpoush, Corey Miller, Hassan Rivaz, Marta Kersten-Oertel, and Yiming Xiao. Textsam-eus: Text prompt learning for sam to accurately segment pancreatic tumor in endoscopic ultrasound. arXiv preprint arXiv:2507.18082, 2025. 3 [63] Yanpeng Sun, Qiang Chen, Xiangyu He, Jian Wang, Haocheng Feng, Junyu Han, Errui Ding, Jian Cheng, Zechao Li, and Jingdong Wang. Singular value fine-tuning: Few-shot segmentation requires few-parameters fine-tuning. Advances in neural information processing systems, 35:3748437496, 2022. 1, 2, 3 [64] Anas M. Tahir, Muhammad E.H. Chowdhury, Amith Khandakar, Tawsifur Rahman, Yazan Qiblawey, Uzair Khurshid, Serkan Kiranyaz, Nabil Ibtehaz, M. Sohel Rahman, Somaya Al-Maadeed, Sakib Mahmud, Maymouna Ezeddin, Khaled Hameed, and Tahir Hamid. Covid-19 infection localization and severity grading from chest x-ray images. Computers in Biology and Medicine, 139:105002, 2021. 6, 16, 18 [65] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific data, page 180161, 2018. 16 [66] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):25792605, 2008. [67] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric Xing. Learning robust global representations by penalizing local predictive power. In NeurIPS, 2019. 16 [68] Zhengbo Wang, Jian Liang, Lijun Sheng, Ran He, Zilei Wang, and Tieniu Tan. hard-to-beat baseline for training-free clip-based adaptation. arXiv preprint arXiv:2402.04087, 2024. 7 [69] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Largescale scene recognition from abbey to zoo. In CVPR, pages 34853492. IEEE, 2010. 6, [70] Yan Xu, Rixiang Quan, Weiting Xu, Yi Huang, Xiaolong Chen, and Fengyuan Liu. Advances in medical image segmentation: comprehensive review of traditional, deep learning and hybrid approaches. Bioengineering, 11(10):1034, 2024. 3 [71] Hantao Yao, Rui Zhang, and Changsheng Xu. Visual-language prompt tuning with knowledge-guided context optimization, 2023. 7, 19, 32, 33 [72] Hantao Yao, Rui Zhang, and Changsheng Xu. Visual-language prompt tuning with knowledge-guided In Proceedings of the IEEE/CVF conference on computer vision and pattern context optimization. recognition, pages 67576767, 2023. 7 [73] Hantao Yao, Rui Zhang, and Changsheng Xu. Tcp: Textual-based class-aware prompt tuning for visuallanguage model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2343823448, 2024. 14 [74] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 19, 2022. 2 [75] Maxime Zanella and Ismail Ben Ayed. Low-rank few-shot adaptation of vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15931603, 2024. 7, 16 [76] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512, 2023. 7 [77] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint arXiv:2111.03930, 2021. 3, 7, 19 [78] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. [79] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong, Andrea Tupini, Yu Wang, Matt Mazzola, Swadheen Shukla, Lars Liden, Jianfeng Gao, Matthew P. Lungren, Tristan Naumann, Sheng Wang, and Hoifung Poon. Biomedclip: multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs, 2024. 3, 7 [80] Zihao Zhao, Yuxiao Liu, Han Wu, Mei Wang, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Zhiming Cui, Qian Wang, et al. Clip in medical imaging: comprehensive survey. arXiv preprint arXiv:2312.07353, 2023. 3 [81] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In CVPR, pages 1681616825, 2022. 1, 3, 6, 7, 19, 32, 33 [82] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. IJCV, 130(9):23372348, 2022. 1, 2, 3, 6, 7, 16, 19, 32, [83] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-aligned gradient for prompt tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15659 15669, 2023. 7 [84] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-aligned gradient for prompt tuning, 2024. 7, 19, 32, 33 [85] Didi Zhu, Zhongyi Sun, Zexi Li, Tao Shen, Ke Yan, Shouhong Ding, Kun Kuang, and Chao Wu. Model tailor: Mitigating catastrophic forgetting in multi-modal large language models. arXiv preprint arXiv:2402.12048, 2024. 3 15 Singular Value Few-shot Adaptation of Vision-Language Models"
        },
        {
            "title": "A Dataset Details",
            "content": "Following CoOp [82] and BiomedCoOp [39], we conducted extensive experiments on 11 natural and 10 biomedical classification benchmark datasets to evaluate the effectiveness of the proposed CLIPSVD. The natural datasets include ImageNet [15], Caltech101 [17], OxfordPets [52], StanfordCars [40], Flowers102 [51], Food101 [8], FGVCAircraft [48], SUN397 [69], DTD [13], EuroSAT [22], and UCF101 [61]. The biomedical datasets consist of CTKidney [29], DermaMNIST [65, 14], Kvasir [55], RETINA [56, 41], LC25000 [7], CHMNIST [33], BTMRI [50], OCTMNIST [34], BUSI [3], COVID-QU-Ex [64], and KneeXray [10]. For distribution shift experiments, we also included ImageNetV2 [60], ImageNet-Sketch [67], ImageNet-A [24], and ImageNet-R [23]. Dataset statistics are provided in Tables S3 and S4."
        },
        {
            "title": "B Detailed Hyperparameters",
            "content": "The hyperparameters reported in Tables S1 and S2 across both natural and biomedical datasets were carefully tuned based on benchmark type and dataset characteristics. consistent batch size (BS) of 32 was maintained throughout to ensure uniformity in training dynamics. For natural datasets, the learning rate (LR) was set to 5 104 for few-shot settings and reduced to 6 104 for base-to-novel evaluations. For the natural few-shot setting, we follow [75] and set the number of iterations to 200 (number of shots), whereas we use epochs for the base-to-novel setting, with the number of training epochs (EP) varying between 2 and 20 depending on the datasets size. In biomedical datasets, LR values were more diverse, ranging from 0.5 103 to 20 103, reflecting the varying difficulty and modality of the medical tasks, while EP was generally higher (up to 200) to accommodate the typically smaller dataset sizes and slower convergence. Notably, BUSI [3] lacked base-to-novel evaluation due to the absence of appropriate class splits."
        },
        {
            "title": "C Domain Generalization",
            "content": "We evaluate the robustness of our method on out-of-distribution datasets. Similar to cross-dataset evaluation, we test our ImageNet-trained model directly on four other ImageNet datasets with different types of distribution shifts, including ImageNetV2 [60], ImageNet-Sketch (ImageNetS) [67], ImageNet-A [24], and ImageNet-R [23]. Table S5 presents the domain generalization results, where methods are trained on ImageNet and evaluated on datasets with various domain shifts (-V2, -S, -A, and -R). CLIP-SVD achieves the highest average accuracy of 62.55%, outperforming MaPLe (62.36%) and other methods. It also leads in two target domains, namely ImageNet-S (-S) (+0.47%) and ImageNet-V2 (-V2) (+0.28%), demonstrating its robustness to domain shifts. These results highlight the effectiveness of CLIP-SVD in generalizing across domains. Effect of Rank-based Singular Value Selection for CLIP-SVD For CLIP-SVD, selective finetuning of the singular values could further reduce the computational requirement. With singular value decomposition, the singular values are naturally ranked in descending order, with respect to their magnitude. In this experiment, we varied the number of non-zero singular values out of the full rank to be adapted according to their ordering of magnitude, in order to investigate their impact on 4-shot model performance across both natural and biomedical domains. Specifically, we tested two configurations: one where only the top singular values (in the descending order) are finetuned and the other where only the bottom singular values (in the ascending order) are adjusted. The results shown in Fig. S1 reveal that, in the natural domain, both top and bottom configurations exhibit steep initial accuracy increase starting from small proportion of adjustable singular values, stabilizing near 78.18% as the ratio approaches full-rank, suggesting limited sensitivity to the ranking of the selected singular values to be adapted. In contrast, the biomedical domain shows more pronounced dependency on ranking of the adjustable singular 16 Dataset ImageNet Caltech101 DTD EuroSAT StanfordCars Flowers102 FGVCAircraft SUN397 OxfordPets UCF101 Food101 Benchmark BS LR (104) EP/IT Dataset Benchmark BS LR (103) EP Few-shot Base-to-Novel 32 32 Cross-dataset Transfer 32 Domain Generalization 32 Few-shot Base-to-Novel Few-shot Base-to-Novel Few-shot Base-to-Novel Few-shot Base-to-Novel Few-shot Base-to-Novel Few-shot Base-to-Novel Few-shot Base-to-Novel Few-shot Base-to-Novel Few-shot Base-to-Novel Few-shot Base-to-Novel 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 5 6 5 5 5 6 5 6 5 6 5 6 5 5 6 5 6 5 6 5 6 5 6 200 10 2 200 20 200 20 200 20 200 10 200 10 200 200 10 200 10 200 10 200 10 BTMRI BUSI COVID-QU-Ex CTKIDNEY Kvasir CHMNIST LC25000 RETINA KneeXray OCTMNIST Few-shot 32 Base-to-Novel 32 Few-shot Base-to-Novel 32 - Few-shot 32 Base-to-Novel 32 Few-shot 32 Base-to-Novel 32 Few-shot 32 Base-to-Novel Few-shot 32 Base-to-Novel 32 Few-shot 32 Base-to-Novel 32 Few-shot 32 Base-to-Novel Few-shot 32 Base-to-Novel 32 Few-shot 32 Base-to-Novel 32 7 9 2 - 0.5 2 1 10 5 3 1 7 3 10 7 8 2 10 20 100 100 100 - 100 50 200 100 60 60 100 100 100 100 60 100 60 100 Table S1: Hyperparameter values across natural datasets and benchmarks. (BS = Batch Size, LR = Learning Rate, EP = Epochs, IT = Iterations) Table S2: Hyperparameter values across biomedical datasets and benchmarks. (BS = Batch Size, LR = Learning Rate, EP = Epochs) Table S3: Summary of natural datasets: Overview of the datasets used in the natural domain, including the number of classes, dataset splits (train, validation, test), and the corresponding handcrafted text prompts used for classification. Test Dataset Hand-crafted prompt Classes Train Val ImageNet Caltech101 OxfordPets StanfordCars Flowers Food101 FGVCAircraft SUN397 DTD EuroSAT UCF101 ImageNetV2 ImageNet-Sketch ImageNet-A ImageNet-R 1,000 100 37 196 102 101 100 397 47 10 101 1,000 1,000 1,000 1,000 1.28M N/A 50,000 2,465 1,649 4,128 3,669 736 2,944 8,041 1,635 6,509 2,463 1,633 4,093 50,500 20,200 30,300 3,334 3,333 15,880 3,970 19,850 1,692 2,820 1,128 8,100 13,500 5,400 3,783 1,898 7,639 photo of [CLASS]. photo of [CLASS]. photo of [CLASS], type of pet. photo of [CLASS]. photo of [CLASS], type of flower. photo of [CLASS], type of food. 3,333 photo of [CLASS], type of aircraft. photo of [CLASS]. [CLASS] texture. centered satellite photo of [CLASS]. photo of person doing [CLASS]. N/A N/A N/A N/A N/A 10,000 N/A 50,889 N/A 50,889 N/A 50,889 photo of [CLASS]. photo of [CLASS]. photo of [CLASS]. photo of [CLASS]. values. Here, finetuning the top singular values consistently outperforms the bottom configuration, reaching 68.02% at full rank ratio, while the bottom approach lags behind by approximately 5% at the 0.5 ratio, highlighting the importance of prioritizing top singular values for specialized medical contexts. 17 Table S4: Summary of biomedical datasets: Overview of the datasets used in the biomedical domain, including the number of classes, dataset splits (train, validation, test), and the corresponding hand-crafted text prompts used for classification."
        },
        {
            "title": "Test",
            "content": "Hand-crafted prompt CTKIDNEY Kvasir RETINA LC25000 CHMNIST BTMRI OCTMNIST BUSI COVID-Qu-Ex KneeXray 4 8 4 5 8 4 4 3 4 5 6,221 2,000 2,108 12,500 2,496 2,854 97,477 389 10,582 5,778 2,487 800 841 5,000 1,000 1,141 10,832 155 4,232 826 3,738 1,200 1,268 7,500 1,504 1,717 1,000 236 6,351 1, photo of [CLASS]. photo of [CLASS]. photo of [CLASS]. photo of [CLASS]. photo of [CLASS]. photo of [CLASS]. photo of [CLASS]. photo of [CLASS]. chest xray of [CLASS]. photo of [CLASS]. Table S5: Domain generalization: Methods are trained on ImageNet using 16-shots and evaluated on datasets with domain shifts (ImageNet-V2, -S, -A, and -R) for classification accuracy (%). Source Target ImageNet -V -S 66.73 71.51 71.02 68.46 53.81 51.71 70.84 71.20 72.24 70.72 72.15 60.83 64.20 64.07 59.55 45.69 43.07 62.15 64.10 64.73 64.07 65.03 46.15 47.99 48.75 39.88 29.21 27.13 43.76 48.97 47.61 49. 49.62 -A 47.77 49.71 50.63 38.83 36.04 27.04 43.91 50.69 49.39 50.90 49.17 -R 73.96 75.21 76.18 64.62 55.26 45.07 71.59 76.70 74.58 76. 76.79 Avg. 59.09 61.72 62.13 54.27 44.00 38.80 58.45 62.33 61.71 62.36 62.55 CLIP CoOp Co-CoOp CLIP-Adapter TIP-Adapter TIP-Adapter-F TaskRes KgCoOp ProGrad MaPLe CLIP-SVD"
        },
        {
            "title": "E Experiments with Other Backbones",
            "content": "In this experiment, we evaluated our proposed method, CLIP-SVD, using the alternative ViT-B/32 backbone in few-shot learning setting for the natural domain. Table S6 presents the average classification accuracy (%) across 11 natural domain benchmarks derived from three randomly sampled support sets for each dataset. Our method consistently outperforms the state-of-the-art techniques across different few-shot settings. Notably, CLIP-SVD achieves the highest performance in all cases, surpassing the second-best method, Tip-Adapter-F, by significant margin, with an improvement of approximately 1.2% at = 1 and 1.4% at = 16. These results highlight the strength of our singular value decomposition-based adaptation strategy in enhancing generalization under limited data conditions. Furthermore, the effectiveness of CLIP-SVD with the ViT-B/32 backbone demonstrates that our approach is adaptable to different model backbones, making it broadly applicable across various vision-language models."
        },
        {
            "title": "F TextSpan Analysis Details",
            "content": "We analyze singular value shifts in the Output-Value (OV) circuit using TextSpan [18], focusing on the last four layers (L8L11) of CLIP and BiomedCLIP ViT-B/16 (Note: layer and head indexing in these experiments begins at 0.). The ViT-B/16 vision encoder employs 12 attention heads per MHSA block. For the natural image domain, we use ImageNet [15], while for the biomedical domain, we aggregate all relevant datasets [29, 55, 56, 41, 7, 33, 50, 34, 3, 64, 10] into single comprehensive dataset. Images from both domains are fed into their corresponding vision 18 Table S6: Evaluation against state-of-the-art techniques for natural domain with ViT-B/32 backbone: This table presents the average classification accuracy (%) obtained from 11 natural domain benchmarks derived from 3 sampled support sets for each dataset. The top-performing results are in bold, and the second-best are underlined."
        },
        {
            "title": "Method",
            "content": "K = 1 = 2 = 4 = 8 = 16 Zero-shot CLIP [57] CoOp [82] CoCoOp [81] ProGrad [84] KgCoOp [71] MaPLe [35] CLIP-Adapter [19] Tip-Adapter-F [77] CLIP-SVD (Ours) 62.8 63.1 64.7 64.9 61.5 62.5 66.5 67.7 65.3 64.8 67.1 66.6 65.2 63.5 69.2 71.0 61.8 68.6 66.7 69.8 68.4 68.7 64.3 71. 73.4 72.2 68.1 73.2 70.5 71.6 67.6 74.1 75.8 74.7 70.7 75.9 72.1 74.1 71.6 77.1 78.5 Table S7: Natural Domain Efficiency comparison of different parameter-efficient tuning methods. We report trainable parameter counts, training, and inference time. Method Trainable Training Time Params (min) Inference Time (s) CoOp CoCoOp MaPLe CLIP-Adapter Tip-Adapter TCP CLIP-LoRA CLIP-SVD (Ours) 2.0K 35.4K 3.5M 131.1K 0 331.9K 184.3K 92.2K 1.84 2.25 2.95 4.93 1.02 9.82 0.88 7.34 18.52 7.40 7.48 24.97 7.58 7.41 7.13 Table S8: Biomedical Domain Efficiency comparison of different parameter-efficient tuning methods. We report trainable parameter counts, training, and inference time. Method Trainable Training Time Params (min) Inference Time (s) CoOp CoCoOp MaPLe CLIP-Adapter Tip-Adapter DCPL BiomedCoOp CLIP-LoRA CLIP-SVD (Ours) 2.0K 44.8K 5.3M 131.1K 0 5.5M 3.1K 221.2K 110.6K 1.19 0.58 2.4 2.85 2.58 1.76 10.25 1.78 7.20 23.6 19.8 7.79 23.52 177.6 7.20 7.58 6.76 encoder network. For the natural domain, we utilize the text corpus from [18], which consists of approximately 3,000 GPT-3.5-generated general image descriptions. In contrast, no large-scale text corpus exists for biomedical images. To address this, we generated new corpus comprising 300 general medical image descriptions and relevant terminologies using GPT-4 [1]. We use the following prompt to query GPT-4: Generate 300 distinct descriptive prompts for medical images that capture specific visual features commonly found in medical scans, such as contrast, shape, color, location, and texture. Using TextSpan, we extract the top three text descriptions from the corpus that best characterize the role of each attention head in each of the final four layers. Based on these top-3 descriptions, GPT-4 is used to assign concise and descriptive title to each heads Assign concise and descriptive title that best captures function: the primary function represented by these descriptions.. The identified roles and associated top-3 descriptions for each head in layers L8L11 for the natural and biomedical domains are detailed in Tables S10 and S11 respectively. t-SNE Visualization We used t-SNE [66] to visualize the image embeddings generated by the fine-tuned model using our CLIP-SVD and compared them with the embeddings from the pre-trained CLIP and BiomedCLIP models. As shown in Figure S3, the embeddings produced by CLIP-SVD exhibit significantly better clustering, indicating that our method enables the model to generate more distinct and well-separated feature representations. This experiment was conducted on the EuroSAT, OxfordPets, CTKidney, and 19 Figure S1: Accuracy comparison for models finetuned with varying rank ratios in the natural (left) and biomedical (right) domains Figure S2: Saliency map comparison between the pre-trained CLIP model and the CLIP-SVD finetuned model. The adapted model shows more focused and semantically aligned attention, highlighting improved localization of relevant image regions. Figure S3: t-SNE visualization of image embeddings: Comparison of embeddings produced by the pre-trained CLIP/BiomedCLIP model and the fine-tuned CLIP-SVD on (a) EuroSAT, (b) Oxford Pets, (c) CTKidney, and (d) LC25000. CLIP-SVD generates more compact and well-separated clusters, indicating improved feature extraction and task-specific alignment. 20 LC25000 datasets, where the improved clustering patterns highlight the enhanced feature extraction capabilities of our approach."
        },
        {
            "title": "H Segmentation Performance",
            "content": "+0.54 +0.20 +0.31 Model 57.73 58. 82.62 82.82 77.24 77.55 Pixel Acc. mIoU mAP CLIP CLIP-SVD (Ours) Table S9: Segmentation Performance in terms of pixel accuracy, mean IoU, and mean average precision (%). We evaluated the segmentation performance of our finetuned model using the TextSpan algorithm [18] on the ImageNet-Segmentation dataset [20], curated subset of 4,276 ImageNet validation images with pixel-level annotations, and compared the results against those of the original pre-trained CLIP model. To perform segmentation, we follow the approach introduced in the TextSpan framework [18], which builds on fine-grained decomposition of CLIPs image representation. Specifically, we use the fact that the CLIP image encoders output can be expressed as sum over attention head contributions across spatial positions. Each image patch contributes vector in the shared image-text embedding space. Given text prompt corresponding to the object class (i.e. an image of [CLASS]\"), we compute heatmap by measuring the similarity between each patchs contribution and the CLIP embedding of the text description. These similarity scores are then aggregated into spatial map that highlights the regions most semantically aligned with the prompt. We also used gScoreCAM [11] to visualize saliency maps for both the pre-trained CLIP model and the version fine-tuned with CLIP-SVD. Table S9 shows the results in terms of pixel accuracy, mean intersection over union (mIoU), and mean average precision (mAP). Our method achieves performance boost across all metrics, with gain of +0.31% in pixel accuracy, +0.54% in mIoU, and +0.20% in mAP. These improvements demonstrate that CLIP-SVD enhances the models ability to localize and segment fine-grained regions in complex natural images, leading to more accurate and consistent segmentation results. This boost in segmentation performance stems from improved feature extraction and attention alignment through singular value adaptation as indicated in Section 4.6. Fine-tuning the singular values helps the models attention heads capture task-relevant features, such as geographic cues in EuroSAT and color-related patterns in Oxford Pets, as shown in Figure S2. This enables better foreground-background differentiation and more precise segmentation boundaries, explaining the observed performance gains."
        },
        {
            "title": "I Computational Cost",
            "content": "We present detailed results in Tables S7 and S8, comparing various PEFT methods in terms of total trainable parameters, training time, and inference time on the DTD and RETINA datasets, respectively. This analysis highlights the performanceefficiency trade-off across methods. All experiments were conducted on single NVIDIA A100 GPU with 40GB of RAM, using batch size of 8 at inference for consistency and fairness. The results demonstrate that CLIP-SVD strikes favorable balance: it requires relatively few trainable parameters, converges more quickly, and introduces no inference overhead, while achieving high accuracy in few-shot settings. Moreover, CLIP-SVD avoids reliance on external modules or architectural modifications, simplifying deployment and improving efficiency, particularly in low-resource or latency-sensitive scenarios. Additional Per-dataset Results We provide the complete per-dataset few-shot results for both the natural and biomedical domains to offer detailed evaluation of our methods performance. The natural and biomedical domain results are summarized in Tables S12 and S13, respectively, which report the classification accuracy across different few-shot settings for each individual dataset. On the other hand, we offer the per-dataset base-to-novel generalization results in Table S14 and S15. This detailed breakdown highlights the consistent improvement achieved by CLIP-SVD over state-of-the-art methods, demonstrating its ability to generalize effectively across diverse datasets and domains. 21 Table S10: Natural Domain Analysis of the attention heads of each layer with the top 3 descriptions returned by TextSpan [18]. Here, L\" denotes layer while H\" denotes attention head. L8.H0: Foundational Shapes & Silhouettes L8.H1: Motion & Directional Cues Rural windmill silhouette plank Intricate clock mechanism Dynamic sports action shot Tranquil boating on lake Lively city parade L8.H2: Contrast & Light-Dark Regions L8.H3: Geometric Patterns & Grids Photograph with the artistic style of fisheye lens Psychedelic color swirls Sunlit meadow path Artwork featuring Morse code typography Cubist still life painting Miniature diorama photography L8.H4: Object Boundaries & Contours L8.H5: Repeated Structures & Textures Cinematic portrait with dramatic lighting whirlpool Image with harlequin patterns Plaid pattern Reflective surfaces Image of scooter L8.H6: Refined Textural Details of Everyday Objects L8.H7: Positional Layout & Spatial Balance Collage of textures Close-up of textured bark Mesmerizing kinetic sculpture Cozy cabin interior Minimalist white backdrop Rolling vineyard landscapes L8.H8: Edge Detection & Detail Refinement L8.H9: Primitive Forms & Visual Anchors scalene quadrilateral Minimalist architectural photography Detailed charcoal sketch Dappled sunlight cloverleaf Ocean sunset silhouette L8.H10: Coarse-to-Fine Visual Transitions L8.H11: Structural Symmetry & Alignment Intricate wood carving Contemplative monochrome portrait Aerial view of marketplace Birds-eye view Detailed illustration of futuristic brain-computer interface Tranquil garden pathway L9.H0: Scene Composition & Perspective L9.H1: Depth Perception & Visual Planes Aerial view of marketplace Glowing neon cityscape Playful siblings zoomed out photo Classic black and white cityscape Antique historical artifact L9.H2: Interaction Between Objects L9.H3: Material Surfaces & Reflection An image of Gymnast Emotional and heartfelt human connection Detailed illustration of futuristic brain-computer interface Aerial view of teeming rainforest Inviting reading nook Dynamic and high-energy dance competition L9.H4: Natural Landscapes & Textures L9.H5: Architectural Layouts & Forms Aerial view of vineyard Enchanting forest nymph aesthetic Delicate and intricate lace patterns Photograph showcasing laughter Minimalist architectural photography Urban labyrinth L9.H6: Facial Features & Gaze Cues L9.H7: Semantic Grouping of Elements Detailed charcoal sketch Intense water sports moment Photograph with the artistic style of light trails Urban rooftop panorama Cozy cabin interior Photo taken in the Japanese tea gardens L9.H8: Soft Lighting & Shadow Play L9.H9: Organic Flow & Movement Impressionist portrait painting Intriguing and enigmatic passageway Ephemeral soap bubble pattern Photograph with abstract geometric overlay Artwork featuring Morse code typography Abstract oil painting L9.H10: Conceptual Boundaries & Themes L9.H11: Temporal Sequences & Visual Narratives Miniature diorama photography emotional candid moment Artwork featuring Morse code typography Colorful hot air balloons Picture taken in Rwanda Joyful family picnic scene Table S10 (continued): Natural Domain Analysis of the attention heads of each layer with the top 3 descriptions returned by TextSpan [18]. Here, L\" denotes layer while H\" denotes attention head. L10.H1: Human Experiences & Objects in Action L10.H0: Aerial Landscapes & Environments"
        },
        {
            "title": "Aerial view of an agricultural field\nImage taken in the Namibian desert\nPicture taken in the Brazilian rainforest",
            "content": "Hands in an embrace Dynamic and high-energy music concert Emergency Medical Technician at work L10.H2: Cultural & Textural Scenes L10.H3: Colorful Festivities & Patterns"
        },
        {
            "title": "Picture taken in the Spanish Flamenco festivals\nOrnate wood carving\nImage captured in the Peruvian Andes",
            "content": "Lively and colorful parade Tie-dye design Soft pastel tones L10.H4: Cozy Indoors & Historical Locations L10.H5: Action, Emotion & Faces"
        },
        {
            "title": "Dynamic Action\nEnergetic children\nPlayful winking facial expression",
            "content": "L10.H6: Organic Forms & Flow L10.H7: Geographic & Cultural Diversity"
        },
        {
            "title": "Picture taken in Pakistan\nBustling cultural market\nImage snapped in the Swiss chocolate factories",
            "content": "L10.H8: Color & Design Aesthetics L10.H9: Sketches, Illustrations & Concepts"
        },
        {
            "title": "Photograph with a yellow color palette\nPhoto taken in the Brazilian samba parade\nColorful hot air balloons",
            "content": "Collage of vintage magazine clippings Detailed charcoal sketch Impressionist-style digital painting L10.H10: Mood, Atmosphere & Highlights L10.H11: Human-Centered Realism & Objects"
        },
        {
            "title": "Urban labyrinth\nImage of a police car\nIntricate gemstone arrangement",
            "content": "L11.H0: Semantic Layout & Spatial Context L11.H1: Human Relationships & Portraiture"
        },
        {
            "title": "Mysterious day scene\nUrban rooftop panorama\nA zoomed out photo",
            "content": "Images of people and emotional interaction Family portraits and group dynamics Visuals of children, couples, and professionals L11.H2: Numbers, Symbols & Temporal Cues L11.H3: Lifestyle & Tranquil Activities Digits and symbolic representations in images Temporal cues like clocks, sunsets, motion blur Confetti, reflections, and expressive timing Calm urban and rural lifestyle scenes Tranquil locations: diners, libraries, markets Cozy homes, beach sunsets, fairgrounds L11.H4: Visual Symbols & Cultural Motifs L11.H5: Rare Words & Abstract Concepts Cultural symbols and calligraphy art Caricatures, retro TV test patterns, mystical motifs Architectural and folkloric aesthetics Abstract words (quasar, zephyr), letter imagery Rare geographic terms, numerals, and geometry Conceptual elements and visual abstractions L11.H6: Global Geographic Locations L11.H7: Color Themes & Tonal Palettes Images tied to global cities and landmarks Natural and urban landscapes across continents Famous locations: Machu Picchu, Santorini, NYC Dominant color schemes (red, blue, gray) Pastel, sepia tones, artistic overlays Color-coded compositions and political art L11.H8: Weather & Natural Forces L11.H9: Everyday Objects & Refined Details Storms, fog, lightning, and natural disasters Atmospheric effects and visual phenomena Wildlife amidst dynamic weather elements Common items like scarves, bowls, wallets Mechanisms, furniture, modern artifacts Textures: wood grain, bubble, fabric L11.H10: Natural Flora, Landscapes & Tranquility L11.H11: Living Creatures & Natural Forms Floral and plant life in serene settings Natural scenes: cherry blossoms, forests Geographic tranquility and biodiversity Wide range of animals: dogs, elephants, fish Insects, feathers, natural surface textures Shells, fur, fruit, snowflakes 23 Table S11: Biomedical Domain Analysis of the attention heads of each layer with the top 3 descriptions returned by TextSpan [18]. Here, L\" denotes layer while H\" denotes attention head. L8.H1: Peripheral Edges & Enhancement Rings L8.H0: Focal Markers & Shape Cues Target signs and air bronchograms Geographic zones of tissue involvement Microcalcifications and pseudocapsule patterns Ring-enhancing lesions and peripheral edema Pseudocapsules and symmetric deviations Finger-in-glove signs and disruption of growth plates L8.H2: Lobe Collapse & Serpentine Patterns L8.H3: Radiologic Artifacts & Diffuse Shapes Collapsed lobes and reverse halo signs Serpiginous lesions and high signal regions Growth plate disruptions and cortical thinning Comet-tail artifacts and nodular regions Fluid levels suggesting benignity Asymmetries and contrast-enhanced zones L8.H4: Flow Voids & Vascular Boundaries L8.H5: Textural Shifts & Patchy Regions An area with decreased perfusion vascular displacement vascular structure with sharp borders Mosaic attenuation and necrotic centers Patchy regions with blurry margins Air bronchograms and target-like patterns L8.H6: Contour Irregularity & Internal Spread L8.H7: Symmetry Deviation & Dense Regions An anatomical displacement spiculated margin zone of tissue infiltration Mass effect and asymmetry in dense zones Patchy consolidations and star-shaped opacities Air bronchograms and peri-lesional edema L8.H8: Clustered Signals & Midline Shifts L8.H9: Scattered Highlights & Artifactual Spots Midline shifts with hyperintense clustering Target signs and septated fluid collections Cortical thinning and pseudocapsule formations An area with decreased perfusion in the left hemisphere bright spot artifact in clustered pattern contrast-enhanced region on axial view L8.H10: Diffuse Zones & Overlapping Shapes L8.H11: Dense Regions & Local Signal Buildup Donut and target signs in scattered areas Diffuse abnormalities and cross-compartment lesions Hypoand hyperintense signals across lobes Zones of infiltration with surrounding edema Septated collections and mass effects Contrast enhancement and vascular structures L9.H0: Symmetry Shifts & Linear Features L9.H1: Ring-Like Structures & Localized Spread deviation from expected symmetry in the left hemisphere serpiginous lesion in the left hemisphere well-defined border suggesting benignity central necrosis in scattered distribution crescent-shaped fluid collection in scattered distribution deviation from anatomical landmarks consistent with inflammation L9.H2: Irregular Densities & Textured Borders L9.H3: Small Patterned Anomalies beam-hardening artifact collapsed lung lobe without enhancement peripherally enhancing collection with sharp borders tree-in-bud pattern without enhancement cavitary lesion without enhancement high signal-to-noise ratio with sharp borders L9.H4: Defined Edges & Subpleural Zones L9.H5: Streaks, Texture, & Soft Borders subpleural nodule central necrosis in scattered distribution lesion crossing compartments zone of tissue infiltration with surrounding edema tram-track sign in the left hemisphere striated muscle texture L9.H6: Sharp Edges & Tissue Mismatch L9.H7: Thickened Zones & Star-Like Forms An anatomical displacement An irregular border with surrounding edema streak artifact region of tissue thickening star-shaped opacity in the upper lobe contrast-enhanced region L9.H8: Homogeneous Textures & Clustered Lines L9.H9: Contour Disruptions & Shape Markers lesion with fluid level homogeneous texture disruption of normal anatomy cystic lesion suggesting benignity reverse halo sign triangular-shaped defect L9.H10: Circumscribed Lesions & Flow Paths L9.H11: Signal Artifacts & Internal Septations well-circumscribed lesion cluster of microcalcifications suggesting malignancy serpiginous lesion in the left hemisphere comet-tail artifact lesion with internal septations without enhancement motion artifact 24 Table S11 (continued): Biomedical Domain Analysis of the attention heads of each layer with the top 3 descriptions returned by TextSpan [18]. Here, L\" denotes layer while H\" denotes attention head. L10.H0: Low-Contrast Regions & Collapse Patterns L10.H1: Localized Irregularities & Shadowing collapsed lung lobe vascular structure with sharp borders focal area of calcification in the left hemisphere deviation from anatomical landmarks lytic lesion consistent with inflammation finger-in-glove sign in the left hemisphere L10.H2: Contrast Rings & Interface Blur L10.H3: Multi-Compartment Spread & Fine Edges An air bronchogram suggesting malignancy blurring of tissue interfaces peripherally enhancing collection consistent with infection lesion crossing compartments in the left hemisphere miliary pattern vascular structure with sharp borders L10.H4: Fused Regions & Curved Forms L10.H5: Sharp Transitions & Highlighted Foci finger-in-glove sign in the left hemisphere region with cortical thinning with surrounding edema lesion with fluid level suggesting benignity streak artifact targetoid lesion cluster of microcalcifications suggesting malignancy L10.H6: Circular Opacities & Clean Boundaries L10.H7: Textured Rings & Septated Centers circular opacity septated fluid collection clean fluid interface central necrosis lesion with calcified rim reverse halo sign L10.H8: Artifact-Like Patterns & Symmetry Shift L10.H9: Smooth Outlines & Diffuse Flow midline shift in clustered pattern beam-hardening artifact disruption of growth plate suggesting benignity serpiginous lesion in the left hemisphere stenotic segment in the upper lobe lesion with irregular internal architecture L10.H10: Star Patterns & Linear Disruptions L10.H11: Texture Aggregates & Uniform Zones star-shaped opacity in the upper lobe shifting fluid level motion artifact suggesting benignity lytic lesion on axial view honeycomb pattern sunburst pattern suggesting benignity L11.H0: Signal Voids & Midline Shifts L11.H1: Blurred Regions & Artifact Shapes collapsed lung lobe low signal-to-noise ratio in the upper lobe lesion crossing compartments solid-cystic component suggesting malignancy finger-in-glove sign in scattered distribution double-density sign suggesting benignity L11.H2: Peripheral Contrast & Mosaic Attenuation L11.H3: Cross-Lobe Flow & Density Buildup mosaic attenuation geographic area of involvement in the upper lobe peripherally enhancing collection consistent with infection cavitary lesion without enhancement lesion crossing compartments shifting fluid level L11.H4: Sharp Fluid Interfaces & Contrast Pockets L11.H5: Ringed Zones & Pattern Complexity tram-track sign on axial view clean fluid interface central necrosis with sharp borders targetoid lesion fluid-fluid level suggesting malignancy mosaic attenuation L11.H6: Rounded Opacities & Midline Distortions L11.H7: Compact Signal Shifts & Grid-Like Forms circular opacity solid-cystic component suggesting malignancy crescent-shaped fluid collection star-shaped opacity in the upper lobe widened mediastinum spiculated margin L11.H8: Converging Edges & Cluster Markers L11.H9: Scattered Intensity & Symmetry Cues low-contrast region in clustered pattern double-density sign suggesting benignity solid-cystic component suggesting malignancy midline shift disruption of normal anatomy clean fluid interface L11.H10: Rimmed Signals & Flow Distributions L11.H11: Sharp Vascular Lines & Compact Zones peripherally enhancing collection consistent with infection change in signal intensity suggesting malignancy beam-hardening artifact vascular structure with sharp borders cluster of microcalcifications suggesting malignancy homogeneous texture 25 Table S12: Natural per-dataset performance comparison of with various methods in few-shot setting in terms of classification accuracy (%). Dataset Method = 1 = = 4 = 8 = 16 66.72 69.56 69.68 69.86 54.20 70.80 69.38 70.55 70.19 70.21 67.70 71.46 93.31 94.00 94.19 95.06 92.27 94.87 94.44 94.98 94.65 94.93 94.43 95.12 44.09 53.83 57.64 62.09 56.01 62.17 58.57 54.79 58.31 57.72 61.00 64. 48.37 66.80 70.39 76.43 71.90 74.00 68.62 63.83 71.06 70.84 84.50 84.65 70.20 70.01 71.40 61.93 72.10 70.83 70.77 70.23 71.10 70.30 72.36 94.27 94.21 95.20 93.85 95.44 94.10 95.11 94.97 94.92 95.20 95.85 66.40 61.92 67.28 63.71 67.30 64.70 58.92 65.87 62.13 66.50 68.28 73.23 71.57 81.11 77.93 76.32 75.53 68.26 72.37 79.22 87.73 89.74 71.60 70.36 73.10 67.27 72.95 71.51 71.02 71.2 72.68 72.33 73. 94.57 94.50 95.79 95.35 95.94 95.50 95.19 95.03 95.80 96.00 96.17 71.17 65.68 72.05 69.82 71.41 68.63 63.78 69.37 65.92 71.33 72.42 81.87 78.44 87.46 85.45 84.63 83.60 73.82 74.93 84.38 92.33 92.68 ImageNet Caltech101 DTD EuroSAT CLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Linear Probing LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe CLIP-SVD (Ours) CLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Linear Probing LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe CLIP-SVD (Ours) CLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Linear Probing LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe CLIP-SVD (Ours) CLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Linear Probing LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe CLIP-SVD (Ours) 67.93 68.80 67.43 31.30 69.29 65.77 69.51 69.03 64.33 62.67 70. 93.30 93.08 93.56 80.08 92.85 92.37 93.80 94.13 90.96 92.57 93.91 45.20 50.24 53.25 36.03 52.34 49.00 48.51 52.50 52.79 52.13 56.05 61.70 62.46 63.95 56.36 65.02 54.80 55.71 60.83 55.10 71.80 75.06 68.60 68.90 68.60 43.83 69.63 68.17 69.84 69.63 66.12 65.10 70.68 93.67 93.43 94.22 87.00 94.14 92.83 94.92 94.20 93.21 93.97 94.44 47.87 52.44 56.32 46.02 56.05 51.70 52.02 55.73 54.35 55.50 60. 66.07 64.65 70.38 59.56 67.99 61.20 46.24 68.97 66.19 78.30 83.30 26 Table S12 (continued): Natural per-dataset performance comparison of with various methods in few-shot setting in terms of classification accuracy (%). Dataset Method = = 2 = 4 = 8 = 16 StanfordCars Flowers FGVCAircraft SUN397 CLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Linear Probing LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe CLIP-SVD (Ours) CLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Linear Probing LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe CLIP-SVD (Ours) CLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Linear Probing LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe CLIP-SVD (Ours) CLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Linear Probing LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe CLIP-SVD (Ours) 65.63 71.13 70.08 74.59 62.34 74.57 72.73 69.10 71.98 71.75 75.30 77.25 70.81 54.83 89.62 91.50 91.65 92.96 82.56 91.14 90.69 89.98 92.67 93.44 24.81 31.10 30.19 35.15 29.46 34.31 26.73 31.29 32.47 32.93 34.87 39.42 62.60 71.30 68.39 70.29 63.07 73.22 70.13 70.50 71.79 71.17 70.67 73.75 76.47 71.61 78.34 71.21 76.12 76.57 70.19 73.53 78.78 79.47 81.10 56.08 92.15 94.98 95.89 95.25 84.17 94.37 89.53 93.51 95.80 95. 37.20 34.12 40.61 37.18 38.83 33.18 29.57 34.97 37.89 42.00 45.91 73.13 70.12 73.60 69.54 75.14 72.50 70.62 72.50 72.91 73.23 75.16 80.90 74.08 83.09 80.10 80.77 78.89 71.68 74.87 81.46 83.57 84.98 56.50 93.68 96.18 97.36 96.44 87.64 96.10 92.90 94.87 97.00 97.50 42.67 37.64 45.59 42.85 41.46 40.93 37.63 36.27 40.39 48.40 52.49 75.30 71.47 74.99 73.29 76.09 74.50 72.05 73.40 75.00 75.53 76. 68.97 68.03 71.39 48.93 70.41 69.37 68.77 68.13 71.94 71.60 73.15 54.83 86.59 90.21 85.48 90.38 76.12 88.47 79.47 88.62 88.93 90.32 29.60 28.08 32.51 23.78 32.32 26.53 30.87 28.07 30.84 30.90 34.85 69.00 66.46 66.74 52.70 70.09 66.40 69.11 69.53 68.51 67.10 71.93 67.10 66.47 67.88 33.70 66.31 67.10 67.92 67.03 67.11 66.60 70.56 54.83 80.74 86.63 72.50 86.22 71.98 82.20 74.63 83.81 83.30 83. 27.60 27.22 29.58 20.13 29.11 15.03 13.20 26.90 27.97 26.73 31.11 67.10 65.37 64.49 39.83 67.56 64.70 68.19 68.43 64.54 64.77 70.37 27 Table S12 (continued): Natural per-dataset performance comparison of with various methods in few-shot setting in terms of classification accuracy (%). Dataset Method = 1 = 2 = 4 = 8 = 16 OxfordPets UCF101 Food101 Average CLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Linear Probing LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe CLIP-SVD (Ours) CLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Linear Probing LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe CLIP-SVD (Ours) CLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Linear Probing LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe CLIP-SVD (Ours) CLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Linear Probing LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe CLIP-SVD (Ours) 89.10 90.87 89.36 91.58 69.96 91.09 91.30 93.01 93.20 93.21 91.90 92.83 67.64 77.30 74.07 80.24 70.87 79.49 23.85 30.66 78.40 77.82 78.47 81.80 85.87 86.47 85.99 86.48 72.96 85.94 82.58 86.64 86.59 85.77 81.77 85.47 65.36 72.65 72.69 75.75 66.79 75.77 73.59 71.98 74.48 74.21 75.37 78.18 91.77 90.69 92.09 79.99 91.91 90.83 93.44 93.10 92.18 92.57 92. 81.87 75.65 82.24 77.51 82.17 26.23 21.78 80.03 88.64 81.37 83.95 86.67 86.42 86.74 78.98 86.76 83.37 86.92 86.90 85.91 83.60 85.59 76.92 74.41 78.51 73.43 77.94 76.45 72.92 75.82 77.93 78.89 80.55 92.03 91.32 92.70 85.80 92.80 91.80 93.25 93.23 92.13 92.83 93.77 84.53 77.35 84.50 81.27 83.75 28.48 24.86 81.43 81.59 85.03 86.58 86.83 86.36 87.24 83.69 87.28 85.17 87.19 87.03 87.01 85.33 86. 79.86 76.44 81.15 78.39 80.32 79.01 75.02 77.26 79.20 81.79 82.97 89.73 88.52 91.10 55.03 89.94 89.20 92.14 92.13 90.55 90.87 92.09 74.10 71.50 76.11 61.08 75.13 25.89 28.85 74.83 74.39 74.60 79.69 86.10 86.03 86.26 62.66 86.12 80.80 86.21 86.60 84.81 81.47 85.58 70.20 70.42 73.08 56.92 72.93 70.13 67.63 71.57 71.78 72.58 76.06 89.03 89.32 90.72 40.70 89.52 92.20 91.17 91.97 89.01 89.10 92. 69.67 68.92 73.27 48.24 72.36 24.96 25.42 72.93 71.91 71.83 76.34 85.90 85.14 86.01 44.63 83.23 82.07 86.10 86.27 82.75 80.50 85.82 67.87 68.89 70.62 45.77 70.35 68.09 66.95 69.51 68.20 69.27 73.20 28 Table S13: Biomedical per-dataset performance comparison of CLIP-SVD with various methods in few-shot setting in terms of classification accuracy (%). Dataset Method = 1 = 2 = 4 = 8 = BTMRI BUSI COVID-QU-Ex CTKIDNEY BiomedCLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Standard LP LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe BiomedCoOp CLIP-SVD (Ours) BiomedCLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Standard LP LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe BiomedCoOp CLIP-SVD (Ours) BiomedCLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Standard LP LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe BiomedCoOp CLIP-SVD (Ours) BiomedCLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Standard LP LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe BiomedCoOp CLIP-SVD (Ours) 56.79 56.80 76.37 77.90 75.98 75.48 74.68 67.83 75.40 76.24 42.36 77.23 78.43 59.75 61.72 59.03 64.54 53.38 60.31 60.17 59.75 62.01 62.29 47.74 59.32 68.12 43.8 46.28 63.84 69.97 60.55 62.32 67.03 63.70 65.91 68.56 33.32 73.28 70.47 42.43 42.19 55.33 60.18 69.54 65.73 68.12 61.07 68.68 67.90 38.00 66.50 74. 57.15 73.75 79.18 77.63 77.11 79.27 71.69 79.79 78.82 50.75 78.55 83.21 61.86 55.93 68.50 65.53 66.10 64.69 65.82 67.37 64.83 42.65 63.27 73.87 48.68 66.77 69.89 68.29 66.19 74.66 69.36 74.86 74.65 36.43 76.26 73.97 44.64 69.89 75.24 78.89 77.06 77.40 73.93 77.43 78.23 39.67 77.16 78.33 60.16 78.97 82.27 81.24 81.61 82.37 78.45 81.07 82.84 56.22 83.30 84.64 63.55 68.78 71.89 68.78 70.05 69.49 70.2 70.62 71.47 45.62 70.34 74. 49.55 73.05 76.07 71.98 72.79 76.37 74.52 75.65 74.93 40.89 78.72 73.14 47.28 73.38 82.07 82.50 79.07 83.52 77.70 77.67 81.13 51.06 83.20 86.34 57.13 67.77 61.94 72.45 71.69 68.82 64.14 70.16 71.46 37.02 70.57 73.62 61.01 61.44 56.35 47.88 55.50 53.53 49.15 55.51 49.15 33.47 50.71 63.56 43.04 58.72 54.01 48.06 56.42 58.37 68.80 54.68 64.22 38.99 71.53 71.08 41.94 51.65 58.99 59.35 61.57 60.57 52.71 62.81 64.66 38.98 64.21 58. 56.80 66.66 59.60 62.24 64.72 63.82 59.47 63.33 66.92 38.01 65.08 63.25 61.44 62.71 61.86 51.41 51.12 48.73 52.26 53.39 46.33 41.38 50.71 61.58 50.42 62.13 54.89 49.91 46.41 58.82 69.36 61.68 60.42 35.86 72.64 71.15 47.17 45.85 46.68 43.82 57.70 54.51 47.88 58.92 54.65 30.62 56.13 55.64 29 Table S13 (continued): Biomedical per-dataset performance comparison of CLIP-SVD with various methods in few-shot setting in terms of classification accuracy (%). Dataset Method = 1 = 2 = 4 = = 16 Kvasir CHMNIST LC25000 RETINA BiomedCLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Standard LP LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe BiomedCoOp CLIP-SVD (Ours) BiomedCLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Standard LP LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe BiomedCoOp CLIP-SVD (Ours) BiomedCLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Standard LP LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe BiomedCoOp CLIP-SVD (Ours) BiomedCLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Standard LP LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe BiomedCoOp CLIP-SVD (Ours) 54.58 54.83 69.61 69.94 72.38 69.36 70.78 68.94 68.28 70.00 53.22 74.08 74.92 30.65 33.26 70.05 71.74 71.07 67.79 68.66 58.58 68.77 69.13 65.87 71.19 75.69 50.03 52.91 83.32 79.57 85.30 82.61 84.66 77.44 82.10 84.72 76.73 85.60 89. 26.26 26.07 43.42 47.37 51.31 46.95 42.22 39.75 42.61 43.09 30.89 45.58 52.63 56.08 69.13 75.86 78.88 72.52 77.14 72.92 72.05 76.03 56.03 77.72 80.08 36.48 69.57 74.51 76.30 72.40 75.00 66.58 69.50 70.99 68.88 74.78 81.05 56.33 87.25 90.41 90.24 89.14 87.50 85.57 84.63 87.86 82.19 88.77 89.98 25.84 48.08 56.07 53.94 53.44 51.87 48.45 49.97 52.26 41.80 56.47 62.33 56.50 74.22 78.00 79.00 75.41 77.88 75.22 72.95 75.88 63.50 78.89 82. 42.06 77.68 80.43 80.34 78.32 79.63 72.16 73.58 75.11 71.99 79.05 84.75 57.56 89.17 92.35 92.77 92.58 92.19 87.38 86.79 90.70 86.73 92.68 91.52 26.05 54.23 62.85 62.27 60.62 59.38 53.91 51.18 50.47 53.78 61.28 68.06 54.83 60.94 64.22 62.00 60.47 64.86 65.50 65.67 64.70 45.17 67.25 72.08 31.67 63.32 58.90 64.42 60.61 59.68 50.82 60.06 59.60 57.76 59.79 66.13 53.47 72.73 71.82 78.40 71.42 76.55 71.76 75.18 74.76 69.80 77.74 82. 25.49 31.07 33.07 46.03 39.37 35.26 36.43 35.17 36.49 31.07 38.67 44.93 54.83 56.72 59.19 54.30 58.27 58.2 59.45 61.67 60.78 41.06 62.17 65.08 31.27 46.14 52.81 58.44 57.18 57.34 49.07 59.02 60.15 48.05 59.82 54.94 54.83 75.37 74.21 74.50 63.05 71.90 63.66 71.80 72.48 67.13 77.56 72.34 25.49 26.52 39.53 39.35 35.77 35.02 32.94 33.54 33.49 27.73 36.64 41.46 Table S13 (continued): Biomedical per-dataset performance comparison of CLIP-SVD with various methods in few-shot setting in terms of classification accuracy (%). Dataset Method = 1 = 2 = = 8 = 16 KneeXray OCTMNIST Average BiomedCLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Standard LP LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe BiomedCoOp CLIP-SVD (Ours) BiomedCLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Standard LP LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe BiomedCoOp CLIP-SVD (Ours) BiomedCLIP CLIP-Adapter Tip-Adapter Tip-Adapter-F Standard LP LP++ CoOp CoCoOp KgCoOp ProGrad MaPLe BiomedCoOp CLIP-SVD (Ours) 29.53 28.96 24.19 26.59 27.83 28.92 23.85 30.66 22.44 23.95 22.56 35.91 27.67 30.00 49.96 38.10 55.20 61.00 59.02 53.37 48.57 52.97 55.07 30.17 54.73 68.97 42.38 45.30 58.33 62.30 62.12 61.30 61.35 57.63 60.92 62.10 44.09 64.34 68.02 28.80 25.76 26.46 22.20 23.75 26.23 21.78 23.37 24.78 24.78 37.7 30. 49.50 53.93 65.00 65.85 63.69 63.67 55.40 61.03 62.17 30.53 58.87 79.07 46.54 62.01 68.12 67.33 65.48 67.74 63.15 66.00 67.06 47.37 68.96 73.26 29.08 33.17 27.67 23.97 26.38 28.48 24.86 24.80 26.27 25.87 39.69 38.37 52.73 53.33 72.50 69.40 68.35 65.47 60.67 62.80 63.33 33.63 66.93 80.67 48.46 67.60 68.12 70.81 70.09 71.48 67.51 67.71 69.21 52.93 73.41 76.46 28.66 33.55 28.38 26.57 26.40 25.89 28.85 28.14 23.83 22.67 37.72 27. 49.73 33.8 53.93 54.21 53.18 53.57 50.93 50.53 55.33 34.00 55.03 66.67 44.70 53.50 54.17 55.82 55.88 55.71 53.91 55.79 56.42 40.89 59.32 62.63 29.00 29.04 30.01 26.02 21.25 24.96 25.42 29.07 30.09 23.41 36.13 27.38 44.00 32.36 46.66 47.25 47.24 52.63 49.33 50.63 51.40 26.63 51.83 50.63 45.53 50.35 52.55 48.91 49.27 52.59 50.88 54.31 53.67 37.99 56.87 56.35 Table S14: Natural Base-to-novel generalization comparison of CLIP-SVD with previous methods CLIP CoOp CoCoOp KgCoOp ProGrad MaPLe CLIP-SVD [57] (Ours) [84] [35] [81] [82] [71]"
        },
        {
            "title": "Dataset",
            "content": "Average on 11 datasets"
        },
        {
            "title": "ImageNet",
            "content": "Caltech"
        },
        {
            "title": "Stanford\nCars",
            "content": "Base 69.34 82.69 Novel 74.22 63.22 HM 71.70 71.66 Base 72.43 76.47 Novel 68.14 67.88 HM 70.22 71.92 Base 96.84 98.00 Novel 94.00 89.81 HM 95.40 93.73 Base 91.17 93.67 Novel 97.26 95.29 HM 94.12 94.47 Base 63.37 78.12 Novel 74.89 60.40 HM 68.65 68.13 80.47 71.69 75. 75.98 70.43 73.10 97.96 93.81 95.84 95.20 97.69 96.43 70.49 73.59 72.01 Flowers102 Base 72.08 97.60 94.87 Novel 77.80 59.67 71.75 81.71 HM 74.83 74. Food"
        },
        {
            "title": "FGVC\nAircraft",
            "content": "SUN"
        },
        {
            "title": "EuroSAT",
            "content": "UCF101 Base 90.10 88.33 Novel 91.22 82.26 HM 90.66 85.19 Base 27.19 40.44 Novel 36.29 22.30 HM 31.09 28.75 Base 69.36 80.60 Novel 75.35 65.89 HM 72.23 72.51 Base 53.24 79.44 Novel 59.90 41.18 HM 56.37 54.24 Base 56.48 92.19 Novel 64.05 54.74 HM 60.03 68. Base 70.53 84.69 Novel 77.50 56.05 HM 73.85 67.46 90.70 91.29 90.99 33.41 23.71 27.74 79.74 76.86 78.27 77.01 56.00 64.85 87.49 60.04 71. 82.33 73.45 77.64 32 80.73 73.60 77.00 75.83 69.96 72.78 97.72 94.39 96.03 94.65 97.77 96. 71.76 75.04 73.36 95.00 74.73 83.65 90.50 91.70 91.09 36.21 33.55 34.83 80.29 76.53 78.36 77.55 54.99 64. 85.64 64.34 73.48 82.89 76.67 79.65 82.48 70.75 76.16 77.02 66.66 71.46 98.02 93.89 95.91 95.07 97.63 96. 77.68 68.63 72.88 95.54 71.87 82.03 82.28 75.14 78.55 76.66 70.54 73.47 97.74 94.36 96.02 95.43 97.76 96. 72.94 74.00 73.47 95.92 72.46 82.56 90.37 90.71 89.59 92.05 89.98 91.38 40.54 27.57 32.82 81.26 74.17 77.55 77.35 52.35 62. 90.11 60.89 72.67 84.33 74.94 79.35 37.44 35.61 36.50 80.82 78.70 79.75 80.36 59.18 68.16 94.07 73.23 82. 83.00 78.66 80.77 84.38 76.29 80.13 78.01 70.71 74.18 98.54 94.00 96.21 96.28 97.60 96.93 78.89 74.03 76. 96.30 76.19 85.07 90.52 91.74 91.13 44.00 37.01 40.20 82.56 78.98 80.73 82.91 65.42 73.13 94.53 74.03 83. 85.66 79.45 82.44 Table S15: Biomedical Base-to-novel generalization comparison of CLIP-SVD with previous methods"
        },
        {
            "title": "Dataset",
            "content": "Average on 9 datasets"
        },
        {
            "title": "BTMRI",
            "content": "COVID-QU-Ex"
        },
        {
            "title": "CHMNIST",
            "content": "LC"
        },
        {
            "title": "OCTMNIST",
            "content": "CLIP CoOp CoCoOp KgCoOp ProGrad MaPLe BiomedCoOp CLIP-SVD [84] [57] (Ours) [35] [82] [71] [39] [81] 75.69 67.33 69.86 82.13 94.98 88.09 75.19 90.34 82.07 83.86 63.01 71.96 82.89 60.45 69. 82.98 44.19 57.67 90.29 85.47 87.81 68.77 58.43 63.18 40.88 59.12 48.34 74.20 50.02 59.76 65.40 49.51 53. 66.17 49.58 56.69 61.36 71.25 65.94 63.99 63.51 63.75 76.66 26.17 39.02 89.19 23.76 37.52 87.16 52.66 65. 57.40 53.33 55.29 36.44 55.35 43.95 50.27 50.00 50.13 78.60 73.90 74.04 82.42 96.84 89.05 75.91 91.63 83. 86.93 78.94 82.74 86.50 61.83 72.11 88.87 42.73 57.71 93.77 97.00 95.36 68.46 67.72 68.09 44.23 78.35 56. 80.33 50.07 61.69 82.64 74.31 78.25 88.83 94.98 91.80 75.39 89.56 81.87 88.31 68.02 76.85 86.78 61.89 72. 90.38 35.42 50.89 96.68 96.36 96.52 84.62 84.88 84.75 43.73 76.52 55.65 89.00 61.13 72.48 Base 49.27 76.71 Novel 67.17 65.34 HM 55.23 68. Base 40.88 82.25 Novel 96.18 94.51 HM 57.37 87.95 75.52 67.74 69.11 77.88 94.84 85.53 Base 53.96 75.92 77.28 87.61 Novel 89.43 90.07 82.12 HM 67.31 82.39 Base 38.55 82.24 Novel 52.99 67.92 HM 44.63 74.40 Base 75.00 86.22 Novel 60.50 58.06 HM 66.97 69. Base 37.63 89.41 Novel 40.69 35.11 HM 39.10 50.42 Base 59.73 90.12 Novel 87.60 87.55 HM 71.03 88.82 Base 45.18 70.98 Novel 55.28 56.90 HM 49.72 63.16 Base 35.89 38.28 Novel 71.90 47.69 HM 47.88 42.47 Base 56.60 75.00 Novel 50.00 50.23 HM 53.10 60.17 81.96 56.56 66. 85.94 53.95 66.29 87.77 42.51 57.28 88.33 95.02 91.55 66.88 65.56 66.21 34.08 63.14 44.27 79.60 50.47 61. 71.90 65.94 67.22 78.00 95.05 85.69 75.42 89.61 81.90 81.67 58.45 68.14 81.56 59.00 68.47 75.45 38.70 51. 88.13 86.44 87.28 60.77 54.91 57.69 37.94 61.19 46.84 68.20 50.13 57."
        }
    ],
    "affiliations": [
        "Department of Computer Science & Software Engineering, Concordia University, Montreal, Canada",
        "Department of Electrical & Computer Engineering, Concordia University, Montreal, Canada"
    ]
}