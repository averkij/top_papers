{
    "paper_title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone",
    "authors": [
        "Jitai Hao",
        "Qiang Huang",
        "Hao Liu",
        "Xinyan Xiao",
        "Zhaochun Ren",
        "Jun Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training high-performing Small Language Models (SLMs) remains costly, even with knowledge distillation and pruning from larger teacher models. Existing work often faces three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce Low-Rank Clone (LRC), an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC matches or surpasses state-of-the-art models trained on trillions of tokens--while using only 20B tokens, achieving over 1,000x training efficiency. Our codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 1 8 7 2 1 . 5 0 5 2 : r Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone Jitai Hao1 Qiang Huang1, Hao Liu2 Xinyan Xiao2 Zhaochun Ren Jun Yu1, 1Harbin Institute of Technology, Shenzhen 2Baidu Inc. 3Leiden University jitaihao@outlook.com, {huangqiang, yujun}@hit.edu.cn {liuhao24, xiaoxinyan}@baidu.com, z.ren@liacs.leidenuniv.nl"
        },
        {
            "title": "Abstract",
            "content": "Training high-performing Small Language Models (SLMs) remains costly, even with knowledge distillation and pruning from larger teacher models. Existing work often faces three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce Low-Rank Clone (LRC), an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers (e.g., Llama-3.2-3BInstruct, Qwen2.5-3B/7B-Instruct) show that LRC matches or surpasses state-ofthe-art models trained on trillions of tokenswhile using only 20B tokens, achieving over 1,000 training efficiency. Our codes and model checkpoints are available at GitHub and Huggingface."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have shown exceptional performance across wide range of Natural Language Processing (NLP) tasks [2, 70, 69, 22, 25]. However, their deployment remains limited in real-world applications due to their immense computational and memory requirements, making them unsuitable for latency-sensitive, edge-based, or privacy-preserving scenarios. As result, there is growing momentum toward developing Small Language Models (SLMs) that offer similar capabilities with significantly lower resource footprints. Yet, despite their smaller size, training high-performing SLMs remains resource-intensive endeavor. For example, state-of-the-art SLMs such as Llama-3.2-3B [4] and Qwen3-1.7B [70] require 9 and 36 trillion tokens, respectively, for pre-training. To reduce such substantial costs, knowledge distillation [30] has emerged as Corresponding authors. Preprint. Under review. Figure 1: LRC results that achieve higher accuracy with 1,000 fewer training tokens, significantly boosting efficiency. key strategy, enabling compact student model to learn from larger and more powerful teacher model [39, 49, 23, 22]. Recent efforts such as Minitron [49] and Sheared Llama [66] have explored the use of existing LLMs [70, 1, 22, 3] to accelerate SLM pre-training. These methods typically combine structured pruning and distillation, first removing unimportant neurons and then recovering performance via distillation or continued training. Despite these advances, current distillation paradigms still fall short in fully utilizing the rich knowledge embedded in teacher models, resulting in limited efficiency and suboptimal student performance. We identify three core challenges in existing approaches: Information Loss from Hard Pruning: Most existing methods use hard pruning, permanently removing selected neurons, channels, attention heads, or entire layers [71, 49, 66, 48]. While reducing model size, it discards valuable information from the teachers weights, e.g., pruning 50% of Llama-7B in LLM-Pruner [46] caused sharp performance drop from 63.25 to 48.98. Inefficient Alignment of Representations: Feature-based distillation methods [35, 63, 49] often use additional projection matrices to align intermediate activations between teacher and student. However, as the students internal states evolve during training, learning effective alignment mappings becomes challenging, reducing distillation efficiency. Underutilization of Informative Activations: Prior work has primarily focused on aligning attention scores [35, 63], while largely overlooking the high-dimensional, information-rich activations from Feed-Forward Networks (FFNs). These FFN signals are crucial to the expressiveness of modern LLMs, as confirmed by our ablation study in Section 4.3. To overcome these challenges, we propose Low-Rank Clone (LRC), highly efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC eliminates the need to train the students weights, except for the RMSNorm parameters [72], which constitute less than 1% of the total, drastically reducing training overhead. Compared to prior multi-stage approaches [49, 66], LRC introduces unified framework that performs soft pruning and knowledge distillation simultaneously via trainable low-rank projection matrices. Each forward pass of LRC consists of two key steps: (1) Low-Rank Projection, which projects the teachers weights into smaller weights that directly serve as the students parameters. (2) Activation Clone, which aligns the students intermediate activations with those of the teacher to preserve behavioral fidelity. This design directly addresses the core limitations of existing methods, offering three key advantages: Minimal Information Loss: By directly generating the students weights from the teacher model, LRC preserves substantially more of the teachers knowledge, even at aggressive compression levels, than hard pruning strategies. Alignment-Free Distillation: The projection matrices naturally handle representational mismatches between teacher and student layers, removing the need for additional alignment modules and improving both efficiency and performance. Full Utilization of Activations: LRC captures broad spectrum of intermediate signals, including underutilized FFN activations, which we show to encode rich and valuable information often ignored by prior methods. We comprehensively evaluate LRC using strong, open-source teacher models such as Llama-3.2-3BInstruct and Qwen2.5-3B/7B-Instruct, showing competitive or superior performance compared to leading SLMs trained on trillions of tokens. As depicted in Figure 1, LRC-1.7B outperforms Qwen31.7B (64.98 vs. 63.17) on standard benchmarks, while requiring up to 1,000 fewer training tokens. These results highlight LRCs potential to drastically improve the cost-efficiency of high-performance SLM development and advance the state of practical, high-performance language models."
        },
        {
            "title": "2 Related Work",
            "content": "To reduce the computational and memory overhead of LLMs, researchers have explored range of techniques, including knowledge distillation [53, 58, 30, 35, 24, 63, 23], structured pruning [15, 71, 57, 46, 48], quantization [16, 67, 43], KV cache compression [68, 27, 34, 73, 45], and efficient training frameworks [6, 56, 74, 26, 33]. Among these, knowledge distillation and structured pruning are most relevant to our work. 2 Knowledge Distillation and Structured Pruning. Knowledge distillation [30, 36] aims to transfer knowledge from large, pre-trained teacher model to smaller student model. Early approaches either use synthetic data generated by the teacher to train the student [11, 51, 31], or minimize the divergence between their output distributions [30, 24, 55]. While effective, these techniques often suffer from limited transfer efficiency and scalability [23, 53]. To improve transfer quality, feature-based distillation methods like TinyBert [35], MiniLM [63] and TED [40] utilize intermediate activations from transformer layers to guide student learning. However, these methods ignore the rich information encoded in the model weights [21], and require additional alignment matrices to bridge discrepancies in hidden states, increasing training overhead. In contrast, LRC circumvents these limitations by using trainable low-rank projection matrices that simultaneously extract weight-level knowledge and serve as implicit alignment layers, eliminating the need for student weight initialization or separate alignment training. Recent approaches such as Minitron [49] and Sheared Llama [66] integrate hard pruning with distillation to compress LLMs. Yet, they rely on multi-stage pipelinepruning followed by distillation or continued trainingwhich increases training cost and sensitivity to pruning ratios. Moreover, hard pruning can cause substantial performance degradation [46]. By contrast, LRC performs soft pruning and distillation in simple single stage, improving both efficiency and model performance. Structured pruning remains key technique for LLM compression [46, 57, 19, 66]. recent example, SliceGPT [7], uses orthogonal projection and PCA [47] to prune weights while maintaining computational equivalence. Nevertheless, PCAs linear assumptions often fail to capture the nonlinear nature of LLM weights, limiting its performance and compression capacity. Instead, LRC adopts learnable low-rank projections that better adapt to the underlying structure of transformer weights, improving both compression fidelity and knowledge retention. Small Language Models (SLMs). SLMs have emerged as practical solution for deploying language models in resource-constrained environments. Recent efforts aim to train SLMs that approach LLMlevel performance [5, 70, 44, 32]. Nonetheless, even with the help of distillation [22], achieving strong performance still typically requires pre-training on tens of trillions of tokens [4], limiting accessibility and practicality. Unlike prior methods, LRC achieves competitive performance with only 10 billion tokens, offer paradigm shift in the efficiency of SLM training."
        },
        {
            "title": "3 Low-Rank Clone",
            "content": "We present Low-Rank Clone (LRC), novel distillation method that aims to construct SLMs approaching behavioral equivalence with strong teacher models. As illustrated in Figure 2, LRC consists of two key steps: (1) Low-Rank Projection that compresses the teachers weights into compact space, and (2) Activation Clone that align the activations of the student with those of the teacher to preserve behavioral fidelity during forward passes. Background and Notation. LRC builds on the transformer architecture as used in models like Llama [62, 22]. Each transformer layer mainly consists of Self Attention mechanism and FFN. The attention mechanism involves four weight matrices: Wq Rdqd, Wk Rdkvd, Wv Rdkvd, and Wo Rdd, where is the hidden size of the model, and dq, dkv denote the query/key/value dimensions. Given an input vector Rd, the attention output is: oattn = Attn(xW , xW , xW )Wo. The FFN employs the SwiGLU activation [54, 62, 70], containing three weight matrices, i.e., Wup Rdmidd, Wgate Rdmidd, and Wdown Rdmidd, where dmid represents the intermediate dimension. The computation of the SwiGLU-based FFN is defined as: offn = SwiGLU(xW up, xW gate)Wdown, where SwiGLU(x, y) = σ(y), with σ being the SiLU activation function, and denoting element-wise multiplication. RMSNorm [72] is typically the normalization technique after both the attention and FFN components, which is defined as: RMSNorm(x) = (cid:80)d i=1 x2 +ϵ 1 g, Figure 2: The overall procedure of LRC. To ensure clarity, attention and normalization modules are omitted. LRC involves two main steps: (1) Low-Rank Projection: applying low-rank projection matrices to compress the teachers weights into lower-dimensional space, which are then assigned to the student; (2) Activation Clone, executing standard forward passes in both models to collect intermediate activations, which are aligned using Mean Squared Error (MSE) loss. where Rd is learnable scaling parameter and ϵ is small constant added for numerical stability. Given vocabulary , the embedding matrix Wemb RV transforms input token indices into embeddings of dimension d. At the output, the language model (LM) head Wlm RV projects the final hidden states back into vocabulary logits. In SLMs, the LM head usually shares weights with the embedding matrix [22, 70], reducing parameter redundancy. 3.1 Low-Rank Projection Conventional feature-based distillation methods typically initialize student weights either from scratch [35] or by pruning subsets of the teacher model [66, 49, 40, 53]. While straightforward, these approaches inevitably discard valuable information and suffer from reduced distillation efficiency. To address this, LRC introduces Low-Rank Projection step that replaces manual initialization with principled, trainable transformation. As shown in Figure 2, set of low-rank projection matrices: m,i, emb, lm, {q, k, v, o, up, gate, down}, 0 < < l, are used to map the teachers weights into lower-dimensional student space, where is the number of layers. These matrices, together with the students RMSNorm parameters [72], constitute the only trainable components in LRC. Since RMSNorm contributes less than 1% of the total trainable parameters, we focus below on the projection process in two stages. Attention and FFN Weight Projection. For each layer i, LRC generates the students attention and FFN weights by applying low-rank projections to the corresponding teacher weights: m,i = mdS m,i RdT where {q, k, v, o, up, gate, down}, . Here, the hidden sizes follow: (1) do = d, (2) dk = dv = dkv, and (3) dgate = dup = ddown = dmid. Superscripts and refer to teacher and student, respectively. m,i RdTdS m,i, , m,i RdT m,iW mdT , and S (1) Embedding and LM Head Projection. The embedding and LM head weights are projected in the same manner: where {emb, lm}, shared vocabulary of the teacher and student models. RV dT = , mW emb, RdTdS , and RV dS . Here, is the (2) Structural Compatibility and Deployment. The resulting student model retains full architectural compatibility and can perform forward passes without modification. Importantly, this enables immediate post-training and inference of the student model. 3.2 Activation Clone While previous methods have leveraged attention states to improve distillation efficiency [63, 35], they largely overlook the rich information contained in FFN activations. To capture more comprehensive 4 Algorithm 1: Overall Procedure of LRC Input: Input token sequence ; number of layers l; RMSNorm constant ϵ; teachers weights lm; low-rank projection matrices {W m,i}, emb, lm; m,i}, emb, {W Output: Clone loss Lclone; Step 1: Low-Rank Projection 1 for = 1 to do 2 foreach {q, k, v, o, up, gate, down} do"
        },
        {
            "title": "W S",
            "content": "m,iW m,i embW Step 2: Activation Clone emb emb; m,i; 4 lm lmW lm; 5 Lclone 0; 6 hT, oT attn, oT attn, oS 7 hS, oS 8 for = 1 to do 9 ffn Forward(T , l, ϵ, {W ffn Forward(T , l, ϵ, {W m,i}, m,i}, emb, emb, lm); lm); Generate student weights Get teacher act. dict. Get student act. dict. foreach {q, k, v, gate, up} do m,i, hT Lclone Lclone + E(hS m,i); attn,iW Lclone Lclone + E(oS attn,i, oT o,i) + E(oS ffn,i, oT ffn,iW down,i); 11 Compute clone loss of interm. states 12 return Lclone; semantic signals, LRC aligns wide range of intermediate activations, treating them as fine-grained anchor points for behavioral replication. Specifically, LRC matches both the internal linear projections hm = xW , where {q, k, v, up, gate}, and the output vectors oattn and offn from the attention and FFN modules, respectively. As depicted in Figure 2, all these activations are aligned using the Mean Squared Error (MSE) loss E, yielding the overall Activation Clone loss Lclone: Lclone = (cid:88) (cid:104) E(oS attn,i, oT attn,iW o,i) + E(oS ffn,i, oT ffn,iW down,i) + E(hS m,i, hT m,i) (cid:105) , (3) (cid:88) where {q, k, v, up, gate}. Following prior work [22, 24], LRC also employs KL divergence loss LKL to align teacher and student logits over the vocabulary and next-token prediction loss LLM to enhance model performance. The total training objective is: = LKL + LLM + αLclone, (4) where α is hyperparameter controlling the weight of activation alignment. Alignment Free Design. Vanilla feature-based distillation approaches require additional projection In contrast, LRC is inherently matrices to reconcile mismatched hidden dimensions [35, 40]. alignment-free, i.e., the same low-rank projection matrices used to generate the students weights (e.g., Wo, Wdown) can be reused directly to align activations during training. This property arises from the structure of transformer modules, where outputs are linear combinations of their respective output projection weights. Here, we illustrate this using the FFN. Formally, we have Lemma 1 as follows: Lemma 1 (Alignment-Free FFN Output Cloning). Let down,i denote the FFN down-projection weight in the student model at layer i, derived via the low-rank projection from the teachers weight down,i and projection matrix down,i, such that: down,i = S down,iW down,i. If the intermediate FFN activations hup,i and hgate,i are perfectly cloned, i.e., up,i = hT hS gate,i = hT up,i, hS gate,i, then the student FFN output is exactly equal to the teacher output passed through the same projection: ffn,iW ffn,i, oT down,i) = 0. E(oS 5 The proof is provided in Appendix A. Lemma 1 shows that LRC needs no additional alignment matricesits low-rank projections suffice for both weight transformation and activation alignment. Remarks. The overall procedure of LRC is summarized in Algorithm 1. The Forward function executes standard transformer forward pass and collects intermediate activations hm,i (for {q, k, v, up, gate}) and the outputs oattn,i and offn,i of the attention and FFN modules at each layer. Pseudo-code for this function is provided in Appendix B."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experiment Setup Training Configuration. We train series of LRC models using strong open-source SLMs as teachers, i.e., Llama-3.2-3B-Instruct [22] for LRC-1.5B, Qwen2.5-3B-Instruct [70] for LRC-1.7B, and Qwen2.5-7B-Instruct for LRC-4B. To fairly compare with Sheared-Llama [66], we also train LRC-2.7B using Llama-2-7B-chat as the teacher. We employ supervised fine-tuning (SFT) to obtain the instruct versions of LRC models. Implementation details are provided in Appendices C.1 and C.2. All models are trained with packed sequences of length 2,048 for computational efficiency. We use the Adam optimizer with β1 = 0.9 and β2 = 0.999, and set the KL divergence temperature to 40. Training runs on 8 NVIDIA H800 GPUs using PyTorch, transformers [65], and deepspeed [6] for distributed parallelism. The hyperparameter settings and model configurations are provided in Appendices C.3 and C.4, respectively. Training Datasets. We construct the training corpus by mixing data from Fineweb-Edu [50], DCLM [38], and CosmopiediaV2 [5]. Fineweb-Edu served as the primary component, selected for its high-quality educational content. To enrich the pre-train data distribution, we incorporate DCLM and CosmopiediaV2, and use OpenHermes [61]. We also utilize UltraChat [18] as supervised finetuning dataset for instruction-tuning. The combined pre-train dataset is randomly shuffled without curriculum settings. Data composition ratios and sources are listed in Appendix C.5. Baselines. We compare LRC against several representative and competitive baselines: (1) Sheared Llama [66], using the same teacher and training data for fair comparison; (2) Minitron [49], evaluated via its released checkpoint; (3) TinyBERT [35], feature-based distillation method adapted to the Llama architecture. We also benchmark LRC against state-of-the-art open-source SLMs of similar sizes, including MiniCPM [32], SmolLM2 [5], Gemma3 [60], InternLM [10], and models from the Qwen3 [69] families. Model checkpoints are listed in Appendix C.2. Evaluation Protocol. In the experiments, all models are evaluated in zero-shot settings using the lm-evaluation-harness framework [20], with transformers [65] serving as the inference backend. We assess performance across suite of downstream tasks covering range of language understanding skills: (1) Scientific and Logical Reasoning (ARC-E [13], ARC-C [13], and LogiQA [42]), (2) Commonsense Understanding (CommonsenseQA (CSQA) [59], PIQA [9], and WinoGrande Table 1: Zero-shot performance comparison between LRC and state-of-the-art publicly available models with fewer than 2B parameters. # Tokens denotes the number of training tokens; N/A indicates unavailable training data. All models including teachers and ours are instruct versions. Model InternLM2-1.8B LRC-1.7B Qwen3-1.7B SmolLM2-1.7B LRC-1.5B MiniCPM-1.2B Teacher # Tokens Dataset ARC-E ARC-C LogiQA CSQA PIQA WinoG BoolQ SciQ MMLU Avg. Qwen2.5-3B 20B Mixed-1.1 2T N/A 71.04 42.06 28.42 70.11 74.27 63.77 75.50 94.50 43.75 62.60 74.62 44.20 30.88 70.19 73.07 63.30 79.82 93.80 54. 64.98 11T Llama3-3B 10B SomlLM Mixed-1.1 69.11 43.52 28.88 51.19 76.01 68.98 68.47 89.80 48.50 60.50 74.75 44.97 30.72 65.77 73.07 62.25 75.78 94.60 49. 63.48 1T N/A 70.16 39.68 30.88 64.29 74.65 60.77 67.58 91.50 44.23 60.42 36T N/A 72.47 43.00 28.42 64.78 72.20 61.48 77.65 93.10 55. 63.17 6 Table 2: Zero-shot performance comparison between LRC and state-of-the-art publicly available models with more than 2B parameters, where the model with -B refers to pre-trained only. Model Gemma3-4B Minitron-4B Qwen3-4B LRC-4B LRC-2.7B-B Sheared-Llama-2.7B-B Teacher # Tokens Dataset ARC-E ARC-C LogiQA CSQA PIQA WinoG BoolQ SciQ MMLU Avg. Nemotron4-15B 94B N/A 4T N/A Qwen2.5-7B 18B Mixed-2.0 36T N/A Llama2-7B 10B Redpajama Llama2-7B 50B Redpajama 82.53 57.08 33.03 69.37 76.44 69.38 83.94 95.50 57.58 69. 79.59 54.35 30.26 71.09 77.64 65.93 82.60 96.60 56.77 68.31 80.47 53.58 33.64 75.76 75.08 65.27 84.95 95.50 68.38 70.29 78.37 52.47 34.10 79.28 76.82 67.72 84.50 95.00 64.41 70. 58.59 29.61 29.03 36.36 66.97 62.43 74.31 85.50 31.20 52.67 67.30 33.58 28.26 18.92 76.17 65.04 65.99 91.10 26.56 52.55 (WinoG) [52]), (3) Reading Comprehension (BoolQ [12]), and (4) World Knowledge (SciQ [64] and MMLU [29]). Downstream task and evaluation metric details are provided in Appendix C.6. 4.2 Main Results We begin by comparing LRC models with fewer than 2B parameters against leading SLMs, as shown in Table 1. LRC-1.5B, distilled from Llama-3.2-3B-Instruct using only 10B tokens, outperforms SmolLM2-1.7B, which was trained on 11T tokens. Similarly, LRC-1.7B, trained from Qwen2.53B-Instruct, achieves the best performance among all models below 2B, surpassing Qwen3-1.7B, which was trained on 36T tokens. These results highlight LRCs remarkable distillation efficiency, achieving superior performance with more than 1000 fewer training tokens. To assess scalability, we further evaluate LRC on larger models in Table 2. LRC-4B, distilled from Qwen2.5-7B-Instruct using just 10B tokens, achieves performance comparable to Qwen3-4B (trained on 36T tokens), and outperforms Minitron-4B, which was trained with 5 more data. We also conduct fair comparison with Sheared-Llama-2.7B-B by replicating its setup using Llama2-7B as the teacher and identical training dataset without dynamic batch loading [66] for improved data quality. Our LRC-2.7B-B still achieves comparable performance while using 5 fewer tokens. Here, -B indicates pre-training only (i.e., no SFT). These findings demonstrate LRCs robustness and generality across diverse teacher-student configurations. Notably, all reported LRC models are followed by SFT. We further analyze the impact of SFT in Appendix D.1. Additionally, we evaluate the LRC performance on few-shot tasks, where the results and analysis are provided in Appendix D.2. 4.3 Ablation Study We conduct an ablation study to assess the contributions of LRCs two core components: Low-Rank Projection and Activation Clone. All experiments use Llama-3.2-3B-Instruct as the teacher and are trained on 2.5B tokens without performing SFT. We report training LM loss as the evaluation metric, as the data contains minimal duplication and training runs for only one epoch. Low-Rank Projection. To assess the impact of low-rank projection, we compare against TinyBERTstyle distillation, where the student is randomly initialized and trained from scratch using MSE loss with attention activations and outputs of each layer. We implement TinyBERT for the Llama architecture. As it relies on attention score maps, TinyBERT struggles to scale to longer contexts since it cannot use FlashAttention [14]. The adaptations are detailed in Appendix C.7. As shown in Figure 3, LRC reaches an LM loss of 3.0 nearly 2.7 faster than TinyBERT, highlighting the benefit of transferring structured weight information through projection rather than learning from scratch. Activation Clone. To measure the contribution of different activation signals in the clone loss Lclone, we conduct both term-level and module-level ablations. Details are provided in Appendix C.8. We also test layer-level ablations, and the results are shown in Appendix D.6. 7 Figure 3: Effect of LRC component ablations on LM loss convergence over training time. Figure 4: The trend of MMLU scores with increasing training tokens. Table 3: Ablation results for removing different terms of the clone loss. Scores significantly higher than the baseline (None, with all losses retained) are underline. Removed Term None Attn Attn Attn Attn FFN gate FFN up FFN down LM Loss 2.639 2. 2.629 2.639 2.636 2.677 2.639 2. Table 3 presents the term-level results when individual activation terms are removed. Removing FFN-related terms, particularly FFN gate, significantly degrades performance, increasing LM loss from 2.639 to 2.677. This confirms that FFN activations carry essential information and that aligning them is crucial for effective behavioral cloning. Figure 3 depicts the module-level results, where we show the impact of dropping all attention-related vs. FFN-related clone losses, as well as removing all clone signals entirely. We observe that LRC w/o Attn, while significantly impacting performance in the early training stages, gradually recovers and converges toward the performance of full LRC in later stages. However, LRC w/o FFN produces substantial performance degradation that persists throughout training, further confirming the critical importance of FFN activations. In addition, when both LRC and LRC w/o All Clone Loss reach an LM loss of 3.0, LRC achieves more than 2 reduction in training time usage, demonstrating the effectiveness of activation clone. Alignment-Free Property. Finally, we evaluate LRCs alignment-free behavior by comparing it to variant (LRC w/o Alignment Free) that trains additional alignment matrices for attention and FFN outputs. As shown in Figure 3, this variant increases trainable parameter size, prolongs training time, and leads to worse final performance. These results confirm that LRCs projection-based alignment is not only sufficient for effective knowledge transfer but also more efficient and stable. 4.4 Model Analysis To better understand the design choices and behavior of LRC, we conduct series of in-depth analyses, focusing on two aspects: (1) performance trend during training and (2) impact of training data quality. Table 4: Impact of training data quality. Performance Trend During Training. We monitor model checkpoints throughout training to examine performance trajectories. Figure 4 shows the variation of MMLU scores, while ARC-C trends are presented in Appendix D.3. These benchmarks were selected due to their alignment with the overall performance trend observed in Table 1. Results show that LRC achieves competitive performance using just 50% of the training tokens. Moreover, model performance continues to improve steadily with more training, confirming LRCs scalability and efficient learning dynamics. Llama3-3B 10B Mixed-2.0 Mixed-1.0 Mixed-1.1 Teacher # Tokens Dataset LRC-1.5B Model Avg. 62.48 61.35 62.12 10B 20B Impact of Training Data Quality. Since LRC requires only small amount of training data to achieve strong performance, we further examine how training data quality affects performance. Fineweb-Edu [50] provides an educational value score for each sample. To evaluate the impact of higher-quality inputs, we construct filtered dataset by retaining samples with scores 4 and retrain LRC-1.5B using Llama-3.2-3B-Instruct as the teacher. As shown in Table 4, training on this filtered data with just 10B tokens (Mixed-1.1) surpasses the performance of the 20B-token setting (Mixed-2.0), both without SFT. This result demonstrates LRCs ability to amplify the benefits of high-quality data, further enhancing its sample efficiency. In addition, we study the effects of quantization and the clone loss weighting parameter α. Due to space limitations, those results are provided in Appendices D.4 and D.5, respectively. 4.5 Efficiency Finally, we analyze the training efficiency of LRC in terms of memory usage and throughput, focusing on weight sharing strategies and overall training speed. Memory-Efficient Weights Sharing. To further reduce memory overhead and accelerate training, we explore weight sharing across low-rank projection matrices. Specifically, we experiment with tying the projection matrices for input components within both the attention and FFN modules. For v, and for FFN, we set Wp attention, we set Wp up. We train LRC-1.5B on 10B tokens from the Mixed-1.0 dataset using Llama-3.2-3B-Instruct as the teacher and α = 1.0. We do not apply SFT to these models. gate = Wp = Wp = Wp Table 5 presents the results of four weight-sharing configurations tested in our experiments. The term All indicates no weight sharing, while IO denotes shared projections for input-only modules. For instance, (All, IO) signifies no weight sharing in attention but with shared weights in the FFN. The results show that the full-parameter setting (All, All) delivers the best performance, albeit with the highest memory cost. Notably, sharing projections in the FFN results in greater performance drop than sharing them in attention. This finding also corroborates the observations from Section 4.3, indicating that FFNs encode richer information and derive greater benefit from dedicated capacity. Table 5: Performance comparison across different low-rank projection structures of LRC-1.5B. Table 6: Throughput of training methods on 8 H800. (Attn, FFN) Avg. Score #Trainable Params Speedup Method # Tokens/Sec (All, All) All) (IO, IO) (All, IO) (IO, 61.22 60.81 60.25 60. 0.93B 0.67B 0.80B 0.53B 1.00 1.07 1.05 1.11 LRC Sheared Llama (Prune) Ordinary Training TinyBERT 84K 30K 146K 65K Throughput. Table 6 reports the token-level throughput of LRC training using 8 H800 GPUs under Zero-2 optimization with deepspeed. For reference, we also measure the ordinary pre-training speed of an equivalently sized model using LlamaFactory [75]. Despite the overhead of computing the teacher models hidden states, LRC maintains over 50% of the throughput of standard training. In contrast, TinyBERT, adapted to the Llama architecture, suffers significantly in throughput, particularly due to its reliance on attention maps as supervision. This requirement prevents the usage of FlashAttention [14], limiting both sequence length and training speed. We also conducted inference throughput tests on vLLM [37], as shown in Appendix D.7. These findings confirm that LRC is not only sample-efficient but also highly practical, offering strong scalability for large-scale training and deployment in real-world settings."
        },
        {
            "title": "5 Conclusions",
            "content": "In this paper, we introduced LRC, simple yet efficient method for distilling SLMs from large teacher models. Unlike prior approaches that rely on hard pruning or extensive retraining, LRC integrates soft pruning and distillation jointly through unified framework. By using trainable low-rank projection matrices, LRC compresses the teachers weights into the student model while aligning intermediate activations, particularly from often-overlooked FFN layers. Extensive experiments across diverse 9 tasks demonstrate that LRC consistently match or outperform state-of-the-art SLMs trained on trillions of tokens, despite using only 10-20B training tokens. Notably, LRC-1.7B outperforms Qwen3-1.7B while achieving over 1,000 training efficiency. These findings position LRC as promising paradigm for building compact, high-performing models under limited resources. Limitations and Broader Impact. This study primarily explores the efficiency of LRC under modest training budgets, leaving its upper performance limits with larger-scale training unexplored. Our work focuses on efficient pre-training and omits extensive post-training stages such as reinforcement learning and alignment tuning. As result, the current LRC models may exhibit limitations in complex downstream tasks, such as mathematical reasoning, coding, and safety-critical scenarios. Despite these gaps, LRC offers substantial societal benefits. By drastically reducing the cost and data requirements of training capable SLMs, it democratizes access to advanced language models, empowering smaller research groups and organizations."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023. [3] Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya Sunil Mahabaleshwarkar, Somshubra Majumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupinder Parmar, Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak Scowcroft, Jason Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha Smelyanskiy, Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, and Chen Zhu. Nemotron-4 340B Technical Report. arXiv preprint arXiv:2406.11704, 2024. [4] Meta AI. Llama 3.2-3b instruct. https://huggingface.co/meta-llama/Llama-3. 2-3B-Instruct, 2025. Accessed: 2025-03-26. [5] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, et al. SmolLM2: When smol goes bigdata-centric training of small language model. arXiv preprint arXiv:2502.02737, 2025. [6] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. DeepSpeed-inference: enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis (SC22), pages 115, 2022. [7] Saleh Ashkboos, Maximilian Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language models by deleting rows and columns. In The Twelfth International Conference on Learning Representations (ICLR), 2024. [8] Shane Bergsma, Nolan Simran Dey, Gurpreet Gosal, Gavia Gray, Daria Soboleva, and Joel Hestness. Straight to Zero: Why linearly decaying the learning rate to zero works best for LLMs. In The Thirteenth International Conference on Learning Representations (ICLR), 2025. 10 [9] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence (AAAI), pages 74327439, 2020. [10] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. [11] Wei-Lin Chiang, Zhuohan Li, Ziqing Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https: // vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. [12] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 29242936, 2019. [13] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [14] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Proceedings of the 36th International Conference on Neural Information Processing Systems (NeurIPS), pages 1634416359, 2022. [15] Lucio Dery, Steven Kolawole, Jean-François Kagy, Virginia Smith, Graham Neubig, and Ameet Talwalkar. Everybody prune now: Structured pruning of llms with only forward passes. arXiv preprint arXiv:2402.05406, 2024. [16] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. In Proceedings of the 36th International Conference on Neural Information Processing Systems (NeurIPS), pages 3031830332, 2022. [17] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. In International Conference on Learning Representations (ICLR), 2022. [18] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional In Proceedings of the 2023 Conference on Empirical Methods in Natural conversations. Language Processing (EMNLP), pages 30293051, 2023. [19] Elias Frantar and Dan Alistarh. SparseGPT: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning (ICML), pages 1032310337, 2023. [20] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 2024. [21] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 54845495, 2021. [22] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783, 2024. [23] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. MiniLLM: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations (ICLR), 2024. [24] Yuxian Gu, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. MiniPLM: Knowledge distillation for pre-training language models. arXiv preprint arXiv:2410.17215, 2024. [25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [26] Jitai Hao, Weiwei Sun, Xin Xin, Qi Meng, Zhumin Chen, Pengjie Ren, and Zhaochun Ren. MEFT: Memory-efficient fine-tuning through sparse adapter. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 23752388, 2024. [27] Jitai Hao, Yuke Zhu, Tian Wang, Jun Yu, Xin Xin, Bo Zheng, Zhaochun Ren, and Sheng Guo. OmniKV: Dynamic context selection for efficient long-context llms. In The Thirteenth International Conference on Learning Representations (ICLR), 2025. [28] Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. Toxigen: large-scale machine-generated dataset for adversarial and implicit hate speech detection. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pages 33093326, 2022. [29] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and In International Jacob Steinhardt. Measuring massive multitask language understanding. Conference on Learning Representations (ICLR), 2021. [30] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [31] Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In Findings of the Association for Computational Linguistics: ACL 2023, pages 80038017, 2023. [32] Shengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, et al. MiniCPM: Unveiling the potential of small language models with scalable training strategies. In First Conference on Language Modeling (COLM), 2024. [33] Angela Jiang, Daniel L-K Wong, Giulio Zhou, David Andersen, Jeffrey Dean, Gregory Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary Lipton, et al. Accelerating deep learning by focusing on the biggest losers. arXiv preprint arXiv:1910.00762, 2019. [34] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LLMLingua: Compressing prompts for accelerated inference of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1335813376, 2023. [35] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 41634174, 2020. 12 [36] Yoon Kim and Alexander Rush. Sequence-level knowledge distillation. In Proceedings of the 2016 conference on Empirical Methods in Natural Language Processing (EMNLP), pages 13171327, 2016. [37] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles (SOSP), pages 611626, 2023. [38] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. DataComp-LM: In search of the next generation of training sets for language models. In Proceedings of the 38th International Conference on Neural Information Processing Systems (NeurIPS), pages 1420014282, 2024. [39] Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, and Ming Yin. Synthetic data generation with large language models for text classification: Potential and limitations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1044310461, 2023. [40] Chen Liang, Simiao Zuo, Qingru Zhang, Pengcheng He, Weizhu Chen, and Tuo Zhao. Less is more: Task-aware layer-wise distillation for language model compression. In International Conference on Machine Learning (ICML), pages 2085220867, 2023. [41] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pages 32143252, 2022. [42] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. LogiQA: challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence (IJCAI), pages 36223628, 2021. [43] Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. SpinQuant: LLM quantization with learned rotations. arXiv preprint arXiv:2405.16406, 2024. [44] Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. MobileLLM: Optimizing sub-billion parameter language models for on-device use cases. In International Conference on Machine Learning (ICML), pages 3243132454, 2024. [45] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. KIVI: tuning-free asymmetric 2bit quantization for kv cache. In International Conference on Machine Learning (ICML), pages 3233232344, 2024. [46] Xinyin Ma, Gongfan Fang, and Xinchao Wang. LLM-pruner: on the structural pruning of large language models. In Proceedings of the 37th International Conference on Neural Information Processing Systems (NeurIPS), pages 2170221720, 2023. [47] Andrzej Mackiewicz and Waldemar Ratajczak. Principal components analysis (pca). Computers & Geosciences, 19(3):303342, 1993. [48] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. ShortGPT: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024. [49] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. In Proceedings of the Compact language models via pruning and knowledge distillation. 38th International Conference on Neural Information Processing Systems (NeurIPS), pages 4107641102, 2024. 13 [50] Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The FineWeb datasets: Decanting the web for the finest text data at scale. In Proceedings of the 38th International Conference on Neural Information Processing Systems (NeurIPS), pages 3081130849, 2024. [51] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4. arXiv preprint arXiv:2304.03277, 2023. [52] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pages 87328740, 2020. [53] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. [54] Noam Shazeer. GLU Variants Improve Transformer. arXiv preprint arXiv:2002.05202, 2020. [55] Makoto Shing, Kou Misaki, Han Bao, Sho Yokoi, and Takuya Akiba. TAID: Temporally adaptive interpolated distillation for efficient knowledge transfer in language models. arXiv preprint arXiv:2501.16937, 2025. [56] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [57] Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. simple and effective pruning approach for large language models. In The Twelfth International Conference on Learning Representations (ICLR), 2024. [58] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 43234332, 2019. [59] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 41494158, 2019. [60] Gemma Team. Gemma 3, 2025. [61] Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. [62] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [63] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. MiniLM: deep selfattention distillation for task-agnostic compression of pre-trained transformers. In Proceedings of the 34th International Conference on Neural Information Processing Systems (NeurIPS), pages 57765788, 2020. 14 [64] Johannes Welbl, Nelson Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 94106, 2017. [65] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. [66] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared LLaMA: Accelerating language model pre-training via structured pruning. In The Twelfth International Conference on Learning Representations (ICLR), 2024. [67] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning (ICML), pages 3808738099, 2023. [68] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations (ICLR), 2024. [69] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. [70] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 Technical Report. arXiv preprint arXiv:2412.15115, 2024. [71] Yifei Yang, Zouying Cao, and Hai Zhao. LaCo: Large language model pruning via layer collapse. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 64016417, 2024. [72] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Proceedings of the 33rd International Conference on Neural Information Processing Systems (NeurIPS), pages 1238112392, 2019. [73] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2O: heavy-hitter oracle for efficient generative inference of large language models. In Proceedings of the 37th International Conference on Neural Information Processing Systems (NeurIPS), pages 3466134710, 2023. [74] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. GaLore: Memory-efficient LLM training by gradient low-rank projection. In International Conference on Machine Learning (ICML), pages 6112161143, 2024. [75] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. [76] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Proof of Lemma 1 Proof: Given the Activation Clone conditions: the students FFN output is: up,i = hT hS up,i, hS gate,i = hT gate,i, up,i, hT gate,i)W down,i. oS ffn,i = SwiGLU(hT down,iW ffn,i = SwiGLU(hT oS = (cid:0)SwiGLU(hT"
        },
        {
            "title": "Since W S",
            "content": "down,i = down,i, we substitute the projection relationship: up,i, hT up,i, hT Thus, the students FFN output exactly matches the teachers FFN output passed through the same projection matrix. The corresponding MSE loss is: down,i) = E(oT ffn,i, oT gate,i)(W gate,i)W down,iW (cid:1) down,i) down,i. down,i) = 0. down,i, oT ffn,iW ffn,iW ffn,iW E(oS down,i This completes the proof. The Pseudo-code of Forward Function Algorithm 2: Transformer Forward Pass (Forward) Input: Input token sequence ; number of layers l; RMSNorm constant ϵ; layer weights {Wq,i, Wk,i, Wv,i, Wo,i, Wgate,i, Wup,i, Wdown,i}l {gattn,i, gffn,i}l i=1; RMSNorm weights i=1, gfinal; embedding weights Wemb; LM head weights Wlm; Output: Intermediate states dictionary h; Attention output dictionary oattn; FFN output dictionary offn; 1 empty dictionary; oattn empty dictionary; offn empty dictionary; 2 Lookup(T , Wemb); 3 for = 1 to do k,i; hv,i xattnW v,i; 7 4 5 q,i; hk,i xattnW Attention Module xattn RMSNorm(x, gattn,i, ϵ); hq,i xattnW oattn,i Attn(hq,i, hk,i, hv,i)Wo,i; + oattn,i; FFN Module xffn RMSNorm(x, gffn,i, ϵ); hgate,i xffnW up,i; offn,i SwiGLU(hup,i, hgate,i)Wdown,i; + offn,i; 11 12 return h, oattn, offn; gate,i; hup,i xffnW 8 9 Store Attention output Store FFN output"
        },
        {
            "title": "C Experiment Details",
            "content": "C.1 Implementation Details of LRC Using the Llama architecture as our implementation example, we add trainable low-rank projection matrices to the transformer-based structure. For each of the seven weight matrices in the original model corresponding to q, k, v, o, gate, up, down, we add corresponding low-rank projection matrix m,i. During model training, we directly generate the students weights when performing forward propagation at each layer, and sequentially complete the forward pass for both teacher and student in that layer. We then calculate the clone loss based on the collected intermediate states. This differs slightly from our pseudo-code description but is computationally equivalent. Table 7: Model checkpoints used in our experiments. Model InternLM2-1.8B Qwen3-1.7B SmolLM2-1.7B MiniCPM-1.2B Gemma3-4B Qwen3-4B Minitron-4B Sheared-Llama-2.7B Qwen2.5-7B Qwen2.5-3B Llama3.2-3B Huggingface Model ID internlm/internlm2-chat-1_8b Qwen/Qwen3-1.7B HuggingFaceTB/SmolLM2-1.7B-Instruct openbmb/MiniCPM-1B-sft-bf16 google/gemma-3-4b-it Qwen/Qwen3-4B nvidia/Nemotron-Mini-4B-Instruct princeton-nlp/Sheared-LLaMA-2.7B Qwen/Qwen2.5-7B-Instruct Qwen/Qwen2.5-3B-Instruct meta-llama/Llama-3.2-3B-Instruct Table 8: Training hyperparameters and statistical values in our experiments. Model Teacher Trained Tokens Pre-train Dataset SFT Dataset Pre-trained Tokens SFT trained Tokens Teacher Hidden Size Student Hidden Size Sequence Length Batch Size (tokens) Clone Loss Weight (α) Learning Rate (Pre-train) Learning Rate (SFT) LR Scheduler Warm-up Ratio Optimizer Adam β1 Adam β2 Temperature for LKL RMSNorm ϵ GPUs Training Time LRC-1.5B LRC-1.7B LRC-4B Llama-3.2-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct 18B Mixed 2.0 UltraChat 18B 0.2B 10B Mixed 1.1 UltraChat 10B 0.2B 20B Mixed 1.1 UltraChat 20B 0.2B 3,072 1,536 2,048 49,152 0.2 1.0 104 1.0 105 Linear 0.005 Adam 0.9 0.999 40 1.0 105 8 H800 34 Hours 2,048 1,200 2,048 32,768 0.5 6.7 105 1.0 105 Linear 0.005 Adam 0.9 0.999 40 1.0 105 8 H800 80 Hours 3,584 2,048 2,048 32,768 0.5 1.0 104 1.0 105 Linear 0.005 Adam 0.9 0.999 40 1.0 105 8 H800 138 Hours During initialization, we ensure that only the necessary weights are trained by setting the requires_grad attribute to True exclusively for the low-rank projection weights and RMSNorm weights of the student. After training, we use the low-rank projections to transform the teachers weights into the students weights. These weights are saved as new model, and the teacher weights are no longer needed. C.2 Checkpoints of Baseline and Teacher Models In our experiments, most baselines were evaluated directly using lm-evaluation-harness on open-source model checkpoints except TinyBert. The detailed configurations of the open-source checkpoints are provided in Table 7. All baseline models utilized the Instruct version, consistent with our choice of the Instruct model for the Teacher. C.3 Hyperparameter Settings We present the hyperparameters used in our experiments in Table 8. Here, Linear denotes scheduler with warmup stage to the specified learning rate, followed by linear decay to zero. We 17 Table 9: Model configuration comparison. Model LRC-1.5B Llama3.2-3B LRC-1.7B Qwen2.5-3B LRC-4B Qwen2.5-7B #Layers #Attn Heads #Attn KV Heads Head Dim Hidden Size FFN Intermediate Size RMSNorm ϵ Vocab Size Tie Word Embeddings 28 24 8 128 1,536 8,192 1.0105 128,256 True 28 24 8 128 3,072 8,192 1.0105 128,256 True 36 16 2 128 1,200 11,008 1.0106 151,936 True 36 16 2 128 2,048 11,008 1.0106 151,936 True 28 28 4 128 2,048 18,944 1.0106 152,064 False 28 28 4 128 3,584 18,944 1.0106 152,064 False Table 10: Training dataset composition (# Tokens). Training Dataset Mixed-1.0 Mixed-1.1-Qwen Mixed-1.1-Llama Mixed-2.0 Redpajama Fineweb-Edu DCLM Cosmopedia V2 OpenHermes 2.5 Redpajama Total 10B 0 0 450M 0 10.5B 20B 0 0 450M 0 20.5B 10B 0 0 450M 0 10.5B 18B 2B 1B 450M 0 21.5B 0 0 0 0 10B 10B used Flash Attention V2 [14] to accelerate training. Notably, the learning rate used for LRC-1.7B is slightly lower than that of other models, as we observed marginal increase in number of loss spikes when using 1.0 104. Therefore, the learning rate was reduced in accordance with the decrease in batch size. We adopted Linear learning rate scheduler, as prior work [8] suggests that this scheduler may be optimal. C.4 Model Configurations Table 9 details the configurations of our LRC models and compares them with their teachers Llama3.23B and Qwen2.5 variants. Key architectural parameters such as layer count, attention heads (Q/KV), hidden/FFN sizes, vocabulary size, and tied embeddings are presented, allowing for direct structural comparison. C.5 Training Dataset Composition We list all datasets used in our experiments, including Mixed-1.0, Mixed-1.1, Mixed-2.0, along with detailed usage quantities in Table 10. These mixed datasets are based on open-source datasets. The Redpajama data was included to enable fair comparison with Sheared Llama. All data used in the experiments are open-source, and their corresponding Huggingface data IDs are listed in Table 11. We randomly sampled 10B high-quality Fineweb-edu data and combined it with the complete OpenHermes dataset to create Mixed-1.0. Building on this, we developed Mixed-1.1 by filtering 20B tokens with an edu_score of at least 4, while still utilizing the entire OpenHermes dataset. Since Fineweb-edu primarily targets educational content, we recognized potential limitations in distributional diversity. To address this, we incorporated DCLM, more diverse dataset containing additional dialogue data. We also integrated Cosmopedia V2, high-quality synthetic dataset, to further enhance overall data quality. These efforts culminated in the creation of the Mixed-2.0 dataset. All data were uniformly shuffled. Not all generated data are necessarily used for training. For cost considerations, we sometimes use only subset of the mixed data. C.6 Downstream Task and Evaluation Metric Details We present the evaluation metrics for the tasks used in our evaluation, primarily following the metrics established in Sheared Llama [66]. Evaluation metrics for different downstream tasks and benchmarks are summarized in Table 12. 18 Table 11: Open source datasets used in our experiments. Dataset Fineweb-Edu Comopedia V2 DCLM OpenHermes-2.5 Redpajama UltraChat Huggingface Data ID HuggingFaceTB/smollm-corpus/fineweb-edu-dedup HuggingFaceTB/smollm-corpus/cosmopedia-v2 mlfoundations/dclm-baseline-1.0 teknium/OpenHermes-2.5 togethercomputer/RedPajama-Data-1T HuggingFaceH4/ultrachat_200k Table 12: Evaluation metrics for different downstream tasks and benchmarks. Downstream Task Benchmark Evaluation Metric Scientific and Logical Reasoning Commonsense Understanding Reading Comprehension World Knowledge Safety or Honesty ARC-E ARC-C LogiQA CSQA PIQA WinoG BoolQ SciQ MMLU Accuracy Accuracy Norm Accuracy Norm Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy ToxiGen TruthfulQA MC2 Accuracy Norm Instruction Following IFeval Instance-Level Loose Accuracy We selected diverse suite of datasets spanning four core categories: Scientific and Logical Reasoning (e.g., ARC, LogiQA), Commonsense Understanding (e.g., CSQA, PIQA, WinoGrande), Reading Comprehension (e.g., BoolQ), and World Knowledge (e.g., SciQ, MMLU). This selection, primarily following the established practices in prior work such as Sheared Llama [66] and Minitron [49], is motivated by the need for comprehensive and multifaceted evaluation of our models capabilities. Tasks within Scientific and Logical Reasoning directly probe the models capacity for complex inference, causal understanding, and the application of logical principles, which are crucial for sophisticated problem-solving. Commonsense Understanding benchmarks assess the models grasp of everyday situations and implicit human knowledge, vital component for generating natural and coherent interactions. Reading Comprehension tasks evaluate the fundamental ability to extract and synthesize information from text, cornerstone of language understanding. Finally, World Knowledge datasets measure the breadth and depth of the models acquired factual information across various domains, reflecting its ability to recall and utilize knowledge effectively. Collectively, these categories provide holistic view of the models cognitive strengths and limitations across different facets of intelligence. While tasks such as mathematical reasoning and code generation are important benchmarks for LLMs, we have deliberately excluded them from our primary evaluation suite for two main reasons. Firstly, for these types of downstream tasks, which often have easily verifiable solutions, there is possibility that current proprietary LLMs have been pre-trained on extensive, high-quality synthetic datasets specifically curated for these domains. This potential inclusion of specialized synthetic data in their pre-training corpora makes it challenging to draw fair comparisons regarding the inherent capabilities developed through general pre-training, as performance could be heavily skewed by access to such data [70]. Secondly, abilities in mathematics and coding are known to be substantially improvable through post-training alignment techniques, most notably reinforcement learning [25]. As our research primarily focuses on the efficiency and effectiveness of the pre-training phase itself, evaluating tasks whose performance is heavily influenced by subsequent post-training optimization stages falls outside the intended scope of this work. Our evaluation therefore centers on tasks that better reflect the foundational knowledge and reasoning abilities acquired directly from the pre-training process on more general textual data. 19 Table 13: Performance of LRC models on general downstream tasks before and after SFT. Model LRC-1.5B LRC-1.5B-B LRC-1.7B LRC-1.7B-B LRC-4B LRC-4B-B ARC-E ARC-C LogiQA CSQA PIQA WinoG BoolQ SciQ MMLU Avg. 74.75 44.97 30.72 65.77 73.07 62.25 75.78 94.60 49.42 63.48 73.40 42.15 31.03 64.46 71.60 61.88 73.27 94.40 50. 62.48 74.62 44.20 30.88 70.19 73.07 63.30 79.82 93.80 54.93 64.98 69.49 42.75 33.26 70.27 71.38 63.85 75.78 89.00 55.13 63.43 78.37 52.47 34.10 79.28 76.82 67.72 84.50 95.00 64. 70.30 78.75 52.22 34.87 78.30 76.61 67.80 84.95 94.30 64.58 70.26 Table 14: Performance of LRC models on safety and instruction-following tasks before and after SFT. Model LRC-1.5B LRC-1.5B-B LRC-1.7B LRC-1.7B-B LRC-4B LRC-4B-B ToxiGen IFeval TruthfulQA 43.19 23.74 46.98 43.19 24.58 47.97 43.30 39.69 47.95 43.30 36.69 53.17 43.72 13.67 50. 43.83 36.09 55.89 C.7 Implementation Details of TinyBERT Since TinyBERT requires using attention score maps as training supervision labels, we cannot use Flash Attention [14], so we had to reduce the max sequence length to 512 to decrease memory usage and improve training efficiency. All other experimental settings are fully aligned, including the students total parameter count, number of training tokens, learning rate, and other hyperparameters. C.8 Details of Experiments about Activation Clone When testing the module-level impact of different clone losses on LM loss convergence in activations clone, we used IO Attn rather than the better-performing All Attn. The definition of IO Attn can be found in Section 4.5. This choice was necessary because our experiments revealed that training with All Attn becomes highly unstable without the constraint provided by clone loss. Therefore, we were limited to using IO Attn for our analysis."
        },
        {
            "title": "D Additional Experiments",
            "content": "D.1 Impacts of SFT on Model Performance Modern LLMs typically undergo two-phase training process: pre-training followed by posttraining [22, 70], where post-training focuses on instruction following, alignment with human preferences, and safety. We compared the performance changes of the LRC model after SFT. To this end, we evaluate the student models on three widely used safety/honesty and instruction-following benchmarks: ToxiGen [28], TruthfulQA [41], and IFeval [76]. The specific metrics are also listed in Table 12. The observed improvement in general downstream tasks post-SFT shown in Table 13, contrasts with the limited gains in safety benchmarks and decline in instruction-following for LRC models which is shown in Table 14. This divergence likely stems from the composition and inherent limitations of the SFT dataset (UltraChat). While SFT enhances knowledge and common task execution, its efficacy for nuanced capabilities like complex instruction adherence and robust safety alignment is more constrained. The SFT data may lack sufficient diversity or targeted examples for these specialized domains. For instance, IFEvals intricate instructions might not be well-represented, potentially leading the model to prioritize fluency or common response patterns learned during SFT over the precise execution of novel, complex directives. 20 Table 15: 5-shot model performance on various benchmarks. Benchmark WinoGrande ARC-C BoolQ MMLU Avg. Gemma3-4B Minitron-4B Qwen3-4B LRC-4B InternLM2-1.8B LRC-1.7B Qwen3-1.7B SmolLM2-1.7B LRC-1.5B MinCPM-1.2B 69.06 73.95 66.85 69.93 65.27 63.38 60.62 69.14 60.77 64.80 60.49 53.58 61.18 58.36 44.03 48.98 52.22 51.88 47.95 44.71 84.77 82.39 85.27 85.69 78.59 81.74 80.61 75.11 79.24 76. 58.33 57.86 70.04 65.10 45.99 54.83 60.15 49.32 50.68 48.68 68.16 66.95 70.84 69.77 58.47 62.23 63.40 61.36 59.66 58.66 Similarly, without substantial corpus of safety-focused demonstrations and negative examples, significant improvements in benchmarks like ToxiGen are unlikely. Although TruthfulQA shows some gains, possibly from increased factuality within the SFT data, the overall pattern suggests that achieving strong instruction-following and safety often necessitates more targeted data or advanced alignment techniques, such as Reinforcement Learning from Human Feedback (RLHF), which are specifically designed to instill these fine-grained behaviors more effectively than standard SFT. D.2 Few-Shot Results and Analyses Following previous works [66, 49], we additionally evaluated the performance of LRC on few-shot tasks, with results presented in Table 15. The findings indicate that LRC models exhibit more modest performance gains in few-shot scenarios compared to baseline models. This is particularly notable given LRCs strong zero-shot capabilities, where it often surpasses competitors like Qwen. Several factors may contribute to this observation. Firstly, as initially posited, models such as Qwen3 and SmolLM2 might benefit from post-training strategies involving increased training data length or the specific collection and construction of data for long-context scenarios. Such data could implicitly bolster their proficiency in in-context learning. Secondly, the superior few-shot adaptability of baseline models could be attributed to more extensive SFT or instruction tuning phases, which are specifically designed to enhance models capacity to learn from small number of examples. Consequently, while LRCs pre-training fosters robust zero-shot generalization, its architecture or training objectives may not be as readily optimized for the distinct skill of rapid adaptation from few-shot demonstrations. We plan to investigate these potential factors in future work, including closer examination of baseline training methodologies and exploring targeted fine-tuning for LRC to improve its few-shot performance. D.3 Performance Trend of ARC-C during Training As shown in Figure 5, the trend exhibited by the ARC-C is generally consistent with that of MMLU. These two results together demonstrate the effectiveness of the LRC method and showcase its scalability. D.4 Impact of Quantization We further investigate whether models trained with LRC can be combined with quantization techniques to further reduce memory requirements. We utilize bitsandbytes [17] to perform 8-bit quantization on our model, and the experimental results are shown in Table 16. The experimental results indicate that the performance loss of our LRC-trained model remains within acceptable limits, which to some extent demonstrates that the numerical range of our model is within normal parameters and does not significantly impact the quantization method. D.5 Choice of α The hyperparameter α controls the relative strength of the activation clone loss Lclone. We experiment with different values of α and report performance in Figure 6. The results exhibit an n-shaped 21 Figure 5: The trend of ARC-C scores with increasing training tokens. Table 16: Impact of quantization using bitsandbytes [17] on model performance. Model SmolLM2-1.7B LRC-1.5B LRC-1.7B Quantization INT8 ARC-E ARC-C LogiQA CSQA PIQA WinoG BoolQ SciQ MMLU Avg. 70.24 43.26 26.88 50.12 75.41 67.64 68.99 89.50 47.38 59.94 BF16 69.11 43.52 28.88 51.19 76.01 68.98 68.47 89.80 48.50 60.50 INT BF16 INT8 BF16 74.79 44.71 30.11 65.27 73.01 61.88 76.15 94.20 49.13 74.75 44.97 30.72 65.77 73.07 62.25 75.78 94.60 49.42 73.82 43.86 31.03 70.68 72.58 62.90 79.69 93.50 53. 74.62 44.20 30.88 70.19 73.07 63.30 79.82 93.80 54.93 63.25 63.48 64.66 64.98 trend: when α is small, the clone loss fails to adequately guide the student to imitate the teachers behavior; when α is extremely large, training becomes unstable due to large gradient norms. One possible explanation is that the mismatch in parameter capacity between teacher and student makes over-enforcing behavioral similarity counterproductive. We leave deeper exploration of this phenomenon to future work. D.6 Clone Loss as Anchor We investigated the impact of removing clone loss from certain layers on the convergence rate of LM loss, aiming to verify whether each layers clone loss accelerates training. We tested removing 50%, 25%, 12.5% of the layers and observed the convergence behavior of the LM loss, with experimental results shown in Figure 7. This experiment demonstrates that the clone loss at each layer is important for LM loss convergence. These clone losses applied to each module in each layer can be viewed as anchor points, constraining the behavior of each student module to remain similar to its teacher counterpart. 22 Figure 6: The impact of α on model average performance. Figure 7: The impact of removing the clone loss from certain layers on the convergence of LM loss. D.7 Inference Speed We tested the inference speed on vLLM [37], with results shown in Table 17. From the perspective of inference speed, our model achieves good level of performance while also maintains satisfactory inference speeds. Table 17: Inference throughput of vLLM. Throughput # Input Tokens/Sec # Output Tokens/Sec LRC-1.5B Qwen3-1.7B SmolLM2-1.7B MiniCPM-1.2B 68,223 71,176 48,285 58,136 15,574 15,404 10,491 13,"
        }
    ],
    "affiliations": [
        "Baidu Inc.",
        "Harbin Institute of Technology, Shenzhen",
        "Leiden University"
    ]
}