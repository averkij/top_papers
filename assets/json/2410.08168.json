{
    "paper_title": "ZeroComp: Zero-shot Object Compositing from Image Intrinsics via Diffusion",
    "authors": [
        "Zitian Zhang",
        "Frédéric Fortier-Chouinard",
        "Mathieu Garon",
        "Anand Bhattad",
        "Jean-François Lalonde"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present ZeroComp, an effective zero-shot 3D object compositing approach that does not require paired composite-scene images during training. Our method leverages ControlNet to condition from intrinsic images and combines it with a Stable Diffusion model to utilize its scene priors, together operating as an effective rendering engine. During training, ZeroComp uses intrinsic images based on geometry, albedo, and masked shading, all without the need for paired images of scenes with and without composite objects. Once trained, it seamlessly integrates virtual 3D objects into scenes, adjusting shading to create realistic composites. We developed a high-quality evaluation dataset and demonstrate that ZeroComp outperforms methods using explicit lighting estimations and generative techniques in quantitative and human perception benchmarks. Additionally, ZeroComp extends to real and outdoor image compositing, even when trained solely on synthetic indoor data, showcasing its effectiveness in image compositing."
        },
        {
            "title": "Start",
            "content": "ZEROCOMP: Zero-shot Object Compositing from Image Intrinsics via Diffusion Zitian Zhang1 Frederic Fortier-Chouinard1 Mathieu Garon2 Jean-Francois Lalonde1 Anand Bhattad3 1Universite Laval, 2Depix Technologies, 3Toyota Technological Institute at Chicago 4 2 0 O 0 1 ] . [ 1 8 6 1 8 0 . 0 1 4 2 : r (a) Target image (b) Intrinsic maps (c) Predicted composite Figure 1. From (a) target background image and (b) available intrinsic maps (depth, normals, albedo) rendered from 3D model, our method ZEROCOMP generates (c) realistic composite, without access to the scene geometry or lighting, and without being trained specifically for object compositing. ZEROCOMP realistically shades the object and adds compelling shadow."
        },
        {
            "title": "Abstract",
            "content": "We present ZEROCOMP, an effective zero-shot 3D object compositing approach that does not require paired composite-scene images during training. Our method leverages ControlNet to condition from intrinsic images and combines it with Stable Diffusion model to utilize its scene priors, together operating as an effective rendering engine. During training, ZEROCOMP uses intrinsic images based on geometry, albedo, and masked shading, all without the need for paired images of scenes with and without composite objects. Once trained, it seamlessly integrates virtual 3D objects into scenes, adjusting shading to create realistic composites. We developed high-quality evaluation dataset and demonstrate that ZEROCOMP outperforms methods using explicit lighting estimations and generative techniques in quantitative and human perception benchmarks. Additionally, ZEROCOMP extends to real and outdoor image compositing, even when trained solely on synthetic indoor data, showcasing its effectiveness in image compositing. 1. Introduction Compositing 3D objects into real photographs has become an essential task in diverse fields such as image editing and visual effects. To achieve high realism, the virtual objectdefined as 3D asset with geometry, texture, etc. must interact with the lighting and elements from the real target scene. To this end, Debevec [18] introduced imagebased lighting (IBL), three-step approach for compositing of 3D objects into real images: 1) capture the high dynamic range (HDR) lighting properties of the target scene to use as virtual light source; 2) approximate the surrounding target scene geometry to catch virtual shadows; and 3) render and composite the virtual object into the scene. Since then, several methods have been proposed to improve upon each of these steps, but the overall approach has remained the same. For example, lighting estimation methods [22] have replaced the need for capturing light probes; depth estimation [4] and camera auto-calibration [32] can approximate the target scene geometry. Recent works [12, 85] has departed from this three-step procedure and leveraged pre-trained diffusion models such as Stable Diffusion [58] (SD). This creates simpler, singlepass approach that creates highly realistic images. However, the object is often adjusted, rotated, or even deformed by the SD model, which leads to unpredictable results. Addressing the limitations of current IBLand SD-based compositing methods, we propose ZEROCOMP, which merges the generative power of SD models with the precision of the IBL framework to enable realistic compositing of virtual 3D objects into images. Central to ZEROCOMP is training ControlNet [86] that leverages pre-trained SD backbone to render images from an intrinsic decomposition of given scene. This training results in zero-shot 1 compositing, allowing object insertion into diverse scenes without specific prior training. Consequently, ZEROCOMP ensures realistic compositing of virtual objects, preserving their shape, pose, and texture, without requiring paired training dataset. This zero-shot capability stems from our rendering-focused training strategy, offering compositing solution within single framework. To achieve this, our key idea is to train ZEROCOMP on simpler proxy task: given decomposition of an image into its intrinsic componentsdepth, normals, albedo, and partial input shadinggenerate fully relit image. This network is trained using synthetic datasets like OpenRooms [44] or InteriorVerse [91]. At inference, new 3D objects are inserted into the depth, normals, and albedo layers as naive composites. The trained model then generates fullyshaded version of the object, faithful to the scene lighting while retaining object identity. In short, ZEROCOMP acts as neural renderer, specifically trained to generate illumination effects such as shading and cast shadows  (Fig. 1)  . To rigorously assess our approach, we compile meticulously curated dataset for evaluating image compositing, utilizing 3D object assets from the Amazon Berkeley Object dataset [15] and background scenes from the Laval HDR Indoor dataset [22]. Extensive evaluation shows that our method competes well with traditional methods relying on explicit lighting estimation and surpasses other SD-based strategies in realism and faithfulness to the object. Given that quantitative metrics often do not correlate well with human perception [27], we conducted user study that demonstrated our approach significantly outshines all other methods. In summary our contributions are: 1) we present ZEROCOMP for zero-shot object compositing, where ControlNet is trained as neural renderer, conditioned on intrinsic layers; 2) novel test dataset for evaluating 3D object compositing methods; 3) thorough evaluation unifying several methods from the recent literature, including lighting estimation, shadow generation, diffusion models and harmonization approaches; and 4) user study demonstrating the performance of ZEROCOMP at generating renders more perceptually pleasing than state-of-the-art methods. 2. Related work Image compositing has long history in vision and graphics. Of note, Reinhard et al. [55] studied color-based harmonization, later revisited in [52]. Issuing from the pioneering work by Burt and Adelson [9], seamless object blending has been studied in [49,59,67]. Lalonde et al. [41] inserted compatible objects from large database into target scene. Karsch et al. [33, 34] inserted synthetic 3D objects through the estimation of geometry and lighting annotations. Since then, image compositing techniques have approached the problem along different dimensions. Using light estimators. This body of work achieves compositing by using HDR lighting for scenes and relighting virtual 3D objects within them. Learning-based methods estimate illumination through both non-parametric models [22] and parametric models [21, 30]. Recent advancements enable editable illumination by merging parametric and non-parametric approaches for lighting adjustments [43, 70, 72]. Efforts to tackle complex scene lighting have led to methods for estimating spatially-varying illumination [13, 23, 24, 42, 65, 92]. Unlike these methods, ZEROCOMP does not rely on explicit lighting estimation and rendering; instead, it jointly learns these tasks. Intrinsic images, or the process of factorizing an image into albedo and shading, has long been studied [3]. More recent approaches also recover other intrinsic maps such as depth and surface normals [20]. Several approaches try to achieve better decomposition through incorporating human annotations [39, 73, 90], ordinal information [10, 94], physical insights [54,79], shade tree structure [25], or multi-view information [50, 78]. Other methods [13, 42, 92] estimate shape, spatially-varying lighting, and non-Lambertian surface reflectance for improved compositing. Recent methods have utilized pretrained StyleGAN or SD for the extraction of intrinsic images, either via latent search [6], low-rank adaptations [19], through probabilistic formulation [38], or though ControlNet [47]. Concurrently, RGBX [83] decomposes images into intrinsic maps and synthesizes them using material and lighting-aware diffusion models. Intrinsic images can be used to reshade inserted objects [5]. This can be extended by adjusting the shading in the foreground and background separately for compatible composites [11]. StyLitGAN [7, 8] relits real images by manipulating StyleGAN latent space. LightIt [36] controls lighting using intrinsics in diffusion models. Similarly, we use intrinsic images for compositing and leverage scene priors through pretrained diffusion models. Shadows. Compositing accurate shadows is crucial for photorealism [14]. Recent advances include shadow generation [46] and harmonization [69] models. Techniques by Sheng et al. [6163] use pixel height information to generate various light effects, including shadows. In contrast, our approach implicitly learns to generate realistic shadows without shadow-specific supervision, eliminating the need for separate shadow generation or correction pipeline. Image harmonization. Methods aiming to harmonize the color distribution of inserted objects for coherence with the background include [40, 52, 55, 66, 68]. Relevant methods also exploit intrinsic images for harmonization [11, 29]. These approaches typically focus on adjusting object colors and, unlike our work, cannot synthesize shadows. Using generative models. Large generative models like Stable Diffusion (SD) [58] have led to the development of image editing applications by adapting pretrained models through fine-tuning [31], or by learning adapters [77]. We 2 Figure 2. Overview of our zero-shot intrinsic compositing pipeline. The input background image xbg (top-left) is first converted to intrinsic layers ibg using specialized networks (top, in yellow). In parallel, the corresponding intrinsic layers of the 3D object iobjexcept the shadingare rendered using graphics engine (middle, in blue). Layers are then composited together to obtain the composited intrinsics icomp (bottom, in green). From this, our trained ZEROCOMP renders the final composite (top-right). employ ControlNet [86] to train model that accepts intrinsic images and learns to render the final image. Recent methods in object compositing have largely leveraged SD. 3DIT [48] uses language instructions for object insertion but requires paired images for training. CustomNet [80] manipulates viewpoint, location, and background, while PhD [88] employs two-step harmonization process. ControlCom [85] and ObjectStitch [64] focus on embedding manipulation. Paint by Example [75] uses exemplar-guided compositing, and AnyDoor [12] uses identity tokens and frequency-aware feature extractor for detailed object representation. More recent works such as DiffusionLight [51] (lighting estimation), Alchemist [60] (material control), DiLightNet [82] (object rendering), and others [53, 89] (multi-view relighting) rely on specificallyIn contrast, our method uniquely designed training sets. integrates intrinsic image handling with ControlNet [86], enabling zero-shot 3D object compositing without explicit feature manipulation or multi-step processes. 3. ZEROCOMP ZEROCOMP is neural renderer that leverages the power of Stable Diffusion [58] and ControlNet [86], which is trained to render images from their intrinsic maps, namely depth, normals, albedo and shading. The distinctive aspect of our model is its ability to integrate 3D objects into 2D images without requiring training on paired images of scenes both with and without the objects. We refer to this capability as zero-shot compositing, allowing the model to perform compositing tasks it hasnt been explicitly trained for. crucial element of ZEROCOMP involves training it to develop an implicit understanding of the lighting and geometry from the intrinsic maps, enabling it to correct the appearance of the shading where it is missing. During inference, objects can be inserted into scenes using their albedo, normals, and depth maps. For the shading component, regions corresponding to the inserted objects are masked, and our trained ZEROCOMP adjusts this shading to align with the scenes original lighting. An overview of our zero-shot intrinsic compositing approach is illustrated in Fig. 2. 3.1. Training ZEROCOMP The training process is meticulously designed to handle the intrinsic components of images where shading is partially available. The model learns to reconstruct scenes from provided intrinsic maps while being conditioned on randomly masked regions within the shading channels. This approach encourages the model to reason about the scenes lighting and geometry autonomously. Training data. The synthetic OpenRooms [44] is used as sole training data. This dataset provides depth, normals, albedo, and partial shading information (division between the image and albedo) necessary to understand and recreate complex scenes. Our method can be easily extended to additional intrinsic maps if available, such as roughness and metallic maps in InteriorVerse [91]. Shading masks. During training, random masks are generated using mix of random rectangles/circles (60% probability) and by removing or keeping the entire shading maps (30% and 10% probability resp., see supp.). Learning objectives and losses. The primary training objective is to accurately render images based on intrinsic maps while effectively inpainting missing shading information. This is formalized through loss function that conditions the rendering process on intrinsic maps and mask indicating regions for shading inpainting. To improve the fidelity of the hue in the background, we follow [45] and use zero terminal SNR, prediction, DDIM scheduling and trailing timestep selection. The loss function is defined as: = Et,x0,ϵ,s vt vt(xt, i, s, t)2 2 , where vt progressively evolves from image to noise over the denoising time steps (see [45]). xt is the generated image at time step t, = {id, in, ia, is} is the intrinsic conditions provided to the model, containing depth, normals, albedo and shading resp., and vt() the model prediction. Et,x0,ϵ,s denotes the expectation over: denoising time steps t, initial image x0, noise ϵ, and shading mask s. (1) By iteratively minimizing across various training steps, ZEROCOMP progressively refines its ability to render realistic images given image intrinsics. We use pretrained Stable Diffusion 2.1 as the backbone model and condition it on intrinsic inputs using ControlNet. The model is trained for 808k steps with batch size of 32 at resolution of 512 512 using 19,709 training samples. 3.2. Zero-shot object compositing using ZEROCOMP Our goal is to place objects into photographs to achieve seamless blend without prior access to intrinsic scene information. To accomplish this, we utilize available pretrained, off-the-shelf models that infer the intrinsic properties of the background ibg from the given photograph (Fig. 2, top). Specifically, we employ ZoeDepth [4] to extract depth maps from the input images. For normals, we leverage StableNormal [76]. Albedo is estimated using Intrinsic Image Diffusion (IID) [37]. The shading information is derived by dividing the original image with its albedo. For the objects we intend to insert (Fig. 2, middle), we use the Blender [16] graphics engine to render its intrinsic layers iobj, except shading which is unknown. As with traditional IBL, this allows the user full control over the object pose and location in the target image. Each intrinsic map from the background and object are then composited together through simple compositing ic,comp = ic,obj + (1 m) ic,bg , (2) where ic {id, in, ia, is} denotes one of the intrinsic maps and the object mask obtained from the graphics engine, resulting in set of composite intrinsics (Fig. 2, bottom). Since the depth scale of the object and the background may not match, we align the object footprint depth (planar projection on the vertical axis) with the background depth by fitting an affine transform to the footprint and applying it to id,obj. An object also affects surroundings (e.g., by casting shadows), so we mask any pixel from the shading map if the pixel estimated 3D position is within distance threshold = λ(max my,obj min my,obj) , (3) where my,obj represents the (3D) coordinate of pixel in the object mask (obtained from the depth map id,obj). This threshold is motivated by the fact that the length of shadow is typically proportional to the object height. In practice, we set the relative shading radius λ = 1.0, and explore different values in Sec. 5. Pixels in the shading map directly above the object are never masked, to avoid unnecessary shadows (on the ceiling, for instance). Finally, our trained ZEROCOMP is run on the composite intrinsics to obtain the final output (Fig. 2, right), where the newly added object appears as natural part of the original scene. This approach enables the insertion of objects into various scenes with realistic lighting interactions including reshading and casting shadows, achieving zero-shot compositing. For all our experiments, we use seed 469, which was shown in [74] to produce the highest-quality generations in SD among 1000 seeds. 3.3. Preserving background fidelity The ControlNet framework does not guarantee perfect reconstruction of the background image. As demonstrated in previous studies, small details are susceptible to loss [93]. To mitigate this, we take inspiration from the differential compositing framework of Debevec [18] and generate shadow opacity ratio from two predictions of ZEROCOMP. Denoting fθ(i) as full inference pass of the model on input intrinsic maps = {id, in, ia, is} (containing depth, normals, albedo and shading resp., c.f. Sec. 3.1), we compute the shadow opacity ratio: = fθ(icomp) fθ(ibg) , (4) where icomp and ibg are the intrinsic maps of the composite and background resp., see Eq. (2). Note that Eq. (4) is computed on grayscale and the result is clamped to [0, 1]. We fix the diffusion with the same seed and use the same shading mask on the background is to minimize discrepancies between both predictions. To further avoid unnecessary opacity unrelated to the object, we set the opacity to 1 if they are outside of the shading mask computed earlier 4 to fit its footprint in the support region, ensuring its bounding box is entirely within the camera frustum. Four random objects are rendered for each image, and those with inconsistent geometry, semantics, or unrealistic albedo are discarded, resulting in total of 213 high-quality images. Finally, we render the object using physically based rendering in Blender [16]. To account for spatially-varying indoor lighting, we warp the panorama by converting it to 3D mesh according to its depth map from [56], and reproject it at the center of the objects bounding box. This approach provides high-quality simulated ground truth, enabling the generation of many more scenes than the 20 available in [23]. The last column of Fig. 4 shows representative subset of realistic image composites. This dataset will be released publicly upon publication of the paper. 5. Evaluation In this section, we conduct comprehensive evaluation of ZEROCOMPs performance as neural renderer for zeroshot compositing. Leveraging the evaluation dataset introduced in Sec. 4, we quantitatively and qualitatively compare against state-of-the-art methods on range of metrics to offer multi-faceted assessment of image quality. We use standard metrics to compare composites with the simulated ground truth, including Peak Signal-to-Noise Ratio (PSNR), Root Mean Square Error (RMSE) and its scaleinvariant version (si-RMSE), Mean Absolute Difference (MAE), Structural Similarity Index Measure (SSIM) [71], Learned Perceptual Image Patch Similarity (LPIPS) [87], and FLIP [1]. While we agree that perceptual metrics are better suited for our task, several researchers [26, 28, 35] highlight the vulnerability of neural network-based metrics like LPIPS to noise and adversarial attacks. To mitigate the influence of the rendering noise present in the training datasets [44, 91], we resize both the test images and references to 256 256 for all methods on LPIPS. Additionally, FLIP addresses this issue by applying spatial filter removing high frequency details imperceptible to humans. During evaluation, we demonstrate that our approach achieves performance comparable to most lighting estimation methods, all without explicitly modeling lighting conditions. We also contrast our method with diffusion and intrinsic imagebased baselines [11, 12, 46, 85], showcasing superiority on most metrics. Given that recent research shows that quantitative metrics do not correlate well with human perception [27, 81], we also conduct human perceptual study, revealing clear improvement over all other methods. This highlights ZEROCOMPs ability to produce perceptually plausible results. Finally, we showcase its extensions to material editing, outdoor scenes, and real-world 2D images. (a) xbg (b) fθ(ibg) (c) fθ(icomp) (d) (e) (f) Comp Figure 3. Overview of different components from our full compositing equation in eq. (5). For (a) given target background image xbg, diffusion models can create artifacts when rendering (b) background fθ(ibg) and (c) composite fθ(icomp) intrinsics. To alleviate this, we compute (d) the shadow opacity ratio of predictions and, together with (e) the object mask m, we can create (f) the final artifacts-free composite x. Please see the insets (top-right of each column) for zoomed-in view of the artifacts created. (c.f. Sec. 3.2). Gaussian blur with kernel 15 15 and σ = 1.5 is applied to to avoid blending artifacts. The final compositing equation is = (1 m) xbg + (icomp) , (5) where is color balance factor computed as the average color ratio of background xbg and the network output (icomp) to account for global color shifts. In Fig. 3, we compare our composition and direct output of the network. 4. Test dataset for 3D object compositing Evaluating the quality of 3D object composites can be cumbersome, and performing uniform evaluation of various methods such as lighting estimation, harmonization, or generative techniques is challenging. We require scenes where 1) the background is real image with known HDR lighting, 2) the scene geometry is defined, 3) virtual object is correctly positioned, and 4) realistic rendering exists. Current evaluation datasets often lack some of these requirements: [12] lack object geometry, [23] does not include objects, [44, 57] use synthetic imagery, and 3D CopyPaste [24] lacks ground truth HDR lighting. Inspired by [24], we propose simple method for automatically generating dataset for evaluating 3D object compositing approaches. Specifically, we leverage the test dataset provided by [17, 72], which contains 2,240 images of 50 field of view, extracted from HDR environment maps in the Laval Indoor HDR Dataset [22], and the result from several lighting estimation methods. For each image, we first find suitable location to insert virtual object by computing normals using DSINE [2] on each crop. We detect support region by selecting normals with an angle less than 15 with the up vector. Images with support regions too small (defined as not fitting circle of 75 pixels radius) are discarded, resulting in 228 admissible background crops. Next, 3D object is chosen at random from the ABO dataset [15] and randomly rotated about its vertical axis. The object is scaled 5 Background Garon19 [23] EMLight [84] ARShadowGAN [46] ControlCom [85] Ours Simulated GT Figure 4. Qualitative comparison with lighting estimation and image-based methods. Results are sorted from worst (top) to best (bottom) PSNR for Ours. Please zoom in and refer to the supplementary material for additional images and methods. 5.1. Lighting estimation method comparison Method PSNR RMSE si-RMSE SSIM MAE LPIPS FLIP Traditional lighting-based compositing methods set the benchmark by estimating scene lighting for realistic 3D object insertion. These methods use full 3D object, delicately curated model for shadow casting, physicallybased rendering engine, and suitable lighting representation (e.g., parametric lights [21, 22, 72], spherical functions [17,23,84], etc.). For optimal results, everything must be perfectly aligned. In contrast, ZEROCOMP only requires placing the object in 2D, generating intrinsics using simple shaders (depth, normals, and albedo), and relies on the network understanding to infer missing information. Despite the task is more challenging, ZEROCOMP achieves competitive results, surpassing many explicit lighting-based techniques [22, 70, 72], as shown in Tab. 1. Qualitative comparisons in Fig. 4 show that ZEROCOMP realistically shades these objects while maintaining their appearance, acting as strong contender to traditional approaches. Gardner17 [22] Garon19 [23] Gardner19 [21] Everlight [17] StyleLight [70] Weber22 [72] EMLight [84] a - t L 25.9 34.2 32.3 33.3 29.3 29.6 32.7 AnyDoor [12] 24.5 25.5 ControlCom [85] Careaga23 [11] 26.6 ARShadowGAN [46] 27.4 b - m ZEROCOMP OR ZEROCOMP IV 31.7 33.0 0.0677 0.0256 0.0293 0.0290 0.0416 0.0403 0.0301 0.0666 0.0566 0.0527 0.0484 0.0303 0. 0.0606 0.0251 0.0281 0.0285 0.0399 0.0380 0.0297 0.0657 0.0554 0.0498 0.0467 0.0295 0.0254 0.967 0.0211 0.0331 0.0664 0.986 0.0089 0.0175 0.0440 0.984 0.0091 0.0211 0.0450 0.982 0.0102 0.0184 0.0462 0.976 0.0139 0.0287 0.0580 0.980 0.0130 0.0239 0.0556 0.981 0.0104 0.0218 0.0471 0.883 0.0265 0.0822 0.1098 0.866 0.0283 0.0711 0.1516 0.965 0.0192 0.0347 0.0884 0.907 0.0213 0.0584 0.0994 0.970 0.0109 0.0269 0.0538 0.973 0.0091 0.0246 0. Table 1. Quantitative evaluation. All metrics are computed on the whole image. Different sections indicate methods, from top to bottom: lighting estimation, image-based compositing and ours. OR and IV refer to OpenRooms [44] and InteriorVerse [91]. 5.2. Image-based compositing method comparison"
        },
        {
            "title": "Our evaluation extends to methods that employ intrinsic\nimage decomposition and generative modeling for object",
            "content": "6 Method Confusion (%) ZEROCOMP OR (ours) EMLight [84] ZEROCOMP IV Garon19 [23] Everlight [17] ControlCom [85] Careaga23 [11] ARShadowGAN [46] 45.0 3.9 41.5 3.9 35.7 3.7 * 31.5 3.6 * 31.4 3.6 * 19.9 3.1 * 5.0 1.7 * 4.8 1.7 * Table 2. Results of our 2AFC user study indicates the perceived realism of the composites, sorted by decreasing confusion (perfect confusion is 50%), and 95% confidence intervals (where * indicate statistically significant difference with ZEROCOMP OR). compositing. Recent methods like AnyDoor [12] and ControlCom [85] similarly employ generative framework with Stable Diffusion backbone, whereas Careaga et al. [11] also rely on intrinsic image decomposition for compositing. Finally, we include ARShadowGAN [46] as an example of shadow generation techniques. All of these methods expect as input an image of the object placed in different scene. We simulate this setup by rendering the object with randomly sampled environment map from our test set, and feeding it to the methods, letting them do the task of relighting appropriately based on the target background. The quantitative results in Tab. 1 (middle bracket) show superior score for ZEROCOMP in all metrics against imagebased compositing methods. We identify two main issues with other approaches: they tend to 1) modify the object itself or its pose [12, 85]; or 2) generate shadows of limited quality [11, 46]. In contrast, ZEROCOMP preserves the original appearance and pose of objects and generates complex and realistic shadows even without access to the full 3D model of the virtual object. 5.3. Human perceptual study While quantitative metrics provide measure of image quality, recent evidence has shown they do not correlate with human perception when evaluating the realism of composited images [27]. We therefore conduct two user studies to evaluate the perceived realism of the images produced by our method compared to other established approaches. We first designed two-alternative forced choice (2AFC) task where participants viewed series of image pairs. Each pair featured ground truth composite and prediction from one method, both showing the same object on the same background. We asked observers, The same virtual object has been inserted in these two images. Click on the image that looks the most realistic (see supp. for instructions). To reduce bias, we randomized the left/right placement of ground truth and predicted images. Each user evaluated 20 pairs per method, totaling 160 comparisons. We randomly sampled non-overlapping subset of the test set for each method. In this scenario, confusion rate of 50% would Radius Input Method λ = 0.5 λ = 1.0 (Ours) λ = 1.5 w/o depth and normals w/o normal w/o depth baseline PSNR SSIM 32.8 31.7 30.8 31.8 31.6 32.1 31.9 0.973 0.970 0.967 0.966 0.965 0.969 0.969 Table 3. Ablation study on the shading radius, different inputs. The baseline and input-ablated models are trained for 220k steps. indicate that users find the generated composites indistinguishable from the ground truth on average. We selected the three lighting estimation methods [17, 23, 84] and the three image-based methods [11,46,85] with the highest PSNR. total of = 47 observers participated in our study. As shown in Tab. 2, our method trained on OpenRooms achieves 45% confusion rate, indicating strong preference for the realism of our composites. ZEROCOMP trained on InteriorVerse (IV) doesnt perform as well, presumably due to weaker shadows. The method with the best quantitative score from Tab. 1, Garon19 [23], is ranked fourth with 31.5%, corroborating recent findings that image evaluation metrics do not correlate well with human perception [27]. We achieve statistically significant better results against all methods, except with EMLight. We therefore conduct second user study where = 19 participants were shown 100 pairs of images, each pair containing one result from ZEROCOMP and the other from EMLight, in randomized order. Users selected our method 55.4 2.2% of the time, demonstrating preference for ZEROCOMP. 5.4. Ablations In Tab. 3, we ablate shading radius (Radius), various intrinsic maps as input (Input). The quantitative difference due to the shading radius is confirmed visually in Fig. 5. However, metrics contradict visual observations when it comes to using different inputs. From Fig. 6, not using the depth or normal maps results in loss of realism. 5.5. Extensions Material editing. By training on additional intrinsic maps such as roughness and metallic available in the InteriorVerse dataset [91], ZEROCOMP can also adjust the materials of the virtual object. In Fig. 7, we modify the roughness (RG) and metallicity (MT) to demonstrate the effectiveness of ZEROCOMP in handling more advanced materials. Outdoor images. Despite being trained only on indoor imagery, ZEROCOMP also generalizes to outdoor scenes and can generate realistic shadows, as demonstrated in Fig. 8. 2D object compositing. ZEROCOMP can also be applied to 2D objects segmented from real images, where 3D λ = 0.5 λ = 1.0 λ = 2.0 Figure 5. Effect of the shading mask radius λ. Generated images (top) and their associated masked shading maps (bottom) are shown. small radius (λ = 0.5) results in unrealistic shadow shapes, while large radius (λ = 2.0) produces overly large shadows and loss of shading detail in the scene. ia {ia, in} {ia, id} {ia, in, id} Figure 6. Ablation on intrinsic conditionings: Training using different combinations of depth (id) and normals (in), while consistently retaining the albedo (ia). Normals aid the network in generating sharp object details, whereas depth enhances shadow strength. Using both conditionings provides optimal results. Original RG = 0, = 1 Original RG = 0, = 1 Figure 7. Training ZEROCOMP on InteriorVerse [91] significantly enhances its performance with shiny objects by allowing precise control over roughness and metallic properties. Figure 8. ZEROCOMP generalizes to outdoor scenes, despite being trained exclusively on indoor scenes. Note how the object shading and cast shadows seamlessly blend with the target background. Figure 9. Using ZEROCOMP to composite real 2D objects without access to 3D model. Intrinsic maps for both the object image and the target background are estimated separately, then composited together and fed to our pipeline. Examples are displayed left to right: object, target background, and predicted composite. the image texture while keeping the rest of the pipeline unchanged. For demonstration purposes, the object was segmented and placed in the target image manually. Fig. 9 shows several such examples, showing our method can be easily extended to the case of 2D object compositing. 6. Discussion We present ZEROCOMP, novel approach for creating realistic image composites with intricate lighting interactions between virtual objects and scenes. Our method achieves zero-shot compositing by training on the simpler proxy task of reconstructing an image from its intrinsic layers using readily available datasets, simplifying the training procedure. Moreover, we present comprehensive evaluation dataset for 3D object composition in real images, where our method exhibits favorable performance compared to various light estimation and generative techniques. Through an extensive user study using the same dataset, we demonstrate that ZEROCOMP achieves the highest perceptual scores among recent methods. This also suggests the need for reliable quantitative metrics for lighting estimation. Limitations. ZEROCOMP depends on intrinsic estimators to decompose the background image. While the model shows robustness to these estimators (see supplement), even with synthetic training data, errors in their predictions can affect rendering quality. Nonetheless, ZEROCOMP has demonstrated impressive results when fully leveraging estimated albedo and shading for both the background and inserted objects (see Fig. 9), suggesting that non-synthetic training data could enhance the models robustness. model is not available. Here, we rely on intrinsic estimators (Sec. 3.2) to estimate the object depth and normals. We use the RGB as the albedo to avoid detrimental noise in Acknowledgements This research was supported by NSERC grants RGPIN 2020-04799 and ALLRP 5865438 23, Mitacs and Depix. Computing resources were provided by the Digital Research Alliance of Canada. The authors thank LouisEtienne Messier and Justine Giroux for their help as well as all members of the lab for discussions and proofreading help."
        },
        {
            "title": "References",
            "content": "[1] Pontus Andersson, Jim Nilsson, Tomas Akenine-Moller, Magnus Oskarsson, Kalle Astrom, and Mark Fairchild. Flip: difference evaluator for alternating images. Proc. ACM Comput. Graph. Interact. Tech., 3(2):151, 2020. 5 [2] Gwangbin Bae and Andrew J. Davison. Rethinking inductive biases for surface normal estimation. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 5 [3] Harry Barrow, Tenenbaum, Hanson, and Riseman. Recovering intrinsic scene characteristics. Comput. vis. syst, 2(3-26):2, 1978. 2 [4] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. 1, 4 [5] Anand Bhattad and David Forsyth. Cut-and-paste object insertion by enabling deep image prior for reshading. In Int. Conf. 3D Vis., 2022. 2 [6] Anand Bhattad, Daniel McKee, Derek Hoiem, and David Forsyth. Stylegan knows normal, depth, albedo, and more. Advances in Neural Information Processing Systems, 36, 2023. 2 [7] Anand Bhattad, Viraj Shah, Derek Hoiem, and David Forsyth. Make it so: Steering stylegan for any image inversion and editing. arXiv preprint arXiv:2304.14403, 2023. [8] Anand Bhattad, James Soole, and DA Forsyth. Stylitgan: In IEEE Conf. Image-based relighting via latent control. Comput. Vis. Pattern Recog., 2024. 2 [9] Peter Burt and Edward Adelson. multiresolution spline with application to image mosaics. ACM Trans. Graph., 2(4):217236, 1983. 2 [10] Chris Careaga and Yagız Aksoy. Intrinsic image decomposition via ordinal shading. ACM Trans. Graph., 43(1):124, 2023. 2 [11] Chris Careaga, Mahdi Miangoleh, and Yagız Aksoy. Intrinsic harmonization for illumination-aware compositing. ACM Trans. Graph., 2023. 2, 5, 6, 7 [12] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. arXiv preprint arXiv:2307.09481, 2023. 1, 3, 5, 6, [13] JunYong Choi, SeokYeong Lee, Haesol Park, Seung-Won Jung, Ig-Jae Kim, and Junghyun Cho. Mair: multi-view attention inverse rendering with 3d spatially-varying lighting estimation. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. 2 [14] Yung-Yu Chuang, Dan Goldman, Brian Curless, David Salesin, and Richard Szeliski. Shadow matting and compositing. In ACM SIGGRAPH Conf., 2003. 2 [15] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, and Jitendra Malik. ABO: Dataset and benchmarks for real-world 3d object understanding. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 2, 5 [16] Blender Online Community. Blender - 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. 4, 5 [17] Mohammad Reza Karimi Dastjerdi, Jonathan Eisenmann, Yannick Hold-Geoffroy, and Jean-Francois Lalonde. Everlight: Indoor-outdoor editable HDR lighting estimation. In Int. Conf. Comput. Vis., 2023. 5, 6, 7 [18] Paul Debevec. Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography. In Conf. Comp. Graph. Int. Tech., ACM SIGGRAPH Conf., pages 189198, 1998. 1, 4 [19] Xiaodan Du, Nicholas Kolkin, Greg Shakhnarovich, and Anand Bhattad. Generative models: What do they know? arXiv preprint do they know things? arXiv:2311.17137, 2023. lets find out! [20] Elena Garces, Carlos Rodriguez-Pardo, Dan Casas, and Jorge Lopez-Moreno. survey on intrinsic images: Delving deep into lambert and beyond. International Journal of Computer Vision, 130(3):836868, 2022. 2 [21] Marc-Andre Gardner, Yannick Hold-Geoffroy, Kalyan Sunkavalli, Christian Gagne, and Jean-Francois Lalonde. In Int. Conf. Deep parametric indoor lighting estimation. Comput. Vis., 2019. 2, 6 [22] Marc-Andre Gardner, Kalyan Sunkavalli, Ersin Yumer, Xiaohui Shen, Emiliano Gambaretto, Christian Gagne, and Jean-Francois Lalonde. Learning to predict indoor illumination from single image. ACM Trans. Graph., 9(4), 2017. 1, 2, 5, 6 [23] Mathieu Garon, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr, and Jean-Francois Lalonde. Fast spatially-varying indoor lighting estimation. In IEEE Conf. Comput. Vis. Pattern Recog., 2019. 2, 5, 6, 7 [24] Yunhao Ge, Hong-Xing Yu, Cheng Zhao, Yuliang Guo, Xinyu Huang, Liu Ren, Laurent Itti, and Jiajun Wu. 3d copypaste: Physically plausible object insertion for monocular 3d detection. In Adv. Neural Inform. Process. Syst., 2024. 2, 5 [25] Chen Geng, Hong-Xing Yu, Sharon Zhang, Maneesh Agrawala, and Jiajun Wu. Tree-structured shading decomposition. In Int. Conf. Comput. Vis., pages 488498, 2023. [26] Sara Ghazanfari, Alexandre Araujo, Prashanth Krishnamurthy, Farshad Khorrami, and Siddharth Garg. Lipsim: In Int. Conf. provably robust perceptual similarity metric. Learn. Represent., 2024. 5 [27] Justine Giroux, Mohammad Reza Karimi Dastjerdi, Yannick Hold-Geoffroy, Javier Vazquez-Corral, and Jean-Francois Lalonde. Towards perceptual evaluation framework for In IEEE Conf. Comput. Vis. Pattern lighting estimation. Recog., 2024. 2, 5, 7 9 [28] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. arXiv Explaining and harnessing adversarial examples. preprint arXiv:1412.6572, 2014. 5 [29] Zonghui Guo, Haiyong Zheng, Yufeng Jiang, Zhaorui Gu, In IEEE Intrinsic image harmonization. and Bing Zheng. Conf. Comput. Vis. Pattern Recog., 2021. 2 [30] Yannick Hold-Geoffroy, Kalyan Sunkavalli, Sunil Hadap, Emiliano Gambaretto, and Jean-Francois Lalonde. Deep outIn IEEE Conf. Comput. Vis. door illumination estimation. Pattern Recog., 2017. 2 [31] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank In Int. Conf. Learn. adaptation of large language models. Represent., 2021. 2 [32] Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver Wang, Kevin Blackburn-Matzen, Matthew Sticha, and David Fouhey. Perspective fields for single image camera calibration. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. 1 [33] Kevin Karsch, Varsha Hedau, David Forsyth, and Derek Hoiem. Rendering synthetic objects into legacy photographs. ACM Trans. Graph., 30(6):112, 2011. [34] Kevin Karsch, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr, Hailin Jin, Rafael Fonte, Michael Sittig, and David Forsyth. Automatic scene inference for 3d object compositing. ACM Trans. Graph., 33(3):115, 2014. 2 [35] Markus Kettunen, Erik Harkonen, and Jaakko Lehtinen. Elpips: robust perceptual image similarity via random transarXiv preprint arXiv:1906.03973, formation ensembles. 2019. 5 [36] Peter Kocsis, Julien Philip, Kalyan Sunkavalli, Matthias Nießner, and Yannick Hold-Geoffroy. Lightit: Illumination modeling and control for diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 2 [37] Peter Kocsis, Vincent Sitzmann, and Matthias Nießner. Intrinsic image diffusion for indoor single-view material estimation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 51985208, 2024. 4 [38] Peter Kocsis, Vincent Sitzmann, and Matthias Nießner. Intrinsic image diffusion for single-view material estimation. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 2 [39] Balazs Kovacs, Sean Bell, Noah Snavely, and Kavita Bala. Shading annotations in the wild. In IEEE Conf. Comput. Vis. Pattern Recog., pages 69987007, 2017. [40] Jean-Francois Lalonde and Alexei Efros. Using color compatibility for assessing image realism. In Int. Conf. Comput. Vis., 2007. 2 [41] Jean-Francois Lalonde, Derek Hoiem, Alexei Efros, Carsten Rother, John Winn, and Antonio Criminisi. Photo clip art. ACM Trans. Graph., 26(3), 2007. 2 [42] Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker. Inverse rendering for complex indoor scenes: Shape, spatially-varying lighting and svbrdf from single image. In IEEE Conf. Comput. Vis. Pattern Recog., 2020. 2 [43] Zhengqin Li, Jia Shi, Sai Bi, Rui Zhu, Kalyan Sunkavalli, Miloˇs Haˇsan, Zexiang Xu, Ravi Ramamoorthi, and Manmohan Chandraker. Physically-based editing of indoor scene 10 lighting from single image. 2022. In Eur. Conf. Comput. Vis., [44] Zhengqin Li, Ting-Wei Yu, Shen Sang, Sarah Wang, Meng Song, Yuhan Liu, Yu-Ying Yeh, Rui Zhu, Nitesh Gundavarapu, Jia Shi, et al. Openrooms: An end-to-end open framework for photorealistic indoor scene datasets. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. 2, 3, 5, 6 [45] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In IEEE/CVF Winter Conf. App. Comp. Vis., 2024. 4 [46] Daquan Liu, Chengjiang Long, Hongpan Zhang, Hanning Yu, Xinzhi Dong, and Chunxia Xiao. Arshadowgan: Shadow generative adversarial network for augmented reality in single light scenes. In IEEE Conf. Comput. Vis. Pattern Recog., 2020. 2, 5, 6, 7 [47] Jundan Luo, Duygu Ceylan, Jae Shin Yoon, Nanxuan Zhao, Julien Philip, Anna Fruhstuck, Wenbin Li, Christian Richardt, and Tuanfeng Wang. Intrinsicdiffusion: Joint inIn ACM SIGtrinsic layers from latent diffusion models. GRAPH Conf., pages 111, 2024. 2 [48] Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, and Tanmay Gupta. Object 3dit: Language-guided 3d-aware image editing. In Adv. Neural Inform. Process. Syst., 2024. 3 [49] Patrick Perez, Michel Gangnet, and Andrew Blake. Poisson image editing. ACM Trans. Graph., 22(3), 2023. 2 [50] Julien Philip, Michael Gharbi, Tinghui Zhou, Alexei Efros, and George Drettakis. Multi-view relighting using geometry-aware network. ACM Trans. Graph., 38(4):781, 2019. 2 [51] Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet, Varun Jampani, Amit Raj, Pramook Khungurn, and Supasorn Suwajanakorn. Diffusionlight: In IEEE Light probes for free by painting chrome ball. Conf. Comput. Vis. Pattern Recog., 2024. 3 [52] Francois Pitie, Anil Kokaram, and Rozenn Dahyot. Ndimensional probability density function transfer and its application to color transfer. In Int. Conf. Comput. Vis., 2005. 2 [53] Yohan Poirier-Ginter, Alban Gauthier, Julien Phillip, JeanFrancois Lalonde, and George Drettakis. diffusion approach to radiance field relighting using multi-illumination synthesis. Comput. Graph. Forum, 43(4), 2024. 3 [54] Mani Ramanagopal, Sriram Narayanan, Aswin Sankaranarayanan, and Srinivasa Narasimhan. theory of joint light and heat transport for lambertian scenes. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1192411933, 2024. [55] Erik Reinhard, Michael Adhikhmin, Bruce Gooch, and Peter Shirley. Color transfer between images. IEEE Comp. Graph. Appl., 21(5):3441, 2001. 2 [56] Manuel Rey-Area, Mingze Yuan, and Christian Richardt. 360monodepth: High-resolution 360 monocular depth estimation. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 5 [57] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In Int. Conf. Comput. Vis., 2021. 5 [58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 1, 2, 3 [59] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. interactive foreground extraction using iterated Grabcut graph cuts. ACM Trans. Graph., 23(3):309314, 2004. 2 [60] Prafull Sharma, Varun Jampani, Yuanzhen Li, Xuhui Jia, Dmitry Lagun, Fredo Durand, Bill Freeman, and Mark Matthews. Alchemist: Parametric control of material propIn IEEE Conf. Comput. Vis. erties with diffusion models. Pattern Recog., 2024. 3 [61] Yichen Sheng, Yifan Liu, Jianming Zhang, Wei Yin, Cengiz Oztireli, He Zhang, Zhe Lin, Eli Shechtman, and Bedrich Benes. Controllable shadow generation using pixel height maps. In Eur. Conf. Comput. Vis., 2022. 2 [62] Yichen Sheng, Jianming Zhang, and Bedrich Benes. Ssn: Soft shadow network for image compositing. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. 2 [63] Yichen Sheng, Jianming Zhang, Julien Philip, Yannick HoldGeoffroy, Xin Sun, He Zhang, Lu Ling, and Bedrich Benes. Pixht-lab: Pixel height based light effect generation for image compositing. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. 2 [64] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. Objectstitch: Generative object compositing. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 3 [65] Pratul Srinivasan, Ben Mildenhall, Matthew Tancik, Jonathan Barron, Richard Tucker, and Noah Snavely. Lighthouse: Predicting lighting volumes for spatiallycoherent illumination. In IEEE Conf. Comput. Vis. Pattern Recog., 2020. [66] Kalyan Sunkavalli, Micah Johnson, Wojciech Matusik, and Hanspeter Pfister. Multi-scale image harmonization. ACM Trans. Graph., 29(4), 2010. 2 [67] Michael Tao, Micah Johnson, and Sylvain Paris. Errortolerant image compositing. Int. J. Comput. Vis., 103:178 189, 2013. 2 [68] Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Xin Lu, and Ming-Hsuan Yang. Deep image harmonization. In IEEE Conf. Comput. Vis. Pattern Recog., 2017. 2 [69] Lucas Valenca, Jinsong Zhang, Michael Gharbi, Yannick Hold-Geoffroy, and Jean-Francois Lalonde. Shadow harmonization for realistic compositing. In ACM SIGGRAPH Asia Conf., 2023. 2 [70] Guangcong Wang, Yinuo Yang, Chen Change Loy, and Ziwei Liu. Stylelight: HDR panorama generation for lighting estimation and editing. In Eur. Conf. Comput. Vis., 2022. 2, 6 [71] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Image quality assessment: from error visibilIEEE Trans. Image Process., Simoncelli. ity to structural similarity. 13(4):600612, 2004. 5 [72] Henrique Weber, Mathieu Garon, and Jean-Francois Lalonde. Editable indoor lighting estimation. In Eur. Conf. Comput. Vis., 2022. 2, 5, 6 [73] Jiaye Wu, Sanjoy Chowdhury, Hariharmano Shanmugaraja, David Jacobs, and Soumyadip Sengupta. Measured albedo in the wild: Filling the gap in intrinsics evaluation. In 2023 IEEE International Conference on Computational Photography (ICCP), pages 112. IEEE, 2023. 2 [74] Katherine Xu, Lingzhi Zhang, and Jianbo Shi. Good seed makes good crop: Discovering secret seeds in text-toimage diffusion models, 2024. 4 [75] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. 3 [76] Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, Stablenormal: Reducing diffusion and Xiaoguang Han. arXiv preprint variance for stable and sharp normal. arXiv:2406.16864, 2024. [77] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2 [78] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. In Int. Conf. Comput. Vis., pages 339351, 2023. 2 [79] Yusaku Yoshida, Ryo Kawahara, and Takahiro Okabe. Light source separation and intrinsic image decomposition under ac illumination. In IEEE Conf. Comput. Vis. Pattern Recog., pages 57355743, 2023. 2 [80] Ziyang Yuan, Mingdeng Cao, Xintao Wang, Zhongang Qi, Chun Yuan, and Ying Shan. Customnet: Zero-shot object customization with variable-viewpoints in text-to-image diffusion models. arXiv preprint arXiv:2310.19784, 2023. 3 [81] Syed Waqas Zamir, Javier Vazquez-Corral, and Marcelo Bertalmio. Vision models for wide color gamut imagIEEE Trans. Pattern Anal. Mach. Intell., ing in cinema. 43(5):17771790, 2019. 5 [82] Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, and Xin Tong. DiLightNet: Fine-grained lightIn ACM ing control for diffusion-based image generation. SIGGRAPH Conf., 2024. [83] Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, and Miloˇs Haˇsan. RGB X: Image decomposition and synthesis using material-and lighting-aware diffusion models. In ACM SIGGRAPH Conf., 2024. 2 [84] Fangneng Zhan, Changgong Zhang, Yingchen Yu, Yuan Chang, Shijian Lu, Feiying Ma, and Xuansong Xie. EMLight: Lighting Estimation via Spherical Distribution Approximation. In AAAI, 2021. 6, 7 [85] Bo Zhang, Yuxuan Duan, Jun Lan, Yan Hong, Huijia Zhu, Weiqiang Wang, and Li Niu. Controlcom: Controllable image composition using diffusion model. arXiv preprint arXiv:2308.10040, 2023. 1, 3, 5, 6, 7 11 [86] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Int. Conf. Comput. Vis., 2023. 1, 3 [87] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In IEEE Conf. Comput. Vis. Pattern Recog., 2018. [88] Xin Zhang, Jiaxian Guo, Paul Yoo, Yutaka Matsuo, and Yusuke Iwasawa. Paste, inpaint and harmonize via denoising: Subject-driven image editing with pre-trained diffusion model. arXiv preprint arXiv:2306.07596, 2023. 3 [89] Xiaoming Zhao, Pratul P. Srinivasan, Dor Verbin, Keunhong Park, Ricardo Martin Brualla, and Philipp Henzler. IllumiNeRF: 3D relighting without inverse rendering. ArXiv, 2024. 3 [90] Tinghui Zhou, Philipp Krahenbuhl, and Alexei Efros. Learning data-driven reflectance priors for intrinsic image decomposition. In Int. Conf. Comput. Vis., pages 34693477, 2015. 2 [91] Jingsen Zhu, Fujun Luan, Yuchi Huo, Zihao Lin, Zhihua Zhong, Dianbing Xi, Rui Wang, Hujun Bao, Jiaxiang Zheng, and Rui Tang. Learning-based inverse rendering of complex indoor scenes with differentiable monte carlo raytracing. In ACM SIGGRAPH Asia Conf. ACM, 2022. 2, 4, 5, 6, 7, 8 [92] Rui Zhu, Zhengqin Li, Janarbek Matai, Fatih Porikli, and Manmohan Chandraker. Irisformer: Dense vision transformers for single-image inverse rendering in indoor scenes. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 2 [93] Zixin Zhu, Xuelu Feng, Dongdong Chen, Jianmin Bao, Le Wang, Yinpeng Chen, Lu Yuan, and Gang Hua. Designing better asymmetric vqgan for stablediffusion. arXiv preprint arXiv:2306.04632, 2023. 4 [94] Daniel Zoran, Phillip Isola, Dilip Krishnan, and William Freeman. Learning ordinal relationships for mid-level vision. In Int. Conf. Comput. Vis., pages 388396, 2015."
        }
    ],
    "affiliations": [
        "Depix Technologies",
        "Toyota Technological Institute at Chicago",
        "Universite Laval"
    ]
}