{
    "paper_title": "Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration",
    "authors": [
        "Junyang Wang",
        "Haiyang Xu",
        "Xi Zhang",
        "Ming Yan",
        "Ji Zhang",
        "Fei Huang",
        "Jitao Sang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid increase in mobile device usage necessitates improved automation for seamless task management. However, many AI-driven frameworks struggle due to insufficient operational knowledge. Manually written knowledge helps but is labor-intensive and inefficient. To address these challenges, we introduce Mobile-Agent-V, a framework that leverages video guidance to provide rich and cost-effective operational knowledge for mobile automation. Mobile-Agent-V enhances task execution capabilities by leveraging video inputs without requiring specialized sampling or preprocessing. Mobile-Agent-V integrates a sliding window strategy and incorporates a video agent and deep-reflection agent to ensure that actions align with user instructions. Through this innovative approach, users can record task processes with guidance, enabling the system to autonomously learn and execute tasks efficiently. Experimental results show that Mobile-Agent-V achieves a 30% performance improvement compared to existing frameworks."
        },
        {
            "title": "Start",
            "content": "Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration Junyang Wang1*, Haiyang Xu2, Xi Zhang2, Ming Yan2, Ji Zhang2, Fei Huang2, Jitao Sang1, 1Beijing Jiaotong University, 2Alibaba Group, {junyangwang, jtsang}@bjtu.edu.cn {shuofeng.xhy, ym119608}@alibaba-inc.com 5 2 0 2 4 2 ] . [ 1 0 1 1 7 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The rapid increase in mobile device usage necessitates improved automation for seamless task management. However, many AI-driven frameworks struggle due to insufficient operational knowledge. Manually written knowledge helps but is labor-intensive and inefficient. To address these challenges, we introduce Mobile-Agent-V, framework that leverages video guidance to provide rich and costeffective operational knowledge for mobile automation. Mobile-Agent-V enhances task execution capabilities by leveraging video inputs without requiring specialized sampling or preprocessing. Mobile-Agent-V integrates sliding window strategy and incorporates video agent and deep-reflection agent to ensure that actions align with user instructions. Through this innovative approach, users can record task processes with guidance, enabling the system to autonomously learn and execute tasks efficiently. Experimental results show that MobileAgent-V achieves 30% performance improvement compared to existing frameworks."
        },
        {
            "title": "Introduction",
            "content": "The reliance on mobile devices has increased, with users performing numerous operations daily, underscoring the need for streamlined interactions. Currently, artificial intelligence advances mobile automation, enhancing productivity research. Systems such as ChatGPT and Claude enable devices to autonomously handle tasks based on user input. The development of Multimodal Large Language Models (MLLMs) has notably improved mobile device operating frameworks, using these models as intelligent agents (Liu et al., 2023b; Zhu et al., 2023; Ye et al., 2023a; Dai et al., 2023; Liu et al., 2023a; Chen et al., 2023; Bai et al., 2023; Ye et al., 2023b; Wang et al., 2023; Lu et al., 2024a; *Work done during internship at Alibaba Group. Corresponding author 1 Figure 1: Comparison between baseline agent, manually written knowledge, and Mobile-Agent-V. The baseline agent, lacking operation knowledge, struggles to complete the task, requiring excessive steps and still failing. Manually written knowledge requires documentation and iterative verification. In contrast, MobileAgent-V leverages operation videos, requiring only execution and recording, making knowledge injection far more efficient. Ye et al., 2024; Wu et al., 2024). These frameworks leverage agents perception, decision-making, and reflection to perform complex tasks across multiple applications, thereby broadening mobile devices autonomous capabilities. Despite progress, existing approaches remain constrained by limited operational knowledge. As shown in Figure 1, current agents struggle with tasks like disabling location recording during photography, even after extensive exploration. This limitation arises from the lack of comprehensive training data, the rapid obsolescence of learned knowledge due to app updates, and the inaccessibility of device-specific operational information. While methods such as Odyssey leverage external task paths, they face scalability and data collection challenges (Lu et al., 2024b). AppAgents selfexploration is costly due to lengthy task sequences (Yang et al., 2023), and Mobile-Agent-V2s reliance on manual programming remains inefficient (Wang et al., 2024b). These challenges underscore the need for more scalable and adaptable solutions in mobile automation. To address these limitations, we propose video guidance as solution. Unlike manually written knowledge, which requires extensive exploration, documentation, and iterative debugging, recording an operation video locally takes only about one minute. In contrast, an agent exploring the task requires numerous interaction steps and substantial time, while manually writing and verifying instructions demands both significant human effort and time. Thus, using videos as means of knowledge injection offers greater scalability for large-scale deployment. As shown in Figuree 1, under the guidance of the video, the agent can efficiently complete instructions that were originally impossible to complete. However, processing interaction videos poses significant challenges: the vast number of frames incurs high computational costs, user interactions are interwoven with dynamic Graphical User Interfaces (GUIs) changes, and redundant visual content makes it difficult to extract relevant information, requiring models to distinguish meaningful actions from background variations. Given that most MLLMs are optimized for static image inputs with tightly coupled visionlanguage alignment, direct video processing is impractical. Instead, we hope to avoid any complex video processing, relying solely on uniform sampling and similarity-based filtering to minimize computational overhead. To achieve this, we introduce Mobile-Agent-V, multi-agent collaboration framework designed to process operational video inputs, extract actionable knowledge, and apply it to mobile device interactions. To reduce keyframe redundancy while ensuring critical information is retained, we introduce sliding window video input mechanism, where only subset of keyframes is fed into the decision agent. video agent analyzes the devices current state and adaptively shifts the window forward, ensuring that the selected frames remain relevant for decision-making. However, the resulting multiframe input still poses challenges for MLLMs in maintaining contextual coherence. To improve accuracy, we introduce reflection agent with longchain-of-thought reasoning to analyze the video and refine the decision agents outputs, enabling error correction when necessary. Experimental results demonstrate that Mobile-Agent-V achieves 30% performance improvement over existing methods in tasks requiring operational knowledge. Our summarized contributions are as follows: We introduce Mobile-Agent-V, novel framework that applies video guidance to enable autonomous mobile device operations. To address challenges in processing longcontext video input, we propose sliding window strategy alongside video agent, ensuring effective guidance through keyframes. We enhance the decision accuracy by incorporating deep-reflection agent. Experimental results show that Mobile-Agent-V achieves performance improvement of 30% compared to existing frameworks."
        },
        {
            "title": "2.1 GUI Agent",
            "content": "To improve user experience, intelligent agent frameworks powered by Large Language Models (LLMs) are rapidly advancing in GUI operations (Wang et al., 2024d; Liu et al., 2025). On the Web, HTML-based parsing dominates due to its interpretability, while some frameworks, such as ChatGPTs web assistant, leverage visual perception (Zhou et al., 2023; Deng et al., 2023; Zheng et al., 2024; He et al., 2024; Lù et al., 2024; Yoran et al., 2024; Reddy et al., 2024). In contrast, PC-based frameworks rely on system APIs or automation tools for enhanced control and flexibility (Zhang et al., 2024a; Tan et al., 2024; Xie et al., 2024). In the mobile domain, key challenge is equipping intelligent agents with operational knowledge, which LLMs often lack. Existing approaches include: (1) training models on operational data, which is costly and lacks scalability (Hong et al., 2023; Cheng et al., 2024; You et al., 2024; Zhang et al., 2024b; Chen and Li, 2024; Lu et al., 2024b; Chai et al., 2024; Rawles et al., 2024; Xu et al., 2024; Li et al., 2024a; Wan et al., 2024; Xing et al., 2024; Liu et al., 2024); (2) enabling autonomous exploration, which is resource-intensive (Yang et al., 2023; Wang et al., 2024c; Li et al., 2024b; Wang et al., 2025); and (3) manually generating knowledge, which is inefficient and relies on iterative human intervention (Wang et al., 2024b)."
        },
        {
            "title": "2.2 Video-guided Agent",
            "content": "Video guidance has become crucial modality for training intelligent agents, enabling them to understand and interact with dynamic environments efficiently. Early works focus on using large language models (LLMs) as central agents for video 2 comprehension. Extending this idea, (Wang et al., 2024e) improves long-term temporal comprehension. Beyond understanding, video guidance has been leveraged for real-world applications. (Wang et al., 2024a) integrates LLMs into video editing workflows and automates language-based video descriptions and edits. Similarly, (Zhang et al., 2024c) introduces an efficient method to retrieve relevant video frames, enabling structured video processing. In robotics, (Chane-Sane et al., 2023) utilizes human demonstration videos to teach robots new manipulation skills without explicit supervision. These works demonstrate the growing role of video-guided agents, from video comprehension and retrieval to real-world task execution, forming the foundation for more advanced multimodal learning systems."
        },
        {
            "title": "3.2 Video Processing",
            "content": "Traditional uniform sampling, commonly used in video understanding, is effective only for realworld videos with relatively static scenes and continuous motion between frames. However, in mobile recordings, most of the frames remain static, while the remaining frames change rapidly due to intermittent human interaction and fast device responses. This makes uniform sampling insufficient for mobile device videos. To address this, we first uniformly sample the at frequency to obtain the keyframe set : = Uniform_Sampling(V, d) (1) Next, we compute the similarity between consecutive keyframes and remove those with similarity above threshold s, resulting in reduced set Fs:"
        },
        {
            "title": "3 Mobile-Agent-V",
            "content": "Fs = {fi sim(fi, fi+1) s} (2) This section introduces Mobile-Agent-V, framework that enhances mobile automation through video guidance. We outline its key components, including video processing, sliding window, video agent, deep-reflection agent, decision agent, and explain how they work together to improve operational efficiency and accuracy."
        },
        {
            "title": "3.1 Framework",
            "content": "The overall workflow of Mobile-Agent-V is shown in Figure 2. Given an input video that captures demonstrated task, the system first extracts keyframes through uniform sampling and redundancy removal. The execution begins with an initial sliding window positioned at the start of the keyframe sequence. At each iteration, the decision agent generates an action Oi based on the current window, video instructions, and historical decisions. If the task is successfully completed, the process terminates. Otherwise, the deep-reflection agent validates and refines the action to ensure alignment with the demonstrated task. The refined decision ROi is then executed on the device, updating its state to Di+1. The video agent subsequently determines the next window starting point Si+1, facilitating dynamic adjustment of the observation scope as the task progresses. This iterative procedure continues until the task is completed or the predefined maximum exploration limit is reached. The complete pipeline is outlined in Algorithm 1. Finally, we filter out keyframes with temporal gaps smaller than threshold fs, yielding the final set of keyframes : = {fi Fs ti+1 ti d} (3) where ti represents the frame index of fi."
        },
        {
            "title": "3.3 Sliding Window",
            "content": "To improve video comprehension by MLLMs, we reduce the input length by selecting only the keyframes relevant to the current operation. This is achieved using sliding window, where the keyframes between the windows start and end points Vw serve as the input for decision-making: Vw = {F k}Si+W k=Si (4) where the is the length of the window. Ideally, with accurate keyframe extraction, the window size would be 2, covering the states before and after the operation to predict state transitions. However, for enhanced robustness, the window size is typically greater than 2, and the start point is shifted backward to capture previous states for better context."
        },
        {
            "title": "3.4 Decision Agent",
            "content": "Action Space. The decision agent is responsible for generating actions that alter the device state. To ensure seamless execution via operational tools, we adopt an action space similar to existing frameworks. Mobile-Agent-V defines six fundamental 3 Figure 2: The framework of Mobile-Agent-V. actions: Click, Scroll, Type, Back, Home, and Done. detailed description of the operating space is shown in the Appendix A.1.6. These correspond to tapping specific location, scrolling in designated direction, entering text into an active input field, navigating to the previous page, returning to the home screen, and completing the task, respectively. Decision Making. Unlike prior approaches that rely on internal operational knowledge, the decision agent in Mobile-Agent-V derives actions directly from video content. This imposes higher demands on contextual adherence. By leveraging the sliding window mechanism, we filter out irrelevant frames, reducing input length while preserving critical information. The i-th operation Oi follows the steps outlined in the following equation: Oi = Da(V wi, Iv, Di, Iu, {Ok}i1 k=1) (5) where Da() is the decision agent, Iv is the instruction completed in the video, Di is the screenshot of the device during the i-th operation, and Iu is the instruction that the user will complete on the current device. Besides this, to track the progress, we also provide the historical operations {Ok}i1 k=1 to the decision agent."
        },
        {
            "title": "3.5 Deep-Reflection Agent",
            "content": "Even with sliding window, handling low-quality keyframes necessitates increasing the window size, as smaller window may be dominated by redundant frames, preventing critical keyframes from being included. In scenarios where perfect keyframe extraction cannot be ensured, the decision agent still faces challenges in reasoning over long multiframe sequences. To address this, we introduce the deep-reflection agent, which performs in-depth validation and refinement of the decision agents outputs. Specifically, it follows structured process: analyzing each operation in the video, identifying the current device state within the recorded sequence, verifying whether the decision agents action aligns with the corresponding operation in the video, and, if discrepancies are detected, refining the action based on the observed trajectory. This reflection mechanism enhances decision accuracy by ensuring strict adherence to the demonstrated operations, leading to final refined decision ROi, formulated as follows: ROi = Ra(V wi, Iv, Di, Iu, Oi) (6)"
        },
        {
            "title": "3.6 Video Agent",
            "content": "To dynamically adjust the sliding window throughout task execution, we introduce the video agent. Initially, the window spans from the first keyframe to the -th keyframe. After each operation, the video agent analyzes the screenshots before and after the operation, keyframes within the current window, and user inputs to identify the corresponding keyframe. Then, it determines the updated window starting point, ensuring adaptive progression. The following is the formula for obtaining the starting point of the + 1-th sliding window: Si+1 = a(V wi, Iv, Ri, Iu) (7) where a() is the video agent, and Ri is the set of screenshots before and after the operation: Ri = {Dk}i+1 k=i (8) Moreover, the video agent handles anomalies such as incorrect transitions leading to unintended states or discrepancies caused by redundant or missing keyframes. To enhance reliability, it can flag 4 inconsistencies and generate diagnostic feedback, facilitating error recovery and improving decisionmaking robustness. Algorithm 1 Mobile-Agent-V pipeline Input: Video , Window length , Video task Iv, User task Iu, Decision agent Da, Reflection agent Ra, Video agent a, Max explorations Me 1: Initialization: 2: Obtain from as Equ. (1) (2) (3) 3: S1 1 4: for = 1 to Me do 5: Obtain Vwi from Oi Da(V wi, Iv, Di, Iu, {Ok}i1 k=1) if Oi == Done then as Equ.( 4) 6: 7: 8: 9: 10: 11: 12: 13: 14: end for break end if ROi Ra(V wi, Iv, Di, Iu, Oi) Di+1 Execute ROi on Device Ri {Dk}i+1 k=i Si+1 a(V wi, Iv, Ri, Iu)"
        },
        {
            "title": "4 Experiments",
            "content": "This section presents comprehensive evaluation of Mobile-Agent-V. We first introduce the evaluation methodology. Next, we describe the experimental setup. We then report the main results. Finally, we conduct qualitative analyses and ablation studies to further examine the contributions of individual components."
        },
        {
            "title": "4.1.1 Benchmark",
            "content": "To assess Mobile-Agent-Vs ability to acquire and apply operational knowledge from videos, we design benchmark requiring extensive external knowledge. The complete instructions are shown in the Appendix A.2.1. The tasks involve devicespecific operations that typically necessitate consulting manuals to determine optimal execution paths. Since such knowledge is rarely embedded in mainstream MLLMs, this benchmark effectively evaluates the knowledge augmentation provided by video guidance. It comprises three difficulty levels: Simple instructions involve common operations with minimal device-specific interactions, solvable through brief exploration even without prior knowledge. For example, turning off automatic bright5 ness is simple instruction, as most MLLMs are aware of the operation path. Normal instructions include medium-frequency operations requiring some device-specific interactions. Successful completion demands either prior knowledge or basic exploration. For instance, disabling the status bar network speed display is normal instructionwhile the prompt clearly specifies that the setting is in the status bar options, the agent still needs to explore to locate the exact entry. Advanced instructions consist of low-frequency, complex operations heavily dependent on devicespecific knowledge. Without external knowledge, these tasks are difficult to complete through exploration. For example, enabling three-finger screenshot is an advanced instruction, as its setting is buried within third-level menu, requiring multiple rounds of exploration to locate the entry."
        },
        {
            "title": "4.1.2 Metrics",
            "content": "We evaluate Mobile-Agent-V using four key metrics: Success Rate (SR), Completion Rate (CR), Decision Accuracy (DA), and Step Count (Step). Success Rate: The proportion of fully completed instructions, measuring end-to-end task execution where all steps must be correct. Completion Rate: The percentage of completed steps, providing finer-grained measure of task progress. Decision Accuracy: The ratio of correct decisions to total decisions, assessing the agents action selection accuracy. Step Count: The number of steps taken to complete an instruction, reflecting execution If the task is incomplete when efficiency. reaching the step limit, the count is set to this upper bound."
        },
        {
            "title": "4.2 Setup",
            "content": "Baselines. We compare Mobile-Agent-V with several open-source agent frameworks, including AppAgent (Yang et al., 2023), Mobile-Agent (Wang et al., 2024c), and Mobile-Agent-v2 (Wang et al., 2024b). To assess its ability to learn operational knowledge from videos, we introduce humancurated knowledge baseline, where an expert manually extracts key operational steps from the video and provides them as textual input. This text replaces the video input in Mobile-Agent-V. Method Basic Instruction Normal Instruction Advanced Instruction SR CR DA Step SR CR DA Step SR CR DA Step AppAgent Mobile-Agent-v1 Mobile-Agent-v 90 80 90 85.0 86.5 90.0 78.0 79.5 84.3 Mobile-Agent-V 100 100 97.8 +10 +10.0 +13.5 5.5 5.3 5. 4.5 -0.5 50 40 60 70.0 72.5 76.3 50.5 48.0 54.4 90 80.3 93.3 +30 +17.0 +25.9 12.0 11.5 10. 6.6 -3.9 10 10 20 40.5 43.0 49.3 25.0 27.8 31.2 60.1 86.8 70 +50 +37.5 +18.9 19.1 19.8 18. 10.9 -7.7 Human-Know. 100 100 98.9 4. 100 95.2 83.7 6.2 70 82. 58.8 10.7 Table 1: Evaluation results across different instructions, where Human-Know. denotes human-curated knowledge. Figure 3: Comparison of video-misaligned instructions and video-aligned instructions. The in-domain means that the video instruction is consistent with the user instruction, and the cross-domain instruction is inconsistent. Models. Both Mobile-Agent-V and baselines utilize GPT-4o as their MLLMs, ensuring consistency with the baselines. The model is accessed via the official API with default hyperparameters. Device and Interaction. Experiments are conducted on OnePlus 7 Pro smartphone using the Android Debug Bridge (ADB) for interaction, maintaining consistency with baselines. Clickable positions are extracted from the devices XML hierarchy, visually marked on screenshots, and used by the agent for precise action selection."
        },
        {
            "title": "4.3 Main Results",
            "content": "Table 1 shows that Mobile-Agent-V consistently outperforms baselines across various instruction difficulty levels. For basic instructions, while baselines generally achieve high success rates, MobileAgent-V reaches 100% success with 97.8% decision accuracy and an average of 4.5 steps. For Normal Instructions, it significantly improves both success and completion rates while reducing the step count. In Advanced Instructions, Mobile-Agent-V achieves 70% success rate50% higher than the best baselinealong with improved completion and decision accuracy, nearly halving the steps. Further analysis indicates that baseline methods, despite their performance on Basic Instructions, suffer from lower decision accuracy that leads to redundant actions. In contrast, Mobile-Agent-Vs precise decision-making streamlines task execution, particularly for complex operations. By effectively extracting and applying video-based knowledge, Mobile-Agent-V overcomes the limitations of relying solely on built-in operational rules. Although human-curated knowledge still offers slight advantages for Advanced Instructions, the minimal performance gap demonstrates that video-based learning is viable alternative to manual annotations, achieving near-human efficiency. It is worth noting that using videos can significantly reduce the preparation time for running compared to manually writing knowledge. We show the specific time required to record video of an instruction and compile the knowledge of an instruction in Appendix A.3."
        },
        {
            "title": "4.4.1 Generalization from Videos",
            "content": "We constructed the Video-Misaligned task by modifying the original instructions so that the logic of the operations in the video aligns with the user task, but the specific actions differ. This setup aims to 6 Figure 4: Comparison of different sliding window sizes."
        },
        {
            "title": "4.5 Ablation Study",
            "content": "To evaluate the effectiveness of the deep-reflection Agent, we conduct an ablation study by comparing performance with and without deep reflection agent, as shown in Figure 6. The results demonstrate that deep reflection agent consistently improves decision-making across various metrics. In cases where the SR and CR are already high, the improvements are marginal, as the decision agent is less prone to errors. However, for more complex tasks with lower baseline performance, incorporating deep reflection agent yields substantial gains, particularly in DA. This highlights its role in refining actions and mitigating inconsistencies in long multi-frame reasoning. The Step exhibits minor variations, suggesting that while deep reflection agent enhances precision, it does not significantly alter action efficiency. The deep reflection agent refines the decision agents outputs by correcting misalignments between predicted and actual actions, mitigating cascading errors in long-horizon tasks. It reduces reliance on perfect keyframe extraction, ensuring robustness in suboptimal visual conditions and improving overall reliability."
        },
        {
            "title": "4.6 Case Study",
            "content": "Figure 7 illustrates multi-agent collaboration scenario within Mobile-Agent-V. The decision agent Figure 5: Comparison of different keyframe quality. test whether Mobile-Agent-V can generalize operation types from video demonstration. As shown in Figure 3, Mobile-Agent-V shows performance drop in the Video-Misaligned condition. While basic instructions remain stable, normal instructions and advanced instructions exhibit more significant declines in success rate and decision accuracy. Nevertheless, the system still completes tasks reasonably well in the misaligned condition, indicating its ability to generalize operational knowledge beyond direct instruction mapping. These results highlight the value of diverse video demonstrations to enhance cross-instruction generalization. 4.4."
        },
        {
            "title": "Impact of Window Size",
            "content": "Figure 4 illustrates the effect of window size on task performance. Larger windows generally improve SR, CR, and DA while reducing steps, particularly for more complex tasks. However, beyond certain threshold, further increasing the window size yields diminishing returns, with some metrics even declining. This decline is likely due to the introduction of irrelevant information, which interferes with decision-making. These findings highlight the importance of balancing temporal context to maximize efficiency. 7 Figure 6: Comparison of w/o DR and w/ DR across different instructions. Figure 7: complete execution case of Mobile-Agent-V. The decision agent initially makes an incorrect action, but the deep-reflection agent verifies the operation video, compares the device state, and corrects the action. utilizes keyframes from sliding window to determine the operation. However, an error occurs as it skips the \"confirm contact\" step, highlighting the challenge of multi-image action following. To correct this, the deep-reflection agent identifies the misalignment and refines the decision, ensuring the correct operation is executed on the device. Meanwhile, the video agent observes the device state and anchors it to the fourth frame in the sliding window, subsequently shifting the window forward by two frames. This adjustment enables the system to display the next interaction, where the contact card is correctly selected. This case demonstrates the seamless coordination within Mobile-Agent-V, effectively leveraging video-based knowledge to enhance task execution and ensure robust decision-making."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce Mobile-Agent-V, novel framework that leverages video guidance to enhance mobile automation by providing rich, cost-effective operational knowledge. To efficiently process video inputs, we propose sliding window mechanism with video agent that dynamically adjusts keyframes based on device states. Additionally, deep-reflection agent improves decision accuracy through iterative reasoning. Experimental results 8 show that Mobile-Agent-V outperforms existing frameworks by up to 30%, demonstrating the effectiveness of video-based knowledge injection. In addition, video guidance can achieve an effect close to manually written knowledge while saving 80% of the time. Mobile-Agent-V enables scalable conversion of videos into operational knowledge, providing new pathway for agent learning."
        },
        {
            "title": "6 Limitations",
            "content": "First, the reliance on video inputs introduces variability in data quality, as suboptimal recordings may affect knowledge extraction. Additionally, although the sliding window mechanism enhances efficiency, it may still overlook critical frames in complex interactions. Lastly, while our framework generalizes across various tasks, its effectiveness may vary depending on the diversity of available video demonstrations. Future work can explore adaptive mechanisms to further optimize efficiency and robustness."
        },
        {
            "title": "References",
            "content": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966. Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Peng Gao, Shuai Ren, and Hongsheng Li. 2024. Amex: Android multiannotation expo dataset for mobile gui agents. arXiv preprint arXiv:2407.17490. Elliot Chane-Sane, Cordelia Schmid, and Ivan Laptev. 2023. Learning video-conditioned policies for unseen manipulation tasks. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 909916. IEEE. Jun Chen, Deyao Zhu Xiaoqian Shen Xiang Li, Zechun Liu Pengchuan Zhang, Raghuraman Krishnamoorthi Vikas Chandra Yunyang Xiong, and Mohamed Elhoseiny. 2023. Minigpt-v2: Large language model as unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478. Wei Chen and Zhiyuan Li. 2024. Octopus v2: OnarXiv device language model for super agent. preprint arXiv:2404.01744. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. 2024. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip: Towards general-purpose vision-language arXiv preprint models with instruction tuning. arXiv:2305.06500. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2web: Towards generalist agent for the web. In Thirty-seventh Conference on Neural Information Processing Systems. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. 2024. Webvoyager: Building an end-toend web agent with large multimodal models. arXiv preprint arXiv:2401.13919. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. 2023. Cogagent: visual language model for gui agents. Preprint, arXiv:2312.08914. Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. 2024a. On the effects of data scale on ui control agents. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Yanda Li, Chi Zhang, Wanqi Yang, Bin Fu, Pei Cheng, Xin Chen, Ling Chen, and Yunchao Wei. 2024b. Appagent v2: Advanced agent for flexible mobile interactions. arXiv preprint arXiv:2408.11824. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. arXiv preprint arXiv:2304.08485. William Liu, Liang Liu, Yaxuan Guo, Han Xiao, Weifeng Lin, Yuxiang Chai, Shuai Ren, Xiaoyu Liang, Linghao Li, Wenhao Wang, et al. 2025. Llmpowered gui agents in phone automation: Surveying progress and prospects. Xiao Liu, Bo Qin, Dongzhu Liang, Guang Dong, Hanyu Lai, Hanchen Zhang, Hanlin Zhao, Iat Long Iong, Jiadai Sun, Jiaqi Wang, et al. 2024. Autoglm: Autonomous foundation agents for guis. arXiv preprint arXiv:2411.00820. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. 2024a. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525. Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. 2024b. Gui odyssey: 9 comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451. Xing Han Lù, Zdenˇek Kasner, and Siva Reddy. 2024. Weblinx: Real-world website navigation with multiturn dialogue. arXiv preprint arXiv:2402.05930. Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo CampbellAjala, et al. 2024. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573. Revanth Gangi Reddy, Sagnik Mukherjee, Jeonghwan Kim, Zhenhailong Wang, Dilek Hakkani-Tur, and Heng Ji. 2024. Infogent: An agent-based framework for web information aggregation. arXiv preprint arXiv:2410.19054. Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. 2024. Towards general computer control: multimodal agent for red dead redemption ii as case study. In ICLR 2024 Workshop on Large Language Model (LLM) Agents. Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, and Zhibo Yang. 2024. Omniparser: unified framework for text spotting key information extraction and table recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1564115653. Bryan Wang, Yuliang Li, Zhaoyang Lv, Haijun Xia, Yan Xu, and Raj Sodhi. 2024a. Lave: Llm-powered agent assistance and language augmentation for video In Proceedings of the 29th International editing. Conference on Intelligent User Interfaces, pages 699 714. Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. 2024b. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. arXiv preprint arXiv:2406.01014. Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. 2024c. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158. Shuai Wang, Weiwen Liu, Jingxuan Chen, Weinan Gan, Xingshan Zeng, Shuai Yu, Xinlong Hao, Kun Shao, Yasheng Wang, and Ruiming Tang. 2024d. Gui agents with foundation models: comprehensive survey. arXiv preprint arXiv:2411.04890. Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. 2024e. Videoagent: Long-form video understanding with large language model as agent. In European Conference on Computer Vision, pages 5876. Springer. Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji. 2025. Mobile-agent-e: Self-evolving mobile assistant for complex tasks. arXiv preprint arXiv:2501.11733. Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. 2024. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. 2024. Osworld: Benchmarking multimodal agents for openended tasks in real computer environments. Preprint, arXiv:2404.07972. Mingzhe Xing, Rongkai Zhang, Hui Xue, Qi Chen, Fan Yang, and Zhen Xiao. 2024. Understanding the weakness of large language model agents within In Proceedings of complex android environment. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 60616072. Yifan Xu, Xiao Liu, Xueqiao Sun, Siyi Cheng, Hao Yu, Hanyu Lai, Shudan Zhang, Dan Zhang, Jie Tang, and Yuxiao Dong. 2024. Androidlab: Training and systematic benchmarking of android autonomous agents. arXiv preprint arXiv:2410.24024. Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771. Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2024. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023a. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. 2023. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079. Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2023b. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257. Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, and Jonathan Berant. 2024. Assistantbench: Can web agents solve realistic and time-consuming tasks? Preprint, arXiv:2407.15711. Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. 2024. Ferret-ui: Grounded mobile In Euroui understanding with multimodal llms. pean Conference on Computer Vision, pages 240 255. Springer. Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. 2024a. UFO: UI-Focused Agent arXiv preprint for Windows OS Interaction. arXiv:2402.07939. Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. 2024b. Android in the zoo: Chain-of-action-thought for gui agents. arXiv preprint arXiv:2403.02713. Lu Zhang, Tiancheng Zhao, Heting Ying, Yibo Ma, and Kyusong Lee. 2024c. Omagent: multi-modal agent framework for complex video understanding with task divide-and-conquer. arXiv preprint arXiv:2406.16620. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024. Gpt-4v(ision) is generalist web agent, if grounded. In Forty-first International Conference on Machine Learning. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. 2023. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Experimental Details This section provides additional details regarding the experimental setup and implementation choices used in Mobile-Agent-V. A.1.1 Sliding Window Size Selection In our experiments, the sliding window size was set to 4. While increasing the window size to 5 is also feasible, experimental analysis demonstrated that the performance improvement was marginal, while the computational cost increased due to the higher token consumption. Therefore, we adopted window size of 4 as balanced trade-off between efficiency and performance. A.1.2 Video Similarity Computation To compute the similarity between video frames, we employed simple yet effective approach based on pixel-wise differences. Given two frames I1 and I2, we first converted them to grayscale representations: 1 = grayscale(I1), 2 = grayscale(I2) (9) Next, we computed the absolute difference between the two grayscale images: = absdiff(I 1, 2) (10) Finally, the similarity score was obtained by counting the number of nonzero pixels in D: = np.count_nonzero(D) total pixels (11) This method effectively captures differences between frames while maintaining computational efficiency. A.1.3 Frame Similarity Threshold Selection As described in the main text, the similarity threshold fs was adjusted according to the characteristics of different applications. For instance, in the Settings app, where UI changes are primarily textbased, we set fs = 0.3 to ensure that more informative frames were retained. Conversely, for the Weather app, where UI elements exhibit significant visual variations, higher threshold of fs = 0.5 was used to prevent excessive redundant frame extraction. A.1.4 Step Limitations and Task Termination"
        },
        {
            "title": "Criteria",
            "content": "To ensure fair evaluation and prevent infinite loops, we imposed an upper bound on the number of execution steps: Basic tasks: 10-step limit. Standard tasks: 15-step limit. Complex tasks: 20-step limit. If an agent reached the step limit without successfully completing the task, the attempt was deemed failure. Additionally, if framework executed the required action but continued performing unnecessary operations beyond the instructions scope, it was also considered failure."
        },
        {
            "title": "Click",
            "content": "id Click_text text"
        },
        {
            "title": "Scroll",
            "content": "direction"
        },
        {
            "title": "Type\nBack\nHome\nDone",
            "content": "text None None None The \"id\" represents the numeric identifier of the detection box to be clicked. The \"text\" specifies the target text to be clicked, used only when no detection box or corresponding ID exists at the target location. The \"direction\" can be either \"up\" or \"down,\" allowing the agent to scroll the screen accordingly. The \"text\" parameter defines the content to be entered into text field. Returns to the previous screen. Navigates to the home screen. Signals task completion. Table 2: Action space definition for Mobile-Agent-V. A.1.5 Video Frame Concatenation for"
        },
        {
            "title": "Visualization",
            "content": "To facilitate interpretation, video frames were concatenated in row-wise fashion. Each frame within the sliding window was annotated with an index to assist the video agent in tracking progress. If fewer than four frames were available in the window, only the existing frames (up to three) were concatenated. The final frame of each sequence was explicitly labeled as the termination state to guide the decision agent in stopping at the appropriate point. A.1.6 Action Space Definition Mobile-Agent-V adopted the same action space as Mobile-Agent-V2. Unlike Mobile-Agent-V2, which utilized OCR and segmentation models to determine interaction coordinates, Mobile-AgentV employed the SoM (Set of Mark) approach to reduce context length. Additionally, to mitigate potential XML parsing issues in certain UI pages, we introduced supplementary click-by-text operation. The complete action space is detailed in Table 2. A.2 Benchmark Details A.2.1 Evaluation Instruction Table 4 below provides detailed breakdown of the benchmark tasks, categorized by application. This benchmark provides structured way to evaluate Mobile-Agent-Vs ability to interpret, align, and execute user instructions of varying complexity. The distinction between video-aligned and videomisaligned instructions ensures robustness against linguistic variations, testing the frameworks adaptability to real-world user interactions. A.2.2 Screen Recording All videos were recorded using the built-in screen recording tool on OnePlus 7 Pro test device. While the tool supports maximum frame rate of 60 Hz, the actual frame rate varies between 30 Hz and 60 Hz depending on the extent of UI changes. All interactions were performed manually at an average frequency of one action every 12 seconds. The videos remain unprocessed, with no modifications such as acceleration, editing, or overlays, preserving the original recordings. Each benchmark instruction corresponds to dedicated operation video, which demonstrates the optimal execution path for the task. A.3 Time Consumption We measured the time required for recording video and manually reviewing the video to document complete operation sequence, as shown in Table 3. While video recording takes less than one minute on average, manual documentation requires approximately five minutes. Utilizing videos reduces time consumption by 80% and eliminates the manual effort involved in writing instructions."
        },
        {
            "title": "Video Recording\nManually Writing Knowledge",
            "content": "<1 min 5 min Table 3: Time consumption for video recording and manual documentation. A.4 Prompt Table 5, 6, and 7 show the prompts of decision, deep-reflection, and video agent respectively. 12 APP Level Video Instruction & Video-Aligned User Instruction Video-Misaligned User Instruction Phone Basic Help me dial 123. Help me dial 321. Normal Please turn on the call recording for me. Please view all call recording for me. Advanced Help me add the mobile number 1234567890 to the blacklist. Help me add the mobile number 9876543210 to the whitelist. Messages Basic Help me set up messages and notifications to be displayed together in Messages. Help me set up messages and notifications not to be displayed together in Messages. Normal Please send message to 123456 with text \"Hello\" Please send message to 9876543210 with text \"Goodbye\". Advanced Send message to 123456 with my current location information. Send message to 987654 with my contact card. Setting Basic Help me turn off the auto brightness in Setting. Help me turn on the auto brightness in Setting. Normal Help me turn off the status bar network speed display. Help me turn off the status bar NFC display. Advanced Help me open three-finger screenshots. Help me open three-finger touch and hold. Photo Basic Help me turn on the shared albums setting in Photos. Help me turn off the shared albums setting in Photos."
        },
        {
            "title": "Normal",
            "content": "Help me clear recently deleted photos. Help me restore recently deleted photos. Advanced Help me set up not to record location when taking photos. Help me set up not to record properties when taking photos. Manager Basic Help me turn on the App cleaner reminder in Phone Manager. Help me turn off the App cleaner reminder in Phone Manager. Normal Help me turn on the automatic phone call for help. Help me turn on the automatic phone call for help and countdown sound. Advanced Help me clean up QQs storage. Help me clean up WhatsApps storage. Recorder Basic Help me start recording. Help me stop recording. Normal Help me change the audio format of my recording. Help me turn on the cloud recording. Advanced Help me show recently deleted recordings. Help me show call recordings. Files Basic Help me view photos in My Files. Help me view videos in My Files. Normal Help me create new tag named \"test\". Help me create new tag named \"mobile\". Advanced Help me turn on the option to show hidden files. Help me turn off the option to show hidden files. Clock Basic Normal Help me start stopwatch in Clock. Help me reset stopwatch in Clock. Help me set the gesture to turn off the alarm to swipe up. Help me set the gesture to turn off the alarm to press button. Advanced Help me delete the last city of the current world clock and add London. Help me delete the first city of the current world clock and add New York. Weather Basic Help me turn on the meteorological alert setting in Weather. Help me turn off the meteorological alert setting in Weather. Normal Help me turn on the rain reminder. Help me turn off the rain reminder. Advanced Help me turn on the UV intensity display and view the UV intensity at your current location. Help me turn on the Sunset display and view the sunset at your current location. Calendar Basic Help me turn on fixed time zone setting in Calendar. Help me turn off fixed time zone setting in Calendar. Normal Help me turn on calendar meeting reminders. Help me turn on fixed time zone. Advanced Help me subscribe to horoscope and choose Help me subscribe to today in history. Aries. Table 4: Benchmark tasks with different difficulty levels and instruction variations. 13 System You are mobile phone operation assistant. Below is description of this conversation. In the following part, will upload large image made up of many screenshots. These screenshots in this image are all from screen recording of mobile phone operation. will tell you the task completed in the screen recording. You need to observe this screen recording. Then, you need to complete new task, which is related to the task in the screen recording. You need to combine the operation experience provided by the screen recording and gradually complete this task. will upload the current screenshot of the device. There will be many detection boxes on this screenshot, and there will be number in the upper left and lower right corners of the detection box. You need to perform operations on the current page. In order to better operate the phone, the following are the operation tools you can use: - Click (id): The \"id\" is the numeric serial number of the detection box you need to click. - Click_text (text): The \"text\" is the text you need to click. This is only used when the detection box and the corresponding id do not exist at the location to be clicked. - Scroll (direction): The \"direction\" selects from \"up\", \"down\", \"left\", and \"right\". You can scroll the page certain distance in the specified direction. - Type (text): The \"text\" is the content you need to enter. - Back: You can use this operation to return to the previous page. - Home: You can use this operation to return to the home page. - Done: You can use this operation when the task is completed. You need to strictly follow the following json output format: \"Thought\": You need to think about how to perform this operation on the current device based on the operation path in the video, \"Operation\": Select one from the operation tools, \"Summary\": Briefly summarize this operation User during the first operation The first image is the screen recording, in which the tasks are completed: {Iv} The second image is the screenshot of the current device, in which you need to complete the following tasks: {Iu} Note: You need to refer to the operation path in the video more than relying on your own operation experience. Because you may make mistakes. Note: You need to refer to the operation path in the video more than relying on your own operation experience. Because you may make mistakes.\" <image: Vw><image: Di> User during subsequent operations The first image is the screen recording, in which the tasks are completed: {Iv} The second image is the screenshot of the current device, in which you need to complete the following tasks: {Iu} Here is your operation history: Step-1: {operation 1} Step-2: {operation 2} ...... Step-n: {operation n} Note: If the operation history and current device can infer that the task has been completed, use Done. Note: You need to refer to the operation path in the video more than relying on your own operation experience. Because you may make mistakes.\" <image: Vw><image: Di> Table 5: The prompt for decision agent."
        },
        {
            "title": "System",
            "content": "You are an expert in mobile phone operation. will upload two images below. The first image is keyframe mosaic from an operation video, in which the completed task is \"{Iv}\"; the second image is screenshot of the current status of the mobile phone. On the mobile phone shown in the second image, the task to be completed is: \"{Iu}\". The user will perform the following operation: {Operation from decision agent} Now please observe whether this operation conforms to the operation path shown in the first image. If it conforms, please output \"True\", otherwise please modify the operation content according to the above json format. The operation should be: - Click (id): The \"id\" is the numeric serial number of the detection box you need to click. - Click_text (text): The \"text\" is the text you need to click. This is only used when the detection box and the corresponding id do not exist at the location to be clicked. - Scroll (direction): The \"direction\" selects from \"up\" and \"down\". You can scroll the page certain distance in the specified direction. - Type (text): The \"text\" is the content you need to enter. - Back: You can use this operation to return to the previous page. - Home: You can use this operation to return to the home page. - Done: You can use this operation when the task is completed. Note: If the operation history and current device can infer that the task has been completed, use Done. You need to think in the following way: 1. Observe the operation of each step in the video (especially frame-3 and frame-4). 2. Anchor the position of the current device in the video. 3. Complete the current step according to the operation in the video. Please output your thought about this step by step before you output your response. User <image: Vw><image: Di> Table 6: The prompt for deep-reflection agent."
        },
        {
            "title": "System",
            "content": "You are mobile phone operation assistant. will provide you with two images. The first image is long picture of key frames from mobile phone operation video, which shows correct operation trajectory to complete the task: {Iv}. The second image is two screenshots before and after an operation from the user. The user want to complete the task: {Iu}. Please note that these two images are not necessarily the complete operation trajectories, they may only be part of the continuous operation. Although the task shown in the video may not be exactly the same as the task the user needs to complete, there is strong correlation between the two. So the user is referring to the operation in the video to complete this task. Now you need to determine which frame of the video the user is in after the device is operated. You need to use number to represent it. If the device is in the state between two frames, the previous frame is output. If the device is not in any frame of the video, please output the number 0 to indicate an operation error and generate an error cause analysis. You need to output in the following json format: {\"Thought\": Your thought of current question, \"Frame\": number, \"Analysis\": If Frame is 0, generate an error cause analysis, otherwise output null, \"Need_Back\": If Frame is 0, you need to think about how to get back on track. If you need to return to the previous page, please output true. If you need to continue to perform an operation on the current page to get back on track, please output false. If Frame is not 0, please output False directly.}"
        },
        {
            "title": "User",
            "content": "Here are the video and operation: <image: Vw><image: Di> Table 7: The prompt for video agent."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Beijing Jiaotong University"
    ]
}