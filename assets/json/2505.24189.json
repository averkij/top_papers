{
    "paper_title": "Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows",
    "authors": [
        "Orlando Marquez Ayala",
        "Patrice Bechard",
        "Emily Chen",
        "Maggie Baird",
        "Jingfei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations."
        },
        {
            "title": "Start",
            "content": "Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows"
        },
        {
            "title": "Jingfei Chen",
            "content": "ServiceNow {orlando.marquez, patrice.bechard, emily.chen, maggie.baird, jingfei.chen}@servicenow.com 5 2 0 2 0 3 ] . [ 1 9 8 1 4 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) such as GPT-4o can handle wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications faster inference, lower costs may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations."
        },
        {
            "title": "Introduction",
            "content": "Generative AI is pushing the boundaries of AIbased software. As Large Language Models (LLMs) become more capable, they have become an integral part of the software stack. Although prompting state-of-the-art models such as GPT-4o (Hurst et al., 2024), Gemini 2.0 Flash (Gemini et al., 2023), or Claude 3.5 Sonnet (Anthropic, 2024) generally suffice for traditional tasks like questionanswering or summarization (Fu et al., 2024), it is not always the case on domain-specific tasks (Trad and Chehab, 2024). While building Flow Generation1, an application that generates low-code workflows based on textual user requirements, we explored whether fine-tuning language model was necessary to achieve the desired software quality. To reduce infrastructure needs, we only considered Small Language Models (SLMs), having less than 15 billion parameters. Enterprise workflows are common way to automate repetitive yet complex processes. They are represented as series of steps that are executed 1https://www.servicenow.com/docs/csh?topicname=flowgeneration.html&version=latest Figure 1: Task consists of generating complete workflow from user requirement. under certain conditions to fulfill specific goals. While low-code workflows tend to be built in easyto-use interfaces, creating them still requires expert knowledge of the enterprise system. Figure 1 shows an example of simple four-step workflow. Some of the challenges in this task are: Every step must exist in the system environment; they vary by installation and users can add custom steps. Workflow structure must follow set of welldefined rules, using concepts such as conditions (e.g., IF) and loops (e.g., FOREACH). Every step generates outputs of data types such as integer or boolean, which can be used in subsequent steps. Steps have inputs that can refer to database tables, columns, and values. For example, the Look Up Incident Records step in Figure 1 1 matches rows in the incident table where the column assigned_to has the same value as the user record from the trigger step. The simplest approach to generate low-code workflows is to use prompt engineering on an offthe-shelf LLM. This would be satisfactory if the steps were high-level descriptions and the logic elements were simple. But to obtain reasonable results in this task, we had to decompose it into sub-tasks (Wies et al., 2023) and use Retrieval-Augmented Generation (RAG) (Gao et al., 2023) at various points in the pipeline. We found out that, while prompting an LLM gives reasonable results, we can do better by fine-tuning an SLM. Our approach was to create small yet representative training dataset for the sub-tasks comprising Flow Generation and fine-tune Mistral-Nemo-12BBase (Mistral, 2024). For evaluation, we labeled about thousand workflows coming from ten domains (around 100 per domain). We also created special dataset by inviting expert enterprise users to interact with the tool and collecting data generated from their usage. We compare our approach against an instruction fine-tuned SLM as the baseline, as well as against variety of closed and open-source LLMs, showing the benefits of fine-tuning. As our task is domain-specific, we devised metric called Flow Similarity (FlowSim), version of tree edit distance that represents workflows as trees. Our contributions are the following: We present case study for how we built an application that generates low-code workflows based on textual user requirements. We demonstrate that fine-tuning an SLM gives better results than prompting much larger LLMs in this domain-specific task. We introduce process to conduct systematic error analysis that reveals model limitations and complements our metrics."
        },
        {
            "title": "2 Related Work",
            "content": "Prompting (McCann et al., 2018; Brown et al., 2020) has emerged as the predominant method for using LLMs, significantly reducing the effort required compared to fine-tuning (Sanh et al., 2021; Wei et al., 2021). However, for domain-specific applications requiring extensive prior knowledge, practitioners often encounter limitations such as constrained context windows (Peng et al., 2023), which hinder the models ability to capture the full scope of the task. To address this, alternative approaches such as RAG (Lewis et al., 2020) or domain-specific fine-tuning (Gururangan et al., 2020) are employed to incorporate relevant knowledge. In various industry-specific settings, specialized models have demonstrated superior performance over state-of-the-art general-domain LLMs (Wu et al., 2023; Tu et al., 2024; Colombo et al., 2024). We add to this body of work by showing that fine-tuned SLM performs better than prompting LLMs in low-code workflow generation. The domain of workflows has been extensively explored in previous work (Zeng et al., 2023; Ayala and Bechard, 2024; Wornow et al., 2024; Li et al., 2024, inter alia). Notably, Bassamzadeh and Methani (2024) compare an approach using RAG against fine-tuning model for workflow generation, while Fan et al. (2024) propose synthetic data generation pipeline to enhance generalization. In our case, we combine RAG with fine-tuning an SLM to obtain the highest possible quality. Evaluation methodologies vary across tasks, with each task having its own quantitative metric for model quality but also its own qualitative error analysis approach (Dou et al., 2024; Vilar et al., 2006; Gauthier-melancon et al., 2022; Bolya et al., 2020). In this work, we outline an error analysis approach for structured output task and leverage it to gain deeper insights into model failure modes."
        },
        {
            "title": "3 Methodology",
            "content": "Because workflows are complex structured outputs requiring diverse data in their steps and inputs, we designed pipeline that relies on RAG and generates the workflow iteratively by asking the LLM to solve sub-tasks. 3.1 Task Decomposition In our domain, workflows are represented as JSON documents, which allows us to break the generation of complete workflows in two stages: 1. Given natural language requirement, generate the plan or outline of the workflow, identifying the step names, the order of execution and, crucially, extracting an annotation (description) from the requirement for each step. 2. For every step in the outline, use the annotation to gather the necessary data from the environment and generate the step inputs. The glue between the two stages are annotations, which also serve as an explanation for the generated step. They allow the model to fill step details, as long as these details are provided by the user. When we retrieve data such as table or column names, these annotations are key part of the search input. Figure 2 shows the JSON representation of the trigger and first component steps for the workflow in Figure 1. The lines in green are generated in the first stage, as part of the outline, and the step inputs in red are generated as part of the second stage. Figure 3: System architecture with UI, AI, and data layers. and displays the workflow outline and step inputs as they are generated. The AI layer contains the LLM and the retriever. The data layer stores the indexed sources of data, from where the retriever suggests steps and artifacts (tables, columns, values) to the LLM. This layer can be replaced in every installation of the system to allow the LLM to generate output specific to each customer. Note that for workflow containing steps, there will be maximum of + 1 LLM calls: one for the outline and one to generate the inputs of each step that accept inputs. Fine-tuning the SLM and the retriever was done using standard approaches: supervised instruction fine-tuning and contrastive loss training."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Datasets The training dataset comprises two tasks, each corresponding to the two pipeline stages. The first task is createFlow, where the model generates the outline given the natural language requirement and basic workflow metadata. The second task is populateInputs, where given flow at step , the annotation of step , and any required environment data, the model generates the inputs of this step. In the former, the model receives suggestions for steps while in the latter, it receives suggestions for artifacts that cannot be found deterministically, such as table names that users refer to in the requirement. We extracted thousands of workflows from internal deployments of the enterprise system for labeling. After discarding those too short or too Figure 2: Sample JSON representation of the trigger and first component steps for the workflow in Figure 1. 3.2 Retrieval-Augmented Generation It has been shown that RAG is necessary to reduce hallucinations when generating structured outputs (Ayala and Bechard, 2024; Bassamzadeh and Methani, 2024). In our pipeline, RAG is used to suggest the following to the LLM: Steps to use in the outline. Since the available steps vary across installations of the system, this includes custom steps added by users. Table names, column names, and values used in steps that read and modify database records in the system. These names and values are used in step inputs. For example, referring to Figure 2, step names such as record_update and look_up_records can be retrieved based on the text Every time user becomes inactive, find all incidents where the user is the assignee. Assign them to their manager.. Then the table name sys_user, column name active, and column value false are retrieved from every time user becomes inactive. 3.3 Pipeline Figure 3 shows the architecture for the overall pipeline. The UI receives the user requirement 3 long, we selected 1,512 workflows. team was trained to express in natural language description or requirement of what these workflows accomplish. downside of using existing workflows is that they are complete while users will create their workflows in draft form and will add complexity iteratively. We therefore created 766 simpler workflows synthetically using domain expertise aiming to cover the data distribution as much as possible. For evaluation, we similarly extracted and labeled workflows from 10 customer deployments, around 100 from each. Since each deployment comes from different domains such as retail and banking, the steps and table names differ from those found in the training dataset. We call this dataset the out-of-domain (OOD) set. However, the same downside applies to this evaluation dataset: these are complete workflows. To address this, we invited expert users of the manual system to simulate interacting with the application and generate workflows. Their submitted requirements formed TEST set of 108 workflows. Table 1 summarizes the number of samples used for training and evaluation. Dataset Internal Synthetic TEST OOD # createFlow 1,512 766 108 1,072 # populateInputs 4,709 2,310 367 4,958 Table 1: Number of examples for training (Internal and Synthetic) and evaluation (TEST and OOD). 4.2 Models For SLMs, we considered several models such as StarCoderBase-7B (Li et al., 2023) and Llama3.1-8B (Grattafiori et al., 2024), but we chose Mistral-Nemo-12B-Base (Mistral, 2024) because this model architecture is well-supported and optimized in our enterprise system. However, we obtained similar results with other SLMs as the key ingredient for our fine-tuning is the training data. As the baseline, we used model of the same architecture and size that has been instruction finetuned but not on data from our workflow domain: Mistral-Nemo-12B-Instruct (Mistral, 2024). This allows us to validate that domain-specific data is helpful. As for LLMs, we try to cover the current landscape by comparing against: Closed-source: GPT-4o-mini, GPT-4o, and Gemini-2.0-Flash. Open-source: Llama-3.3-70B-Instruct (Grattafiori et al., 2024) Reasoning: o3-mini (OpenAI, 2025) with medium reasoning We crafted two prompt templates, one for each task, to use on all models that were not fine-tuned. Both contain sections for Context, Task definition, Inputs, Guidelines, Constraints with examples, and Output format. Before prompting the LLMs, these templates get populated with data from the system and the retrieved suggestions. In the case of the populateInputs prompt, we dynamically fill the template with instructions according to the inputs to populate (e.g., input types). Table 2 gives an idea of the complexity of our prompts by listing the number of input tokens using the GPT-4o tokenizer on the TEST set. # tokens in template Avg # prompt tokens Std Dev # prompt tokens createFlow 3,948 5,958 282 populateInputs 2,225 3,732 Table 2: Number of tokens in templates and average / standard deviation for prompts sent to the LLMs. 4.3 Metrics We devised our own metric called Flow Similarity (FlowSim) to compare generated and expected workflows. As workflows can be represented as trees, similar to how computer programs can be represented as abstract syntax trees, we use the tree edit distance algorithm (Zhang and Shasha, 1989) to compute similarity score, where higher is better. We found that this metric correlates well with human evaluation (details in Appendix A). labeled expected workflow is required per user requirement, but evaluation can be automated. Appendix includes sample workflow tree. Our metric, however, has few limitations. First, we compare against only one workflow whereas there may be several possible versions for the same requirement. Second, we compute exact matches for input strings. Third, it ignores that generated workflows may break basic structure rules (e.g., having an ELSE step without an IF). The first two drawbacks can be addressed by having more than one reference workflow and by using sentence similarity for string inputs, but we leave this to future work. We address the third drawback by computing the percentage of generated examples with at least one such structure error. While the most complete metric is FlowSim of outline and inputs, we also report FlowSim of outline, and the percentage of examples with structure errors. 4 Figure 4: Flow similarity results obtained on the TEST and OOD sets for Outline and Outline with inputs."
        },
        {
            "title": "5 Results",
            "content": "Figure 4 shows the results for our fine-tuned SLM and all other LLMs on the small TEST and the larger OOD datasets. Mistral-Nemo-12BInstruct performs poorly across the board confirming that SLMs require workflow in-domain training data. LLMs of the \"mini\" sort, even o3-mini with medium reasoning, also do not perform well. When we prompt larger LLMs, we see competitive results, especially with Gemini-2.0-Flash and GPT-4o. As generating the outline is simpler task, we see larger gap between our fine-tuned SLM and these models when we generate the complete workflow (outline and inputs). This gap is 7.2% on TEST but 12.4% on OOD, around 10% if we average the two. All models perform worse when the requirements denote unfinished workflows (TEST set), suggesting discrepancy between requirements written by expert users and our annotators. While FlowSim values may be high, it could be that the generated workflow had obvious structure errors, which could severely impact usability or require complex post-processing rules. As shown in Figure 5, while GPT-4o performs reasonably well overall, it makes many such errors. With the same prompt, Gemini-2.0-Flash yields even fewer structure errors than our fine-tuned SLM on the TEST set. Refer to Appendix for further analysis. Our last quantitative comparison helps us determine whether sub-optimal RAG is primarily responsible for errors. Here, we consider only outline generation, as only one retriever call is made to obtain step suggestions, and we consider only the TEST dataset. Table 3 shows that all models perform better with simulated perfect RAG (i.e., all steps in the expected workflow are part of the suggestions) as opposed to the standard case where Figure 5: Percentage of examples with structure errors on the TEST and OOD sets. Lower is better. the retriever may miss some required steps. Nevertheless, we see that the improvement is only maximum of 4%, suggesting that most errors are model errors. Model Mistral-Nemo-12B-Base (Ours) GPT-4o LLama-3.3-70B-Instruct Gemini-2.0-Flash RAG Perfect RAG 77.5 70.5 70.6 68.7 78.8 74.7 74.4 70.2 Table 3: Outline results with perfect RAG (TEST set). Best results are in bold and second best are underlined."
        },
        {
            "title": "6 Error Analysis",
            "content": "6.1 Approach To perform more fine-grained evaluation, our approach to error analysis complements the FlowSim metric by adding interpretability. It makes the process less time-consuming due to its reusability and extensibility. We began with qualitative error analysis of our current production model and based on the patterns found, identified features in the ground truth dataset that appeared to negatively impact model output. These findings were then organized into binary matrix, where 1 indicates features presence in each ground truth sample and 0, an absence (see Figure 6). We then aligned model scores with the 5 matrix, making it easy to identify the features that could be contributing to underperformance. This approach greatly expedites error analysis as model comparison is streamlined. New runs and models can be quickly assessed on particular features without reviewing individual samples. If new model runs have poor performance that does not pattern with current features, we can quickly identify items to review for error analysis and potentially add features to the matrix. sample id 0 1 2 ... feat1 0 1 0 ... 0 feat2 1 1 0 ... 0 ... ... ... ... ... ... feat model score 0 1 0 ... 1 29 67 34 ... 100 Figure 6: Sample binary matrix that characterizes each ground truth sample along feature dimensions, with model scores for each sample. 6.2 Findings We focused our error analysis on the TEST dataset, as it best simulates customer usage of our enterprise application. We identified 24 features across the 108 samples that appeared to impact output quality (see Appendix D). We then subdivided the dataset based on sets of related features to better understand each models ability to (1) create the outline structure, (2) populate inputs, and (3) handle services specific to our enterprise system: STRUCTURE contains structural logic features: FOREACH, PARALLEL, and TRY/CATCH. INPUT contains input-related features: worknotes/descriptions, trigger conditions, and multiple conditions. ENTERPRISE contains features unique to our enterprise system: service-level agreement (SLA), service catalog, and Glide datetime. We compare our fine-tuned SLM and the LLMs that performed best: Gemini-2.0-Flash and GPT-4o. As Table 4 shows, our fine-tuned SLM underperforms the LLMs on flows containing structural logic steps. Closer analysis suggests the SLM frequently misses the dependency steps associated with them (e.g. FOREACH is frequently paired with prior look_up_records step, PARALLEL should always consist of more than one branch). The fine-tuned SLM nevertheless consistently outperforms the LLMs on the remaining two subdivisions. The largest margin is on the ENTERPRISE set, where the fine-tuned SLMs average FlowSim 6 score is 12.16% higher than GPT-4os. The smallest margin is also on ENTERPRISE, where the finetuned SLM surpasses Gemini-2.0-Flash by 5.35%. We suspect that learning by demonstrations is more efficient than including complex instructions in prompt due to the intricacies and specifics of the workflow domain. Lastly, we observed that workflow steps and conditions were often expressed implicitly in the TEST dataset. The requirement lookup incident tasks and close them, for example, entails FOREACH and update step that are not stated overtly. Our results indicate that the fine-tuned SLM is far better than the LLMs on such examples, with FlowSim score of 65.1 to Geminis 57.6 and GPT-4os 58.5. This indicates the value of labeling as these sort of examples were part of the labeling instructions derived from expectations of how the application would be used. Feature STRUCTURE FOREACH PARALLEL TRY/CATCH Average INPUT Worknotes Triggers Multiple Average ENTERPRISE Service Cat SLA Glide Average # Samples Ours Gemini GPT-4o 22 10 5 26 37 23 7 5 5 64.1 56.0 62.2 63. 66.8 70.0 60.1 67.3 63.6 69.6 68.6 66.8 72.7 65.7 68.6 70.0 63 60.4 55.0 59.8 58.9 58.8 67.8 61.6 71.7 55.1 50.4 64. 64.6 63.9 54.3 59.9 51.0 52.2 62.2 54.6 Table 4: Model results on three subsets of features, STRUCTURE , INPUT , and ENTERPRISE . Best results are in bold and second best are underlined. 7 Conclusion We present case study for building an enterprise generative AI application called Flow Generation, which translates textual user requirements into low-code workflows. While prompting stateof-the-art LLMs yield reasonable results, quality can be significantly improved by fine-tuning an SLM. To complement our quantitative metrics, we performed systematic error analysis that reveals strengths and weaknesses of the fine-tuned SLM and the best-performing LLMs. Future work includes improving our custom metric and addressing the gaps identified by our error analysis approach."
        },
        {
            "title": "References",
            "content": "Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. https://www.anthropic.com/news/ claude-3-opus-sonnet-haiku. Model card. Orlando Ayala and Patrice Bechard. 2024. Reducing hallucination in structured outputs via retrievalaugmented generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), pages 228238, Mexico City, Mexico. Association for Computational Linguistics. Nastaran Bassamzadeh and Chhaya Methani. 2024. comparative study of dsl code generation: Finetuning vs. optimized retrieval augmentation. arXiv preprint arXiv:2407.02742. Daniel Bolya, Sean Foley, James Hays, and Judy Hoffman. 2020. Tide: general toolbox for identifying object detection errors. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part III 16, pages 558573. Springer. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Pierre Colombo, Telmo Pires, Malik Boudiaf, Rui Melo, Dominic Culver, Etienne Malaboeuf, Gabriel Hautreux, Johanne Charpentier, and Michael Desa. 2024. Saullm-54b & saullm-141b: Scaling up domain adaptation for the legal domain. Advances in Neural Information Processing Systems, 37:129672 129695. Shihan Dou, Haoxiang Jia, Shenxi Wu, Huiyuan Zheng, Weikang Zhou, Muling Wu, Mingxu Chai, Jessica Fan, Caishuang Huang, Yunbo Tao, and 1 others. 2024. Whats wrong with your code generated by large language models? an extensive study. arXiv preprint arXiv:2407.06153. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and 1 others. 2023. Retrievalaugmented generation for large language models: survey. CoRR. Gabrielle Gauthier-melancon, Orlando Marquez Ayala, Lindsay Brin, Chris Tyler, Frederic Branchaudcharron, Joseph Marinier, Karine Grande, and Di Le. 2022. Azimuth: Systematic error analysis for text classification. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 298310, Abu Dhabi, UAE. Association for Computational Linguistics. Gemini, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, and 1 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah Smith. 2020. Dont stop pretraining: Adapt language models to domains and tasks. arXiv preprint arXiv:2004.10964. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, and 1 others. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459 9474. Shengda Fan, Xin Cong, Yuepeng Fu, Zhong Zhang, Shuyan Zhang, Yuanwei Liu, Yesai Wu, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2024. Workflowllm: Enhancing workflow orchestration capability of large language models. arXiv preprint arXiv:2411.05451. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, and 1 others. 2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161. Xue-Yong Fu, Md Tahmid Rahman Laskar, Elena Khasanova, Cheng Chen, and Shashi Tn. 2024. Tiny titans: Can smaller large language models punch above their weight in the real world for meeting summarization? In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), pages 387394, Mexico City, Mexico. Association for Computational Linguistics. Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, and Yongfeng Zhang. 2024. Autoflow: Automated workflow generation for large language model agents. arXiv preprint arXiv:2407.12821. Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730. 7 Kaizhong Zhang and Dennis Shasha. 1989. Simple fast algorithms for the editing distance between trees and related problems. SIAM Journal on Computing, 18(6):12451262. Mistral. 2024. Mistral nemo. Accessed: 2024. OpenAI. 2025. Openai o3-mini system card. https://cdn.openai.com/o3-mini-system-cardfeb10.pdf. Accessed: March 19, 2025. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, and 1 others. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207. Fouad Trad and Ali Chehab. 2024. Prompt engineering or fine-tuning? case study on phishing detection with large language models. Machine Learning and Knowledge Extraction, 6(1):367384. Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, and 1 others. 2024. Towards generalist biomedical ai. Nejm Ai, 1(3):AIoa2300138. David Vilar, Jia Xu, Luis Fernando DHaro, and Hermann Ney. 2006. Error analysis of statistical machine translation output. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC06), Genoa, Italy. European Language Resources Association (ELRA). Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652. Noam Wies, Yoav Levine, and Amnon Shashua. 2023. Sub-task decomposition enables learning in sequence In The Eleventh International to sequence tasks. Conference on Learning Representations. Michael Wornow, Avanika Narayan, Krista OpsahlOng, Quinn McIntyre, Nigam Shah, and Christopher Re. 2024. Automating the enterprise with foundation models. Proceedings of the VLDB Endowment, 17(11):28052812. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. Bloomberggpt: large language model for finance. arXiv preprint arXiv:2303.17564. Zhen Zeng, William Watson, Nicole Cho, Saba Rahimi, Shayleen Reynolds, Tucker Balch, and Manuela Veloso. 2023. Flowmind: automatic workflow generation with llms. In Proceedings of the Fourth ACM International Conference on AI in Finance, pages 7381."
        },
        {
            "title": "Human Evaluation",
            "content": "to their manager. This workflow is also shown in Figure 1. To validate that our custom metric indicated that generated workflow satisfied the user requirement, we compared it with human scores. We selected 30 random samples from the TEST dataset, generated workflows using our fine-tuned SLM, and asked domain experts in our Quality Engineering department to assign scores from 0 to 10 to each result. score of zero meant that the generated workflow does not at all represent the user requirement and score of ten meant perfect generated workflow. Pearson Spearman Outline 0.78 (4.4e-7) 0.83 (1.5e-8) Outline and inputs 0.78 (2.9e-7) 0.76 (9.1e-7) Table 5: Correlation between human evaluation scores and FlowSim metrics. P-value is shown in parentheses. We performed correlation tests for the outline only as well as for outline and inputs, computing both Pearson and Spearman correlation coefficients along with their associated p-values. As Table 5 shows, the correlation coefficients are higher than 75% in all cases and the p-values are very small, confirming that the correlation numbers are statistically significant. This gives us confidence that the FlowSim metric is appropriate for our use case."
        },
        {
            "title": "B Example of representing a workflow as",
            "content": "a tree"
        },
        {
            "title": "Validation",
            "content": "As Figure 5 shows, many LLMs generate workflows that have obvious structure errors. For instance, on the TEST set, 17.3% of the 108 workflows generated by GPT-4o, and 12% of those generated by Llama-3.3-70B-Instruct have such errors. While some of these errors can be corrected via post-processing, it requires additional engineering effort. The best model would be the one that minimizes this percentage. To take into account this failure mode, we computed another set of metrics where all these \"broken\" workflows received an Outline score of zero, as these are unusable. Consequently, the score for Outline and inputs also becomes zero. Figure 8 shows the FlowSim results obtained for all models after this structure validation is applied. While our fine-tuned SLM still gives significant improvements over prompting LLMs, we observe that GPT-4o suffers substantial degradation compared to Gemini-2.0-Flash. Excluding structure errors, both of these LLMs performed virtually the same."
        },
        {
            "title": "D TEST Dataset Features",
            "content": "A total of 24 features were identified during the qualitative error analysis. They are presented in Table 6, grouped by theme. Some elements were not included in any training data or were rare in the training data. These, we grouped to address training gaps in the future. Others, including flow logics, are common in training data and have regular patterning of model behavior. They are useful groupings to check model performance at granular level without reviewing individual samples. Finally, some of the TEST samples are adversarial and are particularly useful for assessing default behavior and seeing if anything breaks the model. Figure 7: Workflow in Figure 1 represented as tree. The tree in Figure 7 is the representation of the workflow that can be generated from the requirement Every time user becomes inactive, find all incidents where the user is the assignee. Assign them 9 Figure 8: FlowSim results obtained on the TEST and OOD sets for Outline and Outline with inputs, after structure validation is applied. Features by Type NOT INCLUDED IN TRAINING Triggers: null, service catalog Actions: get catalog variables, ask for approval Inputs: glide query, dynamic ME UNCOMMON IN TRAINING DATA Triggers: SLA, conditionals, weekly Inputs: requestor-related FLOW LOGICS DOUNTIL, PARALLEL, FOREACH, TRY-CATCH COMMON Actions: MS Teams / Slack message, notifications / emails, log, worknotes / descriptions Inputs: Complex inputs DIFFICULT / EDGE CASES Out of scope, ambiguous, implicit actions, misleading input, incorrect / incomplete Table 6: Features identified during the error analysis process, grouped by type."
        }
    ],
    "affiliations": [
        "ServiceNow"
    ]
}