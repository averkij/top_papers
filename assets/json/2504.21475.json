{
    "paper_title": "Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines",
    "authors": [
        "Serry Sibaee",
        "Samar Ahmed",
        "Abdullah Al Harbi",
        "Omer Nacar",
        "Adel Ammar",
        "Yasser Habashi",
        "Wadii Boulila"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network architecture featuring geometrically decreasing layers that achieves state-of-the-art results for Arabic RD tasks. Our methodology incorporates a comprehensive dataset construction process and establishes formal quality standards for Arabic lexicographic definitions. Experiments with various pre-trained models demonstrate that Arabic-specific models significantly outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score (0.0644). Additionally, we provide a formal abstraction of the reverse dictionary task that enhances theoretical understanding and develop a modular, extensible Python library (RDTL) with configurable training pipelines. Our analysis of dataset quality reveals important insights for improving Arabic definition construction, leading to eight specific standards for building high-quality reverse dictionary resources. This work contributes significantly to Arabic computational linguistics and provides valuable tools for language learning, academic writing, and professional communication in Arabic."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 A 0 3 ] . [ 1 5 7 4 1 2 . 4 0 5 2 : r a"
        },
        {
            "title": "Graphical Abstract",
            "content": "Advancing Arabic Reverse Dictionary Systems: Transformer-Based Approach with Dataset Construction Guidelines Serry Sibaee,Samar Ahmed,Abdullah Al Harbi,Omer Nacar,Adel Ammar,Yasser Habashi,Wadii Boulila"
        },
        {
            "title": "Highlights",
            "content": "Advancing Arabic Reverse Dictionary Systems: Transformer-Based Approach with Dataset Construction Guidelines Serry Sibaee,Samar Ahmed,Abdullah Al Harbi,Omer Nacar,Adel Ammar,Yasser Habashi,Wadii Boulila We propose semi-encoder neural network with geometrically decreasing hidden layers, achieving state-of-theart performance on Arabic reverse dictionary tasks through efficient semantic representation. We establish formal quality standards for Arabic lexicographic definitions, providing systematic framework to enhance dataset construction for reverse dictionary applications. We present formal abstraction of the reverse dictionary task, offering rigorous theoretical framework for embedding-based word retrieval systems and facilitating reproducibility and future research. We develop RDTL, modular and extensible Python library with configurable training pipelines for reverse dictionary tasks, released as open-source to support the research community.1 1https://github.com/serrysibaee/reverse_dictionary/tree/main Advancing Arabic Reverse Dictionary Systems: Transformer-Based Approach with Dataset Construction Guidelines Serry Sibaeea, Samar Ahmedc, Abdullah Al Harbib, Omer Nacara, Adel Ammara, Yasser Habashia and Wadii Boulilaa aCollege of Computer & Information Sciences, Prince Sultan University, Riyadh, Saudi Arabia bFaculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia cIndependent Researcher, Riyadh, Saudi Arabia I I A R Keywords: Arabic NLP Reverse Dictionary Transformer Models Dataset Construction Semantic Search This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present novel transformer-based approach with semiencoder neural network architecture featuring geometrically decreasing layers that achieves stateof-the-art results for Arabic RD tasks. Our methodology incorporates comprehensive dataset construction process and establishes formal quality standards for Arabic lexicographic definitions. Experiments with various pre-trained models demonstrate that Arabic-specific models significantly outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score (0.0644). Additionally, we provide formal abstraction of the reverse dictionary task that enhances theoretical understanding and develop modular, extensible Python library (RDTL) with configurable training pipelines. Our analysis of dataset quality reveals important insights for improving Arabic definition construction, leading to eight specific standards for building high-quality reverse dictionary resources. This work contributes significantly to Arabic computational linguistics and provides valuable tools for language learning, academic writing, and professional communication in Arabic. 1. Introduction Recent achievements in natural language processing (NLP) have played an essential role in advancing various semantic understanding tasks. Of these, the Reverse Dictionary (RD) is prominent semantic task that tackles the problem of finding word from given description or meaning. Whereas traditional forward dictionary associates given word with its formal definition, RD solves the inverse problem of assisting users identify word when given its meaning. This function has many practical applications in different fields and helps us tackle common linguistic and cognitive issues. Perhaps the most famous use of RD is mitigating the phenomenon known as the \"tip of the tongue\" (TOT), the frustrating stage where one remembers meaning or some synonyms, yet can not reach the specific word itself Brown and McNeill (1966). Using RD to search by meaning of word enables users to create much more accurate language. In academic writing, for example, RD can assist researcher in choosing the most precise term to convey complicated thought Pilehvar (2019). In contract drafting and other specialized domains, RD can assist professionals in selecting precise terminology by mapping natural language descriptions to semantically appropriate words, ensuring clarity and accuracy in legal and business communication Siddique and Beg (2022). In addition, RD also helps learn and teach languages. For those studying second language, such systems can serve as an efficient means of strengthening their vocabulary by assisting them in finding new words and learning their subtle meanings Mane, Patil, Madaswar and Sadavarte (2022). This is what makes RD an integral tool in educational institutions, and the creation of languagelearning apps. While RDs have been developed for languages such as English, French, or Chinese Yan, Li, Qiu and Deng (2020), work to develop them for Arabic has been slower Mickus, Van Deemter, Constant and Paperno (2022). This is due to the complexity of Arabic morphology, diglossia (the coexistence of formal and colloquial dialects), and the frequent omission of diacritics, which can introduce ambiguity. For instance, KSAA-RD shared tasks in 2023 and 2024 focus on this issue and seek to encourage the generation of Arabic RD. But challenges persist, given the intricacies of ORCID(s): Sibaee et al.: Preprint submitted to Elsevier Page 1 of Advancing Arabic Reverse Dictionary Systems Arabic morphology and the limited availability of linguistic resources. Hence, there is strong potential for further development of sound Arabic RD that will contribute to Arabic natural language processing and allow users to benefit from the same lexical precision available in other major languages. This work aims to advance the development of reliable RD models for Arabic by introducing several key contributions: We propose semi-encoder neural network with geometrically decreasing hidden layers, achieving state-of-theart performance on Arabic reverse dictionary tasks through efficient semantic representation. We establish formal quality standards for Arabic lexicographic definitions, providing systematic framework to enhance dataset construction for reverse dictionary applications. We present formal abstraction of the reverse dictionary task, offering rigorous theoretical framework for embedding-based word retrieval systems and facilitating reproducibility and future research. We develop RDTL, modular and extensible Python library with configurable training pipelines for reverse dictionary tasks, released as open-source to support the research community. In this paper, we detail our approach to constructing an Arabic RD model, beginning with an elaboration of the challenges and the need to propose new solution in Section 2. This is followed by description of our methodology in Section 3. Sections 4,5 presents thorough evaluation of the models performance on various tasks, analyzing the results and highlighting key findings in the models and dataset. Finally, Section 6 concludes the paper, discussing the implications of our work and outlining promising directions for future research in this rapidly evolving field. 2. Related Work In this section, we provide an overview of existing reverse dictionary research. We begin by discussing traditional approaches based on semantic resources such as WordNet. Next, we describe neural network methods that rely on recurrent architectures. Finally, We focus on transformer-based approaches and their advancements in this domain. 2.1. Traditional (WordNet-based) Approaches Early reverse dictionary systems often relied on semantic analysis performed on input phrases, leveraging resources like WordNet to identify and compare word senses. In these systems, words and phrases were typically mapped onto semantic space, and distance-based similarity measures were used to find the best-matching candidates. Pilehvar (2019) perform semantic analysis on input phrases using semantic similarity measures to represent words as vectors in semantic space. The semantic space is created with the assistance of WordNet. The system then applies algebraic analysis to select sample of candidate words. These candidate words undergo filtering process and ranking phase before being presented. It uses similarity measure between word and an input phrase based on distance-based similarity metric. Thorat and Choudhari (2016) employ the idea that the significance of words meaning to definition is proportional to its frequency across various definitions. The meaning is extracted from the content words within the phrase. The input phrase is split into its component words, and graph-based search is implemented through related words. distance-based similarity measure computes the words that best represent the meaning of the input phrase. graph encodes the relationships between words in its connectivity matrix, on which the similarity measures are computed. 2.2. Traditional Neural Network Methods In contrast, several studies emphasize the potential of neural networks Fatima, Samad Shaikh, Riaz, Ahmad, ElAffendi, Alyamani, Nabeel, Ali Khan, Yasin and Latif (2022), leveraging their advanced modeling techniques to capture the context and semantic meaning of text more effectively. Pilehvar (2019) present experiments using neural networkbased reverse dictionary system that focuses on distinguishing between the different meanings of word to achieve more accurate semantic understanding. To address this issue, two main neural architectures are employed for processing definitions: the Bag-of-Words (BoW) model and the Recurrent Neural Network (RNN) model, aiming to resolve the challenges associated with polysemy. Bendahman, Breton, Nicolaieff, Billami, Bortolaso and Miloudi (2022) use 1https://github.com/serrysibaee/reverse_dictionary Sibaee et al.: Preprint submitted to Elsevier Page 2 of 12 Advancing Arabic Reverse Dictionary Systems sequential models that integrate various neural networks, starting with embedding layers and progressing through dense layers, Bidirectional Long Short-Term Memory (BiLSTM) networks, and LSTM networks. All glosses are preprocessed to ensure optimal representation of word meanings. The three embedding techniques used are char, sgns, and electra. The results show that models trained with character-based or contextualized embeddings outperform those using Skip-Gram word embeddings. Their model effectively learns to map arbitrary-length phrases to fixedlength continuous-valued word vectors. Chen and Zhao (2022) present model that functions as neural dictionary with two-way indexing and querying, embedding both words and definitions within shared semantic space. The model focuses on two primary tasks: retrieving words based on their definitions and generating definitions for given words. It uses separate encoder and decoder networks for words and definitions, complemented by shared layer that aligns them within the same representation space. The model demonstrates impressive performance on established benchmarks without requiring additional resources, and human evaluations indicate preference for its outputs. Agrawal, Shanly, Vaishnaw and Singh (2021) combine the Continuous Bag-of-Words (CBOW) model, which considers fixed number of surrounding words without accounting for their order, with Recurrent Neural Network (RNN), which uses only the preceding context while preserving word order, to create reverse dictionary that effectively captures both context and word order. Malekzadeh, Gheibi and Mohades (2021) develop four different architectures for Persian reverse dictionary using (phrase, word) pairs sourced from online Persian dictionaries. These architectures include Bag-of-Words (BoW) model, RNN, LSTM, and BiLSTM models with additive attention. Each model maps descriptive phrases to their corresponding words, with the additive attention mechanism notably enhancing their ability to generate relevant word suggestions. The method presented in Zhang, Qi, Liu, Wang, Liu and Sun (2019) employs multi-channel reverse dictionary model based on Bidirectional LSTM with an attention mechanism to encode input queries. Multiple predictors, such as POS tag predictor and morpheme predictor, are incorporated to enhance the accuracy of identifying target words from given definitions. Similarly, the work in Qi, Zhang, Yang, Liu and Sun (2020) adopts comparable approach, introducing an innovative, open-source, online reverse dictionary system. This contribution marks significant advancement in the field. 2.3. Transformer-based Approaches More recent research utilizes transformer architectures capable of processing longer sequences and richer context. By incorporating specialized tokenization strategies and attention mechanisms. These methods significantly improve semantic representation and retrieval accuracy in reverse dictionary applications Alshattnawi, Shatnawi, AlSobeh and Magableh (2024). Hedderich, Yates, Klakow and de Melo (2019) introduce attention mechanisms to integrate multi-sense embeddings, which are vector representations capturing different meanings (senses) of word using word2vec and GloVe Roman, Shahid, Khan, Koubaa and Yu (2021). These embeddings are combined with LSTM models and contextual word embeddings, such as Bidirectional Encoder Representations from Transformers (BERT), to enhance performance in the reverse dictionary task. The findings demonstrate significant improvements in both input sequences and target representations, alongside valuable insights into sense distributions and learned attention patterns. The KSAA-RD shared task Al-Matham, Alshammari, AlOsaimy, Alhumoud, Wazrah, Altamimi, Alharbi and Alaifi (2023) focuses on developing Reverse Dictionary (RD) system specifically for Arabic with cross-lingual reverse dictionaries (CLRD), aiming to advance Arabic natural language processing, foster research, and create tools to enhance understanding and usage of the Arabic language. The winning teams are, respectively, modeled by Elbakry, Gabr, ElNokrashy and AlKhamissi (2023). They leverage multiple BERT-based pre-trained models for RD with output averaging, including camelBERT-MSA, camelBERT-Mix, MARBERTv2, and AraBERTv2. They achieve ranking of 24.20 using ELECTRA embeddings when ensemble embeddings from camelBERT-MSA and MARBERTv2 models are employed. For the CLRD task, the same models attain rank of 12.70 with ELECTRA embeddings. Sibaee, Ahmad, Khurfan, Sabeeh, Bahaaulddin, Belhaj and Alharbi (2023) implemented two-stage approach. In the first stage, the Sentence Transformer (SBERT) is used to generate fixed-length embeddings. Semi-Decoder model is employed in the second stage to transform the inputs into output vectors with two dimensions: ELECTRA (265d) and SGNS (300d). The model achieves high RD rankings, with the best result obtained using ELECTRA embeddings, reaching rank of 28.10. Qaddoumi (2023) introduced modified BERT Multilingual model for both RD and CLRD tasks with data augmentation due to the limited size of the available data. Data augmentation techniques include synonym replacement, random word insertion, deletion, swapping within sentences in English, and random word deletion and swapping in Arabic. They achieve rank of 28.50 for the RD task and 28.10 for the CLRD task with ELECTRA embeddings. Finally, Taylor (2023) employ rule-based approach for RD and CLRD tasks. They do not use neural networks but use SGNS Sibaee et al.: Preprint submitted to Elsevier Page 3 of 12 Advancing Arabic Reverse Dictionary Systems Work WordNet-based MÃ©ndez, Calvo and Moreno-ArmendÃ¡riz (2013) Graph-based Thorat and Choudhari (2016) BoW/RNN Hybrid Pilehvar (2019) BiLSTM Bendahman et al. (2022) Unified Model Chen and Zhao (2022) CBOW-RNN Agrawal et al. (2021) BiLSTM+Attention Malekzadeh et al. (2021) Multi-channel BiLSTM Qi et al. (2020) Multi-sense with BERT Hedderich et al. (2019) Ensemble BERT Elbakry et al. (2023) SBERT+Semi-Decoder Sibaee et al. (2023) BERT Multilingual Qaddoumi (2023) Rule-based Vector Taylor (2023) Word Sense Integration Alshammari et al. (2024) Semi-encoder Sibaee et al. (2024) Multi-task Framework Chen et al. (2024) AraT5 V2 Alheraki and Meshoul (2024) AraT5 V2 Fine-tuned Alharbi (2024) Model Type Traditional Traditional Neural Network Neural Network Neural Network Neural Network Neural Network Neural Network Transformer Transformer Transformer Transformer Traditional Transformer Transformer Transformer Transformer Transformer Primary Language Focus Attention Mechanism Multi-Channel English English English English English English Persian Multi-lingual English Arabic Arabic Arabic/English Arabic Arabic Arabic Arabic Arabic Arabic Pre-trained Base Character/ELECTRA/SGNS BERT camelBERT/MARBERT/AraBERT SBERT/ELECTRA/SGNS BERT Multilingual SGNS/ELECTRA distiluse/MiniLM/mpnet/AraBERT AraBERT/CAMeLBERT multilingual-22-125 AraELECTRA/AraBERT/camelBERT Table 1 Comparison of Reverse Dictionary approaches by model type, language focus, and features. and ELECTRA models for semantic vector representations. They build dataset-based dictionary and expand it using glosses, focusing on normalization, verb inflection, and gloss adjustments while omitting certain parts of speech. The dictionary-based approach with SGNS embeddings achieves score of 43.8 for the RD task, lower than the baseline model, and 48.87 for the CLRD task. Alshammari, Almazrua, Al Wazrah, Almatham, Alhoshan and Alosaimy (2024), in the KSAA-RD shared task, integrated Word Sense Disambiguation (WSD) specifically for Contemporary Arabic into Reverse Dictionary (RD) tasks, enhancing the clarification of word meanings within context. Sibaee, Alharbi, Ahmad, Nacar, Koubaa and Ghouti (2024) used semi-encoder structure for reverse dictionary tasks, while two-stage neural network was employed for word sense disambiguation. In the first stage, each input was processed independently through neural network, and the resulting outputs were then concatenated and passed through another similar network. Various sentence transformer models, including distiluse-base-multilingual-cased-v1, MiniLM-L12-v2, and mpnet-base-v2, were evaluated for their suitability in text encoding tasks alongside the AraBERTv2 model. Using ELECTRA with this model, their system achieved the best rank and an MSE score of 0.0644 and 0.059. Chen, Zhao and Shao (2024) developed multitask framework combining RD, definition generation, and reconstruction using transformers. They explored three segmentation strategies: the whitespace tokenizer (as in the original work), the AraBERTv2 tokenizer based on Farasa, and the CAMeLBERT tokenizer, to segment the definitions and pair them with the provided embeddings, regardless of the model from which they were derived. The AraBERT (Farasa) tokenizer yielded promising performance across metrics in the development phase, with ranking score of 0.4834. Alheraki and Meshoul (2024) leveraged AraT5 V2 with SentencePiece tokenization, combining glosses and Wikipedia examples. word and its definition were combined into single input string, embedded using the multilingual-22-125 model, and cosine similarity was used for vector search to retrieve contextually relevant examples. They trained two models: the first was on glosses only, while the second was on glosses merged with the retrieved examples. Their methodology achieved the highest-ranking score of 0.1781 using ELECTRA embeddings with glosses only. Alharbi (2024), like Alheraki and Meshoul (2024), utilized the AraT5 V2 model for the Arabic RD task with gloss-to-embedding mapping, employing three different architectures of contextualized word embeddings: AraELECTRA, AraBERTv2, and camelBERT-MSA. This approach achieved ranking score of 0.2482 by fine-tuning gloss-based embeddings with the AraT5 V2 model. Table 1 provided comprehensive comparison of the various reverse dictionary approaches, highlighting their model types, language focus, and key features. As shown in Table 1, there has been clear evolution from traditional methods to neural networks and finally to transformer-based architectures, with increasing adoption of attention mechanisms and multi-channel approaches, particularly for Arabic language models. 3. Methodology This section describes the methodology employed in our experiments, starting with the dataset and its augmentation, followed by the model architecture and training approach. Sibaee et al.: Preprint submitted to Elsevier Page 4 of Term (cid:11)(cid:235) (cid:81) (cid:21)(cid:106)(cid:46) (cid:9)(cid:224)(cid:64)(cid:241)(cid:75)(cid:10) (cid:15)(cid:11)(cid:89)(cid:203)(cid:64) (cid:16)(cid:233)(cid:171)(cid:65)(cid:212)(cid:103)(cid:46) Advancing Arabic Reverse Dictionary Systems Definition (cid:201) (cid:21)(cid:147) (cid:11)(cid:241)(cid:203)(cid:64) (cid:9)(cid:224)(cid:64)(cid:241)(cid:75)(cid:10)(cid:89)(cid:203)(cid:64) (cid:11)(cid:16)(cid:174)(cid:203)(cid:64) (cid:15)(cid:89) (cid:9)(cid:147) (cid:241)(cid:235)(cid:240) (cid:44) (cid:16)(cid:233)(cid:170)(cid:74)(cid:10)(cid:162) (cid:9)(cid:89)(cid:203)(cid:64) (cid:16)(cid:233)(cid:131)(cid:80)(cid:89)(cid:211) (cid:64)(cid:241)(cid:9)(cid:75)(cid:15)(cid:241)(cid:187) (cid:9)(cid:225)(cid:75)(cid:10) Translation Estrangement; it is the opposite of union. (cid:16)(cid:233)(cid:17)(cid:75)(cid:67)(cid:17)(cid:74)(cid:203)(cid:64) (cid:88)(cid:65)(cid:16)(cid:174)(cid:9)(cid:74)(cid:203)(cid:64) (cid:250)(cid:206)(cid:171) (cid:16)(cid:135)(cid:202)(cid:162)(cid:16)(cid:29) group of three critics who (cid:11)(cid:248)(cid:10) (cid:88)(cid:64)(cid:241)(cid:203)(cid:64) (cid:12) (cid:11)(cid:15)(cid:130)(cid:203)(cid:64) (cid:113) (cid:201)(cid:74)(cid:10) (cid:11) (cid:202)(cid:103)(cid:46) (cid:13) (cid:67)(cid:211)(cid:240) (cid:233)(cid:11)(cid:74)(cid:46) (cid:9)(cid:75)(cid:64)(cid:241)(cid:107)(cid:46) (cid:9)(cid:172)(cid:81)(cid:107)(cid:46) (cid:90)(cid:65)(cid:211) (cid:232) Table 2 Sample entries from the reverse dictionary dataset. formed the Diwan School: Al-Aqqad, Al-Mazini, and Shukri. They agreed on poetic vision and artistic foundations. The flood eroded its sides and filled it with water. 3.1. Data Description The dataset consists of 31,372 training samples, 3,921 validation samples, and 3,922 test samples. Each sample is represented as pair (def, word), where both the definition and target word are provided in text and embedding formats. For this task, we were given the Electra embeddings (256-dimensional) ğ‘’ Word â„256 Antoun, Baly and Hajj (2021). To enhance model performance, the dataset was expanded by incorporating approximately 84,000 additional samples containing Electra embeddings from an external source. Following prior analysis, the training and development sets were merged, and no separate results are reported for the development subset. Table 2 depicts sample from the dataset. 3.2. The proposed Neural Network Architecture Let ğ· be dataset consisting of pairs (def ğ‘–, word ğ‘–), where each definition def ğ‘– is associated with corresponding word word ğ‘– . We define two embedding functions: ğ‘“ Def â„ğ‘‘ maps definition to vector representation in â„ğ‘‘. ğ‘’ Word â„ğ‘ maps word to vector representation in â„ğ‘. Our model is neural network ğ‘š â„ğ‘‘ â„ğ‘ optimized through extensive experimentation, which learns transformation between the definition and word embeddings: ğ‘š(ğ‘“ (def ğ‘–)) ğ‘’(word ğ‘–). (1) The architecture follows semi-encoder structure with four hidden layers, where the layer sizes decrease geometrically by factor of 2, starting from 8ğ‘  (where ğ‘  = ğ‘‘ = 256) down to ğ‘ : â„1 = 8ğ‘ , â„2 = 4ğ‘ , â„3 = 2ğ‘ , â„4 = ğ‘ . The model is trained using the Mean Squared Error (MSE) loss: = 1 ğ· ğ‘–ğ¼ ğ‘š(ğ‘“ (def ğ‘–)) ğ‘’(word 2. ğ‘–) (2) We employ the GELU activation function for non-linearity and apply an adjustable dropout rate ğ‘‘ [0.2, 0.4] to mitigate overfitting. The AdamW optimizer is used with learning rate of ğœ‚ = 1104, as higher values (e.g., 1103) led to instability in previous experiments and this architicture is chosen after empirical expirements from Sibaee et al. (2023). The described architecture is consistent across all experiments, with modifications restricted to dropout rates and optimizer settings. The compact training process is shown in Figure 1. Sibaee et al.: Preprint submitted to Elsevier Page 5 of 12 Advancing Arabic Reverse Dictionary Systems Text Encoder emb Decoderl ğ‘¦ (cid:233)(cid:212)(cid:103)(cid:81)(cid:75)(cid:10)(cid:240) (cid:233)(cid:74)(cid:10)(cid:202)(cid:171) (cid:9)(cid:173)(cid:162)(cid:170)(cid:16)(cid:75) (cid:44) (cid:209)(cid:107)(cid:81)(cid:16)(cid:75) encoder(text) = ğ‘¥ have mercy on him ğ‘¥ â„ğ‘‘ decoder(ğ‘¥) = ğ‘¦ Loss(ğ‘¦, ğ‘¦) ğ‘¦ = ğ‘’ğ‘šğ‘output (cid:9)(cid:173)(cid:162)(cid:170)(cid:16)(cid:75) (cid:44)(cid:209)(cid:107)(cid:81)(cid:16)(cid:75) (cid:233)(cid:212)(cid:103)(cid:81)(cid:75)(cid:10)(cid:240) (cid:233)(cid:74)(cid:10)(cid:202)(cid:171) Figure 1: Overview of the encoder-decoder flow with embedding and target loss. have mercy on him 3.2.1. Inference and Ranking-Based Evaluation Given test definition def, we retrieve the most relevant word by ranking candidate words based on similarity metric. Similarity Computation We define similarity function ğ‘† â„ğ‘ â„ğ‘ â„, such as cosine similarity: ğ‘†(ğ‘¥, ğ‘¦) = ğ‘¥ ğ‘¦ ğ‘¥ğ‘¦ For each candidate word ğ‘¤ ğ‘‰ , we compute: ğ‘†ğ‘¤ = ğ‘†(ğ‘š(ğ‘“ (def)), ğ‘’(ğ‘¤)). Rank Assignment (3) (4) For each word ğ‘¤, set of metric scores ğ‘€ğ‘¤ = {ğ‘€1(ğ‘¤), ğ‘€2(ğ‘¤), , ğ‘€ğ‘˜(ğ‘¤)} is computed, where each ğ‘€ğ‘–(ğ‘¤) represents different evaluation metric (e.g., MSE, cosine similarity, rank). The rank transformation is defined as: ğ‘…ğ‘¤ = ğ‘…ğ‘¤ = ğ‘¥ğ‘€ğ‘¤ ğ‘¥ğ‘€ğ‘¤ ğŸ£(ğ‘¥ ğ‘€ğ‘¤) if the metric is to be maximized, ğŸ£(ğ‘¥ ğ‘€ğ‘¤) if the metric is to be minimized. where ğŸ£() is the indicator function. Final Ranking Score For each language ğ“, the final ranking score is computed as the mean of all individual ranks ğ‘˜: Final Rank ğ“ = 1 ğ‘˜ ğ‘˜ ğ‘–= ğ‘…ğ‘–,ğ‘¤. The final rankings are aggregated across users and stored for comparison. 4. Results In this section we show our results (trained on the model shown in Section 3). (5) (6) (7) 4.1. Models results In this section we show the results of all tested models in terms of MSE, cosine similarity, and rank  (Table 3)  . Sibaee et al.: Preprint submitted to Elsevier Page 6 of 12 Advancing Arabic Reverse Dictionary Systems Model MSE Cosine Similarity Rank baselineM_elct Alshammari et al. (2024) MARBERTv2 Abdul-Mageed, Elmadany and Nagoudi (2021) ARBERTv2 Abdul-Mageed et al. (2021) OpenAI large OpenAI small Gate v1 Nacar, Koubaa, Sibaee and Ghouti (2025) ATMv2 Nacar and Koubaa (2024) Nomic-embed-text-v2 Nussbaum and Duderstadt (2025) mpnet Reimers and Gurevych (2020) L12 Reimers and Gurevych (2020) LaBSE Reimers and Gurevych (2020) CasedV1 Reimers and Gurevych (2020) Snowflake-arctic-embed Yu, Merrick, Nuti and Campos (2024) 0.145 0.227 0.158 0.177 0.187 0.208 0.222 0.215 0.225 0.220 0.221 0.226 0. 0.736 0.559 0.7071 0.673 0.649 0.596 0.570 0.587 0.562 0.560 0.570 0.560 0.607 0.84 0.174 0.0644 0.077 0.097 0.221 0.183 0.145 0.22 0.22 0.17 0.21 0.117 Table 3 Comparison of model performance based on MSE, cosine similarity, and rank. 5. Discussion In this section we anaylze and discuss the results and models performences from section 4. 5.1. Analyzing models performance The extended experiments further validate our initial findings regarding Arabic language models. ARBERTv2 achieves the best rank metric (0.0644), which is the primary evaluation criterion for our study. This superior rank performance demonstrates ARBERTv2s exceptional ability to preserve relative semantic distances between embeddings in Arabic text. While baselineM_elct shows the lowest MSE (0.145) and highest cosine similarity (0.736), ARBERTv2s rank performance indicates its particular strength in maintaining semantic relationships that are crucial for downstream applications. The rank metric results reveal significant performance gap between Arabic-specific models and general multilingual embeddings. OpenAI models (large and small) perform competitively with rank scores of 0.077 and 0.097 respectively, suggesting that large-scale training across multiple languages can partially compensate for language-specific optimization. However, their rank scores still trail behind ARBERTv2, confirming the advantage of language-specialized training. General embedding models (Nomic-embed-text-v2, Matyroshka, mpnet) show substantially weaker rank performance (0.145-0.22), further emphasizing the importance of language-specific training for optimal semantic representation. MARBERTv2s poor rank performance (0.174) compared to ARBERTv2 highlights that architectural variations within the same model family can significantly impact embedding quality. These findings reinforce our conclusion that the choice of pre-training data and optimization strategy is crucial for developing effective language-specific embedding models, particularly when evaluated on the rank metric that best captures semantic relationship preservation. 5.2. Analyzing the Dataset the dataset (with an average score 3 5 to the following observations: In this section, we outline the key insights derived from analyzing more than 200 randomly selected samples from as depicted in Figure 2). Each sample was evaluated on scale of 1 to 5, leading Lack of systematic and logical methodology for definitions. The definitions exhibit significant variability in formulation. Some focus on morphological forms, while others emphasize specific meaning derived from the general meaning of the Arabic root. Additionally, some definitions are extracted from specialized scientific contexts without specifying the targeted field. Furthermore, certain definitions include the word(s) intended to be defined within the definition itself, which is an unclear approach. Examples illustrating these issues are detailed below. 1. Reliance on morphological forms rather than defining meanings. Several definitions rely on morphological derivations rather than providing clear meanings. For instance, the word (cid:201)(cid:210) (cid:17)(cid:130)(cid:16)(cid:29) (to include) is defined by listing its morphological derivatives: (cid:233)(cid:75)(cid:46) (cid:201)(cid:15)(cid:210) (cid:17)(cid:130)(cid:16)(cid:28)(cid:12)(cid:211) (cid:200)(cid:241)(cid:170) (cid:9)(cid:174)(cid:214)(cid:207)(cid:64)(cid:240) (cid:201)(cid:15)(cid:210) (cid:17)(cid:130)(cid:16)(cid:28)(cid:12)(cid:211) (cid:241)(cid:234)(cid:9)(cid:175) (cid:44) (cid:19) (cid:67)(cid:15)(cid:210) (cid:17)(cid:130)(cid:16)(cid:29) (inclusion, one who includes, and the object being included), none of which convey the actual meaning. Similar Sibaee et al.: Preprint submitted to Elsevier Page 7 of Advancing Arabic Reverse Dictionary Systems Figure 2: Accuracy distribution cases include (cid:168)(cid:241)(cid:9)(cid:74)(cid:75)(cid:10) (to vary), (cid:89)(cid:170)(cid:147) (to ascend), and (cid:9)(cid:173)(cid:203)(cid:65)(cid:109)(cid:26)(cid:16)(cid:39) (to form an alliance), where only verbal nouns and inflections are mentioned instead of semantic explanations. Our evaluation indicates that more than 30% of the dataset follows this pattern. 2. Use of pronouns without clear reference to the defined word. (cid:16)(cid:232)(cid:81)(cid:187) (cid:64) (cid:15)(cid:9)(cid:89)(cid:203)(cid:64) (cid:80)(cid:241)(cid:146) Some definitions employ pronouns without clearly linking them to the term being defined. For example, in definition unclear. (cid:12)(cid:16)(cid:175) (cid:65)(cid:234) (cid:9)(cid:174)(cid:170) (cid:9)(cid:147) (its weakness is memory deficiency), the pronoun (cid:65)(cid:235) (its) is ambiguous, making the 3. Overly specific definitions that do not reflect general meanings. Some definitions are excessively specialized, restricting the broader meaning of term. For example: (cid:16)(cid:232)(cid:64)(cid:88) (cid:11)(cid:9)(cid:172)(cid:240)(cid:81)(cid:109)(cid:204)(cid:39)(cid:64) (cid:65)(cid:238)(cid:69)(cid:46) (cid:104)(cid:46) (cid:81) (cid:9)(cid:106)(cid:16)(cid:74)(cid:130)(cid:29)(cid:10) (cid:21)(cid:9)(cid:74)(cid:211)(cid:11) (tweezers) is defined as (cid:9)(cid:173)(cid:147)(cid:64)(cid:15)(cid:81)(cid:203)(cid:64) (cid:17)(cid:128)(cid:65)(cid:16)(cid:174) ters), which is rare technical definition, whereas its more common meaning is (cid:89)(cid:202)(cid:109)(cid:46)(cid:204)(cid:39)(cid:64) (a tool for removing impurities from the skin). (cid:13) (cid:16)(cid:233)(cid:15)(cid:109)(cid:25)(cid:149) (cid:15)(cid:9)(cid:224) (cid:64) (cid:233)(cid:211)(cid:64)(cid:241)(cid:16)(cid:175) (cid:16)(cid:233)(cid:15)(cid:74)(cid:10)(cid:202)(cid:190) (cid:17)(cid:131) (formalism) is defined within legal context as (cid:15)(cid:16)(cid:174)(cid:107) (to interrogate someone) is narrowly defined in legal contexts as (cid:65)(cid:211) (cid:16)(cid:233)(cid:19) (cid:16)(cid:135) (cid:16)(cid:135) (cid:13) (cid:64) (a tool used by typesetter to extract let- (cid:13) (cid:16)(cid:232)(cid:64)(cid:88) (cid:9)(cid:225)(cid:211) (cid:73)(cid:46) (cid:64) (cid:13) (cid:64)(cid:89)(cid:74)(cid:46)(cid:211) (a principle (cid:13) (cid:11) (cid:9)(cid:89) (cid:9)(cid:103) (cid:203)(cid:64)(cid:241)(cid:16)(cid:175) (cid:9)(cid:175) (cid:233) (cid:15)(cid:74)(cid:10) (cid:9)(cid:146)(cid:16)(cid:175) (cid:250)(cid:10) (cid:64) (cid:15)(cid:16)(cid:174)(cid:107) (to investigate) is also widely used in manuscript stating that the validity of legal acts...), whereas it is more commonly used in philosophy and the arts. (cid:13) (cid:64) (cid:16)(cid:233)(cid:15)(cid:74)(cid:10)(cid:9)(cid:75)(cid:241)(cid:9)(cid:75)(cid:65)(cid:16)(cid:174)(cid:203)(cid:64) (cid:200)(cid:65)(cid:212)(cid:171)(cid:13) (cid:66)(cid:64) (cid:16)(cid:233)(cid:203)(cid:64) (cid:9)(cid:80)(cid:66)(cid:13) (cid:13)(cid:75)(cid:64)(cid:241) (cid:17)(cid:130)(cid:203)(cid:64) (cid:9)(cid:224)(cid:67)(cid:9)(cid:175) (cid:169)(cid:211) (taking someones statements in case), though studies and literary contexts. 4. Definitions that are too field-specific without indicating general meaning. Many definitions are provided from specialized disciplinary perspectives without considering broader meanings. For example: (cid:9)(cid:175)(cid:81)(cid:171) (customary) is given only its legal definition, ignoring its general usage. (cid:9)(cid:224)(cid:89)(cid:75)(cid:46) (the main part of garment), which is narrow textile-related meaning (cid:250)(cid:10) (cid:9)(cid:224)(cid:89)(cid:75)(cid:46) (body) is defined as (cid:72)(cid:46) (cid:241)(cid:17)(cid:74)(cid:203)(cid:64) instead of the more common (cid:213)(cid:230)(cid:132)(cid:109)(cid:46)(cid:204)(cid:39)(cid:64) (body). (cid:81)(cid:16)(cid:75)(cid:240) (chord) is defined in mathematical sense as (cid:17)(cid:73)(cid:202)(cid:17)(cid:74)(cid:214)(cid:207)(cid:64) (cid:81)(cid:16)(cid:75)(cid:240) (the hypotenuse of triangle) rather than the more general meaning (cid:233)(cid:202)(cid:74)(cid:46)(cid:107) (cid:128)(cid:241)(cid:16)(cid:174)(cid:203)(cid:64) (cid:81)(cid:16)(cid:75)(cid:240) (the bowstring of bow). Sibaee et al.: Preprint submitted to Elsevier Page 8 of 12 5. Definitions using illustrative phrases instead of direct explanations. Advancing Arabic Reverse Dictionary Systems Some definitions rely on idiomatic expressions rather than clearly defining the word itself. For example: (cid:16)(cid:175)(cid:80) (my neck is stopper) is given instead of direct definition of the intended meaning, the (cid:16)(cid:232)(cid:88) (cid:21) (cid:64) (cid:11)(cid:89) (cid:11)(cid:131) (cid:250)(cid:10) (cid:16)(cid:230)(cid:74)(cid:46) defnition should include the information of this idiomatic expression then explaining it. 6. Use of redundant phrasing in defined terms. Some terms (defined words) use unnecessarily long formulations when shorter one would suffice. Examples include: (cid:15)(cid:11) (cid:11)(cid:9)(cid:174)(cid:9)(cid:74)(cid:16)(cid:75) (a worshipper performs extra prayers) is defined as (cid:206)(cid:146)(cid:12)(cid:220)(cid:207)(cid:64) (cid:201) (cid:250)(cid:10) whereas simply stating (cid:201) (cid:9)(cid:174)(cid:9)(cid:74)(cid:16)(cid:75) (performed extra prayers) would be clearer. (cid:9)(cid:224)(cid:67)(cid:9)(cid:175) (cid:250)(cid:206)(cid:171) (cid:16)(cid:73)(cid:131)(cid:80) (the bid was awarded to someone) is defined as (cid:16)(cid:73)(cid:131)(cid:80) (the bid was awarded) alone is sufficient and provides adequate context. (cid:12)(cid:16)(cid:233)(cid:146)(cid:16)(cid:175)(cid:65)(cid:9)(cid:74)(cid:214)(cid:207)(cid:64) (cid:16)(cid:233)(cid:146)(cid:16)(cid:175)(cid:65)(cid:9)(cid:74)(cid:214)(cid:207)(cid:64) Though 7. Use of synonyms instead of actual definitions. Some definitions list synonyms rather than providing precise explanation. For example: (cid:129)(cid:171)(cid:65)(cid:16)(cid:174)(cid:16)(cid:74)(cid:211)(cid:240) (cid:44)(cid:201)(cid:210)(cid:11) (cid:21)(cid:234)(cid:12)(cid:211) (negligent and sluggish) is given as the definition of (cid:9)(cid:224)(cid:240)(cid:11) (cid:65)(cid:11)(cid:238) (cid:11)(cid:16)(cid:68)(cid:12)(cid:211) (careless), which does not clarify its meaning. 5.2.1. Standards for Building RD Dataset From the lessons learned from the previous experiments, and to ensure high-quality dataset construction for reverse dictionary application in Arabic, the following standards (which was extracted after analyzing random samples from the dataset) should be maintained. Standards are summarized in Figure 3: 1. Definitions should provide clear meanings rather than morphological forms. Avoid listing derivatives and inflections without conveying the actual meaning of the term. Instead, focus on explicit semantic explanations. 2. Pronoun references should be unambiguous. Ensure that pronouns have clear and direct connection to the defined word to prevent ambiguity in interpretation. 3. Definitions should reflect general meanings before specialized ones. definition should first cover the broadest possible meaning before specifying context-dependent interpretations, ensuring wider applicability. 4. Indicate specialized meanings explicitly. When providing definition from specific field (e.g., legal, mathematical, or medical contexts), explicitly state the domain to avoid confusion. 5. Avoid using illustrative phrases instead of definitions. Definitions should be direct and explanatory rather than relying on idiomatic expressions or metaphors. 6. Eliminate redundant phrasing. Use concise and precise language to convey meaning efficiently, avoiding unnecessary elaboration. 7. Use accurate and informative descriptions instead of synonyms. definition should clarify the meaning rather than merely listing synonyms, ensuring that users fully understand the concept. 8. Maintain logical consistency and systematic structuring. Definitions should follow uniform and logical methodology, ensuring consistency throughout the dataset. 6. Conclusions This study presented the development of an Arabic Reverse Dictionary (RD) system that addresses significant challenges in Arabic natural language processing. Our semi-encoder neural network architecture with geometrically decreasing layers achieved state-of-the-art results, demonstrating the effectiveness of language-specialized models for semantic representation tasks. The comparative analysis of various embedding models confirms that Arabicspecific pre-trained models, particularly ARBERTv2, significantly outperform general multilingual embeddings in maintaining semantic relationships essential for RD applications. Our detailed analysis of the dataset revealed critical insights into the quality and consistency of Arabic lexicographic definitions, leading to the establishment of eight formal standards for constructing high-quality RD resources. These standards emphasize clarity over morphology, Sibaee et al.: Preprint submitted to Elsevier Page 9 of 12 Advancing Arabic Reverse Dictionary Systems Figure 3: Summary of RD writing standards. unambiguous references, general before specific meanings, domain specification, avoidance of figurative language, elimination of redundancy, comprehensive descriptions over mere synonyms, and logical structure. The modular RDTL library we developed provides researchers and developers with configurable training pipelines for RD tasks, facilitating further innovation in this field. Future work should focus on implementing these definition standards in larger datasets, exploring multi-task learning approaches that combine RD with related semantic tasks, and investigating methods to handle Arabics rich morphological structures more effectively. By advancing Arabic RD systems, this work contributes to broader accessibility of precise linguistic tools for Arabic speakers, supporting applications in education, academic writing, professional communication, and general language use. The methods and insights presented here lay foundation for continued progress in Arabic computational linguistics and semantic understanding technologies. 7. Acknowledgment The authors would like to acknowledge the support of Prince Sultan University for paying the Article Processing Charges (APC) of this publication. References Abdul-Mageed, M., Elmadany, A., Nagoudi, E.M.B., 2021. ARBERT & MARBERT: Deep bidirectional transformers for Arabic, in: Zong, C., Xia, F., Li, W., Navigli, R. (Eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association for Computational Linguistics, Online. pp. 70887105. URL: https://aclanthology.org/2021.acl-long.551/, doi:10.18653/v1/2021.acl-long.551. Agrawal, A., Shanly, K.A., Vaishnaw, K., Singh, M., 2021. Reverse dictionary using an improved cbow model, in: Proceedings of the 3rd ACM India Joint International Conference on Data Science & Management of Data (8th ACM IKDD CODS & 26th COMAD), pp. 420420. Al-Matham, R., Alshammari, W., AlOsaimy, A., Alhumoud, S., Wazrah, A., Altamimi, A., Alharbi, H., Alaifi, A., 2023. KSAA-RD shared task: Arabic reverse dictionary, in: Sawaf, H., El-Beltagy, S., Zaghouani, W., Magdy, W., Abdelali, A., Tomeh, N., Abu Farha, I., Habash, N., Khalifa, S., Keleg, A., Haddad, H., Zitouni, I., Mrini, K., Almatham, R. (Eds.), Proceedings of ArabicNLP 2023, Association for Computational Linguistics, Singapore (Hybrid). pp. 450460. URL: https://aclanthology.org/2023.arabicnlp-1.39/, doi:10.18653/v1/2023. arabicnlp-1.39. Sibaee et al.: Preprint submitted to Elsevier Page 10 of 12 Advancing Arabic Reverse Dictionary Systems Alharbi, T., 2024. MISSION at KSAA-CAD 2024: AraT5 with Arabic reverse dictionary, in: Habash, N., Bouamor, H., Eskander, R., Tomeh, N., Abu Farha, I., Abdelali, A., Touileb, S., Hamed, I., Onaizan, Y., Alhafni, B., Antoun, W., Khalifa, S., Haddad, H., Zitouni, I., AlKhamissi, B., Almatham, R., Mrini, K. (Eds.), Proceedings of the Second Arabic Natural Language Processing Conference, Association for Computational Linguistics, Bangkok, Thailand. pp. 692696. URL: https://aclanthology.org/2024.arabicnlp-1.76/, doi:10.18653/v1/2024. arabicnlp-1.76. Alheraki, M., Meshoul, S., 2024. Baleegh at KSAA-CAD 2024: Towards enhancing Arabic reverse dictionaries, in: Habash, N., Bouamor, H., Eskander, R., Tomeh, N., Abu Farha, I., Abdelali, A., Touileb, S., Hamed, I., Onaizan, Y., Alhafni, B., Antoun, W., Khalifa, S., Haddad, H., Zitouni, I., AlKhamissi, B., Almatham, R., Mrini, K. (Eds.), Proceedings of the Second Arabic Natural Language Processing Conference, Association for Computational Linguistics, Bangkok, Thailand. pp. 704708. URL: https://aclanthology.org/2024.arabicnlp-1.78/, doi:10.18653/v1/2024.arabicnlp-1.78. Alshammari, W., Almazrua, A., Al Wazrah, A., Almatham, R., Alhoshan, M., Alosaimy, A., 2024. KSAA-CAD shared task: Contemporary Arabic dictionary for reverse dictionary and word sense disambiguation, in: Habash, N., Bouamor, H., Eskander, R., Tomeh, N., Abu Farha, I., Abdelali, A., Touileb, S., Hamed, I., Onaizan, Y., Alhafni, B., Antoun, W., Khalifa, S., Haddad, H., Zitouni, I., AlKhamissi, B., Almatham, R., Mrini, K. (Eds.), Proceedings of the Second Arabic Natural Language Processing Conference, Association for Computational Linguistics, Bangkok, Thailand. pp. 677685. URL: https://aclanthology.org/2024.arabicnlp-1.74/, doi:10.18653/v1/2024.arabicnlp-1.74. Alshattnawi, S., Shatnawi, A., AlSobeh, A.M., Magableh, A.A., 2024. Beyond word-based model embeddings: Contextualized representations for enhanced social media spam detection. Applied Sciences 14, 2254. Antoun, W., Baly, F., Hajj, H., 2021. AraELECTRA: Pre-training text discriminators for Arabic language understanding, in: Habash, N., Bouamor, H., Hajj, H., Magdy, W., Zaghouani, W., Bougares, F., Tomeh, N., Abu Farha, I., Touileb, S. (Eds.), Proceedings of the Sixth Arabic Natural Language Processing Workshop, Association for Computational Linguistics, Kyiv, Ukraine (Virtual). pp. 191195. URL: https://aclanthology.org/2021.wanlp-1.20/. Bendahman, N., Breton, J., Nicolaieff, L., Billami, M.B., Bortolaso, C., Miloudi, Y., 2022. BL.Research at SemEval-2022 task 1: Deep networks for reverse dictionary using embeddings and LSTM autoencoders, in: Emerson, G., Schluter, N., Stanovsky, G., Kumar, R., Palmer, A., Schneider, N., Singh, S., Ratan, S. (Eds.), Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), Association for Computational Linguistics, Seattle, United States. pp. 94100. URL: https://aclanthology.org/2022.semeval-1.11/, doi:10.18653/v1/2022.semeval-1.11. Brown, R., McNeill, D., 1966. The tip of the tongue phenomenon. Journal of verbal learning and verbal behavior 5, 325337. Chen, P., Zhao, Z., 2022. unified model for reverse dictionary and definition modelling, in: He, Y., Ji, H., Li, S., Liu, Y., Chang, C.H. (Eds.), Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), Association for Computational Linguistics, Online only. pp. 813. URL: https://aclanthology.org/2022.aacl-short.2/, doi:10.18653/v1/2022.aacl-short.2. Chen, P., Zhao, Z., Shao, S., 2024. Cher at KSAA-CAD 2024: Compressing words and definitions into the same space for Arabic reverse dictionary, in: Habash, N., Bouamor, H., Eskander, R., Tomeh, N., Abu Farha, I., Abdelali, A., Touileb, S., Hamed, I., Onaizan, Y., Alhafni, B., Antoun, W., Khalifa, S., Haddad, H., Zitouni, I., AlKhamissi, B., Almatham, R., Mrini, K. (Eds.), Proceedings of the Second Arabic Natural Language Processing Conference, Association for Computational Linguistics, Bangkok, Thailand. pp. 686691. URL: https://aclanthology.org/ 2024.arabicnlp-1.75/, doi:10.18653/v1/2024.arabicnlp-1.75. Elbakry, A., Gabr, M., ElNokrashy, M., AlKhamissi, B., 2023. Rosetta stone at ksaa-rd shared task: hop from language modeling to worddefinition alignment, in: Proceedings of ArabicNLP 2023, Association for Computational Linguistics. p. 477482. URL: http://dx.doi.org/10. 18653/v1/2023.arabicnlp-1.43, doi:10.18653/v1/2023.arabicnlp-1.43. Fatima, R., Samad Shaikh, N., Riaz, A., Ahmad, S., El-Affendi, M.A., Alyamani, K.A., Nabeel, M., Ali Khan, J., Yasin, A., Latif, R.M.A., 2022. natural language processing (nlp) evaluation on covid-19 rumour dataset using deep learning techniques. Computational Intelligence and Neuroscience 2022, 6561622. Hedderich, M.A., Yates, A., Klakow, D., de Melo, G., 2019. Using multi-sense vector embeddings for reverse dictionaries. URL: https: //arxiv.org/abs/1904.01451, arXiv:1904.01451. Malekzadeh, A., Gheibi, A., Mohades, A., 2021. Predict: Persian reverse dictionary. URL: https://arxiv.org/abs/2105.00309, arXiv:2105.00309. Mane, S.B., Patil, H., Madaswar, K., Sadavarte, P., 2022. Wordalchemy: transformer-based reverse dictionary. URL: https://arxiv.org/ abs/2204.10181, arXiv:2204.10181. MÃ©ndez, O., Calvo, H., Moreno-ArmendÃ¡riz, M.A., 2013. reverse dictionary based on semantic analysis using wordnet, in: Advances in Artificial Intelligence and Its Applications: 12th Mexican International Conference on Artificial Intelligence, MICAI 2013, Mexico City, Mexico, November 24-30, 2013, Proceedings, Part 12, Springer. pp. 275285. Mickus, T., Van Deemter, K., Constant, M., Paperno, D., 2022. Semeval-2022 task 1: CODWOE comparing dictionaries and word embeddings, in: Emerson, G., Schluter, N., Stanovsky, G., Kumar, R., Palmer, A., Schneider, N., Singh, S., Ratan, S. (Eds.), Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), Association for Computational Linguistics, Seattle, United States. pp. 114. URL: https://aclanthology.org/2022.semeval-1.1/, doi:10.18653/v1/2022.semeval-1.1. Nacar, O., Koubaa, A., 2024. Enhancing semantic similarity understanding in arabic nlp with nested embedding learning. URL: https: //arxiv.org/abs/2407.21139, arXiv:2407.21139. Nacar, O., Koubaa, A., Sibaee, S.T., Ghouti, L., 2025. Gate: General arabic text embedding for enhanced semantic textual similarity with hybrid loss training. URL: https://huggingface.co/Omartificial-Intelligence-Space/GATE-AraBert-v1. submitted to COLING 2025. Nussbaum, Z., Duderstadt, B., 2025. Training sparse mixture of experts text embedding models. URL: https://arxiv.org/abs/2502.07972, arXiv:2502.07972. Pilehvar, M.T., 2019. On the importance of distinguishing word meaning representations: case study on reverse dictionary mapping, in: Burstein, J., Doran, C., Solorio, T. (Eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Sibaee et al.: Preprint submitted to Elsevier Page 11 of 12 Advancing Arabic Reverse Dictionary Systems Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Association for Computational Linguistics, Minneapolis, Minnesota. pp. 21512156. URL: https://aclanthology.org/N19-1222/, doi:10.18653/v1/N19-1222. Qaddoumi, A., 2023. Abed at KSAA-RD shared task: Enhancing Arabic word embedding with modified BERT multilingual, in: Sawaf, H., ElBeltagy, S., Zaghouani, W., Magdy, W., Abdelali, A., Tomeh, N., Abu Farha, I., Habash, N., Khalifa, S., Keleg, A., Haddad, H., Zitouni, I., Mrini, K., Almatham, R. (Eds.), Proceedings of ArabicNLP 2023, Association for Computational Linguistics, Singapore (Hybrid). pp. 472476. URL: https://aclanthology.org/2023.arabicnlp-1.42/, doi:10.18653/v1/2023.arabicnlp-1.42. Qi, F., Zhang, L., Yang, Y., Liu, Z., Sun, M., 2020. WantWords: An open-source online reverse dictionary system, in: Liu, Q., Schlangen, D. (Eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics, Online. pp. 175181. URL: https://aclanthology.org/2020.emnlp-demos.23/, doi:10.18653/v1/2020. emnlp-demos.23. Reimers, N., Gurevych, I., 2020. Making monolingual sentence embeddings multilingual using knowledge distillation. URL: https://arxiv. org/abs/2004.09813, arXiv:2004.09813. Roman, M., Shahid, A., Khan, S., Koubaa, A., Yu, L., 2021. Citation intent classification using word embedding. Ieee Access 9, 99829995. Sibaee, S., Ahmad, S., Khurfan, I., Sabeeh, V., Bahaaulddin, A., Belhaj, H., Alharbi, A., 2023. Qamosy at Arabic reverse dictionary shared task: Semi decoder architecture for reverse dictionary with SBERT encoder, in: Sawaf, H., El-Beltagy, S., Zaghouani, W., Magdy, W., Abdelali, A., Tomeh, N., Abu Farha, I., Habash, N., Khalifa, S., Keleg, A., Haddad, H., Zitouni, I., Mrini, K., Almatham, R. (Eds.), Proceedings of ArabicNLP 2023, Association for Computational Linguistics, Singapore (Hybrid). pp. 467471. URL: https://aclanthology.org/2023.arabicnlp-1. 41/, doi:10.18653/v1/2023.arabicnlp-1.41. Sibaee, S., Alharbi, A., Ahmad, S., Nacar, O., Koubaa, A., Ghouti, L., 2024. ASOS at KSAA-CAD 2024: One embedding is all you need for your dictionary, in: Habash, N., Bouamor, H., Eskander, R., Tomeh, N., Abu Farha, I., Abdelali, A., Touileb, S., Hamed, I., Onaizan, Y., Alhafni, B., Antoun, W., Khalifa, S., Haddad, H., Zitouni, I., AlKhamissi, B., Almatham, R., Mrini, K. (Eds.), Proceedings of the Second Arabic Natural Language Processing Conference, Association for Computational Linguistics, Bangkok, Thailand. pp. 697703. URL: https: //aclanthology.org/2024.arabicnlp-1.77/, doi:10.18653/v1/2024.arabicnlp-1.77. Siddique, B., Beg, M.M.S., 2022. Building reverse dictionary with specific application to the covid-19 pandemic. Information Technology 14, 2417 2422. URL: https://api.semanticscholar.org/CorpusID:249574030. International Journal of Taylor, S., 2023. UWB at Arabic reverse dictionary shared task: Computing the meaning of gloss, in: Sawaf, H., El-Beltagy, S., Zaghouani, W., Magdy, W., Abdelali, A., Tomeh, N., Abu Farha, I., Habash, N., Khalifa, S., Keleg, A., Haddad, H., Zitouni, I., Mrini, K., Almatham, R. (Eds.), Proceedings of ArabicNLP 2023, Association for Computational Linguistics, Singapore (Hybrid). pp. 461466. URL: https: //aclanthology.org/2023.arabicnlp-1.40/, doi:10.18653/v1/2023.arabicnlp-1.40. Thorat, S., Choudhari, V., 2016. Implementing reverse dictionary, based on word definitions, using node-graph architecture, in: Matsumoto, Y., Prasad, R. (Eds.), Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, The COLING 2016 Organizing Committee, Osaka, Japan. pp. 27972806. URL: https://aclanthology.org/C16-1263/. Yan, H., Li, X., Qiu, X., Deng, B., 2020. BERT for monolingual and cross-lingual reverse dictionary, in: Cohn, T., He, Y., Liu, Y. (Eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, Association for Computational Linguistics, Online. pp. 43294338. URL: https://aclanthology.org/2020.findings-emnlp.388/, doi:10.18653/v1/2020.findings-emnlp.388. Yu, P., Merrick, L., Nuti, G., Campos, D., 2024. Arctic-embed 2.0: Multilingual retrieval without compromise. URL: https://arxiv.org/abs/ 2412.04506, arXiv:2412.04506. Zhang, L., Qi, F., Liu, Z., Wang, Y., Liu, Q., Sun, M., 2019. Multi-channel reverse dictionary model. URL: https://arxiv.org/abs/1912. 08441, arXiv:1912.08441. Sibaee et al.: Preprint submitted to Elsevier Page 12 of"
        }
    ],
    "affiliations": [
        "College of Computer & Information Sciences, Prince Sultan University, Riyadh, Saudi Arabia",
        "Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia",
        "Independent Researcher, Riyadh, Saudi Arabia"
    ]
}