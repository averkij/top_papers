{
    "paper_title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
    "authors": [
        "Hao Dong",
        "Moru Liu",
        "Jian Liang",
        "Eleni Chatzi",
        "Olga Fink"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 5 4 7 3 2 . 5 0 5 2 : r To Trust Or Not To Trust Your Vision-Language Models Prediction Hao Dong1 Moru Liu2 1ETH Zürich Jian Liang3,4 Eleni Chatzi1 Olga Fink5 2Technical University of Munich 3NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences 4University of Chinese Academy of Sciences 5EPFL"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, training-free framework designed to address the critical challenge of estimating when VLMs predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in Vision-Language Models (VLMs) have substantially transformed the field of multimodal learning by integrating visual and textual information within unified framework. Models such as CLIP [47] and SigLIP [56] have been widely adopted for diverse tasks, including zero-shot classification [59], cross-modal retrieval [37], and image captioning [1]. Trained on large-scale image-text datasets scraped from the web, these models learn rich and transferable representations. However, despite their substantial capabilities, VLMs often encounter critical limitations when applied in real-world settings. One pressing concern is misclassification, where the model produces confident, yet incorrect, prediction that may appear both semantically plausible and visually aligned with the input. While much of the existing research has focused on improving the accuracy of VLMs outputs, the equally important issue of trustworthiness, that is, determining whether prediction should be accepted or flagged for human review, remains largely underexplored. This challenge is particularly consequential in safety-critical domains [51, 12] such as autonomous driving, medical diagnostics, and surveillance, where erroneous predictions can lead to severe outcomes. The challenge of misclassification detection (MisD) has been widely studied in the context of unimodal vision models, with numerous approaches proposed, including confidence-based scoring [23, 27], outlier exposure [5, 60, 35], and confidence learning [7, 40]. However, these approaches often overlook the unique complexities of multimodal models, where the interaction between visual inputs Preprint. Under review. and textual semantics introduces additional sources of uncertainty [13, 11]. Recently, Nguyen et al. [42] proposed utilizing human-level concepts to detect misclassification of VLMs. However, their approach necessitates the construction of numerous concepts for each class through the use of large language models, which can be demanding process. Although MisD and out-of-distribution (OOD) detection [14, 32] share the similar goal of identifying problematic inputs for trained model, they target fundamentally distinct challenges. MisD focuses primarily on identifying in-distribution samples that are incorrectly assigned to one of the known classes, often due to their proximity to decision boundaries or atypical feature representations within the learned data manifold. In contrast, OOD detection focuses on identifying inputs from entirely unseen distributions, representing novel or irrelevant stimuli rather than misclassifications within known classes. Consequently, methods tailored for one task often perform poorly on the other [26, 60]. To address the specific challenge of misclassification detection in VLMs, we propose TrustVLM training-free framework for evaluating the reliability of VLM predictions. Traditional zero-shot classification with VLMs relies primarily on the cosine similarity between text and image embeddings, often overlooking the structure and discriminative capacity of the image embedding space. This is critical limitation, as previous work has shown modality gap in VLMs like CLIP, where image and text embeddings reside in distinct regions of the shared representation space [34] (see Fig. 1). In particular, some concepts are more distinguishable in the image embedding space than in the text embedding space (Fig. 1 (b)). Building on this insight, TrustVLM leverages additional information from the image embedding space to design novel confidence-scoring function for improved misclassification detection. Specifically, our framework employs an auxiliary vision encoder to store visual prototypes for each class and assess prediction reliability through image-toimage similarity with these prototypes. Beyond misclassification detection, these prototypes can also improve zero-shot VLM predictions and be fine-tuned for improved downstream performance. We conduct rigorous evaluation of TrustVLM across 17 diverse datasets, 4 architectures, and 2 distinct VLMs. Our method achieves state-of-the-art performance in misclassification detection, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 over existing baselines. In addition, the use of visual prototypes improves the accuracy of zero-shot classification, giving an average improvement of 5.65%. The primary contributions of this work are as follows: We provide an empirical analysis of the limitations of existing MisD paradigms in VLMs, highlighting the value of leveraging information from the image embedding space. We propose TrustVLM, training-free framework that combines image-to-text and image-to-image similarity to compute robust confidence score for improved MisD. We show that visual prototypes not only support more reliable confidence estimation, but also improve zero-shot prediction accuracy, and can optionally be fine-tuned for further gains. We extensively validate TrustVLM across datasets, model architectures, and VLMs, demonstrating its generality and effectiveness. Our source code will be made publicly available to support future research in MisD for VLMs."
        },
        {
            "title": "2 Related Work",
            "content": "Misclassification Detection. The primary goal of MisD is to distinguish misclassified samples from correctly classified ones, often by evaluating the reliability of the prediction. Early approaches used simple baselines such as Maximum Softmax Probability (MSP) [23], although these were limited by model overconfidence. TrustScore [27] estimates the reliability of predictions based on distances to training samples in the feature space, though it can struggle with high-dimensional or domain-shifted data. DOCTOR [18] introduces simple rule-based rejection mechanism for black-box models without retraining, yet its performance depends on carefully tuned thresholds. Another direction involves directly learning confidence scores or training auxiliary components to predict prediction failure, such as learning the true class probability [7] or adding dedicated confidence branches [10]. OpenMix [60] enhances robustness by generating synthetic outliers during training, improving calibration on misclassified samples. Recent literature [42] has also explored MisD with VLMs by leveraging human-level concepts to detect when and interpret why model fails. Out-of-distribution Detection shares similar objective with MisD but addresses fundamentally distinct challenges. OOD detection aims to identify test samples that exhibit semantic shifts without compromising in-distribution (ID) classification accuracy, which can be broadly categorized into post 2 Figure 1: (a) CLIPs image and text embeddings are located in two completely separate regions of the embedding space. (b) The concept of \"dog\" and \"seaplane\" is more distinguishable in the image embedding space than in the text embedding space. hoc methods and training-time regularization. Post hoc methods design OOD scores based on the classification outputs of neural networks, offering the advantage of ease of use without modifying the training procedure or objective [23, 21, 36]. Training-time regularization methods address prediction overconfidence by imposing constant vector norm on the logits during training [54] or using external OOD samples from other datasets during training to improve discrimination between ID and OOD samples [24, 41]. Recently, some works [39, 28, 53] have also explored OOD detection via use of VLMs. However, methods optimized for OOD detection often underperform on MisD [26, 60], underscoring the need for specialized MisD approaches."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Preliminaries Vision-Language Models typically comprise an image encoder that projects high-dimensional images into low-dimensional embedding space and text encoder that embeds natural language into corresponding text embedding space. prominent example is CLIP [47], trained on 400 million image-text pairs, which employs contrastive loss to align image and text embeddings. Specifically, given batch of image-text pairs, CLIP maximizes the cosine similarity for the matched pairs while minimizing it for unmatched ones. During inference, the class names of target dataset are embedded using the text encoder with prompt of the form photo of [CLASS], where [CLASS] is replaced with specific class names. The text encoder then generates text embeddings tc for each class = {1, 2, . . . , C}, and the prediction probability for an input image with embedding fx is computed as: p(y = ˆyx) = exp (cos (fx, tˆy) /τ ) c=1 exp (cos (fx, tc) /τ ) (cid:80)C , (1) where cos(, ) denotes cosine similarity and τ is temperature parameter. The final prediction for is ˆy = argmaxyY p(yx), where ˆy can be either correctly classified or misclassified. Misclassification Detection, also known as failure detection [7], serves as critical safeguard for the reliable deployment of machine learning models in real-world applications. Its primary objective is to distinguish between correctly and incorrectly classified predictions, typically by leveraging confidence scores. Formally, let κ denote confidence-scoring function that quantifies the confidence of the model in its prediction. Given threshold δ R+, decision function can be defined to detect misclassifications based on whether the confidence score exceeds this threshold. For given input x: (cid:26)correct g(x) = misclassified if κ(x) δ, otherwise. (2) Baselines for MisD of VLMs. Given the prediction from Eq. (1), Maximum Softmax Probability [23] can be readily computed as confidence-scoring function. For given input x, MSP is defined 3 Figure 2: The proposed TrustVLM framework comprises three main steps. Initially, visual prototypes for each class are generated and stored using pre-trained vision encoder. Subsequently, the VLMs perform zero-shot classification and yield an image-to-text similarity score, Sit. In the third step, the initial prediction is verified using image-to-image similarity, providing an additional confidence score, Sii. Finally, these two scores are combined to determine the overall prediction confidence. as κ(x) = maxyY p(yx), where p(yx) denotes the predicted probability for class y. Similarly, various confidence scoring functions can be adopted from previous work on OOD detection, such as MaxLogit [21], Energy [36], Entropy [3], and Maximum Concept Matching (MCM) [39]. 3.2 Limitations of the Baselines Consistent with findings from previous work on misclassification detection research [26, 60], the simple MSP often outperforms more sophisticated OOD detection methods, as shown in Tab. 1. This observation suggests that advanced OOD detection methods frequently struggle to effectively capture misclassification errors in VLMs, underscoring the need to develop novel confidence-scoring functions tailored to this setting. Furthermore, the standard paradigm for zero-shot classification with VLM is mainly based on computing the cosine similarity between text and image embeddings (i.e., image-to-text similarity), as defined in Eq. (1). However, this approach often overlooks important characteristics of the image embedding space, such as image-to-image similarity. As demonstrated by Liang et al. [34], modality gap exists within the representation space of VLMs; for example, the CLIP image and text embeddings reside in distinct regions of the joint embedding space, as illustrated in Fig. 1 (a). Consequently, relying solely on image-to-text similarity for zero-shot classification and misclassification detection may neglect critical information, potentially leading to suboptimal performance. For example, Fig. 1 (b) provides concrete example using CLIP embeddings for the concepts dog and seaplane. In this case, the separation margin based on image-to-text similarity is only 0.13, whereas image-to-image similarity could yields substantially larger margin of 0.29, indicating that the two concepts are more clearly distinguishable in the image embedding space. This insight has practical implications. When an image-to-text prediction is incorrect for instance, classifying an image of dog as seaplane the image-to-image similarity between the input image and visual prototype for seaplane would likely be low, helping mitigate overconfidence. Conversely, for correct predictions (e.g., classifying dog image as dog), the image-to-image similarity with the corresponding prototype would generally be high, thereby reinforcing the prediction with greater confidence. Therefore, exploring image-to-image similarity is crucial for designing effective confidence-scoring functions to enhance misclassification detection performance in VLMs. 3.3 Proposed TrustVLM Framework Inspired by the modality gap phenomenon observed in VLMs and the enhanced distinguishability of concepts within the image embedding space, we propose TrustVLM. Our framework leverages information from the image embedding space to design the confidence-scoring function. In addition to the conventional confidence score derived from image-to-text similarity (calculated via Eq. (1)), TrustVLM incorporates second score derived from image-to-image similarity and computed using an auxiliary vision encoder. These two scores are complementary, and their effective combination leads to more robust misclassification detection. The auxiliary vision encoder can be the original CLIP 4 image encoder or other pre-trained vision models, such as MoCo v2 or DINOv2. The selection of more powerful auxiliary vision encoder can further improve misclassification detection performance. Integrate Vision Encoder for Misclassification Detection. Our TrustVLM framework operates in three main steps, as shown in Fig. 2. The first step involves generating and storing visual prototypes. Specifically, for each class c, embeddings are extracted from -shot samples in the training data using pre-trained vision encoder, (e.g., the CLIP image encoder, MoCo v2, or DINOv2). The prototype embedding for class c, Pc, is then computed by averaging these embeddings. These class prototypes {Pc} are subsequently stored. In the second step, for given input image x, zero-shot prediction ˆy is obtained using the VLM, as defined in Eq. (1), where the prediction ˆy could be either correct or wrong. Concurrently, an initial confidence score, Sit = maxyY p(yx), is derived from the image-to-text similarity. The third step focuses on generating complementary image-to-image confidence score. An embedding Ex of the input image is extracted using the same vision encoder employed in the first step. Since in the second step, the VLM believes to be class ˆy, we calculate the cosine similarity between Ex and Pˆy as the image-to-image similarity score Sii = Ex Pˆy. This Sii score is expected to be low if the prediction ˆy is incorrect, as Ex would be compared against an inappropriate prototype, thereby helping to mitigate overconfidence. Conversely, correct prediction ˆy should result in high Sii, reinforcing the predictions reliability. Finally, this verification mechanism yields the combined confidence score for input is κ(x) = Sit + Sii. Integrate Vision Encoder for Zero-shot Classification. Visual prototypes and image-to-image similarity can also be utilized to enhance the zero-shot prediction capabilities of VLMs. Given the visual prototypes {Pc} for each class = {1, 2, . . . , C}, the probability of predicting class ˆy for an input image (with embedding Ex), based on image-to-image similarity, is computed as: p(y = ˆyx) = exp (cos (Ex, Pˆy) /τ ) c=1 exp (cos (Ex, Pc) /τ ) (cid:80)C . (3) Combining Eq. (1) and Eq. (3), we get the ensemble prediction from both image-to-text and imageto-image similarity as: p(y = ˆyx) = exp (cos (fx, tˆy) /τ ) c=1 exp (cos (fx, tc) /τ ) (cid:80)C + exp (cos (Ex, Pˆy) /τ ) c=1 exp (cos (Ex, Pc) /τ ) (cid:80)C . (4) For this variant, termed TrustVLM*, the confidence-scoring function for given input is κ(x) = maxyY p(yx) + Sii. Visual Prototypes with Fine-tuning. Visual prototypes extracted from pre-trained vision encoders are typically fixed by default. In this section, we introduce TrustVLM*(F) to treat these visual prototypes as learnable parameters initialized with their pre-computed values. These parameters are subsequently fine-tuned using stochastic gradient descent. The rationale is that updating the visual prototypes can enhance affinity estimation, thereby enabling more accurate calculation of cosine similarities between test and training images, as demonstrated by [57]. Specifically, we freeze the parameters of the VLMs and the vision encoder, while fine-tuning only the visual prototypes via cross-entropy loss for 10 epochs. Finally, these learned prototypes replace the original fixed prototypes {Pc} in Eq. (4)."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setting Dataset. We evaluate our framework on wide variety of 17 datasets. Fine-grained Classification Datasets, including 10 publicly available image classification datasets: Caltech101 [16], OxfordPets [45], StanfordCars [30], Flowers102 [43], Food101 [2], FGVCAircraft [38], SUN397 [55], DTD [6], EuroSAT [20] and UCF101 [50]. These datasets constitute comprehensive benchmark, which covers diverse set of vision tasks including classification on generic objects, scenes, actions and fine-grained categories, as well as specialized downstream tasks such as recognizing textures and satellite imagery. ImageNet and Its Variants, including ImageNet [9], ImageNetV2 [48], ImageNetSketch [52], ImageNet-A [25], and ImageNet-R [22], with distribution shifts in image style, data domains, etc. We also evaluate our framework on CIFAR-10 and CIFAR-100 [31] to compare with ORCA [42]. 5 Flower102 DTD Aircraft Pets AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 68.59 88.23 167.14 MaxLogit 70.67 88.23 194.67 Energy 53.35 88.23 117.27 Entropy 64.90 88.23 153.63 MCM 55.20 88.23 112.82 DOCTOR 52.19 88.23 112.27 MSP 51.73 88.23 TrustVLM-C 101.42 48.27 88.23 TrustVLM-M 103.68 50.81 88.23 77.30 TrustVLM-D 49.07 88.28 0.52 TrustVLM*-D 49.07 88.28 TrustVLM*(F)-D 0.41 81.61 67.36 395.13 91.93 67.36 433.25 63.48 67.36 319.02 71.02 67.36 333.43 62.48 67.36 314.37 63.98 67.36 313.43 54.91 67.36 302.18 53.42 67.36 298.17 30.06 67.36 268.71 13.04 99.07 124.15 98.42 96.45 7.69 85.23 44.39 731.25 90.01 44.39 780.11 77.05 44.39 575.71 78.45 44.39 583.28 76.51 44.39 575.53 77.36 44.39 576.97 67.27 44.39 563.77 65.50 44.39 574.57 44.10 44.39 562.02 72.14 71.57 554.12 70.96 74.76 544.40 93.77 23.85 48.75 97.71 23.85 56.23 83.96 23.85 21.91 81.65 23.85 44.95 83.84 23.85 21.08 85.61 23.85 21.04 81.51 23.85 20.93 84.31 23.85 20.41 83.21 23.85 20.69 83.05 24.60 20.05 78.63 24.90 20.05 75.37 71.90 89.31 77.03 89.92 89.94 89.89 90.38 90.05 90.30 90. 69.63 64.82 79.03 78.16 79.66 79.81 82.52 83.16 88.55 78.39 80.30 55.73 45.48 73.12 72.58 73.05 72.62 75.20 73.22 75.62 75.36 76.85 74.92 69.00 84.88 78.22 85.82 85.91 88.69 88.29 95.05 95.96 98.26 Caltech101 Cars EuroSAT UCF AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 91.41 65.21 53.72 MaxLogit 94.83 65.21 59.26 Energy 70.52 65.21 15.79 Entropy 75.47 65.21 45.55 MCM 65.81 65.21 12.89 DOCTOR 64.89 65.21 12.23 MSP 55.93 65.21 TrustVLM-C 13.25 58.81 65.21 TrustVLM-M 10.81 50.68 65.21 11.11 TrustVLM-D 62.93 77.11 5.69 TrustVLM*-D 66.06 78.35 TrustVLM*(F)-D 2.61 94.55 93.31 259.81 96.36 93.31 304.90 79.39 93.31 145.61 83.54 93.31 248.87 76.36 93.31 138.00 67.88 93.31 136.27 70.30 93.31 129.45 64.24 93.31 134.25 47.27 93.31 137.54 35.62 97.04 137.97 35.71 97.16 132.54 89.58 65.61 522.56 93.89 65.61 568.57 75.76 65.61 389.07 85.97 65.61 416.37 73.66 65.61 368.77 72.25 65.61 355.66 67.73 65.61 322.52 72.29 65.61 320.12 70.62 65.61 303.79 71.25 65.83 72.96 68.70 66.02 54.35 87.79 42.10 258.47 88.30 42.10 299.84 83.16 42.10 130.10 81.18 42.10 188.00 81.69 42.10 123.41 80.65 42.10 122.44 55.73 42.10 111.95 56.32 42.10 114.60 53.52 42.10 107.13 73.50 83.56 65.26 72.48 85.69 55.81 61.85 55.38 83.97 74.14 85.67 85.98 88.69 87.93 90.21 86.23 87.26 61.94 54.85 79.83 63.77 81.50 81.95 83.67 82.53 82.05 81.73 82. 59.42 53.44 72.18 71.28 74.43 76.39 82.90 83.03 85.48 74.87 77.31 52.43 48.81 82.47 61.00 86.06 86.99 86.81 88.97 90.51 89.48 93.29 Average Food101 SUN397 AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 86.11 63.63 67.81 MaxLogit 90.19 63.63 76.05 Energy 73.14 63.63 55.80 Entropy 77.30 63.63 60.90 MCM 71.40 63.63 54.99 DOCTOR 70.10 63.63 54.80 MSP 63.20 63.63 TrustVLM-C 36.02 63.38 63.63 TrustVLM-M 40.92 54.47 63.63 TrustVLM-D 36.18 59.09 76.33 TrustVLM*-D 35.01 57.82 76.91 TrustVLM*(F)-D 34.16 78.13 83.66 278.17 84.69 83.66 304.50 63.73 83.66 194.33 67.23 83.66 242.95 61.11 83.66 184.41 59.73 83.66 182.39 57.67 83.66 171.89 59.01 83.66 172.98 55.96 83.66 156.16 57.47 84.10 104.25 56.19 84.15 103. 90.46 62.57 278.28 93.47 62.57 307.74 80.97 62.57 196.46 83.62 62.57 231.79 77.37 62.57 190.63 76.42 62.57 188.75 69.20 62.57 177.34 71.63 62.57 179.05 58.44 62.57 168.06 72.82 72.18 112.00 72.68 71.33 104.38 62.77 58.34 75.19 69.09 77.35 77.90 80.39 79.83 83.80 82.31 83.43 64.99 59.44 80.37 72.52 81.78 82.20 84.71 84.40 86.98 84.31 85.84 75.81 72.33 83.76 79.90 84.39 84.51 88.34 86.65 88.52 88.49 88.79 Table 1: Misclassification detection performance on fine-grained classification datasets with CLIP ViT-B/16, where -C, -M, and -D are with CLIP-I, MoCo v2, and DINOv2 as auxiliary vision encoders. AURC is multiplied by 103 following previous work [60]. Implementation Details. We utilize CLIP ViT-B/16 [15] backbone to perform zero-shot prediction on the benchmarks and calculate the related performance metrics. We also compare with ORCA [42] on CLIP ResNet-101 [19] and ViT-B/32 following its setup. To demonstrate the generalization of the proposed framework to different VLMs, we further evaluate on CLIP ResNet-50 and SigLIP [56] ViT-B/16. For the auxiliary vision encoder, we use both the original CLIP image encoder as well as other pre-trained models such as DINOv2 [44] and MoCo v2 [4]. We use 16-shot samples from the training data to calculate the prototypes by default and set the temperature τ to 0.01. Evaluation Metrics. AURC. The area under the risk-coverage curve (AURC) [17] depicts the error rate which is computed by using samples whose confidence is higher than some confidence thresholds. AUROC. The area under the receiver operating characteristic curve (AUROC) [8] depicts the relationship between true positive rate (TPR) and false positive rate (FPR). FPR95. The FPR at 95% TPR denotes the probability that misclassified example is predicted as correct one when the TPR is as high as 95%. ACC. Test accuracy (ACC) is also an important metric. Baselines. We compare our method against well-established confidence-scoring functions, including MaxLogit [21], Energy [36], Entropy [3], MCM [39], MSP [23], and DOCTOR [18], where DOCTOR fully exploits all available information contained in the soft-probabilities of the predictions to estimate the confidence. We also compare with the most recent concept-based method ORCA [42]. 4.2 Results MisD Results on Fine-grained Classification Datasets. As presented in Tab. 1, the simple MSP baseline consistently surpasses prominent OOD detection methods in MisD, including MaxLogit, Energy, and MCM. This indicates that current OOD detection techniques are limited in capturing misclassification errors effectively, highlighting promising direction for future research: the devel6 ImageNet-R ImageNet-A ImageNet-V2 AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 74.19 387.23 MaxLogit 74.19 421.78 Energy 74.19 298.23 Entropy 74.19 341.67 MCM 74.19 290.78 DOCTOR 74.19 290.46 MSP 74.19 TrustVLM-D 274.46 74.62 TrustVLM*-D 266.48 74.83 TrustVLM*(F)-D 264.36 50.08 254.37 50.08 278.89 50.08 189.58 50.08 236.32 50.08 181.67 50.08 180.32 50.08 176.82 53.02 172.65 52.61 176.13 61.20 138.25 61.20 160.84 61.20 79.64 61.20 109.67 75.33 61.20 74.36 61.20 72.10 61.20 71.72 63.92 71.15 63.98 68.58 64.64 77.57 71.62 79.32 79.72 81.27 77.93 77.71 85.22 89.26 79.90 80.38 73.67 71.82 66.02 74.92 72. 64.16 59.15 75.46 71.99 76.67 76.70 79.17 75.94 77.09 72.98 67.39 85.80 80.49 87.14 87.50 88.03 87.50 87.42 79.48 86.55 63.13 64.73 60.81 59.66 57.70 60.03 60.37 86.59 90.00 77.47 78.27 76.93 77.06 70.27 77.67 76.90 ImageNet-Sketch ImageNet AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 59.56 422.41 MaxLogit 59.56 472.36 Energy 59.56 313.83 Entropy 59.56 400.13 MCM 59.56 300.85 DOCTOR 59.56 299.57 MSP 59.56 TrustVLM-D 280.72 63.05 TrustVLM*-D 264.51 64.27 TrustVLM*(F)-D 243.44 45.67 228.56 45.67 253.77 45.67 149.69 45.67 206.72 45.67 140.70 45.67 138.99 45.67 132.10 53.13 126.31 58.20 129. 66.68 286.16 66.68 317.53 66.68 206.19 66.68 258.90 66.68 197.87 66.68 196.74 66.68 187.24 70.58 180.33 71.75 176.89 65.95 61.61 78.26 69.73 80.48 81.00 83.38 78.82 77.33 85.80 89.95 78.01 81.27 74.46 72.74 64.04 77.43 75.28 65.28 58.03 78.37 70.10 80.49 80.74 84.35 74.74 73.80 87.25 92.25 74.27 76.52 70.75 70.51 58.99 84.77 76.90 84.87 89.60 74.56 76.23 71.32 70.36 63.40 74.96 72. 67.39 62.16 79.09 72.79 80.82 81.13 83.24 78.99 78.67 Average Table 2: Misclassification detection performance on ImageNet and its variants with CLIP ViT-B/16. opment of confidence estimation methods that integrate OOD detection and MisD within unified framework. Incorporating an image-to-image similarity confidence score in TrustVLM significantly enhances MisD performance, irrespective of the vision encoder employed (e.g., CLIP-I, MoCo v2, or DINOv2), thereby demonstrating the proposed frameworks versatility. Notably, TrustVLM-D utilizing the DINOv2 encoder yields the best overall performance, achieving average improvements of 20.69% in AURC, 4.78% in AUROC, and 15.63% in FPR95 relative to the strongest baseline. When visual prototypes are employed for zero-shot classification (TrustVLM*-D), the accuracy improves substantially by an average of 12.7%. This enhanced accuracy reduces the overall risk (error rate) across all coverage levels; consequently, the Area Under the Risk-Coverage curve (AURC) also decreases markedly, by an average of 56.06% compared to TrustVLM-D. However, its AUROC and FPR95 metrics are marginally lower than those of TrustVLM-D. potential explanation for this is that as model accuracy improves, the confidence scores for many correct predictions might increase significantly.Nevertheless, some correctly classified but inherently difficult instances may still receive lower confidence scores. If the confidence scores of the few remaining misclassifications become more similar to these \"hard but correct\" instances, the overlap between the confidence distributions of misclassifications and correct classifications increases. This makes it harder for the detector to find good threshold. Finally, fine-tuning on visual prototypes (TrustVLM*(F)-D) further enhances performance across all evaluated metrics compared to TrustVLM*-D. MisD Results on ImageNet and Its Variants. Tab. 2 presents results from large-scale experiments conducted on ImageNet and its variants. As the variant datasets solely provide test splits, samples per class were selected from these test sets to compute prototypes, with the remaining samples utilized for evaluation. Consistent with the findings in Tab. 1, the simple MSP baseline consistently outperforms strong OOD detection methods. Our TrustVLM-D with DINOv2 encoder demonstrates superior overall performance in most cases, achieving average improvements of 9.5% in AURC, 2.11% in AUROC, and 6.96% in FPR95 relative to the strongest baseline. While TrustVLM*-D and TrustVLM*(F)-D yield significant improvements in ACC and AURC, they adversely affect AUROC Figure 3: MisD performance on ImageNet and its variants using CLIP ViT-B/16, under distribution shifts defined by prototypes computed exclusively on ImageNet and deployed directly to variant datasets. 7 ResNet-101 ViT-B/32 CIFAR-10 CIFAREuroSAT Average 85.98 MSP 85.93 ORCA 94.76 TrustVLM-D TrustVLM*-D 90.97 88.92 MSP 89.00 ORCA 94.94 TrustVLM-D TrustVLM*-D 94.06 AUROC FPR95 ACC AUROC FPR95 ACC AUROC FPR95 ACC AUROC FPR95 ACC 52.27 56.16 52.27 85.59 62.82 68.83 62.82 86.23 48.50 53.11 48.50 79.35 58.42 66.50 58.42 79.98 78.01 80.60 78.01 95.63 88.92 90.00 88.92 95. 30.30 34.76 30.30 81.78 41.11 50.00 41.11 82.81 62.98 62.68 26.82 41.88 58.66 52.70 31.85 29.58 75.12 73.83 44.32 59.85 70.00 63.66 48.95 55.11 88.98 86.43 66.69 72.15 80.24 71.29 69.36 72.77 76.14 78.47 86.27 81.77 82.16 83.32 87.65 84.57 73.40 72.38 39.46 65.52 71.09 67.00 45.63 62. 61.73 69.01 74.03 72.08 76.42 77.55 78.88 74.70 80.72 80.46 90.03 82.27 81.15 83.40 89.13 84.95 Table 3: MisD performance compared with ORCA with CLIP ResNet-101 and ViT-B/32. Method Aircraft Caltech101 Cars DTD EuroSAT Flower102 Food101 Pets SUN397 UCF101 Average CLIP-RN50 CoOp [59] Tip-Adapter [57] CuPL [46] TPT [49] DMN [58] TDA [29] ECALP [33] 15.54 22.20 23.70 19.59 17.58 20.22 17.61 21.12 29.34 TrustVLM*-C 19.89 TrustVLM*-M 20.25 TrustVLM*-D TrustVLM*(F)-D 27.27 85.88 87.70 88.80 89.29 87.02 89.09 89.70 89.94 89.98 93.27 96.43 96.59 55.74 40.37 61.30 52.20 63.90 54.70 57.28 48.64 58.46 40.84 58.36 50.53 57.78 43.74 60.56 54.49 66.09 60.34 57.52 65.43 56.08 71.34 56.19 74. 23.70 63.20 72.50 38.38 28.33 44.94 42.11 49.09 75.11 86.85 82.43 85.43 61.75 81.00 83.20 65.44 62.69 68.33 68.74 69.39 90.62 87.25 99.11 98.50 73.95 76.30 76.70 76.94 74.88 74.69 77.75 76.97 74.53 74.85 75.23 75. 83.65 86.20 86.40 84.84 84.49 86.29 86.18 88.20 85.94 90.22 83.65 83.76 58.81 63.40 66.70 62.55 61.46 63.70 62.53 64.97 66.30 67.16 71.45 70.71 58.74 67.00 72.10 58.97 60.82 64.02 64.18 66.67 72.09 73.28 75.34 76. 55.81 66.05 68.87 60.19 57.66 62.02 61.03 64.14 71.03 71.57 73.13 74.52 Table 4: Zero-shot classification results on fine-grained datasets with CLIP ResNet-50. and FPR95 on these large-scale benchmarks. To further evaluate the robustness of our proposed method to distribution shifts, prototypes were computed using only samples per class from the ImageNet training split. These prototypes were then applied directly to the ImageNet variants, thereby obviating the need to compute variant-specific prototypes. As illustrated in Fig. 3, our method demonstrates the overall leading performance of all metrics evaluated in this challenging scenario. Detailed results are provided in the Appendix. Compare with Concept-based Method. We further compare our method against the most recent ORCA [42] with CLIP ResNet-101 and ViT-B/32 on CIFAR-10, CIFAR-100, and EuroSAT. ORCA leverages human-level concepts to detect when and interpret why model fails. As shown in Tab. 3, while ORCA generally outperforms the MSP baseline in most cases, our proposed method demonstrates significant performance margin over ORCA. Specifically, with the CLIP ResNet-101 backbone, our approach achieves maximum improvements over ORCA of 9.57% in AUROC, 35.86% in FPR95, and 47.02% in ACC. When employing the CLIP ViT-B/32 backbone, these respective maximum improvements are 5.94%, 21.37%, and 32.81%. Zero-shot Classification on Fine-grained Classification Datasets. Visual prototypes and imageto-image similarity can also enhance the prediction accuracy of VLMs. In Tab. 4, we compare our method against various zero-shot, few-shot, and test-time adaptation baselines using CLIP ResNet-50. Notably, without requiring any training phase, our method achieves the best overall performance, yielding an average accuracy improvement of 4.36% relative to these baselines. Furthermore, our approach is compatible with diverse vision encoders, including CLIP-I, MoCo v2, and DINOv2, and demonstrates robust performance across all of them. Subsequent fine-tuning of the visual prototypes, as implemented in TrustVLM*(F)-D, further improves accuracy by an additional 1.29% compared to TrustVLM*-D. 4.3 Ablation Studies Different Architectures and VLMs. To demonstrate the versatility of the proposed framework, we evaluated its performance with different architectures and VLMs. Specifically, we replaced the CLIP ViT-B/16 backbone in Tab. 1 with CLIP ResNet-50 and SigLIP ViT-B/16, reporting average performance metrics across 10 fine-grained datasets in Tab. 5. Consistent with the observations in 8 CLIP ResNet-50 SigLIP ViT-B/32 DOCTOR MSP AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 70.69 55.82 149.27 259.56 70.69 55.82 149.13 259.76 70.69 55.82 129.84 TrustVLM-D 226.95 79.99 87.35 73.13 TrustVLM*-D 135.81 Table 5: Ablation on different architectures and VLMs. The average results on fine-grained classification datasets are reported. 73.83 73.11 53.98 59.97 80.55 80.63 87.31 83.51 85.69 85.86 90.35 87.47 58.63 57.31 44.49 53.97 i-t i-i AURC AUROC FPR95 188.75 214.14 168.06 Table 6: Ablation on each component. 82.20 77.15 86.98 70.10 62.26 54.47 Figure 5: Score distribution for correct and wrong predictions. Our TrustVLM achieves better separation between the score distributions of correct and wrong predictions, leading to improved performance in misclassification detection. Tab. 1, our method demonstrates robust compatibility across these varied architectures and VLMs, significantly surpassing the baseline methods in both configurations. More detailed results, including performance on ImageNet and its variants, are provided in the Appendix. Ablation on Each Component. We conducted comprehensive ablation studies to evaluate the contribution of each proposed module, as detailed in Tab. 6. We denote i-t as baseline relying solely on image-to-text similarity (akin to MSP), and i-i as the confidence score derived from our proposed image-to-image similarity module, which utilizes vision encoder. The results indicate that employing either the i-t or i-i component alone yields suboptimal performance. These findings underscore the complementary nature of the two components, with the best results obtained when they are used in combination. Influence of N. We investigated the effect of , the number of samples per class used for computing prototypes, on AUROC performance. As illustrated in Fig. 4, performance steadily improves with an increasing number of samples per class. Notably, using only single sample per class (N = 1) already achieves results superior to the baseline. The performance improvement tends to saturate when exceeds 4. Visualization. We visualized the confidence score distributions for correct and incorrect predictions on the Flower102 dataset in Fig. 5. The baseline MSP exhibits poorer separation in confidence scores between correctly classified and misclassified samples. In contrast, our solution assigns higher confidence scores to correct predictions and lower scores to incorrect ones, leading to more distinct distributions and, thereby, improved misclassification detection. Fig. 6 illustrates TrustVLMs mechanism for mitigating overconfidence. Figure 4: Influence of in prototypes."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we tackle the critical issue of misclassifications in VLMs, which hinders their reliable use, especially in safety-sensitive domains. We introduce TrustVLM, novel training-free framework 9 Figure 6: Illustration of TrustVLMs mechanism. Initially, the incorrect prediction receives higher confidence score Sit than the correct one, indicating overconfidence. By performing verification in the image embedding space using Sii, this overconfidence is mitigated. As result, the final confidence score κ(x) is significantly higher for the correct prediction than for the incorrect one. that substantially improves misclassification detection. TrustVLM uniquely leverages the commonly overlooked image embedding space by incorporating image-to-image similarity with an auxiliary vision encoder to derive more discerning confidence score. The auxiliary vision encoder can also help VLMs make better zero-shot predictions and can be fine-tuned to achieve better performance. Our rigorous evaluations across 17 datasets, 4 architectures, and 2 VLMs demonstrated TrustVLMs state-of-the-art performance. These findings highlight the considerable benefits of our approach in identifying confident, yet incorrect, VLM predictions. By enhancing the ability to determine when VLM outputs are trustworthy, TrustVLM contributes to the safer and more dependable deployment of these powerful models in real-world scenarios. Broader Impact, Limitations, and Future Work Broader Impact. The development of TrustVLM offers significant positive societal impacts by directly addressing the critical need for more reliable and trustworthy Vision-Language Models (VLMs). As VLMs become increasingly integrated into real-world applications, particularly in safetycritical domains such as autonomous driving, medical diagnostics, and public safety surveillance, the ability to accurately discern when models prediction can be trusted is paramount. Erroneous and overconfident predictions in these areas can lead to severe adverse consequences. TrustVLM contributes to mitigating such risks by providing robust framework for misclassification detection, thereby enhancing the safety of VLM-powered systems. Limitations. While TrustVLM demonstrates strong performance across diverse benchmarks, several limitations remain. First, our method assumes access to clean class-level visual prototypes, which may not always be feasible in noisy or open-world settings. Second, our current work primarily focuses on zero-shot classification. While the core principles may be adaptable, the direct applicability and performance of TrustVLM on other VLM tasks, such as visual question answering or image captioning, have not yet been extensively evaluated. Third, the approach relies on fixed auxiliary vision encoder and does not account for scenarios where the underlying data distribution evolves over time, such as in continual learning or streaming environments. Future Work. Building upon the promising results of TrustVLM, several avenues for future research warrant exploration. key direction is the extension of the TrustVLM framework to broader range of multimodal tasks beyond zero-shot classification, including visual question answering, image retrieval, and image captioning, to assess its generalizability and adapt its mechanisms where necessary. Besides, incorporating human-in-the-loop feedback for refining confidence scores may further improve VLM reliability in complex, real-world deployments."
        },
        {
            "title": "B More Details on the Datasets",
            "content": "We mainly evaluate our framework on 15 datasets from Fine-grained Classification Datasets and ImageNet and Its Variants. The fine-grained classification datasets include 10 publicly available image classification datasets, covering species of plants or animals (Flowers102 [43], OxfordPets [45]), scenes (SUN397 [55]), textures (DTD [6]), food (Food101 [2]), transportation(StanfordCars [30], 10 Figure 7: Representative examples from each dataset used in this work. FGVCAircraft [38]), human actions (UCF101 [50]), satellite images (EuroSAT [20]), and general objects (Caltech101 [16]). The ImageNet and Its Variants features the original ImageNet [9], along with several key variants: ImageNetV2 [48], an independent test set with natural images from different source; ImageNet-Sketch [52], comprising black and white sketches; ImageNet-A [25], challenging test set of natural adversarial examples often misclassified by standard ResNet-50 models [19]; and ImageNet-R [22], featuring artistic renditions of ImageNet categories. These variants collectively introduce diverse distribution shifts in image style, data domains, and other factors. Fig. 7 illustrates representative examples from these datasets."
        },
        {
            "title": "C Further Experimental Results",
            "content": "More illustration on TrustVLMs mechanism. Fig. 8 demonstrates more examples on how TrustVLM works. The Sii score is expected to be low if the prediction ˆy is incorrect, as the embedding Ex would be compared against an inappropriate prototype, thereby helping to mitigate overconfidence. Conversely, correct prediction ˆy should result in high Sii, reinforcing the predictions reliability. MisD performance on ImageNet and its variants under distribution shifts. To evaluate the robustness of our proposed method to distribution shifts, the visual prototypes were computed using only samples per class from the ImageNet training split. These prototypes were then applied directly to the ImageNet variants, thereby obviating the need to compute variant-specific prototypes. As illustrated in Tab. 7, our method demonstrates the overall leading performance of all metrics evaluated in this challenging scenario, achieving average improvements of 1.54% in AURC, 1.07% in AUROC, and 4.29% in FPR95 relative to the baseline. Different architectures and VLMs. To demonstrate the versatility of the proposed framework, we evaluated its performance with different architectures and VLMs. Specifically, we replaced the default CLIP ViT-B/16 backbone with CLIP ResNet-50 and SigLIP ViT-B/16, reporting average performance metrics across fine-grained datasets and ImageNet and its variants from Tab. 8 to Tab. 11. Consistent with the observations with CLIP ViT-B/16, our method demonstrates robust compatibility across these varied architectures and VLMs, significantly surpassing the baseline methods in both configurations. 11 Figure 8: More illustration on TrustVLMs mechanism. ImageNet-A ImageNet-V2 AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 73.98 73.98 73.98 47.85 180.06 47.85 178.82 47.85 174.23 60.88 60.88 60.88 75.98 76.07 75.87 87.12 87.46 87.69 60.54 59.43 58. 76.39 75.47 74.94 73.38 71.26 64.82 80.09 80.45 82.18 ImageNet-R DOCTOR 316.50 315.99 TrustVLM-D 324.07 MSP ImageNet AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 59.10 59.10 59.10 46.09 140.21 46.09 138.52 46.09 133.17 66.71 201.87 66.71 200.73 66.71 199.19 74.58 72.85 64.47 70.94 70.29 65.59 80.54 80.82 82. 71.44 70.32 66.03 80.53 81.04 83.08 80.85 81.17 82.24 Average DOCTOR 296.20 294.85 TrustVLM-D 289.52 MSP 77.76 77.76 77.20 ImageNet-Sketch 93.13 93.63 98.42 97.82 Caltech101 Table 7: MisD performance on ImageNet and its variants using CLIP ViT-B/16, under distribution shifts defined by prototypes computed exclusively on ImageNet and deployed directly to variant datasets. Flower102 DTD Aircraft Pets DOCTOR MSP AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 30.65 91.55 26.03 28.71 91.55 25.14 28.71 91.55 TrustVLM-D 16.62 29.97 91.63 TrustVLM*-D 0.13 38.75 83.76 183.86 34.75 83.76 181.52 9.50 83.76 144.44 15.38 99.47 98.27 72.67 61.29 513.33 72.52 61.29 514.42 49.01 61.29 503.98 71.92 72.64 496.11 80.74 27.42 76.98 27.42 75.83 27.42 79.14 28. 95.55 95.54 95.58 95.56 79.14 79.66 87.64 82.72 78.08 77.78 79.56 78.79 7.65 7.65 7.66 7.55 Cars EuroSAT UCF101 DOCTOR MSP 2.31 2.20 TrustVLM-D 2.08 TrustVLM*-D 1.76 AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 55.21 70.31 56.28 70.31 41.23 70.31 64.74 78.03 59.81 89.42 481.80 58.87 89.42 485.10 57.58 89.42 390.00 54.05 89.42 72.64 89.46 31.26 85.35 90.39 31.26 84.54 42.82 31.26 74.35 73.41 82.91 54. 27.50 96.75 18.84 26.25 96.75 18.73 26.25 96.75 18.63 21.13 97.12 22.18 88.73 88.97 92.14 87.91 89.37 89.48 89.57 87.92 72.61 72.31 90.09 75.64 94.88 95.21 95.52 95.65 Food SUN397 AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 58.63 70.69 40.90 57.31 70.69 40.70 44.49 70.69 TrustVLM-D 22.97 53.97 79.99 TrustVLM*-D 23.20 56.72 87.54 132.64 54.70 87.54 131.29 54.70 87.54 117.67 56.14 87.80 97.28 74.81 67.62 149.27 73.63 67.62 149.13 59.27 67.62 129.84 73.78 72.63 87.35 80.73 81.15 85.10 83.11 85.69 85.86 90.35 87.47 84.68 84.84 89.84 89. DOCTOR MSP Average Table 8: Misclassification detection performance on fine-grained classification datasets with SigLIP ViT-B/16. 12 79.28 79.25 82.13 77.68 72.72 72.58 65.09 78.19 ImageNet-Sketch ImageNet-A ImageNet-V2 AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 87.82 299.17 87.82 299.17 87.82 TrustVLM-D 278.77 87.98 TrustVLM*-D 266.12 47.62 124.64 47.62 124.12 47.62 123.39 51.82 122.70 83.03 83.21 83.76 83.05 45.00 43.06 42.05 43.33 91.53 91.90 92.07 91. 21.40 20.92 19.55 19.49 67.64 67.08 65.28 67.17 67.55 67.55 67.55 68.17 DOCTOR MSP ImageNet-R ImageNet AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 68.40 136.52 68.40 135.32 68.40 TrustVLM-D 133.55 69.71 TrustVLM*-D 132. 74.74 133.71 74.74 133.07 74.74 127.94 75.45 124.98 84.98 85.31 85.84 84.76 83.40 83.78 84.36 83.40 63.06 62.00 59.15 64.15 86.80 85.84 84.46 83.96 84.44 84.69 85.63 84. 62.35 61.24 59.61 64.17 67.59 66.03 63.73 67.89 64.29 64.29 64.29 65.14 DOCTOR MSP Average 83.85 83.99 94.74 98.10 Caltech Table 9: Misclassification detection performance on ImageNet and its variants with SigLIP ViT-B/16. Flower102 DTD Aircraft Pets DOCTOR MSP AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 62.33 83.65 154.49 59.83 83.65 154.02 58.00 83.65 TrustVLM-D 106.43 57.67 83.65 TrustVLM*-D 0.23 67.94 61.75 374.42 69.57 61.75 374.63 27.07 61.75 309.72 99.11 130.86 4.55 83.55 40.37 695.10 83.85 40.37 693.70 47.87 40.37 675.03 75.46 71.34 622.50 79.37 15.54 36.51 80.97 15.54 35.63 79.37 15.54 35.02 79.68 20.25 34.37 87.78 88.30 88.62 88.95 75.61 75.58 87.72 77. 74.86 75.15 79.38 75.35 Cars EuroSAT UCF101 DOCTOR MSP AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 72.38 58.82 30.50 68.98 58.82 29.55 49.71 58.82 TrustVLM-D 22.17 62.81 75.34 TrustVLM*-D 6. 69.54 85.88 212.51 66.38 85.88 209.07 36.49 85.88 206.48 40.91 96.43 205.62 74.73 55.74 618.09 72.99 55.74 633.47 70.75 55.74 514.75 70.92 56.08 101.58 83.87 23.70 169.84 87.27 23.70 166.92 54.70 23.70 144.99 74.00 82.43 73.84 84.01 84.77 89.67 85.69 80.23 80.93 81.70 81.41 70.73 67.67 87.02 70. 87.30 87.95 93.16 89.61 Food101 SUN397 AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 73.83 55.82 94.96 73.11 55.82 94.32 53.98 55.82 TrustVLM-D 76.19 59.97 73.13 TrustVLM*-D 70.64 65.77 73.95 209.19 64.86 73.95 206.31 58.46 73.95 178.75 60.27 75.23 112.43 78.84 58.81 259.56 76.41 58.81 259.76 57.38 58.81 226.95 73.47 71.45 135.81 77.49 78.16 84.08 81. 80.55 80.63 87.31 83.51 83.59 83.83 87.05 87.03 DOCTOR MSP Average Table 10: Misclassification detection performance on fine-grained classification datasets with CLIP ResNet-50. 66.71 66.68 72.99 67. 86.78 86.39 75.33 85.08 ImageNet-Sketch ImageNet-A ImageNet-V2 AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 56.36 672.36 56.36 673.06 56.36 TrustVLM-D 629.41 60.34 TrustVLM*-D 583.73 51.50 188.11 51.50 187.18 51.50 172.53 56.13 166.54 22.79 256.91 22.79 255.65 22.79 242.91 27.99 231.81 79.22 79.53 82.75 77. 68.02 66.21 56.44 71.47 84.28 84.51 87.58 83.35 75.41 74.83 62.07 75.42 DOCTOR MSP ImageNet-R ImageNet AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC AURC AUROC FPR95 ACC 44.36 446.07 44.36 444.12 44.36 TrustVLM-D 409.65 51.50 TrustVLM*-D 338. 58.18 353.56 58.18 352.46 58.18 327.64 65.00 297.68 32.99 204.36 32.99 202.30 32.99 183.69 48.05 168.30 78.27 78.62 85.09 71.57 79.44 79.96 84.24 77.91 75.75 74.68 60.89 77.44 77.58 77.86 82.53 75. 74.71 73.94 52.28 80.89 73.82 72.03 58.35 74.34 DOCTOR MSP Average Table 11: Misclassification detection performance on ImageNet and its variants with CLIP ResNet-50."
        },
        {
            "title": "References",
            "content": "[1] Barraco, M., Cornia, M., Cascianelli, S., Baraldi, L., Cucchiara, R.: The unreasonable effectiveness of clip features for image captioning: an experimental analysis. In: CVPR (2022) [2] Bossard, L., Guillaumin, M., Van Gool, L.: Food-101mining discriminative components with random forests. In: ECCV (2014) [3] Chan, R., Rottmann, M., Gottschalk, H.: Entropy maximization and meta classification for out-of-distribution detection in semantic segmentation. In: ICCV (2021) [4] Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297 (2020) 13 [5] Cheng, Z., Zhu, F., Zhang, X.Y., Liu, C.L.: Breaking the limits of reliable prediction via generated data. In: IJCV (2024) [6] Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., Vedaldi, A.: Describing textures in the wild. In: CVPR (2014) [7] Corbière, C., Thome, N., Bar-Hen, A., Cord, M., Pérez, P.: Addressing failure prediction by learning model confidence. In: NeurIPS (2019) [8] Davis, J., Goadrich, M.: The relationship between precision-recall and roc curves. In: ICML (2006) [9] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: large-scale hierarchical image database. In: CVPR (2009) [10] DeVries, T., Taylor, G.W.: Learning confidence for out-of-distribution detection in neural networks. arXiv preprint arXiv:1802.04865 (2018) [11] Dong, H., Chatzi, E., Fink, O.: Towards multimodal open-set domain generalization and adaptation through self-supervision. arXiv preprint arXiv:2407.01518 (2024) [12] Dong, H., Frusque, G., Zhao, Y., Chatzi, E., Fink, O.: NNG-Mix: Improving Semi-supervised Anomaly Detection with Pseudo-anomaly Generation. arXiv preprint arXiv:2311.11961 (2023) [13] Dong, H., Liu, M., Zhou, K., Chatzi, E., Kannala, J., Stachniss, C., Fink, O.: Advances in multimodal adaptation and generalization: From traditional approaches to foundation models. arXiv preprint arXiv:2501.18592 (2025) [14] Dong, H., Zhao, Y., Chatzi, E., Fink, O.: Multiood: Scaling out-of-distribution detection for multiple modalities. In: NeurIPS (2024) [15] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020) [16] Fei-Fei, L., Fergus, R., Perona, P.: Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In: CVPRW (2004) [17] Geifman, Y., El-Yaniv, R.: Selective classification for deep neural networks. In: NeurIPS (2017) [18] Granese, F., Romanelli, M., Gorla, D., Palamidessi, C., Piantanida, P.: Doctor: simple method for detecting misclassification errors. In: NeurIPS (2021) [19] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016) [20] Helber, P., Bischke, B., Dengel, A., Borth, D.: Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (2019) [21] Hendrycks, D., Basart, S., Mazeika, M., Zou, A., Kwon, J., Mostajabi, M., Steinhardt, J., Song, D.: Scaling out-of-distribution detection for real-world settings. ICML (2022) [22] Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al.: The many faces of robustness: critical analysis of out-ofdistribution generalization. In: ICCV (2021) [23] Hendrycks, D., Gimpel, K.: baseline for detecting misclassified and out-of-distribution examples in neural networks. In: ICLR (2017) [24] Hendrycks, D., Mazeika, M., Dietterich, T.: Deep anomaly detection with outlier exposure. In: ICLR (2019) [25] Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., Song, D.: Natural adversarial examples. In: CVPR (2021) [26] Jaeger, P.F., Lüth, C.T., Klein, L., Bungert, T.J.: call to reflect on evaluation practices for failure detection in image classification. arXiv preprint arXiv:2211.15259 (2022) [27] Jiang, H., Kim, B., Guan, M., Gupta, M.: To trust or not to trust classifier. In: NeurIPS (2018) [28] Jiang, X., Liu, F., Fang, Z., Chen, H., Liu, T., Zheng, F., Han, B.: Negative label guided ood detection with pretrained vision-language models. arXiv preprint arXiv:2403.20078 (2024) 14 [29] Karmanov, A., Guan, D., Lu, S., El Saddik, A., Xing, E.: Efficient test-time adaptation of vision-language models. In: CVPR (2024) [30] Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3d object representations for fine-grained categorization. In: ICCVW (2013) [31] Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny images (2009) [32] Li, S., Gong, H., Dong, H., Yang, T., Tu, Z., Zhao, Y.: Dpu: Dynamic prototype updating for multimodal out-of-distribution detection. arXiv preprint arXiv:2411.08227 (2024) [33] Li, Y., Su, Y., Goodge, A., Jia, K., Xu, X.: Efficient and context-aware label propagation for zero-/few-shot training-free adaptation of vision-language model. In: ICLR (2025) [34] Liang, V.W., Zhang, Y., Kwon, Y., Yeung, S., Zou, J.Y.: Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. In: NeurIPS (2022) [35] Liu, M., Dong, H., Kelly, J., Fink, O., Trapp, M.: Extremely simple multimodal outlier synthesis for out-of-distribution detection and segmentation. arXiv preprint arXiv:2505.16985 (2025) [36] Liu, W., Wang, X., Owens, J.D., Li, Y.: Energy-based out-of-distribution detection. In: NeurIPS (2020) [37] Ma, H., Zhao, H., Lin, Z., Kale, A., Wang, Z., Yu, T., Gu, J., Choudhary, S., Xie, X.: Eiclip: Entity-aware interventional contrastive learning for e-commerce cross-modal retrieval. In: CVPR (2022) [38] Maji, S., Rahtu, E., Kannala, J., Blaschko, M., Vedaldi, A.: Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151 (2013) [39] Ming, Y., Cai, Z., Gu, J., Sun, Y., Li, W., Li, Y.: Delving into out-of-distribution detection with vision-language representations. In: NeurIPS (2022) [40] Moon, J., Kim, J., Shin, Y., Hwang, S.: Confidence-aware learning for deep neural networks. In: ICML (2020) [41] Nejjar, I., Dong, H., Fink, O.: Recall and refine: simple but effective source-free open-set domain adaptation framework. arXiv preprint arXiv:2411.12558 (2024) [42] Nguyen, K.X., Li, T., Peng, X.: Interpretable failure detection with human-level concepts. In: AAAI (2025) [43] Nilsback, M.E., Zisserman, A.: Automated flower classification over large number of classes. In: ICVGIP (2008) [44] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023) [45] Parkhi, O.M., Vedaldi, A., Zisserman, A., Jawahar, C.: Cats and dogs. In: CVPR (2012) [46] Pratt, S., Covert, I., Liu, R., Farhadi, A.: What does platypus look like? generating customized prompts for zero-shot image classification. In: ICCV (2023) [47] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML (2021) [48] Recht, B., Roelofs, R., Schmidt, L., Shankar, V.: Do imagenet classifiers generalize to imagenet? In: ICML (2019) [49] Shu, M., Nie, W., Huang, D.A., Yu, Z., Goldstein, T., Anandkumar, A., Xiao, C.: Test-time prompt tuning for zero-shot generalization in vision-language models. In: NeurIPS (2022) [50] Soomro, K., Zamir, A.R., Shah, M.: Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402 (2012) [51] Sun, H., Cao, Y., Fink, O.: Cut: controllable, universal, and training-free visual anomaly generation framework. arXiv preprint arXiv:2406.01078 (2024) [52] Wang, H., Ge, S., Lipton, Z., Xing, E.P.: Learning robust global representations by penalizing local predictive power. In: NeurIPS (2019) [53] Wang, H., Li, Y., Yao, H., Li, X.: Clipn for zero-shot ood detection: Teaching clip to say no. In: ICCV (2023) 15 [54] Wei, H., Xie, R., Cheng, H., Feng, L., An, B., Li, Y.: Mitigating neural network overconfidence with logit normalization. In: ICML (2022) [55] Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: Sun database: Large-scale scene recognition from abbey to zoo. In: CVPR (2010) [56] Zhai, X., Mustafa, B., Kolesnikov, A., Beyer, L.: Sigmoid loss for language image pre-training. In: ICCV (2023) [57] Zhang, R., Zhang, W., Fang, R., Gao, P., Li, K., Dai, J., Qiao, Y., Li, H.: Tip-adapter: Trainingfree adaption of clip for few-shot classification. In: ECCV (2022) [58] Zhang, Y., Zhu, W., Tang, H., Ma, Z., Zhou, K., Zhang, L.: Dual memory networks: versatile adaptation approach for vision-language models. In: CVPR (2024) [59] Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language models. In: IJCV (2022) [60] Zhu, F., Cheng, Z., Zhang, X.Y., Liu, C.L.: Openmix: Exploring outlier samples for misclassification detection. In: CVPR (2023)"
        }
    ],
    "affiliations": [
        "EPFL",
        "ETH Zürich",
        "NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences",
        "Technical University of Munich",
        "University of Chinese Academy of Sciences"
    ]
}