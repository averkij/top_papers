{
    "paper_title": "Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text",
    "authors": [
        "Zhihao Xu",
        "Rumei Li",
        "Jiahuan Li",
        "Rongxiang Weng",
        "Jingang Wang",
        "Xunliang Cai",
        "Xiting Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 1 ] . [ 1 5 5 3 0 1 . 1 0 6 2 : r Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text Zhihao Xu12, Rumei Li2, Jiahuan Li2, Rongxiang Weng2, Jingang Wang2, Xunliang Cai2, Xiting Wang1 1Renmin University of China, 2Meituan, China zhihaoxu@ruc.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains significant challenge. In this work, we propose novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs."
        },
        {
            "title": "1 Introduction",
            "content": "The pursuit of Artificial General Intelligence (AGI) relies on the development of autonomous agents capable of perceiving, reasoning, and acting in complex real-world environments [Team et al., 2025a, Liu et al., 2025, Team et al., 2025b]. Such agents are required to dynamically utilize diverse tools to extend their capabilities and accomplish multi-step tasks. While large language models (LLMs) have demonstrated impressive tool-use capabilities, they still struggle in realistic multi-turn interactions, particularly when faced with ambiguous instructions, long-context dependencies, and unexpected errors [Patil et al.]. These limitations constrain their practical application in agent-based systems [Yao et al., 2024, Barres et al., 2025]. The primary bottleneck in training autonomous agents lies in the scarcity of high-quality, multi-turn tool-use trajectories, which are rarely found in real-world scenarios. To overcome this data scarcity, current research favors tool-centered simulation paradigm  (Fig. 1)  . These methods typically rely on predefined API sets to synthesize user tasks and simulate interactions [Qin et al., 2023, Liu et al., 2024a, Guo et al., 2024, Zeng et al., 2025, Prabhakar et al., 2025, Liu et al., 2024b, Yin et al., 2025]. However, gathering sufficiently diverse and comprehensive tool set is inherently expensive and difficult. The resulting tool-use training data is often limited by the scope of the predefined APIs, while the ultimate goal of agentic training is the exposure to sufficiently broad range of scenarios during training to enable agents to generalize effectively to unseen environments and scenarios [Fang et al., 2025]. This raises critical question: Can we bypass the dependency on predefined tools and synthesize more diverse, high-quality trajectories directly from real-world? In this paper, we answer this question by proposing novel text-based extraction paradigm. We observe that the raw text corpora used for pre-training large language models inherently contain multi-turn tool-use patterns. Although such texts do not contain explicit agentic trajectories, they often document rich, real-world mutli-step problem-solving experiences, which can be extracted and transformed into multi-turn tool-use data Equal Contributions. Corresponding authors. Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text Figure 1 We propose the \"Text to Trajectory\" paradigm, instead of relying on predefined tools to generate tasks and trajectories. (e.g., \"the procedure for hospital reimbursement claims\"). Our preliminary analysis of Ultra-fineweb [Wang et al., 2025] confirms that such texts contain actionable logical sequences spanning diverse domains, indicating that unstructured text serves as an untapped, scalable, and authentic source for synthesizing agentic training data. To operationalize this paradigm, we introduce GEM, data synthesis pipeline that enables generation and extraction of multi-turn tool-use trajectory from text corpora. The GEM pipeline proceeds in four stages: (1) Selection, identifying text segments rich in multi-step workflows; (2) Extraction, deriving structured workflows and tool definitions to map human-described procedures to agent actions; (3) Generation, employing powerful model to convert text and abstract workflows into concrete user-agent interactions; and (4) Refinement, enhancing the complexity and diversity of the trajectories followed by rigorous verification. Extensive experiments validate the effectiveness of our approach. When fine-tuned on data synthesized by our pipeline, the GEM-32B model achieves 14.9% improvement on BFCL V3 Multi-Turn benchmark. Notably, We find that out-of-domain training data generated through our paradigm achieve performance on τ 2-bench that is comparable to models trained on τ -bench in-domain data, thereby demonstrating the strong generalization capability of our approach. Building upon this high-quality data, we further develop specialized Trajectory Synthesizer to internalize the \"text-to-trajectory\" mapping. This synthesizer provides cost-effective, end-to-end solution for large-scale data generation, matching the quality of the multi-stage synthesis pipeline while significantly reducing costs. In summary, our contributions are threefold: We propose new paradigm for agent trajectory synthesis that directly extracts multi-turn tool-use trajectories from text corpora. This paradigm unlocks an untapped, scalable source of authentic human problem-solving behaviors for training autonomous agents. We develop GEM, data synthesis pipeline that transforms raw text into multi-turn tool-use trajectories to prove the effectiveness of our proposed paradigm. We use GEM to synthesize high-quality dataset that leads to significant performance gains: 13.8% on τ 2-bench and 16.5% on BFCL V3 Mutli-Turn. To enable cost-effective and scalable data generation, we develop specialized Trajectory Synthesizer via supervised fine-tuning. We demonstrate that this model successfully distills the full GEM pipeline into an end-to-end generator, matching the original synthesis quality while significantly reducing costs."
        },
        {
            "title": "2 Problem Formulation",
            "content": "In this paper, we address the challenge of acquiring general multi-turn tool-use agent trajectories. We propose novel paradigm that synthesizes such trajectories by extracting tool-use workflows directly from large-scale textual corpora. This approach naturally enjoys these advantages: the scale of real-world text ensures scalability, its inherent diversity covers multiple domains, and its foundation in grounded human problem-solving experiences yields high-quality, realistic agent data. Formally, we define the paradigm as: Input: Let denote large-scale text corpus, where each element ci is raw, unstructured text segment (e.g., document or narrative) that contains multi-step workflows. Output: complete list of tools = {p1, . . . , pm}, where denotes the number of available tools and each pi represents tool defined in the standard OpenAI format. Additionally, structured multi-turn tool-use 2 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text trajectory = {s, (u1, a1, o1), . . . , (un, an, on)} is produced, where denotes the system prompt and at each turn t, ut corresponds to users query or request, at denotes the assistants natural language response or tool call, and ot is the resulting observation from the tools execution."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we introduce GEM, an agentic synthesis pipeline designed to automatically extract and generate multi-turn tool-use trajectories directly from large-scale text corpora. We first conduct preliminary analysis of text corpora in Section 3.1. Subsequently, we detail our trajectory collection pipeline in Section 3.2."
        },
        {
            "title": "3.1 Preliminary Analysis",
            "content": "Figure 2 Distribution of the extracted task category from raw documents, showing the diversity of tasks covered in the text corpora. Figure 3 Preliminary case study. Setup To assess the feasibility of our proposed paradigm, we first perform preliminary analysis on the characteristics of large-scale unstructured data, using the Ultra-fineweb corpus [Wang et al., 2025] as our main data source. We randomly sample approximately 250,000 raw text segments. Each segment is then processed through sequential labeling pipeline: we first use classifier to determine whether the text contains multi-step operational procedures, which helps us identify segments suitable for conversion into tool-use trajectories. Only those segments identified as containing procedural content are subsequently annotated with rich metadata, including platform, domain, and task category, via Qwen3-8B to evaluate the diversity of the corpus. We also conduct preliminary case study. The prompt templates used for metadata annotation are provided in Appendix A.1. Results First, by observing large number of text cases, we find that unstructured textual data inherently contain the three core components essential for constructing agentic trajectories as shown in Figure 3: (1) User Queries, which emerge naturally as goals or problems stated in the text; (2) Environmental Tools, whose descriptions, APIs, or functionalities are often embedded within explanatory or instructional contexts; and (3) Multi-step Workflows, manifesting as step-by-step procedures or operational narratives. We find that around 14% of the sampled segments contain explicit multi-step workflows, indicating substantial reservoir of procedural knowledge. Given the massive scale of available text corpora, this represents significant potential source for generating diverse agent trajectories. Furthermore, the identified procedural segments cover wide spectrum of task categories and application scenariosincluding education & E-learning, data analysis as illustrated in Figure 2. This diversity confirms that textual sources can provide the necessary variety in tasks, tools, and environments required for robust agent training. The detailed domain distribution is further reported in Appendix E. These findings collectively demonstrate that generating multi-turn tool-use trajectories directly from unstructured text is not only feasible but also taps into vast and largely underexplored repository of agentic trajectory. 3 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text Figure 4 Overall Pipeline of GEM. The first phase extracts multi-step workflows from filtered text corpora and defines corresponding tools. The second phase generates and refines trajectories, incorporating diverse behavioral patterns and rigorous validation to produce high-quality training data."
        },
        {
            "title": "3.2 GEM Synthesis Pipeline",
            "content": "We now detail the data synthesis pipeline of GEM to generate and extract multi-turn tool-use data. The overall pipeline is illustrated in Figure 4. All prompts used in this section are in Appendix A. Stage 1: Text Filtering To ensure the quality and realism of generated trajectories, the initial stage filters out raw text segments that do not describe multi-step operations. The filtering procedure uses the same annotation prompt and model as described in Section 3.1. Stage 2: Workflow & Tool Extraction For each retained text segment containing multi-step operations, we proceed to extract structured abstract workflows and synthesize corresponding functional tools directly based on the text segment description. First, the model is instructed to identify all workflows and enumerate the individual steps within each (e.g., search_items and then edit_item). Importantly, the model is encouraged to recognize workflow complexity, including sequential dependencies, conditional logic, and uniqueness constraints, thereby enhancing the richness and practical relevance of the output. Concurrently, the model designs set of API tools in accordance with OpenAI schema standards to support the subsequent trajectory generation. Each tool is designed to perform single, coherent function, with self-explanatory parameter names and well-specified data types. The final structured output comprises the abstract workflow descriptions alongside the complete tool definitions. Stage 3: Trajectory Generation Based on the text, abstract workflows and tools synthesized in the previous stage, we now proceed to generate concrete multi-turn tool-use trajectories. For each workflow and its corresponding set of tools, we employ strong teacher model (GLM-4.6 in this work) to generate preliminary complete trajectory . Our approach synthesizes the full trajectory directly in single pass instead of simulating turn-by-turn conversations via multi-agent system to ensure efficiency. Each generated trajectory include the following core components: System Prompt s: Clear domain-specific rules extracted from the source text, establishing guidelines the assistant must follow throughout the conversation. User Task (u1, ...un): series of natural user requests that may be ambiguous or complex, reflecting real-world scenarios. Assistant Responses (a1, ..., an): Demonstrations of intelligent problem-solving, strict adherence to domain rules, and correct tool invocation with appropriate parameters. Tool Response (o1, ...on): Simulated tool outputs that are both complete and realistic, providing authentic feedback. Here, denotes the number of conversational turns for each role. To capture the diverse practical challenges inherent in real-world multi-turn interactions, we encourage the inclusion of various interaction patterns during 4 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text"
        },
        {
            "title": "Model",
            "content": "GPT-4.1 DeepSeek-V3.2-Exp Gemini-2.5-Flash Qwen3-8B APIGEN-MT TOUCAN MUA Qwen3-8B-GEM Qwen3-32B APIGEN-MT TOUCAN MUA Qwen3-32B-GEM Multi-Turn Multi-Turn Multi-Turn Multi-Turn Overall Acc"
        },
        {
            "title": "Miss Func Miss Param Long Context",
            "content": "Multi-Turn"
        },
        {
            "title": "Base",
            "content": "Proprietary & Large Scale Models 38.88 37.38 36.25 18.00 21.00 21.88 21.13 30.25 28.35 29.50 35.00 26.25 44.88 47.50 41.50 41.50 8B Models 24.00 25.50 27.00 29.50 40.00 32B Models 34.00 36.00 41.00 30.00 52.00 32.50 39.50 36.00 17.00 19.00 21.50 18.00 30.00 24.50 27.00 37.50 25.00 40. 32.50 33.50 32.00 13.50 25.00 21.00 20.00 28.00 25.50 28.50 26.00 23.00 38.50 43.00 35.00 35.50 17.50 14.50 18.00 17.00 23.00 29.50 26.50 35.50 27.00 49. Table 1 Performance comparison on the BFCL V3 benchmark. The results are categorized by model scale. The best performance in each category is highlighted in bold. All metrics are reported as accuracy scores. The GEM-based models are highlighted in blue . generation. These include but are not limited to: refusing requests that exceed the assistants capabilities, clarifying ambiguous user queries, and actively recovering from errors. Stage 4: Refinement We observe that although the initial multi-turn dialogue trajectories are complete, they often lack sufficient complexity and tend to be relatively straightforward. To fully utilize and enhance the quality of these trajectories, we adopt refinement strategy to further process them. Specifically, given , we generate refined trajectory by expanding the variety of tools used, improving the realism of environmental responses, increasing the ambiguity and complexity of user requests, and ensuring the inclusion of non-trivial tool-call chains. We find this refinement essential for obtaining higher-quality agentic data, as demonstrated in Section 4.4. Validation The trajectory filtering process integrates rule-based verification with LLM-based assessments to ensure high-quality outputs. Given refined trajectory , we first apply rule-based check to guarantee structural correctness. This involves verifying that all tools are correctly defined according to the OpenAI format and that each tool call corresponds to valid function within the designated toolset, with argument names and types matching their definitions. We further validate the conversation format to confirm that tool responses meet all requirements and that all role tags are properly closed. Beyond structural correctness, we employ an LLM-based judge (Qwen3-32B in this paper) to detect and eliminate hallucinations. This step specifically examines all tool calls to ensure that every generated parameter value is explicitly grounded in the dialogue context rather than being fabricated. Only those trajectories that successfully pass both validation stages are retained as the final set Tfinal for use in supervised fine-tuning (SFT). Data Synthesizer Generating such trajectories is costly and time-consuming. To address this, we propose training data synthesizer via supervised fine-tuning (SFT), which learns end-to-end mapping from text to multi-turn tool-use trajectories. This approach allows for the cost-effective synthetic training data. For each data instance, the synthesizer takes as input an instruction (\"Turn the following text into multi-turn tool-use trajectories\") paired with text segment. It then generates the corresponding output y, which includes both the necessary tool definitions and the resulting multi-turn tool-use trajectory. Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text"
        },
        {
            "title": "4.1 Experimental Setups",
            "content": "Benchmark. We evaluate our approach on two challenging benchmarks designed to assess multi-turn tool-use capabilities: (1) BFCL V3 [Patil et al.]: This benchmark involves agents interacting with Python-based API environment. We focus specifically on the multi-turn scenarios, which are divided into four categories: Multi Turn Base, Miss Param, Miss Func, and Long Context with 200 tasks per category. (2) τ 2-bench [Barres et al., 2025] (Airline and Retail): This benchmark evaluates user-agent interactions comprehensively within specialized real-world domains. We employ GPT-4.1 as the user simulator Following the original experimental setting and report performance using the Avg@4 and Pass@4 metrics. LLMs and Datasets. We source the training data from Ultra-FineWeb [Wang et al., 2025] and employ GLM-4.6 to generate 10K synthetic trajectories. These trajectories are then used to fine-tune both Qwen3-8B and Qwen3-32B. Additionally, we leverage the same 10K samples to train data synthesizer model based on Qwen3-8B. Baselines. We compare our synthetic data against the following open-source datasets: (1) APIGEN-MT [Prabhakar et al., 2025], (2) Simia-Tau [Li et al., 2025]: We sample 50K multi-turn data from this dataset, (3) MUA [Zhao et al., 2025], and (4) TOUCAN [Xu et al., 2025]: We sample 50K multi-turn data from this dataset. Notably, APIGEN-MT and Simia are in-domain training data generated in the τ -bench environment (Airline and Retail). Training Details. For supervised fine-tuning, we set the learning rate to 5 106 and train for two epochs. Fine-tuning is performed using LLaMA-Factory [Zheng et al., 2024] under full-parameter setting. Further details are provided in Appendix B."
        },
        {
            "title": "4.2 Performance of GEM Synthesis Pipeline",
            "content": "BFCL V3 Results Table 1 presents performance comparisons on the BFCL V3 benchmark. The results of Simia are not included, as we observed its scores to be generally low, likely due to its reliance solely on in-domain τ -bench data. Our proposed GEM method demonstrates clear improvements over baseline models at both the 8B and 32B scales. In the 8B category, Qwen3-8B-GEM achieves an overall accuracy of 30.25%, significantly surpassing the base Qwen3-8B model and outperforming other open-source baselines such as APIGEN-MT and TOUCAN. The performance gain is even more pronounced in the 32B category, where Qwen3-32B-GEM attains an accuracy of 44.88%. This result not only exceeds open-source synthesized datasets like APIGEN-MT and MUA by wide margin but also outperforms proprietary large-scale models, including GPT-4.1 (38.88%) and DeepSeek-V3.2-Exp (37.38%). These findings confirm that our data synthesis strategy effectively enhances function-calling capabilities across various multi-turn interaction categories. τ 2-Bench Results Table 5 presents the performance on the τ 2bench benchmark, covering the Airline and Retail domains. We would like to emphasize that our models are trained on synthetically generated data that is strictly out-of-domain with respect to the τ 2-bench test sets, while APIGEN-MT and SIMIA are synthesized datasets within the τ environment (Retail and Airline), which can be regarded as in-domain training data. Despite this, our approach still demonstrates comparable performance. At the 8B scale, Qwen3-8B-GEM remains highly competitive, achieving results comparable to models like SIMIA that are fine-tuned on in-domain synthetic data, and surpassing APIGEN-MT in the Retail domain with Pass@4 score of 75.44% versus 69.30%. At the 32B scale, our model exhibits strong generalization, outperforming both SIMIA and MUA in the Retail domain with Pass@4 of 86.84% and delivering competitive results in the Airline domain. This indicates that our text-based synthesis pipeline instills fundamental understanding of tool-use reasoning that transfers effectively to unseen, real-world domains, matching or even exceeding the performance of models trained Model Airline Retail Avg@4 Pass@4 Avg@4 Pass@ 8B Models 13.00 Qwen3-8B 23.50 APIGEN-MT 35.50 Simia 20.50 TOUCAN MUA 20.50 Qwen3-8B-GEM 22.00 18.00 42.00 52.00 42.00 40.00 40.00 32B Models 21.00 Qwen3-32B 36.00 APIGEN-MT 38.00 Simia 30.00 TOUCAN MUA 33.00 Qwen3-32B-GEM 35.50 40.00 52.00 62.00 52.00 54.00 56. 38.16 42.54 43.20 26.32 30.26 44.52 43.20 44.52 48.03 43.86 49.56 55.48 66.67 69.30 70.18 52.51 61.40 75.44 70.18 74.56 73.68 72.81 80.70 86.84 Figure 5 Results on τ 2-bench. We report avg@4 and pass@4 metrics for Airline and Retail domains. Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text Model BFCL V3 Multi-Turn Tau2 Overall Base Miss Miss Long Airline Retail Acc Acc Func Param Context Avg@4 Pass@4 Avg@4 Pass@4 Qwen3-8B (Base) 18. 24.00 17.00 13.50 + GEM-GLM (Ultrafineweb) + GEM-Synthesizer (Ultrafineweb) + GEM-Synthesizer (Wikihow) 30.25 28.38 28.50 40.00 41.50 43. 30.00 23.50 22.00 28.00 27.50 27.00 17.50 23.00 21.00 21.50 13.00 22.00 26.00 25. 18.00 40.00 40.00 42.00 38.16 44.52 42.11 39.67 66.67 75.44 73.68 68. Table 2 Effects of data synthesizer and text sources. We compare the impact of different trajectory generation models (GLM vs. Trained Synthesizer) and data sources (Ultrafineweb and Wikihow) on the Qwen3-8B base model. The best performance is highlighted in bold. on more domain-aligned data distributions. Overall, the results underscore the effectiveness of our proposed paradigm."
        },
        {
            "title": "4.3 Performance of GEM Synthsizer",
            "content": "We use 10K trajectories generated by GEM synthesizer to fine-tune Qwen3-8B, as shown in Table 2. When using the same data source (Ultrafineweb), our GEM-Synthesizer achieves an overall accuracy of 28.38% on the BFCL dataset, which is close to the performance obtained by directly using synthesized data generated by GLM-4.6, and also yields the Pass@4 score (73.68%) in the τ 2 Retail domain. We also use another textual source, Wikihow [Koupaee and Wang, 2018], to show the generalization of our synthesizer. The GEM-Synthesizer also achieves high overall performance on both BFCL (28.50%) and on the τ 2-bench Airline domain. These findings indicate that our synthesizer can generate high-quality tool-use trajectories in low-cost, end-to-end way, thereby enabling stronger generalization in complex tool-use scenarios."
        },
        {
            "title": "Model",
            "content": "For example,"
        },
        {
            "title": "Overall Multi Miss Miss",
            "content": "Long Base Func Param Cxt. To validate the effectiveness of our synthetic data pipeline, we conduct an ablation study with BFCL V3 benchmark focusing on two key components: the refinement stage and the LLM-based check as shown in Table 6. We report the ablation results on τ 2-bench in Appendix C. Effect of Refinement. The refinement stage leads to substantial performance improvement. it raises the overall accuracy of Qwen3-32B from 32.50% to 44.88%, gain of over 12 percentage points. This process increases the complexity and quality of the synthetic trajectories, which in turn enables more effective learning of multi-turn tool use. We report more details in Appendix to illustrate out refinement strategy significantly enhances the overall complexity of the synthesized data. Notably, even the original trajectories extracted directly from the original text (though relatively simpler) still provide valuable training signals and contribute to improved tool-calling capability. This indicates that more effectively leveraging information from the original text to synthesize high-quality tool-calling trajectories is promising research direction. 18.00 24.00 17.00 13.50 17.50 Qwen3-8B (Base) 30.25 40.00 30.00 28.00 23.00 Qwen3-8B-GEM 26.00 33.50 23.50 27.00 20.00 (w/o Refine) (w/o LLM-Based Check) 27.38 37.50 25.00 24.00 23.00 28.35 44.00 24.50 25.50 29.50 Qwen3-32B (Base) 44.88 52.00 40.00 38.50 49.00 Qwen3-32B-GEM 32.50 40.00 30.00 27.00 33.00 (w/o Refine) (w/o LLM-Based Check) 44.25 52.00 40.50 40.00 44.50 Figure 6 Ablation study. We conduct ablation study with BFCL V3 benchmark. Effect of LLM-Based Check. This stage consistently improves results by filtering out samples with hallucinations or inconsistencies. For the 8B model, it raises overall accuracy from 27.38% to 30.25%. This mechanism also contributes to stronger performance across both model scales. Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text Figure 7 Data analysis. We find the synthesized trajectories are overall complex. Left: distribution of number of tools. Mid: distribution of number of messages. Right: distribution of number of tool calls in each trajectory. Figure 8 Case study of our generated multi-turn tool-use trajecory."
        },
        {
            "title": "4.5 Data Analysis",
            "content": "Figure 7 presents statistical overview of our synthesized multi-turn tool-use trajectories, highlighting three key dimensions: the number of distinct tools employed, the number of messages per dialogue, and the total number of tool calls. On average, each trajectory involves 8.6 distinct tools, indicating that the synthesis process requires models to meaningfully select and combine multiple tools within single task. Moreover, the trajectory contains an average of 46 turns. The considerable length of these dialogues ensures that tasks cannot be resolved simply, helping models learn to maintain context, track task state, and engage in multi-step planning over prolonged dialogues. In comparison, existing open-source datasets such as APIGEN-MT [Prabhakar et al., 2025] average around 18.5 turns, while TOUCAN [Xu et al., 2025] contains only about 6.24 turns. Finally, each trajectory averages 16.3 tool calls. while APIGEN-MT contains only an average of 4.3 tool calls. This high frequency underscores the multi-step, tool-driven nature of the synthesized tasks. Taken together, these statistics demonstrate that our synthesized dataset exhibits substantial diversity and interaction depth, making it well-suited for training models in multi-turn tool-use scenarios."
        },
        {
            "title": "4.6 Case Study",
            "content": "For the case study illustrated in Figure 8, this synthesized trajectory is based on real-world photo-editing scenario. The synthesis process first extract diverse tools from the text description, and identifies key constraints from the users actions, such as the restriction that font size must be between 8 and 96 points. Building on these tools and rules, the dialogue encompasses variety of realistic interaction patterns: proactively clarifying missing parameters (i.e., image path and text placement), invoking tools in the correct sequence, rejecting requests that violate constraints (i.e., exceeding the font size limit) while offering compliant alternatives, and recovering from errors by retrying with another available printer. It guides the model to learn how to validate inputs, adhere to system constraints, execute tasks step by step, and respond flexibly to errors within multi-turn interactions, thereby achieving reliable tool-use capabilities. We also show complete synthesized trajectory in Appendeix to help readers better understand our methodology. 8 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text"
        },
        {
            "title": "5 Related Work",
            "content": "Tool-Use Data Synthesis To equip LLM-based agents with tool-calling capabilities, prior works focus on synthesizing tool-use training data. ToolBench [Qin et al., 2023] builds large-scale function-calling dataset over thousands of APIs. ToolACE [Liu et al., 2024b] designs an automatic agentic pipeline that iteratively expands an API pool and synthesizes verified function-calling traces with high complexity and diversity. For multi-turn interaction, APIGen-MT [Prabhakar et al., 2025] generates structured task blueprints with ground-truth action sequences and then simulates realistic humanagent dialogues grounded in executable APIs. MagNet [Yin et al., 2025] represents multi-tool workflows as graph-structured function paths and converts them into multi-turn conversations with executable calls. ToolACE-MT [Zeng et al., 2025] adopts non-autoregressive framework that drafts entire dialogues in one shot and refines them via iterative editing and verification. TOUCAN [Xu et al., 2025] crawls MCP servers and synthesizing 1.5M of tool-use data. Unlike prior works that rely on pre-defined tools, our work introduces novel paradigm that directly extracts multi-turn trajectories from text, thereby unlocking authentic and scalable source of tool-use agentic data. Tool-Use Capability Evaluation To systematically assess tool-use capabilities of LLM agents, series of benchmarks have been proposed. ToolBench [Qin et al., 2023] focuses on evaluating whether models can translate natural-language instructions into correct API calls across thousands of real-world tools. The Berkeley Function Calling Leaderboard (BFCL) [Patil et al.] provides large-scale, syntaxand semantics-aware evaluation of function calling across diverse domains and programming languages, using AST-based checks to measure the accuracy and compositionality (e.g., parallel calls) of model-generated function invocations. ACEBench [Chen et al., 2025] evaluates multi-turn tool-use capability from three perspectives: normal, special, and agent. τ -bench [Yao et al., 2024] emulates dynamic conversations between simulated user and toolaugmented agent in domain-specific scenarios, jointly evaluating task success, tool selection, and adherence to domain policies. τ 2-bench [Barres et al., 2025] further extends this line by introducing dual-control environments where both the agent and the user can invoke tools, highlighting the challenges of coordinating tool use and guiding user actions in more realistic customer-service-style tasks. VitaBench [He et al., 2025] focuses on evaluating agents performance on life-serving simulation environment."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper presents novel paradigm and the agentic data synthesis pipeline, which directly synthesize multi-turn tool-use trajectories from text corpora, effectively bypassing the dependency on predefined tools. Models trained on our data achieve significant performance gains on benchmarks, demonstrating the potential of leveraging open-world textual knowledge as scalable source for advancing autonomous agents."
        },
        {
            "title": "References",
            "content": "Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025a. Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, et al. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025. Meituan LongCat Team, Bei Li, Bingye Lei, Bo Wang, Bolin Rong, Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, et al. Longcat-flash technical report. arXiv preprint arXiv:2509.01322, 2025b. Shishir Patil, Huanzhi Mao, Fanjia Yan, Charlie Cheng-Jie Ji, Vishnu Suresh, Ion Stoica, and Joseph Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ -bench: benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. τ 2-bench: Evaluating conversational agents in dual-control environment. arXiv preprint arXiv:2506.07982, 2025. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. 9 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh RN, et al. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. Advances in Neural Information Processing Systems, 37:5446354482, 2024a. Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. arXiv preprint arXiv:2403.07714, 2024. Xingshan Zeng, Weiwen Liu, Lingzhi Wang, Liangyou Li, Fei Mi, Yasheng Wang, Lifeng Shang, Xin Jiang, and Qun Liu. Toolace-mt: Non-autoregressive generation for agentic multi-turn interaction. arXiv preprint arXiv:2508.12685, 2025. Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, et al. Apigen-mt: Agentic pipeline for multi-turn data generation via simulated agent-human interplay. arXiv preprint arXiv:2504.03601, 2025. Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, et al. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920, 2024b. Fan Yin, Zifeng Wang, I-Hung Hsu, Jun Yan, Ke Jiang, Yanfei Chen, Jindong Gu, Long Le, Kai-Wei Chang, Chen-Yu Lee, et al. Magnet: Multi-turn tool-use data synthesis and distillation via graph translation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3260032616, 2025. Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, et al. Towards general agentic intelligence via environment scaling. arXiv preprint arXiv:2509.13311, 2025. Yudong Wang, Zixuan Fu, Jie Cai, Peijun Tang, Hongya Lyu, Yewei Fang, Zhi Zheng, Jie Zhou, Guoyang Zeng, Chaojun Xiao, et al. Ultra-fineweb: Efficient data filtering and verification for high-quality llm training data. arXiv preprint arXiv:2505.05427, 2025. Yuetai Li, Huseyin Inan, Xiang Yue, Wei-Ning Chen, Lukas Wutschitz, Janardhan Kulkarni, Radha Poovendran, Robert Sim, and Saravan Rajmohan. Simulating environments with reasoning models for agent training. arXiv preprint arXiv:2511.01824, 2025. Weikang Zhao, Xili Wang, Chengdi Ma, Lingbin Kong, Zhaohua Yang, Mingxiang Tuo, Xiaowei Shi, Yitao Zhai, and Xunliang Cai. Mua-rl: Multi-turn user-interacting agent reinforcement learning for agentic tool use. arXiv preprint arXiv:2508.18669, 2025. Zhangchen Xu, Adriana Meza Soria, Shawn Tan, Anurag Roy, Ashish Sunil Agrawal, Radha Poovendran, and Rameswar Panda. Toucan: Synthesizing 1.5 tool-agentic data from real-world mcp environments. arXiv preprint arXiv:2510.01179, 2025. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. Mahnaz Koupaee and William Yang Wang. Wikihow: large scale text summarization dataset. arXiv preprint arXiv:1810.09305, 2018. Chen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Yuefeng Huang, et al. Acebench: Who wins the match point in tool usage? arXiv preprint arXiv:2501.12851, 2025. Wei He, Yueqing Sun, Hongyan Hao, Xueyuan Hao, Zhikang Xia, Qi Gu, Chengcheng Han, Dengchang Zhao, Hui Su, Kefeng Zhang, et al. Vitabench: Benchmarking llm agents with versatile interactive tasks in real-world applications. arXiv preprint arXiv:2509.26490, 2025. 10 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text"
        },
        {
            "title": "A Prompts",
            "content": "A.1 Tag Annotation Domain & Category Annotation Determine whether the following text contains multi-step operations involving the use of an APP, website, computer, or other machine (such as robot, elevator, etc), if contains, generate one sentence summary of the task and identify the platform, domain and task category of the multi-step task. # Detaild Instruction 1. Platform category: operator, computer, phone, machine, other 2. Domain category: adult, arts_and_entertainment, autos_and_vehicles, beauty_and_fitness, books_and_literature, business_and_industrial, computers_and_electronics, finance, food_and_drink, games, health, hobbies_and_leisure, home_and_garden, internet_and_telecom, jobs_and_education, law_and_government, news, online_communities, people_and_society, pets_and_animals, real_estate, science, sensitive_subjects, shopping, sports, travel_and_transportation 3. Task category: databases, multimedia_processing, cloud_platforms, calendar_management, cryptocurrency, location_services, communication, search, file_systems, web_scraping, ecommerce_and_retail, customer_data_platforms, developer_tools, virtualization, version_control, research_and_data, aigc, travel_and_transportation, note_taking, language_translation, rag_systems, security_and_iam, social_media, monitoring, weather_services, customer_support, blockchain, knowledge_and_memory, financial_trading, marketing, enterprise_business_intelligence, transportation_logistics, iphone_android, smart_home, education_elearning, robot_control, website_control, gaming_entertainment # Input [text] # Output Format <multi_step>False</multi_step> Or: <multi_step>True</multi_step> <summary>xxx</summary> <domain>Shopping, Sports</domain> <platform>Operator</platform> <task>customer_support</task> A.2 Workflow & Tool Discovery Workflow & Tool Discovery You are an program design expert. Given workflow description in scenario, your task is to design multiple functions to translate the execution process of this workflow into program. # Instruction 1. Extract all intermediate steps in the workflow, if the text contains multiple workflows, output them in list. 2. Convert **every** step to function and represent them as an execution graph (i.e. ( login)->(search_query)->..) 3. Based on the execution graph, generate real API calls that populate the tools with reasonable parameters, simulating use case of actual tool invocation steps. 4. Provide detailed API definitions used in the above process. 11 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text 5. Follow the following steps to generate more complex workflows and tools: - Workflow Exploration: You need to explore multiple workflows or complex constraints that may exist in the document - These workflows represent the possible interaction patterns of real user-agent. - Dependencies: \"X must happen before Y\". - Uniqueness/Limits: \"Only one Admin allowed\", \"Name must be unique\". - Conditionals: \"If user is X, they cannot do Y\". - Tool Design (Functional API Level) Design set of JSON-schema tools based on the text. - The required parameters of tool need to be carefully considered and designed, mirroring the logic of the real world. For example, viewing system data typically requires authorization authentication, and providing user ID, product ID, etc. - It mimics database structure and provides read and write tools. For example, it provides tools for querying user information, along with corresponding tools for modifying user information. - Each tools name should be short and readable, semantically clear and general, reusable (e.g., \"flight_search\" rather than \"flight_detailed_search_for_tom_2025\") - Each tool should implement single, coherent capability. It should not bundle multiple unrelated or multi-stage workflows into one tool. (e.g., create two tools \" plan_trip\" + \"book_trip\" rather than only one tool named \"plan_and_book_trip\") - Each tools parameters should be explicitly defined in the schema with clear types and meanings. Parameter names should be self-explanatory rather than cryptic (e.g., use \" check_in_date\" with type \"string\" and short description, rather than vague parameter named \"d1\"). - The majority of tools describe functional data operations that either retrieve information from or modify the state of the environment (e.g., get_status, update_permissions) # Workflow Description [text] # Output Format <workflow> <description>short task description</description> <steps>Step1: ...nStep2: ...</steps> <execution_graph>(api_name1)->(api_name2, api_name3)->..</execution_graph> <actions>[{\"name\":\"api_name\", \"arguments\": {\"arg_name\": \"value\", ...}}, ... (more API calls as required)]</actions> <tools>[{\"name\":\"api_name1\",\"description\":\"\",\"inputSchema\":{\"type\":\"object\",\"properties\":{\" arg_name1\":{\"type\":\"\",\"description\":\"\"},\"arg_name2\":{\"type\":\"\",\"description\":\"\"}},\" required\":[\"arg_name2\"]}},{\"name\":\"api_name2\",\"description\":\"\",\"inputSchema\":{\"type\":\" object\",\"properties\":{\"arg_name1\":{\"type\":\"\",\"description\":\"\"},\"arg_name2\":{\"type\":\"\",\" description\":\"\"}},\"required\":[\"arg_name2\"]}}]</tools> </workflow> <workflow> (more workflows) </workflow> A.3 Trajectory Generation"
        },
        {
            "title": "Trajectory Generation",
            "content": "You are tasked with generating high-quality multi-turn dialogue trajectories based on given text document. The trajectory should demonstrate an AI assistant helping users complete tasks while strictly following domain-specific rules and constraints. You will be provided with: - list of Available Tool Candidates; - source text document that contains the description of the scenario and task steps; 12 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text ## Completion Requirements 1. System Prompt: Extract and explicitly state ALL important domain-specific rules and constraints from the source text document. Example: <system> You are agent specilized in retail domian. Here are some basic rules to follow: - An order can only be cancelled if its status is pending ... - Modify action can only be called once, and will change the order status to pending (items modifed) ... - ... </system> 2. User Task: Create natural, progressive user requests that test the systems rule enforcement and constraint handling. Here are some features: - Naturalness: Requests should reflect real-world use cases - Ambiguity: User requests are often incomplete, requiring the assistant to analyze or clarify them. Example: <user> want to cancel order #W2575533. </user> (the user do not provide the specific reason, and the assistant should ask for clarification); <user> Recommend me desktop. often go out. </user> (the user do not explicitly state the attribute of the item, but the assistant should analyze and know it based on the stated preference) - Consistent: Users intention, persona, and their behavior should be consistent across the dialogue. - Complex: The users request is challenging enough to test the assistants ability. Users can make requests that violate domain rules and are not allowed to alert the assistant ( e.g., do not ask the model to verify the order status first). At least in one turn, the users request is very complex and require assistant to handle it carefully. Example 1: <user> need to make several changes to my order #W2575533. Can change the E-Reader to different size, swap the Garden Hose color, and also update my shipping address </user > (Requires assistant to: check order status, verify each item can be modified, handle address change separately, remind about one-time modification limit) Example 2: <user> Check my tire pressures. If any of them are low, find me the nearest service station and also check if have enough fuel to get there (Requires: check tire pressure, evaluate condition, conditionally call find_nearest_shop, check fuel level, calculate if sufficient) Example 3: <user> Im planning three-day trip starting from Hangzhou, and need help creating an itinerary. One more thing about the second day - Im trying to be smart about my budget. If end up booking luxury hotel that costs 800 CNY or more per night, then need to be more careful with other expenses: my total spending on both restaurants (lunch and dinner) should stay under 350 CNY, both restaurants should be rated at least 4.0 stars, and the afternoon attraction ticket needs to be less than 120 CNY. </user> (Requires: check multiple constraints) 3. Assistant: Generate Responses that Demonstrate Rule Enforcement, Clear Communication, and Intelligent Problem-Solving: - Reasoning and Adaptive Planning: The Assistant should reason through problem contexts and plan appropriate steps. Sometimes users may not be able to directly provide the parameters for tool calls, and the assistant needs to accurately consider whether the parameter values can be obtained through other information and tools. - Precondition Checks: Before executing tasks, the Assistant should validate any necessary preconditions (e.g., authenticating identity, verifying the status of an order) - Domain Rules and Constraints: The Assistant must follow domain-specific rules at all times. Ensure the assistant tool call and response genuinely addresses those requirements. - User-Centric Principle: The Assistant must accurately understand and satisfy all user For example, if user states \" needs and preferences without breaking domain rules. prefers A, wants B, and tell me C\", the assistant should satisfy all requirements. - No Hallucination: 13 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text - Context Faithfulness: Maintain absolute fidelity to tool outputs. If data contradicts user expectations, explicitly report the discrepancy instead of distorting facts to force match. - Tool call arguments: The Assistant must only use argument values that are explicitly provided or implied by the user. It must not fabricate IDs, names, or other parameters; if any required value is missing or unclear, the Assistant should ask the user to supply it before calling the tool. - Consequence-aware: Before executing any write operation that modifies the environment, the Assistant must actively think and assess its impact. For changes that are irreversible, the assistant should obtain explicit user comformation before proceeding. - Limitations: If the current request is beyond the assistants ability, the Assistant must communicate this limitation clearly. For example: If the users needs are beyond domain rules, the Assistant must explain the limitation but should still respect the users needs. (e.g., \"I cannot do A, but can do B. Should proceed with B?\" -> Wait for conformation). - Correct Tool Calls: - Tool call format: <func>{\"name\":\"exact_tool_name\", \"arguments\": {\"arg\": \"value\"}}</func >. - Exact Matching: The Assistant must ensure that tool calls exactly match the available candidate tools. Calling tools that do not exist is prohibited. - Parameter Validation: The arguments passed to the tools must exactly match the tool definitions, including the required parameters. The Assistant should avoid any missing parameters and validate that the values are accurate. 4. Tool Responses: Structured and Complete - Correctness of Tool Call: success response should only be returned if the tool call is correct (including both the correct function name and parameters). If any part of the tool call is incorrect or incomplete, the tool should return failure response, indicating what went wron - Success Response: The tools success response must return all relevant information in well-structured format (e.g., JSON). This includes not just the requested data, but also any other relevant details, such as order ID, status, product ID, item ID, etc., in case the user requires further context. - Error Response: When there is an error, the tool should return only the error message. No additional information should be provided that directly aids the Assistant in recovering from the error. - Consistent Response Structure: The format and content of the tools responses should remain consistent throughout the conversation. This ensures clarity and reliability in the tools output, helping to maintain smooth user experience. - Dont confuse the order between turns: - user message or tool result should be followed by an assistant message. (<user>...< user> / <tool>...</tool> -> <assistant> ... </assistant>) - If the assistant message includes tool call, it can be followed by tool message; (< assistant>...<func>...</func></assistant> -> <tool>...</tool>) - otherwise, it should be followed by user message. (<assistant> ... </assistant> -> < user> ... </user>) - Tool result cannot followed by user message. (MUST NOT OUTPUT: <tool>...</tool> -> <user >...</user> (incorrect example)) 5. Trajectory Pattern: The trajectory should exhibit multiple interaction patterns, with at least 3 distinct patterns appearing in total. Each individual pattern should be used at most 2 times within single trajectory. [Pattern 1: Domain Rules & User Need conflicts] [Pattern 2: Error Recovery] [Pattern 3: Clarification and Disambiguation] [Pattern 4: Assistants Multi-hop Reasoning] [Pattern 5: Assistants Awareness of Domain Rules] ## Other Requirements - Always use English. - The whole trajectory should be reasonable, realistic (conform to real-world dialogue scenarios and interactions), and fit the context of multi-turn tool usage. 14 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text - Dont confuse the order between rounds: user message or tool result should be followed by an assistant message. If the assistant message includes tool call, it can be followed by tool message; otherwise, it should be followed by user message. Tool result cannot followed by user message. - Tool call format: <func> {{\"name\": \"exact_tool_name_in_toolsets\", \"arguments\": {{\"arg\": \"value\"}}}} </func> ## Given Inputs ### Available Tool Candidates {candidate_tools} ### source text document consisting of Task Steps Description {current_task} ## Output Format You must STRICTLY follow the following output format. Ensure ALL tags are properly opened and closed. Conversations like \"<tool> ... </assistant>\", \"<assistant>...</user>\" are wrong!!! <system> ... </system> <user> ... </user> <assistant> ... <func> {{\"name\": \"...\", \"arguments\": {{...}}}} </func> </assistant> <tool> ... </tool> A.4 Trajectory Refinement"
        },
        {
            "title": "Trajectory Refinement",
            "content": "You are rewriting complex, realistic multi-turn tool-use trajectory for agenic training. Goal: The trajectory should be complex, natural, no-hallucination and show the wisdom of the assistant. It must show the assistants ability for correct tool use, reasoning, context understanding, and communication skills with users. Please carefully consider what problems exist with the existing synthetic trajectories and how to improve them to create high-quality trajectories. You can follow the following guidelines. # Guideline ### System Prompt Complexity You need to refine and upgrade the constraints of the system prompts to make them more structured, systematic, and consistent with real-world logic, in order to fully test the agents ability to make correct tool calls in complex scenarios. You also need to define the database schema in the system prompts, and the data structure returned by the tool will be based on this. 15 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text ### User Request Complextity & Naturalness - Natural: Natural requests may include colloquial language, implied context, vague references to prior steps, or real-world motivations (e.g., saving time/money, lose weight). Avoid overly formal or purely instructional language. - User diversity: Create user profile and maintain the users personality and characteristics throughout the conversation history. - Requires Deep Analysis & Reasoning: The users request must necessitate careful analysis and multi-step reasoning to identify the correct tool(s) and determine appropriate parameter values. - Requires Analysis of Tool Dependencies & Outputs: The request should force the assistant to understand dependencies between tools and use outputs from previous steps to decide which tool to invoke next. - In at least 1 turn, the users request contains multiple constraints, including explicit constraints, implicit requirements that require the assistant to infer. - In at least 1 turn, The user asks question that can only be answered by reasoning across outputs from multiple tool calls in the long context. - Challenging Pitfalls: **MUST INCLUDE AT LEAST 1-2 PITTFALLs** for one trajectory. This pattern sets traps to test the assistants ability to correctly make robust tool calls based on rules, constraints or user preferences. The trajectory must include request for this kind of challenging tool calls, and the assistant must explicitly analyze and identify these pitfalls to achieve robust tool calls. ### Assistant Intelligibility Demonstrate the following capabilities of the assistant: (1) Communication Skills - User Intent Understanding: Accurately interpret the underlying goals and context of the users request. - Confirmation & Clarification: Proactively confirm key details or ask the user to clarify ambiguous information when necessary. - Capability Limitation Awareness: Clearly communicate the boundaries and limitations of the assistants available capabilities. - Result Explanation & Summarization: Interpret, distill, and present tool outputs or complex information in structured, understandable manner. - Proactive Assistance: Anticipate potential user needs based on context and offer helpful suggestions in advance. (2) Robust Tool-Calling Capability - Tool Selection: Choose the most appropriate tool from the available set based on task requirements. - Sequential Tool Usage: Plan and execute multi-tool workflows with correct dependencies and order. - Parameter Handling: Correctly construct and validate complex argument structures (e.g., nested objects, lists), respecting type, range, and format constraints. - Result Analysis: Parse and evaluate tool responses, extract relevant information, and determine validity for subsequent steps. - State Tracking: Maintain awareness of completed and pending steps in multi-step task. - Constraint Analysis: Identify and adhere to real-world constraints and business rules (e.g ., date ranges, mutually exclusive fields, batch limits). - Error Handling: Gracefully manage tool failures, diagnose error causes, and adjust strategy or inform the user appropriately. - Context Management: Effectively retain and utilize conversation history to ensure coherence across complex interactions. (3) Reasoning & Execution Ability - Planning & Task Decomposition: Break down complex or vague user requests into clear, executable step-by-step plans. - Prerequisite Management: Recognize and acquire necessary information or conditions before executing tasks (e.g., querying environment, requesting user input). - Verification and Validation: Perform essential checks before critical operations, such as exploring available tools, confirming permissions, or validating inputs. 16 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text ### Realistic & Complex Environment IMPORTANT NOTES: - The tool called in the trajectory MUST exist in candidate toolsets, with the parameter type and value correct. - If its necessary to add tools when constructing complex trajectories, you can add them in the final output tools block. GOALS: - This increases the difficulty of choosing tools. diverse range of tools must be included, specifically reading and writing tools. - Increase the difficulty of making totally correct tool calls (parameter type, value). For example, include structured inputs (list, dict, nested objects) and meaningful constraints. - The tools should conform to the database schema as much as possible (may defined in system prompt), and mainly include read-write tools. You can add & modify tools. - Unique: Tool call parameters should use unique database fields (such as user ID, product ID, etc.) as much as possible to mimic real logic. - Realistic: Always avoid using empty placeholders. For example, do not return something like \"5+ more results ...\", \"path=/example\". - Success Response: success response should only be returned if the tool call is correct. The tools response must return complete data structure in well-structured format (e. g., JSON). - Error Response: You are allowed to simulate non-simple errors that might occur in the real world. The tools response must return concise error message. Avoid directly telling the assistant how to solve the problem. ### Trajectory Diversity - Reduce the frequency of repeatedly using certain tools to solve problems, avoiding the reduction of trajectory diversity, and retain only the most valuable trajectories for learning. - Include but not limited to the following pattern, and make the following pattern more difficult, diverse and natural. - [Pattern 1: Environment Complexity] - [1.1: Error Recovery] In multi-turn function calling, models may encounter errors, such as invalid input or failed execution that require recovery. If you think of any suitable, non-trivial, real-world scenario errors, please include this pattern. - [1.2: Long Context] Introduce large volumes of extraneous data to test how well the model can extract crucial details from an overwhelming array of information. - [Pattern 2: Clarification] Tests the models ability to recognize when essential information is missing from the user request. - [2.1: Can be inferred from the system] The assistant actively explore ways to find the essential information and complete the task - [2.2: Can not be inferred from the system] The assistant actively clarifies the situation. - [Pattern 3: Identify Limitation] Requires the model to identify that no available function can fulfill the user request. - [Pattern 4: Assistant guides user operation] For example, if user reports no internet access, the assistant uses tool calls to discover that the SIM card is not inserted, and then guides the user to insert it (this process cannot be performed by the assistant alone due to real-world phycial constraints and requires active communication and guidance between the assistant and the user). # Input Data Toolset: {tools} Trajectory: {our_traj} # Output Requirement - Reduce redundancy: Reduce the frequency of repeatedly using certain tools to solve problems, avoiding the reduction of trajectory diversity, and retain only the most valuable trajectories for learning. - Preserve turn order. - The assistant can only call the tool once per round. - Each tool message must be followed by an assistant message. 17 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text - If an assistant message contains tool call, it must be followed by tool message ( tool result). - Tool messages must not be followed directly by user messages. - If the original trajectory violates these rules or misuses tools, fix it in the rewritten version. - Output all candidate tools (the original tools + new tools if needed) - You must strictly follow the following output format. # Output format <toolsets> All candidate tools in JSON, OPENAI format. [ {{ \"name\": \"\", \"description\": \"\", \"inputSchema\": {{ \"type\": \"\", \"properties\": {{ }}, \"required\": [] }} }}, ... ] </toolsets> <system> [role and domain rules here] </system> <user> ... </user> <assistant> ... <func> {{\"name\": \"...\", \"arguments\": {{...}}}} </func> </assistant> <tool> [concrete tool response in JSON format if tool calls are made] </tool> <assistant> ... </assistant> ... (more conversations here) \"\"\" A.5 Hallucination Detection"
        },
        {
            "title": "Hallucination Detection Prompt",
            "content": "You are given multi-turn tool-use trajectory. Please evaluate the trajectory according to the following rubrics. Your job is to score the trajectory on the following binary rubrics. For EACH rubric, you must output 0 or 1 only, according to the criteria below. For each rubric, if there is no hallucination of the following content throughout the entire trajectory, the corresponding rubric score is 1. If any single round does not meet (the condition), the corresponding rubric should be scored as 0. Be strict in your evaluation. 18 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text ## Rubric R1: Tool-call hallucination - Check whether any tool call uses argument values that are not provided or reasonably derivable from the dialogue context. R2: Capability hallucination Check whether the assistant makes incorrect claims about what can or cannot be done. - H2-a False inability: The user request IS solvable using the available tools, but the assistant claims it cannot be done or refuses without justification. - H2-b Missing limitation disclosure: The user request is NOT solvable with the available tools, but the assistant proceeds as if it is solvable, or fails to clearly explain the limitation and offer the closest feasible alternative. R3: Context hallucination Check whether the assistant misinterprets the ongoing context or references things that are not true in the dialogue. - Wrongly referencing previous user constraints, preferences, or decisions. - Cross-turn inconsistency: changing entities/values (IDs, counts, dates, constraints) without new evidence or tool output. - Conflicting summaries: later summary contradicts earlier established facts. ## Input {trajectory} ## Output Format Return single JSON object with EXACTLY these keys and integer values 0 or 1: {{ \"R1\": 0 or 1, \"R2\": 0 or 1, \"R3\": 0 or 1, }} Do NOT output anything else (no explanations, no comments). Hyper-parameter Setting We use the sample hyper-parameter during the experiments. Please refer to Table 3 for details. Hyperparams Values Hyperparams 5e-6 learning rate 0.1 warmup ratio cosine lr scheduler 2 epoch zero3 Deepspeed weight decay max length batch size BF16 tool-call template Hermes Values 0.05 32K 64 True Table 3 SFT Hyperparameters used. Ablation Study on τ 2-bench We report the ablation results of τ 2-bench (Airline, Retail) in Table 4."
        },
        {
            "title": "D More Analysis on Refinement Stage",
            "content": "We report the mean number of messages, tools, and tool calls per trajectory before and after the refinement stage in Table 5. The results demonstrate substantial increase in trajectory complexity across all measured dimensions following refinement. 19 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text Model Qwen3-8B Qwen3-8B-GEM w/o refine w/o LLM-Based Check Qwen3-32B Qwen3-32B-GEM w/o refine w/o LLM-Based Check Airline Retail Avg@4 13.00 22.00 25.00 22.00 21.00 35.50 31.00 35.00 Pass@4 Avg@4 38.16 18.00 44.52 40.00 41.23 36.00 42.76 36.00 43.20 40.00 55.48 56.00 40.35 56.00 56.80 52.00 Pass@4 66.67 75.44 74.56 71.05 70.18 86.84 73.68 82.46 Table 4 Ablation Study on τ 2-bench (Airline, Retail). Method w/o refinement w/ refinement # Number of Messages # Number of Tools # Number of Tool Calls 5.01 8.6 30.05 46.1 7.83 16.3 Table 5 Statistics of synthetic trajectories before and after refinement. The refinement stage significantly increases the complexity of conversation trajectories, as evidenced by the rise in the average number of messages, distinct tools used, and total tool invocations."
        },
        {
            "title": "E Domain Analysis",
            "content": "We analyze the domain distribution in Figure 9. The results demonstrate the diver domains existing in the raw text corpora. Figure 9 Domain Distribution."
        },
        {
            "title": "F Example of Synthesized Trajectory",
            "content": "Please refer to Figure 10 and 11. The original text provides unstructured information regarding photo frame products, measurement methods, and specific size constraints. The system prompt establishes the persona of custom framing specialist, enforcing strict business workflow from user authentication to final order placement. set of functional tools was designed to support this, including authentication, dimension calculation, constraint checking, and order processing. The final trajectory demonstrates the assistants exceptional tool-calling and 20 Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text logical reasoning capabilities, particularly its ability to adapt by adjusting parameters and replanning the tool chain after an initial constraint violation, ultimately ensuring successful transaction. Figure 10 Full example of our generated trajectory (Part 1). Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text"
        }
    ],
    "affiliations": [
        "Meituan",
        "Renmin University of China"
    ]
}