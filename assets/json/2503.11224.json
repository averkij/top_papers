{
    "paper_title": "Technologies on Effectiveness and Efficiency: A Survey of State Spaces Models",
    "authors": [
        "Xingtai Lv",
        "Youbang Sun",
        "Kaiyan Zhang",
        "Shang Qu",
        "Xuekai Zhu",
        "Yuchen Fan",
        "Yi Wu",
        "Ermo Hua",
        "Xinwei Long",
        "Ning Ding",
        "Bowen Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "State Space Models (SSMs) have emerged as a promising alternative to the popular transformer-based models and have been increasingly gaining attention. Compared to transformers, SSMs excel at tasks with sequential data or longer contexts, demonstrating comparable performances with significant efficiency gains. In this survey, we provide a coherent and systematic overview for SSMs, including their theoretical motivations, mathematical formulations, comparison with existing model classes, and various applications. We divide the SSM series into three main sections, providing a detailed introduction to the original SSM, the structured SSM represented by S4, and the selective SSM typified by Mamba. We put an emphasis on technicality, and highlight the various key techniques introduced to address the effectiveness and efficiency of SSMs. We hope this manuscript serves as an introduction for researchers to explore the theoretical foundations of SSMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 4 2 2 1 1 . 3 0 5 2 : r Technologies on Effectiveness and Efficiency: Survey of State Spaces Models Xingtai Lv1, Youbang Sun1, Kaiyan Zhang1, Shang Qu1,2, Xuekai Zhu1 Yuchen Fan1,2, Yi Wu3, Ermo Hua1, Xinwei Long1, Ning Ding1,2, Bowen Zhou1,2 1Department of Electronic Engineering, Tsinghua University 2Shanghai Artificial Intelligence Laboratory, 3Robotics Institute, Carnegie Mellon University lvxt24@mails.tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "State Space Models (SSMs) have emerged as promising alternative to the popular transformer-based models and have been increasingly gaining attention. Compared to transformers, SSMs excel at tasks with sequential data or longer contexts, demonstrating comparable performances with significant efficiency gains. In this survey, we provide coherent and systematic overview for SSMs, including their theoretical motivations, mathematical formulations, comparison with existing model classes, and various applications. We divide the SSM series into three main sections, providing detailed introduction to the original SSM, the structured SSM represented by S4, and the selective SSM typified by Mamba. We put an emphasis on technicality, and highlight the various key techniques introduced to address the effectiveness and efficiency of SSMs. We hope this manuscript serves as an introduction for researchers to explore the theoretical foundations of SSMs."
        },
        {
            "title": "Introduction",
            "content": "Large language models are playing an increasingly significant role in various aspects of real-world applications. Although the Transformer architecture [1] remains the dominant framework for mainstream language models, alternative architectures have emerged, aiming to address some of the inherent limitations of Transformers. Among these non-Transformer architectures, the State Space Model (SSM), particularly Mamba [2] and its variants, has attracted considerable attention and gained widespread application. Compared to Transformers, state space model-based algorithms exhibit immense potential for computational efficiency, excelling in handling long-text tasks. Moreover, the SSM series of models have undergone extensive development and refinement, enabling them to handle various data formats that can be serialized, such as text, images, audio, and video. Their performance on these practical tasks often rivals that of Transformers. Consequently, these models have found widespread use in domains such as healthcare, conversational assistants, and the film industry [3, 4, 5, 6, 7, 8, 9, 10]. The development of the SSM series can be broadly categorized into three distinct stages, marked by three milestone models: the original SSM, the Structured State Space Sequence Model (S4) [11], and Mamba [2]. Initially, the SSM formulation was proposed to describe physical systems in continuous time. The SSMs are then discretized for computer analysis with tractable computations. After discretization, the SSMs can model sequence data, which marks the first stage of the SSMs development. However, these early SSMs were rudimentary and faced several limitations, including weak data-fitting capabilities, lack of parallel computation, and susceptibility to overfitting, making them inadequate for practical applications. Despite these shortcomings, these models offered advantages such as low computational complexity, flexible core formulas that allowed for derivation into various forms, and significant potential for future improvements. S4 and its variant models represent the second stage in the development of the SSM series. S4 took into equal contributions corresponding authors account the time-invariant situation and utilized the convolutional expression form of the core formula of SSM, dramatically reducing computational complexity and enabling efficient computation. This advancement resolved challenges such as exponential memory decay and the inability to capture long-range dependencies effectively, significantly improving the utility of SSMs. Building on the S4 architecture, optimized models like DSS [12] and S4D [13] were introduced. Nevertheless, S4 hardly had any optimizations for the underlying computing hardware and usually struggled to achieve satisfactory performance in practical tasks. The third stage began with the advent of Mamba. By introducing selectivity, optimizing the underlying hardware, and improving the calculation formulas, Mamba achieved better adaptation to current computing hardware and attained practical task performances close to those of the Transformer. Based on Mamba, numerous model architectures developed for practical application scenarios have been continuously proposed, and models that combine the Mamba architecture with the Transformer architecture are also being explored. Figure 1: Technologies of SSM Series and their corresponding relationships. Each technical description card includes the techniques name, mathematical formulation, and objective. Several key techniques have provided technical support and ensured the development of the SSM series. Methods such as Eulers method [14], zero-order hold (ZOH), and bilinear transform [15] discretization enable the transformation of SSMs from continuous-time to discrete-time, allowing these models to effectively handle 2 discrete sequential data. To enhance the performance of SSMs, researchers have introduced mathematical techniques like LegS [16], HiPPO [17], and selective SSM [18]. These techniques enable stable retention of historical sequence information and significantly improve the models text processing capabilities. In terms of efficiency, techniques such as DPLR [19], DFT, IDFT [20], semi-separable matrices [21] lay the foundation for faster computations, and tools like the Woodbury identity [22] and Cauchy kernel [23] streamline the computational processes. Together, these recent advancements greatly improve the computational efficiency of SSMs with little to no loss in performance. Figure 1 presents these techniques and their corresponding relationships with different models in chronological order. In this survey, We first introduce the mathematical formulation of SSMs in Section 2. Next, Section 3 explores additional structures in SSMs with works such as S4, Section 4 introduces selectivity for SSMs leading with Mamba. These sections focus on detailing core techniques, summarizing optimization strategies in model architectures, and discussing theoretical analyses. Section 5 explores the relationship between SSMs and other model structures, particularly Transformer, and examines the trends in SSM architecture optimization. Section 6 highlights the applications of SSMs across various domains, including video processing, molecular modeling, speech and audio analysis, and so on."
        },
        {
            "title": "2 State Space Models: From Continuous To Discrete",
            "content": "The concept of state space, along with the state space model, holds significant historical backdrop. The continuous-time state space model is pivotal and versatile tool for describing dynamical systems. Early applications of state space models include theoretical physics, communication signal processing, system control, and allied fields [107, 108]. Notably, Kalman [109] employs state space models to characterize linear dynamic systems and resolve the optimal estimation problem. By directly discretizing the continuous-time SSM, we obtain the discrete-time SSM, also referred to as the original SSM in this survey. The discrete-time SSM is capable of handling series of discrete input data and has been increasingly gaining attention as sequence model. In this chapter, Section 2.1 introduces the continuous-time State Space Model. Section 2.2 presents the discretization methods and the resulting discrete-time SSM. Section 2.3 discusses several fundamental properties of SSMs, while Section 2.4 explores some model structures with the original SSM. 2.1 Continuous-Time State Space Models The classical linear continuous-time state space model (SSM) was first widely employed in control system theory [109]. It describes dynamical systems through specific differential equations, and can be expressed as the following: h(t) = A(t)h(t) + B(t)x(t), y(t) = C(t)h(t) + D(t)x(t), (1) (2) where x(t) RNin is the input vector signal, h(t) RNs is the hidden state vector signal, and y(t) RNout is the output vector signal. The matrices A(t), B(t), C(t), and D(t) represent the parameter matrices of the state space model. Specifically, A(t) RNsNs is the system matrix, which describes how the current state influences its rate of change. B(t) RNsNin , the control matrix, represents the effect of the input on the state change. C(t) RNoutNs , the output matrix, captures how the system states affect the output. D(t) RNoutNin, the feed-forward matrix, illustrates direct input-output relationships, circumventing system states. Generally, in control systems or real-world physical systems, these parameters are either determined by the inherent characteristics of the system or acquired through conventional statistical inference methods. The classical continuous-time SSM can capture many systems in wide range of subjects. For instance, it can model the RC oscillator circuit system shown in Figure 3, where v, i, L, C, and represent the voltage, current, inductance, capacitance and resistance, respectively. Let the input signal be the systems input voltage [vi(t)] = x(t), the output signal be the systems output voltage [vo(t)] = y(t), and the two components of the systems hidden state be the current in the inductor and the voltage across the capacitor [iL(t), vc(t)]T = h(t). Derived from fundamental physical principles such as Ohms law and Kirchhoffs circuit laws, the state space"
        },
        {
            "title": "2.1 Continuous-Time State Space Models",
            "content": "Original SSM Architecture Variants Application Deep SSM [24]; LOCOST [25];2D-SSM [26];Spectral State Space Models [27];SpaceTime [28];LMUFFT [29] Point2SSM [30];NCDSSM [31] SSM Series Structured SSM Architecture Variants S4 [11]; S4D [13];S4ND [32];DSS [12];S5 [33];LiquidS4 [34];BiGS [35];GSS [36];SMA [37];BST [38];SPADE [39]; GateLoop [40];Decision S4 [41];ConvS5 [42] Appication LS4 [43];SaShiMi [44];TranS4mer [45];DSSformer [46]; S4M [47];ViS4mer [48];DiffuSSM [49];Hieros [50]; R2I [51];GraphS4mer [52];GraphS4 [53];fMRI-S4 [9];Spiking-S4 [10] General Selective SSM Architecture Variants Vision Mamba [2]; Jamba [54];MambaMixer [55]; MoE-Mamba [56];BlackMamba [57]; DenseMamba [58];Hierarchical SSM [59];SIMBA [60];Mamba4Rec [61]; MambaFormer [62];ReMamba [63]; Samba [64] Vision Mamba [65];VMamba [66];GraphMamba [67];MambaND [68];Gamba [69];PanMamba [70];MambaIR [71];VLMamba [72];SpikeMba [73]; Video Mamba Suite [74]; Zigma [75];LocalMamba [76];PlainMamba [77] Multi-Modal Cobra [78];Sigma [79];FusionMamba [80] Medical Application Point Cloud Others Mamba-UNet [3]; Vivim [4];MambaMorph [5]; VM-UNet [81];SwinUMamba [82];nnMamba [83];FD-Vision Mamba [84];Semi-Mamba-UNet [85];PMamba [86];MamaMIR [87];WeakMamba-UNet [88];CMTMMH [89];Caduceus [90];MambaMIL [91]; CMViM [92];T-Mamba [93]; ProMamba [94] 3DMambaComplete [6]; PointMamba [95];Point Cloud Mamba [96] U-Mamba [7];SegMamba [8];ResVMamba [97];O-Mamba [98]; Motion Mamba [99];STGMamba [100];RS3Mamba [101]; MambaTab [102];MambaByte [103]; MambaStock [104];HARMamba [105]; RhythmMamba [106] Figure 2: Typology of SSM Series. The Architecture Variants presents the common SSM architectures, while the Application shows the practical implementations of SSMs in real-world scenarios."
        },
        {
            "title": "2.2 Discretization of Continuous-Time SSMs",
            "content": "Figure 3: The illustration of RC oscillator circuit, where L, C, and denote the inductance, capacitance and resistance, respectively. model for this system can be expressed as: h(t) = A(t)h(t) + B(t)x(t) = (cid:21) (cid:20)R 0 0 1 h(t) + [1]x(t), y(t) = C(t)h(t) + D(t)x(t) = (cid:20)Lt 0 (cid:21) 0 1 h(t) + [1]x(t). (3) (4) SSM effectively describes how systems evolve over time. Much like tracking moving object, the model uses its current position (state) to predict where it will go next. Since every change in the system follows clear mathematical rules, it is possible to predict future behavior or estimate hidden information from partial observations. Furthermore, SSM is particularly suitable for describing complex systems, as it can simultaneously handle multiple inputs and outputs. For example, when tracking rocket, inputs might include factors like fuel and wind speed, states could represent position and velocity, and outputs could correspond to the measurements recorded by the sensors. 2.2 Discretization of Continuous-Time SSMs The above SSM described by Eq. 1,2 is termed the continuous-time SSM because x(t), h(t), and y(t) represent continuous-time signals. However, in most practical applications, the model inputs, hidden states, and outputs are instead discrete sequences, denoted as = (x0, x1, x2, . . . ), = (h0, h1, h2, . . . ), = (y0, y1, y2, . . . ). For instance, when processing natural language, the input and output of the model are token embeddings, which are discrete values rather than continuous functions. Directly applying the continuous-time SSM to discrete data requires fitting the discrete data points into continuous signals, complex and rarely used process. Instead, more effective and practical approach is to discretize the SSM. Essentially, the discrete-time SSM parameters and are expressed in terms of the continuous-time SSM parameters A, B, and an additional time-step parameter . And the discretization of SSMs generally involves addressing the ordinary differential equation (ODE) within the model representation. The discrete-time SSM can be represented by the following difference equations: hk = Ahk1 + Bxk, yk = Chk + Dxk, (5) (6) where xk RNin , hk RNs and yk RNout denote one single data point of the input, hidden state, and output data sequences respectively. RNsNs , RNsNin , RNoutNs , and RNoutNin are model parameters of the discrete-time SSM. Notably, the parameter matrix D, when researched as sequence model, is often disregarded. This is because is generally viewed as skip connection and does not directly influence the state h. Usually, is easy to compute, and can be approximated by other structures. Consequently, Eq. 5,6 are commonly expressed as hk = Ahk1 + Bxk, yk = Chk 5 (7) (8)"
        },
        {
            "title": "2.3 Fundamental Properties of SSMs",
            "content": "Common discretization methods include Eulers method [14], zero-order hold (ZOH), and the bilinear transform [110]: (Euler) = + A, (ZOH) (Bilinear) = (I A/2)1(I + A/2), = exp(A), = B, = (A)1(exp(A) I)B, = (I A/2)1B. (9) (10) (11) The discrete-time SSM can be used as sequence model to model and process data that can be serialized. For example, in natural language processing, given an input sequence = (x0, x1, x2, . . . ) where each xi represents the embedding vector of linguistic token, the SSM can iteratively apply Eq. 7,8 to compute the hidden state-output pairs (h0, y0), (h1, y1), . . . This recurrent computation process ultimately generates the output token representations = (y0, y1, y2, . . . ). When both the input token embeddings and the corresponding desired output token representations are provided, the SSM can be trained to model the relationship between them. This establishes its capability for modeling sequential linguistic information. In this paper, we focus on SSMs as sequence models and use \"SSM\" to refer to the discrete-time SSM unless otherwise specified. 2.3 Fundamental Properties of SSMs The discrete-time SSM described by Eq. 7,8 can handle discrete sequential data and serve as sequence model. We next discuss some fundamental properties of SSMs in this context. crucial property of the discrete-time SSM is its linearity. Specifically, when emphasizing the input and output and simplifying Eq. 7,8 to = SSM(A, B, C)(x), it follows that SSM(A, B, C)(x1 + x2) = SSM(A, B, C)(x1) + SSM(A, B, C)(x2), (12) 0, x2 1, 0, x1 2, . . . ), x2 = (x2 where x1 = (x1 2, . . . ) represent two different sets of input data. Another characteristic of linearity is that the model parameters A, B, are independent of the input data x. On one hand, this linearity enables optimizations for computational efficiency and parallelization, which will be discussed in Chapter 3. On the other hand, linear models often struggle with fitting complex data. Chapter 4 will provide detailed discussion of enabling input-dependent parameterization of model parameters A, B, C, thereby transforming the SSM into nonlinear architecture with enhanced selectivity. 1, x2 Another fundamental property of SSM is that its expression can be derived into various forms. For instance, Eq. 7,8 naturally align with the representation format of recurrent models. These expressions can also lead to the derivation of convolutional forms, such that: = where = (x0, x1, x2, . . . ), = (CB, CAB, CA 2 B, . . . ). (13) This convolutional structure creates the conditions necessary for the efficient computational algorithms introduced in Chapter 3. This diversity of SSM expressions opens up flexibility in architectural modifications. Furthermore, the SSM has several inherent limitations, including non-parallelizable computations, instability in capturing long sequence dependencies, and constrained approximation capacity. These challenges can be addressed by incorporating structured parameters and selectivity, as will be elaborated in Chapters 3 and 4. In this paper, we refer to the SSM described by Eq. 7,8 as the \"original SSM\" or the \"vanilla SSM\". 2.4 Models with Original SSMs Despite its limitations, the original SSM remains widely used in some studies, including recent works, due to its simplicity and ease of implementation. This section presents some models that incorporate the original SSM. These model structures either embed the original discrete SSM within other architectures or integrate new computational modules into the SSM structure. These modifications improve performance on tasks such as time series forecasting, long-text summarization, and two-dimensional data processing. To accomplish long document abstractive summarization, Bronnec et al. [25] construct an encoder layer with an SSM as its central component, termed the LOCOST. Starting from the convolutional representation of SSM, they introduce the Bidirectional SSM, which incorporates causal convolution and cross-correlation to simultaneously consider contextual information before and after each token in the sequence. This architecture builds upon the low computational complexity and superior long sequence handling of SSMs and further achieves improved context capture. Baron et al. [111] propose 2-D SSM layer based on Roessers SSM 6 model [112]. It expands states to capture both horizontal and vertical information, enhancing the SSMs understanding of two-dimensional data. Agarwal et al. [27] apply the Spectral Filtering algorithm to learn linear dynamical systems. The proposed Spectral SSM employs fixed spectral filters that obviate the need for learning. It therefore effectively maps input sequences to new representation space and captures long-term dependencies in sequential data. These features improve the models prediction performance and stability, especially in sequences containing long-range dependencies. Zhang et al. [28] introduce closed-loop prediction mechanism to SSMs, where the SSM at the decoder layer not only receives information from the encoder layer but also incorporates feedback from its own output. This design improves the models flexibility and accuracy in handling long-term prediction tasks."
        },
        {
            "title": "3 Structured State Space Models",
            "content": "The introduction of structured parameters to SSMs is considered to be one of the most remarkable recent advancements for sequence modeling. By the application of various structures to the parameters in SSMs, the effectiveness and efficiency of these models are improved. As one of the most prominent structured SSMs, S4 has garnered widespread attention and influence [11]. Within this chapter, we first discuss the structures of model parameters introduced by S4 with an emphasis on its theoretical motivations. Next, we consider other structural mechanisms beyond S4. Lastly, we explore the role SSMs play as part of more sophisticated models. Section 3.1 provides an overview of the structured SSM, especially S4, from an accessible perspective without delving into details. Sections 3.2, 3.3 elaborate on S4 from theoretical perspective, focusing on its effectiveness and efficiency, respectively. Section 3.4 summarizes alternative structures to which the parameters of SSMs are restricted. Section 3.5 reviews models that integrate structured SSM as core component. 3.1 Overview of Structured State Space Models The term structured SSM refers to SSMs where specific structures have been applied to their system dynamics. The exact structures can be formulated by number of sophisticated methods such as initialization, parameterization or placing explicit constraints on parameters. For example, consider model parameter matrix RN . When its off-diagonal elements are restricted to zero, becomes diagonal matrix and thus qualifies as structured parameter. This diagonal structure reduces storage overhead and computational cost. In contrast to original unconstrained SSMs, structured SSMs such as S4 exhibit number of desirable mathematical properties, which greatly enhance their effectiveness and efficiency. Specifically, S4 demonstrated exceptional performance across wide range of sequence modeling tasks. This is achieved by introducing series of structural features: S4 first leverages HiPPO [17] to constrain parameters to the Legendre memory unit (discussed in detail in Section 3.2). This specific structure enables S4 to address challenges associated with memory retention for long-range dependencies and alleviate issues such as gradient vanishing. Additionally, S4 employs the Diagonal Plus Low-Rank (DPLR) [19] structure (elaborated in Section 3.3), which facilitates the use of efficient algorithms. These algorithms enable parallel training and boost computational efficiency and stability. With the introduction of DPLR, the computational complexity is reduced from O(LN 2 ) to O(L + ). Beyond S4, alternative structured SSM variants employ different structural constraints. These modifications seek to refine and simplify the structured SSM framework and address the various inherent challenges in SSMs, including exponential memory decay, gradient vanishing, gradient explosion, instability in capturing long sequence dependencies, low training efficiency, and limited parallel computing capabilities. 3.2 Effectiveness: High-Order Polynomial Projection Operators (HiPPO) The traditional discrete-time SSM framework suffers from suboptimal performance, with issues such as exponential memory decay and instability in capturing long sequence dependencies. These issues stem from the models inability to effectively retain the input history. S4 seeks to address these issues by utilizing structured parameters, the specific structure of which is derived through the HiPPO framework [17]. Conceptually speaking, HiPPO addresses the following core problem: How to efficiently reconstruct the history of input using state h? This problem can be formulated precisely as an online function approximation problem, which is investigated in HiPPO. In this subsection, we provide detailed exposition of the HiPPO framework, discuss its specific instances and advantages, and explore the relationship between HiPPO and S4. Gu et al. [17] specifically define memory through the online function approximation problem, which involves representing function by storing coefficients of set of basis functions. Mathematically, given function"
        },
        {
            "title": "3.2 Effectiveness: High-Order Polynomial Projection Operators (HiPPO)",
            "content": "f (t) on 0 and probability measure µ, the distance between two functions and can be expressed as f, gµ = (cid:82) 0 (x)g(x)dµ(x), and the norm of function can be defined as L2(µ) = f, 1/2 µ . Moreover, the cumulative history is typically represented as ft := (s)st. Solving the online approximation problem in this setup involves seeking g(t) G, typically subspace spanned by orthogonal bases, that minimizes ft g(t)L2(µ(t)). The HiPPO framework aims to optimize this approximation at every timestep t. Specifically, given an input function (t) : R+ on 0 and time-varying measure family µ(t), the HiPPO framework consists of the following three steps: }n{0,1,...,N 1} according to µ(t). {g(t) 1. Obtain the Set of Bases. The first step in HiPPO is to generate suitable set of basis functions nδn,m, where δ is } forms the orthogonal bases. This set {g(t) the impulse function. λn is typically set to 1 to ensure {g(t) of bases spans an N-dimensional subspace G. } should satisfy g(t) µ(t) = λ , g(t) 2. Calculate the Optimal Coefficients. Given the generated basis set {g(t) }, the next step is finding the optimal representation of input signal ft := (s)st in the form of polynomial g(t) = (cid:80)N 1 . Here, cn(t) denotes the optimal coefficients to construct polynomial g(t) = argmingGft gµ(t). With the orthogonality of the bases, the optimal coefficients can be computed through the following, n=0 cn(t)g(t) cn(t) = (cid:68) ft, g(t) (cid:69) . µ(t) (14) The HiPPO paper denotes the results of the calculations, g(t) and c(t) := (cn(t))0n<N as projt and coeft, respectively. 3. Differentiate Equation 14 and Yield the ODE. The last equation shows how to calculate c(t) given signal (t). However, in continuous-time state space system, c(t) denotes the state and (t) denotes the input which changes over time. In order to establish closer connection with the expression of SSM (Equation 1), it is necessary to find the optimal matrices and B, Furthermore, and should remain independent of (t). This can be achieved by taking the derivative of the previous equation. Given the Equation 14, the differentiation of state c(t) is calculated with respect to the input function (t). This is depicted by the following ordinary differential equation (ODE): dt c(t) = A(t)c(t) + B(t)f (t), (15) where A(t) RN and B(t) RN 1 often remain independent of (t) and are solely related to the measure µ(t) and the orthogonal subspace G. Equation 15 offers general strategy for computing coefficients c(t). In essence, HiPPO can be conceptualized as system: given function (t) and measure µ(t), it outputs the optimal representation coefficients c(t) of (t) under µ(t). It is worth noting that, in most cases, researchers are primarily interested in the dynamic relationships between (t) and c(t), which are represented by A(t) and B(t). Hence, HiPPO can also be viewed as black box: given measure µ(t), it outputs the corresponding structured parameter matrices A(t) and B(t) satisfying the OED Equation 15 and yielding the optimal representation coefficients c(t). Given specific measure, the HiPPO framework is able to generate optimal state-space equations. We discuss two notable SSM-related HiPPO instances, LegT and LegS. The first variant entails the use of the translated Legendre (LegT) measures. LegT can be expressed as µ(t) I[tθ,t](x), where θ denotes the length of the history being memorized. This measure assigns uniform weights to nearby histories. Through the HiPPO framework, the structured parameter matrices and corresponding to LegT can be determined as LegT(x) = 1 θ Ank = 1 θ (cid:26)(1)nk(2n + 1) 2n + 1 if if , Bn = 1 θ (2n + 1)(1)n, (16) which is exactly the parameter matrices of the Legendre Memory Unit (LMU) [113], an improved model of RNN."
        },
        {
            "title": "3.3 Efficiency: Diagonal Plus Low-Rank",
            "content": "The second example comprises the scaled Legendre (LegS) measures [16], denoted as µ(t) for which the corresponding structured parameter matrices and satisfy: LegS(x) = 1 I[0,t](x), tA(t)nk = (2n + 1)1/2(2k + 1)1/2 + 1 if > if = if < , tB(t)n = (2n + 1)1/2, (17) obtained through the HiPPO framework. LegS is an optimization over LegT. It scales the window over time and assigns uniform weights to all history. Advantages of LegS include mitigation of forgetting, invariance to input timescale, faster computations, and bounded gradients and approximation errors. The HiPPO framework outputs the structured parameter matrices A(t) and B(t) that satisfy Equation 15 and yield the optimal coefficients c(t) for representing (t). In other words, setting the same A(t) and B(t) or similar parameter matrices in Equation 1 enables the state h(t) to optimally represent the input x(t), thereby achieving the memorization of history. S4 utilizes the specific structures generated by HiPPO to help with its long-range dependencies. Although the matrices in Equation 17 are optimal in theory, empirical results have shown that making parameters A(t) and B(t) trainable can further improves performance in general. In S4, if the parameters and are not trained, they are restricted to specific structures through HiPPO. However, training the parameters generally yields better results, hence HiPPO is commonly used for initialization. Typically, initialization often involves using the tA(t) and tB(t) corresponding to the LegS (i.e. Equation 17) and discretizing through Equation 11 in S4. 3.3 Efficiency: Diagonal Plus Low-Rank Another crucial challenge addressed by S4 is computational efficiency. The complexity of the calculating the original SSM in Eq. 7,8 is O(LN 2). And its training process lacks parallelizability. S4 overcomes this limitation by introducing of special structure named Diagonal Plus Low-Rank (DPLR) [19] and employing series of algorithms designed for parallel training and efficient computation. The SSM with DPLR structure satisfies = Λ PQ, CN 1, C1N , where Λ CN is diagonal and P, CN 1 is low-rank. The corresponding algorithm begins with the convolutional representation of SSM: by recursively applying Equation 5,8, yk = CA Bx1 + + CABxk1 + CBxk can be derived This equation leads to Bx0 + CA k1 y = x, RL := (CA B)n[L] = (CB, CAB, . . . , CA L1 B) (18) where = (x0, x1, . . . , xL1) is an input of length L. In Equation 18, the complexity of computing is O(LN 2), making the efficient computation of central aspect of the algorithm. The first pivotal step of this algorithm involves shifting the computation of to the frequency domain by computing its Discrete Fourier Transform (DFT) [20]. This can be denoted as ˆKL(z; A, B, C) := L1 (cid:88) n=0 CA Bzn = C(I zL)(I Az)1B = C(I Az)1B, (19) where denotes the conjugate of C, Ω = (cid:8)exp(2πi ). After obtaining the DFT, can be computed using the Fast Fourier Transform (FFT) algorithm, requiring O(LlogL) operations. Its worth noting that in S4, is directly learned via parameterization, obviating the need for computing = C(I ). This step replaces matrix power operations with efficient low-rank matrix inversions, presented as the subsequent step. ) : [L](cid:9), = C(I zL) = C(I The second integral step of this algorithm simplifies the computation of (I Az)1 through using the Woodbury identity [22]. In the low-rank scenario, the Woodbury identity is (Λ + PQ)1 = Λ1 Λ1P(1 + QΛ1P)1QΛ1, where Λ CN is diagonal, P, CN 1, and (Λ + PQ)1 is invertible. Applying Equation 11, ˆKL(z; A, B, C) = C(I Az)1B = 2 1 + (cid:18) 2 1 1 + (cid:19)1 (20) (21)"
        },
        {
            "title": "3.4 Additional Structural Constraints for Structured SSMs",
            "content": "can be derived. By leveraging the Woodbury identity and = Λ PQ in the DPLR structure, ˆKL(z; A, B, C) = = 2 1 + 2 1 + (cid:18) 2 1 1 + Λ + PQ (cid:19)1 (cid:104) CR(z)B CR(z)P(1 + QR(z)P)1QR(z)B (cid:105) , (22) (cid:16) 2 (cid:17)1 1z 1+z Λ . This step transforms the computation of (I Az)1 into the inversion of where R(z) = the diagonal matrix, streamlining the calculation process. The final step of this algorithm focuses on efficiently computing VR(z)U using the Cauchy kernel [23]. Leveraging the previous steps, the computation of is transformed into calculating four expressions in the format of VR(z)U (i.e. CR(z)B, CR(z)P, QR(z)P, QR(z)B). Given VR(z)U = (cid:80) nun , zλn ) : [L](cid:9) entails Cauchy matrix-vector multiplication. computing VR(z)U for all Ω = (cid:8)exp(2πi This process has been extensively researched and requires only O(L + ) operations. S4 constrains the parameter matrices to adhere to the DPLR structure. However, the limited expressivity of DPLR does not compromise the effectiveness of models such as S4 since techniques such as the HiPPO framework inherently satisfy this structure. Specifically, the HiPPO matrices adhere to the Normal Plus Low-Rank (NPLR) structure: = VΛV PQ = V(Λ (VP)(VQ))V, where CN is unitary, Λ is diagonal, and P, RN r. Particularly, the HiPPO matrices corresponding to LegS satisfy = 1. In this scenario, the NPLR structure is unitarily equivalent to DPLR. In summary, S4 introduces the DPLR structure, shifts computations from time to frequency domain using the SSMs convolutional expression, and simplifies calculations through the Woodbury identity and Cauchy kernel. It achieves parallel computation and reduces training complexity to O(N ( + L)). 3.4 Additional Structural Constraints for Structured SSMs Sections 3.2 and 3.3 introduce two types of structured parameters in S4: the LegS structure derived from the HiPPO framework and the DPLR structure. In addition to these two types of structural constraints on model parameters, several other structural constraints have been proposed for structured SSMs. In this section, we examine additional structured parameters and their corresponding Structured SSMs, specifically S4D [13] and DSS [12]. comparative analysis of these two structured SSMs is also presented. The Diagonal State Space (DSS) [12] removes the low-rank component of the DPLR structure, retaining only the diagonalizable term Λ. Compared to S4, DSS further reduces computational complexity with no significant loss of performance. Empirically, it achieves an average accuracy of 81.88 across six tasks in the Long Range Arena (LRA) benchmark [114], closely rivaling the 80.21 of S4. Its kernel from Eq. 13 can be derived and denoted as: = K,L (Λ, (1)1iN , (cid:101)w) = (cid:101)w Λ1 (cid:0)eΛ I(cid:1) elementwise-exp(P), (23) where R1L represents the kernel of length with sampling interval > 0. (cid:101)w C1N is the parameter matrix. Matrix CN is defined as Pi,k = λik. Λ is the diagonal matrix consisting of λ1, . . . , λN , with all λi = 0. The DSS eliminates the need for matrix powers and instead only requires structured matrix-vector product. S4D [13] restricts the parameter matrix to diagonal structure, and its kernel computation method is simplified: = (B C) VL(A) where VL(A)n,ℓ = ℓ n, (24) where is diagonal matrix, signifies the Hadamard product, and denotes Vandermonde matrix that shares the same O(N + L) complexity with the Cauchy kernel used in the S4 algorithm. Compared to S4, DSS and S4D also varies in terms of methods of discretization. DSS adopts ZOH and S4D can use either ZOH or Bilinear discretization method. During the training process, the real parts of the diagonal matrixs elements might turn positive. This change possibly leads to instability, especially when facing longer input sequences. To circumvent this, DSS imposes constraints to ensure the real part of ΛRe remains negative or substitutes the elementwise exp(P) (as in Eq. 23) with row softmax(P ). The operation of row softmax(P ) normalizes each row of elementwise exp(P) by the sum of its elements. However, the 10 3."
        },
        {
            "title": "Integration of Structured SSMs with Other Models",
            "content": "softmax kernel adds complexity, faces challenges with varying input sequence lengths, and incurs additional memory costs. S4D also imposes constraints on the real component ARe of its matrix A, keeping it negative either by encapsulating it within an exponential function = exp(ARe) + AIm or by employing negative-bounding activation function such as ReLU. Differing from S4D, which may either freeze the matrix or train both and independently, DSS directly parameterizes and jointly trains the product of and as (cid:101)w. In essence, S4D provides comprehensive array of optimization strategies for diagonal state space-based models, catering to the varied demands of these algorithms. The following table presents an overview of the configuration differences between DSS and S4D:"
        },
        {
            "title": "DSS",
            "content": "S4D"
        },
        {
            "title": "Discretization Method",
            "content": "Zero-Order Hold (ZOH)"
        },
        {
            "title": "Training Constraints",
            "content": "Trainable B, matrices Negative ΛRe or softmax kernel Negative ARe (via exp or ReLU) Independently trained B, or alone Jointly trained as (cid:101)w 3.5 Integration of Structured SSMs with Other Models Combining the structured SSM and other models or methods has allowed researchers to exploit the long-context abilities and efficiency of structured SSM in diverse scenarios. This section introduces methods for integrating structured SSM with Liquid-Time Constant SSMs (LTCs), gating mechanisms, and transformer architectures. Hasani et al. [34] scale LTCs to long sequences by adopting reparameterization strategies used in S4 and showing that the additional liquid kernel can be efficiently computed based on the S4 kernel. This formulation combines the efficiency of S4s with the input-dependent state transitions of LTCs, achieving state-of-the-art results on several sequence modeling tasks. To fuse S4-based models with gating mechanisms, Mehta et al. [36] replace the attention module in Gated Attention Unit (GAU) with simplified DSS module. The resulting Gated State Space (GSS) layer performs on par with transformer-based models, is more efficient than DSS models, and showcases promising length generalization abilities. In Ren et al. [37], an SSM module is one of the core components of Sparse Modular Activation (SMA) implementation in the proposed model SeqBoat. The SSM produces state representations that control the sparse activation of the subsequent GAU. Wang et al. [35] use bidirectional pairs of S4D modules to replace attention in both transformers and gated units, achieving pretraining without attention. The latter configuration achieves performance similar to BERT. Katsch [40] introduces data-controlled input, output, and hidden state gates, arriving at unified and generalized formulation of linear recurrent models such as S4. Compared with vanilla S4, the proposed GateLoop improves the models expressiveness, leading to better autoregressive language modeling performance. Methods incorporating S4 into transformers commonly use S4 to provide global context while lowering the quadratic computational complexity of attention mechanisms, thus producing architectures practical for modeling long sequences. Zuo et al. [39] introduce the State Space Augmented Transformer (SPADE), which augments efficient transformers with global context. SPADEs bottommost global layer integrates the outputs of both an S4 module and local attention module. Fathi et al. [38] propose hybrid Block-State Transformer (BST) layer, which combines self-attention over input embeddings with cross-attention between inputs and context states. The context states, obtained from SSMs, allow the BST to maintain grasp of the full global context despite only attending to short subsequences."
        },
        {
            "title": "4 Selective State Space Models",
            "content": "In addition to incorporating structured parameters, another key optimization strategy for the SSM series involves the introduction of selectivity: allowing the model to dynamically prioritize specific segments of the input sequence. Mamba [2] is representative example of model centered on Selective SSM, and its introduction has garnered significant attention for the SSM series. Mamba demonstrates marked improvements in both model capability and task performance, driving its widespread adoption in real-world applications. In this chapter, Section 4.1 provides an overview of selective SSM. Section 4.2 elaborates on relevant selective mechanisms. Sections 4.3 and 4.4 introduce optimizations in computational efficiency for the selective SSM in Mamba and Mamba2 [18], respectively. Section 4.5 discusses the overall framework, core components, and variants of Mamba, while Section 4.6 presents analyses on selective SSM."
        },
        {
            "title": "4.1 Overview of Selective State Space Models",
            "content": "4.1 Overview of Selective State Space Models Gu and Dao [2] introduced selectivity mechanisms into SSM to enhance its capability in processing input sequences. The proposed selective SSM, or S6, adjusts model parameters in response to input sequences, enabling dynamic prioritization of specific input segments. In the selective SSM, parameters A, B, and become context-dependent variables that adjust based on input characteristics. We discuss this mechanism in detail in the next section. The selective SSM, combined with additional components including projection layers, activation functions, and one-dimensional convolutional layers, forms the foundation of the influential SSM series, which includes Mamba. The framework of Mamba will be detailed in Section 4.5. As the computational core of Mamba and its derivatives, selective SSM enhances data modeling capabilities and introduces selective attention mechanisms that enable token prioritization. 4.2 Effectiveness: Selectivity Mathematically, selectivity is implemented by dynamically deriving the parameters A, B, and (denoted as Ak, Bk, and Ck in the selective SSM) from the input. Specifically, for the data xk of the k-th input token, xk is processed through linear network to generate parameters k, Bk, and Ck. These are subsequently transformed using the zero-order hold (ZOH) discretization method, yielding Ak = exp(kA) and Bk = (kA)1(exp(kA) I)kBk. Through this approach, parameters Ak, Bk, and Ck become input-dependent. The computational formula for the selective SSM is as follows: hk = Akhk1 + Bkxk, yk = Ckhk. (25) (26) By correlating model parameters with the input, the selective SSM has the ability to assign higher parameter weights to key inputs. It thereby prioritizes critical information and filters out irrelevant noisy tokens within the input sequence. However, this input-dependent parameterization fundamentally alters the computational constraints discussed in Section 3.3. In particular, it invalidates the preconditions for the efficient computation of algorithms. Therefore, two recent approaches have been proposed to improve the efficiency of selective SSMs: the hardware-aware state expansion technique introduced in Mamba and the semiseparable matrices employed in Mamba2. These methods will be discussed in the following two sections. 4.3 Efficiency: Hardware-aware State Expansion Hardware-aware state expansion is strategy proposed in Mamba for the efficient computation of Selective SSM. Since model parameters A, and are influenced by the input, previous conditions for efficient computations no longer hold. Consequently, Mamba has to revert to the original recurrence method to handle computations associated with SSMs. It optimizes the algorithms related to the underlying hardware to achieve efficient computation. The optimization of mamba is related to the computational architecture of modern GPUs. On one hand, Mamba performs the discretization of SSM parameters and recurrence computation directly in the GPU SRAM (Static Random-Access Memory), rather than in the GPU HBM (High-Bandwidth Memory). SRAM has higher speed but smaller memory capacity, whereas HBM offers larger storage capacity but lower speeds. Mamba first loads A, B, C, and from the slow HBM to the fast SRAM. This involves O(BLD + DN ) bytes of memory, where B, L, D, and represent the batch size, sequence length, token embedding dimension, and hidden state dimension. Within the SRAM, it then discretizes and into and B, as described in Section 4.2. Next, parallel associative scan in the SRAM computes the hidden state and ultimately produces the output RBLD. Finally, the output is written back to the HBM. This algorithm reduces the complexity of IOs from O(BLDN ) to O(BLD), resulting in 20-40 time speedup in practice. Notably, when the input sequence is too long to fit entirely into the SRAM, Mamba splits the sequence and applies the algorithm to each segment, using the hidden state to connect the computations of individual segments. On the other hand, Mamba also employs classical recomputation techniques to reduce memory consumption. During the forward pass, Mamba discards the hidden states of size (B, L, D, ) and recomputes them during the backward pass, conserving memory required for storing hidden states. By computing hidden states directly in the SRAM rather than reading them from the HBM, this approach also reduces IOs during the backward pass."
        },
        {
            "title": "4.4 Efficiency: Semiseparable Matrices",
            "content": "4.4 Efficiency: Semiseparable Matrices Mamba2 improves computational efficiency by refining the optimization of the selective SSM formula. It first reformulates the SSM equation into the form = Mx and then leverages the properties of as semiseparable matrix to further streamline computations. This improvement enables Mamba2 to exploit the parallelism and computational optimizations of the Transformer architecture and underlying hardware. Unlike the iterative computations in Mamba, Mamba2 reformulates the SSM computation (Eq. 25) into the form = Mx. Specifically, when setting h0 = B0x0, iterating Equation 25 can yield hk = Akhk1 + Bkxk = Ak . . . A1B0x0 + Ak . . . A2B1x1 + + AkAk1Bk2xk2 + AkBk1xk1 + Bkxk = (cid:88) s=0 k:sBsxs, where k:s := Ak . . . A(s+1), for = k, k:k := I. Substituting the above result into Equation 26, yk = Ckhk = (cid:88) s=0 CkA k:sBsxs (27) (28) can be generated. Equivalently, the relationship between overall output and input can be expressed as = Mx, (29) j:iBi. where Ck R1N and Mji = CjA is the semiseparable matrix, which can be decomposed into several submatrices. To further optimize computation, Mamba2 uses the blocking and decomposition of matrix M. Specifically, for large matrix RN , an appropriate value n1 can be chosen such that mod n1 = 0. This allows to be decomposed into several submatrices Mi Rn1n1. Equation 30 illustrates this approach for = 4 and n1 = 2. For diagonal submatrices, given that their parameter structure aligns with that of but on smaller scale, the same method can be applied iteratively for further decomposition. For off-diagonal submatrices, Mamba2 decomposes them into three parts as shown in Equation 30. This enables efficient matrix multiplication with using the corresponding algorithm and also allows parallel computation. = = C0A C1A C2A C3A 0:0B0 1:0B0 C1A 2:0B0 C2A 3:0B0 C3A 1:1B1 2:1B1 C2A 3:1B1 C3A 2:2B2 3:2B2 C3A 3:3B3 C0A (cid:34) C1A 2:1 3:1 C2A C3A 0:0B0 1:0B0 (cid:35) 1:1 C1A 1:1B1 (cid:104) 1:0B0 1:1B1 (cid:105) C2A C3A 2:2B2 3:2B2 C3A 3:3B3 (30) These two improvements reduce computational complexity and, more importantly, align the underlying computational logic of selective SSM with that of the Transformer architecture. This allows Mamba2 to benefit from efficient hardware optimizations and parallelization for large-scale training. 4.5 Mamba Architecture and Its Variants With its intrinsic selectivity and computational efficiency optimizations, the selective SSM offers both effectiveness and efficiency in language-related tasks. Leveraging selective SSM as its core component, Mamba demonstrates robust model capabilities. This section provides comprehensive overview of Mambas architectural framework and key mechanisms, followed by summary of its various variants."
        },
        {
            "title": "4.6 Analyses of Selective State Space Models",
            "content": "Similar to the Transformer, Mambas design includes elements such as mapping layers and activation functions. Its key distinction is the integration of the selective SSM module. Input data is processed in parallel through two separate pathways, which are then combined via activation or multiplication functions before being passed through final down-projection layer to produce the output. The first pathway consists of an input up-projection layer, one-dimensional convolutional layer, an activation function, and the SSM module. The second pathway includes only an input up-projection layer followed by an activation function. There are several hybrid architectures and variants of Mamba. Among these, Mamba-transformer models represent promising approach combining powerful pre-trained Transformers and efficient SSM status. Specifically, the O(n2) complexity of self-attention in Transformer models leads to high memory usage for long sequence modeling, while the SSM architecture design bypasses this issue. Jamba [54] proposed hybrid model that integrates vanilla Transformer layers, Mamba layers, and MoE layers, achieving context length of 256K tokens on single 80GB GPU. Xu et al. [115] employed Mamba to capture broad trends in long-range sequences and Local Window Transformer for finer details in short-range data. Furthermore, Wang et al. [116] distilled pre-trained transformers, such as Llama, into linear RNN by projecting self-attention weights. This allows powerful LLMs to serve as the initial parameters for SSM states. Besides integrating Mamba with Transformers, recent works have also optimized Mamba internally, adapting it to specific task requirements. Behrouz et al. [55] introduce weighted averaging mechanism to connect selective mixers, allowing layers to directly attend to early-stage features. Pioro et al. [56] and Anthony et al. [57] combine SSMs with MoEs to improve scalability in sequential modeling, achieving improvements in inference and training FLOPs. DenseSSM [58] injects shallow-layer states into deeper layers, thus preserving fine-grained details while maintaining training parallelizability and inference efficiency. HiSS [59] constructs temporal hierarchy by stacking structured SSMs for continuous sequential prediction, which significantly improves performance over state-of-the-art sequence models. Ahamed and Cheng [117] fine-tune structured SSMs specifically for tabular data. Additionally, Wang et al. [103] introduce MambaByte, model trained directly on byte sequences for language modeling. By incorporating speculative decoding, MambaByte achieved 2.6 inference speedup. 4.6 Analyses of Selective State Space Models This section describes the use of theoretical and practical tools to analyze the selective SSM and gain further insights. Analysis focuses on in-context learning (ICL), since the long-context memorization and generalization abilities that arise from the SSM structure significantly impacts results. Using tools from Rough Path Theory, Cirone et al. [118] demonstrate that the hidden state can capture non-linear interactions across timescales, i.e. low-dimensional projection of the inputs signature. This theory explains the superior accuracy and efficiency of selective SSMs. Park et al. [119] evaluate the ICL capabilities of SSMs, revealing that selective SSMs achieve comparable performance to Transformers on standard regression tasks and excel in sparse parity learning. Based on these findings, they propose hybrid architectures as promising approach to further enhance ICL performance. Jelassi et al. [120] demonstrate that 2-layer transformer can copy strings of exponential length, whereas SSMs are constrained by their fixed-size latent states. Empirically, in string copying tasks, Transformers outperformed SSMs in both efficiency and generalization. Additionally, Akyürek et al. [121] identify \"n-gram heads\" as the key factor enabling Transformers to outperform SSMs in in-context learning. They further demonstrate that incorporating these heads into selective SSMs improves performance in natural language modeling."
        },
        {
            "title": "5 Relationships with Other Model Architectures",
            "content": "One compelling aspect of the SSM is its deep connections with other model architectures. The original equations defining SSMs naturally correspond to the structure of recurrent neural networks (RNNs) [122]. With straightforward derivations, they can also align with convolutional neural networks (CNNs) [123]. Several works have been inspired by the combinations of SSMs, RNNs, and CNNs [124, 125, 126, 127]. Furthermore, as SSM models evolve, their formulations and practical implementations increasingly converge with Transformer models [1, 18]. Sections 5.1, 5.2 explore these relationships, focusing on the mathematical links and developmental trajectories of SSM in relation to RNN, CNN, and Transformer."
        },
        {
            "title": "5.1 SSM and RNN, CNN",
            "content": "5.1 SSM and RNN, CNN Mathematically, SSM has strong connections to RNN and CNN. Although the SSM series was initially introduced as an independent framework, its original equations align with the simplified structure of RNN. In the discrete-time SSM equations (Eq. 5,6), parameters and correspond to the hidden layer parameters of an RNN, and correspond to its output layer parameters, and hk represents the hidden states. In the time-invariant scenarios where SSM parameters remain invariant to input, these equations can be reformulated as convolutional operation (Eq. 18), which constitutes the core computation in CNN. This mathematical correspondence elucidates the strengths and weaknesses of SSMs. For example, while SSMs can reduce computational complexity, they may suffer from memory decay similar to RNNs. Such insights support theoretical analysis and architectural improvements. The mathematical flexibility of SSMs also opens up numerous optimization opportunities. prime example is the S4 model, which uses the convolutional equivalence of SSM to significantly enhance computational efficiency [11]. 5.2 SSM and Transformer Throughout their development, SSM models have converged toward the Transformer architecture. Mamba [2], Mamba2 [18], and subsequent research have focused on optimizing SSM formulations to take advantage of hardware acceleration and parallelism, similar to Transformers. For example, Mamba2 reformulates SSM computations into the matrix-vector multiplication format = Mx, aligning with efficient computational frameworks. Moreover, starting from S4, the design of SSM-based models has increasingly mirrored that of Transformers, with attention modules being replaced by SSM modules, along with other minor modifications [38, 39, 18]. There has also been growing interest in connections between SSM and attention mechanisms. Notably, the equivalence between SSM and linear attention has already been established [18]. Recent works have further proposed fusing the two architectures by directly integrating SSM-based model blocks into Transformers [54]. This trend is primarily driven by practical application demands. Early SSMs, such as S4, showcased computational efficiency and the ability to handle long sequences, but their real-world applicability was limited due to their poor compatibility with modern hardware and subpar performance on practical tasks. In contrast, Transformer models excel in both hardware utilization and task performance. The shift toward Transformerlike structures has improved the performance and applicability of SSM models, broadening their adoption in practical scenarios."
        },
        {
            "title": "6 Applications",
            "content": "After covering the theoretical foundations of the original SSM, S4, and Mamba, we introduce their applications in downstream tasks and data contexts. pivotal question arises before applying SSM-based models to practical tasks: What is the most significant characteristic of SSM-based models in downstream applications? First, state space models are highly efficient at modeling long-range dependencies, which enables them to excel in sequence data modeling. This includes text represented as token sequences, videos as numerical frames, and molecular sequences like DNA, RNA, and proteins, among other data categories. Second, enhanced sequence modeling capabilities significantly extend the receptive field of SSM-based models, facilitating their application in diverse image-related and specialized tasks. 6.1 Video Modeling SSMs enable efficient and scalable video modeling, improving performance across wide array of video understanding and generation tasks. Numerous recent works integrate SSM modulesoften through variants of the Mamba operatorinto different video architectures to better capture long-term dependencies while maintaining computational efficiency. For instance, Vivim [129] and VideoMamba [128] use SSM-inspired operations for medical video segmentation and general video action recognition, respectively. This balances broad temporal context with computational tractability. Similarly, SpikeMba [130] employs SSMs alongside spiking neural networks to refine temporal video grounding and reduce confidence biases. RainMamba [131] enhances video deraining by improving locality modeling through Hilbert scanning mechanism. Other lines of research, such as RhythmMamba [132] and Simba [133], use SSM blocks to efficiently model periodic physiological signals for remote photoplethysmography and to strengthen skeleton-based action recognition with structural priors and temporal reasoning. Mamba4D [134] integrates SSM-based modules into point"
        },
        {
            "title": "6.2 Speech and Audio Modeling",
            "content": "Application Video Modeling 6.1 Speech and Audio Modeling 6.2 Molecular Modeling 6.3 3D Signals Modeling 6.4 Time Series Modeling 6. Structured Data Modeling 6.6 Video Understanding Video Generation Speech Enhancement Speech Representation Biology and Chemistry 3D Representation Point Cloud General Time Series VideoMamba [128]; Vivim [129]; Spikemba [130]; Rainmamba [131]; Rhythmmanba [132]; Simba [133]; Mamba4D [134] Matten [135]; DeMamba [136]; DiM [137] SEMamba [138]; SpatialNet [139]; Spmamba [140]; Dual-path Mamba [141] Audio Mamba [142]; MambaSpeech [143]; Ssamba [144] Hyenadna [145];Caduceus [146];ChemMamba [147]; Smiles-mamba [148];Biomamba [149] Gamba [150];Segmamba [151];nmamba [152];CMViM [153] Pointmamba [154];Mamba3d [155]; 3dmambaipf [156] Timemachine [157];TSFMamba [158];MAT;Bio-Mamba4TS [159] Spatiotemporal Vmrnn [160];Simba [161] Graph and Table Graph Mamba [162]; stg-mamba [163]; Mambatab [102] Figure 4: Applications of State Space Models Across Diverse Data Types cloud video analysis for effective long-term spatiotemporal modeling in complex 4D data. For generative tasks, Matten [135] and Diffusion Mamba [137] adopt SSM operators to achieve efficient, scalable video synthesis and latent diffusion without the quadratic complexity of self-attention. DeMamba [136] applies similar principle for detecting AI-generated videos at scale. These studies highlight the versatility and effectiveness of SSM frameworks for video understanding and generation. 6.2 Speech and Audio Modeling In diverse speech and audio processing tasks, SSMs, notably Mamba and its variants, have demonstrated increasing potential. In speech enhancement, Mamba-based networks effectively capture long-term temporal dependencies and show robust performance in both static and moving speaker scenarios. representative example is the streaming SpatialNet variant, where Mamba replaces self-attention modules to achieve linear-time inference and strong generalization on extended audio streams [139]. Similarly, Audio Mamba (AuM) [142] demonstrates that fully SSM-based architecture can match or exceed the performance of established Audio Spectrogram Transformers without relying on self-attention. Beyond enhancement and classification, Mamba offers scalable alternative to Transformers for speech recognition, as shown by the improved semantic-aware modeling of Bidirectional Mamba (BiMamba) compared to its vanilla counterpart [143]. In self-supervised learning, Ssamba [144] uses Mamba to capture rich contextual information in unlabeled audio, with better efficiency and accuracy than self-attention-based models. For speech enhancement tasks specifically, SEMamba [138] incorporates Mambas efficient state-space formulation into regression-based architectures, achieving state-of-the-art perceptual evaluation scores. In speech separation, both SPMamba [140] and Dual-path Mamba [141] exploit the linear complexity and bidirectionality of SSMs to surpass conventional recurrent or attention-based systems in handling long audio sequences, ultimately improving separation quality and computational efficiency. Evidently, Mamba-based SSMs are scalable, effective, and memory-efficient alternatives to Transformer-driven architectures for speech and audio applications."
        },
        {
            "title": "6.3 Molecular Modeling",
            "content": "6.3 Molecular Modeling SSMs are also contributing to molecular modeling tasks within biology and chemistry. In genomics, longrange modeling capabilities are essential for understanding the context-dependent functions of regulatory elements within DNA sequences. Caduceus [146] integrates bi-directional Mamba component (BiMamba) and reverse complement (RC) equivariance to improve variant effect prediction, outperforming larger nonequivariant models. Likewise, HyenaDNA [145] implements long-range sequence modeling without attention, enabling single nucleotide-level resolution across up to one million tokens and achieving state-of-the-art performance on range of genomic benchmarks. Beyond genomics, SSM-based architectures show promise in chemical informatics. By pre-training Mamba-based models on massive SMILES corpora, researchers have developed efficient and scalable chemical foundation models that deliver state-of-the-art results in property prediction, classification, and synthesis yield prediction [147]. SMILES-Mamba [148] employs self-supervised pretraining followed by fine-tuning to substantially enhance drug ADMET prediction accuracy while mitigating the reliance on large labeled datasets. Finally, these successes extend to biomedical text representation. BioMamba [149] efficiently parses and interprets complex biomedical literature, surpassing the performance of domain-specific Transformers. Collectively, these studies show how the advantages of SSMs in capturing long-range dependencies, contextual nuances, and structural patterns support crucial improvements in molecular modeling. 6.4 3D Signals Modeling We next discuss the applications of SSMs in modeling complex 3D signals, spanning tasks from single-view reconstruction and medical imaging to point cloud analysis. For 3D asset generation, Gamba [150] relies on Mamba-based sequential prediction backbone and Gaussian Splatting. It achieves end-to-end single-image 3D reconstruction with millisecond-level speed, significantly outperforming optimization-based counterparts. In medical applications, SegMamba [151] demonstrates that Mamba-based architectures can efficiently handle large volumetric data and model long-range dependencies for 3D medical image segmentation. Similarly, nnMamba [152] merges CNNs with SSMs for 3D segmentation, classification, and landmark detection, achieving state-of-the-art performance across biomedical imaging benchmarks. Complementing these efforts, CMViM [153] integrates Vision Mamba (Vim) into masked autoencoding framework for 3D multi-modal data representation, enabling more accurate Alzheimers disease classification. Other than voxelized representations, SSM-based models such as PointMamba [154] and Mamba3D [155] facilitate point cloud tasks through their linear-complexity global modeling ability. PointMamba significantly reduces computational costs while maintaining effective global feature extraction, whereas Mamba3D enhances local geometric and global contextual modeling, surpassing Transformer-based methods on standard benchmarks. Furthermore, 3DMambaIPF [156] employs selective SSMs for large-scale point cloud denoising and incorporates differentiable rendering to refine geometric accuracy on massive datasets. It can be seen that SSM-based architectures can efficiently capture long-range dependencies and complex structures across wide range of 3D signals and domains. 6.5 Time Series Modeling For time series modeling, SSMs have similarly emerged as compelling approach to challenges such as long-range modeling and efficiency. Several works have demonstrated the versatility of Mamba-based SSMs across time series forecasting tasks. For instance, by integrating Vision Mamba blocks into recurrent architectures, VMRNN [160] combines the strengths of LSTMs with the linear complexity of Mamba for improved spatiotemporal prediction. Moving toward longer-term prediction, TimeMachine [157] leverages multiple Mamba modules to effectively model both channel-mixing and channel-independent patterns, improving the scalability and memory efficiency of long-range forecasting. S-Mamba [158] shows that, compared to Transformers, Mamba can reduce complexity while retaining superior predictive performance, which benefits real-world deployment. Beyond these single-model paradigms, SiMBA [161] merges Mamba with spectral techniques (EinFFT) for channel modeling, narrowing the gap between SSMs and Transformers on both vision and multivariate time series benchmarks. Hybrid solutions have also surfaced: MAT [164] integrates Mamba and Transformer components to capture both longand short-range dependencies, improving accuracy and resource utilization on weather prediction tasks. Bi-Mamba+ [159] introduces gating mechanism and bidirectional processing to refine Mambas contextual understanding. It represents more flexible, data-driven approach that adapts to diverse time series structures."
        },
        {
            "title": "6.6 Structured Data Modeling",
            "content": "6.6 Structured Data Modeling Finally, we cover the applications of SSMs in structured data modeling. In graph representation learning, Graph Mamba Networks (GMNs) [162] use selective SSMs to accurately and efficiently capture long-range dependencies in graphs. This approach alleviates over-squashing and maintains strong performance on smalland large-scale benchmarks without complex positional encodings. Similarly, STG-Mamba [163] extends the selective SSM paradigm to spatial-temporal graph (STG) data, addressing the intricate dynamic evolution and heterogeneity in such systems. It integrates Spatial-Temporal Selective State Space Module (ST-S3M) and Kalman Filtering-inspired GNN layers to achieve state-of-the-art forecasting results at reduced computational costs. In the tabular domain, MambaTab [102] adapts Mamba-based architectures through lightweight plug-and-play approach that bypasses extensive preprocessing steps and parameter tuning. Using fewer parameters, MambaTab outperforms established baselines on range of tabular datasets. The above works on structured data modeling span graphs, time-evolving networks, and tabular structures, showcasing the adaptability of SSM-based models."
        },
        {
            "title": "7 Conclusion",
            "content": "This survey provides an overview of the theoretical motivations, evolutionary trajectory, comparisons with existing models, and real-world applications of the State Space Models (SSMs) series. The SSM series has evolved from its initial conception into advanced architectures such as S4 and Mamba, progressing through stages from the original SSM to structured SSM and selective SSM. This transformation has been driven by techniques aimed at enhancing model effectiveness and computational efficiency. These techniques form the core focus of this survey. SSMs have found extensive applications in domains that require sequence understanding and prediction, such as natural language processing, video comprehension, and time series analysis. With the integration of increasingly sophisticated techniques to improve both effectiveness and efficiency, SSMs are expected to play significant role in tackling more complex challenges."
        },
        {
            "title": "References",
            "content": "[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008, 2017. [2] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [3] Ziyang Wang, Jian-Qing Zheng, Yichi Zhang, Ge Cui, and Lei Li. Mamba-unet: Unet-like pure visual mamba for medical image segmentation, 2024. [4] Yijun Yang, Zhaohu Xing, Lequan Yu, Chunwang Huang, Huazhu Fu, and Lei Zhu. Vivim: video vision mamba for medical video segmentation, 2024. [5] Tao Guo, Yinuo Wang, Shihao Shu, Diansheng Chen, Zhouping Tang, Cai Meng, and Xiangzhi Bai. Mambamorph: mamba-based framework for medical mr-ct deformable registration, 2024. [6] Yixuan Li, Weidong Yang, and Ben Fei. 3dmambacomplete: Exploring structured state space model for point cloud completion, 2024. [7] Jun Ma, Feifei Li, and Bo Wang. U-mamba: Enhancing long-range dependency for biomedical image segmentation, 2024. [8] Zhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, and Lei Zhu. Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation, 2024. [9] Ahmed El-Gazzar, Rajat Mani Thomas, and Guido Van Wingen. fmri-s4: learning shortand long-range dynamic fmri dependencies using 1d convolutions and state space models, 2022. [10] Yu Du, Xu Liu, and Yansong Chua. Spiking structured state space model for monaural speech enhancement, 2024. [11] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [12] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022. [13] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré. On the parameterization and initialization of diagonal state space models, 2022. [14] Leonhard Euler. Institutiones calculi integralis, volume 4. Academia Imperialis Scientiarum, 1794. [15] Arnold Tustin. method of analysing the behaviour of linear systems in terms of time series. Journal of the Institution of Electrical Engineers-Part IIA: Automatic Regulators and Servo Mechanisms, 94(1):130142, 1947. [16] Khalid Hosny. Refined translation and scale legendre moment invariants. Pattern Recognition Letters, 31(7):533538, 2010. [17] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33:14741487, 2020. [18] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. [19] Athanasios Antoulas and BDQ Anderson. On the scalar rational interpolation problem. IMA Journal of Mathematical Control and Information, 3(2-3):6188, 1986. [20] Cooley, Lewis, and Welch. The finite fourier transform. IEEE Transactions on audio and electroacoustics, 17(2):7785, 1969. [21] Israel Gohberg, Thomas Kailath, and Israel Koltracht. Linear complexity algorithms for semiseparable matrices. Integral Equations and Operator Theory, 8(6):780804, 1985. [22] Max Woodbury. The stability of out-input matrices. Chicago, IL, 9:38, 1949. [23] Augustin Louis Cauchy. Exercices danalyse et de physique mathematique: 1, volume 1. Bachelier, imprimeur-libraire, 1840."
        },
        {
            "title": "REFERENCES",
            "content": "[24] Syama Sundar Rangapuram, Matthias Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski. Deep state space models for time series forecasting. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. [25] Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, and Patrick Gallinari. Locost: State-space models for long document abstractive summarization. arXiv preprint arXiv:2401.17919, 2024. [26] Ethan Baron, Itamar Zimerman, and Lior Wolf. 2-dimensional state space layer for spatial inductive bias. In The Twelfth International Conference on Learning Representations, 2024. [27] Naman Agarwal, Daniel Suo, Xinyi Chen, and Elad Hazan. Spectral state space models, 2024. [28] Michael Zhang, Khaled Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher Ré. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. [29] Elise Zhang, Di Wu, and Benoit Boulet. On the performance of legendre state-space models in short-term time series forecasting. In 2023 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE), pages 511, 2023. [30] Jadie Adams and Shireen Elhabian. Point2SSM: Learning morphological variations of anatomies from point clouds. In The Twelfth International Conference on Learning Representations, 2024. [31] Abdul Fatir Ansari, Alvin Heng, Andre Lim, and Harold Soh. Neural continuous-discrete state space models for irregularly-sampled time series, 2023. [32] Eric Nguyen, Karan Goel, Albert Gu, Gordon W. Downs, Preey Shah, Tri Dao, Stephen A. Baccus, and Christopher Ré. S4nd: Modeling images and videos as multidimensional signals using state spaces, 2022. [33] Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for sequence modeling, 2023. [34] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [35] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M. Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2023. [36] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022. [37] Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, and ChengXiang Zhai. Sparse modular activation for efficient sequence modeling. arXiv preprint arXiv:2306.11197, 2023. [38] Mahan Fathi, Jonathan Pilault, Orhan Firat, Christopher Pal, Pierre-Luc Bacon, and Ross Goroshin. Block-state transformers. arXiv preprint arXiv:2306.09539, 2023. [39] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. Efficient long sequence modeling via state space augmented transformer. arXiv preprint arXiv:2212.08136, 2022. [40] Tobias Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling. arXiv preprint arXiv:2311.01927, 2024. [41] Shmuel Bar-David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision s4: Efficient sequencebased rl via state spaces layers, 2023. [42] Jimmy T. H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, and Wonmin Byeon. Convolutional state space models for long-range spatiotemporal modeling, 2023. [43] Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon. Deep latent state space models for time-series generation, 2023. [44] Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. Its raw! audio generation with state-space models, 2022. [45] Md Mohaiminul Islam, Mahmudul Hasan, Kishan Shamsundar Athrey, Tony Braskich, and Gedas Bertasius. Efficient movie scene detection using state-space transformers, 2023. [46] George Saon, Ankit Gupta, and Xiaodong Cui. Diagonal state space augmented transformers for speech recognition, 2023."
        },
        {
            "title": "REFERENCES",
            "content": "[47] Chen Chen, Chao-Han Huck Yang, Kai Li, Yuchen Hu, Pin-Jui Ku, and Eng Siong Chng. neural state-space model approach to efficient speech separation, 2023. [48] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models, 2023. [49] Jing Nathan Yan, Jiatao Gu, and Alexander M. Rush. Diffusion models without attention, 2023. [50] Paul Mattes, Rainer Schlosser, and Ralf Herbrich. Hieros: Hierarchical imagination on structured state space sequence world models, 2024. [51] Mohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, and Sarath Chandar. Mastering memory tasks with world models, 2024. [52] Siyi Tang, Jared A. Dunnmon, Liangqiong Qu, Khaled K. Saab, Tina Baykaner, Christopher Lee-Messer, and Daniel L. Rubin. Modeling multivariate biosignals with graph neural networks and structured state space models, 2023. [53] Ahmed El Gazzar, Rajat Mani Thomas, and Guido Van Wingen. Improving the diagnosis of psychiatric disorders with self-supervised graph state space models, 2022. [54] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. [55] Ali Behrouz, Michele Santacatterina, and Ramin Zabih. Mambamixer: Efficient selective state space models with dual token and channel selection. arXiv preprint arXiv:2403.19888, 2024. [56] Maciej Pioro, Kamil Ciebiera, Krystian Krol, Jan Ludziejewski, and Sebastian Jaszczur. Moe-mamba: Efficient selective state space models with mixture of experts. ArXiv, abs/2401.04081, 2024. [57] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. Blackmamba: Mixture of experts for state-space models. ArXiv, abs/2402.01771, 2024. [58] Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, and Yunhe Wang. Densemamba: State space models with dense hidden connection for efficient large language models. ArXiv, abs/2403.00818, 2024. [59] Raunaq M. Bhirangi, Chenyu Wang, Venkatesh Pattabiraman, Carmel Majidi, Abhinav Gupta, Tess Lee Hellebrekers, and Lerrel Pinto. Hierarchical state space models for continuous sequence-to-sequence modeling. ArXiv, abs/2402.10211, 2024. [60] Badri N. Patro and Vijay S. Agneeswaran. Simba: Simplified mamba-based architecture for vision and multivariate time series, 2024. [61] Chengkai Liu, Jianghao Lin, Jianling Wang, Hanzhou Liu, and James Caverlee. Mamba4rec: Towards efficient sequential recommendation with selective state space models, 2024. [62] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? comparative study on in-context learning tasks, 2024. [63] Danlong Yuan, Jiahao Liu, Bei Li, Huishuai Zhang, Jingang Wang, Xunliang Cai, and Dongyan Zhao. Remamba: Equip mamba with effective long-sequence modeling, 2024. [64] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling, 2024. [65] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model, 2024. [66] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba: Visual state space model, 2024. [67] Chloe Wang, Oleksii Tsepa, Jun Ma, and Bo Wang. Graph-mamba: Towards long-range graph sequence modeling with selective state spaces, 2024. [68] Shufan Li, Harkanwar Singh, and Aditya Grover. Mamba-nd: Selective state space modeling for multi-dimensional data, 2024. [69] Qiuhong Shen, Zike Wu, Xuanyu Yi, Pan Zhou, Hanwang Zhang, Shuicheng Yan, and Xinchao Wang. Gamba: Marry gaussian splatting with mamba for single view 3d reconstruction, 2024."
        },
        {
            "title": "REFERENCES",
            "content": "[70] Xuanhua He, Ke Cao, Keyu Yan, Rui Li, Chengjun Xie, Jie Zhang, and Man Zhou. Pan-mamba: Effective pan-sharpening with state space model, 2024. [71] Hang Guo, Jinmin Li, Tao Dai, Zhihao Ouyang, Xudong Ren, and Shu-Tao Xia. Mambair: simple baseline for image restoration with state-space model, 2024. [72] Yanyuan Qiao, Zheng Yu, Longteng Guo, Sihan Chen, Zijia Zhao, Mingzhen Sun, Qi Wu, and Jing Liu. Vl-mamba: Exploring state space models for multimodal learning, 2024. [73] Wenrui Li, Xiaopeng Hong, Ruiqin Xiong, and Xiaopeng Fan. Spikemba: Multi-modal spiking saliency mamba for temporal video grounding, 2024. [74] Guo Chen, Yifei Huang, Jilan Xu, Baoqi Pei, Zhe Chen, Zhiqi Li, Jiahao Wang, Kunchang Li, Tong Lu, and Limin Wang. Video mamba suite: State space model as versatile alternative for video understanding, 2024. [75] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Schusterbauer, and Björn Ommer. Zigma: dit-style zigzag mamba diffusion model, 2024. [76] Tao Huang, Xiaohuan Pei, Shan You, Fei Wang, Chen Qian, and Chang Xu. Localmamba: Visual state space model with windowed selective scan, 2024. [77] Chenhongyi Yang, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu Wang, Jiaming Liu, and Elliot J. Crowley. Plainmamba: Improving non-hierarchical mamba in visual recognition, 2024. [78] Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, and Donglin Wang. Cobra: Extending mamba to multi-modal large language model for efficient inference, 2024. [79] Zifu Wan, Pingping Zhang, Yuhao Wang, Silong Yong, Simon Stepputtis, Katia Sycara, and Yaqi Xie. Sigma: Siamese mamba network for multi-modal semantic segmentation, 2024. [80] Wenhao Dong, Haodong Zhu, Shaohui Lin, Xiaoyan Luo, Yunhang Shen, Xuhui Liu, Juan Zhang, Guodong Guo, and Baochang Zhang. Fusion-mamba for cross-modality object detection, 2024. [81] Jiacheng Ruan, Jincheng Li, and Suncheng Xiang. Vm-unet: Vision mamba unet for medical image segmentation, 2024. [82] Jiarun Liu, Hao Yang, Hong-Yu Zhou, Yan Xi, Lequan Yu, Yizhou Yu, Yong Liang, Guangming Shi, Shaoting Zhang, Hairong Zheng, and Shanshan Wang. Swin-umamba: Mamba-based unet with imagenet-based pretraining, 2024. [83] Haifan Gong, Luoyao Kang, Yitao Wang, Xiang Wan, and Haofeng Li. nnmamba: 3d biomedical image segmentation, classification and landmark detection with state space model, 2024. [84] Zhuoran Zheng and Jun Zhang. Fd-vision mamba for endoscopic exposure correction, 2024. [85] Chao Ma and Ziyang Wang. Semi-mamba-unet: Pixel-level contrastive and pixel-level cross-supervised visual mamba-based unet for semi-supervised medical image segmentation, 2024. [86] Zi Ye, Tianxiang Chen, Fangyijie Wang, Hanwei Zhang, and Lijun Zhang. P-mamba: Marrying perona malik diffusion with mamba for efficient pediatric echocardiographic left ventricular segmentation, 2024. [87] Jiahao Huang, Liutao Yang, Fanwen Wang, Yang Nan, Angelica I. Aviles-Rivero, Carola-Bibiane Schönlieb, Daoqiang Zhang, and Guang Yang. Mambamir: An arbitrary-masked mamba for joint medical image reconstruction and uncertainty estimation, 2024. [88] Ziyang Wang and Chao Ma. Weak-mamba-unet: Visual mamba makes cnn and vit work better for scribble-based medical image segmentation, 2024. [89] Yuelin Zhang, Kim Yan, Chun Ping Lam, Chengyu Fang, Wenxuan Xie, Yufu Qiu, Raymond Shing-Yan Tang, and Shing Shin Cheng. Motion-guided dual-camera tracker for endoscope tracking and motion analysis in mechanical gastric simulator, 2024. [90] Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. Caduceus: Bi-directional equivariant long-range dna sequence modeling, 2024. [91] Shu Yang, Yihui Wang, and Hao Chen. Mambamil: Enhancing long sequence modeling with sequence reordering in computational pathology, 2024. [92] Guangqian Yang, Kangrui Du, Zhihan Yang, Ye Du, Yongping Zheng, and Shujun Wang. Cmvim: Contrastive masked vim autoencoder for 3d multi-modal representation learning for ad classification, 2024."
        },
        {
            "title": "REFERENCES",
            "content": "[93] Jing Hao, Yonghui Zhu, Lei He, Moyun Liu, James Kit Hon Tsoi, and Kuo Feng Hung. T-mamba: unified framework with long-range dependency in dual-domain for 2d & 3d tooth segmentation, 2024. [94] Jianhao Xie, Ruofan Liao, Ziang Zhang, Sida Yi, Yuesheng Zhu, and Guibo Luo. Promamba: Promptmamba for polyp segmentation, 2024. [95] Dingkang Liang, Xin Zhou, Wei Xu, Xingkui Zhu, Zhikang Zou, Xiaoqing Ye, Xiao Tan, and Xiang Bai. Pointmamba: simple state space model for point cloud analysis, 2024. [96] Tao Zhang, Haobo Yuan, Lu Qi, Jiangning Zhang, Qianyu Zhou, Shunping Ji, Shuicheng Yan, and Xiangtai Li. Point cloud mamba: Point cloud learning via state space model, 2024. [97] Chi-Sheng Chen, Guan-Ying Chen, Dong Zhou, Di Jiang, and Dai-Shi Chen. Res-vmamba: Finegrained food category visual classification using selective state space models with deep residual learning, 2024. [98] Chenyu Dong, Chen Zhao, Weiling Cai, and Bo Yang. O-mamba: O-shape state-space model for underwater image enhancement, 2024. [99] Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang. Motion mamba: Efficient and long sequence motion generation, 2024. [100] Lincan Li, Hanchen Wang, Wenjie Zhang, and Adelle Coster. Stg-mamba: Spatial-temporal graph learning via selective state space model, 2024. [101] Xianping Ma, Xiaokang Zhang, and Man-On Pun. Rs3mamba: Visual state space model for remote sensing images semantic segmentation, 2024. [102] Md Atik Ahamed and Qiang Cheng. Mambatab: plug-and-play model for learning tabular data. In 2024 IEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR), pages 369375. IEEE, 2024. [103] Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, and Alexander M. Rush. Mambabyte: Token-free selective state space model. ArXiv, abs/2401.13660, 2024. [104] Zhuangwei Shi. Mambastock: Selective state space model for stock prediction, 2024. [105] Shuangjian Li, Tao Zhu, Furong Duan, Liming Chen, Huansheng Ning, Christopher Nugent, and Yaping Wan. Harmamba: Efficient and lightweight wearable sensor human activity recognition based on bidirectional mamba, 2024. [106] Bochao Zou, Zizheng Guo, Xiaocheng Hu, and Huimin Ma. Rhythmmamba: Fast remote physiological measurement with arbitrary length videos, 2024. [107] Alexander Findlay. The phase rule and its applications. Longmans, Green, 1904. [108] Zdzislaw Bubnicki et al. Modern control theory, volume 2005925392. Springer, 2005. [109] Rudolph Emil Kalman. new approach to linear filtering and prediction problems. 1960. [110] Albert Gu. Modeling Sequences with Structured State Spaces. Stanford University, 2023. [111] Ethan Baron, Itamar Zimerman, and Lior Wolf. 2-d ssm: general spatial layer for visual transformers. arXiv preprint arXiv:2306.06635, 2023. [112] Donald Givone and Robert Roesser. Multidimensional linear iterative circuitsgeneral properties. IEEE Transactions on Computers, 100(10):10671073, 1972. [113] Aaron Voelker, Ivana Kajic, and Chris Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks. Advances in neural information processing systems, 32, 2019. [114] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: benchmark for efficient transformers. arXiv preprint arXiv:2011.04006, 2020. [115] Xiongxiao Xu, Canyu Chen, Yueqing Liang, Baixiang Huang, Zhiling Lan, and Kai Shu. Sst: Multiscale hybrid mamba-transformer experts for long-short range time series forecasting. 2024. [116] Junxiong Wang, Daniele Paliotta, Avner May, Alexander Rush, and Tri Dao. The mamba in the llama: Distilling and accelerating hybrid models. arXiv preprint arXiv:2408.15237, 2024. [117] Md. Atik Ahamed and Qiang Cheng. Mambatab: plug-and-play model for learning tabular data. 2024."
        },
        {
            "title": "REFERENCES",
            "content": "[118] Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, Cristopher Salvi, and Terry Lyons. Theoretical foundations of deep selective state-space models. ArXiv, abs/2402.19047, 2024. [119] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? comparative study on in-context learning tasks. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 3979339812. PMLR, 2127 Jul 2024. [120] Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying. ArXiv, abs/2402.01032, 2024. [121] Ekin Akyürek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context language learning: Architectures and algorithms. ArXiv, abs/2401.12973, 2024. [122] Jeffrey Elman. Finding structure in time. Cognitive science, 14(2):179211, 1990. [123] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998. [124] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024. [125] Aviv Bick, Kevin Li, Eric Xing, Zico Kolter, and Albert Gu. Transformers to ssms: Distilling quadratic knowledge to subquadratic models. arXiv preprint arXiv:2408.10189, 2024. [126] Sukjun Hwang, Aakash Lahoti, Tri Dao, and Albert Gu. Hydra: Bidirectional state space models through generalized matrix mixers. arXiv preprint arXiv:2407.09941, 2024. [127] Aviv Bick, Tobias Katsch, Nimit Sohoni, Arjun Desai, and Albert Gu. Llamba: Scaling distilled recurrent models for efficient language processing. arXiv preprint arXiv:2502.14458, 2025. [128] Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao. Videomamba: State space model for efficient video understanding. In European Conference on Computer Vision, pages 237255. Springer, 2025. [129] Yijun Yang, Zhaohu Xing, and Lei Zhu. Vivim: video vision mamba for medical video object segmentation. arXiv preprint arXiv:2401.14168, 2024. [130] Wenrui Li, Xiaopeng Hong, Ruiqin Xiong, and Xiaopeng Fan. Spikemba: Multi-modal spiking saliency mamba for temporal video grounding. arXiv preprint arXiv:2404.01174, 2024. [131] Hongtao Wu, Yijun Yang, Huihui Xu, Weiming Wang, Jinni Zhou, and Lei Zhu. Rainmamba: Enhanced In Proceedings of the 32nd ACM locality learning with state space models for video deraining. International Conference on Multimedia, pages 78817890, 2024. [132] Bochao Zou, Zizheng Guo, Xiaocheng Hu, and Huimin Ma. Rhythmmamba: Fast remote physiological measurement with arbitrary length videos. arXiv preprint arXiv:2404.06483, 2024. [133] Soumyabrata Chaudhuri and Saumik Bhattacharya. Simba: Mamba augmented u-shiftgcn for skeletal action recognition in videos. arXiv preprint arXiv:2404.07645, 2024. [134] Jiuming Liu, Jinru Han, Lihao Liu, Angelica Aviles-Rivero, Chaokang Jiang, Zhe Liu, and Hesheng Wang. Mamba4d: Efficient long-sequence point cloud video understanding with disentangled spatialtemporal state space models. arXiv preprint arXiv:2405.14338, 2024. [135] Yu Gao, Jiancheng Huang, Xiaopeng Sun, Zequn Jie, Yujie Zhong, and Lin Ma. Matten: Video generation with mamba-attention. arXiv preprint arXiv:2405.03025, 2024. [136] Haoxing Chen, Yan Hong, Zizheng Huang, Zhuoer Xu, Zhangxuan Gu, Yaohui Li, Jun Lan, Huijia Zhu, Jianfu Zhang, Weiqiang Wang, et al. Demamba: Ai-generated video detection on million-scale genvideo benchmark. arXiv preprint arXiv:2405.19707, 2024. [137] Shentong Mo and Yapeng Tian. Scaling diffusion mamba with bidirectional ssms for efficient image and video generation. arXiv preprint arXiv:2405.15881, 2024. [138] Rong Chao, Wen-Huang Cheng, Moreno La Quatra, Sabato Marco Siniscalchi, Chao-Han Huck Yang, Szu-Wei Fu, and Yu Tsao. An investigation of incorporating mamba for speech enhancement. arXiv preprint arXiv:2405.06573, 2024."
        },
        {
            "title": "REFERENCES",
            "content": "[139] Changsheng Quan and Xiaofei Li. Multichannel long-term streaming neural speech enhancement for static and moving speakers. arXiv preprint arXiv:2403.07675, 2024. [140] Kai Li, Guo Chen, Runxuan Yang, and Xiaolin Hu. Spmamba: State-space model is all you need in speech separation. arXiv preprint arXiv:2404.02063, 2024. [141] Xilin Jiang, Cong Han, and Nima Mesgarani. Dual-path mamba: Short and long-term bidirectional selective structured state space models for speech separation. arXiv preprint arXiv:2403.18257, 2024. [142] Mehmet Hamza Erol, Arda Senocak, Jiu Feng, and Joon Son Chung. Audio mamba: Bidirectional state space model for audio representation learning. arXiv preprint arXiv:2406.03344, 2024. [143] Xiangyu Zhang, Qiquan Zhang, Hexin Liu, Tianyi Xiao, Xinyuan Qian, Beena Ahmed, Eliathamby Ambikairajah, Haizhou Li, and Julien Epps. Mamba in speech: Towards an alternative to self-attention. arXiv preprint arXiv:2405.12609, 2024. [144] Siavash Shams, Sukru Samet Dindar, Xilin Jiang, and Nima Mesgarani. Ssamba: Self-supervised audio representation learning with mamba state space model. arXiv preprint arXiv:2405.11831, 2024. [145] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Michael Wornow, Callum Birch-Sykes, Stefano Massaroli, Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution. Advances in neural information processing systems, 36, 2024. [146] Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. Caduceus: Bi-directional equivariant long-range dna sequence modeling. arXiv preprint arXiv:2403.03234, 2024. [147] Emilio Vital Brazil, Eduardo Soares, Victor Yukio Shirasuna, Renato Cerqueira, Dmitry Zubarev, and Kristin Schmidt. mamba-based foundation model for chemistry. In AI for Accelerated Materials Design-NeurIPS 2024, 2024. [148] Bohao Xu, Yingzhou Lu, Chenhao Li, Ling Yue, Xiao Wang, Nan Hao, Tianfan Fu, and Jim Chen. Smiles-mamba: Chemical mamba foundation models for drug admet prediction. arXiv preprint arXiv:2408.05696, 2024. [149] Ling Yue, Sixue Xing, Yingzhou Lu, and Tianfan Fu. Biomamba: pre-trained biomedical language representation model leveraging mamba. arXiv preprint arXiv:2408.02600, 2024. [150] Qiuhong Shen, Zike Wu, Xuanyu Yi, Pan Zhou, Hanwang Zhang, Shuicheng Yan, and Xinchao Wang. Gamba: Marry gaussian splatting with mamba for single view 3d reconstruction. arXiv preprint arXiv:2403.18795, 2024. [151] Zhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, and Lei Zhu. Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 578588. Springer, 2024. [152] Haifan Gong, Luoyao Kang, Yitao Wang, Xiang Wan, and Haofeng Li. nnmamba: 3d biomedical image segmentation, classification and landmark detection with state space model. arXiv preprint arXiv:2402.03526, 2024. [153] Guangqian Yang, Kangrui Du, Zhihan Yang, Ye Du, Yongping Zheng, and Shujun Wang. Cmvim: Contrastive masked vim autoencoder for 3d multi-modal representation learning for ad classification. arXiv preprint arXiv:2403.16520, 2024. [154] Dingkang Liang, Xin Zhou, Wei Xu, Xingkui Zhu, Zhikang Zou, Xiaoqing Ye, Xiao Tan, and Xiang Bai. Pointmamba: simple state space model for point cloud analysis. arXiv preprint arXiv:2402.10739, 2024. [155] Xu Han, Yuan Tang, Zhaoxuan Wang, and Xianzhi Li. Mamba3d: Enhancing local features for 3d point cloud analysis via state space model. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 49955004, 2024. [156] Qingyuan Zhou, Weidong Yang, Ben Fei, Jingyi Xu, Rui Zhang, Keyi Liu, Yeqi Luo, and Ying He. 3dmambaipf: state space model for iterative point cloud filtering via differentiable rendering. arXiv preprint arXiv:2404.05522, 2024. [157] Md Atik Ahamed and Qiang Cheng. Timemachine: time series is worth 4 mambas for long-term forecasting. arXiv preprint arXiv:2403.09898, 2024. [158] Zihan Wang, Fanheng Kong, Shi Feng, Ming Wang, Xiaocui Yang, Han Zhao, Daling Wang, and Yifei Zhang. Is mamba effective for time series forecasting? arXiv preprint arXiv:2403.11144, 2024."
        },
        {
            "title": "REFERENCES",
            "content": "[159] Aobo Liang, Xingguo Jiang, Yan Sun, and Chang Lu. Bi-mamba4ts: Bidirectional mamba for time series forecasting. arXiv preprint arXiv:2404.15772, 2024. [160] Yujin Tang, Peijie Dong, Zhenheng Tang, Xiaowen Chu, and Junwei Liang. Vmrnn: Integrating vision mamba and lstm for efficient and accurate spatiotemporal forecasting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 56635673, 2024. [161] Badri Patro and Vijay Agneeswaran. Simba: Simplified mamba-based architecture for vision and multivariate time series. arXiv preprint arXiv:2403.15360, 2024. [162] Ali Behrouz and Farnoosh Hashemi. Graph mamba: Towards learning on graphs with state space models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 119130, 2024. [163] Lincan Li, Hanchen Wang, Wenjie Zhang, and Adelle Coster. Stg-mamba: Spatial-temporal graph learning via selective state space model. arXiv preprint arXiv:2403.12418, 2024. [164] Xiongxiao Xu, Yueqing Liang, Baixiang Huang, Zhiling Lan, and Kai Shu. Integrating mamba and transformer for long-short range time series forecasting. arXiv preprint arXiv:2404.14757, 2024."
        }
    ],
    "affiliations": [
        "Department of Electronic Engineering, Tsinghua University",
        "Robotics Institute, Carnegie Mellon University",
        "Shanghai Artificial Intelligence Laboratory"
    ]
}