{
    "paper_title": "Efficient Generative Modeling with Residual Vector Quantization-Based Tokens",
    "authors": [
        "Jaehyeon Kim",
        "Taehong Moon",
        "Keon Lee",
        "Jaewoong Cho"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We explore the use of Residual Vector Quantization (RVQ) for high-fidelity generation in vector-quantized generative models. This quantization technique maintains higher data fidelity by employing more in-depth tokens. However, increasing the token number in generative models leads to slower inference speeds. To this end, we introduce ResGen, an efficient RVQ-based discrete diffusion model that generates high-fidelity samples without compromising sampling speed. Our key idea is a direct prediction of vector embedding of collective tokens rather than individual ones. Moreover, we demonstrate that our proposed token masking and multi-token prediction method can be formulated within a principled probabilistic framework using a discrete diffusion process and variational inference. We validate the efficacy and generalizability of the proposed method on two challenging tasks across different modalities: conditional image generation} on ImageNet 256x256 and zero-shot text-to-speech synthesis. Experimental results demonstrate that ResGen outperforms autoregressive counterparts in both tasks, delivering superior performance without compromising sampling speed. Furthermore, as we scale the depth of RVQ, our generative models exhibit enhanced generation fidelity or faster sampling speeds compared to similarly sized baseline models. The project page can be found at https://resgen-genai.github.io"
        },
        {
            "title": "Start",
            "content": "Efficient Generative Modeling with Residual Vector Quantization-Based Tokens Jaehyeon Kim * 1 Taehong Moon * 2 Keon Lee 2 Jaewoong Cho 2 4 2 0 2 6 1 ] . [ 2 8 0 2 0 1 . 2 1 4 2 : r Abstract We explore the use of Residual Vector Quantization (RVQ) for high-fidelity generation in vectorquantized generative models. This quantization technique maintains higher data fidelity by employing more in-depth tokens. However, increasing the token number in generative models leads to slower inference speeds. To this end, we introduce ResGen, an efficient RVQ-based discrete diffusion model that generates high-fidelity samples without compromising sampling speed. Our key idea is direct prediction of vector embedding of collective tokens rather than individual ones. Moreover, we demonstrate that our proposed token masking and multi-token prediction method can be formulated within principled probabilistic framework using discrete diffusion process and variational inference. We validate the efficacy and generalizability of the proposed method on two challenging tasks across different modalities: conditional image generation on ImageNet 256256 and zero-shot text-to-speech synthesis. Experimental results demonstrate that ResGen outperforms autoregressive counterparts in both tasks, delivering superior performance without compromising sampling speed. Furthermore, as we scale the depth of RVQ, our generative models exhibit enhanced generation fidelity or faster sampling speeds compared to similarly sized baseline models. The project page can be found at https://resgen-genai.github.io. 1. Introduction Recent advancements in deep generative models have shown significant success in high-quality, realistic data generation across multiple domains, including language modeling (Achiam et al., 2023; Touvron et al., 2023; Reid et al., 2024), image generation (Rombach et al., 2022; Saharia *Equal contribution 1NVIDIA 2KRAFTON. Correspondence to: Jaewoong Cho <jwcho@krafton.com>. This work was done when Jaehyeon Kim was at KRAFTON. 1 et al., 2022; Betker et al., 2023), and audio synthesis (Wang et al., 2023; Shen et al., 2024; Rubenstein et al., 2023). While these models have demonstrated remarkable success, particularly with the effective scaling with both data and model sizes (Kaplan et al., 2020; Peebles & Xie, 2023), challenges remain when aiming for high-fidelity generation, especially in terms of balancing generation quality with computational efficiency. The demand for more detailed, high-resolution outputs such as images (Kang et al., 2023; He et al., 2023), videos (Bar-Tal et al., 2024) and audio (Evans et al., 2024; Copet et al., 2024), has led to the exploration of new approaches that can handle long input sequences and complex data structure effectively (Saharia et al., 2022; Ding et al., 2023). One promising approach to address these challenges is Residual Vector Quantization (RVQ) (Lee et al., 2022), which improves data reconstruction quality without increasing sequence length. RVQ extends Vector Quantized Variational Autoencoders (VQ-VAEs) (Van Den Oord et al., 2017) by iteratively applying vector quantization to the residuals of previous quantizations. This process results in token sequences that are shorter in length but deeper in hierarchy, effectively compressing data while maintaining high reconstruction fidelity. However, despite the advantages of RVQ in data compression, generative modeling on RVQ-based token sequences introduces new challenges. The hierarchical depth of these token sequences complicates the modeling process, particularly for autoregressive models whose sampling steps typically scale with the product of sequence length and depth (Lee et al., 2022). Although non-autoregressive approaches have been explored along either sequence length or depth (Borsos et al., 2023; Copet et al., 2024; Kim et al., 2024), existing methods do not effectively eliminate the sampling complexity associated with both dimensions simultaneously. In this paper, we present ResGen, an efficient RVQ-based generative modeling designed to achieve high-fidelity sample quality without compromising sampling speed. Our key idea lies in the direct prediction of vector embeddings of collective tokens rather than predicting each token individually. By forecasting cumulative embeddings, we can estimate correlated tokens across different depths, aligning Efficient Generative Modeling with Residual Vector Quantization-Based Tokens naturally with the residual quantization process. This approach allows us to decouple sampling complexity from both sequence length and depth, resulting in model that generates high-fidelity samples efficiently. Additionally, we extend our approach involving token masking strategy and multi-token prediction mechanism within principled probabilistic framework using discrete diffusion process and variational inference. We validate the efficacy and generalizability of ResGen across two real-world generative tasks: conditional image generation on ImageNet 256256 and zero-shot text-tospeech synthesis. Experimental results demonstrate superior performance over autoregressive counterparts in these tasks. Furthermore, as we scale the depth of RVQ, ResGen exhibits enhanced sampling quality or faster speeds compared to similar-sized baseline generative models. We also analyze model characteristics under varying hyperparameters, such as sampling steps, and examine their impact on generation quality in our ablation study. The rest of the paper is organized as follows. In Section 2, we provide the background for our study to establish the foundational understanding necessary for the subsequent discussion of our method. In Section 3, we introduce the ResGen framework, detailing the formulation of masked token prediction as discrete diffusion process and the decoupling of generation iteration from token sequence length and depth. We also compare our approach with previous methods, highlighting the advantages of our strategy. In Section 4, we provide the context for our study with relevant prior work. In Section 5, we present experimental results that validate the performance of ResGen, along with an ablation study on model performance with different RVQ depths and sampling steps. Finally, in Section 6 and Section 7, we summarize the key findings of the paper, discuss limitations and directions for future research. 2. Background Masked Token Modeling. Masked token modeling, introduced in prior work (Chang et al., 2022), is generative framework that operates on token sequences derived from the quantized encoder outputs of Vector Quantized Variational AutoEncoder (VQ-VAE) (Van Den Oord et al., 2017). The core idea involves randomly masking subset of input tokens and training the model to predict these masked tokens using cross-entropy loss. Formally, given token sequence NL and corresponding binary mask {0, 1}L, where each mi = 0 indicates that token xi is masked, we create masked token sequence by element-wise multiplying and m. The training objective is then formulated as: Lmask(x, m; θ) = log pθ(xix m), (cid:88) i[1,L], mi=0 where θ denotes the model parameters. The masking process involves selecting number of tokens to mask, determined by masking schedule = γ(r) L. Here, indicates the current time step in the unmasking process, normalized to range from zero to one, and γ() is pre-defined masking scheduling function that monotonically decreases from one to zero as increases. During training, is sampled from uniform distribution. In the decoding phase, the model employs an iterative prediction process to progressively fill in the masked sequence. At each iteration, the masking ratio is updated to linearly increase from zero to one. Starting with an entirely masked token sequence, the model predicts the masked tokens, and subset of these predicted tokens is selected to be unmasked based on confidence scores calculated through prediction probabilities. The number of tokens to unmask at each iteration is determined by the masking schedule. Residual Vector Quantization. Residual Vector Quantization (RVQ) has been proposed to improve VQ-VAEs. While previous VQ-VAEs quantize an input by replacing each encoded vector with the nearest embedding from codebook, RVQ iteratively applies vector quantization to the residuals of previous quantizations. Formally, let the output of the encoder in VQ-VAE at the position be hi,0. The residual vector quantizer maps it to sequence of quantized tokens NLD, where is the total depth of the RVQ process: xi,j = arg min v{1,...,V } hi,j1 e(v; j)2 hi,j = hi,j1 e(xi,j; j) for all [1, D], where e(v; j) is the v-th vector embedding from the codebook at depth j, and is the number of embeddings per depth. Here, xi,j represents the selected embedding index for the i-th token at depth j, and hi,j denotes the residual vector after the j-th quantization step. The final reconstructed vector is obtained by summing the embeddings across all depths, zi = (cid:80)D j=1 e(xi,j; j). This iterative quantization process enables RVQ to produce quantized output that closely approximates the original encoder output by increasing the depth of quantization steps. As result, RVQ effectively captures the most significant features in the lower quantization layers, while finer details are progressively captured in higher layers. 2 Efficient Generative Modeling with Residual Vector Quantization-Based Tokens Figure 1. An overview of the forward masking and reverse unmasking processes is shown at the top, with detailed depiction of the reverse unmasking process below. In the top figure, forward masking proceeds from right to left, incrementally masking more tokens, while reverse unmasking progresses from left to right, iteratively revealing the masked tokens. White boxes denote masked tokens and colored boxes represent tokens that have been uncovered. The bottom figure illustrates the reverse unmasking process in detail. Starting from masked residual vector quantization (RVQ) tokens, our method first predicts cumulative RVQ embeddings. These embeddings are then quantized and partially masked again. Through series of iterations, each round predicts the values of the masked tokens and replaces them until the entire token sequence is filled. 3. Method In this section, we introduce our method, ResGen, which iteratively fills tokens in coarse-to-fine manner to achieve efficient and high-fidelity generative modeling with Residual Vector Quantization (RVQ). We structure our discussion into three main parts: We present token masking strategy tailored for RVQ tokens and describe how we model masked token prediction by predicting sum of residual vector embeddings to decouple the generation iterations from the length and depth of token sequences. We show that our proposed token masking and multitoken prediction method can be formulated within principled probabilistic framework using discrete diffusion process and variational inference. We detail the training and sampling techniques of ResGen, focusing on the implementation of the mixture of Gaussians for latent embedding estimation and enhanced sampling strategies based on model confidence scores. 3.1. Masking and Prediction Task Design for RVQ Tokens Token Masking for RVQ Tokens. Our masking strategy progressively masks tokens starting from the highest quantization layers, capitalizing on the hierarchical nature of RVQ where tokens at greater depths capture finer details. Given token sequence from RVQ, NLD, with sequence length and depth D, we apply binary mask {0, 1}LD, where each mi,j indicates whether the token xi,j is masked (mi,j = 0) or not (mi,j = 1). The total number of tokens to mask is determined by masking schedule, = γ(r) D. Here, indicates the current time step in the unmasking process, normalized to range from zero to one, and γ() is pre-defined masking scheduling function that monotonically decreases from one to zero as increases. During training, is sampled from uniform distribution. To distribute the masked tokens across the positions, the number of tokens to mask at each position i, denoted by ki, is sampled without replacement from multinomial distribution with equal probability across all positions, ensuring that (cid:80)L i=1 ki = n. At each position i, ki tokens are masked starting from the highest depth = and moving towards lower depths. This ensures that finer details captured at higher depths are masked before coarser information at lower depths, as illustrated in Figure 1. Multi-Token Prediction of Masked Tokens. We describe the training and decoding phases of our multi-token prediction strategy, which efficiently predicts masked tokens by focusing on predicting the aggregated vector embeddings of collective tokens rather than the individual tokens x. Training: Given the input sequence and the corresponding mask m, the model predicts the sum of masked embeddings such that zi = (cid:80) e(xi,j; j) (1 mi,j) rather Efficient Generative Modeling with Residual Vector Quantization-Based Tokens than the target tokens directly, where e(v; j) denotes the v-th vector embedding from the RVQ codebook at depth j. The training objective is to maximize the log-likelihood of the sum of masked embeddings: Lmask(x, m; θ) = (cid:88) (cid:80) i[1,L], mi,j <D log pθ(zix m), (1) where θ represents the model parameters and the summation over includes only those positions where at least one token is masked, denoted by (cid:80) mi,j < D. To model the distribution pθ, we employ mixture of Gaussian distributions. We modify the training objective to encourage the mixture component usage of the mixture of Gaussian distributions, which is described in Section A.2 of Appendix. This method avoids imposing conditional independence of tokens along the depth, which could harm model performance. Instead, it relies on the key idea that accurately predicting the vector embedding zi is more critical than predicting the individual tokens xi, as the decoder of VQ-VAE operates on vector embeddings. Sampling: In the decoding phase, the model employs an iterative prediction process to progressively fill in the masked sequence. At each iteration, the masking ratio is updated to linearly increase from zero to one. Starting with an entirely masked token sequence, the model progressively fills in the sequence in coarse-to-fine manner. At each step, the model predicts the cumulative masked token embedding zi. These predicted vectors are then quantized into tokens via RVQ quantization. subset of these predicted tokens is randomly selected to be unmasked, where the number of tokens to unmask at each step is determined by the masking schedule. Although the quantization step at each sampling iteration involves sequential operations to reconstruct tokens from embeddings, it adds negligible overhead compared to the model forward pass. We summarize the training and sampling algorithms for ResGen in Algorithm 1 and 2 of Appendix. 3.2. Formulation within Probabilistic Framework We now cast our masked token prediction procedure into principled probabilistic framework based on discrete diffusion model and variational inference. This perspective allows us to view our method as likelihood-based generative process and provides theoretical foundation for its design. Forward Discrete Diffusion Process. Consider the token masking process described in Section 3.1. We can interpret this process as the forward diffusion step of discrete diffusion model on token sequences. The idea is to gradually transform fully unmasked token sequence x(0) into fully masked sequence x(T ) by progressively increasing the number of masked tokens at each step. The masking at each step is governed by discrete random process that determines how many tokens to mask. Let k(t+1) denote the number of tokens to be newly masked at the next step + 1 for the i-th position in token sequence. The total number of tokens to be masked at step + 1 is n(t+1) = (cid:80)L i=1 k(t+1) , where and are the length and the depth of the token sequence. The probabilistic mechanism is that at each step t, we sample the vector k(t+1) = (k(t+1) , ..., k(t+1) ) from multivariate hyL pergeometric distribution, which corresponds to drawing n(t+1) tokens without replacement from the pool of currently unmasked tokens. Formally, if at step there remain LD (cid:80)t τ =1 n(τ ) unmasked tokens in x(t), then drawing n(t+1) tokens to mask can be modeled as: q(k(t+1) x(t)) = τ =1 k(τ ) (cid:0)D(cid:80)t (cid:81) i=1 (cid:0)LD(cid:80)t k(t+1) τ =1 n(τ ) . (cid:1) (cid:1) n(t+1) Once we have sampled k(t+1), we construct x(t+1) from x(t) by masking out the newly selected tokens. Specifically, let ϕ denote the masked token. Then, resulting masked tokens at each sequence position are defined as: x(t+1) i,j = (cid:40) x(t) i,j ϕ if (cid:80)t otherwise τ =1 k(τ ) . An attractive property of this forward diffusion process is that we can write closed-form expressions for both the marginal distributions and conditional distributions at intermediate steps. Since the forward process is defined by incremental masking without replacement, we can directly integrate over all intermediate steps. This yields the marginal distribution of x(t) given x(0): q(x(t) x(0)) = (cid:81) i=1 (cid:0) (cid:0) (cid:1) (cid:80)t τ =1 k(τ ) LD τ =1 n(τ ) (cid:1) , (cid:80)t which expresses the probability of having (cid:80)t τ =1 k(τ ) masked in each segment i, given that total of (cid:80)t tokens have been masked overall up to step t. tokens τ =1 n(τ ) Similarly, we can write the conditional distribution of x(t) given x(t+1) and x(0): q(x(t) x(t+1), x(0)) = (cid:81) i=1 (cid:0)(cid:80)t+1 τ =1 k(τ ) k(t+1) (cid:1) (cid:0)(cid:80)t+1 τ =1 n(τ ) n(t+1) (cid:1) , reflecting the probability of having arrived at x(t) from x(t+1) by considering how many tokens were masked in the 4 Efficient Generative Modeling with Residual Vector Quantization-Based Tokens last step. In this sense, the forward and backward processes are fully characterized by the combinatorial structure of drawing tokens without replacement. Reverse Discrete Diffusion Process. The reverse process aims to reconstruct the original tokens from partially masked sequences. Given the models probability to reconstruct the original tokens pθ(x(0) x(t+1)), the probability to reverse diffusion step pθ(x(t) x(t+1)) is defined as: q(x(t) x(t+1), x(0))pθ(x(0) x(t+1)). (cid:88) x(0) This formulation lets us compute the variational lower bound of the data log-likelihood: log pθ(x(0)) Eq (cid:20) LT + (cid:88) Lt + L0 (cid:21) , where LT = DKL (cid:16) q(x(T ) x(0)) p(x(T )) (cid:17) , Lt = DKL (cid:16) (cid:17) q(x(t) x(t+1), x(0)) pθ(x(t) x(t+1)) , and L0 = log pθ(x(0) x(1)). Here, LT is the prior loss, which becomes zero since x(T ) is fully masked, Lt are the diffusion losses at each step t, and L0 is the reconstruction loss. By combining the diffusion losses and the reconstruction loss, we can derive simplified loss function: Lsimple(x(0); θ) = log pθ(x(0) x(t)), placing equal emphasis on predicting the original tokens at each step. Latent Modeling with Variational Inference. To efficiently handle dependencies across token depths, we adopt multi-token prediction approach inspired by CLaMTTS (Kim et al., 2024). Instead of predicting tokens individually, we predict their cumulative vector embedding z. This approach aligns naturally with the RVQ dequantization process and decouples the generation time complexity from the token depth. We use variational inference to derive the upper bound of the negative log-likelihood log pθ(x(0) x(t)): (cid:20) log p(x(0)z, x(t)) log Eqz pθ(z x(t)) q(z x(0), x(t)) (cid:21) . Assuming p(x(0)z, x(t)) corresponds to RVQ quantization and q(z x(0), x(t)) to RVQ dequantization of the masked tokens, we focus on the remaining term: Lmask(x(0), x(t); θ) = log pθ(zx(t)), which matches the prediction loss in Equation 1. 5 4. Related Work Vector-quantized (VQ) token-based generative models have emerged to harness the powerful generative capabilities of transformers for both autoregressive and non-autoregressive modeling. VQ-GAN (Esser et al., 2021) and DALLE (Ramesh et al., 2021) leverage these discrete representations for image synthesis using transformers, facilitating high-quality generation with manageable computational resources. Discrete diffusion models have been proposed to model token sequences by iteratively refining corrupted tokens or progressively unmasking masked tokens (Austin et al., 2021; Chang et al., 2022; Gu et al., 2022). MaskGIT (Chang et al., 2022) and VQ-Diffusion (Gu et al., 2022) focus on masked token prediction for flat token sequences, improving sampling efficiency over autoregressive models. GIVT (Tschannen et al., 2023) introduces method that replaces softmaxbased token prediction with mixture-of-Gaussians-based vector prediction in masked token prediction, progressively filling masked positions with predicted vectors. Recent works like VAR (Tian et al., 2024) and MAR (Li et al., 2024) propose alternative paradigms to token-based autoregressive modeling. VAR introduces coarse-to-fine next-scale prediction mechanism, effectively capturing hierarchical structures in images. MAR eliminates the reliance on discrete tokens by modeling probabilities in continuousvalued space using diffusion-based approach, simplifying the pipeline while maintaining strong performance. However, these methods primarily deal with flat token sequences and do not consider the hierarchical depth inherent in RVQ. RQ-Transformer (Lee et al., 2022) was the first to demonstrate generative modeling on RVQ tokens using an autoregressive model over the product of sequence length and depth, resulting in increased computational complexity. Vall-E (Wang et al., 2023) predicts the tokens at the first depth autoregressively and then predicts the remaining tokens at each depth in single forward pass sequentially. SoundStorm (Borsos et al., 2023) generates tokens using masked token prediction given semantic tokens, but still has sampling time complexity that increases linearly with the residual quantization depth. NaturalSpeech 2 (Shen et al., 2024) employs diffusion-based generative modeling in the RVQ embedding space instead of token generation. CLaM-TTS (Kim et al., 2024) employs vector prediction for multi-token prediction but operates in an autoregressive manner along the sequence length. In contrast to these approaches, our method offers more efficient solution for generative modeling with RVQ tokens. We propose strategy that predicts the vector embedding of masked tokens, decoupling the sampling time complexity from both sequence length and token depth. By predicting Efficient Generative Modeling with Residual Vector Quantization-Based Tokens cumulative vector embeddings rather than individual tokens, our method efficiently handles the hierarchical structure of tokens, offering enhanced sampling efficiency and highfidelity generation. els are detailed in Appendix A.1. 5.2. Experimental Results 5.2.1. VISION TASK 5. Experiments In this section, we demonstrate the effectiveness of our approach in both image generation and text-to-speech synthesis, highlighting its generalizability and efficiency. 5.1. Experimental Setting Experiment Tasks. For the vision domain, we focus on conditional image generation tasks on ImageNet (Krizhevsky et al., 2017) at resolution of 256 256. In the audio domain, we evaluate our model using two tasks inspired by Voicebox (Le et al., 2023): 1) continuation: given text and 3-second segment of ground truth speech, the goal is to generate seamless speech that continues in the same style as the provided segment; 2) cross-sentence: given text, 3-second speech segment, and its transcript (which differs from the text), the objective is to generate speech that reads the text in the style of the provided segment. Evaluation Metrics. For vision tasks, we employ the Frechet Inception Distance (FID) (Heusel et al., 2017) for comparing it with other state-of-the-art image generative models. For audio tasks, we evaluate the models using the following objective metrics: Character Error Rate (CER), Word Error Rate (WER), and Speaker Similarity (SIM), as described in VALL-E (Wang et al., 2023) and CLaM-TTS (Kim et al., 2024). CER and WER measure the intelligibility and robustness. For SIM, we adopt SIM-o and SIM-r metrics from Voicebox (Le et al., 2023). SIM-o evaluates the similarity between the generated speech and the original target speech, while SIM-r assesses the similarity between the target speech and its reconstruction, which is obtained by processing the original speech through pre-trained autoencoder and vocoder. Baselines and Training Configurations. In the vision domain, we compare our models with recent generative model families, including (1) autoregressive models: RQtransformer (Lee et al., 2022), VAR (Tian et al., 2024), MAR (Li et al., 2024); and (2) non-autoregressive models: MaskGiT (Chang et al., 2022), DiT (Peebles & Xie, 2023). For the audio task, we benchmark the proposed model against state-of-the-art TTS models, including (1) autoregressive models: VALL-E (Wang et al., 2023), SPEAR-TTS (Kharitonov et al., 2023), and CLaM-TTS (Kim et al., 2024); and (2) non-autoregressive models: YourTTS (Casanova et al., 2022), VoiceBox (Le et al., 2023), and DiTTo-TTS (Lee et al., 2024). The training configurations for our modWe first highlight the superior performance of our generative model compared to prior methods. Representatively, we evaluate our method using the RQ-transformer with the same number of RVQ tokens. Specifically, we train our model with depth 4 RVQ tokens, the same configuration used for training the 8x8x4 RQ-transformer. As shown in Table 2, our method not only outperforms the baseline in terms of accuracy but also achieves faster inference times. Notably, it is more parameter-efficient, requiring only 574M parameters. Next, we evaluate the effectiveness of our generative modeling within the context of existing generative model families. The results, presented in Table 1, compare the models across three key aspects: generation quality, memory efficiency, and generation speed. These aspects are critical for practical deployment, ensuring an optimal trade-off between quality, scalability, and computational feasibility. Generation quality is assessed using the FID metric, memory efficiency is evaluated based on the maximum batch size, which refers to the maximum number of latent representations that generative model can process during inference on the same device, and generation speed is measured as the wall-clock time required to generate single sample. Generation quality. Among models with similar parameter counts, ResGen-d16 ranks second only to MAR-L, which achieves an FID of 1.95 with classifier-free guidance (CFG). Despite its slightly higher FID score compared to MARL, ResGen achieves comparable quality with much faster sampling speed, demonstrating its efficiency in balancing quality and resource usage. These results highlight the competitive generation quality of ResGen, which closely rivals the MAR series. Speed efficiency. As illustrated in Figure 2, ResGen-d16 attains one of the fastest sampling times, second only to VAR, making it practical choice for scenarios where speed is critical. Unlike certain models that compromise speed for quality, our method maintains favorable balance, offering both rapid generation and competitive quality. Compared to the MAR series, which achieves marginally better FID scores, ResGens superior speed positions it as an efficient solution for real-time or high-throughput applications. Memory efficiency. ResGen consistently demonstrates strong memory efficiency, achieving the highest maximum latent batch size. ResGen-d16 supports the maximum latent batch size of 1915, exceeding MAR-Bs latent batch size 6 Efficient Generative Modeling with Residual Vector Quantization-Based Tokens Table 1. Comparison of different generative models on class-conditional ImageNet at resolution of 256256. The boldface indicates the best result, the underline denotes the second best, and the asterisk denotes the score reported in the original papers. The code length represents the sequence length of latent representations and the maximum batch size refers to the maximum number of latent representations that generative model can process during inference. Model"
        },
        {
            "title": "MaskGiT",
            "content": "DiT-XL/2 VAR-d16 VAR-d20 VAR-d24 VAR-d30 MAR-B MAR-L MAR-H RQ-Transformer RQ-Transformer ResGen-d8 ResGen-d16 Code length Params FID (w/o CFG) FID (w/ CFG) Maximum batch size 256 256 256 256 256 256 256 256 256 64 64 64 277M 675M 310M 600M 1.0B 2.0B 208M 479M 943M 1.4B 3.8B 574M 574M 6.18* 9.62* 12.18 8.60 6.43 5.31 3.48* 2.60* 2.35* 8.71* 7.55* 6.56 6. - 2.27* 3.30* 2.57* 2.09* 1.92* 2.31* 1.78* 1.55* 3.89* 3.80* 2.71 1. - 1159 247 148 102 60 1738 1167 812 1151 390 1995 Figure 2. The left figure shows the trade-off between sampling speed and generation quality across various generative models. For ResGen, dotted lines indicate performance across different sampling steps, highlighting step-dependent performance improvements. For other models, solid lines connect results corresponding to variations in parameter size. Note that for ResGen, represents the depth of the model. The right figure shows the maximum batch size achievable during inference for each model. Both wall-clock inference time and maximum inference batch size are measured in the same environment. Table 2. Comparison of image generation quality and efficiency between ResGen and the RQ-transformer, evaluated using the same RVQ tokens. Model Params FID (w/o CFG) Inference Time ResGen (Ours) RQ-transformer 594M 821M 9.26 13.11 1.25s 2.38s of 1738. This high memory efficiency allows for handling large-scale generative tasks effectively, alleviating computational bottlenecks in production pipelines. This capability makes ResGen particularly advantageous for deployment in 7 resource-constrained environments. In our supplementary ablation study, presented in Appendix B.2, we investigate how various sampling hyperparameters, such as the number of steps and temperature scaling, affect the models behavior and generation quality. The temperature scaling, which follows strategy similar to that of MaskGIT (Chang et al., 2022), is applied to guide the random unmasking process using the probability of mixture of Gaussian distributions as confidence score, as detailed in Appendix B.1. Efficient Generative Modeling with Residual Vector Quantization-Based Tokens Table 3. Performances for the continuation task (top table) and the cross-sentence task (bottom table). The boldface indicates the best result, the underline denotes the second best, and the asterisk denotes the score reported in the baseline paper. Model Params WER CER SIM-o SIM-r Inference Steps"
        },
        {
            "title": "Ground Truth",
            "content": "n/a - YourTTS 302M Vall-E 364M Voicebox 584M CLaM-TTS DiTTo-en-L 508M DiTTo-en-XL 740M 2.2* 7.57 3.8* 2.0* 2.36* 1.85 1.78*"
        },
        {
            "title": "ResGen",
            "content": "625M 1.79 0.61* 3.06 - - 0.79* 0.50 0.48* 0.49 0.754* 0.754* n/a 0.3928 0.452* 0.593* 0.4767* 0.5596 0.5773* - 0.508* 0.616* 0.5128* 0.5913 0.6075* 0.5743 0. 1 - 64 - 25 25 16 Model WER CER SIM-o YourTTS Vall-E SPEAR-TTS Voicebox CLaM-TTS DiTTo-en-L DiTTo-en-XL 2.56* 7.92 (7.7*) 5.9* - 1.9* 5.11* 2.69 ResGen 1.72 3.18 - 1.92* - 2.87* 0.91 0.89* 0. 0.3755 (0.337*) - - 0.662* 0.4951* 0.6050 0.627* 0.6013 SIM-r - 0.580* 0.560* 0.681* 0.5382* 0.6355 0.6554* 0.6275 5.2.2. AUDIO TASK 6. Conclusion In our Text-to-Speech (TTS) experiments, we first compare our method to autoregressive models that generate RVQ tokens, using the same MelVAE module from CLaM-TTS. As shown in Table 3, our model achieves lower word and character error rates (WER and CER) as well as higher speaker similarity scores (SIM-o and SIM-r) than the baseline and requires fewer inference steps, demonstrating its efficiency in token generation. Notably, our method uses only 16 iterations, which is fewer than the RVQ depth of 32. We then compare the performance of our generative modeling in the audio domain with recent TTS models. While our results do not surpass state-of-the-art methods such as Voicebox and DiTTo-TTS on every metric, the proposed approach demonstrates substantial advantages in computational efficiency and accuracy. Specifically, ResGen achieves the lowest WER and CER in the cross-sentence task, outperforming all baselines. In the continuation task, ResGen attains competitive WER and CER scores, ranking second only to DiTTo-en-XL, while maintaining the smallest number of sampling iterations. This reduction in inference steps emphasizes the computational efficiency of ResGen, making it highly suitable for deployment in scenarios where both accuracy and speed are critical. We present our generated audio samples in the project page. In this work, we propose ResGen, an efficient RVQ-based discrete diffusion model that generates high-fidelity samples while maintaining fast sampling speeds. By directly predicting the vector embedding of collective tokens, our method addresses the typical trade-offs between token depth and inference speed in vector-quantized generative models. We further demonstrate the effectiveness of token masking and multi-token prediction within principled probabilistic framework, employing discrete diffusion process and variational inference. Our experiments on both conditional image generation and zero-shot text-to-speech synthesis validate the strong performance of ResGen, which performs comparably to or exceeds autoregressive models in terms of fidelity and sampling speed. As we scale the depth of RVQ, our model exhibits improvements in generation fidelity or efficiency, showing its scalability and generalizability across different modalities. 7. Limitations and Future Directions The proposed method demonstrates favorable memory efficiency alongside competitive sampling speed and generation quality. One potential avenue for further improvement that is not explored in our work is the utilization of key-value (KV) caching within the transformer architecture. By pro8 Efficient Generative Modeling with Residual Vector Quantization-Based Tokens gressively filling tokens, positions that have been completely filled can reuse their precomputed KV values, thereby reducing redundant computations. This strategy can significantly enhance the sampling speed and reduce overall computational overhead, making it promising direction for future research. While our approach is intricately designed around Residual Vector Quantization (RVQ) tokens, recent developments in quantization methods suggest that Finite Scalar Quantization (FSQ) may offer additional benefits (Mentzer et al., 2024). Extending our approach to support FSQ, however, is not straightforward, as it involves distinct tokenization and embedding processes. Nevertheless, exploring this direction could lead to novel quantization strategies and improved generative performance. Another key observation is that our approach achieves highquality generation with relatively small number of iterations. We hypothesize that this efficiency, compared to conventional diffusion models, stems from the unmasking process rather than denoising process. Since predicting tokens based on completely unmasked tokens is likely easier than predicting them based on noisy inputs, the model benefits from the simpler prediction task. Despite this empirical success, our work lacks theoretical justification for why such low number of inference steps is sufficient. Providing formal theoretical and analytic explanation for this phenomenon represents another promising future direction."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van Den Berg, R. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:1798117993, 2021. Casanova, E., Weber, J., Shulby, C. D., Junior, A. C., Golge, E., and Ponti, M. A. Yourtts: Towards zero-shot multispeaker tts and zero-shot voice conversion for everyone. In International Conference on Machine Learning (ICML), pp. 27092720. PMLR, 2022. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Copet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y., and Defossez, A. Simple and controllable music generation. Advances in Neural Information Processing Systems, 36, 2024. Ding, Z., Zhang, M., Wu, J., and Tu, Z. Patched denoising diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2023. Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Evans, Z., Carr, C., Taylor, J., Hawley, S. H., and Pons, J. Fast timing-conditioned latent audio diffusion. arXiv preprint arXiv:2402.04825, 2024. Gu, S., Chen, D., Bao, J., Wen, F., Zhang, B., Chen, D., Yuan, L., and Guo, B. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1069610706, 2022. He, Y., Yang, S., Chen, H., Cun, X., Xia, M., Zhang, Y., Wang, X., He, R., Chen, Q., and Shan, Y. Scalecrafter: Tuning-free higher-resolution visual generation with diffusion models. In The Twelfth International Conference on Learning Representations, 2023. Bar-Tal, O., Chefer, H., Tov, O., Herrmann, C., Paiss, R., Zada, S., Ephrat, A., Hur, J., Li, Y., Michaeli, T., et al. Lumiere: space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. GANs trained by two time-scale update rule converge to local Nash equilibrium. Advances in neural information processing systems, 30, 2017. Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3): 8, 2023. Kang, M., Zhu, J.-Y., Zhang, R., Park, J., Shechtman, E., Paris, S., and Park, T. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10124 10134, 2023. Borsos, Z., Sharifi, M., Vincent, D., Kharitonov, E., Zeghidour, N., and Tagliasacchi, M. Soundstorm: Efficient parallel audio generation. arXiv preprint arXiv:2305.09636, 2023. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 9 Efficient Generative Modeling with Residual Vector Quantization-Based Tokens Kharitonov, E., Vincent, D., Borsos, Z., Marinier, R., Girgin, S., Pietquin, O., Sharifi, M., Tagliasacchi, M., and Zeghidour, N. Speak, Read and Prompt: High-Fidelity Text-toSpeech with Minimal Supervision. Transactions of the Association for Computational Linguistics, 11:17031718, 12 2023. ISSN 2307-387X. doi: 10.1162/tacl 00618. Kim, J., Kim, S., Kong, J., and Yoon, S. Glow-tts: generative flow for text-to-speech via monotonic alignment search. Advances in Neural Information Processing Systems, 33:80678077, 2020. Kim, J., Lee, K., Chung, S., and Cho, J. Clam-tts: Improving neural codec language model for zero-shot text-to-speech. In The Twelfth International Conference on Learning Representations, 2024. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):8490, 2017. Le, M., Vyas, A., Shi, B., Karrer, B., Sari, L., Moritz, R., Williamson, M., Manohar, V., Adi, Y., Mahadeokar, J., and Hsu, W.-N. Voicebox: Text-guided multilingual universal speech generation at scale. In Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems (NeurIPS), volume 36, pp. 1400514034. Curran Associates, Inc., 2023. Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Rubenstein, P. K., Asawaroengchai, C., Nguyen, D. D., Bapna, A., Borsos, Z., Quitry, F. d. C., Chen, P., Badawy, D. E., Han, W., Kharitonov, E., et al. Audiopalm: large language model that can speak and listen. arXiv preprint arXiv:2306.12925, 2023. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35: 3647936494, 2022. Shen, K., Ju, Z., Tan, X., Liu, E., Leng, Y., He, L., Qin, T., Bian, J., et al. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers. In The Twelfth International Conference on Learning Representations, 2024. Lee, D., Kim, C., Kim, S., Cho, M., and Han, W.-S. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1152311532, 2022. Tian, K., Jiang, Y., Yuan, Z., Peng, B., and Wang, L. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. Lee, K., Kim, D. W., Kim, J., and Cho, J. Ditto-tts: Efficient and scalable zero-shot text-to-speech with diffusion transformer. arXiv preprint arXiv:2406.11427, 2024. Li, T., Tian, Y., Li, H., Deng, M., and He, K. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. Mentzer, F., Minnen, D., Agustsson, E., and Tschannen, M. Finite scalar quantization: Vq-vae made simple. In The Twelfth International Conference on Learning Representations, 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-toimage generation. In International conference on machine learning, pp. 88218831. Pmlr, 2021. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Tschannen, M., Eastwood, C., and Mentzer, F. Givt: Generative infinite-vocabulary transformers. arXiv preprint arXiv:2312.02116, 2023. Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023. Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., and Raffel, C. Byt5: Towards token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291306, 2022. 10 Efficient Generative Modeling with Residual Vector Quantization-Based Tokens A. Training details A.1. Configurations for Training We train our method based on similar architecture to DiT (Peebles & Xie, 2023), adopting the XLarge version but replacing the linear layers of adaptive layer normalization layers for conditioning with bias parameters. For the ImageNet 256x256 task, all variants of ResGen are trained with batch size of 256 across 4 GPUs to 7M iterations. To increase the depth of the RVQ, we warm-start from the checkpoint of RQ-VAE (Lee et al., 2022), excluding the attention layers, and reduce the latent dimension from 256 to 64. These models are trained for an additional 1M steps each, without and with adversarial training, following the same configuration as prior work. The embeddings of RVQ tokens, obtained from the RVQ quantizer, are used as inputs to the model. These embeddings are then projected to match the models hidden size using linear layer. For the Text-to-Speech task, our model, based on the DiT XLarge architecture as in the vision task, is trained using the same configuration as in prior work (Kim et al., 2024), utilizing 4 GPUs for 310M iterations. We employ 4 transformer layers to train linear regression duration predictor for the text inputs, built on top of the pretrained text encoder, ByT5-Large (Xue et al., 2022). The duration predictor is trained to minimize the L2 loss between the mel-spectrogram and the expanded hidden representation of text from ByT5, with alignment achieved using the monotonic alignment search algorithm (Kim et al., 2020). The expanded text hidden representation is downsampled through strided convolution layer to match the length of the RVQ token sequence and combined with the embeddings of RVQ tokens obtained from the RVQ quantizer. These combined representations are then projected to match the models hidden size using linear layer. A.2. Implementation Techniques for Training Mixture of Gaussians Implementation. Our model utilizes mixture of Gaussian distributions to represent the distribution over latent embeddings. Specifically, for each token position i, the model outputs the mixture probabilities πππi = {π(ν) ν=1, the mean vectors for each mixture component {µµµ(ν) ν=1, and additional scale and shift parameters for affine transformations ai and bi RH , where is the number of mixture components and is the embedding dimension. }K }K Training Objective Modification. From Equation 1, the log-likelihood of the target embedding zi is formulated as log pθ(zix m) = log ai + log (cid:80) , I), where zi = (zi bi)/ai. To further encourage the usage of every mixture component, we modify the objective by decomposing it into sum of classification and regression losses. Similar to prior work (Kim et al., 2024), applying Jensens inequality, we have: ( zi; µµµ(ν) ν π(ν) log ai log (cid:88) ( zi; µµµ(ν) π(ν) , I) ν (cid:88) ν log ai (cid:124) q(ν zi, µµµi) log ( zi; µµµ(ν) , I) + DKL(q(ν zi, µµµi) πππi) , (cid:123)(cid:122) regression loss (cid:125) (cid:124) (cid:123)(cid:122) classification loss (cid:125) where q(ν zi, µµµi) is an auxiliary distribution defined as q(ν zi, µµµi) ( zi; µµµ(ν) , I). This choice of ensures that mixture components with mean vectors closer to zi have higher probabilities, while all components retain non-zero probabilities. Consequently, every mixture component contributes to the training process, promoting higher component usage and diversity in the models predictions. Increasing the number of mixture components leads to substantial growth in the output Low-rank Projection. dimensionality of the model, as it scales with H. To accommodate high number of mixtures without incurring excessive computational costs, we adopt low-rank projection approach following the methodology of the prior work (Kim et al., 2024). In this approach, the model outputs low-rank mean vectors {µµµ(ν) (ν) and s(ν): µµµ(ν) zi µµµ(ν) ν=1, which are then transformed using trainable parameters + s(ν). This decomposition allows for efficient computation of the squared distance 2 by expanding it as follows: = (ν) µµµ(ν) }K zi µµµi2 = zi (M µµµi + s)2 = zT zi + µµµT (M )µµµi + sT 2(M zi)T µµµi 2 zT + 2µµµT T s, (2) 11 Efficient Generative Modeling with Residual Vector Quantization-Based Tokens where we omit ν for simplicity. This low-rank projection enables the model to handle large number of mixture components without significant overhead, thereby enhancing both the scalability and performance of the generative process. A.3. Pseudo-code for Training Algorithm 1 Training 1: procedure BinaryMask(n, L, D) 2: 3: 4: 5: Sample k1:L without replacement with total draws n. for = 1 to do mi,1:(Dki) 1 mi,(Dki+1):D 0 end for 6: 7: return 8: end procedure 9: 10: repeat 11: 12: pdata Uniform[0, 1) γ(r) 13: 14: BinaryMask(n, L, D) 15: 16: (cid:80) Take gradient descent step on: (e(x:,j; j) (1 m:,j)) 17: 18: until converged θLmask(x, m; θ) 12 Efficient Generative Modeling with Residual Vector Quantization-Based Tokens B. Sampling details B.1. Sampling with Confidence Scores Inspired by confidence-based sampling with choice temperature, as proposed in MaskGIT (Chang et al., 2022) and GIVT (Tschannen et al., 2023), we unmask tokens based on the log probabilities computed for all masked tokens. These log probabilities are derived from the squared distance between token embeddings and the sampled latent zi at each position i. The log probability log p(xi,j zi) is calculated as log p(xi,j zi) log for all masked positions and j. Here, σj denotes the standard deviation of latents at RVQ depth pre-calculated during RVQ training. The log probabilities are cumulatively summed across depths, and the confidence score is obtained by adding Gumbel noise, scaled by the choice temperature, to them. The choice temperature remains fixed throughout all inference steps in our settings. Tokens with higher confidence scores are prioritized for unmasking and are filled earlier in the iterative generation process. d=1 e(xi,d; d); e(xi,j; j), σ2 zi (cid:80)j1 (cid:17) (cid:16) B.2. Ablation Studies on Sampling We conducted ablation experiments to analyze the characteristics of our sampling algorithm, focusing on hyperparameters such as sampling steps, top-p values, and temperature scale, and their impact on generation quality. As illustrated in Figure 3a, increasing the number of sampling steps improves generation quality in both scenarios: with classifier-free guidance (CFG) and without it. This demonstrates that additional steps enable the model to refine its outputs more effectively, resulting in higher-quality generations. Figure 3b explores the impact of top-p values on generation quality, highlighting different trends depending on the use of CFG. With CFG, higher top-p values promote greater diversity in sampling, resulting in improved generation quality. In contrast, without CFG, lower top-p values lead to reliance on the models confident predictions, thereby enhancing generation quality. These findings suggest that adjusting the top-p value can be beneficial, particularly in the absence of CFG. Finally, Figure 3c highlights the influence of temperature on generation quality. moderate temperature introduces controlled stochasticity during sampling, mitigating the monotonicity inherent in RVQ-token unmasking, where the order is guided by confidence scores. By adjusting the temperature, we achieve balance between diversity and fidelity, optimizing overall generation performance. (a) Step search (b) Top-p search (c) Temperature search Figure 3. Configuration search results for sampling methods with (blue) and without (green) classifier-free guidance (CFG). (a) The effect of varying the number of sampling steps, (b) the impact of different top-p values, and (c) the influence of temperature scaling on confidence scores. 13 Efficient Generative Modeling with Residual Vector Quantization-Based Tokens B.3. Pseudo-code for Sampling Algorithm 2 Sampling 1: procedure BinaryUnmask(n, L, D, m) Compute the number of masked tokens qi = (cid:80)D Sample k1:L from multivariate hypergeometric distribution with maximum number of selection qi, total draws qi n. for = 1 to do j=1(1 mi,j) (cid:80) m[i, (D qi + 1):(D qi + ki)] 1 2: 3: 4: 5: 6: end for return 7: 8: end procedure 9: 10: Initialize fully masked sequence NLD 11: Initialize mask {0, 1}LD with zeros. 12: for = 1, . . . , do pθ(zx m) Apply residual vector quantization for masked tokens: 13: 14: 15: 16: 17: 18: 19: 20: 21: RV Q(z, m) γ(r) if using random sampling then BinaryUnmask(n, L, D, m) else if using confidence-based sampling then Update by selecting tokens with the confidence-based sampling from Section B.1. end if 22: 23: end for 24: Return 14 Efficient Generative Modeling with Residual Vector Quantization-Based Tokens (a) VAR-d30 (FID=1.92) (b) MAR-H (FID=1.55) (c) DiT-XL/2 (FID=2.27) (d) ResGen, ours (FID=1.95) Figure 4. Model comparison on ImageNet 256256 benchmark. 15 Efficient Generative Modeling with Residual Vector Quantization-Based Tokens Figure 5. Randomly generated 256256 samples by ResGen trained on ImageNet."
        }
    ],
    "affiliations": [
        "KRAFTON",
        "NVIDIA"
    ]
}