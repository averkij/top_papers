{
    "paper_title": "Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space",
    "authors": [
        "Zhengrui Ma",
        "Yang Feng",
        "Chenze Shao",
        "Fandong Meng",
        "Jie Zhou",
        "Min Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce SLED, an alternative approach to speech language modeling by encoding speech waveforms into sequences of continuous latent representations and modeling them autoregressively using an energy distance objective. The energy distance offers an analytical measure of the distributional gap by contrasting simulated and target samples, enabling efficient training to capture the underlying continuous autoregressive distribution. By bypassing reliance on residual vector quantization, SLED avoids discretization errors and eliminates the need for the complicated hierarchical architectures common in existing speech language models. It simplifies the overall modeling pipeline while preserving the richness of speech information and maintaining inference efficiency. Empirical results demonstrate that SLED achieves strong performance in both zero-shot and streaming speech synthesis, showing its potential for broader applications in general-purpose speech language models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 1 8 1 3 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Efficient Speech Language Modeling via Energy\nDistance in Continuous Latent Space",
            "content": "Zhengrui Ma 1,2,3, Yang Feng 1,2 * , Chenze Shao 3, Fandong Meng 3, Jie Zhou 3, Min Zhang 4 1 Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences 2 University of Chinese Academy of Sciences 3 Pattern Recognition Center, WeChat AI, Tencent Inc 4 School of Future Science and Engineering, Soochow University *: Corresponding author {mazhengrui21b,fengyang}@ict.ac.cn"
        },
        {
            "title": "Abstract",
            "content": "We introduce SLED, an alternative approach to speech language modeling by encoding speech waveforms into sequences of continuous latent representations and modeling them autoregressively using an energy distance objective. The energy distance offers an analytical measure of the distributional gap by contrasting simulated and target samples, enabling efficient training to capture the underlying continuous autoregressive distribution. By bypassing reliance on residual vector quantization, SLED avoids discretization errors and eliminates the need for the complicated hierarchical architectures common in existing speech language models. It simplifies the overall modeling pipeline while preserving the richness of speech information and maintaining inference efficiency. Empirical results demonstrate that SLED achieves strong performance in both zero-shot and streaming speech synthesis, showing its potential for broader applications in general-purpose speech language models. Demos and code are available at https://github.com/ictnlp/SLED-TTS."
        },
        {
            "title": "Introduction",
            "content": "Text language modeling has achieved tremendous success in recent years. Works such as the GPT series [2, 49] have demonstrated that by modeling text sequences in an autoregressive manner and pretraining on large corpora, language models acquire impressive in-context learning abilities and can perform nearly any downstream natural language processing task. This remarkable potential of autoregressive language modeling has inspired researchers to explore whether speech audio, which also possesses linear structure, can be modeled in similar fashion. Unlike text sequences composed of discrete tokens from finite vocabulary, speech audio is often represented as lengthy sequence of sampling points usually stored as integers or floating-point values within range. This fundamental difference poses significant challenges for autoregressive speech modeling in manner analogous to text. Text language models typically rely on finite vocabulary to acquire categorical distribution at each step, which is essential for stable training via cross-entropy loss and efficient ancestral sampling during inference. To adapt autoregressive modeling to speech, researchers have explored discretizing speech into sequences of discrete tokens using external quantization modules. Examples include discretization on SSL-learned features [24] in [32, 83, 48], discretization on ASR-supervised features in [7, 8, 81, 30] and reconstruction-oriented codec [80, 9, 31, 84, 78, 50, 25] in [1, 71, 86, 10, 79, 37]. However, discretization inevitably creates an information bottleneck, potentially losing the rich details in the raw waveform and thus degrading the reconstruction quality derived from the discretized tokens. One popular approach to mitigating this information loss is residual vector quantization [RVQ; 69, 80], which converts raw waveforms into Preprint. Under review. Figure 1: Different speech language modeling approaches. Left: VALL-E-style hierarchical architecture for RVQ token sequences. Middle: RQ-Transformer-style hierarchical architecture for RVQ token sequences. Right: Architecture for continuous token sequences. multi-stream sequences of discrete tokens. However, RVQ introduces additional complexity, requiring hierarchical autoregressive architectures to effectively model multi-stream sequences [71, 76]. As an alternative to discretizing speech, researchers are starting to explore whether speech language modeling can be performed in continuous latent space [45, 68, 38]. This approach offers several clear advantages. Firstly, it eliminates the need for discretization, enabling speech modeling in latent space with minimal information loss. Secondly, it removes the reliance on hierarchical architecture required for modeling multi-stream sequences in RVQ, significantly reducing modeling complexity. Similar to discrete autoregressive modeling, continuous autoregressive modeling requires capturing per-step distribution, but within continuous space. Unlike the discrete case, where categorical distribution can be constructed using the softmax function, continuous space modeling relies on conditional probabilistic generative module to fulfill this role [67]. This module should be conditioned on features extracted by an autoregressive network, which captures the dependencies between the current step and the preceding ones. This raises an important question: How should we construct such generative module for speech language modeling in continuous latent space? Ideally, we want the conditional generative module to function like the softmax in the discrete caseit should be lightweight, expressive, training-stable and inference-efficient. In speech language modeling, sampling efficiency becomes particularly important as the latent sequences of speech are typically lengthy, and each step in autoregressive generation involves sampling process. In this research, we introduce an approach to speech language modeling in continuous latent space using energy distance (SLED). We model the per-step continuous distribution with lightweight implicit conditional generative module that takes features encoded with autoregressive dependencies and random noise as inputs [36, 59]. To measure the discrepancy between the underlying data distribution and the models distribution, we employ specialized form of maximum mean discrepancy [MMD; 17], known as generalized energy distance [GED; 41]. By selecting an appropriate distance function, the resulting metric becomes strictly proper scoring rule [15, 58], enabling efficient training of the model to capture the autoregressive continuous distribution. Our approach eliminates the need for iterative sampling at each autoregressive step [36, 68] or post-AR refinement [45], significantly improving efficiency in modeling lengthy speech sequences while maintaining sufficient capacity. We demonstrate the effectiveness of our method in zero-shot and streaming speech synthesis, showing its strong potential for application in general-purpose speech language models."
        },
        {
            "title": "2 Maximum Mean Discrepancy and Generalized Energy Distance",
            "content": "Integral probability metrics [46] are types of distance functions between probability distributions over Rn. Let be class of real-valued functions on Rn, integral probability metric is defined as: (1) [Exp(x)[f (x)] Eyq(y)[f (y)]], DF (p, q) = sup 2 where and can be model and data distributions. Due to the richness of function class F, it is often not practical to estimate DF (p, q) with finite samples. Maximum mean discrepancy [MMD; 17] solves this problem by restricting the function class to be the unit ball in reproducing kernel Hilbert space H: MMD(p, q) = sup [Exp(x)[f (x)] Eyq(y)[f (y)]]. (2) H1 Assuming real-valued function k(x, y) is symmetric and positive definite, the kernel defines reproducing kernel Hilbert space such that every critic function can be expressed as (x) = f, k(x, )H. This notion can be extended to the embedding of probability distribution by defining µp such that Exp(x)[f (x)] = f, µpH for all H. Gretton et al. [17] shows that, if the condition for the existence of µp is satisfied, µp = Exp(x)[k(x, )] and MMD can be expressed as the distance between mean embeddings: MMD2 k(p, q) = µp µq = x,xp y,yq [k(x, x) + k(y, y) 2k(x, y)], (3) where x, and y, are independent samples from and q. Müller [46] has shown that DF is pseudometric for any choice of F, which satisfies all the properties of metric1 except that there can be DF (p, q) = 0 when is not identical to q. However, MMD(p, q) is metric when the mean embedding µp is injective [17]. Kernels that satisfy this condition are referred to as characteristic kernels [14]. By selecting characteristic kernel function k(x, y), MMD(p, q) becomes strictly proper scoring rule [15] and can be effectively used to train implicit probabilistic generative models. In an alternative view, Lyons [41] defines generalized energy distance (GED) between two probability distributions on metric spaces (Rn, d) as: GED2 d(p, q) = x,xp y,yq [2d(x, y) d(x, x) d(y, y)]. (4) Several works [15, 41] have pointed out that if the distance function is of negative type or conditionally negative definite, then GED2(p, q) 0, and it forms pseudometric between distributions. Actually, GED is special case of MMD. Sejdinovic et al. [56] shows that any nondegenerate kernel on Rn defines valid semimetric of negative type by:2 d(x, y) = k(x, x) + k(y, y) 2k(x, y), and any semimetric of negative type can be generated by at least one of its induced kernels: k(x, y) = d(x, z) + d(y, z) 2d(x, y), (5) (6) where can be any point on Rn. GED2 associated to kernel that generates [56]. Therefore, one can alternatively use GED as criterion for training implicit generative models, as long as the distance function is chosen appropriately. is equivalent to the MMD"
        },
        {
            "title": "3.1 Language Modeling in Continuous Latent Space",
            "content": "Prior approaches to speech language modeling have commonly depended on an external quantization module, which discretizes speech waveform into sequence of tokens from finite vocabulary for autoregressive modeling in manner analogous to text. The generated speech token sequence can be resynthesized into waveform using either the decoder of the quantization model or an externally trained vocoder [52, 62]. In this research, we explore encoding speech waveforms into sequences of continuous representation and performing autoregressive modeling within this continuous latent space. Given an audio sample RT , we encode it into sequence of continuous representation: = Enc(x) RT fhn, where is the duration of the audio, and and fh denote the sample rates of the audio waveform and the latent sequence. Typically, ranges from hundreds of thousands 1A metric satisfies the following four properties: non-negativity, symmetry, triangle inequality and identity of indiscernibles. 2The definitions of nondegenerate kernels and semimetrics can be found in Sejdinovic et al. [56]. 3 to several hundred thousand, while fh ranges from few to tens. The model aims to capture the distribution of the continuous vector ht, conditioned on all previously generated vectors h<t: p(h) = fh1 (cid:89) t=0 p(hth<t). (7) In the discrete domain, autoregressive models can use softmax function to model the distribution of each step over the vocabulary space. However, autoregressive modeling in the continuous domain is more complex. It requires modeling the distribution in the Rn space for each step, conditioned on the outputs from previous steps: zt = ψ(h<t; θ), ˆht pg(htzt; ϕ) where ψ can be any autoregressive network (e.g., decoder-only Transformer [70]) and can be any conditional generative module on Rn. (8)"
        },
        {
            "title": "3.2 Per-token Generative Modeling via Energy Distance",
            "content": "As discussed earlier, we perform autoregressive modeling in the continuous latent space using conditional generative module per step. The model is responsible for modeling the distribution of the current step ht based on the condition representation zt, which incorporates autoregressive dependencies. Ideally, any conditional generative model can serve as g. However, in the context of speech language modeling, needs to meet certain requirements. Modeling Capability: Considering the diversity of speech audio, requires strong capacity to capture the distribution at each step. Analytically tractable distributions, such as Gaussian distributions, may not have sufficient capability. Sampling Efficiency: Speech audio has high sampling rate. Even though this rate is significantly reduced after encoding into the continuous latent space, the sequences remain lengthy. Given that each step of autoregressive decoding requires sampling operation from g, we aim for to have high sampling efficiency, akin to categorical sampling over vocabulary. If we model using methods such as diffusion [23], which require multiple iterations during sampling, it could significantly increase the latency of autoregressive decoding. Training Stability: Similar to training discrete autoregressive models using cross-entropy loss, we aim to train both the conditional generative module and the main autoregressive network ψ simultaneously using simple and stable training algorithm. Under such constraints, the GAN-style training paradigm [16] may not be appropriate. Based on the above analysis, we construct as lightweight multi-layer perceptron that takes the condition vector zt and noise vector ϵ as inputs, mapping them to the continuous latent space of speech, Rn: ht = g(zt, ϵ; ϕ). (9) This implicitly defines per-step distribution pg(htzt) over Rn. The sampling process involves simply sampling noise vector ϵt and passing it through along with the condition zt. In the network g, we use the AdaLN module [51] to integrate noise into the hidden state of the condition vector zt to introduce randomness. Rather than directly learning dimension-wise scale and shift parameters in the layer normalization module for zt, we treat them as random perturbations for sampling [59]. The scale and shift values are predicted by applying linear transformation to the input noise ϵt. To train the implicit conditional generative module and the autoregressive network ψ simultaneously, we use specialized form of maximum mean discrepancy, namely energy distance, as the metric between the distributions of the data and model. The training is performed by minimizing the energy distance between the simulated and the ground truth latent representation per step, with respect to the parameters of both and ψ. Since the data distribution remains fixed during optimization, terms in Eq. 4 that do not depend on the model parameters can be discarded, reducing the training loss to: LGED = (cid:80) Eht,h [2d(ht, ) d(ht, t)], (10) where is the target latent waveform representation, and ht, are independent samples from pg(htzt). As discussed in Sec. 2, LGED is strictly proper scoring rule, provided that the kernel function corresponding to the distance function is characteristic. [15, 65] suggests that d(x, y) = yβ 2 satisfies the condition when β (0, 2). In our experiments, we set β = 1: LGED = (cid:80) Eht,h [2ht 1 2 ht t1 2], (11) It is worth noting that Eq. 11 is very similar to the root mean squared error, with the addition of the repulsive term E[ht 2]. This repulsive term is crucial for making the loss proper learning objective. In contrast, RMSE is only regression loss, which does not well capture the differences between the distributions. Using RMSE loss as the objective can cause the model to fail to fit the data distribution [18]. We demonstrate this in Sec. 6.1. t1 SLED performs autoregressive modeling in the continuous latent space, thus it cannot determine when to stop by predicting the EOS token. To address this issue, we adopt the approach from [45] and introduce binary classification head to decide whether the output should terminate. We directly project the output of the autoregressive network zt to scalar, followed by sigmoid function, and interpret the result as the probability of whether the output should stop at this step."
        },
        {
            "title": "3.3 Classifier-free Guidance",
            "content": "Continuous latent generation exhibits higher noise levels than its discrete counterpart. To improve the generation quality and the alignment with prompts, we adopt the classifier-free guidance technique [CFG; 22, 59] during inference. At each step of autoregressive generation, we perform an additional forward pass through ψ, during which the prompt text is masked out, to obtain t. We then linearly interpolate zt with and use the result as input to the per-token generative module g: t), hcfg + λ(zt = g(zcfg zcfg = , ϵ; ϕ), (12) where λ is the hyperparameter to control the strength of the guidance and it reverts to naïve conditional generation when λ = 1.0. During training, we randomly mask out the text prompt with probability of 0.1. The EOT token is preserved during masking, as it also serves as the token indicating the beginning of the speech."
        },
        {
            "title": "3.4 Streaming Inference",
            "content": "One appealing feature of our approach is that it is purely autoregressive model and does not require any post-processing module to refine the outputs at earlier positions after the entire autoregressive generation is complete. This desirable property allows our autoregressive modeling approach to support streaming generation [42, 8, 77, 43]. Here, streaming generation has two levels of meaning: 1) After each autoregressive step generates latent vector, it can immediately be used for waveform synthesis. This capability relies on the models autoregressive generation not requiring any postprocessing steps, and the decoder that converts latent vectors to waveforms supporting streaming. 2) Building on this, we aim for the model to begin generating even before the full prompt text is provided. This incremental generation feature is particularly useful to reduce the response latency of GPT-4o-like speech interaction system [12, 73, 3, 75, 13], where the model serves as TTS module following text LLM. Figure 2: Illustration of our streaming inference mechanism. Text and speech tokens are interleaved based on predefined ratio, and the loss is computed only at positions where targets are shown. Modern audio codecs typically have streaming decoder [9], so the first level of streaming generation is naturally supported. To achieve incremental speech synthesis, we alter the order of the text and speech positions within the sequence during autoregressive modeling [8]. As illustrated in Figure 2, we interleave text and speech positions in an : ratio. This design allows the generation of speech vectors for every text tokens received, enabling incremental text-to-speech synthesis. An EOT token is used to indicate the end of the input text stream. Afterwards, the speech vectors are generated in the standard autoregressive way until stop decision is made by the binary classification head. 5 We train the model to generate target vectors and predict whether to stop only at the input positions that require generation during inference. We do not require the model to predict any filling tokens when the next step is text token according to the interleaving ratio. We also adopt the classifier-free guidance outlined in Sec. 3.3 in streaming inference. The model performs unconditional speech generation when the text stream requires masking, with EOT serving as the beginning."
        },
        {
            "title": "4 Related Work",
            "content": "Given great success of autoregressive language modeling approach in text generation, researchers have started exploring whether speech sequences can be modeled in similar way. These studies begin with speech synthesis as foundational task [32, 1, 71], eventually extending to the development of generalpurpose speech language models [47, 83, 10, 81, 37, 30]. [32] was the first to propose discretizing self-supervised representations of speech as semantic tokens and to perform autoregressive speech generation. The method of learning semantic tokens has since evolved to quantize ASR-supervised representations [7, 8]. Due to information loss, waveforms reconstructed from coarse semantic tokens are often unsatisfactory, necessitating modeling with acoustic tokens produced by reconstructionoriented audio codecs [80, 9, 31, 84, 78, 50, 25]. Since the acoustic tokens generated by codecs often consist of multiple sequences from residual quantization, hierarchical architecture is required for autoregressive speech modeling. [71] utilize an autoregressive model to generate the sequence of tokens from the first codebook, followed by non-autoregressive model to generate tokens from the residual codebooks. To mitigate the latency introduced by post-AR refinement, [76, 86] apply the RQ-Transformer [34], which uses smaller nested Transformer to model tokens from multiple codebooks within single autoregressive step. Nonetheless, modeling multi-stream acoustic tokens introduces additional complexity. Inspired by advancements in image generation [67, 36], researchers have begun exploring performing speech language modeling in the continuous latent space. [45] use mel-spectrograms as the latent representation and model the per-step distribution with regression loss. However, using regression loss fails to capture the underlying distribution, requiring post-AR refinement network and handcrafted repetition loss. [68] adopt per-token diffusion loss [36], which requires iterative sampling for each autoregressive step, leading to significant inference latency. [72] models each step via flow matching [39], which also encounters similar problems. [85, 38] models the data at each autoregressive step with Gaussian distribution or GMM and employs VAE to obtain the target distribution parameters for each step. This approach imposes overly strong constraints on the latent space, limiting the models expressive capacity."
        },
        {
            "title": "5.1 Training Datasets",
            "content": "We train SLED on the large-scale Libriheavy [28] dataset. LibriHeavy contains approximately 50,000 hours of speech from 6,736 speakers, deriving from audiobooks from the LibriVox project. BPE tokenizer [57] with vocabulary size of 16,384 is applied for text."
        },
        {
            "title": "5.2 Experimental Settings",
            "content": "Continuous Representation We employ Encodec [9] to extract continuous latent representations. For each frame, we sum the multi-stream token embeddingsdrawn from eight codebooksto form its continuous vector, ensuring minimal information loss. Although plain autoencoder can produce continuous latents, we find that enforcing quantization-regularized or KLregularized latent space substantially benefits downstream language modeling. This finding is in line with the standard practice in latent diffusion models [55]. Considering the representative discrete modeling approach VALL-E [71] also employs Encodec as its tokenizer, this setup also enables fair comparison between continuous and discrete autoregressive modeling while keeping the latent encoder consistent. The resulting continuous vector sequences have sampling rate of 75Hz and latent dimensionality of 128. Model Configurations The autoregressive network incorporates 12 Transformer layers [70]. Instead of using standard Transformer layers, we employ LLaMA [66] layers, which apply pre-normalization using RMSNorm [82], SwiGLU activation function [61], and rotary positional embeddings [64]. 6 Table 1: Performance comparison on 3s Prefix as Prompt and Reference Utterance as Prompt zero-shot speech synthesis tasks. WER-C (%) represents the results using the Conformer-Transducer, whereas WER-H (%) indicates the results with the HuBERT-Large. *: WER obtained by Whisper. System #Params 3s Prefix as Prompt Reference Utterance as Prompt Ground Truth Ground Truth (Encodec) - - 1.78 1.79 2.15 2. 0.668 0.606 1.78 1.79 2.15 2.30 WER-C WER-H SIM WER-C WER-H MaskGCT [74] F5-TTS [6] MegaTTS 3 [26] VALL-E [71] VALL-E 2 [5] ELLA-V [63] VALL-E [20] Llasa [79] CLaM-TTS [29] MELLE [45] FELLE [72] SLED Traditional TTS - - - - - - - - - Discrete-LM-based Approaches - 1.6 2.10 1.58 - 3.8 2.32 2.91 2.32 1.49 0.508 0.529 0.340 0.397 0.740 Continuous-LM-based Approaches - 1.47 1.53 1.59 2.36 1.98 2.27 1.99 0.513 0.539 0.539 0. 1.0B 0.3B 0.3B 0.4B 0.4B 0.4B 0.4B 8B - 0.2B 0.2B 0.2B - - - - 1.5 7.15 3.18 - - 1.47 2.20 1.51 2.63 2.42* 1.82 5.9 2.44 8.90 3.97 - 5.11 2.10 2.89 1. SIM 0.778 0.738 0.687 0.66 0.78 0.580 0.678 0.331 0.395 - 0.538 0.664 0.654 0. Table 2: Performance comparison of Offline and Streaming Inference. Mode : WER-C DNSMOS GT Offline with Prompt Streaming Streaming 5:20 5:45 1.79 1.67 1.51 2.18 2. 3.89 3.58 3.61 3.59 3.54 Table 3: Performance comparison using Energy Distance vs. Mean Squared Error as the objective in 3s Prefix as Prompt experiments. Objective WER-C Root Mean Squared Error Energy Distance 40.60 1. Each layer has 16 attention heads and an embedding dimension of 1,024. We set the FFN hidden dimension to 2,752 to match the number of parameters of standard Transformer with an FFN hidden dimension of 4,096. The dropout rate is set at 0.1. The input latent continuous vectors are projected to the embedding dimensionality using linear layer, followed by layer normalization to ensure stability. The lightweight conditional generative module consists of six residual blocks, each featuring an AdaLN module [51] to modulate the hidden states with noise, followed by two-layer MLP and residual connections. The hidden dimensionality of this generative module is set to 1024, and linear layer is used at the top to project the output back to the target dimensionality. The default value of λ in classifier-free guidance is set to 2.0. Training Details We train the model with batch size of 512 for 300,000 steps using BF16. We optimize the model with AdamW [40], configured with learning rate of 5e-4, weight decay of 0.01, β1 = 0.9, β2 = 0.999 and ϵ = 1 108. The learning rate follows linear decay, warming up to its peak value during the first 32,000 steps. maximum gradient norm clip of 1.0 is applied."
        },
        {
            "title": "5.3 Evaluation Settings",
            "content": "We evaluate zero-shot speech synthesis performance using LibriSpeech test-clean set, ensuring that none of the test speakers are included in the training data. Following [71, 45], we use samples with durations between 4 and 10 seconds, resulting in 2.2-hour subset comprising 1,234 samples and 40 unique speakers. Since the LibriSpeech consists of case-insensitive text without punctuation, while the expected input for models trained on LibriHeavy is unnormalized, we use the test-clean set of 7 (a) WER-C and SIM against CFG in 3s Prefix as Prompt. (b) WER-C and SIM against CFG in Ref Utterance as Prompt. (c) WER-C and DNSMOS against CFG in Streaming Inference. Figure 3: Evaluation of classifier-free guidance effects across generation settings. LibriSpeech-PC [44] to evaluate SLED. The LibriSpeech-PC is nearly identical to LibriSpeech, but its text has been restored to include capitalization and punctuation.3 The evaluation is conducted under two settings: 1) 3s Prefix as Prompt: The model is provided with the text transcription and the first 3 seconds of the utterance as the prompt, and it is expected to perform speech continuation. 2) Reference Utterance as Prompt: The model is given reference utterance from the same speaker along with its transcription as the prompt. Using the text of the target utterance, the model is expected to synthesize speech that retains the characteristics of the reference speaker. For this experiment, we follow the configuration outlined in [5]. We first filter the samples in LibriSpeech test-clean set based on length and order them by sample ID. For each speaker, the i-th speech sample is synthesized using the (i 1)-th sample as the prompt, while the first sample is synthesized using the last sample from the same speaker as the prompt. For Offline and Streaming Inference experiments, we follow previous data settings and no prompt speech is provided. We primarily use the following metrics to assess the intelligibility and in-context learning capability. Word Error Rate (WER) To evaluate the intelligibility of the synthesized speech, we perform speech recognition on the generated audio and calculate the WER against the target text. We utilize two ASR models: Conformer-Transducer [19] and CTC-ASR-tuned HuBERT-Large [24]. Speaker Similarity (SIM) We evaluate the in-context learning capability by measuring speaker similarity between the reference and the generated speech. We extract speaker embeddings using WavLM-TDNN [4] and compute their cosine similarity. Predicted Mean Opinion Score (DNSMOS) We evaluate the overall quality of the generated speech using the DNSMOS score [53, 54], which ranges from 1 to 5, with higher values indicating better quality. We use model trained with ground truth human ratings obtained using ITU-T P.808."
        },
        {
            "title": "5.4 Main Results",
            "content": "Zero-shot TTS Table 1 presents the results of zero-shot TTS. In both the speech continuation and reference utterance prompting tasks, SLED achieves word accuracy surpassing the ground truth (1.59/1.51 vs. 1.78 in WER-C), demonstrating its modeling ability. Interestingly, we found that using reference utterance as prompt results in more accurate synthesis compared to using the prefix, showing the ability of in-context learning. Comparisons of SLED performance across different data scales are provided in App. A. We found that 1000 hours of speech is sufficient to achieve most of the generation and in-context learning capabilities. Further scaling the training data to 50,000 hours enhances these abilities even more. Notably, comparison between SLED and VALL-E highlights key differences in autoregressive modeling approaches. Both models use Encodec as the latent encoder, but SLED performs modeling in the continuous domain, whereas VALL-E operates in the discrete domain. VALL-E-like hierarchical models require an extra non-autoregressive local model (159M) to predict residual tokens. In contrast, SLED relies on lightweight generative module (35M) for continuous vector generation, achieving better parameter efficiency. Despite this, SLED outperforms VALL-E on all metrics. These results show the superiority of our continuous speech language modeling approach. Compared with other continuous approaches, SLED achieves better or similar quality while enjoys the advantage in generation efficiency. MELLE incorporates 3The test-clean set of Librispeech-PC excludes small number of low-quality samples (203 out of 2,620) from the original Librispeech test-clean set, thus the test subset after applying the same protocol contains only 1,154 samples. 8 an NAR refinement, whereas SLED is purely autoregressive and can generate in streaming manner. FELLE must perform multi-step integration iterations along predefined ODE path at each decoding step, while SLED only requires single forward pass through its generative module. Moreover, at roughly the same parameter scale, current speech language modeling approaches still exhibit significant gap in voice cloning compared with traditional TTS models. However, Llasa [79] shows that scaling up can dramatically strengthen discrete autoregressive speech models, which inspires us to further explore the scaling capabilities of continuous autoregressive modeling approaches."
        },
        {
            "title": "Streaming Inference",
            "content": "Table 2 presents comparison of accuracy (WER-C) and perceived quality (DNSMOS) between speech produced via streaming inference and that generated offline. We vary the interleaving ratio between text and speech positions to analyze the impact of streaming policies. In our experiments, we compare two configurations: one generates 20 speech vectors (0.27 seconds of speech) for every 5 subwords received, while the other generates 45 speech vectors (0.6 seconds of speech) per 5 subwords. The latter configuration aligns with [8], which guarantees seamless streaming generation when serving as the speech synthesis module of cascaded speech-LLM system. We find that the two interleaving ratio configurations yield comparable results in both word accuracy and perceived quality. Moreover, the synthesized quality in streaming mode closely matches that of offline synthesis (3.59/3.54 vs. 3.58 in DNSMOS), despite slight drop in word accuracy (2.18/2.20 vs. 1.67 in WER-C), demonstrating the effectiveness of SLED for streaming tasks."
        },
        {
            "title": "6 Analysis",
            "content": "6."
        },
        {
            "title": "Importance of Energy Distance",
            "content": "As discussed in Section 3.2, the learning objective of SLED (Eq. 11) closely resembles root mean squared error. By removing the repulsive term E[ht 2], the energy distance becomes equivalent to the RMSE, with random noise acting as perturbation (which is the L2 part of the regression loss in [45]). Despite their apparent similarity, these two objectives exhibit distinct statistical properties. Energy distance measures the distributional difference, guiding the model to capture the underlying data distribution. In contrast, RMSE is only regression loss. The importance of the repulsive term is demonstrated in Table 3. Removing this term results in model failure, leading to significantly worse word error rate. Listening to the generated samples, we observed that the model trained without the repulsive term failed to generate speech in most cases. t1 This makes us curious why MELLE [45] achieves strong performance using regression loss. We note that [45] introduces flux loss to suppress repetition, which measures the variation of the generated spectral sequence over time by taking the norm of the Mel-spectrogram differences between adjacent frames. [45] reports that generation quality degrades severely without it. Although [45] frames the flux loss as heuristic designed to curb repetition, we argue that its real efficacy stems from approximating the repulsive term in the energy distance. Because consecutive frames in speech differ only slightly, they can be approximately regarded as two samples from the same time step. Under this view, MELLEs flux loss causes its training objective to approximate the energy distance, thereby enabling it to capture distributional differences to some extent."
        },
        {
            "title": "6.2 Effects of Classifier-free Guidance",
            "content": "We investigate the impact of classifier-free guidance (CFG) across generation settings. As shown in Figure 3, without CFG (λ = 1.0 in Eq. 12), the synthesized speech exhibits poor word accuracy, with particularly high WER-C of 6.01 in streaming inference. However, upon applying CFG, the WER-C values decrease dramatically. λ value of 1.5 quickly results in substantial improvement, bringing the WER-C down to approximately 2.0 across all settings. Further tuning of λ leads to even better word accuracy. However, in terms of timbre cloning and perceived speech quality, moderate CFG setting (λ = 1.5) yields the most improvements. Further increasing λ leads to performance degradation. Notably, speech quality in streaming inference declines to level worse than that generated without CFG at λ = 2.5. This experiment demonstrates that classifier-free guidance significantly improves alignment between the text and synthesized speech while enhancing speech quality only with moderate λ value. We recommend setting λ = 2.0 by default to achieve balance between word accuracy and speech quality across all generation settings."
        },
        {
            "title": "7 Conclusion and Limitation",
            "content": "In this work, we propose method for speech language modeling in continuous latent space using energy distance. This approach avoids the complex hierarchical architecture of traditional discrete models, while also preserving rich information in speech. This work has validated the effectiveness of this modeling approach in the speech synthesis task. In the future, we plan to extend it to generalpurpose speech language models. Moreover, the latent encoder employed in the current work was originally designed solely for the audio codec; additionally training continuous latent encoder specifically for this method should further enhance its performance."
        },
        {
            "title": "References",
            "content": "[1] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi, and N. Zeghidour. Audiolm: language modeling approach to audio generation, 2023. URL https://arxiv.org/abs/2209.03143. [2] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165. [3] Q. Chen, Y. Chen, Y. Chen, M. Chen, Y. Chen, C. Deng, Z. Du, R. Gao, C. Gao, Z. Gao, Y. Li, X. Lv, J. Liu, H. Luo, B. Ma, C. Ni, X. Shi, J. Tang, H. Wang, H. Wang, W. Wang, Y. Wang, Y. Xu, F. Yu, Z. Yan, Y. Yang, B. Yang, X. Yang, G. Yang, T. Zhao, Q. Zhang, S. Zhang, N. Zhao, P. Zhang, C. Zhang, and J. Zhou. Minmo: multimodal large language model for seamless voice interaction, 2025. URL https://arxiv.org/abs/2501.06282. [4] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao, J. Wu, L. Zhou, S. Ren, Y. Qian, Y. Qian, J. Wu, M. Zeng, X. Yu, and F. Wei. Wavlm: Large-scale selfIEEE Journal of Selected Topics in Signal supervised pre-training for full stack speech processing. Processing, 16(6):15051518, Oct. 2022. ISSN 1941-0484. doi: 10.1109/jstsp.2022.3188113. URL http://dx.doi.org/10.1109/JSTSP.2022.3188113. [5] S. Chen, S. Liu, L. Zhou, Y. Liu, X. Tan, J. Li, S. Zhao, Y. Qian, and F. Wei. Vall-e 2: Neural codec language models are human parity zero-shot text to speech synthesizers, 2024. URL https://arxiv. org/abs/2406.05370. [6] Y. Chen, Z. Niu, Z. Ma, K. Deng, C. Wang, J. Zhao, K. Yu, and X. Chen. F5-tts: fairytaler that fakes fluent and faithful speech with flow matching, 2024. URL https://arxiv.org/abs/2410.06885. [7] Z. Du, Q. Chen, S. Zhang, K. Hu, H. Lu, Y. Yang, H. Hu, S. Zheng, Y. Gu, Z. Ma, Z. Gao, and Z. Yan. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens, 2024. URL https://arxiv.org/abs/2407.05407. [8] Z. Du, Y. Wang, Q. Chen, X. Shi, X. Lv, T. Zhao, Z. Gao, Y. Yang, C. Gao, H. Wang, F. Yu, H. Liu, Z. Sheng, Y. Gu, C. Deng, W. Wang, S. Zhang, Z. Yan, and J. Zhou. Cosyvoice 2: Scalable streaming speech synthesis with large language models, 2024. URL https://arxiv.org/abs/2412.10117. [9] A. Défossez, J. Copet, G. Synnaeve, and Y. Adi. High fidelity neural audio compression, 2022. URL https://arxiv.org/abs/2210.13438. [10] A. Défossez, L. Mazaré, M. Orsini, A. Royer, P. Pérez, H. Jégou, E. Grave, and N. Zeghidour. Moshi: speech-text foundation model for real-time dialogue, 2024. URL https://arxiv.org/abs/2410. 00037. [11] S. E. Eskimez, X. Wang, M. Thakker, C. Li, C.-H. Tsai, Z. Xiao, H. Yang, Z. Zhu, M. Tang, X. Tan, Y. Liu, S. Zhao, and N. Kanda. E2 tts: Embarrassingly easy fully non-autoregressive zero-shot tts, 2024. URL https://arxiv.org/abs/2406.18009. [12] Q. Fang, S. Guo, Y. Zhou, Z. Ma, S. Zhang, and Y. Feng. Llama-omni: Seamless speech interaction with large language models, 2025. URL https://arxiv.org/abs/2409.06666. [13] Q. Fang, Y. Zhou, S. Guo, S. Zhang, and Y. Feng. Llama-omni2: Llm-based real-time spoken chatbot with autoregressive streaming speech synthesis, 2025. URL https://arxiv.org/abs/2505.02625. 10 [14] K. Fukumizu, A. Gretton, X. Sun, and B. Schölkopf. Kernel measures of conditional dependence. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2007. URL https://proceedings.neurips.cc/paper_files/ paper/2007/file/3a0772443a0739141292a5429b952fe6-Paper.pdf. [15] T. Gneiting and A. E. Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359378, 2007. doi: 10.1198/016214506000001437. [16] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial networks, 2014. URL https://arxiv.org/abs/1406.2661. [17] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. Smola. kernel two-sample test. Journal of Machine Learning Research, 13(25):723773, 2012. URL http://jmlr.org/papers/v13/ gretton12a.html. [18] A. A. Gritsenko, T. Salimans, R. van den Berg, J. Snoek, and N. Kalchbrenner. spectral energy distance for parallel speech synthesis, 2020. URL https://arxiv.org/abs/2008.01160. [19] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang. Conformer: Convolution-augmented transformer for speech recognition, 2020. URL https: //arxiv.org/abs/2005.08100. [20] B. Han, L. Zhou, S. Liu, S. Chen, L. Meng, Y. Qian, Y. Liu, S. Zhao, J. Li, and F. Wei. Vall-e r: Robust and efficient zero-shot text-to-speech synthesis via monotonic alignment, 2024. URL https: //arxiv.org/abs/2406.07855. [21] J. Helcl, B. Haddow, and A. Birch. Non-autoregressive machine translation: Its not as fast as it seems, 2022. URL https://arxiv.org/abs/2205.01966. [22] J. Ho and T. Salimans. Classifier-free diffusion guidance, 2022. URL https://arxiv.org/abs/2207. 12598. [23] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models, 2020. URL https://arxiv. org/abs/2006.11239. [24] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed. Hubert: Selfsupervised speech representation learning by masked prediction of hidden units, 2021. URL https: //arxiv.org/abs/2106.07447. [25] S. Ji, Z. Jiang, W. Wang, Y. Chen, M. Fang, J. Zuo, Q. Yang, X. Cheng, Z. Wang, R. Li, Z. Zhang, X. Yang, R. Huang, Y. Jiang, Q. Chen, S. Zheng, and Z. Zhao. Wavtokenizer: an efficient acoustic discrete codec tokenizer for audio language modeling, 2025. URL https://arxiv.org/abs/2408.16532. [26] Z. Jiang, Y. Ren, R. Li, S. Ji, B. Zhang, Z. Ye, C. Zhang, B. Jionghao, X. Yang, J. Zuo, Y. Zhang, R. Liu, X. Yin, and Z. Zhao. Megatts 3: Sparse alignment enhanced latent diffusion transformer for zero-shot speech synthesis, 2025. URL https://arxiv.org/abs/2502.18924. [27] Z. Ju, Y. Wang, K. Shen, X. Tan, D. Xin, D. Yang, Y. Liu, Y. Leng, K. Song, S. Tang, Z. Wu, T. Qin, X.-Y. Li, W. Ye, S. Zhang, J. Bian, L. He, J. Li, and S. Zhao. Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models, 2024. URL https://arxiv.org/abs/2403.03100. [28] W. Kang, X. Yang, Z. Yao, F. Kuang, Y. Yang, L. Guo, L. Lin, and D. Povey. Libriheavy: 50,000 hours asr corpus with punctuation casing and context. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1099110995. IEEE, 2024. [29] J. Kim, K. Lee, S. Chung, and J. Cho. Clam-tts: Improving neural codec language model for zero-shot text-to-speech, 2024. URL https://arxiv.org/abs/2404.02781. [30] KimiTeam, D. Ding, Z. Ju, Y. Leng, S. Liu, T. Liu, Z. Shang, K. Shen, W. Song, X. Tan, H. Tang, Z. Wang, C. Wei, Y. Xin, X. Xu, J. Yu, Y. Zhang, X. Zhou, Y. Charles, J. Chen, Y. Chen, Y. Du, W. He, Z. Hu, G. Lai, Q. Li, Y. Liu, W. Sun, J. Wang, Y. Wang, Y. Wu, Y. Wu, D. Yang, H. Yang, Y. Yang, Z. Yang, A. Yin, R. Yuan, Y. Zhang, and Z. Zhou. Kimi-audio technical report, 2025. URL https://arxiv.org/abs/2504.18425. [31] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and K. Kumar. High-fidelity audio compression with improved rvqgan, 2023. URL https://arxiv.org/abs/2306.06546. [32] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed, and E. Dupoux. Generative spoken language modeling from raw audio, 2021. URL https://arxiv.org/abs/2102.01192. 11 [33] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi, J. Mahadeokar, and W.-N. Hsu. Voicebox: Text-guided multilingual universal speech generation at scale, 2023. URL https://arxiv.org/abs/2306.15687. [34] D. Lee, C. Kim, S. Kim, M. Cho, and W.-S. Han. Autoregressive image generation using residual quantization, 2022. URL https://arxiv.org/abs/2203.01941. [35] K. Lee, D. W. Kim, J. Kim, S. Chung, and J. Cho. Ditto-tts: Diffusion transformers for scalable text-tospeech without domain-specific factors, 2025. URL https://arxiv.org/abs/2406.11427. [36] T. Li, Y. Tian, H. Li, M. Deng, and K. He. Autoregressive image generation without vector quantization, 2024. URL https://arxiv.org/abs/2406.11838. [37] T. Li, J. Liu, T. Zhang, Y. Fang, D. Pan, M. Wang, Z. Liang, Z. Li, M. Lin, G. Dong, J. Xu, H. Sun, Z. Zhou, and W. Chen. Baichuan-audio: unified framework for end-to-end speech interaction, 2025. URL https://arxiv.org/abs/2502.17239. [38] W. Lin and C. He. Continuous autoregressive modeling with stochastic monotonic alignment for speech synthesis, 2025. URL https://arxiv.org/abs/2502.01084. [39] Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling, 2023. URL https://arxiv.org/abs/2210.02747. [40] I. Loshchilov and F. Hutter. Decoupled weight decay regularization, 2019. URL https://arxiv.org/ abs/1711.05101. [41] R. Lyons. Distance covariance in metric spaces. The Annals of Probability, 41(5), Sept. 2013. ISSN 0091-1798. doi: 10.1214/12-aop803. URL http://dx.doi.org/10.1214/12-AOP803. [42] Z. Ma, S. Zhang, S. Guo, C. Shao, M. Zhang, and Y. Feng. Non-autoregressive streaming transformer for simultaneous translation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 51775190. Association for Computational Linguistics, 2023. URL https: //aclanthology.org/2023.emnlp-main.314/. [43] Z. Ma, Y. Feng, and M. Zhang. Overcoming non-monotonicity in transducer-based streaming generation. In Proceedings of the 42nd International Conference on Machine Learning, 2025. URL https://arxiv. org/abs/2411.17170. [44] A. Meister, M. Novikov, N. Karpov, E. Bakhturina, V. Lavrukhin, and B. Ginsburg. Librispeech-pc: Benchmark for evaluation of punctuation and capitalization capabilities of end-to-end asr models. arXiv preprint arXiv:2310.02943, 2023. [45] L. Meng, L. Zhou, S. Liu, S. Chen, B. Han, S. Hu, Y. Liu, J. Li, S. Zhao, X. Wu, H. Meng, and F. Wei. Autoregressive speech synthesis without vector quantization, 2024. URL https://arxiv.org/abs/ 2407.08551. [46] A. Müller. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29(2):429443, 1997. doi: 10.2307/1428011. [47] T. A. Nguyen, E. Kharitonov, J. Copet, Y. Adi, W.-N. Hsu, A. Elkahky, P. Tomasello, R. Algayres, B. Sagot, A. Mohamed, and E. Dupoux. Generative spoken dialogue language modeling, 2022. URL https://arxiv.org/abs/2203.16502. [48] T. A. Nguyen, B. Muller, B. Yu, M. R. Costa-jussa, M. Elbayad, S. Popuri, C. Ropers, P.-A. Duquenne, R. Algayres, R. Mavlyutov, I. Gat, M. Williamson, G. Synnaeve, J. Pino, B. Sagot, and E. Dupoux. Spirit lm: Interleaved spoken and written language model, 2024. URL https://arxiv.org/abs/2402.05755. [49] OpenAI. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. [50] J. D. Parker, A. Smirnov, J. Pons, C. Carr, Z. Zukowski, Z. Evans, and X. Liu. Scaling transformers for low-bitrate high-quality speech coding, 2024. URL https://arxiv.org/abs/2411.19842. [51] W. Peebles and S. Xie. Scalable diffusion models with transformers, 2023. URL https://arxiv.org/ abs/2212.09748. [52] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux. Speech resynthesis from discrete disentangled self-supervised representations, 2021. URL https://arxiv.org/ abs/2104.00355. 12 [53] C. K. Reddy, V. Gopal, and R. Cutler. Dnsmos: non-intrusive perceptual objective speech quality metric to evaluate noise suppressors. In ICASSP 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 64936497. IEEE, 2021. [54] C. K. Reddy, V. Gopal, and R. Cutler. Dnsmos p.835: non-intrusive perceptual objective speech quality metric to evaluate noise suppressors. In ICASSP 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022. [55] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models, 2022. URL https://arxiv.org/abs/2112.10752. [56] D. Sejdinovic, B. Sriperumbudur, A. Gretton, and K. Fukumizu. Equivalence of distance-based and rkhs-based statistics in hypothesis testing. The Annals of Statistics, 41(5), Oct. 2013. ISSN 0090-5364. doi: 10.1214/13-aos1140. URL http://dx.doi.org/10.1214/13-AOS1140. [57] R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword units, 2016. URL https://arxiv.org/abs/1508.07909. [58] C. Shao, F. Meng, Y. Liu, and J. Zhou. Language generation with strictly proper scoring rules. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 4447444488. PMLR, 2127 Jul 2024. URL https://proceedings. mlr.press/v235/shao24c.html. [59] C. Shao, F. Meng, and J. Zhou. Continuous visual autoregressive generation via score maximization, 2025. URL https://arxiv.org/abs/2505.07812. [60] S. S. Shapiro and M. B. Wilk. An analysis of variance test for normality (complete samples). Biometrika, 52(3-4):591611, 1965. [61] N. Shazeer. Glu variants improve transformer, 2020. URL https://arxiv.org/abs/2002.05202. [62] H. Siuzdak. Vocos: Closing the gap between time-domain and fourier-based neural vocoders for highquality audio synthesis, 2024. URL https://arxiv.org/abs/2306.00814. [63] Y. Song, Z. Chen, X. Wang, Z. Ma, and X. Chen. Ella-v: Stable neural codec language modeling with alignment-guided sequence reordering, 2024. URL https://arxiv.org/abs/2401.07333. [64] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/2104.09864. [65] G. J. Székely and M. L. Rizzo. Energy statistics: class of statistics based on distances. Journal of statistical planning and inference, 143(8):12491272, 2013. [66] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971. [67] M. Tschannen, C. Eastwood, and F. Mentzer. Givt: Generative infinite-vocabulary transformers, 2024. URL https://arxiv.org/abs/2312.02116. [68] A. Turetzky, N. Shabtay, S. Shechtman, H. Aronowitz, D. Haws, R. Hoory, and A. Dekel. Continuous speech synthesis using per-token latent diffusion, 2024. URL https://arxiv.org/abs/2410.16048. [69] A. van den Oord, O. Vinyals, and K. Kavukcuoglu. Neural discrete representation learning, 2017. URL https://arxiv.org/abs/1711.00937. [70] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/abs/1706.03762. [71] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao, and F. Wei. Neural codec language models are zero-shot text to speech synthesizers, 2023. URL https://arxiv.org/abs/2301.02111. [72] H. Wang, S. Liu, L. Meng, J. Li, Y. Yang, S. Zhao, H. Sun, Y. Liu, H. Sun, J. Zhou, Y. Lu, and Y. Qin. Felle: Autoregressive speech synthesis with token-wise coarse-to-fine flow matching, 2025. URL https://arxiv.org/abs/2502.11128. [73] X. Wang, Y. Li, C. Fu, Y. Shen, L. Xie, K. Li, X. Sun, and L. Ma. Freeze-omni: smart and low latency speech-to-speech dialogue model with frozen llm, 2024. URL https://arxiv.org/abs/2411.00774. [74] Y. Wang, H. Zhan, L. Liu, R. Zeng, H. Guo, J. Zheng, Q. Zhang, X. Zhang, S. Zhang, and Z. Wu. Maskgct: Zero-shot text-to-speech with masked generative codec transformer, 2024. URL https://arxiv.org/ abs/2409.00750. [75] J. Xu, Z. Guo, J. He, H. Hu, T. He, S. Bai, K. Chen, J. Wang, Y. Fan, K. Dang, B. Zhang, X. Wang, Y. Chu, and J. Lin. Qwen2.5-omni technical report, 2025. URL https://arxiv.org/abs/2503.20215. [76] D. Yang, J. Tian, X. Tan, R. Huang, S. Liu, X. Chang, J. Shi, S. Zhao, J. Bian, Z. Zhao, X. Wu, and H. Meng. Uniaudio: An audio foundation model toward universal audio generation, 2024. URL https://arxiv.org/abs/2310.00704. [77] Y. Yang, Z. Ma, S. Liu, J. Li, H. Wang, L. Meng, H. Sun, Y. Liang, R. Xu, Y. Hu, Y. Lu, R. Zhao, and X. Chen. Interleaved speech-text language models are simple streaming text to speech synthesizers, 2024. URL https://arxiv.org/abs/2412.16102. [78] Z. Ye, P. Sun, J. Lei, H. Lin, X. Tan, Z. Dai, Q. Kong, J. Chen, J. Pan, Q. Liu, Y. Guo, and W. Xue. Codec does matter: Exploring the semantic shortcoming of codec for audio language model, 2024. URL https://arxiv.org/abs/2408.17175. [79] Z. Ye, X. Zhu, C.-M. Chan, X. Wang, X. Tan, J. Lei, Y. Peng, H. Liu, Y. Jin, Z. Dai, H. Lin, J. Chen, X. Du, L. Xue, Y. Chen, Z. Li, L. Xie, Q. Kong, Y. Guo, and W. Xue. Llasa: Scaling train-time and inference-time compute for llama-based speech synthesis, 2025. URL https://arxiv.org/abs/2502.04128. [80] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi. Soundstream: An end-to-end neural audio codec, 2021. URL https://arxiv.org/abs/2107.03312. [81] A. Zeng, Z. Du, M. Liu, L. Zhang, shengmin jiang, Y. Dong, and J. Tang. Scaling speech-text pre-training with synthetic interleaved data. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=3tukjsVyrE. [82] B. Zhang and R. Sennrich. Root mean square layer normalization, 2019. URL https://arxiv.org/ abs/1910.07467. [83] D. Zhang, S. Li, X. Zhang, J. Zhan, P. Wang, Y. Zhou, and X. Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities, 2023. URL https://arxiv.org/abs/2305. 11000. [84] X. Zhang, D. Zhang, S. Li, Y. Zhou, and X. Qiu. Speechtokenizer: Unified speech tokenizer for speech large language models, 2024. URL https://arxiv.org/abs/2308.16692. [85] X. Zhu, W. Tian, and L. Xie. Autoregressive speech synthesis with next-distribution prediction, 2024. URL https://arxiv.org/abs/2412.16846. [86] Y. Zhu, D. Su, L. He, L. Xu, and D. Yu. Generative pre-trained speech language model with efficient hierarchical transformer, 2024. URL https://arxiv.org/abs/2406.00976."
        },
        {
            "title": "A Results at Different Data Scales",
            "content": "Table 4: Comparison of model performance when trained on LibriSpeech versus LibriHeavy. System 3s Prefix as Prompt Reference Utterance as Prompt WER-C WER-H SIM WER-C WER-H Ground Truth Ground Truth (Encodec) 1.78 1.79 2.15 2.30 0.668 0.606 1.78 1.79 2.15 2. SIM 0.778 0.738 SLED SLED Models trained on the 1,000-hour LibriSpeech 1. 2.28 0.490 2.27 2.88 0.616 Models trained on the 50,000-hour LibriHeavy 1.59 1.99 0.515 1.51 1.97 0. Why not Gaussian? In Sec. 3.2, we claim Gaussian distribution does not have enough capacity to model the per-token continuous distribution in latent space. We have designed experiments to demonstrate this. Specifically, using trained SLED (which we assume has successfully captured the underlying data distribution), we sampled 100 latent vectors from the first speech position and conducted Shapiro-Wilk test [60] to examine whether each dimension follows Gaussian distribution. The result showed that 0 out of 128 dimensions passed the normality test (p > 0.05), indicating that none of the dimensions individually follow normal distribution."
        },
        {
            "title": "C More Analysis of Inference Efficiency",
            "content": "Table 5: Inference latency and RTF of generating 10s speech. *: We use teacher-forcing-style AR forward pass to estimate FLOPs in AR module. Model AR Latency Gen. Module Latency NAR Latency RTF FLOPs* VALL-E SLED 6.91 6.82 0.16 (Softmax) 1.23 (MLP) 0.94 0.8 0.8 8 222.7G 280.05 Table 6: RTF of SLED for different batch sizes. Batch Size SLEDs RTF 1 16 64 0.8 0.055 0.027 As shown, SLED and discrete hierarchical models like VALL-E exhibit no significant difference in RTF. However, SLED largely reduces FLOPs compared with hierarchical models and supports streaming generation. The additional time spent by the MLP heads in continuous generation is offset by the efficiency gains from avoiding the decoding process required by local NAR models. Combined with its RTF being less than 1, SLED is well-suited for supporting real-time applications. Moreover, while SLEDs RTF may appear higher than NAR TTS models such as [6], we argue that comparing RTF between AR and iterative NAR with the batch size fixed to 1 is somewhat misleading. In fact, prior studies on NAR generation [21] have long advocated for evaluating AR and NAR models under larger batch sizesup to the capacity supported by the GPUor equivalently, under the same computational budget (FLOPs). In fact, we observe that increasing the batch size significantly reduces SLEDs RTF."
        }
    ],
    "affiliations": [
        "Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences",
        "Pattern Recognition Center, WeChat AI, Tencent Inc",
        "School of Future Science and Engineering, Soochow University",
        "University of Chinese Academy of Sciences"
    ]
}