{
    "paper_title": "RSQ: Learning from Important Tokens Leads to Better Quantized LLMs",
    "authors": [
        "Yi-Lin Sung",
        "Prateek Yadav",
        "Jialu Li",
        "Jaehong Yoon",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Layer-wise quantization is a key technique for efficiently compressing large models without expensive retraining. Previous methods typically quantize the weights of each layer by \"uniformly\" optimizing the layer reconstruction loss across all output tokens. However, in this paper, we demonstrate that better-quantized models can be obtained by prioritizing learning from important tokens (e.g. which have large attention scores). Building on this finding, we propose RSQ (Rotate, Scale, then Quantize), which (1) applies rotations (orthogonal transformation) to the model to mitigate outliers (those with exceptionally large magnitude), (2) scales the token feature based on its importance, and (3) quantizes the model using the GPTQ framework with the second-order statistics computed by scaled tokens. To compute token importance, we explore both heuristic and dynamic strategies. Based on a thorough analysis of all approaches, we adopt attention concentration, which uses attention scores of each token as its importance, as the best approach. We demonstrate that RSQ consistently outperforms baseline methods across multiple downstream tasks and three model families: LLaMA3, Mistral, and Qwen2.5. Additionally, models quantized with RSQ achieve superior performance on long-context tasks, further highlighting its effectiveness. Lastly, RSQ demonstrates generalizability across various setups, including different model sizes, calibration datasets, bit precisions, and quantization methods."
        },
        {
            "title": "Start",
            "content": "RSQ: Learning from Important Tokens Leads to Better Quantized LLMs Yi-Lin Sung 1 Prateek Yadav 1 Jialu Li 1 Jaehong Yoon 1 Mohit Bansal 1 5 2 0 2 3 ] . [ 1 0 2 8 1 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Layer-wise quantization is key technique for efficiently compressing large models without expensive retraining. Previous methods typically quantize the weights of each layer by uniformly optimizing the layer reconstruction loss across all output tokens. However, in this paper, we demonstrate that better-quantized models can be obtained by prioritizing learning from important tokens (e.g. which have large attention scores). Building on this finding, we propose RSQ (Rotate, Scale, then Quantize), which (1) applies rotations (orthogonal transformation) to the model to mitigate outliers (those with exceptionally large magnitude), (2) scales the token feature based on its importance, and (3) quantizes the model using the GPTQ framework with the second-order statistics computed by scaled tokens. To compute token importance, we explore both heuristic and dynamic strategies. Based on thorough analysis of all approaches, we adopt attention concentration, which uses attention scores of each token as its importance, as the best approach. We demonstrate that RSQ consistently outperforms baseline methods across multiple downstream tasks and three model families: LLaMA3, Mistral, and Qwen2.5. Additionally, models quantized with RSQ achieve superior performance on long-context tasks, further highlighting its effectiveness. Lastly, RSQ demonstrates generalizability across various setups, including different model sizes, calibration datasets, bit precisions, and quantization methods. Our code is available at https://github.com/ylsung/rsq. 1. Introduction Large language models (LLMs) (Georgiev et al., 2024; Achiam et al., 2023) have recently achieved great success and have transformed the landscape of artificial intelligence. 1Department of Computer Science, UNC at Chapel Hill. Correspondence to: Yi-Lin Sung <ylsung@cs.unc.edu>. However, the substantial computational demands associated with these models pose significant challenges to their usage and deployment, especially in resource-constrained scenarios. Weight quantization (Han et al., 2016; Wu et al., 2016) is widely used technique for reducing the computational costs of LLMs by representing weight values with fewer bits. Among various approaches, post-training quantization (PTQ) (Frantar & Alistarh, 2022; Liu et al., 2021) is particularly favored by practitioners, as it enables the quantization of pre-trained LLMs using only small calibration dataset, eliminating the need for expensive retraining. We focus on the layer-wise post-training quantization scheme (Hubara et al., 2020; Li et al., 2021; Frantar et al., 2023) that has been demonstrated to be both effective and efficient for quantizing large models. Layer-wise quantization methods quantize an LLMs weights one layer at time by minimizing the token-level feature distance between the outputs of the original and quantized weights (i.e., the layer reconstruction loss, WX WX2 2). Several advancements have been made to improve layer-wise quantization techniques in the past few years. For example, GPTQ (Frantar et al., 2023) improves the efficiency and stability to compute second-order statistics and their inverses. QuIP# (Tseng et al., 2024) and AQLM (Egiazarian et al., 2024) represent quantized weights with vectors rather than fixed scalars. Additionally, QuIP (Chee et al., 2023) and QuaRot (Ashkboos et al., 2024b) demonstrate through empirical studies that weight outliersparameters with unusually large magnitudescan be effectively mitigated by applying orthogonal transformations. Previous methods commonly perform layer-wise weight quantization by optimizing the layer reconstruction loss across all input tokens uniformly. However, research has shown that LLMs do not treat all tokens equally: (1) StreamingLLM (Xiao et al., 2024) shows that initial tokens often have strong attention scores, (2) H2O (Zhang et al., 2023) reveals that some tokens in KV cache contribute most of the attention values while decoding, and (3) RHO-1 (Lin et al., 2024c) demonstrates not all tokens are equal in training LLMs. Since quantized models inherently lose information due to the reduced capacity of lower bit representations, we argue that it should be particularly crucial RSQ: Learning from Important Tokens Leads to Better Quantized LLMs for them to focus on learning and preserving the most critical information during the quantization process to maximize their performance. Inspired by these insights, we reconsider the conventional approach in quantization methods by optimizing the layer reconstruction loss over only subset of important input tokens (i.e., using only the first 1/4 of the tokens). Our findings reveal that this strategy improves the quantized models accuracy across ten downstream tasks by up to 2.2%. Building on our findings and previous approaches, we propose RSQ to quantize the model in three steps: (1) rotate (orthogonally transform) the model to mitigate weight outliers, (2) scale the token feature based on its importance, and (3) quantize the weights using the GPTQ mechanism while leveraging token importance. We note that the token importance integrates seamlessly into the GPTQ framework in the third step, ensuring both compatibility and efficiency. Fig. 1 illustrates the three steps in RSQ. In this paper, we explore two categories of approaches for obtaining token importance: (1) heuristic approaches and (2) dynamic approaches. Within the heuristic category, we investigate methods such as First-N and First&Last-N, which prioritize initial tokens and combination of initial and final tokens for quantization, respectively. These approaches outperform the conventional quantization method of optimizing across all tokens, achieving peak performance when is roughly 510% of the total tokens. It is important to note that the initial or final tokens do not inherently contain more meaningful semantic information. Instead, their importance likely stems from their positional characteristics and their tendency to receive stronger attention scores (Xiao et al., 2024; Sun et al., 2024a). In this aforementioned approach, token importance was determined solely based on heuristics (e.g., token positions). To further improve performance beyond heuristic methods, we also explore several dynamic approaches for computing token importance individually based on each input. Specifically, we investigate TokenFreq, where less frequent tokens are considered more important; ActNorm, which prioritizes tokens with larger norms; ActDiff, where tokens whose features change less after one layer are assigned higher importance; TokenSim, which gives greater weight to tokens that are less similar to others; and AttnCon, where tokens receiving higher attention scores are considered more important. Among these, AttnCon performs the best as it more explicitly models each tokens impact on the other tokens. Therefore, we adopt it as our final strategy. In these dynamic approaches, we also observe the presence of positional biases, similar to those seen in heuristic methods. To ensure that tokens in less important positions are not wasted, we introduce data augmentation strategy. This involves expanding each sample by shifting it forward by several positions and adding the shifted samples to the calibration dataset. This augmentation ensures more comprehensive utilization of these tokens. We evaluate RSQ on WikiText-2 and diverse set of tasks, including LAMBADA, WinoGrande, ARC, HellaSwag, PIQA, MMLU, GSM8k, and TruthfulQA, using three models: LLaMA3-3B-Instruct, Mistral-Nemo-12B, and Qwen2.5-7B. Our results demonstrate that RSQ achieves absolute improvements of 1.6%, 0.9%, and 0.4% in average accuracy across these tasks for the three models, respectively. To facilitate comprehensive evaluation, we compare RSQ against baseline methods on subsets of various long-context benchmarks, including LongEval, L-Eval, LongICLBench, and LongCodeArena. Our results show that RSQ achieves improvements of 3.0%, 2.5%, 0.6%, and 0.025, respectively, over QuaRot on these benchmarks. Lastly, we demonstrate the generalizability of RSQ across various steps, such as different model sizes, calibration datasets, bit precisions, and quantization methods. Notably, the performance improvement is more pronounced at lower bit precisions, suggesting that learning from important token can be critical component in pursuing effective extreme compression. 2. Related Work Post-training quantization (PTQ) has gained significant attention for its ability to quantize pre-trained models without requiring expensive retraining. Methods such as ZeroQuant (Yao et al., 2022) and QLoRA (Dettmers et al., 2023) apply round-to-nearest techniques to the weights, even without utilizing calibration datasets. While these approaches are extremely efficient, they often yield suboptimal performance due to their lack of information about the data distribution. To address this limitation, several PTQ methods leverage calibration datasets for improved quantization. These data-dependent PTQ methods are also often referred to as layer-wise quantization methods, as they typically quantize models one layer at time for efficiency. Specifically, GPTQ (Frantar et al., 2023) and OBC (Frantar & Alistarh, 2022) quantize weights and adjust the remaining weights with data-derived Hessian matrices (secondorder statistics) accordingly. AWQ (Lin et al., 2024b) minimizes quantization error by rescaling weights based on the activation distribution. QuIP# (Tseng et al., 2024) and AQLM (Egiazarian et al., 2024) represent groups of quantized weights with vectors instead of scalar values. One challenge in weight quantization is the presence of outliers in the weights, as LLMs often contain subset of weights with exceptionally large magnitudes (Dettmers et al., 2024; Kim et al., 2024; Yu et al., 2024). During quantization, these outliers increase the range of the quantized weights. As result, most weights are forced into narrow range to fit the outliers within the quantized representation, 2 RSQ: Learning from Important Tokens Leads to Better Quantized LLMs which ultimately leads to suboptimal performance. One way to address this challenge is through mixed-precision quantization (Dettmers et al., 2024; 2022; Kim et al., 2024); however, current hardware lacks efficient support for inference using this technique. Recent studies, such as QuIP (Chee et al., 2023) and QuaRot (Ashkboos et al., 2024b), have empirically shown that outliers in weights can be effectively mitigated by applying orthogonal transformations (rotations) (Ashkboos et al., 2024a), which reduce their impact and minimize the need for mixed-precision quantization. Different from previous advancements in layer-wise quantization, RSQ focuses on improving the learning objective of layer-wise quantization methods (detailed in Sec. 4.2) by prioritizing important tokens. Our approach also builds on QuaRot by applying rotations to mitigate weight outliers and leveraging GPTQ for quantization. This integration makes RSQ comprehensive and holistic quantization strategy. 3. Background 3.1. Transformer Architecture The core components of transformer are the attention layers and feed-forward network (FFN). An attention layer consists of four linear modules: Wq, Wk, Wv and Wo. Similarly, an FFN contains three linear modules: Wup, Wgate, Wdown. Attention layers are responsible for capturing token dependency and global information while FFNs, positioned after each attention block, perform token-wise feature transformations. 3.2. Removing Outliers with Rotation Transformer architecture exhibits computational invariance (Ashkboos et al., 2024a), allowing an orthogonal transformation (a.k.a. rotation) to be applied to one layer and its transpose to be applied to the subsequent layer without altering the outputs. Specifically, given two-layer module with its output defined as = W2W1X, it follows that: = (W2Q)(QW1)X (1) where both weight matrices, W1 and W2 are orthogonally transformed by orthogonal matrix (QQ = QQ = by definition). This property remains valid even when an RMSNorm (Zhang & Sennrich, 2019) layer is placed between W1 and W2. Prior studies have empirically shown that applying orthogonal transformations to modern pre-trained LLMs effectively reduces weight outliers (Chee et al., 2023; Ashkboos et al., 2024b). Moreover, due to the computational invariance property, these transformations do not alter the models output if they are properly inserted into the model. Concretely, assume we initialize an orthogonal transformation matrix Q, which can be random orthogonal matrix or randomized Hadamard matrix. We transform the following weight matrices from to WQ: Wq, Wk, Wv in attention layers, Wup, Wgate in FFN layers, and the lm head layer. Similarly, we transform the following weight matrices from to QW: Wo in attention layers, Wdown in FFN layers, and the embedding layer. For more details on the rotation and orthogonal transformation, please refer to SliceGPT (Ashkboos et al., 2024a). 3.3. Layer-wise Quantization i=0Wxi Wxi2 Layer-wise quantization offers more efficient alternative to full-model quantization by processing each layer individually. In this approach, the quantized weights for each layer are optimized by minimizing the layer-wise reconstruction loss, which measures the feature distance between the outputs produced by the original weight matrix and the quantized weight matrix W: WX WX2 2 = (cid:80)T 2, where is input tokens features. Note that the reconstruction loss for each token is weighed uniformly across sequence of token features (x1, x2, ..., xT ). The OBC framework (Frantar & Alistarh, 2022) provides an explicit formula to optimally quantize column of weight based on the layer-reconstruction loss, as well as the optimal update of the remaining weights which compensates for the quantization. Specifically, for the q-th column of weight W:,q to be quantized, OBC first quantizes the weight in the round-to-nearest manner ( W:,q = quant(W:,q)), and adjust the remaining weights according to the following formula: δ = W:,q quant(W:,q) H1 qq H1 q,: (2) where = 2XX is the Hessian matrix. After quantizing each layer, we compute its output using the quantized weights, which are then used as input to the next layer. This process is repeated iteratively until all layers are quantized. 4. Methodology In this section, we first present the key observation that motivates our approach to paying different attention to each token (Sec. 4.1). Next, we provide detailed outline of each step in RSQ (Sec. 4.2). Finally, we describe our strategy for determining the importance of different tokens (Sec. 4.3). 4.1. Observation: Less Tokens yet Better Performance As described in the Sec. 3.3, layer-wise quantization methods, such as GPTQ and QuaRot, uniformly minimize the reconstruction loss across all tokens. In the standard setup, using 256 data points from WikiText-2 (Merity et al., 2016), RSQ: Learning from Important Tokens Leads to Better Quantized LLMs Figure 1. Illustration of layer-wise quantization (left), three-step process of RSQ (middle) and the dataset expansion (right). On the middle, circle size and red color intensity represent weight magnitude, with larger circles and deeper red colors indicating greater magnitudes. each containing 4096 tokens, to quantize LLaMA3-8BInstruct (Dubey et al., 2024) to 3-bit precision, QuaRot achieves perplexity of 9.51 on WikiText-2 and an average accuracy of 63.8% across 10 tasks (detailed in Sec. 5.1). Here, we present surprising finding: using only subset of tokens can improve performance. Specifically, we divide the input tokens into four non-overlapping chunks (token IDs: 11024, 10252048, 20493072, 30734096), with each chunk containing 1024 tokens. We then perform four separate quantization runs, applying the reconstruction loss to only one chunk at time. It is important to note that while the loss is computed exclusively on the selected chunk, all tokens are used for the forward pass. Therefore, the earlier tokens, even when not selected, still indirectly contribute to the loss of the later tokens. However, due to the autoregressive nature of modern LLMs, tokens after the N-th chunk do not contribute to the loss if the reconstruction loss is applied solely to the N-th chunk. Interestingly, as shown in Tab. 1, the performance when using the 2nd, 3rd, or 4th chunks is inferior to that of the 1st chunk, despite these chunks having access to more input information (as tokens in the 1st chunk are still used to produce features for subsequent tokens). We hypothesize that this effect arises because LLMs tend to heavily attend to the initial tokens, making it crucial to preserve their features by directly minimizing their reconstruction loss. This hypothesis is further supported by broad observations from prior research (Sun et al., 2024a; Xiao et al., 2024). Moreover, we demonstrate that the quantized LLMs using only the 1st chunk even outperforms the result obtained by using all tokens. We believe this is because the quantized model has to allocate its limited capacity to learn from all tokens, including those that may be less important than the first chunk. Based on this observation, we argue that the current apTable 1. Evaluation results of quantizing LLaMA-3-8B-Instruct with WikiText-2 using different subsets of tokens. Used Token IDs Wiki PPL Avg Acc (%) All: 1 - 4096 9.51.11 63.80.5 1 - 1024 1025 - 2049 - 3072 3073 - 4096 9.27.05 64.50.4 10.26.12 61.70.7 10.16.08 61.40.4 10.25.01 61.30. proach of uniformly applying reconstruction loss to all tokens is suboptimal, as it fails to prioritize and preserve the most critical information in the model after quantization. To address this, we modify the layer-wise quantization objective to account for token importance and propose RSQ, which we detail in the next section. 4.2. RSQ (Rotate, Scale, then Quantize) In the previous section, we demonstrated that simple approachselecting only the first chunkalready leads to improved results. Building on this insight, we further explore more advanced strategies for assigning importance to different tokens, which we detail in Sec. 4.3. Before delving into these strategies, let us first formally introduce the algorithm of RSQ, which quantizes the model in three steps: (1) rotate to reduce outliers in the weights, (2) scale to reweight the input tokens, and (3) quantize via GPTQ mechanism with the scaled tokens. Below, we provide detailed explanation of each step. Rotate. Before quantization, we first mitigate the outliers by rotating the model. As mentioned in Sec. 3.2, computational invariance holds when RMSNorm is used in transformers. However, existing LLaMA-like LLMs, such as LLaMA3, Mistral, and Qwen2.5, use LayerNorm (Ba et al., 2016) instead. Fortunately, following Ashkboos RSQ: Learning from Important Tokens Leads to Better Quantized LLMs et al. (2024a), LayerNorm can be converted to RMSNorm by fusing its linear component into the subsequent linear layer. After this conversion, we initialize as randomized Hadamard matrix (Halko et al., 2011) and apply the rotation matrix to the transformers weights as described in Sec. 3.2. Scale. As described in Sec. 3.3, previous methods treat the layer reconstruction loss of every token equally, that is WX WX2 2. In contrast, RSQ assigns different importance to different tokens and modifies the objective function accordingly: i=0Wxi Wxi2 2 = (cid:80)T (WX WX)R 2 = (cid:88) ri(Wxi Wxi)2 2 (3) i=0 where is diagonal matrix with diagonal entries (r1, r2, ..., rT ) that scale the token representations (x1, x2, ..., xT ), respectively. The specific methods for assigning values to the importance matrix are detailed in the next section. Quantize. Given RSQs proposed objective function (Eq. (3)), as in Sec. 3.3, we follow the GPTQ framework to solve the optimal quantized weight while minimizing the loss. The resulting formulation remains mostly the same, except for modified Hessian matrix HRSQ = 2XR2X, which essentially represents the outer product of the scaled token features (i.e. XR). Next, we quantize the rotated weight by applying the modified Hessian matrix to the weight update formula presented in Eq. (2). Fig. 1 displays the illustration of the three steps of RSQ. 4.3. Strategies to Compute Token Importance 1 , z(l) 2 , ..., z(l) To align with the nature of layer-wise quantization, we compute token importance per layer independently. Furthermore, we avoid using any global information, such as model gradients, as it would violate the layer-wise assumption, where only one layer is accessed at time. During the quantization of the l-th layer, let Z(l) = {z(l) } (z Rd) represent the d-dimensional input features of the current layer (note that Z(l+1) = Layer(l)(Z(l))). We compute the token importance R(l) = {r(l) } (r R) to reweight the input feature of the weight in this layer to {r(l) } before applying GPTQ for weight quantization. Note that we use to represent the input features of layer, distinguishing it from X, which denotes the input features associated with weight. The token importance is kept consistent across all weights within layer, as we observe this yielding better performance. For clarity and simplicity, we omit the superscript for the layer index in the remaining of this section. 2 , ..., r(l) 2 , ..., r(l) 1 , r(l) 1 , r(l) x(l) 1 x(l) 2 x(l) Next, we present several methods for assigning importance to tokens to complete the second step of RSQ. We begin by 5 introducing two heuristic approaches that prioritize training on specific positions within the sequence. First-N. Building on our observation in Sec. 4.1 that using fewer tokens (specifically, tokens from the first chunk) leads to better performance, we further divide the inputs into smaller chunks, each containing fewer tokens. We then evaluate the quantization performance using only the first chunk. Formally, we define ri = 1 when , and ri = 0 for the rest. First&Last-N. This approach extends First-N method. While First-N can be viewed as using the first N/2 tokens and the second N/2 tokens, we explore an alternative approach that uses the first-N/2 and last-N/2 tokens for quantization. We hypothesize that replacing the second chunk with the last chunk may better capture long-term dependencies. Formally, we assign ri = 1 when or > , and ri = 0 for the rest. The aforementioned approaches assign token importance based on positional heuristics, ignoring variations across different samples and layers. To address this limitation, we explore several dynamic approaches where token importance is determined adaptively based on the layer inputs and model characteristics. We rigorously compare these approaches and adopt the most effective one at the end. Token Frequency (TokenFreq). This approach assumes that tokens importance is related to its frequency, and we observe that assigning greater weight to less frequent tokens yields better results. We compute token frequency based on the calibration dataset used for quantization, denoting the occurrence count of token as C(t) R. Given the input token sequence {t1, t2, ..., tT }, we define token importance RT as {C(ti) : 1 }. Finally, we linearly transform the importance values (r R) into bounded range [rmin, rmax]: = rmin + min(R) max(R) min(R) (rmax rmin) (4) Here, rmin and rmax are hyperparameters, and we always set rmax to 1 and adjust rmin to vary the emphasis on less important tokens. Note that we apply Eq. (4) to normalize the scores into bounded range for all dynamic approaches. Activation Norm (ActNorm). Previous studies have shown that inputs with larger norms have greater impact on the layers outputs (Virmaux & Scaman, 2018). Sun et al. (2024a) also show that attention in LLMs tends to concentrate on tokens with larger norms. Based on these, we design an approach to assigning importance scores to tokens based on the norm of their input activations, that is = {zi : 1 }. RSQ: Learning from Important Tokens Leads to Better Quantized LLMs Table 2. Comparison of recent layer-wise quantization approaches with RSQ on multiple downstream tasks. We report perplexity for WikiText and accuracy for all other tasks. The model is quantized to 3-bit. The best-performing method across all quantization approaches is highlighted in bold. We denote the standard deviation across three runs as subscript. Method Wiki LAMBADAopenai LAMBADAstd WinoGrande ArcC ArcE HellaSwag PIQA MMLU GSM8k TruthfulQA Avg Full Model GPTQ QuaRot RSQ 8.311 10.682.04 9.517.11 9.046.01 Full Model GPTQ QuaRot RSQ 6.095 9.537.01 6.782.00 6.673.01 Full Model GPTQ QuaRot RSQ 5.335 9.577.03 8.053.02 8.051.01 71.9 50.72.0 68.81.2 70.80.3 75.8 44.60.6 75.60.3 75.40.3 75.2 53.61.0 67.90.3 68.70.2 65.0 44.61.0 59.71.0 62.30.4 68.3 29.84.3 60.95.8 66.50. 68.6 48.61.8 62.10.4 63.50.5 LLaMA3-8B-Instruct 71.7 65.70.7 70.21.2 70.60.7 56.7 35.81.4 49.61.5 50.31.1 79.6 56.92.3 74.90.9 76.51.3 Mistral-NeMo-12B 75.1 57.31.1 72.80.7 73.50.4 59.1 38.80.2 55.81.5 55.70.5 80.1 58.01.6 76.91.1 77.21.1 Qwen-2.5-7B-Instruct 72.7 62.10.9 68.31.2 68.50.6 59.0 47.11.8 54.01.6 53.71. 76.4 66.92.1 78.72.9 78.22.0 75.8 65.80.2 71.30.0 72.10.2 82.2 60.52.0 78.10.3 78.50.1 85.2 72.80.0 76.70.4 77.10.3 78.5 70.41.7 76.80.7 77.10.5 65.6 50.80.4 57.60.5 60.00. 82.1 71.20.6 79.70.4 80.70.2 68.2 48.01.1 63.50.0 64.30.3 79.9 27.51.5 61.21.7 63.42.4 80.8 37.61.5 71.11.0 71.10.4 81.0 73.70.5 78.80.8 78.81.0 83.3 60.20.5 68.70.6 69.00. 84.4 40.910.5 76.40.7 77.10.8 51.7 44.81.0 48.00.9 50.61.7 54.8 47.01.5 52.50.9 52.90.5 65.5 53.71.5 60.01.4 61.20.9 69.7 51.30.7 63.80.5 65.40.1 72.7 49.30.4 68.71.1 69.60. 75.1 58.01.8 69.20.5 69.60.3 Activation Difference (ActDiff). Our next approach defines token importance based on the feature changes between inputs and outputs (Sajjad et al., 2023). We observe that assigning greater weight to tokens with smaller changes yields better results compared to assigning greater weight to those with larger changes. This suggests that these steady tokens play more crucial role in the model. Specifically, the scores are calculated as = {Layer(zi) zi : 1 }. Token Similarity (TokenSim). This approach assigns token importance based on the pairwise similarity between each token and all other tokens. Our assumption is that tokens that are less similar (has larger distance) to others are more important, as their information is rarer within the sequence. Let RT , where Sij = zi zj, denote the l2 distance between i-th and j-th token features. Formally, the scores are calculated as = {(cid:80) Sij : 1 }. Attention Concentration (AttnCon). Moreover, several works have shown that some tokens contribute most of the values in attention maps (Zhang et al., 2023). Based on this insight, we compute attention concentration to determine token importance. Specifically, consider multi-head (M heads) attention example in given layer. Let RM T represent the attention probability map, where Amij denotes the proportion of attention j-th token receives from the i-th token in the m-th head of the attention. Due to the autoregressive nature of LLMs, Amij = 0 for > i. To calculate the attention concentration of the j-th token, we sum over the second dimension of A, and further sum the scores of every head together. Specifically, the importance score is calculated as = {(cid:80) m,i Amij : 1 }. Note that the computed scores can be seamlessly integrated into the GPTQ framework as described in Sec. 4.2 to preserve the efficiency of the algorithm. We select AttnCon as our final strategy due to its superior performance and present the comparison and analysis of all methods in Sec. 5.2. 4.4. Dataset Expansion In our exploration, we find that important tokens tend to be biased toward specific positions. For example, our heuristic methods inherently select tokens from predefined positions. Moreover, AttnCon consistently assigns higher importance to the initial and final tokens, despite not explicitly enforcing this behavior  (Fig. 14)  . This bias may lead to inefficiency, as tokens in other positions are significantly overlooked. To address this, we propose data expansion, data augmentation technique designed to shift tokens within sequence, ensuring every token can occupy important positions. Specifically, given token sequence of length and an expansion factor of (= 8 in this paper), we generate shifted versions of the sequence by offsetting it by /M , 2T /M , 3T /M , ..., (M 1)T /M . The excessive tokens are then inserted at the beginning of the sequence. This process effectively distributes token importance more evenly, mitigating positional biases and improving overall token utilization. We illustrate this approach in Fig. 1. 5. Experiments We first compare RSQ with recent layer-wise quantization methods on eleven tasks across different model families (Sec. 5.1). Next, we evaluate several design choices in RSQ, such as various scaling strategies and data expansion (Sec. 5.2). We then study the effect of RSQ on longcontext tasks (Sec. 5.3). Lastly, we assess the generalizability of RSQ (Sec. 5.4) across various model sizes, calibration datasets, bit precisions, and quantization methods. Note that we quantize models to 3-bit if not further specified and our 6 RSQ: Learning from Important Tokens Leads to Better Quantized LLMs two heuristic apFigure 2. Evaluation of proaches with varying numbers of used tokens. Figure 3. Evaluation of five dynamic approaches with varying rmin. Figure 4. The effect of expanding the dataset on different methods. experiments are conducted using three different seeds. 5.2. Evaluating Design Choices in RSQ 5.1. Comparison of RSQ Against Baselines Setup. We evaluate RSQ and other baselines across three model families: LLaMA3-8B-Instruct (Dubey et al., 2024), Mistral-NeMo-12B (Jiang et al., 2023), and Qwen-2.5-7BInstruct (Yang et al., 2024). We quantize each model to 3-bit on WikiText-2 with 256 data samples, each with 4096 tokens, which is the setup adopted in recent studies (Egiazarian et al., 2024; Tseng et al., 2024). We use AttnCon as the scaling strategy and set the data expansion factor to 8. The quantized models are tested on diverse set of tasks, including LAMBADA (with two splits: the original papers version and OpenAIs version) (Paperno et al., 2016), WinoGrande (Sakaguchi et al., 2019) and ARC (Challenge and Easy splits) (Clark et al., 2018), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), MMLU (Hendrycks et al., 2021), GSM8k (Cobbe et al., 2021), and TruthfulQA (Lin et al., 2021). Accuracy is used as the evaluation metric. We use GPTQ and QuaRot as the baselines in this study. GPTQ performs layer-wise quantization as described in Sec. 3.3. QuaRot, on the other hand, mitigates outliers by applying rotations (detailed in Sec. 3.2), followed by applying GPTQ to quantize the rotated model. Results. Tab. 2 presents the evaluation results for the 16-bit and 3-bit quantized models using GPTQ, QuaRot, and RSQ. GPTQ demonstrates notable performance gap compared to QuaRot and RSQ, primarily due to the negative impact of outliers in the model, which degrade the quantization quality. When comparing RSQ to QuaRot, our approach achieves superior results in Wiki Perplexity and most evaluation tasks across the three models, where RSQ achieves absolute improvements of 1.6%, 0.9%, and 0.4% in average accuracy against QuaRot, respectively. This indicates that incorporating token importance into the quantization process is an effective strategy, even when using the same number of total tokens. Setup. We follow the same setup described in Sec. 4.1 to quantize LLaMA3-8B-Instruct to 3-bit precision using 256 samples of 4096 tokens each from WikiText-2. To avoid overfitting, we only use the perplexity on WikiText-2 as the evaluation metric in this part. Results. We first compare the performance of two heuristic approaches, First-N and First&Last-N, across different numbers of activated tokens. As shown in Fig. 2, we observe that perplexity decreases steadily as the number of tokens is reduced from 4096 to around 512 or 256, but increases when using fewer tokens for both approaches. This suggests that using the fewest tokens does not necessarily yield the best results. We hypothesize that the model requires certain number of tokens to effectively capture token interactions that are brought by the attention mechanism. We also observe that First&Last-N often outperforms FirstN when using the same number of tokens, achieving their optimal perplexity values of 9.15 and 9.18, respectively. This suggests that learning is more effective when incorporating the last chunk of tokens into the first chunk, rather than using the first and middle chunks. In Fig. 3, we compare five dynamic approaches, TokenFreq, ActNorm, ActDiff, TokenSim and AttnCon, with varying hyperparameter rmin across {0.005, 0.01, 0.02, 0.05, 0.1}. We observe that TokenFreq and ActDiff perform less competitively than the other approaches, suggesting that token frequency and feature changes after layer are not strong indicators of token importance for quantizing LLMs. Among the other three approaches, we find AttnCon reaches its optimal perplexity (9.028) at rmin = 0.01 while ActNorm and TokenSim reach their optimal perplexity (9.075, 9.047, respectively) at rmin = 0.005. These relatively small rmin indicate that placing lower emphasis on less important tokens and focusing more on the most important tokens is beneficial. We adopt AttnCon as our final scaling strategy, as it achieves the best perplexity performance. We also assess the effectiveness of data expansion (M = 8) by incorporating it into each approach using its optimal 7 RSQ: Learning from Important Tokens Leads to Better Quantized LLMs Table 3. Comparison of RSQ and QuaRot across multiple long-context benchmarks using three different calibration dataset configurations. The model is quantized to 3-bit. The best-performing method among all quantization approaches is highlighted in bold. Method =1 Full Model 53.67 LITM =15 45.61 =30 46. Avg TOEFL QuALITY Coursera SFiction 48.54 81.04 60. 52.62 71.88 GSM 81.00 CodeU TopicRet Avg Banking77 TecRED Avg CodeGen 4.44 64.67 59. 59.40 41.65 50.52 0.298 L-Eval LongICLBench LongCodeArena number of samples = 256, sequence length = 4096 QuaRot RSQ 50.801.6 52.230.5 43.472.4 46.032.7 43.292.5 45.593. 45.852.0 47.952.0 72.240.9 76.212.1 52.150.8 54.460.7 46.900.7 49.271.3 65.623.1 63.804.0 63.336.6 66.333. 2.220.9 4.070.5 45.113.5 50.892.4 49.651.4 52.140.3 62.404.3 58.607.7 44.765.2 49.830.7 53.583.2 54.213. 0.206.013 0.231.008 number of samples = 512, sequence length = 2048 QuaRot RSQ 49.171.2 51.391.6 44.321.0 45.890.8 43.510.5 45.871. 45.670.2 47.720.8 71.132.3 73.232.2 51.161.4 53.961.7 46.950.7 48.791.0 58.330.9 61.982.6 65.674.9 70.002. 4.812.1 4.440.0 48.007.0 51.787.0 49.432.0 52.021.9 65.400.6 62.332.2 45.483.1 46.175.5 55.441.8 54.253. 0.205.007 0.227.002 number of samples = 1024, sequence length = 1024 QuaRot RSQ 48.662.6 51.100.2 47.200.9 48.010.7 48.801.7 48.621. 48.221.4 49.240.2 73.112.1 74.722.8 51.821.4 55.782.0 48.641.8 50.580.8 62.241.3 64.853.3 65.000.8 69.331. 2.591.3 3.700.5 40.453.0 47.557.4 49.120.9 52.352.0 59.738.0 61.205.2 46.027.0 46.301.9 52.876.9 53.753. 0.220.001 0.225.002 hyperparameter. As shown in Fig. 4, most scaling strategies benefit from data expansion in terms of perplexity. 5.3. Evaluation on Long-Context Tasks Previous approaches have primarily focused on tasks with relatively short inputs. However, recent large language models (LLMs) have demonstrated impressive long-context capabilities, greatly expanding their potential for user-facing applications such as chatbots, search engines, and collaborative code-writing systems. Neglecting the evaluation of long-context benchmarks may fail to capture the full impact and effectiveness of LLM quantization, particularly in these emerging use cases. To foster complete assessment, in this section, we thus evaluate RSQ and QuaRot against several long-context benchmarks. Setup. We use LLaMA3 as the backbone model for this experiment. Since this model has context length limit of 8k tokens, the dataset samples used for evaluation do not exceed this length. When evaluating long-context tasks, natural question arises: does using longer input sequences in the calibration dataset improve long-context performance? To explore this, we test three different configurations of the calibration dataset. Specifically, we set the number of samples to 256, 512, and 1024, with corresponding sequence lengths of 4096, 2048, and 1024 tokens, respectively. Note that we adjust the number of samples to ensure the total number of tokens remained consistent across configurations. Lost in the Middle (LITM) (Liu et al., 2024a) is retrieval task designed to assess positional biases of the answer placed in the input documents. We set = 1, 15, 30 to indicate that the answer appears in the -th document out of total of 30 documents (avg length=4.5k). Next, we adopt some closed-ended tasks from L-Eval (An et al., 2024) for evaluating the long-context understanding, such as TOEFL (avg length=3.6k) (Tseng et al., 2016; Chung et al., 2018), QuALITY (6.2k) (Pang et al., 2022), Coursera (6.8k), SFiction (7.2k), GSM (4.8k), CodeU (7.4k), and TopicRet (7.6k) (Li et al., 2023). LongICLBench (Li et al., 2024b) focuses on evaluating the long-context in-context learning capabilities of LLMs. From this benchmark, we sample two datasets: Banking77 (avg length=7.7k) (Casanueva et al., 2020) and TecRED (6.6k) (Zhang et al., 2017). LongCodeArena (Bogomolov et al., 2024) is benchmark for code processing tasks that require project-wide context, and we sample the library-based code generation task to evaluate the models ability to utilize the given library. Most of the datasets employ accuracy as evaluation metrics, except we use the F1 score for TecRED and ChrF (Popovic, 2015) for library code generation in LongCodeArena. Results. Tab. 3 displays our evaluation of long-context tasks using three different calibration dataset configurations. RSQ consistently outperforms QuaRot across nearly all benchmarks in all three configurations. This demonstrates that the strategy of prioritizing important tokens produces quantized models that perform effectively on both shortand long-context tasks. Furthermore, the results indicate that focusing on subset of tokens is sufficient to capture the long-term dependencies across tokens. We do not observe clear trend indicating that using calibration dataset with longer sequences results in better or worse performance on long-context tasks. This suggests that simply matching the length distribution between the calibration dataset and downstream tasks is insufficient. more advanced strategy, whether through improving the data or the method sides, is needed to further enhance performance in long-context scenarios. In the LITM evaluation, we observe that LLaMA3-8BInstruct generally performs better when the answer appears in the early documents (P = 1). However, its performance on later documents does not consistently surpass that on middle documents, deviating from the findings reported by Liu et al. (2024a) for other models. Interestingly, we also find that the quantized models outperform the 16-bit model by 24% in average accuracy on LongICLBench, similar to the findings from Hong et al. (2024). This shows that quantization does not necessarily hurt performance in every aspect and can yield improvements in certain scenarios. One possible explanation is that quantization might reduce RSQ: Learning from Important Tokens Leads to Better Quantized LLMs Table 4. Ablation on calibration dataset. Table 5. Ablation on number of bits Metric Method Calibration Dataset Wiki RedPajama C4 PTB Metric Method Number of Bits 4 3 2 Wiki PPL Avg Acc (%) QuaRot RSQ QuaRot RSQ 9.34.03 9.00.01 64.10.2 65.10.2 9.77.09 9.31.03 64.20.3 65.50. 9.90.02 9.41.03 64.00.2 65.60.3 9.98.06 9.61.08 63.70.2 64.50.1 Wiki PPL Avg Acc (%) QuaRot RSQ QuaRot RSQ 8.57.06 8.47.01 68.10.0 68.30.1 9.34.03 9.00.01 64.10.2 65.10. 22.710.5 16.26.12 35.30.8 40.60.3 Figure 5. Ablation on model sizes. weight noise, potentially enhancing the models robustness for some tasks. 5.4. Generalizability of RSQ In this section, we evaluate the generalizability of RSQ under various setups, including its performance across different model sizes, calibration datasets, bit precisions, and quantization methods. Different Model Sizes. We choose three models from the mistral family: Mistral-7B-Instruct-v0.3, Mistral-NeMo12B and Mistral-Small-Instruct-2409, whose sizes are 7B, 12B, and 22B, respectively. We quantize each model to 3-bit on WikiText-2 with 256 data samples, each with 4096 tokens. The results  (Fig. 5)  show that RSQ consistently outperforms QuaRot across all three models, with the performance gap being slightly larger in the 22B model Different Calibration Datasets. In addition to using WikiText-2 as the calibration dataset (Bandari et al., 2024; Ji et al., 2024), we also evaluate RSQs performance with using RedPajama (Weber et al., 2024), C4 (Raffel et al., 2020), and PTB (Marcus et al., 1993). For this experiment, we quantize LLaMA3 to 3-bit using 512 data samples, each with 2048 tokens. The results, presented in Tab. 4, demonstrate that our approach consistently outperforms QuaRot across varying calibration datasets, demonstrating the robustness of the approach. Different Bit Precisions. In previous experiments, we consistently quantized the model to 3-bit. Here, we extend the analysis by exploring the effects of RSQ when quantizing to 2-bit and 4-bit. When quantizing LLaMA3 with WikiText-2 (512 data samples, each containing 2048 tokens), our results in Tab. 5 demonstrate that RSQ outperforms QuaRot across different bit precisions. Notably, the performance gap is larger at lower bit precisions, suggesting that the idea of learning from important tokens can be crucial factor for achieving effective extreme compression. RSQ for vector quantization. In previous experiments, we quantized model weights individually using scalar quantizaTable 6. RSQ + VQ. Method Metrics Wiki PPL Avg Acc (%) QuaRot RSQ 24.690.6 20.080.3 42.51.0 44.30.2 tion. In this section, we extend RSQ to vector quantization (VQ), which better captures the high-dimensional distribution of weights. Specifically, we replace the 2-bit integer grid in scalar quantization with the 2-bit comparable E8P codebook (Tseng et al., 2024) and adapt the quantizer from GPTQ to LDLQ, following the original implementation, as the two are shown to be equivalent in the QuIP paper (Chee et al., 2023). We then quantize LLaMA3 using WikiText-2 (512 data samples, each with 2048 tokens) under this new setup and present the results in Tab. 6. Our findings indicate that vector quantization improves the average accuracy for both QuaRot and RSQ compared to scalar quantization (2bit results in Tab. 5), with our approach achieving the best overall performance. 6. Conclusion This paper first presents an observation that quantizing models based on only the first 25% of tokens leads to improved performance compared to using all tokens. This insight motivated us to modify the GPTQ objective and develop RSQ (Rotate, Scale, then Quantize), which demonstrates effectiveness across diverse benchmarks and configurations. 7. Impact Statements This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was supported by NSF-CAREER Award 1846185, DARPA ECOLE Program No. HR00112390060, and NSF-AI Engage Institute DRL-2112635. Any opinions, findings, and conclusions or recommendations in this work are those of the author(s) and do not necessarily reflect the views of the sponsors."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., 9 RSQ: Learning from Important Tokens Leads to Better Quantized LLMs Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. An, C., Gong, S., Zhong, M., Zhao, X., Li, M., Zhang, J., Kong, L., and Qiu, X. L-eval: Instituting standardIn ized evaluation for long context language models. Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1438814411, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.776. URL https: //aclanthology.org/2024.acl-long.776/. Ashkboos, S., Croci, M. L., do Nascimento, M. G., Hoefler, T., and Hensman, J. SliceGPT: Compress large language models by deleting rows and columns. In ICLR, 2024a. URL https://openreview.net/forum? id=vXxardq6db. Ashkboos, S., Mohtashami, A., Croci, M. L., Li, B., Cameron, P., Jaggi, M., Alistarh, D., Hoefler, T., and Hensman, J. Quarot: Outlier-free 4-bit inference in In NeurIPS, 2024b. URL https: rotated LLMs. //openreview.net/forum?id=dfqsW38v1X. Ba, J., Kiros, J. R., and Hinton, G. E. Layer normalization. ArXiv, abs/1607.06450, 2016. URL https://api. semanticscholar.org/CorpusID:8236317. Badri, H. and Shaji, A. Half-quadratic quantization of large machine learning models, November 2023. URL https://mobiusml.github.io/hqq_blog/. Bandari, A., Yin, L., Hsieh, C.-Y., Jaiswal, A. K., Chen, T., Shen, L., Krishna, R., and Liu, S. Is c4 dataset optimal for pruning? an investigation of calibration data for llm pruning. arXiv preprint arXiv:2410.07461, 2024. Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Piqa: Reasoning about physical commonsense In AAAI, 2020. URL https: Y. in natural language. //api.semanticscholar.org/CorpusID: 208290939. Bogomolov, E., Eliseeva, A., Galimzyanov, T., Glukhov, E., Shapkin, A., Tigina, M., Golubev, Y., Kovrigin, A., van Deursen, A., Izadi, M., and Bryksin, T. set of benchmarks for ArXiv, abs/2406.11612, long-context code models. 2024. URL https://api.semanticscholar. org/CorpusID:270559949. Long code arena: Casanueva, I., Temˇcinas, T., Gerz, D., Henderson, M., and Vulic, I. Efficient intent detection with dual sentence encoders. In Wen, T.-H., Celikyilmaz, A., Yu, Z., Papangelis, A., Eric, M., Kumar, A., Casanueva, 10 I., and Shah, R. (eds.), Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pp. 3845, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. nlp4convai-1.5. URL https://aclanthology. org/2020.nlp4convai-1.5/. Chee, 2-bit quantization of J., Cai, Y., Kuleshov, V., and Sa, C. D. large language modQuip: NeurIPS, 36:43964429, els with guarantees. 2023. URL https://api.semanticscholar. org/CorpusID:260154775. Chung, Y.-A., Lee, H.-Y., and Glass, J. Supervised and unsupervised transfer learning for question answering. In Walker, M., Ji, H., and Stent, A. (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 15851594, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/ N18-1143. URL https://aclanthology.org/ N18-1143/. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018. URL https://api. semanticscholar.org/CorpusID:3922816. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021. URL https://api.semanticscholar. org/CorpusID:239998651. Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Llm.int8(): 8-bit matrix multiplication for transformers at scale. NeurIPS, abs/2208.07339, 2022. URL https://api.semanticscholar. org/CorpusID:251564521. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. NeurIPS, abs/2305.14314, 2023. URL https: //api.semanticscholar.org/CorpusID: 258841328. Dettmers, T., Svirschevski, R. A., Egiazarian, V., Kuznedelev, D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D. SpQR: sparse-quantized representation for near-lossless LLM weight compression. In ICLR, 2024. URL https://openreview.net/ forum?id=Q1u25ahSuy. RSQ: Learning from Important Tokens Leads to Better Quantized LLMs Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Egiazarian, V., Panferov, A., Kuznedelev, D., FranExtreme large language models via addiICML, abs/2401.06118, 2024. https://api.semanticscholar.org/ tar, E., Babenko, A., and Alistarh, D. compression of tive quantization. URL CorpusID:266933421. Frantar, E. and Alistarh, D. Optimal brain compression: framework for accurate post-training quantization and pruning. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), NeurIPS, 2022. URL https: //openreview.net/forum?id=ksVGCOlOEba. Frantar, E. and Alistarh, D. Sparsegpt: Massive language models can be accurately pruned in oneICML, abs/2301.00774, 2023. URL https: shot. //api.semanticscholar.org/CorpusID: 255372747. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generaICLR, abs/2210.17323, tive pre-trained transformers. 2023. URL https://api.semanticscholar. org/CorpusID:253237200. Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Halko, N., Martinsson, P.-G., and Tropp, J. A. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53(2):217288, 2011. Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural network with prunICLR, ing, trained quantization and huffman coding. 2016. URL https://api.semanticscholar. org/CorpusID:2134321. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. X., and Steinhardt, J. Measuring massive multitask language understanding. ICLR, abs/2009.03300, 2021. URL https://api.semanticscholar. org/CorpusID:221516475. Hong, J., Duan, J., Zhang, C., Li, Z., Xie, C., Lieberman, K., Diffenderfer, J., Bartoldson, B. R., Jaiswal, A. K., Xu, K., Kailkhura, B., Hendrycks, D., Song, D., Wang, Z., and Li, B. Decoding compressed trust: Scrutinizing the trustworthiness of efficient LLMs under compression. In Salakhutdinov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N., Scarlett, J., and Berkenkamp, F. (eds.), ICML, volume 235 of Proceedings of Machine Learning Research, pp. 1861118633. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/ v235/hong24a.html. Hubara, I., Nahshan, Y., Hanani, Y., Banner, R., and Soudry, D. Improving post training neural quantization: Layer-wise calibration and integer programming. ArXiv, abs/2006.10518, 2020. URL https: //api.semanticscholar.org/CorpusID: 219792681. Ji, Y., Xiang, Y., Li, J., Xia, Q., Li, P., Duan, X., Wang, Z., and Zhang, M. Beware of calibration data for pruning large language models. arXiv preprint arXiv:2410.17711, 2024. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de Las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.- A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b. ArXiv, abs/2310.06825, 2023. URL https://api.semanticscholar. org/CorpusID:263830494. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de Las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mixtral of experts. ArXiv, abs/2401.04088, 2024. URL https://api.semanticscholar. org/CorpusID:266844877. Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M. W., and Keutzer, K. Squeezellm: Dense-and-sparse quantization. ICML, abs/2306.07629, 2024. URL https://api.semanticscholar. org/CorpusID:259144954. Kuzmin, A., Nagel, M., van Baalen, M., Behboodi, Pruning vs quantizaA., and Blankevoort, T. ArXiv, abs/2307.02973, tion: Which is better? 2023. URL https://api.semanticscholar. org/CorpusID:259360935. Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J., Stoica, I., Ma, X., and Zhang, H. How long can context length of open-source LLMs truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. URL https://openreview.net/ forum?id=LywifFNXV5. RSQ: Learning from Important Tokens Leads to Better Quantized LLMs Li, S., Ning, X., Wang, L., Liu, T., Shi, X., Yan, S., Dai, G., Yang, H., and Wang, Y. Evaluating quantized large language models. ArXiv, abs/2402.18158, 2024a. URL https://api.semanticscholar. org/CorpusID:268041618. Li, T., Zhang, G., Do, Q. D., Yue, X., and Chen, W. Long-context llms struggle with long in-context learning. ArXiv, abs/2404.02060, 2024b. URL https: //api.semanticscholar.org/CorpusID: 268857023. Li, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu, F., Wang, W., and Gu, S. {BRECQ}: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=POWv6hDd9XH. Lin, H., Xu, H., Wu, Y., Cui, J., Zhang, Y., Mou, L., Song, L., Sun, Z., and Wei, Y. Duquant: Distributing outliers via dual transformation makes stronger In NeurIPS, 2024a. URL https: quantized llms. //api.semanticscholar.org/CorpusID: 270226501. Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. MLSys, 2024b. URL https://api.semanticscholar. org/CorpusID:258999941. Lin, S. C., Hilton, J., and Evans, O. Truthfulqa: Measuring how models mimic human falsehoods. In ACL, 2021. URL https://api.semanticscholar. org/CorpusID:237532606. Lin, Z.-W., Gou, Z., Gong, Y., Liu, X., Shen, Y., Xu, R., Lin, C., Yang, Y., Jiao, J., Duan, N., and Chen, W. Rho-1: Not ICLR, abs/2404.07965, all tokens are what you need. 2024c. URL https://api.semanticscholar. org/CorpusID:269042762. Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. TACL, 12:157173, 2024a. Liu, Z., Wang, Y., Han, K., Zhang, W., Ma, S., and Gao, W. Post-training quantization for vision transformer. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), NeurIPS, volume 34, pp. 2809228103. Curran Associates, Inc., 2021. URL https://proceedings.neurips. cc/paper_files/paper/2021/file/ ec8956637a99787bd197eacd77acce5e-Paper. pdf. Liu, Z., Zhao, C., Fedorov, I., Soran, B., Choudhary, D., Krishnamoorthi, R., Chandra, V., Tian, Y., and Blankevoort, T. Spinquant: Llm quantization with learned rotations. ArXiv, abs/2405.16406, 2024b. URL https://api.semanticscholar. org/CorpusID:270062819. Malinovskii, V., Mazur, D., Ilin, I., Kuznedelev, D., Burlachenko, K. P., Yi, K., Alistarh, D., and Richtarik, PV-tuning: Beyond straight-through estimaP. In NeurIPS, tion for extreme LLM compression. 2024. URL https://openreview.net/forum? id=YvA8UF0I37. Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. Building large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313 330, 1993. URL https://aclanthology.org/ J93-2004/. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. ArXiv, abs/1609.07843, 2016. URL https://api.semanticscholar. org/CorpusID:16299141. Pang, R. Y., Parrish, A., Joshi, N., Nangia, N., Phang, J., Chen, A., Padmakumar, V., Ma, J., Thompson, J., He, H., and Bowman, S. QuALITY: Question answering with long input texts, yes! In Carpuat, M., de Marneffe, M.-C., and Meza Ruiz, I. V. (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 53365358, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main. 391. URL https://aclanthology.org/2022. naacl-main.391/. Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernandez, R. The lambada dataset: Word prediction requiring broad discourse context. ArXiv, abs/1606.06031, 2016. URL https://api.semanticscholar. org/CorpusID:2381275. Popovic, M. chrF: character n-gram F-score for automatic MT evaluation. In Bojar, O., Chatterjee, R., Federmann, C., Haddow, B., Hokamp, C., Huck, M., Logacheva, V., and Pecina, P. (eds.), Proceedings of the Tenth Workshop on Statistical Machine Translation, pp. 392395, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/W15-3049. URL https://aclanthology.org/W15-3049/. Raffel, C., Shazeer, N. M., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 12 RSQ: Learning from Important Tokens Leads to Better Quantized LLMs Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 21:140:1140:67, 2020. URL https://api.semanticscholar. org/CorpusID:204838007. Sajjad, H., Dalvi, F., Durrani, N., and Nakov, P. On the effect of dropping layers of pre-trained transformer models. Computer Speech & Language, 77:101429, 2023. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64:99 106, 2019. URL https://api.semanticscholar. org/CorpusID:198893658. Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang, K., Gao, P., Qiao, Y. J., and Luo, P. Omniquant: Omnidirectionally calibrated quantization ICLR, abs/2308.13137, for large language models. 2024. URL https://api.semanticscholar. org/CorpusID:261214575. Sun, M., Chen, X., Kolter, J. Z., and Liu, Z. Massive activations in large language models. COLM, 2024a. Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. simple and effective pruning approach for large language models. ICLR, 2024b. Sung, Y.-L., Yoon, J., and Bansal, M. Ecoflap: Efficient coarse-to-fine layer-wise pruning for vision-language models. In ICLR, 2024. Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and Sa, C. D. QuIP$#$: Even better LLM quantization with hadamard incoherence and lattice codebooks. In ICML, 2024. URL https://openreview.net/forum? id=9BrydUVcoe. Tseng, A., Sun, Q., Hou, D., and De Sa, C. M. Qtip: Quantization with trellises and incoherence processing. Advances in Neural Information Processing Systems, 37: 5959759620, 2025. Tseng, B.-H., syun Shen, S., yi Lee, H., and Lee, L.- S. Towards machine comprehension of spoken content: Initial toefl listening comprehension test by maIn Interspeech, 2016. URL https://api. chine. semanticscholar.org/CorpusID:7488912. Virmaux, A. and Scaman, K. Lipschitz regularity of deep neural networks: analysis and efficient estimation. NeurIPS, 31, 2018. Weber, M., Fu, D. Y., Anthony, Q., Oren, Y., Adams, S., Alexandrov, A., Lyu, X., Nguyen, H., Yao, X., Adams, V., Athiwaratkun, B., Chalamala, R., Chen, K., Ryabinin, M., Dao, T., Liang, P., Re, C., Rish, I., and Zhang, C. Redpajama: an open dataset for training large language models. NeurIPS Datasets and Benchmarks Track, 2024. Wu, J., Leng, C., Wang, Y., Hu, Q., and Cheng, J. Quantized convolutional neural networks for mobile devices. CVPR, pp. 48204828, 2016. URL https://api. semanticscholar.org/CorpusID:9183542. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention ICLR, abs/2309.17453, 2024. URL https: sinks. //api.semanticscholar.org/CorpusID: 263310483. Yang, Q. A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y.-C., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., Qiu, Z., and Quan, S. Qwen2.5 technical report. ArXiv, abs/2412.15115, 2024. URL https://api.semanticscholar. org/CorpusID:274859421. Yao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and He, Y. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. NeurIPS, abs/2206.01861, 2022. URL https: //api.semanticscholar.org/CorpusID: 249395624. The super weight Yu, M., Wang, D., Shan, Q., Reed, C., and Wan, A. in large language models. ArXiv, abs/2411.07191, 2024. URL https: //api.semanticscholar.org/CorpusID: 273963406. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish URL https: your sentence? //api.semanticscholar.org/CorpusID: 159041722. In ACL, 2019. Zhang, B. and Sennrich, R. Root mean square NeurIPS, abs/1910.07467, layer normalization. 2019. URL https://api.semanticscholar. org/CorpusID:113405151. Zhang, Y., Zhong, V., Chen, D., Angeli, G., and Manning, C. D. Position-aware attention and supervised data improve slot filling. In Palmer, M., Hwa, R., and Riedel, S. (eds.), Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 3545, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/ D17-1004. URL https://aclanthology.org/ D17-1004/. 13 RSQ: Learning from Important Tokens Leads to Better Quantized LLMs Zhang, Z. A., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C. W., Wang, Z., and Chen, B. H2o: Heavyhitter oracle for efficient generative inference of NeurIPS, abs/2306.14048, large language models. 2023. URL https://api.semanticscholar. org/CorpusID:259263947. A. Extented Task Details A.1. Downstream Task Details The quantized models are tested on diverse set of tasks, including LAMBADA (with two splits: the original papers version and OpenAIs version) (Paperno et al., 2016) for word prediction, WinoGrande (Sakaguchi et al., 2019) and ARC (Challenge and Easy splits) (Clark et al., 2018) for commonsense reasoning, HellaSwag (Zellers et al., 2019) for commonsense natural language inference, PIQA (Bisk et al., 2020) for physical commonsense reasoning, MMLU (Hendrycks et al., 2021) as comprehensive knowledge benchmark, GSM8k (Cobbe et al., 2021) for grade school math, and TruthfulQA (Lin et al., 2021) to assess the models truthfulness in generation. All datasets use accuracy as the evaluation metric. A.2. Long-context Benchmark Details Lost in the Middle (LITM) (Liu et al., 2024a) is retrieval task designed to assess positional biases of the answer placed in the input documents. We set = 1, 15, 30 to indicate that the answer appears in the -th document out of total of 30 documents. LongEval dataset (Li et al., 2023) includes synthetic retrieval task, where each line consists of key-value pair. Given an input sample of lines, the model is asked to extract the value corresponding to specified key in the query. The hyperparameter controls both the input length and the task complexity. We set = 300, 460, and 620, where the corresponding input lengths are around 4k, 6k, and 8k, respectively. Next, we adopt couple of closed-ended tasks from L-Eval (An et al., 2024) for evaluating the long-context understanding. TOEFL (avg length=3.6k) (Tseng et al., 2016; Chung et al., 2018), QuALITY (6.2k) (Pang et al., 2022), and Coursera (6.8k) are multiple-choice QA tasks, while SFiction (7.2k) is True/False QA task. GSM (4.8k) evaluates the models in-context learning ability, CodeU (7.4k) assesses its capability to deduce program outputs, and TopicRet (7.6k) (Li et al., 2023) is designed as retrieval task. LongICLBench (Li et al., 2024b) focuses on evaluating the long-context in-context learning capabilities of LLMs. From this benchmark, we sample two datasets: Banking77 (avg length=7.7k) (Casanueva et al., 2020) and TecRED (6.6k) (Zhang et al., 2017). These datasets primarily test the models ability to learn and generalize large number of concepts using only few-shot examples. LongCodeArena (Bogomolov et al., 2024) is benchmark for code processing tasks that go beyond single file and require project-wide context. We sample the library-based code generation task to evaluate the ability of the model to solve tasks by using given library. RSQ: Learning from Important Tokens Leads to Better Quantized LLMs B. Extented Related Work Recent state-of-the-art open-source LLMs (Yang et al., 2024; Dubey et al., 2024; Jiang et al., 2024) typically exceed 50 billion parameters. While their released weights make them accessible, their massive weight size poses significant challenges for practitioners and limits the feasibility of finetuning or even inference with these models. Pruning and (Frantar & Alistarh, 2023; Sun et al., 2024b; Sung et al., 2024) and quantization are common strategies to compress these models weight, and this paper focuses on quantization as it is often more effective (Kuzmin et al., 2023). Post-training quantization (PTQ) has gained significant attention for its ability to quantize pre-trained models without requiring expensive retraining. Methods such as ZeroQuant (Yao et al., 2022) and QLoRA (Dettmers et al., 2023) apply round-to-nearest techniques to the weights, even without utilizing calibration datasets. While these approaches are extremely efficient, they often yield suboptimal performance due to their lack of information about the data distribution. To address this limitation, several PTQ methods leverage calibration datasets for improved quantization. These data-dependent PTQ methods are also often referred to as layer-wise quantization methods, as they typically quantize models one layer at time for efficiency. Specifically, GPTQ (Frantar et al., 2023) and OBC (Frantar & Alistarh, 2022) quantize weights and adjust the remaining weights with data-derived Hessian matrices (secondorder statistics) accordingly. AWQ (Lin et al., 2024b) minimizes quantization error by rescaling weights based on the activation distribution. QuIP# (Tseng et al., 2024), AQLM (Egiazarian et al., 2024) and QTIP (Tseng et al., 2025) represent groups of quantized weights with vectors instead of scalar values. Additionally, several approaches focus on fine-tuning parameters introduced during quantization (e.g., quantization indices, group scales, and group zeros), such as OmniQuant (Shao et al., 2024), HQQ (Badri & Shaji, 2023), and PV-Tuning (Malinovskii et al., 2024). One challenge in weight quantization is the presence of outliers in the weights, as LLMs often contain subset of weights with exceptionally large magnitudes (Dettmers et al., 2024; Kim et al., 2024; Yu et al., 2024). During quantization, these outliers increase the range of the quantized weights. As result, most weights are forced into narrow range to fit the outliers within the quantized representation, which ultimately leads to suboptimal performance. One way to address this challenge is through mixed-precision quantization (Dettmers et al., 2024; 2022; Kim et al., 2024); however, current hardware lacks efficient support for inference using this technique. Recent studies, such as QuIP (Chee et al., 2023) and QuaRot (Ashkboos et al., 2024b), have empirically shown that outliers in weights can be effectively mitigated by applying hadamard orthogonal transformations Table 7. Comparison of RSQ and QuaRot on LongEval tasks. The model is quantized to 3-bit. The best-performing method among all quantization approaches is highlighted in bold. Method LongEval L=300 L=460 L=620 Full Model 100 98.80 82.00 Avg 93.60 number of samples = 256, sequence length = QuaRot RSQ 99.470.3 99.600.2 90.872.6 94.673.9 52.806.0 58.0010.3 81.042.9 84.094.8 number of samples = 512, sequence length = QuaRot RSQ 99.670.1 99.800.1 90.872.7 93.731.7 54.473.5 55.337.1 81.670.2 82.952.7 number of samples = 1024, sequence length = QuaRot RSQ 99.800.2 99.670.3 85.531.8 88.336.3 49.602.2 52.0012.9 78.310.9 80.006.5 (hadamard rotations) (Ashkboos et al., 2024a), which reduce their impact and minimize the need for mixed-precision quantization. Furthermore, DuQuant (Lin et al., 2024a) and SpinQuant (Liu et al., 2024b) optimize the rotation matrix through data instead of relying on pre-defined rotations. Different from previous advancements in layer-wise quantization, RSQ focuses on improving the learning objective of layer-wise quantization methods (detailed in Sec. 4.2) by prioritizing important tokens. Our approach also builds on QuaRot by applying rotations to mitigate weight outliers and leveraging GPTQ for quantization. This integration makes RSQ comprehensive and holistic quantization strategy. C. Additional Evaluation Results and Analysis C.1. LongEval We evaluate the quantized LLaMA3 on the LongEval benchmark (details in Appendix A.2) and present the results in Tab. 7. Our findings indicate clear performance improvement with RSQ compared to QuaRot. Moreover, in the LongEval evaluation, we observe larger performance drop as input lengths increase, which aligns with the findings reported by Li et al. (2024a). C.2. Scaling Model Sizes for Qwen2.5 We also demonstrate RSQs and QuaRot performance on different sizes of Qwen2.5 modes (7B, 14B, and 32B) in Fig. 6, showing that RSQ still outperform the baseline for the three models. 15 RSQ: Learning from Important Tokens Leads to Better Quantized LLMs Figure 6. Ablation on model sizes using Qwen2.5. Figure 8. WikiText evaluation with three different context lengths for three models. C.4. Wiki Eval with Different Context Lengths Perplexity on WikiText is widely used metric for evaluating quantization approaches. However, the context length of the evaluation set can significantly impact perplexity values, and previous studies have employed different setups. For instance, for LLaMA models, AQLM (Egiazarian et al., 2024) reports results with context length of 4096, while QuaRot (Ashkboos et al., 2024b) uses 2048. While we use context length of 2048 for most of the experiments in this paper, we also report WikiText perplexity at additional context lengths (512 and 8192) in Fig. 8 for three models. We observe that the performance gap between different approaches remains relatively consistent across context lengths, suggesting that either of the lengths can serve as reliable metric for comparing methods. However, as expected, longer contexts generally lead to lower perplexity, likely because having more preceding tokens for LLMs to attend to improves prediction accuracy. Therefore, we emphasize that when using perplexity as comparison metric, it is crucial to maintain consistent settings to ensure fair and meaningful comparisons. Figure 7. Ablation results on WikiText-2 about quantizing different modules with RSQ. C.3. Applying RSQ on each module independently Generally, we apply RSQ to all transformer modules simultaneously, including query, key, and value projection layers in attention layers and up, gate, and down projection layers in feed-forward networks. In this section, we conduct an ablation study where RSQ is applied independently to each module, while the remaining modules use uniform token scaling (i.e., no scaling). The results, presented in Fig. 7, indicate that while most modules benefit from RSQ, the most significant improvement is observed in proj. We hypothesize that this is because the values (outputs of proj) have the most direct influence on all other tokens compared to other modules. However, we leave deeper exploration of this phenomenon for future work. 16 RSQ: Learning from Important Tokens Leads to Better Quantized LLMs Figure 9. Ablation study of AttnCon on SQ (without rotation). C.5. The Effect of Scaling without Rotation In previous experiments, we applied the scaling strategy to weights after rotation. In this section, we investigate its effect when the weights remain unrotated. We refer to this approach as SQ (Scale, then Quantize) and present the results of applying AttnCon on SQ in Fig. 9. Our findings indicate that the best perplexity is achieved at rmin = 0.1, which is significantly larger than the optimal value observed when weights are rotated (rmin = 0.005). This suggests that scaling is far more effective when applied to rotated weights. We leave further investigation of this phenomenon for future work. D. Visualization We visualize the token importance scores assigned by adaptive approaches for three samples and three layers (the 3rd, 11th, and 21st layers), as shown in Figs. 10 to 14. Note that we clamp the values into the range [0.05-th quantile, 99.95-th quantile] for better visualization. For TokenFreq  (Fig. 10)  , the scores are close to one for most tokens, suggesting that many tokens in sequence appear very less frequently. ActDiff  (Fig. 12)  does not exhibit any clear patterns. In contrast, ActNorm  (Fig. 11)  reveals that the first token tends to have slightly larger norm, particularly in the 3rd and 11th layers. For TokenSim  (Fig. 13)  , we observe that the first token is significantly less similar to others in earlier layers but becomes more similar in deeper layers. Lastly, AttnCon  (Fig. 14)  consistently assigns higher scores to the initial and final tokens across all layers. 17 RSQ: Learning from Important Tokens Leads to Better Quantized LLMs (a) Layer 3 (b) Layer 11 (c) Layer 21 Figures (a) - (c): first example. (d) Layer 3 (e) Layer 11 (f) Layer 21 Figures (d) - (f): second example. (g) Layer 3 (h) Layer (i) Layer 21 Figures (g) - (i): third example. Figure 10. Visualization of TokenFreq scores across three layers for three different examples. 18 RSQ: Learning from Important Tokens Leads to Better Quantized LLMs (a) Layer (b) Layer 11 (c) Layer 21 Figures (a) - (c): first example. (d) Layer 3 (e) Layer 11 (f) Layer Figures (d) - (f): second example. (g) Layer 3 (h) Layer 11 (i) Layer 21 Figures (g) - (i): third example. Figure 11. Visualization of ActNorm scores across three layers for three different examples. 19 RSQ: Learning from Important Tokens Leads to Better Quantized LLMs (a) Layer 3 (b) Layer 11 (c) Layer 21 Figures (a) - (c): first example. (d) Layer 3 (e) Layer 11 (f) Layer 21 Figures (d) - (f): second example. (g) Layer 3 (h) Layer (i) Layer 21 Figures (g) - (i): third example. Figure 12. Visualization of ActDiff scores across three layers for three different examples. 20 RSQ: Learning from Important Tokens Leads to Better Quantized LLMs (a) Layer (b) Layer 11 (c) Layer 21 Figures (a) - (c): first example. (d) Layer 3 (e) Layer 11 (f) Layer Figures (d) - (f): second example. (g) Layer 3 (h) Layer 11 (i) Layer 21 Figures (g) - (i): third example. Figure 13. Visualization of TokenSim scores across three layers for three different examples. 21 RSQ: Learning from Important Tokens Leads to Better Quantized LLMs (a) Layer 3 (b) Layer 11 (c) Layer 21 Figures (a) - (c): first example. (d) Layer 3 (e) Layer 11 (f) Layer 21 Figures (d) - (f): second example. (g) Layer 3 (h) Layer (i) Layer 21 Figures (g) - (i): third example. Figure 14. Visualization of AttnCon scores across three layers for three different examples."
        }
    ],
    "affiliations": [
        "Department of Computer Science, UNC at Chapel Hill"
    ]
}