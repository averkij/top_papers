{
    "paper_title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
    "authors": [
        "Geunhyuk Youk",
        "Jihyong Oh",
        "Munchurl Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 0 9 3 4 0 . 2 1 5 2 : r FMA-Net++: Motionand Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring Geunhyuk Youk KAIST rmsgurkjg@kaist.ac.kr Jihyong Oh Chung-Ang University jihyongoh@cau.ac.kr Munchurl Kim KAIST mkimee@kaist.ac.kr https://kaist-viclab.github.io/fmanetpp site/ (a) Qualitative comparison on challenging real-world videos. (b) Quantitative comparison on the GoPro [33] dataset. Figure 1. FMA-Net++ outperforms state-of-the-art methods in real-world qualitative results and quantitative benchmarks for VSRDB."
        },
        {
            "title": "Abstract",
            "content": "Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposurea key challenge largely overlooked by prior works and common artifact of auto-exposure or low-light capture. We present FMA-Net++, framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Timeaware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware FlowGuided Dynamic Filtering module to infer motionand exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposureand motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multiexposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos. 1. Introduction Joint video super-resolution and deblurring (VSRDB) [11, 18, 54] aims to restore sharp high-resolution (HR) videos from blurry low-resolution (LR) inputs. In practice, blurry LR video is common, and treating SR or deblurring separately is inadequate: SR cannot remove motion blur, while deblurring cannot recover high-frequency details, motivating joint VSRDB approach [36, 54]. Real-world degradations are further driven by two deeply intertwined facthe motion field determines the spatial patterns of tors: blur, and the exposure time controls its temporal extent and intensity [33, 34, 52]. Longer exposures, often used in low-light conditions, lead to severe motion blur, whereas shorter exposures can suffer from low signal-to-noise ratios [6, 63]. Compounding this, camera auto-exposure mechanisms vary the exposure dynamically [21, 52], resultCo-corresponding authors. Figure 2. Conceptual illustration and overview of the FMA-Net++ framework. ing in complex, spatio-temporally variant degradations that standard restoration methods struggle to model. While significant progress has been made in various video restoration [5, 29, 37, 54], most existing methods assume fixed exposure time. This assumption severely limits their robustness, as they struggle to handle the dynamically changing blur severity arising from real-world exposure variations. For instance, VSR [3, 5, 17, 26, 31] and video deblurring [25, 5759, 61] approaches may produce artifacts or inconsistent results when faced with exposure shifts. Even methods designed for unknown degradations, such as Blind VSR [1, 24, 37], typically assume spatiallyinvariant kernels and fail to model the physical process coupling motion and varying exposure. Furthermore, recent VSRDB approaches like FMA-Net [54], despite handling motion-dependent degradation, remained constrained by its fixed-exposure assumption. Thus, VSRDB methods explicitly addressing dynamic exposure are critically needed. Furthermore, beyond the exposure issue, prevailing temporal modeling strategies face inherent limitations: Slidingwindow architectures [17, 26, 43, 46] tend to suffer from limited temporal receptive fields, while recurrent propagation [3, 5, 12, 30, 31] lacks parallelizability, as conceptually compared in Fig. 2(a). To overcome these limits and address the aforementioned exposure issue, we introduce FMA-Net++, sequence-level framework that explicitly models motion-exposure coupling to guide restoration. The core architectural unit of FMA-Net++ is the Hierarchical Refinement with Bidirectional Propagation (HRBP) block. Instead of relying on restrictive sliding windows, such as those in FMA-Net [54], or inherently sequential recurrent structures [3, 5, 31], stacking HRBP blocks enables sequence-level parallelization and hierarchically expands the temporal receptive field to capture long-range dependencies. To handle dynamic exposure for which other methods [5, 18, 53, 54, 58] fail, each HRBP block includes an Exposure Time-aware Modulation (ETM) layer that conditions features on per-frame exposure, producing rich representations in temporal context and exposure information. Leveraging these representations, an exposure-aware Flow-Guided Dynamic Filtering (FGDF) module estimates physically grounded, motionand exposure-aware joint degradation kernels. Architecturally, we decouple degradation learning from restoration: the former predicts these rich priors, and the latter utilizes them to restore sharp HR frames efficiently, as illustrated in Fig. 2(b). To enable realistic and comprehensive evaluation, we construct two new benchmarks, REDS-ME (multiTrained exposure) and REDS-RE (random-exposure). solely on synthetic data, FMA-Net++ achieves state-ofthe-art (SOTA) accuracy and temporal consistency on our new benchmarks and the GoPro [33] dataset, outperforming recent methods in both restoration quality and inference speed, and showing strong generalization to challenging real-world videos (see Fig. 1). The main contributions of this work are as follows: We formulate and address the challenging real-world problem of VSRDB under unknown, dynamically varying exposure, and propose novel Exposure Time-aware Modulation (ETM) layer to explicitly condition features on per-frame exposure information. We design new parallel, sequence-level architecture based on Hierarchical Refinement with Bidirectional Propagation (HRBP) blocks, which effectively models long-range temporal dependencies by hierarchically expanding receptive fields without sequential dependencies. We develop an exposure-aware Flow-Guided Dynamic Filtering (FGDF) that utilizes exposure-conditioned features to estimate physically grounded, spatio-temporally variant degradation kernels capturing the joint effects of motion and exposure. We introduce two new benchmarks, REDS-ME and REDS-RE, for realistic evaluation under dynamic exposure, and demonstrate through extensive experiments that our method achieves state-of-the-art performance, strong real-world generalization, and high efficiency. 2. Related Work 2.1. Joint Video Super-Resolution and Deblurring VSRDB tackles the challenging task of jointly restoring sharp HR videos from blurry LR inputs where degradations arise from the physical coupling of motion and exposure. While single-task approaches for VSR [3, 5, 17, 26, 29, 50] 2 Figure 3. Architecture of FMA-Net++ for joint video super-resolution and deblurring (VSRDB). or video deblurring [25, 29, 58, 59, 64] have advanced, applying them sequentially often amplifies artifacts [36, 54]. However, specific methods tackling this joint VSRDB challenge remain scarce, as the field has received relatively little attention until recently. HOFFR [11], an early deep learning approach, showed promise but struggled with spatially variant blur due to standard CNN limitations. Although FMA-Net [54] introduced Flow-Guided Dynamic Filtering (FGDF) to handle motion-dependent degradation, it remained constrained by sliding-window design with an inherently limited temporal receptive field and fixedexposure assumption, making our approach conceptually distinct in both architecture and problem formulation. More recently, Ev-DeblurVSR [18] attempted to enhance VSRDB by incorporating auxiliary data from event streams (either simulated or captured by event cameras), proposing modules to fuse event signals for deblurring and alignment. However, this approach requires event data unavailable in standard videos and still assumes known and fixed exposure time, limitation explicitly discussed in [18], failing to address the challenges of dynamic exposure variations. These gaps motivate sequence-level, exposure-aware approach for robust VSRDB using only standard RGB inputs. 2.2. Temporal Modeling in Video Restoration Effectively modeling long-range temporal dependencies is crucial for video restoration tasks like VSR. However, prevailing strategies face inherent architectural trade-offs. Sliding-window approaches [17, 26, 41, 50, 54] operate on fixed local neighborhoods, constraining input flexibility and limiting the capture of long-range context. Conversely, recurrent methods [3, 5, 29, 31] propagate information sequentially, enabling longer temporal aggregation but remaining inherently sequential (hence less parallelizable) and potentially prone to vanishing gradients over long sequences [8, 31]. Transformer variants [2, 26, 28] alleviate some issues but are often still applied within slidingwindow context or incur significant computational complexity. Furthermore, most of these works target sharp inputs, lacking robustness to complex real-world degradations. This landscape motivates the need for sequencelevel backbones that hierarchically expand temporal receptive fields while enabling efficient parallel processing. 2.3. Exposure Time-Aware Restoration In real-world videos, auto-exposure mechanisms often vary the exposure time across frames, yielding spatio-temporally variant blur that fixed-exposure models cannot faithfully capture [21, 38, 52]. While most video restoration methods commonly assume fixed exposure [3, 5, 17, 26, 28, 29, 43], recent efforts in related tasks (e.g., video deblurring [21, 38] and frame interpolation [38, 52, 60]) estimate exposure or exploit auxiliary sensing (events) to guide restoration. However, they do not explicitly model the joint effects of motion and exposure within the VSRDB setting, and event-dependent designs limit practicality for standard 3 RGB videos. We instead introduce an Exposure Timeaware Modulation (ETM) layer that injects per-frame exposure information into temporal features and conditions the learning of degradation priors. In particular, we extend Flow-Guided Dynamic Filtering (FGDF) [54] to incorporate these exposure-aware features, yielding jointly position-, motion-, and exposure-dependent kernels that are physically grounded by the capture process, denoted as exposure-aware FGDF. This design enables exposure-aware VSRDB using only conventional RGB inputs and integrates seamlessly with our sequence-level backbone. 3. Method 3.1. Problem Formulation We address joint VSRDB under frame-wise varying exposure, where the per-frame exposure time te,i is unknown at test time. Given the blurry LR video = {Xi}T i=1 RT HW 3, our goal is to restore the corresponding sharp HR video ˆY = { ˆYi}T i=1 RT sHsW 3, where denotes an upscaling factor. The blur in Xi arises physically from integrating latent sharp signal over the exposure interval te,i while the scene moves [33, 38]. We approximate this complex process using discrete, learnable formulation with spatio-temporally variant, position-dependent degradation kernel Ki: Xi Ki , (1) where denotes filtering operation with stride s, and = {Yik, . . . , Yi+k} is short temporal neighborhood of sharp HR frames for small temporal radius k. Conceptually, the kernel Ki captures the joint effects of the exposure time te,i and the local motion field, which together determine the effective spatio-temporal receptive field integrated by each LR pixel. Our framework is designed to solve the corresponding inverse problem in two steps: first, estimate the exposureand motion-aware degradation priors (including the perframe kernels {Ki}T i=1 and associated motion information) directly from the input X; second, use these estimated priors to guide the restoration of ˆY . Detailed physics-based derivations are provided in Sec. 7.1 of Suppl. 3.2. Overall Architecture of FMA-Net++ FMA-Net++ consists of two main networks: Degradation Learning Network (NetD) and Restoration Network (NetR), both guided by pretrained Exposure Timeaware Feature Extractor (ETE). As illustrated in Fig. 3, both NetD and NetR are built upon stacks of HRBP blocks, which enable sequence-level parallel processing while hierarchically expanding the temporal receptive field. Given an input blurry LR sequence X, NetD first estimates degradation priors through the combination of HRBP, Figure 4. Details of an HRBP block. (a) Structure of the HRBP block at (j+1)-th refinement step for i-th frame (Sec. 3.3). (b) Structure of Multi-Attention. FFN refers to the feed-forward network of the transformer [10, 45]. ETM, and the exposure-aware FGDF module, explicitly modeling spatio-temporally variant degradations. NetR then restores the sharp HR video ˆY guided by these priors, also incorporating ETM to ensure exposure-aware feature adaptation during restoration. This decoupled design separates degradation learning from restoration, improving both accuracy and efficiency. 3.3. Hierarchical Refinement with Bidirectional Propagation (HRBP) As the core architectural unit shared by both NetD and NetR, the HRBP block overcomes the fundamental tradeoffs faced by prior temporal modeling strategies (Sec. 2.2): namely, limited temporal receptive fields in sliding-window methods [50, 54] and the lack of parallelizability in sequential recurrent approaches [5, 29]. By stacking HRBP blocks, our architecture enables sequence-level parallel processing. At each refinement level, information from increasingly distant past and future frames is aggregated bidirectionally, thus hierarchically expanding the temporal receptive field to effectively capture long-range dependencies. As shown in Fig. 4(a), each HRBP block iteratively rei RHW and set of multii R2HW (2+1)n for given frame fines the feature map flow-mask pairs at refinement step + 1. Specifically, is defined as: (cid:110) (f ii+1, ok ii+1), (f ii1, ok ii1) (cid:111)n k=1 (2) where is the number of multi-flow-mask pairs, each containing an optical flow and corresponding occlusion mask representing motion towards neighbors 1. Keeping multiple motion hypotheses (n > 1) enhances robustness under severe blur by providing one-to-many correspondences [4, 14]. The refinement process first computes intermediate features i via occlusion-aware warping [15, 36] of neighboring features , followed by fusion i1 using 4 i . + = and is predicted based on using concatenation and convolution. The multi-flow-mask pairs are then updated residually, j+1 , where the residual i . The intermediate feature i is further enhanced through two crucial modules before producing the final output j+1 Multi-Attention. As shown in Fig. 4(b), the multi-attention module employs self-attention [45] to capture spatial dependencies and integrate the propagated hierarchical temporal context. Within NetR, it subsequently applies DegradationAware (DA) attention. This cross-attention mechanism uses query derived from the estimated exposureand motion- (predicted by NetD), while aware degradation kernel KD key and value are projected from the self-attention output. This allows NetR features to adapt specifically to the estimated degradation characteristics of each frame. Exposure Time-aware Modulation (ETM). To handle frame-wise exposure variation, every HRBP block applies ETM via lightweight SFT layer [47]. Conditioned on per-frame exposure embedding ui R1C from the pretrained ETE, it predicts affine parameters (α, β) = M(ui) via shallow network and modulates the attention output ˆF + β. This injects essential exposure information into all refinement stages with negligible overhead, enabling adaptation to dynamic exposure variations (see detailed formulations in Sec. 7.2 of Suppl). In summary, compared to sliding windows [17, 26, 41, 50, 54], HRBP accesses long-range context via hierarchical propagation. Compared to recurrent schemes [3, 5, 29, 31], it avoids sequential dependencies, enabling efficient parallelization and stable training on long sequences. = (1 + α) ˆF as j+1 3.4. Exposure-Aware FGDF Conventional dynamic filtering [16] struggles with motion blur due to its fixed local neighborhood. FGDF [54] addresses this by performing filtering along motion trajectories. Instead of fixed neighbors, FGDF uses estimated optical flow to dynamically guide sampling locations for compact, position-dependent filter weights p. Formally, for reference frame r, the output yr(p) at pixel position is computed as: by the HRBP blocks (Sec. 3.3), the FGDF module within NetD operates on features already infused with exposure information by the ETM layer. Consequently, the predicted filter weights become jointly motion-aware (via flow guidance) and exposure-aware (conditioned on ETM features), enabling more physically grounded degradation modeling that captures the coupled effects of motion and exposure. Aligning with our decoupled design for efficiency, this exposure-aware FGDF is employed exclusively within NetD for prior estimation, while NetR uses simpler upsampling strategy (Sec. 3.5). 3.5. Degradation and Restoration Networks = (cid:8)f As outlined in Sec. 3.2 and illustrated in Fig. 3, our framework comprises two main networks leveraging the HRBP backbone to solve the inverse problem defined in Sec. 3.1. Degradation Learning Network (NetD). NetD aims to estimate degradation priors from the input blurry LR sequence X. It processes through stack of HRBP blocks with integrated ETM layers, producing refined features D,M and multi-flow-mask pairs D,M . From these outputs, NetD predicts two key priors for each frame Xi: (i) image flow- (cid:9), representing momask pairs ii1, oY tion between the sharp HR frame Yi and its neighbors, and (ii) jointly exposureand motion-aware degradation kernels KD , predicted via the exposureaware FGDF module (Sec. 3.4). This kernel formulation, representing the degradation from three consecutive sharp HR frames {Yi1, Yi, Yi+1} to the blurry LR frame Xi, follows the design principle of [54] as it offers robust trade-off between performance and computational cost. The shape of KD explicitly reflects its spatio-temporally variant and position-dependent nature, providing physically meaningful parameterization essential for modeling complex real-world degradations. R3HW k2 ii1 To ensure accurate prior estimation, NetD is trained with reconstruction objective: the predicted priors must reconstruct the blurry LR frame ˆXi from the ground-truth (GT) sharp HR frames as: ˆXi = (cid:0)KD {Yti}i+1 t=i1 (cid:1) , (4) yr(p) = (cid:88) m2 (cid:88) tN (r) k=1 t (pk) xtr (cid:0)p + pk (cid:1), (3) where denotes exposure-aware FGDF operation (Eq. 3) with stride s, and warped HR frame Yti is defined as: where (r) denotes the temporal neighborhood of r, xtr is the neighbor feature warped to using the estimated flow and occlusion masks, are the predicted weights at for neighbor t, and pk indexes the spatial offsets. This formulation generalizes the original FGDF [54] (which focused on the center frame) to arbitrary reference frames. In FMA-Net++, we crucially extend this FGDF mechanism specifically for modeling exposure-varying degradations. Leveraging the exposure-aware features produced Yti = (cid:40) Yi, W(Yt, it, oY it), if = if = (5) where denotes the occlusion-aware backward warping. Restoration Network (NetR). NetR performs the final restoration, taking the blurry LR sequence along with the rich priors predicted by NetD (F D,M , D,M , and KD) as input. It first generates initial features by combining and the context feature D,M using concatenation and 5 Table 1. Quantitative comparison of 4 VSRDB on REDS4-ME for two challenging exposure levels (5 : 4 and 5 : 5). All metrics are computed on the RGB channels. Red and blue indicate the best and second-best performance, respectively. Runtime is measured per LR frame of resolution 180 320. The superscript denotes models retrained on our proposed REDS-ME training set. Methods # Parameters (M) Runtime (s) REDS4-ME-5 : 4 PSNR / SSIM / tOF REDS4-ME-5 : 5 PSNR / SSIM / tOF SwinIR [27] + Restormer [55] HAT [7] + FFTformer [23] BasicVSR++ [5] + RVRT [29] IART [53] + BSSTNet [58] 11.9 + 26.1 20.8 + 16.6 7.3 + 13.6 13.4 + 52.0 0.221 + 0.753 0.352 + 1.414 0.048 + 0.349 1.041 + 0. 26.23 / 0.7464 / 3.775 26.66 / 0.7634 / 3.207 27.28 / 0.7901 / 2.887 27.50 / 0.8006 / 2.578 Super-Resolution + Deblurring Deblurring + Super-Resolution Restormer [55] + SwinIR [27] FFTformer [23] + HAT [7] RVRT [29] + BasicVSR++ [5] BSSTNet [58] + IART [53] 26.1 + 11.9 16.6 + 20.8 13.6 + 7.3 52.0 + 13.4 0.043 + 0.221 0.066 + 0.352 0.019 + 0.048 0.025 + 1. 26.36 / 0.7499 / 3.464 26.36 / 0.7534 / 3.256 26.35 / 0.7492 / 3.314 26.51 / 0.7711 / 3.103 Blind Video Super-Resolution DBVSR [37] Restormer [55] DBVSR [37] BasicVSR++ [5] IART [53] RVRT [29] BSSTNet [58] Ev-DeblurVSR [18] Ev-DeblurVSR [18] FMA-Net [54] FMA-Net [54] FMA-Net++ (Ours) 0.096 24.50 / 0.7208 / 3. 14.1 Joint Video Super-Resolution and Deblurring 26.5 14.1 7.3 13.4 12.9 52.0 8.3 8.3 9.6 9.6 12.8 27.45 / 0.7851 / 2.161 26.77 / 0.7629 / 3.021 27.70 / 0.7922 / 2.302 28.23 / 0.8153 / 2.143 28.11 / 0.8093 / 2.136 28.75 / 0.8342 / 1.893 24.51 / 0.7154 / 3.602 27.40 / 0.7839 / 2.521 26.42 / 0.7958 / 2.503 29.04 / 0.8275 / 1.891 29.66 / 0.8546 / 1.688 0.045 0.096 0.048 1.041 0.385 0.548 0.062 0.062 0.318 0.318 0.074 25.53 / 0.7229 / 4.558 25.92 / 0.7400 / 3.995 26.98 / 0.7621 / 3.164 27.26 / 0.7888 / 2.721 25.84 / 0.7316 / 3.948 25.87 / 0.7356 / 3.739 25.95 / 0.7424 / 3.610 26.33 / 0.7564 / 3.313 22.19 / 0.6122 / 4. 27.12 / 0.7750 / 2.516 26.07 / 0.7405 / 3.765 27.14 / 0.7770 / 2.746 27.64 / 0.7972 / 2.590 27.58 / 0.7944 / 2.558 28.11 / 0.8119 / 2.298 24.38 / 0.7047 / 4.094 26.82 / 0.7672 / 3.059 26.67 / 0.8005 / 2.443 28.51 / 0.8136 / 2.269 29.24 / 0.8453 / 1.956 an RDB [49]. These features are then refined through another stack of HRBP blocks, initializing the multi-flowmask pairs with D,M from NetD to leverage the motion prior. Crucially, within each HRBP block in NetR, the DA attention utilizes the estimated kernel KD as its query, after which ETM continues to provide exposure conditioning, enabling degradationand exposure-adaptive restoration. Finally, the refined features R,M pass through an upsampling block to predict high-frequency residual ˆY res . The final sharp HR frame is obtained by adding this residual to the bilinearly upsampled blurry LR input: ˆYi = ˆY res + Xi s, (6) where denotes the bilinear upsampling. 3.6. Training Strategy We adopt three-stage training strategy to effectively optimize FMA-Net++. First, the ETE is pretrained using supervised contrastive loss [20] on exposure labels to provide reliable guidance features, after which it is frozen. Second, guided by the pretrained ETE, NetD is trained to predict physically plausible degradation priors using reconstruction and motion prior losses. Finally, the entire FMANet++ framework (NetD and NetR) is jointly trained endto-end using restoration loss on the final sharp HR output. Detailed loss formulations and hyperparameter settings are provided in Sec. 7.3 and Sec. 8.1 of Suppl. 4. Experiment Results 4.1. Experimental Setup Datasets. We train FMA-Net++ on the proposed REDSME dataset, derived from REDS [34], utilizing all five synthesized exposure levels (from 5 : 1 to 5 : 5). The data generation process follows standard protocol [33, 34] and is detailed in Sec. 8.2 of Suppl. To evaluate, we use the most challenging levels (5 : 4, 5 : 5) of REDS4-ME (derived from the REDS4 test set). We also employ our proposed REDS-RE benchmark, which is derived from REDS4-ME by temporally mixing frames across all five exposure levels to assess robustness to dynamic exposure variations, along with the GoPro dataset [33] for generalization and challenging real-world videos. Evaluation Metrics. We evaluate restoration quality using PSNR and SSIM [51]. Temporal consistency is measured by tOF [9]. For real-world videos where GT is unavailable, we report no-reference metrics such as NIQE [32] and MUSIQ [19]. We also compare model efficiency in terms of the number of parameters and the runtime. Implementation Details. All implementation details, including network configurations, loss functions, etc., are provided in Sec. 8.1 of Suppl. for reproducibility. 4.2. Comparisons with State-of-the-Art Methods We compare FMA-Net++ against SOTA methods across single-image SR (SwinIR [27], relevant categories: 6 Table 2. Quantitative comparison of 4 VSRDB on REDS-RE and GoPro [33] datasets. Methods Restormer [55] DBVSR [37] BasicVSR++ [5] IART [53] RVRT [29] BSSTNet [58] EV-DeblurVSR [18] FMA-Net [54] FMA-Net++ (Ours) REDS-RE PSNR / SSIM / tOF 27.79 / 0.7953 / 1.775 27.30 / 0.7742 / 2.398 28.14 / 0.8044 / 1.904 28.68 / 0.8248 / 1.852 28.56 / 0.8208 / 1.926 29.33 / 0.8427 / 1.602 27.94 / 0.7987 / 2.039 29.29 / 0.8413 / 1.614 30.13 / 0.8643 / 1.360 GoPro PSNR / SSIM / tOF 27.54 / 0.8350 / 3.302 26.05 / 0.7815 / 4.730 27.40 / 0.8282 / 3.285 27.76 / 0.8394 / 3.302 27.64 / 0.8364 / 3.223 28.57 / 0.8650 / 2.753 27.25 / 0.8247 / 3.536 28.83 / 0.8655 / 2.727 30.49 / 0.9018 / 2.091 (GoPro [33]). On REDS-RE, featuring dynamic exposure transitions within sequences, the performance advantage of FMA-Net++ over other methods widens considerably compared to REDS-ME. This result strongly validates the effectiveness of our explicit exposure-aware modeling (ETM) in adapting to realistic varying exposure conditions where fixed-exposure assumptions struggle. On the unseen GoPro dataset, which exhibits different motion and scene characteristics from REDS-ME, FMA-Net++ again achieves the best performance across all metrics, indicating strong generalization ability beyond the training domain. Qualitative Results. Fig. 5 presents visual comparisons on synthetic benchmarks (REDS4-ME-5 : 5 and GoPro) that contain severe motion blur, while Fig. 1(a) shows results on challenging real-world videos captured with smartphone. On both synthetic and real-world data, FMA-Net++ consistently restores sharper details, cleaner edges, and more legible text with fewer artifacts, achieving the best perceptual quality (NIQE/MUSIQ). We omit multi-modal methods such as Ev-DeblurVSR [18] from the real-world comparison, as they are fundamentally not applicable to standard RGB videos that lack the required event data. This highlights the strong practicality and generalization of our approach, which achieves these results using only conventional RGB inputs despite being trained solely on synthetic data. Further qualitative results are provided in Sec. 10 of Suppl. Table 3. Comparison of temporal modeling strategies on REDS4ME-5 : 5. Our hierarchical strategy achieves the best accuracy and temporal consistency. Temporal Modeling Strategy Runtime (s) Sliding window-based [54] Recurrent-based [3, 5] Hierarchical-based (Ours) 0.314 0.086 0. PSNR 28.57 29.11 29.24 tOF 2.231 1.989 1.956 5. Ablation Study We present ablation studies validating our key design choices. Further ablation studies and detailed analyses can be found in Sec. 9 of Suppl. 5.1. Effectiveness of Hierarchical Architecture Figure 5. Qualitative comparisons of 4 VSRDB on REDS4-ME5 : 5 and GoPro [33]. Each scene contains severe motion blur with different characteristics. Best viewed in zoom. HAT [7]), single-image deblurring (Restormer [55], FFTformer [23]), VSR (BasicVSR++ [5], IART [53]), video deblurring (RVRT [29], BSSTNet [58]), Blind VSR (DBVSR [37]), and joint VSRDB (FMA-Net [54], EvDeblurVSR [18]). For fair comparison in the joint VSRDB setting under dynamic exposure conditions, relevant SOTA methods were adapted and retrained on our REDSME training set, denoted by in Tables 1 and 2. Quantitative Results. Table 1 presents the performance on REDS4-ME across two challenging exposure levels (5 : 4 and 5 : 5), representing severe motion blur. FMA-Net++ consistently outperforms all baselines across PSNR, SSIM, and tOF. For instance, FMA-Net++ achieves significant gains of 0.62 dB / 0.73 dB over the second-best model, FMA-Net on level 5 : 4 / 5 : 5, respectively. Furthermore, FMA-Net++ demonstrates superior efficiency compared to methods with similar complexity like RVRT [29]. It achieves remarkably higher performance while being significantly faster (over 5.2 speedup). This efficiency primarily arises from our parallelizable HRBP architecture. Combined with the high accuracy, this highlights the effectiveness of our overall design. The benefits of our upsampling choice are analyzed in the ablation study (Sec. 5.3). Table 2 evaluates robustness to dynamic exposure (REDS-RE) and generalization ability to an unseen dataset To validate the advantages of our proposed hierarchical temporal architecture, which is conceptually compared with 7 Table 4. Ablation study on the exposure time-aware feature extractor (ETE) for multiple datasets. Methods # Parameters (M) Runtime (s) FMA-Net++ w/o ETE FMA-Net++ w/ ETE 9.8 12.8 0.071 0.074 In-distribution REDS4-ME-5 : 4 REDS4-ME-5 : 5 PSNR / tOF PSNR / tOF 29.12 / 2.054 29.55 / 1.764 29.24 / 1.956 29.66 / 1.688 Out-of-distribution REDS-RE PSNR / tOF 29.72 / 1.436 30.13 / 1.360 GoPro PSNR / tOF 29.78 / 2.267 30.49 / 2.091 other temporal modeling strategies in Fig. 2(a), we quantitatively verify its effectiveness by comparing the full FMANet++ against two variants built upon its core components but employing different temporal modeling strategies: (i) sliding-window version processing three frames at time, similar to [54], and (ii) recurrent version where the hierarchical refinement is adapted for sequential propagation. All variants maintain the same number of HRBP blocks and utilize identical ETM and multi-attention mechanisms. Table 3 presents the comparison results on REDS4-ME5 : 5. Our hierarchical FMA-Net++ demonstrates substantial improvements over both variants. Compared to the slidingwindow variant, it yields markedly better results across all metrics, effectively overcoming the limitations imposed by fixed temporal receptive field. Compared to the recurrent variant, it still achieves superior performance in both PSNR and tOF. The noticeable gain in temporal consistency might stem from its non-recurrent hierarchical structure, which mitigates gradient-vanishing issues that can affect sequential propagation over long sequences. We also empirically observe that this design achieves the most stable training dynamics among the compared variants. Furthermore, in terms of efficiency, the hierarchical design also demonstrates modest speed advantage over the recurrent approach. Overall, these results validate that our hierarchical strategy serves as highly effective backbone for highquality and temporally consistent video restoration. 5.2. Effectiveness of Exposure-Aware Modeling We evaluate the contribution of our explicit exposure-aware modeling pipeline by comparing the full FMA-Net++ with ETE guidance against variant trained and tested without the ETE module. Table 4 summarizes the performance across multiple datasets. Incorporating ETE consistently improves both PSNR and tOF across all test sets. While the improvements on the in-distribution test sets (REDS4-ME-5 : 4 and 5 : 5) are noticeable, the performance gains become considerably more pronounced on the out-of-distribution datasets. Specifically, on REDS-RE, which features dynamic exposure transitions, and on the unseen GoPro dataset with different degradation characteristics, the advantage of using ETE widens significantly. This highlights that explicitly conditioning the features via ETM, guided by the ETE, is crucial for enhancing the models robustness and generalization ability when facing dynamically changing or entirely novel exposure conditions encountered in real-world scenarios. See more analyses and visual results in Sec. 9.1 of Suppl. Table 5. Ablation study on key components and design choices of FMA-Net++ on REDS4-ME-5 : 5. Methods # Params Runtime (s) REDS4-ME-5:5 PSNR / SSIM / tOF The number of HRBP blocks (a) = 1 (b) = 2 (c) self-attn [55] + SFT [48] 7.7 9.4 0.035 0.048 Multi-Attention 28.29 / 0.8174 / 2.461 28.74 / 0.8339 / 2.151 13. 0.066 28.86 / 0.8378 / 2.132 Low-Frequency Prediction in NetR (d) FGDF-based low-freq. prediction (e) FMA-Net++ (full configuration) 13.9 12. 0.087 29.29 / 0.8456 / 1.954 0.074 29.24 / 0.8453 / 1.956 5.3. Effectiveness of HRBP and Core Components We conduct ablation studies to validate the key components and design choices of FMA-Net++, summarizing the main results in Table 5. First, we investigate the impact of our hierarchical refinement strategy by varying the number of stacked HRBP blocks (M ). As shown in Table 5(a, b, e), increasing from 1 to 2 and finally to our full configuration (M = 4, row e) progressively improves both PSNR and tOF. This demonstrates the effectiveness of hierarchically expanding the temporal receptive field. As visualized in Fig. 11 of Suppl, features become progressively sharper and more structurally aligned through the stacked blocks, further validating our hierarchical design. Next, we validate the effectiveness of the DegradationAware (DA) attention within NetRs multi-attention module. Replacing DA attention with standard SFT layer [48] for modulation significantly degrades performance, confirming that explicitly leveraging the estimated degradation priors via DA attention is crucial for targeted restoration. Finally, we analyze our asymmetric design choice for efficiency. Compared to symmetric variant employing the complex FGDF for upsampling in NetR as well, our default approach (using bilinear upsampling plus residual) achieves comparable PSNR and tOF with approximately 1.1M fewer parameters and 15% faster inference. Given the marginal accuracy differences relative to the higher cost, we adopt the lightweight upsampling choice in our main model. 6. Conclusion In this paper, we addressed the challenging problem of joint VSRDB under unknown and dynamically varying exposure conditions. To tackle this challenge, we introduced FMA-Net++, novel framework built upon that enables effective sequence-level HRBP blocks temporal modeling with efficient parallel processing. injects per-frame Crucially, our proposed ETM layer exposure conditioning into the features. This allows our exposure-aware FGDF module to predict physically grounded degradation kernels that capture the coupled effects of motion and exposure. Extensive experiments on the proposed REDS-ME and REDS-RE benchmarks, as well as GoPro and real-world videos, demonstrate that FMA-Net++ achieves SOTA results, showcasing superior performance, efficiency, and robustness while generalizing effectively despite being trained solely on synthetic data."
        },
        {
            "title": "References",
            "content": "[1] Haoran Bai and Jinshan Pan. Self-supervised deep blind video super-resolution. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(7):46414653, 2024. 2 [2] Jiezhang Cao, Yawei Li, Kai Zhang, and Luc Van Gool. arXiv preprint Video super-resolution transformer. arXiv:2106.06847, 2021. 3 [3] Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Basicvsr: The search for essential components in video super-resolution and beyond. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49474956, 2021. 2, 3, 5, 7, 1 [4] Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Understanding deformable alignment in video super-resolution. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 973981, 2021. 4 [5] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and Improving video superChen Change Loy. Basicvsr++: resolution with enhanced propagation and alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 59725981, 2022. 2, 3, 4, 5, 6, 7, 1 [6] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. Learning to see in the dark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 32913300, 2018. 1 [7] Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. Activating more pixels in image superIn Proceedings of the IEEE/CVF resolution transformer. Conference on Computer Vision and Pattern Recognition, pages 2236722377, 2023. 6, 7 [8] Benjamin Naoto Chiche, Arnaud Woiselle, Joana FronteraPons, and Jean-Luc Starck. Stable long-term recurrent video super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 837846, 2022. [9] Mengyu Chu, You Xie, Jonas Mayer, Laura Leal-Taixe, and Nils Thuerey. Learning temporal coherence via selfsupervision for gan-based video generation. ACM Transactions on Graphics (TOG), 39(4):751, 2020. 6 [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, 9 Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 4, 2 [11] Ning Fang and Zongqian Zhan. High-resolution optical flow and frame-recurrent network for video super-resolution and deblurring. Neurocomputing, 489:128138, 2022. 1, 3 [12] Muhammad Haris, Gregory Shakhnarovich, and Norimichi Ukita. Recurrent back-projection network for video superIn Proceedings of the IEEE/CVF Conference resolution. on Computer Vision and Pattern Recognition, pages 3897 3906, 2019. 2 [13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 770778, 2016. [14] Ping Hu, Simon Niklaus, Stan Sclaroff, and Kate Saenko. Many-to-many splatting for efficient video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 35533562, 2022. 4 [15] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. Advances in Neural Information Processing Systems, 28, 2015. 4, 1 [16] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc Gool. Dynamic filter networks. Advances in Neural Information Processing Systems, 29, 2016. 5, 6 [17] Younghyun Jo, Seoung Wug Oh, Jaeyeon Kang, and Seon Joo Kim. Deep video super-resolution network using dynamic upsampling filters without explicit motion comIn Proceedings of the IEEE/CVF Conference pensation. on Computer Vision and Pattern Recognition, pages 3224 3232, 2018. 2, 3, 5 [18] Dachun Kai, Yueyi Zhang, Jin Wang, Zeyu Xiao, Zhiwei Xiong, and Xiaoyan Sun. Event-enhanced blurry video super-resolution. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 41754183, 2025. 1, 2, 3, 6, 7 [19] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 51485157, 2021. 6 [20] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:1866118673, 2020. 6, [21] Taewoo Kim, Jeongmin Lee, Lin Wang, and Kuk-Jin Yoon. Event-guided deblurring of unknown exposure time videos. In European Conference on Computer Vision, pages 519 538. Springer, 2022. 1, 3 [22] Diederik Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2014. 3 [23] Lingshun Kong, Jiangxin Dong, Jianjun Ge, Mingqiang Li, and Jinshan Pan. Efficient frequency domain-based transformers for high-quality image deblurring. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 58865895, 2023. 6, 7 [24] Suyoung Lee, Myungsub Choi, and Kyoung Mu Lee. Dynavsr: Dynamic adaptive blind video super-resolution. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 20932102, 2021. 2 [25] Dasong Li, Xiaoyu Shi, Yi Zhang, Ka Chun Cheung, Simon See, Xiaogang Wang, Hongwei Qin, and Hongsheng Li. simple baseline for video restoration with grouped spatialtemporal shift. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9822 9832, 2023. 2, 3 [26] Wenbo Li, Xin Tao, Taian Guo, Lu Qi, Jiangbo Lu, and Jiaya Jia. Mucan: Multi-correspondence aggregation network for In European Conference on Comvideo super-resolution. puter Vision, pages 335351. Springer, 2020. 2, 3, 5 [27] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 18331844, 2021. [28] Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang, Rakesh Ranjan, Yawei Li, Radu Timofte, and Luc Van Gool. arXiv preprint Vrt: video restoration transformer. arXiv:2201.12288, 2022. 3 [29] Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan, Eddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, Radu Timofte, and Luc Gool. Recurrent video restoration transformer with guided deformable attention. Advances in Neural Information Processing Systems, 35:378393, 2022. 2, 3, 4, 5, 6, 7, 1 [30] Jiayi Lin, Yan Huang, and Liang Wang. Fdan: Flow-guided deformable alignment network for video super-resolution. arXiv preprint arXiv:2105.05640, 2021. 2 [31] Chengxu Liu, Huan Yang, Jianlong Fu, and Xueming Qian. Learning trajectory-aware transformer for video superIn Proceedings of the IEEE/CVF Conference resolution. on Computer Vision and Pattern Recognition, pages 5687 5696, 2022. 2, 3, 5 [32] Anish Mittal, Rajiv Soundararajan, and Alan Bovik. Making completely blind image quality analyzer. IEEE Signal Processing Letters, 20(3):209212, 2012. 6 [33] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene In Proceedings of the IEEE/CVF Conference deblurring. on Computer Vision and Pattern Recognition, pages 3883 3891, 2017. 1, 2, 4, 6, 7, 3, [34] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte, and Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and superthe resolution: Dataset and study. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 19962005, 2019. 1, 6, 3 [35] Mehdi Noroozi, Paramanand Chandramouli, and Paolo Favaro. Motion deblurring in the wild. In German Conference on Pattern Recognition, pages 6577. Springer, 2017. 3, 6 In Proceedings of [36] Jihyong Oh and Munchurl Kim. Demfi: deep joint deblurring and multi-frame interpolation with flow-guided attentive correlation and recursive boosting. In European Conference on Computer Vision, pages 198215. Springer, 2022. 1, 3, 4 [37] Jinshan Pan, Haoran Bai, Jiangxin Dong, Jiawei Zhang, and Jinhui Tang. Deep blind video super-resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 48114820, 2021. 2, 6, 7 [38] Wei Shang, Dongwei Ren, Yi Yang, Hongzhi Zhang, Kede Ma, and Wangmeng Zuo. Joint video multi-frame interpolation and deblurring under unknown exposure time. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1393513944, 2023. 3, 4, 6 [39] Hyeonjun Sim, Jihyong Oh, and Munchurl Kim. Xvfi: In Proceedings of the extreme video frame interpolation. IEEE/CVF International Conference on Computer Vision, 2021. 1 [40] Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo Sapiro, Wolfgang Heidrich, and Oliver Wang. Deep video In Proceedings of the deblurring for hand-held cameras. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12791288, 2017. 3, [41] Xin Tao, Hongyun Gao, Renjie Liao, Jue Wang, and Jiaya Jia. Detail-revealing deep video super-resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 44724480, 2017. 3, 5 [42] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In European Conference on transforms for optical flow. Computer Vision, pages 402419. Springer, 2020. 2 [43] Yapeng Tian, Yulun Zhang, Yun Fu, and Chenliang Xu. Tdan: Temporally-deformable alignment network for video super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 33603369, 2020. 2, 3 [44] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9 (11), 2008. 4 [45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 4, 5, [46] Longguang Wang, Yulan Guo, Li Liu, Zaiping Lin, Xinpu Deng, and Wei An. Deep video super-resolution using hr optical flow estimation. IEEE Transactions on Image Processing, 29:43234336, 2020. 2 [47] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Recovering realistic texture in image super-resolution by the deep spatial feature transform. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 606615, 2018. 5, 2 In Proceedings of [48] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Recovering realistic texture in image super-resolution by the deep spatial feature transform. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018. 8 In Proceedings of [49] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In European Conference on Computer Vision Workshops, 2018. 10 [62] Shangchen Zhou, Jiawei Zhang, Wangmeng Zuo, Haozhe Xie, Jinshan Pan, and Jimmy Ren. Davanet: Stereo deblurring with view aggregation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1099611005, 2019. 3, 6 [63] Shangchen Zhou, Chongyi Li, and Chen Change Loy. Lednet: Joint low-light enhancement and deblurring in the dark. In European conference on computer vision, pages 573589. Springer, 2022. 1 [64] Chao Zhu, Hang Dong, Jinshan Pan, Boyang Liang, Yuhao Huang, Lean Fu, and Fei Wang. Deep recurrent neural network with multi-scale bi-directional propagation for video deblurring. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 35983607, 2022. 3 [50] Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, and Chen Change Loy. Edvr: Video restoration with enhanced In Proceedings of the deformable convolutional networks. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 00, 2019. 2, 3, 4, 5 [51] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600612, 2004. [52] Wenming Weng, Yueyi Zhang, and Zhiwei Xiong. Eventbased blurry frame interpolation under blind exposure. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15881598, 2023. 1, 3 [53] Kai Xu, Ziwei Yu, Xin Wang, Michael Bi Mi, and Enhancing video super-resolution via imAngela Yao. In Proceedings of the plicit resampling-based alignment. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 25462555, 2024. 2, 6, 7 [54] Geunhyuk Youk, Jihyong Oh, and Munchurl Kim. Fmanet: Flow-guided dynamic filtering and iterative feature refinement with multi-attention for joint video super-resolution and deblurring. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4455, 2024. 1, 2, 3, 4, 5, 6, 7, 8 [55] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image In Proceedings of the IEEE/CVF Conference restoration. on Computer Vision and Pattern Recognition, pages 5728 5739, 2022. 6, 7, 8, 3 [56] Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen, Gangshan Wu, and Limin Wang. Extracting motion and appearance via inter-frame attention for efficient video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5682 5692, 2023. 3 [57] Huicong Zhang, Haozhe Xie, and Hongxun Yao. Spatiotemporal deformable attention network for video deblurring. In European Conference on Computer Vision, pages 581 596. Springer, 2022. [58] Huicong Zhang, Haozhe Xie, and Hongxun Yao. Blur-aware spatio-temporal sparse transformer for video deblurring. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26732681, 2024. 2, 3, 6, 7 [59] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Wei Liu, and Hongdong Li. Adversarial spatio-temporal learning for video deblurring. IEEE Transactions on Image Processing, 28(1):291301, 2018. 2, 3 [60] Youjian Zhang, Chaoyue Wang, and Dacheng Tao. Video frame interpolation without temporal priors. Advances in Neural Information Processing Systems, 33:1330813318, 2020. 3 [61] Zhihang Zhong, Ye Gao, Yinqiang Zheng, and Bo Zheng. Efficient spatio-temporal recurrent neural network for video In European Conference on Computer Vision, deblurring. pages 191207. Springer, 2020. 2 11 FMA-Net++: Motionand Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring"
        },
        {
            "title": "Supplementary Material",
            "content": "In this Supplementary Materials, we first provide the detailed formulations of our method, including the physicsbased degradation model, full equations for our HRBP block, and detailed loss functions (Sec. 7). Subsequently, we detail the implementation and training setup, including network architectures and our new dataset generation pipeline (Sec. 8). Additionally, we present further ablation studies and visual analyses that validate our design choices (Sec. 9). We also provide additional qualitative comparisons on synthetic and real-world videos (Sec. 10). Finally, we discuss the limitations of our FMA-Net++ (Sec. 11). 7. Detailed Method 7.1. Detailed Problem Formulation As mentioned in Sec. 3.1 of the main paper, we address VSRDB under dynamically varying exposure. While most existing video restoration methods [3, 5, 29] have advanced temporal modeling, they typically assume fixed exposure time and do not explicitly address the impact of frame-wise dynamic exposure on the degradation model. Physics-based Degradation Model. foundational model in video deblurring [33] models blur formation as the temporal integration of latent sharp scene over fixed exposure interval te: = 1 te (cid:90) te 0 S(τ )dτ, (7) where is the blurry frame and S(τ ) is the latent sensor signal at continuous time τ . While this captures the core idea of temporal integration, it simplifies the process by not explicitly accounting for the continuous motion field or dynamically varying exposure. We generalize this physical model to incorporate these crucial real-world factors. The degradation process for the i-th blurry LR frame Xi at position is more accurately defined as: Xi(p) = Ds (cid:16) 1 te,i (cid:82) it+te,i it (q + (q, τ ), τ ) dτ (cid:17) , (8) where Ds is the spatial downsampling operator, is the HR coordinate corresponding to p, denotes the frame interval, te,i is the unknown, per-frame dynamic exposure time, and S(, τ ) is the latent sensor signal displaced by the continuous motion field (q, τ ). Learnable Degradation Kernel Formulation. Since directly inverting the continuous physical model (Eq. 8) is 1 intractable, we approximate it with discrete, learnable model, as shown in Eq. 1 of the main paper: Xi Ki , (9) where = {Yik, . . . , Yi+k} is short temporal neighborhood of sharp HR frames for small temporal radius k. To be rigorous, the ideal conceptual kernel Ki at position is complex function of both the exposure time te,i and the local motion field : Ki (p) = (te,i, {M (q, τ ) Ω (p) ; τ [i t, + te,i]}) , (10) where Ω (p) denotes the spatial neighborhood of HR coordinates corresponding to the LR pixel p, and the range of τ defines the temporal integration interval. This formulation (Eq. 10) explicitly shows how exposure and motion jointly create the spatio-temporally variant blur. Our framework learns to approximate this ideal kernel. The kernel predicted by our network, KD , is practical, learnable approximation of Ki. Our NetD achieves this by using the pretrained ETE to estimate the properties of te,i and using learned optical flow to approximate the continuous motion field . The FGDF module is then explicitly conditioned on both estimated parameters to predict KD , ensuring it is physically-plausible representation of the real-world degradation. 7.2. Detailed HRBP Block We provide the detailed update equations for the Hierarchical Refinement with Bidirectional Propagation (HRBP) block, expanding upon Sec. 3.3 of the main paper. Feature Refinement. As shown in Fig. 4(a) of the main paper, the intermediate refined feature i and the updated multi-flow-mask pairs j+1 are computed as: (11) i1i)(cid:1); = Conv(cid:0)concat(F i , i1i = W(F i1, ), , i + Conv(concat(f = j+1 where denotes the occlusion-aware backward warping [15, 36, 39]. Multi-Attention. As shown in Fig. 4(b) of the main paper, the Degradation-Aware (DA) attention in NetR uses query derived from the predicted degradation kernel KD . This degradation feature kj )), is computed as: (cid:1). = Conv(cid:0)KD kj (12) lead to representation drift, where the embedding tends to encode scene content rather than distinct exposure conditions. By freezing the ETE, we structurally prevent this feature entanglement, ensuring the embedding functions as decoupled sensor prior. This stable anchoring encourages the network to more faithfully model the physics of motion-exposure coupling, thereby enhancing training stability. Consequently, the framework achieves robust outof-distribution generalization, as validated in Table 4 of the main paper. The ETE is thus trained alone using the following contrastive loss: Le = 1 (cid:88) qB (cid:88) pP log exp (cid:0)qp/α(cid:1) (cid:80) pB{q} exp (qp/α) , (15) where denotes the anchor, contains positive samples with the same exposure label as the anchor, is the minibatch, and α is temperature parameter. In the second stage, guided by the pretrained ETE, we train NetD to predict physically-plausible degradation priors. To ensure the predicted priors are accurate, we use composite loss function LD as: LD = l1( ˆX, X) + λ1 (cid:88) l1(Yi1i, Yi) + λ2l1(f , i=1 RAFT). (16) The first term is the reconstruction loss, ensuring the predicted priors can reconstruct the original blurry LR input X. The second term is warping loss, where Yi1i represents the sharp HR neighboring frames warped into the current frame using the predicted image flow-mask pairs . This loss ensures the predicted motion priors are aci curate. The third term provides additional, direct supervision to the optical flow component, , within these pairs using pseudo-GT flows generated by pretrained RAFT model [42], which helps NetD produce physically meaningful motion estimations. In the final stage, we jointly train the entire FMA-Net++ framework with NetD and NetR. The total loss is defined as: Ltotal = l1( ˆY , ) + λ3LD, (17) where ˆY is the final restored HR sequence from NetR. The first term is the primary restoration loss, and the second term finetunes the pretrained NetD during joint training. 8. Detailed Experimental Setup We provide the implementation details omitted from Sec. 4 of the main paper. Figure 6. An overview of our three-stage training strategy. The final DA attention output is then computed using standard attention [10, 45]: DA(Q, K, ) = SoftMax (cid:16) QK (cid:17) , (13) , and K, are projected from where is projected from kj the self-attention output. Exposure Time-aware Modulation (ETM). The ETE module, which extracts the guidance signal ui R1C from the input frame Xi, consists of ResNet-18 [13] backbone. It is pretrained to distinguish exposure settings using supervised contrastive learning [20]. The shallow network Mj predicting the affine parameters (α, β) for the SFT layer [47] is implemented using simple convolutional layers. The modulation is applied as: = (1 + α) ˆF i + β; where α, β = Mj(ui), (14) is the feature map from the multi-attention modF j+1 where ˆF ule. 7.3. Detailed Training Strategy We adopt three-stage training strategy to effectively and stably train the components of FMA-Net++. This progressive approach ensures that each specialized module is welloptimized before being integrated into the full framework. Fig. 6 provides schematic overview of this strategy. We first pretrain the ETE to provide reliable guidance signal. This staged approach is adopted for two critical reasons. First, supervised contrastive learning [20] necessitates large batch sizes to learn discriminative representations, which is computationally infeasible when training the full framework end-to-end due to memory constraints. Second, and more importantly, we freeze the ETE after this pretraining stage. This design choice provides stable and invariant exposure reference space for the feature modulation in NetD and NetR. We empirically observed that co-optimizing the ETE with restoration objective can 2 Figure 7. Example frames from our REDS-ME dataset across five exposure levels and two different scenes. Each column corresponds to an exposure ratio from 5 : 1 (shortest exposure) to 5 : 5 (longest exposure). Longer exposures lead to increasingly severe motion blur, effectively simulating real-world camera motion. 8.1. Implementation Details We train FMA-Net++ using the Adam optimizer [22] with the default setting on 4 NVIDIA A6000 GPUs. In the first training stage, the ETE is trained with mini-batch size of 128, learning rate of 0.01, and α = 0.5 in Eq. 15. In the second stage, NetD is trained with mini-batch size of 8, using an initial learning rate of 2 104 that is reduced by half at 70%, 85%, and 95% of the total 280K iterations. The third stage jointly trains both NetD and NetR with the same batch size and learning rate schedule as in the second stage. FMA-Net++ is trained on 10-frame input sequences with patch size of 64 64 and evaluated on full-length videos. The SR scale factor is set to = 4 throughout all experiments. The number of HRBP blocks is = 4 for both NetD and NetR, and the number of multi-flow-mask pairs is = 9. For the input to the first HRBP block in NetD, these pairs are initialized with no initial motion and full visibility (i.e., = 0 and = 1). The degradation kernel size is kd = 20. The loss coefficients in Eqs. 16 and 17 are set to λ1 = 104, λ2 = 104, and λ3 = 0.1. Additionally, we adopt the multi-Dconv head transposed attention (MDTA) and Gated-Dconv feed-forward network (GDFN) modules proposed in Restormer [55] for the attention and feed-forward network in our multi-attention block. 8.2. REDS-ME and REDS-RE Benchmarks One significant challenge in VSRDB under dynamic exposures is the lack of benchmarks for performance evaluation. To address this, we construct two new benchmarks, REDSME (Multi-Exposure) and REDS-RE (Random-Exposure). Both REDS-ME and REDS-RE are derived from the REDS dataset [34], as described in the main paper. The data generation process follows the physical degradation formulation in Eq. 8, serving as its discrete, practical approximation. We follow widely adopted pipeline [33 35, 40, 62]: (1) the original 120 fps REDS videos are first interpolated to 1920 fps using EMA-VFI [56] to obtain sufficient intermediate frames for realistic motion blur simulation; (2) to simulate temporal integration, we average consecutive high-framerate frames, and the resulting blurry HR videos are then spatially downsampled using bicubic interpolation. We adopt this blur-then-downsample order as it better reflects real-world image formation and mitigates aliasing through temporal averaging [33, 34]. This yields realistic blurry LR videos suitable for robust VSRDB evaluation. To construct REDS-ME, we synthesize five variants of blurry videos by averaging different numbers of consecutive high-framerate frames, resulting in five exposure levels denoted by ratios from 5 : 1 (shortest exposure with minimal motion blur) to 5 : 5 (longest exposure with severe motion blur). This five-level configuration is motivated by the original REDS datasets temporal sampling strategy, where 120 fps source videos are converted to 24 fps sequences, covering temporal span equivalent to five consecutive source frames. Hence, our exposure ratios 5 : 15 : 5 align naturally with this structure while providing systematic range of motion-blur intensities. These levels also serve as pseudolabels for pretraining our ETE module. Fig. 7 shows example frames from our REDS-ME dataset. For training, we utilize all five exposure variations from the REDS-ME training set. For evaluation, we adopt the two most challenging exposure levels, 5 : 4 and 5 : 5, from REDS4-ME, which exhibit the most severe motion blur. REDS4-ME is derived from the REDS4 subset1, commonly used test set in prior works [26, 31, 50, 54]. Furthermore, to explicitly evaluate robustness under dynamically varying exposure conditions, we construct the REDS-RE benchmark by temporally mixing frames from all five exposure levels within each REDS4-ME test scene. To simulate the temporal inertia and smoothness of realworld auto-exposure mechanisms, we employ structured, interval-based random walk strategy rather than simple frame-wise randomization. Specifically, the exposure level is updated only at fixed intervals (every 5 or 7 frames). At 1Clips 000, 011, 015, and 020 from the REDS training set. 3 overlap for longer exposures (5 : 4, 5 : 5) is attributable to the inherent visual ambiguity caused by severe motion blur, yet the overall clustering trend demonstrates that the ETE captures exposure-dependent structure in the feature space. each update step, the exposure level is uniformly sampled to increment, decrement, or remain constant, constrained within the range of available levels (5 : 1 to 5 : 5). As visualized in Fig. 12, this process yields diverse, step-wise exposure trajectories that effectively approximate realistic, smooth, yet non-stationary capture scenarios. Additionally, we assess the generalization ability of our model using the standard GoPro dataset [33], which differs from REDS in motion patterns and exposure characteristics. Following standard VSRDB protocols [54], we apply bicubic downsampling to the blurry GoPro videos to generate the blurry LR inputs. Figure 9. Effect of ETE guidance on the exposure-aware degradation kernels predicted by NetD. For severely blurred frame from REDS4-ME-5 : 5, the kernel becomes spatially diffuse with correct exposure guidance (5 : 5) and highly concentrated with incorrect guidance (5 : 1), demonstrating exposure-dependent behavior. The Effect of ETE on Exposure-Aware Kernels. To visually demonstrate the synergy between our exposureaware modeling and other modules, we analyze how the guidance from ETE directly influences the degradation kernels predicted by FGDF. Fig. 9 illustrates this relationship. We take severely blurred frame from REDS4-ME-5 : 5 and provide our trained NetD with two exposure-aware features extracted from the same scene but under different exposure conditions: (i) correct guidance from the corresponding 5 : 5 frame, and (ii) incorrect guidance from the 5 : 1 frame of the same scene. When provided with the correct guidance, NetD predicts spatially diffuse kernel that accurately models the motion blur, whereas incorrect exposure guidance yields highly concentrated kernel, indicating that the model is misled into assuming less severe blur. This confirms that FGDF is effectively and sensitively conditioned on the exposure information provided via ETM. Table 6. Sensitivity analysis of FMA-Net++ to ETE guidance on REDS4-ME-5 : 5. Input frames are fixed (5 : 5), while the exposure guidance features are varied from 5 : 1 to 5 : 5. Input Frame Exposure-aware feature PSNR / tOF 5 : 5 from 5 : 5 (Correct) from 5 : 4 from 5 : 3 from 5 : 2 from 5 : 1 Baseline w/o ETE (from Table 4) 29.24 / 1.956 29.20 / 1.972 29.13 / 2.012 29.11 / 2.027 29.07 / 2.041 29.12 / 2.054 Sensitivity to ETE Guidance. We further investigate how this sensitivity to guidance affects the final restoration performance. We evaluate our model on the REDS4ME-5 : 5 input frames using exposure guidance features extracted from all five exposure levels (5 : 1 to 5 : 5), and Figure 8. t-SNE [44] visualization of exposure time-aware features ui extracted by ETE, showing their distinguishability across different exposure levels. 9. Further Ablation Studies In this section, we provide further ablation studies and detailed visual analyses that were omitted from Sec. 5 of the main paper. We first present detailed quantitative and qualitative analyses of our core contributions, namely exposureaware modeling and architectural design. We then provide ablations for other key components, such as our filtering mechanism and loss functions. 9.1. Detailed Analysis of Exposure-Aware Modeling (ETE) While Sec. 5.2 of the main paper demonstrated the quantitative contribution of the ETE module, here we provide the full, detailed analyses, including feature visualizations and sensitivity studies. Visualization of Exposure Features. To analyze how ETE encodes exposure-specific information, we visualize the per-frame features ui using t-SNE [44] in Fig. 8. The 2D projections show clearly separated clusters for shorter exposure levels (5 : 1 to 5 : 3), indicating that ETE successfully captures distinguishable characteristics. The partial 4 present the results in Table 6. The results reveal an insightful characteristic of our framework. As expected, performance gradually degrades as the provided guidance deviates from the correct one, confirming that our model is indeed effectively leveraging the ETE guidance. However, the key observation is that performance does not severely fail even with the most incorrect guidance. This demonstrates desirable robustness: FMA-Net++ does not blindly depend on the ETE predictions but can instead rely on the strong spatio-temporal context provided by its HRBP backbone to achieve reasonable restoration, highlighting its robust design. 9.2. Analysis of Architectural Design We provide further analyses on our architectural design to validate the choices discussed in the main paper (primarily Sec. 3.3 and Sec. 5.3). This section details the impact of multi-flow hypotheses, visualizes the hierarchical refinement process, and provides direct qualitative comparison against the FMA-Net framework. Table 7. Ablation study for the number of multi-flow-mask pairs (n) on REDS4-ME-5 : 5. # # Params (M) Runtime (s) = 1 = 5 = 9 11.9 12.3 12.8 0.073 0.074 0. REDS4-ME-5:5 PSNR / SSIM / tOF 28.52 / 0.8248 / 2.357 28.97 / 0.8387 / 2.106 29.24 / 0.8453 / 1.956 Effect of the Number of Multi-Flow-Mask Pairs. We analyze how the number of multi-flow-mask pairs affects performance and stability in motion estimation. As shown in Table 7, increasing consistently improves restoration quality with negligible computational overhead. larger number of pairs enables the model to establish more one-tomany correspondences, effectively leveraging multiple motion hypotheses, which is especially important under severe motion blur where single flow estimation can be unreliable. Fig. 10 visualizes this effect. With only one pair (n = 1), the predicted optical flow is noisy and spatially distorted, failing to capture accurate motion boundaries. In contrast, using nine pairs (n = 9) produces much cleaner and sharper flow fields that align well with object motion. This confirms that the multi-flow mechanism remains effective for robust motion modeling under challenging degradation conditions. We thus retain this component and set = 9 in our final configuration. Visualization of Hierarchical Feature Refinement. We visualize the intermediate representations of the refined feature R,j across four refinement stages in Fig. 11 to illustrate how the HRBP blocks progressively operate. As shown in the figure, the initial stage exhibits noisy and Figure 10. Effect of the number of multi-flow-mask pairs (n) on predicted optical flow for severely blurred scene. spatially diffuse activations, while later stages produce increasingly sharper and more structurally aligned features, with high-frequency details (e.g., building edges) becoming more prominent. This progressive sharpening provides strong evidence that our hierarchical refinement strategy iteratively enhances feature quality, leading to sharper and more temporally consistent outputs. Figure 11. Visualization of the progressive feature refinement through HRBP blocks across four iterations. Qualitative Comparison with FMA-Net. As shown in Tables 1 and 2 of the main paper, our FMA-Net++ outperforms the retrained FMA-Net (which uses slidingwindow approach). To complement these quantitative results, we further provide direct visual comparison between the two models in challenging scenes that contain strong motion blur and low spatial redundancy (e.g., human faces). As shown in Fig. 13, FMA-Net suffers from temporal misalignment and produces distorted facial structures, while our FMA-Net++ reconstructs sharper edges and more temporally consistent details. These results visually confirm that the proposed hierarchical refinement and exposure-aware modeling provide notable improvements over the FMA-Net framework. Table 8. Comparison of exposure-aware FGDF and conventional dynamic filtering (CDF) [16] on REDS4-ME-5 : 5, reporting NetD performance. Each cell reports PSNR/tOF values averaged within each motion magnitude interval. Network: NetD [0,20) [20,40) 40 CDF exposure-aware FGDF 47.67 / 0.046 48.57 / 0.040 42.92 / 0.228 44.21 / 0.197 34.99 / 0.688 37.38 / 0.637 Table 9. Ablation on the loss coefficients (λ1 and λ2) used in LD on the REDS4-ME-5 : 5 test set. Hyperparameters PSNR / SSIM / tOF Analysis on Warping Loss (λ1) λ1 = 103 λ1 = 5 105 29.13 / 0.8395 / 2.013 29.20 / 0.8437 / 1. Analysis on RAFT Supervision (λ2) λ2 = 0 (w/o RAFT) λ2 = 103 λ2 = 5 105 λ1 = 104, λ2 = 104 (Final Model) 29.07 / 0.8347 / 2.143 29.12 / 0.8391 / 2.022 29.16 / 0.8409 / 1.998 29.24 / 0.8453 / 1.956 loss terms are essential for achieving optimal performance. First, adjusting the weight (λ1) of the warping loss term significantly affects the final restoration quality: an overly large weight interferes with the primary reconstruction objective, while weight that is too small fails to enforce accurate alignment in the sharp HR space. Second, removing the RAFT supervision (λ2 = 0) causes notable drop in performance, confirming that pseudo-GT flow supervision is crucial for learning accurate motion priors. Our chosen coefficients (λ1 = λ2 = 104) provide the best trade-off, yielding the highest performance across all metrics. 10. Additional Qualitative Results We provide additional qualitative comparisons complementing the results shown in the main paper (Fig. 1(a) and Fig. 5). Further results on challenging scenes from the REDS4-ME-5 : 5 and GoPro test sets are shown in Fig. 14. Finally, examples on real-world video restoration are presented in Fig. 15. Crucially, these real-world smartphone videos contain continuous auto-exposure transitions that naturally fall between the discrete synthetic levels (5 : 1 to 5 : 5) used during training. Although the ETE is pretrained only on discrete exposure anchors, the successful restoration in these scenarios provides clear empirical evidence that FMA-Net++ does not rely on rigid exposure bins. Instead, the exposure-aware feature space exhibits smooth transitions in practice, allowing the model to generalize well to intermediate, unseen exposure states and adapt its restoration accordingly. These results further demonstrate the effectiveness of FMA-Net++ over SOTA baselines in restoring sharp details and cleaner structures. 11. Limitations 11.1. Limitations of Synthetic Datasets Our proposed benchmarks, REDS-ME and REDS-RE, are constructed by averaging high-framerate frames to simulate motion blur under varying exposure conditions. While this approach follows standard protocols in video deblurring and restoration [33, 35, 38, 40, 62], the linear averaging process may not fully capture the complex and nonlinear responses Figure 12. Visualization of synthesized exposure trajectories in the REDS-RE benchmark. Each colored line represents the evolution of the exposure level for different test scene. The trajectories follow step-wise random walk with varying update intervals, simulating the stable yet dynamic nature of real-world auto-exposure adjustments. Figure 13. Qualitative comparison between FMA-Net++ (Ours) and FMA-Net [54] in challenging scene featuring facial details and severe motion blur. 9.3. Effect of Exposure-Aware FGDF FGDF was originally introduced in FMA-Net [54] to perform motion-aware filtering along optical-flow trajectories. In FMA-Net++, we enhance FGDF by conditioning the filtering weights on exposure-aware features (Sec. 3.4). To verify that this extension preserves its motion-aware advantage, we compare the exposure-aware FGDF with the conventional dynamic filtering (CDF) [16] on REDS4-ME-5 : 5. As shown in Table 8, the exposure-aware FGDF maintains clear and significant performance advantage over CDF across all motion magnitudes. This result confirms that our exposure-aware conditioning effectively strengthens the underlying motion-aware degradation modeling, especially in challenging high-motion, long-exposure scenarios. 9.4. Analysis of Loss Functions We validate the design of our composite loss function LD (Eq. 16), which guides the training of NetD. Specifically, we analyze the impact of the coefficients for the warping loss (λ1) and the RAFT supervision loss (λ2) on the REDS4-ME-5 : 5 test set. As summarized in Table 9, both 6 of real-world camera sensors. Moreover, our datasets do not account for other challenging factors such as spatiallyvarying lighting or sensor noise, which can be coupled with exposure changes. Nevertheless, this controlled setup provides practical and systematic way to analyze exposureinduced degradation under varying conditions. The construction of new benchmarks that more faithfully model these intricate, real-world camera pipelines remains an important challenge for future research. 11.2. Limitations of Flow-based Motion Modeling FMA-Net++ relies on 2D optical flow to model motion between frames. As with most flow-based video restoration approaches [3, 5, 53, 54, 58], this inherently limits reliability under large out-of-plane rotations or complex non-rigid motions, where 2D correspondences become ambiguous. While our hierarchical refinement and exposure-aware design mitigate some of these issues in practice, fully addressing such 3D motion effects would require more advanced motion models (e.g., 3D motion fields or geometry-aware representations), which we leave as an interesting direction for future work. 7 Figure 14. Additional qualitative comparisons on the REDS4-ME-5 : 5 and GoPro [33] datasets. These scenes feature severe motion blur and complex textures, representing challenging degradation scenarios. FMA-Net++ consistently reconstructs sharper structural details and cleaner edges while effectively suppressing motion artifacts, outperforming state-of-the-art methods. Best viewed in zoom. 8 Figure 15. Additional qualitative comparisons on challenging real-world videos captured with smartphones. These videos contain continuous auto-exposure transitions and non-uniform motion blur, deviating significantly from the discrete synthetic conditions used during training. FMA-Net++ exhibits strong generalization, recovering legible text and fine textures while preserving natural exposure characteristics, and consistently achieves the best perceptual scores (NIQE / MUSIQ). Best viewed in zoom."
        }
    ],
    "affiliations": [
        "Chung-Ang University",
        "KAIST"
    ]
}