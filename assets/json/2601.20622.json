{
    "paper_title": "SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation",
    "authors": [
        "Boyu Li",
        "Lin-Ping Yuan",
        "Zeyu Wang",
        "Hongbo Fu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sketching provides an intuitive way to convey dynamic intent in animation authoring (i.e., how elements change over time and space), making it a natural medium for automatic content creation. Yet existing approaches often constrain sketches to fixed command tokens or predefined visual forms, overlooking their freeform nature and the central role of humans in shaping intention. To address this, we introduce an interaction paradigm where users convey dynamic intent to a vision-language model via free-form sketching, instantiated here in a sketch storyboard to motion graphics workflow. We implement an interface and improve it through a three-stage study with 24 participants. The study shows how sketches convey motion with minimal input, how their inherent ambiguity requires users to be involved for clarification, and how sketches can visually guide video refinement. Our findings reveal the potential of sketch and AI interaction to bridge the gap between intention and outcome, and demonstrate its applicability to 3D animation and video generation."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 2 ] . [ 1 2 2 6 0 2 . 1 0 6 2 : r SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation Boyu Li Arts and Machine Creativity The Hong Kong University of Science and Technology Hong Kong, China blibr@connect.ust.hk Lin-Ping Yuan Computer Science and Engineering The Hong Kong University of Science and Technology Hong Kong, China yuanlp@cse.ust.hk Zeyu Wang Computational Media and Arts The Hong Kong University of Science and Technology (Guangzhou) Guangzhou, China Computer Science and Engineering The Hong Kong University of Science and Technology Hong Kong, China zeyuwang@ust.hk Hongbo Fu Arts and Machine Creativity The Hong Kong University of Science and Technology Hong Kong, China fuplus@gmail.com Figure 1: Example interaction with SketchDynamics. (A) user sketches storyboard of traffic-light scene (red stop, yellow flashing, green go). (B-Left) The system proactively raises clarification questions (e.g., whether the car starts moving immediately or after delay, and whether car SVG should be provided), and the user responds by selecting options or uploading assets. (B-Right) The user further refines the generated frames by specifying that the yellow light flashes twice and by sketching velocitytime curve to indicate changes in the cars speed. (C) At the bottom, the final video clips illustrate the produced animation sequence. Corresponding author. Please use nonacm option or ACM Engage class to enable CC licenses This work is licensed under Creative Commons Attribution 4.0 International License. CHI 26, April 1317, 2026, Barcelona, Spain 2026 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-2278-3/2026/04 https://doi.org/10.1145/3772318.3791071 Abstract Sketching provides an intuitive way to convey dynamic intent in animation authoring (i.e., how elements change over time and space), making it natural medium for automatic content creation. Yet existing approaches often constrain sketches to fixed command tokens or predefined visual forms, overlooking their free-form nature and the central role of humans in shaping intention. To address this, CHI 26, April 1317, 2026, Barcelona, Spain Boyu Li, Lin-Ping Yuan, Zeyu Wang, and Hongbo Fu we introduce an interaction paradigm where users convey dynamic intent to visionlanguage model via free-form sketching, instantiated here in sketch storyboard to motion graphics workflow. We implement an interface and improve it through three-stage study with 24 participants. The study shows how sketches convey motion with minimal input, how their inherent ambiguity requires users to be involved for clarification, and how sketches can visually guide video refinement. Our findings reveal the potential of sketchAI interaction to bridge the gap between intention and outcome, and demonstrate its applicability to 3D animation and video generation. CCS Concepts Human-centered computing Interactive systems and tools. Keywords Free-Form Sketch, Dynamic Intention, Creativity Support ACM Reference Format: Boyu Li, Lin-Ping Yuan, Zeyu Wang, and Hongbo Fu. 2026. SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation . In Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI 26), April 1317, 2026, Barcelona, Spain. ACM, Yokohama, Japan, 20 pages. https://doi.org/10.1145/3772318."
        },
        {
            "title": "1 Introduction\nSketching is one of the most accessible and intuitive ways for users\nto visually convey dynamic authoring intent, i.e., how they wish\nelements to change over time and space, particularly in expressive\ncontent domains such as film and animation [4, 7]. Traditionally,\nsketches are used in two complementary ways to express dynamic\nintent. The first is as storyboards [27]. From early Disney anima-\ntion [51] to contemporary film production [52], sketches are com-\nbined with scripts to depict keyframes and narrative flow. The\nsecond is as free-form annotations, where marks such as arrows,\ndashed lines, and curves are used to suggest temporal evolution\nor spatial transformations [13]. Together, these practices enable\npeople to represent sequences of motion and interaction without\nrequiring full implementation, offering a lightweight yet powerful\nfoundation for subsequent production.",
            "content": "Building on these practices, research has explored how to interpret users authoring intent from sketches and translate that intent into final content. For example, generative models can add and remove objects in video based on sketch drawn on single video frame [44, 45], or generate an entire video sequence from storyboard sketch [43, 75]. Beyond generation, sketches have also been explored for incremental authoring [14, 31, 34], where users construct animations step by step and each stroke is recognized [66] or matched to predefined command [14], triggering corresponding system actions (e.g., appearing, translating, or morphing) that gradually accumulate into complete animated result. However, these approaches often constrain the expressive potential of sketches, limiting them either to static layout representations or to predefined visual forms that encode specific animation commands. As result, sketches shift from being an approach for open-ended creativity to functioning as rigid control tokens, missing the opportunity to harness their flexibility and spontaneity for expressing free-form dynamic intent, such as motion and interaction [63]. We aim to investigate how free-form sketches can effectively capture dynamic intent and be leveraged as foundation for generating dynamic content. Fortunately, recent advances in visionlanguage models (VLMs) have introduced powerful visual understanding capabilities, including the ability to interpret abstract and informal sketches. Such capabilities have been demonstrated in recent works such as CodeShaping [69], though primarily in the domain of code generation. We select motion graphics as our starting point, as they offer accessible yet expressive animations with clear motion dynamics, making them practical setting for investigating the alignment between sketches and generated animations. Specifically, we focus on explainer-style motion graphics with the roles of teaching aid and visual discourse [11], which are widely used across diverse domains and age groups [29], prioritizing the clarity of motion over visual realism to effectively communicate concepts [24]. Building on this foundation, we conduct three-stage study to explore how sketch-based interactions can drive dynamic content creation. In the first stage, to study sketch expressiveness and VLM interpretation, we provide unified web interface where users can draw free-form sketch storyboards to generate animated videos through code-based rendering. The results highlight the strong potential of free-form sketches to convey animation intent with minimal, intuitive input (see Fig. 5 for collection of such examples), though their inherent ambiguity often leaves user intention underspecified. To address this, the second stage introduces clarification cue that adaptively matches sketch ambiguity level, offering lightweight interface interactions for users to intervene in the VLMs interpretation. Findings suggest that clarification turns sketch ambiguity into resource for intent articulation, but some intentions only become concrete when users view the generated output and refine it accordingly. Therefore, in the last stage, we develop an intuitive visual editing approach that allows users to express their intent on top of the generated result directly. The refinement mechanism closes the gap between system output and user intent, allowing precise control with low effort. In general, rather than pursuing high-quality video results, our contribution lies in reactivating the expressive freedom of sketches, which are casual and intuitive drawings that anyone can produce to freely convey dynamic intent without any constraints. Here are our contributions: We propose sketch-based dynamic intent expression paradigm where sketches are not predefined forms or fixed commands, but as open-ended prompts, in which sketch ambiguity, VLM interpretation, and user intervention collaboratively shape dynamic content. We implement SketchDynamics, proof-of-concept instantiation of this paradigm, using free-form sketch storyboards to generate explainer-style motion graphics, supported by an adaptive clarification interface and multimodal input for iterative refinement. Through three-stage user study, we validate the effectiveness of this paradigm. We demonstrate its capacity to support diverse free-form sketches, enabling users to articulate precise dynamic intent with minimal interaction. SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation CHI 26, April 1317, 2026, Barcelona, Spain"
        },
        {
            "title": "2 Related Work\n2.1 Understand Authoring Intent in Sketch\nSketching has long been valued for its ability to convey authoring\nintent through just a few strokes, making it a widely adopted prac-\ntice in early-stage ideation across fields such as architecture [38, 72],\nart, animation, and user interface design [34]. Numerous HCI re-\nsearchers have explored how sketches can produce meaningful\nresults for rapid prototyping, emphasizing their immediacy and\nfluidity as a medium [10].",
            "content": "A major part of this research treats sketches or single strokes as predefined symbols or commands [55, 66, 71]. With simple stroke recognition methods [19] like the $1 recognizer [66], the system could classify sketches and trigger specific operations. seminal example is SILK [34], which allowed interface designers to hand-draw UI widgets and manipulate them with drawn strokes. Lineogrammer [73] extended this idea by beautifying sketches into clean diagrams and introducing more flexible action sketches, such as joining lines or lasso selection, enabling intuitive manipulation without tool switching. Using similar technical pipeline, K-Sketch [14] introduces an interface that enables novice users to create 2D animations through informal sketches. These interfaces provided immediate feedback by recognizing drawn shapes as components and commands, preserving the speed and informality of sketching. However, as shown in Fig 2, these systems interpret sketches through rigid one-to-one mappings, translating recognized symbols (e.g., arrows, lassos) directly into predefined graphical transformations (e.g., translation, rotation) or specific animation effects (e.g., morphing, fading). To support such mappings, designers needed to predefine gesture categories and their corresponding actions, meaning that users had to learn and conform to the systems sketchaction vocabulary. As result, while interaction felt informal and fluid, the semantic space of motion intent remains constrained to what had been explicitly encoded, preventing sketches from expressing abstract, causal, or contextual forms of motion intent. Therefore, some research has moved beyond hand-coded sketch mapping, learning to infer higher-level intent behind sketch [26]. Instead of asking what symbol was drawn?, these systems learn to ask what does the user mean by this drawing? In user interface design, Swire [22] exemplifies this shift by training neural network on thousands of drawn UIs: it can interpret free-form sketch of UI layout in terms of its semantic similarity to existing interface examples. Samuelsson et al. [56] conducted an elicitation study with software engineers to understand how sketches might express code editing commands. Building on such insights, Code Shaping [69] introduced an interactive environment where developers draw directly over their source code to invoke edits. Behind this is the ability of current visionlanguage models: rather than simple classification, such models can understand quite abstract sketches, interpret them, and generate new content. However, VLMs cannot always understand what users want to express in free-form drawings. Beyond the limitations of current algorithms for sketch understanding, an important reason is that sketches inherently carry ambiguity [63]. Some prior work has addressed this by imposing additional constraints to guide the AIs interpretation. For example, Code Shaping ultimately introduced command brushes, restricted set of sketch shapes each mapped to specific edit operation, in order to improve recognition reliability. While this approach improved accuracy, it effectively reintroduced symbolic shorthand, thereby limiting the expressive freedom of sketching. In contrast, we view ambiguity not as flaw to be eliminated but as feature that can drive creative workflows [17]. Our approach further explores engaging users in co-interpretation, where the system proposes an interpretation and the user iteratively refines or corrects it. This paradigm is especially well-suited to animation and dynamic media authoring, where sketches are inherently provisional and iterative refinement aligns with how creators naturally explore temporal ideas [13]."
        },
        {
            "title": "2.2 Sketch as a Generation Prompt\nSketches have been widely explored as prompts for generative algo-\nrithms due to their intuitive and expressive nature. Since sketches\nprovide a quick and intuitive way to communicate spatial struc-\nture and layout, they have been widely used to guide the gen-\neration of static content such as images [9, 41, 67, 74], 3D mod-\nels [48, 49], websites [2, 58]. Building on these applications, recent\nwork [25, 32, 43, 45, 75] has begun to explore the use of sketches\nin dynamic content generation, where conveying temporal and se-\nmantic change becomes essential. In pixel-based video generation,\nsketches can be drawn directly onto images to indicate motion tra-\njectories or directional flow, serving as intuitive cues for generating\nanimated sequences [32]. Alternatively, sketches are used as masks\nor editing guides, allowing users to specify which regions of a video\nshould change and how those transformations evolve over time\ntime [45, 61, 68].",
            "content": "While sketch cues overlaid on images are effective for indicating short-term motion or localized edits, recent works [12, 16, 18, 35] have begun exploring sketch storyboards to support longer-range structure and narrative purely through sketch. Traditionally used in early-stage film and animation ideation, sketch storyboards offer sequence-based representation of motion and narrative intent, making them natural fit for guiding generative algorithms [20]. Different systems adopt sketch storyboards in domain-specific ways. SketchVideo[43] treats sketches as static illustrations of each keyframes visual structure, using carefully drawn outlines to guide photorealistic image generation. Sketch2Anim[75], on the other hand, relies on predefined sketch format, typically stick figure combined with trajectory, to represent skeletal movement across frames. In both cases, the sketch input is tightly constrained and tailored to the specific task. While effective for domain-specific tasks, these constrained sketch representations fall short for motion graphics video creation, where compositions often feature multiple vector-based elements moving in abstract, stylized, or non-physical ways [5, 33]. Such sketch formats impose rigid structural or pose constraints, limiting users ability to articulate complex and unconventional animation ideas [69]. To overcome these limitations, we adopt free-form sketch storyboards that support open-ended drawing without constraints, enabling richer expression of spatial, temporal, and stylistic intent. This flexibility allows sketches to function not as fixed blueprints but as adaptable cues that guide generation. Building on this foundation, our system further supports an interactive, iterative authoring CHI 26, April 1317, 2026, Barcelona, Spain Boyu Li, Lin-Ping Yuan, Zeyu Wang, and Hongbo Fu Figure 2: Prior systems [14, 2931] treat sketches as symbols to be mapped to animation commands (Top). In contrast, we treat sketches as semantic expressions of motion intent (Bottom), leveraging VLM-based commonsense reasoning and human-AI clarification to interpret why and how motion happens. process, allowing users to refine their animations over multiple rounds rather than relying on one-shot generation."
        },
        {
            "title": "Creators",
            "content": "Creating motion graphics or vector animations traditionally requires substantial expertise and professional tools such as Adobe After Effects [15]. HCI research has long sought to lower this barrier by developing more intuitive authoring paradigms. Early sketchdriven systems such as Draco [30] and Kitty [29] let users draw scenes and motion cues directly on the canvas, inferring causal animations without requiring keyframe editing, while SketchStory [36] applied similar ideas to animated data storytelling. These works emphasize continuous, pen-on-canvas interaction over traditional timelines. Building on this, more recent end-to-end systems scaffold the entire workflow for novices: Katika [24] guides users from storyboards or scripts to finished videos via libraries of pre-designed animations, and DancingBoard [8] streamlines motion-comic production with guided steps and automated animation suggestions. Some works focus on more specialized domains of vector animation. In data visualization, tools such as Data Illustrator [47] and Data Animator [60] enable designers to create animated charts as if drawing static graphics, automatically interpolating between states. For text animation, works like The Kinetic Typography Engine [37] and TextAlive [28] demonstrated powerful systems where users could algorithmically define and control motion. Piet [57] further extends this direction by providing targeted interfaces that facilitate color authoring in motion graphics videos. Most recently, AI-driven authoring has emerged: Keyframer [62] and AnyAni [54] use large language models to translate natural-language prompts into editable vector animations, supporting iterative refinement through text or direct manipulation. Building on this trajectory, researchers have begun to explore how visionlanguage models (VLMs) can support motion graphics generation. MoVer [50] introduces motion verification DSL that encodes spatio-temporal properties of animations in first-order logic, demonstrating how formal reasoning can be applied to ensure correctness of motion behaviors. LogoMotion [46] analyzes vector graphics (SVGs) to automatically suggest motion designs, showing how pre-existing assets can be transformed into animations. These works highlight the potential of AI models to reason about both semantics and temporal logic in vector-based media. However, gap remains. Existing systems have largely focused on short, self-contained examples with relatively few animated elements, often suited for toy demos or tightly scoped use cases. In contrast, many real motion graphics videoseven simple explainersalready involve multiple objects evolving over time, which introduces additional complexity for authoring and interpretation. Supporting such scenarios requires more direct and flexible visual guidance, enabling creators to express dynamic intent without relying solely on predefined templates or textual prompts. In this paper, we explore freeform sketches as visual guidance mechanism. Sketches offer an intuitive way to externalize spatialtemporal ideas, while VLMs can interpret these abstractions into vector animations. This combination bridges informal creative expression with structured animation output, extending bi-directional editing [21] to non-programmers."
        },
        {
            "title": "3 SketchDynamics\nFree-form sketch storyboards are often used for quick idea explo-\nration in dynamic content creation, and they also hold promise\nas input for generation. Yet it remains unclear how such informal\nsketches can reliably convey a creator‚Äôs intent across the authoring\nprocess. To explore this, we developed a proof-of-concept system,\nSketchDynamics, which examines how free-form sketches can func-\ntion as a medium for expressing animation ideas. With this system,\nusers sketch simple storyboards, AI interprets the drawings, trans-\nlates them into vector-based animation code, and renders them into\nvideo. To situate the study in a prototyping context that mirrors\nearly creative practice, we did not require participants to have ad-\nvanced drawing skills, encouraging them to use simple and intuitive",
            "content": "SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation CHI 26, April 1317, 2026, Barcelona, Spain Figure 3: Study illustration showing the three stages of our investigation: (1) sketch input and direct generation, (2) clarification through disambiguation cues, and (3) iterative refinement with contextual editing. Each stage involved eight new participants in the study. sketches within just few minutes as way of communicating their animation intentions."
        },
        {
            "title": "We design the system iteratively through the following three",
            "content": "stages (see Fig 3): Stage 1: Understanding sketch and interpretation. In this stage, we implemented unified interface that understood sketch storyboards from users and transformed them into motion graphic videos (Sec. 4.1). We explored how participants naturally expressed animation intent through free-form sketches and how these sketches were interpreted when directly converted into videos. The study findings revealed that participants tended to use highly diverse and abstract sketches for quick prototyping. However, even though the VLM demonstrated impressive capacity to interpret many inputs, it still often failed when confronted with ambiguous sketches. Stage 2: Investigating ambiguity in sketch interpretation. Through empirical use in the first stage, we identified recurring cases where user sketches were too abstract or ambiguous for the system to interpret reliably (e.g., an arrow could indicate either translation or rotation). This led us to investigate lightweight way for enabling users to intervene in the machines interpretation process. In this stage, we proposed clarification cue mechanism (see Sec. 5.1), which separated ambiguity into four levels and provided adaptive interfaces for user input. Our study showed that participants generally considered clarification requests reasonable and helpful, noting that such prompts not only reduced misinterpretation but also assisted them in further shaping their intent. Stage 3: Exploring more intuitive iterative editing. Not all ambiguities could be resolved in the early stage, as some sketches were too underspecified to be faithfully interpreted until video was generated to provide additional context. In this stage, we investigated how users could refine the output when it did not meet their expectations, introducing frame-based interaction method (see Sec. 6.1) that combined keyframe extraction and annotation to offer clearer guidance toward the desired result. Our study showed that the refinement strategy was efficient and intuitive, allowing participants to annotate video frames to express intent with minimal effort."
        },
        {
            "title": "4.1 Initial Implementation\nTo begin with, we designed a unified web interface where users\ncan directly explore how their free-form sketches translate into\nmotion graphics. The interface consisted of three connected views\n(Fig.4). In the sketch view (Fig. 4-A), users drew on a central canvas\nusing freehand strokes. The toolbar at the top provided lightweight\nediting functions such as undo, clear, pen, eraser, color, and pen\nsize. Sketches could be exported or imported in multiple formats\n(PNG, SVG, or JSON data), and entire storyboards could be saved\nor reloaded. A sidebar displayed all sketches as page thumbnails, al-\nlowing users to navigate between frames or remove sketches. Once\nsketches were created, they were organized in the storyboard view\n(Fig. 4-B). The interface displayed all sketches in sequence, giving\nusers an overview of how their drawings formed a storyboard. Each\nsketch could optionally be annotated with short text notes, but the\ninterface emphasized sketches as the primary mode of input. When\nthe storyboard was complete, the user entered the AI video view\n(Fig. 4-C). Here, a single click on [Generate Script] sent the story-\nboard to the model, which returned executable code. By pressing\n[Render Preview], the code was compiled into video and displayed\nin the built-in player, allowing the user to immediately check the\ngenerated result.",
            "content": "Behind the interface, we carefully engineered the sketch-to-video translation process to support this interaction. The first step was ensuring that the VLM could interpret sketches and generate corresponding animation code. To this end, we designed prompts with paired sketch descriptions, code examples, and formatting rules, CHI 26, April 1317, 2026, Barcelona, Spain Boyu Li, Lin-Ping Yuan, Zeyu Wang, and Hongbo Fu Figure 4: Interface design for the first Stage. (A) The user sketches frame-level storyboard to explain the law of reflection, drawing incident and reflected light rays on mirror surface. (B) adds brief notes at the bottom to specify the cues, and (C) generates preview video from the storyboard. asking the model to output executable scripts. central challenge was how to represent the storyboard so the model could reason across sketches effectively. We tested two input strategies: (1) feeding each sketch individually with its script, and (2) compositing all sketches and text scripts into single storyboard input. Results indicated that the second approach produced more coherent and temporally consistent animations, likely because the model could better reason across sequential sketches. This strategy was therefore adopted in subsequent development. The second step was to render the generated code into video. We instructed the VLM to output executable Manim [1] Python code, which was then compiled into animations. Manim, declarative animation library widely used for educational explainer videos, provided the rendering backend. This code-first design not only enabled fine-grained control at the script level (e.g., timing, trajectories, styles) but also ensured that animations were built from scalable vector-based primitives, making the video extensible and well-suited for future iterative editing."
        },
        {
            "title": "4.2 Study Design\nParticipants. Consistent with previous research [69], we recruited\neight participants (P1‚ÄìP8; five male, three female) from a local\nuniversity, with an average age of 26.2 years (SD = 4.2). All partici-\npants were right-handed and had prior exposure to motion graphics\nas viewers. Five participants reported previous experience in cre-\nating motion graphics using tools such as After Effects [15] or\nCapCut [6], while the remaining three had only basic video editing\nexperience, primarily limited to simple cutting and sequencing. We\nintentionally recruited participants with varying levels of expertise\nto capture perspectives from both experienced and novice creators,\nwhile ensuring that all had sufficient familiarity with motion graph-\nics as consumers to provide informed feedback on the generated\nresults.",
            "content": "Tasks. Since our system was designed to support storyboard creation from scratch, we did not impose narrowly constrained task or theme. Instead, we asked participants to imagine creating short explainer video for concept of their choice. To keep the task manageable while still showcasing the workflow, we provided three guidelines: (1) the video should illustrate concept that can be reasonably explained through basic shapes, (2) participants should avoid overly complex shapes or animation techniques, and (3) the storyboard should be concise, consisting of around four sketches. Specifically, we limited concepts to logical explanations to evaluate functional accuracy; We restricted visuals to basic shapes to abstract away illustrative details, allowing participants to focus their cognitive effort on articulating dynamic logic (e.g., causality, flow) rather than static aesthetics; and we focused on fundamental animation primitives (e.g., displacement, deformation) rather than high-dimensional tasks like character animation, as their complexity makes it difficult to verify the specific animation intent. We conducted the experiment on desktop interface with mouse-based interaction, as our focus was not on the precision of sketching. We restricted visuals to basic shapes to abstract away illustrative details, allowing participants to focus their cognitive effort on articulating dynamic logic (e.g., causality, flow) rather than static aesthetics Procedure. Participants began the experiment by completing survey to gather information on their prior experience with video creation and sketching (Appendix. C). Next, they were given brief overview of the project and shown the core features of the interface. Some example cases were presented to illustrate the expected input and outcome. We specifically presented examples of free-form sketches and encouraged participants to use more flexible sketching methods to express their ideas, with less emphasis on scripting. Following the introduction, participants had 10-minute exploration phase, where they could experiment with the sketching tools, test the generation of animation previews, and become familiar with the interfaces capabilities. Participants then moved on to the creation phase, where each participant was tasked with creating three separate videos, each representing different content. This meant that SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation CHI 26, April 1317, 2026, Barcelona, Spain Figure 5: Selected sketches with scripts and animation excerpts from participants storyboards in Stage 1. Above the sketch is the script(N/A means no script); the gray text below describes the animation. These examples illustrate common ways sketches were used to convey animation intentions (e.g., translation, scaling, rotation, appearance). The categories shown here are not meant as strict taxonomy but as illustrative examples; in practice, participants free-form drawings were far more varied and often mixed multiple notations within single sketch. Some animations shown are refined versions to better reflect participants intended outcomes. they needed to create three distinct sketch storyboards and review three generated videos, completing the task in approximately 30 minutes. Finally, the session concluded with semi-structured interview and questionnaire, where participants provided feedback on their experience in translating ideas into sketches, as well as their thoughts on the generated videos. The similarity between participants intended animations and the generated outputs was analyzed using both qualitative feedback and the Alignment metric from the questionnaire."
        },
        {
            "title": "4.3 Results\nAcross all sessions, we collected 24 attempts. Not all were success-\nful: five outputs were judged by participants as failures, where the\ngenerated video significantly diverged from their intended idea. As\na first-stage probe, such outcomes are acceptable and highlight the\ncurrent gap between sketch expressiveness and model interpreta-\ntion. Participants appreciated that they were allowed to attempt\nanimations across a wide variety of domains, rather than being\nconstrained to a specific area. The topics participants chose to\nexplore were diverse, covering but not limited to VR interaction,\ndata visualization, phone interaction, traffic rules, celestial motion,\nalgorithm pipelines, mathematical concepts, physical rules, and\nlogo animation. The unsuccessful cases provide further insight into\nthe current limitations of sketch-based intent expression. These\nincluded a block colliding back and forth with a wall (P2), a maze-\nwalking path (P2), a rainfall animation (P5), a bouncing ball (P7),",
            "content": "and grid-warping effect (P8). Based on different animation intents and sketch types, we selectively demonstrate some results from the user in Fig 5. Expanding the Role of Sketches. In our study, sketches served not only as way to specify layout or static elements, but also as medium for expressing animation intent. When attempting to represent dynamics, participants resorted to wide variety of abstract conventions such as arrows (Fig. 5-B), onion skinning for future states (Fig. 5-D), or index numbers for sequencing (Fig. 5-C). However, these conventions were highly diverse and often idiosyncratic, shaped by individual habits and prior experiences. For instance, P3 emphasized that everyone has their own way of drawing arrows, mine is from engineering class, while P6 noted that sometimes just use circles, sometimes line, it depends on what comes to mind first. This diversity meant that the same animation could be represented in multiple ways, and similar sketches could point to different intentions. As P4 reflected, the arrow could mean movement, or just attention, it depends on the context. Indeed, 7 out of 8 participants explicitly remarked that they have their own strategies to depict movement, often no regulation (P2) or quite random (P5). This means that free-form sketching opened up broad expressive space, but one that was highly individualized and semantically unstable, requiring interpretation and negotiation when used to communicate temporal behaviors. CHI 26, April 1317, 2026, Barcelona, Spain Boyu Li, Lin-Ping Yuan, Zeyu Wang, and Hongbo Fu Sketch Interpretation Beyond Expectations. Participants often experimented with sketches that were highly abstract or loosely defined, yet many of these were still interpreted meaningfully by the model. Several participants created inventive sketch forms to convey intentfor example, using gesture line or symbolic marks to indicate motion or transformation. Despite this, 6 out of 8 participants admitted that, at the beginning, they doubted whether such free-form drawings could be understood by the machine. P8 further reflected that some sketches were inherently ambiguous, different people could read it differently. While some outputs indeed diverged from their expectations, participants often adapted by redrawing their sketches or supplementing them with short text scripts to clarify intent. Interestingly, in 7 cases, participants reported being positively surprised when the system successfully captured the essence of their sketches in ways they had not anticipated. Although the user ratings  (Fig. 12)  indicate that most participants felt the animations were not perfect match to their intended outcome, they still perceived the results as capturing their intent to some extent and described them as unexpected but meaningful. Semantics over geometry. recurring theme was that the VLM often prioritized semantic intent rather than literal geometric fidelity. Instead of replicating sketches exactly, the model tended to generate canonical or cleaned-up versions of the intended motion. For instance, when P6 drew wobbly sine curve, the output appeared as smooth sinusoid. P6 described this as beautified, appreciating that the messy strokes became clearer in the final animation. Similarly, P9 noted that their rough arrows were rendered as consistent trajectories, which they found nicer than expected. However, this abstraction also created moments of mismatch. P4, who attempted to specify very precise trajectory, complained that the output ignored the exact angle drew. Likewise, P11 commented that it feels like it understood the idea but not the detail. In total, 7 of 8 participants remarked that the system often generalized their sketches to capture meaning rather than form, with reactions ranging from surprise and delight to mild frustration. Several (e.g., P3, P5) responded by redrawing shapes or adding text notes to enforce precision. Balancing detail and abstraction in sketching. Participants varied in how much detail they invested in their sketch storyboards. At one extreme, some treated the canvas almost like frame-byframe storyboard, drawing each scene carefully to make their intent explicit. While this approach produced clearer outputs, participants also described it as tedious (P3) and too much effort for short video (P7). In contrast, many preferred using abstract marks that conveyed animation intent more quickly. As P6 explained, this was the fastest way to show the idea, even though it sometimes made the systems interpretation less predictable. P8 similarly noted that abstract cues save time but leave ambiguity. This trade-off reflects central tension: detailed sketches improve clarity but increase workload, while abstract sketches reduce effort but demand stronger interpretive support from the model. Reflecting this tension, P5 emphasized that although they were willing to rely on abstract sketches, they expected the system to just understand the sketch without needing additional textual specification."
        },
        {
            "title": "5.1 Clarification Cue\nPrior work has highlighted that sketches are inherently ambigu-\nous [59, 64] and that such ambiguity can be a productive resource\nin design interpretation [17]. Our observations in the first stage\nconfirmed this tendency in the context of animation authoring:\nparticipants often relied on abstract arrows, shorthand marks, or\nimprovised symbols to convey motion (Fig. 6), which were effi-\ncient for them but difficult for others, or the VLM, to interpret.\nEven when the intended action was clear, essential details such as\nduration, scale, or speed were frequently left unspecified. These\nfindings illustrate that ambiguity in sketching arises not only from\nmultiple possible interpretations but also from underspecified pa-\nrameters and context-dependent symbols, all of which can cause\nmodel outputs to diverge from user expectations.",
            "content": "Rather than treating ambiguity as an error to be eliminated, we approached it as variable condition to be managed. Our design philosophy was to preserve the freedom of freehand sketching while offering just-in-time interventions to clarify intent when needed. This philosophy echoes early ideas of free-form sketch beautification [23], supporting multiple candidates for ambiguity and requesting the users interaction. To this end, we adopted layered prompting strategy that adaptively maps the degree of ambiguity to different levels of system intervention. This draws on the principle of progressive disclosure: users only encounter as much clarification effort as the ambiguity of their sketches demands, thereby minimizing disruption while maintaining interpretability. Concretely, based on previous research [3, 14] about sketch ambiguity and empirical results from the first stage, we developed four complementary mechanisms that align with increasing levels of sketch ambiguity, ranging from low to very high, each offering progressively richer forms of clarification, as shown in Fig. 6. Quick confirm. For low-uncertainty cases such as uncertain strokes or vague geometry [64], the system generates primary guess SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation CHI 26, April 1317, 2026, Barcelona, Spain Figure 6: Illustration of ambiguity clarification. From left to right, the examples show sketch ambiguities ranging from low to high. Top: different types of ambiguities in storyboards, Bottom: the corresponding types of information needed to resolve them, mapped to our clarification strategies. and asks the user for yes/no confirmation (e.g., Should this line be used as motion path?). Multiple choice. When sketch could plausibly mean several different things [17], the system presents alternative previews and asks the user to pick the intended one (e.g., curved arrow could be shown as either rotation or decorative arrow). Fill value. For actions that are recognized but lack specific parameters, the interface asks for numeric or scalar values. While many aspects of the video could in principle be parameterized, we prompt only for those with significant impact on the outcome. For example [14], the system may ask how long ball should take to traverse path, while leaving minor details such as fade-in duration to sensible defaults. Text or upload resources. In cases where sketches are highly abstract or symbolic [17, 63], the system allows users to provide free-form clarification, either as short text note or by uploading an external reference asset. After completing sketch storyboard, users proceed to generation in the AI video view. When they press [Generate Script], if the VLM confidently interprets the sketches, the interface produces the animation code and renders preview video directly. When ambiguities are detected, however, the interface does not produce arbitrary results; instead, it pops up clarification panel. For example, if curved arrow could be read as either rotation or decoration, the interface presents two animated previews for the user to select the intended one. In another case, if users symbol is too abstract, they can upload custom SVG asset (e.g., heart icon to represent an object appearing with pop effect). For implementation, as shown in Fig 7, the clarification mechanism is supported through prompt engineering and contextual memory. The VLM is explicitly instructed to request clarifications only when necessary and only on aspects that critically affect the generated animation, avoiding excessive or redundant queries. When the user provides input, such as selecting between preview options or uploading an external SVG asset, these responses are added as supplementary context for regeneration. In addition, the system maintains an intention disambiguation memory, which records user-provided clarifications and automatically reuses them when the same sketch is edited and regenerated later."
        },
        {
            "title": "5.3 Results\nWe collected a total of 87 clarification cues across the 24 creation\nattempts. In addition, participants were free to revise their sketches\nand re-generate videos within the same attempt; for analysis, we\ncounted all cues triggered during that cycle under a single attempt.\nThis occasionally inflates the count for sketches that went through\nseveral iterations, but more accurately reflects the total clarification\neffort needed to reach a satisfactory result. In two cases (T5 and\nT10), participants ultimately judged the final output as misaligned\nwith their expectations, both involving physics-inspired animations\nsuch as bouncing or collision. Overall, participants expressed a pos-\nitive attitude toward the clarification cues. Rather than perceiving\nthem as interruptions, several described them as ‚Äúhelpful checks‚Äù\n(P3) or ‚Äúa way to steer the system back on track‚Äù (P7). Even when\nadditional questions extended the workflow, participants valued",
            "content": "CHI 26, April 1317, 2026, Barcelona, Spain Boyu Li, Lin-Ping Yuan, Zeyu Wang, and Hongbo Fu Figure 7: Example workflow of the clarification cue mechanism. The system detects ambiguities in sketches, classifies them by level, prepares them as JSON for the interface generator, and prompts users with lightweight clarifications (e.g., confirmation, choice, value input, or asset upload). User responses are used as supplementary prompt for regeneration and stored in an intent disambiguation memory to avoid repeated queries in future generations. the cues as opportunities to make their intent explicit and avoid wasted generations. Overall Cue Usage. Figure 8 summarizes the distribution of cue types per attempt in the form of heatmap. The pattern shows that multiple choice cues were triggered most frequently, reflecting the prevalence of sketches with more than one plausible interpretation, which is the most common ambiguity. Quick confirm cues were relatively rare, as the model often resolved low-ambiguity cases confidently without further questioning. Fill value cues also appeared infrequently because the system was designed to ask only for parameters with major impact on the outcome, and that case is relatively uncommon. By contrast, text/upload cues occurred at moderate level, indicating that participants frequently drew highly abstract or symbolic sketches (e.g., rough icons) that required extra resources or textual hints. In these cases, the prototype demonstrated its ability to reason semantically about the sketch and prompt the user for relevant supplementary information. The heatmap also reveals strong variation across attempts. We did not include detailed per-attempt breakdowns of cue types in the paper, as these would be too granular, but the heatmap variation and density of cue usage across the study are meaningful. Some creations, such as T5 and T13, required dense sequence of cues (up to 56 within one attempt), while others (e.g., T7, T14) proceeded with almost no intervention. This disparity highlights how cue demand was closely tied to the degree of abstraction in the sketch. For instance, P3 commented that only needed help when drew something unusual, whereas P8 described the cue prompts as relief when was not sure if the sketch was understandable. User Reactions to Cue. Participants generally responded positively to the clarification cues, with 7 out of 8 noting that they helped avoid misleading outputs and made them more aware of the ambiguity in their sketches. Quick confirm cues were described as lightweight and easy to dismiss, while multiple choice cues were seen as the most necessary since arrows and curves often admitted multiple interpretations. For example, in T10 participant drew looping arrow and appreciated being asked to choose between rotation and decorative arrow. Fill value cues divided opinions: some found specifying duration or speed helpful, while others felt it broke the free-flow nature of sketching. Text/Upload cues, though requiring more effort, were valued when rough sketches stood in for specific icons. In T23, participant drew rough star sketch, which triggered clarification cue suggesting the upload of pentagram icon. P8 remarked that they were surprised the system could interpret such an abstract mark as symbolic request for sparkle effect. Overall, all participants felt that the clarification cues made sense in resolving sketch ambiguities. According to the user rating  (Fig. 12)  , compared with stage 1, clarification cues generally align the users intention with the final animations, and participants did SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation CHI 26, April 1317, 2026, Barcelona, Spain Figure 8: Heatmap of clarification cue usage across 24 tasks (T1 to T24). The ùë¶-axis displays four types of clarification cues: Quick confirm, Multiple choice, Fill value, and Text/Upload. Each block in the grid represents the total Cue Count (frequency of use) for specific cue type within specific task. Color intensity corresponds to this count, with the scale shown in the legend on the right (darker orange indicates higher count, up to 3). not find these reasonable checks to be significantly disruptive to their creative process. The Result after Cue. After clarification, 19 out of 24 attempts produced outputs that participants rated as closer to their intended animation, compared to the initial generation. In 7 attempts, users explicitly described the cue as the turning point that made the result usable (e.g., P6: the timing became right only after entered the value). Fill value cues directly improved temporal qualities: in T13 and T21, entering duration smoothed otherwise abrupt motions. Text/Upload cues often corrected semantic mismatches: in T23, replacing hand-drawn star with an SVG asset transformed the effect into what P8 called finally what had in mind. By contrast, Multiple choice cues mainly resolved directional intent; in T10, choosing rotation instead of arrow symbol shifted the system output to match the participants storyboard. Failures still occurred: 2 attempts involving physical dynamics remained unsatisfactory even after cues, suggesting limits in the models reasoning rather than the cue mechanism. Taken together, the logs show that cues did not simply add extra interaction but often directly determined whether sketch led to usable animation. Workflow Integration. Participants emphasized that the clarification cues were most valuable when integrated smoothly into the overall authoring workflow, rather than appearing as separate steps. In practice, cues were triggered inline during generation and resolved within few clicks, which participants described as not disruptive (P3) and just part of the flow (P6). Several participants noted that without these cues, the system might guess wrong (P5) or return outputs that look nothing like what wanted (P2). By contrast, lightweight confirmations or multiple-choice previews felt like natural checkpoints that improved trust in the system. Importantly, participants highlighted that the cues helped them understand how the model interpreted their sketch, giving them sense of control. As P8 put it, can see what the AI thinks, and fix it right away. Overall, rather than slowing down creation, clarification cues were seen as seamlessly woven into the workflow, balancing automation with user agency."
        },
        {
            "title": "6.1 Refinement Cue\nThe refinement workflow begins once an initial video has been\ngenerated. Instead of redrawing or modifying the entire sketch\nstoryboard to produce a new result, users can refine the output\ndirectly within the video itself. As shown in Fig. 9, the interface\nprocesses the video into a set of keyframe images, which act as\nsemantic anchors that provide stable, context-rich entry points\nfor targeted edits. Users can select a target keyframe in which\nthey want to modify the related animation, and express refinement\nintention in two complementary ways. They may sketch directly\non the paused frame to indicate spatial or motion adjustments\n(e.g., extending a trajectory line to lengthen an object‚Äôs movement).\nAlternatively, they may issue concise textual refinements such as\n‚Äúfade slower‚Äù or ‚Äúloop twice.‚Äù To enable refinement, we implement\ntwo key steps. By combining the immediacy of visual sketching with\nthe precision of lightweight textual input, this approach supports a\nhybrid refinement workflow that flexibly captures both spatial and\ntemporal aspects of animation.",
            "content": "To enable such refinement interaction, we implement two key steps. First, to provide interpretable keyframes as anchors for users to apply intentions, we use the VLM to analyze the initial animation code, detect salient timestamps, and expose them as keyframes. CHI 26, April 1317, 2026, Barcelona, Spain Boyu Li, Lin-Ping Yuan, Zeyu Wang, and Hongbo Fu Second, to provide full context for localized video editing, we supply the VLM with user refinements (sketched on keyframe or expressed in text), the timestamp of the keyframe, all extracted keyframes, and the initial animation to be modified. The VLM then updates only the relevant portion of the code, and the revised code is rendered into an updated preview, ensuring efficient and focused iteration."
        },
        {
            "title": "6.2 Study Design\nWe recruited eight new participants (P1‚ÄìP8; four male, four female)\nfrom the same university, with an average age of 25.8 years (SD\n= 3.7). None had participated in the previous stages. At this stage,\nparticipants were given access to the full set of mechanisms intro-\nduced in the paper, including both clarification cues and refinement\ncues.",
            "content": "The task design followed the same open-ended format as in earlier stages: participants were asked to create short explainer-style animation of concept of their choice using free-form sketching. However, to emphasize iterative creation and refinement, each participant was instructed to generate an initial video and then improve it through at least one round of refinement using the new mechanism. Since the refinement process required more time than earlier tasks, participants were asked to complete one animation project within the 30-minute session."
        },
        {
            "title": "6.3 Results\nOverall Use of Refinement Cues. Across the eight participants,\nwe collected 12 edited videos with a total of 55 refinement opera-\ntions. On average, participants performed 4.6 refinements per task,\nthough the distribution was skewed: some participants stopped\nafter one or two small adjustments (P2, P5), while others explored\nmore iterative editing cycles with up to eight refinements (P7). Most\nrefinements (36/55) were sketch-based, while the others were text-\nbased. This division reflected a functional split: spatial animations\n(e.g., paths, shape sizes, directions) were usually expressed visually,\nwhile temporal refinements (e.g., timing, repetition) were handled\nwith short text cues.",
            "content": "Efficiency and Locality. Participants emphasized the efficiency of directly correcting generated results instead of redrawing entire storyboards. In 10 of the 12 final outputs, users reported that the unaffected portions of the video remained stable, which they considered crucial to maintaining creative momentum. In the user ratings  (Fig. 12)  , although completing animations in this stage took longer than in the previous stages, participants did not report corresponding increase in perceived effort. This suggests that the interface interaction is efficient even with the added steps, and the improved animation outcomes justified the additional time. In interviews, six participants noted that this locality reduced frustration: dont have to start over againjust fix the part dont like (P4). Compared to Stage 1, where small sketch changes could lead to unrelated alterations, Stage 3s refinement preserved coherence and increased trust in the system. Preferred Strategies. Different editing strategies were observed. Some participants (P3, P5, P7) intervened early, pausing the video within the first few seconds to adjust motions before they propagated. Others (P1, P2, P8) preferred to watch complete draft first and then make targeted corrections. These choices also influenced how edits were sequenced. For instance, as shown in Fig. 10, P8 ensured that the overall structure of the neural network animation was correct before refining details such as layer-wise flow and color changes, describing the process as just small fixes once it was understood. In contrast, P5 followed step-by-step approach, editing the rockets trajectory and fall before addressing the explosion and pacing, noting preference to go scene by scene. Strategy selection was shaped not only by personal preference but also by the nature of the task and the systems prior knowledge. In P8s case, the VLM correctly recognized the rough sketch as fully connected network and applied concepts such as backpropagation, which made the creation process more automatic and reduced the need for lowlevel specification. As P8 remarked, it felt like the system already knows what mean, which shaped his choice to focus on high-level adjustments rather than redrawing details. User Perception of Control. Seven out of eight participants reported that refinement cues gave them stronger sense of control than in previous stages. They valued being able to tell the AI exactly whats wrong, instead of hoping it guesses right (P7). This contributed to perceived shift from one-shot generation to an iterative, collaborative process. However, two participants also tested the system with higher-level semantic requests (e.g., make this look more natural), which were not successfully interpreted. These limitations revealed boundaries of the refinement mechanism: while effective for concrete spatial-temporal edits, it struggled with abstract stylistic or semantic adjustments. Nonetheless, participants widely agreed that refinement cues transformed the interaction into more predictable and empowering authoring experience."
        },
        {
            "title": "7.1 Video Generation\nCurrently, video generation has garnered significant attention [40],\nwith recent work such as SketchVideo [43] leveraging sketch story-\nboards to produce videos. However, that approach primarily relies\non detailed keyframe layouts, which demand considerable drawing\nexpertise and are often accompanied by extensive textual scripts. In\ncontrast, our approach emphasizes minimal sketches, accessible to a\nbroad range of users, for expressing abstract concepts, thereby low-\nering the entry barrier. Through the integration of these sketches\nwith our proposed mechanisms, we enrich their interpretive depth\nand provide stronger support for video generation. At this stage, our\ndemonstration is still conceptual and involves partial manual im-\nplementation: we primarily use our prototype to generate prompts\nfor external image and video generation models, with the core",
            "content": "SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation CHI 26, April 1317, 2026, Barcelona, Spain Figure 9: Example of the refinement cue workflow. In this case, the user intends the Earth to move along its orbit. The process begins with (1) reviewing the initial video. Next, (2) the system automatically extracts set of representative keyframes, from which the user selects the one most relevant to their intended edit. (3) On the chosen keyframe, the user adds an arrow to indicate the desired motion. Finally, (4) the system updates the underlying code accordingly and regenerates the video, producing an edited version where the Earth rotates along the orbit. contribution lying in the translation of user intent into effective prompts (see Fig. 11-A). Compared with commercial systems such as Higgsfields draw-to-video feature1 , which generates videos by annotating over existing images, our concept enables users to communicate their intent before generation through iterative, backand-forth interaction, even when the initial input is an abstract sketch drawn from scratch."
        },
        {
            "title": "8 Discussion\nWe began with the question of how humans can communicate dy-\nnamic intent to AI through free-form sketches. Across three stages,\nwe progressively shaped this process through our proposed design\ninterventions, first by probing expressiveness, then by supporting\nclarification, and finally by enabling refinement. This revealed both",
            "content": "1https://higgsfield.ai/flow/draw the potential and the limits of sketch-based interaction. In the discussion, we connect these results to broader themes and outline implications for future research."
        },
        {
            "title": "8.2 Shaping Intent with AI\nIn our study, we observed that users often do not have a fully-\nformed animation intent at the beginning of the authoring process.\nInstead, their intent gradually evolves through interaction and it-\neration. Similar observations have been made in other domains\ninvolving generative AI [70], where the core challenge lies in sup-\nporting users in shaping their ideas fluidly rather than expecting\nthem to articulate a complete intent upfront. While some existing",
            "content": "CHI 26, April 1317, 2026, Barcelona, Spain Boyu Li, Lin-Ping Yuan, Zeyu Wang, and Hongbo Fu Figure 10: Authoring process of two participants. The actions are sequentially listed (only resolved clarification cues are listed), and representative steps are selected for illustration. SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation CHI 26, April 1317, 2026, Barcelona, Spain Figure 11: Demonstration of extensibility of SketchDynamics. (A) Video generation example of car driving along forest road and exploding. (B) 3D dynamic scene example of block collision simulation. Top: user-drawn sketch storyboard. Middle: interactive clarification and refinement cues. Bottom: corresponding output video frames. systems attempt to address this by increasing output diversity, they often lack precise alignment with user intentions due to limited vision-language understanding capabilities. Other systems rely on complex, multi-step prompt engineering workflows to help users refine their ideas, but these often interrupt the creative flow. We argue that sketching provides lightweight and effective medium for expressing and gradually refining animation intent. Prior systems, such as K-Sketch [14], have highlighted that traditional animation tools are often too complex for novices due to their emphasis on frame-by-frame control. In contrast, simple sketches can allow users to communicate temporal dynamics without extensive effort, making the process significantly more accessible. In our exploration, users can either draw rough sketches from scratch or annotate on top of generated results. This supports more fluid, back-and-forth interaction with the VLM understanding and generation. For example, during Stage 3 of our study, users often began sketching without clear goal in mind. It was only when presented with clarification cuessuch as candidate suggestions or disambiguation optionsthat they began to consciously shape their intent. This observation echoes findings from recent work like AnyAni [54], where novice users struggled to articulate animation needs and required iterative refinement through system feedback. Crucially, we avoid traditional end-to-end generation pipeline in which the AI produces complete result in one shot, leaving little room for user intervention. Instead, we preserve the opportunity for users to continually influence and reinterpret how the AI understands their sketches. This not only allows users to refine the output but also helps them reflect on and reshape their own ideas. Through this iterative, co-creative loop, the final animation gradually emergesbalancing the strengths of both human intuition and AI generation."
        },
        {
            "title": "8.3 Sketching Strategies\nOur study reveals that participants naturally employed sketches at\ndifferent levels of abstraction. On the one hand, high-level sketches\nwere frequently used to represent rough shapes, motion directions,\nor symbolic marks. On the other hand, users occasionally turned to",
            "content": "low-level sketches, such as precise motion paths or data diagrams, when finer control was needed. Because our system relies primarily on the reasoning capabilities of VLMs, it was more effective at interpreting high-level sketches that conveyed semantic intent, while its ability to handle low-level details was less consistent. In practice, this meant that abstract annotations were readily understood, substantially lowering the entry barrier for novice users [13, 14], whereas capturing accurate shapes or trajectories often depended on chance alignment with the models visual interpretation. More generally, the two levels demand different forms of technical support. Low-level sketches require specialized algorithms to preserve geometric fidelity and temporal precision, as in ControlNet [74], whereas high-level semantics benefit more from the broad reasoning capacities of VLMs [46]. Future work should explore hybrid authoring workflows that fluidly integrate both modalities. Adaptive systems that allow users to begin with high-level, accessible annotations and selectively refine with low-level precision sketches could enable smooth transitions between rapid ideation and detailed specification, aligning authoring effort with creative goals. In our analysis, we found participants used two main ways to sketch dynamic intent. The first was in-image motion annotations, where they drew arrows, motion curves, or ghosted outlines directly onto single frame. This was concise way to define specific trajectory or transformation. The second was across-frame sequencing, the classic storyboard method where they conveyed motion by drawing successive keyframes. This approach naturally encoded temporal evolution and was better for showing complex compositional changes. This duality reflects clear trade-off between efficiency and precision. In-image annotations were fast and preferred for simple translations or rotations. Sequencing, while more work, offered far more control over timing and complex interactions. Interestingly, each mode created unique problems for the VLM. frame crowded with in-image annotations could become mess of conflicting cues. Yet with sequencing, if the temporal gap between two frames was too large, the model might misinterpret single moving element as two completely separate objects. CHI 26, April 1317, 2026, Barcelona, Spain Boyu Li, Lin-Ping Yuan, Zeyu Wang, and Hongbo Fu"
        },
        {
            "title": "8.5 Sketch Parsing\nIn this study, we employed the powerful visual understanding capa-\nbilities of VLMs to parse users‚Äô free-hand sketches. However, due\nto the limitations of current general-purpose VLMs, sketches had\nto be exported as images and analyzed at the pixel level, rather\nthan being further parsed in the vector space. As a result, the VLM\ncould provide an overall semantic interpretation, but was unable to\nfully leverage the structural and temporal information embedded\nin stroke data. While prior work has explored stroke-level anal-\nysis [66], existing methods are limited to basic classification and\npredefined symbols, which undermines support for true free-form\nauthoring. There are also attempts that have explored feeding SVGs\nor stroke sequences into LLMs for interpretation, but purely textual\nreasoning often struggles to capture 2D spatial information [65].\nConsequently, our system could not effectively explain the specific\nmeaning of a stroke(e.g., strokes that function as static layouts,\ndynamic annotations). In free-hand drawing scenarios, this com-\nplexity became even more pronounced, as the stroke may play\nmultiple roles and can not be easily classified. Our findings point\ntoward the need for a cross-modal VLM specifically designed for\nsketch parsing in future work. Compared with pixel-only input,\nintegrating vector- and stroke-level parsing could unlock richer\nopportunities, as stroke sequences data inherently encode order,\ntiming, and grouping cues essential for animation authoring. For",
            "content": "example, if stroke is drawn at the end, it might be animation anootation. This also open new possibilities for sketch-based authoring systems, where the systems can reason more naturally about the sketching process and provide responsive feedback."
        },
        {
            "title": "9 Conclusion\nThis paper explores how free-form sketches can serve as an in-\ntuitive medium for expressing animation intent in the automatic\ncreation of dynamic content. Using simple motion graphic scenar-\nios as a starting point, we investigate the alignment between user\nintentions and generated results. Our overarching goal is to enable\nnovice users to communicate animation ideas through minimal,\ninformal inputs and to have the machine effectively interpret and\nrealize these intentions. We improve the process of simple static\nsketches to convey dynamic intent across three stages: In the first\nstage, we support a basic interface to transform free-form sketch\nstoryboards to motion graphics with the assistance of VLM. Observ-\ning the ambiguity and misalignment in this initial interaction, the\nsecond stage introduces clarification mechanisms that surface and\nresolve uncertainty in the sketch interpretation process. Building on\nthis clarified understanding, the third stage incorporates intuitive\nvisual post-editing tools, enabling users to close the gap between\nuser intention and the generated outcome. Together, these stages\nform a continuous feedback loop that transforms VLMs from one-\nshot generators into interactive collaborators, capable of handling\nuncertainty, supporting intent refinement, and empowering users\nto co-construct dynamic content through sketch-based interaction.",
            "content": "Acknowledgments This work was partially supported by the National Natural Science Foundation of China (Project No. 62502410). SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation CHI 26, April 1317, 2026, Barcelona, Spain References [1] 3b1b. 2025. Manim: Animation Engine for Explanatory Math Videos. https: //www.manim.community/. [2] Minhazul Arefin, Kazi Mojammel Hossen, and Robin Khan. 2023. pic2code: Generating HTML Code from Handwritten Picture. In 2023 26th International Conference on Computer and Information Technology (ICCIT). 16. https://doi. org/10.1109/ICCIT60459.2023. [3] Danilo Avola, Maria Chiara Caschera, Fernando Ferri, and Patrizia Grifoni. 2010. Classifying and Resolving Ambiguities in Sketch-Based Interaction. International Journal of Virtual Technology and Multimedia Vol.1 (03 2010), 104139. https: //doi.org/10.1504/IJVTM.2010.032056 [4] Sukanya Bhattacharjee and Parag Chaudhuri. 2020. Survey on Sketch Based Content Creation: from the Desktop to Virtual and Augmented Reality. Computer Graphics Forum 39, 2, 757780. https://doi.org/10.1111/cgf.14024 arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14024 [5] Ron Brinkmann. 2008. The Art and Science of Digital Compositing: Techniques for Visual Effects, Animation and Motion Graphics. Morgan Kaufmann. [6] CapCut. 2025. Capcut All-In-One Video Editor & Graphic Design Tool. https: //https://www.capcut.com/. [7] Senthil Chandrasegaran, Devarajan Ramanujan, and Niklas Elmqvist. 2018. How Do Sketching and Non-Sketching Actions Convey Design Intent?. In Proceedings of the 2018 Designing Interactive Systems Conference (Hong Kong, China) (DIS 18). Association for Computing Machinery, New York, NY, USA, 373385. https: //doi.org/10.1145/3196709.3196723 [8] Longfei Chen, Shengxin Li, Ziang Li, and Quan Li. 2025. DancingBoard: Streamlining the Creation of Motion Comics to Enhance Narratives. In Proceedings of the 30th International Conference on Intelligent User Interfaces (IUI 25). Association for Computing Machinery, New York, NY, USA, 477503. https: //doi.org/10.1145/3708359.3712167 [9] Shu-Yu Chen, Wanchao Su, Lin Gao, Shihong Xia, and Hongbo Fu. 2020. Deepfacedrawing: Deep Generation of Face Images From Sketches. ACM Trans. Graph. 39, 4, Article 72 (Aug. 2020), 16 pages. https://doi.org/10.1145/3386569.3392386 [10] Yilan Chen, Kin Chung Kwan, and Hongbo Fu. 2023. Autocompletion of repetitive stroking with image guidance. Computational Visual Media 9, 3 (2023), 581596. https://doi.org/10.1007/s41095-022-0288-2 [11] Fanny Chevalier, Nathalie Henry Riche, Catherine Plaisant, Amira Chalbi, and Christophe Hurter. 2016. Animations 25 Years Later: New Roles and Opportunities. In Proceedings of the International Working Conference on Advanced Visual Interfaces (Bari, Italy) (AVI 16). Association for Computing Machinery, New York, NY, USA, 280287. https://doi.org/10.1145/2909132.2909255 [12] James Davis, Maneesh Agrawala, Erika Chuang, Zoran Popoviƒá, and David Salesin. 2006. Sketching Interface for Articulated Figure Animation. In ACM SIGGRAPH 2006 Courses (Boston, Massachusetts) (SIGGRAPH 06). Association for Computing Machinery, New York, NY, USA, 15es. https://doi.org/10.1145/1185657.1185776 [13] Richard Davis and James Landay. 2004. Informal Animation Sketching: Requirements and design. AAAI Fall Symposium - Technical Report (01 2004). [14] Richard C. Davis, Brien Colwell, and James A. Landay. 2008. K-Sketch: Kinetic Sketch Pad for Novice Animators. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Florence, Italy) (CHI 08). Association for Computing Machinery, New York, NY, USA, 413422. https://doi.org/10.1145/ 1357054.1357122 [15] Adobe After Effect. 2025. Adobe After Effects - Motion graphics software. https: //www.adobe.com/products/aftereffects.html. [16] Maxime Garcia, Remi Ronfard, and Marie-Paule Cani. 2019. Spatial Motion Doodles: Sketching Animation in VR Using Hand Gestures and Laban Motion Analysis (MIG 19). Association for Computing Machinery, New York, NY, USA, Article 10, 10 pages. https://doi.org/10.1145/3359566.3360061 [17] William W. Gaver, Jacob Beaver, and Steve Benford. 2003. Ambiguity as Resource for Design. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Ft. Lauderdale, Florida, USA) (CHI 03). Association for Computing Machinery, New York, NY, USA, 233240. https://doi.org/10.1145/ 642611.642653 [18] Martin Guay. 2015. Sketching Free-Form Poses and Motions for Expressive 3D Character Animation. HAL 2015 (2015). http://dml.mathdoc.fr/item/NNT: 2015GREAM [19] Tracy Hammond, Brian Eoff, Brandon Paulson, Aaron Wolin, Katie Dahmen, Joshua Johnston, and Pankaj Rajan. 2008. Free-Sketch Recognition: Putting the CHI in Sketching. In CHI 08 Extended Abstracts on Human Factors in Computing Systems (Florence, Italy) (CHI EA 08). Association for Computing Machinery, New York, NY, USA, 30273032. https://doi.org/10.1145/1358628.1358802 [20] John Hart. 2013. The Art of the Storyboard: filmmakers introduction. Routledge. [21] Brian Hempel, Justin Lubin, and Ravi Chugh. 2019. Sketch-n-Sketch: OutputDirected Programming for SVG. In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (New Orleans, LA, USA) (UIST 19). Association for Computing Machinery, New York, NY, USA, 281292. https://doi.org/10.1145/3332165.3347925 [22] Forrest Huang, John F. Canny, and Jeffrey Nichols. 2019. Swire: Sketch-based User Interface Retrieval. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI 19). Association for Computing Machinery, New York, NY, USA, 110. https://doi.org/10.1145/3290605.3300334 [23] Takeo Igarashi, Satoshi Matsuoka, Sachiko Kawachiya, and Hidehiko Tanaka. 1997. Interactive Beautification: Technique for Rapid Geometric Design. In Proceedings of the 10th Annual ACM Symposium on User Interface Software and Technology (Banff, Alberta, Canada) (UIST 97). Association for Computing Machinery, New York, NY, USA, 105114. https://doi.org/10.1145/263407.263525 [24] Amir Jahanlou and Parmit Chilana. 2022. Katika: An End-to-End System for Authoring Amateur Explainer Motion Graphics Videos. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI 22). Association for Computing Machinery, New York, NY, USA, Article 502, 14 pages. https://doi.org/10.1145/3491102.3517741 [25] Hao Jin, Hengyuan Chang, Xiaoxuan Xie, Zhengyang Wang, Xusheng Du, Shaojun Hu, and Haoran Xie. 2024. Sketch-Guided Motion Diffusion for Stylized Cinemagraph Synthesis. arXiv:2412.00638 [cs.CV] https://arxiv.org/abs/2412.00638 [26] Bo Kang, Joseph J. LaViola Jr., and Pamela Wisniewski. 2017. Structured Input Improves Usability and Precision for Solving Geometry-based Algebraic Problems. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA) (CHI 17). Association for Computing Machinery, New York, NY, USA, 46924702. https://doi.org/10.1145/3025453.3025468 [27] Jun Kato, Kenta Hara, and Nao Hirasawa. 2024. Griffith: Storyboarding Tool Designed with Japanese Animation Professionals. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI 24). Association for Computing Machinery, New York, NY, USA, Article 233, 14 pages. https://doi.org/10.1145/3613904.3642121 [28] Jun Kato, Tomoyasu Nakano, and Masataka Goto. 2015. TextAlive: Integrated Design Environment for Kinetic Typography. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (Seoul, Republic of Korea) (CHI 15). Association for Computing Machinery, New York, NY, USA, 34033412. https://doi.org/10.1145/2702123. [29] Rubaiat Habib Kazi, Fanny Chevalier, Tovi Grossman, and George Fitzmaurice. 2014. Kitty: Sketching Dynamic and Interactive Illustrations. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (Honolulu, Hawaii, USA) (UIST 14). Association for Computing Machinery, New York, NY, USA, 395405. https://doi.org/10.1145/2642918.2647375 [30] Rubaiat Habib Kazi, Fanny Chevalier, Tovi Grossman, Shengdong Zhao, and George Fitzmaurice. 2014. DRACO: Sketching Animated Drawings with Kinetic Textures. In ACM SIGGRAPH 2014 Studio (Vancouver, Canada) (SIGGRAPH 14). Association for Computing Machinery, New York, NY, USA, Article 34, 1 pages. https://doi.org/10.1145/2619195.2656322 [31] Rubaiat Habib Kazi, Tovi Grossman, Nobuyuki Umetani, and George Fitzmaurice. 2016. Motion Amplifiers: Sketching Dynamic Illustrations Using the Principles of 2D Animation. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (San Jose, California, USA) (CHI 16). Association for Computing Machinery, New York, NY, USA, 45994609. https://doi.org/10.1145/ 2858036.2858386 [32] KLING. 2025. KLING AI: Next-Generation AI Creative Studio. https://klingai. com/. [33] Jon Krasner. 2013. Motion Graphic Design: Applied History and Aesthetics. Routledge. [34] James A. Landay and Brad A. Myers. 1995. Interactive Sketching for the Early Stages of User Interface Design. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA) (CHI 95). ACM Press/Addison-Wesley Publishing Co., USA, 4350. https://doi.org/10.1145/ 223904.223910 [35] James A. Landay and Brad A. Myers. 1996. Sketching Storyboards to Illustrate Interface Behaviors. In Conference Companion on Human Factors in Computing Systems (Vancouver, British Columbia, Canada) (CHI 96). Association for Computing Machinery, New York, NY, USA, 193194. https://doi.org/10.1145/257089.257257 [36] Bongshin Lee, Rubaiat Habib Kazi, and Greg Smith. 2013. SketchStory: Telling More Engaging Stories with Data through Freeform Sketching. IEEE Transactions on Visualization and Computer Graphics 19, 12 (Dec. 2013), 24162425. https: //doi.org/10.1109/TVCG.2013.191 [37] Johnny C. Lee, Jodi Forlizzi, and Scott E. Hudson. 2002. The Kinetic Typography Engine: An Extensible System for Animating Expressive Text. In Proceedings of the 15th Annual ACM Symposium on User Interface Software and Technology (Paris, France) (UIST 02). Association for Computing Machinery, New York, NY, USA, 8190. https://doi.org/10.1145/571985.571997 [38] Boyu Li, Linjie Qiu, Duotun Wang, Qianxi Liu, Ryo Suzuki, Mingming Fan, and Zeyu Wang. 2025. DesignMemo: Integrating Discussion Context into Online Collaboration with Enhanced Design Rationale Tracking. Proc. ACM Hum.- Comput. Interact. 9, 7, Article CSCW398 (Oct. 2025), 32 pages. https://doi.org/10. 1145/3757579 [39] Boyu Li, Linping Yuan, Zhe Yan, Qianxi Liu, Yulin Shen, and Zeyu Wang. 2024. AniCraft: Crafting Everyday Objects as Physical Proxies for Prototyping 3D Character Animation in Mixed Reality. In Proceedings of the 37th Annual ACM CHI 26, April 1317, 2026, Barcelona, Spain Boyu Li, Lin-Ping Yuan, Zeyu Wang, and Hongbo Fu Symposium on User Interface Software and Technology (Pittsburgh, PA, USA) (UIST 24). Association for Computing Machinery, New York, NY, USA, Article 99, 14 pages. https://doi.org/10.1145/3654777.3676325 [40] Boyu Li, Lin-Ping Yuan, and Zeyu Wang. 2025. VideoCraft: Mixed Realityempowered Video Generation Workflow with Spatial Layer Editing for Concept Video Creation (UIST 25). Association for Computing Machinery, New York, NY, USA, Article 19, 16 pages. https://doi.org/10.1145/3746059.3747606 [41] Haichuan Lin, Yilin Ye, Jiazhi Xia, and Wei Zeng. 2025. SketchFlex: Facilitating Spatial-Semantic Coherence in Text-to-Image Generation with Region-Based Sketches. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI 25). Association for Computing Machinery, New York, NY, USA, Article 546, 19 pages. https://doi.org/10.1145/3706598.3713801 [42] Yuyu Lin, Jiahao Guo, Yang Chen, Cheng Yao, and Fangtian Ying. 2020. It Is Your Turn: Collaborative Ideation With Co-Creative Robot through Sketch. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI 20). Association for Computing Machinery, New York, NY, USA, 114. https://doi.org/10.1145/3313831. [43] Feng-Lin Liu, Hongbo Fu, Xintao Wang, Weicai Ye, Pengfei Wan, Di Zhang, and Lin Gao. 2025. SketchVideo: Sketch-based Video Generation and Editing. In Proceedings of the Computer Vision and Pattern Recognition Conference. 23379 23390. [44] Feng-Lin Liu, Shi-Yang Li, Yan-Pei Cao, Hongbo Fu, and Lin Gao. 2025. Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers (SIGGRAPH Conference Papers 25). Association for Computing Machinery, New York, NY, USA, Article 152, 12 pages. https: //doi.org/10.1145/3721238.3730623 [45] Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, and Jiaya Jia. 2024. Generative Video Propagation. arXiv:2412.19761 [cs.CV] https://arxiv.org/abs/2412.19761 [46] Vivian Liu, Rubaiat Habib Kazi, Li-Yi Wei, Matthew Fisher, Timothy Langlois, Seth Walker, and Lydia Chilton. 2025. LogoMotion: Visually-Grounded Code Synthesis for Creating and Editing Animation. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI 25). Association for Computing Machinery, New York, NY, USA, Article 157, 16 pages. https://doi. org/10.1145/3706598.3714155 [47] Zhicheng Liu, John Thompson, Alan Wilson, Mira Dontcheva, James Delorey, Sam Grigg, Bernard Kerr, and John Stasko. 2018. Data Illustrator: Augmenting Vector Design Tools with Lazy Data Binding for Expressive Visualization Authoring. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI 18). Association for Computing Machinery, New York, NY, USA, 113. https://doi.org/10.1145/3173574.3173697 [48] Zhaoliang Lun, Matheus Gadelha, Evangelos Kalogerakis, Subhransu Maji, and Rui Wang. 2017. 3D Shape Reconstruction from Sketches via Multi-view Convolutional Networks. In 2017 International Conference on 3D Vision (3DV). 6777. https://doi.org/10.1109/3DV.2017.00018 [49] Zhongjin Luo, Dong Du, Heming Zhu, Yizhou Yu, Hongbo Fu, and Xiaoguang Han. 2024. SketchMetaFace: Learning-Based Sketching Interface for HighFidelity 3D Character Face Modeling . IEEE Transactions on Visualization & Computer Graphics 30, 08 (Aug. 2024), 52605275. https://doi.org/10.1109/TVCG. 2023. [50] Jiaju Ma and Maneesh Agrawala. 2025. MoVer: Motion Verification for Motion Graphics Animations. ACM Trans. Graph. 44, 4, Article 33 (July 2025), 17 pages. https://doi.org/10.1145/3731209 [51] Krystina Madej and Newton Lee. 2020. Disney Stories. Springer. [52] Timothy Matthews. 2012. Sketch-based Digital Storyboards and Floor Plans for Authoring Computer-Generated Film Pre-visualisations. Ph. D. Dissertation. Nelson Mandela Metropolitan University. [53] Kushin Mukherjee, Holly Huey, Xuanchen Lu, Yael Vinker, Rio Aguina-Kang, Ariel Shamir, and Judith E. Fan. 2023. SEVA: Leveraging Sketches to Evaluate Alignment Between Human and Machine Visual Abstraction. In Proceedings of the 37th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS 23). Curran Associates Inc., Red Hook, NY, USA, Article 2934, 18 pages. [54] Tianrun Qiu and Yuxin Ma. 2025. AnyAni: An Interactive System with Generative AI for Animation Effect Creation and Code Understanding in Web Development. arXiv:2506.21962 [cs.HC] https://arxiv.org/abs/2506.21962 [55] Dean Rubine. 1991. Specifying Gestures by Example. In Proceedings of the 18th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH 91). Association for Computing Machinery, New York, NY, USA, 329337. https: //doi.org/10.1145/122718.122753 [56] Sigurdur Gauti Samuelsson and Matthias Book. 2020. Eliciting Sketched Expressions of Command Intentions in an IDE. Proc. ACM Hum.-Comput. Interact. 4, ISS, Article 200 (Nov. 2020), 25 pages. https://doi.org/10.1145/ [57] Xinyu Shi, Yinghou Wang, Yun Wang, and Jian Zhao. 2024. Piet: Facilitating Color Authoring for Motion Graphics Video. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI 24). Association for Computing Machinery, New York, NY, USA, Article 148, 17 pages. https://doi.org/10.1145/3613904.3642711 [58] Sarah Sonje, Harsh Dave, Jaswantsingh Pardeshi, and Sheetal Chaudhari. 2022. draw2code: AI based Auto Web Page Generation from Hand-drawn Page Mockup. In 2022 IEEE 7th International conference for Convergence in Technology (I2CT). 17. https://doi.org/10.1109/I2CT54291.2022.9824521 [59] Masaki Suwa and Barbara Tversky. 1997. What Do Architects and Students Perceive in Their Design Sketches? Protocol Analysis. Design Studies 18, 4 (1997), 385403. https://doi.org/10.1016/S0142-694X(97)00008-2 Descriptive models of design. [60] John Thompson, Zhicheng Liu, and John Stasko. 2021. Data Animator: Authoring Expressive Animated Data Graphics. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI 21). Association for Computing Machinery, New York, NY, USA, Article 15, 18 pages. https://doi.org/10.1145/3411764.3445747 [61] Bekzat Tilekbay, Saelyne Yang, Michal Adam Lewkowicz, Alex Suryapranata, and Juho Kim. 2024. ExpressEdit: Video Editing with Natural Language and Sketching. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI 24). Association for Computing Machinery, New York, NY, USA, 515536. https://doi.org/10.1145/3640543.3645164 [62] Tiffany Tseng, Ruijia Cheng, and Jeffrey Nichols. 2024. Keyframer: Empowering Animation Design using Large Language Models. https://arxiv.org/abs/2402. [63] Winger S.W. Tseng and Linden J. Ball. 2011. How Uncertainty Helps Sketch Interpretation in Design Task. In Design Creativity 2010, Toshiharu Taura and Yukari Nagai (Eds.). Springer London, London, 257264. [64] Barbara Tversky and Masaki Suwa. 2009. Thinking with Sketches. https://doi. org/10.1093/acprof:oso/9780195381634.003.0004 [65] Yael Vinker, Tamar Rott Shaham, Kristine Zheng, Alex Zhao, Judith Fan, and Antonio Torralba. 2025. SketchAgent: Language-Driven Sequential Sketch Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2335523368. [66] Jacob O. Wobbrock, Andrew D. Wilson, and Yang Li. 2007. Gestures Without Libraries, Toolkits or Training: $1 Recognizer for User Interface Prototypes. In Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology (Newport, Rhode Island, USA) (UIST 07). Association for Computing Machinery, New York, NY, USA, 159168. https://doi.org/10.1145/1294211.1294238 [67] Chufeng Xiao, Deng Yu, Xiaoguang Han, Youyi Zheng, and Hongbo Fu. 2021. Sketchhairsalon: Deep Sketch-Based Hair Image Synthesis. ACM Trans. Graph. 40, 6, Article 216 (Dec. 2021), 16 pages. https://doi.org/10.1145/3478513.3480502 [68] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. 2024. Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion. In ACM SIGGRAPH 2024 Conference Papers (Denver, CO, USA) (SIGGRAPH 24). Association for Computing Machinery, New York, NY, USA, Article 113, 12 pages. https://doi.org/10.1145/3641519.3657481 [69] Ryan Yen, Jian Zhao, and Daniel Vogel. 2025. Code Shaping: Iterative Code Editing with Free-form AI-Interpreted Sketching. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI 25). Association for Computing Machinery, New York, NY, USA, Article 872, 17 pages. https://doi. org/10.1145/3706598. [70] Seo young Lee, Matthew Law, and Guy Hoffman. 2025. When and How to Use AI in the Design Process? Implications for Human-AI Design Collaboration. International Journal of HumanComputer Interaction 41, 2 (2025), 15691584. https://doi.org/10.1080/10447318.2024.2353451 arXiv:https://doi.org/10.1080/10447318.2024.2353451 [71] Lin-Ping Yuan, Boyu Li, Jindong Wang, Huamin Qu, and Wei Zeng. 2024. Generating Virtual Reality Stroke Gesture Data From Out-Of-Distribution Desktop Stroke Gesture Data. In IEEE Conference Virtual Reality and 3D User Interfaces. IEEE, 732742. [72] Nurcan Yƒ±ldƒ±zoƒülu. 2024. Sketching Versus Digital Design Tools in Architectural Design. Journal of Computational Design 5 (08 2024). https://doi.org/10.53710/ jcode.1504947 [73] Robert C. Zeleznik, Andrew Bragdon, Chu-Chi Liu, and Andrew Forsberg. 2008. Lineogrammer: Creating Diagrams by Drawing. In Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology (Monterey, CA, USA) (UIST 08). Association for Computing Machinery, New York, NY, USA, 161170. https://doi.org/10.1145/1449715.1449741 [74] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding Conditional Control to Text-to-Image Diffusion Models. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV). 38133824. https://doi.org/10.1109/ICCV51070.2023. 00355 [75] Lei Zhong, Chuan Guo, Yiming Xie, Jiawei Wang, and Changjian Li. 2025. Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation. arXiv preprint arXiv:2504.19189 (2025). SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation CHI 26, April 1317, 2026, Barcelona, Spain User Rating in Three Stages Fig. 12 illustrates the cumulative benefit of our design interventions across the three stages. We observe substantial upward trend in Alignment and Control, shifting from predominantly negative ratings in Stage 1 to highly positive ones in Stage 3. This validates that while the VLM alone (Stage 1) often struggles with interpretation, the integration of clarification and refinement effectively bridges the gulf between user intent and system output. Notably, despite the additional interaction steps introduced in the final system, Effort ratings improved compared to Stage 1. This suggests that users perceived the targeted refinement process as significantly less laborious and frustrating than the repetitive trial-and-error redrawing required in the baseline. Furthermore, Flow and Ease remained consistently high, indicating that the added mechanisms were seamlessly integrated without disrupting the creative momentum. Visual Understanding Capability Experiment Before the main study, we conducted pilot tests to check feasibility, focusing on prompt design and the VLMs ability to interpret freeform sketches. The results were encouraging: the prompts guided the model effectively, and the VLM showed strong potential in handling abstract input. Here we show how the VLM with prompt engineering interpreted several Stage 1 sketches, illustrating how rough or symbolic drawings were mapped into animation concepts. This is not part of our core contribution but serves as an important foundation for the paper. User Study Survey and Interview Questions This appendix provides the detailed instruments used in our threestage user study to evaluate SketchDynamics. These include the participant background survey, the post-task Likert scale questionnaire, and the semi-structured interview guide. C.1 Pre-study Questionnaire: Background and"
        },
        {
            "title": "Experience",
            "content": "Before the experiment, participants provided background information to help us understand their prior expertise in animation and sketching: Post-task Questionnaire C.2 After completing the tasks in each stage, participants rated their experience based on the following statements using 7-point Likert scale (1: Strongly Disagree, 7: Strongly Agree) Alignment: The generated results reflected what had in mind. Ease: The interface was easy to use and understand. Flow: The workflow felt smooth without disrupting my creative process. Control: felt in control of how my input was interpreted. Effort: The effort required to use the system was reasonable. Explore: The system encouraged me to try new ways of expressing my ideas. Semi-structured Interview Guide C.3 The following questions guided our qualitative analysis across the three stages of the study: C.3.1 General Questions (All Stages). What was your primary strategy for translating animation ideas into free-form sketches? Were there specific animation intents that you found particularly difficult to express through sketching? Did the systems interpretation of your sketches surprise you? In what way? C.3.2 Stage 2: Clarification Cues. How did you perceive the clarification requests (e.g., multiplechoice, fill-value)? Did they feel like interruptions or helpful checks? Did the clarification process help you better define animation details (like timing or speed) that you hadnt initially considered?. C.3.3 Stage 3: Refinement Cues. How does the experience of refining generated video compare to redrawing storyboard from scratch? Did the ability to edit specific video frames increase your sense of control over the AI collaborator? Which did you find more useful for refinement: drawing on the frame or providing text prompts? Why? Demographics: Age, Gender, and Handedness. Motion Graphics Experience: How often do you watch motion graphics or explainer videos? (1: Never, 7: Daily). Have you ever created motion graphics using tools such as After Effects, CapCut, or others? Rate your proficiency in video editing and animation software (1: Novice, 7: Expert). Sketching Experience: How often do you use sketching for ideation or communication? (1: Never, 7: Daily). Rate your comfort level with drawing/sketching (1: Very Uncomfortable, 7: Very Comfortable). CHI 26, April 1317, 2026, Barcelona, Spain Boyu Li, Lin-Ping Yuan, Zeyu Wang, and Hongbo Fu Figure 12: Self-defined Likert scale results across the three stages. Alignment: The generated results reflected what had in mind. Ease: The interface was easy to use and understand. Flow: The workflow felt smooth without disrupting my creative process. Control: felt in control of how my input was interpreted. Effort: The effort required to use the system was reasonable. Explore: The system encouraged me to try new ways of expressing my ideas. Table 1: Examples of sketch types and the animation intentions they convey"
        },
        {
            "title": "Virtual Camera\nMovement",
            "content": "Plan for virtual camera sweep across six regions in Z-shaped path: Play Garden Lake (top row), then down to Sports Lawn Cafe (bottom row), followed by an exit movement to the left."
        },
        {
            "title": "Null",
            "content": "The word CHI appears, then gradually fades while moving downward (represented with dashed lines). square morphs into triangle and then into line, shown sequentially within the canvas. pendulum suspended from pivot with an arrow pointing left at the bob, indicating oscillatory motion. Blue indicates future The central circle enlarges to the blue dashed size while the five surrounding circles shrink to their dashed sizes."
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}