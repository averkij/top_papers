{
    "paper_title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
    "authors": [
        "Kartik Narayan",
        "Yang Xu",
        "Tian Cao",
        "Kavya Nerella",
        "Vishal M. Patel",
        "Navid Shiee",
        "Peter Grasch",
        "Chao Jia",
        "Yinfei Yang",
        "Zhe Gan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address information-seeking and knowledge-intensive user queries. Existing approaches, such as retrieval augmented generation (RAG) methods, search agents, and search equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and poorly constructed search queries, which result in inefficiencies and suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1, the first multimodal LLM capable of performing on-demand, multi-turn web searches and dynamically crafting queries for both image and text search tools. Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops of the input image making the image search more effective, and can iteratively adapt text search queries based on retrieved information, thereby enabling self-reflection and self-correction. Our approach relies on a two-stage training pipeline: a cold start supervised finetuning phase followed by an online reinforcement learning optimization. For training, we introduce DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated pipeline intermixed with real-world information from web search tools. This dataset contains diverse, multi-hop queries that integrate textual and visual information, teaching the model when to search, what to search for, which search tool to use and how to reason over the retrieved information. We conduct extensive experiments across a range of knowledge-intensive benchmarks to demonstrate the superiority of our approach. Finally, we analyze the results and provide insights that are valuable for advancing multimodal web-search."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 1 0 8 2 1 . 0 1 5 2 : r DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search Kartik Narayan1, Yang Xu2, Tian Cao2, Kavya Nerella2, Vishal M. Patel1, Navid Shiee2, Peter Grasch2, Chao Jia2, Yinfei Yang2, Zhe Gan 1Johns Hopkins University, 2Apple Work done during an internship at Apple. Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address informationseeking and knowledge-intensive user queries. Existing approaches, such as retrieval augmented generation (RAG) methods, search agents, and search equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and poorly constructed search queries, which result in inefficiencies and suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1, the first multimodal LLM capable of performing on-demand, multi-turn web searches and dynamically crafting queries for both image and text search tools. Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops of the input image making the image search more effective, and can iteratively adapt text search queries based on retrieved information, thereby enabling self-reflection and self-correction. Our approach relies on two-stage training pipeline: cold start supervised finetuning phase followed by an online reinforcement learning optimization. For training, we introduce DeepMMSearchVQA, novel multimodal VQA dataset created through an automated pipeline intermixed with real-world information from web search tools. This dataset contains diverse, multi-hop queries that integrate textual and visual information, teaching the model when to search, what to search for, which search tool to use and how to reason over the retrieved information. We conduct extensive experiments across range of knowledge-intensive benchmarks to demonstrate the superiority of our approach. Finally, we analyze the results and provide insights that are valuable for advancing multimodal web-search. Date: October 15,"
        },
        {
            "title": "Introduction",
            "content": "Multimodal Large Language Models (MLLMs) (Hurst et al., 2024; Team et al., 2023; Li et al., 2024a; Bai et al., 2025; Chen et al., 2024; You et al., 2023; Wang et al., 2024; Deitke et al., 2024) combine pre-trained visual encoders with large language models (LLMs), and have achieved remarkable progress across range of visual perception, grounding and generation tasks. These capabilities have made them integral to wide range of everyday intelligent assistance applications. Despite these advances, they continue to struggle with knowledgeintensive and information-seeking visual question answering (VQA) (Chen et al., 2023; Mensink et al., 2023), which requires not only accurate recognition of visual entities but also access to relevant background knowledge. The sheer breadth of open-world visual knowledge places many queries in the long-tail distribution, and inevitably demands information beyond models internal training corpus. Constructing ever-larger training datasets is impractical, as it requires costly pipelines of data collection, cleaning, organization, and retraining. Furthermore, because the web is continuously updated, static training corpora quickly become outdated, leaving MLLMs unable to answer questions that require access to up-to-date information. For example, Qwen2.5-VL (Bai et al., 2025), last updated on January 2025, fails to answer: <image>Where is the boat race happening? [Answer: The image shows the annual Pacu Jalur boat races in Riau Province, Indonesia]. The image is provided in Appendix F.1. To address these limitations, recent research has sought to integrate search tools with MLLMs to provide 1 Figure 1 Unlike previous baselines, which lack self-reflection, self-correction, and cropped image-based search, the proposed DeepMMSearch-R1 is capable of performing on-demand, multi-turn web searches with enhanced image search that incorporates an intermediate cropping tool to select the most relevant region of an image. It demonstrates self-reflection and self-correction abilities, iteratively refining its text queries to better navigate noisy real-world web information. The model outperforms other baselines, notably GPT-4o, and is competitive with the GPT-o3 model. dynamic access to external information. These existing approaches can be broadly classified into three categories: (1) Retrieval-Augmented Generation (RAG) methods: Ling et al. (2025); Qi et al. (2024); Liu et al. (2024e) which rely on external knowledge bases; however, no static corpus can capture the full breadth of open-world knowledge, making this assumption unrealistic in practice. Furthermore, the rigid retrieve-then-generate pipeline of RAG-based methods often results in excessive and unnecessary retrieval. (2) Search Agents: Li et al. (2024c,b) prompt LLMs/MLLMs to perform multi-turn web searches and incorporate the retrieved content into the models context for subsequent turns. These agents are typically implemented as plug-and-play modules rather than being optimized for interaction with noisy, real-world web-search results. As consequence, they often fail to reason effectively over retrieved content and struggle to generalize in open-world scenarios not seen during pretraining. More recent efforts fall in the category of (3) Search-Equipped MLLMs (Jin et al., 2025; Song et al., 2025; Chen et al., 2025), which are trained to operate in unison with search tools and to reason over retrieved content. However, most existing works remain confined to text search, severely constraining their applicability to multimodal knowledge-intensive question answering. Wu et al. (2025) is the first work that extends retrieval into the multimodal domain by incorporating an image search tool. Nonetheless, it faces significant limitations. First, while the model can autonomously decide which tool to invoke, it is restricted to single call per tool, limiting its capacity for self-reflection and self-correction. Second, their proposed image search tool is limited to retrieving information based on whole input image, without question-specific guidance or focus. In real-world deployment, however, background content and additional visual entities can often introduce noise during image search. Such noise can lead to suboptimal retrieval and incorrect identification of the relevant visual entity, creating major bottleneck that renders image search meaningfully less efficient in practice (see Figure 1(left)). To overcome the two key limitations identified in prior works, we propose DeepMMSearch-R1, model capable of performing on-demand, multi-turn web searches with dynamic query generation for both image and text search tools as shown in Figure 1(right). Specifically, DeepMMSearch-R1 can adaptively generate and refine text-search queries over multiple turns through self-reflection and self-correction, using the retrieved content as feedback along with the original question. To improve the effectiveness of image search, we address the challenges posed by background noise and the presence of distracting visual entities by introducing an intermediate image cropping tool, which in our case is Grounding DINO (Liu et al., 2024b). DeepMMSearch-R1 first generates referring expression of the visual entity most pertinent to the question, which is then used by the cropping tool to dynamically identify and crop the corresponding region of the image. The resulting crop is used for image search, retrieving more contextually relevant results. This targeted approach significantly enhances retrieval quality and significantly boosts overall performance. We adopt two-stage training pipeline consisting of an initial supervised finetuning (SFT) phase followed by online reinforcement learning (RL) using GRPO algorithm (Shao et al., 2024). Our goal is to teach the model when to search, which tool to use, what to search for, and how to reason over retrieved content to determine the next action, whether that is providing final answer or refining the query for another search. Our contributions are summarized below: 1. Proposed Dataset: We introduce DeepMMSearchVQA, novel dataset containing diverse, multihop VQA samples with multi-turn conversations. It offers balanced representation across knowledge categories and includes both search-required and search-free questions. The dataset teaches the model: when and what to search, which tool to use, and how to reason over retrieved content. 2. Multimodal Search Tool Integration: We construct real-world multimodal search pipeline composed of three tools: (1) text search tool that enables the model to issue targeted queries for retrieving relevant webpages and acquiring up-to-date factual knowledge; (2) grounding tool based on Grounding DINO (Liu et al., 2024b) that identifies and crops the relevant region of an input image based on model-generated referring expression of the visual entity in question; and (3) an image search tool that retrieves web content, including titles and descriptions, based on the input image (cropped or whole), helping the model gather web information to recognize unfamiliar visual entity. 3. Performance Improvement: We achieve state-of-the-art performance, surpassing previous opensource baselines (see Figure 1), through two-stage training process: cold-start initialization with SFT, followed by online RL using GRPO. We discuss the impact of self-reflection, self-correction, and cropped image search, and provide additional analysis of tool call behavior, which together serve as valuable resource for advancing multimodal web-search tool integration in MLLMs."
        },
        {
            "title": "2 Proposed Data: DeepMMSearchVQA",
            "content": "There is lack of instruction tuning dataset to equip multimodal LLMs with web-search capabilities. To fill this gap, we propose DeepMMSearchVQA, consisting of multi-turn conversations that integrate structured tool call annotations and web-retrieved information obtained from both image and text search tools. We follow two core principles for the dataset generation: (1) the dataset should be diverse and cover the entire spectrum of the knowledge taxonomy, and (2) the questions should include both search-free and search-required types, with multiple conversational turns to foster reasoning, self-reflection and self-correction. An overview of the automated pipeline used for dataset construction is presented in Figure 2(top). We employ the InfoSeek (Chen et al., 2023) training set as our base corpus and generate the conversations with reasoning distilled from Gemini-2.5-Pro (Team et al., 2023). We provide the prompt used in Appendix E.1. The model decides which tool to invoke and what query to issue, outputting structured tool tags that If the entire image is relevant to answering the question, the are included in each training example. model appends <img_search>img</img_search> at the end of its output. When the question pertains to specific visual entity within the image, such as an object, logo, or person, the model invokes cropped image search query by appending <img_search>[referring expression of the visual entity]</img_search>. 3 Figure 2 (top) DeepMMSearchVQA Data Generation Pipeline: It begins by passing questionimage pair (q, i) to Gemini, which produces reasoning and concludes with an action tag. We then apply checks A, B, and C: if either or fails, the example is discarded; if passes, the final answer is reached and the example is saved. Otherwise, the pipeline invokes search tool guided by the action tag. This tool retrieves the top-k web results, which are then summarized and fed back into Gemini, incorporating web-retrieved information in its context for subsequent turns in the reasoning process. (bottom) DeepMMSearchVQA Statistics: Knowledge taxonomy, Distribution of examples across different numbers of conversational turns, Proportion of questions with text search, image search and cropped image search. In cases where the model can confidently identify the visual entity but requires additional factual information from external sources, it invokes the text-search tool with focused query by outputting <text_search>[search query]</text_search>. In some examples, the model issues multiple refined text searches, which are crucial for capturing self-reflection and self-correction capabilities. Once sufficient information has been gathered, the model provides the final response inside as <answer>[final answer]</answer> tag. Before generating any tool tag, the model explains its decision inside <reason>...</reason> block, ensuring that the reasoning process can be captured in the dataset. All the information retrieved from image or text search is returned to the model within <information>...</information>, which is then incorporated into subsequent reasoning and tool selection. This structured interaction design allows us to capture Gemini-2.5-Pros reasoning, tool selection, self-reflection, and self-correction capabilities in our training dataset. An illustration of the data is shown in Figure 1, and we provide more examples in Appendix G. We randomly select subset of 200,000 samples from the InfoSeek training set and generate multi-turn conversations annotated with tool tags, reasoning steps, and web-retrieved information. To ensure quality, we retain only those conversations in which Gemini-2.5-Pros predictions match the ground-truth answers provided in InfoSeek, yielding refined set of approximately 47, 000 conversations. Subsequently, we employ Gemini-2.5-Pro to categorize the questions according to knowledge taxonomy. From these categories, we sample 10,000 VQA examples to achieve an approximately balanced distribution across knowledge types. We further ensure that the dataset is evenly divided between search-required and search-free questions. Figure 2(bottom) presents the knowledge taxonomy, the proportion of questions requiring image search, text search, or both, as well as the distribution of examples across different numbers of conversational turns. The resulting set of 10,000 VQA samples constitutes the training corpus for the supervised finetuning stage."
        },
        {
            "title": "3 DeepMMSearch-R1 Training Recipe",
            "content": "We follow two-stage training pipeline. In the first stage, we perform supervised finetuning as cold-start initialization. This equips the model with grounding, image search and text search tools, and enables it to reason over the web-retrieved content. In the second stage, we perform an online GRPO optimization to further refine the models tool-selection ability and improve the efficiency of its search behavior."
        },
        {
            "title": "3.1 Supervised Finetuning Stage",
            "content": "We employ Qwen2.5-VL-7B-Instruct as our base model and perform supervised finetuning exclusively on the LLM module, while keeping both the vision encoder and vision projection layers frozen. This approach preserves the strong pretrained image representations and ensures that adaptation is directed toward enhancing the LLMs ability to reason over web-retrieved information and adhere to structured tool-use protocols. To enable efficient training, we incorporate LoRA adapters with rank of = 8 across all transformer blocks of the LLM, thereby providing sufficient expressivity to capture the new behaviors required for web-information augmented reasoning while maintaining manageable number of trainable parameters. The SFT data consists of multi-turn conversations that include reasoning sequences, tool-call annotations, and web-retrieved content from search tools. Through exposure to these structured conversations, the model learns when to initiate searches, which tool to use, how to formulate effective queries, how to integrate retrieved information into reasoning, and how to comply with the strict formatting conventions, which are all necessary for the seamless integration of search tools. Training Objective. We adopt the standard Causal Language Modeling (Causal LM) objective. Given multimodal input (x, I), consisting of textual question and an accompanying image, along with multi-turn conversation that includes the complete reasoning trace, tool calls, and final answer, the model is trained to predict each token in the target sequence conditioned on all previous tokens: LSFT = (cid:88) t=1 log πθ(y x, I, <t) . Here, denotes the length of the target sequence, and πθ is the models conditional distribution. Importantly, the web-retrieved information from the search tools are masked during loss computation, ensuring that training is concentrated on reasoning and structured tool calls, rather than being influenced by raw web-retrieved information."
        },
        {
            "title": "3.2 Reinforcement Learning Stage",
            "content": "GRPO. The reinforcement learning stage relies on Group-Relative Policy Optimization (GRPO), introduced in DeepSeekMath (Shao et al., 2024). GRPO extends Proximal Policy Optimization (PPO) by stabilizing training through comparisons among candidate responses generated for the same prompt. Instead of evaluating each rollout independently, GRPO computes advantages relative to the mean reward within group of sampled rollouts. Given an input (x, I), the policy generates rollouts {y(i)}K i=1, each associated with reward R(i). The advantage for single rollout is then defined as A(i) = R(i) R, where is the average reward across the group. This centering removes the dependency on the absolute scale of rewards and focuses the optimization on identifying responses that are better than the group average. The objective is then optimized with clipped importance-weighted surrogate, similar to PPO, but incorporating this group-relative advantage. Mathematically, the update is expressed as (cid:104) LGRPO = Ei,t min (cid:0)ρ(i) A(i), clip(ρ(i) , 1 ϵ, 1 + ϵ)A(i)(cid:1)(cid:105) β KL(πθ πref), where ρ(i) is the ratio between the probabilities of token under the current and old policies, ϵ controls the clipping range, and β scales the KL regularization with respect to frozen reference model. This formulation encourages relative improvements within each batch of responses, yielding stable optimization even under noisy reward signals. Rollouts. The rollouts are generated from the model checkpoint after SFT. The SFT model interacts with the grounding tool, image search tool, and text search tool using the learned tool-call tag schema, incorporating feedback from these tools into subsequent turns. This process continues until either final response is produced or the maximum number of turns is reached. When generating responses, if the model cannot confidently identify visual entity in an image, it initiates either full-image search or cropped-image search. If the entity is identifiable but additional factual information is required, the model issues one or more text search queries to retrieve relevant details from the web. Each rollout thus represents complete reasoning trajectory, annotated with the tag schema learned during SFT. During training, constraints are applied on the number of tool calls and the maximum token length per trajectory, requiring the model to balance accuracy with efficiency. 5 Reward. The GRPO optimization uses composite reward balancing factual accuracy and structural compliance. We employ gpt-5-chat-latest as the reward model which judges semantic correctness of the predictions against the ground truth. The correctness score, denoted s, is binary (s {0, 1}), indicating whether the models final answer is judged correct. In parallel, format score sfmt measures adherence to the required output schema, ensuring correct tag usage and valid tool-call structure. The final reward is computed as Rtotal = (1 λfmt) + λfmt sfmt, where λfmt is the format penalty coefficient. This reward formulation drives the model to produce responses that are both factually reliable and consistent with the structured protocol required for seamless tool use."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Multimodal Search Tools. To retrieve additional context and up-to-date information, we employ multimodal search pipeline composed of three tools: text search tool, an image search tool, and grounding tool. The text search tool operates on DeepMMSearch-R1 generated textual queries, which are processed by an in-house web search API to retrieve relevant documents from large-scale index. The top five results are then condensed by an LLM-based summarization module, producing concise outputs directly relevant to the users question. To retrieve additional information about visual entity, DeepMMSearch-R1 is trained to utilize the image search tool. When the model determines that the question concerns only specific region of the image, it first produces referring expression that concisely describes the region of interest. GroundingDINO (Liu et al., 2024b) is then employed to ground this expression, yielding bounding box that is cropped and used as input for retrieval. In other cases, when the entire image is relevant, the original image is directly used without grounding. The resulting visual input is then passed to our in-house image search API, which retrieves visually similar images from the web along with surrounding context and page metadata. An LLM-based summarization module condenses the top five retrieved results, producing concise description of the visual entity. Neither the text nor the image search index or API were modified for use with DeepMMSearch-R1, demonstrating the models ability to operate seamlessly with standardized retrieval tooling. We employ gpt-5-chat-latest as the LLM summarizer to summarize search results of both the tools. This step is essential for keeping the retrieved content concise in order to avoid exceeding the models maximum context length. The prompt used for summarization is provided in Appendix E.7. Implementation Details. We finetune Qwen2.5-VL-7B-Instruct using the LLaMA-Factory framework (Zheng et al., 2024) with LoRA (rank 8) applied across all target modules. Training is performed for 3 epochs with learning rate of 1e4, cosine scheduler, and bf16 mixed precision on 1 node with 8 H100 GPUs. The global batch size is 8, and input masking is applied to optimize only on generated outputs for the multi-turn VQA dataset. For online RL optimization, we adopt the GRPO algorithm in the veRL (Sheng et al., 2024) framework, using gpt-5-chat-latest as the reward model. We set λfmt = 0.1, apply KL-penalty of 0.001, and use clip ratio of 0.2. Training runs for 30 epochs on 4 nodes 8H100 GPUs with batch size of 512, and rollout number of 8. warmup phase of 45 steps is applied with learning rate initialized at 2e6. The maximum response length is 8192 tokens, and input masking is again used to focus optimization on generated outputs. Additional implementation details are provided in Appendix D. Baselines. To benchmark the effectiveness of DeepMMSearch-R1, we evaluate it against diverse set of strong baselines, including closed-source models (GPT-4o and GPT-o3) as well as open-source models from the Qwen2.5-VL family. We organize our comparisons into four evaluation workflows: (1) Direct Answer, where models are instructed to produce concise answer without using any external retrieval; (2) RAG Workflow, where models are required to perform exactly two retrieval steps for each question, first conducting an image search, followed by text search. In this setting, the retrieved image results are provided in the first round to guide text query generation, and the retrieved text results are supplied in the second round to produce the final answer. While this workflow maximizes exposure to external knowledge, it can also introduce noise when irrelevant or low-quality search content is retrieved; (3) Prompt based Search Agents, where the base model is prompted to make use of the multimodal search tools and the retrieved results are incorporated in generating the final response; and (4) Web-search-equipped MLLMs, which refers to models capable of performing on-demand, multi-turn search. Prior works such as Search-R1 (Jin et al., 2025) and ReSearch (Chen et al., 2025) are restricted to text-based retrieval and therefore cannot be considered true baselines. MMSearch-R1 (Wu et al., 2025), on the other hand, supports multimodal retrieval and serves as our only baseline. The prompts used for all the workflows are detailed in the Appendix E. Datasets. We use the InfoSeek (Chen et al., 2023) dataset to construct DeepMMSearchVQA, which serves as the training set for SFT stage. For online GRPO optimization, we employ the FVQA (Wu et al., 2025) training set. We select the FVQA dataset because it contains higher proportion of questions requiring image search compared to the InfoSeek dataset used in the SFT stage. This choice encourages more frequent image search tool calls, which is essential for achieving better performance in multimodal information-seeking VQA. For evaluation, we employ the InfoSeek 6 Model Average InfoSeek Enc-VQA SimpleVQA DynVQA OKVQA A-OKVQA InternVL2.5-8B InternVL3-8B Qwen2.5-VL-7B Qwen2.5-VL-32B GPT-4o o3 Qwen2.5-VL-7B Qwen2.5-VL-32B GPT-4o o3 Qwen2.5-VL-7B Qwen2.5-VL-32B MMSearch-R1-7B* DeepMMSearch-R1-7B (SFT) DeepMMSearch-R1-7B (RL) 40.46 41.53 43.11 50.04 50.16 60.38 36.00 35.50 41.50 47.49 48.24 50.94 50.56 56.23 57.13 Direct Answer 19.70 21.50 18.75 27.25 27.15 49.15 RAG Workflow 38.95 42.00 44.50 49.15 17.57 16.85 26.38 31.09 35.96 48.22 41.13 40.20 45.86 50.34 35.44 37.51 47.48 47.29 48.27 53. 29.71 28.53 35.93 35.74 Prompt based Search Agent 29.75 28.61 27.85 32.80 46.89 48.67 Web-Equipped MLLMs 41.33 47.45 47. 36.85 50.35 52.25 53.90 52.02 55.87 13.71 17.38 20.14 29.23 31.19 41.68 39.02 40.98 43.22 47.41 22.38 40.00 40.14 43.08 45. 74.61 72.85 63.10 78.22 71.96 80.36 34.64 35.37 38.76 51.70 77.15 72.87 59.89 67.52 67.80 81.75 83.06 82.79 87.16 86.46 89.78 32.58 25.94 40.70 50. 85.41 82.71 71.27 76.94 73.45 Table 1 Performance comparison of DeepMMSearch-R1 with baselines of three categories. *The reported numbers are obtained by evaluating the model using the same image-search and text-search APIs that we use. For fair comparison, we follow the evaluation prompt on which the baseline was trained. test split along with DynVQA (Li et al., 2024c), SimpleVQA (Cheng et al., 2025), Encyclopedic-VQA (Mensink et al., 2023), OKVQA (Marino et al., 2019), and A-OKVQA (Schwenk et al., 2022) as benchmark datasets. Due to the large size of InfoSeek and Encyclopedic-VQA, we randomly sample 2000 examples from the test split of each. For SimpleVQA, we include all QA examples written in English. OK-VQA and A-OKVQA consist of relatively simple questions derived from COCO (Lin et al., 2014) images, requiring little to no search. These benchmarks evaluate models on outside-knowledge questions, but since many modern MLLMs now include COCO in their pretraining (Bai et al., 2025), the datasets have become easier and largely search-free. Evaluation Metric. We evaluate model performance using the LLM-as-Judge framework, where LLM is employed to assess the accuracy of responses. We adopt this approach to capture nuanced correctness in the multimodal, open-ended VQA task, which is often challenging for traditional automatic metrics. Specifically, we use gpt-5-chat-latest as the judging model. It is provided with the question, the ground-truth answer, and the models response, and then determines whether the response is correct. The full evaluation prompt is provided in Appendix E.5."
        },
        {
            "title": "4.2 Results and Analysis",
            "content": "Web-search equipped MLLMs outperform RAG workflows and prompt-based search agent baselines. As shown in Table 1, DeepMMSearch-R1-7B (RL) surpasses both RAG workflows and prompt-based search agent baselines by significant margin (+21.13 and +8.89, respectively), while achieving competitive performance with the OpenAI o3 model (OpenAI, 2025). On datasets such as OK-VQA and A-OKVQA, we observe substantial drop in RAG workflow performance compared to direct answering. This is because the majority of questions in these datasets (> 50%) can be answered without search, and incorporating web-search results into the models reasoning introduces noise, leading to performance decline. In contrast, the prompt-based search agent baselines exhibit more stable performance gain, as the model is explicitly prompted to invoke multimodal search tools and incorporate retrieved results only when necessary. However, since these models are not explicitly trained to interact with web-search tools, their performance remains inferior to that of web-search equipped MLLMs. DeepMMSearch-R1-7B (RL) delivers the largest performance boost, validating the importance of training models to use search tools rather than relying on fixed retrieval strategies or test-time prompting. These results validate that finetuning models to leverage search tools and associated tag schema improves performance and also makes the retrieval cost effective by making web-search more efficient and intelligent. Cropped image search and distilled self-reflection and self-correction capabilities boost performance. We showcase the impact of enabling multiple text searches and cropped image search capability in Figure 3(left).The SFT base model refers to the setup with whole-image search and single text search call. We see that, on average, performance improves with distilled self-reflection and self-correction. This enables the model to iteratively refine its queries in response to retrieved web results and better navigate noisy real-world information. similar trend is observed with cropped image search, yielding an average improvement of +1.75 across six datasets, highlighting its effectiveness. It helps mitigate background noise and makes the search more effective, and is particularly useful for 7 Figure 3 (left) Impact of self-reflection, self-correction and cropped image search on performance. (right) Effect of the ratio of search-required to search-free data, and of sampling strategies when curating SFT data. Figure 4 Tool usage statistics after supervised finetuning and online RL on DynVQA and OK-VQA benchmarks. answering questions that concern single visual entity in the image rather than the entire scene. We also observe that improvements on SimpleVQA and DynVQA are relatively higher, which aligns with expectations since these datasets are newer and contain higher proportion of questions that require search. Search-balanced SFT data with examples uniformly sampled from all knowledge taxonomy categories provides better performance. Firstly, we perform ablations with different ratios of search-required and search-free examples in the SFT data to study their impact on performance. From Figure 3(right), we observe that when the percentage of search-required questions is high, the finetuned model exhibits excessive search behavior and performs poorly on OKVQA and A-OKVQA, which require fewer search calls. Conversely, when the proportion of searchrequired questions in the SFT data is reduced, the model shows performance drop on datasets with more challenging information-seeking questions, such as InfoSeek, Enc-VQA, DynVQA, and SimpleVQA. We therefore conclude that 50:50 balance provides the most effective configuration, as it distills search tool call behavior well and yields the best average performance across all datasets. Secondly, we examine the influence of maintaining balanced distribution of examples across all categories in the knowledge taxonomy. As shown in Figure 3(right), uniformly sampling examples from each category leads to better average performance compared to random sampling. SFT enables tool use, while RL refines the tool-selection behavior by reducing unnecessary calls. We summarized the tool usage of our model after the SFT and RL stages for two datasets in Figure 4. DynVQA is newer dataset with more questions requiring external information, while OKVQA requires fewer search calls. The tool usage behavior of our model aligns with the nature of each dataset, leveraging tools for 87.7% of the samples in DynVQA compared to 43.5% in OKVQA. Moving from the SFT to RL stage, we make three critical observations regarding tool use behaviour. (1) The model performs more image searches compared to text searches, resulting in an increase in both image search and mixed search tool calls. This behavior is desirable, as most questions are multimodal in nature, requiring both the identification of visual entities and the retrieval of factual information from the web. (2) After RL training, the model invokes multiple text searches more frequently, highlighting the role of RL in promoting self-reflection and self-correction. Specifically, we observe +1.54% and +2.64% more samples where the model refines its queries when the retrieved web information is insufficient to answer the question. (3) The model drastically reduces its reliance on cropped image searches (36.81% on DynVQA and 34.86% on OK-VQA), yet still achieves overall performance gains. While this may seem counterintuitive, closer analysis shows that the model becomes more selective, and performs cropping operation only when necessary. For instance, the SFT model sometimes performed cropped image searches unnecessarily, whereas the RL model corrected these errors as shown in Figure 5. This observation reinforces the importance of RL in refining tool-use behavior and making it more efficient. We further observe that DeepMMSearch-R1-7B (RL) exhibits cropped image searches or self-reflection behavior in 8 Figure 5 After SFT, the model performs unnecessary cropping. RL training refines the tool-use behavior, making it more efficient and invoking tools only when required. Models OCRBench MMVet AI2D MathVista MINI MMBench DocVQA InfoVQA Qwen2.5-VL-7B-Instruct DeepMMSearch-R1-7B (RL) 88.30 87.60 68.30 69.81 83.74 82. 68.20 66.80 83.84 83.76 94.97 94.63 82.58 81.63 Table 2 DeepMMSearch-R1 performance on general VQA benchmarks in comparison to Qwen2.5-VL. 11.64% of questions on DynVQA and 18.95% on the OKVQA dataset, which constitutes key part of our contributions. Overall, this analysis reinforces that SFT training equips the model with tool-use capabilities, while RL training refines tool selection, making it more efficient and better targeted for multimodal information-seeking tasks. SFT using LoRA modules and Online GRPO with KL penalty maintains general VQA capabilites. To quantify the impact of SFT+RL training on the proposed models general VQA and reasoning ability, we benchmark DeepMMSearchR1-7B (RL) on range of benchmarks, including OCRBench (Liu et al., 2024d), MMVet (Yu et al., 2023), AI2D (Kembhavi et al., 2016), MathVista (Lu et al., 2024a), MMBench (Liu et al., 2024c), DocVQA (Mathew et al., 2021), and InfoVQA (Mathew et al., 2022). As shown in Table 2, we observe that the model maintains its performance across these datasets, suggesting that the proposed model effectively learns to interact with web-search tools while preserving its general visual understanding and reasoning capabilities. This can be attributed to: (1) the use of LoRA module in the SFT stage which updates only limited subset of parameters, minimizing deviation from the pretrained backbone and preserving core multimodal reasoning capabilities, and (2) the use of KL-divergence penalty during the Online GRPO stage that constrains the policy updates from straying too far from the supervised distribution, thereby regularizing learning and preventing catastrophic drift. These training strategies enable the model to acquire web-search abilities while maintaining, and in some cases improving, general VQA performance."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose DeepMMSearch-R1, novel multimodal large language model designed to enhance visual question answering in knowledge-intensive and information-seeking contexts by integrating on-demand, multi-turn web search capabilities. Our approach addresses the limitations of prior retrieval-augmented methods and multimodal agents by enabling dynamic, iterative query refinement through self-reflection and self-correction, as well as incorporating cropped image search tool. We achieve this with two-stage training pipeline: (1) supervised finetuning (SFT) stage using the proposed DeepMMSearchVQA, which equips the model with tool-use capabilities, followed by (2) online reinforcement learning (RL) via GRPO, which further refines tool-use behavior to make it more efficient and effective. DeepMMSearch-R1 outperforms prior baselines across six benchmarks. We believe DeepMMSearch-R1 represents compelling step forward in real-world, multimodal information-seeking AI, with promising applications in web agents, education, and digital assistance. Future works may explore expanding tool diversity, long-context reasoning, and scaling training to broader multilingual and multimodal domains."
        },
        {
            "title": "Acknowledgment",
            "content": "The authors would like to thank David Haldimann and Juan Lao Tebar for valuable discussions and feedback."
        },
        {
            "title": "Ethics Statement",
            "content": "This work introduces methods to enhance multimodal LLMs with real-time web-search capabilities. While such systems offer clear benefits in improving informativeness and adaptability, they also raise ethical risks. Retrieved content may include biased, outdated, or misleading information, and automatic summarization can amplify misinformation or raise copyright concerns. Moreover, the approach depends on external infrastructure, which may limit accessibility for resource-constrained institutions. We encourage responsible deployment practices, including source attribution, content filtering, and human oversight in high-stakes applications."
        },
        {
            "title": "Reproducibility statement",
            "content": "We have taken care to ensure that our work can be reproduced by the research community. All details of our training and evaluation setup are provided in the paper, including data generation pipeline, base model architecture, datasets, and training procedures. We report all hyperparameters used for both supervised finetuning and reinforcement learning, along with implementation details such as batch sizes, learning rates, and optimization schedules. Additionally, we provide the full prompts used for dataset generation, evaluation, and reward modeling in Appendix E. Together, these resources make it possible to replicate our experiments and verify our results."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Pan, Wen Zhang, Huajun Chen, Fan Yang, et al. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470, 2025. Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. Can pre-trained vision and language models answer visual information-seeking questions? arXiv preprint arXiv:2302.11713, 2023. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, et al. Simplevqa: Multimodal factuality evaluation for multimodal large language models. arXiv preprint arXiv:2502.13059, 2025. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv e-prints, pages arXiv2409, 2024. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 39293938. PMLR, 2020. Apple and the Apple logo are trademarks of Apple Inc., registered in the U.S. and other countries and regions. 10 Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David Ross, and Alireza Fathi. Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2336923379, 2023. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Chaoyou Fu, Guanglu Song, et al. Mmsearch: Benchmarking the potential of large models as multi-modal search engines. arXiv preprint arXiv:2409.12959, 2024. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images, 2016. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Chuanhao Li, Zhen Li, Chenchen Jing, Shuo Liu, Wenqi Shao, Yuwei Wu, Ping Luo, Yu Qiao, and Kaipeng Zhang. Searchlvlms: plug-and-play framework for augmenting large vision-language models by searching up-to-date internet knowledge. Advances in Neural Information Processing Systems, 37:6458264603, 2024b. Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Philip Yu, Fei Huang, et al. Benchmarking multimodal retrieval augmented generation with dynamic vqa dataset and self-adaptive planning agent. arXiv preprint arXiv:2411.02937, 2024c. Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2668926699, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. Zihan Ling, Zhiyao Guo, Yixuan Huang, Yi An, Shuai Xiao, Jinsong Lan, Xiaoyong Zhu, and Bo Zheng. Mmkb-rag: multi-modal knowledge-based retrieval-augmented generation framework. arXiv preprint arXiv:2504.10074, 2025. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023b. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https://llava-vl.github.io/blog/ 2024-01-30-llava-next/. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pages 3855. Springer, 2024b. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024c. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024d. 11 Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Rar: Retrieving and ranking augmented mllms for visual recognition. arXiv preprint arXiv:2403.13805, 2024e. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024a. Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024b. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Anton Belyi, et al. Mm1: methods, analysis and insights from multimodal llm pre-training. In European Conference on Computer Vision, pages 304323. Springer, 2024. Thomas Mensink, Jasper Uijlings, Lluis Castrejon, Arushi Goel, Felipe Cadar, Howard Zhou, Fei Sha, André Araujo, and Vittorio Ferrari. Encyclopedic vqa: Visual questions about detailed properties of fine-grained categories. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 31133124, 2023. Vishwesh Nath, Wenqi Li, Dong Yang, Andriy Myronenko, Mingxin Zheng, Yao Lu, Zhijian Liu, Hongxu Yin, Yee Man Law, Yucheng Tang, et al. Vila-m3: Enhancing vision-language models with medical expert knowledge. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1478814798, 2025. OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/introducing-o3-and-o4-mini/, April 2025. Accessed: October 15, 2025. Jingyuan Qi, Zhiyang Xu, Rulin Shao, Yang Chen, Jin Di, Yu Cheng, Qifan Wang, and Lifu Huang. Rora-vlm: Robust retrieval-augmented vision language models. arXiv preprint arXiv:2410.08876, 2024. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In European conference on computer vision, pages 146162. Springer, 2022. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024a. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024b. Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, Liangfu Cao, Pengxin Zhan, Lunhao Duan, Shiyin Lu, Minghao Fu, Jianshan Zhao, Yang Li, and Qing-Guo Chen. Ovis-u1 technical report. arXiv preprint arXiv:2506.23044, 2025. 12 Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, et al. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing Systems, 37:121475121499, 2024. Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. Mmsearch-r1: Incentivizing lmms to search. arXiv preprint arXiv:2506.20670, 2025. Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, et al. Visrag: Vision-based retrieval-augmented generation on multi-modality documents. arXiv preprint arXiv:2410.10594, 2024. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. Mm1. 5: Methods, analysis & insights from multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566, 2024a. Zhixin Zhang, Yiyuan Zhang, Xiaohan Ding, and Xiangyu Yue. Vision search assistant: Empower vision-language models as multimodal search engines. arXiv preprint arXiv:2410.21220, 2024b. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025."
        },
        {
            "title": "Appendix",
            "content": "In the appendix, we provide our LLM usage statement, elaborate on related works and describe the datasets used. Additionally, we focus on its implementation and provide extensive details about the prompts used for dataset curation and the evaluation. Finally, we provide visual examples of the proposed dataset."
        },
        {
            "title": "Table of Contents",
            "content": "A LLM Usage Statement Related Works B.1 Multimodal LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 RAG-based search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Prompt-based search agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Web-search equipped MLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Datasets C.1 InfoSeek . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 FVQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Encyclopedic VQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 SimpleVQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.5 DynVQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.6 OKVQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.7 A-OKVQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Implementation Details Prompts E.1 SFT dataset generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 WebSearch Equipped MLLMs evaluation Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 RAG Workflow Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Prompt-based Search Agent Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.5 LLM-as-judge prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.6 Prompt for gpt as reward model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.7 LLM Summarizer Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Content F.1 Introduction Figure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . DeepMMSearchVQA Samples 15 15 15 15 16 16 16 16 16 17 17 17 17 18 19 20 20 20 21 22"
        },
        {
            "title": "Go to Appendix Index",
            "content": ""
        },
        {
            "title": "A LLM Usage Statement",
            "content": "We use large language model (LLM) as supportive tool in this work. It was employed to assist in debugging portions of the code, generating and refining visualization figures, and improving the clarity of the manuscript through proofreading, grammar checking, and polishing the overall writing style. The LLMs role was limited to these supportive tasks, while all substantive research ideas, methodological decisions, analyses, interpretations, and final code implementations were developed and validated independently by us."
        },
        {
            "title": "B Related Works",
            "content": "B.1 Multimodal LLMs Multimodal large language models (MLLMs) combine visual encoders with powerful text-based large language models, enabling them to process and reason over both textual and visual inputs. Recent models such as GPT-4o (Hurst et al., 2024), Gemini (Team et al., 2023), Qwen2.5-VL (Bai et al., 2025), InternVL (Chen et al., 2024), LLaVA series (Li et al., 2024a; Lin et al., 2023; Liu et al., 2023b,a, 2024a), Phi (Abdin et al., 2024), MM1 (McKinzie et al., 2024; Zhang et al., 2024a), OVIS (Lu et al., 2024b; Wang et al., 2025), VILA (Lin et al., 2024; Nath et al., 2025), Gemma (Team et al., 2024a,b) have demonstrated strong capabilities in visual perception, grounding, and multimodal reasoning, achieving remarkable progress in tasks like visual question answering, captioning, and multimodal dialogue. These advances highlight their potential as core components in real-world applications such as digital assistants, education, and information access. Despite these strengths, MLLMs face fundamental limitations in addressing knowledge-intensive or information-seeking queries (Mensink et al., 2023; Chen et al., 2023; Cheng et al., 2025; Li et al., 2024c). Their training relies on static corpora, which inevitably leads to outdated knowledge as the real world evolves. Furthermore, the breadth of open-world knowledge follows long-tail distribution, and it is infeasible to cover every rare or emerging fact within fixed training dataset (Mensink et al., 2023). This makes MLLMs struggle with long-tail knowledge and information that requires up-to-date context. B.2 RAG-based search The RAG paradigm as the name suggests retrieves external information from fixed knowledge corpora using vector search and augments it into the model context to generate factually grounded responses. Early contributions in this space include REALM (Guu et al., 2020), which introduced retrieval-augmented pretraining by jointly optimizing dense retriever with language model to enable knowledge-intensive tasks. RAG (Lewis et al., 2020) further advanced this paradigm by integrating generative seq2seq model with neural retrieval, demonstrating strong gains in open-domain question answering. Recent efforts have extended retrieval augmentation to multimodal settings. REVEAL (Hu et al., 2023) presented retrieval-augmented visual-language pretraining framework, in which the memory, encoder, retriever and generator are all pre-trained end-to-end on massive amount of data. VisRAG (Yu et al., 2024) proposed vision-language model based RAG pipeline that directly embeds documents as images for retrieval, avoiding information loss from text parsing. This strategy enables the joint filtering of retrieved documents, retaining only the most relevant and accurate references. RoRA-VLM (Qi et al., 2024) introduced two-stage retrieval process with image-anchored textual-query expansion to synergistically combine the visual and textual information in the query and retrieve the most relevant multimodal knowledge snippets. Moreover, they improve the robustness of retrieval-augmented vision-language model by injecting adversarial noise in the training process. RaR (Liu et al., 2024e) proposed retrieving-and-ranking augmented multimodal framework tailored for visual recognition, highlighting the role of retrieval quality in multimodal perception tasks. Recently, MMKB-RAG (Ling et al., 2025) proposed novel multi-modal RAG framework that leverages the inherent knowledge boundaries of models to dynamically generate semantic tags for the retrieval process. Despite these advances, RAG methods rely on static corpora, and work with an unrealistic assumption that all information can be captured within fixed dataset. In real-world scenarios, web information is dynamic and constantly evolving, and the complexity of retrieval remains high. These factors pose significant challenges for adopting RAG in real-world, open-ended VQA. B.3 Prompt-based search agents The prompt-based search agents act as plug-and-play modules that can be integrated with existing multimodal LLMs without additional finetuning. In this setup, the MLLM functions as an agent, incorporating web-retrieved information into its responses. For example, VSA (Zhang et al., 2024b) enables any vision-language model to operate as multimodal automatic search engine. Its pipeline follows three steps: (1) visual content formulation, where the model identifies the object of interest; (2) web-knowledge search, where it generates multiple sub-questions and"
        },
        {
            "title": "Go to Appendix Index",
            "content": "15 queries the web; and (3) summarization, where it consolidates the retrieved information before answering the users query. Similarly, MMSearch (Jiang et al., 2024) introduces the MMSearch-Engine, pipeline that augments large multimodal models with search capabilities through requerying, reranking, and summarization. OmniSearch (Li et al., 2024c) further advances this idea by proposing self-adaptive planning agent for multimodal retrieval. It dynamically decomposes complex questions into sequential sub-questions and selects retrieval actions accordingly. At each step, the planner evaluates prior retrieval feedback (via solver) to decide whether to refine the query, switch retrieval mode (e.g., text, image, web), or generate new sub-questions. This flexible, feedback-driven process replaces rigid heuristics with query-planning loop, better suited for dynamic, multi-hop, and multimodal VQA scenarios. However, across these approaches, the base model itself is not trained to engage effectively with web-retrieved information and external search tools, leaving it less capable of handling the noisy and complex nature of such real-world web information. B.4 Web-search equipped MLLMs Recent work focuses on R1-optimization of MLLMs to equip web-search capabilities in MLLMs. This trend follows from the success of reasoning models such as OpenAI o1,o3 and DeepSeek-R1. DeepResearcher (Zheng et al., 2025) uses multi-agent browsing architecture and the GRPO algorithm to learn to navigate, extract, and filter information from arbitrary web pages under realistic constraints (e.g., API limits, network latency, anti-crawling). R1-Searcher (Song et al., 2025) presents two-stage outcome-based reinforcement learning framework that allows LLMs to autonomously invoke external search systems during reasoning for knowledge-intensive tasks. In stage one, the model is rewarded for learning to trigger retrieval (without regard to answer correctness), and in stage two it is further trained to integrate retrieved evidence to maximize answer accuracy. Search-R1 (Jin et al., 2025) incorporates retrieved-token masking, which prevents the RL objective from directly optimizing over retrieved content, stabilizing training when mixing generated and retrieved tokens. However, all these works are restricted to text search and are unable to perform an image search, which limits their applicability in mulitmodal knowledge-intensive question answering. MMSearch-R1 (Wu et al., 2025) is the only prior work that performs multimodal retrieval, but it has notable limitations. First, although the model can autonomously decide which tool to use, it is constrained to single invocation per tool, which limits its ability to revise decisions through self-reflection and self-correction. Second, in knowledge-intensive VQA tasks, it is essential to precisely identify the visual entity in the image that the question refers to. However, in real-world settings, background clutter and the presence of irrelevant visual entities often introduce noise into the retrieval process. This noise can hinder accurate localization of the target entity, leading to suboptimal retrieval and reduced effectiveness of image search in practice. To address these limitations, we propose DeepMMSearch-R1, which performs image search using relevant image crops and can iteratively refine its text search queries to better navigate noisy real-world web information."
        },
        {
            "title": "C Datasets",
            "content": "C."
        },
        {
            "title": "InfoSeek",
            "content": "InfoSeek (Chen et al., 2023) is large-scale knowledge-intensive visual question answering dataset designed for information-seeking tasks. It consists of 8,900 human-written questionanswer pairs over 806 entities and 527 entity types, as well as 1.35 million automatically generated QA triplets covering 11,481 entities across 2,739 entity types. The dataset is split into UNSEEN ENTITY and UNSEEN QUESTION partitions to test generalization. InfoSeek is widely used for evaluating multimodal models in knowledge retrieval and reasoning beyond surface-level recognition. C.2 FVQA FVQA (Wu et al., 2025) is multimodal search VQA dataset constructed to enable evaluation and training of models that must decide when and how to perform external searches in knowledge-intensive setting. The FVQA training split (FVQA-train) comprises around 6,000 imagequestionanswer samples (FVQA-auto-vc) focused on visual knowledge, plus 7,000 text-knowledge examples drawn from InfoSeek (FVQA-auto-txt), and an additional 800 manually annotated FVQA-manual-train samples. The test split (FVQA-test) is manually curated for higher quality and diverse knowledge demands. C.3 Encyclopedic VQA Encyclopedic VQA (Mensink et al., 2023) is large-scale visual question answering dataset that focuses on visual questions about detailed properties of fine-grained object categories and specific instances. It comprises 221,000 unique questionanswer pairs, each associated with up to 5 different images, yielding total of around 1,000,000 (1 M)"
        },
        {
            "title": "Go to Appendix Index",
            "content": "16 image-question-answer instances. The dataset is backed by controlled knowledge base derived from Wikipedia, where each QA is linked to supporting evidence from Wikipedia articles. C.4 SimpleVQA SimpleVQA (Cheng et al., 2025) is multimodal benchmark created to evaluate the factuality of MLLMs in answering short, natural-language visual questions. It contains 2,025 high-precision imagequestionanswer pairs, spanning 9 task categories (e.g. Object Identification & Recognition, Time & Event, Person & Emotion, Location & Building, Text Processing, Quantity & Position, Art & Culture, Object Attributes) and 9 topic domains. SimpleVQAs design ensures coverage across domains, concise and clear answer formats, and suitability for automated evaluation (e.g. via LLM-as-judge). It is intended to challenge MLLMs abilities to ground answers in factual knowledge rather than hallucinate, and is often used to probe the knowledge boundaries of vision-language models. C.5 DynVQA DynVQA (Li et al., 2024c) is benchmark dataset constructed to assess multimodal retrieval-augmented generation (mRAG) systems on dynamic visual question answering tasks that require adaptive retrieval strategies. It contains 1,452 questions spanning 9 domains, evenly split across English (715) and Chinese (737) items. The questions are categorized into three dynamic types: (1) those with rapidly changing answers (385 questions, 26.5%), (2) multimodal-knowledge questions requiring non-textual evidence (178 questions, 12.3%), and (3) multi-hop questions requiring multi-step reasoning (112 questions, 7.7%). Across all questions, 59.6% require external visual knowledge beyond what is directly in the image, and 26.7% require more than two reasoning hops. DynVQA is designed with temporal dynamism, and some answers may change over time. Therefore, the dataset is periodically updated to maintain answer correctness. C.6 OKVQA OKVQA (Marino et al., 2019) is knowledge-based visual question answering dataset in which the visual content alone is insufficient to answer questionsmodels must draw on external knowledge. It comprises 14,055 open-ended questionanswer (QA) pairs associated with 14,031 images. Each QA is annotated with 5 ground truth answers per question. To reduce dataset bias, frequently repeated answers were pruned, such that questions whose most common answer appeared more than 5 times were removed. The dataset covers diverse set of 10 knowledge categories (e.g., Vehicles & Transportation; Cooking & Food; Science & Technology) determined via crowd annotations. Baseline VQA models that perform well on standard VQA benchmarks show significant performance drops on OKVQA, highlighting the difficulty of knowledge retrieval and reasoning in this setup. C.7 A-OKVQA A-OKVQA (Augmented OK-VQA) is crowdsourced visual question answering benchmark designed to require commonsense and world knowledge beyond simple fact lookup. It comprises approximately 24,903 questionanswerrationale triplets spread across 17.1K training, 1.1K validation, and 6.7K test splits. Each question is accompanied by both multiple-choice (MC) options and direct-answer (DA) alternatives, along with rationale (one explanatory sentence) for the train split. To ensure diversity, A-OKVQA filters out trivial or overly repetitive questions, enforces quality control via crowd annotation, and clusters similar images to discourage repetitive phrasing. Compared to OKVQA, A-OKVQA contains about twice as many questions and adds rationale annotations to support explainable reasoning."
        },
        {
            "title": "D Implementation Details",
            "content": "We use the LLaMA-Factory framework to perform supervised finetuning. Our base model is Qwen2.5-VL-7B-Instruct, which we finetune using LoRA with rank of 8 applied across all target modules. Training is performed for 3 epochs with learning rate of 1e-4, following cosine scheduler with warmup ratio of 0.1. We enable bf16 mixed precision for computational efficiency. The model is trained using 1 node with 8 Nvidia H100 GPUs, with per-device batch size is set to 1, with gradient accumulation of 1 step, resulting in global batch size of 8. Since the VQA dataset consists of multi-turn conversations, we apply input masking during training to ensure that the model is optimized only on the generated outputs. For online RL optimization, we adopt the GRPO algorithm implemented in the veRL framework. The reward model is GPT-4o, which evaluates generated responses and provides feedback for optimization, with λfmt set to 0.1. We apply KL-penalty with coefficient of 0.001 and the clip ratio is set to 0.2. Training is performed for 20 epochs on 4 nodes each with 8 Nvidia H100 GPUs. We use batch size of 256 with mini-batch size of 64, and set the rollout number to 8 per iteration. warmup phase of 45 steps is applied to stabilize learning rates, which are"
        },
        {
            "title": "Go to Appendix Index",
            "content": "17 initialized at 2e-6. The image search/cropped image search tool can be called once while the text-search tool can be called multiple times, with total tool calls restricted to 10 per rollout. The maximum response length is set to 8192 tokens We again mask the input tokens to ensure that optimization focuses only on the generated outputs."
        },
        {
            "title": "E Prompts",
            "content": "E.1 SFT dataset generation Initial Prompt You are an expert visual assistant. Your task is to answer users question based on the provided image. Step 1: Analyze the Image Carefully examine the image and the users question: {question}. Identify all recognizable entities, objects, text, and other visual clues. Step 2: Plan Your Action Based on your analysis, you must perform one of the following actions. You must include your thinking process inside <reason>...</reason> block before choosing an action. Action 1: Answer Directly If you can confidently identify the visual element and have the internal knowledge regarding the facts sufficient to answer the question, provide direct, concise answer inside <answer>...</answer> tag. Example: <answer>The construction of Eiffel Tower was finished on 03/31/1889.</answer> Action 2: Use Image Search If you are not sure about the visual element and need to identify the visual element in the image, you can use one of the following image search methods. Cropped search (Preferred for specific questions): Use this if the question is clearly about specific visual element such as an object, person, animal, plant, aircraft, etc., or if the background is irrelevant. Describe the visual element concisely inside the <img_search>...</img_search> tags. Example: <img_search>the face of the person on the left</img_search> <img_search>the red logo on the baseball cap</img_search> Whole image search: Only use this if the question is about the entire scene in general, its location, or the overall context. Output only: <img_search><img></img_search>. Note: Do not output <img_search><img></img></img_search>. Action 3: Use Text Search If you can identify the visual element confidently but need more specific information to answer the question, invoke the text search tool. Generate focused query and output it as <text_search>your search query</text_search>. Remember, search results will be provided to you in subsequent turn. You can analyze the search results and decide your next action. You can perform image search only once, but have the option to perform multiple text searches to gather relevant information. All search results will be placed inside <information>...</information>. Here is the image and question: <image>{question}"
        },
        {
            "title": "Go to Appendix Index",
            "content": "18 After Image Search You have received information from an image search. Your goal is to use this new information to answer the original question: {question}. Step 1: Analyze the Results Review the provided information within the <information>...</information> block. Synthesize what youve learned about the visual element in question. Step 2: Plan Your Next Action Include your thinking process inside <reason>...</reason> block. Then, choose one of the following actions: Action 1: Answer Directly If the image search results have helped you identify the visual element and you can confidently answer the question with your internal knowledge, provide the final, concise answer inside an <answer>...</answer> tag. Action 2: Use Text Search If the image search results have helped you identify the visual element but you need more specific details to answer the question, invoke the text search tool. Formulate precise query based on the image search results and output it as <text_search>your search query</text_search>. You can use the text search tool multiple times in subsequent turns if needed. After Text Search You have received results from text search. Your goal is to analyze this new information and decide the next best step to answer the original question: {question}. Step 1: Analyze the Results Review the new information provided in the <information>...</information> block. Compare it against the information you already have and what is still needed to answer the question. Step 2: Plan Your Next Action Include your thinking process inside <reason>...</reason> block. Then, choose one of the following actions: Action 1: Answer Directly If you have now gathered all the necessary information, provide the final, concise answer inside an <answer>...</answer> tag. Action 2: Search Again If the results are helpful but still insufficient, perform another text search. Create new, more specific, or modified query to find the missing facts. Output the new query as <text_search>your refined search query</text_search>. Action 3: Give Up If you have exhausted your search attempts and believe the answer cannot be found from the provided information, conclude by outputting <answer>Unable to answer due to lack of relevant information</answer>. E.2 WebSearch Equipped MLLMs evaluation Prompt The evaluation prompt is same as SFT data generation prompts as detailed in Section E.1."
        },
        {
            "title": "Go to Appendix Index",
            "content": "19 E.3 RAG Workflow Prompt Initial Prompt You are helpful assistant designed to answer questions about images using external knowledge. You are given question accompanied by an image that cannot be answered without external knowledge. You are provided with question, the corresponding image, and text summary from reverse image search that identifies the main visual subject. Based on all this information, your task is to formulate single, effective query for search engine (e.g., Google) to find the specific facts needed to answer the question. Question: {question} Reverse Image Search Information: {information} Provide only the text query you will use for the search, in the format <text_search>your query</text_search>. Final Answer Prompt You have now received the results from your text search. Your goal is to analyze the text search results to provide final concise answer to the original question based on the image provided. Original Question: {question} Text Search Results: {information} Follow the following process: 1. Briefly explain your reasoning process by analyzing the facts from the search results that are relevant to the question. Enclose this reasoning inside <reason>your reason</reason> tags. 2. Provide the final, direct answer to the question between <answer> and </answer> tags. If the information is insufficient, respond ONLY with: <answer>Unable to answer due to lack of relevant information.</answer> E.4 Prompt-based Search Agent Prompt The prompt-based search agent prompts are same as SFT data generation prompts as detailed in Section E.1. E.5 LLM-as-judge prompt LLM-as-judge Prompt You are an impartial judge evaluating models answer for visual question answering task. Your task is to determine if the Predicted Answer is correct by comparing it against the Ground-Truth Answer(s). IMPORTANT INSTRUCTION: The Ground-Truth Answer(s) field may contain alternate correct answers. The predicted answer should be considered CORRECT if it is semantically equivalent to at least ONE of the provided ground-truth answers. Please respond with only [CORRECT] if the prediction is correct, and [INCORRECT] otherwise. Evaluation Details Question: {question} Ground-Truth Answer(s): {references_for_prompt} Predicted Answer: {candidate}"
        },
        {
            "title": "Go to Appendix Index",
            "content": "20 E.6 Prompt for gpt as reward model GPT-4o as reward model prompt You are strict evaluation judge for short-answer matching. Given models final answer and list of gold answers, decide if the models answer matches ANY gold answer. Rules: 1. Semantic Equivalence: Consider synonyms, paraphrases, and common aliases as valid matches. Example: \"NYC\" \"New York City\". 2. Ignore Trivial Differences: Do not penalize differences in articles, punctuation, word order, or casing. Example: \"The Pacific Ocean\" \"pacific ocean\". 3. At Least One Match: If the models answer aligns with ANY gold answer based on the rules, set match=true. Otherwise, match=false. 4. Numerical Flexibility: For answers involving numbers, an answer is MATCH if it meets any of these criteria: (a) Range Inclusion: The model provides range that contains the gold answer. Example: Model: \"20 to 24\", Gold: [\"21\"]. (b) Reasonable Rounding: The models answer is reasonably rounded version of the gold answer. Example: Model: \"176\", Gold: [\"176.124\"]. (c) Unit Conversion: The models answer is equivalent but in different unit. Example: Model: \"3 km\", Gold: [\"3000 m\"]. 5. Substantive Difference: If the meaning, entity, or value differs in way not covered by the rules above, it is NOT match. Example: \"Jupiter\" = \"Mars\". Example: \"5.2\" = \"52\". Example: Model: \"10-15\", Gold: [\"16\"] NO MATCH. Output Format: MATCH: true/false REASON: concise explanation focusing only on why the answer matches or does not match. E.7 LLM Summarizer Prompt Image Search Summarization Prompt Based on the following text extracted from the title and description of the retrieved images obtained from Google Lens search, concisely describe the primary visual content (such as faces, objects, locations, events, logos, or text) of the original image in four to five sentences. Extracted Text: {formatted_results}"
        },
        {
            "title": "Go to Appendix Index",
            "content": "21 Text Search Summarization Prompt Review all the provided text references to find the most relevant information to answer the question. Analyze the relevant facts from these references into single, concise summary of 1012 sentences that answers the question. Question: {original_question} References: {references_text}"
        },
        {
            "title": "F Additional Content",
            "content": "F."
        },
        {
            "title": "Introduction Figure",
            "content": "Figure F.1 An image of boat race."
        },
        {
            "title": "Go to Appendix Index",
            "content": ""
        },
        {
            "title": "G DeepMMSearchVQA Samples",
            "content": "Figure G.1 sample in DeepMMSearchVQA."
        },
        {
            "title": "Go to Appendix Index",
            "content": "23 Figure G.2 sample in DeepMMSearchVQA."
        },
        {
            "title": "Go to Appendix Index",
            "content": ""
        }
    ],
    "affiliations": [
        "Apple",
        "Johns Hopkins University"
    ]
}