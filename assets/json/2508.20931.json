{
    "paper_title": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench",
    "authors": [
        "Venkatesh Mishra",
        "Amir Saeidi",
        "Satyam Raj",
        "Mutsumi Nakamura",
        "Jayanth Srinivasa",
        "Gaowen Liu",
        "Ali Payani",
        "Chitta Baral"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in reasoning and planning capabilities of large language models (LLMs) have enabled their potential as autonomous agents capable of tool use in dynamic environments. However, in multi-turn conversational environments like $\\tau$-bench, these agents often struggle with consistent reasoning, adherence to domain-specific policies, and extracting correct information over a long horizon of tool-calls and conversation. To capture and mitigate these failures, we conduct a comprehensive manual analysis of the common errors occurring in the conversation trajectories. We then experiment with reformulations of inputs to the tool-calling agent for improvement in agent decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA) framework, which automatically reformulates user queries augmented with relevant domain rules and tool suggestions for the tool-calling agent to focus on. The results show that IRMA significantly outperforms ReAct, Function Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in overall pass^5 scores. These findings highlight the superior reliability and consistency of IRMA compared to other methods in dynamic environments."
        },
        {
            "title": "Start",
            "content": "How Can Input Reformulation Improve Tool Usage Accuracy in Complex Dynamic Environment? Study on τ -bench Venkatesh Mishra Amir Saeidi* Satyam Raj Mutsumi Nakamura Jayanth Srinivasa Gaowen Liu Ali Payani Chitta Baral Arizona State University Cisco Research {vmishr23, ssaeidi1, chitta}@asu.edu, {jasriniv, gaoliu, apayani}@cisco.com 5 2 0 2 8 2 ] . [ 1 1 3 9 0 2 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in reasoning and planning capabilities of large language models (LLMs) have enabled their potential as autonomous agents capable of tool use in dynamic environments. However, in multi-turn conversational environments like τ -bench, these agents often struggle with consistent reasoning, adherence to domain-specific policies, and extracting correct information over long horizon of tool-calls and conversation. To capture and mitigate these failures, we conduct comprehensive manual analysis of the common errors occurring in the conversation trajectories. We then experiment with reformulations of inputs to the tool-calling agent for improvement in agent decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA) framework, which automatically reformulates user queries augmented with relevant domain rules and tool suggestions for the tool-calling agent to focus on. The results show that IRMA significantly outperforms ReAct, Function Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in overall pass^5 scores. These findings highlight the superior reliability and consistency of IRMA compared to other methods in dynamic environments."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in Large Language Models (LLMs) (Annepaka and Pakray, 2025) have created the potential for them to be used as autonomous agents in complex real-world tasks like travel-booking, customer-support, and enterprise operations (Chen et al., 2024a; Wang et al., 2024; Singh et al., 2024; Yang et al., 2024). However, such complex tasks require the need of reasoning and planning capabilities beyond just language processing: they require the ability on behalf of these agents to be able to invoke suitable tools1 which * Equal Contribution 1The terms tool-use, tool-calling and function-calling are used interchangeably in this paper Figure 1: Comparison of the IRMA framework with other methods. The Input Reformulation framework takes the user query, domain policies, and available tools to generate structured input for the assistant agent. This augmented input enables the assistant to make more accurate decisions from the first turn. can complete tasks through logic implemented in computer programs leading to deterministic outcomes. Recent research (Yao et al., 2024; Lu et al., 2024; Yan et al., 2024), which benchmarks the simulation of such real-world problem-solving settings, shows that LLM-agents significantly falter in correctly solving these tasks and commit errors that range from generative hallucinations to failure to adhere to context and domain-specific policy violations by incorrect reasoning about actions over extended interactions. Figure 2: Overview of the tasks conducted for evaluating and improving tool-calling capabilities of language agents in τ -bench (Yao et al., 2024). Stage 1) involved human evaluators manually evaluating simulated conversation trajectories to find common failure modes of the language agents. Stage 2) employs human-in-the-loop approach to experiment with various prompt reformulations to improve agent correctness. Stage 3) automates this process through the IRMA framework, which leads to improved agent behavior. These shortcomings underscore the need for more fine-grained evaluations and methods that can diagnose and address the nuanced failure modes of LLM agents in complex, real-world interactions that employ natural language as form of communication. Thus, our main focus in this work is to find and mitigate the causes of why language agents fail to solve simulations of real-world conversational requests that require complex reasoning and relevant information processing according to the situation at hand. To this end, we utilize τ - bench (Yao et al., 2024) as an appropriate test-bed for such investigation as it emulates realistic airline and retail dialogues. We define the reasoning about actions of language agents as the ability to generate context-aware inference and decision-making tokens for selecting the next best action (a tool-call in this context). Additionally, we define and evaluate the planning capabilities of the agents through decision-making for tool-calling over multiple toolcalls in the correct sequential manner to complete goal. Inspired by recent work in context engineering (Mei et al., 2025), we propose three-pronged sequential approach. First, we develop comprehensive error classification that categorizes common reasoning and planning mistakes in multiturn tool-calling simulation. This taxonomy serves as diagnostic guideline to systematically identify and understand the causes of failures for LLM agents. Second, we manually experiment with input reformulations of the user requests to evaluate whether the correct prompt reformulations can guide the tool-calling agents towards correct decision-making through appropriate toolcalling/response to the user. Third, we automate this prompt-reformulation process by building multi-agent LLM framework (5.2), called InputReformulation Multi-Agent (IRMA), which further optimizes the input reformulation with augmentation of follow-up questions (5.1). Before the tool-calling agent invokes or responds to any tool output, our automated framework supplies targeted guidance that ensures strict adherence to domain-specific rules and well-placed follow-up questions to extract accurate information, thereby enhancing its reasoning and planning capabilities in dynamic environments. Our results show that the IRMA framework not only outperforms ReAct (Yao et al., 2023), Function Calling, and Self-Reflection (Renze and Guven, 2024) on pass@1, but also achieves 20% and 22.4% higher accuracy on Airline tasks compared to Gemini 1.5 Pro-FC and Claude 3.5 Haiku-FC, respectively. IRMA also demonstrates stronger reliability, with higher scores on pass^4 and pass^5 (Figure 4). In addition, IRMA solves tasks in fewer turns than competing methods, highlighting its efficiency (Figure 6). Lastly, IRMA shows greater robustness, with an increased performance gap on pass^5 after removing tasks affected by ground truth and instruction errors in the airline and retail domains. The main contributions of our work are: 1. Fine-grained causal-centric error classification of failure modes occurring in multi-turn tool-use conversational benchmark. 2. We propose the Input-Reformulation MultiAgent Framework (IRMA), verificationloop-free approach that improves functioncalling agents by reformulating prompts with structured and contextually relevant information. IRMA guides the agent to better follow domain policies by enriching its input with key constraints and tool-related context. 3. We perform an in-depth evaluation of IRMAs performance across reliability, consistency, and accuracy. Furthermore, our analysis of efficiency reveals that IRMA is able to solve tasks using fewer interaction turns than competing methods."
        },
        {
            "title": "2 Related Works",
            "content": "Tool-Integration for LLMs The ReAct framework, introduced by Yao et al. (2023), is one of the first approaches to explore the potential of Large Language Models (LLMs) as tool-using agents by integrating reasoning and acting within LLMs. Toolformer (Schick et al., 2023) presents finetuning approach to teach LLMs to invoke tool calls. ToolEVO (Chen et al., 2024b) and ToolLLM (Qin et al., 2023) employ tree search algorithms for integrating and evaluating tool-learning capabilities in LLMs. ToolACE (Liu et al., 2024b), AutoTools (Shi et al., 2025), and APIGen (Liu et al., 2024c) introduce automated frameworks designed to generate accurate, complex, and high-quality tool-learning data, with works like (Prabhakar et al., 2025; Yin et al., 2025) extending this to multi-turn interactive conversational settings. Tool-Use Benchmarks LLMs have been extensively evaluated on invoking external functions in both single-turn and interactive multi-turn conversational test beds. API-Bench (Patil et al., 2024) and API-Bank (Li et al., 2023) are two prominent benchmarks designed to evaluate the functioncalling capabilities of LLMs in single-turn scenarios. NESTful (Basu et al., 2024) focuses on evaluating LLMs ability to handle nested sequences of API calls. ToolQA-D (Chen et al., 2024b) gauges robustness in changing API specifications. τ -bench (Yao et al., 2024) and ToolSandbox (Lu et al., 2024) emulate realistic dialogues requiring policy-compliant tool use over multi-turn useragent interactions, where each step modifies an external environment. While these existing multiturn benchmarks evaluate the overall success of tool-calling agents, they lack fine-grained analysis of reasoning errors while following complex domain rulesa gap our work addresses through the construction of fine-grained error classification by evaluating τ -bench. Improving LLM Tool-Use Recent research has explored diverse strategies to enhance the tool-use capabilities of LLMs, focusing on API calling and web-environment interactionby leveraging techniques such as synthetic data generation, reinforcement learning, and memory augmentation. Liu et al. (2024c) introduces APIGen, an automated pipeline that generates high-quality, verifiable single-turn function-calling datasets, enabling small models to outperform GPT-4 on the BFCL (Patil et al., 2025). APIGen-MT (Prabhakar et al., 2025) extends the framework to show improvement in models on multi-turn scenarios through blueprintdriven simulation of humanagent dialogues. ReTool (Feng et al., 2025) integrates dynamic code execution within the reasoning process and training via outcome-driven RL, which significantly improves multi-step reasoning. Nemotron-ToolN1 (Zhang et al., 2025) uses an RL framework to teach precise tool invocation and explicit reasoning, achieving state-of-the-art on API-Bank (Li et al., 2023) and BFCL. ARTIST (Singh et al., 2025) integrates agentic reasoning with RL, enabling LLMs to decide autonomously when and how to call tools. Memento (Zhou et al., 2025) employs memoryaugmented, case-based planner for continual adaptation without retraining, achieving strong generalization on GAIA (Mialon et al., 2023) and DeepResearcher (Zheng et al., 2025) benchmarks. While these works mark shift toward adaptive, planningdriven, and memory-augmented LLM agents by leveraging training methods, our proposed IRMA framework explores tool-use improvement from the perspective of context engineering (Mei et al., 2025) principles."
        },
        {
            "title": "3 Problem Statement",
            "content": "To evaluate the tool-usage capabilities of current Large Language Models (LLMs), we adopt the benchmark provided by τ -bench (Yao et al., 2024). This benchmark is specifically designed to assess language agents in realistic, multi-turn interaction settings. τ -bench includes tasks from two domains: (1) Airline, comprising 50 tasks centered around flight reservation scenarios, and (2) Retail, containing 115 tasks focused on shopping and order management. In this setup, both the user and the customer-service assistant are simulated by LLMs, enabling controlled environment for analyzing interactive behavior. The customer-service agent is the language agent that generates the tokens signifying which tools are to be invoked, while following the specific domain policies (refer Appendix C) Each task is framed as Partially Observable Markov Decision Process (POMDP) (Details in Appendix A), where the assistant agent must generate appropriate function calls based on user inputs. These function calls are executed in an external environment, which then returns outputs that shape the ongoing dialogue. The interaction continues until the user ends the conversation, and the performance of the assistant is evaluated based on final rewards. These rewards reflect how closely the agents actions align with gold-standard trajectories and how well it fulfills the users goals. key challenge in τ -bench arises from the dynamic nature of user-agent interactions, where both user inputs and agent responses can vary across runs. This variability requires the agent to consistently execute correct action sequences, regardless of the conversational path. However, current results indicate that even state-of-the-art LLMs struggle to reliably complete these tasks as the number of trials increases. To address this limitation, we conduct root-cause analysis of common agent errors (4) and introduced IRMA, multi-agent framework (5) designed to improve agent reliability in this challenging setting."
        },
        {
            "title": "4 Error-Classification",
            "content": "To identify the failure modes of LLMs, human evaluators conducted experiments using GPT-4o (Hurst et al., 2024) as the base model for both the user and the assistant agent across all tasks in τ - bench (Yao et al., 2024). Both ReAct and functioncalling agent configurations were used to generate up to five trials per task in each domain. Evaluators manually reviewed the resulting multi-turn conversation trajectories from the retail and airline domains. While prior studies (Sun et al., 2024; Winston and Just, 2025; Cemri et al., 2025) have examined failures related to tool availability, definition errors, or tool set complexity, our analysis focuses specifically on the contextual reasoning limitations of LLMs in generating tool calls within dynamic, multi-turn interactions. Although τ -bench provides general taxonomy of failure types for the retail domain, our classification is more cause-oriented than effect-oriented. By framing errors in terms of their underlying causes, we can more effectively inform the design of targeted interventions, such as retrieval-augmented memory to mitigate context retention issues or follow-up question generation (5.1) to reduce hallucinations from context drift. The following subsections (4.14.4) provide detailed breakdown of the identified error types. 4.1 User Instruction Hallucination User instruction errors occur when the LLMsimulated user deviates from the original task instruction, typically in the later stages of conversation. These errors highlight the limitations of LLMs in maintaining instruction fidelity over long contexts, especially when multiple follow-up turns introduce competing directives. Another contributing factor is context drift, where the model increasingly relies on recent inputs or high-probability continuations, leading it to overlook or forget the initial user intent. An Example illustrating this error is provided in Figure 8 in Appendix D."
        },
        {
            "title": "4.2 Agent Hallucination",
            "content": "Agent hallucination errors arise when the assistant agent generates incorrect or incomplete responses that fail to fully satisfy the users request. For example, the agent may neglect to process all items specified by the user or incorrectly fulfill request by selecting the wrong item or applying it to the wrong order. These errors reflect underlying challenges with LLM memory limitations (Shan et al., 2025) and the degradation of instruction-following abilities over long contexts (Liu et al., 2023). As prior context accumulates, excessive or outdated information can distort the models understanding, leading to hallucinated outputs and ultimately incorrect decisions (Zhang et al., 2024)."
        },
        {
            "title": "4.3 Domain Policy Violation",
            "content": "Domain policy violations occur when tool-calling agents make decisions that contradict the domainspecific constraints defined for task completion. For instance, in Retail task 19 (Figure 9), the agent attempts to exchange the users office chair and pet bed even when the order is no more in delivered status: prerequisite domain rule required to be satisfied for exchange. This leads to the agent violating the domain rule (see Figure 12): An order can only be exchanged if its status is delivered... Such violations may also arise when the user issues an invalid request, and the agent proceeds to fulfill it without adhering to the applicable domain rules. This error is caused due to similar reasons as mentioned in 4.1 and 4.2. 4.4 Contextual Misinterpretation Contextual misinterpretation errors occur when the tool-calling agent misunderstands the intent or nuance of the users request and generates function calls using inappropriate tools for the given context. For example, if user asks to return an item and receive different one in exchange, human familiar with the domain policies would recognize this as an exchange request. However, the LLM-based agent may misinterpret it as simple return, failing to grasp the full context and thereby invoking the wrong tool."
        },
        {
            "title": "5 Method",
            "content": "As outlined in the previous sections, complex dynamic environments such as τ -bench present reliability challenges. Specifically, the user simulator may hallucinate during interactions, generating questions that do not adhere to the provided instructions. In this study, we aim to improve the assistant agents tool-calling performance in τ -bench by enabling more accurate decision-making. Unlike prior approaches that monitor and correct agent actions through verification or reflection, our method focuses on enhancing the quality of the agents input before any action is taken. To achieve this, we first introduce novel prompting strategy: Follow-up Question Acting (FACT), designed to support decision-making in dynamic settings. We then present the Input Reformulation Multi-Agent (IRMA) framework that reformulates the agents input to guide more effective and context-aware decisions."
        },
        {
            "title": "5.1 FACT: Follow-up question ACTing",
            "content": "Although reasoning-based prompting techniques like ReAct outperform non-reasoning methods such as Act, they remain inefficient in dynamic environments. As shown in Figure 3, ReAct often calls tool prematurely, triggers an error, and only then asks clarifying questions, leading to longer conversations and increased interaction issues. To overcome this, we introduce Follow-up Question ACTing (FACT), prompting method that first gathers information through targeted questions before calling tool. Our results in Figure 6 show that Figure 3: FACT agent demonstrates superior user guidance, avoiding tool-call errors encountered by the ReAct Agent. FACT is more effective than ReAct and performs comparably to Function Calling. We refer readers to Appendix E.1. Another advantage of FACT is its ability to involve the user in the loop. When the user simulator hallucinates or provides misleading input, FACT detects the issue and hands off the conversation to human, ensuring more robust handling of unreliable inputs. In summary, FACT is more efficient, reliable, and consistent than other methods in dynamic environments. However, in long conversations, it may forget domain rules and tools due to system prompt limitations, leading to domain violations. To address this, we propose the InputReformulation Multi-Agent Framework (IRMA), which restructures the user prompt to retain key information like domain rules and relevant tool list within the assistants input."
        },
        {
            "title": "Framework",
            "content": "Our analysis reveals three key failure cases for assistant agents. First, in long conversations, the agent may forget the users initial request and respond only partially. Second, it may violate domain rules by forgetting constraints from lengthy policy lists. Third, tool selection becomes harder over time, especially when tools have similar names (e.g., \"search_direct_flight\" vs. \"search_onestep_flight\"), leading to incorrect choices. We hypothesize that combining user queries with crucial context, such as domain rules and relevant tools, can improve the assistant agents decisionmaking. To test this, we conducted human-in-theloop experiment with prompt engineers who reformulated queries using additional policy and tool information. In most cases, the agent successfully completed the tasks, motivating us to automate this input reformulation process. Based on this insight, we propose the Input Reformulation Multi-Agent Framework (IRMA). In contrast to prior methods that focus on post-hoc correction of the agents behavior, such as SelfReflection, PlanGen (Parmar et al., 2025), or other verification-based approaches, IRMA centers on enhancing the quality of the input provided to the assistant agent. This approach enhances decisionmaking at the input stagebefore any action is takenensuring more accurate and context-aware responses. The framework comprises three core modules: memorization, constraints, and tool suggestion. Memorization This module is independent of the language model and is responsible for storing the user queries throughout the interaction trajectory. It helps the agent retain awareness of the initial request and make decisions accordingly. The conversation history is maintained within <memory> tags. Constraints One of the main reasons the agent makes incorrect decisions is domain policy violation. key insight from the human-in-the-loop experiment was the positive impact of providing concise list of domain constraints to guide the assistant agents decisions. To address this challenge, we define dedicated agent that generates checklist of relevant domain constraints based on the user query. If the user query is response to follow-up question from the assistant, the agent is prompted to return None. The generated constraint list is stored within <constraints> tags to ensure the assistant agent receives structured and interpretable input prompt. Model Method τ -Retail τ -Airline Overall Qwen 2.5 32B Llama 3.1 70B DeepSeek v31 Phi-4 14B Open-Source Models ReAct ReAct ReAct ReAct 24.4 50.4 58.3 32.2 Close-Source Models Gemini 1.5 pro1 Claude 3.5 Haiku2 Claude 3.5 Sonnet2 gpt-4o gpt-4o gpt-4o gpt-4o (ours) FC FC FC FC ReAct SR IRMA 54.9 51.0 62.6 60.5 51.8 51.1 58.3 25.0 26.0 22.8 28.0 25.2 22.8 36.0 42.4 39.6 44.8 47. 24.7 38.2 40.6 30.1 40.1 36.9 49.3 51.4 45.7 47.9 51.8 Table 1: Performance of various open and closed-source models in Pass^1 for retail and airline domains in τ - bench across 5 runs. SR stands for the Self-Reflection agent. 1 indicates results from (Cognition, 2025); 2 indicates results from (Anthropic, 2024a) In summary, the IRMA framework aims to replicate the input reframing performed by researchers during the human-in-the-loop experiment. Unlike other techniques such as verification, selfreflection, or agentic verification methods, IRMA functions in loop-free manner and focuses on strengthening the input by reformulating the user query. This approach not only improves accuracy but also offers better cost-effectiveness compared to alternative methods. In the next section, we provide comparative analysis of IRMA against existing techniques."
        },
        {
            "title": "6.1 Experimental Setup",
            "content": "We present the baseline models and comparison methods, followed by an analysis of the IRMA framework using various evaluation metrics and ablation studies (refer Appendix G) to assess the impact of its individual components on τ -bench performance. Tool Suggestion Although the number of available tools is limited, the assistant agent sometimes struggles to select the most relevant tool for given user query. In some cases, after encountering an error or receiving an empty output, the agent may lose track of other parts of the users request. To mitigate this, we introduce Tool Selector agent that generates short list of tools most relevant to the user query, along with one-line explanation for each suggestion. This list is stored within <tool_suggested> tags to help the assistant agent focus on selecting the most appropriate tool. Models and Methods We evaluated IRMA against range of open-source and closed-source language models. The open-source models include Qwen2.5-32B (Qwen et al., 2025), LLaMA-3.170B (Grattafiori et al., 2024), DeepSeek-v3 (Liu et al., 2024a), and Phi-4-14B (Abdin et al., 2024), while the closed-source models comprise Claude 3.5 (Anthropic, 2024b), Gemini 1.5 (Team et al., 2024), and GPT-4o (Hurst et al., 2024). In addition, we compared IRMA with three widely adopted prompting strategies: (1) ReAct, reasoningbased prompting technique; (2) Function CallFigure 4: Comparison of IRMA and other techniques across five runs with varying values of K. The figure shows significant performance difference between IRMA and other methods on pass^5. Note that all methods use GPT-4o as the base model. See Appendix for more details. ing, designed specifically to enhance models tool-calling capability; and (3) Self-Reflection, method aimed at improving tool-use performance by addressing errors in the agents actions. Evaluation To evaluate performance, we use the pass^k metrics, which measure the reliability and consistency of models across different prompting strategies. The pass^k metric (pronounced \"pass hat k\") is defined as the probability that all of the independently sampled outputs successfully complete the task, averaged across all tasks. Specifically, if task is run for independent trials and of those are successful (i.e., have correct result with reward = 1), an unbiased estimate of pass ^k can be computed using the following formula: pass^k = Etask (cid:19)(cid:21) (cid:20)(cid:18)c (cid:19)(cid:30)(cid:18)n This metric provides insight into how likely model is to succeed given multiple attempts, capturing both reliability and diversity in its outputs."
        },
        {
            "title": "6.2 Experimental Results",
            "content": "As outlined in the τ -bench, in real-world scenariosreliability and consistency are often more critical than the average success rate (measured by pass@1). We argue that an ideal agentic method should exhibit three key properties: (1) Accuracy, (2) Reliability, and (3) Consistency. Accordingly, we begin by comparing results using pass@1 to assess accuracy, and then evaluate the performance of state-of-the-art methods using pass^k to measure reliability and consistency. IRMA outperforms other state-of-the-art methods in tool calling. We conducted evaluations of multiple methodsFunction Calling (FC), ReAct, and Self-Reflectioneach executed over five trials. These experiments were performed using the GPT4o model. The results, presented in Table 1, show that the IRMA framework outperforms ReAct, SelfReflection, and FC by 6.1%, 3.9%, and 0.4%, respectively, in overall pass@1 score. Additionally, in the airline tasks, which represent the most challenging scenarios within the dynamic environment, IRMA on GPT-4o achieves improvements of 20%, 22.4%, and 9.2% compared to Gemini 1.5 ProFC, Claude 3.5 Haiku-FC, and Claude 3.5 SonnetFC, respectively. These findings highlight IRMAs strong accuracy in real-world tasks and demonstrate its effectiveness over existing methods. Figure 5: Error statistics across Airline and Retail tasks. GT: Ground Truth errors; UI: User Instruction errors. IRMA is more reliable and consistent than other methods in dynamic settings. The results in Table 1 show that the performance of IRMA on retail pass^1 is slightly lower than that of GPT-4o-FC. For this reason, we further explored the perforIRMA solves tasks more efficiently and effectively, using fewer turns than others. One of the primary reasons assistant agents make incorrect decisions in the final turns is the length of the conversation, which often causes them to forget important rules and instructions. In an ideal scenario, an assistant should resolve the users query with the fewest but most effective actions. To investigate this aspect, we analyzed the distribution of turns in successful task completions by IRMA, ReAct, FC, and Self-Reflection, as shown in Figure 6. The results show that, in retail tasks, IRMA completes tasks with 7.9 points fewer turns than Self-Reflection and 3.1 points fewer than FC. In airline tasks, IRMA requires 8.3 fewer turns than Self-Reflection, 1.1 fewer than FC, and 3.3 fewer than ReAct. These results demonstrate IRMAs superior efficiency compared to other state-of-the-art methods. Input Reformulation framework vs SelfReflection The central concept of IRMA is to reformulate the agents input under the assumption that supplying sufficient and well-structured information enables the agent to act more reliably and consistently in real-world scenarios. To evaluate this, we implemented the Self-Reflection method (Appendix F), which analyzes the agents previous actions and extracts relevant information from domain rules to guide future decisions (see section E.1 for implementation details). As shown in Figure 4, IRMA outperforms Self-Reflection in both airline and retail tasks, achieving 3.9% higher overall score in pass@1. More notably, IRMA exceeds Self-Reflection by 19.1% in pass^5, highlighting its superior reliability in real-world environment. In summary, while ReAct and Self-Reflection perform well in certain settings, they fall short in complex, dynamic environments like τ -bench. Role-play methods, including verification techniques, are also inefficient, as real-world scenarios require assistant agents to act based on limited information, with each action affecting the environment. Although Function Calling was designed for tool use, our results show it lacks reliability in decision-making and offers limited controllability, even in GPT-4o with tailored system prompts. Combining FACT with GPT-4o-FC led to 12% performance drop, highlighting the need for more robust approaches. In contrast, IRMA Figure 6: Comparison of IRMA and other methods based on the number of turns in successful tasks in the Airline and Retail domains. mance of other methods using pass^k to evaluate their reliability and consistency. The results in Figure 6 show that IRMA, compared with ReAct and FC on GPT-4o, is much more reliable and consistent, outperforming ReAct and FC by 16.1% and 12.6%, respectively, in overall scores on pass^5. IRMA is more robust on tasks with GT and UI errors. As explained in the previous sections, τ -bench suffers from two major issues: (1) Ground Truth (GT) errors and (2) User Instruction (UI) errors. Figure 5 shows the distribution of these errors across the airline and retail tasks. We progressively removed tasks affected by these problems, and the results revealed that the performance of all three methods improved, with IRMA showing slightly greater gains compared to the others. We hypothesize that IRMA is more robust to hallucination-related issues. Specifically, in tasks with GT errors, IRMA tends to avoid incorrect tool calls or invalid actions and instead produces safe and accurate responses. key observation is the change in performance difference between IRMA and FC on pass^5. Before removing tasks with GT and IRMA outperformed FC by 10%. UI errors, However, after removing these problematic tasks, the performance gap widened to 16.1% on average. Similar patterns were observed for other methods as well, reinforcing the claim that IRMA is more robust and less sensitive to noisy supervision and ambiguous instructions compared to existing techniques. consistently delivers higher accuracy, reliability, and consistency in dynamic environments like τ -bench."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we investigate the limitations of stateof-the-art LLM-based tool-calling agents in complex, multi-turn environments, focusing on the retail and airline domains of τ -bench. Through detailed analysis of conversation trajectories, we identify four major failure modes: user instruction hallucination, agent hallucination, domain-policy violations, and contextual misinterpretation, all of which stem from limitations in memory retention, contextual reasoning, and adherence to domain constraints across extended interactions. To address these challenges, we propose the Input Reformulation Multi-Agent (IRMA) framework, designed to enhance the structure of the assistant agents input. Our results show that IRMA not only outperforms other methods in pass^1 but also demonstrates significantly higher reliability, achieving an overall score of 43% pass^5 in τ -bench. Moreover, by leveraging the FACT agent, IRMA exhibits greater efficiency in task completion. In conclusion, IRMA shows robust and consistent behavior in the unreliable and dynamic environment of τ -bench, highlighting its effectiveness in real-world tool-use scenarios."
        },
        {
            "title": "Limitations",
            "content": "Although the Input Reformulation Multi-Agent (IRMA) framework demonstrated superior performance on τ -bench, several limitations remain. As shown in Figure 4, while IRMA exhibits greater reliability compared to other methods, its performance on pass^5 still hovers around 43%. This indicates that there is still considerable room for improving the reliability of tool-using agents in real-world scenarios. Another limitation of this work is that our experiments and analysis are restricted to the τ -bench benchmark. It would be valuable to evaluate IRMA across broader range of real-world environments to assess its generalizability. Moreover, our observations suggest that beyond the error taxonomy we proposed, τ -bench itself suffers from issues related to unfair reward modeling. Building truly dynamic and reliable evaluation environmentespecially one that can control for the correctness of user instructionswould have significant impact on the field. Such an environment would enable more rigorous development and evaluation of agentic frameworks research into robust, and encourage further real-world agent behavior. Ultimately, we believe this work contributes meaningfully to the research community and provides strong foundation for developing more reliable and consistent agentic methods for dynamic environments."
        },
        {
            "title": "Ethics Statement",
            "content": "We have utilized AI assistants, specifically Grammarly and ChatGPT, to correct grammatical errors and rephrase sentences."
        },
        {
            "title": "Acknowledgement",
            "content": "We thank the anonymous reviewers for their constructive suggestions. We extend our gratitude to the Research Computing (RC), and Enterprise Technology at ASU for providing computing resources, and access to the GPT API version for experiments. This work was in part supported by gift award from Cisco Research."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. 2024. Phi-4 technical report. arXiv preprint arXiv:2412.08905. Yashwanth Annepaka and Prasenjit Pakray. 2025. Large language models: survey of their development, capabilities, and applications. Knowledge and Information Systems, 67:29673022. Anthropic. 2024a. Claude 3.5 models and comhttps://www.anthropic.com/news/ puter use. 3-5-models-and-computer-use. Accessed: 202505-20. Anthropic. 2024b. Claude 3.5 sonnet. https://www. 4 anthropic.com/news/claude-3-5-sonnet. min read. Kinjal Basu, Ibrahim Abdelaziz, Kiran Kate, Mayank Agarwal, Maxwell Crouse, Yara Rizk, Kelsey Bradford, Asim Munawar, Sadhana Kumaravel, Saurabh Goyal, et al. 2024. Nestful: benchmark for evaluating llms on nested sequences of api calls. arXiv preprint arXiv:2409.03797. Mert Cemri, Melissa Pan, Shuyi Yang, Lakshya Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, et al. 2025. Why do multi-agent llm systems fail? arXiv preprint arXiv:2503.13657. Aili Chen, Xuyang Ge, Ziquan Fu, Yanghua Xiao, and Jiangjie Chen. 2024a. Travelagent: An ai assistant for personalized travel planning. arXiv preprint arXiv:2409.08069. Guoxin Chen, Zhong Zhang, Xin Cong, Fangda Guo, Yesai Wu, Yankai Lin, Wenzheng Feng, and Yasheng Wang. 2024b. Learning evolving tools for large language models. arXiv preprint arXiv:2410.06617. Scaled Cognition. 2025. Apt-1: Adaptive prompt tuning for llms. https://www.scaledcognition. com/blog/apt-1. Accessed: 2025-05-19. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. 2025. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-bank: comprehensive benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024a. arXiv preprint Deepseek-v3 technical report. arXiv:2412.19437. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172. Lost Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, et al. 2024b. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920. Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh RN, et al. 2024c. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. Advances in Neural Information Processing Systems, 37:5446354482. stateful, conversational, interactive evaluation benchmark for llm tool use capabilities. arXiv preprint arXiv:2408.04682. Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, et al. 2025. survey of context engineering for large language models. arXiv preprint arXiv:2507.13334. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations. Mihir Parmar, Xin Liu, Palash Goyal, Yanfei Chen, Long Le, Swaroop Mishra, Hossein Mobahi, Jindong Gu, Zifeng Wang, Hootan Nakhost, et al. 2025. Plangen: multi-agent framework for generating planning and reasoning trajectories for complex problem solving. arXiv preprint arXiv:2502.16111. Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. 2025. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. 2024. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 37:126544126565. Akshara Prabhakar, Zuxin Liu, Weiran Yao, Jianguo Zhang, Ming Zhu, Shiyu Wang, Zhiwei Liu, Tulika Awalgaonkar, Haolin Chen, Thai Hoang, et al. 2025. Apigen-mt: Agentic pipeline for multi-turn data generation via simulated agent-human interplay. arXiv preprint arXiv:2504.03601. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, et al. 2024. Toolsandbox: Matthew Renze and Erhan Guven. 2024. Self-reflection in llm agents: Effects on problem-solving performance. arXiv preprint arXiv:2405.06682. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551. Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, and Yong Wu. 2025. Cognitive memory in large language models. arXiv preprint arXiv:2504.02441. Zhengliang Shi, Shen Gao, Lingyong Yan, Yue Feng, Xiuyi Chen, Zhumin Chen, Dawei Yin, Suzan Verberne, and Zhaochun Ren. 2025. Tool learning in the wild: Empowering language models as automatic tool agents. Preprint, arXiv:2405.16533. Harmanpreet Singh, Nikhil Verma, Yixiao Wang, Manasa Bharadwaj, Homa Fashandi, Kevin Ferreira, and Chul Lee. 2024. Personal large language model agents: case study on tailored travel planning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 486514, Miami, Florida, US. Association for Computational Linguistics. Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. 2025. Agentic reasoning and tool integration for llms via reinforcement learning. arXiv preprint arXiv:2505.01441. Jimin Sun, So Yeon Min, Yingshan Chang, and Yonatan Bisk. 2024. Tools fail: Detecting silent errors in faulty tools. arXiv preprint arXiv:2406.19228. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345. Cailin Winston and René Just. 2025. taxonomy of failures in tool-augmented llms. In Proceedings of the International Conference on Automation of Software Test (AST). Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. Berkeley function calling leaderboard. Yingxuan Yang, Qiuying Peng, Jun Wang, and Weinan Zhang. 2024. Multi-llm-agent systems: Techniques and business perspectives. arXiv preprint arXiv:2411.14033. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik τ -bench: benchmark for Narasimhan. 2024. tool-agent-user interaction in real-world domains. Preprint, arXiv:2406.12045. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Fan Yin, Zifeng Wang, Hsu, Jun Yan, Ke Jiang, Yanfei Chen, Jindong Gu, Long Le, Kai-Wei Chang, ChenYu Lee, et al. 2025. Magnet: Multi-turn tool-use data synthesis and distillation via graph translation. arXiv preprint arXiv:2503.07826. Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, and Guilin Liu. 2025. Nemotron-research-tooln1: Exploring tool-using language models with reinforced reasoning. arXiv preprint arXiv:2505.00024. Yuxiang Zhang, Jing Chen, Junjie Wang, Yaxin Liu, Cheng Yang, Chufan Shi, Xinyu Zhu, Zihao Lin, Hanwen Wan, Yujiu Yang, Tetsuya Sakai, Tian Feng, and Hayato Yamana. 2024. ToolBeHonest: multilevel hallucination diagnostic benchmark for toolaugmented large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1138811422, Miami, Florida, USA. Association for Computational Linguistics. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160. Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, and Jun Wang. 2025. Memento: Fine-tuning llm agents without fine-tuning llms. Preprint, arXiv:2508.16153. Task Definition in τ -bench Following Yao et al. (2024), each task in τ -bench is modelled as partially observable Markov decision process (POMDP) = S, A, O, , R, U. We briefly restate every component and specify how they instantiate in the retail and airline domains. State space : The hidden state is factored into = Sdb Suser where Sdb is snapshot of the underlying database (orders, flights, balances etc.) and Suser stores the latent user context (identity, revealed preferences, dialogue progress). Action space : The agent can either (i) invoke an API tool that queries or mutates the database (Adb) or (ii) send free-form respond message to the user (Auser). Thus = Adb Auser. Observation space : After each action the environment returns either JSON payload/error from the database (Odb) or the next user utterance produced by an LLM simulator (Ouser), yielding = Odb Ouser. Transition function : : is deterministic for database tools (state is updated, observation is the tool output) and stochastic for respond, which calls the user simulator to sample the next utterance and potentially reveal more of the instruction. Reward function : At dialogue termination we compare the execution log to gold reference: (1) hashes of mutable tables must match, (2) all mandatory natural-language outputs must appear If both hold, = 1, in the agents responses. otherwise 0. Method Pass^1 Pass^2 Pass^3 Pass^4 Pass^ ReAct IRMA FC Self reflection 0.4941 0.5706 0.5529 0.5167 0.3735 0.4912 0.4353 0.3750 0.3206 0.4471 0.3794 0.3139 0.2882 0.4235 0.3353 0.2778 0.2647 0.4118 0.2941 0. Table 3: Results of different methods on all Airline tasks, excluding the tasks with ground-truth errors. Method Pass^1 Pass^2 Pass^3 Pass^4 Pass^5 ReAct IRMA FC Self reflection 0.5226 0.6258 0.6000 0.5556 0.4065 0.5387 0.4774 0. 0.3516 0.4903 0.4161 0.3528 0.3161 0.4645 0.3677 0.3111 0.2903 0.4516 0.3226 0.2778 Table 4: Results of different methods on all Airline tasks, excluding the tasks with ground-truth errors and user instruction errors. B.2 Retail results Tables 5-7 represent the results of the baseline and our implemented methods in the Retail domain. Instruction space : Each task provides fixed natural-language instruction describing the user goal, persona and constraints. The user simulator may disclose incrementally; therefore the agent must act under partial observability. Method Pass^1 Pass^2 Pass^3 Pass^4 Pass^5 ReAct IRMA FC Self-reflection 0.5182 0.5826 0.6052 0.5113 0.3704 0.4783 0.4522 0. 0.2999 0.4261 0.3643 0.3017 0.2573 0.3948 0.3043 0.2383 0.2260 0.3739 0.2609 0.1826 Table 5: Results of different methods on all Retail tasks. This causal decomposition lets us pinpoint failure modes such as wrong tool arguments (action-level), policy violations (transition-level), or hallucinated user messages (observation-level). Pass^k Results B.1 Airline results Method Pass^1 Pass^2 Pass^3 Pass^4 Pass^5 ReAct IRMA FC self-reflection 0.5304 0.5982 0.6164 0.5250 0.3804 0.4911 0.4616 0. 0.3080 0.4375 0.3732 0.3098 0.2643 0.4054 0.3125 0.2446 0.2321 0.3839 0.2679 0.1875 Table 6: Results of different methods on all Retail tasks, excluding the tasks with ground-truth errors. Tables 2-4 refer to pass^k results of the baselines and our implemented methods. As explained in 6.2, IRMA performs better when there are no ground-truth or user instruction errors. All results are obtained using GPT-4o as the LLM in the agent frameworks. Method Pass^1 Pass^2 Pass^3 Pass^4 Pass^5 ReAct ours FC self reflection 0.5562 0.6248 0.6381 0.5562 0.4048 0.5171 0.4838 0.4171 0.3200 0.4629 0.3933 0.3305 0.2818 0.4305 0.3314 0. 0.2476 0.4095 0.2857 0.2000 Table 7: Results of different methods on all Retail tasks, excluding the tasks with ground-truth errors and user instruction errors. Method Pass^1 Pass^2 Pass^3 Pass^4 Pass^5 ReAct IRMA FC Self-reflection 0.396 0.452 0.424 0. 0.2779 0.3680 0.3120 0.3140 0.2279 0.3280 0.2660 0.2560 0.200 0.308 0.232 0.224 0.180 0.300 0.200 0.200 Table 2: Results on all Airline tasks."
        },
        {
            "title": "C Domain Policies",
            "content": "Figures 12 and 13 are the domain policies present for the retail and airline domains in the τ -bench. These rules are injected verbatim as the system prompt to every tool-calling agent. An agent that violates any of themeven if it successfully fulfills the users requestreceives zero reward, so strict compliance is essential. The Tool-Calling Agent has to strictly operate under the constraints of these policies to correctly solve user requests."
        },
        {
            "title": "D Failure Example",
            "content": "Figures 8 and 9 show an example of errors occurring in the conversational trajectories simulating task 19 (retail) of the user-agent interactions as enumerated in subsections of 4. Error 1 in Figure 8 shows an example of User Instruction Hallucination occurring in the very first user turn. Error 2 in Figure 9 shows an example of Domain Policy Violation error. The user instruction for Task 19 is provided in Figure 7. This instruction represents the original user instruction provided to the LLM-simulated user. It is the script the user has to follow to provide requests to the agent."
        },
        {
            "title": "E Input Reformulation Multi Agent",
            "content": "framework E.1 Follow-up question ACTing (FACT)"
        },
        {
            "title": "Agent",
            "content": "The primary difference between FACT and other prompting techniques lies in the instruction section of the system prompt (refer to Figure 10). Self-Reflection Framework To check the effectiveness of self-reflection as an alternative against the baselines and IRMA, we implement multi-agent LLM self-reflection pipeline, consisting of retriever LLM agent and verifier LLM agent. Contrary to input reformulation, where the prompt provided in the user query is reformulated, the self-reflection agent pauses the tool-calling LLM agent before the execution environment executes the tool-call. All of the previous user queries are provided as input to the retriever agent to extract the relevant domain policy rules based on the user intent reflected from the user requests in the conversation. The retrieved rules are provided to the verifier agent along with the toolcalling agents planned tool call. The verifier agent then verifies whether the tool-call is correct by providing reflective justification based on determining whether any domain rule has been violated or not. The overall pipeline of the self-reflection agent is provided in Figure 11. The reflective feedback loop from verifier is set to be one-time loop as the execution of the loop is very latency-heavy and invoking it multiple times might not be ideal in real-world customer-agent scenarios. IRMA Ablations () Pass^1 Pass^2 Pass^3 Pass^4 Pass^5 M + M+ + + + 0.416 0.416 0.424 0.428 0.448 0.38 0.452 0.27 0.276 0.268 0.31 0.294 0.264 0.368 0.212 0.206 0.19 0.26 0.214 0.212 0.328 0.18 0.164 0.14 0.236 0.16 0.18 0.308 0.16 0.14 0.1 0.22 0.12 0.16 0.3 Table 8: Results of IRMA component ablations on Airline tasks. Bold scores represent the best scores. Italic scores represent the second-best scores. represents the Memory sub-agent of IRMA, represents the Constraint sub-agent, and represents the Tool-Suggestor."
        },
        {
            "title": "G Ablation Studies on IRMA",
            "content": "We ablate the three IRMA modulesMemory (M), Constraint (C), and Tool (T)and evaluate them on the airline subset. Across all Pass^k metrics, the full configuration (M+C+T) achieves the best performance, indicating strong complementarity among modules. Among the ablations, M+C is consistently the strongest, ranking second overall in terms of better reliability at higher values of k. This pattern suggests that instruction retention (M) and policy/constraint adherence (C) account for most gains in long-horizon reasoning and plan stability, while tool disambiguation (T) provides the additional performance improvement needed to reach state-of-the-art performance. In sum, each module targets distinct failure modecarryover of instructions (M), rule compliance (C), and tool selection/parameterization (T)but their integration is necessary for robust behavior in dynamic tool-use settings. The results of the ablation experiments are provided in Table 8. Method () Pass^1 Pass^2 Pass^3 Pass^4 Pass^5 ReAct FC IRMA 0.188 0.236 0.208 0.09 0.1179 0.144 0.052 0.062 0. 0.032 0.036 0.08 0.02 0.02 0.06 Table 9: Performance of IRMA in the Airline domain using GPT-4o-mini as the LLM-backbone model in all the modules of IRMA. Bold scores represent the best scores. We also test IRMA using GPT-4o-mini as the LLM backbone, to test the effect of IRMA with smaller LLM. As shown in Table 9, the results indicate that the benefits of IRMA are not tied to particular larger models reasoning strength and transfer to smaller function-calling backbones. Conceptually, IRMA does not replace parametric reasoning; rather, its structured inputsmemory, constraints, and tool suggestionsamplify models ability to retain instructions, follow domain rules, and disambiguate tools across long contexts, yielding more stable performance under multiple attempts. Method () Pass^1 Pass^2 Pass^3 Pass^4 Pass^ IRMA (R) IRMA (F) 0.4 0.452 0.302 0.368 0.256 0.328 0.232 0.308 0.22 0. Table 10: IRMA comparison with Follow-up questioning disabled (IRMA (R)) and enabled (IRMA (F)) in the Airline domain with GPT-4o backbone. To isolate the effect of follow-up questioning, we create controlled variant that swaps IRMAs system prompt with the standard ReAct prompt while keeping all information-consolidating componentsmemory, constraint extraction, and tool suggestionsunchanged. This IRMA + ReActprompt baseline places both agents on identical inputs and differs only in the instructions provided to the final tool-calling agent. As reported in Table 10, IRMA consistently outperforms this baseline across Pass^k metrics, indicating that targeted follow-up questioning provides gains beyond ReAct. Figure 7: User instruction of Task 19 (Retail) in τ -bench. Figure 8: Part 1 of the conversation trajectory simulation of Task 19 (Retail). Figure 9: Part 2 of the conversation trajectory simulation of Task 19 (Retail). (a) Part 1 of the FACT system prompt Figure 10: FACT System prompt. (b) Part 2 of the FACT system prompt Figure 10: FACT System prompt. Figure 11: Overview of the pipeline showcasing the working of the self-reflection framework. The italicized text inside dotted green dotted text boxes refer to prompt gists provide to the Retriever and Verifier LLM Agent. The self-reflection only activates when the assistant generates the tokens to invoke tool call. (a) Part 1 of the Retail Domain Rules Figure 12: Domain Policies of the Retail Domain (b) Domain Policies of the Retail Domain Figure 12: Domain Policies of the Retail Domain (a) Part 1 of the Airline Domain Rules Figure 13: Domain Policies of the Airline Domain. (b) Part 2 of the Airline Domain Rules Figure 13: Domain Policies of the Airline Domain"
        }
    ],
    "affiliations": [
        "Arizona State University",
        "Cisco Research"
    ]
}