{
    "paper_title": "AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation",
    "authors": [
        "Moayed Haji-Ali",
        "Willi Menapace",
        "Aliaksandr Siarohin",
        "Ivan Skorokhodov",
        "Alper Canberk",
        "Kwot Sin Lee",
        "Vicente Ordonez",
        "Sergey Tulyakov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video generation that leverages the activations of frozen video and audio diffusion models for temporally-aligned cross-modal conditioning. The key to our framework is a Fusion Block that enables bidirectional information exchange between our backbone video and audio diffusion models through a temporally-aligned self attention operation. Unlike prior work that uses feature extractors pretrained for other tasks for the conditioning signal, AV-Link can directly leverage features obtained by the complementary modality in a single framework i.e. video features to generate audio, or audio features to generate video. We extensively evaluate our design choices and demonstrate the ability of our method to achieve synchronized and high-quality audiovisual content, showcasing its potential for applications in immersive media generation. Project Page: snap-research.github.io/AVLink/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 1 ] . [ 1 1 9 1 5 1 . 2 1 4 2 : r AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation Moayed Haji-Ali1,2, Alper Canberk2 Willi Menapace2 Kwot Sin Lee2 Aliaksandr Siarohin2 Vicente Ordonez1 Ivan Skorokhodov2 Sergey Tulyakov2 1Rice University 2Snap Inc Project Webpage: https://snap-research.github.io/AVLink Figure 1. Compared to current Video-to-Audio and Audio-to-Video methods, AV-Link provides unified framework for these two tasks. Rather than relying on feature extractors pretrained for other tasks (e.g. CLIP [63], CLAP [16]), we directly leverage the activations from pretrained frozen Flow Matching models using Fusion Block to achieve precise time alignment between modalities. Our approach offers competitive semantic alignment and improved temporal alignment in self-contained framework for both modalities."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction We propose AV-Link, unified framework for Video-toAudio and Audio-to-Video generation that leverages the activations of frozen video and audio diffusion models for temporally-aligned cross-modal conditioning. The key to our framework is Fusion Block that enables bidirectional information exchange between our backbone video and audio diffusion models through temporally-aligned self attention operation. Unlike prior work that uses feature extractors pretrained for other tasks for the conditioning signal, AV-Link can directly leverage features obtained by the complementary modality in single framework i.e. video features to generate audio, or audio features to generate video. We extensively evaluate our design choices and demonstrate the ability of our method to achieve synchronized and high-quality audiovisual content, showcasing its potential for applications in immersive media generation. *Work mainly done during an internship at Snap Inc. Generative models are becoming proficient at synthesizing high-quality video [6, 33, 53, 58, 70, 78] and audio [19, 23, 48, 49, 79], producing content that is increasingly realistic and contextually coherent. As result, recent work has tackled the challenging task of Video-toAudio (V2A) generation where the goal is to add sound that is both semantically and temporally aligned to video content [52, 62, 85, 97]. Current V2A models [52, 62, 85, 97] typically rely on pretrained visual feature extractors such as CLIP [63], MetaCLIP [86], ImageBind [21], and CAVP [52] to condition audio generation and create soundscapes that are semantically aligned with the visual frames. An even more challenging task is Audio-to-Video (A2V) where the goal is to generate video that could plausibly visualize the sounds in given input audio. Although this task is even more ambiguous, some recent work has demonstrated encouraging results [41, 93, 96]. similar to V2A, these methods have explored conditioning video generators on audio features extracted with pretrained audio models such as ImageBind [21], CLAP [16] and BEATs [10]. While the use of pretrained feature extractors has been important in achieving semantic alignment, these features do not exhibit precise temporal alignment capabilities, fundamental aspect of cross-modal audio-video generation. Moreover, since these models require feature extractors pretrained on complementary data for each modality, it forces separate treatment for V2A and A2V tasks. To address these limitations, recent work generates audio and video jointly within single model [35, 67] to unify the multimodal generative task. Although promising in concept, these models have faced considerable challenges and present performance gap with respect to V2A and A2V which we show can be reconducted to the absence of informative multimodal features in early diffusion timesteps. In this work, we tackle cross-modal audiovisual generation within unified architecture named AV-Link that is capable of semantically and temporally aligned generation for either V2A or A2V as shown in Fig. 1. Starting from two well-trained flow models for audio and video generation, we use their activations as conditioning signals for either V2A or A2V synthesis, bypassing the need for pretrained feature extractors. Such activations contain rich semantics [51], but most importantly they are also temporally-aligned as generated media (e.g. audio and video) originates from such activations. This approach is supported by prior findings which showed that diffusion activations are semantically rich spatially aligned, making them useful for downstream pixelaligned tasks such as semantic keypoint matching [26, 51] segmentation [91] and depth estimation [34]. To make use of such features, we freeze the pretrained generators and introduce Fusion Block that enables bidirectional information exchange between audio and video modalities by connecting activations from the pretrained generators through temporally-aligned self-attention operation and conditioning both backbones through symmetric feature reinjection (see Fig. 2). This approach offers superior semantic and temporal alignment compared to pretrained features alone. Another benefit is that it fully leverages the capabilities of the pretrained audio and video networks and only adds set of Fusion Blocks, which is relatively small. This compact design simplifies deployment for applications requiring comprehensive functionality: video, audio, video-to-audio, and audio-to-video generation. To foster future work, we present insights into diffusion features and cross-modal architectures. First, we inspect activations to identify noise levels and model blocks that yield the most effective conditioning features. Next, we explore different mechanisms for injecting such features into the generators. Finally, we show that diffusion features outperform specialized pretrained feature extractors. In the V2A task, when evaluated against the state-of-theart on VGGSounds [9] our method shows the best overall performance and drastically improves temporal alignment as measured by Onset ACC [15] by up to 36.5% points over the best baseline. User studies show strong preference for our method which is preferred over Movie Gen Audio [62] 56.8% of the times with respect to semantic alignment, and 63.6% with regards to temporal alignment. In the V2A task, our method strongly surpasses TempoTokens [77] in both video quality and audio-video alignment, with user studies showing an overwhelming preference for our method. To summarize, we introduce the following contributions: AV-Link, unified framework to address Audio-to-Video and Video-to-Audio tasks. For the first time in this context, we propose the use of diffusion features to sidestep the need for specialized pretrained feature extractors and to improve temporal alignment. Fusion Block based on temporally-aligned self attention operation and symmetric feature reinjection mechanism to allow bidirectional information exchange between the frozen generators for each modality To foster future work in this direction, we present extensive ablations into diffusion feature selection strategies and mechanisms for cross-modal feature injection and show that such features outperform commonly used pretrained feature extractors. 2. Related Work Text-to-Audio Generation Text-to-audio generation methods [19, 23, 29, 49, 54, 88] aim to produce audio signals based on given text condition. Common approaches [23, 29] leverage latent diffusion models [25], where they represent audio as Mel-spectrograms, encoding them with 2D Variational Autoencoder (VAE) and using U-Net architecture for denoising. Successive works employ scalable transformer-based diffusion backbones to achieve superior quality. Make-an-Audio 2 [29] encodes the spectrogram with 1D VAE and uses transformer backbone. GenAU [23] introduced scalable dataset collection framework and transformer architecture to produce high-quality sound effects. Movie Gen [62] directly encodes waveforms with VAE, bypassing intermediate Mel-spectrogram representations to enable generation at higher resolution of 44kHz. Such approaches operate in text-conditioned setting [19, 29, 49], and showed the importance of textencoding in achieving better quality and text-audio alignment. Our work focuses on establishing unified framework for Video-to-Audio and Audio-to-Video leveraging diffusion model activations as the main conditioning signal, posing more complex challenge due to the need for precise temporal alignment with the conditioning modality. Video-to-Audio Generation The task of generating audio from silent videos recently emerged as difficult generative task [2, 7, 8, 11, 12, 22, 38, 40, 44, 46, 56, 57, 59, 2 65, 73, 74, 81, 89, 94] and gained relevance not only for its potential in simplifying movie production but also for the recent advent of high-quality video generation methods [3, 4, 6, 33, 43, 58, 62, 92, 99] which produce no audio output. Video feature representations play crucial role in achieving highly aligned generation and have been the focus of most recent work [30, 31, 52, 62, 66, 69, 80, 82, 84, 97]. SpecVQGAN [31] employs cross-modal transformer conditioned on ImageNet-pretrained visual backbones. Im2Wav [69] uses per-frame CLIP [63] features to condition audio generation. SonicVisionLM [84] bridges the visual and audio modalities through pretrained textual representation. V2A-Mapper [80] aligns the CLIP [63] and CLAP [16] to leverage pretrained text-to-audio model for V2A. Diff-Foley [52] and Frieren [82] address alignment using CAVP, an ad-hoc contrastive video-audio representation. FoleyCrafter [97] augments frozen audio generator with learnable semantic adapter and temporal controller. Movie Gen Audio [62] obtains strong video-audio alignment by conditioning on descriptive text caption and per-frame MetaCLIP [86] features. Despite such advancements, obtaining precise temporal alignment remains challenging. We show that activations extracted from diffusion backbones provide rich and temporally aligned representations that sidestep the need for ad-hoc extractors. Audio-to-Visual Generation Similar to the Video-toAudio task, recent work has focused on building suitable audio representations, paying special attention to time Lee et al. [42] employ Sound Inversion alignment. Encoder to map audio features to the StyleGAN latent space. Seeing-and-Hearing [85] propose video-audio aligner based on ImageBind [21]. TPoS [32] manipulates generated images conditioned on audio representations extracted through an ad-hoc temporally-aligned audio encoder. TempoTokens [93] aligns frozen video generator to the audio modality by employing features from finetuned BEATs encoder. AADif [41] leverages the CLAP [16] audio encoder and audio magnitude to condition generation. Recently, AVSyncD [96] encodes audio into temporally-dependent tokens using ImageBind [21]. We show that temporally-aligned Video-to-Audio and Audioto-Video generation can be achieved within the same framework by leveraging existing diffusion activations without the need for specialized feature representations. Joint Video-Audio Generation Recently, several works emerged that generate the audio and video modalities jointly. [24, 36, 45, 72, 75, 90] MM-Diffusion [67] proposes diffusion framework based on Coupled U-Net for simultaneously denoising the audio and video streams. Seeing-and-Hearing [85] employs ImageBind [21] to establish alignment between modalities. AVDiT [35] introduces multimodal DiT design and learns multiple conditional distributions over audio and video through mixture Figure 2. Design of the proposed Fusion Block connecting the frozen video and audio backbones. RoPE-based temporal alignment mechanism establishes correspondences between modalities that are leveraged by self attention. Video and audio features are symmetrically reinjected into the frozen generators. The block is regularly applied multiple times throughout the backbones. of noise levels, which enables the framework to perform range of tasks including joint audio-video generation, Video-to-Audio, and Audio-to-Video. Such approaches, however present lower Video-to-Audio performance than ad-hoc methods [62, 97], thus in this work, we focus on the conditional generation tasks. 3. Method Starting with pretrained and frozen Text-to-Audio (T2A) and Text-to-Video (T2V) generators, we aim to combine the two under unified architecture to produce either Video-toAudio (V2A) GV 2A or Audio-to-Video (A2V) GA2V generators. To achieve this, we propose AV-Link, symmetric architecture that benefits from the activations of one generator to condition the generator of the other modality. In Sec. 3.1, we describe the background on Flow Matching models. We then describe in Sec. 3.2, the architecture of our pretrained audio and video generators. Finally, in Sec. 3.3, we describe the architecture of the multimodal Fusion Blocks that connect the two generators. 3.1. Background We base our generative models on the Flow Matching framework [47, 50]. Flow Matching expresses generation 3 of data X1 pd as the progressive transformation of X0 following path connecting samples from the two distributions. In its simplest formulation [50], the path is instantiated as linear interpolation between the samples: Xt = tX1 + (1 t)X0, (1) and X0 pn = (0, I) originate from noise distribution. We can move along the path following the velocity vt = dXt dt = X1 X0 approximated by learnable minimizing: = Etpt,X1pd,X0pn (cid:13) (cid:13)G(Xt, t) vt (cid:13) 2 2, (cid:13) (2) where pt indicates training distribution over time t, which we instantiate as logit normal distribution [17]. At inference time, an ODE solver such as first-order Euler can be employed to produce samples X1 starting from Gaussian noise X0 using the models velocity estimates. 3.2. Base Models aDl and Dl , where Audio Model. Given an audio signal from audio dataset DA, we produce corresponding Mel-spectrogram and adopt 1D-VAE [23] to encode it into 1D sequence of latent. The latent audio representation modeling A1 thus has shape RT respectively indicate the number of tokens and their dimensionality produced by the autoencoder. The sequence is modeled by DiT [60] (see Fig. 2) consisting of stack of identical blocks containing self attention operation followed by an MLP. Information on the current timestep is injected through adaptive layernorm [61] and the attention operator is augmented with 1D-RoPE [71] to encode positional information. To condition on text prompts, we encode them using the T5 [64] encoder and insert cross attention layer attending to the text embeddings after each self attention operation [62]. The model contains 576M parameters and is trained to generate variable-length audio clips at 16kHz. We train it using the Flow Matching objective (Eq. (2)) over 100k iterations, with learning rate of 3e 4 and batch size of 1024, distributed across 8 A100 GPUs. During inference, pre-trained Vocoder [37] converts the generated Mel-spectrograms into waveforms. Video Model. Given an RGB video V1, we flatten it to shape RT indicates the total number of video pixels. We adopt DiT architecture symmetric to the one of the audio model, where 3D-RoPE embeddings [13] encode positional information. Full self attention is employed to capture spatial and temporal relationships. The model has 576M parameters and is trained to generate 5.16-second videos at resolution of 36 64 pixels at 6 frames-perseconds, using the Flow Matching objective in Eq. (2). Training is conducted for 250,000 iterations with learning rate of 0.0003 and batch size of 512 on 16 A100 GPUs. v3, where 3.3. Multimodal Fusion Block Cross-modal generation requires high-quality representation for the conditioning modality [52, 97]. In the context of V2A, obtaining high-quality video features has been the focus of recent works, which explored the usage of contrastive representations such as ImageBind [85], CLIP [97], MetaCLIP [62], or built ad hoc video representations [52, 97]. While such representations capture the overall video semantic, we observe that they lack precise audio-video temporal alignment, producing characteristic temporal misalignment (see Sec. 4.4). On the other hand, the video generator is capable of generating videos from scratch, implying that its activations contain semantically and temporally aligned information that can be leveraged for V2A. We thus propose fully symmetric framework for V2A and A2V that leverages activations from the frozen generator of the conditioning modality to achieve aligned crossmodal generation using no ad-hoc feature extractors. Fusion Blocks We address V2A and A2V by linking frozen audio and video generators GA, GV through the Fusion Block depicted in Fig. 2. Consider the output activations of audio and video DiT blocks xa RTaDa , xv RTvDv at diffusion timestep ta, tv, respectively and notice one that one of the two, depending on the task, is always derived from ground truth conditioning input. The Fusion Block produces video-aware audio activations ˆxa RTaDa and audio-aware video activations ˆxv RTvDv : ˆxa, ˆxv = FusionBlock(xa, xv, ta, tv). (3) The Fusion Block presents symmetric design that equally treats the different modalities. Initially, we project activations into common dimension to produce xa RTaD, xv RTvD respectively. Then, multi-head self attention block is applied to the concatenation of audio and video features {xa, xv}. Finally, an MLP projects the output back to the original dimensionality. Different sets of learnable parameters are used for tokens of each modality to allow for better modality alignment [17]. Symmetric Feature Reinjection Both ˆxa and ˆxv are passed to the subsequent DiT blocks in the respective frozen backbone to inject cross-modal information. Such design, which we call symmetric feature reinjection, allows us to further improve the quality of the activations extracted from successive blocks from the conditioning modality. In contrast to using static feature extractors [52, 62, 97], symmetric feature reinjection continuously refines the conditioning features according to the generated modality. Multimodal Temporal Alignment To facilitate temporal alignment between modalities, we propose the use of temporally-aligned 1D RoPE [71] embeddings in the self attention operation. We express 1D temporal RoPE as: RoPE(xn, τ (n)) = xneiτ (n)θbase, (4) where represents an audio or video token for simplified two-channel case, represents its index on the temporal axis, θbase is the base frequency, and τ is temporal alignment function defined as: τ (n) = (cid:40) n, ηv ηa , if is video token if is an audio token Where ηa, ηv are the number of tokens to represent one second of audio and video respectively. Intuitively, the time-aligned embeddings ensure that each temporally corresponding audio and video tokens are rotated by RoPE by the same angle, set to be proportional to their position on the temporal axis, thus establishing temporal correspondence which facilitates temporally-synced multimodal information exchange. Time-Aware Feature Fusion Information on the flow timesteps for audio ta and video tv are injected through adaptive adaLN-Zero [60], with audio and video parameter sets receiving information on both timesteps. Such design creates conditioning features that are most suited for the current flow time for the generated modality, increasing the effectiveness of symmetric feature reinjection. Training We train the Fusion Block on paired audio-video dataset DAV keeping both generators frozen and using the rectified flow objective. For the case of V2A, we use the training objective: = Eta,tv,A1,V1,A0 (cid:13)GV 2A(Ata , Vtv , ta, tv) (A1 A0)(cid:13) (cid:13) 2 2, (cid:13) ta pta , tv ptv , A1, V1 DAV , A0 pn, (5) and use fully symmetric one for A2V. Keeping the audio and video generators frozen, we train the Fusion Blocks for 50k iterations with learning rate of 3e4 and batch size of 256 for the V2A and A2V tasks. Additionally, we consider using separate parameter sets for the V2A and A2V tasks or training them jointly (see Sec. 4.4). We discuss architectural details in Appx. B, and training details in Appx. C. Inference In the V2A setting, we consider the input conditioning video V1 and compute partially noised video Vtv at the chosen flow time tv, which we set to fixed level during the entire inference phase to serve as the input for the video backbone. As we later show in 4.4, it is crucial to inject some noise into the conditioning modality for extracting useful features from the conditioning model. We sample A0 from Gaussian noise at ta = 0 and use velocity predictions from GV 2A to progressively denoise it into A1. We perform sampling using first-order Euler solver with 64 sampling steps. Text prompts can optionally be used for both modalities to condition generation. We perform A2V inference completely symmetrically. Figure 3. Visualization of Audio-to-Video and Video-to-Audio generation performance for various value of flow timesteps for conditioning features. Best performance is achieved when conditioning features are close to be fully denoised, i.e. [0.8, 0.98]. Prompt FAD FD CLAP IS IB-AI IB-AV Ons. ACC Diff-Foley [52] S&H [85] FoleyCrafter [97] Ours (VGGSounds) Ours-Joint Ours S&H [85] FoleyCrafter [97] Ours (VGGSounds) Ours-Joint Ours 11.00 28.71 29.22 66.51 18.66 4.62 13.68 2.02 13.15 2.19 14.17 1.58 10.72 25.44 18.05 2.99 13.19 1.91 13.74 2.23 11.99 1.33 0.06 0.025 0.080 0.129 0.122 0. 0.186 0.212 0.236 0.224 0.228 7.88 2.09 9.10 10.06 9.24 9.93 6.06 10.88 13.57 13.44 12.40 0.115 0.179 0.204 0.203 0.205 0.207 0.30 0.208 0.225 0.212 0.214 0.121 0.189 0.215 0.214 0.215 0. 0.318 0.219 0.238 0.224 0.226 0.14 0.128 0.166 0.405 0.409 0.531 0.08 0.179 0.429 0.471 0.54 Table 1. Comparison of AV-Link against prior work on the V2A task on the VGGSounds [9] benchmark. We report variant of our method with Fusion Blocks trained on VGGSounds only. 4. Experiments This section first introduces datasets (see Sec. 4.1) and evaluation protocol (see Sec. 4.2). We then and compare our method to the state-of-the-art in V2A and A2V in Sec. 4.3 and present an analysis of the proposed components in Sec. 4.4. We present extensive qualitative results supporting our evaluation in Appx. and the Website. 4.1. Datasets We train the base video model on an internal automatically captioned video dataset and the base audio model on ambient and music sound clips sourced from AutoReCap and AutoReCap-XL [23], FMA [14], Magna [39], MTG [5], and Song Describer [55]. The Fusion blocks are trained on 200,000 videos from VGGSounds [9] and the temporallystrong AudioSet [20] dataset. VGGSounds is an audiovisual dataset containing 126,000 samples. The temporallyaligned AudioSet includes 103,000 audio-video clips with strong audio-visual alignment. Additionally, we finetune the model on high-quality internal captioned audio-video dataset comprising 250,000 clips. Audio captions are generated using the AutoCap [23] audio captioning model. 4.2. Evaluation Protocol We separately evaluate our model on the V2A and A2V tasks. We consider conditioning video or audio input from 5 CLAP IS IB-AI IB-AV FAD FD CLAP IS IB-AI IB-AV Ons. ACC Movie Gen Benchmark VGGSounds Conditioning timestep t: - Uniform samp. - Fixed (ours) Conditioning features type: - CAVP - CAVP w/FT - CLIP - MetaCLIP - Diffusion features (ours) Fusion block arrangement: - After Block-1 - After Block-11 - After Block-22 - Interleaved (ours) Feature injection: - Concat. to text w/FT - Direct alignment - Direct alignment w/FT - w/o symm. feature reinj. - Symm. cross attention - Fusion blocks (ours) 0.216 0.192 4.19 6.53 0.103 0.150 0.111 0.155 6.91 4.79 27.33 18. 0.108 0.131 8.12 9.21 0.180 0.210 0.190 0.222 0.184 0.169 0.171 0.177 0.192 0.170 0.182 0.146 0. 0.186 0.098 0.110 0.120 0.170 0.192 4.62 5.69 3.26 4.77 6.53 5.67 5.92 4.29 6.53 4.12 2.38 3.15 4.57 5.70 6.53 0.116 0.136 0.143 0.147 0.150 0.129 0.138 0.120 0. 0.124 0.029 0.028 0.059 0.118 0.150 0.120 0.143 0.150 0.151 0.155 0.135 0.140 0.123 0.155 0.128 0.030 0.030 0.062 0.123 0.155 3.63 3.33 2.49 2.60 4.79 5.02 5.30 6.90 4. 3.36 8.16 9.26 6.60 8.47 4.79 24.36 23.81 21.47 19.72 18.91 20.90 20.14 25.20 18.91 20.35 42.35 36.51 30.27 22.30 18.91 0.098 0.098 0.117 0.125 0.131 0.114 0.122 0.098 0. 0.100 0.065 0.07 0.102 0.126 0.131 7.37 9.18 8.56 8.74 9.21 7.84 8.52 6.83 9.21 9.56 3.90 6.45 6.17 8.09 9.21 0.172 0.197 0.234 0.247 0.210 0.173 0.184 0.168 0. 0.186 0.094 0.129 0.136 0.194 0.210 0.180 0.208 0.247 0.259 0.222 0.170 0.191 0.174 0.222 0.196 0.100 0.137 0.143 0.210 0.222 0.413 0.415 0.383 0.371 0.386 0.373 0. 0.433 0.382 0.37 0.415 0.355 0.283 0.257 0.365 0.410 0.415 Table 2. V2A ablation results of our method. Variants marked with FT indicate backbone finetuning when few parameters are introduced. Figure 4. Qualitative V2A results. Our model achieved the best temporal alignment, matching closely the bouncing and drumming sounds entailed by the video modality. See the Appendix and Website for additional results. the test set and sample corresponding audio or video using our V2A or A2V model respectively. As prior work does not consistently adopt conditioning text prompts at inference [52, 62, 97], we report results for both settings. In all experiments, we use 8 fusion blocks (186M parameters), learning rate of 0.0001, and batch size of 128 on 8 A100 GPUs for the ablation, increasing to 256 on 16 A100 GPUs for final experiments. Baselines. For the task of V2A, we compare with Diff-foley [52], FoleyCrafter [97], Seeing and Hearing [85] (S&H), and Movie Gen [62]. Diff-foley conditions an audio generator on CAVP, an audio-aligned video feature extractor by concatenating it with text embeddings. FoleyCrafter adapts pre-trained audio generator for A2V using semantic adapter with frame-based CLIP features [63] and trains temporal adapter that aligns audio based on an onset mask predicted from the video, inspired by ControlNet. S&H uses training-free approach that optimizes cosine similarity in ImageBind [21] between video and generated audio. Movie Gen proposed to concatenate MetaClip [86] framewise features with audio tokens. For A2V, we compare with TempoToken [93], which optimizes latent tokens on pretrained video generator using BEATs [10] audio features. Metrics We follow the protocol of previous work [52, 85, 6 93] and adopt 2048 videos from the VGGSounds [9] test set. We employ Frechet Audio Distance (FAD) [48], Inception Score (IS) [68] and CLAP [83] score as measures for audio quality and prompt following. We also report FID [27] and FVD [76] as measures of video generation quality for the A2V task. Additionally, we leverage ImageBind [21] similarity scores between audio-image (IB-AI) and audio-video (IB-AV) modalities to measure semantic alignment. Onset detection accuracy (Onset ACC) [15] is used as the metric for measuring temporal alignment for the V2A task. For user studies, we ask evaluators to express preferences for one of two results based on Audio Quality, Video Quality, paired Audio-Video Quality, Semantic Alignment and Temporal Alignment between the two modalities. We report full details on user studies in Appx. D. Furthermore, We adopt the newly introduced Movie Gen V2A sound effects Benchmark [62], consisting of 527 generated video samples, for ablation studies and evaluating against Movie Gen. Note that since ground truth audio is not available for this benchmark, we omit audio-aligned metrics. 4.3. Results Video-to-Audio We present quantitative results comparing our method with baseline approaches on the V2A task in Tab. 1. Our method achieves the highest scores across all metrics, except for IB-AI and IB-AV, where S&H reports superior values. This is expected, as S&H explicitly optimizes for these metrics as part of its approach, making these scores less indicative of overall quality. Notably, our method demonstrates significantly better temporal alignment, with an Onset Accuracy of 0.53 compared to the top baseline score of 0.17. Such improvements are evident with and without providing audio text descriptions. For more balanced comparison, we also evaluate our model trained only on VGGSounds, showing that it still substantially outperforms baselines, highlighting the benefit of using temporally-aligned diffusion features. This advantage is further confirmed by our user studies in Tab. 4, where AV-Link is consistently preferred, particularly for temporal alignment. Additionally, while Movie Gen achieves better sound quality due to its larger model (13B parameters), our methods audio-video temporal alignment is preferred 63.6% of the time. We also illustrate our models strong temporal alignment in Fig. 4 and on the Website, where we perform inference on videos requiring precise temporal audio correspondence such as bouncing and tapping sounds. While baselines capture semantic content, they fall short in synchronizing the precise onset of these events. Audio-to-Video On the A2V task, our method surpasses TempoTokens [77] in both video quality and audio-video alignment, as shown in Tab. 3. User studies in Tab. 4 also indicate that our approach is preferred for generation quality, semantic, and temporal alignment, with preference greater than 74.8% across all settings. Additionally, qualitative results in Fig. 5 and on the Website demonstrate that our method produces videos that align closely with the audio signal, both semantically and temporally. Our generated video events (e.g., explosions) occur precisely in sync with corresponding audio events, showcasing the strength of our AV-Link in maintaining temporal coherence. 4.4. Ablation studies This section evaluates the contribution of each methods design choices to output quality. Unless otherwise specified, we employ frozen audio and video generators, and focus on the V2A task, whose findings we expect to generalize to A2V due to the symmetric nature of our approach. We report the main ablation results in Tab. 2 for models trained on VGGSounds dataset. To obtain more accurate qualitative and quantitative comparisons between baselines, we perform evaluation using single fixed seed for all test videos. Which is the optimal conditioning flow timestep? Diffusion models learn distinct features at different layers and for different diffusion timesteps [1, 51, 87]. Thus, the choice of flow time distribution for the conditioning modality pt determines the quality of the conditioning features. Taking the V2A example, using high-noise flow timestep tv would destroy the video signal, leading to loss of conditioning information, while flow time tv = 1 (i.e. with no noise) would produce uninformative video activations, as for tv = 1, Eq. (2) is minimized when the generator function is the identity GV (V1, 1) = V1 EV0pn=N (0,I)[V0] = V1. We study the optimal choice of tv empirically by training generator GV 2A where the video time is uniformly sampled tv U(0, 1) during training. At inference, we conduct evaluation for different values of tv and notice that the optimal value is close to fully denoised video (see Fig. 3). Intuitively, small changes in the video can determine large variations in the sound, such as smoke from gunshot, thus the model benefits from noise level where high-frequency details are still not erased by noise. symmetric analysis in Fig. 3 shows similar behavior for ta in the A2V task. In addition, training with uniform distribution of for the conditioning modality rather than fixing it at its optimal value led to slower convergence and reduced performance (see Tab. 2). Therefore, after identifying the optimal conditioning timestep to be 0.96 for V2A and 0.8 for A2V, we train subsequent models using these fixed timesteps. Can V2A and A2V models share the same parameters? We jointly train V2A and A2V models by sharing the Fusion Block parameters between the two tasks. As reported in Tab. 1 and Tab. 3. Using separate set of parameters contributes to marginal but consistent improvement over parameter sharing. Can both modalities be generated at the same time? Fig. 3 suggests key issue with joint video-audio diffu7 sion: Audio often depends on element that becomes only visible at the end of the sampling trajectory. For instance, accurately generating the sound of gunshot requires highfrequency visual details, such as the smoke of an explosion, Similarly, sounds like thunder and explosions only become distant at the later sampler steps, making jointly denoising the two modalities challenging. This may explain the lower performance of current methods in this task [35, 67, 85], and suggest that audio-video generation benefits from factorization into video generation followed by V2A and vice-versa. Which layers yield the best conditioning features? In Tab. 2, we ablate over four different designs where the multimodal Fusion Blocks are placed at early video DiT layers, in the middle, at the end, or regularly interleaved with the video DiT blocks. Interleaving the Fusion Blocks outperforms the other variants, indicating that feature diversity and regular modality alignment benefit the model. Are Fusion Blocks necessary? We evaluate several methodologies for injecting conditioning features into the model for which we show results in the Feature injection section of Tab. 2. Concat. to text follows Diff-foley [52] in concatenating conditioning features to the text embeddings. Direct alignment follows Movie Gen [62] in repeating the visual features to match the number of audio frames and sum them together. Symm. cross attention replaces the symmetric design of Fusion Blocks based on self attention with two separate blocks based on cross attention: the first for injecting features into the audio backbone, the second for the video. The Fusion Block design consistently outperforms the baselines. Additionally, we explore using video backbone as static feature extractor without reinjecting the output back to the subsequent video DiT blocks. While this approach allows for caching the video features and thus improves sampling efficiency, it significantly outperforms our method that progressively refines the conditioning features through reinjecting them back to subsequent DiT blocks. How do diffusion features compare to pretrained representations? We experimented with various pretrained feature extractors in place of frozen generator activations as inputs to the Fusion Blocks. As shown in Tab. 2 diffusion features outperform all representations on the Movie Gen benchmark. On VGGSounds it shows the strongest temporal alignment as expressed by Onset ACC. User studies against baselines employing such representations (see Tab. 4) confirm improved temporal and semantic alignment. Prompt FID FVD12 IB-AI IB-AV TempoTokens [93] Ours-Joint Ours TempoTokens [93] Ours-Joint Ours 103.09 41.07 34. 76.28 32.89 32.90 2406.60 416.17 352.87 1247.70 297.75 228.68 0.112 0.131 0.165 0.167 0.193 0.206 0.114 0.144 0. 0.173 0.206 0.210 Table 3. GSounds [9]. Comparison to baselines on the A2V task on VGConfiguration Prompt A-Qual. V-Qual. AV-Qual. Sem. Align. Temp. Align Video-to-Audio: -Diff-Foley [52] -Seeing and Hearing [85] -Seeing and Hearing [85] -FoleyCrafter [97] -FoleyCrafter [97] -Movie Gen [62] 78.0 86.5 76.2 66.5 57.4 34.4 - - - - - - Audio-to-Video: -TempoTokens [93] -TempoTokens [93] - - 86.8 95.6 86.1 97.1 87.7 76.3 64.3 52.8 78.0 83.2 84.9 95.1 86.8 75.5 67.2 56.8 74.8 75.6 83.7 95.5 88.1 80.0 65.5 63. 78.4 72.4 Table 4. User study comparing AV-Link against baselines. Results in % of votes in favor of our method. Figure 5. Qualitative A2V results. Our model generates semantically and temporally aligned content showing to the explosions and drummingsilencedrumming events implied by the audio modality. We show 3.3s of our samples at 3 FPS while only 2s of TempoTokens samples, hence the difference in frame count. See the Appendix and Website for additional results. 5. Conclusion We present AV-Link, unified approach for Video-to-Audio and Audio-to-Video generation. It leverages time-aligned activations from pretrained flow models, bypassing the use of specialized feature extractors. To enable this, we introduce Fusion Block that allows for bidirectional information exchange between frozen generators for each modality through the use of temporally-aligned self attention and symmetric feature reinjection. Extensive ablations and comparisons show performance improvements over all metrics, with particular regard to temporal alignment. Acknowledgements. The authors thank Maryna Diakonova and Ke Ma for their help in preparing the data and user studies. We also thank Chaoyang Wang, Sherwin Bahmani, Tsai-Shien Chen and Ziyi Wu for their guidance."
        },
        {
            "title": "References",
            "content": "[1] Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko. Label-efficient semantic segmentation with diffusion models, 2022. 7 [2] Burak Can Biner, Farrin Marouf Sofian, Umur Berkay Karakas, Duygu Ceylan, Erkut Erdem, and Aykut Erdem. Sonicdiffusion: Audio-driven image generation and editing with pretrained diffusion models, 2024. 2 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv, 2023. 3 [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. 3 [5] Dmitry Bogdanov, Minz Won, Philip Tovstogan, Alastair Porter, and Xavier Serra. The mtg-jamendo dataset for automatic music tagging. In Machine Learning for Music Discovery Workshop, ICML (ICML 2019), Long Beach, CA, United States, 2019. [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. Technical report, OpenAI, 2024. 1, 3 [7] Changan Chen, Puyuan Peng, Ami Baid, Zihui Xue, WeiNing Hsu, David Harwath, and Kristen Grauman. Action2sound: Ambient-aware generation of action sounds from egocentric videos, 2024. 2 [8] Gehui Chen, Guanan Wang, Xiaowen Huang, and Jitao Sang. Semantically consistent video-to-audio generation using multimodal language large model, 2024. 2 [9] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: large-scale audio-visual dataset. In ICASSP, 2020. 2, 5, 7, 8 [10] Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, Wanxiang Che, Xiangzhan Yu, and Furu Wei. Beats: audio pre-training with acoustic tokenizers. In ICML, pages 51785193, 2023. 2, 6 [11] Ziyang Chen, Prem Seetharaman, Bryan Russell, Oriol Nieto, David Bourgin, Andrew Owens, and Justin Salamon. Video-guided foley sound generation with multimodal controls, 2024. [12] Xin Cheng, Xihua Wang, Yihan Wu, Yuyue Wang, and Ruihua Song. Lova: Long-form video-to-audio generation, 2024. 2 [13] Xiangxiang Chu, Jianlin Su, Bo Zhang, and Chunhua Shen. Visionllama: unified llama backbone for vision tasks, 2024. 4 [14] Michael Defferrard, Kirell Benzi, Pierre Vandergheynst, and Xavier Bresson. FMA: dataset for music analysis. In 18th International Society for Music Information Retrieval Conference (ISMIR), 2017. 5 9 [15] Yuexi Du, Ziyang Chen, Justin Salamon, Bryan Russell, and Andrew Owens. Conditional generation of audio from video via foley analogies, 2023. 2, 7 [16] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. Clap learning audio concepts from natIn ICASSP. IEEE, 2023. 1, 2, ural language supervision. [17] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 4, 1 [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 2 [19] Zach Evans, CJ Carr, Josiah Taylor, Scott H. Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. In ICML, 2024. 1, 2 [20] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and humanlabeled dataset for audio events. In ICASSP, 2017. 5 [21] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In CVPR, 2023. 1, 2, 3, 6, 7 [22] Wei Guo, Heng Wang, Jianbo Ma, and Weidong Cai. Gotta hear them all: Sound source aware vision to audio generation, 2024. 2 [23] Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Guha Balakrishnan, Sergey Tulyakov, and Vicente Ordonez. Taming data and transformers for audio generation. arXiv, 2024. 1, 2, 4, 5 [24] Akio Hayakawa, Masato Ishii, Takashi Shibuya, and Yuki Mitsufuji. Discriminator-guided cooperative diffusion for joint audio and video generation, 2024. [25] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv, 2023. 2 [26] Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. Unsupervised semantic correspondence using stable diffusion, 2023. 2 [27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. 7 [28] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2022. 1 [29] Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, and Zhou Zhao. Make-an-audio 2: Temporal-enhanced textto-audio generation. arXiv, 2023. 2 [30] Zhiqi Huang, Dan Luo, Jun Wang, Huan Liao, Zhiheng Li, and Zhiyong Wu. Rhythmic foley: framework for seamless audio-visual alignment in video-to-audio synthesis, 2024. [31] Vladimir Iashin and Esa Rahtu. Taming visually guided In British Machine Vision Conference sound generation. (BMVC), 2021. 3 [32] Yujin Jeong, Wonjeong Ryoo, Seunghyun Lee, Dabin Seo, Wonmin Byeon, Sangpil Kim, and Jinkyu Kim. The power of sound (tpos): Audio reactive video generation with stable diffusion, 2023. 3 [33] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. arXiv, 2024. 1, 3 [34] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In CVPR, 2024. 2 [35] Gwanghyun Kim, Alonso Martinez, Yu-Chuan Su, Brendan Jou, Jose Lezama, Agrim Gupta, Lijun Yu, Lu Jiang, Aren Jansen, Jacob Walker, and Krishna Somandepalli. versatile diffusion transformer with mixture of noise levels for audiovisual generation. arXiv, 2024. 2, 3, [36] Gwanghyun Kim, Alonso Martinez, Yu-Chuan Su, Brendan Jou, Jose Lezama, Agrim Gupta, Lijun Yu, Lu Jiang, Aren Jansen, Jacob Walker, and Krishna Somandepalli. versatile diffusion transformer with mixture of noise levels for audiovisual generation, 2024. 3 [37] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. In NeurIPS, 2020. 4 [38] Saksham Singh Kushwaha and Yapeng Tian. Vintage: Joint video and text conditioning for holistic audio generation, 2024. 2 [39] Edith Law, Kris West, Michael I. Mandel, Mert Bay, and J. S. Downie. Evaluation of algorithms using games: The In International Society for Music case of music tagging. Information Retrieval Conference, 2009. 5 [40] Junwon Lee, Jaekwon Im, Dabin Kim, and Juhan Nam. Video-foley: Two-stage video-to-sound generation via temporal event condition for foley sound, 2024. 2 [41] Seungwoo Lee, Chaerin Kong, Donghyeon Jeon, and Nojun Kwak. Aadiff: Audio-aligned video synthesis with text-toimage diffusion, 2023. 1, [42] Seung Hyun Lee, Gyeongrok Oh, Wonmin Byeon, Chanyoung Kim, Won Jeong Ryoo, Sang Ho Yoon, Hyunjun Cho, Jihyun Bae, Jinkyu Kim, and Sangpil Kim. Sound-guided semantic video generation, 2022. 3, 2 [43] Jiachen Li, Qian Long, Jian Zheng, Xiaofeng Gao, Robinson Piramuthu, Wenhu Chen, and William Yang Wang. T2vturbo-v2: Enhancing video generation model post-training through data, reward, and conditional guidance design. arXiv, 2024. 3 [44] Ruiqi Li, Siqi Zheng, Xize Cheng, Ziang Zhang, Shengpeng Ji, and Zhou Zhao. Muvi: Video-to-music generation with semantic alignment and rhythmic synchronization, 2024. 2 [45] Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Language-guided joint audio-visual editing via one-shot adaptation, 2024. 3 [46] Yan-Bo Lin, Yu Tian, Linjie Yang, Gedas Bertasius, and Heng Wang. Vmas: Video-to-music generation via semantic alignment in web music videos, 2024. 2 [47] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. [48] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D. Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. ICML, 2023. 1, 7 [49] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley. AudioLDM 2: Learning holistic audio generation with self-supervised pretraining. arXiv, 2023. 1, 2 [50] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow, 2022. 3, 4 [51] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence, 2024. 2, 7 [52] Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao. Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models. NeurIPS, 36, 2024. 1, 3, 4, 5, 6, 8, [53] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. CVPR, 2023. 1 [54] Navonil Majumder, Chia-Yu Hung, Deepanway Ghosal, Wei-Ning Hsu, Rada Mihalcea, and Soujanya Poria. Tango 2: Aligning diffusion-based text-to-audio generations through direct preference optimization, 2024. 2 [55] Ilaria Manco, Benno Weck, Seungheon Doh, Minz Won, Yixiao Zhang, Dmitry Bogdanov, Yusong Wu, Ke Chen, Philip Tovstogan, Emmanouil Benetos, Elio Quinton, Gyorgy Fazekas, and Juhan Nam. The song describer dataset: corpus of audio captions for music-and-language In Machine Learning for Audio Workshop at evaluation. NeurIPS 2023, 2023. 5 [56] Yuxin Mao, Xuyang Shen, Jing Zhang, Zhen Qin, Jinxing Zhou, Mochu Xiang, Yiran Zhong, and Yuchao Dai. Tavgbench: Benchmarking text to audible-video generation, 2024. 2 [57] Xinhao Mei, Varun Nagaraja, Gael Le Lan, Zhaoheng Ni, Ernie Chang, Yangyang Shi, and Vikas Chandra. Foleygen: Visually-guided audio generation, 2023. 2 [58] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformers for text-to-video synthesis. In CVPR, 2024. 1, 10 [59] Santiago Pascual, Chunghsin Yeh, Ioannis Tsiamas, and Joan Serr`a. Masked generative video-to-audio transformers with enhanced synchronicity, 2024. 2 [60] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 4, 5, 1 [61] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with general conditioning layer. In AAAI, 2018. 4 [62] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie Gen: Cast of Media Foundation Models. arXiv, 2024. 1, 2, 3, 4, 6, 7, [63] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 3, 6 [64] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research (JMLR), 2022. 4 [65] Aashish Rai and Srinath Sridhar. Egosonics: Generating synchronized audio for silent egocentric videos, 2024. 3 [66] Yong Ren, Chenxing Li, Manjie Xu, Wei Liang, Yu Gu, Rilin Chen, and Dong Yu. STA-V2A: Video-to-Audio Generation with Semantic and Temporal Alignment, 2024. 3 [67] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. arXiv, 2023. 2, 3, 8 [68] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NeurIPS, 2016. 7 [69] Roy Sheffer and Yossi Adi. hear your true colors: Image guided audio generation. arXiv, 2024. [70] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In ICLR, 2023. 1 [71] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. 4, 1 [72] Kun Su, Xiulong Liu, and Eli Shlizerman. From vision to audio and beyond: unified model for audio-visual representation and generation, 2024. 3 [73] Mingzhen Sun, Weining Wang, Yanyuan Qiao, Jiahui Sun, Zihan Qin, Longteng Guo, Xinxin Zhu, and Jing Liu. Mmldm: Multi-modal latent diffusion model for sounding video generation, 2024. 3 [74] Vanessa Tan, Junghyun Nam, Juhan Nam, and Junyong Noh. Motion to dance music generation using latent diffusion In SIGGRAPH Asia 2023 Technical Communicamodel. tions, New York, NY, USA, 2023. Association for Computing Machinery. 3 [75] Ioannis Tsiamas, Santiago Pascual, Chunghsin Yeh, and Joan Serr`a. Sequential contrastive audio-visual learning, 2024. 3 [76] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv, 2018. [77] Ilpo Viertola, Vladimir Iashin, and Esa Rahtu. Temporally aligned audio for video with autoregression, 2024. 2, 7 [78] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In ICLR, 2022. 1 [79] Apoorv Vyas, Bowen Shi, Matthew Le, Andros Tjandra, Yi-Chiao Wu, Baishan Guo, Jiemin Zhang, Xinyue Zhang, Robert Adkins, William Ngan, Jeff Wang, Ivan Cruz, Bapi Akula, Akinniyi Akinyemi, Brian Ellis, Rashel Moritz, Yael Yungster, Alice Rakotoarison, Liang Tan, Chris Summers, Carleigh Wood, Joshua Lane, Mary Williamson, and WeiNing Hsu. Audiobox: Unified audio generation with natural language prompts, 2023. 1 [80] Heng Wang, Jianbo Ma, Santiago Pascual, Richard Cartwright, and Weidong Cai. V2a-mapper: lightweight solution for vision-to-audio generation by connecting foundation models, 2023. 3 [81] Xihua Wang, Yuyue Wang, Yihan Wu, Ruihua Song, Xu Tan, Zehua Chen, Hongteng Xu, and Guodong Sui. Tiva: In Proceedings Time-aligned video-to-audio generation. of the 32nd ACM International Conference on Multimedia (MM 24), page 10 pages, Melbourne, VIC, Australia, 2024. ACM. 3 [82] Yongqi Wang, Wenxiang Guo, Rongjie Huang, Jiawei Huang, Zehan Wang, Fuming You, Ruiqi Li, and Zhou Zhao. Frieren: Efficient video-to-audio generation with rectified flow matching, 2024. 3, [83] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In ICASSP, 2023. 7 11 [84] Zhifeng Xie, Shengye Yu, Qile He, and Mengtian Li. Sonicvisionlm: Playing sound with vision language models. arXiv, 2024. 3 [85] Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, and Qifeng Chen. Seeing and hearing: Open-domain visualIn CVPR, audio generation with diffusion latent aligners. pages 71517161, 2024. 1, 3, 4, 5, 6, 8, 2 [86] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data, 2024. 1, 3, 6 [87] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panoptic segmentation with text-to-image diffusion models, 2023. [88] Jinlong Xue, Yayue Deng, Yingming Gao, and Ya Li. Auffusion: Leveraging the power of diffusion and large language models for text-to-audio generation, 2024. 2 [89] Qi Yang, Binjie Mao, Zili Wang, Xing Nie, Pengfei Gao, Ying Guo, Cheng Zhen, Pengfei Yan, and Shiming Xiang. Draw an audio: Leveraging multi-instruction for video-toaudio synthesis, 2024. 3 [90] Ruihan Yang, Hannes Gamper, and Sebastian Braun. Cmmd: Contrastive multi-modal diffusion for video-audio conditional modeling, 2024. 3 [91] Xingyi Yang and Xinchao Wang. Diffusion model as representation learner, 2023. 2 [92] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv, 2024. 3 [93] Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan Schwartz, and Yossi Adi. Diverse and aligned audio-to-video generIn AAAI, pages ation via text-to-video model adaptation. 66396647, 2024. 1, 3, 6, 7, 8, [94] Fuming You, Minghui Fang, Li Tang, Rongjie Huang, Yongqi Wang, and Zhou Zhao. Momu-diffusion: On learning long-term motion-music synchronization and correspondence, 2024. 3 [95] Lin Zhang, Shentong Mo, Yijing Zhang, and Pedro Morgado. Audio-synchronized visual animation, 2024. 2 [96] Lin Zhang, Shentong Mo, Yijing Zhang, and Pedro Morgado. Audio-synchronized visual animation, 2024. 1, 3 [97] Yiming Zhang, Yicheng Gu, Yanhong Zeng, Zhening Xing, Yuancheng Wang, Zhizheng Wu, and Kai Chen. Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds. arXiv, 2024. 1, 3, 4, 5, 6, 8, 2 [98] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, ChienChin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023. 2 [99] Yuan Zhou, Qiuyue Wang, Yuxuan Cai, and Huan Yang. Allegro: Open the black box of commercial-level video generation model. arXiv, 2024. 3 12 AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "We discuss limitations in Sec. A. We then include details on the architecture (Sec. B), training and inference (Sec. C), and evaluation (Sec. D). We highly encourage the reader to visit the attached Website for extensive qualitative results and comparisons. A. Limitations Our base video backbone is low-resolution and low-fps RGB model. While the model achieves state-of-the-art V2A and A2V performance, leveraging high-resolution latent video model may further improve performance. Exploring model scaling to further improve feature quality is an exciting avenue for future work. Additionally, the feature reinjection alters activations in the conditioning modality generator. While beneficial, it introduces additional compute at each sampling step as caching prior activations becomes infeasible. Step distillation techniques reduce this effect by reducing the number of sampling steps and constitute an orthogonal line of work. B. Architecture Details We base our audio and video backbones on shared DiT [60] architecture. For the video model, We use an pixel-based flow matching model with an initial patchification operation of 22 using patch dimension of 1024. We employ 24 DiT blocks. Each DiT block is composed of self attention operation, followed by cross attention operation attending to text conditioning signals, and final MLP. We use 16 heads for each attention operation and hidden dimension of 4096 for the final MLP. Adaptive layer normalization is used within each block to condition the model on the flow time t. Each attention operator makes use of QK-Normalization [17] to improve training stability when trained in BF16 precision, and 3D RoPE [71] positional embeddings. For the audio model, we employ latent model with 24 DiT blocks with hidden dimension of 1024 for the patches. We use 16 attention heads for each self-attention and crossattention operation, and an MLP hidden size of 4096. We follow [23] for encoding the audio and converting the generated Mel-spectrograms to waveform. Both models have 576M trainable parameters. The Fusion blocks similarly have hidden dimension of 1024 and 16 heads for the self-attention operation. Their final MLP layers have hidden dimension of 4096. Each Fusion block has 23.25M parameters and we use 8 Fusion blocks for all of our experiments. C. Training and Inference Details This section presents additional details on training and inference, and discusses training and inference time. C.1. Training Details For all training phases, we train our models using the AdamW optimizer with learning rate of 3e-4, beta factors of 0.9 and 0.99, epsilon of 1e-8, weight decay of 0.01 and 10,000 steps warmup. The base video model is trained for 250,000 steps on automatically-captioned internal dataset with total batch size of 512 on 16 A100 GPUs. The base audio models is trained for 100,000 with total batch size of 1024 on 8 A100 GPUs. We drop text condition 10% of the times to enable classifier free guidance (CFG) [28] during inference. The fusion blocks are trained for 50,000 steps on 16 A100 GPUs with batch size of 256. Ablation experiments are trained on 8 A100s with total batch size 128. We drop the generated modality text prompt (e.g audio text prompt in V2A task) 50% of the time and the conditioning modality text prompt (e.g video text prompt in V2A task) 20% of the time. For all of our experiments, both the audio and video backbone are kept frozen unless otherwise specified. C.2. Inference details We perform inference starting from pure gaussian noise for the modality to generate and use the models velocity estimates together with an Euler sampler to progressively transform the noise to the clean generated sample. We found using classifier free guidance on the conditioning modality to be instrumental towards obtaining good multimodal alignment. When conditioning on more than one modality (e.g. video and audio text prompt), we drop both conditions siIn all multaneously to compute the unconditional signal. of our experiments, we use 64 sampling steps and CFG weight of 5.0. While all of our generated video results are evaluated using the native 36x64 resolution, the generated videos displayed on the website have been upsampled from 36x64 to 144x256 spatial resolution using an internal upsampler for demo purposes. C.3. Training and Inference Time Training. The base video model was trained for 25 days, while the audio model and the fusion blocks were trained (1, 1) and observe significantly faster convergence as shown in Fig. 6. D.2. Baselines Selection Below, we include details on baselines selection and inference procedures. Video-to-Audio. We compare our method against DiffFoley [52], FoleyCrafter [97], Seeing and Hearing [85], Frieren [82], and Movie Gen A2V [62]. Frieren: Since the code was released 6 days before the deadline, we only included qualitative comparison with Frieren on the Website. We use the Frieren (reflow) model, 64 sampling steps, CFG of 5.0, and the other recommended hyper-parameters for inference. Movie Gen A2V: Since Movie Gen is closed source model and audio samples are released for the Movie Gen Benchmark only, we include user studies and extensive comparison on the Website comparing our method against the released benchmark, showing that AV-Link achieves superior temporal alignment. FoleyCrafter: We employ FoleyCrafter with default settings for inference. Seeing and Hearing: We use the official code for V2A and we follow the default sampling parameters. We exclude Seeing and Hearing qualitative comparison without prompt as it produces barely audiable sounds in this setting. Diff-Foley: We use the official code and set sampling steps to 64 and CFG to 5.0. For the rest of the parameters, we use the default setting. For all of the baselines, we generate the audio at their recommended length from the full-length videos and crop it to 5.16s for fair comparison with our method. Audio-to-Video. To the best of the authors knowledge, TempoToken [93] is the only in-the-wild A2V baseline with publicly available code. We exclude Seeing and Hearing [85] A2V results as their code is not available for this task. Additionally, we exclude joint audio-video generation methods such as MMDiffusion [67], as they were trained on the very limited Landscapes Videos dataset [42], which contains only 928 videos and lacks generalization beyond landscape scenarios. We also exclude sound-guided image animation methods [42, 95], as they address fundamentally different task. We use the offical implementation of TempoToken with default parameters to generate 2 seconds videos. For the quntitative comparison, we crop our generated videos to 2 seconds for fair comparison with TempoToken. D.3. Additional V2A results Fig. 7 shows additional V2A results comparing our method to baselines. To better showcase the capabilities of AVLink, we record series of in-the-wild videos that require Figure 6. Comparison between different parametrizations for the Logit-Normal training distributions pt for the flow timestep t. When the location (i.e. the mean of the normal distribution) is shifted towards higher noise levels, we observe faster model convergence. for 8 and 4 days, respectively. All experiments utilized PyTorch Fully Sharded Data Parallel (FSDP) [98] for efficient distributed training. Inference. We measure throughput of 27.85s per sample using batch size of one on an A100 to perform 64 sampling steps for both the A2V and V2A tasks. This is limitation of our method in the V2A task where the it results in slower sampling time compared with previous approaches [52, 97]. We make, however, two important considerations. First, major use case for V2A is sonification of generated soundless videos. Such generation, when performed by state-of-the-art large-scale text-to-video generators usually takes several minutes. Second, since our method relies on flow model, distillation methods similar to the ones adopted by previous approaches [82] can significantly improve inference time by performing sampling in few steps. We regard this direction as as an interesting avenue for future work which is orthogonal to ours. D. Additional Evaluation Results and Details This section presents additional evaluation results an details. We highly encourage the reader to experience the generated audios and videos on the Website. D.1. Noise Sampling Scheduler For training the video and audio model, we use the LogitNormal distribution pt for the flow timestep with location 0.0 and scale 1.0, following [18]. When training the Fusion block, which builds on pretrained audio and video backbones, we adopt different approach. We noticed that early diffusion steps are the most critical for correctly following the conditioning modalities. Toward the end of the sampling path, however, the model relies more on the generated modality signal alone. Therefore, since multimodal alignment is the main purpose of the Fusion blocks, we shift the flow time training distribution to sample from more noisy steps. More specifically, Instead of parametrizing the normal distribution component as (0, 1), we set it to 2 high degree of temporal alignment for the audio modality and run inference for all baselines. Our method produces highly aligned audio results that capture the audio semantic entailed by the visual modality, while baselines produce degraded results. We attribute this phenomenon to the lack of access to visual features that are precisely aligned to the video content. On the contrary, the use of activations from the video generation backbone allows AV-Link to produce precise alignment in this scenario. D.4. Additional A2V results In Fig. 8 we show additional visualizations of A2V results produced by AV-Link in the case where textual descriptions for the visual modality are given, and refer the reader to the Website for A2V results produced without textual description. AV-Link can generate visual content that is wellaligned to the audio modality. D.5. User study details We hire team of professional annotators to perform the user studies. total of 50 samples are generated for each method in the V2A and A2V tasks. We present users with paired videos with accompanying audio generated by different methods, and ask them to express preference for one of the two based on Audio Quality, Video Quality, paired Audio-Video Quality, Semantic Alignment and Temporal Alignment between the two modalities. For the case of V2A evaluation, we formulate instructions for annotators for each such aspect as follows: Audio Quality Which audio has the best quality? Only listen to the audio and ignore the video content. Audio-Video Quality Which audio-video pair has the best quality? Semantic Alignment Which audio is semantically closer to the video content? Temporal Alignment Which audio is more temporally aligned to the video content? Formulation of questions for the A2V case is completely symmetric. For each generated pair of samples, we ask 5 different users to express preference to increase robustness of evaluation. 3 Figure 7. Qualitative V2A results comparing our method to baselines on in-the-wild videos captured by the authors that require precise temporal alignment. AV-Link produces audio signals that closely align to the visual modalities, while baselines often produce audio that is unrelated or not correctly synchronized with the visual content. See the Website for more results. 4 Figure 8. Qualitative A2V results produced by our method. AV-Link produces videos that present high level of alignment to the conditioning audio signal. See the Website for more results."
        }
    ],
    "affiliations": [
        "Rice University",
        "Snap Inc"
    ]
}