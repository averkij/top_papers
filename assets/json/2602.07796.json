{
    "paper_title": "Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents",
    "authors": [
        "Jiatong Li",
        "Changdae Oh",
        "Hyeong Kyu Choi",
        "Jindong Wang",
        "Sharon Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 ] . [ 1 6 9 7 7 0 . 2 0 6 2 : r Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents Jiatong Li1, Changdae Oh1, Hyeong Kyu Choi1, Jindong Wang2 and Sharon Li1 1University of WisconsinMadison, 2William & Mary Abstract Eliciting reasoning has emerged as powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more introverted by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is vital lever for agent optimization. Overall, our study suggests that information transparency awareness is crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent. Date: February 6, 2026 Contact: {jli2947, sharonli}@cs.wisc.edu 1. Introduction Large language models (LLMs) have recently demonstrated remarkable gains on complex tasks by leveraging test-time reasoning (Muennighoff et al., 2025), often implemented through explicit thinking such as chain-ofthought prompting or dedicated reasoning modules. By explicitly encouraging models to think before acting, prior work has shown promising improvements on wide range of closed-world benchmarks, including mathematical reasoning (Sprague et al., 2025) and code generation (Zhang et al., 2024). As result, thinking before acting has increasingly become popular design in modern LLM-based agents (Kim et al., 2025). However, most real-world agents do not operate in closed, isolated manners. Instead, they are userengaged (Wang et al., 2024; Yao et al., 2025): they interact with humans over multiple turns, operate under partial observability, and must actively communicate to elicit missing information, clarify intent, and guide users toward successful outcomes. In such settings, success depends not only on internal deliberation, but also on how agents communicate, disclose information, and guide users throughout the interaction. This raises fundamental yet unexplored question: Is mandatory thinking still beneficial for LLM agents in realistic, user-engaged scenarios? In this paper, we present systematic empirical study on the effect of thinking in user-engaged LLM Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents Figure 1 An overview of the research framework. agents. Across seven representative LLMs and established user-engaged benchmarks, we find consistent and counterintuitive pattern: enabling thinking frequently degrades agent performance in multi-turn, user-facing environments. This effect holds across different model families, tasks, and two common instantiations of thinking, including Thinking-as-a-Function (TaaF), where reasoning is enforced as mandatory pre-action function call in tool-using agents, and Thinking-as-a-Prefix (TaaP), where reasoning is elicited through ReAct-style prompt prefixes in text-based agents (Yao et al., 2023). The observed degradation reflects systematic interaction-level phenomenon rather than isolated failures or prompt artifacts. Consequently, the use of thinking in user-engaged environments requires rigorous understanding. To understand why thinking can be detrimental in these settings, we move beyond aggregate metrics and carefully analyze how thinking changes agent behavior during interaction. Our study unveils that enabling thinking consistently reduces information disclosure (i.e., proactively presenting environment observation to the user) across nearly all models. Instead of proactively disclosing pivotal information, thinking agents become more introverted, offering narrower and more selective information. This reduction in outward communication weakens active information exchange and, in interactive environments where progress depends on clarification through dialogue, can significantly hinder task completion. Through qualitative case studies, we further show how such omissions propagate through interaction: by failing to surface critical constraints or alternatives at key decision points, thinking agents inadvertently steer the trajectory toward failure. Our findings have direct implications for the design, deployment, and evaluation of LLM-based agents in user-engaged settings. Importantly, our study suggests that agent design should explicitly prioritize transparency, proactive information disclosure, and clarification over purely internal deliberation. To validate the practical benefits of our analysis, we further develop information-disclosure prompting and demonstrate empirically how it can improve performance across diverse model families, indicating that the observed introverted behavior is correctable design choice rather than an inherent limitation of reasoning-capable models. More broadly, our results highlight the importance of evaluating reasoning mechanisms in realistic, user-engaged settings, as improvements observed on closed-world benchmarks may not reliably translate to interactive agent performance. We summarize our contributions below. Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents 1. We conduct comprehensive empirical study on the effect of mandatory thinking in user-engaged LLM agents across multiple models, benchmarks, and thinking instantiations. We observe an interesting phenomenon that thinking is not always beneficial, and can even be harmful, in interactive scenarios. 2. We provide fine-grained behavioral analysis revealing how thinking makes LLM agents introverted, by discovering that thinking reduces response length and information disclosure to users, helping explain the observed performance degradation. 3. We propose an effective yet simple add-on prompting strategy that enhances information disclosure to boost agent performance, demonstrating the potential of information transparancy awareness in agent optimization. 2. Related Work User-engaged agent has become significant category of modern LLM agents in both industrial use and academic research (Clarke et al., 2022; Deshpande et al., 2025). They communicate with the user via language response (Qian et al., 2024) and interact with the environment via formatted action (Yao et al., 2023) or tool calling (OpenAI, 2025a) to achieve the users goal. As their impact becomes broader, sophisticated evaluation for them is increasingly crucial. Recent studies in this field mainly focus on specific tasks (Wang et al., 2024; Yao et al., 2025) or limited running environments (Deng et al., 2024; Lu et al., 2025; Rawles et al., 2025), representing two perspectives of simulating realistic agent environments. In contrast, our work focuses on understanding how internal reasoning mechanisms influence agent-user interaction across diverse user-engaged scenarios, providing behavior-level perspective that complements prior benchmark-driven studies. Test-time scaling through thinking is milestone method to enable LLMs to handle complicated tasks (Muennighoff et al., 2025; Sprague et al., 2025; Wei et al., 2022). Regarding LLM agents, massive effort has been devoted to using thinking to enhance action planning (Qiao et al., 2024; Yao et al., 2023), learning from memories (Shinn et al., 2023), or improving tool calling (Liu et al., 2024; Song et al., 2023; Wu et al., 2025a). Despite its early success, the effectiveness of thinking has recently been challenged (Ghosal et al., 2025). Recent works discovered that thinking length is sometimes negatively related to performance (Levy et al., 2024; Wu et al., 2025b; Yang et al., 2025). Others found that LLMs can act like System-1 (quick, short) or System-2 (deliberate, long) thinkers according to different instantiations (Li et al., 2025; Sivaprasad et al., 2025). Meanwhile, Shen et al. (2025) found that thinking trades off the interaction of web agents, impacting goal achievement negatively. Our work complements these studies by analyzing the impact of test-time thinking on agent-user interaction in user-engaged scenarios, which is underexplored. 3. Preliminaries 3.1. User-Engaged Agent Tasks We study user-engaged agents operating in partially observable environments (Gao et al., 2025), where the agent must interact with users and tools to uncover intent and task-relevant information to achieve complex final goal, such as travel booking or item retail. At each turn 洧노, the agent policy 洧랢洧랚(洧녩洧노洧녶洧노) maps the current context (interaction history and tool outputs) to an action: either natural language response or structured tool call. Crucially, language responses act as environmental dynamics; they directly shape the users subsequent actions and the resulting trajectory. Task success thus depends on the agents ability to ask for clarification and disclose information to the user by aligning the interaction with the final goal. 3 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents Figure 2 Overall performance of agents with or without thinking on Retail and Airline. 3.2. Formulation of Agents Thinking Agent thinking can be viewed as an internal, user-invisible generation process that precedes action selection. Specifically, thinking does not alter the environment; rather, it serves as an intervention on the action sampling distribution. Given an observation 洧녶洧노, thinking agent first generates internal tokens 洧랦洧노, then samples an action 洧녩洧노 洧랢洧랚(洧녶洧노, 洧랦洧노). We consider two common instantiations of this: Thinking-as-a-Function (TaaF) is designed for tool-calling agents, where thinking is implemented as mandatory function call executed before any response or tool invocation. The prompt instruction for TaaF is shown in Appendix A.1. Thinking-as-a-Prefix (TaaP) is designed for textual agents (Nakano et al., 2021), where thinking is elicited via prompt prefix (e.g., Thought:...) and precedes the action generation. This setting follows standard ReAct-style prompting (Yao et al., 2023), shown in Appendix A.1. 4. Evaluation of Thinking Agents In this section, we empirically evaluate whether enabling thinking improves the performance of LLM agents in user-engaged, multi-turn interaction scenarios. We comprehensively compare agents with and without thinking across multiple models, environments, and thinking instantiations. We describe the experimental setup in Sec. 4.1 and main observations in Sec. 4.2. 4.1. Experimental Setup Models. We conduct comprehensive evaluation of seven representative LLMs, spanning both proprietary and open-source categories. We select four proprietary models, GPT-5 (OpenAI, 2025c), GPT-4.1 (OpenAI, 2025b), GPT-4o (OpenAI, 2024), and Gemini-2.5-Pro (Gemini Team, 2025), based on their superior performance and widespread adoption in real-world agent scenarios. Additionally, we include three open-source models: DeepSeek-V3.1 (DeepSeek, 2025), gpt-oss-120b (OpenAI, 2025d), and Qwen3-30B-A3B Instruct (Qwen Team, 2025). 4 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents"
        },
        {
            "title": "Statistics",
            "content": "洧랦 -Retail 洧랦 -Airline TS-Phone Total # Inter. Turns # Task Cases # Tools 1,840 115 16 600 50 14 1,568 224 Table 1 Statistics of evaluated scenarios."
        },
        {
            "title": "Agent",
            "content": "洧랦 -Retail 洧랦 -Airline TS-Phone Pass@1 Pass@1 Pass@1 Pass@1 MSim MSim GPT-5 GPT-4. GPT-4o Gemini-2.5-Pro DeepSeek-V3.1 gpt-oss-120b Tool-calling TaaF Tool-calling TaaF Tool-calling TaaF Tool-calling TaaF Tool-calling TaaF Tool-calling TaaF 75.22 73.21 69.03 68. 64.04 57.89 67.83 37.43 73.04 66.35 60.00 53.91 -2.01 -0. -6.15 -37.00 -6.69 -6.09 50.00 42.00 58.00 52. 44.00 42.00 58.00 20.00 54.17 58.00 34.00 18.00 -8.00 -6. -2.00 -38.00 +3.83 -14.00 67.27 61.18 69.24 68. 66.47 66.35 48.04 28.92 69.44 69.23 67.94 67.39 -6.09 -0. -0.12 -19.12 -0.21 -0.55 Table 2 Performance (%) of agents without or with thinking-as-a-function (TaaF). Evaluation Scenarios. To ensure comprehensive evaluation, we consider three representative user-engaged scenarios from established benchmarks: 洧랦 -Retail and 洧랦 -Airline from 洧랦 -bench (Yao et al., 2025), and TS-Phone (i.e., MULTIPLE_USER_TURN) from ToolSandbox (Lu et al., 2025). Table 1 provides the statistics of each scenario, including the number of tasks, the number of tools available, and the average number of turns. 洧랦 -Retail: an agent assists user to exchange, return, or cancel items on shopping platform. 洧랦 -Airline: an agent assists user in booking or updating flight tickets on an airline platform TS-Phone: phone assistant that performs user-specified actions such as setting reminders. Evaluation metrics. We follow the metrics used in the original benchmark setting (Lu et al., 2025; Yao et al., 2025). For 洧랦 -Retail and 洧랦 -Airline, we use outcome-based evaluation metric Pass@1, measuring whether the final state matches the task goal. We report Pass@1 averaged over all tasks in each benchmark. For TS-Phone, we use process-based metric that measures the similarity between the agents action trajectory and the ground-truth milestone sequence, denoted as milestone similarity (MSim). Formally, MSim= 1 洧녰=1 sim(洧녺洧녰 洧 + is the 洧녰-th ground-truth milestone action in the milestone 洧녴 direct acyclic graph (DAG), and 洧녭 (洧녺洧녰 洧 +) is the agent action that best aligns with it. The similarity function sim() can be instantiated as either ROUGE-L measure for response (Lin, 2004), or abstract syntax tree (AST) matching for tool calling (Patil et al., 2025). Overall, MSim captures the average alignment between the milestone DAG and the agents executed action flow. 洧 +)), where 洧녺洧녰 洧 +, 洧녭 (洧녺洧녰 洧녴 4.2. Thinking Can Hurt User-Engaged Agents Figure 2 summarizes the performance of agents with and without thinking on 洧랦 -Retail and 洧랦 -Airline. Contrary to the common belief that explicit reasoning broadly improves agent behavior (Yao et al., 2023), we find that enabling thinking often degrades performance in user-engaged scenarios. 5 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents Figure 3 Response scale of agents with or without thinking. The average number of tokens for trajectories measures the scale."
        },
        {
            "title": "Model",
            "content": "w/o think w/ think 洧녷-val Cohens Qwen3-30B-A3B 1.42 1.69 1.30 1.75 gpt-oss-120b 1.24 1.34 DeepSeek-V3.1 Gemini-2.5-Pro 0.42 0.90 0.64 1.02 GPT-4o GPT-4.1 0.23 0.49 3.09 2.25 GPT-5 0.16 0.53 1.03 1.47 0.40 0.76 0.34 0.74 0.12 0.40 0.08 0.27 3.04 1. <0.001 0.042 <0.001 0.144 <0.001 <0.001 0.360 0.7451 0.1469 0.6394 0.0831 0.5258 0.3181 0.0280 Table 3 Paired 洧노-test of the number of long agent responses for each task. Rows ordered by the Cohens value, which quantifies the difference between the two mean values. Notably, all GPT-series models exhibit degraded performance, and several open-source models suffer substantial declines. For instance in Table 2, the performance of GPT-5 drops by 6.09% on TS-Phone; the performance of Gemini-2.5-Pro drops by 37.00% on 洧랦 -Retail; the performance of DeepSeek-V3.1 drops by 6.69% on 洧랦 -Retail. This pattern is not specific to particular benchmark or evaluation protocol. As shown in Table 2 and Table 6, similar trend persists under different thinking instantiations (Thinking-as-aFunction and Thinking-as-a-Prefix). The effect across models, tasks, and metrics suggests that the observed degradation reflects systematic interaction-level phenomenon. The observed performance degradation naturally leads to deeper question: why does thinking harm agent performance in interactive settings? To answer this question, we conduct additional empirical analysis and methodological exploration in the following sections, aiming to uncover how thinking changes agent behavior at the interaction level. 5. Empirical Analysis of Thought Effect In this section, we delve deep into understanding why thinking degrades the agents performance in userengaged settings. Our analysis proceeds in three steps. First, we quantify how thinking affects output (Sec. 5.1). Second, we examine how reduced output manifests as diminished information disclosure using 6 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents"
        },
        {
            "title": "Task Summary",
            "content": "User Intention (visible to user, invisible to agent): You are Isabella Johansson, and you live in 32286. You want to return the skateboard, garden hose, backpack, keyboard, bed, and also cancel the hose you just ordered (if cancelling one item is not possible, forget about it, you just want to cancel the hose and nothing else). You want to know how much you can get in total as refund. You are extremely brief but patient. Ground Truth (invisible to both): Return the skateboard, garden hose, backpack, keyboard and bed. Dont cancel the hose the user just ordered because the order includes other items. Figure 4 summary of the task case in 洧랦 -Retail. The blue highlights the milestone goal and action."
        },
        {
            "title": "Agent Response",
            "content": "I found some one-stop options that depart later. Here are the available flights. Option 1: HAT136 -> HAT039; Option 2: HAT218 -> HAT039. Which option would you prefer, or would you prefer to look at other dates?"
        },
        {
            "title": "Response Taxonomy",
            "content": "a1. found some one-stop options that depart later. a2. Here are the available flights. a3. Option 1: HAT136 -> HAT039; a4. Option 2: HAT218 -> HAT039. a5. Which option would you prefer? a6. Would you prefer to look at other dates? Figure 5 An example of response taxonomy. The response is divided into atomic statements. Yellow denotes information disclosure, while blue denotes user engagement request. fine-grained response taxonomy (Sec. 5.2). Finally, we present qualitative case study illustrating how insufficient disclosure propagates into task failure (Sec. 5.3). 5.1. Thinking Reduces Output Scaling We begin by investigating the impact of thinking on output scaling by analyzing changes in response token length before and after enabling the thinking mechanism in agents. We perform trajectory-level analysis on the Retail and Airline benchmarks. Specifically, we consider two metrics: (1) the average response length per trajectory and (2) the frequency of long responses within trajectory, with at least 150 tokens. Details are available at Appendix A.4. As illustrated in Figure 3, there is pronounced decline in response length across most LLMs when thinking is enabled. This pattern is further confirmed by paired statistical tests reported in Table 3, which demonstrate significant decrease in the number of long responses for the majority of models. For instance, DeepSeek-V3.1 produces an average of 1.24 long responses per trajectory without thinking, but drops sharply to 0.40 with thinking. These results suggest that thinking forces agents to generate more concise outputs. While brevity can be beneficial in some settings, in interactive scenarios, it may also reduce clarification and information exchange. To further investigate the implications of this reduced output, we conduct semantic analysis of the responses. 5.2. Thinking Reduces Information Disclosure in Agent Responses To examine how reduced output length affects interaction quality, we introduce an atomic response taxonomy that decomposes each agent response into semantically indivisible units, referred to as atomic statements. 7 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents Figure 6 Average number of information disclosure per task of agents with or without thinking. Each atomic statement is categorized into one of two functional types: (1) information disclosure, which communicates the current world state or available options to users, and (2) user engagement request, which prompts the user for input or action. For example, as illustrated in Figure 5, an airline agents response can be explicitly partitioned into six atomic statements: four instances of information disclosure and two user engagement requests. Maintaining sufficient information disclosure and engagement is indispensable for effective agent-user interaction. For the scalable analysis, we propose an autonomous taxonomy annotation pipeline that efficiently parses atomic responses and classifies them into categories (see Appendix A.5). We validate the reliability of our pipeline via human-in-the-loop annotation (see Appendix A.6). Using this taxonomy, we compare the frequency of information disclosure in the scenarios. We focus primarily on information disclosure as it provides the essential context and affordances required for meaningful user engagement, effectively serving as proxy for the agents interactive capacity. As shown in Figure 6, enabling thinking leads to substantial and consistent reduction in information disclosure to users across nearly all models, spanning both proprietary and open-source architectures. These results suggest that thinking renders agents more introverted: rather than proactively summarizing observations or enumerating options, thinking agents tend to provide narrower, more selective information. As result, this diminishes active information exchange with the user. In user-engaged scenarios, where the users intent is partially observable and must be clarified through interaction, this reduction in disclosure can significantly hinder task progress. 5.3. Case Study: Insufficient Information Disclosure Misleads the User To concretely illustrate how reduced information disclosure can lead to task failure, we conduct qualitative case study across three scenarios. We first manually sample the cases where the thinking agent fails while the non-thinking agent succeeds. Subsequently, we perform backtracking analysis of the failure and success propagation within the agents actions. Finally, we provide trajectory comparison to demonstrate the specific impact of thinking on agent-user interaction. We consider retail task scenario, e.g., user wanting to return skateboard, with detailed task description provided in Figure 4. The trajectories for the thinking and non-thinking agents are shown in Figure 7, respectively (additional cases are presented in Appendix A.7). critical decision point in this scenario hinges on whether the agent informs the user that an item they bought cannot be canceled individually 8 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents Trajectory of Agent w/ Thinking Trajectory of Agent w/o Thinking User Initial Request: User Initial Request: Hi, need help returning few items and cancelling an item. Agent Thought & Action: [Thought] She wants to return items from delivered orders (#W3792453, #W7181492, # W5565470) and cancel pending garden hose order (#W2575533). need to confirm the details with her before proceeding, includ ing the payment method for the return refund and the reason for cancellation. [Response] Here are the details found for your requests: 1. Return items from delivered orders: ... 2. Cancel pending order: Garden Hose (Order #W2575533) Failure: the item cannot be cancelled individually To proceed, please confirm: ... - You want to cancel the pending garden hose order (#W2575533) because it is no longer needed or ordered by mistake the agent does not disclose that Hi, Id like to return and cancel some items. Agent Action: [Response] To confirm, you want to cancel only the Garden Hose (black, 50 ft, vinyl, Item ID: 5206946487) from your pending order #W2575533, with the reason \"no longer needed.\" Please note: can only cancel the entire pending order, not individual items. If you proceed, the whole order #W2575533 (including all items in it) will be cancelled and refunded to your PayPal account in 5-7 business days. Success: discloses that the item cannot be cancelled individually Would you like to cancel the entire order #W2575533, or would you prefer to keep the other items? Please confirm how you want to proceed. the agent proactively User Response: User Response: Yes, thats correct. Please refund to my original PayPal. The hose order should be cancelled because it was ordered by mistake. Id prefer to keep the other items, so lets skip cancelling. Just return the other items. How much refund total? Figure 7 Trajectories of agent with or without thinking in 洧랦 -Retail. The red highlights the failure path. Red text labels the failure pattern. The blue highlights the pivotal task success path. Blue text labels pivotal information disclosure that leads to task success. due to order constraints. This requires sufficient level of information disclosure, as conceptualized in Section 5.2. For the agent with thinking (see the left of Figure 7), we observe that the internal monologue overlooks the specific constraints regarding item cancellation. This localized action planning curtails the information disclosure provided in the subsequent response, causing the agent to omit the necessary warning. Consequently, this lack of transparency misleads the user into making an incorrect decision, resulting in task failure. In contrast, the agent without the explicit thinking proactively discloses the cancellation constraint. This active communication alerts the user to the fact that cancelling the hose alone is infeasible, which in turn empowers the user to make correct decision based on sufficient information. This case study demonstrates that the thinking may inadvertently lead to insufficient information disclosure, thereby leading to downstream user errors. 6. Discussion and Practical Implications Our findings have several important implications for the design, deployment, and evaluation of LLM-based agents in interactive settings. 9 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents"
        },
        {
            "title": "Prompt",
            "content": "洧랦 -Retail 洧랦 -Airline Avg. Pass@1 GPT-4o, tool-calling agent"
        },
        {
            "title": "Vanilla\nInfoDis",
            "content": "64.04 66.95 (+2.91) 44.00 46.00 (+2.00) 54.02 56.48 (+2.46) Gemini-2.5-Pro, tool-calling agent"
        },
        {
            "title": "Vanilla\nInfoDis",
            "content": "67.83 70.43 (+2.60) 58.00 58.00 (+0.0) 62.92 64.22 (+1.30) DeepSeek-V3.1, tool-calling agent"
        },
        {
            "title": "Vanilla\nInfoDis",
            "content": "73.04 74.78 (+1.74) 52.00 56.00 (+4.00) 62.52 65.39 (+2.87) Table 4 Task Pass@1 (%) of agents with information disclosure prompting v.s. vanilla prompting. Information transparency can drive agents goal achievement. Our findings suggest that the next generation of user-engaged agents must move beyond raw reasoning power toward transparency-aware design. While current paradigms prioritize internal thinking, future agents must treat external interaction as top priority of the task process. We demonstrate the potential of this direction by our preliminary experiments with Information Disclosure (InfoDis), an add-on prompt that simply instructing agents to prioritize informative transparency (see Appendix A.8 for technical details). As shown in Table 4, we observed immediate and consistent performance gains across various model families GPT-4o (OpenAI), Gemini-1.5-Pro (Google), and DeepSeek-V3.1 (DeepSeek), such as +4.00% increase in the 洧랦 -Airline for DeepSeek-V3.1. The fact that such lightweight prompting intervention yields such significant improvements across diverse model families signals major optimization frontier. It suggests that introverted behavior by reasoning is not an inherent limitation, but design gap that can be bridged. We argue that instilling transparency awareness into LLMs, whether through architectural shifts, supervised fine-tuning, or reinforcement learning, represents potential lever for better agentic system design. This path promises new class of agents that not only solve tasks in isolation, but actively manage user uncertainty through strategic disclosure and clarification. User-centric reasoning LLMs development. Our empirical analysis demonstrates counterintuitive tradeoff in user-engaged scenarios: thinking can hinder goal achievements by rendering agents more introverted. Our results suggest that the common thinking pattern of LLMs should be reconsidered in user-engaged scenarios. In existing settings, thinking is designed for complex but fully observed, single-turn tasks. Such thinking pattern can suppress information disclosure and reduce exploratory communication, which is harmful to more general user-engaged scenarios. Therefore, developers should pursue new data and training strategies to guide LLMs to acquire more user-centric thinking patterns for agentic utility. Broader lessons for agent evaluation. Finally, our results caution against transferring intuitions from closed-world reasoning benchmarks directly to interactive agent deployments. While explicit thinking can improve performance in static tasks, it may introduce unintended side effects in dynamic, user-facing systems. For practitioners, this underscores the importance of validating reasoning techniques under realistic interaction conditions before deploying them in production agents. 10 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents 7. Conclusion In this work, we investigated the impact of mandatory thinking in user-engaged LLM agents, challenging the prevailing assumption about the effectiveness of reasoning. Through comprehensive evaluation across seven representative models and multiple benchmarks, we uncovered consistent serendipity: enforcing thinking before actions can degrade performance in user-engaged settings. Our analysis shows that this degradation arises from systematic behavioral shift, where thinking shortens responses and reduces information disclosure, making agents more introverted and less effective at guiding users through underspecified tasks. Qualitative case studies further illustrate how such omissions propagate into task failures. To translate these insights into practical utility, we presented simple information-disclosure prompting method, and showed that it can reliably recover and improve performance across model families. Overall, our results highlight the importance of evaluating reasoning mechanisms in realistic interactive environments and motivate interaction-aware agent design that balances internal deliberation with proactive communication."
        },
        {
            "title": "Limitations",
            "content": "While our study provides systematic analysis of how explicit reasoning affects agent behavior in userengaged scenarios, we acknowledge several limitations that offer avenues for future research. First, our evaluation is centered on three representative user-engaged benchmarks that primarily represent task-oriented dialogues. Future work should explore whether the thinking effect persists in more open-ended or creative collaboration scenarios where the users intent is less structured. Moreover, our behavioral analyses rely on automated response taxonomy and thought categorization pipelines supported by large language models, with human validation. While we observe strong agreement between model-based and human annotations, subtle categorization errors may remain, particularly for borderline cases involving implicit disclosure or nuanced user engagement."
        },
        {
            "title": "Ethical Considerations",
            "content": "This study presents an empirical analysis of the thinking effect within user-engaged agents. The methodology and findings detailed herein are intended to advance the research and practical application of LLM agents, providing an empirical foundation and reference for future development in the field. Furthermore, we remain committed to transparency and reproducibility; all data resources and models utilized in this research are derived from publicly available sources and are used in accordance with their respective licensing agreements. We anticipate no significant foreseeable negative social impacts resulting from this work."
        },
        {
            "title": "Acknowledgements",
            "content": "The authors would like to thank Seongheon Park and Leitian Tao for their valuable suggestions on the manuscript. This work is supported in part by the AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation under awards IIS-2237037 and IIS-2331669, Office of Naval Research under grant number N00014-231-2643, Schmidt Sciences Foundation, Open Philanthropy, Alfred P. Sloan Fellowship, and gifts from Google and Amazon. 11 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents"
        },
        {
            "title": "References",
            "content": "C. Clarke, J. Peper, K. Krishnamurthy, W. Talamonti, K. Leach, W. Lasecki, Y. Kang, L. Tang, and J. Mars. One agent to rule them all: Towards multi-agent conversational AI. In Findings of the Association for Computational Linguistics: ACL 2022, May 2022. DeepSeek. Deepseek-v3.1 release. https://api-docs.deepseek.com/news/news250821, 2025. DeepSeek API Docs. S. Deng, W. Xu, H. Sun, W. Liu, T. Tan, L. Liujianfeng, A. Li, J. Luan, B. Wang, R. Yan, and S. Shang. Mobile-bench: An evaluation benchmark for LLM-based mobile agents. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2024. K. Deshpande, V. Sirdeshmukh, J. B. Mols, L. Jin, E.-Y. Hernandez-Cardona, D. Lee, J. Kritz, W. E. Primack, S. Yue, and C. Xing. MultiChallenge: realistic multi-turn conversation evaluation benchmark challenging to frontier LLMs. In Findings of the Association for Computational Linguistics: ACL 2025, July 2025. H. Gao, J. Geng, W. Hua, M. Hu, X. Juan, H. Liu, S. Liu, J. Qiu, X. Qi, Y. Wu, H. Wang, H. Xiao, Y. Zhou, S. Zhang, J. Zhang, J. Xiang, Y. Fang, Q. Zhao, D. Liu, Q. Ren, C. Qian, Z. Wang, M. Hu, H. Wang, Q. Wu, H. Ji, and M. Wang. survey of self-evolving agents: On path to artificial super intelligence, 2025. G. Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/2507.06261. S. S. Ghosal, S. Chakraborty, A. Reddy, Y. Lu, M. Wang, D. Manocha, F. Huang, M. Ghavamzadeh, and A. S. Bedi. Does thinking more always help? mirage of test-time scaling in reasoning models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. J. Kim, B. Shin, J. Chung, and M. Rhu. The cost of dynamic reasoning: Demystifying ai agents and test-time scaling from an ai infrastructure perspective, 2025. M. Levy, A. Jacoby, and Y. Goldberg. Same task, more tokens: the impact of input length on the reasoning performance of large language models. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024. Z.-Z. Li, D. Zhang, M.-L. Zhang, J. Zhang, Z. Liu, Y. Yao, H. Xu, J. Zheng, P.-J. Wang, X. Chen, Y. Zhang, F. Yin, J. Dong, Z. Li, B.-L. Bi, L.-R. Mei, J. Fang, X. Liang, Z. Guo, L. Song, and C.-L. Liu. From system 1 to system 2: survey of reasoning large language models, 2025. C.-Y. Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, July 2004. Z. Liu, H. Hu, S. Zhang, H. Guo, S. Ke, B. Liu, and Z. Wang. Reason for future, act for now: principled architecture for autonomous LLM agents, 2024. J. Lu, T. Holleis, Y. Zhang, B. Aumayer, F. Nan, H. Bai, S. Ma, S. Ma, M. Li, G. Yin, Z. Wang, and R. Pang. ToolSandbox: stateful, conversational, interactive evaluation benchmark for LLM tool use capabilities. In L. Chiruzzo, A. Ritter, and L. Wang, editors, Findings of the Association for Computational Linguistics: NAACL 2025, pages 11601183. Association for Computational Linguistics, 2025. N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. Cand칟s, and T. B. Hashimoto. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2028620332, 2025. 12 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, and J. Schulman. Webgpt: Browser-assisted question-answering with human feedback. CoRR, abs/2112.09332, 2021. OpenAI. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. OpenAI. calling. function-calling, 2025a. OpenAI Platform Documentation."
        },
        {
            "title": "Function",
            "content": "https://platform.openai.com/docs/guides/ OpenAI. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, 2025b. OpenAI Platform Documentation. OpenAI. Gpt-5 is here. https://openai.com/gpt-5/, 2025c. OpenAI Platform Documentation. OpenAI. gpt-oss-120b & gpt-oss-20b model card, 2025d. URL https://arxiv.org/abs/2508. 10925. S. G. Patil, H. Mao, F. Yan, C. C.-J. Ji, V. Suresh, I. Stoica, and J. E. Gonzalez. The berkeley function calling leaderboard (BFCL): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. C. Qian, B. He, Z. Zhuang, J. Deng, Y. Qin, X. Cong, Z. Zhang, J. Zhou, Y. Lin, Z. Liu, and M. Sun. Tell me more! towards implicit user intention understanding of language model driven agents. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10881113, 2024. S. Qiao, N. Zhang, R. Fang, Y. Luo, W. Zhou, Y. Jiang, C. Lv, and H. Chen. AutoAct: Automatic agent learning from scratch for QA via self-planning. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024. A. Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. C. Rawles, S. Clinckemaillie, Y. Chang, J. Waltz, G. Lau, M. Fair, A. Li, W. E. Bishop, W. Li, F. CampbellAjala, D. K. Toyama, R. J. Berry, D. Tyamagundlu, T. P. Lillicrap, and O. Riva. Androidworld: dynamic benchmarking environment for autonomous agents. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025, 2025. J. Shen, H. Bai, L. Zhang, Y. Zhou, A. Setlur, S. Tong, D. Caples, N. Jiang, T. Zhang, A. Talwalkar, and A. Kumar. Thinking vs. doing: Agents that reason by scaling test-time interaction. In Workshop on Scaling Environments for Agents, 2025. N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: language agents with verbal reinforcement learning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. S. Sivaprasad, P. Kaushik, S. Abdelnabi, and M. Fritz. theory of response sampling in LLMs: Part descriptive and part prescriptive. In W. Che, J. Nabende, E. Shutova, and M. T. Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2025. C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 29983009, 2023. 13 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents Z. R. Sprague, F. Yin, J. D. Rodriguez, D. Jiang, M. Wadhwa, P. Singhal, X. Zhao, X. Ye, K. Mahowald, and G. Durrett. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. In The Thirteenth International Conference on Learning Representations, 2025. X. Wang, Z. Wang, J. Liu, Y. Chen, L. Yuan, H. Peng, and H. Ji. MINT: Evaluating LLMs in multi-turn In The Twelfth International Conference on Learning interaction with tools and language feedback. Representations, 2024. J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi, Q. V. Le, and D. Zhou. Chain-ofthought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc., 2022. J. Wu, J. Zhu, Y. Liu, M. Xu, and Y. Jin. Agentic reasoning: streamlined framework for enhancing LLM reasoning with agentic tools. In W. Che, J. Nabende, E. Shutova, and M. T. Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2025a. Y. Wu, Y. Wang, Z. Ye, T. Du, S. Jegelka, and Y. Wang. When more is less: Understanding chain-of-thought length in llms, 2025b. W. Yang, S. Ma, Y. Lin, and F. Wei. Towards thinking-optimal scaling of test-time compute for LLM reasoning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. S. Yao, N. Shinn, P. Razavi, and K. R. Narasimhan. {$tau$}-bench: benchmark for underline{T}oolunderline{A}gent-underline{U}ser interaction in real-world domains. In The Thirteenth International Conference on Learning Representations, 2025. K. Zhang, J. Li, G. Li, X. Shi, and Z. Jin. CodeAgent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1364313658. Association for Computational Linguistics, 2024. A. Appendix A.1. Thinking Prompt Instructions We provide our prompts to implement TaaF and TaaP in the following two color boxes."
        },
        {
            "title": "TaaF Prompt Instruction Example",
            "content": "{environment_prompt_instruction} ### You must always call the think function to think about your next step before taking any other actions (including function calling and responding to user). This is at the HIGHEST priority and must be followed. 14 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents"
        },
        {
            "title": "TaaP Prompt Instruction Example",
            "content": "{environment_prompt_instruction} # Instruction You need to act as an agent that use the above tools to help the user according to the above policy. At each step, your generation should have exactly the following format: Thought: <A single line of reasoning to process the context and inform the decision making . Do not include extra lines.> Action: {{\"name\": <The name of the action>, \"arguments\": <The arguments to the action in json format>}} ... Your response can be like this: Thought: Since the user asks for the weather of San Francisco in USA, the unit should be in fahrenheit. can query get_current_weather to get the weather. Action: {{\"name\": \"get_current_weather\", \"arguments\": {{\"location\": \"San Francisco, CA\", \"format\": \"fahrenheit\"}}}} A.2. Details of Evaluation Scenarios Table 5 presents summarization of the scenarios for our agent evaluation. A.3. Analysis of Agent Thought Contents Quantitative analysis on agent thoughts is inherently challenging due to their linguistic complexity and their implicit influence on task trajectories. To address this, we propose hierarchical thought taxonomy informed by three key observations: (1) complex agent thoughts can be decomposed into sequence of simpler, atomic units; (2) each atomic thought serves either as action planning or state perception; and (3) the subject of an atomic thought is either the user or the environment. Based on these principles, we established two-level hierarchical classification system. An example showing all thougt categories is shown in Figure 8. For our empirical analysis, we randomly sampled 20% of the trajectories from our dataset and employed GPT-4.1 to decompose and categorize the agent thoughts accordingly. The distribution of this taxonomy is illustrated in Figure 9, where yellow segments represent action planning (including user-interactive actions and tool-calling actions) and blue segments represent state perception (including user intention confirmation and database observation summarization). Our visualization reveals two primary findings. First, action planning dominates the thought processes of many agents, particularly within the GPT series. In these models, over half of the generated thoughts focus on planning future actions based on existing observations. While such proactive planning is essential for complex reasoning, it may be suboptimal in user-engaged scenarios where current observations are often insufficient for task completion without further user clarification. Second, the specific instantiation of the \"thinking\" process significantly shifts thought content. For example, the proportion of tool-calling thoughts in GPT-5 increases from 44.4% to 59.5% when transitioning from Thinking-as-a-Function (TaaF) to Thinking-as-aPrefix (TaaP) framework. This suggests that thinking patterns are highly sensitive to framework design and prompt instructions. Ultimately, while our taxonomy effectively characterizes agent thinking patterns, it does 15 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents"
        },
        {
            "title": "Agent",
            "content": "洧랦 -Retail 洧랦 -Airline Tool-calling, Act Tool-calling, Act TS-Phone Tool-calling"
        },
        {
            "title": "Online flight agent",
            "content": "Built-in phone assistant"
        },
        {
            "title": "Scenario Goal",
            "content": "the user Assist change/return/cancel items to exretail Assist the user to book/update/cancel flights Assist the user for personalized phone use"
        },
        {
            "title": "Databases",
            "content": "Users, orders, products Users, reservations, flights Sandbox, setting, contact, messaging, reminder"
        },
        {
            "title": "Success Criteria",
            "content": "Final database snapshot; pivotal output contents Final database snapshot; pivotal output contents Trajectories of milestone actions"
        },
        {
            "title": "Evaluation\nMethod",
            "content": "Check database snapshot & pivotal contents in responses Check database snapshot & pivotal contents in responses Check the coverage of milestone actions Evaluation Type Outcome-based Outcome-based Process-based Table 5 summarization of features of evaluated scenarios. LLM GPT-5 GPT-4.1 GPT-4o Gemini-2.5-Pro DeepSeek-V3.1 gpt-oss-120b Qwen3-30B-A3B Agent Act-only ReAct (TaaP) Act-only ReAct (TaaP) Act-only ReAct (TaaP) Act-only ReAct (TaaP) Act-only ReAct (TaaP) Act-only ReAct (TaaP) Act-only ReAct (TaaP) 洧랦 -Retail 洧랦 -Airline TS-Phone Pass@1 Pass@1 Pass@1 Pass@1 MSim MSim 72.17 73.91 66.96 67.83 66.67 66. 64.35 64.35 73.21 69.30 16.52 10.43 54.39 22.61 +1.74 +0. -0.31 0.0 -3.91 -6.09 -31.78 50.00 52. 46.94 51.02 44.90 40.00 60.00 60.00 52.00 56.00 14.00 30.00 34.00 40. +2.00 +4.08 -4.90 0.0 +4.00 +16. +6.00 - - - - - - - - - - - - - - - - - - - - - - - - - - - - Table 6 Performance (%) of agents without or with thinking-as-a-prefix (TaaP). not directly measure their causal impact on subsequent actions; these effects are further explored through our analysis of response lengths, action taxonomies, and qualitative case studies. A.4. Settings of Output Scaling Analysis Metric of response length In our analysis, we chose the number of tokens (Eq. 1) and the number of long responses (Eq. 2) for quantifying agent response length. We use the o200k_base tokenizer, which is widely applied for OpenAI LLM series. # of Tokens Per Natural Language Response This analysis (see Figure 3) presents the impact of thinking on response-level output scaling. Specifically, for each agent, the average # tokens per natural language 16 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents"
        },
        {
            "title": "Agent Thought",
            "content": "The user wants to exchange an item. have successfully authenticated the user with user ID. Now need to get their user details to view their orders. Also, need to ask the user for details of the exchange."
        },
        {
            "title": "Thought Taxonomy",
            "content": "a1. The user wants to exchange an item. a2. have successfully authenticated the user with user ID. a3. Now need to get their user details to view their orders. a4. need to ask the user for details of the exchange. Figure 8 An example of thought taxonomy. The entire thought is divided into atomic statements. Blue denotes state perception, including user intention confirmation (a1) and database observation summarization (a2). Yellow denotes action planning, including tool-calling action (a3) and user-interactive action (a4). response is averaged on all responses, as defined in the following: Avg(# Tokens per response) = 1 洧녨洧洧뉧롐멇롐 洧녜 洧녢洧녰 洧녰=1 洧녱=1 洧녩洧녰 洧녱 洧냪(洧녩洧녰 洧녱 洧눞洧洧뉧롐멇롐), (1) where 洧녩洧녰 The 洧눞洧洧뉧롐멇롐 denotes the response action space, which is subset of the action space. The 洧냪(洧녩洧녰 洧녱=1 洧냪(洧녩洧녰 indicates whether 洧녩洧녰 denotes the total number of responses of the agent. 洧녱 is the 洧녱-th agent action on the 洧녰-th trajectory. Here 洧녢洧녰 denotes the number of turns in the 洧녰-th trajectory. 洧녱 洧눞洧洧뉧롐멇롐) 洧녱 洧눞洧洧뉧롐멇롐) 洧녱 belongs to natural language responses. The 洧녨洧洧뉧롐멇롐 = 洧녜 洧녢洧녰 洧녰= # of Long Responses Per Trajectory This analysis (see Table 3) presents the impact of thinking on trajectory-level output scaling. Specifically, we define long responses as natural language responses with at least 150 tokens. Next, for given agent, we count the average number of long responses on all trajectories, as defined in the following: Avg(# Long responses per trajectory) = 1 洧녜 洧녜 洧녢洧녰 洧녰= 洧녱=1 洧냪(洧녩洧녰 洧녱 洧눞洧洧뉧롐멇롐 洧녩洧녰 洧녱 150). (2) Overall, the two aspects of output scaling comprehensively characterize the effect of thinking on model verbosity at both the individual response level and the aggregate trajectory level. A.5. Detailed Experiment Setup of Response Taxonomy Considering efficiency and accuracy, we utilize the strong natural language understanding and few-shot learning abilities of proprietary LLMs to automatically annotate response taxonomy for trajectory data. The few-shot prompt instruction is shown in Figure 10, 11, 12, 13, and 14. In practice, to ensure more accurate annotation, we provide fine-grained two-level hierarchical taxonomy in the LLM prompt instruction. The two-level taxonomy has clearer definition for the LLM to annotate sentences. Specifically, for information disclosure, we propose the observation summarization and choice enumeration. For user engagement request, we propose the close-ended inquiry and open-ended inquiry. The definition of these categories is presented in Figure 11. 17 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents Figure 9 An overview of the distribution of atomic thought categories for LLM agents classified by GPT 4.1 prompting. 18 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents"
        },
        {
            "title": "Prompt Template For Response Taxonomy",
            "content": "You are helpful assistant in analyzing the response of an agent. Given the agents response text, your task goal is to: 1. Split the agents response into atomic statements and 2. categorize each atomic statement into an appropriate class. Here are specific task definition and examples for you to refer to. ### WHAT IS ATOMIC STATEMENT AND HOW TO GET IT### An atomic statement is semantically indivisible natural sentence statement, OR an item of well structured statement such as enumeration. For natural sentences , an atomic statement contains only one subject, one object, and one predicate. subordinate clause could be viewed as part of an atomic statement. Example 1 (natural sentence): \"I need to first list all product types to see what t-shirt products we have, then can get details about each t-shirt product to see the available options. should not make up information but use the available tools to provide accurate data.\" The atomic statements are: [\"I need to see what t-shirt products we have\" to see what t-shirt products we have.\", \"I can get details about each t-shirt product to see the available options.\" \"I should not make up information but use the available tools to provide accurate data.\"] Example 2 (natural sentence + well structured): \"We have T-Shirt product with multiple available options! Currently, there are **9 different t-shirt choices available** in our online store. Here are the available options: 1. **Blue, Medium, Cotton, Crew Neck** - $50.88 2. **Purple, XL, Cotton, Crew Neck** - $49.67 3. **Red, XXL, Cotton, Crew Neck** - $46.85 4. **Black, XXL, Polyester, V-Neck** - $49.52 5. **Black, Small, Polyester, Crew Neck** - $52.88 6. **Purple, Small, Polyester, V-Neck** - $53.48 7. **Blue, Small, Cotton, V-Neck** - $53.43 8. **Black, XXL, Cotton, Crew Neck** - $53.27 9. **Red, Large, Cotton, V-Neck** - $46.66\" Figure 10 Prompt template for response taxonomy (1/5). 19 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents The atomic statements are: [\"We have T-Shirt product with multiple available options!\", \"Currently, there are **9 different t-shirt choices available** in our online store.\", \"Here are the available options.\", \"1. **Blue, Medium, Cotton, Crew Neck** - $50.88\", \"2. **Purple, XL, Cotton, Crew Neck** - $49.67\", \"3. **Red, XXL, Cotton, Crew Neck** - $46.85\", \"4. **Black, XXL, Polyester, V-Neck** - $49.52\", \"5. **Black, Small, Polyester, Crew Neck** - $52.88\", \"6. **Purple, Small, Polyester, V-Neck** - $53.48\", \"7. **Blue, Small, Cotton, V-Neck** - $53.43\", \"8. **Black, XXL, Cotton, Crew Neck** - $53.27\", \"9. **Red, Large, Cotton, V-Neck** - $46.66\"] Figure 11 Prompt template for response taxonomy (2/5)."
        },
        {
            "title": "Info Disclosure",
            "content": "User Engagement w/o think w/ think w/o think w/ think GPT-4.1 Human Expert 47.8 47.1 32.5 37.8 11.2 11.0 9.0 10. Table 7 Average number of response categories per trajectory annotated by GPT-4.1 and human expert. A.6. Human-in-the-loop Validation of Response Taxonomy To validate the effectiveness of the automatic response taxonomy pipeline, we conduct human-in-the-loop experiment of response taxonomy annotation. Ideally, if the automatic pipeline is valid, then we could observe similar number of response categories for human annotation and model annotation. Specifically, we select approximately 1,060 atomic responses from 20 trajectories (10 trajectories from 洧랦 -Retail and 10 trajectories from 洧랦 -Airline, with DeepSeek-V3.1 as target agent of investigation). For each scenario, half of the trajectories are from agents without thinking, while the other half are from agents with thinking. Next, we do the manual validation based on the random assignment criteria. As shown in Table 7, the human-in-the-loop validation result demonstrates an excellent consistency of annotation between the human expert and GPT-4.1. For both categories, we observe consistent drop in frequency per trajectory, as the frequency drops from 47.1 to 37.8 with the human experts annotation. This result demonstrates the effectiveness of use the automatic pipeline for efficient and reliable annotation in this study. Also, we observe that the human-in-the-loop annotation is time-consuming, taking 2h 10min to complete for 20 trajectories. On average, each trajectory takes 6 min 30 for human expert to annotate. A.7. Other Cases of Agent Trajectories In the case study, we also study task cases in 洧랦 -Airline and TS-Phone, respectively. Both scenarios are marked by the partially observed environment and dynamic user interaction. Case study of 洧랦 -Airline. Figure 16, 17 and 18 show summary of the task case, trajectory snapshots of the agent with thinking and the agent without thinking, respectively. The key challenge of this task is that 20 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents Example 3 (multiple parallel questions): \"Is there specific type of t-shirt you re looking for, or would you like information about any of these options?\" The atomic statements are: [\"Is there specific type of t-shirt youre looking for?\", \"Would you like information about any of these options?\"] ### WHAT IS THE CATEGORIZATION OF ATOMIC STATEMENTS AND HOW TO GET IT ### The categorization of atomic statements has hierarchical structure. Any atomic statement belongs to one specific category. Here is the specific defintion of the hierarchical categorization: 1. Information disclosure: The agents atomic statement discloses existing information and does not require user to take any action. - 1.1 Observation Summarization: The agents atomic statement retells its previous observation (and future plan). EXAMPLE: \"We have T-Shirt product with multiple available options!\" - 1.2 Choice Enumeration: The agents atomic statement enumerates possible choices in well-structured form, usually starts with number or bullet. EXAMPLE: \"2. **Blue, Medium, Cotton, Crew Neck** - $50.88\" 2. User engagement request: The agents atomic statement requires the user to take one or more action. The statement could be question or an instruction. - 2.1 Close-ended inquiry: The agents atomic statement requires the user to take action(s) from limited space, such as select choice, from its previously listed contents. you must look at if feasible user actions within the statement depends on the context. EXAMPLE: \"Is there specific type of t-shirt youre looking for\" Note: to decide whether user-involved state is close-ended, - 2.2 Open-ended inquiry: The agents atomic statement requires the user to take action(s) from an unlimited space. Although sometimes the semantic of the statement depends on the context, but the user action space is always unlimited. EAMPLE: \"Would you prefer to look at other products?\" Figure 12 Prompt template for response taxonomy (3/5). 21 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents ### TASK OUTPUT FORMAT ### Your output format should be json and strictly follow the format here: json [ { }, ... ] \"atomic_response\": <The splitted atomic response text>, \"category\": <The category of the atomic response, could be observation_summarization/choice_enumeration/close_ended_inquiry/ open_ended_inquiry> For example, given the original response: \"I found some one-stop options that depart later in the day. Here are the available flights from New York (JFK) to Seattle (SEA) with departures after 11:00 AM EST: **Option 1:** - **First leg:** Flight HAT136 (JFK -> ATL) - Departs 7:00 PM, arrives 9:30 PM - **Second leg:** Flight HAT039 (ATL -> SEA) - Departs 10:00 PM, arrives 3:00 AM next day - **Total economy price:** $ **Option 2:** - **First leg:** Flight HAT218 (JFK -> ATL) - Departs 6:00 PM, arrives 8:30 PM - **Second leg:** Flight HAT039 (ATL -> SEA) - Departs 10:00 PM, arrives 3:00 AM next day - **Total economy price:** $261 Would you like to proceed with one of these options, or would you prefer to look at other dates?\" Figure 13 Prompt template for response taxonomy (4/5). 22 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents Your output should be: json [ { }, { }, { \"atomic_response\": \"I found some one-stop options that depart later in the day.\", \"category\": \"observation_summarization\" \"atomic_response\": \"Here are the available flights from New York (JFK) to Seattle (SEA) with departures after 11:00 AM EST.\", \"category\": \"observation_summarization\" \"atomic_response\": \"**Option 1:** - **First leg:** Flight HAT136 (JFK -> ATL) - Departs 7:00 PM, arrives 9:30 PM - **Second leg:** Flight HAT039 (ATL -> SEA) - Departs 10:00 PM, arrives 3:00 AM next day - **Total economy price:** $255\", \"category\": \"choice_enumeration\" }, { \"atomic_response\": \"**Option 2:** - **First leg:** Flight HAT218 (JFK -> ATL) - Departs 6:00 PM, arrives 8:30 PM - **Second leg:** Flight HAT039 (ATL -> SEA) - Departs 10:00 PM, arrives 3:00 AM next day - **Total economy price:** $261\", \"category\": \"choice_enumeration\" }, { }, { } ] \"atomic_response\": \"Would you like to proceed with one of these options ?\", \"category\": \"close_ended_inquiry\" \"atomic_response\": \"Or would you prefer to look at other dates?\", \"category\": \"open_ended_inquiry\" ### TASK BEGINS ### Now, please finish your task. Here is the agents response: Figure 14 Prompt template for response taxonomy (5/5). 23 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents the users initial returning flight from IAH to LAS has already been nonstop, but the user doesnt remember. Therefore, the agent needs to disclose this information to the user to prevent unnecessary flight bookings. As shown in Figure 17, although the thinking agent found available non-stop flights for both outbound and return, it fails to disclose existing flight booking information to the user. This misleads the user to unnecessarily update the return flight to HAT131, causing task failure. Case study of TS-Phone. Figure 19 presents the case of TS-Phone. The key challenge of this task lies in the semantic gap between the ground truth (the time length until Christmas day) and the pre-given user initial request (what is the date of Christmas day). The agent must actively interact with the user to propel the user to express the accurate intention. As shown in Figure 19, the agent with thinking indeed accurately answers the exact date of Christmas Day. However, it does not disclose any other information or any requests to the user. In practice, this introverted response pattern can reduce the interaction intention of the user, leading to task failure. Differently, the non-thinking agent not only accurately answers the date of Christmas Day, but also discloses its current situation and requests. e.g., it cannot look up the exact date at the moment because the Wi-Fi is disabled. Semantically, this more active response pattern encourages the user to actively respond to the agent, too, as shown in the difference between the tone of the user in the two trajectories. A.8. Details of InfoDis Experiment Vanilla Prompting. The vanilla prompt is the initial system message to tool-calling agents. For each scenario, the vanilla prompt consists of demonstration of the sceanrio, and the task policy. The task policy regulates the rules and constraints of agent actions in different situations. InfoDis Prompting. The information disclosure (InfoDis) prompting template is shown in Figure 15. To ensure fairness, we do not include any task-specific few-shot examples in the template. In practice, the InfoDis prompting is added to the end of the task policy prompt (vanilla prompt) of agents. Therefore, InfoDis is implemented as an add-on of user-engaged agents."
        },
        {
            "title": "InfoDis Prompting Template",
            "content": "{task_policy_prompt} ## Information Disclosure - You must interact with the user actively and disclose as much information as possible to ensure the user is well-informed about the current state of the database and any potential changes. Figure 15 The template of information disclosure (InfoDis) prompting. 24 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents"
        },
        {
            "title": "Task Summary",
            "content": "User Intention (visible to user, invisible to agent): Your user id is james_taylor_7043. You want to change your upcoming one-stop flight from LAS to IAH to nonstop flight. Your reservation ID is 1N99U6. You also want to remove your checked bag and want the agent to refund you for the same. Ground Truth (invisible to both): Change the existing one-stop flight from LAS to IAH (HAT284 & HAT152) to non-stop flight HAT266. Keep the original non-stop return flight HAT112 unchanged. Figure 16 summary of task case (洧랦 -Airline). The blue highlights the milestone goal and action. Trajectory of Agent w/ Thinking Agent Thought & Action: [Thought] The user wants to change their round-trip itinerary from LAS to IAH to nonstop flights. The current reservation is in economy class, with two passengers , travel insurance, and one checked bag. The outbound date is 2024-05-19 and the return is 2024-05-27. have found available nonstop flights for both segments. need to confirm with the user which specific nonstop flights they want for each segment, and then ask for payment or refund method for any fare difference before proceeding. [Response] found available nonstop flights for both your outbound (LAS to IAH on 2024-05-19) and return (IAH to LAS on 2024-05-27) segments. Could you please confirm which nonstop flights you would like to select for each segment from the options below? Outbound (LAS to IAH, 2024-05-19): 1. HAT175: Departs 17:00, Arrives 20:00, Economy seats available 2. HAT266: Departs 13:00, Arrives 16:00, Economy seats available Return (IAH to LAS, 2024-05-27): 1. HAT112: Departs 22:00, Arrives 01:00 (+1), Economy seats available 2. HAT131: Departs 18:00, Arrives 21:00, Economy seats available 3. HAT190: Departs 01:00, Arrives 04:00, Economy seats available 4. HAT286: Departs 22:00, Arrives 01:00 (+1), Economy seats available Failure: flight is already non-stop) to the user. the agent does not disclose existing flight status (the original return Please let me know your preferred flights for both segments, and will then provide the fare difference and request your payment or refund method before proceeding. User Response: For the outbound flight, Id like to choose HAT266 departing at 13:00. For the return, Ill take HAT131 departing at 18:00. Figure 17 Failed trajectory of agent with thinking. The red highlights the failure path. Red text labels the failure pattern. Missing information disclosure misleads the user to make wrong decision. 25 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents Trajectory of Agent w/o Thinking Agent Action: [Response] Here are the details of your current reservation: - Trip type: Round trip - Origin: LAS (Las Vegas) - Destination: IAH (Houston) - Current flights: - LAS to PHX (HAT284), PHX to IAH (HAT152) on 2024-05-19 - IAH to LAS (HAT112) on 2024-05-27 - Cabin: Economy - Passengers: James Taylor, Ivan Taylor - Insurance: Yes You want to change your outbound flight (LAS to IAH on 2024-05-19) to nonstop flight. The return flight (IAH to LAS on 2024-05-27) will remain unchanged. Would you like to keep the same cabin class (economy) for the new nonstop flight, or do you want to change the cabin class as well? Also, please confirm which payment method you would like to use for any fare difference or refund (gift card or credit card from your profile). Users Immediate Response: Id like to keep the economy cabin class, and please use my credit card for any payment or refund. Users Response After Agents Flight Retrieval: Id like to book HAT266 for both passengers. Also, want to remove my checked bag from this reservation and get refund for it. Figure 18 Successful trajectory of agent without thinking. The blue highlights pivotal information disclosure. 26 Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents"
        },
        {
            "title": "Task Summary",
            "content": "User Intention (visible to user, invisible to agent): Search how many days it is till Christmas Day. Christmas Day is 12/25/2025. Do not leak this information. Ground Truth (invisible to both): It is 52 days till Christmas Day. Trajectory of Agent w/ Thinking Trajectory of Agent w/o Thinking User Initial Request (fixed, pre-given by task setting): Christmas Day when? Static User Initial Request: Christmas Day when? Agent Thought & Action: Agent Action: [Thought] Christmas Day is fixed-date holiday, celebrated on December 25 each year . The user might be asking for the date in this year, so Ill need to confirm the current year to provide an exact answer. [Response] Christmas Day will be on December 25, 2025. Failure: information of current status, which fails to encourage the user to express the deeper request. the agent does not disclose any User Response: Ok, thanks. Agent Thought & Action: [Thought] The user thanked me for providing the Christmas Day date, so the task is complete. No further action is needed unless the user requests more information. [Response] Youre welcome! If you need help with any other dates or events, just let me know. [Response] It looks like my Wi-Fi is currently disabled, so cant look up the exact date at the moment. However, Christmas Day is celebrated every year on December 25th. Would you like me to enable Wi -Fi and check the specific date for this year? User Response: Yeah, go ahead and turn Wi-Fi on and check how many days until Christmas Day. Agent Action: [Response] There are 52 days and about 23 hours until Christmas Day (December 25th) this year. If you need countdown or reminder, let me know! Figure 19 The red highlights the failure path. Red text labels the failure pattern. The blue highlights pivotal information disclosure."
        }
    ],
    "affiliations": [
        "University of Wisconsin-Madison",
        "William & Mary"
    ]
}