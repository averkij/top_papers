{
    "paper_title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch",
    "authors": [
        "Anton Ehrmanntraut",
        "Julia Wunderle",
        "Jan Pfister",
        "Fotis Jannidis",
        "Andreas Hotho"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the prominence of decoder-only language models, encoders remain crucial for resource-constrained applications. We introduce ModernGBERT (134M, 1B), a fully transparent family of German encoder models trained from scratch, incorporating architectural innovations from ModernBERT. To evaluate the practical trade-offs of training encoders from scratch, we also present LL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German decoder-only models via LLM2Vec. We benchmark all models on natural language understanding, text embedding, and long-context reasoning tasks, enabling a controlled comparison between dedicated encoders and converted decoders. Our results show that ModernGBERT 1B outperforms prior state-of-the-art German encoders as well as encoders adapted via LLM2Vec, with regard to performance and parameter-efficiency. All models, training data, checkpoints and code are publicly available, advancing the German NLP ecosystem with transparent, high-performance encoder models."
        },
        {
            "title": "Start",
            "content": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch Anton Ehrmanntraut1,3 and Julia Wunderle1,4 and Jan Pfister4 Fotis Jannidis2,3 and Andreas Hotho2,4 Computer Philology and History of Contemporary German Literature3 Data Science4 CAIDAS Center for Artificial Intelligence and Data Science JMU Julius-Maximilians-Universität Würzburg {firstname.lastname}@uni-wuerzburg.de3 {lastname}@informatik.uni-wuerzburg.de4 5 2 0 2 9 1 ] . [ 1 6 3 1 3 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite the prominence of decoder-only language models, encoders remain crucial for resource-constrained applications. We introduce ModernGBERT (134M, 1B), fully transparent family of German encoder models trained from scratch, incorporating architectural innovations from ModernBERT. To evaluate the practical trade-offs of training encoders from scratch, we also present LLäMmlein2Vec (120M, 1B, 7B), family of encoders derived from German decoder-only models via LLM2Vec. We benchmark all models on natural language understanding, text embedding, and long-context reasoning tasks, enabling controlled comparison between dedicated encoders and converted decoders. Our results show that ModernGBERT 1B outperforms prior state-of-the-art German encoders as well as encoders adapted via LLM2Vec, with regard to performance and parameter-efficiency. All models, training data, checkpoints and code are publicly available5, advancing the German NLP ecosystem with transparent, highperformance encoder models."
        },
        {
            "title": "Introduction",
            "content": "Despite the recent dominance of decoder-only large language models (LLMs), parameter-efficient encoder models remain crucial for language technology, particularly for local deployments such as retrieval-augmented generation (RAG). Their bidirectional attention confers strong understanding capabilities with lower resource requirements, making them attractive for consumer hardware. In the German NLP landscape, GBERTLarge (337M parameters; Chan et al., 2020) remains popular encoder, performing competitively with much larger German-capable decoder LLMs across tasks (Pfister and Hotho, 2024), despite its modest size and 1,2 These authors contributed equally to this work. 5ModernGBERT and LLäMmlein2Vec, including all code, data and intermediary checkpoints will be published upon acceptance under research-only RAIL license. Figure 1: Performance on SuperGLEBer benchmark. markers: encoders, markers: decoders. Dashed arrows: LLM2Vec conversion gains. Models of the same family are colored in the same color. limited training data (163 GB). More recently, ModernBERT (Warner et al., 2024) introduced several architectural improvements for English encoders, including enhanced relative positional embeddings and efficient attention patterns enabling long context processing. Building on this progress and inspired by the success of LLäMmlein (Pfister et al., 2024), family of German decoder-only LLMs transparently trained on approximately 6 TB of RedPajamaV2 (Weber et al., 2024) text, we introduce ModernGBERT family of fully open, highperformance German encoder models with 134M and 1B parameters. These models provide foundation to explore the impact of ModernBERTs architectural innovations on German encoder performance. They also allow us to investigate how parameter scaling influences model quality when trained on large-scale monolingual corpora. To better assess the practical utility and tradeoffs of training encoder models from scratch, we further present LLäMmlein2Vec encoders (120M, 1B, and 7B), derived from decoder-only models using LLM2Vec (BehnamGhader et al., 2024). Since all models are based on the same training datasets, this setup provides foundation for systematically analyzing the relationship between different architectures and training strategies. We extensively evaluate these models during and post training via: natural language understanding (SuperGLEBer, Pfister and Hotho, 2024), embedding performance (MTEB; Enevoldsen et al., 2025; Muennighoff et al., 2023; Wehrli et al., 2023), and long-context understanding (Question Answering Needle-in-a-Haystack). Our findings reveal: ModernGBERT 134M and 1B are highly competitive German encoders, scaling well with size (8,192 tokens), with 1B surpassing the previous SotA GBERTLarge. Our LLäMmlein2Vec 7B also outperforms GBERTLarge, though dedicated encoders still outperform converted models of similar size. Note: Throughout the paper, we highlight interesting findings and insights we gained during the process in little boxes like this one."
        },
        {
            "title": "2.1 Pre-training Dataset",
            "content": "We pre-trained ModernGBERT on the same data as LLäMmlein decoder models (Pfister et al., 2024), using the open-source RedPajamaV2 dataset (Weber et al., 2024).6 This dataset comprises German CommonCrawl snapshots from 20142023. As we intend to keep datasets constant between ModernGBERT and LLäMmlein, we follow LLäMmleins data pipeline and select the higher quality document-level deduplicated head and middle partitions, excluding the lower quality tail partition. For our 134M model, we only selected the head partition. We used the same processing pipeline as Pfister et al. (2024): First, paragraphlevel deduplication using Bloom filter to remove redundant content like GDPR notices and web boilerplate, improving data diversity. Then, token-toword ratio filter to further improve text quality. The 6Common Crawl Foundation Terms of Use"
        },
        {
            "title": "Dataset",
            "content": "# tokens # sequences LONG-Head (ext1) LONG-Head/Middle (ext1) 52B 90B 6,813,019 11,785,941 HQ (ext2) Fineweb2 OpenLegalData Wikipedia Fineweb2-long 14.4B 43,191,271 7,640M 42,319,173 53,798 19,004 799,296 407M 143M 6,211M median length 7,755 8, 199 194 7,583 7,515 7,902 Table 1: Composition of the context extension datasets. final dataset is approximately 6 TB, consisting of 2 TB from head and 4 TB from middle. Using GBERTLarge tokenizer, this results in about 1.27T tokens."
        },
        {
            "title": "2.2 Context Extension Dataset",
            "content": "ModernBERT enhances its context capacity from 1,024 to 8,192 by finetuning in two phases: on 250B-token subsample of 8,192-token sequences from its original pre-training dataset (ext1), followed by curated 50B-token dataset with mixed sequence lengths, including short and long ones (ext2, up to 8,192 tokens) (Gao et al., 2025). Following this setup, we proceed to construct our own German context extension datasets for the two phases  (Table 1)  : for the first phase (ext1), we take the same approach and subsample long sequences from our pre-training datasets (resulting in LONG-Head from the head partition for our 134M model, and LONG-Head/Middle from the head and middle partition for our 1B model). For the second phase (ext2) on high-quality dataset we coin HQ in Table 1, we use the German portion of the Fineweb2 dataset (Penedo et al., 2024)7. Aiming for similar distribution, we first take randomized sample of Fineweb2, and added separate sample from Fineweb2, selecting long documents with 8,192 tokens, splitting them into sequences of 8,192 tokens (Fineweb2-long). Furthermore, we add additional long documents by including the 2023 German Wikipedia8 and the 2022 OpenLegalData dump,9 also split to sequences of up to 8,192 tokens. The entire HQ dataset consists of 14.4B tokens. Table 1 summarizes these three resulting datasets. 7Open Data Commons Attribution License (ODC-By) v1.0 8Creative Commons Attribution-ShareAlike 3.0 9https://de.openlegaldata.io/; database licensed under Open Database License (ODbL v1.0), cases are exempt from copyright law"
        },
        {
            "title": "3.1 ModernGBERT: Scaling SotA Encoders",
            "content": "The ModernGBERT models adapt the ModernBERT architecture and training strategy for German. ModernGBERT 134M matches the base ModernBERT model size (22 layers, 768 hidden units, but 16M fewer parameters due to smaller vocabulary size), while ModernGBERT 1B consists of 28 layers and hidden size of 2,048. Full architectural details can be found in Table 4. Both models follow the ModernBERT pretraining recipe: masked language modeling (MLM) with no next-sentence-prediction, 30% masking rate, and sequences up to 1,024 tokens (10,000 RoPE theta). For training, we use our German pre-training corpus (Section 2.1): ModernGBERT 1B trains on the head then middle partitions for total of 1.27T tokens; ModernGBERT 134M is trained only on the head partition (0.47T tokens), as downstream evaluation showed early saturation (Section 5.1). Details of the training procedure are shown in Table 5. After MLM, we extend context length in two phases, following ModernBERT, raising the RoPE theta to 160,000 and training on longer sequences. In the first extension phase (ext1), models are trained on the LONG-Head for the 134M model or LONG-Head/Middle for the 1B model. In ext2, both models are trained on the HQ dataset. As we did not intend to develop novel German tokenizer, we utilized the original BERT-style tokenizer from GBERTLarge (resulting in 31,168word embedding layer - multiple of 64 for compute efficiency). While LLäMmlein (Pfister et al., 2024) provides dedicated German Llama-style tokenizer, our preliminary ablations consistently showed degraded downstream performance. This degradation is consistent with results by Warner et al. (2024), who observed similar behavior with (English) Llama-style tokenizers during the development of the original ModernBERT. We therefore retained the GBERTLarge tokenizer. Throughout the training checkpoints are saved and evaluated, and all are released publicly to support further research. In addition, inspired by Pythia (Biderman et al., 2023) we provide full training provenance by logging and releasing the order of data points seen during training; thus, all checkpoints can be linked with the exact data points seen up to that checkpoint. LLM2Vec (BehnamGhader et al., 2024) proposes method to convert decoder-only LLMs into effective text encoders through the following steps: First, the causal attention mask is replaced with full attention mask, enabling bidirectional attention across tokens. Second, the model is trained using masked next token prediction (MNTP) objective. Third, unsupervised contrastive learning (SimCSE) is applied, improving embedding quality by maximizing agreement between differently dropped-out versions of the same input. However, we intend to remain closely aligned with ModernGBERTs training objectives and not reproduce LLM2Vec results. For this reason, we trained our models exclusively using the MNTP objective, which is most similar to MLM. ModernGBERT performs two context extension phases, each using two datasets per model. Similarly, we train all three LLäMmlein models using the same respective two datasets as employed by ModernGBERTs context extensions (Section 2.2): the LLäMmlein2Vec 120M model variant follows ModernGBERT 134M (LONG-Head for phase the LLäMmlein2Vec one, HQ for phase two); 1B and 7B models follow ModernGBERT 1B (LONG-Head/Middle for phase one, HQ tokens for phase two). For each model, we apply MNTP training separately on each respective dataset, resulting in two distinct adapter modulesone per phase. We evaluate both individual adapters (ext1 & ext2) as well as merged model (ext1+2) where both adapters are combined. Notably, the models achieve comparable results even without seeing the full training data as exemplary shown in Table 7 - this is comparable to the observations in Pfister et al. (2024), indicating possibilities of reducing compute in future trainings. However, for consistency and comparability, we report results using the fully trained models throughout the paper10. We also increased the sequence length to 8,192 and set RoPE theta to 160,000, otherwise, we follow the default LLM2Vec parameters (see Table 6 for more details). 10except for the LLäMmlein2Vec 7B trained on the LONGHead/Middle model, which we trained on 64 nodes with 4 H200 each for 14 hours, before stopping the training due to compute constraints  (Table 6)"
        },
        {
            "title": "4.1 SuperGLEBer",
            "content": "We assess our final models using the German SuperGLEBer benchmark (Pfister and Hotho, 2024), which includes 29 tasks across text classification, sequence tagging, question answering, and sentence similarity. These tasks cover diverse domains such as news, legal texts, and consumer reviews. For each task, models are fine-tuned with QLoRA (Dettmers et al., 2023) by default, or LoRA as fallback. In addition to evaluating final checkpoints, we follow LLäMmlein (Pfister et al., 2024) and evaluate intermediate checkpoints on the same representative SuperGLEBer subset as selected by the classification tasks NLI (ConPfister et al.: neau et al., 2018), FactClaiming Comments (Risch et al., 2021), DB Aspect (Wojatzki et al., 2017), and WebCAGe (Henrich et al., 2012), the sequence tagging task EuroParl (Faruqui and Padó, 2010), and the sentence similarity task PAWSX (Liang et al., 2020)."
        },
        {
            "title": "4.2 Massive Text Embedding Benchmark",
            "content": "We further evaluate the models on the German subset of the Massive Text Embedding Benchmark MTEB(deu,v1) (Enevoldsen et al., 2025). The specific tasks can be found in Table 8. In addition to text pair classification and semantic textual similarityalready covered by the SuperGLEBer benchmarkMTEB includes clustering (Wehrli et al., 2023), as well as reranking and retrieval tasks. These latter tasks provide more comprehensive assessment of general-purpose sentence embeddings, focusing on the models ability to produce robust semantic representations. To adapt the base models for embedding tasks, we fine-tune them using the Sentence-Transformer framework (Reimers and Gurevych, 2019) in supervised setup. Fine-tuning employs 10,000 samples from the German portion of the machinetranslated multilingual mMARCO passage ranking dataset (Bonifacio et al., 2022), maximizing similarity between query and positive passages, while minimizing similarity to negative passages. Sentence embeddings are obtained by mean pooling over the final token representations. We use InfoNCE loss with batch size of 128 and learning rate of 5 105. We apply QLoRA for efficient training (falling back to LoRA for the GBERT family, where quantization is not supported)."
        },
        {
            "title": "4.3 Long-Context Understanding",
            "content": "Evaluating long-context capabilities in German is hindered by the scarcity of native high-quality datasets, with translations from English often introducing artifacts. To address this, we construct QuestionAnswering Needle-In-a-Haystack (QA-NIAH) evaluation (Ivgi et al., 2023; Hsieh et al., 2024) based on the human-annotated GermanQuAD dataset (Möller et al., 2021). Given question, the goal is to extract the answer span from long document. We adapt GermanQuAD to QA-NIAH setup as follows: for each questionparagraph (needle) pair, we sample up to 3 distractor paragraphs and shuffle them with the needle, forming haystack document of up to 1,024 tokens. The answer always appears only in the needle paragraph. For evaluation, we increase distractors to up to 20, yielding documents up to 8,192 tokens. This yields 11,518 training and 2,204 test questionhaystack pairs. We fine-tune models on the QA-NIAH training set using QLoRA, and evaluate on the test set, following the SuperGLEBer benchmark procedure. Training uses sequences up to 1,024 tokens; evaluation uses up to 8,192 tokens, assessing generalization to longer contexts."
        },
        {
            "title": "5 Evaluation Results",
            "content": "5."
        },
        {
            "title": "Intermediate Model Evaluation",
            "content": "To track pre-training progress, we evaluated intermediate checkpoints on six representative SuperGLEBer tasks, following Pfister et al. (2024) (see Section 4.1). All checkpoints will be released for future analysis. Figure 3 shows that ModernGBERT 1Bs average performance steadily improves throughout training, while ModernGBERT 134M quickly saturates. To quantify these trends, we evaluated the full SuperGLEBer suite on four checkpoints for ModernGBERT 1B and three for 134M (comparison done using Wilcoxon signed-rank tests): ModernGBERT 134M plateaued after 72B tokens (15% of data), with no further significant improvements. In contrast, ModernGBERT 1B showed significant gains over the same dataset portion (p < 0.0001), and additional gains during training on the middle partition (p < 0.00052). Performance then plateaus after 864B tokens (67% of entire pre-training dataset), with the SuperGLEBer score increasing only slightly from 0.777 to 0.791 despite 406B more tokens."
        },
        {
            "title": "Avg",
            "content": "Class. NER PAWSX QA"
        },
        {
            "title": "GeBERTaBase\nGeBERTaLarge\nGeBERTaXLarge",
            "content": "XLM-RoBERTaBase XLM-RoBERTaLarge XLM-RoBERTaXLarge 0.718 0.768 0.716 0.749 0.767 0.689 0.730 0.758 LLäMmlein 120M 0.676 LLäMmlein2Vec 120M 0.684 LLäMmlein 1B LLäMmlein2Vec 1B LLäMmlein 7B LLäMmlein2Vec 7B ModernGBERT 134M(cid:51) ModernGBERT 134M ModernGBERT 1B(cid:51) ModernGBERT 1B 0.733 0.762 0.747 0.787 0.730 0.749 0.800 0.808 0.723 0. 0.715 0.743 0.770 0.693 0.714 0.750 0.702 0.703 0.781 0.776 0.810 0.799 0.716 0. 0.806 0.812 0.786 0.799 0.778 0.791 0.807 0.754 0.787 0.802 0.712 0.741 0.773 0. 0.805 0.838 0.782 0.805 0.839 0.845 0.561 0.654 0.559 0.619 0.643 0.505 0.583 0. 0.477 0.472 0.548 0.615 0.524 0.670 0.589 0.612 0.681 0.699 0.803 0. 0.813 0.844 0.848 0.802 0.837 0.822 0.812 0.819 0.828 0.843 0.851 0.842 0.833 0. 0.874 0.876 Intermediate checkpoint evaluation of Figure 2: ModernGBERT 1B during pre-training, on the SuperGLEBer tasks: NLI (top) and PAWSX (bottom). To improve trend visibility, we adjusted the y-axis scale and plotted every second checkpoint. Analyzing on the six selected subtasks that were run on every checkpoint (Section 5.1), for the 134M variant, only PAWSX showed significant positive Spearman rank correlation between training token count and performance (r = 0.655; < 0.003), while the others did not. For 1B, all but EuroParl showed significant positive correlations (r > 0.57; < 0.00014). In particular, even though the aggregate score remains largely stable in the final third of the pre-training, on complex tasks such as NLI and PAWSX, we still see slight improvements with increased training (Figure 2). These saturation patterns, including per-task trends and overall performance plateaus, are consistent with findings by Pfister et al. (2024) for decoder models, and by Antoun et al. (2024) for their ModernBERT variant ModernCamemBERT (with 136M parameters) trained on French. Our results confirm that although small ModernBERT models saturate quickly, larger models benefit from additional data. Extrapolating the observed scaling behavior between ModernGBERT 134M and 1B, we hypothesize that training larger, 7B-sized encoder could make further use of an extensive monolingual datasets, improving performance beyond ModernGBERT 1B. Table 2: Performance comparison on SuperGLEBer. (cid:51) indicates ModernGBERT without context extension. Confirmation: Our findings corroborate Antoun et al. (2024) and Pfister et al. (2024): small ModernGBERT models reach saturation early, while scaling model and dataset size enables improvements."
        },
        {
            "title": "5.2 Final Model Evaluation",
            "content": "Natural Language Understanding We evaluate all final models on the full SuperGLEBer benchmark. Table 2 averages the scores per task type, while Table 7 provides more fine-grained scores. We compare our models to established encoders: GBERT (Chan et al., 2020), GeBERTa (Dada et al., 2023), and XLM-RoBERTa (Conneau et al., 2020; Goyal et al., 2021). Our ModernGBERT consistently outperforms comparable and larger models. The 134M variant achieves an average score of 0.749, surpassing all similar-sized baselines, includ- (0.718), XLM-RoBERTaBase ing GBERTBase (0.689), GeBERTaBase (0.716), and even XLMRoBERTaLarge (0.730), as well as LLäMmlein 1B (0.733). The ModernGBERT 1B variant achieves new state-of-the-art average score across the entire SuperGLEBer of 0.808, outperforming GBERTLarge (0.768) by 4% and beating the seven times larger LLäMmlein2Vec 7B (0.787). It leads in three of four evaluation categories, including classification (0.812), NER (0.845), and QA (0.876). Only on sentence similarity (0.699), our seven times larger LLäMmlein2Vec 7B achieves better results. ModernGBERT scales well, with performance improving for larger model sizes, again suggesting that scaling ModernBERT-style encoders can leverage large monolingual corpora effectively. In the SuperGLEBer setting, adding context extension improved ModernGBERTs average by 1.9% for the 134M model (from 0.730 to 0.749) and by 0.8% for the 1B variant (from 0.800 to 0.808). No large improvements were to be expected from our context extension, as SuperGLEBer tasks do not make use of long contexts. Adaptation via LLM2Vec yields consistent gains across models. Thus, our first LLM2Vec tuning (analogous to ext1, Section 2.2) showed the most prominent positive effect, while the second finetune using the ext2 datasets showed only marginal increase, or even decrease in performance. The same holds for mixture of the two LLM2Vec adapters (ext1+2). The LLäMmlein2Vec 7B achieves the strongest results among the LLM2Vec models (0.787). Conversion of LLäMmlein 120M, 1B, 7B improved the average score by 0.8%, 2.9%, and 4.0% respectively. This effect is especially pronounced in PAWSX, with scores increasing by up to 14.6% for LLäMmlein 7B and 6.7% for LLäMmlein 1B. Observation: LLM2Vec yields the best improvement on similarity-related tasks. Comparing the LLäMmlein2Vec with the ModernGBERT family, we find that on similarly sized models, ModernGBERT always outperforms the transformed decoders by large margin. Only the much larger LLäMmlein2Vec 7B approaches the performance of ModernGBERT 1B. Observation: With similar data and model sizes, training encoders from scratch outperforms LLM2Vec converted models. Text Embedding We evaluate models on the MTEB benchmark, which covers six task categories: classification, pair classification, clustering, reranking, retrieval, and short text similarity (STS) tasks. While Table 3 summarizes the outcomes, all results are presented in Table 9. In general, supervised fine-tuning on mMARCO yields consistent improvements across all model types. While classification performance sometimes declines, substantial gains can be observed in other areas: 25% on average for reranking, 26% for retrieval and 25% for STS."
        },
        {
            "title": "GBERTBase\nGBERTBase",
            "content": ""
        },
        {
            "title": "GBERTLarge\nGBERTLarge",
            "content": "XLM-RoBERTaBase XLM-RoBERTaBase XLM-RoBERTaLarge XLM-RoBERTaLarge XLM-RoBERTaXLarge XLM-RoBERTaXLarge LLäMmlein2Vec 120M LLäMmlein2Vec 120M LLäMmlein2Vec 1B LLäMmlein2Vec 1B LLäMmlein2Vec 7B LLäMmlein2Vec 7B ModernGBERT 134M(cid:51) ModernGBERT 134M(cid:51) ModernGBERT 134M ModernGBERT 134M ModernGBERT 1B(cid:51) ModernGBERT 1B(cid:51) ModernGBERT 1B ModernGBERT 1B"
        },
        {
            "title": "Clustering Reranking Retrieval",
            "content": "0.360 0.500 0.412 0.521 0.248 0.403 0.264 0.460 0.301 0.479 0.315 0. 0.399 0.540 0.376 0.557 0.383 0.485 0.376 0.501 0.374 0.549 0.366 0. 0.274 0.318 0.336 0.334 0.173 0.247 0.172 0.259 0.225 0.342 0.261 0. 0.308 0.343 0.249 0.339 0.293 0.303 0.296 0.312 0.318 0.339 0.307 0. 0.118 0.374 0.206 0.389 0.024 0.247 0.048 0.343 0.090 0.362 0.139 0. 0.183 0.433 0.169 0.477 0.139 0.364 0.120 0.404 0.097 0.463 0.088 0. 0.226 0.461 0.297 0.493 0.008 0.299 0.026 0.416 0.142 0.407 0.224 0. 0.276 0.511 0.266 0.522 0.241 0.432 0.213 0.446 0.199 0.511 0.191 0. Table 3: Performance comparison on MTEB. Avg refers to the average over all six task groups, not only the ones shown here. (cid:51) indicates ModernGBERT without context extension, while marks the variant with additional training. Observation: Fine-tuning yields the largest gains in reranking, retrieval, and STS tasks. The best overall average performance is achieved by the fine-tuned LLäMmlein2Vec 7B (0.557), closely followed by the fine-tuned ModernGBERT 1B (0.551), despite the latter being significantly smaller. LLäMmlein2Vec models generally show strong performance after fine-tuning, particularly when trained with the extension dataset of the first phase (ext1). Using the second extension phase (ext2) or combining both adapters into the base model (ext1+2) harms the performance. Interestingly, the latter shows the largest fine-tuning gains among the three variants. The ModernGBERT models perform competitively to similarly sized models. Before finetuning, ModernGBERT 1B (avg. 0.366) already outperforms most encoder-only models, such as GeBERTaXLarge (0.325) or XLM-RoBERTaXLarge (0.301), but not GBERTLarge (0.412). However, after fine-tuning, it demonstrates clear superiority among native encoder-only models by at least 3% on the average score. As with our observations on the SuperGLEBer benchmark, ModernGBERTs context extension did not show significant improvements here. Comparing ModernGBERT and LLäMmlein2Vec, we find that before fine-tuning, the LLäMmlein2Vec 1B and 7B models produce better representations than ModernGBERT 1B. However, after fine-tuning, ModernGBERT 1B surpasses the 1B variant of LLäMmlein2Vec on average and closely aligns with the larger 7B model. Long-Context Understanding Table 10 reports results on our German Question-Answering Needle-in-a-Haystack benchmark. Next to the overall accuracy, we also present accuracy on subsets of the test dataset, consisting only of short (<1,024), medium (1,024 to 4,095), resp. long (4,096 to 8,192) sequences. The evaluation focuses on LLMs supporting up to 8,192 tokens: ModernGBERT, the encoder-converted LLäMmlein2Vec, as well as their original decoder counterparts. Notably, LLäMmlein models were pre-trained with maximum context of 2,048 tokens. ModernGBERT 1B demonstrates strong long-context performance across all lengths, outperforming all encoders. The first extension phase during ModernGBERT training yielded strong improvements, increasing accuracy by approximately factor 3, but the final extension phase on the HQ dataset slightly decreased performance by few percentage points, particularly for the 134M variant. Regarding LLM2Vec, sufficiently long conversion improved long-context understanding. Conversion of LLäMmlein 120M and 1B decoders (with native context length of 2,048) improved accuracy by factor 1.3 resp. 2, both not as pronounced in comparison to the ModernGBERT encoders. For LLäMmlein2Vec 7B however (with LLM2Vec training on approximately half of our ext1 dataset), it decreased by 51%, with no correct answers on haystacks of >4,096 tokens. Given the intensive compute requirements, we did not explore further optimizations regarding context extension of the LLäMmlein2Vec 7B model. Observation: On small training datasets, LLM2Vec tuning limits the understanding of long-context samples. 5."
        },
        {
            "title": "Inference Efficiency",
            "content": "We evaluate inference efficiency across varying sequence lengths using four synthetic datasets, each containing 8,192 documents composed of random tokens. Following Warner et al. (2024), we created two datasets using fixed length sequences of either 512 or 8,192 tokens. The other two datasets feature normal distributed sequence lengths (either mean 256, variance 64; or mean 4,096, variance 1,024) to better simulate real-world conditions. Our ModernGBERT models adopt ModernBERTs unpadding approach: padding tokens are removed and sequences in batch are concatenated, allowing Flash Attention to handle variable-length attention masks. The computational equivalence is facilitated by carefully crafting an appropriate attention mask. In contrast, all other models rely on conventional padding. Table 11 summarizes our findings. Among smaller models (134M resp. 120M), ModernGBERT and LLäMmlein2Vec achieve comparable efficiency on fixed-length data, both only surpassed by GBERTBase and XLM-RoBERTaBase in terms of efficiency on short sequences. For the 1B variants, ModernGBERT consistently outperforms LLäMmlein2Vec 1B and 7B variations in inference speed, likely due to its architectural decisions optimized for efficiency, such as ensuring that weight matrices have dimension of multiples of 64, and are divisible into 128 256 block for efficient tiling on the GPU. Gains are most pronounced for variable-length datasets, where ModernGBERTs unpadding yields clear benefits: the 134M ModernGBERT is the most efficient model on variable length, and the 1B variant substantially outpaces its LLäMmlein2Vec counterpart. Furthermore, given the comparable task performance for e.g. ModernGBERT 1B and LLäMmlein2Vec 7B on MTEB (see Table 9), the ModernGBERT model is 10 times as fast on variable length long context documents. The same trend is even more pronounced for ModernGBERT 1B, compared to its 1B LLäMmlein2Vec counterpart, where LLäMmlein2Vec is consistently outperformed by it similarly sized ModernGBERT version, which on top is twice as efficient on these long documents. Observation: When considering the tradeoff between computational efficiency and downstream performance metrics, ModernGBERT consistently emerges as the optimal solutionfrequently outperforming LLäMmlein2Vec on both dimensions simultaneously."
        },
        {
            "title": "6 Related Work",
            "content": "Next-Generation Encoders Several recent efforts have extended ModernBERT to new languages and domains, including adaptations for French (Antoun et al., 2024) and Japanese (Sugiura et al., 2025). Concurrent to our work, several alternative encoder architectures have been proposed. Breton et al. (2025) introduced NeoBERT, an English encoder scaled to 250M, incorporating similar architectural innovations like ModernBERT, but scaling up layers rather than hidden dimension, switching from GeLU to SwiGLU activation, and using modified training scheme (Cosine scheduler, reduced masking). Their model surpasses ModernBERT-large on GLUE and MTEB with 100M fewer parameters, although its scalability with model size remains unexplored. Likewise, Boizard et al. (2025) recently presented EuroBERT (210M, 610M, 2.1B), multilingual encoder family featuring architectural changes similar to those of ModernBERT, but retaining some architectural details (RMSNorm layer normalization, SiLU activation function, Llama-style tokenizer) from the Llama family, resembling our LLäMmlein2Vec architecture. Antoun et al. (2025) compared French ModernBERT and DeBERTaV3 under controlled conditions, finding DeBERTaV3 to be superior on downstream tasks but significantly slower in training and inference. Tuning decoder-only LLMs into Encoders Few works have investigated converting decoder-only LLMs into encoders, besides LLM2Vec (Section 3.2). Recent studies predominantly address either distilling text embedders (Li and Li, 2024; Lee et al., 2025, 2024; Ma et al., 2025) or finetuning LLMs as bidirectional encoders for specific tasks (Li et al., 2023; Dukic and Snajder, 2024), with evaluation typically focused on English or multilingual settings. Concurrently, MAGNET (Khosla et al., 2025) was proposed for converting decoder LLMs into foundational encoders, similarly to LLM2Vec. Unlike LLM2Vec, MAGNET employs both bidirectional and causal attention and adds missing-span generation objective."
        },
        {
            "title": "7 Conclusion",
            "content": "We have demonstrated that both architectural advances in ModernBERT and the LLM2Vec decoder transformation method yield strong German encoder models. The proposed ModernGBERT family, especially the 1B variant, sets new state-ofthe-art for German encoders, outperforming previous models while remaining suitable for practical deployment as drop-in replacement for GBERT, capable of handing sequences of up to 8,192 tokens. Our learning dynamics analysis confirms that larger encoder architectures can effectively exploit terabyte-scale German monolingual corpora, with performance consistently improving with increased model size and data. These trends suggest that even larger encoder models could yield further gains, which we leave to future work. comparison of ModernGBERT, and LLäMmlein2Vec (derived from LLäMmlein) both based on the same dataset, shows that dedicated encoder training yields superior results, justifying its computational expense when parameter efficiency is essential. By releasing ModernGBERT, along with full training transparency, intermediate checkpoints, and detailed documentation, we aim to facilitate further development and understanding within the German LLM community."
        },
        {
            "title": "Acknowledgements",
            "content": "The authors gratefully acknowledge the scientific support and HPC resources provided by the Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich AlexanderUniversität Erlangen-Nürnberg (FAU) under the NHR project b233cb. NHR funding is provided by federal and Bavarian state authorities. NHR@FAU hardware is partially funded by the German Research Foundation (DFG) 440719683. Furthermore, we gratefully acknowledge the HPC resources provided by the JuliaV2 cluster at the Universität Würzburg (JMU), which was funded as DFG project as Forschungsgroßgerät nach Art 91b GG under INST 93/1145-1 FUGG. The project staff is partially funded by the DFG 529659926. The data science chair is part of the CAIDAS, the Center for Artificial Intelligence and Data Science, and is supported by the Bavarian High-Tech Agenda, which made this research possible. This publication was partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under the project LitBERT, project no. 529659926."
        },
        {
            "title": "Limitations",
            "content": "Despite the ModernGBERT models being notable advancement in the German NLP landscape, several limitations persist: 1) Monolingual focus. Although the focus on German is strength for this specific context, ModernGBERT is unable to utilize multilingual contexts or perform cross-lingual tasks, hindering applicability in some scenarios. 2) Limited coding capabilities. High-quality German resources for coding are rare, and no code is included in the training dataset. This restricts its capabilities in code retrieval applications. 3) Evaluation scope. While we rigorously evaluated our models on the German SuperGLEBer and MTEB benchmarks, these benchmarks are limited in their domain, and other domains such as literature, medical domains, or technical subjects were not tested. Furthermore, our benchmarks do not strictly probe for German factual knowledge, for instance, knowledge about German geography, or common German TV shows. 4) No custom tokenizer. We utilized the original BERT-style GBERT tokenizer due to its availability and persistent usage. However, we did not invest in developing custom tokenizer, like the BPE-style OLMo tokenizer used in ModernBERT. Consequently, ModernGBERTs tokenizer cannot, e.g., differentiate between various whitespace characters or encode emoji. 5) Evaluation of long-context understanding. Due to the absence of high-quality native German evaluation datasets, we had to rely on non-natural QA-NIAH sequences, only broadly testing for long context understanding. Contrast this with English benchmarks such as Bench-MC (Zhang et al., 2024) or LongBench-v2 (Bai et al., 2025), which include full novels along with questions that require attention to many information scattered throughout the novel. In future work, we plan on developing dedicated high-quality non-synthetic German long-context evaluation benchmark."
        },
        {
            "title": "References",
            "content": "Wissam Antoun, Francis Kulumba, Rian Touchent, Éric de la Clergerie, Benoît Sagot, and Djamé Seddah. 2024. Camembert 2.0: smarter french language model aged to perfection. Wissam Antoun, Benoît Sagot, and Djamé Seddah. 2025. ModernBERT or DeBERTaV3? examining architecture and data influence on transformer encoder models performance. ArXiv:2504.08716. Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2025. LongBench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. ArXiv:2412.15204. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. LLM2Vec: Large language models are secretly powerful text encoders. In First Conference on Language Modeling, Philadelphia, PA, USA. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, Usvsn Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. 2023. Pythia: suite for analyzing large language models across training and scaling. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 23972430. PMLR. Nicolas Boizard, Hippolyte Gisserot-Boukhlef, Duarte M. Alves, André Martins, Ayoub Hammal, Caio Corro, Céline Hudelot, Emmanuel Malherbe, Etienne Malaboeuf, Fanny Jourdan, Gabriel Hautreux, João Alves, Kevin El-Haddad, Manuel Faysse, Maxime Peyrard, Nuno M. Guerreiro, Patrick Fernandes, Ricardo Rei, and Pierre Colombo. 2025. EuroBERT: Scaling multilingual encoders for european languages. ArXiv:2503.05500. Hamed Bonab, Mohammad Aliannejadi, Ali Vardasbi, Evangelos Kanoulas, and James Allan. 2021. CrossIn Proceedings market product recommendation. of the 30th ACM International Conference on Information & Knowledge Management, page 110119, New York, NY, USA. Association for Computing Machinery. Luiz Bonifacio, Vitor Jeronymo, Hugo Queiroz Abonizio, Israel Campiotti, Marzieh Fadaee, Roberto Lotufo, and Rodrigo Nogueira. 2022. mMARCO: multilingual version of the MS MARCO passage ranking dataset. ArXiv:2108.13897. Lola Le Breton, Quentin Fournier, Mariam El Mezouar, and Sarath Chandar. 2025. NeoBERT: nextgeneration BERT. ArXiv:2502.19587. Daniel Cer, Mona Diab, Eneko Agirre, Iñigo LopezGazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and In Proceedings crosslingual focused evaluation. of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 114, Vancouver, Canada. Association for Computational Linguistics. Branden Chan, Stefan Schweter, and Timo Möller. 2020. Germans next language model. In Proceedings of the 28th International Conference on Computational Linguistics, pages 67886796, Barcelona, Spain (Online). International Committee on Computational Linguistics. Xi Chen, Ali Zeynali, Chico Camargo, Fabian Flöck, Devin Gaffney, Przemyslaw Grabowicz, Scott A. Hale, David Jurgens, and Mattia Samory. 2022. SemEval-2022 task 8: Multilingual news article similarity. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), pages 10941106, Seattle, United States. Association for Computational Linguistics. Aaron Chibb. 2022. German-english false friends in multilingual transformer models: An evaluation on robustness and word-to-word fine-tuning. huggingface:aari1995/false_friends_en_de. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440 8451, Online. Association for Computational Linguistics. Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating crosslingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 24752485, Brussels, Belgium. Association for Computational Linguistics. Amin Dada, Aokun Chen, Cheng Peng, Kaleb Smith, Ahmad Idrissi-Yaghir, Constantin Seibold, Jianning Li, Lars Heiliger, Christoph Friedrich, Daniel Truhn, Jan Egger, Jiang Bian, Jens Kleesiek, and Yonghui Wu. 2023. On the impact of cross-domain data on In Findings of the AsGerman language models. sociation for Computational Linguistics: EMNLP 2023, pages 1380113813, Singapore. Association for Computational Linguistics. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. QLoRA: Efficient finetuning of quantized llms. ArXiv:2305.14314. David Dukic and Jan Snajder. 2024. Looking right is sometimes right: Investigating the capabilities of decoder-only LLMs for sequence labeling. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1416814181, Bangkok, Thailand. Association for Computational Linguistics. Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzeminski, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystrøm, Roman Solomatin, Ömer Çagatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafał Poswiata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Björn Plüster, Jan Philipp Harries, Loïc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek Šuppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael Günther, Mengzhou Xia, Weijia Shi, Xing Han Lù, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, and Niklas Muennighoff. 2025. MMTEB: Massive multilingual text embedding benchmark. ArXiv:2502.13595. Manaal Faruqui and Sebastian Padó. 2010. Training and evaluating german named entity recognizer with semantic generalization. In Conference on Natural Language Processing. Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha Ranganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tur, and Prem Natarajan. 2023. MASSIVE: 1M-example multilingual natural language understanding dataset with 51 typologically-diverse languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 42774302, Toronto, Canada. Association for Computational Linguistics. Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. 2025. How to train long-context language models (effectively). ArXiv:2410.02660. Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, and Alexis Conneau. 2021. Larger-scale transformers for multilingual masked language modeling. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 2933, Online. Association for Computational Linguistics. Verena Henrich, Erhard Hinrichs, and Tatiana Vodolazova. 2012. WebCAGe web-harvested corpus annotated with GermaNet senses. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 387396, Avignon, France. Association for Computational Linguistics. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024. RULER: Whats the real context size of your long-context language models? ArXiv:2404.06654. Maor Ivgi, Uri Shaham, and Jonathan Berant. 2023. Efficient long-text understanding with short-text models. Transactions of the Association for Computational Linguistics, 11:284299. Xueguang Ma, Xi Victoria Lin, Barlas Oguz, Jimmy Lin, Wen tau Yih, and Xilun Chen. 2025. DRAMA: Diverse augmentation from large language models to smaller dense retrievers. ArXiv:2502.18460. Phillip Keung, Yichao Lu, György Szarvas, and Noah A. Smith. 2020. The multilingual amazon reviews corpus. ArXiv:2010.02573. Savya Khosla, Aditi Tiwari, Kushal Kafle, Simon Jenni, Handong Zhao, John Collomosse, and Jing Shi. 2025. MAGNET: Augmenting generative decoders with representation learning and infilling capabilities. ArXiv:2501.08648. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2025. NV-embed: Improved techniques for training llms as generalist embedding models. ArXiv:2405.17428. Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, MingWei Chang, and Iftekhar Naim. 2024. Gecko: Versatile text embeddings distilled from large language models. ArXiv:2403.20327. Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. 2021. MTOP: comprehensive multilingual task-oriented semantic parsing benchmark. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 29502962, Online. Association for Computational Linguistics. Xianming Li and Jing Li. 2024. BeLLM: Backward dependency enhanced large language model for sentence embeddings. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 792804, Mexico City, Mexico. Association for Computational Linguistics. Zongxi Li, Xianming Li, Yuzhang Liu, Haoran Xie, Jing Li, Fu lee Wang, Qing Li, and Xiaoqin Zhong. 2023. Label supervised LLaMA finetuning. ArXiv:2310.01208. Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming Zhou. 2020. XGLUE: new benchmark dataset for cross-lingual pre-training, understanding and generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 60086018, Online. Association for Computational Linguistics. Philip May. multilingual huggingface:PhilipMay/stsb_multi_mt."
        },
        {
            "title": "Machine\nbenchmark",
            "content": "2021. sts translated dataset. Timo Möller, Julian Risch, and Malte Pietsch. 2021. GermanQuAD and GermanDPR: Improving nonEnglish question answering and passage retrieval. In Proceedings of the 3rd Workshop on Machine Reading for Question Answering, pages 4250, Punta Cana, Dominican Republic. Association for Computational Linguistics. Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2023. MTEB: Massive text embedding benchmark. ArXiv:2210.07316. James ONeill, Polina Rozenshtein, Ryuichi Kiryo, Motoko Kubota, and Danushka Bollegala. 2021. wish would have loved this one, but didnt multilingual dataset for counterfactual detection in product reviews. ArXiv:2104.06893. Guilherme Penedo, Hynek Kydlíˇcek, Vinko Sabolˇcec, Bettina Messmer, Negar Foroutan, Martin Jaggi, Leandro von Werra, and Thomas Wolf. 2024. Fineweb2: sparkling update with 1000s of languages. Huggingface:HuggingFaceFW/fineweb-2. Jan Pfister and Andreas Hotho. 2024. SuperGLEBer: German language understanding evaluation benchIn Proceedings of the 2024 Conference of mark. the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 79047923, Mexico City, Mexico. Association for Computational Linguistics. Jan Pfister, Julia Wunderle, and Andreas Hotho. 2024. LLäMmlein: Compact and competitive german-only language models from scratch. ArXiv:2411.11171. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. Julian Risch, Anke Stoll, Lena Wilms, and Michael Wiegand. 2021. Overview of the GermEval 2021 shared task on the identification of toxic, engaging, and fact-claiming comments. In Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments, pages 112, Duesseldorf, Germany. Association for Computational Linguistics. Issa Sugiura, Kouta Nakayama, and Yusuke Oda. 2025. llm-jp-modernbert: ModernBERT model trained on large-scale japanese corpus with long context length. ArXiv:2504.15544. Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli. 2024. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. ArXiv:2412.13663. Maurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and Ce Zhang. 2024. RedPajama: an open dataset for training large language In Advances in Neural Information Promodels. cessing Systems, volume 37, pages 116462116492. Curran Associates, Inc. Silvan Wehrli, Bert Arnrich, and Christopher Irrgang. 2023. German text embedding clustering benchmark. In Proceedings of the 19th Conference on Natural Language Processing (KONVENS 2023), pages 187 201, Ingolstadt, Germany. Association for Computational Lingustics. Michael Wojatzki, Eugen Ruppert, Sarah Holschneider, Torsten Zesch, and Chris Biemann. 2017. Germeval 2017: Shared task on aspect-based sentiment in social media customer feedback. In Proceedings of the GermEval 2017 Shared Task on Aspect-based Sentiment in Social Media Customer Feedback, pages 112, Berlin, Germany. Marco Wrzalik and Dirk Krechel. 2021. GerDaLIR: German dataset for legal information retrieval. In Proceedings of the Natural Legal Language Processing Workshop 2021, pages 123128, Punta Cana, Dominican Republic. Association for Computational Linguistics. Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019. PAWS-X: cross-lingual adversarial dataset for paraphrase identification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 36873692, Hong Kong, China. Association for Computational Linguistics. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024. Bench: Extending long context evaluation beyond In Proceedings of the 62nd Annual 100K tokens. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15262 15277, Bangkok, Thailand. Association for Computational Linguistics. Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. 2023. MIRACL: multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11:11141131."
        },
        {
            "title": "Details",
            "content": "Table 4 provides an overview of the model architectures for the ModernGBERT (134M, 1B) and LLäMmlein2Vec (120M, 1B, 7B) model families. Detailed training settings regarding the pretraining phase, context extension phase one and two, for ModernGBERT are listed in Table 5, and those for LLäMmlein2Vec, covering MNTP training, can be found in Table 6."
        },
        {
            "title": "B Evaluation Results",
            "content": "In the following we present the full evaluation results on SuperGLEBer, MTEB, NIAH, and our efficiency benchmarks for German-capable encoder models, specifically our ModernGBERT (134M, 1B) and LLäMmlein2Vec (120M, 1B, 7B). B.1 SuperGLEBer In Figure 3 we illustrate the training progress, evaluating several intermediate checkpoints of ModernGBERT 134M and 1B. Notably, while the smaller model did not show significant improvements after approximately 15% of its training data, the 1B model improves performance until 67%. Table 7 presents the results on the full SuperGLEBer benchmark, comparing various Germancapable encoder models. Notably, ModernGBERT 1B sets new state of the art, surpassing the previously leading encoder model GBERTLarge as well as the seven times larger LLäMmlein2Vec 7B. Transforming the LLäMmlein decoders into encoders (in particular, the +ext1 variant) improves the average score and yields notable gains on similarity and sequence tagging tasks. B.2 Massive Text Embedding Benchmark (MTEB) all tasks summarize We included in the MTEB(deuv1) benchmark in Table 8 and report the corresponding results in Table 9. In addition to the base model outcomes, we also present scores for models after supervised"
        },
        {
            "title": "Parameters",
            "content": "134M 1B 120M 1B 7B"
        },
        {
            "title": "ModernGBERT",
            "content": "LLäMmlein2Vec"
        },
        {
            "title": "Vocabulary\nUnused Tokens\nLayers\nHidden Size\nTransformer Block\nActivation Function\nAttention Heads\nHead Size\nIntermediate Size\nNormalization\nNorm Epsilon\nRoPE theta\nGlobal Attention\nLocal Attention Window\nLocal Attn RoPE theta",
            "content": "31,168 66 22 768 Pre-Norm GeLU 12 64 1,152 LayerNorm 1 105 160,000 Every three layers 128 10,000 31,168 66 28 2,048 Pre-Norm GeLU 32 64 3,072 LayerNorm 1 105 160,000 Every three layers 128 10,000 32,064 54 32 4,096 Post-Norm SiLU 32 128 11,008 32,064 54 12 768 Post-Norm SiLU 12 64 2,048 32,064 54 22 2,048 Post-Norm SiLU 32 64 5,632 RMSNorm RMSNorm RMSNorm 1 105 160,000 Every layer 1 105 160,000 Every layer 1 105 160,000 Every layer Table 4: Model design of the ModernGBERT and LLäMmlein2Vec model family."
        },
        {
            "title": "Pretraining Phase",
            "content": "Context Extension: Phase One Context Extension: Phase Two 134M 0.47T 1B 1.27T 134M 52B 8,192 160,000 1B 90B 96 5 105 1 106 42.1 134M 1B 14.4B 8,192 160, 96 8 96 3 3 104 5 106 1-sqrt 12.8 109 1 105 1 106 2.0 8.3 96 8 3 104 1 105 5."
        },
        {
            "title": "Batch Size",
            "content": "Warmup (tokens)"
        },
        {
            "title": "Microbatch Size",
            "content": "Learning Rate Schedule Warmup (tokens) Decay (tokens)"
        },
        {
            "title": "Weight Decay",
            "content": "1,024 10,000 4,608 4,928 3 109 96 8 104 5 105 Trapezoidal 15 109 1 10 Training Time (hours) 31.3 446."
        },
        {
            "title": "Megatron Megatron",
            "content": "Dropout (attn out) Dropout (all other layers) 0.1 0."
        },
        {
            "title": "Optimizer\nBetas\nEpsilon",
            "content": "StableAdamW (0.90, 0.98) 1"
        },
        {
            "title": "Training Hardware\nTraining Strategy",
            "content": "16 H100 Distributed DataParallel, bfloat16 Table 5: ModernGBERT training settings. Dropout and below are shared across all phases. LLäMmlein2Vec 120M LLäMmlein2Vec 1B LLäMmlein2Vec 7B"
        },
        {
            "title": "Training Tokens\nMax Sequence Length\nRoPE theta",
            "content": "Ext1 52B Ext2 14.4B 8,192 160,000 Ext 90B Ext2 14.4B Ext1 90B Ext 14.4B 8,192 160,000 8,192 160,"
        },
        {
            "title": "Batch Size",
            "content": "32 32 32 32"
        },
        {
            "title": "Training Hardware\nTraining Duration",
            "content": "64 H200 64 H200 10h41 3h40 37h24 6h 256 H200 14h25 128 H200 9h39 Table 6: LLäMmlein2Vec training settings. Due to limited resources we had to terminate the 7B model training on the first extension dataset early ."
        },
        {
            "title": "Average\nmixed",
            "content": "8 1 7 0 . 8 6 7 0 . 0 4 7 0 . 6 1 7 0 . 9 4 7 0 . 7 6 7 0 . 9 8 6 0 . 0 3 7 0 . 8 5 7 0 . 0 1 7 0 . 4 4 7 0 . 6 7 6 0 . 0 8 6 0 . 4 8 6 0 . 5 7 6 0 . 7 2 6 0 . 3 3 7 0 . 2 6 7 0 . 2 6 7 0 . 4 4 7 0 . 7 1 7 0 . 7 4 7 0 . 7 8 7 0 . 3 8 7 0 . 0 4 7 0 . 0 3 7 0 . 9 4 7 0 . 0 0 8 0 . 8 0 8 0 . QA m. t. F1 3 0 8 . 2 3 8 0 . 2 3 8 0 . 3 1 8 . 4 4 8 0 . 8 4 8 0 . 2 0 8 . 7 3 8 0 . 2 2 8 0 . 5 3 8 . 8 6 8 0 . 2 1 8 0 . 1 2 8 . 9 1 8 0 . 8 1 8 0 . 0 3 7 . 8 2 8 0 . 2 4 8 0 . 3 4 8 . 6 3 8 0 . 8 2 8 0 . 1 5 8 . 2 4 8 0 . 2 4 8 0 . 1 2 8 . 3 3 8 0 . 6 3 8 0 . 4 7 8 . 6 7 8 0 . similarity pearson corr 1 6 5 0 . 4 5 6 0 . 3 0 6 0 . 9 5 5 0 . 9 1 6 0 . 3 4 6 0 . 5 0 5 0 . 3 8 5 0 . 6 5 6 0 . 1 5 5 0 . 3 7 5 0 . 7 7 4 0 . 1 7 4 0 . 2 7 4 0 . 5 5 4 0 . 9 3 4 0 . 8 4 5 0 . 5 1 6 0 . 5 1 6 0 . 0 8 5 0 . 7 5 5 0 . 4 2 5 0 . 0 7 6 0 . 7 5 6 0 . 9 3 6 0 . 9 8 5 0 . 1 2 6 0 . 1 8 6 0 . 9 9 6 0 . g n a fi a avg micro F1 6 8 7 . 9 9 7 0 . 9 8 7 0 . 8 7 7 . 1 9 7 0 . 7 0 8 0 . 4 5 7 . 7 8 7 0 . 2 0 8 0 . 6 1 7 . 4 4 7 0 . 2 1 7 0 . 9 2 7 . 1 4 7 0 . 0 3 7 0 . 3 8 6 . 3 7 7 0 . 1 1 8 0 . 2 1 8 . 4 0 8 0 . 4 8 7 0 . 5 0 8 . 8 3 8 0 . 7 3 8 0 . 3 9 7 . 2 8 7 0 . 5 0 8 0 . 9 3 8 . 5 4 8 0 . other micro F1 6 0 8 . 0 3 1 8 . 0 0 1 8 . 0 1 0 8 . 0 4 0 8 . 0 8 1 8 . 0 0 8 7 . 0 4 0 8 . 0 6 1 8 . 0 3 3 7 . 0 3 5 7 . 0 7 3 7 . 0 4 5 7 . 0 1 6 7 . 0 7 5 7 . 0 8 2 7 . 0 5 8 7 . 0 5 1 8 . 0 8 1 8 . 0 2 1 8 . 0 9 9 7 . 0 8 0 8 . 0 5 3 8 . 0 4 3 8 . 0 4 0 8 . 0 4 9 7 . 0 9 0 8 . 0 7 3 8 . 0 0 4 8 . 0 NER micro F1 5 0 7 0 . 4 4 7 0 . 7 0 7 0 . 7 8 6 0 . 6 3 7 0 . 3 6 7 0 . 8 4 6 0 . 0 2 7 0 . 6 4 7 0 . 8 4 6 0 . 8 0 7 0 . 3 1 6 0 . 7 2 6 0 . 0 6 6 0 . 0 2 6 0 . 5 0 5 0 . 8 2 7 0 . 5 9 7 0 . 0 9 7 0 . 5 7 7 0 . 7 2 7 0 . 6 9 7 0 . 1 5 8 0 . 8 4 8 0 . 7 4 7 0 . 4 3 7 0 . 1 9 7 0 . 9 4 8 0 . 8 6 8 0 . avg mixed 3 2 7 . 0 5 8 7 . 0 6 3 7 . 0 5 1 7 . 0 3 4 7 . 0 7 7 . 0 3 9 6 . 0 4 1 7 . 0 0 5 7 . 0 7 3 7 . 0 0 9 7 . 2 0 7 . 0 0 0 7 . 0 3 0 7 . 0 7 9 6 . 0 7 5 6 . 0 1 8 7 . 9 7 7 . 0 6 7 7 . 0 5 5 7 . 0 7 9 6 . 0 0 1 8 . 0 9 9 7 . 7 9 7 . 0 9 0 7 . 0 6 1 7 . 0 5 3 7 . 0 6 0 8 . 0 2 1 8 . other mixed 9 4 7 . 0 6 1 8 . 0 2 6 7 . 0 7 3 7 . 0 5 7 7 . 5 9 7 . 0 7 1 7 . 0 9 3 7 . 0 8 7 7 . 0 8 6 7 . 0 7 1 8 . 2 3 7 . 0 8 3 7 . 0 9 3 7 . 0 9 3 7 . 0 8 0 7 . 0 6 0 8 . 6 0 8 . 0 8 0 8 . 0 9 8 7 . 0 2 3 7 . 0 5 3 8 . 0 2 2 8 . 2 2 8 . 0 4 4 7 . 0 4 4 7 . 0 0 6 7 . 0 3 3 8 . 0 6 3 8 . WSD micro F1 4 1 8 . 0 7 3 8 . 0 4 0 8 . 0 9 8 7 . 0 5 9 7 . 0 3 8 . 0 0 9 7 . 0 4 1 8 . 0 0 2 8 . 0 8 0 8 . 0 9 4 8 . 8 9 7 . 0 3 9 7 . 0 7 9 7 . 0 2 7 7 . 0 9 4 7 . 0 9 3 8 . 0 2 8 . 0 5 3 8 . 0 5 1 8 . 0 7 8 7 . 0 3 7 8 . 0 9 3 8 . 8 4 8 . 0 6 8 7 . 0 0 8 7 . 0 6 0 8 . 0 8 5 8 . 0 6 7 8 . match ACC 8 3 7 . 0 0 1 8 . 0 8 8 7 . 0 6 6 7 . 0 3 8 7 . 6 9 7 . 0 1 4 7 . 0 5 9 7 . 0 5 1 8 . 0 8 6 7 . 0 9 1 8 . 0 1 7 . 0 6 9 6 . 0 1 1 7 . 0 6 8 6 . 0 1 8 5 . 0 0 9 7 . 7 2 8 . 0 5 7 7 . 0 4 5 7 . 0 2 9 6 . 0 1 2 8 . 0 3 1 8 . 8 0 8 . 0 0 1 6 . 0 9 6 7 . 0 9 7 7 . 0 7 2 8 . 0 6 2 8 . sent. micro F1 0 2 6 . 0 3 7 6 . 0 7 1 6 . 0 9 1 6 . 0 3 2 6 . 1 7 6 . 0 6 4 5 . 0 9 5 5 . 0 2 1 6 . 0 9 2 6 . 0 3 1 7 . 0 8 5 . 0 1 7 5 . 0 4 6 5 . 0 8 6 5 . 0 0 9 4 . 0 0 1 7 . 4 9 6 . 0 2 0 7 . 0 8 8 6 . 0 6 6 5 . 0 9 3 7 . 0 2 4 7 . 3 4 7 . 0 7 4 6 . 0 7 1 6 . 0 7 4 6 . 0 6 4 7 . 0 5 4 7 . tox. macro F1 7 3 5 . 0 4 0 6 . 0 1 6 5 . 0 0 3 5 . 0 1 5 5 . 6 0 6 . 0 0 3 5 . 0 2 1 5 . 0 8 5 5 . 0 9 1 5 . 0 6 8 5 . 0 1 5 . 0 0 8 4 . 0 1 9 4 . 0 1 7 4 . 0 8 4 4 . 0 3 0 6 . 7 8 5 . 0 5 7 5 . 0 8 2 5 . 0 3 0 5 . 0 2 3 6 . 0 3 3 6 . 2 1 6 . 0 4 5 5 . 0 3 0 5 . 0 6 2 5 . 0 0 1 6 . 0 5 3 6 . a l m t 1 1 1 7 3 5 3 1 9 3 1 6 0 4 7 8 8 9 7 2 1 6 8 4 . 3 1 8 0 2 1 0 2 1 0 2 0 2 1 0 2 1 1 1 1 1 7 7 7 7 B E M - a R R - a T o X - 2 . 3 l 1 . 3 l e ä ) 1 5 / 1 ( 2 l ä ) 2 + 1 ( 2 l ä ) 1 ( 2 l ä ) 2 ( 2 l ä ) 1 5 / 1 ( 2 l ä ) 2 + 1 ( 2 l ä ) 1 ( 2 l ä ) 2 ( 2 l ä n m ä ) 2 + 1 ( 2 l ä ) 1 ( 2 l ä ) 2 ( V 2 l ä i M ä s T e r T e r a B s R e L B 3 - u g 1 4 3 1 4 3 1 2 + 1 + B e M"
        },
        {
            "title": "T\nR\nE\nB\nG\nn\nr\ne\nd\no\nM",
            "content": "2 + 1 + B e M"
        },
        {
            "title": "T\nR\nE\nB\nG\nn\nr\ne\nd\no\nM",
            "content": "c n e c n c n e c e c n e c n c n e c n c n e c s a g v e a r n e m c T . ) 4 2 0 2 , o a s fi ( w o , r n g s e i v . 6 1 u i t a e s o e a e i . x e a e h w e k y h w o n e t d r a , u r L p : 7 a , y a e c e t e e h V 2 d u e a e s W . a o e e e c e e b n t - r a i c l m o f . o / i k s o E r M . ) 1 5 / 1 + ( d i r d / H a N o % 0 2 f Figure 3: Intermediate Checkpoint Evaluation. Note that the solid line represents the mean of the six tasks selected for the intermediate checkpoint evaluation (NLI, FactClaiming Comments, DB Aspect, WebCAGe, EuroParl, PAWSX Similarity). The box plots show the distribution of scores for those checkpoints evaluated across all 29 SuperGLEBer tasks. We compared each pair of these checkpoints using Wilcoxon signed-rank tests, and highlighted significant increases with brackets. Brackets of pairs without significant increases are not displayed. (Accordingly, all pairs of 134M checkpoints show no significant increase.) Four checkpoints failed to converge during fine-tuning on some task, leading to the visible outliers. Similar behavior has been observed by Antoun et al. (2025). training on the mMARCO dataset. Notably, the fine-tuned versions consistently outperform their base counterparts, with particularly strong improvements in reranking, retrieval, and s2s tasks. LLäMmlein2Vec 7B achieves the best results closely followed by ModernGBERT 1B. GBERT 1B performs strongly across sequence lengths, surpassed only by the eight-times larger LLaMA 3.2. For LLäMmlein 120M and 1B, MNTP training on the LONG-Head or LONGHead/Middle datasets improves performance on longer contexts. B.3 Needle-in-a-Haystack B.4 Efficiency The results on the Question-Answering Needle-ina-Haystack test are presented in Table 10. ModernFinally, we depict the outcomes of our model efficiency tests in Table 11. The smaller Modern-"
        },
        {
            "title": "Reference",
            "content": "ONeill et al. (2021) Keung et al. (2020) Li et al. (2021) Li et al. (2021) FitzGerald et al. (2023) FitzGerald et al. (2023)"
        },
        {
            "title": "PairClassification",
            "content": "FalseFriendsGermanEnglish Average Precision Chibb (2022) PawsXPairClassification Average Precision Yang et al. (2019)"
        },
        {
            "title": "STS",
            "content": "BlurbsClusteringP2P BlurbsClusteringS2S TenKGnadClusteringP2P TenKGnadClusteringS2S"
        },
        {
            "title": "MIRACLReranking",
            "content": "GermanQuAD-Retrieval GermanDPR Xmarket GerDaLIR GermanSTSBenchmark STS22 V-measure V-measure V-measure V-measure nDCG@10 MRR@5 DCG@10 nDCG@10 nDCG@"
        },
        {
            "title": "Spearman\nSpearman",
            "content": "Wehrli et al. (2023) Wehrli et al. (2023) Wehrli et al. (2023) Wehrli et al. (2023) Zhang et al. (2023) Möller et al. (2021) Möller et al. (2021) Bonab et al. (2021) Wrzalik and Krechel (2021) May (2021); Cer et al. (2017) Chen et al. (2022) Table 8: Overview of tasks included in the German MTEB(deu, v1) benchmark, grouped by six categories: classification, pairclassification, clustering, reranking, retrieval and STS. GBERT model is the most efficient on variablelength input, while the 1B variant substantially outperforms its LLäMmlein2Vec counterpart."
        },
        {
            "title": "Params",
            "content": "111M 0.634 0.632 337M 0.649 0.646 135M 0.623 0.620 139M 0.632 0.611 406M 0.642 0.618 887M 0.626 0. 279M 0.442 0.555 561M 0.510 0.576 3.48B 0.456 0.609 120M 0.546 120M 0.599 120M 0.457 120M 0. 120M 0.339 120M 0.517 1B 1B 1B 1B 1B 1B 7B 7B 7B 7B 7B 7B 0.641 0.670 0.617 0.666 0.337 0.647 0.683 0.687 0.679 0. 0.349 0.677 134M 0.639 0.602 134M 0.642 0.629 1B 1B 0.665 0. 0.659 0."
        },
        {
            "title": "A\nv\ne\nr\na\ng\ne",
            "content": "0.504 0.601 0.544 0.662 0.554 0.614 0.535 0.613 0.533 0.611 0.536 0. 0.506 0.529 0.510 0.574 0.519 0.564 0.529 0.575 0.525 0.588 0.530 0. 0.542 0.625 0.541 0.622 0.538 0.611 0.558 0.636 0.555 0.628 0.525 0. 0.537 0.606 0.536 0.612 0.544 0.641 0.540 0."
        },
        {
            "title": "A\nv\ne\nr\na\ng\ne",
            "content": "0.274 0.318 0.336 0.334 0.269 0.328 0.312 0.318 0.287 0.311 0.278 0. 0.173 0.247 0.172 0.259 0.225 0.342 0.261 0.308 0.202 0.295 0.098 0. 0.308 0.343 0.299 0.330 0.075 0.325 0.249 0.339 0.323 0.337 0.072 0. 0.293 0.303 0.296 0.312 0.318 0.339 0.307 0."
        },
        {
            "title": "A\nv\ne\nr\na\ng\ne",
            "content": "0.118 0.374 0.206 0.389 0.141 0.346 0.174 0.374 0.223 0.374 0.108 0. 0.024 0.247 0.048 0.343 0.090 0.362 0.139 0.325 0.118 0.305 0.046 0. 0.183 0.433 0.189 0.433 0.062 0.421 0.169 0.477 0.187 0.471 0.047 0. 0.139 0.364 0.120 0.404 0.097 0.463 0.088 0."
        },
        {
            "title": "R\ne\nt\nr\ni\ne\nv\na\nl",
            "content": "0.226 0.461 0.297 0.493 0.187 0.389 0.213 0.430 0.274 0.432 0.058 0. 0.008 0.299 0.026 0.416 0.142 0.407 0.224 0.425 0.117 0.339 0.009 0. 0.276 0.511 0.280 0.499 0.010 0.481 0.266 0.522 0.309 0.517 0.005 0. 0.241 0.432 0.213 0.446 0.199 0. 511 0.191 0."
        },
        {
            "title": "S\nT\nS",
            "content": "0.402 0.613 0.438 0.603 0.375 0.538 0.429 0.611 0.424 0.616 0.342 0. 0.333 0.539 0.320 0.593 0.372 0.590 0.188 0.592 0.205 0.498 0.107 0. 0.442 0.660 0.431 0.644 0.217 0.640 0.333 0.682 0.462 0.680 0.182 0. 0.449 0.602 0.445 0.606 0.418 0.681 0.410 0."
        },
        {
            "title": "Average",
            "content": "0.360 0.500 0.412 0.521 0.358 0.472 0.382 0.493 0.397 0.494 0.325 0. 0.248 0.403 0.264 0.460 0.301 0.479 0.315 0.471 0.271 0.439 0.188 0. 0.399 0.540 0.393 0.532 0.206 0.521 0.376 0.557 0.419 0.553 0.197 0. 0.383 0.485 0.376 0.501 0.374 0.549 0.366 0."
        },
        {
            "title": "GBERTBase\nGBERTBase",
            "content": ""
        },
        {
            "title": "GBERTLarge\nGBERTLarge",
            "content": "gerturax-3 gerturax-"
        },
        {
            "title": "GeBERTaBase\nGeBERTaBase",
            "content": ""
        },
        {
            "title": "GeBERTaLarge\nGeBERTaLarge",
            "content": ""
        },
        {
            "title": "GeBERTaXLarge\nGeBERTaXLarge",
            "content": "XLM-RoBERTaBase XLM-RoBERTaBase XLM-RoBERTaLarge XLM-RoBERTaLarge XLM-RoBERTaXLarge XLM-RoBERTaXLarge (ext1) LLäMmlein2Vec LLäMmlein2Vec (ext1) LLäMmlein2Vec (ext2) LLäMmlein2Vec (ext2) (ext1+2) LLäMmlein2Vec LLäMmlein2Vec (ext1+2) LLäMmlein2Vec (ext1) LLäMmlein2Vec (ext1) LLäMmlein2Vec (ext2) LLäMmlein2Vec (ext2) LLäMmlein2Vec (ext1+2) LLäMmlein2Vec (ext1+2) LLäMmlein2Vec (ext1) LLäMmlein2Vec (ext1) LLäMmlein2Vec (ext2) LLäMmlein2Vec (ext2) LLäMmlein2Vec (ext1+2) LLäMmlein2Vec (ext1+2) ModernGBERT ModernGBERT ModernGBERT + ext1+2 ModernGBERT + ext1+ ModernGBERT ModernGBERT ModernGBERT + ext1+2 ModernGBERT + ext1+2 Table 9: Results on MTEB(deu, v1) of the German MTEB Benchmark. For each task type, scores were averaged across respective unique tasks. We provide results for basis models as well as after supervised training on mMARCO . In all cases, evaluation was done in zero-shot fashion without further finetuning on the above tasks. Best scores are indicated in bold."
        },
        {
            "title": "Params",
            "content": "<1,024 tok. 1,024 to 4,095 tok. 4,096 to 8,192 tok."
        },
        {
            "title": "Model",
            "content": "LLäMmlein LLäMmlein LLäMmlein 120M 1B 7B LLäMmlein2Vec (ext1) 120M 120M LLäMmlein2Vec (ext2) LLäMmlein2Vec (ext1+2) 120M LLäMmlein2Vec (ext1) LLäMmlein2Vec (ext2) LLäMmlein2Vec (ext1+2) LLäMmlein2Vec (ext1) LLäMmlein2Vec (ext2) LLäMmlein2Vec (ext1+2) 1B 1B 1B 7B 7B 7B 134M ModernGBERT ModernGBERT + ext1 134M ModernGBERT + ext1+2 134M ModernGBERT ModernGBERT + ext1 ModernGBERT + ext1+2 1B 1B 1B 0.286 0.517 0.529 0.315 0.252 0. 0.588 0.555 0.462 0.597 0.605 0.580 0.552 0.536 0.540 0.556 0.617 0.601 0.124 0.230 0.310 0.206 0.047 0. 0.448 0.297 0.209 0.207 0.176 0.327 0.168 0.410 0.393 0.233 0.506 0.526 0.049 0.088 0.122 0.044 0.000 0. 0.232 0.003 0.033 0.000 0.000 0.000 0.013 0.238 0.201 0.023 0.406 0.383 0.091 0.165 0.216 0.120 0.031 0. 0.333 0.144 0.123 0.111 0.099 0.156 0.105 0.323 0.296 0.136 0.457 0.451 Table 10: QA-NIAH results. Metric is Exact Match. All tokens are counted per the models respective tokenizer. ModernGBERT models marked with w/o ext refer to the model checkpoint after pre-training but before the context extension phases, those marked with w/o ext2 to the models after extension phase one, but before phase two."
        },
        {
            "title": "GBERTBase\nGBERTLarge",
            "content": "gerturax-"
        },
        {
            "title": "GeBERTaBase\nGeBERTaLarge\nGeBERTaXLarge",
            "content": "111M 337M 135M 139M 406M 887M XLM-RoBERTaBase XLM-RoBERTaLarge XLM-RoBERTaXLarge 279M 561M 3.48B 2.33 0.13 7.25 0.77 4.13 0.40 9.79 0.04 27.30 0.09 42.20 0.36 2.28 0.08 7.27 0.53 57.70 0. LLäMmlein2Vec LLäMmlein2Vec LLäMmlein2Vec"
        },
        {
            "title": "ModernGBERT\nModernGBERT",
            "content": "3.74 0.75 120M 1B 27.30 0.16 7B 143.00 0.22 5.25 0.27 15.70 1.66 8.26 0.81 19.40 0.08 54.10 0.40 83.80 0.70 5.05 0.19 15.90 1.04 123.00 0.71 7.17 0.53 53.90 0.37 288.00 0. 6.69 0.14 42.70 0.12 180.00 0.19 8.39 0.35 59.70 0.30 304.00 0.41 134M 1B 3.60 0.29 22.60 0. 3.70 0.74 22.50 0.18 5.42 0.33 28.70 0.31 4.71 0.75 26.20 0.36 Table 11: Model Throughput. Numbers are seconds per million tokens. All models were run on an RTX A6000 with Bfloat16 data type and with Flash Attention 2, except models with , which did not implement Flash Attention 2. Reported uncertainty is the empirical standard deviation on 10 repetitions."
        }
    ],
    "affiliations": [
        "CAIDAS Center for Artificial Intelligence and Data Science",
        "Computer Philology and History of Contemporary German Literature",
        "Data Science",
        "JMU Julius-Maximilians-Universität Würzburg"
    ]
}