{
    "paper_title": "Large Language Models Do NOT Really Know What They Don't Know",
    "authors": [
        "Chi Seng Cheang",
        "Hou Pong Chan",
        "Wenxuan Zhang",
        "Yang Deng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent work suggests that large language models (LLMs) encode factuality signals in their internal representations, such as hidden states, attention weights, or token probabilities, implying that LLMs may \"know what they don't know\". However, LLMs can also produce factual errors by relying on shortcuts or spurious associations. These error are driven by the same training objective that encourage correct predictions, raising the question of whether internal computations can reliably distinguish between factual and hallucinated outputs. In this work, we conduct a mechanistic analysis of how LLMs internally process factual queries by comparing two types of hallucinations based on their reliance on subject information. We find that when hallucinations are associated with subject knowledge, LLMs employ the same internal recall process as for correct responses, leading to overlapping and indistinguishable hidden-state geometries. In contrast, hallucinations detached from subject knowledge produce distinct, clustered representations that make them detectable. These findings reveal a fundamental limitation: LLMs do not encode truthfulness in their internal states but only patterns of knowledge recall, demonstrating that \"LLMs don't really know what they don't know\"."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 3 3 0 9 0 . 0 1 5 2 : r Large Language Models Do NOT Really Know What They Dont Know Chi Seng Cheang1 Hou Pong Chan2 Wenxuan Zhang3 Yang Deng1 2DAMO Academy, Alibaba Group 1Singapore Management University 3Singapore University of Technology and Design cs.cheang.2025@phdcs.smu.edu.sg, houpong.chan@alibaba-inc.com wxzhang@sutd.edu.sg, ydeng@smu.edu.sg"
        },
        {
            "title": "Abstract",
            "content": "Recent work suggests that large language models (LLMs) encode factuality signals in their internal representations, such as hidden states, attention weights, or token probabilities, implying that LLMs may know what they dont know. However, LLMs can also produce factual errors by relying on shortcuts or spurious associations. These error are driven by the same training objective that encourage correct predictions, raising the question of whether internal computations can reliably distinguish between factual and hallucinated outputs. In this work, we conduct mechanistic analysis of how LLMs internally process factual queries by comparing two types of hallucinations based on their reliance on subject information. We find that when hallucinations are associated with subject knowledge, LLMs employ the same internal recall process as for correct responses, leading to overlapping and indistinguishable hidden-state geometries. In contrast, hallucinations detached from subject knowledge produce distinct, clustered representations that make them detectable. These findings reveal fundamental limitation: LLMs do not encode truthfulness in their internal states but only patterns of knowledge recall, demonstrating that LLMs dont really know what they dont know."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) demonstrate remarkable proficiency in generating coherent and contextually relevant text, yet they remain plagued by hallucination (Zhang et al., 2023b; Huang et al., 2025), phenomenon where outputs appear plausible but are factually inaccurate or entirely fabricated, raising concerns about their reliability and trustworthiness. To this end, researchers suggest that the internal states of LLMs (e.g., hidden representations (Azaria and Mitchell, 2023; Gottesman and Geva, 2024), attention weights (Yüksekgönül et al., 2024), output token logits (Orgad et al., 2025; 1 Figure 1: Illustration of three categories of knowledge. Associated hallucinations follow similar internal knowledge recall processes with factual associations, while unassociated hallucinations arise when the models output is detached from the input. Varshney et al., 2023), etc.) can be used to detect hallucinations, indicating that LLMs themselves may actually know what they dont know. These methods typically assume that when model produces hallucinated outputs (e.g., Barack Obama was born in the city of Tokyo in Figure 1), its internal computations for the outputs (Tokyo) are detached from the input information (Barack Obama), thereby differing from those used to generate factually correct outputs. Thus, the hidden states are expected to capture this difference and serve as indicators of hallucinations. However, other research (Lin et al., 2022b; Kang and Choi, 2023; Cheang et al., 2023) shows that models can also generate false information that is closely associated with the input information. In particular, models may adopt knowledge shortcuts, favoring tokens that frequently co-occur in the training corpus over factually correct answers (Kang and Choi, 2023). As shown in Figure 1, given the prompt: Barack Obama was born in the city of, an LLM may rely on the subject tokens representations (i.e., Barack Obama) to predict hallucinated output (e.g., Chicago), which is statistically associated with the subject entity but under other contexts (e.g., Barack Obama studied in the city of Chicago). Therefore, we suspect that the internal computations may not exhibit distinguishable patterns between correct predictions and input-associated hallucinations, as LLMs rely on the input information to produce both of them. Only when the model produces hallucinations unassociated with the input do the hidden states exhibit distinct patterns that can be reliably identified. To this end, we conduct mechanistic analysis of how LLMs internally process factual queries. We first perform causal analysis to identify hidden states crucial for generating Factual Associations (FAs) factually correct outputs grounded in subject knowledge. We then examine how these hidden states behave when the model produces two types of factual errors: Associated Hallucinations (AHs), which remain grounded in subject knowledge, and Unassociated Hallucinations (UHs), which are detached from it. Our analysis shows that when generating both FAs and AHs, LLMs propagate information encoded in subject representations to the final token during output generation, resulting in overlapping hidden-state geometries that cannot reliably distinguish AHs from FAs. In contrast, UHs exhibit distinct internal computational patterns, producing clearly separable hiddenstate geometries from FAs. Building on the analysis, we revisit several widely-used hallucination detection approaches (Gottesman and Geva, 2024; Yüksekgönül et al., 2024; Orgad et al., 2025) that adopt internal state probing. The results show that these representations cannot reliably distinguish AHs from FAs due to their overlapping hidden-state geometries, though they can effectively separate UHs from FAs. Moreover, this geometry also shapes the limits the effectiveness of Refusal Tuning (Zhang et al., 2024), which trains LLMs to refuse uncertain queries using refusal-aware dataset. Because UH samples exhibit consistent and distinctive patterns, refusal tuning generalizes well to unseen UHs but fails to generalize to unseen AHs. We also find that AH hidden states are more diverse, and thus refusal tuning with AH samples prevents generalization across both AH and UH samples. Together, these findings highlight central limitation: LLMs do not encode truthfulness in their hidden states but only patterns of knowledge recall and utilization, showing that LLMs dont really know what they dont know."
        },
        {
            "title": "2 Related Work",
            "content": "Existing hallucination detection methods can be broadly categorized into two types: representationbased and confidence-based. Representationbased methods assume that an LLMs internal hidden states can reflect the correctness of its generated responses. These approaches train classifier (often linear probe) using the hidden states from set of labeled correct/incorrect responses to predict whether new response is hallucinatory (Li et al., 2023; Azaria and Mitchell, 2023; Su et al., 2024; Ji et al., 2024; Chen et al., 2024; Ni et al., 2025; Xiao et al., 2025). Confidence-based methods, in contrast, assume that lower confidence during the generation led to higher probability of hallucination. These methods quantify uncertainty through various signals, including: (i) tokenlevel output probabilities (Guerreiro et al., 2023; Varshney et al., 2023; Orgad et al., 2025); (ii) directly querying the LLM to verbalize its own confidence (Lin et al., 2022a; Tian et al., 2023; Xiong et al., 2024; Yang et al., 2024b; Ni et al., 2024; Zhao et al., 2024); or (iii) measuring the semantic consistency across multiple outputs sampled from the same prompt (Manakul et al., 2023; Kuhn et al., 2023; Zhang et al., 2023a; Ding et al., 2024). response is typically flagged as hallucination if its associated confidence metric falls below predetermined threshold. However, growing body of work reveals critical limitation: even state-of-the-art LLMs are poorly calibrated, meaning their expressed confidence often fails to align with the factual accuracy of their generations (Kapoor et al., 2024; Xiong et al., 2024; Tian et al., 2023). This miscalibration limits the effectiveness of confidence-based detectors and raises fundamental question about the extent of LLMs self-awareness of their knowledge boundary, i.e., whether they can know what they dont know (Yin et al., 2023; Li et al., 2025). Despite recognizing this problem, prior work does not provide mechanistic explanation for its occurrence. To this end, our work addresses this explanatory gap by employing mechanistic interpretability techniques to trace the internal computations underlying knowledge recall within LLMs."
        },
        {
            "title": "3 Preliminary",
            "content": "Transformer Architecture Given an input sequence of tokens t1, ..., tT , an LLM is trained to model the conditional probability distribution of the next token p(tT +1t1, ..., tT ) conditioned on the preceding tokens. Each token is first mapped to continuous vector by an embedding 2 LLaMA-3-8B Mistral-7B-v0.3 Factual Association Associated Hallucination Unassociated Hallucination Total 3,506 1,406 7, 12,293 3,354 1,284 7,655 12,293 Table 1: Dataset statistics across categories. layer. The resulting sequence of hidden states is then processed by stack of Transformer layers. At layer ℓ 1, ..., L, each token representation is updated by Multi-Head Self-Attention (MHSA) and Feed-Forward Network (MLP) module: hℓ = hℓ1 + aℓ + mℓ, (1) where aℓ and mℓ correspond to the MHSA and MLP outputs, respectively, at the ℓ-layer. Internal Process of Knowledge Recall Prior work investigates the internal activations of LLMs to study the mechanics of knowledge recall. For example, an LLM may encode many attributes that are associated with subject (e.g., Barack Obama) (Geva et al., 2023). Given prompt like Barack Obama was born in the city of , if the model has correctly encoded the fact, the attribute Honolulu propagates through self-attention to the last token, yielding the correct answer. We hypothesize that non-factual predictions follow the same mechanism: spurious attributes such as Chicago are also encoded and propagated, leading the model to generate false outputs. Categorization of Knowledge To investigate how LLMs internally process factual queries, we define three categories of knowledge, according to two criteria: 1) factual correctness, and 2) subject representation reliance. Factual Associations (FA) refer to factual knowledge that is reliably stored in the parameters or internal states of an LLM and can be recalled to produce correct, verifiable outputs. Associated Hallucinations (AH) refer to nonfactual content produced when an LLM relies on input-triggered parametric associations. Unassociated Hallucinations (UH) refer to nonfactual content produced without reliance on parametric associations to the input. Dataset Construction Our study is conducted under basic knowledge-based question answering setting. The model is given prompt containing subject and relation (e.g., Barack Obama was born in the city of ) and is expected to predict the corresponding object (e.g., Honolulu). To build the dataset, we collect knowledge triples (subject, relation, object) form Wikidata. Each relation was paired with handcrafted prompt template to convert triples into natural language queries. The details of relation selection and prompt templates are provided in Appendix A.1. We then apply the label scheme presented in Appendix A.2: correct predictions are labeled as FAs, while incorrect ones are classified as AHs or UHs depending on their subject representation reliance. Table 1 summarizes the final data statistics. Models We conduct the experiments on two widely-adopted open-source LLMs, LLaMA-3 (Dubey et al., 2024) and Mistral-v0.3 (Jiang et al., 2023). Due to the space limit, details are presented in Appendix A.3, and parallel experimental results on Mistral are summarized in Appendix B."
        },
        {
            "title": "4 Analysis of Internal States in LLMs",
            "content": "To focus our analysis, we first conduct causal interventions to identify hidden states that are crucial for eliciting factual associations (FAs). We then compare their behavior across associated hallucinations (AHs) and unassociated hallucinations (UHs). Prior studies (Azaria and Mitchell, 2023; Gottesman and Geva, 2024; Yüksekgönül et al., 2024; Orgad et al., 2025) suggest that hidden states can reveal when model hallucinates. This assumes that the models internal computations differ when producing correct versus incorrect outputs, causing their hidden states to occupy distinct subspaces. We revisit this claim by examining how hidden states update when recalling three categories of knowledge (i.e., FAs, AHs, and UHs). If hidden states primarily signal hallucination, AHs and UHs should behave similarly and diverge from FAs. Conversely, if hidden states reflect reliance on encoded knowledge, FAs and AHs should appear similar, and both should differ from UHs."
        },
        {
            "title": "4.1 Causal Analysis of Information Flow",
            "content": "We identify hidden states that are crucial for factual prediction. For each knowledge tuple (subject, relation, object), the model is prompted with factual query (e.g., The name of the father of Joe Biden is). Correct predictions indicate that the model successfully elicits parametric knowledge. Using causal mediation analysis (Vig et al., 2020; Finlayson et al., 2021; Meng et al., 2022; Geva et al., 3 (a) Factual Associations (b) Associated Hallucinations (c) Unassociated Hallucinations Figure 2: Effect of interventions across layers of LLaMA-3-8B. The heatmap shows JS divergence between the output distribution before and after intervention. Darker color indicates that the intervened hidden states are more causally influential on the models predictions. Top row: patching representations of subject tokens. Middle row: blocking attention flow from subject to the last token. Bottom row: patching representations of the last token. 2023), we intervene on intermediate computations and measure the change in output distribution via JS divergence. large divergence indicates that the intervened computation is critical for producing the fact. Specifically, to test whether token is hidden states in the MLP at layer ℓ are crucial for eliciting knowledge, we replace the computation with corrupted version and observe how the output distribution changes. Similarly, following Geva et al. (2023), we mask the attention flow between tokens at layer ℓ using window size of 5 layers. To streamline implementation, interventions target only subject tokens, attention flow, and the last token. Notable observations are as follows: Obs1: Hidden states crucial for eliciting factual associations. The results in Figure 2a show that three components dominate factual predictions: (1) subject representations in early-layer MLPs, (2) mid-layer attention between subject tokens and the final token, and (3) the final token representations in later layers. These results trace clear information flow: subject representation, attention flow from the subject to the last token, and last-token representation, consistent with Geva et al. (2023). These three types of internal states are discussed in detail respectively (4.2-4.4). Obs2: Associated hallucinations follow the same information flow as factual associations. When generating AHs, interventions on these same components also produce large distribution shifts (Figure 2b). This indicates that, although outputs are factually wrong, the model still relies on encoded subject information. Obs3: Unassociated hallucinations present different information flow. In contrast, interventions during UH generation cause smaller distribu4 Figure 3: Norm ratio curves of subject representations in LLaMA-3-8B, comparing AHs and UHs against FAs as the baseline. tion shifts (Figure 2c), showing weaker reliance on the subject. This suggests that UHs emerge from computations not anchored in the subject representation, different from both FAs and AHs."
        },
        {
            "title": "4.2 Analysis of Subject Representations",
            "content": "The analysis in 4.1 reveals that unassociated hallucinations (UHs) are processed differently from factual associations (FAs) and associated hallucinations (AHs) in the early layers of LLMs, which share similar pattern. We examine how these differences emerge in the subject representations and why early-layer modules behave this way."
        },
        {
            "title": "4.2.1 Norm of Subject Representations",
            "content": "(cid:80)n i=1 hℓ = 1 To test whether subject representations differ across categories, we measure the average L2 norm of subject-token hidden activations across layers. For subject tokens ts1, .., tsn at layer ℓ, the average norm is hℓ si2, computed by Equation (1). We compare the norm ratio between hallucination samples (AHs or UHs) and correct predictions (FAs), where ratio near 1 indicates similar norms. Figure 3 shows that in LLaMA3-8B, AH norms closely match those of correct samples (ratio 0.99), while UH norms are consistently smaller, starting at the first layer (ratio Figure 4: Comparison of subspace overlap ratios. 0.96) and diverging further through mid-layers. Figure 5: Sample distribution across different subject popularity (low, mid, high) in LLaMA-3-8B, measured by monthly Wikipedia page views. Findings: At early layers, UH subject representations exhibit weaker activations than FAs, whereas AHs exhibit norms similar to FAs. do not. This indicates that the model has sufficiently encoded subject representations into parametric knowledge for FAs and AHs but not for UHs."
        },
        {
            "title": "4.2.2 Relation to Parametric Knowledge",
            "content": "We next investigate why early layers encode subject representations differently across knowledge types by examining how inputs interact with the parametric knowledge stored in MLP modules. Inspired by Kang et al. (2024), the output norm of an MLP layer depends on how well its input aligns with the subspace spanned by the weight matrix: poorly aligned inputs yield smaller output norms. For each MLP layer ℓ, we analyze the downdown and its input xℓ. projection weight matrix ℓ Given the input xℓ corresponding to the subject tokens, we compute its overlap ratio with the top singular subspace Vtop of ℓ down: r(xℓ s) = (cid:13) (cid:13)xℓ (cid:13) VtopV top s2 xℓ (cid:13) 2 (cid:13) (cid:13) . (2) higher overlap ratio r(xℓ alignment to the subspace spanned by ℓ ing to larger output norms. s) indicates stronger down, leadTo highlight relative deviations from the factual baseline (FA), we report the relative ratios between AH/FA and UH/FA. Focusing on the layer with the largest UH norm shift, Figure 4 shows that UHs have significantly lower r(xℓ s) than AHs in both LLaMA and Mistral. This reveals that early-layer parametric weights are more aligned with FA and AH subject representations than with UH subjects, producing higher norms for the former ones. These results also suggest that the model has sufficiently learned representations for FA and AH subjects during pretraining but not for UH subjects. Similar to FAs, AH hidden activations Findings: align closely with the weight subspace, while UHs"
        },
        {
            "title": "4.2.3 Correlation with Subject Popularity\nWe further investigate why AH representations\nalign with weight subspaces as strongly as FAs,\nwhile UHs do not. A natural hypothesis is that\nthis difference arises from subject popularity in the\ntraining data. We use average monthly Wikipedia\npage views as a proxy for subject popularity during\npre-training and bin subjects by popularity, then\nmeasure the distribution of UHs, AHs, and FAs.\nFigure 5 shows a clear trend: UHs dominate among\nthe least popular subjects (94% for LLaMA), while\nAHs are rare (1%). As subject popularity rises,\nUH frequency falls and both FAs and AHs become\nmore common, with AHs rising to 14% in the high-\npopularity subjects. This indicates that subject rep-\nresentation norms reflect training frequency, not\nfactual correctness.",
            "content": "Findings: Popular subjects yield stronger earlylayer activations. AHs arise mainly on popular subjects and are therefore indistinguishable from FAs by popularity-based heuristics, contradicting prior work (Mallen et al., 2023a) that links popularity to hallucinations."
        },
        {
            "title": "4.3 Analysis of Attention Flow",
            "content": "Having examined how the model forms subject representations, we next study how this information is propagated to the last token of the input where the model generates the object of knowledge tuple. In order to produce factually correct outputs at the last token, the model must process subject representation and propagate it via attention layers, so that it can be read from the last position to produce the outputs (Geva et al., 2023). To quantify the specific contribution from subject tokens (s1, ..., sn) to the last token, we comFigure 7: Cosine similarity of target-token hidden states across layers in LLaMA-3-8B. Figure 6: Subject-to-last attention contribution norms across layers in LLaMA-3-8B. Values show the norm of the attention contribution from subject tokens to the last token at each layer. pute the attention contribution from subject tokens to the last position: aℓ last = (cid:88) (cid:88) Aℓ,h last,sk (hℓ1 sk ℓ,h )W ℓ,h . (3) where Aℓ,h i,j denotes the attention weight assigned by the h-th head in the layer ℓ from the last position to subjec token j. Here, aℓ last represents the subject-to-last attention contribution at layer ℓ. Intuitively, if subject information is critical for prediction, this contribution should have large norm; otherwise, the norm should be small. Figure 6 shows that in LLaMA-3-8B, both AHs and FAs exhibit large attention-contribution norms in mid-layers, indicating strong information flow from subject tokens to the target token. In contrast, UHs show consistently lower norms, implying that their predictions rely far less on subject information. Yüksekgönül et al. (2024) previously argued that high attention flow from subject tokens signals factuality and proposed using attention-based hidden states to detect hallucinations. Our results challenge this view: the model propagates subject information just as strongly when generating AHs as when producing correct facts. Findings: Mid-layer attention flow from subject to last token is equally strong for AHs and FAs but weak for UHs. Attention-based heuristics can therefore separate UHs from FAs but cannot distinguish AHs from factual outputs, limiting their reliability for hallucination detection."
        },
        {
            "title": "4.4 Analysis of Last Token Representations",
            "content": "Our earlier analysis showed strong subject-to-last token information transfer for both FAs and AHs, but minimal transfer for UHs. We now examine how this difference shapes the distribution of lasttoken representations. When subject information is weakly propagated (UHs), last-token states receive Figure 8: t-SNE visualization of last tokens representations at layer 25 of LLaMA-3-8B. little subject-specific update. For UH samples sharing the same prompt template, these states should therefore cluster in the representation space. In contrast, strong subject-driven propagation in FAs and AHs produces diverse last-token states that disperse into distinct subspaces. To test this, we compute cosine similarity among last-token representations hℓ . As shown in Figure 7, similarity is high ( 0.9) for all categories in early layers, when little subject information is transferred. From mid-layers onward, FAs and AHs diverge sharply, dropping to 0.2 by layer 25. UHs remain moderately clustered, with similarity only declining to 0.5. Figure 8 shows the t-SNE visualization of the last tokens representations at layer 25 of LLaMA3-8B. The hidden representations of UH are clearly separated from FA, whereas AH substantially overlap with FA. These results indicate that the model processes UH differently from FA, while processing AH in manner similar to FA. More visualization can be found in Appendix C. This separation also appears in the entropy of the output distribution (Figure 9). Strong subjectto-last propagation in FAs and AHs yields lowentropy predictions concentrated on the correct or associated entity. In contrast, weak propagation in UHs produces broad, high-entropy distributions, spreading probability mass across many plausible candidates (e.g., multiple possible names for The name of the father of <subject> is). 6 LLaMA Mistral Methods AH Only UH Only AH Only UH Only 0.81 0.02 0.65 0.02 Subject 0.58 0.04 0.87 0.01 Attention 0.69 0.03 0.93 0.01 0.63 0.02 0.92 0.01 Last Token 0.89 0.00 0.49 0.01 Probability 0.84 0.01 Subject Pop. 0.48 0.01 0.46 0.00 0.52 0.01 0.86 0.01 0.87 0.01 0.57 0.02 0.58 0. 0.91 0.01 0.92 0.02 Figure 9: Distribution of last token probabilities. Finding: From mid-layers onward, UHs retain clustered last-token representations and highentropy outputs, while FAs and AHs diverge into subject-specific subspaces with low-entropy outputs. This provides clear signal to separate UHs from FAs and AHs, but not for FAs and AHs."
        },
        {
            "title": "5 Revisiting Hallucination Detection",
            "content": "The mechanistic analysis in 4 reveals that Internal states of LLMs primarily capture how the model recalls and utilizes its parametric knowledge, not whether the output is truthful. As both factual associations (FAs) and associated hallucinations (AHs) rely on the same subject-driven knowledge recall, their internal states show no clear separation. We therefore hypothesize that internal or black-box signals cannot effectively distinguish AHs from FAs, even though they could be effective in distinguishing unassociated hallucinations (UHs), which do not rely on parametric knowledge, from FAs. Experimental Setups To verify this, we revisit the effectiveness of widely-adopted white-box hallucination detection approaches that use internal state probing as well as black-box approaches that rely on scalar features. We evaluate on three settings: 1) AH Only (1,000 FAs and 1,000 AHs for training; 200 of each for testing), 2) UH Only (1,000 FAs and 1,000 UHs for training; 200 of each for testing), and 3) Full (1,000 FAs and 1,000 hallucination samples mixed of AHs and UHs for training; 200 of each for testing). For each setting, we use five random seeds to construct the training and testing datasets. We report the mean AUROC along with its standard deviation across seeds. White-box methods: We extract and normalize internal features and then train probe. Subject representations: last subject token hidden state from three consecutive layers (Gottesman and Geva, 2024). Attention flow: attention weights from the last token to subject tokens across all layers (YükTable 2: Hallucination detection performance on AH Only and UH Only settings. Figure 10: Hallucination detection performance on the Full setting (LLaMA-3-8B). sekgönül et al., 2024). Last-token representations: final token hidden state from the last layer (Orgad et al., 2025). Black-box methods: We test two commonly used scalar features, including answer token probability (Orgad et al., 2025) and subject popularity (average monthly Wikipedia page views) (Mallen et al., 2023a). As discussed in 4.2.3 and 4.4, these features are also related to whether the model relies on encoded knowledge to produce outputs rather than with truthfulness itself. Experimental Results Table 2 shows that hallucination detection methods behave very differently in the AH Only and UH Only settings. For whitebox probes, all approaches effectively distinguish UHs from FAs, with last-token hidden states reaching AUROC scores of about 0.93 for LLaMA and 0.92 for Mistral. In contrast, performance drops sharply on the AH Only setting, where the lasttoken probe falls to 0.69 for LLaMA and 0.63 for Mistral. Black-box methods follow the same pattern. Figure 10 further highlights this disparity under the Full setting: detection is consistently stronger on UH samples than on AH samples, and adding AHs to the training set significantly dilutes performance on UHs (AUROC 0.9 on UH Only vs. 0.8 on Full). These results confirm that both internal probes and black-box methods capture whether model draws on parametric knowledge, not whether its outputs are factually correct. Unassociated hallucinations are easier to detect because they bypass 7 this knowledge, while associated hallucinations are produced through the same recall process as factual answers, leaving no internal cues to distinguish them. As result, LLMs lack intrinsic awareness of their own truthfulness, and detection methods relying on these signals risk misclassifying associated hallucinations as correct, fostering harmful overconfidence in model outputs."
        },
        {
            "title": "6 Challenges of Refusal Tuning",
            "content": "A common strategy to mitigate potential hallucination in the models responses is to fine-tune LLMs to refuse answering when they cannot provide factual response, e.g., Refusal Tuning (Zhang et al., 2024). For such refusal capability to generalize, the training data must contain shared feature pattern across hallucinated outputs, allowing the model to learn and apply it to unseen cases. Our analysis in the previous sections shows that this prerequisite is not met. The structural mismatch between UHs and AHs suggests that refusal tuning on UHs may generalize to other UHs, because their hidden states occupy common activation subspace, but will not transfer to AHs. Refusal tuning on AHs is even less effective, as their diverse representations prevent generalization to either unseen AHs or UHs. Experimental Setups To verify the hypothesis, we conduct refusal tuning on LLMs under two settings: 1) UH Only, where 1,000 UH samples are paired with 10 refusal templates, and 1,000 FA samples are preserved with their original answers. 2) AH Only, where 1,000 AH samples are paired with refusal templates, with 1,000 FA samples again leave unchanged. We then evaluate both models on 200 samples each of FAs, UHs, and AHs. response matching any refusal template is counted as refusal, and we report the Refusal Ratio as the proportion of samples eliciting refusals. This measures not only whether the model refuses appropriately on UHs and AHs, but also whether it over-refuses on FA samples. Experimental Results Figure 11 shows that training with UHs leads to strong generalization across UHs, with refusal ratios of 82% for LLaMA. However, this effect does not transfer to AHs, where refusal ratios fall to 28%, respectively. Moreover, some FA cases are mistakenly refused (29.5%). These results confirm that UHs share common activation subspace, supporting generalFigure 11: Refusal tuning performance across three types of samples (LLaMA-3-8B). ization within the category, while AHs and FAs lie outside this space. By contrast, training with AHs produces poor generalization. On AH test samples, refusal ratio is only 33%, validating that their subject-specific hidden states prevent consistent refusal learning. Generalization to UHs is also weak (23.5%), again reflecting the divergence between AH and UH activation spaces. Overall, these findings show that the generalizability of refusal tuning is fundamentally limited by the heterogeneous nature of hallucinations. UH representations are internally consistent enough to support refusal generalization, but AH representations are too diverse for either UH-based or AH-based training to yield broadly applicable and reliable refusal capability."
        },
        {
            "title": "7 Conclusions and Future Work",
            "content": "In this work, we revisit the widely accepted claim that hallucinations can be detected from models internal states. Our mechanistic analysis reveals that hidden states encode whether models are reliance on their parametric knowledge rather than truthfulness. As result, detection methods succeed only when outputs are detached from the input but fail when hallucinations arise from the same knowledge-recall process as correct answers. These findings lead to three key implications. First, future evaluations should report detection performance separately for Associated Hallucinations (AHs) and Unassociated Hallucinations (UHs), as they stem from fundamentally different internal processes and require distinct detection strategies. Second, relying solely on hidden states is insufficient for reliable hallucination detection. Future research should integrate LLMs with external feedback mechanisms, such as fact-checking modules or retrieval-based verifiers, to assess factuality more robustly. Third, future studies should prioritize improving AH detection. Because AHs occur more frequently in widely known or highly 8 popular topics (4.2.3), their undetected errors pose greater risks to user trust and the practical reliability of LLMs. publicly available resources, and no private or sensitive information about individuals is included. We employ the LLM tools for polishing."
        },
        {
            "title": "Limitations",
            "content": "We identify several limitations of our work. Focus on Factual Knowledge While our analysis identifies failure cases of hallucination detection methods, our study is primarily limited to factual completion prompts. It does not extend to longform or open-ended text generation tasks (Wei et al., 2024; Min et al., 2023; Huang and Chen, 2024). Future work should broaden this investigation to these tasks in order to draw more comprehensive conclusions. Lack of Analysis on Prompt-based Hallucination Detection Approaches Our analysis focuses on white-box hallucination detection methods based on internal states and two black-box approaches based on external features. We do not include verbalization-based strategies (Lin et al., 2022a; Tian et al., 2023; Xiong et al., 2024; Yang et al., 2024b; Ni et al., 2024; Zhao et al., 2024), such as prompting the model to report or justify its confidence explicitly, which constitute different line of approach. Exploring such approaches may offer complementary insights into how models internally represent and express uncertainty. Applicability to Black-box LLMs or Large Reasoning Models Our study is limited to opensource LLMs. Conducting mechanistic analyses on commercial black-box LLMs is not permitted due to access restrictions. Future work could explore alternative evaluation protocols or collaboration frameworks that enable partial interpretability analyses on such systems. In addition, recent studies (Mei et al., 2025; Zhang et al., 2025) have begun examining the internal states of large reasoning models for hallucination detection, suggesting promising direction for extending our methodology to models with multi-step reasoning capabilities."
        },
        {
            "title": "Ethical Considerations",
            "content": "This work analyzes the internal mechanisms of large language models using data constructed from Wikidata (Vrandecic and Krötzsch, 2014), which is released under the Creative Commons CC0 1.0 Universal license, allowing unrestricted use and redistribution of its data. All data are derived from"
        },
        {
            "title": "References",
            "content": "Amos Azaria and Tom M. Mitchell. 2023. The internal In Findstate of an LLM knows when its lying. ings of the Association for Computational Linguistics: EMNLP 2023, pages 967976. Chi Seng Cheang, Hou Pong Chan, Derek F. Wong, Xuebo Liu, Zhaocong Li, Yanming Sun, Shudong Liu, and Lidia S. Chao. 2023. Can lms generalize to future data? an empirical analysis on text summarization. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1620516217. Association for Computational Linguistics. Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. 2024. INSIDE: llms internal states retain the power of hallucination detection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Michael Han Daniel Han and Unsloth team. 2023. Unsloth. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, and Xueqi Cheng. 2024. Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models. CoRR, abs/2402.10612. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, and 82 others. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. Matthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart M. Shieber, Tal Linzen, and Yonatan Belinkov. 2021. Causal analysis of syntactic agreement mechanisms in neural language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 18281843. Association for Computational Linguistics. 9 Zorik Gekhman, Eyal Ben-David, Hadas Orgad, Eran Ofek, Yonatan Belinkov, Idan Szpektor, Jonathan Herzig, and Roi Reichart. 2025. Inside-out: Hidden factual knowledge in llms. CoRR, abs/2503.15299. Katie Kang, Amrith Setlur, Claire J. Tomlin, and Sergey Levine. 2024. Deep neural networks tend to extrapolate predictably. In The Twelfth International Conference on Learning Representations, ICLR 2024. Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023. Dissecting recall of factual associations in auto-regressive language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1221612235. Association for Computational Linguistics. Daniela Gottesman and Mor Geva. 2024. Estimating knowledge in large language models without generating single token. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, pages 39944019. Nuno Miguel Guerreiro, Elena Voita, and André F. T. Martins. 2023. Looking for needle in haystack: comprehensive study of hallucinations in neural machine translation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pages 10591075. Association for Computational Linguistics. Chao-Wei Huang and Yun-Nung Chen. 2024. Factalign: Long-form factuality alignment of large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 16363 16375. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2025. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Trans. Inf. Syst., 43(2):42:1 42:55. Ziwei Ji, Delong Chen, Etsuko Ishii, Samuel Cahyawijaya, Yejin Bang, Bryan Wilie, and Pascale Fung. 2024. LLM internal states reveal hallucination risk faced with query. In Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 88104, Miami, Florida, US. Association for Computational Linguistics. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Preprint, arXiv:2310.06825. Cheongwoong Kang and Jaesik Choi. 2023. Impact of co-occurrence on factual knowledge of large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 77217735. Sanyam Kapoor, Nate Gruver, Manley Roberts, Katie Collins, Arka Pal, Umang Bhatt, Adrian Weller, Samuel Dooley, Micah Goldblum, and Andrew Gordon Wilson. 2024. Large language models must be taught to know what they dont know. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter InferencePfister, and Martin Wattenberg. 2023. time intervention: Eliciting truthful answers from language model. Advances in Neural Information Processing Systems, 36:4145141530. Moxin Li, Yong Zhao, Wenxuan Zhang, Shuaiyi Li, Wenya Xie, See-Kiong Ng, Tat-Seng Chua, and Yang Deng. 2025. Knowledge boundary of large language models: survey. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, pages 51315157. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022a. Teaching models to express their uncertainty in words. Trans. Mach. Learn. Res., 2022. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022b. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, pages 3214 3252. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023a. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, pages 98029822. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 98029822. Association for Computational Linguistics. Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 90049017. Association for Computational Linguistics. Zhiting Mei, Christina Zhang, Tenny Yin, Justin Lidard, Ola Shorinwa, and Anirudha Majumdar. 2025. Reasoning about uncertainty: Do reasoning models know when they dont know? CoRR, abs/2506.18183. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in gpt. Advances in neural information processing systems, 35:1735917372. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, pages 1207612100. Shiyu Ni, Keping Bi, Jiafeng Guo, and Xueqi Cheng. 2024. When do llms need retrieval augmentation? mitigating llms overconfidence helps retrieval augmentation. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 1137511388. Association for Computational Linguistics. Shiyu Ni, Keping Bi, Jiafeng Guo, Lulu Yu, Baolong Bi, and Xueqi Cheng. 2025. Towards fully exploiting LLM internal states to enhance knowledge boundary perception. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 2431524329. Association for Computational Linguistics. Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, and Yonatan Belinkov. 2025. Llms know more than they show: On the intrinsic representation of LLM hallucinations. In The Thirteenth International Conference on Learning Representations, ICLR 2025. Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. 2021. Simple entity-centric quesIn Proceedings tions challenge dense retrievers. of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 61386148. Association for Computational Linguistics. Weihang Su, Changyue Wang, Qingyao Ai, Yiran Hu, Zhijing Wu, Yujia Zhou, and Yiqun Liu. 2024. Unsupervised real-time hallucination detection based on the internal states of large language models. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 1437914391. Association for Computational Linguistics. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D. Manning. 2023. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 54335442. Association for Computational Linguistics. Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. CoRR, abs/2307.03987. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020. Investigating gender bias in language models using causal mediation analysis. Advances in neural information processing systems, 33:12388 12401. Denny Vrandecic and Markus Krötzsch. 2014. Wikidata: free collaborative knowledgebase. Commun. ACM, 57(10):7885. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, and Quoc V. Le. 2024. Long-form factuality in large language models. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingfaces transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771. Chenghao Xiao, Hou Pong Chan, Hao Zhang, Mahani Aljunied, Lidong Bing, Noura Al Moubayed, and Yu Rong. 2025. Analyzing llms knowledge boundary cognition across languages through the lens of internal representations. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 24099 24115. Association for Computational Linguistics. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2024. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. 11 An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and 22 others. 2024a. Qwen2.5 technical report. CoRR, abs/2412.15115. Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2024b. Alignment for honesty. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. 2023. Do large language models know what they dont know? In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 86538665. Association for Computational Linguistics. Gal Yona, Roee Aharoni, and Mor Geva. 2024. Narrowing the knowledge evaluation gap: Open-domain question answering with multi-granularity answers. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 67376751. Association for Computational Linguistics. Mert Yüksekgönül, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece Kamar, and Besmira Nushi. 2024. Attention satisfies: constraint-satisfaction lens on factual errors of language models. In The Twelfth International Conference on Learning Representations, ICLR 2024. Hanning Zhang, Shizhe Diao, Yong Lin, Yi R. Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. 2024. R-tuning: Instructing large language models to say dont know. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, pages 71137139. Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A. Malin, and Kumar Sricharan. 2023a. Sac3: Reliable hallucination detection in black-box language models via semantic-aware cross-check consistency. CoRR, abs/2311.01740. Qingjie Zhang, Yujia Fu, Yang Wang, Liu Yan, Tao Wei, Ke Xu, Minlie Huang, and Han Qiu. 2025. On the self-awareness of large reasoning models capability boundaries. Preprint, arXiv:2509.24711. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. 2023b. Sirens song in the AI ocean: survey on hallucination in large language models. CoRR, abs/2309.01219. Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Chong Meng, Shuaiqiang Wang, Zhicong Cheng, Zhaochun Ren, and Dawei Yin. 2024. Knowing what llms DO NOT know: simple yet effective self-detection method. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, pages 70517063."
        },
        {
            "title": "A Datasets and Implementations",
            "content": "A.1 Selected Relations and Prompt Templates We employed set of criteria to select relations from Wikidata in order to construct our dataset. Our criteria largely follow the framework proposed by Gekhman et al. (2025). Specifically, we require that each factual query in the dataset be unambiguous: given subjectrelation pair, the object should be unique and easy verifiable. The criteria are as follows: Avoid granularity ambiguity. We exclude relations whose answers can vary in their level of detail. For example, in location queries, the response could be expressed as city, state, or country, making it ill-defined (Yona et al., 2024). Avoid surface-level guessing. We exclude relations whose correct answers can often be inferred from shallow patterns. For instance, country of citizenship can frequently be guessed from shallow lexical patterns, rather then reflecting actual memorization (Mallen et al., 2023b). Following these criteria, Gekhman et al. (2025) narrowed the 24 relations introduced by Sciavolino et al. (2021) down to four. However, we observe that their filtering primarily addresses ambiguity at the relation and object levels, but does not consider ambiguity at the subject level. In practice, some relations involve subjects that are inherently ambiguous. For example, the relation record label can be problematic because many songs share identical names, leading to unclear subjectobject mappings. To mitigate such cases, we apply an additional subject-level filtering step and restrict our dataset to relations where the subject is person, thereby reducing ambiguity. In addition, we manually include certain relations to strengthen the dataset. Concretely, we use the following four relations: P22 (father), P25 (mother), P26 (spouse), and 12 Relation Prompt Template father mother spouse date of birth The name of the father of [subject] is The name of the mother of [subject] is The name of the spouse of [subject] is The birth date of [subject] is Table 3: Relations and prompt templates for querying factual knowledge of models. [subject] is placeholder replaced with subject entities. (date of birth). We show the list of the templates used to create our dataset in Table 3. A.2 Labeling Scheme We follow the criteria in 3 to label the data samples into different categories: Factual Correctness: We construct correctness labels through two-stage process. First, we use spaCy1 Named Entity Recognizer to extract the target entity from the models output. If it matches the ground truth, the answer is marked correct. Otherwise, or if extraction fails, we rely on Qwen2.5-14B-Instruct (Yang et al., 2024a) as an automatic judge to compare the predicted answer with the ground truth. Following Gekhman et al. (2025), we design the evaluation prompt, which is shown in Figure 12. Subject Representation Reliance: We assess whether prediction relies on the subjects representation by blocking attention from subject tokens and measuring the resulting distribution shift. If the subject is crucial, masking disrupts information flow and yields large shift; if not, the effect is minimal. Concretely, we compare the output distributions of the original prompt and the masked prompt (e.g., with Barack Obama masked), using JensenShannon (JS) divergence to quantify the difference. high JS divergence indicates strong reliance on the subject, while low value suggests limited contribution. We then set threshold based on the average JS divergence across all correct answers, assuming these inherently depend on subject representations. A."
        },
        {
            "title": "Implementation Details",
            "content": "Checkpoints and GPU resources. All the checkpoints used in our experiments are provided by the Hugging Face Transformers library (Wolf et al., 1https://spacy.io/ 2019). Specifically, we use the checkpoint metallama/Meta-Llama-3-8B2 and mistralai/Mistral7B-v0.33 for the experiments of response generation (3), hidden-state analysis (4) and accessing the performance of hallucination detection methods (5). For refusal tuning (6), we use checkpoints provided by the Unsloth framework (Daniel Han and team, 2023), namely unsloth/llama-3-8b4 and unsloth/mistral-7b-v0.35, which enable more efficient fine-tuning. All experiments are conducted on 4 NVIDIA L40S GPUs. Decoding algorithm. We employ greedy decoding (temperature = 0) for response generation, with models run in BF16 precision. PEFT settings for refusal tuning. For refusal tuning, we fine-tune with both models using QLoRA (Dettmers et al., 2023), implemented with the Unsloth framework (Daniel Han and team, 2023), with rank = 8, and α = 8. QLoRA adapters are applied to all attention and MLP modules, and each model is fine-tuned for one epoch."
        },
        {
            "title": "B Parallel Experiments on Mistral",
            "content": "This section is for documenting parallel experiments conducted on the Mistral-7B-v0.3 model under the same settings as described in the main text (Figures 1320). The results from Mistral exhibit similar patterns to those observed in LLaMA, as described before. Specifically, we find consistent patterns in the models internal computations, hidden-state behaviors, and the performance of hallucination detection and refusal tuning experiments."
        },
        {
            "title": "C More Visualization on Hidden States",
            "content": "In this section, we provide t-SNE visualization of subject tokens hidden states in Figure 21 and Figure 22. Compared to the last-token representations, the t-SNE visualization of subject-token hidden states shows that unassociated hallucinations (UHs) are moderately separated from factual and associated samples, but the separation is less distinct than in the last-token representations. This observation 2https://huggingface.co/meta-llama/ Meta-Llama-3-8B 3https://huggingface.co/mistralai/ Mistral-7B-v0. 4https://huggingface.co/unsloth/llama-3-8b 5https://huggingface.co/unsloth/mistral-7b-v0. 3 13 LLM Judge Prompt will give you factual query (e.g., The name of the father of <subj>), gold answer to the factual query, and proposed answer generated by an LLM. You need to compare the proposed answer to the gold answer and assign it one of the possible grades using the steps below. Possible grades are: A: CORRECT B: INCORRECT C: WRONG GOLD D: ERROR Spelling errors, synonyms, abbreviations, or hedging expressions (e.g., it is possible that) should not alter the grade if the person referred to in the proposed answer matches the gold answer. Steps: Step 1: If the gold answer does not correspond to an answer for the question, output and finish. Otherwise, proceed to Step 2. Step 2: Extract all predicted entities from the proposed answer. Proceed to Step 3. Step 3: If each predicted entity refers to the answer mentioned in the gold answer, output and finish. Otherwise, proceed to Step 4. Step 4: If the predicted entity does not refer to the gold answer, output and finish. Otherwise, proceed to Step 5. Step 5: Double-check whether the proposed answer refers to different answer from the gold answer. If it does, output B. Otherwise, output and finish. Input format: Question: {question} Gold answer: {gold_answer} Proposed answer: {proposed_answer} Instruction: Output your reasoning steps. After that, conclude your response with Output: followed by the letter (A, B, C, or D). Do not provide any further explanation. Figure 12: LLM Judge prompt used for evaluation. aligns with the results in 5, where the hallucination detection performance using last-token hidden states outperforms that based on subject-token representations. 14 (a) Factual Associations (b) Associated Hallucinations (c) Unassociated Hallucinations Figure 13: Effect of interventions across layers of Mistral-7B-v0.3. The heatmap shows JS divergence between the output distribution before and after intervention. Darker color indicates that the intervened hidden states are more causally influential on the models predictions. Top row: patching representations of subject tokens. Middle row: blocking attention flow from subject to the last token. Bottom row: patching representations of the last token. Figure 14: Norm ratio curves of subject representations in Mistral-7B-v0.3, comparing AHs and UHs against FAs as the baseline. At earlier layers, the norm of UH samples is significantly lower than that of AH samples. Figure 17: Cosine similarity of target-token hidden states across layers in Mistral-7B-v0.3. From mid-layers onward, FAs and AHs diverge sharply as subject information propagates, while UHs remain more clustered, confirming weaker subject-dependent updates. Figure 15: Sample distribution across different subject popularity (low, mid, high) in Mistral-7B-v0.3, measured by monthly Wikipedia page views. Figure 18: t-SNE visualization of last tokens representations at layer 25 of Mistral-7B-v0.3. Figure 16: Subject-to-last attention contribution norms across layers in Mistral-7B-v0.3. Values show the norm of the attention contribution from subject tokens to the last token at each layer. Figure 19: Hallucination detection performance on the Full setting (Mistral-v0.3-7B). 15 Figure 20: Refusal tuning performance across three types of samples (Mistral-v0.3-7B). Figure 21: t-SNE visualization of subject tokens representations at layer 11 of LLaMA-3-8B. Figure 22: t-SNE visualization of subject tokens representations at layer 11 of Mistral-7B-v0.3."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Singapore Management University",
        "Singapore University of Technology and Design"
    ]
}