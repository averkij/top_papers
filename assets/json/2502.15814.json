{
    "paper_title": "Slamming: Training a Speech Language Model on One GPU in a Day",
    "authors": [
        "Gallil Maimon",
        "Avishai Elmakies",
        "Yossi Adi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples at - https://pages.cs.huji.ac.il/adiyoss-lab/slamming ."
        },
        {
            "title": "Start",
            "content": "Slamming: Training Speech Language Model on One GPU in Day Gallil Maimon*, Avishai Elmakies*, Yossi Adi *Equal Contribution The Hebrew University of Jerusalem gallil.maimon@mail.huji.ac.il 5 2 0 2 9 1 ] . [ 1 4 1 8 5 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce Slam, recipe for training highquality Speech Language Models (SLMs) on single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples - https://pages.cs.huji.ac.il/adiyosslab/slamming. Figure 1: Comparing Topic-StoryCloze performance of different SLMs as function of training compute. Model size is indicated by the size of the circle."
        },
        {
            "title": "Introduction",
            "content": "Speech Language Models (SLMs) have gained significant interest from researchers (Peng et al., 2024a; Cui et al., 2024; Ji et al., 2024; Latif et al., 2023), demonstrating remarkable performance in traditional speech tasks (Wang et al., 2023; Elmakies et al., 2025), diverse generative applications (Yang et al., 2023, 2024b), and reasoning over speech and audio signals (Tang et al., 2024; Chu et al., 2023). SLMs can generally be classified into two main categories: (i) generative speech Language Models (LMs) (which can also incorporate text) and (ii) speech-aware LMs. The first category follows similar pre-training approach to text-based Large Language Models (LLMs), directly maximising the likelihood of speech considering both input and output, typically by representing audio as sequence of discrete tokens. The second category consists of pre-trained text LMs adapted to process speech inputs. In this work, we focus on the first. Training high-quality SLMs can be highly resource intensive (Hassid et al., 2024; Cuervo and Marxer, 2024; Zeng et al., 2024; Nguyen et al., 2025; Défossez et al., 2024). For example, Nguyen et al. (2025) trained their SLM on approximately 570k hours of speech data, while Défossez et al. (2024) utilised around 7M hours. Additionally, Cuervo and Marxer (2024) proposed SLM scaling laws, suggesting that training high-quality SLMs requires 3X more data compared to text-based counterparts. These computational demands restrict the required fundamental research aimed at enhancing SLMs, such as advancements in speech tokenisation, efficient acoustic modelling, etc. In the Natural Language Processing (NLP) community, numerous studies have investigated efficient model training techniques, including masked language models such as Cramming (Geiping and Goldstein, 2023) and ModernBERT (Warner et al., 2024), along with next-token prediction LLMs such as MobileLLM (Liu et al., 2024b). These methods include implementation efficiencies, architectural 1 improvements, data selection strategies, and enhancements to the overall training pipeline. We open-source all code, models, training recipes, and synthetic datasets. Inspired by Cramming (Geiping and Goldstein, 2023) in text, we investigate compute-limited SLM training, which we term Slamming. We pose the question: Is it possible to train high-quality SLMs using single GPU within 24 hours? For that, we conduct an extensive empirical analysis exploring how different training components influence performance. From this, we derive training recipe that maximises model performance within fixed compute budget. Specifically, we investigate the impact of model initialisation and architecture, various optimisers and learning rate schedulers, data selection strategies - including the role of synthetic data, text-interleaving and preference optimisation. We believe that developing these training strategies and proving their feasibility will empower the speech and audio research community to advance SLMs beyond the scope of large, well-funded academic and industrial labs. Figure 1 illustrates the performance of various SLMs relative to their training compute budget, with circle sizes representing the size of the models. Furthermore, we compare our results with the scaling performance predicted from Cuervo and Marxer (2024). Although the authors present somewhat pessimistic view of the computational resources needed to train highquality SLMs, we empirically show that reality is more promising, demonstrating that it is possible to significantly exceed the predicted performance per unit of compute. We encourage the community to refine and expand scaling laws specifically tailored for SLM training across various settings. Our main contributions are: 1. We introduce Slam, training recipe for efficiently training high-quality SLMs using single A5000 GPU within 24 hours. 2. We carry out extensive experiments exploring model initialisation and architecture, optimisation, data collection and generation, and training objectives (i.e., preference optimisation and text-speech interleaving), providing insights into the impact of each component on model performance. 3. Building on these insights, we scale the compute budget to two A100 GPUs for 48 hours and demonstrate that our model achieves performance on par with state-of-the-art models that require substantially more compute."
        },
        {
            "title": "2 Related Work",
            "content": "Efficient training. Enhancing the efficiency of neural network training has been extensively studied (Shen et al., 2023). Hajimolahoseini et al. (2023); Wang et al. (2024) examined the impact of data selection on Large Language Model (LLM) training and introduced efficient data selection methods. Muhamed et al. (2024) proposed using structured sparse gradients to enhance compute efficiency in LLM training, while Rawat et al. (2024) explored the potential of leveraging smaller language models to improve the training efficiency of larger LLMs. Lv et al. (2024) investigated the use of low-dimensional projections for attention parameters to enhance training efficiency. Meanwhile, Neiterman and Ben-Artzi (2024) proposed applying LayerDrop as technique to optimise neural network training. More closely related to our work, Li et al. (2023) propose training strategy for developing LLMs within 100k$ budget. Warner et al. (2024) introduce ModernBERT, an efficient training pipeline for optimising BERT models, while Izsak et al. (2021) outline method for training BERT model in 24 hours using 8 GPUs. The most relevant work to ours is Cramming (Geiping and Goldstein, 2023), where the authors conduct an in-depth analysis of masked LM training on single GPU in one day. While these studies offer valuable insights, they primarily focus on training text models, such In the speech doas LLMs and masked LMs. main, similar research has been conducted on self-supervised representation models (Liu et al., 2024a), but not on SLMs. In this work, we address this gap by focusing on efficient SLM training. Generative speech language models were explored under various setups (Lakhotia et al., 2021; Kharitonov et al., 2021). Lakhotia et al. (2021) were the first to show how raw and uncurated speech data can be leveraged into building Generative Spoken Language Modeling (GSLM) system. Next, Borsos et al. (2023) proposed cascade version using both coarse and fine speech tokens. Such modelling framework opened up new and promising research direction for processing and modelling spoken data, such as speech resynthesis (Polyak et al., 2021), speaking style conversion (Kreuk et al., 2021; Maimon and Adi, 2 2023), dialogue modelling (Nguyen et al., 2022), speech-to-speech translation (Popuri et al., 2022; Peng et al., 2024b), etc. Nachmani et al. (2024) proposed augmenting text Language Model (LM) with continuous speech data to improve spoken question-answering tasks. Recently, Park et al. (2024) proposed SLM based on state-space models (Gu et al., 2021) to further push long contextefficient modelling, while Lin et al. (2024) proposed to fine-tune SLMs using direct preference optimisation (Rafailov et al., 2024) obtained from text LLM rankings. Similar to text LLMs, training SLMs often demands large-scale datasets. For instance, Moshi (Défossez et al., 2024) was trained on 7 million hours of speech data, SpiritLM (Nguyen et al., 2025) utilized 560k hours, and TWIST (Hassid et al., 2024) was trained on approximately 150k. Recently, Cuervo and Marxer (2024) introduced the first scaling laws for SLMs, suggesting that achieving comparable performance to text LMs requires three times more tokens. In this work, we focus on reducing the computational demands while maintaining performance comparable to leading SLMs."
        },
        {
            "title": "3 Setup",
            "content": "In this study, we explore decoder-only generative SLMs, which aim at maximising the likelihood of speech samples represented as discrete tokens. We examine both purely speech-based SLMs trained on speech tokens and joint speech-text SLMs using interleaving strategies (Nguyen et al., 2025). Similarly to Hassid et al. (2024); Lakhotia et al. (2021), we obtain speech tokens by quantising continuous latent representations of self-supervised speech representation model using the k-means algorithm, often known as semantic tokens. Specifically, we utilise multilingual HuBERT (Hsu et al., 2021) model running at 25 Hz, as employed in Hassid et al. (2024). We then train SLMs by minimising the negative log-likelihood of the input segments. Unless mentioned otherwise, all SLMs are trained using single A5000 GPU (24GB VRAM) along with 16 CPU cores for 24 hours. We deliberately focus on this constrained compute budget, assuming that most academic labs can access similar resources, thereby ensuring the accessibility of our research. The training data is pre-processed, i.e. extracting HuBERT units and dividing data into chunks, and stored prior to model training. As result, this pre-processing time is excluded from the compute budget. This approach, aligned with Geiping and Goldstein (2023), is practical since many research experiments utilise the same pre-processed data. We additionally do not count the time for running validation and visualisations as they are not used as part of the optimisation pipeline and only used for demonstration purposes. Evaluation metrics. We assess all SLMs using four distinct evaluation metrics. The first three are based on likelihood evaluation, while the fourth is generative metric. For likelihood based modelling we consider sBLIMP (Dunbar et al., 2021), Spoken Story Cloze (SSC)), and Topic StoryCloze (TSC) (Hassid et al., 2024). For modellinglikelihood metrics, we evaluate the likelihood assigned by the SLMs to pairs of speech utterances, consisting of positive example and distractor. We calculate the percent of pairs in which the SLM assigns higher likelihood to the positive sample. sBLIMP focuses on grammatical abilities thus the negative is ungrammatical version of the positive. SSC and TSC focus on semantic modelling abilities. In SSC, the distractor suffix is taken from the original textual StoryCloze dataset (Mostafazadeh et al., 2016), allowing to assess fine-grained semantic speech understanding. In TSC, however, the distractor suffix is drawn from different topic, enabling us to evaluate the models ability to understand the overall semantic concept. Finally, to assess the generative abilities of SLMs, we compute generative perplexity (GenPPL). Following the approach of (Lakhotia et al., 2021; Hassid et al., 2024), we provide the SLM with short speech prompt and generate speech tokens continuation. We use unit-vocoder with duration prediction to convert the tokens into speech (Polyak et al., 2021; Hassid et al., 2024). The generated speech is then transcribed, and its Perplexity (PPL) is evaluated using pre-trained text LLM. To minimise the impact of token repetition on PPL measurements, we ground the generated text using diversity metrics derived from the auto-BLEU score (Lakhotia et al., 2021). Similarly to Lin et al. (2024) we use bigram autoBLEU. In other words, we ensure that all models achieve similar auto-BLEU scores, allowing for fair comparison of PPL. Specifically, we transcribe speech segments using Whisper-large-v3turbo model (Radford et al., 2023) and measure PPL using Llama-3.2-1B model (LLama, 2024). Software efficiency. To maximise performance 3 within 24 hours of model training, we leverage multiple efficient implementations. Through extensive performance testing, we found that using bfloat16 (Kalamkar et al., 2019) alongside FlashAttention2 (Dao, 2023) and data packing provided the most efficient compute performance in our setup. We also experimented with model compilation using torch.compile (Ansel et al., 2024), but it lacked native compatibility with FlashAttention2 at the time of our study, and its performance without FlashAttention2 was subpar. Future work could investigate this further with more efficient attention implementations (Shah et al., 2024; Li et al., 2024). To enable rapid and scalable experimentation, we developed specialised library for SLM training that supports various model architectures, training objectives, and evaluation metrics. This library accommodates both TWIST-style training, textspeech interleaving, preference optimisation, etc. We will open-source this package along all models weights and training recipes, aiming to empower the community to further explore SLMs."
        },
        {
            "title": "Investigations",
            "content": "With this setup, we systematically analyse and ablate each component of the training pipeline, ultimately refining an optimised cook-book for training SLMs. We specifically examine the influence of model family, initialisation, size, and architectural choices (e.g., dropout, positional embedding, etc.). We analyse optimisation parameters and data characteristics. Lastly, we explore alternative training objectives beyond standard next-token prediction, including speech-text interleaving and direct preference optimisation using synthetic data."
        },
        {
            "title": "4.1 Model & Optimisation",
            "content": "Hyper-parameters. Unless specified otherwise, we use context length of 512 tokens and an effective batch size of 256, employing gradient accumulation when necessary, as preliminary results indicated this configuration yields the best overall performance. We set the peak learning rate to 1e3 to enhance training speed and use warmup period of 1% of the total training steps, as this proved more effective than the fixed 100-step warmup used in the original TWIST. To improve training stability, particularly with large learning rates, we apply gradient normalisation with norm of 0.5 at no additional cost, following Geiping and Goldstein (2023). Unless modified later in our investigation, Figure 2: Comparing PPL of different models of similar parameter count, with and without TWIST initialisation. we use an inverse-square root scheduler and the AdamW optimiser (Loshchilov, 2017). Initialisation. Hassid et al. (2024) empirically demonstrated that initialising SLMs with pretrained text LMs can enhance convergence speed and improve model performance. We examine the effect of this initialisation within our setup across different model types. To do so, we train multiple models, both with and without TWIST initialisation, while staying within our compute budget. As shown in Figure 2, TWIST initialisation benefits all evaluated models at the beginning of training, though its overall impact by the end varies. Notice, the x-axis in Figure 2 represents theoretical FLOPs, calculated as 6 Nparams Dtokens following Hoffmann et al. (2022). However, due to variations in model architecture and implementation, practical efficiency differs, leading to varying amounts of compute processed within 24 hours. Results suggest that benefits of TWIST initialisation can be substantial, especially for topperforming models like Qwen2.5. As result, we prioritise investigations based on existing pretrained text LMs. Interestingly, the results in Figure 2 demonstrate that Qwen2.5 outperforms other models even without TWIST initialisation, perhaps suggesting that their architectural design choices or size might also provide some benefit. Optimal model size & family. Cuervo and Marxer (2024) conducted scaling analysis on GSLM-style SLMs, estimating the optimal model size and token count for compute-efficient model. However, using text LM initialisation might impact these findings. As we observe, TWIST initialisa4 Figure 3: Comparing PPL of different models under TWIST initialisation. Figure 4: Comparing validation PPL of our best model with different optimisers and schedulers. tion greatly impact model performance, suggesting that prioritising larger models may be more effective than simply increasing the dataset size. Additionally, various model families gain different advantages from TWIST initialisation; for example, Qwen2.5 models show significantly better performance compared to OPT models. In Figure 3, we compare the results under the pre-defined compute budget within model families1. We note that the best model sizes for both MobileLLM (Liu et al., 2024b), SmolLM2 (Allal et al., 2025) and Pythia (Biderman et al., 2023) are 300M parameters, while for OPT the best is 125M. According to Cuervo and Marxer (2024), the estimated optimal model size is approximately 66M parameters. However, the best-performing model, Qwen2.5, is significantly larger. Since there are no smaller models in this family, it is difficult to determine whether this deviation is due to the quality of the initialisation or other factors. Moving forward, we proceed with both OPT-125M and Qwen2.5-0.5B. Dropout. The original OPT models includes dropout to mitigate overfitting. Although dropout is beneficial for regularisation, it effectively decreases the number of gradient updates per parameter without shortening the update-step wall time. Hence, reduces the number of parameter updates per second. Following Geiping and Goldstein (2023), we experiment with removing dropout and observed improved performance in our setup. Positional Encoding. Transformers rely on posi1We use the text LM original names for clarity, but note that the actual size will be notably smaller due to reduced vocabulary size, e.g Qwen2.5-0.5B has 358M parameters. Full model sizes can be found in Appendix B. tional encoding to capture the order of input tokens. Many modern LMs, including the Qwen models, use Rotary Position Embedding (Su et al., 2024). This method uses hyperparameter, θ, to control the trade-off between granularity and the ability to handle long contexts. θ is often tuned to accommodate longer context lengths (Yang et al., 2024a; Roziere et al., 2023). Since our context length is significantly shorter than that of the original LLM, we explore reducing θ for potential performance gains. Our findings show that setting θ = 10, 000 with context length of 1024 enhances performance, so we adopt this configuration moving forward. We note that since we increase the context length, we need to reduce the batch size as well, to not run into memory problems when training. We reduce the batch size by half and keep the same amount of gradient accumulation steps, which gives us an effective batch size of 128. Optimiser and Scheduler. Various optimisers and schedulers have been developed to enhance training efficiency, reduce memory usage (Shazeer and Stern, 2018; Dettmers et al., 2022), or accelerate convergence (Pagliardini et al., 2024; Chen et al., 2023). With limited compute, faster convergence and better memory efficiency can be especially important. We first consider efficient optimisers, specifically AdamW with fused kernels, and 8-bit AdamW, but observe no notable improvements in batch size or runtime compared to standard AdamW. This could do with the relatively small model size, resulting in minimal memory footprint of the optimisers. We then compare AdamW with two state-of-the-art optimisers: AdaLomo (Lv et al., 2023) and AdEMAMeix (Pagliardini et al., 5 Model Data Metric OPT125M Qwen-0.5B Div. Syn. sBLIMP sSC tSC GenPPL 55.28 55.06 55.88 55. 56.45 56.17 56.60 56.10 55.46 55.00 54.52 54.78 55.59 55.37 53.50 53.72 75.18 74.83 70.82 70.18 78.01 77.13 71.14 70.66 96.8 116.6 160.3 172. 88.3 101.3 145.4 161.8 Table 1: Analysing impact of training data diversity and synthetic data on SLM performance. The default Slam recipe does not use diverse data (only Libri-light and LibriSpeech), but uses the synthetic sTinyStories data. 2024). Results, presented in Figure 4, suggest that with the original InverseSqrt scheduler used by Hassid et al. (2024), using AdEMAMeix improves validation loss, compared to AdamW, with AdaLomo far behind. Next, we analyse cosine decay learning rate scheduler, in place of the original InverseSqrt as this was shown to improve convergence (Loshchilov and Hutter, 2016). We consider the previous optimisers, and provide the validation loss throughout training in Figure 4. We see that this notably improved the loss for AdamW, and slightly harmed results for AdEMAMeix. Overall, AdamW with cosine schedule provide the best setup, far outperforming the original setup."
        },
        {
            "title": "4.2 Data",
            "content": "Next, we examine how the training data-mix influences performance in compute-constrained setting. Specifically, we explore whether diversity in accents, speaking styles, etc. is beneficial and assess whether synthetic data can enhance semantic modelling abilities. Diverse Data. We begin by examining how dataset diversity impacts model performance. Many leading speech datasets, such as those based on audiobooks (Panayotov et al., 2015; Kahn et al., 2020), consist of relatively clean, single-speaker recordings within specific content domain. To introduce greater diversity in speaking styles and content, we curate additional datasets, including VoxPopuli (Wang et al., 2021b), Tedlium (Hernandez et al., 2018), PeopleSpeech (Galvez et al., 2021), and SWC (Baumann et al., 2018). For all mentioned datasets, we use the official data cleaning and preprocessing scripts when available. Specifically, for Libri-light, we apply the official Voice Activity Figure 5: Analysing the optimal part of the 24 hour compute budget that should be used for DPO, with the rest used for pre-training. Detection model to remove silences and generate smaller audio segments. To evaluate the impact of dataset diversity, we compare the performance of SLMs trained using our best training recipes using subset of LibriSpeech and Libri-light against all curated datasets. This comparison is conducted for both OPT-125M, which processes large number of tokens during training, and Qwen-0.5B, which encounters significantly less data due to model size. Results are summarised in Table 1. We observe that dataset diversity has an overall negative effect on model performance. We hypothesise this is due to the models struggling in modelling rich and complex audio under such low compute resources. Synthetic Data. Recent studies have highlighted the potential of synthetic data generated through Text-to-Speech (TTS) (Cuervo and Marxer, 2024) or direct text-to-unit conversion (Zeng et al., 2024). Hence, we examine the impact of including synthetically generated speech within our constrained compute setup. To do so, we synthesised the TinyStories dataset (Eldan and Li, 2023) using single-speaker TTS model (Wang et al., 2021a), as it is computationally efficient. Additionally, prior research has shown that HuBERT units largely remove speaker information (Maimon and Adi, 2023). TinyStories has been demonstrated to enhance text LM performance and improve SLMs (Cuervo and Marxer, 2024). Results are presented in Table 1. Results indicate that incorporating such synthetic data into the training data-mix significantly boosts both modelling and generative performance metrics, across all evaluated setups. We also consider adding the synthetic data to the original TWIST recipe, and the results in the botCompute (GPU days) Parameters sBLIMP sStoryCloze tStoryCloze GenPPL Auto-BLEU TWIST-350M (Hassid et al., 2024) TWIST-1.3B (Hassid et al., 2024) TWIST-7B (Hassid et al., 2024) TWIST-13B (Hassid et al., 2024) Scaled Optimal (Cuervo and Marxer, 2024) Predicted Optimal (Cuervo and Marxer, 2024) TWIST-350M (Original recipe) TWIST-350M + sTinyStories Slam (-DPO) (ours) Slam (ours) 40*V100 160*V100 ? ? ? 1*A5000 1*A5000 1*A5000 1*A5000 1*A5000 305M 1B 7B 13B 823M 78M 305M 305M 358M 358M 56.20 57.00 59.00 59.20 61.3 56.85 - 52.4 55.3 55.4 56.7 - 70.6 74.1 76.4 78.0 54. 70.49 137.3 131.8 93.7 - - - 3.46 3.20 3.06 - - - 51.52 .19 51.21 .26 56.45 .17 58.86 . 53.65 .57 54.17 .54 55.59 .30 58.04 .51 68.80 .47 72.40 .18 78.01 .27 82.04 .21 259.2 6.7 159.0 6.0 88.3 1.0 62.8 4.1 3.26 .46 4.18 .24 3.47 .17 3.88 .11 Table 2: Comparing slamming to leading SLMs, and predicted optimal performance for the compute. We also consider TWIST-350M using our code and compute budget, but with the original training recipe. indicates distance to min/max of 3 seeds. tom of Table 2 suggests that while this helps with semantic metrics, it is far from enough without other optimisations we introduced. We see that across all datasets, and specifically with our best mixture Libri-Light, LibriSpeech and sTinyStories, Qwen-0.5B outperforms OPT-125M so we continue with it to the final stages."
        },
        {
            "title": "4.3 Text Interleaving",
            "content": "Several recent SLMs combine both speech and text modalities, either predicting both simultaneously (Défossez et al., 2024; Fang et al., 2024; Xie and Wu, 2024) or training on interleaved data (Nguyen et al., 2025; Zeng et al., 2024). Beyond enhancing cross-modal abilities, this has been shown to improve the semantic capabilities of SLMs, even in speech-only evaluations. Building on these studies, we investigate whether speech-text interleaving can enhance semantic ability in speech-only tasks, even under strict computational constraints. For this we use Whisper-large-v3-turbo to get aligned transcriptions of our data, except sTinyStories for which we get alignment from the TTS. We follow Zeng et al. (2024) by selecting speech spans with length from Poisson distribution with λ = 10 totalling 30% of the interleaved data. Following Nguyen et al. (2025) we train with balanced batches regarding token count between text data, speech data and interleaved data. We use subset of RedPajama (Weber et al., 2024) filtered by Gopher (Rae et al., 2021) rules as our text data. We find that under our setup performance is notably worse than speech-only Slam in all metrics. We hypothesise that in the slamming setup the larger vocabulary (leading to more parameters), and fewer speech tokens led to under-trained models. We leave for future work to find the minimal compute budget to benefit from text-interleaving or to consider other efficient approaches."
        },
        {
            "title": "4.4 Synthetic Data Preference Optimisation",
            "content": "Preference optimisation methods have been shown to enhance the performance of text LLMs (Ouyang et al., 2022) and, more recently, SLMs (Lin et al., 2024). With preference optimisation, we aim to train our model to generate outputs that better align with specified reward function or preference set. We evaluate how preference optimisation affects SLM performance while considering our constrained computational budget. Using an off-policy approach with pre-generated preference data, we apply DPO to enhance training efficiency. Specifically, we synthetically generate the SWAG (Zellers et al., 2018) text corpus for evaluating semantic knowledge. SWAG consists of text prefixes paired with multiple possible suffixes, where only one is semantically plausible. For preference data, we use the first sentence as the prompt, the correct suffix as the positive continuation, and randomly chosen incorrect suffix as the rejected continuation. To ensure quality, we filter out samples with repetitive patterns, identified by an auto-BLEU score above 0.3. We generate all recordings using Kokoro TTS (Hexgrad, 2025), incorporating four speakers (two male and two female), evenly split between British and American accents. This process results in total of 47k SWAG preference pairs. For DPO we use β = 0.1 (see Appendix for full hyperparameters). In initial tests, we observe that after DPO training, the model shows increased likelihood at the cost of repeated patterns, known issue with DPO (Lanchantin et al., 2025). To address this, we apply repetition penalty with factor of 1.1, following the approach of Keskar et al. (2019), and find that it helps mitigate the problem. Future work could explore alternative solutions, such as proposed by Lanchantin et al. (2025)."
        },
        {
            "title": "We begin by examining how the allocation of",
            "content": "7 GPUs Params Num tokens sBLIMP sStoryCloze tStoryCloze GenPPL Auto-BLEU Speech only pre-training GSLM (Lakhotia et al., 2021) SyllableLM (Baade et al., 2024) TWIST-350M (Hassid et al., 2024) TWIST-1.3B (Hassid et al., 2024) TWIST-7B (Hassid et al., 2024) TWIST-13B (Hassid et al., 2024) Scaled Optimal (Cuervo and Marxer, 2024) Moshi (Défossez et al., 2024) SpiritLM (Nguyen et al., 2025) 8*V100 4*A40 8*V100 32*V100 32*V100 32*V100 ?*H100 64*A100 100M 300M 305M 1B 7B 13B 823M 7B 7B 1B 16B 10.8B 10.8B 36B 36B 82B ? 100B Joint speech-text pre-training / preference optimisation Zeng et al. (2024) Moshi (Défossez et al., 2024) SpiritLM (Nguyen et al., 2025) AlignSLM-1.3B (Lin et al., 2024) AlignSLM-7B (Lin et al., 2024) ?*H100 64*A100 64*A100 64*A100 9B 7B 7B 1B 7B 1T 720B 100B 10.8B + 158B 36B + 158B Slam (-DPO) Slam Slam (scaled) 2*A100 1*A5000 2*A100 358M 358M 358M 16.7B 1.4B + 5M 16.7B + 9M 54.2 63.7 56.20 57.00 59.00 59.20 61.3 58.9 58.0 58.8 58.3 59.8 62.3 58.53 58.86 61.11 53.3 52.4 55.3 55.4 56.7 58.7 54.8 62.4 60.8 61.0 55.0 61. 58.15 58.04 61.30 66.6 75.4 70.6 74.1 76.4 78.0 81.8 72.9 82.9 83.0 82.9 80.0 86.8 80.71 82.04 84.18 137.3 131.8 93.74 67.3 62.8 46.6 3.46 3.20 3.06 3.25 3.88 3.75 Table 3: Analysing the effect of scaling up compute for Slam. Number tokens refers to total, not necessarily unique, tokens used for training (estimated from the provided information). We separately mark DPO tokens with +. budget for DPO impacts performance, particularly when it comes at the cost of shorter pre-training phase. Figure 5 depicts the results. We observe significant improvements across all metrics when applying DPO for at least 30 minutes compared to not using DPO at all. However, allocating higher proportion of the budget to DPO does not yield further gains and can even degrade model performance. Thus we stick to 30 minutes out of 24 hours for DPO, using the rest for pre-training."
        },
        {
            "title": "5 Final Recipe",
            "content": "Building on these empirical findings, we develop the final Slam recipe. Using it, we train SLMs based on Qwen2.5-0.5B. We then compare Slam to the TWIST model family across various sizes: 350M, 1.3B, 7B, and 13B. We also present results for TWIST-350M using our computational constraints but following TWISTs original training recipe, along with our synthetic data. Finally, we report results for the top-performing model from (Cuervo and Marxer, 2024), including their predicted optimal performance under our compute budget based on SLM scaling laws. Results are reported in Table 2. The results indicate that Slam delivers performance that is either superior or on par with baseline models while requiring significantly fewer computational resources (e.g., single A5000 for day compared to 160 days on V100)."
        },
        {
            "title": "Increasing Compute",
            "content": "Similarly to Geiping and Goldstein (2023), we analyse whether the proposed approach holds well also in increased compute budget. We opt for 48 hours on 2 A100 GPUs as reasonable academic budget for larger scale tests, and represents 10 times more compute than the Slamming setting. We use exactly the same Slam recipe for more steps, and increase the batch size times 2. We provide the full results in Table 3. We note that the performance continues to improve across all metrics, also outperforming methods which have far larger compute scales. We note that DPO training on synthetic data for 2 epochs, notably boosts performance."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work we show that training high quality SLMs with very modest compute budget, is feasible. We give these main guidelines: (i) Do not skimp on the model - not all model families are born equal and the TWIST initialisation exaggerates this, thus it is worth selecting stronger / bigger text-LM even if it means less tokens. we found Qwen2.5 to be good choice; (ii) Utilise synthetic training data - pre-training on data generated with TTS helps lot; (iii) Go beyond next token prediction - we found that DPO boosts performance notably even when using synthetic data, and as little as 30 minutes training massively improves results; (iv) Optimise hyper-parameters - as researchers we often dis-regard this stage, yet we found that tuning learning rate schedulers and optimising code efficiency can improve results notably. We hope that these insights, and open source resources will be of use to the research community in furthering research into remaining open questions in SLMs."
        },
        {
            "title": "Limitations",
            "content": "While the SLMs trained under Slamming compute budget performed notably well compared to other SLMs trained with much more compute they might perform less well in other areas. For instance, evaluating their abilities on acoustic or prosodic elements as in SALMon (Maimon et al., 2024) could show further challenges of low resource settings. Furthermore, we focus in this study on the well used HuBERT (Hsu et al., 2021) model as tokeniser, and while we do not make any adjustments specifically for it, future work might wish to investigate our cramming approach with new tokenisers, such as Mimi (Défossez et al., 2024) and SylBoost (Baade et al., 2024)."
        },
        {
            "title": "Ethical Statement",
            "content": "The broader impact of this study is, as in any generative model, the development of high quality and natural speech synthesis. We hope that allowing training SLMs under low-resource settings, and open sourcing resources to aid this goal, will have positive impact on inclusivity and accessibility of SLM research beyond well funded labs."
        },
        {
            "title": "References",
            "content": "Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, et al. 2025. Smollm2: When smol goes bigdata-centric training of small language model. arXiv preprint arXiv:2502.02737. Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. 2024. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pages 929947. Alan Baade, Puyuan Peng, and David Harwath. 2024. Syllablelm: Learning coarse semantic units for speech language models. arXiv preprint arXiv:2410.04029. Timo Baumann, Arne Köhn, and Felix Hennig. 2018. The spoken wikipedia corpus collection: Harvesting, alignment and an application to hyperlistening. Language Resources and Evaluation, 53:303 329. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: suite for analyzing large language modIn International els across training and scaling. Conference on Machine Learning, pages 23972430. PMLR. Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. 2023. Audiolm: language modeling approach to audio generation. IEEE/ACM transactions on audio, speech, and language processing, 31:25232533. Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V. Le. 2023. Symbolic discovery of optimization algorithms. Preprint, arXiv:2302.06675. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. 2023. Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models. arXiv preprint arXiv:2311.07919. Santiago Cuervo and Ricard Marxer. 2024. Scaling properties of speech language models. arXiv preprint arXiv:2404.00685. Wenqian Cui, Dianzhi Yu, Xiaoqi Jiao, Ziqiao Meng, Guangyan Zhang, Qichao Wang, Yiwen Guo, and Irwin King. 2024. Recent advances in speech language models: survey. arXiv preprint arXiv:2410.03751. Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. Preprint, arXiv:2307.08691. Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. 2024. Moshi: speechtext foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037. Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 2022. 8-bit optimizers via block-wise quantization. Preprint, arXiv:2110.02861. Ewan Dunbar, Mathieu Bernard, Nicolas Hamilakis, Tu Anh Nguyen, Maureen De Seyssel, Patricia Rozé, Morgane Rivière, Eugene Kharitonov, and Emmanuel Dupoux. 2021. The zero resource speech challenge 2021: Spoken language modelling. arXiv preprint arXiv:2104.14700. Ronen Eldan and Yuanzhi Li. 2023. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759. Avishai Elmakies, Omri Abend, and Yossi Adi. 2025. Unsupervised speech segmentation: general approach using speech language models. Preprint, arXiv:2501.03711. Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. 2024. Llama-omni: Seamless speech interaction with large language models. arXiv preprint arXiv:2409.06666. Daniel Galvez, Greg Diamos, Juan Manuel Ciro Torres, Juan Felipe Cerón, Keith Achorn, Anjali Gopi, David Kanter, Max Lam, Mark Mazumder, and Vijay Janapa Reddi. 2021. The peoples speech: large-scale diverse english speech recognition dataset for commercial usage. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1). Jonas Geiping and Tom Goldstein. 2023. Cramming: Training language model on single gpu in one day. In International Conference on Machine Learning, pages 1111711143. PMLR. Albert Gu, Karan Goel, and Christopher Ré. 2021. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396. Habib Hajimolahoseini, Omar Mohamed Awad, Walid Ahmed, Austin Wen, Saina Asani, Mohammad Hassanpour, Farnoosh Javadi, Mehdi Ahmadi, Foozhan Ataiefard, Kangling Liu, et al. 2023. Swiftlearn: data-efficient training method of deep learning models using importance sampling. arXiv preprint arXiv:2311.15134. Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, et al. 2024. Textually pretrained speech language models. Advances in Neural Information Processing Systems, 36. François Hernandez, Vincent Nguyen, Sahar Ghannay, Natalia Tomashenko, and Yannick Esteve. 2018. Tedlium 3: Twice as much data and corpus repartition for experiments on speaker adaptation. In Speech and Computer: 20th International Conference, SPECOM 2018, Leipzig, Germany, September 1822, 2018, Proceedings 20, pages 198208. Springer. Hexgrad. 2025. Kokoro-82m (revision d8b4fc7). Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM transactions on audio, speech, and language processing, 29:34513460. Peter Izsak, Moshe Berchansky, and Omer Levy. 2021. How to train bert with an academic budget. arXiv preprint arXiv:2104.07705. Shengpeng Ji, Yifu Chen, Minghui Fang, Jialong Zuo, Jingyu Lu, Hanting Wang, Ziyue Jiang, Long Zhou, Shujie Liu, Xize Cheng, et al. 2024. Wavchat: survey of spoken dialogue models. arXiv preprint arXiv:2411.13577. Jacob Kahn, Morgane Riviere, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazaré, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al. 2020. Libri-light: benchmark for asr with limited or no supervision. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 76697673. IEEE. Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. 2019. study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322. Nitish Shirish Keskar, Bryan McCann, Lav Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858. Eugene Kharitonov et al. 2021. Text-free prosodyaware generative spoken language modeling. arXiv preprint arXiv:2109.03264. Felix Kreuk et al. 2021. Textless speech emotion conversion using decomposed and discrete representations. arXiv preprint arXiv:2111.07402. Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. 2021. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336 1354. Jack Lanchantin, Angelica Chen, Shehzaad Dhuliawala, Ping Yu, Jason Weston, Sainbayar Sukhbaatar, and Ilia Kulikov. 2025. Diverse preference optimization. arXiv preprint arXiv:2501.18101. Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Yi Ren, Heriberto Cuayáhuitl, Wenwu Wang, Xulong Zhang, Roberto Togneri, Erik Cambria, et al. 2023. Sparks of large audio arXiv preprint models: survey and outlook. arXiv:2308.12792. Junyan Li, Delin Chen, Tianle Cai, Peihao Chen, Yining Hong, Zhenfang Chen, Yikang Shen, and Chuang Gan. 2024. Flexattention for efficient high-resolution vision-language models. In European Conference on Computer Vision, pages 286302. Springer. Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, et al. 2023. Flm-101b: An open llm and how to train it with $100 budget. arXiv preprint arXiv:2309.03852. 10 Guan-Ting Lin, Prashanth Gurunath Shivakumar, Aditya Gourav, Yile Gu, Ankur Gandhe, Hung-yi Lee, and Ivan Bulyko. 2024. Align-slm: Textless spoken language models with reinforcement learning from ai feedback. arXiv preprint arXiv:2411.01834. Andy Liu, Yi-Cheng Lin, Haibin Wu, Stefan Winkler, and Hung-yi Lee. 2024a. Efficient training of selfsupervised speech foundation models on compute budget. In 2024 IEEE Spoken Language Technology Workshop (SLT), pages 961968. IEEE. Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. 2024b. Mobilellm: Optimizing sub-billion parameter language models for on-device use cases. In Forty-first International Conference on Machine Learning. Team LLama. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Loshchilov. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. Ilya Loshchilov and Frank Hutter. 2016. Sgdr: StochasarXiv tic gradient descent with warm restarts. preprint arXiv:1608.03983. Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, and Xipeng Qiu. 2023. Adalomo: Low-memory optimization with adaptive learning rate. arXiv preprint arXiv:2310.10195. Xingtai Lv, Ning Ding, Kaiyan Zhang, Ermo Hua, Ganqu Cui, and Bowen Zhou. 2024. Scalable efficient training of large language models with lowarXiv preprint dimensional projected attention. arXiv:2411.02063. Gallil Maimon and Yossi Adi. 2023. Speaking style conversion in the waveform domain using discrete self-supervised units. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 80488061. Gallil Maimon, Amit Roth, and Yossi Adi. 2024. suite for acoustic language model evaluation. arXiv preprint arXiv:2409.07437. Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. corpus and evaluation framework for deeper understanding of commonsense stories. arXiv preprint arXiv:1604.01696. Aashiq Muhamed, Oscar Li, David Woodruff, Mona Diab, and Virginia Smith. 2024. Grass: Compute efficient low-memory llm training with structured sparse gradients. arXiv preprint arXiv:2406.17660. Eliya Nachmani et al. 2024. Spoken question answering and speech continuation using spectrogram-powered In The Twelfth International Conference on llm. Learning Representations. Evgeny Hershkovitch Neiterman and Gil Ben-Artzi. 2024. Layerdropback: universally applicable approach for accelerating training of deep networks. arXiv preprint arXiv:2412.18027. Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta Costa-Jussa, Maha Elbayad, Sravya Popuri, Christophe Ropers, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, et al. 2025. Spiritlm: Interleaved spoken and written language model. Transactions of the Association for Computational Linguistics, 13:3052. Tu Anh Nguyen et al. 2022. Generative spoken dialogue language modeling. arXiv preprint arXiv:2203.16502. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Matteo Pagliardini, Pierre Ablin, and David Grangier. 2024. The ademamix optimizer: Better, faster, older. Preprint, arXiv:2409.03137. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 52065210. Se Jin Park, Julian Salazar, Aren Jansen, Keisuke Kinoshita, Yong Man Ro, and RJ Skerry-Ryan. 2024. Long-form speech generation with spoken language models. arXiv preprint arXiv:2412.18603. Jing Peng, Yucheng Wang, Yu Xi, Xu Li, Xizhuo Zhang, and Kai Yu. 2024a. survey on speech large language models. arXiv preprint arXiv:2410.18908. Yifan Peng et al. 2024b. Mslm-s2st: multitask speech language model for textless speech-to-speech translation with speaker style preservation. arXiv preprint arXiv:2403.12408. Adam Polyak et al. 2021. Speech resynthesis from discrete disentangled self-supervised representations. arXiv preprint arXiv:2104.00355. Sravya Popuri et al. 2022. Enhanced direct speechto-speech translation using self-supervised prearXiv preprint training and data augmentation. arXiv:2204.02967. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR. Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36. Ankit Singh Rawat, Veeranjaneyulu Sadhanala, Afshin Rostamizadeh, Ayan Chakrabarti, Wittawat Jitkrittum, Vladimir Feinberg, Seungyeon Kim, Hrayr Harutyunyan, Nikunj Saunshi, Zachary Nado, et al. 2024. little help goes long way: Efficient llm training by leveraging small lms. arXiv preprint arXiv:2410.18779. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. 2024. Flashattention-3: Fast and accurate attention with arXiv preprint asynchrony and low-precision. arXiv:2407.08608. Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. Preprint, arXiv:1804.04235. Li Shen, Yan Sun, Zhiyuan Yu, Liang Ding, Xinmei Tian, and Dacheng Tao. 2023. On efficient training of large-scale deep learning models: literature review. arXiv preprint arXiv:2304.03589. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and Chao Zhang. 2024. SALMONN: Towards generic hearing abilities for large language models. In The Twelfth International Conference on Learning Representations. Changhan Wang, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Ann Lee, Peng-Jen Chen, Jiatao Gu, and Juan Pino. 2021a. fairseq s^2: scalable and integrable speech synthesis toolkit. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 143152. Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. 2021b. VoxPopuli: large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 9931003, Online. Association for Computational Linguistics. Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. 2023. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111. Jiachen Wang, Tong Wu, Dawn Song, Prateek Mittal, and Ruoxi Jia. 2024. Greats: Online selection of high-quality data for llm training in every iteration. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, et al. 2024. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. arXiv preprint arXiv:2412.13663. Maurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and Ce Zhang. 2024. Redpajama: an open dataset for training large language models. NeurIPS Datasets and Benchmarks Track. Zhifei Xie and Changqiao Wu. 2024. Mini-omni: Language models can hear, talk while thinking in streaming. arXiv preprint arXiv:2408.16725. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024a. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Dongchao Yang et al. 2023. Uniaudio: An audio foundation model toward universal audio generation. arXiv preprint arXiv:2310.00704. Dongchao Yang et al. 2024b. Uniaudio 1.5: Large language model-driven audio codec is few-shot audio task learner. arXiv preprint arXiv:2406.10056. Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP). Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Shengmin Jiang, Yuxiao Dong, and Jie Tang. 2024. Scaling speech-text pre-training with synthetic interleaved data. arXiv preprint arXiv:2411.17607. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068."
        },
        {
            "title": "A Full Slam Recipe",
            "content": "We provide below the full training recipe, including hyperparameters for the best, Slam recipe. In Table 4 we see the Slam (-DPO) pre-training recipe and in Table 5 we see the Slam DPO training recipe. Table 4: Slam (-DPO) Pre Training Recipe Parameter Text Base Model TWIST initialisation Data Train Time RoPE theta Context length Per device Batch Size Gradient Accumulation Base Learning Rate Warmup Ratio Optimizer Learning Rate Scheduler Max Grad Norm Dtype Value Qwen2.5-0.5B True Librilight + Librispeech + sTinyStories 23.5 hours 17625 steps 10000 1024 8 16 1e 3 1% AdamW cosine with min 5e 5 0.5 bfloat Table 5: Slam DPO Training Recipe Parameter Initial Model Data Train Time RoPE theta Context length Per device Batch Size Gradient Accumulation Base Learning Rate Optimizer Learning Rate Scheduler Max Grad Norm Dtype DPO β Value Slam (-DPO) SpokenSwag with auto-bleu smaller than 0.3 0.5 hour 813 steps 10000 1024 4 16 5e 5 AdamW inverse sqrt 0.5 bfloat16 0."
        },
        {
            "title": "B Model Sizes",
            "content": "Table 6: Model names and parameter counts after changing vocabulary to speech only units (500). Model Name Number of Params MobileLLM-125M (Liu et al., 2024b) MobileLLM-350M (Liu et al., 2024b) OPT-125M (Zhang et al., 2022) OPT-350M (Zhang et al., 2022) QWEN2.5-0.5B (Yang et al., 2024a) SmolLM2-135M (Allal et al., 2025) SmolLM2-360M (Allal et al., 2025) Pythia-160M (Biderman et al., 2023) Pythia-410M (Biderman et al., 2023) 106,492,608 315,117,120 87,015,936 305,714,176 358,347,904 106,492,608 315,117,120 85,827,072 303,339,520 As mentioned, we use the original names of the text LMs used for clarity and consistency, but note that the actual parameter counts after resizing the vocabulary to speech-units only can be very different. In Table 6 we provide an extensive list of models and sizes."
        },
        {
            "title": "C Dataset Statistics",
            "content": "We use and synthesise several datasets. In this section we give exact details of number of samples, splits used, domains etc. For pre-training we use Libri-Light (Kahn et al., 2020) and LibriSpeech (Panayotov et al., 2015). For Libri-Light we randonly select one percent of samples as validation, whereas for LibriSpeech we use the original dev-clean and dev-other splits. Both of these datasets are English speech only, focused in the audio-book domain. We also synthesise sTinyStories for pre-training which consists of synthetically generated English short stories. We use the official train split for training. Full dataset sizes are in Table 7. We also investigate diverse datasets for pretraining: SWC (Baumann et al., 2018), Tedlium (Hernandez et al., 2018), PeopleSpeech (Galvez et al., 2021) and VoxPopuli (Wang et al., 2021b). We only take English subsets for all datasets, yet they can still contain diverse accents. These datasets are in the following domains SWC - read Wikipedia articles, Tedlium - short lectures, PeopleSpeech - diverse data including many local council gatherings etc, VoxPopuli - from European Parliament meetings. For SWC specifically, we use the text alignment to create chunks, remove silence from the audio and remove mis-aligned chunks. We use full training splits where provided, otherwise splitting 99% for training. The dataset sizes are described in 7. Table 7: Dataset train set sizes that we use. Dataset Number of Hours Number of Tokens Libri-Light (Kahn et al., 2020) LibriSpeech (Panayotov et al., 2015) SWC (Baumann et al., 2018) Tedlium (Hernandez et al., 2018) PeopleSpeech (Galvez et al., 2021) VoxPopuli (Wang et al., 2021b) sTinyStories 50K 960 750 1.6K 7K 24K 30K 3.5B 67M 19M 110M 480M 1.64B 2.2B For DPO we synthesise SpokenSwag based on the SWAG (Zellers et al., 2018) dataset. We use only the official train set and filter only the gold standard labels. We end up with 47k sample pairs which end up to be 4.5M tokens."
        },
        {
            "title": "D AI Tool Usage",
            "content": "AI based tools may have been used in writing parts of the code for this study, or para-phrasing some of the writing within the paper, yet all the content was thoroughly checked by the authors, with these only being used as assistive tools."
        }
    ],
    "affiliations": [
        "The Hebrew University of Jerusalem"
    ]
}