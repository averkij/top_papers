{
    "paper_title": "EfficientViM: Efficient Vision Mamba with Hidden State Mixer based State Space Duality",
    "authors": [
        "Sanghyeok Lee",
        "Joonmyung Choi",
        "Hyunwoo J. Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "For the deployment of neural networks in resource-constrained environments, prior works have built lightweight architectures with convolution and attention for capturing local and global dependencies, respectively. Recently, the state space model has emerged as an effective global token interaction with its favorable linear computational cost in the number of tokens. Yet, efficient vision backbones built with SSM have been explored less. In this paper, we introduce Efficient Vision Mamba (EfficientViM), a novel architecture built on hidden state mixer-based state space duality (HSM-SSD) that efficiently captures global dependencies with further reduced computational cost. In the HSM-SSD layer, we redesign the previous SSD layer to enable the channel mixing operation within hidden states. Additionally, we propose multi-stage hidden state fusion to further reinforce the representation power of hidden states, and provide the design alleviating the bottleneck caused by the memory-bound operations. As a result, the EfficientViM family achieves a new state-of-the-art speed-accuracy trade-off on ImageNet-1k, offering up to a 0.7% performance improvement over the second-best model SHViT with faster speed. Further, we observe significant improvements in throughput and accuracy compared to prior works, when scaling images or employing distillation training. Code is available at https://github.com/mlvlab/EfficientViM."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 2 ] . [ 1 1 4 2 5 1 . 1 1 4 2 : r EfficientViM: Efficient Vision Mamba with Hidden State Mixer based State Space Duality"
        },
        {
            "title": "Sanghyeok Lee",
            "content": "Joonmyung Choi Hyunwoo J. Kim* Department of Computer Science and Engineering, Korea University {cat0626, pizard, hyunwoojkim}@korea.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "have prior works environments, For the deployment of neural networks in resourcebuilt constrained lightweight architectures with convolution and attention for capturing local and global dependencies, respectively. Recently, the state space model has emerged as an effective global token interaction with its favorable linear computational cost in the number of tokens. Yet, efficient vision backbones built with SSM have been explored less. In this paper, we introduce Efficient Vision Mamba (EfficientViM), novel architecture built on hidden state mixer-based state space duality (HSM-SSD) that efficiently captures global dependencies with further reduced computational cost. In the HSM-SSD layer, we redesign the previous SSD layer to enable the channel mixing operation within hidden states. Additionally, we propose multi-stage hidden state fusion to further reinforce the representation power of hidden states, and provide the design alleviating the bottleneck caused by the memory-bound operations. As result, the EfficientViM family achieves new state-of-the-art speed-accuracy trade-off on ImageNet-1k, offering up to 0.7% performance improvement over the second-best model SHViT with faster speed. Further, we observe significant improvements in throughput and accuracy compared to prior works, when scaling images or employing distillation training. Code is available at https://github.com/mlvlab/EfficientViM . 1. Introduction Efficient vision architectures for resource-constrained environments, such as mobile and edge devices, have been consistently explored in computer vision tasks, including image classification [5, 23, 26, 44, 45, 60, 84], object detection [31, 51, 61, 69], segmentation [24, 73, 75, 76, 85], *Corresponding author. 1 Figure 1. Comparison of efficient networks on ImageNet1K [10] classification. The family of our EfficientViM, marked as red and blue stars, shows the best speed-accuracy trade-offs. indicates the model trained with distillation following [63]. etc. Earlier works [5, 18, 42] have explored the efficient convolutional neural networks (CNN). One such technique is depthwise separable convolutions (DWConv), introduced in Xception [5], which has been widely adopted for modeling lightweight architectures, especially CNN-based networks including MobileNet [22, 23, 53] and following works [60, 67]. Meanwhile, with the advent of Vision Transformers (ViTs) [12], attention mechanisms have become key operation for capturing long-range dependencies within image patches. However, the substantial computational complexity of selfattention presents challenges for designing efficient architectures. To address this, prior works have attempted to approximate self-attention with reduced cost [6, 29, 70, 74], or restrict the number of tokens [1, 4, 7, 30, 38, 78]. In parallel, for on-device deployment, several works [3, 36, 44, 45, 48, 66, 80, 83] devoted effort to developing hybrid ViTs combined with CNNs (DWConv). While prior works have demonstrated superior performance over traditional CNNs, the inherent quadratic complexity of self-attention in the number of tokens remains major bottleneck limiting their efficiency and scalability. Recently, state space models (SSMs) [9, 1416, 58] have emerged as promising alternative to self-attention, offering favorable linear computational complexity while maintaining global receptive field. Mamba [9, 15] has introduced selective scanning mechanisms on SSM (S6) to process sequences with the hardware-aware parallel algorithm. Following the success of Mamba in NLP, vision Mambas [25, 37, 77, 87] have extended the concept of SSM to vision tasks. These works have studied multi-path scanning mechanisms to address the causal constraints of SSM that are not desirable for image processing. More recent works like VSSD [56] and Linfusion [35] further eliminated the causal mask in the state space dual model (SSD) of Mamba2 [9], introducing non-causal state space duality (NC-SSD). While vision Mambas demonstrate improved performance over previous SOTA methods, they still have relatively slower speeds than lightweight vision models. In this paper, we present an Efficient Vision Mamba (EfficientViM), new family of mamba-based lightweight vision backbone built with fast yet effective SSD layer called Hidden State Mixer-based SSD (HSM-SSD). In the HSM-SSD layer, we transfer the channel mixing operations of the standard SSD layer, including linear projection and gating function, from the image feature space to the hidden state space. These hidden states are considered as the reduced latent representation of the sample. We observe that this design mitigates the major bottleneck of the SSD layer while maintaining the generalization ability of the model. Also, we introduce multi-stage hidden state fusion approach that generates model predictions by combining the original logits with those derived from the hidden states at each stage, leading to an enhanced representation power of the hidden state. After breaking down the runtime of the HSM-SSD layer, we present the macro design that minimizes memory-bound operations, prioritizing practical performance in real-world applications over theoretical metrics like FLOPs. Through extensive experiments, we demonstrate that our EfficientViM achieves the new speedaccuracy state-of-the-art trade-off as shown in Figure 1. In particular, EfficintViM-M2 outperforms the previous SOTA model SHViT [80], and pioneering work ModelNetV3 [22] with 0.6% performance improvement even bringing about 7% and 80% speed-ups, respectively. In summary, the contributions of EfficintViM are threefold: We propose novel mamba-based lightweight architecture called EfficientViM, leveraging the linear computational cost of the global token mixer. We introduce HSM-SSD, which makes the major overhead of the SSD layer controllable by adjusting the number of hidden states. With design minimizing memory-bound operations and incorporating multi-stage hidden state fusion, EfficientViM achieves the best speed-accuracy tradeoffs. 2. Preliminaries State space models (SSM). Inspired by the linear timeinvariant (LTI) continuous system, SSM maps the input sequence x(t) to the output sequence y(t) as h(t) = ˆAh(t) + ˆBx(t), y(t) = ˆCh(t), (1) where h(t) RN 1 is hidden state, ˆA RN , ˆB RN 1, ˆC R1N are the projection matrix, and is the number of states. To adapt this continuous-time system for discrete data in deep learning, given the multivariate input sequences = (cid:2)x RLD with txt R1D, Mamba [15] first generates parameters as ˆB, = xWB, xWC RLN , = xW RL, where WB, WC RDN , RD1 are learnable matrices. Then, the discretized form of SSM with zero-order hold discretization is defined as 1 , . . . , (cid:3) ht = Atht1 + xt, yt = Ctht, (2) where RLD, RN D, At = et ˆA RN , Bt = (t ˆA)1(et ˆA I) ˆBt ˆBt R1N . In this formulation, ˆA RN is learnable diagonal matrix, and all projection matrices At, Bt, Ct enable the linear time-variant discrete system that selectively attends to inputs and hidden state of each timestamp t. State space duality (SSD). Mamba2 [9] further simplifies the diagonal form of the evolution matrix ˆA into the scalar form as ˆa resulting in RL via the same discretization step. Then, the state space duality (SSD) reformulates Equation (2) as matrix transformation: = SSD(x, a, B, C) = (cid:0)M (CB)(cid:1) x, Mij = (cid:81)i k=j+1 ak 1 0 if > if = if < , (3) where RLL, and indicates Hadamard product. Note that the lower triangular matrix acts as causal mask, which is suboptimal for image processing. To address this, non-causal SSD (NC-SSD) [35, 87] has been studied as an alternative to SSD by defining the mask as Mij = (cid:81)L k=j+1 ak, resulting in M1j = M2j = . . . = MLj. Further, in VSSD [87], it is simplified as Mij = aj resulting in = NC-SSD(x, a, B, C) = Ch, = (a1 B)x = (cid:88)L i=1 aiB xi, (4) where 1N RN is one vector for broadcasting a. Since the cumulative multiplication of restricts the receptive fields as discussed in [56, 57], we adopt this version of NC-SSD as our starting point for an efficient token mixer."
        },
        {
            "title": "Method",
            "content": "Complexity O(LD2 + L2D) O(LD2) Attention [68] Linear Attention [28] O(LD2 + LN D) SSD [9], NC-SSD [56] O(N D2 + LN D) HSM-SSD Table 1. Complexity comparison of global tokens mixers. L: # tokens, : # states, D: # channels. 3. Method We present hidden state mixer-based SSD (HSM-SSD) for capturing global context with reduced costs, detailed in Section 3.1. Then, we discuss the additional techniques to improve both speed and performance with HSM-SSD in Section 3.2. After that, we outline the overall architecture and block designs to construct EfficientViM in Section 3.3. 3.1. Hidden State Mixer-based SSD We start with brief discussion on the computational cost of the NC-SSD layer depicted in Figure 2a. The entire process of the NC-SSD layer can be summarized as ˆB, C, , x, = Linear(xin) a, = Discretization(ˆa, ˆB, ), B, C, := DWConv(B, C, x), = NC-SSD(x, a, B, C), xout = Linear(y σ(z)), (5) (6) (7) (8) (9) where xin, xout, x, RLD, and σ is the activation function. The computational costs of Equations (5) to (7) with constant kernel size is O(LD2 + LN D). Subsequently, executing NC-SSD and the output projection requires O(LN D) and O(LD2), respectively. Given that the number of states is typically much smaller than the number of channels (i.e., D), the total complexity is predominantly determined by the linear projections involved in generating x, z, and xout, i.e., O(LD2). As result, optimizing the linear projection in SSD blocks is crucial for scalability. We delve into optimizing these computations for efficient layer designs. NC-SSD (Equation (4)) can be factorized into two steps. First, it obtains shared global hidden state RN through weighted linear combination of input xi RN using importance weights RL. states Second, the outputs for each input are generated by projecting hidden state with their corresponding RLN . Here, if we denote the projected input as xinWin removing DWConv, the following holds: = (a1 = ((a1 B)(xinWin) B)xin)Win = hinWin, (10) (a) NC-SSD layer (b) HSM-SSD layer Figure 2. Illustration of (left) NC-SSD and (right) HSM-SSD layer. In the HSM-SSD layer, the computationally heavy projections are handled with the reduced hidden state in HSM as highlighted. Red, blue, and orange colors indicate the operation requiring the complexities of O(LD2), O(LN D), and O(N D2). B)xin RN D. where Win RDD, and hin = (a1 By computing hin first, it becomes the linear projection to the hidden states. This approach reduces the cost from O(LD2) to O(N D2) which relies on the number of states. In other words, we could alleviate the major costs of the layer by adjusting the number of states like L. Hidden State Mixer. The next step is to alleviate the cost of gating and output projection in Equation (9), which still remains O(LD2). To address this, we focus on shared global hidden state h. Note that the hidden state itself is the reduced latent array that compresses the input data with significantly smaller sequence . Based on this observation, we propose Hidden State Mixer (HSM) that performs channel mixing, including the gating and output projection, directly on the reduced latent array as highlighted in Figure 2b. To this end, we approximate the output of the NC-SSD layer as follows: xout = (y) = Linear(y σ(z)) = (Ch σ(xinWz))Wout ((h σ(hinWz))Wout) = Cf (h), (11) where = Ch from Equation (4), and indicates channel mixing of gating function followed by linear projection with the learnable matrix Wz, Wout RDD. Contrary to the original NC-SSD layer where Ch is computed first and then fed to , we apply the gating and projection directly to the hidden states with HSM. Then, the final output xout is generated by projecting updated hidden states with C. Consequently, the total complexity of capturing global 3 Algorithm 1 HSM-SSD Layer Input: xin RLD Output: xout RLD 1: ˆB, C, Linear(xin) 2: ˆB, DWConv( ˆB, C) 3: A, Discretization(ˆa, ˆB, ) 4: hin (A B)xin 5: h, Linear(hin) 6: Linear(h σ(z)) 7: xout Ch 8: Return xout O(LN D) O(LN 2D) O(LD) O(LN D) O(N D2) O(N D2) O(LN D) from all stages, including the original logit z(0) obtained from the output of the last stage, which is defined as = (cid:88) s=0 (s) ˆβ z(s), (s) ˆβ = exp(β(s)) i=0 exp(β(i)) (cid:80)S , (13) where β(s) is learnable scalar. By training with this combined logit, we explicitly reinforce the representational power of the hidden states, as they contribute to the final predictions. It also enriches the information by integrating both low-level and high-level features, thereby enhancing the generalization ability of the model during inference. Single-head HSM-SSD. The multi-head design in the attention mechanism [68] allows it to selectively attend to the features from independent representation subspaces within each head. SSD-based models [9, 56] generally adopt multi-head variant called multi-input SSD, where the input and are defined for each head while B, and are shared across the head. However, recent work [80] has pointed out that significant portion of real runtime in multi-head selfattention is driven by memory-bound operations. In our preliminary experiments, we also figured out that multi-head configuration has become bottleneck for HSM-SSD as summarized in Figure 3. As shown in the figure, the real runtime of multi-head HSM-SSD is largely bounded by memory access, requiring almost quarter of the total runtime. Hence, we eliminate all tensor manipulation caused by multi-head (e.g., reshape, copy operation). Concurrently, to mimic the capability of multi-head in capturing diverse relationships, we set RLN , ˆa RN to enable the importance weights RLN estimate the importance of the tokens per state. Then, the input for the hidden state mixer becomes hin = (A B)xin. (14) As result, our single-head HSM-SSD with state-wise importance weights achieved higher throughput (17, Figure 3. Runtime breakdown of HSM-SSD with EfficientViMM2. Matrix multiplication is the only factor affecting FLOPs, while the others highlighted in red are memory-bounded. context in the HSM-SSD layer becomes O(N D2 + LN D), which is negligible as gets smaller. Refer to Table 1 for comparison of the big-O complexities with previous global token mixers. Proposition 1. Let = L, a1 = IL, and RLL be diagonal. Then, HSM-SSD(x, a, B, C) is equivalent to NC-SSD(x, a, B, C) including gating and output projection, as xout = (y) = Cf (h). See supplement for proof. Remark 1. Although we utilize gating and linear projection for the HSM to emulate the operation in the original SSD layer, other methods are also available. For example, inspired by the Perceiver architecture [27], we could implement global token mixer for the function in Eq. (11) to efficiently handle high-dimensional inputs using reduced set of latent variables. Moreover, HSM-SSD can recursively apply HSM-SSD itself as the hidden state mixer, further reducing computational complexity and forming recurrent architecture unrolled across depth. 3.2. HSM-SSD layer Multi-stage hidden state fusion. To further improve the performance of EfficientViM, we introduce multi-stage hidden-state fusion (MSF) mechanism that fuses the prediction logits leveraging hidden states from multiple stages of the network. Let {h(s)}S s=1 denote the hidden states at the last block of each stage s, where is the total number of stages. For each h(s), we compute global representation ˆh through simple average over the hidden states: (s) (s) ˆh ="
        },
        {
            "title": "1\nN",
            "content": "(cid:88)N i=1 h(s) . (12) Then, each global representation ˆh RD is normalized and projected to generate its corresponding logits z(s) Rc, where indicates the number of classes. We set the final logit of EfficientViM as weighted sum of the logits (s) 4 Figure 4. (left) Overall architecture and (right) block design of EfficientViM. The dotted line indicates skip connection for multi-stage hidden state fusion (MSF). Illustration of the HSM-SSD layer in the EfficientViM block is presented in Figure 2. img/s) with single-head over multi-head (15,703 img/s) with competitive performance (see Table 8.b) under similar FLOPs. The pseudocode of the HSM-SSD layer with the single-head design is provided in Algorithm 1. 3.3. EfficientViM In this subsection, we present EfficientViM, an efficient vision mambas built upon the HSM-SSD layer. The overall architecture is illustrated in Figure 4. Block design. In each block of EfficientViM, we sequentially stack the HSM-SSD layer and feed-forward network (FFN) to facilitate global information aggregation and channel interaction, respectively. FFN layer consists of two consecutive 11 convolution layers, known as pointwise convolution, with an expansion ratio of 4. To capture the local context with minimal computational costs, we incorporate 33 depthwise convolution (DWConv) layer before both the NC-SSD and FFN layers. Each layer is combined with residual connection using layer scale following [64, 65]. For normalization, we apply layer normalization (LN) only before the HSM-SSD layer for numerical stability, while batch normalization (BN) is used for DWConv and FFN considering its faster speed over LN. Overall architecture. The stem layer first maps the 3 input image to the down-sized feature map 16 16 D1 through the four consecutive 33 convolutional layers with the stride of 2. Then, the resulting feature map is fed into the three stages built with EfficientViM blocks. To achieve hierarchical architecture and improve efficiency, we downscale the feature map while increasing the number of channels at the end of each stage via the downsampling layer adopted from [36, 53, 87]. Regarding activation functions, we use SiLU only in the HSM-SSD layer, and others use ReLU [47] since the latencies of the complex activation functions (e.g., Gelu [21], DynamicReLU [2], etc.) are largely dependent on devices, as discussed in previous works [36, 67, 80]. Detailed architecture specifications of EfficientViM variants are available in Table 2. Model # Blocks # Channels # States EfficientViM-M1 EfficientViM-M2 EfficientViM-M3 EfficientViM-M4 [2, 2, 2] [2, 2, 2] [2, 2, 2] [3, 4, 2] [128, 192, 320] [128, 256, 512] [224, 320, 512] [224, 320, 512] [49, 25, 9] [49, 25, 9] [49, 25, 9] [64, 32, 16] Table 2. Specification of EfficientViM variants. 4. Experiments In this section, we demonstrate the effectiveness of EfficientViM in image classification (Section 4.1). Then, we conduct experiments to analyze the extensibility of EfficientViM with scaled images and distillation in Section 4.2. We also provide ablation studies and analysis of EfficientViM in Section 4.3. See the supplement for implementation details and more experiments. 4.1. Image Classification. Comparison with efficient vision backbones. For comparison of EfficientViM with prior works, we conduct ImageNet-1K [10] classification. To validate the effectiveness of EfficietViM in speed-accuracy trade-offs, we measure the throughput (im/s), and latency (ms) with the batch size of 256 in NVIDIA RTX 3090 along with the accuracy and present the results in Table 3. In summary, EfficientViM outperforms all previous efficient networks in both speed and accuracy. After training 450 epochs, EfficientViMM1 shows competitive performance with MobileNetV3-L 0.75 [22] and EfficientViT-M3 [36] while achieving about 90% and 30% speedup. Further, EfficientViM-M2 achieves about 4 faster speed, with 0.2% performance improvement compared to MobileViTV2 0.75 [45] and FastViTT8 [66]. EfficientViM-M3 and M4 achieve 77.9% and 79.7% accuracy, respectively, surpassing all previous works in throughput and accuracy within each section. Also, EfficientViM consistently outperforms the previous SOTA network, SHViT [80], across model sizes while reducing latency, which demonstrates the superiority of EfficientViM. 5 Method Venue MobileViTV2 0.5 [45] MobileOne-S0 [67] EMO-1M [83] MobileFormer-96M [3] SHViT-S1 [80] EfficientViM-M1 MobileNetV3-L 0.75 [22] EfficientViT-M3 [36] EfficientViM-M1 Arxiv 2022 CVPR 2023 ICCV 2023 CVPR 2022 CVPR 2024 - ICCV 2019 CVPR 2023 - EfficientFormerV2-S0 [33] NeurIPS 2022 CVPR 2023 EfficientViT-M4 [36] ECCV 2022 EdgeViT-XXS [48] ICCV 2023 EMO-2M [83] ICCV 2019 MobileNetV3-L 1.0 [22] CVPR 2022 MobileFormer-151M [3] CVPR 2024 SHViT-S2 [80] EfficientViM-M2 - Arxiv 2022 MobileViTV2 0.75 [45] ICCV 2023 FastViT-T8 [66] EfficientViM-M2 - ICLR 2024 EfficientMod-XXS [43] CVPR 2023 ConvNeXtV2-A [72] CVPR 2023 EfficientViT-M5 [36] CVPR 2023 MobileOne-S2 [67] CVPR 2024 SHViT-S3 [80] ECCV 2022 EdgeViT-XS [48] EfficientViM-M3 - MobileFormer-294M [3] CVPR 2022 EfficientFormerV2-S1 [33] NeurIPS 2022 EfficientViM-M3 - ConvNeXtV2-F [72] MobileViTV2 1.0 [45] MobileOne-S3 [67] EfficientMod-XS [43] EMO-6M [83] FastViT-T12 [66] MobileFormer-508M [3] MobileOne-S4 [67] SHViT-S4 [80] EfficientViM-M4 MobileViTV2 1.25 [45] EfficientViM-M4 CVPR 2023 Arxiv 2022 CVPR 2023 ICLR 2024 ICCV 2023 ICCV 2023 CVPR 2022 CVPR 2023 CVPR 2024 - Arxiv 2022 - Input Size 2562 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2562 2562 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2562 2242 2242 2242 2562 2242 2242 2562 2562 2562 2562 Epochs Token Mixer Throughput (im/s) Thrrel Latency Top-1 (%) (ms) Params (M) FLOPs (M) 300 300 300 450 300 300 600 300 450 300 300 300 300 600 450 300 300 300 300 300 300 300 300 300 300 300 450 300 450 300 300 300 300 300 300 450 300 300 300 300 450 Att. Conv Att. Att. Att. SSD Conv Att. SSD Att. Att. Att. Att. Conv Att. Att. SSD Att. Att. SSD Att. Conv Att. Conv Att. Att. SSD Att. Att. SSD Conv Att. Conv Att. Att. Att. Att. Conv Att. SSD Att. SSD 6,702 13,313 6,945 11,554 19,868 20,731 10,846 16,045 20,731 1,350 15,807 5,990 4,990 9,493 8,890 15,899 17,005 4,409 4,365 17,005 7022 7563 11,105 5,360 11,873 4,405 11,952 6,576 1,248 11,952 6,405 2,977 4,181 5,321 3,266 2,741 4,586 3,041 8,024 8,170 2,409 8,170 0.32 0.64 0.34 0.56 0.96 1.00 0.52 0.77 1.00 0.08 0.93 0.35 0.29 0.56 0.52 0.93 1.00 0.26 0.26 1. 0.59 0.63 0.93 0.45 0.99 0.37 1.00 0.55 0.10 1.00 0.78 0.36 0.51 0.65 0.40 0.34 0.56 0.37 0.98 1.00 0.24 1.00 0.149 0.075 0.144 0.087 0.050 0.048 0.092 0.062 0.048 0.741 0.063 0.167 0.200 0.105 0.112 0.063 0.059 0.227 0.229 0.059 0.142 0.132 0.090 0.187 0.084 0.227 0.084 0.152 0.801 0.084 0.156 0.336 0.239 0.188 0.306 0.365 0.218 0.329 0.124 0.122 0.415 0. 70.2 71.4 71.5 72.8 72.8 72.9 73.3 73.4 73.5 73.7 74.3 74.4 75.1 75.2 75.2 75.2 75.4 75.6 75.6 75.8 76.0 76.2 77.1 77.4 77.4 77.5 77.6 77.9 77.9 77.9 78.0 78.1 78.1 78.3 79.0 79.1 79.3 79.4 79.4 79.4 79.6 79.6 1.4 2.1 1.3 4.6 6.3 6.7 4.0 6.9 6.7 3.5 8.8 4.1 2.3 5.4 7.6 11.4 13.9 2.9 3.6 13. 4.7 3.7 12.4 7.8 14.2 6.7 16.6 11.4 6.1 16.6 5.2 4.9 10.1 6.6 6.1 6.8 14.0 14.8 16.5 19.6 7.5 19.6 466 275 261 96 241 239 155 263 239 407 299 556 439 217 151 366 355 1030 705 355 583 552 522 1299 601 1136 656 294 668 656 785 1844 1896 778 961 1419 508 2978 986 1111 2857 Table 3. Comparison of efficient networks on ImageNet-1K [10] classification. Results are sorted by accuracy. We also denote the relative throughput Thrrel of each method compared to EfficientViM in each split. Method Thr. Thrrel Top-1 Params FLOPs EfficientViM-M2 ViM-T [87] LocalViM-T [87] EfficientVMamba-T [43] MSVMamba-N [57] EfficientVMamba-S [43] EfficientViM-M4 MSVMamba-M [57] 17,005 2.08 1,612 0.20 593 0.07 2,763 0.34 2,060 0.25 1,350 0.17 8,170 1.00 1,527 0.19 75.8 76.1 76.2 76.5 77.3 78.7 79.6 79. 13.9M 355M 7.1M 1500M 8.0M 1500M 6.0M 800M 6.9M 864M 11.0M 1300M 19.6M 1111M 11.9M 1507M Table 4. Comparison of EfficientViM with vision Mambas. Thrrel is relative throughput compared to EfficientViM-M4. Comparison with vision Mambas. In Table 4, we compare our EfficientViM with the recent vision Mambas including ViM [87], LocalViM [25], EfficientVMamba [49], and MSVMamba [57]. Notably, EfficientViM brings promising speed-ups over the prior works. For instance, EfficinetViM-M2 is almost 10 and 29 faster than ViMT and LocalViM-T with comparable accuracy. Also, EfficientViM-M4 shows about 314 higher throughput than other methods in the table even with competitive performances. EfficientViM-M4 outperforms MSVMamba-N and EfficientVMamba-S by 3.1% and 2.3%, respectively, while achieving approximately 4 and 6 higher throughput. These results demonstrate that EfficientViM is highly 6 Method Size Thr. Thrrel Top-1 Params FLOPs 3842 3,685 0.99 SHViT-S4 [80] EfficientViM-M4 3842 3,724 1. 16.5M 2225M 21.3M 2379M 81.0 80.9 5122 2,122 0.86 SHViT-S4 [80] EfficientViM-M4 5122 2,452 1.00 82.0 81.9 16.5M 3973M 21.3M 4154M Table 5. Classification results on ImageNet-1K [10] after finetuning with higher resolutions. Figure 5. Latency comparison of recent efficient networks for an extremely high-resolution image. efficient architecture among mamba-based vision models. 4.2. Extensibility of EfficientViM EfficientViM with high-resolution images. Following [80], we also explore the applicability of EfficientViM on high-resolution images after fine-tuning 30 epochs at resolution of 3842, followed by an additional 30 epochs at 5122. For fair comparison, we use the EfficientViMM4 pre-trained for 300 epochs. In 3842 size, EfficientViM demonstrates competitive performance as presented in TaInterestingly, when the resolution increases, the ble 5. throughput gap between EfficientViM and SHViT [80] gets larger, resulting in more than 15% speedup compared to SHViT in 5122 while achieving comparable accuracy. We further investigate the scalability of our method in extremely high-resolution images beyond 5122, by comparing the latency of the models while scaling the resolution from 5122 to 40962. As depicted in Figure 5, we observe an advantage of EfficientViM over the recent state-of-the-art method on extremely high-resolution images. EfficientViM shows about 3, 4, and 7 faster speed compared to SHViT, EMO [83], and MobileOne [67], respectively. This notable result highlights the scalability of EfficientViM for high-resolution images based on linear cost of HSM-SSD. Training with distillation. We compare EfficientViM with the prior works [33, 54, 66, 80] trained with distillation objectives in DeiT [63]. We train the model for 300 epochs, using RegNetY-160 [50] as the teacher model. Table 6 shows that distillation is effective for EfficientViM. Compared to FastViT-T8&T12 [66], EfficientViM-M2&M4 exMethod SHViT-S1 [80] EfficientViM-M1 Size Thr. Thrrel Top-1 Params FLOPs 2242 19,868 0.96 74.0 2242 20,731 1.00 74.6 6.3M 241M 6.7M 239M EfficientFormerV2-S0 [33] 2242 1,350 0.08 75.7 3.5M 407M 2242 6,102 0.36 75.7 SwifitFormer-XS [54] 3.5M 605M 2242 15,672 0.94 76.2 11.4M 366M SHViT-S2 [80] 2562 4,365 0.26 76.7 FastViT-T8 [66] 3.6M 705M 2242 17,005 1.00 76.7 13.9M 355M EfficientViM-M2 2242 7,022 0.59 77.1 4.7M 583M EfficientMod-XXS [43] 2242 11,873 0.99 78.3 14.2M 601M SHViT-S3 [80] 2242 4,675 0.39 78.5 6.1M 988M SwifitFormer-S [54] EfficientFormerV2-S1 [33] 2242 1,248 0.10 79.0 6.1M 668M 2242 11,952 1.00 79.1 16.6M 656M EfficientViM-M EfficientMod-XS [43] SHViT-S4 [80] FastViT-T12 [66] EfficientViM-M4 2242 5,321 0.65 79.4 6.6M 778M 2562 8,024 0.98 80.2 16.5M 986M 2562 2,741 0.34 80.3 6.8M 1419M 2562 8,170 1.00 80.7 19.6M 1111M Table 6. Comparison of efficient networks after training with distillation objective in [63]. Thrrel is relative throughput compared to EfficientViM in each split. Method Memory Thr. Thrrel Top-1 Params EfficientViT-M4 [36] EMO-2M [83] MobileNetV3-L 1.0 [22] Mobile-Former-151M [3] SHViT-S2 [80] FastViT-T8 [80] EfficientViM-M 870M 15,807 0.93 74.3 2656M 4,990 0.29 75.1 2643M 9,493 0.56 75.2 1800M 8,890 0.52 75.2 879M 15,899 0.94 75.2 2811M 4,365 0.26 75.6 969M 17,005 1.00 75.8 8.8M 2.3M 5.4M 7.6M 11.4M 3.6M 13.9M Table 7. Comparsion on peak memory usage during inference. hibits more than 3 higher throughput along with comparable or even better performance. Also, EfficientViM outperforms SHViT [80] up to 0.8% while running at higher speed. After training with distillation, EfficientViM still outperforms all other models in speed-accuracy trade-offs, and further establishes promising Pareto front as shown in Figure 1. 4.3. Analysis and Ablation studies Memory Efficiency. Our models have relatively large number of parameters compared to prior works. Yet, the memory usage of the model in the device is determined by the memory I/O during inference rather than just the number of parameters alone. We here analyze the peak memory usage of the networks and provide the results in Table 7. Despite the highest number of parameters, EfficientViM shows competitive memory efficiency while achieving the best throughput. Notably, we observe that EfficientViM requires only about 1/3 of the peak memory usage of the models having lower parameters, e.g., EMO [83], MobileNetV3 [22], and FastViT [66]. Furthermore, the models with relatively large number of parameters (e.g., EfficientViT, SHViT, and EfficientViM) demonstrate high throughput and low memMethod Thr. Top-1 Params FLOPs (A) EfficientViM-M2 (Base) 17,005 (B) EfficientViM-M3 11,952 75.4 77.5 13.9M 355M 16.6M 656M a. Token Mixers (Base: HSM-SSD) (C) () NC-SSD [56] (D) () Self-Attention [68] 9,786 13,038 76.2 76.1 13.0M 382M 13.6M 416M b. Head designs (Base: Single-head with RLN ) (E) () Multi-head 15,703 75.4 13.9M 352M c. Multi-stage fusion (F) () None 17,317 75.1 13.0M 354M Table 8. Ablation studies on EfficientViM. All ablations are conducted with EfficientViM-M2. ory consumption, highlighting that the number of parameters is not critical factor for memory and time efficiency. Overall, our EfficientViM achieves the best speed and performance maintaining memory efficiency. Ablation studies. We conduct ablation studies with EfficietViM-M2 after training 300 epochs and provide the results in Table 8. First, we compare HSM-SSD with other global token mixers, by replacing them with other methods including NC-SSD [56] and self-attention (SA) [68]. Considering that EfficientViM-M3 with HSM-SSD shows 77.5% with throughput of 11,952 (im/s), NC-SSD and SA show poorer speed-accuracy trade-off than HSM-SSD. Regarding head choices, we observe that our single-head design brings significant speed-up (15703 17005 (im/s)) without performance degradation. Lastly, multi-stage fusion with hidden states (75.4%) surpasses the accuracy of EfficientViM without fusion (75.1%) under similar throughput. Note that all ablated models are placed under the Pareto front of EfficientViM as shown in Figure 6. Please refer to the supplement for additional ablation studies. 5. Related Works Efficient vision backbones. Earlier works [5, 22, 23, 26, 42, 53, 60, 67, 84] have studied the to improve the trade-off between accuracy and computational efficiency in CNN architectures. To reduce the complexity of convolution, Xception [5] introduced depthwise separable convolutions (DWConv), which have become major techniques used across modern efficient models. For instance, the pioneering work MobileNet [23] constructed lightweight architectures based on DWConv. ShuffleNet [84] and GhostNet [18] also have explored additional techniques to enhance CNN by shuffling the channel and generating more feature maps. Following ViTs [12], several works [3, 36, 44, 45, 48, 66, 80, 83] built efficient vision backbones with attention [68] mechanism. Some works sparsified the attention [7, 11, 38] to reduce query and key, while another line Figure 6. Ablation studies on EfficientViM. Refer to Table 8 for the corresponding models. Red line indicates the Pareto frontier of EfficientViM. of works [6, 29, 70, 74] have approximated attention itself with reduced cost. More recently, EfficientFormer [32], EfficientViT [36], and SHViT [80] have proposed hardwarefrienly ViT architectures for real-world applications, focusing on actual speed in practical use, rather than FLOPs. Vision Mambas. State space model (SSM) [9, 1416, 58] has become popular global token mixer with its favorable linear cost. Especially, Mamba [15] has introduced selective scan mechanism on SSM (S6) to enable time-variant operation. Following the success of Mambas [9, 15], several works [25, 37, 43, 55, 62, 77, 87] have proposed SSM-based Vision backbones. With the flattened patches, ViM [87] applied SSM bi-directionally considering the causal nature in Mamba. Similarly, VMamba [37], PlainMamba [77], and LocalMamba [25] have been proposed with the multi-path scanning mechanism. GroupMamba [55], and EfficientVMamba [49] have tried to further enhance the efficiency by splitting the channels and sharing the learnable parameters for each path. Recent works, VSSD [56] and Linfusion [35] introduced non-causal SSD to overcome causal properties in Mamba. 6. Conclusion In this paper, we propose novel mamba-based architecture, Efficient Vision Mamba (EfficientViM), built with hidden state mixer-based SSD (HSM-SSD). The HSMSSD layer mitigates the bottleneck of the SSD layer by rearranging the channel mixing operation within the hidden states which can be viewed as reduced latent representations. we also introduce multi-stage hidden state fusion that integrates both low-level and high-level features. With the minimization of memory-bound operations, EfficinetViM consistently outperforms previous models in the speed-accuracy trade-offs. Our comprehensive experiments demonstrate the efficiency of EfficientViM with promising performance improvements over previous works in various settings. 8 EfficientViM: Efficient Vision Mamba with Hidden State Mixer based State Space Duality"
        },
        {
            "title": "Distillation",
            "content": "FT Head: Mask R-CNN [20] Epochs Batch size Weight decay Warmup Epochs Cooldown Epochs Learning rate Min Learning rate Optimizer (Momentum) Gradient Clipping Learning rate scheduler Rand Augment Mixup Cutmix Mixup switch prob Random erasing prob Label smoothing EMA decay rate Teacher model 300/450 300 2048 0.05 20 10 2e-3 2e30 + 30 1024 1e-8 0 0 1e-3 1e-5 Adamw (0.9, 0.999) 0.02 Cosine rand-m9-mstd0.5-inc1 0.8 1.0 0.5 0.25 0.1 0.9995 RegNetY-"
        },
        {
            "title": "None",
            "content": "Table A. Settings for training EfficientViM. FT: finetuning with higher resolution images. A. Implementation Details We use the ImageNet-1K [10] to validate the effectiveness of EfficientViM on the image classification task. For training EfficientViM, we follow the training recipes of previous works [56, 65, 80]. Specifically, all models are trained from scratch with batch size of 2,048 for 300 epochs using AdamW optimizer [39] with warmup of 20 epochs and cooldown of 10 epochs. Following [3, 22, 33], we also report the results after training 450 epochs. During training, we adopt cosine annealing [40] scheme with the initial learning rate of 2103 decreasing to 2105. The weight decay of 0.05 and gradient clipping with threshold of 0.02 are used. Also, MESA [13] and EMA with the decay rate of 0.9995 is adopted following [17, 56]. For data augmentation, we follow DeiT [63] using Mixup [82] & CutMix [81] with Label smoothing [59], RandAugment [8], and Random Erasing [86]. We report the throughput and latency with the batch size of 256 on Nvidia RTX 3090 GPU. Additionally, we finetune the model with the batch size of 1024, using cosine annealing with the initial learning rate of 1 103, for 30 epochs at resolution of 3842, followed by an additional 30 epochs at 5122. For fair comparison, we employ pre-trained models trained for 300 epochs. Also, to report the throughput of the models with extremely 9 Method EfficientNet-B0 [60] PoolFormer-S1 [79] FastViT-SA12 [66] SHViT-S4 [80] EfficientViM-M Lat. (ms) APb APb 50 APb 0.95 1.49 1.63 0.52 0.45 31.9 51.0 37.3 59.0 38.9 60.5 39.0 61.2 39.3 60.2 Head: RetinaNet [52] 75 APm APm 34.5 29.4 47.9 40.1 34.6 55.8 42.2 35.9 57.6 41.9 35.9 57.9 42.5 35.8 57. 50 APm 75 31.2 36.9 38.1 37.9 37.4 Method Lat. (ms) AP AP50 AP75 APs APm APl PVTV2-B0 [71] MobileFormer-508M [3] EdgeViT-XXS [48] SHViT-S4 [80] EfficientViM-M4 0.87 1.09 0.94 0.52 0.45 39.5 23.1 40.4 37.2 57.2 40.3 22.9 41.2 38.0 58.3 38.7 59.0 41.0 22.4 42.0 38.8 59.8 41.1 22.0 42.4 41.1 22.1 42.4 38.8 59. 49.7 49.7 51.6 52.7 52.8 Table B. Instance segmentation and object detection and results on COCO-2017 [34]. high-resolution images in Figure 5, we start with batch size of 256 and halve it once the memory exceeds the GPU limit as the resolution increases. Regarding training with distillation, all settings are the same as in the main table  (Table 3)  except for the guidance from the teacher model of RegNetY-160 [50] following DeiT [63]. B. Dense Predictions We validate the effectiveness of EfficientViM in dense prediction tasks using the COCO-2017 [34] dataset. For training the models, we follow the settings of previous works [36, 48, 66, 80], where Mask R-CNN [20] and RetinaNet [52] are used for instance segmentation and object detection. After training the model for 12 epochs (1x schedule) with batch size of 16, we report the performances and backbone latencies with the resolution of 5122, following [80]. Table demonstrates that EfficientViM exhibits competitive performances while maintaining faster speed in dense predictions. Specifically, in instance segmentation with Mask R-CNN, EfficientViM-M4 surpasses SHViT-S4 with the 0.3% improvements in APb while reducing latency by 0.7ms. Similarly, in object detection with RetinaNet, EfficientViM-M4 achieves the best average precision of 38.8% with the lowest latency. C. Ablation Studies Here, we show the effectiveness of HSM-SSD by ablating the proposed components. The results are summarized in Table C, and Figure A. First, we compare HSM-SSD with other global token mixers, by replacing them with other Method Thr. Top-1 Params FLOPs (A) EfficientViM-M2 (Base) 17,005 (B) EfficientViM-M3 11, 75.4 77.5 13.9M 16.6M 355M 656M a. Token Mixers (Base: HSM-SSD) (C) () NC-SSD [56] (D) () Self-Attention [68] 9,786 13, 76.2 76.1 13.0M 13.6M 382M 416M b. Head designs (Base: Single-head (SH) with RLN ) (E) () Multi-head (F) () SH w. RL 15,703 17,081 75.5 75.2 13.9M 13.9M 352M 352M c. Multi-stage fusion (Base: h(s)) (G) () Fusion with x(s) (H) () None 17,041 17,317 75.3 75.1 13.4M 13.0M 355M 354M d. # states (N ) of each stage (Base: [49, 25, 9]) (I) (J) ()[9, 25, 49] ()[25, 25, 25] 16,476 16,991 75.4 75.2 14.0M 13.9M 407M 373M e. Normalization (Base: Partial LN) (L) () Full BN 17,432 NaN 13.9M 355M Table C. Ablation studies on EfficientViM. All ablations are conducted with EfficientViM-M2. See Figure for visualization comparing the ablated models with the Pareto front of EfficientViM. methods including (C) NC-SSD [56] and (D) self-attention (SA) [68]. Considering that EfficientViM-M3 with HSMSSD shows 77.5% with throughput of 11,952 (im/s), NC-SSD and SA show poorer speed-accuracy trade-off than HSM-SSD. Regarding head choices, we observe that our single-head design brings significant speed-up (17005 (im/s)) over (E) multi-head (15,703 (im/s)) without performance degradation. Additionally, defining the importance score per state as RLN to mimic multi-head leads to the +0.2% gain with minor increase in latency, compared to using the original score (F) RL. Note that all ablates models (C-F) are placed under the Pareto front of EfficientViM (Figure Aa), which proves the efficacy of HSM-SSD and our single-head design. Also, multi-stage fusion with hidden states (75.4%) surpasses the accuracy of (G) the fusion with the output feature maps x(s) (75.3%) and (H) the EfficientViM without fusion (75.1%) under similar throughput. For the number of states , we observe that an increasing schedule with respect to the stages is more effective than (I) decreasing or (J) constant schedule. See (G)-(J) in Table and Figure Ab for the ablation studies on multi-stage fusion and the number of states. Additionally, for normalization, using (L) batch normalization (BN) across all operations is simple and fast, but, this approach leads to numerical instability. Therefore, we apply layer normalization (LN) only before HSM-SSD, and BN for the rest. (a) Ablations on the token mixer and head design (C-F). (b) Ablations on Multi-stage fusion and # states (G-J). Figure A. Ablation studies on EfficientViM. Refer to Table for the corresponding models. Red line indicates the Pareto frontier of EfficientViM. Method Token Mixer Thr. Top-1 Params FLOPs VSSD-M [56] VSSD-T [56] VSSD-T NC-SSD NC-SSD 1459 947 HSM-SSD 1660 82.5 84.1 82.7 14M 2.3G 24M 4.5G 24M 3.7G Table D. Comparison of HSM-SSD with NC-SSD. D. Comparison of HSM-SSD with NC-SSD To show the advantage of the proposed HSM-SSD over NCSSD, we replace NC-SSD of VSSD-T [56] with HSM-SSD and train the model following the original training configuration provided in the official repository. In Table D, after replacing the token mixer with HSM-SSD, VSSD-T with HSM-SSD demonstrates significant increase in the throughput (1.8). Notably, compared to scaling down the models to smaller sizes (e.g., VSSD-M), replacing the token mixer with HSM-SSD provides better speed-accuracy trade-offs (14% faster, +0.2% accuracy), highlighting the advantage of HSM-SSD over NC-SSD. We also provide the qualitative comparison of HSM-SSD with NC-SSD in the following section. 10 (a) ResNet50 (b) DeiT-S (c) Swin-T (d) VMamba-T (e) VSSD-T (f) EfficientViM-M4 Figure B. Comparison of Effective Receptive Fields (ERF) [41] E. Effective Receptive Field of HSM-SSD In this section, we qualitatively compare the HSM-SSD with previous token mixers including convolutions in ResNet50 [19], attentions in vision Transformers such as DeiT-S [63] and Swin-T [38], and SSM and SSD variants in vision Mambas like VMamba-T [37] and VSSD-T [56]. We here analyze the Effective Receptive Fields (ERF) [41] of each model, which quantifies the region of the input that contributes to the output. In Figure B, we visualize the ERF with respect to the central pixel of the output feature maps. Among various models, EfficientViM-M4 with HSM-SSD shows global receptive field rather than focusing on specific region. For instance, ResNet50 shows relatively small ERF due to its intrinsic locality of convolution. The attention mechanism in DeiT-S predominantly focuses on the central pixel itself, and the shifted window attention in Swin-T limits the global receptive field. Further, since SSM is conducted after flattening the image patch both vertically and horizontally in VMamba-T, it generates an unnatural cross pattern in ERF. VSSD-T shows relatively better global receptive field, yet it still largely depends on the close region. On the other hand, EfficientViM-M4 generates global effective receptive field (ERF) similar to that of VSSD-T but extracts more information from all regions, enabling it to capture the global dependencies better. F. CPU & Mobile Latency To understand the applicability of EfficientViM in resource-constrained environment, we here provide the latencies of vision backbones in GPU, CPU, and mobile devices (Table E). The latencies are measured with batch size of 256 on an NVIDIA RTX 3090 GPU, 16 on an AMD EPYC 7742 CPU, and 1 on an iPhone 16 (iOS 18.1). For mobile latencies, we use CoreML [46] library. EfficientViM-M4 achieves the highest accuracy of 79.6% with the lowest latency on GPUs and competitive latency on CPU and iPhone 16. Although EfficientViM-M4 shows slightly higher latency than few of the prior works in CPU and mobile, EfficientViM-M4 shows significantly lower latency as the resolution increases (Figure C). In the resolution of 20482, EfficientViM-M4 achieves at least 58% Method Latency (ms) GPU CPU Mobile Top-1 MobileViTV2 1.0 [45] EfficientMod-XS [43] MobileFormer-508M [3] FastViT-T12 [66] MobileOne-S4 [67] SHViT-S4 [80] EfficientViM-M4 0.34 0.19 0.22 0.37 0.33 0.12 0. 138.8 33.1 29.7 81.3 79.9 27.4 32.1 1.1 0.9 2.1 1.8 1.0 0.9 1.0 78.1 78.3 (79.4) 79.3 79.1 (80.3) 79.4 79.4 (80.2) 79.6 (80.7) Table E. Latency comparison of EfficientViM-M4 with prior works. The number in parentheses indicates the performance with distillation. and 20% reductions in latency compared to previous works in iPhone 16 and CPU, respectively. To summarize, EfficientViM serves as general solution suitable for both GPU and edge devices. Furthermore, EfficientViM is an effective backbone for real-world applications where high-resolution images are given, such as in image generation, object detection, and instance segmentation. G. Proof for Proposition B = IL, and RLL Proposition 1. Let = L, a1 be diagonal. Then, HSM-SSD(x, a, B, C) is equivalent to NC-SSD(x, a, B, C) including gating and output projection, as xout = (y) = Cf (h). Proof. Given Equations (10) and (11) in the main paper as = (a1 = (cid:0)(a1 B)(xinWin) B)xin (cid:1) Win = hinWin, xout = (y) = Linear (y σ(z)) = (Ch σ (xinWz)) Wout ((h σ (hinWz)) Wout) = Cf (h), (10) (11) it is sufficient to show that Cf (h) of HSM-SSD is equivalent to (y) = (Chσ(xinWz))Wout in order to prove the proposition. Here, based on the assumption, the following 11 (a) Mobile latency comparison. (b) CPU latency comparison. Figure C. Mobile and CPU latency comparison. holds: Cf (h) = ((h σ(hinWz))Wout) = (cid:0)(h σ(((a1 = C(h σ(xinWz))Wout = (Ch σ(xinWz))Wout = (y) B)xin)Wz))Wout (cid:1)"
        },
        {
            "title": "References",
            "content": "[1] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. ICLR, 2022. 1 [2] Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu. Dynamic relu. In ECCV, 2020. 5 [3] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu. MobileIn CVPR, former: Bridging mobilenet and transformer. 2022. 1, 6, 7, 8, 9, 11 [4] Joonmyung Choi, Sanghyeok Lee, Jaewon Chu, Minhyuk Choi, and Hyunwoo Kim. vid-tldr: Training free token merging for light-weight video transformer. In CVPR, 2024. 1 [5] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR, 2017. 1, [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, ICLR, 2021. et al. Rethinking attention with performers. 1, 8 [7] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. NeurIPS, 2021. 1, 8 [8] Ekin Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc Le. Autoaugment: Learning augmentation strategies from data. In CVPR, 2019. 9 [9] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In International Conference on Machine Learning (ICML), 2024. 2, 3, 4, 8 [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. 1, 5, 6, 7, 9 [11] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: general vision transformer backbone with cross-shaped windows. In CVPR, 2022. [12] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. 1, 8 [13] Jiawei Du, Daquan Zhou, Jiashi Feng, Vincent Tan, and Sharpness-aware training for free. Joey Tianyi Zhou. NeurIPS, 2022. 9 [14] Daniel Fu, Tri Dao, Khaled Saab, Armin Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: ToICLR, wards language modeling with state space models. 2023. 2, 8 [15] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv:2312.00752, 2023. 2, 8 [16] Albert Gu, Karan Goel, and Christopher Re. Efficiently modICLR, eling long sequences with structured state spaces. 2022. 2, [17] Dongchen Han, Ziyi Wang, Zhuofan Xia, Yizeng Han, Yifan Pu, Chunjiang Ge, Jun Song, Shiji Song, Bo Zheng, and Gao Huang. Demystify mamba in vision: linear attention perspective. arXiv:2405.16605, 2024. 9 [18] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. Ghostnet: More features from cheap operations. In CVPR, 2020. 1, 8 [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 12 Deep residual learning for image recognition. 2016. 11 In CVPR, [20] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In ICCV, 2017. 9 [21] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv:1606.08415, 2016. 5 [22] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In ICCV, 2019. 1, 2, 5, 6, 7, 8, 9 [23] Andrew Howard. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv:1704.04861, 2017. 1, 8 [24] Jie Hu, Linyan Huang, Tianhe Ren, Shengchuan Zhang, Rongrong Ji, and Liujuan Cao. You only segment once: Towards real-time panoptic segmentation. In CVPR, 2023. 1 [25] Tao Huang, Xiaohuan Pei, Shan You, Fei Wang, Chen Qian, and Chang Xu. Localmamba: Visual state space model with windowed selective scan. arXiv:2403.09338, 2024. 2, 6, 8 [26] Forrest Iandola. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and 0.5 mb model size. arXiv:1602.07360, 2016. 1, 8 [27] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, 2021. [28] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive In ICML, pages 5156 transformers with linear attention. 5165. PMLR, 2020. 3 [29] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. ICLR, 2020. 1, 8 [30] Sanghyeok Lee, Joonmyung Choi, and Hyunwoo Kim. Multi-criteria token fusion with one-step-ahead attention for efficient vision transformers. In CVPR, 2024. 1 [31] Quanquan Li, Shengying Jin, and Junjie Yan. Mimicking very efficient network for object detection. In CVPR, 2017. 1 [32] Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Efficientformer: Vision transformers at mobilenet speed. NeurIPS, 2022. [33] Yanyu Li, Ju Hu, Yang Wen, Georgios Evangelidis, Kamyar Salahi, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Rethinking vision transformers for mobilenet size and speed. In ICCV, 2023. 6, 7, 9 [34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 9 [35] Songhua Liu, Weihao Yu, Zhenxiong Tan, and XinLinfusion: 1 gpu, 1 minute, 16k image. chao Wang. arXiv:2409.02097, 2024. 2, 8 [36] Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, and Yixuan Yuan. Efficientvit: Memory efficient vision transformer with cascaded group attention. In CVPR, 2023. 1, 5, 6, 7, 8, 9 [37] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba: Visual state space model. NeurIPS, 2024. 2, 8, [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: In Hierarchical vision transformer using shifted windows. ICCV, 2021. 1, 8, 11 [39] Loshchilov. Decoupled weight decay regularization. ICLR, 2019. 9 [40] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv:1608.03983, 2016. 9 [41] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in deep convolutional neural networks. NeurIPS, 2016. 11 [42] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In ECCV, 2018. 1, 8 [43] Xu Ma, Xiyang Dai, Jianwei Yang, Bin Xiao, Yinpeng Chen, Yun Fu, and Lu Yuan. Efficient modulation for vision networks. ICLR, 2024. 6, 7, 8, [44] Sachin Mehta and Mohammad Rastegari. Mobilevit: lightweight, general-purpose, and mobile-friendly vision transformer. ICLR, 2022. 1, 8 [45] Sachin Mehta and Mohammad Rastegari. Separable selfattention for mobile vision transformers. arXiv:2206.02680, 2022. 1, 5, 6, 8, 11 [46] Core ML. https://developer.apple.com/documentation/coreml, 2017. 11 [47] Vinod Nair and Geoffrey Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010. [48] Junting Pan, Adrian Bulat, Fuwen Tan, Xiatian Zhu, Lukasz Dudziak, Hongsheng Li, Georgios Tzimiropoulos, and Brais Martinez. Edgevits: Competing light-weight cnns on mobile devices with vision transformers. In ECCV, 2022. 1, 6, 8, 9 Efficientvmamba: Atrous selective scan for light weight visual mamba. arXiv:2403.09977, 2024. 6, 8 [49] Xiaohuan Pei, Tao Huang, and Chang Xu. [50] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollar. Designing network design spaces. In CVPR, 2020. 7, 9 [51] Redmon. You only look once: Unified, real-time object detection. In CVPR, 2016. 1 [52] T-YLPG Ross and GKHP Dollar. Focal loss for dense object detection. In CVPR, 2017. 9 [53] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018. 1, 5, 8 [54] Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Swiftformer: Efficient additive attention for transformerbased real-time mobile vision applications. In ICCV, 2023. 7 [55] Abdelrahman Shaker, Syed Talal Wasim, Salman Khan, Juergen Gall, and Fahad Shahbaz Khan. Groupmamba: Parameter-efficient and accurate group visual state space model. arXiv:2407.13772, 2024. 8 13 [74] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystromformer: nystrom-based algorithm for approximating self-attention. In AAAI, 2021. 1, 8 [75] Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, and Vikas Chandra. Efficientsam: Leveraged masked image pretraining for efficient segment anything. In CVPR, 2024. [76] Jiacong Xu, Zixiang Xiong, and Shankar Bhattacharyya. Pidnet: real-time semantic segmentation network inspired by pid controllers. In CVPR, 2023. 1 [77] Chenhongyi Yang, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu Wang, Jiaming Liu, and Elliot Crowley. Plainmamba: Improving non-hierarchical mamba in visual recognition. BMVC, 2024. 2, 8 [78] Hongxu Yin, Arash Vahdat, Jose Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. A-vit: Adaptive tokens for efficient vision transformer. In CVPR, 2022. 1 [79] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In CVPR, 2022. 9 [80] Seokju Yun and Youngmin Ro. Shvit: Single-head vision transformer with memory efficient macro design. In CVPR, 2024. 1, 2, 4, 5, 6, 7, 8, 9, 11 [81] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019. 9 [82] Hongyi Zhang. mixup: Beyond empirical risk minimization. ICLR, 2018. 9 [83] Jiangning Zhang, Xiangtai Li, Jian Li, Liang Liu, Zhucun Xue, Boshen Zhang, Zhengkai Jiang, Tianxin Huang, Yabiao Wang, and Chengjie Wang. Rethinking mobile block for efficient attention-based models. In ICCV, 2023. 1, 6, 7, 8 [84] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In CVPR, 2018. 1, 8 [85] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-time semantic segmentation on high-resolution images. In ECCV, 2018. 1 [86] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In AAAI, 2020. 9 [87] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. ICML, 2024. 2, 5, 6, 8 [56] Yuheng Shi, Minjing Dong, Mingjia Li, and Chang Xu. Vssd: Vision mamba with non-casual state space duality. arXiv:2407.18559, 2024. 2, 3, 4, 8, 9, 10, [57] Yuheng Shi, Minjing Dong, and Chang Xu. Multi-scale vmamba: Hierarchy in hierarchy visual state space model. NeurIPS, 2024. 2, 6 [58] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. ICLR, 2023. 2, 8 [59] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016. 9 [60] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, 2019. 1, 8, 9 [61] Mingxing Tan, Ruoming Pang, and Quoc Le. Efficientdet: Scalable and efficient object detection. In CVPR, 2020. [62] Lv Tang, HaoKe Xiao, Peng-Tao Jiang, Hao Zhang, Jinwei Chen, and Bo Li. Scalable visual state space model with fractal scanning. arXiv:2405.14480, 2024. 8 [63] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In ICML, 2021. 1, 7, 9, 11 [64] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herve Jegou. Going deeper with image transformers. In ICCV, 2021. 5 [65] Hugo Touvron, Matthieu Cord, and Herve Jegou. Deit iii: Revenge of the vit. In ECCV, 2022. 5, 9 [66] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. Fastvit: fast hybrid vision In ICCV, transformer using structural reparameterization. 2023. 1, 5, 6, 7, 8, 9, [67] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. Mobileone: An improved one millisecond mobile backbone. In CVPR, 2023. 1, 5, 6, 7, 8, 11 [68] Vaswani. Attention is all you need. NeurIPS, 2017. 3, 4, 8, 10 [69] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Yunhe Wang, and Kai Han. Gold-yolo: Efficient object detector via gather-and-distribute mechanism. NeurIPS, 2024. 1 [70] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv:2006.04768, 2020. 1, 8 [71] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer, 2022. [72] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In CVPR, 2023. 6 [73] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. NeurIPS, 2021."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Engineering, Korea University"
    ]
}