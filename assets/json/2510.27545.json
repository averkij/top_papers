{
    "paper_title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities",
    "authors": [
        "Travis Davies",
        "Yiqi Huang",
        "Alexi Gladstone",
        "Yunxin Liu",
        "Xiang Chen",
        "Heng Ji",
        "Huxian Liu",
        "Luhui Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Implicit policies parameterized by generative models, such as Diffusion Policy, have become the standard for policy learning and Vision-Language-Action (VLA) models in robotics. However, these approaches often suffer from high computational cost, exposure bias, and unstable inference dynamics, which lead to divergence under distribution shifts. Energy-Based Models (EBMs) address these issues by learning energy landscapes end-to-end and modeling equilibrium dynamics, offering improved robustness and reduced exposure bias. Yet, policies parameterized by EBMs have historically struggled to scale effectively. Recent work on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs to high-dimensional spaces, but their potential for solving core challenges in physically embodied models remains underexplored. We introduce a new energy-based architecture, EBT-Policy, that solves core issues in robotic and real-world settings. Across simulated and real-world tasks, EBT-Policy consistently outperforms diffusion-based policies, while requiring less training and inference computation. Remarkably, on some tasks it converges within just two inference steps, a 50x reduction compared to Diffusion Policy's 100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior models, such as zero-shot recovery from failed action sequences using only behavior cloning and without explicit retry training. By leveraging its scalar energy for uncertainty-aware inference and dynamic compute allocation, EBT-Policy offers a promising path toward robust, generalizable robot behavior under distribution shifts."
        },
        {
            "title": "Start",
            "content": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities"
        },
        {
            "title": "Luhui Hu\nZhiCheng AI",
            "content": "5 2 0 2 1 3 ] . [ 1 5 4 5 7 2 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Implicit policies parameterized by generative models, such as Diffusion Policy [5], have become the standard for policy learning and VisionLanguageAction (VLA) models [26] in robotics. However, these approaches often suffer from high computational cost, exposure bias, and unstable inference dynamics, which lead to divergence under distribution shifts. Energy-Based Models (EBMs) [28] address these issues by learning energy landscapes endto-end and modeling equilibrium dynamics, offering improved robustness and reduced exposure bias. Yet, policies parameterized by EBMs have historically struggled to scale effectively. Recent work on Energy-Based Transformers (EBTs) [18] demonstrates the scalability of EBMs to high-dimensional spaces, but their potential for solving core challenges in physically embodied models remains underexplored. We introduce new energy-based architecture, EBT-Policy, that solves core issues in robotic and real-world settings. Across simulated and real-world tasks, EBT-Policy consistently outperforms diffusion-based policies, while requiring less training and inference computation. Remarkably, on some tasks it converges within just two inference steps, 50 reduction compared to Diffusion Policys 100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior models, such as zero-shot recovery from failed action sequences using only behavior cloning and without explicit retry training. By leveraging its scalar energy for uncertainty-aware inference and dynamic compute allocation, EBT-Policy offers promising path toward robust, generalizable robot behavior under distribution shifts.1 1This is part of broader effort on world models for robot manipulation; future updates and hyperparameter tuning are expected to further improve performance and results. Figure 1. EBT-Policy Diagram. EBT-Policy functions through searching for low energy action trajectory in cartesian or joint space (z) through energy minimization. Further experiments will also be updated. 1. Introduction Robots with human-level understanding, reasoning, and planning in the physical world remains as one of the cardinal objectives of AI [16]. Currently, behavior cloning has re-gained traction through Diffusion Policy [5] to help achieve this objective: instead of fitting brittle explicit policy that maps from robots environmental observations to actions, the method encodes goals and observations into shared latent space and learns the score (gradient) of the demonstration distribution [5, 22, 47]. discrete noise schedule then provides ground-truth denoising targets, turning the generation of manipulation sequence into reverse-diffusion process that iteratively denoises from pure noise to feasible action trajectory. Diffusion and flow-based policies, however, suffer from several practical challenges. primary concern is their reliance on an externally defined noise scheduler to model data distributions [18, 50]. This reliance introduces disconnect between the models generative dynamics and the true distribution of robot demonstrations, often resulting in slow convergence, high computational cost during training, and inefficient inference due to the large number of denoising steps required [32, 43, 57]. Another critical issue is exposure bias [29, 40], an inherent flaw in their chainstructured generation process that differs from the training procedure. Errors introduced in early stages of the diffusion sequence propagate downstream and accumulate through successive steps [31], amplifying prediction inconsistencies. This cascading error effect makes diffusion-based policies particularly difficult to train or interpret in realworld robotic settings where distribution shifts are common. Energy-Based Models (EBMs) [11, 28] naturally offer promise in resolving many of the core issues challenging diffusion models, through the avoidance of relying on specific schedulers for noise, and the learning of an energy landscape that can reduce error accumulation issues during inference [18]. Particularly, by sampling with Markov Chain Monte Carlo (MCMC) guided by the energy, the policy repeatedly re-evaluates and corrects intermediate proposals, instead of committing to fixed denoising trajectory, which can reduce cascading mistakes and schedule sensitivity [14, 18]. While Implicit attractive, theoretically Policies parametrized through EBMs have failed to become standard for policy learning due to challenges with scalability and training stability [11, 12]. Therefore, motivated by recent work demonstrating scalable and stable approach for training EBMs deemed Energy-Based Transformers (EBTs) [18], we introduce EBT-Policy, modern recipe for Implicit Policies based on EBTs. EBT-Policy broadly involves training EBMs with regularized losses, as opposed to the contrastive losses commonly used in existing Energy-Based Implicit Policies, the usage of modern Transformer architectures, and the usage of several energy landscape regularization techniques to better shape the energy landscape during training [18]. To further enhance training stability and inference efficiency, we introduce additional components including energy-scaled MCMC step sizes, pre-sample normalization, Nesterov-accelerated gradients [39], and scaled Langevin Dynamics. These mechanisms collectively improve training and inference stability, enabling more effective action sampling. As result of Energy-Based training, EBT-Policy naturally enables dynamic inference computation through iterative energy minimization and uncertainty-driven behavior through the energy scalar, which we demonstrate through experiments. Across both simulated and real-world tasks, we find training and inference with EBT-Policy to occur much faster than that of Diffusion Policy. For example, during inference, we find EBTs often can complete tasks to high success rate with only 2 inference steps, as compared to Diffusion Policy requiring 100 steps to complete the same task at the same success rate. Further, we achieve state-of-theart results on Squre and Tool Hang task in robomimic, outperforming Diffusion Policy by as much as 24%. Qualitative results also suggest that EBT-Policy successfully captures uncertainty in difficult portions of robotics tasks, offering promise towards world where robots are more aware of real-world challenges. Lastly, EBT-Policy demonstrates emergent capabilities, where EBT-Policy learns retry behavior without any explicit training data or supervision to do so, achieving capability that has long been sought after in Behavior Cloning [45]. Our results demonstrate the strong potential of Implicit policies parametrized by EBMs that act more dynamically, with uncertainty awareness, and with emergent capabilities. 2. Related Work 2.1. Implicit Behavior Cloning with Diffusion Implicit Behavior Cloning (IBC) frames policy learning as conditional energy modeling over actions; at inference, actions are obtained by minimizing the learned energy, enabling multimodality without teacher forcing [17]. Within Behavior Cloning (BC), however, Diffusion-based generative policies [22, 33, 47] have emerged as the dominant approach [2, 4, 5, 7, 8, 10, 23, 30, 36, 41, 48, 49, 54, 57, 59], which amortize the score (gradient of the energy function) prediction, rather than differentiating an underlying explicit scalar energy function. The success of Diffusion has in part been due to high training stability and capacity to model multimodal high-dimensional action distributions [4, 5, 36]. 2.2. Implicit Behavior Cloning with Energy-Based"
        },
        {
            "title": "Models",
            "content": "In contrast to diffusion-based BC, implicit Energy-Based Models (EBMs) learn an explicit energy landscape whose equilibrium dynamics define the policy [11, 28]. Previously, energy-based policies [17, 34] were found to be unstable for training due to requiring an exponential number of negative samples with increasing data dimensionality [27]. Recently, however, explicit Energy-Based methods have seen some promise in improved scalability [18], which offers promise in resolving some of the challenges with Diffusion. 2.3. Reasoning and Emergent Behavior Reasoning has become hot topic within AI, with several approaches emerging, including Chain-of-Thought [51], reFigure 2. Explaining Uncertainty Modeling. 12 frames are grouped into three phases: (1) Tool Insertion, (2) Hook Hanging Attempt, and (3) Recovery & Successful Retry. Color bar beneath each frame encodes per-frame energy predicted by the model, where lower energy indicates higher certainty in EBT-Policy. Notably, red (Step 7) marks the failure that triggers an EBT-Policy retry, while green (Step 11) marks the successful correction. Together, these steps highlight EBT-Policys interpretability and physical reasoning: using energy-based uncertainty to decide whether to continue or retry and how to adjust actions. Explaining Energy Minimization. EBT-Policy receives inputs (RGB frames, robotic proprioception, and language instructions) and assigns an energy to candidate action trajectories. Starting from noisy initialization, the trajectory is iteratively updated by gradient descent on this energy, yielding starting states to final executable plan. Optimization terminates when the energy converges to minimum, as illustrated by the energy-landscape sketch. cursive reasoning [20], and Reinforcement Learning for reasoning [19]. Within Robotics, reasoning has often been performed with Vision-Language-Action (VLA) models [3, 26, 58], which can leverage rich set of pretrained knowledge in Vision-Language Models to reason. The most similar works to ours involving reasoning leverage Energy-Based Models for reasoning, which have demonstrated strong generalization performance across discrete and continuous modalities [13, 15, 18]. In the robotics community, several existing works achieve emergent retry behavior [1, 6, 35]. However, these works focus on achieving retry behavior via planning, goalconditioning, or hierarchical executionnot from vanilla behavior-cloned policy that solely outputs instantaneous actions. To the best of our knowledge, EBT-Policy is the first approach to demonstrate emergent retry behavior solely from action prediction and without training data for retries when performing behavior cloning. 3. EBT-Policy Formulation We formulate visuomotor control policies as Energy-Based Transformers (EBTs), which we refer to as EBT-Policy. EBT-Policies represent complex, multi-modal, action distributions through an energy landscape spanning the action space, framing energy-based learning and inference as continuous optimization/sampling process. This formulation enables flexible and implicit modeling of unnormalized action likelihoods, while maintaining stability and scalability across high-dimensional visuomotor tasks. 3.1. Energy-Based Transformers Energy-Based Transformers (EBTs) [18] belong to the broader class of Energy-Based Models (EBMs) [28], which learn to map inputs to scalar energy, often referred to as the energy function. Particularly, EBMs learn probability distribution parametrized in the form of Boltzmann distribution pθ(x) = eEθ (x) . In this work, we focus on unnorZ(θ) malized EBMs, which forgo the intractable partition function Z(θ), in exchange for modeling unnormalized probabilities [18]. In this approach, the energy function quantifies the models internally inferred compatibility between an input and candidate predictions, and the generation problem corresponds to minimizing this energy, often by gradient descent. Intuitively, the energy scalar can be interpreted as compatibility estimate between the input variables in the forward pass, where the energy minimization process on the backward pass can be interpreted as series of optimizations by minimizing this scalar energy. EBTs usually generate predictions via iterative energy minimization using gradient descent, which can be expressed as: ˆyi+1 = ˆyi αiˆyEθ(x, ˆyi) + ηi where Eθ(x, ˆyi) denotes the predicted energy for candidate prediction ˆyi given input x, αi as the energy-scaled step size, and Langevin Dynamics noise ηi. This sampling procedure is approximately equivalent to Markov Chain Monte Carlo (MCMC) method known as Langevin Dynamics, in which the prediction ˆyi+1 is updated via gradient descent with respect to the scalar energy function and some noise. We iterate the number of inference steps adaptively according to the current energy level, and scale α as function of the energy to enable stable convergence and high flexibility. 3.2. EBTs for Robot Learning The objective of EBT-Policy is to generate sequence of robot actionseither in joint space or Cartesian space conditioned on recent multimodal sensory observations and, optionally, natural language instructions. Formally, let denote the space of language commands, the space of RGB visual observations, and the space of proprioceptive states. At each time step N+, the input to the policy is defined as: (ℓ, ot) (cid:0)X h(cid:1) , where ℓ represents natural language instruction and ot = (Xth+1:t, zth+1:t) denotes an h-step observation window, with Xth+1:t corresponding to the sequence of RGB frames and zth+1:t corresponding to proprioceptive state histories. The policy π maps these inputs to sequence of future actions over prediction horizon π : h An where An denotes the continuous action space. The output sequence at:t+n1 = (at, at+1, . . . , at+n1) corresponds to the robots planned actions for the next steps, aimed at achieving the task described by ℓ. In the EBT formulation, the policy is modeled as an energy minimization process rather than learning an explicit policy [17, 28], which is defined as: π : argminaAn Eθ(ℓ, ot, a) where the energy function is learned such that it assigns low energy to action trajectories consistent with the given sensory observations and language instruction. During inference, the EBT iteratively refines sampled actions via gradient-based updates to descend toward low-energy region, effectively sampling from the manifold of taskconsistent action sequences. 3.3. Diffusion vs. Energy-Based Policies While both Diffusion Policy and EBT-Policy generate actions through iterative refinement, they differ fundamentally in their probabilistic formulation and inference dynamics. DPs are score-based models that represent the data distribution via stochastic denoising process, where network ϵθ(at, t, ℓ, ot) learns to reverse forward noise process. Through denoising score matching, this network implicitly estimates the gradient of the conditional log-likelihood, log p(a ℓ, ot) which corresponds to the direction of increasing probability under the true data distribution. This formulation avoids the intractable partition function in classical energy-based models by directly modeling the score field, resulting in stable training and well-behaved likelihood gradients [5]. In contrast, EBTs define an unnormalized energy function Eθ(ℓ, ot, a) whose negative gradient yields the same score: log p(a ℓ, ot) = aEθ(ℓ, ot, a) Rather than learning to denoise stochastic samples, EBTs learn this energy landscape directly and infer actions by descending it toward low-energy (high-likelihood) states. Conceptually, diffusion models approximate the gradient of the energy, while EBTs explicitly learn the energy itself making EBTs explicitly model the underlying data density with equilibrium dynamics. 3.4. Why EBTs Work for Robot Policies 1. Equilibrium dynamics. EBTs learn single, timeinvariant explicit energy landscape which enables dynamics where OOD data points are brought closer to the data manifold. These dynamics effectively reduce exposure bias and compounding error, which commonly haunt Diffusion/Flow models once they start to drift OOD [18]. Additionally, we find that these dynamics lead to far fewer inference steps, faster training than diffusion/flow models as confirmed in Figure 5. We believe this is due to the non-reliance on noise schedule, time-variance, and ODE solvers, which impose strict constraints on model representations and the path from noise to data. Rather, EBTs can take whatever path from noise to data works best, as paths are learned end-toend rather than being based on fixed schedule. 2. Uncertainty modeling. Energy landscapes can succesfully capture uncertainty [9, 18], where uncertainty can be represented as high energy or landscapes with several local minima. Consqeuently, EBTs naturally enable uncertaintyaware sampling, where sampling continues until the energy has converged (or the gradient goes below certain norm threshold); this results in hard states getting more gradient steps, and easy states getting less, resulting in interpretable, compute-adaptive behaviour, which we verify in Figure 2. 3. OOD robustness via discriminative learning. The same scalar energy acts as verifier: low energy plausible, high energy reject. This built-in discriminator makes EBTs noticeably less brittle to environmental changes than generative diffusion/flow policies, as we verify in Figure 6. 3.5. Towards Embodied Reasoning in Robotics We claim that EBTs offer unified computational framework for embodied System 2 reasoning in robots [18, 25]. Unlike Vision-Language-Action (VLA) [2, 3, 24, 26, 41] (a) Collect Dish (b) Fold Towel EBT-Policy Hyperparameters Value Base step size (ηb) Step size scaling factor (c) Min. LD scale (σmin) Max. LD scale (σmax) Num. of base MCMC train steps Num. of randomized additional steps Max. MCMC inference steps Gradient clipping norm 1000 1.5 0.002 0.2 6 3 20 1. Table 1. Training hyperparameters for EBT-Policy. (c) Pick and Place Figure 3. Demonstrations from tabletop, real-world tasks. architectures and diffusion-based policies [4, 5, 36, 49, 54, 57], which rely on discrete tokenization, fixed compute budgets, and/or externally imposed reasoning modules, EBTs integrate perception, reasoning, and control within single energy-minimization objective. This formulation enables robots to reason natively in continuous sensory space and to allocate computation adaptively based on internal uncertainty. Three properties characterize this native reasoning capability: 1. Continuous-native sensor grounding. EBTs operate directly on continuous sensory representations (e.g., proprioceptive and camera data), enabling reasoning in the robots native embodiment space rather than over discrete space (e.g., written text). 2. Uncertainty-aware dynamic inference. The number of inference steps is dynamically allocated according to task complexity and internal energy gradients, allowing EBTs to modulate computational effort based on internal uncertainty in observation and goal data. 3. Self-supervised cognitive optimization. Training is driven by internal energy dissipation-minimizing energy for consistent action trajectories, providing practical solution to self-supervised System 2 training procedure. 3.6. EBT-Policy Ingredients Training energy-based trajectory (EBT) policies is hindered by two issues: action demonstration data is inherently multimodal [5, 36], tempting the model into local energy minima, and the iterative MCMC refinement creates long gradient chains that can often explode in practice, much like the well-documented issues of traditional recurrent-style models [42]. We introduce design tweaks that curb both problems and stabilize learning. 3.6.1. Learning Multimodal Action Distributions We introduce several training-only mechanisms that enable the EBT-Policy to capture and explore multi-modal action distributions. Randomized Sampling Steps. The total number of Markov Chain Monte Carlo (MCMC) sampling steps is randomized to promote stochastic exploration across different energy modes. Scaled Langevin Dynamics. Each sampling step is augmented with scaled Langevin Dynamics [52], which injects controlled stochasticity into the sampling process. The magnitude of the injected noise follows cosine Langevin Dynamics annealing schedule [37], applied to the noise standard deviation such that (0, σt). The noise level smoothly decays from σmax to σmin over total steps: σt = σmin + 1 2 (σmax σmin) (cid:20) 1 + cos (cid:19)(cid:21) (cid:18) π (1) High-energy regions therefore receive stronger noise for broad exploration, while low-energy regions use smaller noise for precise convergence. Step Size Randomization. We use MCMC step size randomization by sampling the initial step size as η (ηb/c, ηb c), where ηb denotes the base step size and is scaling factor controlling the mean and variance. This stochasticity encourages diverse sampling trajectories and improves robustness against local minima. Moreover, the base step size ηb contributes to training stability by inversely scaling with the predicted gradient magnitudes, thereby mitigating the risk of exploding gradients. Nesterov Acceleration. Finally, we employ Nesterovs accelerated gradients [39] to enable smoother traversal of complex energy surfaces and to help escape shallow local minima. 3.6.2. Model Stability Algorithm 1 EBT-Policy Training To address training instability and prevent excessive parameter updates, we incorporate several stabilization mechanisms that collectively ensure smooth and reliable optimization of EBT Policies. Energy-Scaled Step Sizes. To prevent overshooting or undershooting minima, we employ energy-scaled step sizes, defined as: αi = η exp (cid:0)Eθ(x, ˆyi)(cid:1) which adaptively modulate update magnitudes in proportion to predicted energy, yielding well-conditioned optimization. Pre-Sample Normalization. Action trajectories are normalized to [1, 1] prior to input into the EBT, preventing uncontrolled growth of action magnitudes and gradients during sampling. For this paper, we use RMSNorm for presample normalization [55]. Gradient Clipping. To mitigate exploding gradients causing instability and prevent large updates from saturating model weights, we apply gradient clipping [56], constraining the global gradient norm to maximum value of 1.0. Empirically, we find this technique to be the most critical component for ensuring stable and reliable training of EBT Policies. 3.7. EBT-Policy Training Algorithm 1 outlines the training procedure for the EBT Policy. During each training iteration, the target trajectory is initialized as pure noise. The model then encodes its conditional inputs, including multimodal sensory observations and, when available, language instructions. Subsequently, randomized Markov Chain Monte Carlo (MCMC) step sampling is performed, where trajectories are normalized, and Langevin Dynamics are injected according to an annealed noise schedule σi. The model predicts the energy landscape Eθ, after which the step size is randomized and scaled proportionally to the predicted energy. The loss is then accumulated over each sampled trajectory by comparing the denoised trajectory with the ground-truth demonstration. 3.8. Dynamic Inference in EBT-Policy Algorithm 2 describes the dynamic inference process of the EBT-Policy. Unlike conventional diffusionor samplingbased policies that use fixed number of inference steps, EBT-Policy performs adaptive energy descent, dynamically determining the number of Markov Chain Monte Carlo (MCMC) updates required for convergence. The process continues until either the maximum step limit is reached or the energy gradient norm g2 falls below threshold τ . This dynamic termination criterion allows EBT-Policy to automatically allocate computation based on its interInputs: Context x, Target y, Context encoder fθ(x), EBT decoder Eθ(z, ˆy) Hparams: MCMC Steps 1: Sample ˆy0 (0, I) 2: fθ(x) 3: 0 4: for = 0 to 1 do 5: (2) 6: ˆyi RMSNorm(ˆyi) + (0, σi) ˆyi+1 ˆyi αiˆyEθ(z, ˆyi) + MSE(ˆyi+1, y) 7: 8: end for 9: update Eθ, return nal certainty staterequiring fewer steps for low-energy (high-certainty) regions and more steps for uncertain states. This makes inference both computationally efficient and confidence-aware, enabling EBTs to adaptively balance precision and runtime without manual tuning. Algorithm 2 EBT-Policy Dynamic Inference Inputs: Context x, Context encoder fθ(x), EBT decoder Eθ(x, ˆy) Hparams: Max MCMC steps , Gradient cutoff τ 1: Sample ˆy0 (0, I) 2: fθ(x) 3: 0 4: 5: while < and g2 > τ do ˆyi RMSNorm(ˆyi) 6: ˆyEθ(z, ˆyi) ˆyi+1 ˆyi αi + 1 8: 7: 9: 10: end while 11: return ˆyM 4. Experiments To evaluate the proposed EBT-Policy, we conduct range of experiments across both simulated benchmarks and physical robotic environments. Two architectural variants of EBT-Policy are considered, as shown in Table 2. The distinction between the two variants of EBT-Policy serve distinct experimental goals: 1. Controlled simulation benchmarks. EBT-Policy-S adopts the same architecture as Diffusion Policy to enable fair, one-to-one comparison under identical simulation conditions, isolating the effect of energy-based versus diffusion-based objectives. 2. Real-world robotic manipulation evaluation. EBTPolicy-R is deployed in physical environments and comProperty EBT-Policy-S EBT-Policy-R"
        },
        {
            "title": "Task",
            "content": "30M Simulation"
        },
        {
            "title": "Language Encoder",
            "content": "N/A 100M Real World T5-S [44]"
        },
        {
            "title": "Vision Encoder",
            "content": "ResNet-18 [21] DINOv3-S [46] Table 2. Comparison of EBT-Policy variants. EBT-Policy-S is compact Transformer used for controlled simulation studies, while EBT-Policy-R is larger multimodal variant designed for real-world, language-conditioned, and multitask policy learning. (a) Lift (b) Can (c) Square (d) Tool Hang Figure 4. Representative tasks in robomimic [38]. pared with state-of-the-art diffusion-based policies to assess scalability and real-world applicability, under fixed 100M parameter budget for fairness. Comparisons with billion-parameter Vision-Language-Action or robot foundation models are omitted, as their internet-scale pretraining obscures fair comparison of policy objectives. All baselines are diffusion transformers of comparable size (150M) trained on the same robot-demonstration data, providing controlled evaluation of the energy-based formulation. 4.1. Task Design Simulation Tasks. We first evaluate in simulation using the robomimic benchmark suite [38], focusing on four manipulation tasks of increasing difficulty: Lift, Can, Square, and Tool Hang. Each task in robomimic is characterized along several axes: task difficulty, positional variance, goal precision, object dynamics, and temporal horizon-designed to progressively challenge policy robustness and generalization. The robomimic environment provides standardized and reproducible setting for evaluating robot policy performance. Real-World Tasks. To assess real-world transfer, we design set of general manipulation tasks on dual-arm robotic platform. These tasks require multi-task learning, robustness to real-world sensor noise, and goal understanding: FoldTowel: Both arms collaboratively fold towel twice, testing deformable-object manipulation and robustness to initial-state variability. PlacePan: The arm grasps pan and places it on rack Figure 5. Success Rates During Training. EBT-Policy exhibits rapid performance improvement, reaching 100% success by epoch 30, using just 2 iterations for predicting actions. Diffusion Policy (DP), on the other hand, only reaches 100% success rate after 90 epochs, and uses 50 times more steps than EBT-Policy at inference, demonstrating how EBT-Policy is more efficient than DP during both training and inference. at varying positions and orientations, evaluating spatial generalization and placement precision. PickAndPlace: One arm grasps black block, placing it on purple circle, for another arm to pick up the black block and put it in pink container. These tasks provide systematic assessment of EBTs ability to sustain stable performance under realistic environmental variability. 4.2. Experiment Setup We also compare the proposed EBT-Policy with Diffusion Policy [5]. Diffusion Policy acting as the primary baseline, enabling direct comparison between diffusion-based and energy-based policy learning. All models are trained on the same dataset, with default parameters provided in opensource implementations used. For simulation benchmarks both Diffusion Policy and EBT-Policy-R share identical architectures, hyperparameters, and augmentations, differing only in timestep conditioning and training objective. For Diffusion Policy, we additionally train 10and 100-step variants to analyze the relationship between inference steps and task success. Simulation experiments use the open-source robomimic datasets, with 50 episodes tested for success rate per simulation rollout. Real-world data is collected by us via teleoperation system for dual-arm, 4 RGB camera, tabletop setup. Real-World Task Fold Towel Collect Pan Pick And Place D.P. [5] EBT-Policy-R 10 86 65 75 90 92 Table 3. Real-World task success rates (%). Comparison of EBT-Policy-R and DP in success rates for three tasks. Numbers in bold highlights EBT-Policy-R outperforms DP in all three tasks. Benchmark Task D.P. [5] (n=10) D.P. (n=100) EBT-Policy-S Lift 0 100 100 Can Square Tool Hang 0 100 100 0 92 98 0 44 68 Table 4. Simulated task maximum success rates (%). Performance results on four robomimic tasks. SR in bold of our EBTPolicy-S still exceed DPs in simulation benchmarks with both fewer (10) steps and more (100) step. 5. Results Simulation results are shown in Table 4, as shown in our results, our model outperforms Diffusion Policy on all four tasks. As shown in the results, Diffusion Policy was found to only perform well when the number of iterations was as high as 100 inference steps, which is often tenfold larger than the number of iteration steps required for EBT-Policy. This demonstrates that the compute resources for EBTPolicy during inference is much more economic than for Diffusion Policy. As illustrated in Figure 5, the EBT-policy demonstrates superior success rates and training efficiency compared to the diffusion policy (DP). Specifically, the EBT-policy achieves 100% success rate within just 30 epochs. In contrast, even with up to 100 iterations, the highest-performing DP variant fails to exceed 60% success rate over 100 epochs. This marked difference underscores the advantages of the verification-based approach in terms of both learning speed and final performance outcomes. Noteworthy is the EBT-policys computational efficiency; it requires approximately 50 times fewer iterations and 55% fewer epochs than the DP to achieve comparable results. This indicates not only its robustness during training but also its ability to converge more quickly by focusing on plausible action sequences rather than generating them from scratch. Furthermore, reducing the iteration parameter significantly degrades the DPs performance. When set to 10 iterations, the DPs success rate plummets to 0%, starkly contrasting with the EBT-policy, which maintains high performance even at 2 iterations. This comparison vividly illustrates the EBT-policys remarkable efficiency and reliability. Figure 6. EBT-Policy Emergent Retry Behavior. diagram illustrating the different outcomes of Diffusion Policy and EBTPolicy after encountering failure causing covariate shift. When Diffusion Policy drifts OOD (e.g., by missing the hook), it fails catastrophically, causing compounding error and eventually divergence. EBT-Policy, after failing to hook the first time, is able to recover, and retry hooking despite being in an OOD state. This is particularly challenging as EBT-Policy has never seen such data or been explicitly trained to recover from such failed states. We hypothesize this emergent retry behavior is due to the equilibrium dynamics of the energy function, which demonstrates the emergent reasoning capabilities of EBT-Policy. 6. Discussion core finding in our experiments is EBT-Policys emergent retry behavior observed in the Tool Hang simulation task (Figure 6). When the hook is displaced from in-distribution configurations, EBT-Policy adaptively formulates new recovery strategies, maintaining stable control and ultimately completing the task. In contrast, Diffusion Policy (DP) collapses under similar out-of-distribution (OOD) perturbations, releasing the tool and failing to recover. Both policies initially perform well during the indistribution phase, successfully grasping and positioning the tool, but when minor contact forces cause the hook to rotate into novel configuration, DP loses stability, whereas EBT repositions the tool and completes the placement. To the best of our knowledge, such emergent retry behavior has not previously been observed in vanilla behavior cloning (BC) policies, which are typically brittle under OOD conditions. This distinction aligns with the principles outlined by [18, 53], suggesting that discriminative, energybased architectures exhibit greater robustness in OOD states than generative counterparts. We hypothesize that this robustness gap is further amplified in robotics, where OOD states naturally arise due to the stochastic and dynamic nature of real-world environments. Moreover, this emergent behavior supports the theoretical arguments of EBT [18] that energy-based verification and incremental energy minimization can yield models capable of native reasoning-like behavior. In repeated runs of the Tool Hang task, EBT-Policy consistently demonstrated this retry capability, recovering from perturbations that would terminate standard generative or behavior cloning policies. To further analyze this phenomenon in Figure 6, we tracked EBT-Policys predicted energy trajectories across sampled robot rollouts showed in Figure 2. The results revealed that the retry behavior closely corresponds to iterative energy minimization: the model initially enters highenergy states following failed attempt, then progressively lowers its energy as it converges toward stable, taskcompleting configuration. This alignment suggests that EBT-Policys emergent retry behavior is intrinsically tied to its uncertainty modeling and energy-based optimization. The findings further indicate emergent physical reasoning: EBT-Policy distinguishes successful from failed contact events and adapts its actions to re-establish effective interaction with the environment. Together, these results point to compelling direction for future investigation into reasoning-driven robotic control. In simulation, the maximum success rates achieved are somewhat lower than those reported in the original Diffusion Policy paper. This discrepancy is likely due to differences in hyperparameter configurations, while we matched the released real-world parameters as closely as possible, minor differences may have affected the simulated outcomes. The main challenges we identified for EBT-Policy involve training stability and sampling under highly multimodal action distributions. While Diffusion Policy continues to outperform in stability and multimodal sampling, the new hyperparameter strategies and architectural refinements we introduce partially mitigate these issues. Future research should focus on improving optimization stability and mode coverage to fully leverage the benefits of energybased implicit policies. 7. Conclusion We presented EBT-Policy, an implicit policy that replaces iterative denoising with energy minimization over trajectories. By learning equilibrium dynamics, EBT-Policy delivers more stable inference, greater robustness to distribution shift, and substantially lower compute than diffusionand flow-based baselines. On our simulated and real-world benchmarks, it consistently outperforms Diffusion Policy while using far fewer training epochs and as few as two inference iterations (up to 50 fewer than DP). Beyond efficiency and accuracy, EBT-Policy exhibits emergent retry and adjustment after failure, guided by its scalar energy as an uncertainty signal. This behavior indicates interpretable physical reasoning: distinguishing successful from failed contacts and adapting actions to reestablish effective interaction. Together, these results lay solid foundation for energy-based world models in manipulation, where single energy function measures consistency across observations, language, actions, and dynamics. It is excited to explore scaling EBT-Policy architectures, improved optimization schemes, and larger, more diverse datasets to further enhance performance and generalization. These directions cement EBT-Policy as foundation for energy-based world models and advance unified, reasoningcapable robot policies under single energy function."
        },
        {
            "title": "References",
            "content": "[1] Suneel Belkhale, Yuchen Cui, and Dorsa Sadigh. Hydra: Hybrid robot actions for imitation learning. In Conference on Robot Learning, pages 21132133. PMLR, 2023. 3 [2] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. π0: vision-language-action flow model for general robot control, 2024. 2, 4 [3] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-2: Vision-language-action models transfer web knowledge to robotic control, 2023. 3, 4 [4] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, 2024. 2, 5 [5] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion, 2024. 1, 2, 4, 5, 7, 8 [6] Geoffrey Cideron, Baruch Tabanpour, Sebastian Curi, Sertan Girgin, Leonard Hussenot, Gabriel Dulac-Arnold, Matthieu Geist, Olivier Pietquin, and Robert Dadashi. Get back here: Robust imitation by return-to-distribution planning. arXiv preprint arXiv:2305.01400, 2023. 3 [7] Travis Davies, Jiahuan Yan, Xiang Chen, Yu Tian, Yueting Zhuang, Yiqi Huang, and Luhui Hu. Spatially visual perception for end-to-end robotic learning, 2024. 2 [8] Travis Davies, Yiqi Huang, Yunxin Liu, Xiang Chen, Huxian Liu, and Luhui Hu. Tenma: Robust cross-embodiment robot manipulation with diffusion transformer, 2025. [9] Anna Dawid and Yann LeCun. Introduction to latent variable energy-based models: path toward autonomous machine intelligence. Journal of Statistical Mechanics: Theory and Experiment, 2024(10):104011, 2024. 4 [10] Ria Doshi, Homer Walke, Oier Mees, Sudeep Dasari, and Sergey Levine. Scaling cross-embodied learning: One policy for manipulation, navigation, locomotion and aviation. arXiv preprint arXiv:2408.11812, 2024. 2 [11] Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models, 2020. 2 [12] Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor MorImproved contrastive divergence training of energy datch. based models. arXiv preprint arXiv:2012.01316, 2020. 2 [13] Yilun Du, Shuang Li, Joshua B. Tenenbaum, and Igor Mordatch. Learning iterative reasoning through energy minimization, 2022. 3 [14] Yilun Du, Jiayuan Mao, and Joshua Tenenbaum. Learning iterative reasoning through energy diffusion. arXiv preprint arXiv:2406.11179, 2024. 2 [15] Yilun Du, Jiayuan Mao, and Joshua B. Tenenbaum. Learning iterative reasoning through energy diffusion, 2024. 3 [16] Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman, Brian Ichter, Danny Driess, Jiajun Wu, Cewu Lu, and Mac Schwager. Foundation models in robotics: Applications, challenges, and the future, 2023. 1 [17] Pete Florence, Corey Lynch, Andy Zeng, Oscar Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning, 2021. 2, 4 [18] Alexi Gladstone, Ganesh Nanduru, Md Mofijul Islam, Peixuan Han, Hyeonjeong Ha, Aman Chadha, Yilun Du, Heng Ji, Jundong Li, and Tariq Iqbal. Energy-based transformers are scalable learners and thinkers, 2025. 1, 2, 3, 4, 8 [19] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 3 [20] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015. 7 [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. 1, 2 [23] Yiqi Huang, Travis Davies, Jiahuan Yan, Xiang Chen, Yu Tian, and Luhui Hu. Robograsp: universal grasping policy for robust robotic control, 2025. 2 [24] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, and Ury Zhilinsky. vision-language-action model with open-world generalization, 2025. 4 π0.5: [25] Daniel Kahneman. Thinking, Fast and Slow. Farrar, Straus and Giroux, New York, 2011. 4 [26] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An opensource vision-language-action model, 2024. 1, 3, 4 [27] Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162, 2022. 2 [28] Yann LeCun, Sumit Chopra, Raia Hadsell, Aurelio Ranzato, and Fu Jie Huang. tutorial on energy-based learning. In Tutorial on Energy-Based Learning, 2006. 1, 2, 3, 4 [29] Mingxiao Li, Tingyu Qu, Ruicong Yao, Wei Sun, and MarieFrancine Moens. Alleviating exposure bias in diffusion models through sampling with shifted time steps, 2024. [30] Shuang Li, Yihuai Gao, Dorsa Sadigh, Song. arXiv:2503.00200, 2025. 2 Unified video action model. and Shuran arXiv preprint [31] Yangming Li and Mihaela van der Schaar. On error propagation of diffusion models, 2024. [32] Yiming Li, Nael Darwiche, Amirreza Razmjoo, Sichao Liu, Yilun Du, Auke Ijspeert, and Sylvain Calinon. Geometryaware policy imitation, 2025. 2 [33] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. 2 [34] Minghuan Liu, Tairan He, Minkai Xu, and Weinan Zhang. Energy-based imitation learning, 2021. 2 [35] Minghuan Liu, Zixuan Chen, Xuxin Cheng, Yandong Ji, RiZhao Qiu, Ruihan Yang, and Xiaolong Wang. Visual wholebody control for legged loco-manipulation. arXiv preprint arXiv:2403.16967, 2024. 3 [36] Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation, 2025. 2, [37] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts, 2017. 5 [38] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Martın-Martın. What matters in learning from offline human demonstrations for robot manipulation, 2021. 7 [39] Yurii Nesterov. method for solving the convex programming problem with convergence rate o(1/kˆ2). Soviet Mathematics Doklady, 27(2):372376, 1983. 2, 5 [40] Mang Ning, Enver Sangineto, Angelo Porrello, Simone Input perturbation reduces Calderara, and Rita Cucchiara. exposure bias in diffusion models, 2023. [41] NVIDIA, :, Johan Bjorck, Fernando Castaneda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Jim Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Pang Wei Koh, Allyson Ettinger, and Yejin Choi. The generative ai paradox: what it can create, it may not understand, 2023. 8 [54] Rujia Yang, Geng Chen, Chuan Wen, and Yang Gao. Fp3: 3d foundation policy for robotic manipulation, 2025. 2, 5 [55] Biao Zhang and Rico Sennrich. Root mean square layer normalization, 2019. [56] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: theoretical justification for adaptivity, 2020. 6 [57] Qinglun Zhang, Zhen Liu, Haoqiang Fan, Guanghui Liu, Bing Zeng, and Shuaicheng Liu. Flowpolicy: Enabling fast and robust 3d flow-based policy via consistency flow matching for robot manipulation, 2024. 2, 5 [58] Xingcheng Zhou, Xuyuan Han, Feng Yang, Yunpu Ma, and Alois C. Knoll. Opendrivevla: Towards end-to-end autonomous driving with large vision language action model, 2025. 3 [59] Chuning Zhu, Raymond Yu, Siyuan Feng, Benjamin Burchfiel, Paarth Shah, and Abhishek Gupta. Unified world models: Coupling video and action diffusion for pretraining on large robotic datasets, 2025. 2 Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, and Yuke Zhu. Gr00t n1: An open foundation model for generalist humanoid robots, 2025. 2, 4 [42] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks, 2013. 5 [43] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for visionlanguage-action models, 2025. [44] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2023. 7 [45] Stephane Ross and Drew Bagnell. Efficient reductions for In Proceedings of the thirteenth interimitation learning. national conference on artificial intelligence and statistics, pages 661668. JMLR Workshop and Conference Proceedings, 2010. 2 [46] Oriane Simeoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothee Darcet, Theo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herve Jegou, Patrick Labatut, and Piotr Bojanowski. Dinov3, 2025. 7 [47] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022. 1, 2 [48] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy, 2024. [49] Dian Wang, Stephen Hart, David Surovik, Tarik Kelestemur, Haojie Huang, Haibo Zhao, Mark Yeatman, Jiuguang Wang, Robin Walters, and Robert Platt. Equivariant diffusion policy, 2024. 2, 5 [50] Runqian Wang and Yilun Du. Equilibrium matching: Generative modeling with implicit energy-based models, 2025. 2 [51] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. 2 [52] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics. In International Conference on Machine Learning, 2011. 5 [53] Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena D. Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Chandu, Benjamin Newman,"
        }
    ],
    "affiliations": []
}