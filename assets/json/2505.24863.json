{
    "paper_title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time",
    "authors": [
        "Junyu Zhang",
        "Runpei Dong",
        "Han Wang",
        "Xuying Ning",
        "Haoran Geng",
        "Peihao Li",
        "Xialin He",
        "Yutong Bai",
        "Jitendra Malik",
        "Saurabh Gupta",
        "Huan Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents AlphaOne ($\\alpha$1), a universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. $\\alpha$1 first introduces $\\alpha$ moment, which represents the scaled thinking phase with a universal parameter $\\alpha$. Within this scaled pre-$\\alpha$ moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as a Bernoulli stochastic process. After the $\\alpha$ moment, $\\alpha$1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate $\\alpha$1's superior reasoning capability and efficiency. Project page: https://alphaone-project.github.io/"
        },
        {
            "title": "Start",
            "content": "ALPHAONE: Reasoning Models Thinking Slow and Fast at Test Time Junyu Zhang"
        },
        {
            "title": "Haoran Geng",
            "content": "Runpei Dong Peihao Li"
        },
        {
            "title": "Huan Zhang",
            "content": "University of Illinois Urbana-Champaign"
        },
        {
            "title": "UC Berkeley",
            "content": "Equal contributions. Correspondence: {junyuz6, runpeid2, huanz}@illinois.edu 5 2 0 2 0 3 ] . [ 1 3 6 8 4 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This paper presents ALPHAONE (α1), universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. α1 first introduces α moment, which represents the scaled thinking phase with universal parameter α. Within this scaled pre-α moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as Bernoulli stochastic process. After the α moment, α1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate α1s superior reasoning capability and efficiency. Project page: https://alphaone-project.github.io/"
        },
        {
            "title": "Introduction",
            "content": "The most effortful forms of slow thinking are those that require you to think fast. Thinking, Fast and Slow (Kahneman, 2011) Large Reasoning Models (LRMs) such as OpenAI o1 (Jaech et al., 2024) and DeepSeekR1 (DeepSeek-AI et al., 2025) have demonstrated unprecedented progress in approaching humanlike system-2 reasoning capabilities, enabling slow thinkingslowing down reasoning progress1 at test timefor solving complex reasoning problems that require high-order cognitive processing. These advanced models are trained to utilize slow thinking via reinforcement learning, enabling LRMs 1Consider reasoning progress as metric ranging from 0 to 1, indicating the start and the end of reasoning, respectively. It increases slowly or fast at the pace of thinking. See Fig. 1 and Section 2 for more illustrative and detailed explanation. 1 Figure 1: Conceptual illustration of reasoning modulation strategies. Our α1 employs slow-to-fast reasoning schedule controlled by α. α1 scales more efficiently than monotonously increasing method s1 (yellow) and generally outperforms monotonously decreasing (purple) approaches. to slow down reasoning progress automatically. Is such automatic slowing down of reasoning progress determined by LRMs sufficiently reliable? According to Kahneman (2011), humans typically think fast first and activate slow thinking when running into difficulty, through conscious control of system-1-to-2 reasoning transitioning, resulting in overall comprehensive but efficient reasoning. While similar to human systems and interesting results have been observed, lot of works have pointed out that the LRMs themselves are prone to overthinking (Chen et al., 2024b; Sui et al., 2025; Pu et al., 2025; Yang et al., 2025c) or underthinking (Su et al., 2025; Yang et al., 2025d; Wang et al., 2025). This is because of the inability of LRMs to find the optimal human-like system-1-to-2 reasoning transitioning and limited reasoning capabilities, leading to unsatisfactory reasoning performance. To overcome this limitation, existing works scale LRMs at test time in mainly two ways. i) paralrepresents α moment (Section 3.1). α1 applies dense reasoning Figure 2: Overview of ALPHAONE (α1). Here modulation via user-defined slow thinking scheduling in pre-α moment. In addition, α1 utilizes post-α moment modulation by replacing slow thinking transitioning tokens wait to </think>, which fosters fast thinking. Specifically, α determines when the slow-to-fast reasoning transition occurs. For example, reducing α from 1.4 to 1.0 shifts the α moment earlier, resulting in shorter slow reasoning phase and accelerating the annealing of pwait. lel scaling: this line of research follows Best of strategy and typically samples times and outputs the best answer using criteria such as self-consistency (Wang et al., 2023; Wan et al., 2024a; Zhou et al., 2025; Ma et al., 2025) and perplexity (Fang et al., 2025). ii) sequential scaling: this family of approaches addresses the overthinking/underthinking issues via early reasoning stopping (Fu et al., 2025; Yang et al., 2025a; Xu et al., 2025) and promoting for reinforcing reasoning (Muennighoff et al., 2025; Wang et al., 2025), respectively. For example, Xu et al. (2025) proposes Chain of Draft, prompting LRMs to think fast strictly within 5 words to significantly reduce overthinking. s1 (Muennighoff et al., 2025) proposes to foster reasoning continuously via appending slow-reasoning transition token wait multiple times when LRMs are about to end. However, it is unclear if such monotonous reasoning increment or reduction is optimal, and the appropriate moment for slow thinking transitioning is still underexplored. Hence, instead of test-time scaling with an automatic slowing down by LRMs themselves or simply increasing or reducing slow thinking, we are interested in finding: Can we modulate reasoning progress universally, and develop better slow thinking transitioning strategy with it? To answer this question, we present ALPHAONE (α1), which efficiently scales LRMs at test time through universal reasoning progress modulation. We introduce alpha moment, parameterized by α 0, where the thinking process is scaled by α times throughout the whole generation sequence. To be specific, within certain token length scaled by α, we stochastically append the reasoning transition token wait after structural delimiters nn under Bernoulli(pwait), inspired by the observation that these two frequently co-exist (Yang et al., 2025c). Here, pwait is scheduled to change over time to activate slow thinking. For example, simple linear annealing over time indicates slow thinking first, then fast thinking strategy. However, we observe that amplifying slow thinking enables LRMs to sustain it automatically. Thus, when pwait reaches 0, we replace wait with </think> to deactivate slow thinking and switch to fast reasoning. In this fashion, α1 unifies prior methods like s1 (Muennighoff et al., 2025), where α1 reduces to s1 if pwait is 1 or 0 at the end of reasoning segment within certain reasoning token length. However, different from these works that only explore sparse slow reasoning modulation, α1 modulates reasoning continuously, supporting both sparse and dense modulation strategies. 2 Takeaways We present some insightful findings from evaluating three different α1 LRMs, ranging from 1.5B to 32B across six reasoning benchmarks, including math, code generation, and scientific problem reasoning: i) Slow thinking first, then fast thinking, leads to better LRM reasoning. Surprisingly, this differs from humans who commonly think fast, followed by slow thinking (Kahneman, 2011), emphasizing the requirement of dedicated test-time scaling strategies for LRMs. ii) Slow thinking can bring efficient test-time scaling. While slow thinking slows down reasoning, the overall token length is significantly reduced with α1, inducing more informative reasoning progress brought by slow thinking. iii) Slow thinking transitioning in high frequency is helpful. Interestingly, we find that α1 appending wait significantly more (e.g., over 2 more than s1) achieves much better results."
        },
        {
            "title": "2 Background & Problem Statement",
            "content": "Revisiting Reasoning Models Following the success of OpenAIs o1 model (Jaech et al., 2024), modern LRMs solve complex reasoning problems via thinking-then-answering paradigm (DeepSeek-AI et al., 2025; Qwen Team, 2025; Huang et al., 2024b). Generally, special end-of-thinking token </think> is generated as end-of-thinking moment, transitioning from the thinking phase to the answering phase. During the thinking process, LRMs automatically transit between slow thinking and fast thinking, utilizing selfreflection as chain of thoughts (Wei et al., 2022). Slow Thinking Transitioning To leverage human-like system-2 slow thinking that helps solve complex reasoning problems, o1-style LRMs automatically transit between fast thinking and slow thinking. To be specific, during the thinking process, LRMs frequently generate slow thinking transitioning tokens such as wait, hmm, and alternatively, etc. Once these tokens are generated, LRMs slow down reasoning, where previous reasoning chains are self-reflected and corrected immediately. Hence, reasoning following the transitioning token can be viewed as slow thinking, while the rest is generally fast thinking. Reasoning Progress Let the overall answer sequence generation process be reasoning progress [0, 1], where 0 and 1 indicate the start and the end of reasoning, respectively. Notably, reasoning progress represents the overall problem-solving progress instead of the number of generated tokens, where reasoning progress closer to 1 represents the reasoning chain is more informative. For example, the reasoning progress can be closer to 1 while generating fewer tokens, indicating more efficient reasoning. However, it is intractable to measure the exact progress obtained. Hence, we define the reasoning progress following reasoning velocity assumption. Given the total time = > 0 spent on generating the whole sequence, the reasoning velocity at timestep t, Vt is defined as dP dt , where dt is the infinitesimal of time. We assume: Assumption 1. The reasoning velocity of slow thinking is smaller than that of fast thinking. See Fig. 1, different reasoning strategies result in different reasoning progress achieved over time. 2.1 Reasoning Progress Modulation: Universal View of Test-Time Scaling There are mainly two components that must be modulated: i) Thinking phase budget. As discussed before, o1-like LRMs follow think-thenanswer paradigm. Therefore, modulating reasoning via scaling up or down the thinking phase budget is required. ii) Slow thinking scheduling. Within the thinking phase, the transition to slow thinking should also be modulated, thus increasing or reducing slow thinking according to certain plan specified by users (e.g., slow thinking first, and then fast thinking). With user-defined scheduling, the modulation of slow thinking transitions vary arbitrarily, ranging from sparse modulationwhere little is adjustedto dense modulation, where adjustments are frequent and extensive. Based on the above analysis, we establish unified perspective on test-time scaling and identify key limitations in existing approachesnamely, their failure to consider both reasoning schedule and overall thinking budget jointly. For instance, s1 modulates reasoning by sparsely increasing slow thinking (i.e., adding two wait tokens), but overlooks broader thinking budget adjustments (Muennighoff et al., 2025). Conversely, Chain-of-Draft (CoD) reduces the thinking budget while neglecting the scheduling of slow thinking (Xu et al., 2025). As result, while LRMs are indirectly guided to reason more or lesssometimes achieving deeper reasoning or pruning unproductive thoughtswe instead aim to explicitly and universally modulate the reasoning process by jointly considering both components, as introduced next."
        },
        {
            "title": "3 ALPHAONE",
            "content": "3.3 Post-α Moment Modulation We introduce ALPHAONE (α1), universal reasoning progress modulation framework for test-time scaling of LRMs, which is illustrated in Fig. 2. In the following, we first introduce α moment in Section 3.1, moment that the thinking phase budget is scaled at least α. In Section 3.2 and Section 3.3, we detail how we modulate slow thinking scheduling pre-α moment and modulating fast thinking encouragement post-α moment, respectively. 3.1 α Moment for Universal Modulation To modulate the thinking phase budget, we propose to scale the thinking phase by at least α, where α > 1 is universal modulating parameter. Formally, given the average thinking phase token length think > 0 generated by an LRM, we scale the thinking phase token length to αN , where the moment when the generated token length reaches αN is dubbed as α moment. In addition to scaling the thinking phase, we modulate the thinking phase via slow thinking scheduling before the α moment, thus achieving both controllable and scalable thinking. Note that α moment does not represent the new thinking phase transitioning moment, because the thinking phase typically continues after α moment, which we will elaborate later."
        },
        {
            "title": "3.2 Pre-α Moment Modulation",
            "content": "Following previous works (Yang et al., 2025c; Muennighoff et al., 2025), we activate slow thinking before α moment via appending wait after frequently co-generated structural delimiters nn. Moreover, the activation of slow thinking is conducted following user-specified scheduling plan, such as slow thinking, then fast thinking. Stochastic Reasoning Transitioning Our α1 achieves such scheduling by modeling the activation of slow thinking as Bernoulli stochastic process. Specifically, α1 append wait following Bernoulli(pwait). Let = 0, 1, . . . , Tm be the timestamps of generated tokens before α moment, where Tm = αN think represents the timestamp of α moment. pwait is determined by user-specified scheduling function S(t), pwait := S(t), = 0, 1, . . . , Tm. This scheduling function can be arbitrary functions, such as linear annealing and linear increase. α1 adopts linear annealing, which we find the most effective and efficient (See Section 4.3.1). (1) While an LRM significantly increases slow thinking through pre-α modulation, this extended thinking phase often exhibits slow thinking inertia, making it difficult to transition back to fast thinking. Notably, without post-α moment modulation, the LRM substantially reduces the likelihood of generating </think>. Furthermore, inserting few </think> tokens does not effectively overcome the inertia, failing to fully restore fast thinking. Deterministic Reasoning Termination After the α moment, we guide α1 to transition into fast reasoning by disabling further slow thinking. Specifically, any generated slow reasoning transition token wait is replaced with </think> to explicitly mark the end of the thinking phase, reinforcing shift to fast thinking before entering the answering phase. This deterministic termination strategy allows α1 to conclude reasoning naturally and consistently, enabling more efficient test-time scaling."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Benchmarks To comprehensively evaluate the reasoning capability of LRMs, we conduct systematic evaluations on six benchmarks covering three reasoning categories: i) mathematical reasoning, including AIME 2024 (AIME24) (Mathematical Association of America, 2024), , AMC23 (AI-MO, 2024), and Minerva-Math (Minerva) (Lewkowycz et al., 2022); ii) code generation, including LiveCodeBench (LiveCode) (Jain et al., 2025); iii) including OlympiadBench scientific problems, (Olympiad) (He et al., 2024). We report the problem-solving accuracy by average Pass@1 (%), and the average number of generated tokens. Base Models We use three o1-like open-source LRMs as the base model, including DeepSeek R1 distilled DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al., 2025), as well as recently larger LRM Qwen QwQ 32B (Qwen Team, 2025). Implementations Without additional specifications, we use temperature of 0.6, top-p of 0.95, and the maximum token length is set to 8192. We set α as 1.4, and we obtain the average thinking phase token length generated by an LRM on any benchmark by randomly sampling 10 test questions and averaging the generated token length before benchmarking. See more details in Section A. 4 Table 1: Systematic comparison of reasoning results on mathematical, coding, and science reasoning benchmarks with DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and Qwen QwQ 32B. P@1: Pass@1 (%); #Tk: number of generated tokens; P@1 (%): average Pass@1 result boost over the base model. For fair comparison, S1 (Muennighoff et al., 2025) directly applies budget forcing at test-time without supervised fine-tuning, which is same as CoD and our α1 that are training-free. Method AIME24 AMC Minerva MATH500 LiveCode Olympiad MATHEMATICAL CODING SCIENCE P@1 #Tk P@1 #Tk P@ #Tk P@1 #Tk P@1 #Tk P@ #Tk P@1 BASE s1 COD 26.7+3.4 30.0+6.7 α1 (Ours) 30.0+6.7 BASE s1 COD 46.7+0.0 43.3-3.4 α1 (Ours) 50.0+3.3 DeepSeek-R1-Distill-Qwen-1.5B 23.3 7280 57.5 5339 32.0 4935 79. 3773 17.8 6990 38.8 5999 N/A 6418 31.6-0.4 5826 78.2-1.0 4733 17.0-0.8 7025 38.5-0.3 6673 +0.15 7798 57.5+0.0 5415 29.0-3.0 4005 81.4+2.2 3136 20.3+2.5 6657 40.6+1.8 5651 +2.95 6994 65.0+7.5 5916 70.0+12.5 4952 34.2+2.2 4586 81.0+1.8 3852 24.8+7.0 5426 45.5+6.7 4944 +6.15 DeepSeek-R1-Distill-Qwen-7B 46. 6648 82.5 4624 40.4 4191 87.6 3239 43.5 5885 50.4 5385 N/A 7295 80.0-2.5 6078 87.5+5.0 6827 90.0+7.5 5673 42.3+1.9 6510 92.8+5.2 5848 44.0+0.5 5979 54.2+3.8 6007 +1.48 3594 43.4+3.0 2142 88.8+1.2 2094 45.0+1.5 5593 53.5+3.1 4520 +1.73 4397 42.3+1.9 4124 91.2+3.6 4337 49.8+6.3 5067 55.7+5.3 4883 +4.65 Qwen QwQ-32B BASE s1 40.0 4058 77. 2901 47.8 2199 90.2 1951 67.0 5092 53.6 3230 N/A COD 3068 46.7-1.1 2433 90.8+0.6 2218 66.5-0.5 5260 55.1+1.5 3454 +0.63 2400 47.4-0.4 1464 90.6+0.4 1421 66.8-0.2 4984 57.2+3.6 2844 +2.10 α1 (Ours) 53.3+13.3 3141 87.5+10.0 2286 46.0-1.8 1441 89.4-0.8 1668 75.8+8.8 5824 56.1+2.5 2504 +5.33 4221 77.5+0.0 3959 80.0+2.5 43.3+3.3 46.7+6.7 Baselines We compare our α1 against the vanilla LRM and two training-free, test-time scaling baselines. i) BASE: The original LRM that transitions between slow and fast thinking automatically, without any external modulation. ii) S1 (Muennighoff et al., 2025): baseline that enforces monotonically increasing slow thinking pattern by appending approximately two wait tokens near the end of the reasoning phase to prolong slow thinking. For fair comparison, we apply S1 at test time without supervised fine-tuning used in its original implementation. iii) CHAIN OF DRAFT (COD) (Xu et al., 2025): baseline that enforces monotonically decreasing slow thinking pattern by prompting the model to constrain each slow thinking step to no more than five words, thereby sharply reducing the thinking budget."
        },
        {
            "title": "4.2 Main Results",
            "content": "Table 1 shows the systematic comparison results of our α1 and baseline methods, and we observe: i) α1 consistently yields higher problem-solving accuracy than all baseline methods across all models and benchmarks. Notably, compared to the base model, α1 improves the 1.5B LRM by clear margin of +6.15%, while reducing nearly 14% token length. This demonstrates both the effectiveness and efficiency of α1. ii) Compared to baseline test-time scaling methods, including s1 and CoD, α1 still achieves significantly better results. Specifically, the average accuracy boost over all benchmarks and models of α1 is +3.12% and +4.62% higher than CoD and s1, respectively. iii) Surprisingly, we observe that while α1 modulates reasoning densely without restrictions on reducing the thinking budget (instead, we use α > 1 that increases the thinking budget), the average thinking phase token length generated by α1 is only about +4.4% higher than the monotonically decreasing baseline CoD (4231 vs. 4053), which is about +21.0% more efficient than the monotonically increasing baseline s1 (4231 vs. 5357). This indicates that α1 achieves more efficient reasoning than baselines, which we provide analysis later. 5 Figure 3: Visualization of different scheduling strategies. We detail the functions in Section 4.3.1. Here represents α moment, which we elaborate in Section 3.1, and denotes the end of the thinking phase. Figure 4: Ablation study of different scheduling strategies on (a-b) AMC23 and (c-d) OlympaidBench."
        },
        {
            "title": "4.3 Analytic Results",
            "content": "In this section, we analyze α1 by systematically addressing the following five questions:"
        },
        {
            "title": "4.3.1 What scheduling strategy is better?\nAs shown in Fig. 3, we study four variants of\nscheduling strategies for S(t) defined in Eq. (1),\nwhere Tm = αN think represents the timestamp of\nα moment:",
            "content": "Constant: S(t) := pconstant, where pconstant [0, 1] is constant probability. This represents consistently more slow thinking strategy, and the increase is large when pconstant is larger. Note that when pconstant = 0 and α = 1, it degenerates to vanilla reasoning models; and when pconstant < 0.1 and α > 1, it degenerates to s1-like model, where only about two wait are appended. Linear increase: S(t) := 1 Tm t, where = {0, 1, . . . , Tm} and 1 > 0 indicates the inTm creasing coefficient. This scheduling function indicates fast-to-slow thinking strategy. Exponential anneal: S(t) := exp(γt), where = {0, 1, . . . , Tm} and γ > 0 is hyperparameter that controls annealing speed (here we use γ = 0.3). This scheduling function indicates slow-to-fast thinking strategy. Linear anneal: S(t) := 1 Tm + 1, where 1 < 0 indicates the annealing coefficient. Tm Its modulation is similar to exponential anneal scheduling. Fig. 4 shows the results of α1 using these four different scheduling strategies. We observe: i) Linear anneal consistently yields the highest reasoning accuracy, indicating that the slow thinking first, then fast thinking is better slow thinking scheduling strategy. ii) Similar to linear anneal, exponential anneal also follows an annealing slow thinking scheduling, where the improvement on the 1.5B model further demonstrates the efficacy of the slow thinking, then fast thinking strategy. However, such annealing scheduling may lead to an unstable performance boost compared to linear anneal."
        },
        {
            "title": "4.3.2 Can α-moment scale the thinking phase",
            "content": "budget? Fig. 5 shows the results of α1 with different αmoments determined by scaling α from 0 to maximum value subject to the 8192 token length budget. We observe: i) α-moment enables scalable thinking phase budgeting. By scaling up α, the average thinking phase token length is accordingly scaled up. ii) Interestingly, while the thinking phase is scaled up, there exists trade-off between the optimal value of α and the resulting reasoning accu6 Figure 5: Scaling property of α. We scale α from 0 to the maximum value restricted by the maximum token length, and plot the corresponding reasoning Pass@1 and average thinking phase token length on AMC23 and MATH500. phase token length, computed by dividing the current thinking phase token length by the maximum token length. Higher REP indicates stronger performance with better reasoning efficiency. We report the REP of CoD, s1, and α1 on six reasoning benchmarks with Deepseek-R1-distillQwen-1.5B. Fig. 6 shows that α1 achieves higher REP on most benchmarks, indicating more favorable balance between reasoning performance and efficiency. Notably, α1 outperforms CoD by +6.62 and s1 by +11.68 on Olympiad-Bench, and exceeds CoD by +14.22 on Minerva-Math."
        },
        {
            "title": "4.3.4 How frequent should slow thinking",
            "content": "transitioning be? α1 modulate slow thinking transitioning via sampling from Bernoulli(pwait), which leads to another question of how large should pwait be that can bring better result. To study this question, we use the constant scheduling function and scale pconstant from 0 to 1 to increase the frequency of transitioning to slow thinking. This is because the constant scheduling is sampling process with certain probability, and the value of the probability determines how frequently the slow thinking transitioning token will be sampled. Fig. 7 shows the results, from which we observe: i) An extremely low or high frequency of transitioning to slow thinking brings unsatisfactory results (e.g., pconstant = 0.1). Similar to the scaling of the thinking phase dedget (e.g., modualting α), the slow thinking frequency also needs to be carefully selected. ii) While an extremely dense or sparse slow thinking transitioning leads to unsatisfactory results, the reasoning performance is decent across large range of pconstant, demonstrating that increasing slow thinking generally brings improved reasoning. 7 Figure 6: Scaling efficiency analysis with REP using Deepseek-R1-distill-Qwen-1.5B. The REP metric is introduced in Eq. (2). racy. This indicates that monotonously increasing the thinking phase budget does not consistently bring better reasoning performance, and it is critical to find the optimal α-moment that results in satisfactory improvement."
        },
        {
            "title": "4.3.3 Does α1 scale more efficiently?\nTo quantitatively evaluate how different methods\ntrade off reasoning efficiency and accuracy, we in-\ntroduce the FREP(Amethod; Abase, Tnorm) (Reason-\ning Efficiency-Performance, REP) metric. The\nREP metric is defined as:",
            "content": "FREP(Amethod; Abase, Tnorm) = Amethod Abase Tnorm (2) where Amethod and Abase denote the reasoning accuracy of the evaluated method and the base model, respectively. Tnorm is the normalized thinking Figure 7: Scaling property of wait frequency under constant scheduling on AMC23 and OlympiadBench. Increasing pconstant leads to higher frequency of yielding wait in the Bernoulli process Bernoulli(pwait). Table 2: Ablation study on post-α moment modulation. Without post-α modulation represents our α1 without the suppression of the slow thinking inertia after the α moment. Method Post-α Moment Modulation AIME24 AMC23 P@1 #Tk P@1 #Tk DeepSeek-R1-Distill-Qwen-1.5B BASE α1 (Ours) α1 (Ours) N/A 23.3 26.7 30.0 7280 7929 5916 57.5 47.5 70. 5339 6903 4951 DeepSeek-R1-Distill-Qwen-7B BASE α1 (Ours) α1 (Ours) N/A 38.8 30.0 50.0 5999 7666 82.5 75.0 90.0 4624 5878 4397 4.3.5 Is post-α moment modulation necessary? Typical test-time scaling methods focus on the modulation of slow thinking within the thinking phase, while α1 consists of post-α moment modulation that encourages fast thinking. To validate its necessity of enforcing fast thinking in the end, we conduct an ablation study on utilizing the post-α moment modulation, shown in Table 2. We observe: i) Pre-α moment modulation of slow thinking is insufficient. When the post-α moment modulation is reduced to single operation, the performance of α1 significantly drops. This is because the increase of slow thinking during pre-α moment brings slow thinking inertia (as discussed before in Section 3.3), leading to slow thinking intensive reasoning. ii) By utilizing post-α moment modulation, α1 successfully ends in fast thinking, which demonstrates the necessity of combining both slow thinking and fast thinking."
        },
        {
            "title": "5 Related Works",
            "content": "5.1 Large Reasoning Models Large Reasoning Models are rapidly emerging as family of foundation models (Bommasani et al., 2021) that target human-level system-2 reasoning (Kahneman, 2011). Starting from OpenAIs o1 (Jaech et al., 2024) in 2024, numerous efforts follow this thinking-then-answering paradigm. Notably, o1-like Large Language Models (LLMs) can solve increasingly complex reasoning problems after thorough chain of thoughts (Wei et al., 2022; Yao et al., 2023; Besta et al., 2024), such as the IMO competition. These advanced models are mainly developed via large-scale reinforcement learning (RL) to align human preference (Christiano et al., 2017; Schulman et al., 2017; Shao et al., 2024b; DeepSeek-AI et al., 2025), where reward model is used to judge model answers (Uesato et al., 2022; Lightman et al., 2024). Notable efforts replicating o1s success include DeepSeek R1, Qwen QwQ, and Phi-4 (Abdin et al., 2024; DeepSeek-AI et al., 2025; Qwen Team, 2025), which typically utilize special end-of-thinking token </think>, after which solution is output to the user. Recently, some researchers have explored applying RL during post-training fine-tuning, where promising results have been obtained (Chow et al., 2025; Qu et al., 2025; Zuo et al., 2025)."
        },
        {
            "title": "5.2 Reasoning with Test-Time Scaling",
            "content": "Reasoning with test-time scaling has recently become useful strategy that empowers LLMs with scalable reasoning capability at test time. The mainstream scaling methods lie in two categories, i.e., i) parallel scaling and ii) sequential scaling. The key idea of parallel scaling is Best-of-N (BoN) 8 sampling, where the best choice is selected using uncertainty criteria like self-consistency (Wang et al., 2023), reward model (Lightman et al., 2024; Cobbe et al., 2021), or perplexity (Fang et al., 2025). Specifically, one line of work focuses on sequence-level sampling (Cobbe et al., 2021; Gui et al., 2024; Wan et al., 2024a; Sun et al., 2024; Chow et al., 2025; Sessa et al., 2025; Amini et al., 2025; Zhou et al., 2025; Zeng et al., 2025; Kang et al., 2025), while another line of work utilizes token-/steplevel sampling including beam-/treebased searching (Kool et al., 2019; Xie et al., 2023; Zhang et al., 2023; Hao et al., 2023; Qiu et al., 2024; Gao et al., 2024; Yu et al., 2024; Wan et al., 2024b; Chen et al., 2024a). Meanwhile, sequential scaling enhances or reduces slow thinking over single answer generation process. This technique typically relies on an iterative refinement and revision of answers generated by LLMs themselves (Zelikman et al., 2022; Madaan et al., 2023) or external feedback (Chen et al., 2024c; Gou et al., 2024; Huang et al., 2024a; Kamoi et al., 2024; Zheng et al., 2024). Following this line of research, recent works have been devoted to addressing the underthinking and overthinking issues of modern LRMs via reinforcing (Muennighoff et al., 2025) and restricting (Xu et al., 2025) slow thinking, respectively. Given the non-conflict between parallel scaling and sequential scaling, there exists another group of hybrid scaling methods that leverage both strategies (Li et al., 2025; Zeng et al., 2025)."
        },
        {
            "title": "Limitations and Broader Impact",
            "content": "Limitations While ALPHAONE provides universal view of test-time scaling of LRMs, and significant performance boost has been achieved, we identify some possible limitations as follows. i) ALPHAONE targets at o1-style LRMs, where tokens such as wait is proved effective in transitioning into slow thinking. However, future LRMs may use different slow thinking transitioning strategy, leading to possibility of incompatibility with our framework. ii) ALPHAONE relies on α-moment throughout reasoning modulation, and the average thinking phase token length is typically required. This paper obtains it by first running LRMs on 10 random samples, which requires marginal cost. However, in case that no test questions are available, ALPHAONE can only rely on an empirical thinking phase length that may be suboptimal. Broader Impact This work targets complex reasoning problems with LRMs, which we believe will lead to no ethical concerns. However, since LRMs are modern variants of LLMs, any ethical concerns raised by LLMs can potentially exist."
        },
        {
            "title": "Acknowledgments",
            "content": "Huan Zhang is partially funded by the AI2050 program at Schmidt Sciences (AI2050 Early Career Fellowship). The authors thank Heng Dong for his valuable suggestions on this project."
        },
        {
            "title": "References",
            "content": "In this paper, we study the problem of test-time scaling of large reasoning models with our framework, ALPHAONE (α1). ALPHAONE starts from universal view of reasoning modulation targeting two key aspects: thinking phase budgeting and slow thinking scheduling. We introduce α-moment, which is determined by α that scales the thinking phase budget by at least α. ALPHAONE operates by scheduling slow thinking before the α-moment, and fast thinking after the α-moment that eliminates slow thinking inertia. Using ALPHAONE, we investigate the test-time scaling from various aspects, including the overall slow and fast thinking transitioning plan, thinking phase budget scaling property, and efficiency of test-time scaling, etc. Insightful findings are obtained, e.g., slow thinking first, then fast thinking leads to better reasoning capability of LRMs. Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, and 8 others. 2024. Phi-4 technical report. CoRR, abs/2412.08905. AI-MO. 2024. AIMO Validation Dataset - AMC. https://huggingface.co/datasets/AI-MO/ aimo-validation-amc. Accessed: 2025-05-19. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, and 8 others. 2022. Flamingo: visual language model for few-shot learning. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing 9 Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Afra Amini, Tim Vieira, Elliott Ash, and Ryan Cotterell. 2025. Variational best-of-n alignment. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2024. Graph of thoughts: Solving elaborate problems with large language models. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 1768217690. AAAI Press. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, and 34 others. 2021. On the opportunities and risks of foundation models. CoRR, abs/2108.07258. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024a. Alphamath almost zero: Process supervision without process. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2024b. Do NOT think that much for 2+3=? on the overthinking of o1-like llms. CoRR, abs/2412.21187. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2024c. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Yinlam Chow, Guy Tennenholtz, Izzeddin Gur, Vincent Zhuang, Bo Dai, Aviral Kumar, Rishabh Agarwal, Sridhar Thiagarajan, Craig Boutilier, and Aleksandra Faust. 2025. Inference-aware fine-tuning for best-ofn sampling in large language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 42994307. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 81 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948. Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. 2024. DreamLLM: Synergistic multimodal comprehension and creation. In The Twelfth International Conference on Learning Representations. Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang. 2025. What is wrong with perplexity for long-context language modeling? In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Li Fei-Fei. 2023. The Worlds See: Curiosity, Exploration, and Discovery at the Dawn of AI. Flatiron books: moment of lift book. Yichao Fu, Junda Chen, Yonghao Zhuang, Zheyu Fu, Ion Stoica, and Hao Zhang. 2025. Reasoning without self-doubt: More efficient chain-of-thought through certainty probing. In ICLR 2025 Workshop on Foundation Models in the Wild. Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen. 2024. Interpretable contrastive monte carlo tree search reasoning. CoRR, abs/2410.01707. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2024. CRITIC: large language models can self-correct with In The Twelfth Intertool-interactive critiquing. national Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Lin Gui, Cristina Garbacea, and Victor Veitch. 2024. Bonbon alignment for large language models and the sweetness of best-of-n sampling. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. 10 Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 81548173. Association for Computational Linguistics. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024. Training large language models to reason in continuous latent space. CoRR, abs/2412.06769. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. 2024. Olympiadbench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 38283850. Association for Computational Linguistics. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2024a. Large language In The models cannot self-correct reasoning yet. Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. 2024b. O1 replication journey - part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? CoRR, abs/2411.16489. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, and 80 others. 2024. Openai o1 system card. CoRR, abs/2412.16720. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2025. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. 2025. T2i-r1: Reinforcing image generation with collaborative semantic-level and tokenlevel cot. arXiv preprint arXiv:2505.00703. Daniel Kahneman. 2011. Thinking, fast and slow. macmillan. Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. 2024. When can llms Actually correct their own mistakes? critical survey of selfcorrection of llms. Trans. Assoc. Comput. Linguistics, 12:14171440. Zhewei Kang, Xuandong Zhao, and Dawn Song. 2025. Scalable best-of-n selection for large language models via self-certainty. CoRR, abs/2502.18581. Wouter Kool, Herke van Hoof, and Max Welling. 2019. Stochastic beams and where to find them: The gumbel-top-k trick for sampling sequences without In Proceedings of the 36th Internareplacement. tional Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 34993508. PMLR. Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo. 2024. Volcano: Mitigating multimodal hallucination through self-feedback guided revision. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 391404. Association for Computational Linguistics. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Dacheng Li, Shiyi Cao, Chengkun Cao, Xiuyu Li, Shangyin Tan, Kurt Keutzer, Jiarong Xing, Joseph E. Gonzalez, and Ion Stoica. 2025. S*: Test time scaling for code generation. CoRR, abs/2502.14382. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. 2025. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858. 11 Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Mathematical Association of America. 2024. American Invitational Mathematics Examination AIME. American Invitational Mathematics Examination AIME 2024. Accessed: 2025-05-15. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel J. Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. CoRR, abs/2501.19393. OpenAI. 2024. Introducing gpt-4o and more tools to chatgpt free users. OpenAI. 2025. Thinking with images. Accessed: 202505-25. Xiao Pu, Michael Saxon, Wenyue Hua, and William Yang Wang. 2025. Thoughtterminator: Benchmarking, calibrating, and mitigating overarXiv preprint thinking in reasoning models. arXiv:2504.13367. Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, and Kaisheng Ma. 2024. Shapellm: Universal 3d object understanding for embodied interaction. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XLIII, volume 15101 of Lecture Notes in Computer Science, pages 214238. Springer. Zekun Qi, Wenyao Zhang, Yufei Ding, Runpei Dong, Xinqiang Yu, Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, Jiazhao Zhang, Jiawei He, Jiayuan Gu, Xin Jin, Kaisheng Ma, Zhizheng Zhang, He Wang, and Li Yi. 2025. Sofar: Languagegrounded orientation bridges spatial reasoning and object manipulation. CoRR, abs/2502.13143. Jiahao Qiu, Yifu Lu, Yifan Zeng, Jiacheng Guo, Jiayi Geng, Huazheng Wang, Kaixuan Huang, Yue Wu, and Mengdi Wang. 2024. Treebon: Enhancing inference-time alignment with speculative tree-search and best-of-n sampling. CoRR, abs/2410.16033. Yuxiao Qu, Matthew Y. R. Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, and Aviral Kumar. 2025. Optimizing test-time compute via meta reinforcement fine-tuning. CoRR, abs/2503.07572. Qwen Team. 2025. Preview of qwen qwen1.5https://qwenlm.github.io/blog/ 32b. qwq-32b-preview/. Accessed: 2025-03-20. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. CoRR, abs/1707.06347. Pier Giuseppe Sessa, Robert Dadashi-Tazehozi, Léonard Hussenot, Johan Ferret, Nino Vieillard, Alexandre Ramé, Bobak Shahriari, Sarah Perrin, Abram L. Friesen, Geoffrey Cideron, Sertan Girgin, Piotr Stanczyk, Andrea Michi, Danila Sinopalnikov, Sabela Ramos Garea, Amélie Héliou, Aliaksei Severyn, Matthew Hoffman, Nikola Momchev, and Olivier Bachem. 2025. BOND: aligning llms with best-of-n distillation. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. 2024a. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024b. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300. Jinyan Su, Jennifer Healey, Preslav Nakov, and Claire Cardie. 2025. Between underthinking and overthinking: An empirical study of reasoning length and correctness in llms. arXiv preprint arXiv:2505.00127. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Ben Hu. 2025. Stop overthinking: survey on efficient reasoning for large language models. CoRR, abs/2503.16419. Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. 2024. Fast best-of-n decoding via speculative rejection. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Chameleon Team. 2024. modal early-fusion foundation models. abs/2405.09818. Chameleon: MixedCoRR, Jonathan Uesato, Nate Kushman, Ramana Kumar, H. Francis Song, Noah Y. Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with processand outcome-based feedback. CoRR, abs/2211.14275. Guangya Wan, Yuqi Wu, Jie Chen, and Sheng Li. 2024a. Dynamic self-consistency: Leveraging reasoning paths for efficient LLM sampling. CoRR, abs/2408.17017. Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. 12 2024b. Alphazero-like tree-search can guide large In Fortylanguage model decoding and training. first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Han Wang, Gang Wang, and Huan Zhang. 2024. Steering away from harm: An adaptive approach to defending vision language model against jailbreaks. CoRR, abs/2411.16721. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2025. Thoughts are all over the place: On the underthinking of o1-like llms. CoRR, abs/2501.18585. Haoran Wei, Youyang Yin, Yumeng Li, Jia Wang, Liang Zhao, Jianjian Sun, Zheng Ge, and Xiangyu Zhang. 2024. Slow perception: Lets perceive geometric figures step-by-step. CoRR, abs/2412.20631. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Yana Wei, Liang Zhao, Kangheng Lin, En Yu, Yuang Peng, Runpei Dong, Jianjian Sun, Haoran Wei, Zheng Ge, Xiangyu Zhang, and Vishal M. Patel. 2025. Perception in reflection. CoRR, abs/2504.07165. Penghao Wu and Saining Xie. 2024. V*: Guided visual search as core mechanism in multimodal llms. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1308413094. IEEE. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Qizhe Xie. 2023. Self-evaluation guided beam search for reasoning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. 2024. Llava-critic: Learning to evaluate multimodal models. CoRR, abs/2410.02712. Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. 2025. Chain of draft: Thinking faster by writing less. CoRR, abs/2502.18600. Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Zheng Lin, Li Cao, and Weiping Wang. 2025a. Dynamic early exit in reasoning models. arXiv preprint arXiv:2504.15895. Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, and Tong Zhang. 2025b. Embodiedbench: Comprehensive benchmarking multi-modal large language models for visiondriven embodied agents. CoRR, abs/2502.09560. Wang Yang, Xiang Yue, Vipin Chaudhary, and Xiaotian Han. 2025c. Speculative thinking: Enhancing small-model reasoning with large model guidance at inference time. arXiv preprint arXiv:2504.12329. Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Towards thinking-optimal scaling Wei. 2025d. of test-time compute for LLM reasoning. CoRR, abs/2502.18080. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Jingyu Wang, and Wenbing Tao. 2025. Perceptionr1: Pioneering perception policy with reinforcement learning. CoRR, abs/2504.07954. Fei Yu, Anningzhe Gao, and Benyou Wang. 2024. Ovm, outcome-supervised value models for planning in mathematical reasoning. In Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 858 875. Association for Computational Linguistics. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, and Xipeng Qiu. 2025. Revisiting the test-time scaling of o1-like models: Do they truly possess testtime scaling capabilities? CoRR, abs/2502.12215. Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan. 2023. Planning with large language models for code generation. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. 13 Jingnan Zheng, Han Wang, An Zhang, Tai D. Nguyen, Jun Sun, and Tat-Seng Chua. 2024. Ali-agent: Assessing llms alignment with human values via agentbased evaluation. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Zhi Zhou, Tan Yuhao, Zenan Li, Yuan Yao, LanZhe Guo, Xiaoxing Ma, and Yu-Feng Li. 2025. Bridging internal probability and self-consistency for effective and efficient LLM reasoning. CoRR, abs/2502.00511. Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. 2025. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, and Bowen Zhou. 2025. Ttrl: Test-time reinforcement learning. CoRR, abs/2504.16084."
        },
        {
            "title": "APPENDIX",
            "content": "A Additional Implementation Details A.1 Computaional Budget . A.2 Hyper-parameters & Parameters A.3 Benchmarks . . . . . . . . . . . . . . . . . . . 15 15 15 15 Additional Ablation Study 16 B.1 Scheduling Strategy . . 16 16 B.2 Scaling Efficiency Analysis . B.3 Slow Thinking Transitioning Tokens 16 . . . . . . . . . Artifacts Statements C.1 Model Artifacts . . C.2 Data Artifacts . . . . . . . . . . . . . . . . . . . Future Works"
        },
        {
            "title": "E Qualitative Examples",
            "content": "16 16 16"
        },
        {
            "title": "A Additional Implementation Details",
            "content": "A.1 Computaional Budget We used 8 NVIDIA L40S GPUs and 4 NVIDIA A100 80GB GPUs for the experiments. A.2 Hyper-parameters & Parameters For reproducibility, we provide the complete set of average thinking phase token length think in Table 3, which are obtained by randomly sampling 10 test questions on each benchmark and averaging the generated token lengths. Since the effective range of α observed in Figure 5 is relatively broad, practical implementations can tolerate variance in this measurement. A.3 Benchmarks AIME 2024 The AIME 2024 dataset is specialized benchmark collection consisting of 30 problems from the 2024 American Invitational Mathematics Examination (Mathematical Association of America, 2024). These problems cover core secondary-school mathematics topics such as arithmetic, combinatorics, algebra, geometry, number theory and probability. The collection places rigorous demands on both solution accuracy and conceptual depth. 15 AMC 2023 The AMC 2023 dataset consists of 40 problems selected from the AMC 12A and 12B contests. These exams are sponsored by the Mathematical Association of America and target U.S. students in grade 12 and below, featuring challenges in algebra, geometry, number theory, and combinatorics (AI-MO, 2024). Minerva Math Minerva Math (Lewkowycz et al., 2022) consists of 272 undergraduate-level STEM problems harvested from MITs OpenCourseWare. These problems span solid-state chemistry, information and entropy, differential equations, and special relativity. Each includes clearly delineated answer191 verifiable by numeric checks and 81 by symbolic solutions. The benchmark is specifically designed to evaluate multi-step scientific reasoning capabilities in language models. MATH500 MATH500 comprises selection of 500 problems extracted from the MATH benchmark (Lightman et al., 2024). The collection covers range of high-school mathematics domains, including Prealgebra, Algebra and Number Theory. To ensure comparability with prior work, we use the exact problem set originally curated by OpenAI for evaluation. LiveCodeBench LiveCodeBench (Jain et al., 2025) is contamination-free benchmark for evaluating large language models on code. The suite is continuously updated, gathering new problems over time. It currently comprises 400 Python programming tasks released between May 2023 and March 2024, each paired with test samples for correctness verification. Beyond basic code generation, LiveCodeBench also measures advanced capabilities such as self-repair, code execution and test-output prediction. OlympiadBench OlympiadBench (He et al., 2024) consists of 8,476 Olympiad-level problems that evaluate mathematical and physical reasoning in AI systems. It features wide difficulty range, open-ended problem generation, expert solution annotations, detailed difficulty labels, and multilingual coverage. The subset we use in our paper contains 675 open-ended, text-only math competition problems in English. Table 3: Average thinking phase token length think across different benchmarks. The results are obtained by running LRMs on randomly sampled 10 samples. Model AIME24 AMC23 Minerva MATH500 LiveCodeBench OlympiadBench DeepSeek-R1-Distill-Qwen-1.5B DeepSeek-R1-Distill-Qwen-7B Qwen QwQ-32B 4130 4751 2597 3303 3243 3101 3064 1710 2435 2352 1493 2172 3120 4915 3417 3330 2052 Figure 8: Ablation study of different scheduling strategies on AIME24."
        },
        {
            "title": "B Additional Ablation Study",
            "content": "B.1 Scheduling Strategy In addition to the results in Fig. 4 tested on AMC23 and Olympiad, we also show the results tested on AIME24 in Fig. 8. From the results, we observe that the linear increase consistently yields the best performance, which aligns with our previous observation. This further provides evidence that slow-then-fast thinking is an efficient slow-thinking scheduling strategy. B.2 Scaling Efficiency Analysis As shown in Fig. 9, α1 consistently achieves positive REP with Deepseek-R1-distill-Qwen-7B, demonstrating stable gains over the base model. Similar to Fig. 6, it outperforms CoD and s1 across nearly all benchmarks, particularly on LiveCodeBench and AIME24. Figure 9: Scaling efficiency analysis with REP using Deepseek-R1-distill-Qwen-7B."
        },
        {
            "title": "C Artifacts Statements",
            "content": "C.1 Model Artifacts We utilize three models in our work: DeepSeekR1-Distill-Qwen-1.5B and DeepSeek-R1-DistillQwen-7B, both released under the MIT License, which permits commercial use, modification, and redistribution. These models are distilled from Qwen-2.5 series (Apache 2.0 License). Additionally, we use Qwen QwQ-32B, which is released under the Apache License 2.0, allowing both research and commercial usage. We comply with all respective license terms in our use of these models. B.3 Slow Thinking Transitioning Tokens C.2 Data Artifacts We provide an ablation study on different slowthinking transitioning tokens on the AIME2024 dataset. As illustrated in Table 4, the empirical results show that using Wait, can help the model excel in both performance and efficiency. Other reasoning transition tokens like Hmm, and Alternatively, do not achieve comparable results, likely because they introduce less effective cues for reasoning modulation. We employ publicly available datasets in our experiments. AIME24, Minerva-Math, LiveCodeBench, and OlympiadBench are released under the MIT License, which permits unrestricted use, modification, and redistribution. The AMC23 dataset does not have an explicitly specified license, so we treat it as having an unspecified license and exercise caution in its usage. We ensure full compliance with the respective license terms of all datasets used. 16 Table 4: Ablation study on different slow thinking transitioning tokens on AIME24 (8192). Transitioning Token Deepseek-R1-1.5B Deepseek-R1-7B Wait, Hmm, Alternatively, P@ 30.0 20.0 16.7 #Tk 5916 6595 6713 P@1 #Tk 50.0 46.7 43. 6827"
        },
        {
            "title": "E Qualitative Examples",
            "content": "We present qualitative examples from different models and benchmarks to illustrate both cases: instances where α1 helps the model answer correctly, and instances where it still fails to produce the correct answer. Examples show that by appending wait frequently after nn can slow down the thinking process and may help the model achieve better performance. The examples can be found in the following pages."
        },
        {
            "title": "D Future Works",
            "content": "While our α1 has been demonstrated successful and effective in scaling LRMs at test time, there are some intriguing future works that we are considering: More sophisticated slow thinking scheduling. This work focuses on simple strategies like the slow-to-fast schedule, which shows strong performance. However, optimal scheduling remains an open question, as human reasoning patterns are complex and not yet fully understood (Kahneman, 2011). Promising directions include modulating reasoning progress during both training and inference, or learning separate progress modulation model aligned with human preferencesakin to progress reward model (Uesato et al., 2022; Lightman et al., 2024). Transitioning-token-agnostic modulation. As shown in Table 4, the choice of transitioning token (e.g., wait) affects performance due to model-specific training data. This limitation is shared by many test-time scaling methods relying on open-source LRMs like DeepSeekR1 (DeepSeek-AI et al., 2025), in contrast to restricted-access models like OpenAI o1. While α1 supports flexible token choices, removing the dependency on transitioning tokens altogether could further enhance generalization. Multimodal reasoning with multimodal LLMs. Multimodal LLMs are rapidly advancing and show growing potential in reasoning tasks (Alayrac et al., 2022; Liu et al., 2023; OpenAI, 2024; Team, 2024; Dong et al., 2024; Qi et al., 2024; Zou et al., 2025; Wang et al., 2024). Although they currently trail behind text-only LRMs, efforts to enhance their reasoning abilities are gaining momentum (Hao et al., 2024; Xiong et al., 2024; Wei et al., 2024; Wu and Xie, 2024; Shao et al., 2024a; Lee et al., 2024; Wei et al., 2025; Jiang et al., 2025; Yu et al., 2025). Multimodal reasoning remains challenging, as it requires both image understanding and generation (OpenAI, 2025). We aim to extend our α1 framework to this domain, fostering synergistic multimodal comprehension and creation (Dong et al., 2024). Another promising direction is embodied reasoninggrounding multimodal understanding in real-world interactions with spatial intelligence (Fei-Fei, 2023; Qi et al., 2025; Yang et al., 2025b). 17 Successful Example of DeepSeek-R1-Distill-Qwen-1.5B on OlympiadBench: Base fails while α1 passes chemist has three bottles, each containing mixture of acid Question: and water: - bottle contains 40g of which 10 is acid, - bottle contains 50g of which 20 is acid, and, - bottle contains 50g of which 30 is acid. She uses some of the mixture from each of the bottles to create mixture with mass 60g of which 25 is acid. Then she mixes the remaining contents of the bottles to create new mixture. What percentage of the new mixture is acid? Response from Base: <think> Okay, so Ive got this chemistry problem here, and need to figure out the percentage of acid in the new mixture. Let me try to understand the problem step by step. First, there are three bottles, each with mixture of acid and water. The masses and acid concentrations are given as follows: - Bottle A: 40 grams, 10% acid. - Bottle B: 50 grams, 20% acid. - Bottle C: 50 grams, 30% acid. The chemist is going to use some mixture from each bottle to create new mixture thats 60 grams with 25% acid. Then, after using some from each bottle, she mixes the remaining contents to create new mixture. ... </think> The chemist has three bottles containing acid and water with the following compositions: - Bottle A: 40 grams of solution, 10% acid... The percentage of the new mixture is 18.75 ). Response from α1: <think> Okay, so Ive got this chemistry problem here, and need to figure out the percentage of acid in the new mixture. Let me try to understand the problem step by step. Wait, actually, the problem says that the chemist uses some mixture from each bottle to create 60g mixture with 25% acid. Then, she mixes the remaining contents of the bottles to create new mixture. Hmm, need to clarify if she uses some from each bottle to make 60g, and then mixes the leftover from each bottle. Or is it that she uses some from each bottle to make 60g, and then uses the leftovers from each bottle to make another mixture? The wording is bit unclear. Wait, let me re-read the problem. Wait, actually, the problem says: ... </think> The percentage of acid in the new mixture is 17.5%. Step-by-Step Explanation: 1. Initial Acid Quantities: - Bottle A: 40g of 10% acid 4g acid. - Bottle B: 50g of 20% acid 10g acid... Answer: The new mixture has 17.5 % acid. Correct Answer: 17. 18 Failure Example of DeepSeek-R1-Distill-Qwen-1.5B on AMC23: α1 still fails Question: tan2023x = a1tanx+a3tan3x+a5tan5x++a2023tan2023x is a2023? 1+a2tan2x+a4tan4x++a2022tan2022x There is unique sequence of integers a1, a2, , a2023 such that whenever tan2023x is defined. What Response from α1: <think> Okay, so need to find the value of a2023 in the given expression for tan(2023x). The expression is fraction where the numerator is sum of odd-powered terms of (tan x) multiplied by coefficients a1, a3, . . . , a2023, and the denominator is sum of even-powered terms of (tan x) multiplied by coefficients a2, a4, . . . , a2022. The problem states that this sequence of integers is unique, so need to figure out what a2023 is. Wait, before jumping into that, should recall the tangent multiple-angle formulas. remember that tan(nx) can be expressed as ratio of polynomials in tan x, where the numerator is an odd-degree polynomial and the denominator is an even-degree polynomial ...</think> After carefully considering the expansion of tan(2023x) and testing various formulas ... which is the coefficient of tan2023x in the numerator, we look at the coefficient for ( = 2023 ) in the numerator sum: [a2023 = (1)20231(cid:0)2023 Therefore, a2023 = 1.nn[n 1 n] (cid:1) = (1)2022 1 = 1] 2023 Correct Answer: -1 19 Successful Example of DeepSeek-R1-Distill-Qwen-7B on Minerva Math: Base fails while α1 passes Question: particular star has an absolute magnitude = 7. If this star is observed in galaxy that is at distance of 3Mpc, what will its apparent magnitude be? Response from Base: <think> Okay, so need to find the apparent magnitude of star given its absolute magnitude and the distance to the galaxy its in. Hmm, lets see. remember that absolute magnitude is how bright star appears at standard distance, which think is 10 parsecs. Apparent magnitude, on the other hand, is how bright it appears from Earth, which depends on how far away it actually is. The formula that relates absolute magnitude (M), apparent magnitude (m) ...</think> To find the apparent magnitude of star given its absolute magnitude and the distance to the galaxy it is in, we use the distance modulus formula. The formula is:nn[ - = 5log(d) - 5 ]nnwhere (m) is the apparent magnitude...Rounding to one decimal place, the apparent magnitude is approximately 20.4.nn[n 20.4 n] Response from α1: <think> Okay, so need to find the apparent magnitude of star given its absolute magnitude and the distance to the galaxy its in. Hmm, remember that absolute magnitude is how bright star appears at standard distance, which is 10 parsecs, right? And apparent magnitude is how bright it appears from Earth, The formula that relates absolute which depends on its actual distance. magnitude (M), apparent magnitude (m), and distance (d) in parsecs is the distance modulus formula. think its something like = + 5 + 5 log10(d). Wait, let me make sure. Is it plus or minus 5? think its plus 5 because as the distance increases, the apparent magnitude increases, meaning it becomes dimmer. So, yes, = + 5 log10(d) + 5. Wait, hold on, no, thats not right...Then, subtract 5:nn[ 25.3855 - 5 = 20.3855 ] So, the apparent magnitude is approximately 20.3855, which rounds to 20.39.n nTherefore, the apparent magnitude of the star is 20.39 . Correct Answer: 20.39 Failure Example of DeepSeek-R1-Distill-Qwen-7B on LiveCodeBench: α1 still fails Question: You are given 0-indexed string and dictionary of words dictionary. You have to break into one or more non-overlapping substrings such that each substring is present in dictionary. There may be some extra characters in which are not present in any of the substrings. Return the minimum number of extra characters left over if you break up optimally. Example 1: Input: = \"leetscode\", dictionary = [\"leet\",\"code\",\"leetcode\"]. Output: 1 Explanation: We can break in two substrings: \"leet\" from index 0 to 3 and \"code\" from index 5 to 8. There is only 1 unused character (at index 4), so we return 1. Example 2: Input: = \"sayhelloworld\", dictionary = [\"hello\",\"world\"]. Output: 3 Explanation: We can break in two substrings: \"hello\" from index 3 to 7 and \"world\" from index 8 to 12. The characters at indices 0, 1, 2 are not used in any substring and thus are considered as extra characters. Hence, we return 3. Constraints:1 <= s.length <= 50, 1 <= dictionary.length <= 50, 1 <= dictionary[i].length <= 50, dictionary[i] and consists of only lowercase English letters, dictionary contains distinct words Response from α1: <think> Okay, need to solve this problem where have to find the minimum number of extra characters left when breaking string into substrings that are all present in given dictionary. Hmm, lets think about how to approach this. Wait, the problem says that the string must be broken into one or more non-overlapping substrings, each of which is in the dictionary. ... </think> To solve this problem, we need to break given string into substrings such that each substring is present in given dictionary. ... ### Solution Code pythonn nclass Solution:n def minExtraChar(self, s: str, dictionary: List[str]) -> int:n = len(s)n dp = [float(inf)] * (n + 1)n dp[0] = 0 # Base case: 0 characters, 0 extran for in range(1, + 1):n for in range(1, + 1):n start = - ln if start < 0:n continuen substring = s[start:i]n if substring in dictionary:n if dp[start] + (n - i) < dp[i]:n dp[i] = dp[start] + (n - i)nn return dp[n] ### Explanation - Initialization: The DP array is initialized to handle the base case where no characters are processed, resulting in zero extra characters. ... - Result Extraction: The final result is found in dp[n], which gives the minimum number of extra characters after processing the entire string. This approach ensures that we efficiently find the optimal way to split the string into valid substrings while minimizing the extra characters, leveraging dynamic programming to explore all possible valid splits. 21 Successful Example of QwQ-32B on MATH500: Base fails while α1 passes Question: There are 360 people in my school. 15 take calculus, physics, and chemistry, and 15 dont take any of them. 180 take calculus. Twice as many students take chemistry as take physics. 75 take both calculus and chemistry, and 75 take both physics and chemistry. Only 30 take both physics and calculus. How many students take physics? Response from Base: <think> Okay, lets try to tackle this problem step by step. Hmm, so there are 360 people in the school. Out of these, 15 dont take any of the three subjects: calculus, physics, or chemistry. That means 360 - 15 = 345 students take at least one of those subjects. Alright, let me note down all the given information: - Total students: 360 - Take all three (calculus, physics, chemistry): 15 - Dont take any: 15 - Take calculus: 180...</think> we need to determine the number of students taking Students not taking any subjects: 15, so students taking at least one Given the problem, physics. Here is the step-by-step reasoning:nn 1. Total students: 360 2. subject: (360 - 15 = 345) 3. Students taking calculus (C): 180 4. Students taking all three subjects (C, P, Ch): 15...Thus, the number of students taking physics is ( 115 ). Response from α1: <think> Okay, lets try to tackle this problem step by step. Hmm, so there are 360 people in the school. 15 take all three subjects: calculus, physics, and chemistry. Also, 15 dont take any of them. That means the number of students taking at least one of the subjects is 360 - 15 = 345. Wait, the total number in the union of calculus, physics, and chemistry is 345. Let me note down the given data first: - Total students: 360 - Take all three (Calculus, Physics, Chemistry): ...</think> So simplifying: 180 + + Ch - 180 +15 = + Ch +15 = 345 Hence, + Ch = 330. Since Ch = 2P, substituting: + 2P = 3P = 330 = 110. All steps check out, and the Venn diagram regions confirm consistency. Therefore, the number of students taking physics is 110 . Final Answern 110 . Correct Answer: 22 Failure Example of QwQ-32B on AIME24: α1 still fails Question: Torus is the surface produced by revolving circle with radius 3 around an axis in the plane of the circle that is distance 6 from the center of the circle (so like donut). Let be sphere with radius 11. When rests on the outside of S, it is externally tangent to along circle with radius ri, and when rests on the outside of S, it is externally tangent to along circle with radius ro. The difference ri ro can be written as , where and are relatively prime positive integers. Find + n. [asy] unitsize(0.3 inch); draw(ellipse((0,0), 3, 1.75)); draw((-1.2,0.1).(-0.8,-0.03)..(-0.4,-0.11)..(0,-0.15)..(0.4,-0.11).. (0.8,-0.03)..(1.2,0.1)); draw((-1,0.04)..(-0.5,0.12)..(0,0.16)..(0.5,0.12).. (1,0.04)); draw((0,2.4)(0,-0.15)); draw((0,-0.15)(0,-1.75), dashed); draw((0,-1.75)(0,-2.25)); draw(ellipse((2,0), 1, 0.9)); draw((2.03,-0.02)(2.9,-0.4)); [/asy] Response from α1: <think> Okay, so need to solve this problem about torus and sphere. Let me try to understand the setup first. The torus is formed by revolving circle with radius 3 around an axis in the same plane, which is 6 units away from the center of the circle. That makes senseits like donut shape where the tube radius is 3 and the distance from the center of the tube to the center of the torus is 6. Then theres sphere with radius 11. The problem says that when the torus rests on the outside of the sphere, it can be externally tangent to the sphere along circle with radius ri, and similarly along another circle with radius ro. The question is asking for the difference ri - ro, expressed as fraction m/n where and are coprime, and then + n. Wait, actually, the problem mentions...</think> Okay, lets start by setting up coordinate system where the sphere is centered at the origin ((0, 0, 0) with radius 11. The torus (T) is generated by revolving circle of radius 3 around an axis that is 6 units away from the center of the generating circle. Lets place the generating circle in the (x)-(y) plane, centered at ((6, 0, 0)). The axis of revolution is the (z)-axis, so the toruss axis is the (z)-axis. When the torus rests on the outside of the sphere, they are externally tangent along circle. To find the radii (ri) and (ro) of these circles of tangency, we need to consider the cross-sectional view in the (x)-(y) plane. Here, the sphere appears as circle of radius 11 centered at the origin, and the generating circle of the torus is circle of radius 3 centered at ((6, 0)). For the torus to be externally tangent to the sphere, the distance between their centers must equal the sum of their radii. However, since the sphere is much larger, the torus must be positioned such that the distance from the spheres center to the toruss generating circles center plus the spheres radius equals the distance from the spheres center to the farthest point on the torus. ... Correct Answer:"
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign"
    ]
}