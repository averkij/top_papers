{
    "paper_title": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model",
    "authors": [
        "Ling Team",
        "Anqi Shen",
        "Baihui Li",
        "Bin Hu",
        "Bin Jing",
        "Cai Chen",
        "Chao Huang",
        "Chao Zhang",
        "Chaokun Yang",
        "Cheng Lin",
        "Chengyao Wen",
        "Congqi Li",
        "Deng Zhao",
        "Dingbo Yuan",
        "Donghai You",
        "Fagui Mao",
        "Fanzhuang Meng",
        "Feng Xu",
        "Guojie Li",
        "Guowei Wang",
        "Hao Dai",
        "Haonan Zheng",
        "Hong Liu",
        "Jia Guo",
        "Jiaming Liu",
        "Jian Liu",
        "Jianhao Fu",
        "Jiannan Shi",
        "Jianwen Wang",
        "Jianxin Lai",
        "Jin Yang",
        "Jun Mei",
        "Jun Zhou",
        "Junbo Zhao",
        "Junping Zhao",
        "Kuan Xu",
        "Le Su",
        "Lei Chen",
        "Li Tang",
        "Liang Jiang",
        "Liangcheng Fu",
        "Lianhao Xu",
        "Linfeng Shi",
        "Lisha Liao",
        "Longfei Zheng",
        "Meng Li",
        "Mingchun Chen",
        "Qi Zuo",
        "Qiang Cheng",
        "Qianggang Cao",
        "Qitao Shi",
        "Quanrui Guo",
        "Senlin Zhu",
        "Shaofei Wang",
        "Shaomian Zheng",
        "Shuaicheng Li",
        "Shuwei Gu",
        "Siba Chen",
        "Tao Wu",
        "Tao Zhang",
        "Tianyu Zhang",
        "Tianyu Zhou",
        "Tiwei Bie",
        "Tongkai Yang",
        "Wang Hong",
        "Wang Ren",
        "Weihua Chen",
        "Wenbo Yu",
        "Wengang Zheng",
        "Xiangchun Wang",
        "Xiaodong Yan",
        "Xiaopei Wan",
        "Xin Zhao",
        "Xinyu Kong",
        "Xinyu Tang",
        "Xudong Han",
        "Xudong Wang",
        "Xuemin Yang",
        "Xueyu Hu",
        "Yalin Zhang",
        "Yan Sun",
        "Yicheng Shan",
        "Yilong Wang",
        "Yingying Xu",
        "Yongkang Liu",
        "Yongzhen Guo",
        "Yuanyuan Wang",
        "Yuchen Yan",
        "Yuefan Wang",
        "Yuhong Guo",
        "Zehuan Li",
        "Zhankai Xu",
        "Zhe Li",
        "Zhenduo Zhang",
        "Zhengke Gui",
        "Zhenxuan Pan",
        "Zhenyu Huang",
        "Zhenzhong Lan",
        "Zhiqiang Ding",
        "Zhiqiang Zhang",
        "Zhixun Li",
        "Zhizhen Liu",
        "Zihao Wang",
        "Zujie Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Ring-1T, the first open-source, state-of-the-art thinking model with a trillion-scale parameter. It features 1 trillion total parameters and activates approximately 50 billion per token. Training such models at a trillion-parameter scale introduces unprecedented challenges, including train-inference misalignment, inefficiencies in rollout processing, and bottlenecks in the RL system. To address these, we pioneer three interconnected innovations: (1) IcePop stabilizes RL training via token-level discrepancy masking and clipping, resolving instability from training-inference mismatches; (2) C3PO++ improves resource utilization for long rollouts under a token budget by dynamically partitioning them, thereby obtaining high time efficiency; and (3) ASystem, a high-performance RL framework designed to overcome the systemic bottlenecks that impede trillion-parameter model training. Ring-1T delivers breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a silver medal-level result on the IMO-2025, underscoring its exceptional reasoning capabilities. By releasing the complete 1T parameter MoE model to the community, we provide the research community with direct access to cutting-edge reasoning capabilities. This contribution marks a significant milestone in democratizing large-scale reasoning intelligence and establishes a new baseline for open-source model performance."
        },
        {
            "title": "Start",
            "content": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model Ling Team, Inclusion AI See Contributions section (Sec. 6) for full author list. We present Ring-1T, the first open-source, state-of-the-art thinking model with trillionscale parameter. It features 1 trillion total parameters and activates approximately 50 billion per token. Training such models at trillion-parameter scale introduces unprecedented challenges, including train-inference misalignment, inefficiencies in rollout processing, and bottlenecks in the RL system. To address these, we pioneer three interconnected innovations: (1) IcePop stabilizes RL training via token-level discrepancy masking and clipping, resolving instability from training-inference mismatches; (2) C3PO++ improves resource utilization for long rollouts under token budget by dynamically partitioning them, thereby obtaining high time efficiency; and (3) ASystem, high-performance RL framework designed to overcome the systemic bottlenecks that impede trillion-parameter model training. Ring-1T delivers breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains silver medal-level result on the IMO-2025, underscoring its exceptional reasoning capabilities. By releasing the complete 1T parameter MoE model to the community, we provide the research community with direct access to cutting-edge reasoning capabilities. This contribution marks significant milestone in democratizing large-scale reasoning intelligence and establishes new baseline for open-source model performance. Date: Oct 22, 2025 Code: https://github.com/inclusionAI/Ring-V2 Model: https://huggingface.co/inclusionAI/Ring-1T 5 2 0 2 1 ] . [ 1 5 5 8 8 1 . 0 1 5 2 : r Figure 1 Performance comparison of Ring-1T and existing open-weights and close-weights models across benchmarks."
        },
        {
            "title": "Introduction",
            "content": "Artificial intelligence is undergoing pivotal transition: Large Language Models (LLMs) are advancing beyond static corpora of human knowledge, becoming dynamic processors that transform information into actionable insights and understanding (Team et al., 2025; DeepSeek-AI, 2025). This progression towards more general intelligence is empirically validated by their core capability complex, adaptive problem-solving. Recent breakthroughs in solving high-difficulty human competition problems provide concrete evidence of significantly advanced reasoning abilities in large language models. For instance, models (OpenAI, 2025; Team, 2025) have achieved 100% accuracy on the AIME-2025 (AIME, 2025) and HMMT-2025 1, and reached medal-level performance at the International Mathematical Olympiad (IMO) (OpenAI, 2025)a hallmark of sophisticated human intellect. This evolution beyond static knowledge repositories is driven by training on trillions of tokens across diverse domains, coupled with reinforcement learning-optimized reasoning techniques (OpenAI, 2024; DeepSeek-AI, 2025) that enable models to dynamically scale their capabilities with thinking effort, pointing toward higher levels of general intelligence. While related work (DeepSeek-AI, 2025; Zhipu-AI, 2025) has made valuable contributions to the open-source community, the frontier of trillion-parameter thinking models remains uncharted territory. Scaling to this level introduces formidable challenges, such as severe training instability and prohibitive computational costs. In this work, we introduce Ring-1T, novel Mixture-ofExperts (MoE) thinking model scaled to unprecedented sizeand demonstrate breakthrough methodologies for efficient trillion-parameter training. By solving fundamental stability and efficiency challenges at this scale, we enable robust large-scale reasoning training while providing extensive implementation insights. Our Ring-1T, the first open-source reasoning model with one trillion total parameters, is built upon the Ling 2.0 (Ling-Team, 2025) architecture and trained from the Ling-1T-base. With approximately 50 billion activated parameters per token, Ring-1T achieves state-of-the-art performance across multiple challenging benchmarksdespite relying solely on natural language reasoning capabilities. It significantly outperforms existing open-source models, achieving scores of 93.4 on AIME-2025, 86.72 on HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Remarkably, in the IMO2025 evaluation within AWorld 2, Ring-1T achieved silver medal-level result by correctly solving four problems and partially proving Problem 2, all within single submission, and without relying on code generation or external symbolic solvers. Realizing this breakthrough required addressing fundamental challenges in trillion-scale RL training. We pioneered three interconnected innovations: IcePop eliminates catastrophic training-inference misalignment in RL training by clipping excessive-discrepancy tokens. This selective correction pops out unstable contributions while preserving efficient updates, thereby stabilizing training without slowing inference. C3PO++ introduces budget-controlled rollout scheduling mechanism that eliminates rolloutstage bottlenecks. Thus, it avoids inefficient single-pass processing of oversized sequences, reducing computational overhead while enabling their efficient reuse through batched continuation. ASystem is high-performance reinforcement learning (RL) framework designed for largescale asynchronous training. It adopts SingleController + SPMD (Single Program, Multiple 1https://www.hmmt.org/www/archive/problems 2https://github.com/inclusionAI/AWorld 2 Data) to enable fully asynchronous operations, multi-phase masking acceleration, and efficient data packing/sharding. The structure of this paper is organized as follows: Section 2 describes our comprehensive training methodology, which includes Long Chain-of-Thought Supervised Fine-Tuning (Long-CoT SFT) and large-scale reinforcement learning (RL), including our key algorithmic contributions, IcePop and C3PO++, as well as the underlying training framework, Asystem. Finally, Section 3 presents thorough evaluation of our models performance against leading open-weights and closed-weights models on established benchmarks."
        },
        {
            "title": "2 Approach",
            "content": "The Ring-1T model was developed via multi-stage pipeline, commencing with long-CoT SFT and advancing through reasoning and general RL phases. While the complete pipeline is integral to the models performance, the primary focus of this report is on RL components. We first summarize the long-CoT SFT process that primes the model for RL in Section 2.2. The core of our discussion then turns to the novel RL algorithm, which is elaborated upon in Section 2.3. Additionally, we introduce ASystem in Section 2.5, the distributed system that enabled the large-scale RL training necessary for this project. 2.1 Training Pipeline Figure 2 The training pipeline of Ring-1T. We use Ling-1T-base model (Ling-Team, 2025), novel Mixture-of-Experts model with total of 1 Trillion parameters and an activation of 50 Billion parameters, as our base model. The training of Ring-1T consists of three stages, comprising long-CoT SFT, reasoning-oriented RL, and general-oriented RL, to cultivate powerful thinking model. Long-CoT SFT: We collect and synthesize large amount of multi-domain reasoning trajectory data covering mathematics, code, science, etc. Through large-scale supervised fine-tuning, the model learns general reasoning patterns and domain-specific reasoning skills, establishing solid foundation for large-scale RL training. Reasoning RL: We construct comprehensive, challenging, and high-quality RL dataset encompassing math, code, science, and logic tasks with verifiable outcomes. The models comprehensive reasoning performance is enhanced via RLVR (Reinforcement Learning from Verifiable Rewards). This process involves sampling extensive reasoning trajectories and refining the policy using verifiable rewards, which are provided by carefully designed multi-domain verifiers. General RL: Following large-scale reinforcement learning on verifiable tasks, we conduct second RL stage focused on general tasks. This phase employs RLHF (Reinforcement Learning from Human Feedback) to recalibrate the models capability distribution, preserving its core reasoning strength while enhancing human alignment, instruction following, creative writing, safety, and overall usability. 2.2 Long-CoT SFT In this stage, we aim to endow the base model with fundamental long-chain reasoning abilities through Long Chain-of-Thought Supervised Fine-Tuning (Long-CoT SFT). This process serves as the foundation for subsequent reinforcement learning, equipping it with the capability to sustain coherent, multi-step thinking processes for complex problems. Data Collection comprehensive, high-quality dataset featuring Long-CoT reasoning patterns was constructed to effectively activate the base models reasoning capability. The query pool originates from tripartite sourcing strategy: open-source repositories, expert manual generation, and LLM-based synthesis. Following data collection, rigorous data-cleansing protocols were applied. To ensure the resulting datasets quality, we designed rigorous data processing pipeline comprising four sequential steps: 1) Deduplication, where we employed exact matching to remove repetitive samples; 2) Harmful Content Filtering, where data samples containing toxic or harmful information were identified and purged; 3) Data Decontamination, where we utilized both hashing and exact string matching techniques to detect and eliminate any samples that overlap with existing benchmarks; and 4) Low-Quality Sample Filtering, which removeed various noise sources including invisible control codes and extraneous Unicode characters. The final data is predominantly composed of four domains: Mathematics (46%), STEM (26%), Code (20%), and Others (8%). Training We conduct Long-CoT SFT on our Ling-1T-base model (Ling-Team, 2025) to obtain model with preliminary thinking capabilities. The training data are packed into 64k-length sequences. For this stage, the model was trained for 3 epochs with learning rate of 2 104. We employed cosine decay scheduler with 30 warmup steps and applied weight decay of 0.1 throughout the process."
        },
        {
            "title": "2.3 Large-Scale Reinforcement Learning",
            "content": "Reinforcement learning (RL) is the critical next step for translating knowledge from pre-training and SFT into advanced thinking. To enable RL at the unprecedented scale of 1 Trillion parameters, we developed novel RL algorithm and specialized infrastructure, Asystem. This integrated solution overcomes fundamental challenges in training efficiency and stability, allowing us to systematically explore the models complex problem-solving capabilities. 4 2.3.1 RL Data High-quality and diverse data are critical for effective reinforcement learning. To this end, we introduce carefully curated, multi-domain dataset spanning five core areas: math, code, science, logic, and general domains: Math: We extend the dataset from Ling-Team et al. (2025) with mathematically rigorous problems from authoritative sources. Our curation ensures completeness, high complexity, and verifiable solutions, yielding high-quality corpus for large-scale reinforcement learning. Code: Aside from the dataset employed in Ling-Team et al. (2025), we develop multi-phase workflow for synthesizing, validating, quality-scoring, and selecting additional test cases. This process ensures that each problem is equipped with sufficient number of high-quality test cases. The final dataset contains programming problems with verified correct solutions and carefully tested cases. Science: We developed crowdsourced science dataset of high-difficulty problems spanning physics, chemistry, and biology. To ensure complexity for reinforcement learning, all multiplechoice questions were reformatted into an open-ended format. For organic chemistry, we established dedicated image-semantization pipeline that converts visual information such as molecular structures into structured textual descriptions. Finally, we applied Pass-rate filtering strategy to select only the highest-quality items. Logic: Our logic reasoning dataset spans five domains: visual pattern induction (Chollet et al., 2025), grid puzzles (Sudoku), pathfinding (mazes), arithmetic reasoning (24 Game), and propositional logic (Knights and Knaves). We synthesized problems by integrating public resources such as Hodel (2024), Li et al. (2025), and Liu et al. (2025) into an in-house game generator, enabling scalable and controlled creation. quality control process ensures each task is solvable and non-trivial during both generation and post-processing. The final curated collection balanced across domains and complexity levels for reinforcement learning. General Data: We constructed comprehensive dataset for general reasoning by aggregating problems from two primary sources: public repositories and real-world user interactions. From public sources, we incorporated established general datasets including Magpie (Xu et al., 2024), WMT (Feng et al., 2025), RLVR-IFEval 3, and AutoIF 4. To enhance practical alignment, we further integrated real-world user preference data such as arena-human-preference-100k and arena-human-preference-140k 5. Additionally, we augmented this collection with problems sourced from social media platforms such as Zhihu and StackOverflow. Finally, we employ multi-stage curation pipeline involving parsing, reformulation, and deduplication, with quality assured by dual scoring system of LLMs and rule-based metrics. Furthermore, fine-grained metadata annotations on each sample enable dynamic sampling and cross-domain blending, strategy that significantly improves training efficiency and model performance on complex tasks. 2.3.2 IcePop: Discard All Noisy Gradient Updates Contemporary reinforcement learning training frameworks typically utilize distinct engines for model training and inference processes. Throughout our experiments, we have observed that this 3https://huggingface.co/datasets/allenai/RLVR-IFeval 4https://huggingface.co/datasets/Post-training-Data-Flywheel/AutoIF-instruct-61k 5https://huggingface.co/datasets/lmarena-ai 5 Figure 3 We integrate C3PO++ and IcePop into Ring-1T, which enhances both training efficiency and effectiveness of RL. separation may lead to discrepancies in probability calculations, potentially introducing instability to RL training. This problem is particularly pronounced in the training of MoE models with RL due to the inherent usage of the dynamic routing mechanism. Additionally, in long CoT settings, these discrepancies can gradually accumulate across iterations and become further amplified. 2 µ(cid:1) δt. Theorem 1. (Compounding Probability Discrepancy) Let πinfer(; θ) and πtrain(; θ) be the policy model (cid:0)πinfer(; θt) πtrain(; θt)(cid:1) be probability disloaded by inference and training engines, and δt = DKL crepancy at step t. Under certain conditions and step size µ > 0, there exist constant η > 0 such that δt+1 (cid:0)1 + η To address this compounding mismatch issue in MoE RL, we propose IcePop, variant of GRPO that suppresses unstable training updates through double-sided masking calibration. IcePop only calibrates gradients within the acceptable region and discards all noisy gradient updates beyond that boundary, effectively aligning πtrain with πinfer. This is achieved through two key techniques: Double-sided calibration: We calibrate token-level gradients within region defined by lower and upper limits, well preserving the alignments between training and inference probabilities. Masking: We exclude tokens with excessive probability deviation from gradient computation, constraining gradient updates in stable region. Integrating these techniques yields the following objective function for IcePop: JIcePop(θ) = xD,{yi}G i=1 πinfer(x;θold) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G i=1 1 yi yi t=1 (cid:104) (cid:16) πtrain(yi,t x, yi,<t; θold) πinfer(yi,t x, yi,<t; θold) (cid:17) ; α, β (cid:16) min ri,t (cid:98)Ai,t, clip (ri,t, 1 ε, 1 + ε) (cid:98)Ai,t (cid:17) γDKL(πθπref) (cid:35) (cid:105) , (1) where ri,t = πtrain(yi,tx,yi,<t; θ) πtrain(yi,tx,yi,<t; θold) , M(k) is the masking function defined as below: M(k) = (cid:40) if [α, β], 0 otherwise (2) where α, β controls the lower and upper limits. Thus, the gradient of IcePop is θJIcePop(θ) aπinfer(θold) (cid:34) (cid:32) (cid:33) πtrain(a; θold) πinfer(a; θold) θ log πtrain(a; θ) ˆA r(a) . (3) (cid:33)(cid:35) For πtrain(a; θold) πinfer(a; θold) < α and πtrain(a; θold) πinfer(a; θold) > β, IcePop discards all noisy gradients outside the region that may introduce potential instability into the training process. more detailed introduction of IcePop could refer to our blog 6. 2.3.3 C3PO++: Dynamically Partition Rollouts with Budget Figure 4 C3PO++ improves reinforcement learning efficiency for large thinking models by maintaining rollout buffer across policy model versions. Once the rollout in an iteration reaches the token budget, optimization is performed; unfinished rollouts are stored in the buffer and resumed by the updated policy in the next iteration. We introduce C3PO++, an extension of C3PO (Ling-Team et al., 2025) that incorporates budgetcontrolled rollout partition mechanism. This approach dynamically partitions rollout generation to prevent idleness of computational resources caused by individual long rollouts. The system incorporates two modules: high-throughput inference pool Pinfer with capacity Ω infer for parallel generation, and training pool Qtrain with capacity Ω train for collecting completed trajectories. Similar to the idea of C3PO, we regulate the rollout generation with token budget (Φ), which stabilizes the training updates and enables highly efficient rollout process. 6https://ringtech.notion.site/icepop 7 The C3PO++ procedure is detailed in Algorithm 1. At iteration t, the inference engine πinfer;θt populates the inference pool by generating rollouts in parallel, while tracking the cumulative number of generated tokens in real-time. When rollout reaches terminal state (i.e., [EOS]), it will be moved from Pinfer to the training pool Qtrain and counted towards the training tokens C. Inference proceeds until reaches the token budget Φ. At this point, the training engine πtrain;θt updates the parameters with the completed trajectories in Qtrain regulated by the token budget, which may include samples resumed from earlier inference versions. We denote the number of partitions sequence has undergone as the retention period. For each iteration, the retention period of unfinished rollouts will be automatically increased by 1. Before each iteration, rollouts whose retention period exceeds threshold σ are purged from Pinfer. Meanwhile, new prompts may be sampled to refill Pinfer until it reaches capacity Ω infer. After the model parameters are updated to θt+1, the inference engine πinfer;θt+1 initiates new iteration of rollout generation, continuing the process for rollouts within the valid retention period and monitored by the token budget. Algorithm 1: C3PO++ Input: initial parameters θ0; inference engine πinfer;θ; training engine πtrain;θ; token budget Φ; infer; retention threshold σ. inference pool capacity Ω Output: Sequence of parameter updates θ0 θ1 1 State: Inference pool Pinfer (capacity Ω Pinfer ; Qtrain ; 0 ; 2 while not converged do 3 infer); training pool Qtrain. begin 0; for Pinfer do in parallel if retention(o, σ) then Pinfer Pinfer{o} ; while (C < Φ) do if Pinfer < Ω infer then // remove overextended rollouts from inference pool Pinfer Pinfer sample_prompt() ; // maintain full inference pool for Pinfer do in parallel // generate the next token for rollouts in parallel πinfer;θt (o) ; if terminal(o) then + ; Qtrain Qtrain {o} ; Pinfer Pinfer{o} ; // cumulate token amount // save completed rollouts for training // remove completed rollouts from inference θt+1 Update(θt, Qtrain) ; Qtrain ; + 1 ; return θt"
        },
        {
            "title": "2.3.4 Training Recipe",
            "content": "All policy optimization was conducted using the ASystem framework. We employ the AdamW optimizer with hyperparameters β1 = 0.9, β2 = 0.999, weight decay of 0.01, and with the MoE router bias held fixed. For the reasoning RL stage, we implemented the proposed IcePop (α = 0.5, 8 4 5 6 8 9 10 11 12 14 15 16 17 18 20 β = 5) and C3PO++ algorithms. The training configuration used learning rate of 2 106, KL coefficient of 0.0, and sampling temperature of 1.0. Each training step utilized 480 unique prompts, with 8 rollouts sampled per prompt and maximum length of 65,536 tokens. For the general RL stage, we utilized GRPO with learning rate of 3 106, KL coefficient of 0.0, and sampling temperature of 1.0. Each step in this stage consisted of 80 unique questions with 8 outputs each, and maximum length of 32,768 tokens. 2.4 Experiments and Analysis This section presents experiments to validate the effectiveness of our proposed methods: IcePop, which ensures stable policy optimization, and C3PO++, which enables efficient rollout generation. 2.4.1 IcePop Setup To evaluate the effectiveness of IcePop, we conduct preliminary experiments on the Ringmini-2.07 model, which is MoE model with 16.8B total parameters and 0.75B activated parameters. We compare three settings: (1) IcePop with α = 0.5, β = 5, (2) TIS (Yao et al., 2025) with the officially recommended setting, which mitigates the training-inference mismatch issue with importancesampling correction, and (3) Vanilla GRPO without the KL-term. For fair comparison, we use the same training dataset for all models. Preliminary results on Ring-mini-2.0. As shown in Figure 5, we can see that IcePop consistently outperforms TIS on the challenging benchmark AIME25, with large gain along the training process, and finally improves the base score (63%) by over 14%, and expands the performance gap with TIS by relative 6%. Figure 5 The performance comparison on AIME25 (Avg@64). We evaluate all models using the same setting. Experiments on Ring-1T. As training progresses, we can see from Figure 6 that the original GRPO suffers from training instability, as both the gradient norms and the probability discrepancy between the inference and training engines tend to increase rapidly. However, after applying IcePop, we can observe that the mismatch issue has been largely mitigated, stabilizing the RL training process. 7https://huggingface.co/inclusionAI/Ring-mini-2.0 Figure 6 The training dynamics before and after applying IcePop. 2.4.2 C3PO++ We compare C3PO++ with the baseline setting that omits our budget-controlled rollout partition mechanism, assessing training efficiency and effectiveness in terms of training time, training reward, and benchmark performance. Training Time. As illustrated in Figure 7, C3PO++ substantially reduces the time of the rollout phase, achieving an approximately 2.5 times speedup per step. Since rollout duration usually accounts for large portion of training time in RL, the training optimization designed by C3PO++ yields about 1.5 times speedup for the end-to-end phase per step, significantly boosting the training efficiency for reinforcement learning. Figure 7 Comparison of time cost between C3PO++ and the baseline. Reward and Performance. As shown in Figure 8, the reward curve of C3PO++ remains close to that of the baseline, suggesting that our optimization in rollout management maintains 10 comparable training dynamics in the reinforcement learning process. On the representative reasoning benchmarks, C3PO++ achieves performance on par with the baseline, demonstrating its strength in producing competitive results. Figure 8 Comparison of reward and benchmark performance between C3PO++ and the baseline. 2.5 Large-Scale RL Infrastructure: ASystem Figure 9 An overview of ASystem RL training framework. Training Ring-1T with reinforcement learning requires specialized infrastructure that can manage its unprecedented scale. The sheer size of the model, coupled with the inherent complexity of distributed RL workflows, poses unique challenges in memory management, state synchronization, and computational throughput. To this end, we developed ASystem, high-performance RL framework whose components are co-designed with the requirements of Ring-1T in mind. As illustrated in Figure 9, ASystems architecture is built around unified execution environment and includes the following key components, each engineered to address specific bottleneck in the 11 RL training for trillion-parameter model: Hybrid Runtime: The core of ASystem, this runtime seamlessly integrates training and inference workloads. For Ring-1T, this means we can conduct massive parallel policy evaluation (inference) and model weight updates (training), eliminating the overhead of data transfer between separate systems and ensuring efficient utilization of thousands of GPUs. AMem: AMem is GPU memory management library designed to overcome the critical memory bottleneck in large-scale RL training, like that of our 1T model. It optimizes memory usage and data transfer, enabling larger batches, fewer OOM errors, and faster deployment with minimal code changes and no loss of accuracy. AState: AState is high-performance weight synchronization framework for RL. It efficiently addresses the challenge of distributing updated model parameters from trainers to inference actors using zero-redundancy peer-to-peer mechanism, enabling synchronization of trillionparameter models in under 10 seconds. ASandbox: serverless environment for rapid scenario validation. By offering millisecondscale cold start and high-throughput isolation, ASandbox accelerates evaluation of Ring-1T rollouts during large-scale RL training. This foundational design, based on SingleController + SPMD (Single Program, Multiple Data) architecture, delivers significant advantages for robust large-scale training. It provides plugand-play support for training, inference, and reward model backends, facilitating independent debugging and development at scale. Crucially, by separating the control flow from the data flow, ASystem effectively mitigates the single-point data flow bottlenecks prevalent in mainstream SingleController frameworks. Furthermore, the system incorporates mechanisms for fast-fail reporting and automatic recovery from slow training and hangs, thereby enhancing overall training stability and efficiency for demanding workloads like our Ring-1T model. 2.5.1 Hybrid Runtime: Unified Training-Inference Execution Environment Hybrid Runtime is an integrated training-inference system designed for large-scale LLM reinforcement learning. It provides high-performance, elastic, and scalable foundation by unifying efficient resource scheduling, linear scalability, comprehensive parallelism strategies, and unified execution engine. The system is architected to support models of diverse architectures, scales, and training paradigms on large-scale clusters. To bridge the gap between dynamic training and real-time inference in reinforcement learning (RL), we introduce AState, high-speed framework for synchronizing weights between training and inference. AState provides unified weight management API that supports diverse model architectures, deployment topologies, and pipeline paradigms without requiring framework modifications. At its core, zero-redundancy peer-to-peer transmission mechanism delivers only necessary weight shards, enabling in-place updates on inference engines to eliminate costly data copies. This is complemented by hardwaresoftware co-design that optimizes data movement through NUMA topology and CPU-GPU affinity awareness, alongside multi-transport communication layer (integrating RDMA, NCCL, and shared memory) that dynamically selects the optimal protocol based on data size and hardware topology. Consequently, AState achieves sub-second parameter updates, ensuring inference rollouts use the latest model and maintaining the critical training-inference alignment essential for stable policy optimization. To enhance GPU memory efficiency, we introduce AMem, memory and data transfer library optimized for RL workloads on GPU clusters. AMem enhances memory management efficiency through three key mechanisms: (1) Memory Switching, for the transparent release and resumption of training state, including NCCL communications and CUDA graphs; (2) Distributed Multipath Transfer Shen et al. (2025), which aggregates bandwidth across multiple channels; and (3) Unified Memory Pooling, for dynamic allocation across GPUs and nodes. By enabling larger batch sizes, reducing out-of-memory (OOM) errors, and accelerating system startup, AMem alleviates common bottlenecks in large-scale RL. The library is designed for transparency, requiring no model modifications and ensuring no impact on RL convergence, thereby providing robust infrastructure support for the Hybrid Runtime and AState components. 2.5.2 ASandbox: An On-Demand Serverless Sandbox Engine ASandbox is serverless sandbox engine for RL, providing rapid, isolated environments for tasks like code execution and terminal simulation. Integrated with Kubernetes and deployable as standalone FaaS cluster, it executes RL tasks via function calls. It offers specialized sandboxes (e.g., math, code, STEM, terminal) supporting HTTP and MCP protocols. To ensure the consistent, stable feedback critical for RL training, it features: 1) Security: Kernel-level isolation via secure containers (runsc, kata); 2) Availability: Automatic node failure detection and isolation; 3) Speed: 100ms startup via image caching, cgroups, and fork; 4) Scalability: 5,000 QPS/200ms throughput via scheduling partitions. 2.5.3 AReaL: High-Performance RL Algorithm Framework ASystem is unified, high-performance foundation for distributed reinforcement learning. Its reinforcement learning component, AReaL, is an open-source framework (Fu et al., 2025) built to prioritize algorithm development by balancing ease of use with system flexibility. It offers both single-controller and SPMD interfaces through minimalist APIs and an extensible plugin mechanism, allowing researchers to focus on algorithmic innovation. AReaL is characterized by several key features as follows: Asynchronous Multi-Stage Pipeline: fully decoupled architecture that concurrently executes trajectory generation, reward computation, and training. This overlap eliminates rollout long-tail issues and maximizes hardware utilization. Efficient Data Management: Intelligent data packing and sharding minimize padding and rebalancing overhead, reducing computational waste and training stalls. Fault Tolerance: The system features automated error detection, retry, and recovery mechanisms to ensure stability amidst hardware and software failures. Massive Scalability: By separating control and data planes, AReaL avoids the single-controller bottleneck, enabling seamless scaling across large clusters."
        },
        {
            "title": "3 Evaluation",
            "content": "This section presents the performance of our Ring-1T on suite of challenging benchmarks spanning mathematics, coding, and logical reasoning, as well as other general tasks, comparing it against leading reasoning models. 13 3.1 Benchmarks To comprehensively assess Ring-1T, we conduct evaluations across wide range of benchmarks, primarily covering 8 domains: knowledge, coding, math, reasoning, alignment, healthcare, multiturn, and agent. Knowledge: GPQA-Diamond (Rein et al., 2023), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), Phybench (Qiu et al., 2025), AGIEval (Zhong et al., 2023), TriviaQA (Joshi et al., 2017), CMMLU (Li et al., 2023). Coding: LiveCodeBench-v6 (2408 to 2505) (Jain et al., 2025), CodeForces 8, Aider 9. Math: AIME 2025 (AIME, 2025), Omni-MATH (Gao et al., 2024), HMMT 2025, CNMO 2024, FinanceReasoning (Tang et al., 2025), UGMathBench (Xu et al., 2025). Reasoning: ARC-AGI-1 (Chollet et al., 2024), BBEH (Kazemi et al., 2025), ZebraLogic (Lin et al., 2025), HLE (Phan et al., 2025). Alignment: ArenaHard v2 (Li et al., 2024), Creative Writing v3 (Paech, 2025), IFEval (Zhou et al., 2023). Healthcare: HealthBench (Arora et al., 2025). Multi-turn: MultiChallenge (Sirdeshmukh et al., 2025). Agent: BFCL v3 (Yan et al., 2024). We benchmark Ring-1T against leading open-weights models (DeepSeek-V3.1-Terminus-Thinking, and Qwen-35B-A22B-Thinking-2507) and proprietary API models (Gemini-2.5-pro, GPT-5-Thinking). All evaluations use controlled experimental conditions with standardized configurations. 3.2 Evaluation Settings All thinking models are evaluated under standardized pipeline for fair comparison. Benchmarks including AIME 2025, LiveCodeBench-v6, ARC-AGI-1, CodeForces, HMMT 2025, ZebraLogic, HLE, and BFCL v3 are assessed with 128K context window, extended via YaRN (Peng et al., 2023) for models with insufficient native context. Benchmarks including Aider, CNMO 2024, and BBEH use 64K context window. Other benchmarks use 32K context window. For baseline models, we use their official hyperparameters for open-weights models, while using vendor-recommended settings for proprietary APIs. For baselines with officially reported results, we report the higher score between our reproduction and the official release."
        },
        {
            "title": "3.3 Results",
            "content": "Table 1 provides comprehensive comparison of Ring-1T against leading thinking models. The following sections provide detailed analysis of its performance across different aspects: Mathematical Reasoning Ring-1T demonstrates leading mathematical reasoning capabilities, as evidenced by its performance on challenging benchmarks. By relying solely on natural language reasoning, it achieves 93.40% on AIME 25 and 86.72% on HMMT 25, securing the second-highest rank overall and leading all open-weights models. Furthermore, the model delivers competitive 8The Codeforces was assessed through problems from 14 Div. 2 contests of Codeforces, combined with expert-designed test cases, followed by the computation of expected ratings and competitor proportions. It is worth noting that the highest rating attainable is 2209. 9https://aider.chat/docs/benchmarks.html#the-benchmark 14 Table 1 Performance comparison across multiple benchmarks. Benchmark Architecture # Total Params # Activated Params AIME 2025 (Avg@64) Omni-MATH HMMT25 (Avg@16) CNMO 2024 FinanceReasoning UGMathBench LCB-v6 (2408-2505) (Avg@4) CodeForces (rating) CodeForces (percentile) Aider ARC-AGI-1 BBEH ZebraLogic HLE GPQA-Diamond MMLU-Pro C-Eval Phybench AGIEval TriviaQA CMMLU ArenaHard v2 (win-rate) ArenaHard v2 (Elo) Creative Writing v3 IFEval HealthBench MultiChallenge BFCL v3 Open Weights Close Weights Ring-1T DeepSeek-V3.1Terminus-Thinking Qwen3-235B-A22Gemini2.5-Pro Thinking-2507 GPT-5Thinking (High) MoE 1T 50B 93.40 82.63 86.72 88.54 87.42 76.47 78.30 2088 97.85 78.57 55.94 59.63 95.15 16.03 78.63 80.54 91.53 42.65 88.13 78.59 89.14 81.59 84.52 85.40 85.21 57. 50.92 68.82 Math Coding MoE 671B 37B 89.06 81.93 86.10 85.42 87.76 77. 75.33 2073 97.76 92.86 Reasoning 40.62 61.04 96.33 17.82 Knowledge 81.00 85.00 91.22 47.91 89.83 82.77 89.20 Alignment 60.27 62.73 85.24 89.09 Healthcare 50.19 Multi-turn 45.79 62. Agent MoE 235B 22B 92.30 82.52 83.90 89.50 87.65 77.00 75.72 2055 97.55 88.91 48.12 60.00 97.03 13.95 81.10 84.40 93.13 42.61 90.01 79.63 90. 80.18 81.72 85.49 87.80 55.56 52.75 73.53 - - - 88.00 82.14 82.50 80.64 87.33 74. 70.65 1837 93.47 94.36 45.44 51.51 92.40 21.60 86.40 85.62 90.14 55.01 88.99 84.45 88.83 79.20 80.92 85.70 88.72 49.39 54. 61.36 - - - 94.60 82.90 93.30 87.93 89.33 80.18 80.60 1918 86.18 95.49 65.70 72.78 98.00 24.84 86.05 86.21 88.27 48.53 88.56 86.32 87. 82.91 83.18 89.69 95.38 67.20 69.60 57.21 Results reported in official model documentation. Blue denotes Ring-1T achieving state-of-the-art among open-source models. Bold indicates first place overall performance. Underline indicates second place overall performance. 15 results across specialized mathematical domains, scoring 82.63% on Omni-MATH and 88.54% on CNMO 2024. These results highlight particular proficiency in complex, Olympiad-style problem-solving. This demonstrates that stable and efficient RL training recipe, together with diverse and high-quality math training dataset, drives superior mathematical reasoning across competition-level benchmarks. Moreover, we evaluate the mathematical reasoning capabilities of Ring-1T on the IMO 2025. Specifically, Ring-1T is integrated into the multi-agent framework AWorld (Yu et al., 2025) and tasked with solving the problems through pure natural language reasoning, without relying on code generation or external symbolic solvers. The model successfully solved Problems 1, 3, 4, and 5 on its first attempt, performance corresponding to the IMO silver medal level. On its third attempt, it generated nearly complete geometric proof for Problem 2. For the most challenging Problem 6, which no AI participant solved correctly during IMO 2025, Ring-1T converged to the same incorrect answer (4048) as Gemini 2.5 Pro, whereas the correct answer is 2112. detailed case study is available in Appendix E. We believe the outstanding natural language reasoning ability will generalize to broader range of tasks, paving the way for enhanced overall performance. Coding Capabilities As the results show, Ring-1T demonstrates exceptional performance in programming tasks that demand iterative refinement and deep logical reasoning, establishing leading position among both open-weights and closed-weights models. On LiveCodeBench-v6 (2408-2505), it achieves top score of 78.30%, outperforming DeepSeek-V3.1 by 2.97 points and Qwen3-235B-A22B-Thinking-2507 by 2.58 percentage points. Furthermore, on CodeForces (rating), Ring-1T attains score of 2088, which is the highest score among all models and exceeds the performance of both open-source competitors and closed-source APIs. It indicates that our carefully synthesized dataset shapes Ring-1Ts robust performance on programming applications, which forms strong foundation for future endeavors on agentic applications. Logical Reasoning Ring-1T demonstrates promising capabilities in other logical reasoning tasks. Powered by sourcing carefully selected logical games from multiple domains, Ring-1Tachieves score of 55.94% on the challenging ARC-AGI-1 benchmark, ranking second overall. This performance places it only behind GPT-5-Thinking (65.70%) and represents substantial improvement of +15.32 percentage points over DeepSeek-V3.1 (40.62%) and +7.82 points over Qwen3-235B-A22BThinking-2507 (48.12%). Human Alignment In addition to the reasoning RL training, we also leverage general RL training stage to equip the reasoning model with strong performance on general tasks. From Table 1, Ring-1T achieves strong alignment with human preferences in complex scenarios. On the ArenaHard v2 benchmark, it attains an 81.59% win-rate, ranking second overall and trailing GPT-5-Thinking by only 1.32 percentage points. It also leads all models with an Elo rating of 84.52. In Creative Writing v3, Ring-1T scores 85.40%, performing within 0.1 percentage points of the leading open-source model. These results confirm Ring-1Ts effectiveness in balancing human preference alignment with broad capabilitiesa critical advantage for real-world deployment. Healthcare Capabilities On HealthBench, Ring-1T attains score of 57.93%, ranking second overall and leading the field of open-source models. This performance indicates proficient clinical knowledge integration and suggests the models viability for complex healthcare tasks."
        },
        {
            "title": "4 Conclusion",
            "content": "In this work, we have presented Ring-1T, landmark achievement as the first open-weights 1 Trillion parameter thinking model. This project successfully addressed the profound and unprecedented system and algorithmic challenges inherent in scaling reinforcement learning to trillion-parameter regime. The core of our contribution lies in three interconnected innovations: the IcePop for resolving training-inference mismatches, the C3PO++ for efficient long-trajectory rollouts, and the ASystem framework that eliminates scalability bottlenecks and ensures training stability. Together, these advancements enabled the stable and efficient training of Ring-1T, which has demonstrated breakthrough, state-of-the-art performance across rigorous set of benchmarks spanning mathematical reasoning, competitive programming, and general intelligence. These results validate our approach and systems, demonstrating that trillion-parameter reasoning models are not only feasible but also exhibit exceptional capability."
        },
        {
            "title": "5 Limitations & Future Work",
            "content": "Despite its landmark achievements, Ring-1T and its associated training systems have several limitations that point to fruitful directions for future research. Model Architecture & Inference Efficiency: The models use of GQA (Ainslie et al., 2023) provides solid balance between performance and speed. However, for our Ring-1T thinking model, which generates extensive internal thought processes, the inference cost imposed by GQA remains non-trivial. Future work will therefore explore alternative mechanisms, such as MoBA (Lu et al., 2025) or advanced linear attention variants, to achieve the higher throughput required for efficient inference. Training-Inference Consistency: While our IcePop methodology mitigates the major traininginference mismatch, it does not achieve perfect training-inference consistency. Underlying numerical discrepancies between the training and inference computational operators persist as latent source of instability. Resolving this fundamental systems challenge is imperative for the stable scaling of future models. Capability Deficiencies: The training strategy for Ring-1T was optimized for foundational natural language reasoning, leaving advanced agentic skills (e.g., tool use) under-optimized. Future iterations will position Ring-1T as base for such capabilities, integrating specialized data and training paradigms like agentic RL to cultivate sophisticated autonomous problemsolving. Additionally, minor issues such as identity confusion and linguistic code-switching, attributed to data impurity and insufficient regularization, will be addressed through refined data curation techniques. 17 Xiaodong Yan Xiaopei Wan Xin Zhao Xinyu Kong Xinyu Tang Xudong Han Xudong Wang Xuemin Yang Xueyu Hu Yalin Zhang Yan Sun Yicheng Shan Yilong Wang Yingying Xu Yongkang Liu Yongzhen Guo Yuanyuan Wang Yuchen Yan Yuefan Wang Yuhong Guo Zehuan Li Zhankai Xu Zhe Li Zhenduo Zhang Zhengke Gui Zhenxuan Pan Zhenyu Huang Zhenzhong Lan Zhiqiang Ding Zhiqiang Zhang Zhixun Li Zhizhen Liu Zihao Wang Zujie Wen"
        },
        {
            "title": "6 Contributors",
            "content": "Authors are listed alphabetically by the first name. Ling Team Anqi Shen Baihui Li Bin Hu Bin Jing Cai Chen Chao Huang Chao Zhang Chaokun Yang Cheng Lin Chengyao Wen Congqi Li Deng Zhao Dingbo Yuan Donghai You Fagui Mao Fanzhuang Meng Feng Xu Guojie Li Guowei Wang Hao Dai Haonan Zheng Hong Liu Jia Guo Jiaming Liu Jian Liu Jianhao Fu Jiannan Shi Jianwen Wang Jianxin Lai Jin Yang Jun Mei Jun Zhou Junbo Zhao Junping Zhao"
        },
        {
            "title": "Kuan Xu\nLe Su\nLei Chen\nLi Tang\nLiang Jiang\nLiangcheng Fu\nLianhao Xu\nLinfeng Shi\nLisha Liao\nLongfei Zheng\nMeng Li\nMingchun Chen\nQi Zuo\nQiang Cheng\nQianggang Cao\nQitao Shi\nQuanrui Guo\nSenlin Zhu\nShaofei Wang\nShaomian Zheng\nShuaicheng Li\nShuwei Gu\nSiba Chen\nTao Wu\nTao Zhang\nTianyu Zhang\nTianyu Zhou\nTiwei Bie\nTongkai Yang\nWang Hong\nWang Ren\nWeihua Chen\nWenbo Yu\nWengang Zheng\nXiangchun Wang",
            "content": "denotes corresponding authors."
        },
        {
            "title": "References",
            "content": "Moonshot AI. checkpoint-engine, 2025. https://github.com/MoonshotAI/checkpoint-engine. AIME. Aime problems and solutions. https://artofproblemsolving.com/wiki/index.php/AIMEProblemsandSolutions, 2025. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023. https://arxiv.org/abs/2305.13245. Rahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, Johannes Heidecke, and Karan Singhal. Healthbench: Evaluating large language models towards improved human health. CoRR, abs/2505.08775, 2025. doi: 10.48550/ ARXIV.2505.08775. https://doi.org/10.48550/arXiv.2505.08775. Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report. arXiv preprint arXiv:2412.04604, 2024. François Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. ARC-AGI-2: new challenge for frontier AI reasoning systems. CoRR, abs/2505.11831, 2025. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. https://arxiv. org/abs/2501.12948. Tiantian Fan, Lingjun Liu, Yu Yue, Jiaze Chen, Chengyi Wang, Qiying Yu, Chi Zhang, Zhiqi Lin, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Bole Ma, Mofan Zhang, Gaohong Liu, Ru Zhang, Haotian Zhou, Cong Xie, Ruidong Zhu, Zhi Zhang, Xin Liu, Mingxuan Wang, Lin Yan, and Yonghui Wu. Truncated proximal policy optimization, 2025. https://arxiv.org/abs/2506.15050. Zhaopeng Feng, Shaosheng Cao, Jiahan Ren, Jiayuan Su, Ruizhe Chen, Yan Zhang, Zhe Xu, Yao Hu, Jian Wu, and Zuozhu Liu. Mt-r1-zero: Advancing llm-based machine translation via r1-zero-like reinforcement learning, 2025. https://arxiv.org/abs/2504.10160. Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, and Yi Wu. Areal: large-scale asynchronous reinforcement learning system for language reasoning, 2025. https://arxiv.org/abs/2505.24298. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: universal olympiad level mathematic benchmark for large language models, 2024. https://arxiv.org/abs/2410.07985. Horace He and Thinking Machines Lab. Defeating nondeterminism in llm inference. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml.20250910. https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/. Michael Hodel. Addressing the abstraction and reasoning corpus via procedural example generation. CoRR, abs/2404.07353, 2024. Jian Hu, Xibin Wu, Wei Shen, Jason Klein Liu, Zilin Zhu, Weixun Wang, Songlin Jiang, Haoran Wang, Hao Chen, Bin Chen, Weikai Fang, Xianyu, Yu Cao, Haotian Xu, and Yiming Liu. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework, 2025. https://arxiv.org/abs/2405.11143. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. In NeurIPS 2023, 2023. http://papers.nips.cc/paper_files/paper/2023/ hash/c6ec1844bec96d6d32ae95ae694e23d8-Abstract-Datasets_and_Benchmarks.html. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. https://openreview.net/forum?id=chfJJYC3iL. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. triviaqa: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv e-prints, art. arXiv:1705.03551, 2017. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit Jain, Virginia Aglietti, Disha Jindal, Peter Chen, et al. Big-bench extra hard. arXiv preprint arXiv:2502.19187, 2025. 19 Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2023. Peiji Li, Jiasheng Ye, Yongkang Chen, Yichuan Ma, Zijie Yu, Kedi Chen, Ganqu Cui, Haozhan Li, Jiacheng Chen, Chengqi Lyu, Wenwei Zhang, Linyang Li, Qipeng Guo, Dahua Lin, Bowen Zhou, and Kai Chen. Internbootcamp technical report: Boosting LLM reasoning with verifiable task scaling. CoRR, abs/2508.08636, 2025. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The arena-hard pipeline. Blog post.[Accessed 07-02-2025], 2024. Bill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, and Yejin Choi. Zebralogic: On the scaling limits of llms for logical reasoning. arXiv preprint arXiv:2502.01100, 2025. Ling-Team. Ling-2.0 techinical report, 2025. https://github.com/inclusionAI/Ling-V2. Ling-Team, Bin Hu, Cai Chen, Deng Zhao, Ding Liu, Dingnan Jin, Feng Zhu, Hao Dai, Hongzhi Luan, Jia Guo, et al. Ring-lite: Scalable reasoning via c3po-stabilized reinforcement learning for llms. arXiv preprint arXiv:2506.14731, 2025. Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, and Junxian He. Synlogic: Synthesizing verifiable reasoning data at scale for learning logical reasoning and beyond. CoRR, abs/2505.19641, 2025. Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, Zhiqi Huang, Huan Yuan, Suting Xu, Xinran Xu, Guokun Lai, Yanru Chen, Huabin Zheng, Junjie Yan, Jianlin Su, Yuxin Wu, Neo Y. Zhang, Zhilin Yang, Xinyu Zhou, Mingxing Zhang, and Jiezhong Qiu. Moba: Mixture of block attention for long-context llms, 2025. https://arxiv.org/abs/2502.13189. OpenAI. Openai o1 system card, 2024. https://arxiv.org/abs/2412.16720. OpenAI. Openai gpt-5, 2025. https://openai.com/gpt-5/. Samuel Paech. Eq-bench creative writing benchmark v3. https://github.com/EQ-bench/creative-writing-bench, 2025. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Haoxu Zhang, Yi Hu, Chenyang Wang, Chencheng Tang, Haoling Chang, Qi Liu, Ziheng Zhou, Tianyu Zhang, Jingtian Zhang, Zhangyi Liu, Minghao Li, Yuku Zhang, Boxuan Jing, Xianqi Yin, Yutong Ren, Zizhuo Fu, Weike Wang, Xudong Tian, Anqi Lv, Laifu Man, Jianxiang Li, Feiyu Tao, Qihua Sun, Zhou Liang, Yushu Mu, Zhongxuan Li, Jing-Jun Zhang, Shutao Zhang, Xiaotian Li, Xingqi Xia, Jiawei Lin, Zheyu Shen, Jiahang Chen, Qiuhao Xiong, Binran Wang, Fengyuan Wang, Ziyang Ni, Bohan Zhang, Fan Cui, Changkun Shao, Qing-Hong Cao, Ming xing Luo, Muhan Zhang, and Hua Xing Zhu. Phybench: Holistic evaluation of physical perception and reasoning in large language models, 2025. https://arxiv.org/abs/2504.16074. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. https://arxiv.org/abs/2311.12022. Ao Shen, Rui Zhang, and Junping Zhao. Flexlink: Boosting your nvlink bandwidth by 27 https://arxiv.org/abs/2510. 15882. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020. https://arxiv.org/abs/1909.08053. Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, and Chen Xing. Multichallenge: realistic multi-turn conversation evaluation benchmark challenging to frontier llms. arXiv preprint arXiv:2501.17399, 2025. Zichen Tang, Ziyan Ma, Haoyang He, Jiacheng Liu, Zhongjun Yang, Zihua Rong, Rongjin Li, Kun Ji, Qing Huang, Xinyang Hu, et al. Financereasoning: Benchmarking financial numerical reasoning more credible, comprehensive and challenging. arXiv preprint arXiv:2506.05828, 2025. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu. Kimi k2: Open agentic intelligence, 2025. https://arxiv.org/abs/2507.20534. Qwen Team. Qwen3-max: Just scale it, September 2025. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlupro: more robust and challenging multi-task language understanding benchmark. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. http://papers.nips.cc/paper_files/paper/2024/hash/ ad236edc564f3e3156e1b2feafb99a24-Abstract-Datasets_and_Benchmarks_Track.html. Xin Xu, Jiaxin Zhang, Tianhao Chen, Zitong Chao, Jishan Hu, and Can Yang. Ugmathbench: diverse and dynamic benchmark for undergraduate-level mathematical reasoning with large language models. arXiv preprint arXiv:2501.13766, 2025. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing, 2024. https://arxiv.org/abs/2406. 08464. Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_ leaderboard.html, 2024. Feng Yao, Liyuan Liu, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your efficient rl framework secretly brings you off-policy rl training, August 2025. https://fengyao.notion.site/off-policy-rl. Chengyue Yu, Siyuan Lu, Chenyi Zhuang, Dong Wang, Qintong Wu, Zongyue Li, Runsheng Gan, Chunfeng Wang, Siqi Hou, Gaochi Huang, et al. Aworld: Orchestrating the training recipe for agentic ai. arXiv preprint arXiv:2508.20404, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization, 2025. https://arxiv.org/abs/2507. 18071. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs, 2024. https://arxiv.org/abs/2312.07104. Zhipu-AI. Glm-4.6, 2025. https://z.ai/blog/glm-4.6. 21 Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models, 2023. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Zilin Zhu, Chengxing Xie, Xin Lv, and slime Contributors. slime: An llm post-training framework for rl scaling. https://github.com/THUDM/slime, 2025. GitHub repository. Corresponding author: Xin Lv."
        },
        {
            "title": "A Related Work",
            "content": "A.1 Stable Reinforcement Learning To accelerate the training process of reinforcement learning on large-scale language models, current RL frameworks employ different backend implementations for training and inference stages. However, such heterogeneity inevitably introduces unaligned calculations of token probability, which consequently brings instability issues to the training process. To solve this problem, GSPO (Zheng et al., 2025) previously employed routing replay training strategy, which cached the activated experts in the old policy model with parameter θold in advance and replayed these routing modes in the current policy model when computing the importance ratios. However, the routing replay strategy will increase both computation and memory overhead to the RL framework. Rather, they proposed an algorithm-level solution, which defines sequence-level importance ratio and performs clipping on that, avoiding training crashes caused by high-variance gradients. In comparison to GSPO, the techniques employed by IcePop are independent of sequence-level optimization, suggesting that they can be incorporated into ongoing research efforts. TIS (Yao et al., 2025) also addresses the probability discrepancy problem that exists between the training and inference stages. They proposed using importance sampling correction to address the divergence and demonstrated promising results. Different from our work, for gradients exhibiting significant divergence, i.e., tokens outside our masking range, TIS opts to keep updating those tokens by applying moderating coefficient to their gradients. Empirically, we find that as training progresses, these minor disturbances can gradually amplify and ultimately lead to plateau in benchmark performance. A.2 Efficient Reinforcement Learning For reasoning models that generate long sequences, the rollout phase often consumes large percentage of training resources and time. To improve the training efficiency, TPPO (Fan et al., 2025) extends PPO by implementing truncated rollout strategy and eliminating biased estimation from incomplete trajectories. Prior work (Fu et al., 2025) incorporates high-efficiency generation management suitable for an asynchronous RL system with interruptible rollout workers. In contrast to the existing works addressing low utilization of computing resources at the inference stage, C3PO++ stands out by applying dynamic cutoff to rollout generation based on token budget, balancing the stabilization of training updates with improved inference efficiency. Meanwhile, it supports high-throughput inference procedures in parallel, maximizing compute utilization and alleviating the rollout bottleneck during training. Our empirical analysis shows that the design of C3PO++ improves inference efficiency without sacrificing training performance, providing solid foundation for integrating advanced training algorithms. A.3 Reinforcement Learning Infrastructure Training large-scale reinforcement learning models, particularly at the trillion-parameter level, imposes extraordinary demands on the underlying infrastructure. Below, we review how existing systems address the following three primary challenges and highlight their limitations, which collectively motivated the design of ASystem. Memory Efficiency: Managing the massive GPU memory footprint of model states, activations, and experience data throughout the training cycle without introducing significant 23 overhead remains an open problem. Popular training and inference frameworksincluding vLLM (Kwon et al., 2023), SGLang (Zheng et al., 2024), Megatron-LM Shoeybi et al. (2020), and RL-specific systems such as VeRL (Sheng et al., 2024) and OpenRLHF (Hu et al., 2025)typically retain model states and communication groups in GPU memory throughout execution, leading to static and inefficient memory usage. The NVIDIA Collective Communication Library (NCCL) does not natively support live memory offloading. Although it provides plugin interfaces, general and efficient solution remains absent. recent effort, Slime (Zhu et al., 2025), proposes destroying and re-creating NCCL communication groups to free memory, but the subsequent re-initialization overhead is prohibitive at scale, often consuming several minutes and severely disrupting training stability. State Synchronization: Efficiently and reliably propagating model weights across distributed training and rollout workers is essential for policy consistency. Early RL frameworks such as OpenRLHF (Hu et al., 2025) relied on distributed file systems (e.g., NFS) for checkpoint sharing, suffering from limited bandwidth and throughput with synchronization latencies of tens of minutes. More recent systems, including VeRL (Sheng et al., 2024) and the checkpointengine (AI, 2025), have shifted toward using NCCL for direct peer-to-peer weight synchronization. While this reduces dependency on shared storage, the approach still suffers from redundant data movement and poor performance in handling numerous small tensors, keeping end-to-end synchronization in the minute range. System Orchestration & Reproducibility: Providing flexible, stable, and deterministic execution environment is crucial for rapid iteration. Existing frameworks like OpenRLHF and VERL are typically architected with tight coupling between components, making backend integration costly and slow. Furthermore, rollout behavior in these systems remains inherently non-deterministic due to fluctuating batch sizes and the non-associative nature of floatingpoint arithmetic across distributed workers (He and Lab, 2025). The absence of systematic support for deterministic execution and metric alignment complicates reliable ablation studies and hinders root-cause analysis when reward improvement stagnates. In contrast to the limitations above, ASystem is designed from the ground up to holistically address these core infrastructure challenges. Our AMem component enables efficient live memory offloading without the excessive re-initialization overhead seen in approaches like Slime (Zhu et al., 2025). The AState system overcomes the inefficiencies of existing synchronization methods through zero-redundancy P2P transmission mechanism and hardware-aware multi-transport communication layer, supporting sub-second weight synchronization and in-place updates. Finally, the Hybrid Runtime offers unified, RL-specific API that abstracts away backend complexities in networking, memory management, and weight exchange. Crucially, it incorporates comprehensive precision alignment mechanismcomprising Tracker, Analyzer, and Replayer modulesto ensure end-to-end reproducibility and deterministic execution. Together, these components allow ASystem to deliver the stability, efficiency, and developer agility necessary for training massive models such as our Ring-1T."
        },
        {
            "title": "B Theoretical Analysis for IcePop",
            "content": "Theorem 2 (Compounding probability discrepancy). Let πinfer(; θ) and πtrain(; θ) be the policy model loaded by inference and training engines, and denote the probability discrepancy as δ(θ) = DKL (cid:0)πinfer(; θ) πtrain(; θ)(cid:1), δt := δ(θt). 24 Consider the update with step size of µ θt+1 = θt + µ gt, gt = aπinfer(;θt) (cid:2)A(a) θ log πtrain(a; θt)(cid:3), and write gt = + bt with = aπtrain(;θt) (cid:2)A(a) θ log πtrain(a; θt)(cid:3), bt := gt . Assume there exists neighborhood that contains the iterates {θt} where (A1) L-smoothness. δ is differentiable with L-Lipschitz gradient, i.e. (cid:12) δ(θ + ) δ(θ) δ(θ), (cid:12) (cid:12) (cid:12) 2 2. (A2) Bias alignment. There is > 0 such that (cid:10)δ(θt), bt (cid:11) δt. (A3) Bounded on-policy drift. There is 0 such that (cid:10)δ(θt), (cid:12) (cid:12) (cid:11)(cid:12) (cid:12) M. (A4) Local gradient bound. There is 0 such that gt G. Then for any stepsize µ (0, µ] (with µ chosen so that (A1)(A4) hold throughout the trajectory), there exist constants such that η := > 0, κ := + 2 µ G2 0, δt+1 (cid:0)1 + η µ(cid:1) δt κ µ. Consequently, if δt δc := 2κ η , then Proof. By (A1) with = µgt, δt+1 (cid:0)1 + η 2 µ(cid:1) δt. δt+1 = δ(θt + µgt) δt + µ(cid:10)δ(θt), gt (cid:11) 2 µ2gt2. Decompose gt = + bt and apply (A2)(A3): (cid:11) = (cid:10)δ(θt), (cid:10)δ(θt), gt (cid:11) + (cid:10)δ(θt), bt (cid:11) + δt. Use (A4) to bound the quadratic term: 2 µ2gt2 2 µ2G2 2 µ µ G2."
        },
        {
            "title": "Combine the inequalities to obtain",
            "content": "δt+1 δt + µ (c δt M) 2 µ µ G2 = (1 + ηµ) δt (cid:0)M + 2 µG2(cid:1)µ. Setting η := and κ := + 2 µG2 gives the first claim: δt+1 (1 + ηµ) δt κ µ. 25 For the compounding form, rewrite (1 + ηµ) δt κµ = (cid:16) 1 + η 2 µ (cid:17) δt + (cid:16) η 2 δt κ (cid:17) µ. Hence, if δt 2κ/η, the last term is nonnegative and δt+1 (cid:16) 1 + η 2 µ (cid:17) δt. Preliminary Analysis for IcePop on Ring-mini-2.0 We also analyze IcePop in terms of the probability discrepancy between training and inference engines, training stability, exploration ability, and ill-conditioned tokens. Training Stability. We believe that stable training process serves as solid foundation and sufficient space to showcase the power of reinforcement learning. It is worth noting that both IcePop and TIS mitigate the instability of RL training within 600 gradient steps (see Figure 10), avoiding rapid training crashes occurring in the baseline setting. Figure 10 (Left) Training reward. The reward of baseline collapses after 180200 steps. Both IcePop and TIS maintain stable growth. (Right) Gradient norm. Baseline explodes, IcePop and TIS remain stable. Probability Discrepancy. Without addressing the mismatch issues, the probability difference grows rapidly, as shown in the baseline setting. In contrast, both TIS and IcePop keep the KL divergence of training-inference probability within reasonable range. Although the maximum probability difference rises for all three methods as training proceeds, the discrepancy of IcePop remains relatively low and even decreases within 400 steps (see the left in Figure 11). We also notice that TIS consistently shows larger extreme discrepancies and faster growth than ours, probably due to including the noisy policy updates during training. Exploration Space. In the right of Figure 11, we observed that the log probabilities of IcePop consistently maintain relatively lower values than those of TIS, which implicitly indicates that our method avoids overconfident predictions, thus ensuring larger scope for exploring space, where low-probability tokens are more likely to be chosen, eventually increasing the diversity of responses. 26 Figure 11 (Left) The maximum of probability discrepancy. (Right) The log probability of tokens. Baseline increases rapidly and drops to the bottom, while IcePop is relatively steady. Ill-conditioned Tokens. In our experiments, we found that the clipping ratio from our masking mechanism stays around 12 of training tokens (see Figure 12). As training progresses, the clipping ratio rises sharply, suggesting that increasingly subtle but harmful gradient updates occur and necessitate higher clipping ratio. We also conducted detailed analysis of the clipped tokens. The following right figure shows that, compared to all tokens, clipped tokens have higher entropy, indicating that the clipped tokens play an important role in training. Figure 12 (Left) Clipping ratio. IcePop maintains 12 of tokens clipped in our default setting. (Right) The comparisons of token entropy between all tokens and clipped tokens. Compared to all tokens, clipped tokens show higher proportion of high-entropy tokens. Sensitivity Analysis. We compare how different masking ranges for the calibration ratio affect training. Specifically, we experiment with three masking ranges: (1) α = 0.5, β = 5.0 (default range), (2) α = 0.5, β = 2.0 (narrow range), and (3) α = 0.4, β = 5.0 (wider range). As shown in Figure 13, 1. The default masking range [0.5, 5.0] not only stabilizes training but also enriches sampling diversity. 2. The narrow masking range [0.5, 2.0] immediately destabilizes training, as shown in the volatility of gradient norm and the sharp increase in probability discrepancy. 3. The wide masking range [0.4, 5.0] still stabilizes training, yet includes tokens with higher log probability compared to the default setting. 27 Figure 13 The training dynamics under different masking ranges."
        },
        {
            "title": "D RL Data Analysis",
            "content": "This section analyzes the composition of our SFT and RL datasets. We first present the domain distribution of the SFT data in Figure 14, illustrating its diversity, which underpins the models broad knowledge base. Subsequently, Figure 15 details the complexity distribution of the RL data. Figure 14 The domain distribution of SFT Data. 28 Figure 15 The difficulty distribution of RL Data."
        },
        {
            "title": "E IMO Case",
            "content": "As established in the main paper, our proposed model Ring-1T achieved silver-medal performance at IMO 2025. The detail IMO 2025 Results from Ring-1T could refer to the AWorld 10. Below, we present detailed solution from Ring-1T to the competitions first problem: Question: line in the plane is called sunny if it is not parallel to any of the xaxis, the yaxis, or the line + = 0. Let 3 be given integer. Determine all nonnegative integers such that there exist distinct lines in the plane satisfying both of the following: 1. for all positive integers and with + + 1, the point (a, b) lies on at least one of the lines; and 2. exactly of the lines are sunny. Solution: Generated by Ring-1T:"
        },
        {
            "title": "Summary",
            "content": "a. Verdict have found complete solution. The nonnegative integers satisfying the conditions are precisely = 0, = 1, and = 3. All other values of (i.e., even integers 2, odd integers 5, and > n) are impossible. b. Method Sketch The problem requires covering all points (a, b) Z+ Z+ with + + 1 using exactly distinct lines, where exactly are sunny (not parallel to the x-axis, y-axis, or + = 0). Key Observations: The set of required points forms triangular grid Pn with T(n) = n(n+1) Non-sunny lines fall into three categories: horizontal (y = c), vertical (x = d), and diagonal (x + = e). sunny line can cover at most (cid:4) k+1 (cid:5) points from any subset isomorphic to Pk, due to geometric constraints points. 2 2 on collinearity in the grid. Critical Results: Even 2: The minimum number of uncovered points U(k) = k(k+1) exceeds the maximum coverage 2 2 by sunny lines, making it impossible. Odd 5: For = 2t 1 with 3, the minimal uncovered set Pk has t(2t 1) points. While sunny lines could theoretically cover this if each had points, only 3 disjoint t-point sunny lines exist in Pk, leaving 2t(t 2) points that cannot be covered by the remaining 2t 4 lines (as 2t(t 2) > (2t 4)(t 1) for 3). 10https://github.com/inclusionAI/AWorld/tree/main/examples/imo/samples/samples%20from%20Ring-1T 29 = 0, 1, 3: Explicit constructions show these are always possible: * = 0: Use diagonal lines + = 2, . . . , + 1. * = 1: Use 1 horizontal lines and one sunny line through the single uncovered point. * = 3: Use 3 horizontal lines to leave subgrid isomorphic to P3, which can be partitioned into 3 sunny lines. Construction Strategy: For = 3, the uncovered subgrid P3 is covered by three specific sunny lines with slopes 1, 2, and 1 2 , verified explicitly for P3 and generalized via coordinate transformation for larger n."
        },
        {
            "title": "Detailed Solution",
            "content": "Step 1: Understanding the Point Set Let = + 1. The set of required points is: Pn = {(a, b) Z+ Z+ : + N}. This set has T(n) = n(n+1) line (x + = s). 2 points. Each point lies on one horizontal line (y = b), one vertical line (x = a), and one diagonal Step 2: Maximum Coverage by Non-Sunny Lines Define M(m, n) as the maximum number of points in Pn covered by non-sunny lines. It is known that: M(m, n) = m(2n + 1) 2 , achieved by selecting the largest non-sunny lines (e.g., top rows). If = non-sunny lines are used, the minimum number of uncovered points is: U(k) = T(n) M(n k, n) = k(k + 1) 2 . These U(k) points must be covered by sunny lines. Step 3: Impossibility for Even 2 For even 2, U(k) = k(k+1) sunny lines cover at most: 2 . Any sunny line can cover at most (cid:4) k+1 2 (cid:5) = 2 points from set isomorphic to Pk. Thus, k 2 = k2 2 < k(k + 1) 2 = U(k). Hence, no configuration exists for even 2. Step 4: Impossibility for Odd 5 Let = 2t 1 with 3. Then U(k) = t(2t 1). The maximum number of points per sunny line in Pk is t, but only 3 disjoint t-point sunny lines exist in Pk (e.g., slopes 1, 2, 1 2 ), covering 3t points. The remaining points are: With 3 = 2t 4 lines left, each can cover at most 1 points, yielding maximum coverage: t(2t 1) 3t = 2t(t 2). (2t 4)(t 1) = 2t2 6t + 4. Since 2t(t 2) = 2t2 4t > 2t2 6t + 4 for 3, the remaining points cannot be covered. Thus, no configuration exists for odd 5. Step 5: Possibility for = 0 Use the lines + = 2, 3, . . . , + 1. These are all non-sunny (slope 1) and partition Pn, covering all T(n) points. Hence, = 0 is valid. Step 6: Possibility for = 1 Use 1 horizontal lines = 1, 2, . . . , 1, covering all points with 1. The only uncovered point is (1, n) (since + + 1 = = 1). Cover (1, n) with sunny line, e.g., = + (n 1) (slope 1, sunny). This line is distinct from the horizontal lines, so all lines are distinct and exactly = 1 is sunny. 30 Step 7: Possibility for = 3 Use = 3 horizontal lines = 1, 2, . . . , m, covering all points with m. The uncovered points satisfy + 1, 1, + N. Substituting = m, this becomes + 4, forming subgrid isomorphic to P3 with 6 points: (1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (3, 1). Cover P3 with three sunny lines: L1: (1, 1), (2, 2) (slope 1), L2: (1, 2), (3, 1) (slope 1 2 ), L3: (1, 3), (2, 1) (slope 2). These lines are sunny (slopes not 0, , 1) and disjoint, covering all 6 points. Transforming back to original coordinates (b = + m), the lines remain distinct and sunny. Combined with the non-sunny horizontal lines, this gives distinct lines with exactly = 3 sunny lines. Step 8: Completeness of Characterization = 0: Always possible via diagonal lines. = 1: Always possible via 1 horizontal lines and one sunny line. = 3: Always possible for 3 via the above construction. Even 2: Impossible due to coverage deficit. Odd 5: Impossible due to geometric constraints on Pk. > n: Impossible since there are only lines. No other values of are feasible."
        },
        {
            "title": "Final Answer",
            "content": "All nonnegative integers satisfying the conditions are = 0, = 1, and = 3. Thus, the solution is: = 0, 1,"
        }
    ],
    "affiliations": [
        "Inclusion AI"
    ]
}