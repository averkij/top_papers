{
    "paper_title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation",
    "authors": [
        "Hongyu Zhang",
        "Yufan Deng",
        "Shenghai Yuan",
        "Peng Jin",
        "Zesen Cheng",
        "Yian Zhao",
        "Chang Liu",
        "Jie Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-video (T2V) generation has made significant strides with diffusion models. However, existing methods still struggle with accurately binding attributes, determining spatial relationships, and capturing complex action interactions between multiple subjects. To address these limitations, we propose MagicComp, a training-free method that enhances compositional T2V generation through dual-phase refinement. Specifically, (1) During the Conditioning Stage: We introduce the Semantic Anchor Disambiguation to reinforces subject-specific semantics and resolve inter-subject ambiguity by progressively injecting the directional vectors of semantic anchors into original text embedding; (2) During the Denoising Stage: We propose Dynamic Layout Fusion Attention, which integrates grounding priors and model-adaptive spatial perception to flexibly bind subjects to their spatiotemporal regions through masked attention modulation. Furthermore, MagicComp is a model-agnostic and versatile approach, which can be seamlessly integrated into existing T2V architectures. Extensive experiments on T2V-CompBench and VBench demonstrate that MagicComp outperforms state-of-the-art methods, highlighting its potential for applications such as complex prompt-based and trajectory-controllable video generation. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 8 2 4 4 1 . 3 0 5 2 : r MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation Hongyu Zhang1* Jie Chen1.2(cid:66) Zesen Cheng1 1School of Electronic and Computer Engineering, Peking University, Shenzhen, China 3Tsinghua University, Beijing, China 2Peng Cheng Laboratory, Shenzhen, China. Yufan Deng1* Yian Zhao1 Shenghai Yuan1 Chang Liu3 Peng Jin Figure 1. Overall pipeline for MagicComp. (a) Our MagicComp comprises two core modules: Semantic Anchor Disambiguation (SAD) for resolving inter-subject ambiguity during conditioning, and Dynamic Layout Fusion Attention (DLFA) for spatial-attribute binding via fused layout masks in denoising. (b) MagicComp is training-free framework, which effectively address the challenges (e.g., semantic confusion, misaligned spatial relationship, missing entities) in compositional video generation with minimal additional inference overhead."
        },
        {
            "title": "Abstract",
            "content": "Text-to-video (T2V) generation has made significant strides with diffusion models. However, existing methods still struggle with accurately binding attributes, determining spatial relationships, and capturing complex action interactions between multiple subjects. To address these limitations, we propose MagicComp, training-free method that enhances compositional T2V generation through dualphase refinement. Specifically, (1) During the Conditioning Stage: We introduce the Semantic Anchor Disambiguation (cid:66) Corresponding author. Equal contribution. to reinforces subject-specific semantics and resolve intersubject ambiguity by progressively injecting the directional vectors of semantic anchors into original text embedding; (2) During the Denoising Stage: We propose Dynamic Layout Fusion Attention, which integrates grounding priors and model-adaptive spatial perception to flexibly bind subjects to their spatiotemporal regions through masked attention modulation. Furthermore, MagicComp is model-agnostic and versatile approach, which can be seamlessly integrated into existing T2V architectures. Extensive experiments on T2V-CompBench and VBench demonstrate that MagicComp outperforms state-of-the-art methods, highlighting its potential for applications such as complex promptbased and trajectory-controllable video generation. Project page: https: // hongyuzhang. github .io/ MagicComp-Page/. 1. Introduction Recent text-to-video (T2V) generation has achieved significant breakthroughs, driven by the scaling of data and model sizes [10, 16, 42, 49, 51, 52] as well as the development of novel network architectures [1, 35, 25, 32]. These advancements are propelling the field toward new era of high-quality visual content synthesis [15, 22, 27, 48, 50]. However, existing models consistently struggle to process prompts involving compositional semantics, especially those with multiple subjects, intricate spatial configurations, and dynamic interactions. These limitations often lead to issues such as semantic leakage, misaligned spatial relationships, and subject missing, making compositional T2V generation persistently challenging task [39]. To address these challenges, existing methods primarily adopt two strategies: (1) Layout-based approaches, which leverage object-grounding layouts to achieve attribute and motion binding for individual subjects through masked attention mechanisms [12, 26, 40, 43]. (2) Inferenceenhanced approaches, which employ multi-round evaluations [19, 24] based on large language models (LLMs) [34] and multi-modal large language models (MLLMs) [33], or conduct test-time optimization [47] to iteratively refine the generation process and enhance compositional quality. However, these methods still exhibit significant limitations: (1) Coarse grounding conditions, such as blobs and 2D bounding boxes, fail to capture fine-grained shape variations, leading to unnatural and inconsistent subject appearances. Additionally, existing layout-based approaches require either large-scale fine-tuning or LoRA tuning [17], both of which impose computational and training overhead. (2) While test-time optimization and multi-round evaluation improve quality, they considerably increase substantial inference burdens. Therefore, developing training-free compositional video generation framework that achieves high-quality results without significantly increasing inference time remains critical yet underexplored challenge. The primary challenge in compositional T2V lies in addressing two interrelated objectives: (1) preventing semantic ambiguity and leakage among multiple subjects and (2) ensuring precise attributelocation binding for each subject across temporal sequences. Notably, achieving both objectives simultaneously is challenging, especially when the model is frozen. natural approach, therefore, is to decompose these objectives into distinct phases during sampling, enabling progressive refinement at each stage. Guided by the preceding analysis and insights, we propose MagicComp, training-free framework that incrementally resolves inter-subject semantic disambiguation and achieves spatio-temporal binding between subjects and their textual prompts through sequential refinement. During the conditioning phase, text encoders might produce semantically ambiguous embeddings, where distinct subjects (e.g., \"dog\" vs. \"table\") exhibit unexpectedly high similarity due to cross-token interference. To address this, we introduce the Semantic Anchor Disambiguation (SAD), which generates the subject anchor embeddings through independently encoding each subject, then computes directional vectors between these anchors and adaptively integrates them into the original text embeddings. The strength of this integration is dynamically modulated by semantic confusion scales and denoising timesteps, ensuring balance between semantic disambiguation and contextual coherence. During the denoising stage, we present the Dynamic Layout Fusion Attention (DLFA) to establish precise spatio-temporal binding between each subject and its associated attributes and locations. Unlike previous approaches [40, 43] that rigidly constrain attention with predefined masks, DLFA first estimates coarse subject layouts via LLM-generated priors, then dynamically refines them through model-adaptive perception of the correlation between subject-specific text embeddings and video token embeddings, enabling finegrained control over each subjects shape and details. By integrating the SAD with DLFA, we introduce MagicComp, training-free and model-agnostic framework that seamlessly enhances mainstream architectures (e.g., DiT-based and UNet-based approaches) and achieves substantial improvements in compositional T2V tasks. We incorporate MagicComp into both CogVideoX [48] and VideoCrafter2 [8], and evaluate its performance on T2V-CompBench [39] and VBench [20]. Quantitative comparisons across these benchmarks demonstrate that our method achieves superior performance. Beyond standard metrics, we highlight its applicability in complex promptbased and trajectory-controllable video generation scenarios, emphasizing its versatility and practical potential. Our contributions are summarized as follows: We propose MagicComp, model-agnostic approach for compositional text-to-video generation that avoids additional training or parameter updates. This design significantly reduces overhead and inference time, making compositional video synthesis more efficient and practical. We introduce two key modules: (i) Semantic Anchor Disambiguation (SAD), which enhances subject-specific semantics and eliminates inter-subject ambiguity through adaptive integration of semantic anchors directional guidance into text embeddings; and (ii) Dynamic Layout Fusion Attention (DLFA), which ensures spatio-temporal subject consistency by combining LLM prior layouts with model-adaptive perception layouts to achieve more flexible attention modulations. Comprehensive results on T2V-CompBench and VBench demonstrate MagicComps superior performance compared to state-of-the-art compositional methods. 2. Related Work Text-to-video generation. Early methods [2, 6, 8, 21, 41], such as MagicTime [51, 52] and AnimateDiff [14], integrating temporal modules into the 2D U-Net architecture [38], thereby seamlessly extending image generation models to video generation models. More recent works, including OpenSora Plan [27], CogVideoX [48], LTXVideo [15], and HunyuanVideo [23], incorporate 3D fullattention mechanism, recognized for its superior scalability [32]. This advantage significantly enhances models ability to capture dynamics in video data, driving rapid progress in T2V [7, 9, 11, 25, 46, 50, 53]. However, these methods still face challenges in accurately preserving attribute consistency, modeling spatial relationships, and capturing complex interactions among multiple subjects, further research are required to address these limitations. Compositional video generation. Compositional video generation remains challenging task, particularly in managing multiple objects and their interactions across spatial and temporal dimensions. While compositional image generation has seen significant advancements [29, 31, 45], video generation has lagged behind. Recent works, such as VideoTetris [40] and Vico [47], have improved compositional variations and temporal consistency but still face difficulties in handling dynamic object interactions. Layoutbased approaches, such as LVD [26], often lack precise spatiotemporal guidance. More recent methods leverage large language models (LLMs) [19, 24, 28] to enhance compositional control through fine-grained scene planning. Although effective, these approaches require substantial computational resources for training. In contrast, MagicComp, the proposed method, operates within training-free framework, making it more efficient solution. 3. Method 3.1. Overall Framework of MagicComp As illustrated in Figure 2, MagicComp employs dualphase refinement strategy during the sampling process. In the conditioning stage, SAD module performs an additional encoding step, processing each subject in the prompt independently to generate semantically purified anchor embeddings. These anchors are then used to compute semantic directional vectors, which are adaptively fused into the original prompt embeddings to mitigate semantic leakage and ambiguity between subjects (see Sec. 3.2). In the denoising stage, DLFA applies fine-grained subject-aware masked attention by combining prior LLM layout masks with modeladaptive perception masks. The prior masks provide coarse estimates of subject size and position, while the modeladaptive perception masks, derived from text-video embedding correlations, enable precise manipulation of subject shapes and details. Overall, the integration of both stages allows for training-free generation of compositional videos, eliminating semantic ambiguity while ensuring precise attributelocation binding. 3.2. Semantic Anchor Disambiguation As revealed in [18, 44], text encoders such as T5 [37] and CLIP [36] allow tokens to interact with each other during the encoding process, which may cause semantic leakage between subjects with fine-grained attributes or similar semantics. As illustrated in Figure 3, the T5 embeddings of brown dog\" and gray cat\" exhibit remarkably high similarity, disrupting the text-video attention correlation between these two subjects and resulting in visually confused content (e.g., cat resembling dog). To mitigate these issues, SAD module is proposed to eliminate semantic confusion between subjects and provide more instructive conditional text embedding. For prompt containing subjects, SAD module first obtains the embedding representation and the corresponding subject token embeddings [Pi]M i=1 = [P1, P2..., PM ]. Following this, it independently encodes each subject in the prompt to obtain the subject anchor embeddings [Ai]M i=1 = [A1, A2..., AM ]. The core of SAD module lies in leveraging the pure semantic information of each subject in [Ai]M i=1 to appropriately eliminate semantic confusion while preserving the contextual relationships in [Pi]M i=1. Specifically, SAD module first calculates the confusion scale to quantify semantic confusion, then adjusts the intensity of semantic disambiguation interpolation accordingly. Confusion Scale Calculation. For subject k, an intuitive way to assess its semantic confusion is to evaluate the extent to which it deviates from its original semantics while converging towards the semantics of other subjects. This can be quantified as the degree to which Pk moves away from Ak and closer to [Ai]M i=1 (where = k) in the semantic space: i=1 1[i=k] exp (cid:0)cos (cid:0)Pk, Ai i=1 exp (cid:0)cos (cid:0)Pk, Ai (cid:80)M (cid:1) /τ (cid:1) (cid:1) /τ (cid:1) sk = (cid:80)M (1) . where sk denotes the confusion scale for subject k, Pk and Ai refers to pooled embeddings and τ indicates the temperature factor which is experimentally set to 0.2. Similarly, confusion scale for all the subjects can be represented as [si]M i=1, which are then utilized as the scaling factors for the strength of semantic disambiguation interpolation. Figure 2. Detailed architecture of MagicComp. The dual-phase refinement strategy of MagicComp contains two core steps: (a) Semantic Anchor Disambiguation (SAD) module for inter-subject disambiguation during the conditioning stage. We only display disambiguation process of subject cat\" for simplicity, other subjects follow the similar way. (b) Dynamic Layout Fusion Attention (DLFA) module for precise attributelocation binding of each subject during the denoising stage. original embeddings [Pi]M i=1 to eliminate semantic confusion according to the confusion scales [si]M i=1. However, determining the appropriate interpolation strength should also consider the specific denoising time step. Since the early stages of denoising require clearer semantics to capture the basic layout and shape of each subject, while the later stages of denoising focus more on leveraging the context restored in [Pi]M i=1 to understand the interactions between different subjects. To achieve this, we additionally incorporate time-aware strength attenuated function ω(t) = 1 to formulate the final interpolation process: = Pi (ω(t) si i). (3) i=1 is replaced by [P where indicates the interpolated subject embeddings, and refers to the broadcasting, respectively. Finally, [Pi]M ]M i=1 to obtain the semantically disambiguated condition. As shown in Figure 3, after incorporating the SAD module, the semantic similarity between brown dog\" and gray cat\" approaches normal level, and their cross-attention maps in the subsequent denoising stage exhibit decoupled pattern. This further demonstrates the effectiveness of the SAD module. Figure 3. Visualization of the disambiguation effect brought by SAD. (a) Cos similarity between the pooled embeddings of brown dog\" and gray cat\" under different settings. Standard\" indicates the cos similarity is computed when each subject are independently encoded by T5. (b) Cross attention maps between the middle frame video tokens and the pooled subject tokens. Semantic Disambiguation Interpolation. To disambiguate the semantics of subject k, we must identify the pathways in the semantic space that direct it away from the semantic of other subjects and closer to its own. Specifically, the pathway can be represented as the semantic directional vector Ak from other subjects to subject using the pooled subject anchor embeddings [Ai]M i=1: Ak = (cid:88) i=1 (Ak Ai). (2) 3.3. Dynamic Layout Fusion Attention The semantic directional vectors for all [Ai]M i=1 can be obtained in similar manner. the subjects After obtaining [Ai]M i=1, we can interpolate it into the To conduct spatio-temporal binding between each subject and its respective textual attributes and locations, DLFA is devised to dynamically guide the subject-specific region conditioning on the corresponding subject in the text subject-specific thresholds: = CountTrue(Lprior δi = Sortdesc (k) (Corri) . ). (5) where δi refers to the threshold for the i-th subject and Sortdesc (k) () is the operator that retrieves the k-th largest value. We then threshold the attention correlation for each subject:"
        },
        {
            "title": "Ladapt\ni",
            "content": "= I(Corri > δi). (6) Figure 4. Comparison of different masking strategy. (a) Visualization of prior layout mask and model-adaptive perception layout. (b) Comparison of the generated videos. prompt. Unlike training-based methods relying solely on prior layout masks [12, 26, 43], DLFA conducts the subjectaware masked attention via coarse-to-fine masking strategy: first coarsely localizing subjects via prior layout mask generated by LLM, then refining their shapes and details through the introduction of model-adaptive perception layout mask. 1 , Lprior , ..., Lprior Prior Layout Mask. Given subjects specified in the prompt, we utilize LLM to automatically generate prior layouts Lprior = {Lprior }, where Lprior {0, 1}Nvideo indicates the binary layout mask for i-th subject with the number of the video tokens Nvideo. Concurrently, we extract corresponding textual token masks subject = {T subject }, 2 where subject {0, 1}Ntext with Ntext being the numi ber of textual tokens. , ..., subject , subject 1 Model-adaptive Perception Layout Mask. Unfortunately, the prior layout masks often manifest as coarse 2D bounding boxes, which is misaligned with the diverse geometric profiles of real-world subjects. To address this limitation, we introduce an additional model-adaptive perception layout that is dynamically derived from video tokens exhibiting strong cross-modal correlations with their corresponding subject tokens. Given text query for subjects } and video key video, we first ob- {Qtext 1 tain the attention correlation Corri for the i-th subject: , ..., Qtext , Qtext 2 Similarly, we can obtain the model-adaptive perception layouts Ladapt = {Ladapt , ..., Ladapt , Ladapt 2 }. 1 Subject-aware Masked Attention. After constructing the prior layout masks Lprior and model-adaptive perception layout masks Ladapt, we combine these two types of masks for each subject individually: = Lprior Lf use Lf use = {Lf use 1 , Ladapt , Lf use , ..., Lf use }. (7) The fused masks, Lf use, incorporate both the subjects position and size as determined by the LLM, along with the objects shape dynamics and details, which are adaptively perceived by the model. As result, we can leverage Lf use to conduct subject-aware masked attention in MMDiT [48]. Following the similar way in [40, 43], the masking strategy adheres to the following principle: For subject i, we only allow attention interactions between its corresponding video tokens within the layout Lf use and the text tokens associated with subject i, ensuring precise subject binding and inter-frame consistency. For video tokens outside the layout regions and text tokens unrelated to the subjects, we enable unrestricted interactions with all tokens to ensure contextual coherence (see Appendix 1 for details). Furthermore, to explicitly enforce alignment between generated subjects and their predefined layouts Lf use, we introduce localized independent noise sampling strategy, which conducts initial noise sampling independently within each subjects designated layout region. As shown in Figure 4, DLFA captures shape dynamic of the subject, thereby enabling more flexible attributelocation binding compared to static prior layout masks. 4. Experiment Corri = Qtext video . (4) where Qtext indicates the pooled embedding for the ith subject token. Afterwards, the problem lies in how to determine an appropriate correlation threshold to define the model-perceived layout. Fortunately, the prior layouts Lprior already includes approximate information about object sizes, which serve as size references for deriving The setups are described in Section 4.1. Quantitative and qualitative comparisons are provided in Sections 4.2 and 4.3, respectively. Ablation studies are discussed in Section 4.4. Various applications are showcased in Section 4.5. 4.1. Experimental Setups Implementation Details. We build upon the diffusers for video generation and develop MagicComp using two mainstream models: CogVideoX [48], which adopts DIT-based Model T2V-CompBench VBench Consist-attr Spatial Motion Action Interaction Numeracy Multi-obj Spatial-rela T2V model: ModelScope [41] ZeroScope [2] Latte [30] VideoCrafter2 [8] Open-Sora 1.2 [54] Open-Sora-Plan v1.1.0 [27] AnimateDiff [14] Pika [35] Gen-3 [5] Dream Machine [4] Compositional T2V model: LVD [26] VideoTetris [40] DreamRunner [43] Vico [47] VideoRepair [24] CogVideoX-2B [48] CogvideoX-2B + Ours 0.5483 0.4495 0.5325 0.6750 0.6600 0.7413 0.4883 0.6513 0.7045 0.6900 0.5595 0.7125 0.7350 0.6980 0.7475 0.6775 0.7665 0.4220 0.4073 0.4476 0.4891 0.5406 0.5587 0.3883 0.5043 0.5533 0.5397 0.5469 0.5148 0.6040 0.5432 0.6000 0.4848 0.6012 0.2662 0.2319 0.2187 0.2233 0.2388 0.2187 0.2236 0.2221 0.3111 0.2713 0.2699 0.2204 0.2608 0.2412 - 0.2379 0. 0.4880 0.4620 0.5200 0.5800 0.5717 0.6780 0.4140 0.5380 0.6280 0.6400 0.4960 0.5280 0.5840 0.6020 - 0.5700 0.6140 0.7075 0.5550 0.6625 0.7600 0.7400 0.7275 0.6550 0.6625 0.7900 0.7725 0.6100 0.7600 0.8225 0.7800 - 0.7250 0.8025 0.2066 0.2378 0.2187 0.2041 0.2556 0.2928 0.0884 0.2613 0.2169 0.2109 0.0991 0.2609 - - 0.2931 0.2568 0. 0.3898 - 0.3453 0.4066 0.5183 0.4035 0.3831 0.4308 0.5364 - - - - 0.6321 - 0.6263 0.6659 0.3409 - 0.4153 0.3586 0.6856 0.5311 0.4428 0.6103 0.6509 - - - - - - 0.6990 0.7325 Table 1. Evaluation results on T2V-CompBench and VBench. T2V-Comp includes six evaluation metrics, while VBench introduces two new metrics: Multi-obj and Spatial-rela. Best/2nd best scores are bolded/underlined. indicates the commercial models. Model CogvideoX-2B w/ SAD w/ DLFA w/ DLFA & SAD VideoCrafter2 w/ SAD w/ DLFA w/ DLFA & SAD Consist-attr 0.6775 0.7225 0.7380 0.7665 0.6750 0.7020 0.7125 0.7203 Spatial 0.4848 0.5353 0.5734 0.6012 0.4891 0.5132 0.5349 0.5435 Interaction Numeracy Time 75s 77s 85s 87s 14s 15s 16s 17s 0.7250 0.7575 0.7800 0.8025 0.7600 0.7625 0.7850 0.7900 0.2568 0.2845 0.2964 0.3079 0.2041 0.2294 0.2463 0.2553 Table 2. Ablation study of SAD and DLFA modules. Removing any of these modules significantly reduces model performance. Model CogvideoX-2B Consist-attr 0.6775 Spatial 0.4848 w/ 3D Region Attention w/ DLFA w/ Word Swap w/ SAD 0.7248 0. 0.6995 0.7225 0.5632 0.5734 0.4767 0.5353 Interaction Numeracy 0.7250 0.7625 0. 0.7275 0.7575 0.2568 0.2878 0.2964 0.2463 0.2845 Table 3. Ablation study on incorporating other approaches on CogvideoX-2B. Replacing SAD and DLFA with other naive strategies leads to performance drop. architecture, and VideoCrafter2 [8], which utilizes UNetbased design. All videos are generated on an A100 GPU. Evaluated Models. We compare our approach against two groups of baseline models: text-to-video (T2V) models and compositional T2V models. The T2V base models include ModelScope [41], ZeroScope [2], Latte [30], VideoCrafter2 [8], Open-Sora 1.2 [54], Open-Sora-Plan v1.1.0 [27] and AnimateDiff [14], as well as the commercially available closed-source models Pika [1], Gen-3 [5] and Dream Machine [4]. The compositional T2V models include LVD [26], VideoTetris [40], DreamRunner [43], Vico [47], and VideoRepair [24]. Benchmark and Evaluation Metrics. We employ two widely-used T2V generation benchmarks, T2VCompBench [39] and VBench [20], for evaluation. These benchmarks comprehensively assess the alignment between text prompts and generated videos across various scenarios. For T2V-CompBench, we select six categories to examine compositional abilities in text-to-video generation: consistent attribute binding, spatial relationships, motion binding, action binding, object interactions, and generative numeracy, with each category containing 100 prompts. For VBench, two evaluation aspects are utilized: multiple objects and spatial relationships. 4.2. Quantitative Comparisons We conduct comprehensive quantitative evaluation of different methods, and the results are presented in Table 1. Our method demonstrates outstanding performance across multiple evaluation metrics. Specifically, MagicComp achieves the highest scores in Consist-attr, Motion, Numeracy, Multi-obj, and Spatial-rela, outperforming all other models in these categories. Among T2V models, Open-Sora-Plan v1.1.0 [27] and Dream Machine [4] obtain the highest Action scores, while Gen-3 [5] performs Figure 5. Qualitative Comparison on T2V-CompBench. Our MagicComp significantly outperforms existing approaches across various compositional generation tasks, and the methods such as Vico [47] and CogVideoX [48] struggle to capture fine-grained concepts. well in Motion but still falls behind our model. For compositional T2V models, although DreamRunner [43] excels in Spatial and Interaction, and VideoRepair [28] achieves the best Consist-attr score, while Vico [47] demonstrates strong Multi-obj performance, these models require substantial computational resources to generate videos. In contrast, MagicComp is low-cost, zero-shot approach that effectively captures fine-grained object relationships, understands numerical dependencies, and preserves consistent compositional attributes. These results verify the effectiveness of our method in compositional T2V generation. 4.3. Qualitative Comparisons This section qualitatively compares the generation quality across consistency attributes, spatial relations, motion, action, interaction dynamics, and numerical understanding among Open-Sora-Plan [27], VideoTetris [40], Vico [47], CogVideoX-2B [48], and our proposed MagicComp. Specifically, we select representative prompts from the T2V-CompBench for analysis, as shown in Figure 5. OpenSora-Plan, VideoTeris, Vico, and CogVideoX-2B struggle to maintain consistent object attributes and spatial relationships, resulting in inaccurate visual representations and compromised temporal coherence. For instance, in Figure 7. Application about trajectory-controllable video generation. By incorporating the proposed methods, CogVideoX [48] can achieve trajectory control seamlessly without additional cost. lar experiments using the VideoCrafter2 model, where consistent performance improvements are observed after introducing SAD and DLFA. This further validates the generalizability of our proposed modules across diverse model architectures. More importantly, adding SAD and DLFA to CogVideoX and VideoCrafter2 increases inference time by only 16% and 21.4%, respectively, demonstrating the efficiency of the proposed methods. Moreover, we conduct another ablation study on CogVideoX-2B (see Table 3) to assess the advantage of the proposed methods over other naive strategies. Comparing with 3D Region Attention [43], we find that DLFA outperforms 3D Region Attention across all evaluation metrics. Moreover, directly replacing subject embeddings with semantic anchors (i.e., Word Swap) causes significant performance drop, proving the effectiveness of SAD in dynamically integrating semantic directional vectors into the original embeddings. Collectively, these results confirm the effectiveness of the proposed SAD and DLFA modules. 4.5. Applications We further demonstrate the practical applicability of MagicComp in more complex and challenging scenarios. Figure 6 showcases its effectiveness in complex prompt-driven video generation, successfully interpreting sophisticated textual description involving multiple intricate elements, such as skeleton pirate, ghost ship, dark sea, glowing lanterns, and treasure map. Additionally, Figure 7 highlights MagicComps capability in trajectory-controllable video generation, effectively guiding object movements along specified paths while significantly outperforming existing methods. These results further highlight MagicComps potential for broader range of video generation tasks. 5. Conclusion In this paper, we introduce MagicComp, novel trainingfree approach designed to address critical limitations in compositional text-to-video (T2V) generation, particularly in attribute binding, spatial relationship reasoning, and multi-object consistency. Our Semantic Anchor Disambiguation (SAD) and Dynamic Layout Fusion Attention (DLFA) modules effectively refine semantic interpretation Figure 6. Application on complex prompt-based video generation. It is evident that among all models, only MagicComp strictly follows the prompt to generate complex scenarios. depicting spatial relations (e.g., \"A book on the left of bird\"), Vico and VideoTeris show limited spatial comprehension, whereas MagicComp demonstrates clear and acIn action and intercurate scene layout understanding. action dynamics, MagicComp outperforms all comparative methods by capturing and maintaining coherent actions throughout the generated frames. Moreover, in evaluating numeracy prompts, MagicComp distinctly showcases improved numerical comprehension and precise scene composition. These qualitative findings demonstrate that MagicComp consistently surpasses existing methods, including those trained extensively, highlighting the effectiveness of our training-free framework in accurately modeling various compositional and relational concepts. For additional qualitative comparisons, please refer to the Appendix. 4.4. Ablation Study We conduct ablation studies on SAD and DLFA modules to evaluate their effectiveness. As shown in Table 2, integrating SAD module into the baseline CogvideoX-2B model significantly improves all metrics. Specifically, Consistattr increases from 0.6775 to 0.7225, Spatial improves from 0.4848 to 0.5353, Interaction rises from 0.7250 to 0.7575, and Numeracy advances from 0.2568 to 0.2845. These results underscore the effectiveness of SAD in resolving semantic ambiguities. Furthermore, incorporating the DLFA module yields additional enhancements, confirming its ability to adaptively enhance spatial and interactional information. Combining SAD and DLFA modules together produces the highest overall performance, which proves their collaborative effectiveness. Furthermore, we conduct simiand enhance localized attention. Extensive experiments demonstrate that MagicComp consistently achieves superior performance over state-of-the-art methods, validating its versatility and generalization capabilities across different benchmarks, such as T2V-CompBench and VBench. Given its model-agnostic and flexible nature, MagicComp offers robust framework applicable to diverse T2V architectures, thus providing promising potential for future research directions, including complex prompt-driven and trajectorycontrollable video generation tasks."
        },
        {
            "title": "References",
            "content": "[1] Pika. https://www.pika.art/, 2023. 2, 6 [2] Zeroscope. https : / / huggingface . co / cerspense/zeroscope_v2_576w, 2023. 3, 6 [3] Dreamina. https://dreamina.capcut.com/aitool/platform, 2024. 2 [4] Luma ai. https://lumalabs.ai/dream-machine, 2024. 6 [5] Gen-3. https : / / runwayml . com / blog / introducing-gen-3-alpha/, 2024. 2, 6 [6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 3 [7] Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. Ditctrl: Exploring attention control in multi-modal diffusion transformer for tuning-free multi-prompt longer video generation. arXiv preprint arXiv:2412.18597, 2024. 3 [8] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024. 2, 3, 6 [9] Liuhan Chen, Zongjian Li, Bin Lin, Bin Zhu, Qian Wang, Shenghai Yuan, Xing Zhou, Xinhua Cheng, and Li Yuan. Od-vae: An omni-dimensional video compressor for imarXiv preprint proving latent video diffusion model. arXiv:2409.01199, 2024. 3 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 2 [11] Weixi Feng, Chao Liu, Sifei Liu, William Yang Wang, Arash Vahdat, and Weili Nie. Blobgen-vid: Compositional textto-video generation with blob video representations. arXiv preprint arXiv:2501.07647, 2025. [12] Weixi Feng, Chao Liu, Sifei Liu, William Yang Wang, Arash Vahdat, and Weili Nie. Blobgen-vid: compositional textto-video generation with blob video representations. arXiv preprint arXiv:2501.07647, 2025. 2, 5 [13] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024. 2 [14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textIn The to-image diffusion models without specific tuning. Twelfth International Conference on Learning Representations, 2024. 3, 6 [15] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime arXiv preprint arXiv:2501.00103, video latent diffusion. 2024. 2, 3 [16] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2 [17] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [18] Taihang Hu, Linxuan Li, Joost van de Weijer, Hongcheng Gao, Fahad Shahbaz Khan, Jian Yang, Ming-Ming Cheng, Kai Wang, and Yaxing Wang. Token merging for trainingfree semantic binding in text-to-image synthesis. Advances in Neural Information Processing Systems, 37:137646 137672, 2025. 3 [19] Kaiyi Huang, Yukun Huang, Xuefei Ning, Zinan Lin, Yu Wang, and Xihui Liu. Genmac: compositional text-to-video generation with multi-agent collaboration. arXiv preprint arXiv:2412.04440, 2024. 2, 3 [20] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 2, 6 [21] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. In ICCV, 2023. 3 [22] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2 [23] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [24] Daeun Lee, Jaehong Yoon, Jaemin Cho, and Mohit Bansal. improving text-to-video generation via misVideorepair: alignment evaluation and localized refinement. preprint arXiv:2411.15115, 2024. 2, 3, 6 arXiv sive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024. 2, 6 [25] Zongjian Li, Bin Lin, Yang Ye, Liuhan Chen, Xinhua Cheng, Shenghai Yuan, and Li Yuan. Wf-vae: Enhancing video vae by wavelet-driven energy flow for latent video diffusion model. arXiv preprint arXiv:2411.17459, 2024. 2, 3 [26] Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and In ICLR, Boyi Li. Llm-grounded video diffusion models. 2024. 2, 3, 5, 6 [27] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 2, 3, 6, 7 [28] Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal. Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning. arXiv preprint arXiv:2309.15091, 2023. 3, 7 [29] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. Compositional visual generation with composable diffusion models. In European Conference on Computer Vision, pages 423439. Springer, 2022. 3 [30] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 6 [31] Weili Nie, Sifei Liu, Morteza Mardani, Chao Liu, Benjamin Eckart, and Arash Vahdat. Compositional text-to-image generation with dense blob representations. arXiv preprint arXiv:2405.08246, 2024. Video generation models as world simula- [32] OpenAI. https : / / openai . com / index / video - tors. generation - models - as - world - simulators/, 2023. Accessed: 2024-2. 2, 3 [33] OpenAI. Hello gpt-4o, 2024. 2 [34] OpenAI. GPT-4 technical report, 2024. 2 [35] Pika. Pika art 2.0s scene ingredients: Redefining personalized video creation, 2024. Accessed: 2025-02-26. 6 [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3, 1 [37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 3 [38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234241. Springer, 2015. [39] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: comprehen- [40] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, Di Zhang, and Bin Cui. Videotetris: towards compositional text-to-video generation. In NeurIPS, 2024. 2, 3, 5, 6, 7 [41] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3, 6 [42] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. International Journal of Computer Vision, pages 120, 2024. 2 [43] Zun Wang, Jialu Li, Han Lin, Jaehong Yoon, and Mohit Bansal. Dreamrunner: Fine-grained storytelling video generation with retrieval-augmented motion adaptation. arXiv preprint arXiv:2411.16657, 2024. 2, 5, 6, 7, 8 [44] Tianyi Wei, Dongdong Chen, Yifan Zhou, and Xingang Pan. Enhancing mmdit-based text-to-image models for similar subject generation. arXiv preprint arXiv:2411.18301, 2024. [45] Song Wen, Guian Fang, Renrui Zhang, Peng Gao, Hao Dong, and Dimitris Metaxas. Improving compositional text-to-image generation with large vision-language models. arXiv preprint arXiv:2310.06311, 2023. 3 [46] Jingyun Xue, Hongfa Wang, Qi Tian, Yue Ma, Andong Wang, Zhiyuan Zhao, Shaobo Min, Wenzhe Zhao, Kaihao Zhang, Heung-Yeung Shum, et al. Follow-your-pose v2: Multiple-condition guided character image animation for stable pose control. arXiv preprint arXiv:2406.03035, 2024. 3 [47] Xingyi Yang and Xinchao Wang. video generation as flow equalization. arXiv:2407.06182, 2024. 2, 3, 6, 7 Compositional arXiv preprint [48] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3, 5, 6, 7, [49] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. 2 [50] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440, 2024. 2, 3 [51] Shenghai Yuan, Jinfa Huang, Yujun Shi, Yongqi Xu, Ruijie Zhu, Bin Lin, Xinhua Cheng, Li Yuan, and Jiebo Luo. Magictime: Time-lapse video generation models as metamorphic simulators. arXiv preprint arXiv:2404.05014, 2024. 2, 3 [52] Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Rui-Jie Zhu, Xinhua Cheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: benchmark for metamorphic evaluation of text-to-time-lapse video generation. Advances in Neural Information Processing Systems, 37:2123621270, 2025. 2, 3 [53] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation. arXiv preprint arXiv:2407.21705, 2024. 3 [54] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "1. Subject-aware Masked Attention in DLFA 2. Adapt MagicComp on VideoCrafter2 3. More Implementation Details 4. Enhanced input prompt via LLM 5. More Qualitative Results Demonstration 6. Limitation 1 1 1 2 2 1. Subject-aware Masked Attention in DLFA We categorize the attention masks of subject-aware masked attention into two types: (1) The subject attention mask asksubject, which facilitates spatio-temporal binding of each subject. (2) The context attention mask askcontext, which enables the context of text tokens and background video tokens to interact freely with all tokens. Specifically, asksubject is divided into four sub-masks based on video-text token interactions: asksubject = (cid:20)M asksubject asksubject vv tv (cid:21) asksubject asksubject tt vt (8) where v, t, and denote video-to-video, video-to-text, text-to-video, and text-to-text attentions, respectively. For asksubject , we ensure that the visual self-attention of subject is only applied to the corresponding tokens within its layout: vv asksubject vv = (cid:88) (Lf use i=1 Lf use ). (9) where indicates the tensor outer product operations. Similarly, for the text self-attention asksubject , we only allow interactions between the internal tokens of subject: tt asksubject tt = (cid:88) (T subject subject ). (10) i=1 Subsequently, for the cross-attention operation between video and text asksubject , we only allow interactions between the video tokens in the layout region of subject and the text tokens belonging to the same subject: vt asksubject vt = (cid:88) i= (Lf use subject ). (11) the"
        },
        {
            "title": "And\nM asksubject",
            "content": "cross-attention can be obtained in similar way: between text tv and video"
        },
        {
            "title": "M asksubject",
            "content": "tv = (cid:88) i=1 (T subject Lf use ). (12) After establishing the subject attention mask asksubject, the context attention mask askcontext, we construct which allows video tokens outside the layout regions and text tokens not belonging to the subjects to interact freely with all tokens, thereby preserving global contextual coherence. Finally, we can achieve subject-aware masked attention by injecting asksubject and askcontext into the standard self-attention mechanism: askf use = asksubject + askcontext Subject-Attn = Softmax askf use(cid:17) (cid:16) QKT (13) V. 2. Adapt MagicComp on VideoCrafter2 MagicComp can be seamlessly integrated into U-Net-based models, such as VideoCrafter2, with minimal adaptation effort. Specifically, SAD module can be directly added to the conditioning stage of VideoCrafter2, reducing semantic confusion in CLIP text embeddings [36]. For DLFA module, we utilize cross-attention to generate model-adaptive perception layout masks. After obtaining the fused layout masks, we design the attention mechanism to ensure that, within each frame, video tokens inside subject layout region attend only to other video tokens in the same region via self-attention and interact exclusively with their corresponding subject text tokens through cross-attention. Video tokens outside the layout regions adhere to the original rules of VideoCrafter2. 3. More Implementation Details The hyperparameters of both SAD and DLFA modules remain consistent across different model architectures. For SAD module, we cache the anchor embeddings of each subject during the conditioning stage, and only perform interpolation operations in the subsequent denoising steps, which incurs negligible additional inference overhead. For DLFA module, masked attention modulation is applied during the first 10% of the denoising process to guide the generation of global attributes (e.g., subject shapes and locations) in the early stages. For other details, please refer to Table 1. Hyperparameters CogVideoX VideoCrafter2 Sampler Denoising step Guidance scale Resolution Number of frames DPM-Solver 50 6 720 480 49 DPM-Solver 50 12 512 320 16 Table 1. More implementation details. 4. Enhanced input prompt via LLM We utilize ChatGLM-4 [13] to generate coarse prior layouts for each textual prompt and extract the specified subjects. For detailed generation prompts and the corresponding data format, please refer to Figure 1. 5. More Qualitative Results Demonstration We provide additional qualitative demonstrations highlighting the superior performance of our MagicComp across multiple critical attributes, including attribute consistency (Figures 2 and 3), motion binding and action (Figure 4 and 5), and numeracy (Figure 6), as well as specific comparisons with VideoCrafter2 in Figure 7. MagicComp consistently generates visually coherent and contextually accurate videos, clearly outperforming comparative methods on the T2V-Compbench dataset. 6. Limitation While MagicComp significantly enhances attribute binding, spatial reasoning, and multi-object consistency in compositional video generation, its performance remains constrained by the inherent limitations of the underlying T2V architectures. Specifically, the framework may struggle with highly complex or semantically ambiguous promptsparticularly those requiring precise trajectory specifications or frame-level temporal control. Furthermore, despite being model-agnostic by design, MagicComps effectiveness is contingent on the pretrained backbone models semantic grounding and representation capabilities. Future research could further refine these aspects by exploring more sophisticated disambiguation techniques and enhanced temporal modeling strategies. Figure 1. Instruction prompt for prior layout generation. Figure 2. Qualitative results on Consist-attr. Figure 3. Qualitative results on Consist-attr. Figure 4. Qualitative results on Motion. Figure 5. Qualitative results on Action & Motion. Figure 6. Qualitative results on Numeracy. Figure 7. Qualitative results on VideoCrafter2."
        }
    ],
    "affiliations": [
        "Peng Cheng Laboratory, Shenzhen, China",
        "School of Electronic and Computer Engineering, Peking University, Shenzhen, China",
        "Tsinghua University, Beijing, China"
    ]
}