{
    "paper_title": "Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting",
    "authors": [
        "Duochao Shi",
        "Weijie Wang",
        "Donny Y. Chen",
        "Zeyu Zhang",
        "Jia-Wang Bian",
        "Bohan Zhuang",
        "Chunhua Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinuities at object boundaries often lead to fragmented or sparse point clouds, degrading rendering quality -- a well-known limitation of depth-based representations. To tackle this issue, we introduce PM-Loss, a novel regularization loss based on a pointmap predicted by a pre-trained transformer. Although the pointmap itself may be less accurate than the depth map, it effectively enforces geometric smoothness, especially around object boundaries. With the improved depth map, our method significantly improves the feed-forward 3DGS across various architectures and scenes, delivering consistently better rendering results. Our project page: https://aim-uofa.github.io/PMLoss"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 7 2 3 5 0 . 6 0 5 2 : r Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting Duochao Shi1, Weijie Wang1, 4, Donny Y. Chen2, Zeyu Zhang2, 4, Jia-Wang Bian3, Bohan Zhuang1, Chunhua Shen1 1Zhejiang University, China 2Monash University, Australia 3MBZUAI 4GigaAI"
        },
        {
            "title": "Abstract",
            "content": "Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinuities at object boundaries often lead to fragmented or sparse point clouds, degrading rendering qualitya well-known limitation of depth-based representations. To tackle this issue, we introduce PM-Loss, novel regularization loss based on pointmap predicted by pre-trained transformer. Although the pointmap itself may be less accurate than the depth map, it effectively enforces geometric smoothness, especially around object boundaries. With the improved depth map, our method significantly improves the feed-forward 3DGS across various architectures and scenes, delivering consistently better rendering results. Our project page: https://aim-uofa.github.io/PMLoss"
        },
        {
            "title": "Introduction",
            "content": "Novel view synthesis (NVS) is long-standing topic in computer vision and graphics, recently drawing increasing attention due to advances in neural rendering, particularly 3D Gaussian Splatting (3DGS) [1]. While NVS models take 2D images as inputs and outputs, their primary goal is to recover the underlying 3D scene structure. Hence, smooth and accurate geometry is essential for generating high-quality novel views. This has led to series of research efforts aimed at enhancing visual quality by learning more precise and consistent geometric representations [2, 3, 4, 5, 6]. Although 3DGS models have ultra-fast rendering speed, reconstructing them for unseen scenes requires time-consuming per-scene optimization process, limiting their usability in real-world applications. This challenge has led to the development of feed-forward 3DGS methods [7, 8], the main focus of our work. Unlike per-scene tuning methods that improve visual quality by learning better geometry, feed-forward 3DGS models typically fall short in geometric quality, despite significant progress aimed at enhancing appearance [9, 10, 11, 12, 13, 14]. The core issue lies in the representation used by feed-forward methods, which rely on depth maps. Most feed-forward models predict depth maps and then unproject them to form 3D Gaussians. Since depth maps often contain discontinuities near object boundaries [15, 16], directly unprojecting them transfers these artifacts to the 3D representation, resulting in degraded geometry quality. Recently, 3D reconstruction has been dominated by new line of research that adopts representation known as the pointmap [17]. Unlike depth maps, which represent scalar value R1 in camera space, pointmaps encode set of 3D points R3 in world space, allowing for smoother and more accurate modeling of geometry. In addition, pointmaps simplify the traditional multi-view stereo 1Equal contribution. Preprint. Under review. Figure 1: Feed-forward 3DGS models, e.g., DepthSplat [9], rely on unprojected depth to form 3D Gaussians. The inherent discontinuities of depth near object boundaries often propagate into distorted 3D point clouds (top left) and degraded rendering (bottom left). Our PM-Loss addresses this by using the prior from pointmap, achieving higher-quality geometry (top right) and rendering (bottom right). (MVS) [18, 19] process by reformulating it as direct regression task through neural network. These advantageous properties have contributed to the success of many recent feed-forward approaches to 3D reconstruction [20, 21, 22, 23, 24, 25]. The success of pointmaps in accurate and regression-based 3D reconstruction motivates us to introduce them as strong prior to reduce artifacts in depth map based feed-forward 3DGS. This is not straightforward, as pointmaps implicitly encode coarse camera poses [17], while feed-forward 3DGS performs best with explicitly provided accurate poses [7, 8, 11], making it challenging to leverage the geometry prior effectively. Existing methods that adopt pointmap priors in feed-forward 3DGS often assume pose-free setting [26, 27]. While this avoids the pose issue by ignoring it, novel view evaluation either relies on testing with specific dataset already used by the pointmap model (e.g., ScanNet [28] in Splatt3R [26]) or requires slow test time pose alignment (e.g., NoPoSplat [27]), both of which hinder real world usability. Although one might inject camera poses into pointmap-based feed-forward models using, for example, Plücker ray embeddings, this approach is suboptimal as it requires expensive retraining to realign the pose distribution implicitly embedded in pointmaps and does not enhance the quality of scene details. We propose novel method to transfer the geometry prior from pointmap regression models to feedforward 3DGS by formulating it as simple yet effective training loss. Unlike prior methods [26, 27] that attach an additional Gaussian head to the pointmap backbone, introducing the pose dilemma and requiring customization for each specific model, our approach is plug-and-play and avoids pose issues entirely. In particular, our PM-Loss guides the learning of point clouds unprojected from predicted depth by taking the global pointmap predicted by large-scale 3D reconstruction model, e.g., Fast3R [20], VGGT [23], as pseudo-ground truth. This guidance requires that the source and target points are in the same space and that efficient measurements are available. For the former, we find that the Umeyama algorithm can efficiently align the two point clouds (see Tab. 5), leveraging the one-to-one correspondence between depth maps and pointmaps. For the latter, the Chamfer loss is used to directly regularize them in 3D space, leading to significantly better geometry quality than those applied in 2D space (see Tab. 3). By distilling the geometry prior embedded in the pointmap predicted by pre-trained 3D reconstruction model, our method helps ease discontinuities caused by unprojected depth and significantly boosts the quality of predicted 3D point clouds and rendered novel views for feed-forward 3DGS modelsSee Fig. 1. To verify the effectiveness of our PM-Loss, we apply it to train two representative feed-forward 3DGS models, MVSplat [8] and DepthSplat [9], on two large-scale benchmarks, RealEstate10K [29] and DL3DV-10K [30]. Experiments demonstrate that our PM-Loss improves both the quality of the 3D Gaussians and the rendered novel views across all reported metrics. Extensive ablation studies and analysis further validate architectural design choices, as well as efficient memory and runtime usage of our PM-Loss. Given its plug-and-play, efficient, and effective nature, we believe that PM-Loss will play an important role in training feed-forward 3DGS in the future. Our contributions are threefold, 2 We pinpoint an unexposed yet critical issue that leads to lower-quality 3D Gaussians predicted by feed-forward 3DGS models, rooted in the long-standing discontinuity issue of depth. We introduce novel training loss, PM-Loss, designed to improve 3D Gaussian quality by leveraging the geometry prior from pointmaps obtained from pre-trained 3D reconstruction models. Extensive experiments on existing feed-forward 3DGS models across two large-scale datasets demonstrate the effectiveness of our PM-Loss in enhancing the quality of both 3D Gaussians and rendered novel views."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 3D Gaussian Splatting Novel view synthesis (NVS) techniques have evolved significantly, transitioning from traditional image-based methods [32, 31] to modern neural rendering approaches [1, 33]. Early neural methods, such as NeRF [33], represent scenes implicitly. While they can achieve high-fidelity results, they are typically hampered by slow rendering speeds and the requirement for dense input views. More recently, explicit representations like 3D Gaussian Splatting (3DGS) [1] and its subsequent variants[2, 3, 4, 5, 6] have emerged, offering significantly faster rendering speeds due to their rasterizationfriendly nature. Several recent works have explored leveraging geometric priors to optimize the 3D Gaussian representation. Specifically, some methods [34, 35] incorporate depth information as prior for optimizing the geometry of the Gaussians. However, common limitation of these approaches is their reliance on monocular depth estimation models [36, 37]. Such monocular priors, especially when estimated independently per-image, often suffer from inconsistencies and multi-view misalignment, hindering geometric accuracy. Our approach mitigates this multi-view inconsistency by leveraging supervision from multi-view consistent 3D point prior derived from pretrained models. 2.2 Feed-forward 3D Gaussian Splatting Moving beyond per-scene optimization, pixelSplat [7] presented pioneering feed-forward approach for 3DGS, predicting Gaussian parameters directly from two input views with help of epipolar transformers. Subsequently, MVSplat [8] improved the efficiency of feed-forward 3DGS by proposing cost-volume based feature fusion method, enhancing its practicality. DepthSplat [9] further explored incorporating depth priors [36, 37] into the feed-forward 3DGS framework, aiming to improve the geometric accuracy of the predictions. Same as the previous methods [13, 34, 35, 38, 39, 40, 41, 42, 43], these feed-forward methods are often challenged by the difficulty in ensuring accurate multi-view consistency during feature processing or prior fusion, leading to geometric inaccuracies in the generated 3D Gaussian representation. 2.3 3D Reconstruction using Pointmaps Recent advancements in 3D pointmap reconstruction methods [17, 21, 23, 25, 44, 45, 46] that produce highly accurate 3D point clouds have gained significant attention. Representative works in this area include DUSt3R [17], which utilizes large Transformer-based model for robust multi-view feature fusion to generate dense 3D points. Building upon DUSt3R [17], MV-DUST3R [21] further extends its capability to handle an arbitrary number of input views by facilitating information exchange across them, typically considering one as reference view. Fast3R [20] focuses on highly efficient reconstruction, demonstrating the ability to process over 1000 images in single forward pass. VGGT [23] infers key 3D attributes of scene by combining features from models like DINO and DPT. While these methods excel in geometric reconstruction accuracy, they are typically not designed for direct novel view synthesis and often incur significant training costs. Our method, PM-Loss, aims to bridge the gap by combining the strengths of efficient feed-forward 3DGS networks and the accurate geometric priors provided by point-map-based large models, resulting in improved geometric quality for feed-forward NVS. 3 Figure 2: Overview of PM-Loss. The process begins by estimating dense point map of the scene using pre-trained model. This estimated point map then serves as direct 3D supervision for training feed-forward 3D Gaussian Splatting model. Crucially, unlike conventional methods relying predominantly on 2D supervision, our approach leverages explicit 3D geometric cues, leading to enhanced 3D shape fidelity."
        },
        {
            "title": "3 Methodology",
            "content": "Our goal is to train neural network that directly predicts 3D Gaussian Splatting (3DGS) model from one or more input images for novel view synthesis, eliminating the need for per-scene optimization. To enhance the quality of the predicted 3D Gaussians, we introduce novel PointMap Loss (PMLoss) that regularizes the predicted 3D structure. PM-Loss leverages pointmapsstructured 2Dto-3D representations regressed from input images using pretrained Vision Transformer[47]to provide image-aligned supervision for geometry learning. We begin by introducing the necessary preliminaries, followed by detailed description of our PM-Loss design. 3.1 Preliminary Feed-forward 3D Gaussian Splatting. The method aims to reconstruct set of 3D Gaussians directly from one or several input images in single forward pass. The general architecture involves an encoder-decoder structure. First, an encoder network processes the input image(s) to extract high-level features : = Encoder(I) (1) These features are then typically fused with camera pose information Pcam and potentially other supplementary information Saux through fusion module, Fuse(). subsequent Gaussian head network, Ghead(), then predicts the parameters for collection of 3D Gaussians. These parameters include the mean (center) µ R3, covariance Σ R33 (often represented by scale and rotation), opacity α R, and color R3 (or spherical harmonic coefficients) for each Gaussian: ({µi, Σi, αi, ci}N i=1) = Ghead(Fuse(F, Pcam, Saux)) (2) In typical feed-forward 3DGS pipelines, the Gaussian means µi are derived by unprojecting predicted depth maps. Specifically, for each pixel (u, v) in an input image, depth value is predicted. This depth, along with the camera intrinsic matrix and camera-to-world transformation matrix Tcw = [Rexttext], is used to compute the 3D position of the corresponding Gaussian center µuv: pcam(u, v) = d(u, v) 1 (cid:33) (cid:32)u 1 µuv = Rext pcam(u, v) + text (3) (4) While this approach is efficient, it often suffers from geometric inaccuracies. Depth maps inherently struggle with discontinuities, particularly around object boundaries. These discontinuities, when unprojected, lead to fragmented or misplaced Gaussians, thereby degrading the geometric quality of the 3D scene representation and subsequently impacting the novel view synthesis quality. 4 Pointmap Regression. pointmap is structured 3D representation in which each pixel (u, v) of an uv R3 in world coordinates. Unlike depth maps, input 2D image is associated with 3D point which provide only per-pixel Z-values, pointmaps directly represent full 3D coordinates (XYZ). They are typically regressed from images in feed-forward manner using pretrained deep neural networks, often based on Vision Transformer (ViT) [47] architectures. Let Rpm denote such pointmap regression model. For each of the nimg input images Ij (with resolution ) and its camera pose Pcam,j, Rpm outputs set of 3D points: {p j,u,v R3 (u, v) are pixel coordinates in Ij} = Rpm(Ij, Pcam,j) (5) , where Ntotal_pts = nimg . This provides dense 3D geometric prior that is These per-image pointmaps are aggregated to form the global reference point cloud XPM = {p R3}Ntotal_pts k=1 leveraged in our PM-Loss. 3.2 PM-Loss To address geometric inaccuracies in feed-forward 3DGS, existing methods such as DepthSplat [9] incorporate monocular depth priors. However, these priors are typically derived and supervised in 2D image space, which may not translate effectively into consistent 3D geometry. Instead, we advocate for directly regularizing geometry learning in 3D space. Given batch of nimg input images, each of resolution , the feed-forward 3DGS model aims to directly predict set of 3D Gaussian centers. We denote the collection of these predicted centers as X3DGS, containing total of Ntotal_pts = nimg points, where each point µk R3 represents the center of 3D Gaussian in world coordinates. To guide the learning of accurate and consistent geometry, we introduce 3D supervision signal R3 for derived from pretrained pointmap regression model. This model predicts 3D point each pixel, resulting in reference point cloud XPM of the same cardinality. Formally, we define the predicted and reference point sets as: k=1 , XPM = {p X3DGS = {µk R3}Ntotal_pts (6) where Ntotal_pts = nimg . Two point clouds, X3DGS and XPM, exhibit natural one-to-one correspondence, as each µk and originate from the same pixel (i, j) of specific input image within the batch. While the absolute accuracy of XPM might be less than that achievable via finelytuned depth map unprojection in well-textured regions, the pointmaps tend to exhibit better geometric smoothness and completeness, especially near object boundaries. We leverage this smoothness prior as form of 3D supervision to guide the feed-forward 3DGS model toward learning more coherent and consistent geometry. R3}Ntotal_pts k=1 Efficient point cloud alignment. Although both X3DGS and XPM represent the scenes 3D structure in world coordinates, directly supervising X3DGS using XPM is non-trivial. In practice, the two point clouds are often misaligned due to differences in scale, rotation, or translationstemming from pose inaccuracies or implicit coordinate systems used by the pretrained model generating XPM. Without addressing these discrepancies, point-wise supervision can introduce misleading gradients. Therefore, accurate alignment is crucial for effectively distilling the geometric prior from XPM into X3DGS. Traditional alignment methods such as Iterative Closest Point (ICP)[48] are computationally expensive, particularly for dense point clouds, making them impractical for integration into the training loop. However, in our case, both the Gaussian centers X3DGS (from per-pixel depth predictions) and the pointmap outputs XPM exhibit one-to-one correspondence with input image pixels. Specifically, for each pixel (u, v), there exists matched pair µuv X3DGS and uv XPM. Such natural correspondences allow us to apply the Umeyama algorithm [49], closed-form and efficient solution for estimating the optimal similarity transformation (scale, rotation, and translation) between the two point sets. Given Ntotal_pts corresponding points XPM = {p (target), the Umeyama algorithm estimates the optimal scale factor R+, rotation matrix SO(3), and translation vector R3 by solving the following minimization problem: (source) and X3DGS = {µk}Ntotal_pts k}Ntotal_pts k=1 k=1 (s, R, t) = argmin s,R,t 1 Ntotal_pts Ntotal_pts (cid:88) k=1 sRp + µk2 (7) 5 Figure 3: Qualitative comparisons on DL3DV(top two rows) and RealEstate10K(bottom two rows) under the 2-view extrapolation setting. Adding PM-Loss leads to significant improvements in rendering object boundaries. The estimated transformation (s, R, t) is then applied to each point XPM to obtain the aligned pointmap compute the proposed supervision loss in consistent coordinate frame. Single-directional Chamfer Loss. Given the aligned point clouds X3DGS and PM-Loss LPM as single-directional Chamfer distance from X3DGS to that, for each point in X3DGS, we can efficiently identify its nearest neighbor in reliable geometric supervision. Formally, the loss is defined as: + t}Ntotal_pts PM = {sRp k=1 PM, we define the PM. This formulation ensures PM to provide in the original pointmap . This alignment enables us to LPM(X3DGS, PM) = 1 Ntotal_pts (cid:88) µX3DGS min pX PM µ p2 2 (8) This formulation effectively acts as regularisation term, penalizing deviations of the predicted Gaussian centers from the geometry prior suggested by the pointmap. We adopt simple mean squared error (MSE) averaged over all 3D Gaussian centers, which promotes stable training and ensures smooth gradient propagation. key insight of our proposed PM-Loss is to re-compute the nearest neighbors in 3D space for supervision, rather than directly relying on the natural one-to-one pixel correspondence, which degenerates to depth loss. This design makes the supervision more robust to pose misalignments and prediction noise. We conduct ablation studies and report the quantitative results in Table 3."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings Datasets. We evaluated our method using three datasets: DL3DV, RealEstate10K, and DTU. DL3DV [30] is challenging large-scale collection comprising 10,510 real-world scenes. Following [9], we used the DL3DV-Benchmark (140 scenes) for Novel View Synthesis (NVS) testing and the remaining DL3DV scenes for training. RealEstate10K[29], comprising camera trajectories from 6 Table 1: Quantitative comparisons on DL3DV under View Extrapolation. Both MVSplat[8] and DepthSplat[9] show better rendering quality with the addition of PM-Loss. Method DL3DV [30] RealEstate10K [29] PSNR SSIM LPIPS PSNR SSIM LPIPS 18.46 DepthSplat DepthSplat+PM 20.77 MVSplat 16.79 MVSplat+PM 19.25 0.689 0.705 0.592 0. 0.261 0.245 0.322 0.291 20.43 22.48 19.52 22.18 0.788 0.814 0.757 0. 0.218 0.194 0.231 0.199 Table 2: Quantitative comparison on DTU with varying input numbers. Adding PM-Loss consistently improves geometry across different numbers of input views. Input Method Acc Comp Overall Mean Med. Mean Med. Mean Med. 2-view DepthSplat 0.264 0.101 0.182 0.200 0.051 0.125 DepthSplat+PM 0.232 0.099 0.165 0.166 0.045 0. 4-view DepthSplat 0.169 0.066 0.123 0.117 0.022 0.051 DepthSplat+PM 0.156 0.069 0.113 0.076 0.022 0.049 6-view DepthSplat 0.162 0.048 0.105 0.070 0.017 0.044 DepthSplat+PM 0.150 0.053 0.102 0.068 0.016 0. 80,000 YouTube video clips (10M frames), is split into 67,477 training and 7,289 testing scenes. We used its test split for NVS evaluation, excluding scenes with too few views. DTU [50] features 128 scenes from controlled lab environments with ground truth models from structured light scanner and corresponding depth maps. We assessed Gaussian splat geometric quality on 16 of these scenes, following [51] and [18]. Baselines. To evaluate our proposed method, we apply it to two representative feed-forward 3D Gaussian Splatting (3DGS) models: MVSplat [8] and DepthSplat [9]. Our experiments compare the performance of models fine-tuned with PM-Loss against that of the same models fine-tuned with their original training objectives, ensuring both are evaluated in fine-tuned state. For fair comparison under otherwise identical conditions, both sets of models started from public pre-trained weights and were subsequently fine-tuned on the DL3DV dataset using the same batch size and total number of training iterations. Metrics. For Novel View Synthesis (NVS) evaluation, we focused on extrapolation scenarios to assess improvements in geometric quality, particularly at object edges. In these tests, two images served as context views. The target view was selected from outside the spatial region covered by these context views. This setup often results in target views near scene or object boundaries, which are challenging and help to demonstrate enhancements in edge geometry and rendering quality when applying our PM-Loss. Table 1 presents the results of these extrapolation tests. In addition to NVS, to quantitatively assess the geometric quality of the generated 3D Gaussians, we treat the centers (µ) of all 3D Gaussians as point cloud and compare this representation against the ground truth (GT) point clouds provided by DTU. We use three standard point cloud metrics: Accuracy (Acc), Completeness (Comp), and the Overall Chamfer Distance (Overall), all where lower values indicate better performance. For each of these metrics, we report both the Mean and Median (Med.) values, see in 2. Implementation Details. Our method was implemented in PyTorch. The Chamfer distance computation within our proposed PM-Loss utilizes the implementation from PyTorch3D [52]. All experiments were conducted on single NVIDIA A100 GPU. We used the AdamW optimizer, fine-tuning each model variant (i.e., original base models and PM-Loss enhanced models) for 100, 000 iterations with learning rate of 2 104. For the pointmap supervision component of PM-Loss, we adopted VGGT [23] to generate pseudo ground truth, specifically using its publicly available 1B parameter model. The weighting coefficient for PM-Loss, λP , was set to 0.005. Training and testing resolutions adhered to the original configurations of the base models: 256 448 for DepthSplat and 256 256 for MVSplat. Further training details are provided in the Appendix. 7 Figure 4: Qualitative comparison of unprojected 3D Gaussians on DL3DV dataset. Our method effectively constrains the 3D Gaussians, significantly reducing floating artifacts and noise near border. Figure 5: Qualitative comparison of unprojected 3D Gaussians on the DTU dataset. 4.2 Comparisons and Analysis In this section, we present detailed experimental results and analysis to verify the assumptions and effectiveness of our PM-Loss, covering rendered view quality, 3D point cloud quality, alternative design choices, memory and time efficiency of each component. Analysis of visual quality improvement. By regularizing the predicted point clouds, our PM-Loss helps achieve better 3D Gaussian quality, which in turn leads to improved rendered novel view quality. As reported in Tab. 1, it is clearly evident that our method boosts the performance of all baseline models on two large-scale datasets, with consistent gain of at least 2 dB in PSNR. We hypothesize that this improvement comes mainly from the boundary regions of the scene, as our loss is particularly effective in addressing discontinuities around the boundaries. This is further supported by the qualitative results shown in Fig. 3. Although baseline models render central regions of the scenes reasonably well, they often fail to reconstruct scene boundaries due to discontinuities rooted in the depth representation, resulting in black regions in the rendered views. In contrast, our PM-Loss provides additional regularization in 3D space by leveraging the geometry prior from the pointmap, which does not suffer from discontinuous boundaries. This enables more accurate recovery of those regions and significantly improves the visual quality of rendered views from designated viewpoints. Analysis of point cloud quality improvement. We present qualitative comparison of point cloud quality against DepthSplat on DL3DV in Fig. 4. The results support our motivation. DepthSplat Table 3: Quantitative comparison of different distance measurements. Our 3D nearest-neighbors Chamfer loss outperforms the 2D one-to-one depth loss, highlighting the benefits of 3D regularization. Distance Measurement Acc Comp Mean Med. Mean Med. Mean Med. Overall 2D one-to-one depth loss 3D nearest-neighbors chamfer loss 0.254 0.232 0.096 0.099 0.175 0.165 0.179 0.166 0.048 0.045 0.114 0. Table 4: Quantitative comparison of different pointmaps. Higher quality pointmaps, such as VGGT, notably improve PM-Losss effectiveness, though our approach performs well across different pointmap sources. pointmap DL3DV [30] RealEstate10K [29] PSNR SSIM LPIPS PSNR SSIM LPIPS w/o pointmap 18.46 20.51 Fast3R [20] 20.77 VGGT [23] 0.689 0.690 0.705 0.261 0.257 0.245 20.43 22.43 22.48 0.788 0.812 0.814 0.218 0.197 0.194 Table 5: Timing breakdown of PM-Loss. Our PM-Loss is efficient, with total time of 65.0 ms for typical operation. Component Time (ms) Alignment (Umeyama) Chamfer Distance Total 0.9 64.1 65. produces 3D Gaussians with noisy floating-point artifacts around the border, while our PM-Loss yields sharper and cleaner borders. Since DL3DV contains only real-world scenes without ground truth point clouds, we also evaluate on standard multi-view stereo benchmark that provides ground truth point clouds and depth maps. As reported in Tab. 2 and shown in Fig. 5, the quality of the point cloud in terms of accuracy, completeness, and overall scores in DTU matches our findings on DL3DV and RE10K. Additionally, since DepthSplat can be evaluated with varying numbers of input views, we perform similar tests using inputs ranging from 2 to 6 views. The consistent improvements brought by our PM-Loss further confirm its robustness and broad applicability. Impact of varying distance measurement. As stated in Sec. 3.2, one key insight of our PM-Loss is applying the regularization in 3D space, avoiding direct use of the one-to-one correspondence between pointmap and depth, which could potentially degenerate the regularization to 2D depth loss. We demonstrate the effectiveness of this design choice in Tab. 3, compared to the 2D pointmap loss used in VGGT or Fast3R (excluding the confidence-aware component, as our backbone does not predict confidence mask). Our approach of regularizing in 3D space (nearest-neighbors Chamfer loss) consistently outperforms the 2D alternative (one-to-one depth loss) across several challenging point cloud metrics. We hypothesize that this gain comes from the Chamfer loss computing distances based on nearest neighbors in world space, rather than relying on pixel-aligned correspondence in camera space. This makes it more robust to slight pose misalignments and leads to better effectiveness. Impact of varying pointmaps. Since our PM-Loss targets distilling the geometry prior knowledge from predicted point maps, one might wonder how the quality of pointmaps affects its effectiveness. We conduct experiments with alternative pointmap sources, comparing our default choice VGGT with another recent state-of-the-art counterpart, Fast3R. As reported in Tab. 4, using pointmaps from Fast3R yields slightly lower performance on both datasets, while still significantly outperforming the baseline without pointmap regularization. This suggests that our PM-Loss is not tied to the VGGT architecture, although higher quality pointmaps do improve its effectiveness. We also observe larger gain from VGGT over Fast3R on DL3DV (+0.2dB PSNR) than on RE10K (+0.05dB PSNR), indicating that our PM-Loss is particularly effective in more complex real-world scenarios. Efficiency of the default components. The extra computation cost of our PM-Loss mainly comes from two components, point cloud alignment and Chamfer loss calculation. Although we use pointmap as pseudo ground truth, it can be efficiently extracted offline (around 0.3s/scene for VGGT) and loaded through the dataloader as training input, so it does not count toward the total training cost. As reported in Tab. 5, our PM-Loss introduces only minor time overhead of about 60ms for large volume of 458,752 3D Gaussians (4 views with 256 448 resolution and pixel-aligned prediction) , making it efficient to integrate into most existing feed-forward 3DGS models. This efficiency is 9 Table 6: Computation time comparison of alignment methods. Our alignment method (Umeyama) is significantly more efficient than the commonly used ICP method. Table 7: Memory usage of PM-Loss. We report our max GPU memory usage during training DepthSplat with VGGT-1B pointmap model. Method Time (ms) Umeyama ICP 0.9 238.3 Method Max VRAM (GB) DepthSplat DepthSplat+PM 51.79 57.88 partly due to the point-to-point correspondence between the pointmap and the depth-unprojected point clouds, which allows fast alignment using Umeyama[49]. Without such an inherent correspondence, alignment would become significantly more challenging and costly, e.g., using ICP[48] would take around 250ms for the same number of 3D Gaussians (see Tab. 6). This further supports the choice of using pointmap as our geometry prior. Regarding memory, our PM-Loss increases training VRAM by 6.09GB Tab. 7. This additional VRAM is largely consumed by the VGGT-1B model during pointmap generation. Crucially, this generation is preprocessing step. It can be performed entirely offline, meaning the VGGT-1B model itself would not need to occupy VRAM during the actual training process; only the resulting pointmap data would be loaded. Furthermore, our PM-Loss is purely regularization of training time and does not introduce any additional cost during testing."
        },
        {
            "title": "5 Conclusion",
            "content": "We present PM-Loss, simple yet effective training loss that leverages geometry priors from pointmaps to improve feed-forward 3DGS. By regularizing in 3D space using global pointmaps as pseudo ground truth, PM-Loss alleviates depth-induced discontinuities near object boundaries, leading to significantly improved geometry and rendering quality. Our PM-Loss can be seamlessly integrated into existing training pipelines and introduce no inference overhead. Extensive experiments and analysis on multiple backbones and large-scale datasets demonstrate its broad applicability and efficiency. We believe PM-Loss offers practical solution for training more robust and accurate feed-forward 3DGS models. Limitation and discussion. The effectiveness of our PM-Loss is bounded by the quality of the pretrained pointmap model, as errors in the pointmap may propagate into the feed-forward 3DGS model through our loss. Leveraging stronger pointmap models from future 3D reconstruction advances is promising direction."
        },
        {
            "title": "References",
            "content": "[1] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [2] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 conference papers, pages 111, 2024. [3] Lue Fan, Yuxue Yang, Minxing Li, Hongsheng Li, and Zhaoxiang Zhang. Trim 3d gaussian splatting for accurate geometry representation. arXiv preprint arXiv:2406.07499, 2024. [4] Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, and Xiaojuan Qi. 3dgsr: Implicit surface reconstruction with 3d gaussian splatting. ACM Transactions on Graphics (TOG), 43(6):112, 2024. [5] Yaniv Wolf, Amit Bracha, and Ron Kimmel. Surface reconstruction from gaussian splatting via novel stereo views. arXiv e-prints, pages arXiv2404, 2024. [6] Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, and Bo Dai. Gsdf: 3dgs meets sdf for improved rendering and reconstruction. arXiv preprint arXiv:2403.16964, 2024. [7] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In CVPR, pages 1945719467, 2024. [8] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In ECCV, pages 370386. Springer, 2024. [9] Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, and Marc Pollefeys. Depthsplat: Connecting gaussian splatting and depth. In CVPR, 2025. [10] Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, Joao Henriques, Christian Rupprecht, and Andrea Vedaldi. Flash3d: Feed-forward generalisable 3d scene reconstruction from single image. arXiv preprint arXiv:2406.04343, 2024. [11] Chen Ziwen, Hao Tan, Kai Zhang, Sai Bi, Fujun Luan, Yicong Hong, Li Fuxin, and Zexiang Xu. Long-lrm: Long-sequence large reconstruction model for wide-coverage gaussian splats. arXiv preprint arXiv:2410.12781, 2024. [12] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In ECCV, pages 119. Springer, 2024. [13] Yunsong Wang, Tianxin Huang, Hanlin Chen, and Gim Hee Lee. Freesplat: Generalizable 3d gaussian splatting towards free view synthesis of indoor scenes. NeurIPS, 37:107326107349, 2024. [14] Chuanrui Zhang, Yingshuang Zou, Zhuoling Li, Minmin Yi, and Haoqian Wang. Transplat: Generalizable 3d gaussian splatting from sparse multi-view images with transformers. In AAAI, volume 39, pages 98699877, 2025. [15] Michael Ramamonjisoa, Yuming Du, and Vincent Lepetit. Predicting sharp and accurate occlusion boundaries in monocular depth estimation using displacement fields. In CVPR, pages 1464814657, 2020. [16] Libo Sun, Jia-Wang Bian, Huangying Zhan, Wei Yin, Ian Reid, and Chunhua Shen. Sc-depthv3: Robust self-supervised monocular depth estimation for dynamic scenes. IEEE transactions on pattern analysis and machine intelligence, 46(1):497508, 2023. [17] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, pages 2069720709, 2024. [18] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In ECCV, pages 767783, 2018. [19] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In CVPR, pages 24952504, 2020. [20] Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In CVPR, 2025. [21] Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, and Zhicheng Yan. Mv-dust3r+: Single-stage scene reconstruction from sparse views in 2 seconds. In CVPR, 2025. [22] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arXiv:2410.03825, 2024. [23] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In CVPR, 2025. [24] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking accurate monocular geometry estimation for open-domain images with optimal training supervision. In CVPR, 2025. [25] Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. arXiv preprint arXiv:2408.16061, 2024. [26] Brandon Smart, Chuanxia Zheng, Iro Laina, and Victor Adrian Prisacariu. Splatt3r: Zero-shot gaussian splatting from uncalibrated image pairs. arXiv preprint arXiv:2408.13912, 2024. [27] Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, and Songyou Peng. No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images. In ICLR, 2025. [28] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In ICCV, pages 1222, 2023. [29] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: learning view synthesis using multiplane images. ACM Transactions on Graphics (TOG), 37(4):112, 2018. [30] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In CVPR, pages 2216022169, 2024. [31] Shenchang Eric Chen and Lance Williams. View interpolation for image synthesis. In Proceedings of the 20th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 93, page 279288, New York, NY, USA, 1993. Association for Computing Machinery. [32] Steven Seitz and Charles Dyer. View morphing. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 2130, 1996. [33] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [34] Jaeyoung Chung, Jeongtaek Oh, and Kyoung Mu Lee. Depth-regularized optimization for 3d gaussian splatting in few-shot images, 2024. [35] Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. Fsgs: Real-time few-shot view synthesis using gaussian splatting. In ECCV, 2024. [36] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. [37] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. In NeurIPS, 2024. [38] Yunsong Wang, Tianxin Huang, Hanlin Chen, and Gim Hee Lee. Freesplat++: Generalizable 3d gaussian splatting for efficient indoor scene reconstruction, 2025. [39] Zhiyuan Min, Yawei Luo, Jianwen Sun, and Yi Yang. Epipolar-free 3d gaussian splatting for generalizable novel view synthesis, 2024. [40] Xin Fei, Wenzhao Zheng, Yueqi Duan, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, and Jiwen Lu. Pixelgaussian: Generalizable 3d gaussian reconstruction from arbitrary views, 2024. 12 [41] Gyeongjin Kang, Jisang Yoo, Jihyeon Park, Seungtae Nam, Hyeonsoo Im, Sangheon Shin, Sangpil Kim, and Eunbyung Park. Selfsplat: Pose-free and 3d prior-free generalizable 3d gaussian splatting. In CVPR, 2025. [42] Weijie Wang, Donny Y. Chen, Zeyu Zhang, Duochao Shi, Akide Liu, and Bohan Zhuang. Zpressor: Bottleneck-aware compression for scalable feed-forward 3dgs, 2025. [43] Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, and Jianfei Cai. Mvsplat360: Feed-forward 360 scene synthesis from sparse views. NeurIPS, 37:107064107086, 2024. [44] Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding image matching in 3d with mast3r, 2024. [45] Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon Wetzstein. Flare: Feed-forward geometry, appearance and camera estimation from uncalibrated sparse views. In CVPR, 2025. [46] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In CVPR, 2025. [47] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [48] Paul Besl and Neil McKay. Method for registration of 3-d shapes. In Sensor fusion IV: control paradigms and data structures, volume 1611, pages 586606. Spie, 1992. [49] S. Umeyama. Least-squares estimation of transformation parameters between two point patterns. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(4):376380, 1991. [50] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanæs. Large scale multi-view stereopsis evaluation. In CVPR, pages 406413, 2014. [51] Yuedong Chen, Haofei Xu, Qianyi Wu, Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. Explicit correspondence matching for generalizable neural radiance fields, 2023. [52] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. Accelerating 3d deep learning with pytorch3d, 2020."
        },
        {
            "title": "A More Experimental Analysis",
            "content": "Evaluation under view interpolation settings. Beyond the main papers extrapolation experiments, we also tested our PM-Loss in view interpolation settings, where target views lie between the two context views. These tests used the DepthSplats [9] offical evaluation set on DL3DV dataset, with two input views. Tab. shows the quantitative results for this setup. Applying our PM-Loss (denoted as +PM) consistently improves novel view synthesis for both DepthSplat [9] and MVSplat [8] across all metrics. This demonstrates that our distilled geometric priors are also beneficial for interpolated views. Despite these positive interpolation results, our main paper primarily focuses on extrapolation. Extrapolation scenarios more rigorously test the geometric integrity of 3D reconstructions, especially near scene or object boundaries. This focus helps to clearly quantify and showcase improvements in 3D Gaussian geometric quality, key contribution of our PM-Loss. Table A: NVS on DL3DV with DepthSplat [9]s offical eval set. Our PM-Loss (+PM) improves rendering for both MVSplat [8] and DepthSplat [9]. Method View Interpolation PSNR SSIM LPIPS DepthSplat 19.05 DepthSplat+PM 19. MVSplat 17.59 MVSplat+PM 17.81 0.610 0.641 0.514 0.520 0.313 0.298 0.396 0.380 Memory efficiency with offline pointmap preprocessing As the main paper (Sec. 4.2) notes, pointmap generation is best performed as an offline preprocessing step. We analyze our PM-Losss training VRAM footprint under this optimal workflow here. Tab. includes data for both offline and online pointmap processing scenarios when applying our PM-Loss (denoted as +PM) to DepthSplat. Focusing on the recommended offline approachwhere the pointmap generators parameters do not consume training VRAMour PM-Loss adds only modest 0.96GB to DepthSplats VRAM usage (52.75GB vs. 51.79GB). This small overhead, even when handling large number of 3D Gaussians (e.g., approximately 4 106), primarily accounts for storing the loaded pointmap data and the computations for our online loss components (alignment and Chamfer distance). This highlights the efficiency of our core PM-Loss component. Table B: VRAM footprint of different pointmap processing strategies. Comparison of maximum GPU memory for baseline DepthSplat, and DepthSplat with PM-Loss (offline vs. online VGGT-1B pointmap processing) during training. Method Max VRAM (GB) DepthSplat DepthSplat+PM (offline process) DepthSplat+PM (online process) 51.79 52.75 57.88 Discussion of existing pointmap based feed-forward 3DGS. Integrating pointmap priors into feed-forward 3DGS faces challenges from pose discrepancies (Discussed in Sec. 1). Some methods, like NoPoSplat [27], use specialized Gaussian heads for pointmap features. However, this approach has trade-offs, as shown in Tab. C: NoPoSplat has significantly larger model (611M parameters vs. MVSplats [8] 12M), and its rendering quality without test-time pose alignment (PSNR 24.70) is notably below MVSplats (PSNR 26.39). While NoPoSplat achieves its best quality (PSNR 26.79) with test-time pose alignment, this step is slow, averaging 2.84 seconds per instance, thereby limiting real-time applicability. NoPoSplats case illustrates that specialized heads for pointmap integration can lead to large models and require costly test-time optimizations for optimal performance. Motivated by these limitations, our PM-Loss instead distills pointmap priors via an efficient training loss, aiming for better balance between rendering quality, model size, and inference speed. 14 Table C: Quantitative comparison of NoPoSplats variants and MVSplat. Key metrics, model size, and inference time for NoPoSplat (with/without pose alignment) compared to the MVSplat baseline. Method Rendering Quality PSNR SSIM LPIPS Parameters (M) Time (ms) NoPoSplat (w/o pose align) 24.70 26.79 NoPoSplat (w/ pose align) 26.39 MVSplat 0.818 0.878 0.869 0.145 0.124 0.128 611 611 12 2.8 2840 1.4 Figure A: Limitation: Pointmap Errors Propagate to 3DGS. Left: VGGT pointmap with inaccuracies (e.g., for sky regions). Right: DepthSplat + PM-Losss failure case 3D Gaussians showing propagated errors from the pointmap prior."
        },
        {
            "title": "B Limitation and Societal Impacts",
            "content": "Limitation: dependence on pointmap quality. As stated in the main paper, our PM-Losss effectiveness is limited by the quality of the pre-trained pointmap model providing geometric priors. Errors in this pointmap can propagate via our PM-Loss, impacting the final 3DGS models geometric quality. Fig. illustrates this dependency. The left side shows VGGT pointmap with inaccuracies, while the right displays DepthSplat + PM-Losss 3D Gaussians where these prior errors are visibly reflected. Thus, poor pointmap quality in challenging regions constrains our PM-Losss performance. Leveraging improved pointmap models in the future could further enhance our approach. Broader societal impacts. Our work significantly improves 3D shape quality from feed-forward Gaussian Splatting models, presenting both opportunities and ethical considerations. Benefits include enhanced AR/VR experiences, accelerated 3D content creation, and improved cultural heritage preservation. However, the heightened realism also poses risks. major risk is misuse for creating believable fake content; for example, high geometric quality could enable convincing fake 3D environments reconstructed from AI-generated images or videos, potentially fueling propaganda or scams and reducing digital trust. Furthermore, the synthetic nature of these models warrants caution in safety-critical applications, such as autonomous vehicle training, due to potential inaccuracies."
        },
        {
            "title": "C More Implementation Details",
            "content": "More baseline details. For our experiments involving DepthSplat [9], it is important to note the specific version used. The results presented in this paper are based on the initial version of DepthSplat, which was publicly released in October 2024. We acknowledge that an updated version of the DepthSplat model architecture was made available in March 2025. However, all experiments reported herein were conducted using the aforementioned October 2024 release to maintain consistency with the experimental timeline of our work. More loss function details. Our total training loss, denoted as Ltotal, consists of two primary components: rendering loss term, Lrender, and our proposed geometric consistency loss, LPM. The detailed formulation of LPM itself is provided in the main paper(Sec. 3.2). The overall loss is defined 15 Figure B: More comparisons on DL3DV(top two rows) and RealEstate10K(bottom four rows) under the 2-view extrapolation setting. Adding PM-Loss leads to significant improvements in rendering object boundaries. as: Ltotal = Lrender + λPMLPM (9) Here, λPM is the weighting coefficient for LPM. As specified in the main papers Implementation Details section, we set λPM = 0.005. The rendering loss, Lrender, follows common practices established in prior works [8, 9]. It combines Mean Squared Error (LMSE) term and an LPIPS (LLPIPS) perceptual loss term: Lrender = λMSELMSE + λLPIPSLLPIPS (10) The LMSE term quantifies the pixel-wise squared differences between the rendered output and the ground-truth image. The LLPIPS term assesses perceptual similarity between the rendered and groundtruth images. We set the respective weights for these components as λMSE = 1.0 and λLPIPS = 0.05. More training details. We fine-tune all baseline models on the DL3DV dataset, starting from their publicly available pre-trained weights. To fairly evaluate our PM-Loss, we prepare two versions for each baseline: one fine-tuned with PM-Loss integrated, and control version fine-tuned strictly following the baselines original methodology. This isolates the impact of PM-Loss. While largely adhering to original training configurations, minor adjustments were made for single GPU fine-tuning. 16 Figure C: More comparison of 3D Gaussians on DL3DV dataset. Our method effectively constrains the 3D Gaussians, significantly reducing floating artifacts and noise near border. For DepthSplat [9], we use batch size of 1, with the number of context views randomly chosen from 2-6 and target views fixed at 4 during training. For MVSplat [8], the batch size is 12, with 2 fixed context views and 4 fixed target views. Code and pre-trained weights will be made available. More testing details. Here we further details our Novel View Synthesis (NVS) extrapolation protocol, introduced in the main paper (Sec. 4.1 ). We use two context views and select target views outside their direct span. This setup rigorously assesses geometric quality, particularly at object edges, by ensuring genuine extrapolation that challenges models at scene peripheries. Our specific sampling strategy, based on image sequence indices, adapts to dataset characteristics. For the RealEstate10K dataset, context views are separated by 75 image indices; we then sample 15 target views before the first context view and 15 after the second (30 total extrapolation views). For the DL3DV dataset, due to its larger inter-frame viewpoint changes, we use context gap of 20 indices, sampling 10 target views before the first and 10 after the second context view (20 total extrapolation views). More Visual Comparisons This section presents supplementary visual comparisons to further demonstrate the impact of our PMLoss. Fig. showcases additional novel view synthesis results from the DL3DV and RealEstate10K datasets, particularly under the 2-view extrapolation setting, highlighting the improved rendering of object boundaries when our PM-Loss is applied. Furthermore, Fig. visualizes the quality of the reconstructed 3D Gaussians on the DL3DV dataset, illustrating the reduction in floating artifacts and border noise achieved by our approach."
        }
    ],
    "affiliations": [
        "GigaAI",
        "MBZUAI",
        "Monash University, Australia",
        "Zhejiang University, China"
    ]
}