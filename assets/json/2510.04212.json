{
    "paper_title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention",
    "authors": [
        "Haiquan Qiu",
        "Quanming Yao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The pursuit of computational efficiency has driven the adoption of low-precision formats for training transformer models. However, this progress is often hindered by notorious training instabilities. This paper provides the first mechanistic explanation for a long-standing and unresolved failure case where training with flash attention in low-precision settings leads to catastrophic loss explosions. Our in-depth analysis reveals that the failure is not a random artifact but caused by two intertwined phenomena: the emergence of similar low-rank representations within the attention mechanism and the compounding effect of biased rounding errors inherent in low-precision arithmetic. We demonstrate how these factors create a vicious cycle of error accumulation that corrupts weight updates, ultimately derailing the training dynamics. To validate our findings, we introduce a minimal modification to the flash attention that mitigates the bias in rounding errors. This simple change stabilizes the training process, confirming our analysis and offering a practical solution to this persistent problem."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 2 1 2 4 0 . 0 1 5 2 : r Preprint. WHY LOW-PRECISION TRANSFORMER TRAINING FAILS: AN ANALYSIS ON FLASH ATTENTION Haiquan Qiu Quanming Yao Department of Electronic Engineering, Tsinghua University qhq22@mails.tsinghua.edu.cn, qyaoaa@tsinghua.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "The pursuit of computational efficiency has driven the adoption of low-precision formats for training transformer models. However, this progress is often hindered by notorious training instabilities. This paper provides the first mechanistic explanation for long-standing and unresolved failure case where training with flash attention in low-precision settings leads to catastrophic loss explosions. Our indepth analysis reveals that the failure is not random artifact but caused by two intertwined phenomena: the emergence of similar low-rank representations within the attention mechanism and the compounding effect of biased rounding errors inherent in low-precision arithmetic. We demonstrate how these factors create vicious cycle of error accumulation that corrupts weight updates, ultimately derailing the training dynamics. To validate our findings, we introduce minimal modification to the flash attention that mitigates the bias in rounding errors. This simple change stabilizes the training process, confirming our analysis and offering practical solution to this persistent problem. Code is available at https: //github.com/ucker/why-low-precision-training-fails."
        },
        {
            "title": "INTRODUCTION",
            "content": "The pursuit of training ever-larger and more powerful transformer models is relentless drive for computational efficiency (Brown et al., 2020; Hoffmann et al., 2022). key strategy in this endeavor is the adoption of low-precision numerical formats (Micikevicius et al., 2017; Wang et al., 2018; Kalamkar et al., 2019; Liu et al., 2024), which promise substantial reductions in memory footprint and significant boosts in training speed. In industrial practice, it is common to use BF16 for memory-bound operations like flash attention while pushing compute-bound operations like FFNs to even lower precisions such as FP8 (Liu et al., 2024; Qwen-Team, 2025). This highlights the heightened sensitivity of attention mechanisms to numerical precision. Despite the development of stabilization techniques like QK normalization (Henry et al., 2020; Qwen-Team, 2025), QK-clip (KimiTeam, 2025) and Gated Attention (Qiu et al., 2025; Qwen-Team, 2025), the path to further reducing precision is often blocked by lack of understanding of the underlying failure mechanisms. This paper confronts this challenge by dissecting notorious and long-standing failure issue involving flash attention. By reducing the memory complexity of the attention mechanism from quadratic to linear with respect to sequence length, flash attention has become cornerstone algorithm for efficient transformer training, making it indispensable for handling the long contexts required by modern large-scale models (Dao et al., 2022; Dao, 2024; Shah et al., 2024). This failure, arising in low-precision settings (flash-attention Issue 337, 2024; nanoGPT Issue 303, 2023; nanoGPT Issue 524, 2024; nanoGPT Issue 554, 2024; Lee et al., 2024; Golden et al., 2024), presents significant bottleneck. We focus on specific, reproducible failure case reported by the community (nanoGPT Issue 303, 2023; nanoGPT Issue 524, 2024), which has remained unresolved for over two years. Our in-depth analysis provides the first mechanistic explanation for this failure, revealing that it is not random artifact but direct consequence of two intertwined phenomena: the emergence of similar low-rank representations across different training steps and tokens, and the compounding effect of biased rounding errors inherent in low-precision arithmetic. We demonstrate how these biased rounding errors act as coefficients for the low-rank representations, causing them to accumulate as biased gradient update to the weights. This pushes the spectral norm of weights and activations to increase abnormally, ultimately overwhelming the training dynamics. To validate our analysis, 1 Preprint. we introduce minimal modification to flash attention that mitigates the bias in rounding errors, allowing the low-rank weight updates to cancel out during training. This experiment confirms our analysis and stabilizes the training process, offering practical solution to this persistent problem. Notations We use bold lowercase letters for vectors (e.g., δ, m, rm) and bold uppercase letters for matrices (e.g., Q, K, V). diag(v) denotes diagonal matrix with the elements of vector on its diagonal. denotes the element-wise product. We use Python-style indexing, e.g., M[i, :] denotes the i-th row of matrix M. Subscripts lp and hp distinguish between low-precision (BF16) and high-precision (FP32) computations. Binary strings are represented using typewriter font (e.g., 101010). The symbol denotes an element-wise equality comparison, analogous to Pythons == operator, which returns binary tensor. The where function mimics torch.where. Unless specified otherwise, operations follow PyTorchs broadcasting rules. Key claims are highlighted in light cyan box at the end of some sections."
        },
        {
            "title": "2 PRELIMINARY",
            "content": "2.1 LOW-PRECISION TRAINING Low-precision training is cornerstone of modern deep learning, enabling the development of increasingly large models by reducing memory usage and accelerating computation (Liu et al., 2024; Tseng et al., 2025; Hao et al., 2025). This is achieved by representing weights, activations, and gradients using numerical formats with fewer bits than the standard 32-bit single-precision (FP32). Mixed-precision training (Micikevicius et al., 2017) has been widely adopted in practice, which combines 16-bit formats like FP16 or bfloat16 (BF16) for most computations with an FP32 master copy of weights to maintain accuracy. While FP16 offers higher precision, its limited dynamic range often leads to gradient underflow, requiring techniques like loss scaling. In contrast, BF16, originally developed for Googles TPUs and now widely supported, provides the same dynamic range as FP32, making it more robust against underflow and preferred choice for training large language models (Kalamkar et al., 2019; Wang & Kanwar, 2019). However, the reduced precision of BF16 can still introduce numerical errors that lead to training failure, the focus of this paper. The bfloat16 (BF16) format is 16-bit floating-point representation with 1 sign, 8 exponent, and 7 significand bits. It matches the dynamic range of 32-bit single-precision (FP32) but has lower precision, making it popular choice for balancing computation and numerical range in deep learning. Adding two BF16 numbers involves aligning their exponents, adding the significands, normalizing the sum, and rounding the result to fit the 7-bit fraction. This final rounding step, typically round to nearest, ties to even, is one of the primary sources of error. While this rounding method is designed to be unbiased for random data, sequence of operations on data with specific distribution could lead to biased rounding error. This accumulation of error in one direction is critical factor to the training failure observed in low-precision settings. See Section for details of BF16 addition. 2.2 FLASH ATTENTION Flash Attention (FA) (Dao et al., 2022; Dao, 2024; Shah et al., 2024) is an I/O-aware exact attention algorithm designed to overcome the memory bottleneck of standard attention. Standard attention, defined as = softmax(αQK)V, requires materializing the attention score matrix = αQK, leading to memory complexity of O(N 2) with respect to the sequence length . Flash attention reduces this to O(N ) by partitioning the input matrices Q, K, RN into blocks and processing them iteratively. These blocks are loaded from high-bandwidth memory (HBM) into fast on-chip SRAM, minimizing costly memory transfers. In this paper, we focus on analyzing flash attention 2. The forward pass of FA computes the output and log-sum-exp statistics using an online softmax method (Algorithm 2). It iterates through blocks of (outer loop) and blocks of K, (inner loop). For each query block Qi, it maintains running statistics: the maximum score mi and the normalization factor ℓi. In each inner loop step, it computes unnormalized attention scores P(j) ) and updates an unnormalized output by accumulating the product P(j) Vj. This accumulation is key focus of our analysis. After iterating through all key/value blocks, the final output block Oi is correctly normalized. This = exp(S(j) m(j) 2 Preprint. Figure 1: Analysis in different sections. Our paper traces the causal chain of training failure (blue box) in reverse to identify the root causes. tiling strategy avoids materializing the full score matrix. The backward pass (Algorithm 3) leverages the same tiling strategy. It first computes key intermediate term, δ = rowsum(dO O), which is central to our investigation. Then, it recomputes attention scores on-the-fly to calculate the gradient of the scores, dS(j) . The final gradients dQ, dK, dV are accumulated block-wise. This approach maintains I/O efficiency in both passes. δi), where dP(j) = dOiV = P(j) (dP(j) i"
        },
        {
            "title": "3 ROOT CAUSES OF INSTABILITY IN FLASH ATTENTION",
            "content": "In Section 3.2, we narrow down the source We first introduce the failure case in Section 3.1. of the numerical errors within the flash attention. Further analysis in Section 3.3 reveals that the failure stems from combination of two factors: the emergence of low-rank representations and the accumulation of biased rounding errors inherent to BF16 arithmetic. The full process of such failure from root cause to loss exploding is shown in Fig. 1. 3.1 THE FAILURE CASE OF LOW-PRECISION FLASH ATTENTION Our investigation targets well-documented and perthe catastrophic loss explosion that ocsistent failure: curs when training Generative Pre-trained Transformer 2 (GPT-2) models with flash attention in BF16 precision (nanoGPT Issue 303, 2023; nanoGPT Issue 524, 2024; nanoGPT Issue 554, 2024). This failure case, reported for over two years, manifests as sudden loss explosion after several thousand training steps (see Fig. 8). While empirical workarounds like reverting to standard attention or using higher precision (FP32) stabilize training, they come at the cost of efficiency. This instability is not an isolated incident; the broader community has observed similar failures when training large language models (Kimi-Team, 2025; Qwen-Team, 2025). These failures are often empirically linked to phenomena such as large spectral norms of weights, large activations (Yang et al., 2023; Rybakov et al., 2024), and attention sinks (Xiao et al., 2023), leading to suite of fixes including QK normalization (Henry et al., 2020; Qwen-Team, 2025), QK-clipping (Kimi-Team, 2025), and Gated Attention (Qiu et al., 2025; Qwen-Team, 2025). Despite these interventions, fundamental understanding of the root causes has remained elusive. The absence of clear causal chain from numerical error to loss explosion has left the community reliant on ad-hoc patches rather than principled solutions, hindering progress in robust low-precision training. This paper provides the first mechanistic explanation by reproducing the failure, dissecting its root causes, and proposing practical, principled solution. Figure 2: The failure case using BF16 and flash attention results in sudden loss explosion, while the stable configuration converges. 3 Preprint. To reproduce the failure, we employ GPT-2 architecture with 12 layers, 12 attention heads, an embedding dimension of 768, and context length of 1024. The model is pre-trained on the OpenWebText dataset (Gokaslan et al., 2019). For deterministic reproducibility, we deviate from standard random data loader by recording and reusing the exact sequence of data batches from an initial run that led to the failure. This ensures all subsequent experiments process identical data in the same order, isolating the failure from data-related randomness. We train the model using the AdamW optimizer with β1 = 0.9, β2 = 0.95, and zero weight decay. The learning rate follows cosine schedule with 2000-iteration linear warmup to peak of 1 103, decaying to 1 105. We apply global gradient clipping with maximum norm of 1.0. Training is conducted on 4 NVIDIA A100 (80GB) GPUs using PyTorchs Distributed Data Parallel (DDP) module. We use automatic mixed precision with BF16 for the forward pass and FP32 for the backward pass. Each GPU processes micro-batch size of 32, and with gradient accumulation over 4 steps, the effective global batch size is 524,288 tokens per optimization step. 3."
        },
        {
            "title": "ISOLATING THE SOURCE OF FAILURE WITHIN FLASH ATTENTION",
            "content": "To pinpoint the source of failure within flash attention, we conduct series of targeted experiments. We systematically modify the algorithmdisabling tiling, selectively replacing flash attention with standard implementation, and performing key computations in high precisionto narrow down the potential causes of instability. To accelerate this analysis, we monitor empirical indicators such as the spectral norm of weights to quickly identify failing configurations. Tiling is not the Source of Failure. To determine if the block-wise processing in flash attention was responsible for the failure, we conduct an experiment where we disabled tiling by setting the block size equal to the sequence length. This forces the algorithm to compute with the full matrices at once. The training process still failed, leading to the same loss explosion. This finding rules out the tiling strategy as the cause of the problem. For all subsequent experiments, we therefore use this non-tiled setup to simplify the analysis and focus on the core numerical computations. Failure Originates in Single Layer. We first analyze the spectral norms of weights across all layers (Yang et al., 2023; Rybakov et al., 2024). This reveals an anomalous spike specifically within the second layers attention (see Fig. 9). We confirm this finding with two targeted experiments: (1) using flash attention only in layer 2 was sufficient to reproduce the training failure, and (2) replacing flash attention with standard attention in layer 2, while retaining it in all other layers, restored training stability. These results conclusively identify the flash attention in layer 2 as the origin of the failure. So, subsequent analysis focuses on this module to dissect the failure mechanism. Failure is Linked to the Computation of δ. The backward pass of flash attention uses the term δ = rowsum(dO O) RN for computational efficiency. An alternative, mathematically equivalent formulation computes this term as δ = rowsum(dP P), where dP = dOV. We find that replacing the efficient computation with this alternative formulation restored training stability. This experiment demonstrates that numerical errors introduced when computing in BF16 is likely the primary source of failure, as the alternative formulation which avoids training failure is equivalent to use computed in FP32. Numerical Errors in are the Source of Failure. Building on the finding that the computation of δ is critical, we further isolate the source of error to the output matrix Olp in low-precision δlp = rowsum(dO Olp). We conduct two key experiments. First, instead of using the low-precision from the forward pass to compute δ, we recomput it as PV in FP32 within the backward pass; this change stabilize training. Second, we find that computing in high precision (FP32) during the forward pass, i.e., δhp = rowsum(dO Ohp), while keeping all other operations in BF16, also restored stability. This evidence conclusively demonstrates that numerical errors introduced during the BF16 computation of are the direct cause of the failure (refer to Claim 1). Figure 3: WQ of attention head 8 has the largest spectral norm. Subsequent analysis focuses on this head. Preprint. Failure is Localized to Specific Attention Heads. To further narrow down the source of failure, we analyze individual attention heads by tracking the spectral norm (Yang et al., 2023; Rybakov et al., 2024) of their query projection matrices (WQ), as shown in Fig. 3. This reveals that few heads exhibit disproportionately large spectral norms. We confirm their role by selectively computing the output in high precision for these outlier heads (1, 7, 8, 9, 11, and 12), which is sufficient to restore training stability. Since head 8 shows the largest spectral norm, we focus our subsequent analysis on this head to dissect the precise failure mechanism. Claim 1. Low-precision δlp = rowsum(dO Olp) causes training failure."
        },
        {
            "title": "3.3 THE ROOT CAUSES OF TRAINING FAILURE",
            "content": "Our investigation in this section uncovers the two interconnected root causes of the training failure. In Section 3.3.1, we demonstrate how low-precision δlp drives the training failure by biased weight update, and find that bias arises from the emergence of similar low-rank representations R, whose coefficients (δlp δhp)[T ] are biased towards positive values, causing the error to accumulate rather than cancel out. In Section 3.3.2, we trace the origin of these positive coefficients (δlp δhp)[T ] to biased rounding errors inherent in the BF16 addition within the PV product. 3.3.1 CAUSE 1. SIMILAR LOW-RANK MATRICES BIAS WEIGHT UPDATES This section traces the training failure to biased weight update. We first analyze the difference between the high and low precision gradients calculated with δhp and δlp, respectively. Then we find that the low-rank representations and biased (δlp δhp)[T ] lead to loss explosion. Analysis of the Error between High and Low Precision Gradients To understand how numerical errors propagate into the gradients, we analyze the difference between the high-precision (hp) and low-precision (lp) gradients for the query matrix, dQ. The gradient dQ is computed from the gradient of the attention scores, dS, as dQ = dSK. The score gradient is given by dS = αP (dP δ), where δ = rowsum(dO O) is the only term that differs between the highand low-precision backward passes based on Section 3.2, and α is scaling factor in attention. The difference between the high-precision and low-precision query gradients can be derived as: dQhp dQlp = (dShp dSlp)K = (αP (dP δhp) αP (dP δlp)) = (αP (δlp δhp)) =α diag(δlp δhp)(PK). (1) In the final step, we express the row-wise scaling operation (δlp δhp) (follow broadcasting rule) as matrix multiplication, where diag(δlp δhp) is diagonal matrix whose diagonal entries are the elements of the vector difference δlp δhp. This formulation reveals that the gradient error is directly proportional to the error in δ and is modulated by the term PK. The gradient of the query projection matrix, dWQ, is given by the outer product of the input features and the query gradient dQ. The difference between the high-precision (hp) and low-precision (lp) gradients for WQ can be expressed as: dWQ hp dWQ lp = (dQhp dQlp)X = α(PK)diag(δlp δhp)X, = α (cid:88)N =1 (δlp δhp)[T ] (PK)[T ]X[T ], (2) where (PK)[T ] and X[T ] are the -th row vectors of their matrices. This equation shows that the total gradient error is weighted sum of rank-1 matrices, with weights given by the error in δ. In Fig. 4, the rows of PK (panels Similar Low-rank Updates of Weight Cause Training Failure a, d) and (panels b, e) exhibit strong structural similarity across different training steps and token positions. This implies that the resulting rank-1 matrices, (PK)[T ]X[T ], are also highly similar to one another. For instance, Fig. 4 (panels c, f) shows this similarity for tokens 50 and 718 at training steps 6610 and 6619, respectively. Because these rank-1 error components are structurally consistent, we can approximate the total gradient difference as dWQ hp dWQ lp α (cid:88)N = (δlp δhp)[T ]R (3) 5 Preprint. (a) (b) (c) (d) (e) Figure 4: PK, X, and (PK)[T ]X[T ] at different batch indices and training steps. (c) and (f) show that (PK)[T ]X[T ] for different tokens and training steps have some similar columns in input features 546 and 678. (f) where denotes the common low-rank structure emerging across different tokens and training steps. Eqn.(3) shows that the accumulation of the low-rank error direction is governed by the scalar term (cid:80)N =1(δlpδhp)[T ]. If this sum is biased towards non-zero values, the error across different training steps will accumulate rather than cancel out. We track the cumulative sum of (cid:80)N =1(δlp δhp)[T ] over sequence of training steps (6580 to 6680) leading up to the failure, as shown in Fig. 5(a). The plot reveals that this sum is consistently positive, indicating systematic bias. This bias causes the error in the low-rank direction to compound with each training step. Because is also similar for different steps, this ultimately corrupts the weight updates, increases the spectral norm  (Fig. 9)  and activations (Yang et al., 2023; Rybakov et al., 2024), and causes the training failure (refer to Claim 2). The following section finds the root cause of this positive bias by analyzing the weights and gradients at training step 6619, point of significant positive contribution identified in Fig. 5(a). Claim 2. Weight update in low-precision training is biased by (δlp δhp)[T ]R, which arises from the structurally similar matrices (denoted as R) across tokens and training steps, and its positively-biased coefficient (δlp δhp)[T ]. This bias accumulates error, preventing cancellation, increasing weight spectral norm and activation, and leading to loss explosion. (a) Positively-biased (δlpδhp)[T ] (b) Large values in features 20 & (c) Large error in features 20 & 29 29 Figure 5: Analysis of δ = rowsum(dO O). Preprint. (a) Most V[:, i] are negative (b) Large error when P[T, i] = 1 Figure 6: Analysis of PV (c) Detailed view of (b)"
        },
        {
            "title": "3.3.2 CAUSE 2. BIASED ROUNDING ERROR LEADS TO POSITIVE (δlp − δhp)[T ]",
            "content": "This section investigates the origin of the positive bias in (δlp δhp)[T ]. We trace this error to the interaction between dO and the numerical discrepancy in Olp Ohp, which itself arises from biased rounding errors in BF16 addition during the PV computation. Locate the Large Error in (δlp δhp)[T ] We first investigate the source of the positive bias in (cid:80)N =1(δlp δhp)[T ]. As analyzed in Section 3.2, the error in δ stems from the product of the upstream gradient dO and the numerical error in the low-precision output, Olp Ohp. To dissect this, we focus on token position = 718 where the error component (δlp δhp)[T ] is positive. In Fig. 5(b) and (c), we observe strong sign correlation between the gradient dO[T, :] and the output error Olp[T, :] Ohp[T, :] for specific feature dimensions such as 20 and 29 (also observed across other tokens). In these dimensions, both dO and the output error Olp Ohp are consistently negative. This alignment ensures their product, which contributes to the error in δ, is positive. The fact that the output error tends to be negative (Olp[T, i] < Ohp[T, i]) indicates that the low-precision computation of is systematically biased towards more negative values. Our subsequent analysis, therefore, focuses on identifying the origin of this computational bias. The output is computed from an intermediate unnormalized output, O. The computation involves safe softmax followed by matrix multiplication and normalization: = exp(S rowmax(S)), = PV, = O/rowsum( P). Further experiments pinpoint the source of failure to the computation of the unnormalized output, = PV. We find that computing only this product in FP32 is sufficient to stabilize training. To understand the origin of this bias, we examine the difference between the low-precision and highprecision computation of single element, O[T, i] (for feature index = 20 in our analysis): Olp[T, i] Ohp[T, i] = ( Plp[T, :]V[:, i])lp ( Php[T, :]V[:, i])hp (4) where the inputs and are themselves the results of prior BF16 operations. Specifically, the subscript ()lp here computes dot product in FP32 with the final result rounded to BF16, while ()hp computes entirely in FP32. To understand how the error Olp[T, i] Ohp[T, i] becomes systematically negative, we plot the cumulative error as the sum over token positions progresses in Fig. 6(b) and (c): Oerror(t) = (cid:16)(cid:88)t t=1 (cid:17) P[T, t]V[t, i] lp (cid:16)(cid:88)t t=1 (cid:17) P[T, t]V[t, i] . hp (5) The figure shows that the error accumulates in significant negative steps. These steps occur at token positions where the corresponding attention probability P[T, t] is exactly 1 (also observed in other token positions). This happens when the pre-softmax score S[T, t] is the maximum value in its row, causing exp(S[T, t] max(S[T, :])) to evaluate to exp(0) = 1. Analysis of Biased Rounding Error Furthermore, the bias arises from the interaction of these unit values with the distribution of the value matrix V. As observed in Fig. 6(a), for the problematic feature dimension = 20, the values of V[:, i] are predominantly negative. When P[T, t] = 1, the product P[T, t]V[t, i] is simply V[t, i], negative BF16 number. systematic error then occurs when two such negative BF16 numbers are added. In floating-point arithmetic, adding two numbers 7 Preprint. with the same sign can cause the resulting significand to overflow (e.g., 1.xxxx + 1.yyyy = 10.zzzz), requiring right shift and an exponent increment to re-normalize. The bits shifted out of the 7-bit BF16 fraction determine the rounding direction. When adding two negative numbers, the rounding operation (e.g., round-to-nearest) can introduce consistent bias. To illustrate how this rounding bias occurs, consider the addition of two significands that cause an overflow, requiring right shift for normalization. The bit that is shifted out (the rounding bit) determines the rounding direction. We show all possible additions of the last two 2-bit numbers, where the green bit represents the bit that would be shifted out (the rounding bit): 00 + 00 = 00 01 + 10 = 11 00 + 11 = 11 10 + 11 = 101 00 + 01 = 01 01 + 11 = 100 00 + 10 = 10 10 + 10 = 100 In these cases, green 1 indicates that rounding up is required, whereas 0 indicates no rounding is needed. Because the operands P[T, t]V[t, i] are negative and have large exponent (because P[T, t] = 1 does not make P[T, t]V[t, i] smaller), even small rounding error is magnified, resulting in significant negative error in the final sum. This systematic negative bias in the computation of is the ultimate source of the training failure. Remark 1. For P[T, t] < 1, its product with V[t, i] has non-zero least 16 bits. When rounding to BF16, this will not introduce biased rounding error. 01 + 01 = 10 11 + 11 = Analysis of Rounding Error in Fig. 6 We consider the addition of two negative values, P[T, t1]V[t1, i] and P[T, t2]V[t2, i], from Fig. 6. Their BF16 values are computed in FP32 and then rounded. For this example, we start with their exact FP32 representations, which have zero bits beyond the 7th fraction bit: 11000000000110100000000000000000 (2.40625) 11000000000100110000000000000000 (2.296875) Since their exponents are identical, the addition is performed on their significands: (1.00110100000000000000000) + (1.00100110000000000000000) = 10.01011010000000000000000 The result overflows the significands format, requiring normalization. The significand is shifted right by one bit, and the exponent is incremented: Significand: 10.0101101 1.00101101 Exponent: 10000000 10000001 The exact result in FP32 is 1 10000001 00101101000000000000000, which corresponds to 4.703125. To store this result in BF16, it must be rounded to 7 fraction bits. The significand is 1.00101101. The 7-bit fraction is 0010110, and the first truncated bit is 1. According to the round to nearest, ties to even rule, this requires rounding up (adding 1 to the last bit of the fraction): BF16 Fraction: 0010110 + 1 = 0010111 The final BF16 result is 1100000010010111, which represents 4.71875. This rounded value is more negative than the true sum of 4.703125. The error introduced by this single addition is 0.015625. When such rounding events occur systematically across many additions in the PV product, the errors accumulate, creating the negative bias in that ultimately destabilizes training. Claim 3. With more than one P[T, t] = 1 and negative V[t, i], the addition in PV can cause significand overflow. This necessitates right shift, which often forces round-down, introducing negative rounding error in and leading to positive (δlp δhp)[T ]."
        },
        {
            "title": "4 EXPERIMENT: MITIGATE BIAS IN ROUNDING ERROR",
            "content": "Our analysis traces the failure to biased rounding errors in the PV computation. This occurs when multiple identical maxima in row of the pre-softmax scores cause corresponding elements in to become exactly 1. To validate our findings, we modify the softmax to detect this specific condition and adjust the normalization, ensuring all elements of are strictly less than 1. This prevents the biased rounding and restores training stability. 8 Preprint. rs = rowsum(S rm) rm = rowmax(S), = where(rm > 0 rs > 1, βrm, rm), β > 1 = where(rm < 0 rs > 1, 0, m) = exp(S m) To prevent biased rounding, we introduce targeted modification to the safe softmax computation. The core idea is to dynamically adjust the normalization factor only when row of the score matrix contains multiple identical maximum values. This adjustment ensures that the argument to the exponential function, m, becomes strictly negative at these maximal positions, which in turn guarantees that all elements of = exp(S m) are less than 1. naive approach, such as subtracting small fixed constant, is insufficient as it introduces new systematic rounding errors (see Section C); hence, dynamic maximum strategy is required. Our modification is presented above. This modification prevents elements of from becoming exactly 1. If rows maximum value, rm, is positive and repeated, the normalization factor is adjusted to = βrm (with β > 1). This makes the new maximum in the exponent (β 1)rm, which is strictly negative. If rm is negative and repeated, we set = 0, which also ensures the exponents maximum remains negative. In both scenarios, this adjustment guarantees that max(S m) < 0 and thus max( P) < 1, preventing the conditions that lead to biased rounding. Crucially, this modification is mathematically equivalent to standard attention in exact arithmetic, as it leverages the shift-invariance property of the softmax function (softmax(z) = softmax(z c)). Our method simply chooses different row-wise constant to ensure numerical stability. In our experiments, we set β [2, 8], as smaller values risk having the result round back to 1, while larger values risk underflow. This modification is integrated into the standard flash attention tiling algorithm (line with magenta in Algorithm 1) without altering the backward pass. As shown in Fig. 7, this simple change (β = 7) successfully stabilizes training, confirming our analysis. Further design details are provided in Section C."
        },
        {
            "title": "5 CONCLUSION",
            "content": "Figure 7: Comparison of the stabilized FA and the original FA. This paper presents the first mechanistic explanation for notorious loss explosion in low-precision flash attention training. We pinpoint the root cause to an interplay between emergent low-rank representations and biased BF16 rounding errors. minimal, targeted modification to flash attention validates our analysis by restoring stability. Our analytical workflow provides blueprint for diagnosing similar numerical instabilities in other architectures, scales, and low-precision formats, paving the way for more robust and efficient large-scale model training. Discussion Our findings are consistent across various hardware (NVIDIA A100, RTX 4090, Huawei Ascend 910B) and mechanistically explain empirical observations of training instability. The growth of weight spectral norms results from the accumulation of low-rank error matrix in the gradients. We also clarify the role of attention sinks: by attracting high attention scores, they are more likely to produce attention probabilities of 1, which triggers the biased rounding error in the PV computation. This provides direct numerical link between the architectural behavior of sinks and the arithmetic instability that derails training. Limitations Our analysis focuses on specific failure case in GPT-2 model. The generalizability of our findings to other architectures, larger scales, or different low-precision formats like FP8 requires further investigation. Additionally, our proposed mitigation is tailored to the specific rounding error identified and may not address other sources of numerical instability. 9 Preprint."
        },
        {
            "title": "REFERENCES",
            "content": "Sami Ben Ali, Silviu-Ioan Filip, and Olivier Sentieys. stochastic rounding-enabled low-precision floating-point mac for dnn training. In 2024 Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 16. IEEE, 2024. Paul Balanca, Sam Hosegood, Carlo Luschi, and Andrew Fitzgibbon. Scalify: scale propagation for efficient low-precision llm training. arXiv preprint arXiv:2407.17353, 2024. Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Prince, Bjorn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach, and Douglas Orr. u-µp: The unitscaled maximal update parametrization. arXiv preprint arXiv:2407.17465, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Maxim Fishman, Brian Chmiel, Ron Banner, and Daniel Soudry. Scaling fp8 training to trilliontoken llms. arXiv preprint arXiv:2409.12517, 2024. flash-attention Issue 337. Discussion on flash-attention issue #337: pretraining loss blows up with triton flash attention. https://github.com/Dao-AILab/flash-attention/ issues/337, 2024. Accessed: 2025-09-07. Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http: //Skylion007.github.io/OpenWebTextCorpus, 2019. Alicia Golden, Samuel Hsia, Fei Sun, Bilge Acun, Basil Hosmer, Yejin Lee, Zachary DeVito, arXiv preprint Is flash attention stable? Jeff Johnson, Gu-Yeon Wei, David Brooks, et al. arXiv:2405.02803, 2024. Zhiwei Hao, Jianyuan Guo, Li Shen, Yong Luo, Han Hu, Guoxia Wang, Dianhai Yu, Yonggang Wen, and Dacheng Tao. Low-precision training of large language models: Methods, challenges, and opportunities. arXiv preprint arXiv:2505.01043, 2025. Xin He, Jianhua Sun, Hao Chen, and Dong Li. Campo:{Cost-Aware} performance optimization for {Mixed-Precision} neural network training. In 2022 USENIX Annual Technical Conference (USENIX ATC 22), pp. 505518, 2022. Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers. arXiv preprint arXiv:2010.04245, 2020. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Tianjin Huang, Ziquan Zhu, Gaojie Jin, Lu Liu, Zhangyang Wang, and Shiwei Liu. Spam: Spikeaware adam with momentum reset for stable llm training. arXiv preprint arXiv:2501.06842, 2025. Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019. 10 Preprint. Kimi-Team. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Joonhyung Lee, Jeongin Bae, Byeongwook Kim, Se Jung Kwon, and Dongsoo Lee. To fp8 and back again: Quantifying the effects of reducing precision on llm training stability. CoRR, 2024. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, and Bharat Kaul. Mixed precision training with 8-bit floating point. arXiv preprint arXiv:1905.12334, 2019. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022. Igor Molybog, Peter Albert, Moya Chen, Zachary DeVito, David Esiobu, Naman Goyal, Punit Singh Koura, Sharan Narang, Andrew Poulton, Ruan Silva, et al. theory on adam instability in largescale machine learning. arXiv preprint arXiv:2304.09871, 2023. nanoGPT Issue 303. Discussion on nanogpt issue #303: Gradient explosion when training with bfloat16. https://github.com/karpathy/nanoGPT/issues/303, 2023. Accessed: 2025-09-07. nanoGPT Issue 524. Discussion on nanogpt issue #524: Training loss becomes nan when using bfloat16. https://github.com/karpathy/nanoGPT/issues/524, 2024. Accessed: 2025-09-07. nanoGPT Issue 554. Discussion on nanogpt issue #554: Loss diverges when using bfloat16 with flash attention. https://github.com/karpathy/nanoGPT/issues/554, 2024. Accessed: 2025-09-07. Badreddine Noune, Philip Jones, Daniel Justus, Dominic Masters, and Carlo Luschi. 8-bit numerical formats for deep neural networks. arXiv preprint arXiv:2206.02915, 2022. Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, et al. Fp8-lm: Training fp8 large language models. arXiv preprint arXiv:2310.18313, 2023. Sergio Perez, Yan Zhang, James Briggs, Charlie Blake, Josh Levy-Kramer, Paul Balanca, Carlo Luschi, Stephen Barlow, and Andrew William Fitzgibbon. Training and inference of large language models using 8-bit floating point. arXiv preprint arXiv:2309.17224, 2023. Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, et al. Gated attention for large language models: Non-linearity, sparsity, and attention-sink-free. arXiv preprint arXiv:2505.06708, 2025. Qwen-Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Preprint. Oleg Rybakov, Mike Chrzanowski, Peter Dykas, Jinze Xue, and Ben Lanir. Methods of improving llm training stability. arXiv preprint arXiv:2410.16682, 2024. Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37:6865868685, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Albert Tseng, Tao Yu, and Youngsuk Park. Training llms with mxfp4. arXiv preprint arXiv:2502.20586, 2025. Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. Advances in neural information processing systems, 31, 2018. Ruizhe Wang, Yeyun Gong, Xiao Liu, Guoshuai Zhao, Ziyue Yang, Baining Guo, Zhengjun Zha, and Peng Cheng. Optimizing large language model training using fp4 quantization. arXiv preprint arXiv:2501.17116, 2025. Shibo Wang and Pankaj Kanwar. to high performance on cloud https://cloud.google.com/blog/products/ai-machine-learning/ tpus. bfloat16-the-secret-to-high-performance-on-cloud-tpus, August 2019. Accessed: 2025-09-07. Bfloat16: The secret Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari S. Morcos, Ali Farhadi, and Ludwig Schmidt. Stable and low-precision training for large-scale vision-language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=sqqASmpA2R. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Greg Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022. Greg Yang, James Simon, and Jeremy Bernstein. spectral condition for feature learning. arXiv preprint arXiv:2310.17813, 2023. Ruizhe Zhao, Brian Vogel, Tanvir Ahmed, and Wayne Luk. Reducing underflow in mixed precision In Proceedings of the Twenty-Ninth International Conference on training by gradient scaling. International Joint Conferences on Artificial Intelligence, pp. 29222928, 2021. Jiecheng Zhou, Ding Tang, Rong Fu, Boni Hu, Haoran Xu, Yi Wang, Zhilin Pei, Zhongling Su, Liang Liu, Xingcheng Zhang, et al. Towards efficient pre-training: Exploring fp4 precision in large language models. arXiv preprint arXiv:2502.11458, 2025. Preprint."
        },
        {
            "title": "A RELATED WORK",
            "content": "A.1 MIXED-PRECISION BF16 TRAINING. Contemporary large language model (LLM) pretraining almost universally employs mixed-precision arithmetic. Early efforts by Micikevicius et al. (2017) demonstrated that FP16 trainingusing an FP32 master copy of weights and fixed loss scalingcould match FP32 accuracy for many models. However, the narrow exponent range of FP16 often causes many gradients to underflow, necessitating careful tuning. The bfloat16 (BF16) format, with an 8-bit exponent and 7-bit mantissa, retains the wide dynamic range of FP32 while halving storage cost. Kalamkar et al. (2019) showed that BF16 achieves convergence parity with FP32 on large models without specialized tuning. Since then, BF16 has become the default 16-bit format for many large-scale training frameworks, with native support in PyTorch and TensorFlow (Wang & Kanwar, 2019). BF16 mixed precision has enabled training of landmark LLMs at unprecedented scales, including GPT-3 (175B) (Brown et al., 2020), Google PaLM (540B) (Chowdhery et al., 2023), DeepMind Gopher (280B) (Rae et al., 2021), Chinchilla (70B) (Hoffmann et al., 2022), and Metas LLaMA family (7B-65B) (Touvron et al., 2023). To handle the massive memory footprint, parallel training frameworks such as Megatron and DeepSpeed integrate BF16 training with techniques like the Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020). Despite its advantages, BF16 training can still exhibit instabilities. Empirical studies show that FP16 is highly unstable even with loss scaling, whereas BF16 eliminates most precision-related tuning and failures Wang & Kanwar (2019). However, Lee et al. (2024) report that roughly 10% of GPT-2 pretraining runs diverged under pure BF16, compared to 0% under TF32. This suggests that while BF16 substantially improves stability, complementary stabilization techniques remain necessary at scale. A.2 STABILIZING LOW-PRECISION TRAINING Gradient Scaling. Early work by Micikevicius et al. (2017) introduced FP16 mixed-precision training, where weights, activations, and gradients are stored in half-precision while maintaining master FP32 copy. They also proposed loss scaling to prevent FP16 underflows. Even with loss scaling, some underflow can still occur in deep networks. To address this, Zhao et al. (2021) introduced gradient scaling, which dynamically computes per-layer scaling factors to avoid both underflow and overflow. Ultra-Low-Precision (FP8/INT8) Training. To further reduce cost, recent works explore FP8 or INT8 precision for training and inference. However, naive FP8 training tends to diverge. Lee et al. (2024) note that the direct application of FP8 to LLM training is unstable without additional stabilization techniques. To address this, Perez et al. (2023) propose dynamically adjusted per-tensor scaling factors for FP8 matrix multiplications. Using this scheme, they successfully train GPTand LLaMA-style models up to 70B parameters entirely in FP8. Similarly, Peng et al. (2023) introduce FP8-LM, framework that progressively applies FP8 to gradients, optimizer states, and distributed communication, achieving 39% memory reduction and 75% speedup compared to BF16. Balanca et al. (2024) present SCALIFY, which propagates scale factors throughout the computation graph to ensure stable FP8 operations without manual tuning. These approaches collectively demonstrate that careful scaling management enables FP8 or INT8 training to match BF16 performance while reducing memory and compute requirements. Optimizer and Gradient Stabilization. Optimizer algorithms play critical role in training stability. Molybog et al. (2023) theoretically analyze Adam and show that catastrophic divergence often arises when the update direction becomes uncorrelated with the true descent direction in large-scale models. To address gradient instability, Huang et al. (2025) propose SPAM (Spike-Aware Adam with Momentum Reset), which detects and mitigates rare but severe gradient spikes by resetting momentum and applying spike-aware clipping. In parallel, Wortsman et al. (2023) investigate loss spikes in vision-language models and show that AdamW often underestimates the second moment before spikes occur. They propose hybrid AdamW-AdaFactor optimizer that adaptively corrects 13 Preprint. second-moment underestimation, outperforming gradient clipping alone. These methods highlight how optimizer modifications directly mitigate divergence in low-precision regimes. Activation and Architectural Techniques. The choice of activation functions and initialization strategies also impacts stability. Fishman et al. (2024) observe that the SwiGLU activation amplifies outliers during long FP8 training runs. They introduce Smooth-SwiGLU, modified activation that prevents outlier amplification, enabling stable trillion-token FP8 training. In the vision-language domain, Wortsman et al. (2023) show that layer-scale zero initialization and carefully designed low-precision linear layers (e.g., SwitchBack) further improve stability in int8 training. BF16 ADDITION The bfloat16 (Brain Floating-Point) format is 16-bit floating-point representation widely used in deep learning for its balance between computational efficiency and numerical range. It consists of 1 sign bit, 8 exponent bits, and 7 fraction (or mantissa) bits. This structure gives bfloat16 the same dynamic range as the 32-bit single-precision format (FP32) but with significantly less precision. The addition of two bfloat16 numbers, say and b, follows the standard procedure for floating-point arithmetic: 1. Exponent Alignment: The exponents of the two numbers are compared. The number with the smaller exponent has its significand (the combination of the implicit leading bit and the fraction) shifted to the right until its exponent matches the larger one. Each right shift increases the exponent by one. Bits shifted past the available precision are lost, which is an initial source of error. 2. Significand Addition: The aligned significands are added together. The sign of the result is determined by the signs and magnitudes of the operands. 3. Normalization: The result is normalized to ensure it conforms to the 1.xxxx... 2e format. If the addition resulted in an overflow (e.g., 10.xxxx...), the significand is shifted right and the exponent is incremented. If it resulted in cancellation (e.g., 0.00xx...), the significand is shifted left and the exponent is decremented until the leading bit is 1. 4. Rounding: The resulting significand, which may have more than 7 fraction bits after normalization, must be rounded. The standard mode is round to nearest, ties to even. This means if the truncated part is greater than half the value of the last storable bit (LSB), the number is rounded up. If it is less, it is rounded down. If it is exactly half, it is rounded to the nearest value with an even LSB. The key source of numerical error comes from steps 1 and 4. During exponent alignment, precision is lost from the smaller-magnitude number. After addition, the result must be rounded back to the 7-bit fraction, which introduces another rounding error. While round to nearest, ties to even is designed to be unbiased for random data, sequence of additions on data with specific distribution (e.g., mostly negative numbers being added together) can lead to biased rounding error, where the accumulated error consistently pushes the result in one direction. This accumulation of biased error is critical factor in the training failure observed in low-precision settings."
        },
        {
            "title": "C DESIGN CONSIDERATIONS FOR MITIGATING BIASED ROUNDING ERROR",
            "content": "IN FLASH ATTENTION Use Dynamic Maximum Value Rather Than Fixed Offset fixed offset would cause the computed values of to be consistently rounded in one direction during BF16 conversion, introducing fixed error. Since the elements of often share the same sign, this fixed rounding error in does not average out to zero when computing PV. This leads to biased error in the output O, which in turn creates biased term δ, reintroducing the very failure we aim to solve. Dynamic Maximum is Applied Conditionally Our modification is applied conditionallyonly when row contains multiple identical maximum valuesto avoid introducing new numerical instabilities. An unconditional adjustment is not better option. For example, if row has single, very 14 Preprint. large positive maximum value rm, applying our rule would mean calculating exp(S βrm). The largest term in the exponent would become (β 1)rm, and exp((β 1)rm) could underflow to zero. This would cause the normalization factor to become zero, leading to division-by-zero error when computing the output O. By applying the modification only in the specific case that causes biased rounding, we preserve the numerical stability of the standard online softmax in all other scenarios. Explanation on Dealing Negative Repeated Row Maximum We also explore alternative stabilization methods for the negative, repeated row maximum (rm < 0). One approach involves setting the normalization factor = γrm for some γ (0, 1). This makes the new maximum value in the exponent (1 γ)rm. However, we observe that if γ is close to 1, this new maximum approaches zero. In low-precision arithmetic, exp((1 γ)rm) can round to exactly 1, reintroducing the very failure we aim to prevent. Consequently, we found that setting γ = 0 (i.e., = 0) is robust choice, as it ensures the maximum value in the exponent remains sufficiently negative. Algorithm 1 Stablized Flash Attention by Mitigating Biased Rounding Error: Forward Pass Require: Matrices Q, K, RN d, block sizes Bc, Br, β > 1. 1: Divide into Tr = (cid:108) Bc blocks K1, . . . , KTc and V1, . . . , VTc, of size Bc each. blocks Q1, . . . , QTr of size Br each, and divide K, in to (cid:108) Br Tc = (cid:109) (cid:109) 2: Divide the output RN into Tr blocks O1, . . . , OTr of size Br each, and divide the logsumexp into Tr blocks L1, . . . , LTr of size Br each. = (0)Brd RBrd, ℓ(0) = (0)Br RBr , m(0) = ()Br RBr . 3: for 1 Tr do 4: 5: 6: Initialize O(0) for 1 Tc do Compute S(j) rm = rowmax(S(j) m(j) m(j) Compute m(j) wise), ℓ(j) Compute O(j) = em(j1) 7: 8: 9: 10: = QiKT RBrBc . ), rs = rowsum(S(j) rm) = where(rm > 0 rs > 1, βrm, rm) = where(rm < 0 rs > 1, 0, m(j) ) = max(m(j1) m(j) ℓ(j1) , rm) RBr , P(j) + rowsum( P(j) )O(j1) = exp(S(j) ) RBr . Vj. + P(j) m(j) = diag(em(j1) m(j) ) RBrBc (point11: 12: 13: end for Compute Oi = diag(ℓ(Tc) Compute Li = m(Tc) )1O(Tc) + log(ℓ(Tc) ). 14: 15: Write Oi as the i-th block of O. 16: Write Li as the i-th block of L. 17: end for 18: Return the output and the logsumexp L. . 15 Preprint. Algorithm 2 Flash Attention: Forward Pass Require: Matrices Q, K, RN d, block sizes Bc, Br. (cid:108) 1: Divide into Tr = Br (cid:108) Bc Tc = (cid:109) (cid:109) blocks K1, . . . , KTc and V1, . . . , VTc, of size Bc each. blocks Q1, . . . , QTr of size Br each, and divide K, in to 2: Divide the output RN into Tr blocks O1, . . . , OTr of size Br each, and divide the logsumexp into Tr blocks L1, . . . , LTr of size Br each. = (0)Brd RBrd, ℓ(0) = (0)Br RBr , m(0) = ()Br RBr . RBrBc . = QiKT = max(m(j1) = em(j1) m(j) , rowmax(S(j) m(j) ℓ(j1) )O(j1) )) RBr , P(j) + rowsum( P(j) Vj. + P(j) = exp(S(j) ) RBr . = diag(em(j1) m(j) ) 3: for 1 Tr do 4: 5: 6: Initialize O(0) for 1 Tc do Compute S(j) Compute m(j) RBrBc (pointwise), ℓ(j) Compute O(j) 7: 8: 9: 10: end for Compute Oi = diag(ℓ(Tc) Compute Li = m(Tc) )1O(Tc) + log(ℓ(Tc) ). 11: 12: Write Oi as the i-th block of O. 13: Write Li as the i-th block of L. 14: end for 15: Return the output and the logsumexp L. . Algorithm 3 Flash Attention: Backward Pass Require: Matrices Q, K, V, O, dO RN d, vector RN , block sizes Bc, Br. 1: Divide into Tr = (cid:108) Bc blocks K1, . . . , KTc and V1, . . . , VTc, of size Bc each. (cid:108) Br Tc = (cid:109) (cid:109) blocks Q1, . . . , QTr of size Br each, and divide K, in to 2: Divide into Tr blocks O1, . . . , OTr of size Br each, divide dO into Tr blocks dO1, . . . , dOTr of size Br each, and divide into Tr blocks L1, . . . , LTr of size Br each. 3: Initialize dQ = (0)N and divide it into Tr blocks dQ1, . . . , dQTr of size Br each. Divide dK, dV RN in to Tc blocks dK1, . . . , dKTc and dV1, . . . , dVTc , of size Bc each. 4: Compute δ = rowsum(dO O) RN (pointwise multiply), and divide it into Tr blocks δ1, . . . , δTr of size Br each. 5: for 1 Tc do 6: 7: 8: RBrBc . = QiKT = exp(Sij Li) RBrBc . Initialize dKj = (0)Bcd, dVj = (0)Bcd. for 1 Tr do Compute S(j) Compute P(j) Compute dVj dVj + (P(j) Compute dP(j) Compute dS(j) Update dQi dQi + dS(j) Compute dKj dKj + dS(j) = dOiV = P(j) Kj RBrd. RBrBc . (dP(j) dQi RBcd. )dOi RBcd. δi) RBrBc. 9: 10: 11: 12: 13: end for 14: 15: 16: end for 17: Return dQ, dK, dV. Preprint. (a) Loss in nanoGPT Issue #303 (b) Loss in nanoGPT Issue #524 Figure 8: Loss curves of two independent runs of GPT-2 training with flash attention in BF16 reported in the Github issue of nanoGPT. 17 Preprint. Figure 9: Spectral norm across layers and training steps. 18 Preprint. Figure 10: Token difference visualization"
        }
    ],
    "affiliations": [
        "Department of Electronic Engineering, Tsinghua University"
    ]
}