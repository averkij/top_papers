{
    "paper_title": "ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality",
    "authors": [
        "Yefei He",
        "Feng Chen",
        "Yuanyu He",
        "Shaoxuan He",
        "Hong Zhou",
        "Kaipeng Zhang",
        "Bohan Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we propose ZipAR, a training-free, plug-and-play parallel decoding framework for accelerating auto-regressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Given a partially decoded set of visual tokens, in addition to the original next-token prediction scheme in the row dimension, the tokens corresponding to spatially adjacent regions in the column dimension can be decoded in parallel, enabling the ``next-set prediction'' paradigm. By decoding multiple tokens simultaneously in a single forward pass, the number of forward passes required to generate an image is significantly reduced, resulting in a substantial improvement in generation efficiency. Experiments demonstrate that ZipAR can reduce the number of model forward passes by up to 91% on the Emu3-Gen model without requiring any additional retraining."
        },
        {
            "title": "Start",
            "content": "ZipAR: Accelerating Auto-Regressive Image Generation through Spatial Locality Yefei He1,2 Feng Chen3 Yuanyu He1 Shaoxuan He1 Hong Zhou1 Kaipeng Zhang2 Bohan Zhuang1 1Zhejiang University, China 2Shanghai AI Laboratory, China 3The University of Adelaide, Australia"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we propose ZipAR, training-free, plug-and-play parallel decoding framework for accelerating auto-regressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Given partially decoded set of visual tokens, in addition to the original next-token prediction scheme in the row dimension, the tokens corresponding to spatially adjacent regions in the column dimension can be decoded in parallel, enabling the next-set prediction paradigm. By decoding multiple tokens simultaneously in single forward pass, the number of forward passes required to generate an image is significantly reduced, resulting in substantial improvement in generation efficiency. Experiments demonstrate that ZipAR can reduce the number of model forward passes by up to 91% on the Emu3-Gen model without requiring any additional retraining. 4 2 0 2 5 ] . [ 1 2 6 0 4 0 . 2 1 4 2 : r Figure 1: Up to 91% forward step reduction with ZipAR. Samples are generated by Emu3-Gen model with next-token prediction paradigm (the first column) and ZipAR (the right three columns). Work in progress. Corresponding authors. Email: zhouhong_zju@zju.edu.cn, kp_zhang@foxmail.com Figure 2: (a) An overview of the training and decoding pipeline for auto-regressive (AR) visual generation models. For models trained with next-token prediction objective, each forward pass generates single visual token. (b) Medusa [2] and Jacobi [16] decoding predict multiple adjacent tokens in sequence order. (c) MAR [13] predicts multiple tokens in random order. (d) The proposed ZipAR predicts multiple spatially adjacent tokens."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large language models (LLMs) with the next-token prediction paradigm [1, 23, 19] have demonstrated remarkable capabilities in addressing text-related tasks. Building on these successes, many studies [14, 24, 18, 9, 25] have extended this paradigm to the generation of visual content, leading to the development of auto-regressive (AR) visual generation models. These models not only produce high-fidelity images and videos that rival or even exceed the performance of state-of-the-art diffusion models but also facilitate unified multimodal understanding and generation. However, their slow generation speed remains significant barrier to widespread adoption. To generate high-resolution images or videos, these models must sequentially produce thousands of visual tokens, requiring numerous forward passes and resulting in high latency. To reduce the number of forward passes required for generating lengthy responses, several studies [2, 16, 12] have proposed the next-set prediction paradigm for LLMs, as depicted in Figure 2(b). These approaches involves introducing multiple decoding heads [2] or small draft models [12], which generate several candidate tokens that are later evaluated by the original model. However, these methods incur additional costs, as they require extra draft models or the training of new decoding heads. Another approaches use the jacobi decoding methods [16, 8, 20], iteratively updates sequences of tokens until convergence. However, in practice, the acceleration achieved by these methods is marginal, as LLMs often fail to generate correct tokens when errors exist in preceding ones. Furthermore, none of these approaches exploit the unique characteristics of visual content, and parallel decoding framework specifically tailored for AR visual generation has yet to be developed. In this paper, we introduce ZipAR, parallel decoding framework designed to accelerate AR visual generation. As depicted in Figure 2(a), common AR visual generation models produce visual tokens in raster order, where the first token in row cannot be generated until the last token in the preceding row is decoded despite their spatial separation. However, visual content inherently exhibits strong locality, which is widely utilized inductive bias for visual tasks [6, 11]. Specifically, there are significant spatial correlations between spatially adjacent tokens (e.g., token 5 and token 1 in 2 Figure 3: The attention scores of visual tokens in the Lumina-mGPT-7B [14] and LlamaGen-XL [17] models. Slash lines indicate that significant attention scores are allocated to tokens at fixed intervals, corresponding to tokens in the same column of previous rows. The full attention scores are presented by storing the attention scores of each visual token during decoding and concatenating them. Figure 2(a)) compared to tokens that are adjacent only in the generation order (e.g., token 5 and token 4), which makes the raster-order sequential dependency suboptimal. Empirical evidence, as shown in Figure 3, further supports this observation, with significant attention allocated to tokens in the same column of the previous row. This motivates us to propose decoding tokens from the next row without waiting for the full decoding of the current row, enabling the parallel decoding of multiple tokens in single forward pass. Specifically, we define fixed window size to determine whether two tokens are spatially adjacent. Tokens outside this window in adjacent rows are considered irrelevant. Consequently, once the number of generated tokens in row exceeds the window size, decoding of the next row begins in parallel with the current row. With an appropriately chosen window size, multiple rows can be decoded simultaneously. Unlike speculative [12] or Medusa [2] decoding methods, all tokens generated in parallel by ZipAR are produced using the original model head, without the need for further evaluation or updates. As result, ZipAR can be seamlessly implemented in training-free, plug-and-play manner for auto-regressive visual generation models, without introducing additional overhead. Experiments across multiple auto-regressive visual generation models demonstrate the effectiveness and robustness of ZipAR, achieving 91% forward steps reduction on Emu3-Gen with minimal degradation in image quality."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Auto-regressive Visual Generation The success of Transformer models in text-based tasks has inspired studies [22, 7, 26] to apply auto-regressive modeling to visual content generation. These methods can be classified into two main categories: GPT-style approaches that utilize the next-token prediction paradigm [7, 24, 14, 17] and BERT-style approaches that employ masked prediction models [4, 3, 13, 26]. More recently, VAR [21] modified the traditional next-token prediction paradigm to next-scale prediction, resulting in faster sampling speeds. Models trained using next-token prediction can leverage the infrastructure and training techniques of large language models (LLMs) and pave the way towards unified multi-modal understanding and generation. However, they are generally less efficient during sampling compared to models that predict multiple tokens in single forward pass. In this paper, we focus on accelerating visual generation models trained with the next-token prediction objective, hereafter referred to as auto-regressive visual generation models unless otherwise specified. 2.2 Efficient Decoding of LLMs. Efforts to reduce the number of forward passes required for LLMs to generate lengthy responses can be broadly categorized into two main approaches. The first approach involves sampling multiple candidate tokens before verifying them with the base LLM. Speculative decoding [12] utilizes small draft LLM to generate candidate tokens, which are then verified in parallel by the base LLM. While 3 this approach can potentially generate multiple tokens in single evaluation, deploying multiple models introduces significant memory overhead and engineering challenges. Medusa [2] addresses this by employing multiple decoding heads for the base LLM, enabling self-speculation. However, due to the large vocabulary size of LLMs, the parameters in each decoding head can be substantial. The second approach, Jacobi decoding [16, 20], involves randomly guessing the next tokens in sequence, which are iteratively updated by the LLMs. Over time, the n-token sequence converges to the same output as that generated by the next-token prediction paradigm. However, in practice, vanilla Jacobi decoding offers only marginal speedup over auto-regressive decoding, with an average speedup of just 1.05. This limited improvement is largely due to the causal attention mechanism, which rarely produces correct token when preceding tokens are incorrect. Lookahead [8] decoding enhances efficiency by leveraging n-grams generated from previous Jacobi iterations, which are verified in parallel during the decoding process. CLLMs [10] further improve the efficiency of Jacobi decoding by fine-tuning the model with consistency loss, requiring it to map arbitrary points on the Jacobi trajectory to fixed point. However, none of these approaches are designed for auto-regressive visual generation or incorporate visual inductive biases. In contrast, the proposed ZipAR takes advantage of the spatial locality inherent in visual content, offering significant acceleration without the need for retraining. Moreover, ZipAR is orthogonal to the aforementioned methods, and can be combined with them to achieve even greater acceleration."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries Auto-regressive (AR) visual generation models with the next-token prediction paradigm have shown exceptional versatility across various vision-language tasks, including generating high-quality images and videos. As shown in Figure 2(a), pre-trained VQ-VAE models [22, 7] are commonly employed to convert images or videos into visual tokens. The process begins with visual encoder that extracts feature maps at reduced spatial resolution. These feature maps are then subjected to vector quantization to produce discrete latent representations, known as visual tokens. These tokens are arranged in one-dimensional sequence to serve as input for AR models. Although various methods exist to flatten these tokens, the row-major order (raster order) is empirically validated to offer the best performance [7], making it the prevalent method for visual generation. During the image generation phase, AR models generate visual tokens sequentially in this raster order. Finally, the complete sequence of visual tokens is rearranged into two-dimensional structure and processed through visual decoder to reconstruct the images. 3.2 Inference with ZipAR Figure 4: toy example of the ZipAR framework. The window size is set to 2 in this toy example. As analyzed in Section 3.1, AR visual generation models with raster order generate visual tokens row by row, completing each row sequentially from left to right before proceeding to the next. However, images inherently exhibit strong spatial locality. Intuitively, in high-resolution image, the starting pixel of row is more closely related to the starting pixel of the preceding row than to the ending pixel of the preceding row due to their spatial proximity. Empirical evidence, as shown in Figure 3, also indicates that significant attention scores are allocated to tokens within the same column of the 4 previous row. Building on these observations, we propose ZipAR, simple yet effective parallel decoding framework for auto-regressive visual generation models. Unlike conventional parallel decoding methods that predict multiple consecutive tokens in single forward pass, our approach decodes tokens from different rows in parallel. The key idea is that it is unnecessary to wait for an entire row to be generated before initiating the decoding of the next row, as spatially distant tokens contribute minimally to attention scores. To formalize this, we define local window size s. Given the tokens xi,j located in row and column j, we assume that tokens beyond xi1,j+s in the previous row have negligible impact on the generation of xi,j based on the spatial locality of visual tokens. Consequently, the criterion for initiating the generation of token xi,j can be formulated as: C(i, j) = (cid:26)1, if {xi1,k + s} 0, otherwise (1) Here, denotes the set of decoded tokens, and C(i, j) = 1 indicates that token xi,j is ready to be generated. Once the first token in row is generated, subsequent tokens in the row can be generated sequentially, along with the unfinished portion of the preceding row, following next-token prediction paradigm. However, to initiate the decoding of the first token xi,0 in row i, the last token of the row 1 is required as input to the auto-regressive model, despite it has not yet been generated in the ZipAR framework. To address this, we propose several solutions tailored to different types of AR visual generation models. Some methods [14, 24] support generating images with dynamic resolutions, typically by appending extra end-of-row tokens at the end of each row. With these special tokens placed at fixed positions, we can insert the end-of-row tokens in advance when initiating the generation of the next row. Since the values of these tokens are predetermined, there is no need to update them subsequently. Conversely, for models that lack end-of-row tokens, we temporarily assign values to the last token in row 1 to decode token xi,0. This value can be derived from the most spatially adjacent token that have been decoded."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details To assess the effectiveness of our proposed method, we integrate it with three state-of-the-arts auto-regressive visual generation models: LlamaGen [17], Lumina-mGPT [14] and Emu3-Gen [24]. For text-guided image generation with LlamaGen, we generate 30,000 images and compute CLIP scores against MS-COCO 2014-val dataset with CLIP ViT-B/32 model [15]. For class-conditional image generation with LlamaGen on ImageNet, we report the widely adopted Frechet Inception Distance (FID) to evaluate the performance. We sample 50,000 images and evaluate them with ADMs TensorFlow evaluation suite [5]. 4.2 Quantitative Results 4.2.1 Class-conditional Image Generation In this subsection, we quantitatively assess the performance of class-conditional image generation on the ImageNet 256 256 benchmark using the LlamaGen model, as detailed in Table 1. The model employs 24 24 feature map and necessitates 576 forward passes to generate an image under the next-token prediction (NTP) paradigm. By incorporating ZipAR with window size of 16, the number of forward passes is reduced by 30.5%, with only 0.16 increase in FID. Fine-tuning the pre-trained model for single epoch with an ZipAR-oriented attention mask consistently enhances performance. Specifically, ZipAR-16 reduces the number of forward passes by 30.5% while achieving lower FID compared to the original model with NTP paradigm. ZipAR-12 further reduces the number of forward passes by 45.8% while maintaining an FID of 3.49. 4.2.2 Text-guided Image Generation In this subsection, we quantitatively evaluate the effects of ZipAR on text-guided image generation performance. The LlamaGen-XL model processes feature map of 32 32 and requires 576 forward 5 Table 1: Quantitative evaluation on ImageNet 256 256 benchmark. Here, NTP denotes the next-token prediction paradigm. ZipAR-n denotes the ZipAR paradigm with window size of n. Step is the number of model forward passes required to generate an image. The latency is measured with batch size of 1."
        },
        {
            "title": "Model",
            "content": "LlamaGen-L Method NTP ZipAR-16 ZipAR-12 ZipAR-8 Step 576 400 (-30.5%) 312 (-45.8%) 224 (-61.1%) Latency (s) 15.20 10.38 (-24.7%) 8.38 (-39.2%) 6.56 (-52.4%) FID 3.16 3.32 4.50 10.22 passes to generate an image under the next-token prediction (NTP) paradigm. As shown in Table 2, ZipAR-12 results in minimal performance degradation, with 0.001 decrease in the CLIP score, while reducing inference latency by 61.0%. Notably, ZipAR-4 significantly reduces the inference latency by 83.3% and retains good semantic information, as indicated by CLIP score of 0.280. Visualization results are presented in Figures 1 and 6. Moreover, we observe that the acceleration ratio for the text-to-image LlamaGen-XL model is higher than that for the class-conditional LlamaGen-L model, primarily due to the higher spatial resolution of the feature maps and generated images. This suggests that ZipAR achieves greater generation efficiency improvements when generating higher-resolution images. Table 2: Quantitative evaluation on MS-COCO dataset. Here, NTP denotes the next-token prediction paradigm. ZipAR-n denotes the ZipAR paradigm with window size of n. Step is the number of model forward passes required to generate an image. The latency is measured with batch size of 1. Model LlamaGen-XL Method NTP ZipAR-16 ZipAR-12 ZipAR-8 ZipAR-4 Step 1024 544 (-46.8%) 424 (-58.6%) 304 (-70.3%) 184 (-82.0%) Latency (s) 33.17 17.65 (-46.8%) 12.91 (-61.0%) 9.44 (-71.5%) 5.51 (-83.3%) CLIP Score 0.287 0.287 0.286 0.285 0.280 4.3 Qualitative Visualizations In this subsection, we present non-cherry-picked visualizations of images generated using the next-token prediction (NTP) paradigm and the proposed ZipAR framework across Emu3-Gen [24], LlamaGen [17], and Lumina-mGPT [14], as shown in Figures 5 to 7. Notably, ZipAR can reduce the number of model forward steps by up to 91%, 82%, and 67% for Emu3-Gen, LlamaGen, and Lumina-mGPT, respectively, while still producing high-fidelity images rich in semantic information."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "In this paper, we have proposed ZipAR, new parallel decoding framework designed to accelerate auto-regressive visual generation. ZipAR leverages the spatial locality inherent in visual content and predicts multiple spatially adjacent visual tokens in single model forward pass, thereby significantly enhancing generation efficiency compared to the traditional next-token-prediction paradigm. Extensive experiments demonstrate that ZipAR can reduce the number of model forward steps by up to 91% on the Emu3-Gen model with minimal impact on image quality. In the future, we anticipate that integrating ZipAR with other methods that employ the next-setprediction paradigm, such as Medusa [2] and Jacobi decoding [16], will further enhance acceleration ratios. 6 Figure 5: Samples generated by Emu3-Gen model with next-token prediction paradigm (the first column) and ZipAR under different configurations (the right three columns). The classifier-free guidance is set to 6.0. 7 Figure 6: Samples generated by LlamaGen-XL model with next-token prediction paradigm (the first column) and ZipAR under different configurations (the right three columns). The classifier-free guidance is set to 7.5. 8 Figure 7: Samples generated by Lumina-mGPT-7B-768 model with next-token prediction paradigm (the first column) and ZipAR under different configurations (the right three columns). The classifierfree guidance is set to 3."
        },
        {
            "title": "References",
            "content": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] T. Cai, Y. Li, Z. Geng, H. Peng, J. D. Lee, D. Chen, and T. Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. [3] H. Chang, H. Zhang, J. Barber, A. Maschinot, J. Lezama, L. Jiang, M.-H. Yang, K. Murphy, W. T. Freeman, M. Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. [4] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11315 11325, 2022. [5] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [6] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020. [7] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [8] Y. Fu, P. Bailis, I. Stoica, and H. Zhang. Break the sequential dependency of llm inference using lookahead decoding. arXiv preprint arXiv:2402.02057, 2024. [9] Y. Ge, S. Zhao, J. Zhu, Y. Ge, K. Yi, L. Song, C. Li, X. Ding, and Y. Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [10] S. Kou, L. Hu, Z. He, Z. Deng, and H. Zhang. Cllms: Consistency large language models. arXiv preprint arXiv:2403.00835, 2024. [11] Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Handwritten digit recognition with back-propagation network. Advances in neural information processing systems, 2, 1989. [12] Y. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 1927419286. PMLR, 2023. [13] T. Li, Y. Tian, H. Li, M. Deng, and K. He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. [14] D. Liu, S. Zhao, L. Zhuo, W. Lin, Y. Qiao, H. Li, and P. Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining. arXiv preprint arXiv:2408.02657, 2024. [15] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [16] A. Santilli, S. Severino, E. Postolache, V. Maiorca, M. Mancusi, R. Marin, and E. Rodol√†. Accelerating transformer inference for translation via parallel decoding. arXiv preprint arXiv:2305.10427, 2023. [17] P. Sun, Y. Jiang, S. Chen, S. Zhang, B. Peng, P. Luo, and Z. Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [18] C. Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [19] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [20] Y. Teng, H. Shi, X. Liu, X. Ning, G. Dai, Y. Wang, Z. Li, and X. Liu. Accelerating auto-regressive text-to-image generation with training-free speculative jacobi decoding. arXiv preprint arXiv:2410.01699, 2024. [21] K. Tian, Y. Jiang, Z. Yuan, B. Peng, and L. Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. [22] A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [23] R. Vavekanand and K. Sam. Llama 3.1: An in-depth analysis of the next-generation large language model, 2024. 10 [24] X. Wang, X. Zhang, Z. Luo, Q. Sun, Y. Cui, J. Wang, F. Zhang, Y. Wang, Z. Li, Q. Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [25] C. Wu, X. Chen, Z. Wu, Y. Ma, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, C. Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. [26] L. Yu, Y. Cheng, K. Sohn, J. Lezama, H. Zhang, H. Chang, A. G. Hauptmann, M.-H. Yang, Y. Hao, I. Essa, et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1045910469, 2023."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory, China",
        "The University of Adelaide, Australia",
        "Zhejiang University, China"
    ]
}