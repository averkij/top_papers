{
    "paper_title": "Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment",
    "authors": [
        "Yuming Yang",
        "Mingyoung Lai",
        "Wanxu Zhao",
        "Xiaoran Fan",
        "Zhiheng Xi",
        "Mingqi Wu",
        "Chiyue Huang",
        "Jun Zhao",
        "Haijun Lv",
        "Jian Tong",
        "Yunhua Zhou",
        "Yicheng Zou",
        "Qipeng Guo",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory's average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection."
        },
        {
            "title": "Start",
            "content": "2026-1-21 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment Yuming Yang1,2, Mingyoung Lai3, Wanxu Zhao1, Xiaoran Fan1, Zhiheng Xi1, Mingqi Wu1, Chiyue Huang4, Jun Zhao1, Haijun Lv2, Jian Tong2, Yunhua Zhou2, Yicheng Zou2,, Qipeng Guo2, Tao Gui1, Qi Zhang1, and Xuanjing Huang1 1Fudan University, 2Shanghai AI Laboratory, 3University of Toronto, 4University of Sydney Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of datastudent suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the models current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), simple metric that captures both alignment and informativeness to assess the suitability of reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of trajectorys average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection. 1. Introduction Recent advances in reasoning-oriented large language models (LLMs) are largely driven by their ability to generate long chain-of-thought (CoT) trajectories [12, 48]. Beyond enabling complex inference at test time, such trajectories also provide powerful supervision signals for training student models [28, 35] or cold-starting reinforcement learning [39] through supervised fine-tuning (SFT). Yet, stronger reasoning teachers do not necessarily yield better students [11, 22]. Our extensive experiments show that the post-training effectiveness of reasoning trajectories varies substantially across student models, indicating that the suitability between data and student is critical for effective learning. Existing data engineering methods assess data suitability primarily through the students probability assignments [18, 47], favoring high-likelihood trajectories that align closely with the models current behavior. Such trajectories, however, often provide limited new learning signals. In contrast, more informative trajectories are typically less familiar to the student and thus overlooked by these methods. This leads to fundamental Informative Alignment challenge: how to identify reasoning data that are both well aligned with the student and sufficiently informative? To address this challenge, we propose simple yet effective metric, Rank-Surprisal Ratio (RSR), which quantifies the suitability of reasoning trajectory for given student by jointly capturing alignment and informativeness. Motivated by our preliminary analysis, we argue that the dilemma between providing new signals and aligning with students existing behavior can be resolved by trajectories exhibiting absolute unfamiliarity and relative familiarity. Concretely, effective trajectories should deviate from the students own generations, receiving low absolute probability under the student model, while still containing behavioral Corresponding authors. Inquiries may be sent to: yumingyang23@m.fudan.edu.cn, zouyicheng@pjlab.org.cn, qz@fudan.edu.cn * Code is available at https://github.com/UmeanNever/RankSurprisalRatio. 6 2 0 2 0 ] . [ 1 9 4 2 4 1 . 1 0 6 2 : r Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment patterns within the students prior experience, such that their tokens rank relatively high in the models prediction distribution over the vocabulary (Figure 1). Based on this insight and consistent numerical patterns observed in simulation studies, we define our suitability metric, Rank-Surprisal Ratio, as the ratio between trajectorys average token-wise rank1 and its average negative log-likelihood (surprisal). RSR can be computed with single forward pass, requires no additional verifier or test data, and is straightforward to interpret. Lower RSR indicates better informative alignment, identifying trajectories that are both informative and well aligned with the student. We validate the effectiveness of Rank-Surprisal Ratio through correlation analyses on 5 student LLMs using math reasoning trajectories generated by 11 representative teacher models. Across all students, the RSR of trajectories exhibits strong correlation with post-training performance, achieving an average Spearman correlation of 0.86 and consistently outperforming alternative metrics. Furthermore, to explore its practical value in data engineering, we apply RSR to trajectory selection and teacher selection. Our experiments show that RSR not only selects more effective training trajectories for each problem from candidates generated by diverse teachers, but also identifies more suitable teacher models using only small amount of data, consistently outperforming existing selection methods across all five students in both settings. Figure 1: Illustration of the intuition behind RSR. Suitable reasoning trajectories should balance informativeness and alignment by having low absolute probability but relatively high-ranked tokens under the student model. Our main contributions are three-fold: We present systematic distillation study across wide range of teacher and student models, showing that the effectiveness of reasoning trajectories differs across students and highlighting the importance of datastudent suitability. We propose Rank-Surprisal Ratio, simple metric that quantifies the suitability of reasoning trajectory by jointly capturing alignment and informativeness, achieving strong correlation with post-training performance. We demonstrate the practical utility of Rank-Surprisal Ratio in two data engineering scenarios, trajectory selection and teacher selection, where it serves as an effective criterion and outperforms existing methods. 2. The Need for Student-Specific Data To understand which types of reasoning trajectories most effectively improve student models after SFT, we conduct comprehensive large-scale study involving five widely adopted student models and eleven diverse reasoning-oriented teacher models, yielding 55 teacherstudent pairings. We perform SFT experiments for each of these pairs. 2.1. Experimental Settings Our teacher-student pairing study involves two major steps: (1) For each teacher model, we prompt it to generate long CoT response for each math problem in our 5000-problem set (see A.1), forming trajectory dataset specific to that teacher. (2) For each teacherstudent pair, we fine-tune the student model on the corresponding teacher dataset and evaluate its reasoning performance. All students are pretrained base models. To reduce variance induced by stochastic trajectory sampling, we perform three independent generation runs for each teacher and conduct SFT separately on each resulting dataset for every teacherstudent pair. Reported results are averaged over these three runs. More implementation details are provided in the appendix. 1Higher-ranked tokens have lower rank values. 2 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment Teacher Models Params Deepseek-R1 Qwen-3-235B-Thinking GPT-OSS-120B Nemotron-Super QwQ-32B Qwen-3-30B-Thinking Magistral-Small GPT-OSS-20B Phi-4-Reasoning-Plus Qwen-3-8B Qwen-3-4B-Thinking 671B 235B 120B 49B 32B 30B 24B 20B 14B 8B 4B Qwen-3-14B LLaMA-3.1-8B Qwen-2.5-7B Qwen-3-4B Qwen-2.5-3B Student Models (Base) Teacher Performance 77.1 71.8 66.7 72.2 77.4 77.2 68.8 69.5 54.1 74.6 76.8 28.1 22.0 15.2 23.7 27.1 26.7 22.8 17.9 14.5 26.5 28.2 47.3 45.0 40.7 48.3 52.0 50.0 47.6 42.7 35.2 52.0 51.8 55.8 53.4 47.9 56.4 61.2 58.8 52.2 48.4 40.2 61.2 61.9 29.6 26.4 22.9 33.0 33.0 31.2 30.6 24.4 18.2 34.2 33. 91.1 91.2 88.3 82.3 85.2 92.3 71.0 83.4 72.7 82.5 87.3 Table 1: Distillation results showing post-training reasoning performance of student models trained on trajectories from different teacher models, evaluated by average Acc@4 on AIME25, AIME24, AMC23, and MATH500. Darker and lighter shading indicate the best and second-best results, respectively. Student performance varies significantly across teacherstudent pairs, highlighting the importance of datastudent suitability. Teachers We use 11 reasoning LLMs spanning 4B to 671B parameters across multiple model families, including GPT-OSS [3], DeepSeek [12], Qwen [39], LLaMA-Nemotron [4], and Phi [1]. Benchmarks We evaluate the reasoning performance of the fine-tuned student models on four standard math benchmarks: AIME25, AIME24, AMC23, and MATH500 [14] using the Acc@4 metric, and report results averaged across all benchmarks. 2.2. Results Table 1 presents the results of our teacherstudent pairing distillation study, revealing that: Teacher capability, whether measured by Stronger teachers do not necessarily produce better students. parameter scale or reasoning performance, does not reliably predict student improvement. For example, the 671B and 235B models often underperform smaller teachers such as QwQ-32B on multiple students. Similarly, teachers with strong reasoning performance do not consistently yield the best outcomes for all student models. The effectiveness of teacher Datastudent suitability is critical for eliciting reasoning improvements. trajectories is highly student-specific and depends critically on their suitability for the student model. Pairing strong teachers (Deepseek-R1) with much weaker students (Qwen-2.5-3B) often fails to yield strong performance, while weaker teachers (Qwen-3-4B-Thinking) may likewise be ineffective at improving stronger students (Qwen-3-14B). Moreover, teachers from distant model families (GPT-OSS) often lead to inferior results, suggesting that unfamiliar reasoning patterns are harder for students to absorb. Overall, we find no simple teacherstudent pairing rule based on surface attributes such as parameter scale or model family, indicating that reasoning data suitability is nuanced property requiring deeper investigation. 3. Measuring Data-Student Suitability In this section, we explore metrics for measuring datastudent suitability, with the goal of jointly capturing informativeness and alignment. We begin by introducing two fundamental token-level measures: surprisal and rank ( 3.1), and analyzing the limitations of existing probability-based metrics ( 3.2). We then abstract our insights on suitable reasoning data and conduct simulation studies to identify quantitative patterns ( 3.3). Finally, we propose our trajectory-level metric ( 3.4). 3.1. Surprisal and Rank We introduce two different methods to quantify the amount of information carried by token with respect to student model. They serve as building blocks of our metric. Surprisal (Negative Log-Likelihood) common measure is based on the probability of generating the 3 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment Teacher Models Qwen-3-8B Qwen-3-30B-Thinking Nemotron-Super Deepseek-R1 Magistral-Small GPT-OSS-20B Student Performance Probability-based Metrics Avg-Surprisal Avg-Surplocal Rank-Surprisal Metrics Avg-RSRtoken Avg-RSRfilter token RSR (Ours) 52.0 50.0 48.3 47.8 47.6 42. 0.65 0.77 0.60 0.83 0.55 1.36 1.16 1.27 1.04 1.35 1.03 1.78 2.01107 2.25107 5.51107 2.98107 3.58107 5.15106 3.15 3.47 3.95 3.31 4.38 11.10 2.89 2.95 3.08 3.00 3.09 3.83 Table 2: Comparison of the students post-training performance and datastudent suitability metrics across trajectories from different teacher models, evaluated on Qwen-2.5-7B. Darker shading indicates higher performance or better suitability. Metrics whose trends align with performance (e.g., RSR) provide more reliable suitability estimates. Complete metric scores are provided in C.2. current token 洧노洧녲 given its preceding context c洧녲 = (洧노1, . . . , 洧노洧녲1) under the student model 洧랚. For numerical stability, probabilities are typically transformed into the log space, and the negative log-likelihood is used as measure of informativeness, also known as Surprisal [13] Surprisal(洧노洧녲) = log 洧녷洧랚 (洧노洧녲 c洧녲 ) (1) Another method considers the rank of the current token within the models prediction distribution Rank over the vocabulary 洧눰. Formally, given the conditional distribution 洧녷洧랚( c洧녲), the rank of token 洧노洧녲 is defined as the number of tokens with strictly higher probability [32]. Rank(洧노洧녲) = 1 + 洧노洧눰 I[洧녷洧랚(洧노 c洧녲) > 洧녷洧랚(洧노洧녲 c洧녲)] (2) Unlike surprisal, rank captures relative familiarity of the token by comparing target token against alternative candidates, revealing signals overlooked by probability-based measures. For instance, token may receive low absolute probability while still being among the top-ranked candidates. 3.2. Limitations of Probability-Based Metrics Existing work primarily relies on surprisal or log-probability to assess data suitability. For example, Zhang et al. [47] selects trajectories based on the average log-probability of response tokens under the student model. Since surprisal is the negation of log-probability, we implement this metric as the average surprisal, denoted as Avg-Surprisal. More recently, Just et al. [18] computes token-level log-probability based on local context clocal (several preceding sentences), which we implement as average local surprisal (Avg-Surplocal). Under these 洧녲 metrics, trajectories with lower surprisal are considered more suitable. However, as shown in Table 2, lower surprisal (i.e., higher probability) does not necessarily lead to better posttraining reasoning performance. Both Avg-Surprisal and Avg-Surplocal assign lower surprisal to trajectories from Nemotron-Super and Magistral-Small, yet training on such trajectories fails to improve reasoning performance. Similar patterns are observed across all student models, indicating that probability-based metrics tend to favor data that are familiar but insufficiently informative. At the other extreme, trajectories with very high surprisal (e.g., GPT-OSS-20B) also perform poorly. In contrast, trajectories with moderate surprisal values (e.g., Qwen-3-8B) achieve better results. This observation motivates us to investigate the mechanism underlying the surprisal trade-off. 3.3. Insight and Simulation The above analysis suggests that effective teacher trajectories should balance the informativeness of the data with their alignment to the students current behavior: they should be neither overly similar to the students own generations nor excessively deviant from its predicted distribution. 4 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment Trajectories 洧녦洧냢 (from 洧녨洧냢) 洧녦洧냣 (from 洧녨洧냣) 洧녦洧냤 (from 洧녨洧냤) 洧녦洧냥 (from 洧녨) Prob. 0.41 0.10 0.08 0.35 Surprisal Rank RSRtoken 2.49 4.31 11.57 2.93 1.69 1.30 2.23 1. 1.38 2.73 4.73 1.67 Figure 2: Simulation of the student models token-level prediction distribution 洧녨. Tokens with low probability but relatively high rankcharacteristic of the 洧녨洧냣 modeyield smaller token-level ranksurprisal ratios. Table 3: Simulation results for different types of trajectories, reporting average token probability, surprisal, rank, and RSRtoken. RSRtoken shows promise as reliable metric for identifying suitable reasoning trajectories ( 3.4). At first glance, this balance may appear to pose dilemma. However, we argue that it can be resolved by viewing informativeness and alignment through the lens of absolute unfamiliarity and relative familiarity. Informativeness does not require trajectories to be entirely unfamiliar; rather, it suffices that they deviate from the dominant patterns and thus have low absolute probability of being generated by the student. Conversely, alignment does not require exact agreement with the students outputs, but instead that the corresponding tokens have relatively higher likelihoods than other candidates in the vocabulary. Building on this insight, we propose that effective reasoning trajectories should deviate from the students own generations while still exhibiting behavioral patterns within the students prior experience. Therefore, tokens in such trajectories are assigned low absolute probability (high surprisal) by the student model while still ranking relatively high (low rank values) in its prediction distribution. To validate these quantitative patterns and identify features that characterize effective learning trajectories, we conduct simulation study. Simulation Setting We simulate the student models token-level prediction distribution for reasoning trajectories and examine the numerical patterns exhibited by teacher trajectories that we consider effective. Specifically, we model the students prediction distribution as bimodal distribution over the vocabulary 洧눰. The first mode, denoted as 洧녨洧냢, represents tokens that follow the students dominant patterns, which arise from students massive general training data. The second mode 洧녨洧냣, represents tokens whose patterns resemble strong reasoning trajectories, reflecting the students prior exposure to relevant data. We instantiate both 洧녨洧냢 and 洧녨洧냣 as Zipf distributions [26, 52] over 洧눰. Then the students overall token-level prediction distribution 洧녨 is constructed as mixture of 洧녨洧냢 and 洧녨洧냣: 洧녨 = 洧랢 洧녨洧냢 + (1 洧랢) 洧녨洧냣, 洧녨洧냢, 洧녨洧냣 Zipf(洧띺), (3) where 洧랢 = 洧洧냢 洧洧냢+洧洧냣 and 洧洧냢 > 洧洧냣. Based on 洧녨, we simulate four types of reasoning trajectories and examine their token-level surprisal and rank: (i) 洧녦洧냢, sampled from 洧녨洧냢, representing trajectories that closely follow the students dominant patterns; (ii) 洧녦洧냣, sampled from 洧녨洧냣, representing trajectories that deviate from the dominant mode while aligning with certain minor patterns within the student; (iii) 洧녦洧냤, sampled from distribution distinct from both 洧녨洧냢 and 洧녨洧냣, representing misaligned trajectories; and (iv) 洧녦洧냥, sampled from 洧녨, representing trajectories that reflect the students overall predictive behavior. More simulation details are provided in A.6. Figure 2 presents the simulated bimodal distribution 洧녨, where effective reasoning Simulation Results trajectories are expected to align with the distribution of 洧녨洧냣, and thus resemble 洧녦洧냣. As shown in Table 3, 洧녦洧냣 exhibits much higher average surprisal (lower absolute probability) than 洧녦洧냢 and 洧녦洧냥, while maintaining relatively low rank values (higher-ranked), consistent with the quantitative patterns implied by our analysis. Crucially, the results suggest promising direction for measuring data suitability, which we formalize in 3.4. 5 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment 3.4. Proposed Metric: Rank-Surprisal Ratio Token-Level Metric Motivated by the complementary properties of surprisal and rank, we explore metrics that combine these two signals to jointly capture informativeness and alignment. One promising choice is the token-level ratio of rank to surprisal, which captures the relative relationship between the two signals and is denoted as RSRtoken: RSRtoken(洧노洧녲) = Rank(洧노洧녲) Surprisal(洧노洧녲) . (4) Table 3 shows that trajectories of the preferred type 洧녦洧냣 achieve the lowest average RSRtoken (1.30), whereas the misaligned 洧녦洧냤 achieve the highest, suggesting that this ratio may be reliable indicator for identifying effective reasoning trajectories that balance informativeness and alignment. From Token-Level to Trajectory-Level While RSRtoken shows encouraging behavior in simulation, directly applying this token-level ratio to assess the overall suitability of real trajectory presents non-trivial challenges. In particular, naively averaging RSRtoken over all response tokens often leads to large and unstable values  (Table 2)  . This instability stems from tokens that receive extremely high probabilities under certain contexts, yielding near-zero surprisal and unbounded ratios that dominate the trajectory-level average. natural solution is to exclude tokens with very low surprisal when computing the average. Let 洧눮洧냩 (x) denote the set of response tokens whose surprisal lies in the top 洧냩% within trajectory x. We define filtered average as Avg-RSRfilter token(x) = 洧노洧녲洧눮洧냩 (x) RSRtoken(洧노洧녲) 洧눮洧냩 (x) Empirically, we find that using the top 30% highest-surprisal tokens yields stronger correlation with posttraining performance  (Table 2)  . This suggests that tokens with higher surprisal have greater impact on student learning and should be emphasized when computing the average. Accordingly, instead of hard filtering, we adopt surprisal-weighted average of the token-level ratios. simple derivation shows that this weighted average is equivalent to trajectory-level ratio between the sum of token ranks and the sum of token surprisals. For brevity, we denote 洧洧녲 = Rank(洧노洧녲) and 洧멇롐 = Surprisal(洧노洧녲): 洧녲 洧멇롐 RSRtoken(洧노洧녲) 洧녲 洧멇롐 = 洧洧녲 洧녲 洧멇롐 洧멇롐 洧녲 洧멇롐 = 洧녲 Rank(洧노洧녲) 洧녲 Surprisal(洧노洧녲) The resulting metric yields concise form that can be interpreted as the ratio of average rank to average surprisal at the trajectory level. In practice, we further observe that extremely unfamiliar tokens can attain very large rank values due to the large vocabulary size, which also leads to numerical instability. Since tokens with excessively large ranks are effectively indistinguishable from the students perspective, we clip rank values at threshold 洧洧녴洧녩洧논. Thus, we define our final trajectory-level metric, Rank-Surprisal Ratio (RSR), as ) 洧녲 min(Rank(洧노洧녲), 洧洧녴洧녩洧논 洧녲 Surprisal(洧노洧녲) RSR(x) = (7) Interpretation Our metric admits simple interpretation. The numerator, Rank, captures relative familiarity: lower rank values indicate that tokens in this trajectory are preferred among alternative candidates by the student and align with the models existing behavior. The denominator, Surprisal, captures absolute unfamiliarity: higher surprisal indicates deviation from dominant patterns and provides informative learning signals. lower RSR therefore identifies trajectories that better balance alignment and informativeness, corresponding to effective reasoning supervision. 4. Correlation Analysis Preliminary results in Table 2 have shown that Rank-Surprisal Ratio aligns well with post-training reasoning performance. To provide more rigorous evaluation and further demonstrate the effectiveness of RSR in measuring data-student suitability, we conduct comprehensive correlation analyses. 6 (5) (6) Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment StudentAgnostic StudentSpecific Metrics Teacher Params Avg-Token Length Teacher Performance Verified Accuracy LLM-judged Quality Rule-based Quality Avg-Surprisal Avg-Surplocal Avg-Rank Influence Score G-Norm GRACE Rank-Surprisal Ratio (Absolute) Spearman Correlation with Post-Training Performance Qwen-3-14B LLaMA-3.1-8B Qwen-2.5-7B Qwen-3-4B Qwen-2.5-3B Average 0.04 0.49 0.49 0.54 0.61 0.55 0.24 0.31 0.41 0.52 0.44 0.25 0.85 0.34 0.68 0.34 0.43 0.52 0.56 0.42 0.40 0.64 0.19 0.54 0.58 0.85 0.2 0.45 0.13 0.25 0.46 0.75 0.55 0.54 0.68 0.32 0.51 0.66 0. 0.02 0.57 0.23 0.35 0.61 0.65 0.55 0.59 0.61 0.47 0.57 0.75 0.82 0.26 0.47 0.03 0.10 0.40 0.75 0.70 0.72 0.62 0.59 0.70 0.69 0.85 0.01 0.53 0.23 0.33 0.52 0.65 0.49 0.51 0.59 0.11 0.55 0.59 0. Table 4: Spearman correlation between different metrics and post-training reasoning performance across student models, reporting absolute values. \"Student-Agnostic\" metrics are computed independently of the specific student model. Our metric, Rank-Surprisal Ratio, achieves the highest correlation across all students. 4.1. Main Analysis For each of the five student models, we compute average trajectory-level suitability (or quality) scores for reasoning datasets generated by eleven teacher models ( 2.1) using different metrics. We then measure the correlation between these dataset-level scores and the students average post-training reasoning performance on the corresponding teacher data. We primarily report Spearmans correlation coefficient, as Pearson correlation exhibits similar trends. We use clipping threshold 洧洧녴洧녩洧논 = 100 for our RSR metric in all subsequent experiments. Additional analysis details are provided in A.7, and complete metric scores are reported in C.2. Compared Metrics We compare RSR against diverse set of metrics for evaluating reasoning trajectories. These include previously discussed teacher-side indicators (e.g., teacher model performance), basic statistics such as token length, as well as probability-based metrics (e.g., average surprisal and local surprisal; [18]) and rank-based metrics. We also consider commonly used trajectory quality measures, such as rule-based quality scores derived from word frequency [43], LLM-judged quality scores, and answer accuracy on verifiable questions. In addition, we include recent student-specific data suitability metrics, including gradient-based scores (G-Norm and GRACE; [29]) and influence scores [16]. Table 4 shows that Rank-Surprisal Ratio consistently exhibits strong correlation with post-training Results reasoning performance across all student models, achieving an average Spearman correlation of 0.86 and outperforming all alternative metrics. These results indicate the effectiveness and practical value of RSR. In contrast, surprisal-based and rank-based metrics alone yield substantially weaker correlations (at most 0.59), highlighting the importance of capturing both informativeness and alignment through the rank-surprisal ratio. 4.2. Ablation Study The derivation of Rank-Surprisal Ratio involves several design components as well as hyperparameter 洧洧녴洧녩洧논. We conduct an ablation study to examine how these choices affect correlation performance. As shown in Table 5, removing either rank clipping or the surprisal-weighted averaging substantially degrades the correlation, validating the necessity of both components in our metric. In addition, the Reduced sample size setting estimates the dataset-level RSR using only 200 trajectories per teacher instead of the full 5,000. The comparable correlations observed under reduced sample size Variants Avg. Corr. Rank-Surprisal Ratio (洧洧녴洧녩洧논 = 100) No rank clipping No weighted avg. (Avg-RSRtoken) Rank clipping: 洧洧녴洧녩洧논 = 50 Rank clipping: 洧洧녴洧녩洧논 = 500 Filtered average (Avg-RSRfilter Reduced sample size (200) token) 0.856 0.700 0.391 0.696 0.822 0.793 0.864 0.156 0.465 0.160 0.034 0.064 0.007 Table 5: Ablation study for Rank-Surprisal Ratio. denotes the change in average correlation. Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment Selection Methods Random Token Length洧녴洧녩洧논 Rule-based Quality洧녴洧녩洧논 LLM-judged Quality洧녴洧녩洧논 Surprisal洧녴洧녰洧녵 G-Norm洧녴洧녰洧녵 Rank-Surprisal Ratio洧녴洧녰洧녵 Qwen-3-14B AIME24 AIME25 AMC23 MATH500 Avg. 70.2 86.2 71.4 87.5 72.3 91.3 73.4 90.6 73.4 88.1 72.7 89.4 78.6 93.1 88.6 84.8 92.0 93.6 92.8 92.4 94.6 59.2 61.7 58.3 60.0 62.5 59.2 67. 46.7 51.7 47.5 49.2 50.0 50.0 59.2 L3.1-8B Q2.5-7B Q3-4B Q2.5-3B Avg. 22.1 27.3 25.8 25.6 23.5 26.1 28.5 Avg. 45.7 45.4 51.6 51.8 46.4 49.5 53.2 Avg. 53.9 51.3 58.0 59.1 53.3 59.1 61.4 Avg. 27.9 27.1 31.2 32.8 28.9 30.9 34. Table 6: Trajectory selection results showing post-training reasoning performance of student models trained on datasets selected by different methods. 洧녴洧녩洧논 and 洧녴洧녰洧녵 indicate maximizing or minimizing the corresponding metric. Model names are abbreviated as for Qwen and for LLaMA. Additional results, including GPQA evaluation, are provided in C.4. and alternative hyperparameter settings (e.g., 洧洧녴洧녩洧논 = 500) indicate that RSR is robust to both data scarcity and reasonable variations in 洧洧녴洧녩洧논. More ablation results are provided in C.3. 5. Practical Applications Given the reliable data-student suitability estimation provided by Rank-Surprisal Ratio and its strong correlation with post-training performance, we further examine its practical value as data selection criterion in two representative scenarios. 5.1. Trajectory Selection The trajectory selection task aims to identify the most effective reasoning trajectory Experimental Setting from set of candidates for given problem or prompt. We adopt 33-to-1 setting, where each candidate pool contains 33 trajectories generated by 11 teacher models (3 per teacher; see 2.1), and one trajectory is selected according to the selection criterion. This procedure is repeated for all 5,000 training problems and all student models, yielding student-specific 5,000-trajectory teacher datasets for each selection method. We then fine-tune student models on the constructed datasets and evaluate their post-training reasoning performance on standard math benchmarks, consistent with previous experiments. We compare RSR against random selection baseline and multiple previously discussed metrics. For metric-based methods, candidate trajectories are scored and selected by either maximizing or minimizing the score. Results As shown in Table 6, datasets selected by Rank-Surprisal Ratio consistently achieve the best posttraining reasoning performance among all selection methods across student models, demonstrating the effectiveness of RSR in identifying suitable trajectories. Moreover, the performance achieved by RSR is comparable to, and for four students even surpasses, the best performance of any single teacher for each student model  (Table 1)  , which serves as strong upper bound obtained via brute-force search over teacher datasets. These results underscore the practical value of RSR for selecting effective trajectories prior to training. Additional trajectory selection results, including complete scores, further analysis, GPQA evaluation, and experiments under reduced-teacher setting, are provided in C.4. 5.2. Teacher Selection The teacher selection task aims to identify the most suitable teacher model for Experimental Setting generating reasoning trajectories to train given student. We consider low-resource setting in which generating full training data for every teacher model is infeasible. Instead, we sample small set of 200 trajectories from each candidate teacher, score them using different metrics, and select the teacher model based on the average trajectory-level score. We use 6 diverse teacher models (Deepseek-R1, Qwen-3-235B-Thinking, Nemotron-Super, Qwen-3-30B-Thinking, Magistral-Small, and GPT-OSS-20B) as the candidate pool, avoiding consistently well-performing teachers to ensure non-trivial selection task. Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment Selection Methods Teacher Params洧녴洧녩洧논 Token Length洧녴洧녩洧논 Rule-based Quality洧녴洧녩洧논 LLM-judged Quality洧녴洧녩洧논 Surprisal洧녴洧녰洧녵 GRACE洧녴洧녰洧녵 Rank-Surprisal Ratio洧녴洧녰洧녵 Oracle Qwen-3-14B LLaMA-3.1-8B Qwen-2.5-7B Qwen-3-4B Top-1 77.1 71.8 72.2 71.8 68.8 72.2 77.2 77.2 Top-2 Top-1 71.8 77.1 77.1 77.2 72.2 68.8 77.1 28.1 22.0 23.7 22.0 22.8 22.8 26.7 77.1 28.1 Top22.0 28.1 28.1 26.7 23.7 28.1 28.1 26.7 Top-1 47.8 45.0 48.2 45.0 47.6 47.6 50.0 50.0 Top-2 Top-1 55.8 45.0 53.4 47.8 56.4 47.8 53.4 50.0 52.2 48.2 52.2 48.3 47.8 58.8 58.8 48.2 Top-2 Top-1 29.6 53.4 26.4 55.8 55.8 33.0 26.4 58.8 30.6 56.4 30.6 58.8 31.2 55. 56.4 33.0 Avg. 45.7 45.7 47.2 46.3 45.6 45.6 48.3 48.7 26.4 29.6 29.6 31.2 33.0 26.4 30.6 31. Qwen-2.5-3B Top-2 Table 7: Teacher selection results showing post-training reasoning performance of student models trained on data from teachers selected by different methods. Top-1 and Top-2 denote the highestand second-highestranked teachers for each student. \"Oracle\" corresponds to the ground-truth best and second-best teachers. Results As shown in Table 7, both the best and second-best teachers selected by Rank-Surprisal Ratio yield strong post-training reasoning performance, achieving average results close to oracle teachers and outperforming other selection methods. Notably, as also observed in our ablation study  (Table 5)  , RSR remains effective when using only 200 trajectories per teacher for measurement, highlighting its robustness and practical value for identifying suitable teachers in low-resource settings. 6. Related Work Knowledge distillation is powerful approach for transferring knowledge from Knowledge Distillation large models to smaller ones [15], and has been widely used in training LLMs [30]. Prior work shows that stronger teachers do not necessarily yield better students, often due to capability gaps [24, 46] or off-policy data [5]. Recent studies address this by better aligning teacher supervision with student behavior, achieving an implicit balance through approaches such as on-policy distillation [2], integration with reinforcement learning [25, 49], SFT optimization [37, 42], teaching assistants [8, 27], and interleaved sampling [31, 38]. By contrast, our work explicitly quantifies the trade-off between informativeness and alignment in distillation, enabling principled identification of effective teacher data for given student. Long CoT trajectories provide strong supervision for improving student SFT with Reasoning Trajectories models reasoning performance via SFT [34, 35]. In line with findings in the general domain that high-quality data improves SFT [41, 51], many studies focus on constructing high-quality CoT data, either by selecting better prompts [40, 44] or by filtering reasoning trajectories [6, 17, 23, 36, 43, 53]. Recognizing that optimal reasoning data may vary across students, recent work explores student-specific trajectory selection strategies [16, 18, 29]. Our work also studies student-specific data selection, but differs from prior work by measuring datastudent suitability from the perspective of informative alignment and by conducting more comprehensive evaluations across wider range of teacher models. 7. Conclusion In this paper, we study datastudent suitability in reasoning distillation and propose Rank-Surprisal Ratio (RSR), simple metric for identifying suitable reasoning trajectories for given student. Motivated by our analysis, RSR jointly captures trajectorys informativeness and alignment with the students behavior, favoring trajectories with low absolute probability but relatively high-ranked tokens. Experiments across diverse teacherstudent pairs show that RSR strongly correlates with post-training performance and consistently outperforms existing metrics. We further demonstrate its effectiveness in both trajectory and teacher selection. Overall, our results highlight informative alignment as promising direction for reasoning distillation. 9 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment"
        },
        {
            "title": "Limitations",
            "content": "Although our work proposes an effective metric for selecting reasoning trajectories to distill student models, the performance of data selection is inherently constrained by the diversity and quality of candidate trajectories or teacher models. When none of the available teacher trajectories are well suited to given student, the gains from selection alone may be limited. promising direction for future work is to use our metric to guide the rewriting or synthesis of reasoning trajectories, rather than selecting from fixed pool. In addition, the derived metric takes simple and intuitive form with clear interpretation. However, it remains unclear whether it arises from deeper mathematical principles. We have not yet identified suitable theoretical framework to analyze this aspect, which we leave for future investigation. Finally, due to resource constraints, we primarily focus on mathematical reasoning tasks with extensive controlled studies, while also including additional evaluation on GPQA. Extending our evaluation to other domains,such as code or additional forms of reasoning, would be valuable. However, since our analysis relies on large-scale trajectory generation and approximately 200 SFT experiments, such extensions would require substantially greater computational resources and are therefore left for future work."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, et al. Phi-4-reasoning technical report. arXiv preprint arXiv:2504.21318, 2025. 2.1 [2] Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Sta켻czyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. On-policy distillation of language models: Learning from self-generated mistakes. In International Conference on Learning Representations, 2023. 6 [3] Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. 2.1 [4] Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, et al. Llama-nemotron: Efficient reasoning models. arXiv preprint arXiv:2505.00949, 2025. 2.1 [5] Howard Chen, Noam Razin, Karthik Narasimhan, and Danqi Chen. Retaining by doing: The role of on-policy data in mitigating forgetting. CoRR, abs/2510.18874, 2025. [6] Xinghao Chen, Zhijing Sun, Wenjin Guo, Miaoran Zhang, Yanjun Chen, Yirong Sun, Hui Su, Yijie Pan, Dietrich Klakow, Wenjie Li, and Xiaoyu Shen. Unveiling the key factors for distilling chain-of-thought reasoning. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 1509415119. Association for Computational Linguistics, 2025. 6 [7] Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. Pearson correlation coefficient. Noise reduction in speech processing, pages 14, 2009. A.7 [8] Dongyi Ding, Tiannan Wang, Chenghao Zhu, Meiling Tao, Yuchen Eleanor Jiang, and Wangchunshu Zhou. Micota: Bridging the learnability gap with intermediate cot and teacher assistants. ArXiv, abs/2507.01887, 2025. 6 [9] Lishui Fan, Yu Zhang, Mouxiang Chen, and Zhongxin Liu. Posterior-grpo: Rewarding reasoning processes in code generation. CoRR, abs/2508.05170, 2025. B.1 [10] Roger B. Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamile Lukosiute, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, and Samuel R. Bowman. Studying large language model generalization with influence functions. CoRR, abs/2308.03296, 2023. B. 10 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment [11] Etash Kumar Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah M. Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, and Ludwig Schmidt. Openthoughts: Data recipes for reasoning models. CoRR, abs/2506.04178, 2025. 1 [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, Hao Zhang, Hanwei Xu, Honghui Ding, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jingchang Chen, Jingyang Yuan, Jinhao Tu, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaichao You, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingxu Zhou, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Tao Yun, Tian Pei, Tianyu Sun, Tao Wang, Wangding Zeng, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nat., 645(8081):633638, 2025. 1, 2.1 [13] John Hale. probabilistic earley parser as psycholinguistic model. In Language Technologies 2001: The Second Meeting of the North American Chapter of the Association for Computational Linguistics, NAACL 2001, Pittsburgh, PA, USA, June 2-7, 2001. The Association for Computational Linguistics, 2001. 3.1 [14] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 2. [15] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in neural network. CoRR, abs/1503.02531, 2015. 6 [16] Prateek Humane, Paolo Cudrano, Daniel Z. Kaplan, Matteo Matteucci, Supriyo Chakraborty, and Irina Rish. Influence functions for efficient data selection in reasoning. CoRR, abs/2510.06108, 2025. 4.1, 6, B.3 [17] Gangwei Jiang, Yahui Liu, Zhaoyi Li, Qi Wang, Fuzheng Zhang, Linqi Song, Ying Wei, and Defu Lian. What makes good reasoning chain? uncovering structural patterns in long chain-of-thought reasoning. CoRR, abs/2505.22148, 2025. 6 [18] Hoang Anh Just, Myeongseob Ko, and Ruoxi Jia. Distilling reasoning into student llms: Local naturalness for selecting teacher data. CoRR, abs/2510.03988, 2025. 1, 3.2, 4.1, 11 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment [19] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. A.2 [20] Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Llms can easily learn to reason from demonstrations structure, not content, is what matters! CoRR, abs/2502.07374, 2025. A.1 [21] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. A.1 [22] Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, and Radha Poovendran. Small models struggle to learn from strong reasoners. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 2536625394. Association for Computational Linguistics, 2025. [23] Jinrui Liu, Jeff Wu, Xuanguang Pan, Gavin Cheung, Shuai Ma, and Chongyang Tao. Air: Post-training data selection for reasoning via attention head influence. arXiv preprint arXiv:2512.13279, 2025. 6 [24] Renjie Luo, Jiaxi Li, Chen Huang, and Wei Lu. Through the valley: Path to effective long cot training for small language models. CoRR, abs/2506.07712, 2025. 6 [25] Lu Ma, Hao Liang, Meiyi Qiang, Lexiang Tang, Xiaochen Ma, Zhen Hao Wong, Junbo Niu, Chengyu Shen, Runming He, Bin Cui, and Wentao Zhang. Learning what reinforcement learning cant: Interleaved online fine-tuning for hardest questions. CoRR, abs/2506.07527, 2025. 6 [26] Nikolay Mikhaylovskiy. Zipf and heaps laws for tokens and llm-generated texts. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1546915481, 2025. 3. [27] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 51915198, 2020. 6 [28] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel J. Cand칟s, and Tatsunori Hashimoto. s1: Simple test-time scaling. CoRR, abs/2501.19393, 2025. 1 [29] Abhishek Panigrahi, Bingbin Liu, Sadhika Malladi, Sham M. Kakade, and Surbhi Goel. In good graces: Principled teacher selection for knowledge distillation. CoRR, abs/2511.02833, 2025. 4.1, 6, B.4, B.5 [30] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4. CoRR, abs/2304.03277, 2023. [31] Jingyu Peng, Maolin Wang, Hengyi Cai, Yuchen Li, Kai Zhang, Shuaiqiang Wang, Dawei Yin, and Xiangyu Zhao. Adaswitch: Adaptive switching generation for knowledge distillation. ArXiv, abs/2510.07842, 2025. 6 [32] Abhilasha Ravichander, Jillian Fisher, Taylor Sorensen, Ximing Lu, Maria Antoniak, Bill Yuchen Lin, Niloofar Mireshghallah, Chandra Bhagavatula, and Yejin Choi. Information-guided identification of training data imprint in (proprietary) large language models. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pages 19621978. Association for Computational Linguistics, 2025. 3.1 [33] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. C.4.2 12 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment [34] Safal Shrestha, Minwu Kim, Aadim Nepal, Anubhav Shrestha, and Keith Ross. Warm up before you train: Unlocking general reasoning in resource-constrained settings. CoRR, abs/2505.13718, 2025. 6 [35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 1, 6 [36] Xiaojun Wu, Xiaoguang Jiang, Huiyang Li, Jucai Zhai, Dengfeng Liu, Qiaobo Hao, Huang Liu, Zhiguo Yang, Ji Xie, Ninglun Gu, Jin Yang, Kailai Zhang, Yelun Bao, and Jun Wang. Beyond scaling law: data-efficient distillation framework for reasoning. CoRR, abs/2508.09883, 2025. 6 [37] Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, and Xu Yang. On the generalization of SFT: reinforcement learning perspective with reward rectification. CoRR, abs/2508.05629, 2025. 6 [38] Wenda Xu, Rujun Han, Zifeng Wang, Long T. Le, Dhruv Madeka, Lei Li, William Yang Wang, Rishabh Agarwal, Chen-Yu Lee, and Tomas Pfister. Speculative knowledge distillation: Bridging the teacher-student gap through interleaved sampling. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. 6 [39] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025. 1, 2. [40] Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Xiaojun Wu, Honghao Liu, Hui Xiong, and Jian Guo. Select2reason: Efficient instruction-tuning data selection for long-cot reasoning. CoRR, abs/2505.17266, 2025. 6 [41] Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Tao Gui, Qi Zhang, and Xuanjing Huang. Measuring data diversity for instruction tuning: systematic analysis and reliable metric. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 1853018549. Association for Computational Linguistics, 2025. 6 [42] Junjie Ye, Yuming Yang, Yang Nan, Shuo Li, Qi Zhang, Tao Gui, Xuan-Jing Huang, Peng Wang, Zhongchao Shi, and Jianping Fan. Analyzing the effects of supervised fine-tuning on model knowledge from token and parameter levels. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 471513, 2025. 6 [43] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. LIMO: less is more for reasoning. CoRR, abs/2502.03387, 2025. 4.1, 6, B.2 [44] Qianjin Yu, Keyu Wu, Zihan Chen, Chushu Zhang, Manlin Mei, Lingjun Huang, Fang Tan, Yongsheng Du, Kunlin Liu, and Yurui Zhu. Rethinking the generation of high-quality cot data from the perspective of llm-adaptive question difficulty grading. CoRR, abs/2504.11919, 2025. [45] Jerrold Zar. Spearman rank correlation. Encyclopedia of biostatistics, 7, 2005. A.7 [46] Chen Zhang, Qiuchi Li, Dawei Song, Zheyu Ye, Yan Gao, and Yao Hu. Towards the law of capacity gap in distilling language models. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 2250422528. Association for Computational Linguistics, 2025. 6 13 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment [47] Dylan Zhang, Qirun Dai, and Hao Peng. The best instruction-tuning data are those that fit. CoRR, abs/2502.04194, 2025. 1, 3. [48] Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, and Chen Ma. What, how, where, and how well? survey on test-time scaling in large language models. CoRR, abs/2503.24235, 2025. 1 [49] Xuechen Zhang, Zijian Huang, Yingcong Li, Chenshun Ni, Jiasi Chen, and Samet Oymak. BREAD: branched rollouts from expert anchors bridge SFT & RL for reasoning. CoRR, abs/2506.17211, 2025. 6 [50] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. A.5 [51] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: less is more for alignment. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 6 [52] George Kingsley Zipf. The psycho-biology of language: An introduction to dynamic philology. Routledge, 1936. 3. [53] Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, and Mengdi Wang. Reasonflux-prm: Trajectory-aware prms for long chain-of-thought reasoning in llms. CoRR, abs/2506.18896, 2025. 6 14 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment A. Details of Experiments A.1. Determining Training Problem Set All teacher trajectory datasets are constructed using fixed problem set to avoid confounding effects from variations in problem composition. We focus on mathematical reasoning and curate 5,000 math problems from the widely used NuminaMath dataset [21]. Following prior work [20], we apply preprocessing steps such as difficulty filtering to ensure the problem quality. Specifically, our training dataset consists of problems drawn from the MATH, AIME/AMC, and Olympiads. SkyThought provides difficulty-labeled version of NuminaMath in which each problem is assigned an integer difficulty score from 0 to 9. Using this scale, we randomly sampled 1,667 problems from the MATH subset with difficulty above three, 1,667 problems from the Olympiads subset with difficulty above eight, and 1,666 problems from the AIME/AMC subset, yielding balanced training set of 5,000 problems. We choose 5,000 training problems for three main reasons. First, prior studies have shown that strong reasoning capabilities can be learned from training sets of around 1,000 problems. Second, high-quality reasoning problems are relatively scarce, making further scaling less effective in practice. Third, our cross-study incurs quadratic computational cost across teacherstudent pairs. Considering these factors, we find 5,000 problems to offer good trade-off between representativeness and computational feasibility. A.2. Teacher Trajectory Generation For each teacher model, we generate reasoning trajectories for 5,000 problems over three independent runs using vLLM [19] under maximum generation budget of 31,000 tokens. We adopt the official model sampling hyperparameters and prompt template recommendation; for example, Qwen models use temperature=0.6, top_p=0.95, top_k=20, and min_p=0. Each problem is appended with the instruction: \"Return your final response within boxed{}.\" If sampled trajectory exceeds the token budget, we resample up to 10 times; if all attempts still exceed the budget, we truncate the final trajectory. Across all teacher models, truncation rates are below 1%, which helps preserve the quality of training trajectories even for teachers that tend to produce long outputs. The resulting training dataset of teacher trajectories is constructed by using the original problem as the user prompt and prepending the system prompt: \"Please reason step by step, and put your final answer within boxed{}.\" to each problem. A.3. Teacher Models and Student Models As discussed earlier, our experiments adopt more diverse set of teacher models than prior work. Specifically, we consider the teacher variants shown in Table 8. For student models, we select five open-source pretrained base models from the Qwen and LLaMA families: Qwen-3-14B, LLaMA-3.1-8B, Qwen-2.5-7B, Qwen-3-4B, and Qwen-2.5-3B. A.4. Benchmark Evaluation Teacher Models DeepSeek-R1-0528 Qwen-3-235B-Thinking-2507 GPT-OSS-120B (high) LLaMA-3.3-Nemotron-Super-49B-v1.5 QwQ-32B Qwen-3-30B-Thinking-2507 Magistral-Small-2506 GPT-OSS-20B (high) Phi-4-Reasoning-Plus Qwen-3-8B (thinking) Qwen-3-4B-Thinking-2507 We use vLLM together with the Math-Verify package to evaluate posttrained models on mathematics benchmarks. Our evaluated benchmarks, AIME25, AIME24, AMC23, and MATH500, are widely used and span varying difficulty levels, assessing mathematical reasoning and multistep problem-solving across diverse domains. We additionally conduct evaluation on GPQA-Diamond beyond mathematics, as described in C.4.2. We adopt the Acc@4 metric as the final score, which averages results over four independent evaluations per problem. Evaluating single model checkpoint typically takes around half an hour using 8 H200 GPUs. Table 8: List of teacher models used in our experiments. 15 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment During inference, we use temperature of 0.6, top_p of 0.95, and top_k set to 1. The maximum generation length is set to 32,768 tokens, consistent with the maximum sequence length used during finetuning. Responses that exceed this limit are truncated. This differs from trajectory generation, where we resample multiple times to avoid truncation, as here we aim to evaluate the models reasoning ability under fixed context-length constraint. Under this setting, the comparable performance of the Qwen-3-235B and Qwen-3-30B in Table 1 may be attributable to truncation effects, which we consider reasonable outcome given the imposed length limit. A.5. Details of Model Fine-Tuning We use LLaMA-Factory [50] framework to conduct SFT with reasoning trajectory datasets. For different student models, we conduct grid search for best set of hyperparameters. The final setting is reported in the Table 9. During SFT, all student models use maximum sequence length of 32,768. Models Qwen-3-14B LLaMA-3.1-8B Qwen-2.5-7B Qwen-3-4B Qwen-2.5-3B Learning Rate Batch Size 2.0E-05 2.0E-05 2.0E-05 2.0E-05 5.0E-05 64 64 64 64 64 Epoch 8 10 10 10 10 Most experiments are conducted on NVIDIA H200 GPUs. single SFT experiment using the 14B model takes approximately 6 hours on 8 H200 GPUs for training. Table 9: Training hyperparameters for different student models. For trajectory selection experiments in 5.1, we fine-tune each selected dataset with three different random seeds and report performance averaged over these runs. For teacherstudent distillation experiments in 2.1, however, each teacher already yields three independent trajectory datasets, so we do not further vary random seeds for fine-tuning. A.6. Details of Simulation Study In practice, we simulate 洧녨 by sampling 洧洧냢 tokens from 洧녨洧냢 and 洧洧냣 tokens from 洧녨洧냣. We use 洧洧냢 = 1,000,000, 洧洧냣 = 250,000, 洧눰 = 50, and for each simulated dataset, we sample 10,000 tokens to compute average metrics. Based on preliminary fit to reasoning data, we set the Zipf exponent to 洧띺 = 2.3. Figure 3 depicts the distributions of 洧녨洧냢 and 洧녨洧냣 over the vocabulary, with their mixture representing the simulated token-level prediction distribution 洧녨 of the student model. Figure 2 is derived from this by ranking the tokens from higher to lower probability. A.7. Details of Correlation Analysis Figure 3: Simulation of the student models tokenlevel prediction distribution 洧녨, mixture of Zipf distributions 洧녨洧냢 and 洧녨洧냣. We use the Spearman correlation coefficient [45] because our analysis focuses on monotonic consistency rather than strict linear relationships between metrics and post-training performance. We also report Pearson correlation results [7] in C.2. We report the absolute values of Spearman correlation coefficients in Table 4. For the Average values, however, we first average the correlation coefficients across student models and then take the absolute value. We consider this aggregation more appropriate, as correlations with opposite signs across different student models should offset each other; consequently, this averaged value can be lower than the result obtained by averaging absolute correlations. Since post-training performance is computed by averaging three generation runs per teacher ( 2.1), we likewise average the dataset-level metric over the three trajectory datasets generated by the same teacher to obtain its final score. We observe that our datset-level metric varies marginally across different datasets generated by the same teacher. 16 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment B. Details of Compared Metrics B.1. LLM-judged Quality We use Qwen3-32B-Instruct in Non-Thinking configuration as an automatic judge to evaluate reasoning trajectories. Under fixed evaluation prompt, the judge produces structured assessment at two levels. First, it assigns dimension-wise scores over five criteriaFactual Accuracy, Logical Rigor, Solution Completeness, Reasoning Efficiency, and Presentation Qualityeach accompanied by brief justification. Second, it aggregates the dimensional assessments into an overall score in [0, 1] together with concise rationale. For dataset-level comparison, we report the mean of the trajectory-level overall scores. The complete evaluation prompt is given in Table 27, adapted from [9]. B.2. Rule-based Quality We implement the rule-based criterion used for filtering the LIMO Dataset [43]. Each response is scored using the following weighted indicators: Elaborated reasoning (30%): total word length. Self-Verification (20%): frequency of \"check\" and \"verify\". Exploratory Approach (25%): frequency of \"perhaps\" and \"might\". Adaptive Granularity (25%): frequency of \"therefore\" and \"since\". To ensure fair comparison across responses of varying lengths, we compute relative keyword frequencies by normalizing absolute keyword counts with respect to the total word count. To account for differences in scale across criteria, we then independently standardize each criterions scores into z-scores, which empirically improves correlation. B.3. Influence Score We adopt the Influence Score method based on the second-order approximation of Influence Functions [16]. The core principle is to treat the infinitesimal up-weighting of training sample as local perturbation to the optimization objective, thereby estimating the induced marginal change in the loss of the evaluation set. In this framework, higher influence score signifies that the gradient direction of the training sample aligns with minimizing the evaluation loss in the neighborhood of the converged parameters. We implement this baseline using the Kronfluence framework. To make the computation tractable for LLMs, we employ the EK-FAC strategy to approximate the Hessian matrix [10]. Specifically, we first precompute the EK-FAC factors on reference model. Since we do not know which trajectory is better priori, we use the model trained with randomly selected trajectories per problem as the reference model, corresponding to the Random variant in Table 6. Then, we compute the pairwise influence scores between the training samples and the evaluation dataset. To optimize for memory and computational efficiency, we apply low-rank approximation to the query gradients (rank = 4) and utilize bfloat16 precision for the Inverse Hessian-Vector Product calculations. Finally, we average these pairwise scores across the evaluation set to obtain the sample-level influence score 洧(洧녬), which serves as our baseline for ranking data suitability. B.4. G-Norm G-Norm [29] assesses the suitability of generated data for student model by measuring the magnitude of the students local gradient signal induced by the data. Formally, G-Norm calculates the magnitude of the loss gradient with respect to the student models parameters. To address the computational constraints associated with high-dimensional parameter spaces, the method utilizes random projection for dimensionality reduction. For each reasoning trajectory (洧논, 洧녽), we compute the gradient of the loss derived from the student model and apply length normalization to eliminate bias towards longer sequences. These gradients are then projected onto lower-dimensional subspace via fixed random matrix, followed by the computation of their 洧2 norms. B.5. GRACE We adopt GRACE [29] as baseline metric for further evaluating the generated reasoning trajectory dataset from gradient-based perspective. GRACE characterizes the geometry of the optimization landscape by analyzing 17 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment Metrics Rank-Surprisal Ratio Spearman Pearson Student Models Qwen-3-14B LLaMA-3.1-8B Qwen-2.5-7B Qwen-3-4B Qwen-2.5-3B Average 0.856 0.794 0.918 0.805 0.845 0.880 0.845 0.811 0.855 0.654 0.818 0. Table 10: Comparison of Spearman and Pearson correlation results on Rank-Surprisal Ratio (absolute values). the spectral structure of the student model gradients. This approach allows for holistic assessment of how effectively the generated data spans the parameter space required for model optimization. The calculation proceeds by first projecting the high-dimensional gradients onto lower-dimensional subspace via fixed random matrix to ensure computational feasibility. To address the empirical tendency of gradient norms to diminish in longer sequences, the method rescales the projected vectors logarithmically based on the response length. GRACE then computes the metric using cross-validation strategy where the dataset is divided into multiple partitions. The gradients from held-out partition are weighted by the regularized inverse covariance matrix estimated from the remaining data. This process effectively quantifies the expected squared norm of the gradients after whitening them with the estimated spectrum of the distribution. Because GRACEs cross-validation strategy yields dataset-level metric rather than per-sample score, we apply GRACE for teacher selection but not for trajectory-level selection. B.6. Others Here we give formal definitions for Avg-Surprisal and Avg-Surplocal. Given trajectory 洧논 = (洧노1, . . . , 洧노洧녢 ), Avg_Surprisal(x) = 1 洧녢 洧녢 洧녲=1 log 洧녷洧랚(洧노洧녲 c洧녲) Avg_Surplocal(x) = 1 洧녢 洧녢 洧녲=1 log 洧녷洧랚(洧노洧녲 clocal 洧녲 ) (8) (9) C. More Results and Analysis C.1. Computational Cost for Rank-Surprisal Ratio The computation of Rank-Surprisal Ratio requires only single forward pass through the student model. During the forward pass, we collect each tokens surprisal and rank from the model logits. Computing RankSurprisal Ratio over the 5,000-trajectory dataset typically takes about one hour on single H200 GPU, which is significantly cheaper than SFT. C.2. Additional Results for Correlation Analysis Table 10 compares Spearman and Pearson correlations. The results show that Rank-Surprisal Ratio also exhibits strong Pearson correlation with post-training performance. Table 22, 23, 24, 25, and 26 present the full metric assessment results across different teacher trajectory datasets on Qwen-3-14B, LLaMA-3.1-8B, Qwen-2.5-7B, Qwen-3-4B, and Qwen-2.5-3B, respectively. C.3. Additional Ablation Study We conduct some additional ablation studies. Results are shown in Table 11. The Fixed student variant computes RSR using fixed model (Qwen-3-14B) instead of the target student, and the resulting drop in correlation highlights the importance of student-specific estimation. Avg-Rank (clip at 洧洧녴洧녩洧논 = 100) applies rank clipping to Avg-Rank, but it still fails to achieve better correlation than rank alone. Rank Minus Surprisal is computed by subtracting the surprisal from the (clipped) rank. Although this 18 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment Variants Avg. Corr. Rank-Surprisal Ratio Fixed student use Rank1.05 use Rank0.95 use Surprisal1.05 use Surprisal0.95 Avg-Rank (clip at 洧洧녴洧녩洧논 = 100) Rank Minus Surprisal Rank-Entropy Ratio 0.856 0.785 0.845 0.787 0.847 0.873 0.552 0.585 0.764 0.071 0.011 0.069 0.009 0.017 0.304 0.271 0.092 Table 11: Additional ablation study for Rank-Surprisal Ratio. denotes the change in average correlation. metric also models the relationship between rank and surprisal, this simple subtraction does not align well with post-training performance. RankEntropy Ratio is computed as the ratio between the (clipped) average rank and the average token entropy, where entropy is loosely related to surprisal. While it achieves good correlation, its correlation is still lower than that of RSR. We also experiment with different exponent choices when computing Rank-Surprisal Ratio (with the default setting corresponding to power 1 for both rank and surprisal), for example \"use Rank1.05\". The results indicate that our metric is robust to these variations and may yield higher correlation with hyperparameter tuning. Nevertheless, we use the default power-1 setting to keep the formulation simple. C.4. Additional Results for Trajectory Selection C.4.1. Ablation Study on Trajectory Selection Variant Selection Methods Qwen-3-14B LLaMA-3.1-8B Qwen-2.5-7B Qwen-3-4B Qwen-2.5-3B Math Avg. Math Avg. Math Avg. Math Avg. Math Avg. Rank-Surprisal Ratio洧녴洧녰洧녵 With Correctness Filtering Fewer Candidates per Teacher 78.6 77.5 77.6 28.5 27.9 27.8 53.2 52.3 52.6 61.4 60.7 60.9 34.8 34.9 33. Average 51.3 50.7 50.5 Table 12: Ablation study of trajectory selection with RSRmin-based variant methods across student models. \"Math Avg.\" denotes the average performance over AIME24, AIME25, AMC23, and MATH500. Table 12 presents trajectory selection results for two selection method variants based on Rank-Surprisal Ratio, offering further insights. For \"With Correctness Filtering\", we consider combined setting that uses both RSR and verified correctness for trajectory selection. Specifically, for problems with verifiably correct trajectories, we first discard incorrect ones and then select among the remaining correct trajectories based on RSR. The results show no significant improvement over selecting trajectories solely based on RSR, particularly for larger student models. This suggests that correctness is sometimes less critical than overall data suitability. For \"Fewer Candidates per Teacher\", we evaluate an 11-to-1 setting in which each candidate pool contains 11 teacher trajectories (one per teacher), instead of the original 33-to-1 setting (three per teacher). This setting focuses on selecting the best trajectory for each problem across different teachers, rather than across multiple generations from the same teacher. The results are comparable, with slight performance gap relative to the 33-to-1 setting, indicating that RSR remains effective even when each teacher provides only single trajectory. These findings further suggest that while RSR captures suitability differences across generations from the same teacher, such differences are less pronounced than those across different teacher models. C.4.2. Additional Evaluation on GPQA and Full Results To more comprehensively evaluate the impact of different trajectory selection methods on post-trained models reasoning capabilities beyond mathematical problems, we conduct additional evaluation on the GPQA-Diamond benchmark [33]. GPQA-Diamond consists of 198 challenging multiple-choice questions spanning biology, 19 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment Selection Methods Random LLM-judged Quality洧녴洧녩洧논 Rank-Surprisal Ratio洧녴洧녰洧녵 Qwen-3-14B LLaMA-3.1-8B Qwen-2.5-7B Qwen-3-4B Qwen-2.5-3B GPQA 48.5 53.5 55.1 GPQA 26.8 27.3 31. GPQA 35.4 35.4 38.9 GPQA 43.4 45.5 45.5 GPQA 22.2 21.2 31. Average 35.3 36.6 40.4 Table 13: Comparison of different trajectory selection methods on the GPQA-Diamond benchmark across student models. physics, and chemistry. Table 13 summarizes the evaluation results of different trajectory selection methods on GPQA-Diamond. Although the results are less stable than those on mathematical benchmarks due to the out-of-domain nature of this evaluation, datasets selected by RSR still achieve the best overall post-training performance. This suggests that RSR can identify suitable trajectories that consistently improve student models general reasoning capabilities, even when training solely on mathematical problems. The complete trajectory selection results underlying Table 6 are presented in Table 17, 18, 19, 20, and 21. C.4.3. Analysis of Datasets Selected by RSR Selected Datasets RSR洧녴洧녰洧녵 on Q3-14B RSR洧녴洧녰洧녵 on L3.1-8B RSR洧녴洧녰洧녵 on Q2.5-7B RSR洧녴洧녰洧녵 on Q3-4B RSR洧녴洧녰洧녵 on Q2.5-3B Data Composition over Teacher Models Deepseek-R1 Q3-235B GPT-120B Nemotron QwQ Q3-30B Magistral GPT-20B Phi-4 Q3-8B Q3-4B 6.2% 5.5% 4.1% 2.7% 2.4% 4.5% 1.0% 1.0% 2.2% 0.8% 0.1% 0.0% 0.7% 0.0% 0.0% 0.1% 3.7% 1.7% 0.3% 2.4% 67.3% 48.8% 55.7% 76.6% 45.1% 6.7% 3.9% 5.3% 4.1% 4.0% 1.4% 9.1% 5.9% 3.1% 12.2% 0.0% 0.1% 0.2% 0.0% 0.0% 2.9% 2.1% 8.6% 0.2% 21.3% 6.5% 0.8% 17.6% 6.9% 0.6% 4.5% 6.1% 0.2% 26.0% 6.6% Metrics RSR 2.57 2.69 2.67 2.56 2. Table 14: Data composition and metrics of trajectory datasets selected by RSR in the trajectory selection experiments ( 5.1) across different student models. Model names are abbreviated as for Qwen and for LLaMA. Table 14 shows the data composition of datasets selected by RSR across 11 teacher models. The resulting distributions vary across student models, demonstrating the metrics ability to select different teacher trajectories tailored to different students. QwQ-32B is generally preferred across student models, consistent with its stable performance. For clearer contrast in data composition among student models, we refer readers to C.4.4, where trajectory selection is performed with fewer teachers and consistently strong teachers such as QwQ-32B are removed. The RSR values of the selected datasets are also reported in Table 14. These datasets consistently achieve substantially lower RSR values than the teacher trajectory datasets (see C.2 for complete metrics), validating that our selection procedure effectively identifies trajectories with low RSR for each problem. C.4.4. Additional Trajectory Selection Experiments with Fewer Teacher Models To better reflect practical scenarios in which only few teachers trajectories are available and generally suitable teachers may be absent, we conduct additional experiments that select trajectories from reduced set of teachers. Specifically, we select trajectories from candidates generated by seven teachers: DeepSeek-R1, Qwen-3-235B-Thinking, NemotronSuper, Qwen-3-30B-Thinking, Magistral-Small, GPTOSS-20B, and Qwen-3-8B. This teacher set is formed by combining the teachers used in Table 2 and 7. Selection Methods Qwen-3-14B Qwen-2.5-7B Math Avg. Math Avg. Random LLM-judged Quality洧녴洧녩洧논 Rank-Surprisal Ratio洧녴洧녰洧녵 72.8 74.4 76.8 47.3 48.3 50. Table 15: Comparison of different trajectory selection methods under reduced-teacher setting. The results are shown in Table 15. Datasets selected by RSR still achieve superior post-training reasoning 20 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment Selected Datasets Data Composition over Teacher Models Deepseek-R1 Qwen-3-235B Nemotron Qwen-3-30B Magistral GPT-OSS-20B Qwen-3-8B RSR洧녴洧녰洧녵 on Qwen-3-14B RSR洧녴洧녰洧녵 on Qwen-2.5-7B 27.40% 14.52% 21.00% 8.60% 1.42% 6.94% 28.84% 20.76% 4.92% 10.64% 0.24% 0.14% 16.18% 38.40% Table 16: Data composition of trajectory datasets selected by RSR under reduced-teacher setting (see the setting in C.4.4 and results in Table 15). performance compared with the baselines, demonstrating the effectiveness of our metric when only limited number of teachers are available. We also observe that the performance gap narrows when selecting from seven teachers compared with eleven teachers, which is expected and suggests that larger candidate space enables trajectory selection to more effectively identify high-quality training data. Moreover, Table 16 presents the data composition of datasets selected by RSR from the reduced set of seven teacher models. The distributions differ markedly for Qwen-3-14B and Qwen-2.5-7B: the former student model tends to favor teachers such as DeepSeek-R1 and Qwen-3-30B, whereas the latter student model shows stronger preference for smaller models such as Qwen-3-8B. These results further demonstrate the effectiveness of RSR in selecting teacher trajectories that are well suited to specific student models. D. Others D.1. License for Artifacts and Data Consent All artifacts used in this paper are publicly available for academic research purposes, including AIME, AMC, MATH500, and NuminaMath. D.2. Data Statement The training datasets consist solely of mathematics problems and solutions and contain no offensive content or personal information. D.3. AI Assistant Usage Statement We used ChatGPT for writing refinement and minor coding assistance. AI assistants were not involved in research innovation, and all core contributions were developed solely by the authors. 21 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment Selection Methods Random Token Length洧녴洧녩洧논 Rule-based Quality洧녴洧녩洧논 LLM-judged Quality洧녴洧녩洧논 Surprisal洧녴洧녰洧녵 G-Norm洧녴洧녰洧녵 Rank-Surprisal Ratio洧녴洧녰洧녵 Qwen-3-14B AIME24 AIME25 AMC23 MATH500 Math Avg. GPQA-Diamond 88.6 84.8 92.0 93.6 92.8 92.4 94.6 86.3 87.5 91.3 90.6 88.1 89.4 93. 59.2 61.7 58.3 60.0 62.5 59.2 67.5 46.7 51.7 47.5 49.2 50.0 50.0 59.2 48.5 53.5 55.1 70.2 71.4 72.3 73.4 73.4 72.7 78.6 Table 17: Full post-training evaluation results for trajectory selection on Qwen-3-14B. \"Math Avg.\" denotes the average over AIME24, AIME25, AMC23, and MATH500. Selection Methods Random Token Length洧녴洧녩洧논 Rule-based Quality洧녴洧녩洧논 LLM-judged Quality洧녴洧녩洧논 Surprisal洧녴洧녰洧녵 G-Norm洧녴洧녰洧녵 Rank-Surprisal Ratio洧녴洧녰洧녵 LLaMA-3.1-8B AIME24 AIME25 AMC23 MATH500 Math Avg. GPQA-Diamond 50.8 57.4 58.0 58.6 56.8 57.6 63.6 29.4 36.9 29.4 38.1 25.6 36.9 36.9 26.8 27.3 31.3 5.8 6.7 9.2 4.2 6.7 4.2 8.3 2.5 8.3 6.7 1.7 5.0 5.8 5. 22.1 27.3 25.8 25.6 23.5 26.1 28.5 Table 18: Full post-training evaluation results for trajectory selection on LLaMA-3.1-8B. \"Math Avg.\" denotes the average over AIME24, AIME25, AMC23, and MATH500. Selection Methods Random Token Length洧녴洧녩洧논 Rule-based Quality洧녴洧녩洧논 LLM-judged Quality洧녴洧녩洧논 Surprisal洧녴洧녰洧녵 G-Norm洧녴洧녰洧녵 Rank-Surprisal Ratio洧녴洧녰洧녵 Qwen-2.5-7B AIME24 AIME25 AMC23 MATH500 Math Avg. GPQA-Diamond 82.8 75.6 84.8 87.0 79.2 79.2 86.6 62.5 61.9 72.5 66.9 63.1 66.3 71. 18.3 22.5 24.2 30.0 22.5 27.5 29.2 19.2 21.7 25.0 23.3 20.8 25.0 25.8 35.4 35.4 38.9 45.7 45.4 51.6 51.8 46.4 49.5 53.2 Table 19: Full post-training evaluation results for trajectory selection on Qwen-2.5-7B. \"Math Avg.\" denotes the average over AIME24, AIME25, AMC23, and MATH500. Selection Methods Random Token Length洧녴洧녩洧논 Rule-based Quality洧녴洧녩洧논 LLM-judged Quality洧녴洧녩洧논 Surprisal洧녴洧녰洧녵 G-Norm洧녴洧녰洧녵 Rank-Surprisal Ratio洧녴洧녰洧녵 Qwen-3-4B AIME24 AIME25 AMC23 MATH500 Math Avg. GPQA-Diamond 85.2 76.6 85.4 88.8 78.8 89.4 88.8 68.8 71.9 77.5 77.5 71.9 79.4 77.5 30.8 28.3 32.5 32.5 33.3 33.3 35.0 30.8 28.3 36.7 37.5 29.2 34.2 44.2 43.4 45.5 45. 53.9 51.3 58.0 59.1 53.3 59.1 61.4 Table 20: Full post-training evaluation results for trajectory selection on Qwen-3-4B. \"Math Avg.\" denotes the average over AIME24, AIME25, AMC23, and MATH500. 22 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment Selection Methods Random Token Length洧녴洧녩洧논 Rule-based Quality洧녴洧녩洧논 LLM-judged Quality洧녴洧녩洧논 Surprisal洧녴洧녰洧녵 G-Norm洧녴洧녰洧녵 Rank-Surprisal Ratio洧녴洧녰洧녵 Qwen-2.5-3B AIME24 AIME25 AMC23 MATH500 Math Avg. GPQA-Diamond 61.2 56.0 63.8 68.4 62.0 59.0 70.8 36.3 37.5 45.0 39.4 39.4 46.3 45.0 22.2 21.2 31.3 7.5 10.0 10.0 10.8 8.3 9.2 11.7 6.7 5.0 5.8 12.5 5.8 9.2 11.7 27.9 27.1 31.2 32.8 28.9 30.9 34. Table 21: Full post-training evaluation results for trajectory selection on Qwen-2.5-3B. \"Math Avg.\" denotes the average over AIME24, AIME25, AMC23, and MATH500. Metrics Avg-Token Length Teacher Performance Verified Accuracy Rule-based Quality LLM-judged Quality G-Norm Influence Score (105) Avg-Surprisal Avg-Rank GRACE Avg-RSRtoken (108) Avg-RSRfilter token RSR (200 sample) RSR Deepseek-R1 Q3-235B GPT-120B Nemotron QwQ-32B Q3-30B Magistral GPT-20B 3822.7 0.834 0.784 -0.393 0.863 66.665 0.244 1.276 365.886 0.154 0.348 53.858 3.679 3.645 12077.7 0.911 0.849 0.226 0.908 33.615 0.161 0.660 49.412 0.028 1.564 8.713 2.916 2.925 2552.3 0.883 0.801 -0.484 0.896 72.106 0.418 1.162 430.379 0.168 0.706 62.383 3.504 3.527 10993.3 0.710 0.776 0.100 0.815 35.455 -0.008 0.410 41.968 0.025 31.957 22.701 3.252 3. 10887.1 0.923 0.857 -0.106 0.963 34.569 0.671 0.591 58.448 0.030 4.078 10.437 2.915 2.923 12571.1 0.912 0.859 -0.031 0.966 33.581 0.696 0.616 55.609 0.028 2.970 9.962 2.943 2.940 8798.2 0.823 0.786 0.259 0.882 31.801 1.447 0.424 65.293 0.025 2.598 13.663 3.342 3.352 9070.4 0.852 0.820 0.395 0.901 39.701 0.795 0.629 55.249 0.031 0.768 9.674 2.684 2.673 Phi-4 3643.1 0.727 0.804 -0.389 0.823 62.448 -0.348 1.068 303.840 0.113 0.315 43.930 3.348 3.360 Q3-8B 10473.9 0.825 0.803 0.465 0.911 33.913 0.816 0.485 45.098 0.023 2.463 8.799 2.961 3. Q3-4B 13145.8 0.873 0.835 -0.042 0.951 33.580 0.633 0.581 49.145 0.026 2.569 9.079 2.904 2.918 Table 22: Full metric assessment results on Qwen-3-14B. Model names are abbreviated with for Qwen. Metrics Avg-Token Length Teacher Performance Verified Accuracy Rule-based Quality LLM-judged Quality G-Norm Influence Score (106) Avg-Surprisal Avg-Rank GRACE Avg-RSRtoken (108) Avg-RSRfilter token RSR (200 sample) RSR Deepseek-R1 Q3-235B GPT-120B Nemotron QwQ-32B Q3-30B Magistral GPT-20B 3822.7 0.834 0.784 -0.393 0.863 109.119 2.755 1.530 66.664 0.853 0.284 17.525 4.104 4.038 12077.7 0.911 0.849 0.226 0.908 52.768 2.865 0.945 10.328 0.162 1.073 3.604 2.995 2.996 10887.1 0.923 0.857 -0.106 0.963 57.108 1.258 0.899 11.206 0.181 0.972 3.805 2.960 2. 2552.3 0.883 0.801 -0.484 0.896 120.459 2.361 1.418 72.356 1.005 0.588 18.711 3.976 3.971 12571.1 0.912 0.859 -0.031 0.966 55.501 1.330 0.927 11.101 0.177 0.872 3.791 3.044 3.044 10993.3 0.710 0.776 0.100 0.815 43.666 0.932 0.668 8.463 0.111 1.843 7.881 3.000 3.020 9070.4 0.852 0.820 0.395 0.901 56.478 1.359 0.953 11.028 0.183 0.578 3.706 2.848 2.818 8798.2 0.823 0.786 0.259 0.882 55.882 1.123 0.724 11.762 0.185 0.976 4.019 2.997 3.016 Phi-4 3643.1 0.727 0.804 -0.389 0.823 95.909 2.205 1.277 49.643 0.639 0.555 13.173 3.608 3. Q3-8B 10473.9 0.825 0.803 0.465 0.911 48.365 1.125 0.754 8.721 0.143 0.727 3.206 2.857 2.882 Q3-4B 13145.8 0.873 0.835 -0.042 0.951 53.058 1.243 0.866 9.757 0.159 0.934 3.458 2.946 2.945 Table 23: Full metric assessment results on LLaMA-3.1-8B. Model names are abbreviated with for Qwen. Metrics Avg-Token Length Teacher Performance Verified Accuracy Rule-based Quality LLM-judged Quality G-Norm Influence Score (106) Avg-Surprisal Avg-Rank GRACE Avg-RSRtoken (107) Avg-RSRfilter token RSR (200 sample) RSR Deepseek-R1 Q3-235B GPT-120B Nemotron QwQ-32B Q3-30B Magistral GPT-20B 3822.7 0.834 0.784 -0.393 0.863 78.025 -0.788 1.356 35.233 1.491 0.515 9.553 3.887 3.827 12077.7 0.911 0.849 0.226 0.908 42.327 -0.520 0.825 6.192 0.168 2.980 2.671 2.999 3. 12571.1 0.912 0.859 -0.031 0.966 39.127 -0.766 0.799 6.438 0.199 2.694 2.709 3.030 3.023 10887.1 0.923 0.857 -0.106 0.963 40.408 -0.767 0.767 6.411 0.142 2.253 2.705 2.950 2.951 10993.3 0.710 0.776 0.100 0.815 32.527 -0.466 0.553 4.724 0.119 3.582 4.359 3.067 3.091 8798.2 0.823 0.786 0.259 0.882 38.592 -0.732 0.597 6.413 0.126 5.508 3.063 3.066 3.086 9070.4 0.852 0.820 0.395 0.901 45.587 -0.737 0.820 6.312 0.160 3.055 2.542 2.803 2.779 2552.3 0.883 0.801 -0.484 0.896 84.024 -0.812 1.236 36.628 1.760 1.104 9.663 3.670 3. Phi-4 3643.1 0.727 0.804 -0.389 0.823 73.562 -0.740 1.131 25.818 0.908 0.824 7.163 3.471 3.468 Q3-8B 10473.9 0.825 0.803 0.465 0.911 38.400 -0.727 0.647 5.000 0.120 2.009 2.434 2.864 2.888 Q3-4B 13145.8 0.873 0.835 -0.042 0.951 37.860 -0.770 0.748 5.683 0.139 2.590 2.551 2.935 2.940 Table 24: Full metric assessment results on Qwen-2.5-7B. Model names are abbreviated with for Qwen. 23 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment Metrics Avg-Token Length Teacher Performance Verified Accuracy Rule-based Quality LLM-judged Quality G-Norm Influence Score (104) Avg-Surprisal Avg-Rank GRACE Avg-RSRtoken (108) Avg-RSRfilter token RSR (200 sample) RSR Deepseek-R1 Q3-235B GPT-120B Nemotron QwQ-32B Q3-30B Magistral GPT-20B 3822.7 0.834 0.784 -0.393 0.863 67.150 -1.202 1.319 72.151 0.436 0.203 18.829 3.937 3.888 12077.7 0.911 0.849 0.226 0.908 33.263 -0.284 0.721 9.172 0.148 1.314 3.641 2.947 2.958 12571.1 0.912 0.859 -0.031 0.966 33.036 -0.013 0.685 10.267 0.151 1.785 4.058 2.961 2.955 10887.1 0.923 0.857 -0.106 0.963 33.884 0.068 0.650 10.346 0.145 2.187 4.197 2.908 2.917 10993.3 0.710 0.776 0.100 0.815 39.333 -1.214 0.450 7.503 0.139 29.884 16.064 3.124 3. 8798.2 0.823 0.786 0.259 0.882 35.749 3.601 0.474 10.747 0.148 4.915 6.773 3.197 3.216 9070.4 0.852 0.820 0.395 0.901 43.248 1.919 0.693 9.773 0.165 1.122 3.619 2.660 2.649 2552.3 0.883 0.801 -0.484 0.896 70.922 -1.077 1.208 82.658 0.524 0.361 21.345 3.771 3.794 Phi-4 3643.1 0.727 0.804 -0.389 0.823 62.701 -0.965 1.102 54.562 0.425 0.182 14.369 3.579 3.588 Q3-8B 10473.9 0.825 0.803 0.465 0.911 38.211 1.395 0.526 7.710 0.124 3.123 4.010 2.881 2.919 Q3-4B 13145.8 0.873 0.835 -0.042 0.951 33.109 -1.149 0.615 9.011 0.116 2.229 3.912 2.919 2. Table 25: Full metric assessment results on Qwen-3-4B. Model names are abbreviated with for Qwen. Metrics Avg-Token Length Teacher Performance Verified Accuracy Rule-based Quality LLM-judged Quality G-Norm Influence Score (105) Avg-Surprisal Avg-Rank GRACE Avg-RSRtoken (108) Avg-RSRfilter token RSR (200 sample) RSR Deepseek-R1 Q3-235B GPT-120B Nemotron QwQ-32B Q3-30B Magistral GPT-20B 3822.7 0.834 0.784 -0.393 0.863 67.096 -2.311 1.454 119.646 0.991 0.0768 24.997 4.095 4.037 12077.7 0.911 0.849 0.226 0.908 29.027 -1.288 0.903 18.247 0.096 0.953 4.829 3.094 3.095 2552.3 0.883 0.801 -0.484 0.896 74.196 -2.157 1.346 145.459 0.602 0.177 29.896 3.929 3.944 10887.1 0.923 0.857 -0.106 0.963 27.843 -2.730 0.847 19.906 0.147 0.746 5.092 3.017 3. 12571.1 0.912 0.859 -0.031 0.966 27.225 -2.927 0.885 19.034 0.092 0.805 4.977 3.109 3.103 10993.3 0.710 0.776 0.100 0.815 26.490 -1.694 0.608 16.483 0.077 2.188 8.041 3.029 3.050 8798.2 0.823 0.786 0.259 0.882 27.733 -2.873 0.657 24.048 0.093 2.848 6.096 3.084 3.107 9070.4 0.852 0.820 0.395 0.901 30.065 -3.711 0.891 21.422 0.100 1.916 5.273 2.885 2.860 Phi-4 3643.1 0.727 0.804 -0.389 0.823 62.386 -2.555 1.210 90.940 0.426 1.814 18.963 3.603 3.622 Q3-8B 10473.9 0.825 0.803 0.465 0.911 25.996 -3.225 0.704 16.967 0.075 0.723 4.517 2.891 2. Q3-4B 13145.8 0.873 0.835 -0.042 0.951 26.518 -2.812 0.822 17.978 0.085 0.791 4.751 2.991 2.994 Table 26: Full metric assessment results on Qwen-2.5-3B. Model names are abbreviated with for Qwen. 24 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment"
        },
        {
            "title": "Prompt",
            "content": "You are meticulous and highly critical evaluator of AI reasoning. Your primary goal is to identify and quantify subtle flaws, logical gaps, inefficiencies, and hidden assumptions. Do not default to high score. Your starting assumption should be critical, and you must rigorously justify every point awarded. First, please carefully read the following problem statement: <Problem> {question} </Problem> Now, please carefully read the following candidates chain-of-thought reasoning: <Reasoning> {reasoning_to_evaluate} </Reasoning> When evaluating this reasoning, you must adhere to the following five key evaluation criteria and the scoring rubric below. Scoring Guidelines and Calibration: You must use the full 0.0 to 1.0 scale. Scores should not be clustered at the top. Use this rubric to anchor your scores: 1.0 (Exceptional/Flawless): Reserved for reasoning that is not only correct but also elegant, insightful, and comprehensive. It is perfectly structured and leaves no room for doubt. This score should be exceedingly rare. 0.8 - 0.9 (Excellent but Imperfect): The core reasoning is valid and well-supported, but there may be very minor, superficial issues (e.g., trivial typo in formula that doesnt affect the outcome, slightly awkward phrasing). The conclusion is unaffected. 0.5 - 0.7 (Competent but Flawed): The reasoning is generally on the right track but contains noticeable and nontrivial flaws. Examples include: minor factual error, logical leap that requires the reader to fill in the blanks, an inefficient method where much simpler one exists, or partially incomplete answer. 0.2 - 0.4 (Poor): The reasoning contains fundamental flaws that largely invalidate the process or conclusion. Examples include: significant factual error, clear logical fallacy, misunderstanding of the core problem constraints. 0.0 - 0.1 (Unacceptable): The reasoning is completely incorrect, irrelevant, nonsensical, or makes no meaningful attempt to solve the problem. Crucial Instruction for High Scores: To combat score inflation, you must justify high scores with the same rigor as low scores. For any criterion where you assign score of 0.9 or 1.0, your justification must explicitly state what makes the reasoning exceptional and why it lacks even subtle flaws. Evaluation Criteria: Factual Accuracy: Scrutinize every claim, formula, and piece of domain knowledge. Is it precisely correct? Assess the application of problem constraints, paying close attention to edge cases and boundary conditions. Penalize any inaccuracy, no matter how small. Logical Rigor: Probe for hidden assumptions and unstated premises. Does each conclusion necessarily and unambiguously follow from the preceding steps? Identify any logical fallacies, contradictions, or jumps in reasoning. chain is only as strong as its weakest link. Solution Completeness: Does the reasoning address all parts of the problem statement exhaustively? Does it consider all possible cases, sub-problems, and nuances? An answer that is correct for one case but ignores others is incomplete. Reasoning Efficiency: Is this the most direct and economical path to the solution? Penalize any unnecessary complexity, redundant steps, or exploration of irrelevant tangents, even if they eventually lead to the correct answer. The cognitive effort should be proportionate to the problems complexity. Presentation Quality: How clearly is the reasoning communicated? Is the structure logical and easy to follow? Ambiguous language, poor organization, or confusing sequence of steps should be penalized. An observer should be able to verify the reasoning process without difficulty. For each of the five evaluation criteria, please give score from 0.0 to 1.0 (in 0.1 increments) and brief, clear justification for that score in the JSON structure. 25 Which Reasoning Trajectories Teach Students to Reason Better? Simple Metric of Informative Alignment Your output must be single, valid JSON object. The format of the JSON object is as follows: json { \"dimensional_evaluation\": { \"factual_accuracy\": { \"score\": <float between 0.0 and 1.0>, \"reason\": \"<Your justification for the factual accuracy score>\" }, \"logical_rigor\": { \"score\": <float between 0.0 and 1.0>, \"reason\": \"<Your justification for the logical rigor score>\" }, \"solution_completeness\": { \"score\": <float between 0.0 and 1.0>, \"reason\": \"<Your justification for the solution completeness score>\" }, \"reasoning_efficiency\": { \"score\": <float between 0.0 and 1.0>, \"reason\": \"<Your justification for the reasoning efficiency score>\" }, \"presentation_quality\": { \"score\": <float between 0.0 and 1.0>, \"reason\": \"<Your justification for the presentation quality score>\" } }, \"overall_score\": <float between 0.0 and 1.0>, \"overall_reason\": \"<A concise summary justifying the overall score by synthesizing the key findings from the dimensional evaluation.>\" } Table 27: Evaluation prompt for LLM-judged quality assessment."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai AI Laboratory",
        "University of Sydney",
        "University of Toronto"
    ]
}