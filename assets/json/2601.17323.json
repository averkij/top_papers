{
    "paper_title": "SkyReels-V3 Technique Report",
    "authors": [
        "Debang Li",
        "Zhengcong Fei",
        "Tuanhui Li",
        "Yikun Dou",
        "Zheng Chen",
        "Jiangping Yang",
        "Mingyuan Fan",
        "Jingtao Xu",
        "Jiahua Wang",
        "Baoxuan Gu",
        "Mingshan Chang",
        "Yuqiang Xie",
        "Binjie Mao",
        "Youqiang Zhang",
        "Nuo Pang",
        "Hao Zhang",
        "Yuzhe Jin",
        "Zhiheng Xu",
        "Dixuan Lin",
        "Guibin Chen",
        "Yahui Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized. Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 2 ] . [ 1 3 2 3 7 1 . 1 0 6 2 : r SkyReels-V3 Technique Report SkyReels Team"
        },
        {
            "title": "Abstract",
            "content": "Video generation serves as cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, conditional video generation model, built upon unified multimodal in-context learning framework with diffusion Transformers. SkyReelsV3 model supports three core generative paradigms within single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce highfidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design comprehensive data processing pipeline that leverages cross-frame pairing, image editing, and semantic rewriting, effectively mitigating copypaste artifacts. During training, an imagevideo hybrid strategy combined with multiresolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audioconditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized. Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-ofthe-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3."
        },
        {
            "title": "Introduction",
            "content": "World models aim to capture, simulate, and forecast the dynamics of complex real-world environments, and they form fundamental basis for deploying artificial intelligence in practical scenarios [11]. In between, video generation frameworks encode rich geometric, semantic, and physical knowledge through the synthesis of visual sequences, thereby enabling effective modeling and prediction of the physical world, especially for multi-modal conditions. In recent years, diffusion-based Transformers [13, 4] architecture has driven significant advances in video generation. wide range of commercial systemsincluding Veo [7], Sora [12], Seedance [6, 2], Kling [9], as well as opensource models such as Wanx [14], HunyuanVideo [15, 8], SkyReels [1], and CogVideoX [17], have demonstrated strong performance across multiple dimensions. However, multimodal in-context in video generation is still under-explored. In this repport, we introduce SkyReels-V3, unified multimodal condition video generation framework, designed to support wide range of high-quality video synthesis tasks within single model family. Built upon multimodal in-context learning paradigm, SkyReels V3 seamlessly integrates visual reference, video, audio, and textual inputs to enable flexible and controllable video generation. The framework natively supports three core capabilities: reference images to video, video-to-video extension, and audio-guided video generation, also known as talking avatar. At the architectural Preprint. Figure 1: Reference Images to Video Results. SkyReels-V3 can facilitate dynamic interplay between different subjects within specified contexts. Figure 2: Reference Images to Video Results. SkyReels-V3 can enable dynamic interactions between diverse subjects (characters/objects) within arbitrary scenes. level, SkyReels-V3 incorporates large-scale diffusion Transformers while with carefully designed alignment strategies with multimodal condition and advanced spatiotemporal consistency modeling. Through hybrid imagevideo as well as multi-resolution joint optimization, the system achieves precise instruction following, high-fidelity motion generation and superior sub-domain capacity such as robust identity preservation and precise audio-visual synchronization. These design choices allow SkyReels-V3 to move beyond frame-level synthesis, enabling coherent narrative progression and cinematic-quality visual composition. With its strong generalization ability and modular design, SkyReels-V3 can be applied to diverse real-world scenarios, including professional video production, virtual avatars, short-form content creation, live-stream commerce, and digital entertainment. Extensive evaluations demonstrate that SkyReels-V3 reaches or surpasses industry-leading performance across key metrics, making it powerful open-source foundation for next-generation video generation research and applications."
        },
        {
            "title": "2 Methods and Evaluation",
            "content": "The SkyReels-V3 model family supports range of capabilities, including multi-reference image-tovideo synthesis, audio-guided video generation, and video-to-video extension. This chapter provides detailed descriptions and performance evaluation to these features. 2.1 Reference Images to Video SkyReels-V3 can synthesizes temporally coherent video sequences conditioned on multiple visual references and textual prompt. Given one to four reference images, which may correspond to characters, objects, or background scenes, the model generates videos that preserve identity attributes, spatial composition, and narrative continuity while following high-level semantic instructions. 2 Figure 3: Reference Images to Video Results. SkyReels-V3 can enable instant video creation for diverse live commerce hosts and settings. Figure 4: Reference Images to Video Results. SkyReels-V3 can make advertising and product demonstration with one picture. Reference-Preserving Data Construction. For multi-reference image-guided video generation, the quality of the reference image-to-target video pairs is crucial. To this end, we introduce dedicated data processing pipeline. Initially, we filter video data from massive in house dataset, selecting clips that exhibit both high visual quality and significant dynamic motion. Then, reference frames are selected from continuous video sequences using cross-frame pairing strategy [5, 3], ensuring temporal diversity while maintaining semantic consistency. Image editing models [10, 16] are then applied to extract subject regions and perform background completion, together with semantic rewriting, to construct training pairs that avoid trivial frame copying and reduce copy-and-paste artifacts. Furthermore, we have developed multiple filtering steps to remove distorted and inconsistent reference images generated by editing models. 3 Table 1: Quantitative comparison of video generation models on reference consistency, instruction following, and visual quality. Higher is better."
        },
        {
            "title": "Model",
            "content": "Reference Consistency Instruction Following Visual Quality Vidu Q2 Kling 1.6 PixVerse V5 SkyReels-V3 0.5961 0.6630 0.6542 0.6698 27.84 29.23 29.34 27.22 0.7877 0.8034 0.7976 0. Figure 5: Single-shot Video Extension Results. Multi-Reference Conditioning. To effectively integrate heterogeneous reference inputs, SkyReelsV3 employs unified multi-reference conditioning strategy that jointly encodes visual and textual information. For each reference image, we encode it using the video VAE and subsequently concatenate the resulting latent representation with the video latents. By allowing up to four reference images, the model supports flexible scene composition and enables fine-grained control over subject appearance and background structure. This design facilitates complex multi-subject and multi-element video generation without requiring explicit manual composition. Training Strategy. We train the model using an imagevideo hybrid training scheme that jointly leverages large-scale image and video datasets. This approach enhances generalization by exposing the model to both static appearance cues and dynamic motion patterns. In addition, multi-resolution joint training is employed to improve robustness across different spatial scales and aspect ratios, enabling the model to natively support wide range of output configurations. Benchmark. We construct test set comprising 200 data pairs, with sources spanning scenarios such as film and television, e-commerce, and advertising. The types of reference images include those featuring characters, animals, objects, and background scenes. The models capabilities are evaluated from three primary perspectives: Reference Consistency, Instruction Following, and Visual Quality. Specifically, Reference Consistency encompasses aspects such as facial consistency, clothing consistency, object consistency, and background consistency. Visual Quality is assessed based on image quality, dynamic degree, aesthetic quality, and motion smoothness. SkyReels-V3 has been benchmarked against leading contemporary models, with comparative results presented in Table 1. The results demonstrate that the SkyReels-V3 model achieves state-of-the-art performance within the industry. We also show qualitative results in Figure 1, Figure 2, Figure 3, and Figure 4. These results indicate that the model demonstrates strong generalization capabilities across wide range of scenarios. 4 Figure 6: Shot-switching Video Extension (Cut In) Results. Figure 7: Shot-switching Video Extension (Cut Out) Results. 2.2 Video Extension The SkyReels-V3 Video Extension Model is designed to extend an input video segment into temporally coherent and semantically consistent subsequent content under textual guidance. Given an initial video clip, the model generates continuation segments that preserve motion dynamics, scene structure, and visual style, while maintaining narrative coherence across extended temporal horizons. It supports that: (i) Dual extension modes. The model supports both single-shot video extension and shot switching video extension. For shot switching video extension, five predefined transition types are supported, and the mode can be selected either manually or through automatic detection. (ii) High-quality visual synthesis. The system produces visually coherent extensions with stable composition, smooth motion, and seamless temporal continuity. (iii) Style-consistent generation. Visual style cues from the input video are explicitly preserved, enabling faithful continuation across realistic, cinematic, and domain-specific aesthetics. (iv) High-definition and flexible outputs. The model supports 720p video generation with adjustable extension durations ranging from 5 to 30 seconds for single-shot extension, as well as multiple aspect ratios (1:1, 3:4, 4:3, 16:9, and 9:16). To achieve this, we introduce: (i) Shot switching detector. We have developed shot switching detector to analyze long-form videos. It identifies whether shot transitions (cuts) are present and classifies their types. Currently supported transition types include single shot, cut-in, cut-out, multiangle, shot/reverse shot, and cut-away. This detector enables the construction of effective training data. (ii) Unified multi-segment positional encoding and hierarchical training. unified positional encoding scheme, combined with hybrid hierarchical data training, enables accurate motion modeling and smooth transitions across complex, multi-segment video extensions. (iii) Robust spatiotemporal modeling. The model effectively handles challenging scenarios, including rapid motion, multisubject interactions, and abrupt scene changes, while enforcing physical plausibility and temporal consistency. Figure 5 presents the results for single-shot extension, and Figure 6 to 10 illustrate the extension results for cut-in, cut-out, multi-angle, shot/reserve shot, and cut-away. The model generalizes well across diverse application scenarios, including cinematic content creation, short-form series production, game cutscenes, and long-form video enhancement, producing high-definition outputs with sharp visual details and natural motion dynamics. Figure 8: Shot-switching Video Extension (Multi Angle) Results. Figure 9: Shot-switching Video Extension (Shot/Reverse Shot) Results. 2.3 Talking Avatar Talking avatar model enables high-quality audio-conditioned video generation from single portrait image and an input audio clip. The system is designed to produce temporally coherent, visually realistic videos with accurate audiovisual synchronization, supporting long-form generation and multi-character interactions. It key improvements includes: (i) High-fidelity visual synthesis and precise lip synchronization. The model can generate 720p videos at 24 fps, delivering smooth motion and fine-grained facial details. It supports multiple languages and speech types, ensuring that lip movements are closely aligned with phoneme-level audio dynamics, thereby enhancing realism and perceptual authenticity. (ii) Multi-style character generalization. The framework is compatible with wide range of visual styles, including photorealistic humans, cartoons, animals, and stylized characters. This flexibility enables broad applicability in virtual avatars, brand representation, and creative content generation. (iii) Long-form coherent video generation. The model supports minutelevel video synthesis in single forward generation process, maintaining identity consistency, motion continuity, and expressive stability over extended durations. This makes it suitable for applications such as instructional videos, news narration, and long-form storytelling. (iv) Multi-character scene support. The system is optimized for scenarios involving multiple avatars, allowing explicit role assignment and coordinated interactions. This capability facilitates the generation of dialogues, interviews, and other complex conversational scenes. Note that in multi-person scenes, the mask must be used to specify which character is speaking. Talking avatar model jointly analyzes audio signals, visual inputs, and textual cues to infer appropriate facial expressions, head movements, and camera dynamics, resulting in semantically and emotionally aligned video generative performances. To achieve accurate lip synchronization, the model is trained using dedicated audiovisual alignment strategies with region masking that explicitly model the correspondence between speech units and facial motion. This design ensures robust performance across diverse languages, speaking styles, singing voices, and rapid speech patterns. Furthermore, key-frame-constrained generation framework is introduced to improve temporal stability in long videos. The model first establishes structurally important key content and then generates smooth transitions between key frames, ensuring consistent character appearance and natural motion flow throughout the sequence. In internal evaluations against representative mainstream talking avatar models, talking avatar model demonstrates superior performance across multiple dimensions, including overall visual quality, lip synchronization accuracy, and expressive realism. The results indicate clear advantage in producing stable, high-quality, and perceptually convincing talking avatar videos. Figure 10: Shot-switching Video Extension (Cut Away) Results. Figure 11: Generation results with multiple objectives and styles. The results show the models generalization across various non-human subjects as well as different styles."
        },
        {
            "title": "3 Conclusion",
            "content": "In this work, we presented SkyReels-V3, unified multimodal video generation framework that integrates reference-based video synthesis, video extension, and audio-driven talking avatar generation within single in-context learning paradigm. By jointly modeling visual, temporal, and auditory signals, SkyReels-V3 advances video generation from short, frame-level synthesis toward coherent, narrative-level content creation. Through innovations in multimodal conditioning, hybrid imagevideo training, hierarchical spatiotemporal modeling, and efficient token fusion strategies, the proposed system achieves strong subject consistency, high-fidelity motion generation, and robust instruction following across diverse tasks and aspect ratios. Extensive empirical evaluations demonstrate that SkyReels-V3 attains competitive performance on multiple benchmarks, rivaling leading closed-source models in various domain performance. Overall, SkyReels-V3 represents significant step toward scalable, controllable, and general-purpose video generation systems, and provides solid foundation for future research in multimodal generative modeling and cinematic-level video synthesis."
        },
        {
            "title": "References",
            "content": "[1] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, et al. Skyreels-v2: Infinite-length film 7 Figure 12: Multi-person results. It presents dialogue scenario, where characters correctly respond to conversational audio by switching between speaking and idle states. It showcase performance in multi-person scenes, with coordinated behavior for both speakers and listeners. Figure 13: Minute-long video generation. It show consistent visual effects with accurate audio alignment. generative model. arXiv preprint arXiv:2504.13074, 2025. [2] Siyan Chen, Yanfei Chen, Ying Chen, Zhuo Chen, Feng Cheng, Xuyan Chi, Jian Cong, Qinpeng Cui, Qide Dong, Junliang Fan, et al. Seedance 1.5 pro: native audio-visual joint generation foundation model. arXiv preprint arXiv:2512.13507, 2025. [3] Zhuowei Chen, Bingchuan Li, Tianxiang Ma, Lijie Liu, Mingcong Liu, Yi Zhang, Gen Li, Xinghui Li, Siyu Zhou, Qian He, et al. Phantom-data: Towards general subject-consistent video generation dataset. arXiv preprint arXiv:2506.18851, 2025. [4] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Scaling diffusion transformers to 16 billion parameters. arXiv preprint arXiv:2407.11633, 2024. [5] Zhengcong Fei, Debang Li, Di Qiu, Jiahua Wang, Yikun Dou, Rui Wang, Jingtao Xu, Mingyuan Fan, Guibin Chen, Yang Li, et al. Skyreels-a2: Compose anything in video diffusion transformers. arXiv preprint arXiv:2504.02436, 2025. [6] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [7] Google. Veo. https://deepmind.google/models/veo/, 2024. 8 Table 2: Quantitative comparison of talking avatar models on audiovisual synchronization, visual quality, and character consistency. Higher is better."
        },
        {
            "title": "Model",
            "content": "AudioVisual Sync Visual Quality Character Consistency OmniHuman 1.5 KlingAvatar HunyuanAvatar SkyReels-V3 8.25 8.01 6.72 8.18 4.60 4.55 4.50 4.60 0.81 0.78 0.74 0.80 [8] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [9] Kuaishou. Kling. https://klingai.com, 2024. [10] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. [11] Yutaka Matsuo, Yann LeCun, Maneesh Sahani, Doina Precup, David Silver, Masashi Sugiyama, Eiji Uchibe, and Jun Morimoto. Deep learning, reinforcement learning, and world models. Neural Networks, 152:267275, 2022. [12] OpenAI. Sora. https://openai.com/sora/, 2024. [13] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [14] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [15] Bing Wu, Chang Zou, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Jack Peng, Jianbing Wu, Jiangfeng Xiong, Jie Jiang, et al. Hunyuanvideo 1.5 technical report. arXiv preprint arXiv:2511.18870, 2025. [16] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. [17] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024."
        },
        {
            "title": "Contributors and Acknowledgments",
            "content": "Debang Li, Zhengcong Fei, Tuanhui Li, Yikun Dou, Zheng Chen, Jiangping Yang, Mingyuan Fan Jingtao Xu, Jiahua Wang, Baoxuan Gu, Mingshan Chang, Yuqiang Xie, Binjie Mao, Youqiang Zhang Nuo Pang, Hao Zhang, Yuzhe Jin, Zhiheng Xu, Dixuan Lin, Guibin Chen, Yahui Zhou"
        }
    ],
    "affiliations": [
        "SkyworkAI"
    ]
}