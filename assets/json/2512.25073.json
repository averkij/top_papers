{
    "paper_title": "GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction",
    "authors": [
        "Yi-Chuan Huang",
        "Hao-Jen Chien",
        "Chin-Yang Lin",
        "Ying-Huan Chen",
        "Yu-Lun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a $25\\times$ speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 3 7 0 5 2 . 2 1 5 2 : r GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction Yi-Chuan Huang Hao-Jen Chien Chin-Yang Lin Ying-Huan Chen Yu-Lun Liu"
        },
        {
            "title": "National Yang Ming Chiao Tung University",
            "content": "Figure 1. Overview and comparison. (Top) Our method, GaMO (Geometry-aware Multi-view Diffusion Outpainter), expands sparse input views into wide-FOV outpainted views via multi-view diffusion model, which are then used to refine 3D Gaussian Splatting (3DGS) [28] reconstruction, producing high-fidelity novel views with improved geometric consistency and visual clarity. (Bottom) Qualitative comparison with existing methods, including 3DGS [28], FSGS [117], Difix3D [81], and GuidedVD-3DGS [114]. Previous approaches suffer from holes, ghosting, or inconsistent geometry when trained with sparse inputs. In contrast, our method effectively mitigates these artifacts and achieves superior image quality."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and priorbased techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometryaware Multi-view Outpainter), framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which 1 inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multiview conditioning and geometry-aware denoising strategies in zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-theart reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving 25 speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/ 1. Introduction Reconstructing complete 3D scenes from limited input views is fundamental problem with numerous tangible applications, ranging from virtual property tours to immersive telepresence. However, it remains notoriously difficult, often resulting in broken geometry and visible visual artifacts. Previous approaches attempted to address the sparsity of input views through regularization, semantic priors, or geometric constraints [22, 33, 54, 71, 97, 117]. These methods remain limited in handling unobserved regions. Recently, diffusion-based approaches [1, 81, 83, 85, 114] have generated novel views to improve the reconstruction quality for sparse observations. Nevertheless, these methods show three fundamental limitations: (1) novel view generation mainly focuses on enhancing angular coverage of existing geometry and often overlooks the extension beyond the periphery, leaving persistent holes and ghostings in the reconstruction; (2) geometric and photometric inconsistencies across novel and input views inevitably become prominent as view overlap increases due to internal diffusion variations; (3) novel view generation requires elaborate trajectory planning and camera pose sampling, making the process time-consuming. Fig. 2 shows that when using multi-view diffusion models [3], adding more diffusion-generated novel views degrades reconstruction quality, suggesting that sparse views alone can yield robust results. Increasing the number of novel views from 3 to 11 counterintuitively worsens both geometric accuracy (SSIM) and perceptual quality (LPIPS) by introducing inconsistencies and artifacts into the reconstructed scene. We observe that outpainting, rather than novel view generation, offers more suitable paradigm for enhancing sparseview 3D reconstruction. By generating missing content around existing input views, outpainting naturally preserves geometric consistency and avoids multi-view alignment challenges. We propose GaMO (Geometry-aware Multiview Outpainting) to address the aforementioned limitations: (1) it expands the field of view(FOV) to cover unobserved regions, reducing holes and edge blur; (2) it utilizes existing viewpoints for content expansion rather than fusing information Figure 2. Motivation: Outpainting vs. novel view generation. Using multi-view diffusion [3], we train 3DGS [28] with three strategies: interpolated novel views (green), 3-view baseline (blue), and outpainting (orange). Top: Visual comparison shows interpolating additional novel views (311 total) introduces inconsistencies and artifacts. Bottom: Quantitative metrics (SSIM, LPIPS) show that adding novel views degrades quality due to inconsistencies, while outpainting consistently improves both geometric and perceptual quality. from multiple novel views in 3D space, preventing misalignment and ghosting artifacts; and (3) it completes reconstruction efficiently in single outpainting pass without complex trajectory planning, achieving 25 speedup over video diffusion-based methods. Following [114, 115], we evaluate on Replica [63] and ScanNet++ [102] with improvements across all metrics. We summarize our contributions as follows: We establish outpainting as superior paradigm for sparseview reconstruction, eliminating common issues including holes, ghosting artifacts, and geometric inconsistencies. We develop geometry-aware outpainting approach with novel conditioning and denoising strategies in zero-shot manner without finetuning. We achieve state-of-the-art performance on Replica and ScanNet++ across 3, 6, and 9 input views, with substantial gains in geometric accuracy and perceptual quality, while reducing reconstruction time to under 10 minutes. 2. Related Work Sparse-view 3D Gaussian Splatting. While 3DGS [28] achieves remarkable quality with dense inputs, sparseview reconstruction remains challenging, particularly for indoor scenes [34]. Recent methods employ depth regularization [10, 33], proximity-guided unpooling [117], dualfield co-regularization [110], and robust handling of unposed inputs [37]. Feed-forward approaches leverage cost volumes [5, 7], Gaussian bundle adjustment [12], or transformer architectures [66, 76, 93]. Methods combining depth 2 priors include DN-Splatter [69] with depth and normal cues, SplatFields [52] regularizing spatial autocorrelation, and large model priors [17, 61, 74, 105]. While these methods regularize 3D representations, our work augments training data through geometry-aware outpainting for more complete scene coverage. Multi-view diffusion models for 3D. Multi-view diffusion enables consistent 3D generation through multi-view attention [60], synchronized volume attention [46], orthogonal view generation [59], and cross-domain diffusion [47]. Recent advances enforce consistency via 3D feature unprojection [98], epipolar attention [20], depth-guided attention [19], and differentiable rasterization [48]. Video diffusion models provide temporal consistency for multi-view synthesis [8, 15, 30, 70, 109]. Additional methods include mesh generation [92], epipolar constraints [35], combined 2D-3D priors [42], correspondence-aware attention [65], and 3D feature fields [4]. These methods generate novel views from different poses. Our work performs multi-view outpainting to expand field-of-view of existing views, maintaining stronger geometric consistency for sparse-view scene reconstruction. Diffusion priors for 3D reconstruction. Diffusion models provide learned priors through Score Distillation Sampling [36, 57, 78]. Improvements address oversmoothing [49], provide unified frameworks [50], and optimize both 3D models and priors [9, 99]. Video diffusion serves as powerful priors [39, 51, 103]. Reconstruction methods use multi-view conditioning [83], pseudo-observation enhancement [44], scene-grounding guidance [114], iterative refinement [45, 81, 85], and various coupling strategies [21, 43, 53, 91, 96]. Native 3D diffusion includes latent approaches [16, 31, 82, 84, 100] and RL finetuning [87]. While these methods generate additional views or provide guidance, they face multi-view inconsistency. Our insight: diffusion models suit outpainting known views better than hallucinating novel perspectives, maintaining stronger geometric grounding. Geometry-aware generation. Geometric consistency leverages Plucker coordinates for camera conditioning [23, 26, 90, 94, 111] and epipolar constraints or voxel representations [68] for multi-view consistency [20, 29, 53, 79, 89, 101]. Depth and normal conditioning proves critical [11, 14, 19, 27, 47, 48, 56, 72]. Recent panoramic generation [73, 113] and video outpainting [107] typically operate in 2D or single-view scenarios. Our approach uniquely combines multi-view outpainting with geometry awareness through coarse 3DGS rendering, opacity-based masking, and noise resampling for consistent, geometrically plausible FOV expansion. Outpainting and FOV expansion. Diffusion-based outpainting includes panoramic methods [13, 25, 58, 86, 104, 108, 113] and restoration tasks [41, 67]. For 3D scenarios, methods employ visibility-aware inpainting [40, 80], video diffusion priors [40], NeRF-guided training [106], iterative 3DGS updates [107], and multi-view SDS [6]. General sparse-view baselines include feed-forward prediction [5, 7], regularized optimization [55, 88], and NeRF-based methods [38, 54, 64, 71, 97]. These works require per-scene finetuning or focus on single-view outpainting. Our method performs zero-shot multi-view outpainting using pre-trained MVGenMaster [3] with geometry-aware mechanisms ensuring cross-view consistency without scene-specific training. 3. Preliminaries 3D Gaussian Splatting (3DGS) [28] uses collection of anisotropic 3D Gaussian primitives to present scene. Each Gaussian is defined by its center position µ R3, 3D covariance matrix Σ, an opacity value α [0, 1], and spherical harmonic coefficients for view-dependent color. The covariance matrix is decomposed into scaling vector R3 and rotation quaternion R4 as Σ = RSST RT , where is derived from and = diag(s). The Gaussian function is: (cid:18) G(x) = exp 1 2 (x µ)T Σ1(x µ) (cid:19) . (1) To render given viewpoint, 3DGS projects each 3D Gaussian onto the 2D image plane, obtaining 2D Gaussian G(u), where is pixel coordinates. The color of pixel is computed via α-blending of ordered Gaussians: C(u) = (cid:88) iN i1 (cid:89) ciσi (1 σj), j=1 (2) where denotes the set of Gaussians overlapping pixel u, sorted in depth order, ci represents the color of the i-th Gaussian, and σi = αiG i(u) is the opacity contribution. Diffusion Models generate samples through learned denoising process that reverses forward noising process. The forward process gradually adds Gaussian noise to data x0 over timesteps: xt = 1 αtϵ, where ϵ (0, I) and αt is predefined noise schedule. The reverse process learns to denoise xt back to x0 by training neural network ϵθ to predict the noise at each timestep. The training objective is the simplified loss function: (cid:2)ϵ ϵθ(xt, t, c)2(cid:3) , Lsimple = Et,x0,ϵ αtx0 + (3) where represents conditioning information, and the model learns to minimize the mean squared error between the true noise ϵ and the predicted noise. During inference, samples are generated by iteratively denoising from pure noise xT (0, I) using the DDIM [62] sampling process. 3 where σi = αiG i(u) denotes the opacity contribution of the i-th Gaussian at pixel u. The opacity mask is then obtained by thresholding the opacity map with = I(O < ηmask), where ηmask is threshold value and I() is the indicator function. Regions where = 1 correspond to areas with low opacity that require outpainting. Coarse Rendering. We render color image Icoarse with the enlarged FOV from the coarse 3DGS model. This coarse rendering serves as reference that provides geometric and appearance priors to the diffusion model, maintaining consistency between outpainted and existing scene content. 4.2. GaMO: Geometry-aware Multi-view Diffusion"
        },
        {
            "title": "Outpainter",
            "content": "Our geometry-aware outpainting method operates through three key components: (1) multi-view conditioning that provides structural and appearance guidance; (2) mask latent blending that integrates coarse geometry priors during denoising; and (3) iterative mask scheduling with noise resampling that ensure smooth transitions. The model operates in latent space using DDIM sampling [62] for efficient denoising. Multi-View Conditioning. Given set of sparse input RGB images {Ii}N i=1 and their corresponding camera parameters {Πi}N i=1, our model generates outpainted views conditioned on camera representations, geometric correspondences, and appearance features, as illustrated in Fig. 4(a). y) to align with the enlarged FOV. For camera representation, we employ Plucker ray embeddings [95] that provide dense 6D ray parameterizations for each pixel, compactly encoding both ray origin and direction for geometry-aware reasoning. The embedding of each input view Pr is derived from its corresponding camera parameters Πr, while the embedding of the outpainted view uses the same camera parameters with scaled focal lengths (f x, For geometric correspondence, we warp input RGB images and Canonical Coordinate Maps (CCM) to align with the expanded FOV by unprojecting pixels to 3D and reprojecting onto the outpainted camera plane, producing Cwarp rt and warp rt. We then downsample the original inputs by factor Sk and place them at the center of the warped features, creating augmented signals aug rt where the center preserves exact input information while the periphery retains warped geometric structure to guide outpainting. rt and Caug For appearance features, the input RGB images are encoded through variational autoencoder (VAE) to obtain clean latent features zr. The noisy latent features zt are randomly generated and will be denoised to generate the outpainted views."
        },
        {
            "title": "All",
            "content": "conditioning signals are processed through lightweight convolutional encoders. For input views, Plucker ray embeddings Pr, CCM Cr, and RGB images Figure 3. Overview of Our Pipeline. Given sparse input views, our method follows three-stage process. (a) Coarse 3D Initialization: We obtain geometry priors from initial 3D reconstruction, including an opacity mask and coarse render that provide essential structural cues. (b) GaMO: Geometry-aware Multi-view Outpainter: Using the geometry priors, GaMO generates outpainted views with enlarged FOV via multi-view diffusion model. (c) Refined Reconstruction: The outpainted views are used to refine the 3D reconstruction, resulting in improved completeness and consistency. 4. Method As shown in Fig. 2, outpainting existing input views is more effective than generating novel views from new camera poses. To address challenges in sparse-view 3D reconstruction, we perform geometry-aware outpainting by leveraging multi-view diffusion models. By expanding the fieldof-view (FOV) of input images, our method simultaneously fills holes, fixes blurred boundaries, and preserves geometric consistency without modifying existing content, resulting in significantly simpler and faster reconstruction process. As illustrated in Fig. 3, our pipeline consists of three stages: coarse 3D initialization to obtain geometry priors (Sec. 4.1), geometry-aware multi-view outpainting to generate enlarged FOV views (Sec. 4.2), and refined 3D reconstruction using the outpainted views (Sec. 4.3). 4.1. Coarse 3D Initialization To ensure geometric consistency in the diffusion model, we use DUSt3R [75] to generate an initial point cloud and train coarse 3DGS model to capture the scene geometry. Using this coarse model, we identify outpainting regions by rendering an opacity mask with FOV wider than the input views. We also render coarse color image to provide appearance priors for the outpainting process in Sec. 4.2. Opacity Mask. We enlarge the FOV by reducing the focal lengths with scaling ratio Sk < 1 (i.e., = fx Sk, = fy Sk). For each target outpainted view, we first render an opacity map by α-blending the opacity values of the Gaussians: O(u) = i1 (cid:89) (cid:88) σi (1 σj), iN j=1 (4) 4 Figure 4. Overview of GaMO (Geometry-aware Multi-view Diffusion Outpainter). (a) Multi-view Diffusion Conditioning: Sparse input views are encoded into clean latents and combined with multi-view conditions, including Plucker ray embeddings for input views (Pr) and the target view with enlarged FOV (P ), along with original and augmented Canonical Coordinate Map (CCM) and RGB, to provide both geometric and appearance cues for diffusion model conditioning. (b) Denoising Process: Coarse geometry priors (opacity mask and coarse render) guide the denoising through mask latent blending performed at multiple timesteps (t1, t2, ..., tN ) with progressive dilation and noise resampling, generating outpainted views with enlarged FOV (c). Ir are jointly added to the clean latent features zr. For the target outpainted view, rt are jointly added to the noisy latent features zt. We then condition the pre-trained diffusion model with the fused features to generate outpainted view latents in zero-shot manner: rt, and aug , Caug pθ(ztzr, Pr, Cr, Ir, , Caug rt, aug rt), (5) where θ denotes the pre-trained model [3] parameters. These multi-view conditions ensure that the diffusion process maintains geometric consistency across views, even under an enlarged FOV. Denoising Process with Mask Latent Blending. As the central component of our geometry-aware framework, mask latent blending integrates coarse geometry priors from the coarse 3D initialization (Sec. 4.1) into the diffusion loop. As outlined in Alg. 1, this process ensures that outpainted content respects existing scene structures while generating plausible peripheral regions. Fig. 4(b) shows that the opacity mask and coarse rendering Icoarse provide consistent structural guidance throughout denoising. At selected denoising timesteps {t1, t2, ..., tN }, we perform mask latent blending between the denoised latent and the coarse geometry prior. To ensure both latents share the same noise level, we add noise to the coarse latent, which is obtained by encoding the coarse rendering into latent space, before blending them using latent-space mask Mlatent. The mask evolution is controlled by iterative mask scheduling (Sec. 4.2): zblend tk = (1 M(k) latent) zcoarse tk + M(k) latent ztk , (6) where ztk is the denoised latent, zcoarse tk is the coarse latent 5 Algorithm 1 Geometry-aware Multi-view Outpainter }M j= zs1 = Denoise(zs, conditions) if {t1, t2, . . . , tN } then 1: Input: Coarse render Icoarse, opacity mask 2: Output: Outpainted views {Sout 3: Setup: Noise schedule Σ = {σ1, . . . , σT } 4: Setup: latent blending iterations {t1, t2, . . . , tN } 5: Setup: resampling iterations 6: zcoarse Encode(Icoarse) 7: zT (0, I) 8: for = T, . . . , 1 do 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end if 19: 20: end for }M 21: {Sout zcoarse s1 = AddNoise(zcoarse, σs1) zblend s1 = IMS(M, zs1, zcoarse s1 ) zs1 = zblend s1 for in do ˆz0 = Predict(zs1) zresamp AddNoise(ˆz0, σs) zs1 Denoise(zresamp j=1 = Decode(z0) , conditions) end for Eq. (5) Eq. (6) Eq. (7) with matching noise level (Alg. 1, line 11), M(k) latent is the dilated mask at iteration k, and denotes element-wise multiplication. Iterative Mask Scheduling and Noise Resampling. To gradually integrate generated content with the existing geometric structure, Iterative Mask Scheduling progressively adjusts M(k) latent over iterations to control the ratio between outpainting and known coarse regions. The mask dilation is progressively reduced as denoising proceeds, allowing Table 1. Quantitative comparison on Replica [63] and ScanNet++ [102] datasets with 6 input views. 3DGS-based methods (3DGS, FSGS, Difix3D, GenFusion, GuidedVD-3DGS, Ours) use DUSt3R [75] initialization, while InstantSplat uses MASt3R [32]. Method Replica Dataset [63] ScanNet++ Dataset [102] PSNR SSIM LPIPS FID PSNR SSIM LPIPS FID 3DGS [28] FSGS [117] InstantSplat [12] Difix3D [81] GenFusion [85] GuidedVD-3DGS [114] Ours 24.74 23.91 23.09 21.86 23.98 25.67 25.84 0.862 0.846 0.849 0.811 0.855 0.861 0.877 0.124 0.145 0.141 0.188 0.142 0.147 0.109 79.19 91.19 85.93 93.28 80.83 76.26 72. 21.71 21.69 21.19 20.62 21.96 22.98 23.41 0.818 0.801 0.811 0.764 0.808 0.815 0.835 0.186 0.298 0.193 0.244 0.218 0.204 0.181 117.30 188.89 117.89 118.85 125.20 122.70 108.06 the model to first explore peripheral content and later refine geometry within coarse regions. To maintain smooth transitions across blended regions, we perform noise resampling after each blending operation. After blending, we perform noise resampling times on the blended latent to eliminate boundary artifacts and ensure smooth integration between the coarse geometry and generated content (Alg. 1, lines 1417). Specifically, we first predict the clean latent ˆz0 from the blended latent, then add noise back to the current timestep tk: zresamp tk = (cid:112)αtk ˆz0 + (cid:112)1 αtk ϵ, (7) where ˆz0 is the predicted clean latent from zblend and ϵ (0, I) denotes sampled Gaussian noise. This resampling prevents boundary artifacts and ensures smooth blending. tk This framework ensures that outpainted regions seamlessly blend with known content while maintaining geometric plausibility, with the coarse 3DGS geometry providing structural guidance throughout the generation process. Importantly, it requires only inference without fine-tuning the backbone diffusion model. }N }M 4.3. 3DGS Refinement with Outpainted Views Given the original input views {I gt i=1 and the generated outpainted views {Sout j=1 from Sec. 4.2, we refine the 3DGS model by jointly optimizing with both sets of views. During training, we sample either an input view or an outpainted view for supervision at each iteration. Loss for Input Views. We employ the standard 3DGS reconstruction loss [28] to ensure accurate reconstruction of the observed regions: Linput = (1 λs)L1(Ii, gt ) + λsLD-SSIM(Ii, gt ), (8) where Ii denotes the rendered image from input viewpoint, gt is the ground truth input view, and λs is weighting factor that balances the L1 loss and structural similarity loss. Loss for Outpainted Views. Relying solely on reconstruction loss fails to fill unobserved regions and causes artifacts. We incorporate perceptual loss [24] LLPIPS to provide 6 balanced gradients across outpainted and original regions, effectively guiding training while maintaining perceptual consistency. The loss is: Lrecon = (1 λs)L1(Sj, Sout ) + λsLD-SSIM(Sj, Sout ), Loutpainted = Lrecon(Sj, Sout ) + λpercLLPIPS(Sj, Sout ), where Sj is the rendered outpainted image and Sout generated outpainted image. (9) is the 5. Experiments 5.1. Experimental Setups Datasets and Evaluation Protocol. We evaluate on Replica [63] and ScanNet++ [102], following prior works [114, 115]. Experiments use 6 input views per scene by default, and we additionally test 3 and 9 views for sparsity analysis. We further include Mip-NeRF 360 [2] for generalization (see supplementary material). Evaluation uses standard metrics: PSNR, SSIM [77], LPIPS [112], and FID [18]. Baselines. We compare against several state-of-the-art sparse-view reconstruction methods: (1) vanilla 3DGS; (2) FSGS [117], using depth-guided Gaussian unpooling; (3) InstantSplat [12], employing MASt3R priors and selfsupervised bundle adjustment; (4) Difix3D [81], applying single-step diffusion refinement; (5) GenFusion [85], integrating reconstruction and video diffusion via cyclical fusion; and (6) GuidedVD-3DGS [114], leveraging video diffusion and evaluated using the authors official implementation and settings. For fair comparison, all methods except InstantSplat use DUSt3R [75] for initialization. Implementation Details. For coarse initialization, we train 3DGS for 10,000 iterations with λs = 0.2 and opacity threshold ηmask = 0.6. For outpainting, we use the multi-view diffusion model [3] with focal-length scaling Sk = 0.6, DDIM sampling [62] with = 50 steps, and perform latent blending at timesteps t1 = 0.7T , t2 = 0.5T , t3 = 0.3T with noise resampling = 3. Input and outpainted views share the same resolution (differing only in FOV), with dimensions Figure 5. Qualitative comparison on Replica [63] and ScanNet++ [102] datasets with 6 sparse views. 3DGS-based methods use DUSt3R [75] initialization, while InstantSplat uses MASt3R [32]. Our method produces better coverage (fewer black holes), better geometric consistency (less ghosting), and fewer artifacts compared to baselines. White boxes highlight challenging regions. Best viewed zoomed in. set as multiples of 64. Before refinement, we alpha-blend downscaled inputs at the center. For refinement, we optimize 3DGS for 3,000 iterations (3 views) or 7,000 iterations (6/9 views) with λperc = 0.1, alternating supervision between input and outpainted views. 5.2. Comparisons Quantitative Results. Tab. 1 show that our method achieves superior performance across all metrics on both datasets. On Replica, we achieve PSNR of 25.84 dB, outperforming GuidedVD-3DGS by 0.17 dB while significantly improving perceptual quality with 25.9% lower LPIPS and 4.3% lower FID. On ScanNet++, our method obtains 23.41 dB PSNR and 0.835 SSIM, with 11.3% and 11.9% improvements in LPIPS and FID over GuidedVD-3DGS. Notably, our approach is 25 faster than GuidedVD-3DGS , while delivering better results. Qualitative Results. Fig. 10 shows visual comparisons on representative scenes. Our method produces more complete reconstructions by effectively addressing the key challenges in sparse-view reconstruction: reducing black holes in unobserved regions, minimizing rendering artifacts, and improving geometric consistency. These results demonstrate the effectiveness of GaMO in generating high-quality outpainted views that enhance 3D reconstruction. 5.3. Ablation Studies rations (e.g., 1.) in figures correspond to table rows, and letters (e.g., (a), (b)) denote different visual comparison aspects. Latent Blending Strategies. Tab. 2 and Fig. 6 present ablation results on our latent blending design. Augmenting the warped features with downscaled reference RGB and CCM (rows 1-2) prevents incorrect hallucinations in known regions (a). Mask latent blending (rows 2-3) prevents severe geometric misalignment (b, red circle) and improves PSNR by 0.66 dB. Hard masking (rows 4-5) produces sharper boundaries (c) with 0.64 dB gain over soft masking. Finally, noise resampling (rows 3 vs. 5) reduces blending artifacts by 0.24 dB, generating more coherent results (d). Mask Blending Scheduling. Tab. 3 and Fig. 7 present ablation results on mask blending scheduling strategies. Singlestep blending (row 1) is insufficient as coarse geometry is easily washed out during denoising, while multi-step blending (row 2) better preserves geometric cues (a-top). Blending at every step (row 3) achieves slightly higher PSNR/SSIM but causes blurred boundaries (a-bottom) and increases denoising time, making the range-based approach preferable. Finally, Iterative Mask Scheduling (rows 2 vs. 4) substantially improves perceptual quality through progressive mask dilation, providing better geometric guidance and smoother transitions for more coherent details (b). We conduct comprehensive ablation studies on Replica and ScanNet++ datasets with 6 input views. To separately assess the outpainted view quality and novel view synthesis performance, we center crop the input images to 0.6 of their original size. For each ablation, we provide quantitative results and visual comparisons, where numbered configu3DGS Refinement Components. Tab. 4 and Fig. 8 present ablation results on 3DGS refinement components. Point cloud re-initialization using outpainted views (rows 1 vs. 3) enables the successful generation of Gaussian points in outpainted regions (a). Perceptual loss (rows 2 vs. 3) effectively fills holes and reduces artifacts by providing better gradient 7 Table 2. Ablation on latent blending strategies. Impact of augmented condition, hard/soft mask blending, and noise resampling on outpainting and 3DGS refinement quality. Evaluated on cropped views from Replica [63] and ScanNet++ [102]. Cropped Input Views Outpainted view Novel view # Aug. Hard Soft Noise PSNR SSIM LPIPS PSNR SSIM LPIPS 1 2 3 4 5* 18.97 19.11 19.77 19.37 20.01 0.776 0.779 0.797 0.797 0.800 0.210 0.207 0.199 0.196 0.190 22.37 22.53 23.52 23.22 23.53 0.821 0.822 0.839 0.840 0.839 0.197 0.192 0.174 0.173 0. Figure 6. Qualitative ablation on outpainting components. (a) Augmented conditioning aligns outpainted and known regions. (b) Mask latent blending provides essential geometric guidance. (c) Hard masks provide more accurate boundary information than soft masks. (d) Noise resampling eliminates blending boundary artifacts. Red circles highlight problem regions. White boxes show zoom-ins. Corresponds to Tab. 2 Table 3. Ablation on mask blending scheduling. Comparison of blending at different timesteps: tk (single-step), t1tN (multi-step), All (every step), and IMS (Iterative Mask Scheduling). Our full method (row 4*) combines multi-step blending with progressive IMS, achieving the best perceptual quality. Time (s) denotes generation time for all outpainted views. Cropped Input Views Outpainted view Novel view tk # 1 2 3 4* t1tN All IMS PSNR SSIM LPIPS Time (s) PSNR SSIM LPIPS 20.09 19.85 20.31 20. 0.804 0.799 0.809 0.801 0.198 0.173 0.201 0.169 85 93 167 93 23.38 23.53 23.67 23.65 0.837 0.839 0.842 0.839 0.176 0.179 0.178 0. guidance for outpainted regions (b), producing cleaner and more realistic renderings. 6. Conclusion We introduce GaMO, establishing outpainting as more suitable paradigm for novel view generation in sparse-view 3D reconstruction. By extending existing views rather than generating new perspectives, our approach preserves geometric consistency while providing broader spatial coverage, Figure 7. Ablation on mask blending scheduling strategies. (a) Comparing mask blending at different timesteps: single step tk, multi-step t1tN , and all iterations. (b) Our Iterative Mask Scheduling (IMS) progressively shrinks the region requiring outpainting, producing more plausible and coherent details with better alignment. Corresponds to Tab. 3. Table 4. Ablation on 3DGS refinement components. Impact of point re-initialization and perceptual loss on reconstruction quality. Both components contribute to improved novel view synthesis, with row 3* (our full method) achieving the best perceptual quality. Replica & ScanNet++ Novel view Point re-init. Percep. Loss PSNR SSIM LPIPS 24.80 25.14 24.93 0.860 0.857 0.861 0.140 0.139 0. # 1 2 3* Figure 8. Qualitative ablation on 3DGS refinement. (a) Point cloud re-initialization from outpainted views enables successful Gaussian generation in outpainted regions. (b) Perceptual loss enhances detail recovery in outpainted regions. White boxes show zoom-ins. Corresponds to Tab. 4. effectively mitigating holes and artifacts in generation-based methods. Extensive experiments demonstrate significant improvements over state-of-the-art with 25 speedup and superior reconstruction quality. Our method also exhibits strong zero-shot generalization, establishing outpainting as more principled and efficient approach for sparse-view 3D reconstruction. Limitations. GaMO cannot recover occluded content invisible from all input views. Performance depends on input view distribution, as clustered or misaligned views yield poor results. Future work could explore adaptive outpaint scale selection and hybrid approaches for challenging scenarios. Acknowledgements. This research was funded by the National Science and Technology Council, Taiwan, under Grants NSTC 112-2222-E-A49-004-MY2 and 113-2628E-A49-023-. The authors are grateful to Google, NVIDIA, and MediaTek Inc. for their generous donations. Yu-Lun Liu acknowledges the Yushan Young Fellow Program by the MOE in Taiwan."
        },
        {
            "title": "References",
            "content": "[1] Titas Ancikeviˇcius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan Bilen, Niloy Mitra, and Paul Guerrero. Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation. In CVPR, 2023. 2 [2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 54705479, 2022. 6, 14, 16 [3] Chenjie Cao, Chaohui Yu, Shang Liu, Fan Wang, Xiangyang Xue, and Yanwei Fu. Mvgenmaster: Scaling multi-view generation from any image via 3d priors enhanced diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 60456056, 2025. 2, 3, 5, 6, 14, 16, 17 [4] Eric Chan, Koki Nagano, Matthew Chan, Alexander Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. Generative novel view synthesis with 3d-aware diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42174229, 2023. 3 [5] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1945719467, 2024. 2, 3 [6] Honghua Chen, Chen Change Loy, and Xingang Pan. Mvipnerf: Multi-view 3d inpainting on nerf scenes via diffusion prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53445353, 2024. 3 [7] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision, pages 370386. Springer, 2024. 2, 3 [8] Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, and Jianfei Cai. Mvsplat360: Feed-forward 360 scene synthesis from sparse views. Advances in Neural Information Processing Systems, 37:107064107086, 2024. [9] Zixuan Chen, Ruijie Su, Jiahao Zhu, Lingxiao Yang, JianHuang Lai, and Xiaohua Xie. Vividdreamer: Towards highfidelity and efficient text-to-3d generation. arXiv preprint arXiv:2406.14964, 2024. 3 [10] Jaeyoung Chung, Jeongtaek Oh, and Kyoung Mu Lee. Depth-regularized optimization for 3d gaussian splatting 9 in few-shot images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 811820, 2024. 2 [11] Yiquan Duan, Xianda Guo, and Zheng Zhu. Diffusiondepth: Diffusion denoising approach for monocular depth estimation. In European Conference on Computer Vision, pages 432449. Springer, 2024. 3 [12] Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, et al. Instantsplat: Unbounded sparse-view pose-free gaussian splatting in 40 seconds. arXiv preprint arXiv:2403.20309, 2(3):4, 2024. 2, 6, 15 [13] Mengyang Feng, Jinlin Liu, Miaomiao Cui, and Xuansong Xie. Diffusion360: Seamless 360 degree panoramic image generation based on diffusion models. arXiv preprint arXiv:2311.13141, 2023. [14] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. In European Conference on Computer Vision, pages 241258. Springer, 2024. 3 [15] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view diffusion models. arXiv preprint arXiv:2405.10314, 2024. 3 [16] Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, and Tong He. Gvgen: Text-to-3d generation with volumetric representation. In European Conference on Computer Vision, pages 463479. Springer, 2024. 3 [17] Zongqi He, Zhe Xiao, Kin-Chung Chan, Yushen Zuo, Jun Xiao, and Kin-Man Lam. See in detail: Enhancing sparseview 3d gaussian splatting with local depth and semantic regularization. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. 3 [18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Advances in Neural Information Processing Systems, 2017. 6 [19] Hanzhe Hu, Zhizhuo Zhou, Varun Jampani, and Shubham Tulsiani. Mvd-fusion: Single-view 3d via depth-consistent multi-view generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 96989707, 2024. 3 [20] Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, Yangguang Li, Xinyuan Chen, Yan-Pei Cao, Ding Liang, Yu Qiao, Bo Dai, et al. Epidiff: Enhancing multi-view synthesis via localized epipolar-constrained diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 97849794, 2024. [21] Mude Hui, Zihao Wei, Hongru Zhu, Fei Xia, and Yuyin Zhou. Microdiffusion: Implicit representation-guided diffusion for 3d reconstruction from limited 2d microscopy projections. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1146011469, 2024. 3 [22] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on diet: Semantically consistent few-shot view synthesis. In ICCV, 2021. 2 [23] Chenhao Ji, Chaohui Yu, Junyao Gao, Fan Wang, and Cairong Zhao. Campvg: Camera-controlled panoramic arXiv video generation with epipolar-aware diffusion. preprint arXiv:2509.19979, 2025. 3 [24] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision (ECCV), pages 694711. Springer, 2016. 6 [25] Nikolai Kalischek, Michael Oechsle, Fabian Manhardt, Philipp Henzler, Konrad Schindler, and Federico Tombari. Cubediff: Repurposing diffusion-based image models for panorama generation. In The Thirteenth International Conference on Learning Representations, 2025. [26] Yash Kant, Aliaksandr Siarohin, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, and Igor Gilitschenski. In ProceedSpad: Spatially aware multi-view diffusers. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1002610038, 2024. 3 [27] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 94929502, 2024. 3 [28] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 1, 2, 3, 6, 14, 15, 16, 17 [29] Orest Kupyn, Fabian Manhardt, Federico Tombari, and Christian Rupprecht. Epipolar geometry improves video generation models. arXiv preprint arXiv:2510.21615, 2025. 3 [30] Jeong-gi Kwak, Erqun Dong, Yuhe Jin, Hanseok Ko, Shweta Mahajan, and Kwang Moo Yi. Vivid-1-to-3: Novel view synthesis with video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67756785, 2024. 3 [31] Yushi Lan, Fangzhou Hong, Shuai Yang, Shangchen Zhou, Xuyi Meng, Bo Dai, Xingang Pan, and Chen Change Loy. Ln3diff: Scalable latent neural fields diffusion for speedy 3d generation. In European Conference on Computer Vision, pages 112130. Springer, 2024. [32] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with mast3r. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. 6, 7 [33] Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, and Lin Gu. Dngaussian: Optimizing sparseview 3d gaussian radiance fields with global-local depth normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 20775 20785, 2024. 2 [34] Ming-Feng Li, Yueh-Feng Ku, Hong-Xuan Yen, Chi Liu, Yu-Lun Liu, Albert YC Chen, Cheng-Hao Kuo, and Min Sun. Genrc: Generative 3d room completion from sparse image collections. In European Conference on Computer Vision, pages 146163. Springer, 2024. 2 [35] Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wei Xue, Wenhan Luo, et al. Era3d: High-resolution multiview diffusion using efficient row-wise attention. Advances in Neural Information Processing Systems, 37:5597556000, 2024. 3 [36] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards highfidelity text-to-3d generation via interval score matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 65176526, 2024. 3 [37] Chin-Yang Lin, Cheng Sun, Fu-En Yang, Min-Hung Chen, Yen-Yu Lin, and Yu-Lun Liu. Longsplat: Robust unposed 3d gaussian splatting for casual long videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2741227422, 2025. 2 [38] Chin-Yang Lin, Chung-Ho Wu, Chang-Han Yeh, Shih-Han Yen, Cheng Sun, and Yu-Lun Liu. Frugalnerf: Fast convergence for extreme few-shot novel view synthesis without learned priors. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1122711238, 2025. [39] Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. Reconx: Reconstruct any scene from sparse views with video diffusion model. arXiv preprint arXiv:2408.16767, 2024. 3 [40] Kunhao Liu, Ling Shao, and Shijian Lu. Novel view extrapolation with video diffusion priors. arXiv preprint arXiv:2411.14208, 2024. 3 [41] Kuan-Hung Liu, Cheng-Kun Yang, Min-Hung Chen, YuLun Liu, and Yen-Yu Lin. Corrfill: Enhancing faithfulness in reference-based inpainting with correspondence guidance in diffusion models. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 1618 1627. IEEE, 2025. 3 [42] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1007210083, 2024. 3 [43] Minghua Liu, Chong Zeng, Xinyue Wei, Ruoxi Shi, Linghao Chen, Chao Xu, Mengqi Zhang, Zhaoning Wang, Xiaoshuai Zhang, Isabella Liu, et al. Meshformer: High-quality mesh generation with 3d-guided reconstruction model. Advances in Neural Information Processing Systems, 37:5931459341, 2024. 3 [44] Xinhang Liu, Jiaben Chen, Shiu-Hong Kao, Yu-Wing Tai, and Chi-Keung Tang. Deceptive-nerf/3dgs: Diffusiongenerated pseudo-observations for high-quality sparse-view reconstruction. In European Conference on Computer Vision, pages 337355. Springer, 2024. 3 [45] Xi Liu, Chaoyi Zhou, and Siyu Huang. 3dgs-enhancer: Enhancing unbounded 3d gaussian splatting with viewconsistent 2d diffusion priors. Advances in Neural Information Processing Systems, 37:133305133327, 2024. 3 [46] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. 3 [47] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 99709980, 2024. 3 [48] Yuanxun Lu, Jingyang Zhang, Shiwei Li, Tian Fang, David McKinnon, Yanghai Tsin, Long Quan, Xun Cao, and Yao Yao. Direct2. 5: Diverse text-to-3d generation via multi-view 2.5 diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8744 8753, 2024. 3 [49] Artem Lukoianov, Haitz Saez de Ocariz Borde, Kristjan Greenewald, Vitor Guizilini, Timur Bagautdinov, Vincent Sitzmann, and Justin Solomon. Score distillation via reparametrized ddim. Advances in Neural Information Processing Systems, 37:2601126044, 2024. 3 [50] David McAllister, Songwei Ge, Jia-Bin Huang, David Jacobs, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Rethinking score distillation as bridge between image distributions. Advances in Neural Information Processing Systems, 37:3377933804, 2024. 3 [51] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, and Filippos Kokkinos. Im-3d: Iterative multiview diffusion and reconstruction for high-quality 3d generation. arXiv preprint arXiv:2402.08682, 2024. 3 [52] Marko Mihajlovic, Sergey Prokudin, Siyu Tang, Robert Maier, Federica Bogo, Tony Tung, and Edmond Boyer. Splatfields: Neural gaussian splats for sparse 3d and 4d reconIn European Conference on Computer Vision, struction. pages 313332. Springer, 2024. [53] Norman Muller, Katja Schwarz, Barbara Rossle, Lorenzo Porzi, Samuel Rota Bulo, Matthias Nießner, and Peter Kontschieder. Multidiff: Consistent novel view synthesis from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1025810268, 2024. 3 [54] Michael Niemeyer, Jonathan Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In CVPR, 2022. 2, 3 [55] Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, and Nima Khademi Kalantari. Coherentgs: Sparse novel view synthesis with coherent 3d gaussians. In European Conference on Computer Vision, pages 1937. Springer, 2024. 3 depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2828528295, 2024. 3 [57] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 3 [58] Hao Shi, Yu Li, Kailun Yang, Jiaming Zhang, Kunyu Peng, Alina Roitberg, Yaozu Ye, Huajian Ni, Kaiwei Wang, and Rainer Stiefelhagen. Fishdreamer: Towards fisheye semantic completion via unified image outpainting and segmentation. arXiv preprint arXiv:2303.13842, 2023. [59] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. 3 [60] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 3 [61] Meng-Li Shih, Ying-Huan Chen, Yu-Lun Liu, and Brian Curless. Prior-enhanced gaussian splatting for dynamic scene reconstruction from casual video. arXiv preprint arXiv:2512.11356, 2025. 3 [62] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. 3, 4, 6 [63] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, et al. The replica dataset: digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019. 2, 6, 7, 8, 14, 15, 17 [64] Chih-Hai Su, Chih-Yao Hu, Shr-Ruei Tsai, Jie-Ying Lee, Chin-Yang Lin, and Yu-Lun Liu. Boostmvsnerfs: Boosting mvs-based nerfs to generalizable view synthesis in largescale scenes. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 3 [65] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and Yasutaka Furukawa. Mvdiffusion: Enabling holistic multiview image generation with correspondence-aware diffusion. Advances in Neural Information Processing Systems, 2023. [66] Shengji Tang, Weicai Ye, Peng Ye, Weihao Lin, Yang Zhou, Tao Chen, and Wanli Ouyang. Hisplat: Hierarchical 3d gaussian splatting for generalizable sparse-view reconstruction. arXiv preprint arXiv:2410.06245, 2024. 2 [67] Shr-Ruei Tsai, Wei-Cheng Chang, Jie-Ying Lee, Chih-Hai Su, and Yu-Lun Liu. Lightsout: Diffusion-based outpainting In Proceedings of the for enhanced lens flare removal. IEEE/CVF International Conference on Computer Vision, pages 63536363, 2025. 3 [68] Tao Tu, Shun-Po Chuang, Yu-Lun Liu, Cheng Sun, Ke Zhang, Donna Roy, Cheng-Hao Kuo, and Min Sun. Imgeonet: Image-induced geometry-aware voxel representation for multi-view 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 69967007, 2023. 3 [56] Suraj Patni, Aradhye Agarwal, and Chetan Arora. Ecodepth: Effective conditioning of diffusion models for monocular [69] Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, and Juho Kannala. Dn-splatter: Depth and normal priors for gaussian splatting and meshing. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 24212431. IEEE, 2025. 3 [70] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2024. 3 [71] Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth ranking for few-shot novel view synthesis. In ICCV, 2023. 2, 3 [72] Ning-Hsu Albert Wang and Yu-Lun Liu. Depth anywhere: Enhancing 360 monocular depth estimation via perspective distillation and unlabeled data augmentation. Advances in Neural Information Processing Systems, 37:127739127764, 2024. 3 [73] Qian Wang, Weiqi Li, Chong Mou, Xinhua Cheng, and Jian Zhang. 360dvd: Controllable panorama video generation with 360-degree video diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69136923, 2024. 3 [74] Qisen Wang, Yifan Zhao, Jiawei Ma, and Jia Li. How to use diffusion priors under sparse views? Advances in Neural Information Processing Systems, 37:3039430424, 2024. 3 [75] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. 4, 6, 7, 14 [76] Yunsong Wang, Tianxin Huang, Hanlin Chen, and Gim Hee Lee. Freesplat: Generalizable 3d gaussian splatting towards free view synthesis of indoor scenes. Advances in Neural Information Processing Systems, 37:107326107349, 2024. [77] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibilIEEE Transactions on Image ity to structural similarity. Processing, 13(4):600612, 2004. 6 [78] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in neural information processing systems, 36:84068441, 2023. 3 [79] Zhen Wang, Qiangeng Xu, Feitong Tan, Menglei Chai, Shichen Liu, Rohit Pandey, Sean Fanello, Achuta Kadambi, and Yinda Zhang. Mvdd: Multi-view depth diffusion models. In European Conference on Computer Vision, pages 236253. Springer, 2024. 3 [80] Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Jie-Ying Lee, Bo-Hsu Ke, Chun-Wei Tuan Mu, Yi-Chuan Huang, Chin-Yang Lin, Min-Hung Chen, Yen-Yu Lin, et al. Aurafusion360: Augmented unseen region alignment for referencebased 360deg unbounded scene inpainting. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1636616376, 2025. 3 [81] Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, and Huan Ling. Difix3d+: Improving 3d reconstructions with single-step diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2602426035, 2025. 1, 2, 3, 6, [82] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. Advances in Neural Information Processing Systems, 37:125116125141, 2024. 3 [83] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. Reconfusion: 3d reconstruction with diffusion priors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2155121561, 2024. 2, 3 [84] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. Advances in Neural Information Processing Systems, 37: 121859121881, 2024. 3 [85] Sibo Wu, Congrong Xu, Binbin Huang, Andreas Geiger, and Anpei Chen. Genfusion: Closing the loop between reconstruction and generation via videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 60786088, 2025. 2, 3, 6, 15, 16 [86] Tianhao Wu, Chuanxia Zheng, and Tat-Jen Cham. Panodiffusion: 360-degree panorama outpainting via diffusion. arXiv preprint arXiv:2307.03177, 2023. 3 [87] Desai Xie, Jiahao Li, Hao Tan, Xin Sun, Zhixin Shu, Yi Zhou, Sai Bi, Soren Pirk, and Arie Kaufman. Carve3d: Improving multi-view reconstruction consistency for diffusion models with rl finetuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 63696379, 2024. [88] Chao Xu, Ang Li, Linghao Chen, Yulin Liu, Ruoxi Shi, Hao Su, and Minghua Liu. Sparp: Fast 3d object reconstruction and pose estimation from sparse views. In European Conference on Computer Vision, pages 143163. Springer, 2024. 3 [89] Chenfeng Xu, Huan Ling, Sanja Fidler, and Or Litany. 3difftection: 3d object detection with geometry-aware diffusion features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10617 10627, 2024. 3 [90] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Cameracontrollable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. 3 [91] Haiyang Xu, Yu Lei, Zeyuan Chen, Xiang Zhang, Yue Zhao, Yilin Wang, and Zhuowen Tu. Bayesian diffusion models for 3d shape reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1062810638, 2024. 3 [92] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 3 [93] Jiale Xu, Shenghua Gao, and Ying Shan. Freesplatter: Posefree gaussian splatting for sparse-view 3d reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2544225452, 2025. 2 [94] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. arXiv preprint arXiv:2311.09217, 2023. 3 [95] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, and Kai Zhang. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. In International Conference on Learning Representations, 2024. 4 [96] Yuxuan Xue, Xianghui Xie, Riccardo Marin, and Gerard Pons-Moll. Human-3diffusion: Realistic avatar creation via explicit 3d consistent diffusion models. Advances in Neural Information Processing Systems, 37:9960199645, 2024. 3 [97] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regularization. In CVPR, 2023. 2, 3 [98] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hongdong Li. Consistnet: Enforcing 3d consistency for multiview images diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 70797088, 2024. 3 [99] Xiaofeng Yang, Yiwen Chen, Cheng Chen, Chi Zhang, Yi Xu, Xulei Yang, Fayao Liu, and Guosheng Lin. Learn to optimize denoising scores: unified and improved diffusion prior for 3d generation. In European Conference on Computer Vision, pages 136152. Springer, 2024. 3 [100] Xiuyu Yang, Yunze Man, Junkun Chen, and Yu-Xiong Wang. Scenecraft: Layout-guided 3d scene generation. Advances in Neural Information Processing Systems, 37:8206082084, 2024. [101] Weicai Ye, Chenhao Ji, Zheng Chen, Junyao Gao, Xiaoshui Huang, Song-Hai Zhang, Wanli Ouyang, Tong He, Cairong Zhao, and Guofeng Zhang. Diffpano: Scalable and consistent text to panorama generation with spherical epipolaraware diffusion. Advances in Neural Information Processing Systems, 37:13041332, 2024. 3 [102] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1222, 2023. 2, 6, 7, 8, 14, 15 [103] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67966807, 2024. 3 [104] Hang Yu, Ruilin Li, Shaorong Xie, and Jiayan Qiu. ShadowIn Proceedings of the enlightened image outpainting. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 78507860, 2024. 3 [105] Hanyang Yu, Xiaoxiao Long, and Ping Tan. Lm-gaussian: Boost sparse-view 3d gaussian splatting with large model priors. arXiv preprint arXiv:2409.03456, 2024. 3 [106] Rui Yu, Jiachen Liu, Zihan Zhou, and Sharon Huang. Nerfenhanced outpainting for faithful field-of-view extrapolation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1682616833. IEEE, 2024. [107] Zhongrui Yu, Martina Megaro-Boldini, Robert Sumner, and Abdelaziz Djelouah. Unboxed: Geometrically and temporally consistent video outpainting. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 73097319, 2025. 3 [108] Xiaoding Yuan, Shitao Tang, Kejie Li, and Peng Wang. Camfreediff: Camera-free image to panorama generation with diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1640816417, 2025. 3 [109] Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. 4diffusion: Multi-view video diffusion model for 4d generation. Advances in Neural Information Processing Systems, 37:1527215295, 2024. 3 [110] Jiawei Zhang, Jiahe Li, Xiaohan Yu, Lei Huang, Lin Gu, Jin Zheng, and Xiao Bai. Cor-gs: sparse-view 3d gaussian splatting via co-regularization. In European Conference on Computer Vision, pages 335352. Springer, 2024. 2 [111] Jason Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. Cameras as rays: Pose estimation via ray diffusion. arXiv preprint arXiv:2402.14817, 2024. 3 [112] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018. 6 [113] Shaofeng Zhang, Jinfa Huang, Qiang Zhou, Zhibin Wang, Fan Wang, Jiebo Luo, and Junchi Yan. Continuous-multiple image outpainting in one-step via positional query and diffusion-based approach. arXiv preprint arXiv:2401.15652, 2024. [114] Yingji Zhong, Zhihao Li, Dave Zhenyu Chen, Lanqing Hong, and Dan Xu. Taming video diffusion prior with scenegrounding guidance for 3d gaussian splatting from sparse inputs. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 61336143, 2025. 1, 2, 3, 6, 14, 15, 16 [115] Yingji Zhong, Kaichen Zhou, Zhihao Li, Lanqing Hong, Zhenguo Li, and Dan Xu. Empowering sparse-input neural radiance fields with dual-level semantic guidance from dense novel views. arXiv preprint arXiv:2503.02230, 2025. 2, 6 [116] Jensen Zhou, Hang Gao, Vikram Voleti, Aaryaman Vasishta, Chun-Han Yao, Mark Boss, Philip Torr, Christian Rupprecht, and Varun Jampani. Stable virtual camera: Generative view synthesis with diffusion models. arXiv preprint arXiv:2503.14489, 2025. 14, 16, 17 [117] Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. Fsgs: Real-time few-shot view synthesis using gaussian splatting. In European conference on computer vision, pages 145163. Springer, 2024. 1, 2, 6, 15 13 A. Overview This supplementary material provides extended technical details, additional experiments, and complementary analysis for GaMO. Section presents the full implementation of our proposed Iterative Mask Scheduling (IMS), including blending strategy and scheduling behavior. Section reports additional quantitative comparisons on Replica [63] and ScanNet++ [102] under 3-, 6-, and 9-view settings, while Section provides further qualitative comparisons across the same input-view regimes. Section evaluates GaMO on Mip-NeRF 360 [2] scenes to demonstrate generalization to outdoor and unbounded environments. Section compares our geometry-aware outpainting design against two multiview diffusion baselines, Stable-Virtual Camera [116] and MVGenMaster [3], and includes reconstructed results after refinement. Section offers stage-wise runtime analysis measured on single NVIDIA RTX 4090 GPU, showing that GaMO reconstructs 6-view scene in under nine minutes. Finally, Section discusses failure cases and visualizations that illustrate remaining challenges such as heavy occlusions. B. Iterative Mask Scheduling Implementation To maximize the utilization of coarse geometry during outpainting while preserving the generative diversity of the diffusion model, we introduce an Iterative Mask Scheduling (IMS) strategy. This approach dynamically adjusts the mask region throughout the denoising process, enabling the model to freely hallucinate missing content in early timesteps while progressively aligning with the coarse initialization in later stages. Design Rationale. As demonstrated in the ablation studies in the main paper (Tab. 3), we found that applying mask latent blending at specific denoising steps yields significantly better results than continuous blending throughout the entire denoising process. Based on these findings, we strategically select three representative timesteps corresponding to the early, middle, and late stages of denoising to participate in the latent blending process. At each stage, we employ progressively shrinking mask sizes to control the degree of interference with the denoising process: larger masks in early stages allow more freedom for generation, while smaller masks in later stages enforce stronger alignment with coarse geometry. Implementation Details. As illustrated in Fig. 9, we generate three mask levels through morphological dilation: M(k) latent = Dilate(M base, kernel = 5, iterations = ), (10) where base denotes the downsampled base mask from the coarse geometry opacity map, aligned to the latent space resolution of 6448 via adaptive max pooling. The Dilate() operation applies iterative max pooling with 5 5 kernel 15 10 Figure 9. Iterative Mask Scheduling visualization. Top: coarse render and opacity mask derived from coarse 3D initialization. Bottom: progressive mask shrinking at three denoising steps (t = 35, 25, 15) with 2, 1, and 0 dilation iterations, respectively. to expand the masked region. During the denoising process from = 50 to = 0, we apply M(35) latent at = 25, and M(15) latent at = 15, as visualized in Fig. 9. This staged approach balances generative freedom with geometric consistency, as validated by our ablation experiments. latent at = 35, M(25) C. More Quantitative Comparison We provide additional quantitative results on Replica [63] and ScanNet++ [102] datasets with varying numbers of input views (3, 6, and 9 views), as shown in Tab. 5 and Tab. 6. We focus our comparison on 3DGS [28] and GuidedVD3DGS [114], competitive state-of-the-art diffusion-based method. Evaluation Protocol. For Replica, we follow the evaluation protocol from [114] for all three view settings. For ScanNet++, the 6-view setting follows [114], while the 3-view and 9-view settings use manually selected views to maximize spatial coverage. All methods use DUSt3R [75] for point cloud initialization. Results. Our method consistently outperforms baselines across most metrics and view settings. On Replica, we achieve the best SSIM and LPIPS scores across all view counts. On ScanNet++, we obtain superior performance across all metrics in all view settings. Notably, our method maintains competitive quality with GuidedVD-3DGS [114] while being significantly faster (approximately 6-9 minutes vs. 3+ hours). D. More Qualitative Comparison We provide additional qualitative results across 3-, 6-, and 9view settings on Replica [63] and ScanNet++ [102] datasets, as shown in Fig. 10. We compare against 3DGS [28], 14 Table 5. Quantitative comparison on Replica [63] with 3, 6, and 9 input views. Method Replica (3 views) Replica (6 views) Replica (9 views) PSNR SSIM LPIPS Time PSNR SSIM LPIPS Time PSNR SSIM LPIPS Time 3DGS [28] GuidedVD-3DGS [114] Ours 20.39 24.22 23. 0.818 0.845 0.855 0.154 0.155 0.122 1m28s 24.41 3h32m 25.67 25.84 6m43s 0.862 0.861 0.877 0.124 0.147 0.109 1m51s 26.09 3h38m 25.81 27.50 8m23s 0.890 0.862 0.900 0.100 0.144 0.095 1m53s 3h46m 9m12s Table 6. Quantitative comparison on ScanNet++ [102] with 3, 6, and 9 input views. Method ScanNet++ (3 views) ScanNet++ (6 views) ScanNet++ (9 views) PSNR SSIM LPIPS Time PSNR SSIM LPIPS Time PSNR SSIM LPIPS Time 3DGS [28] GuidedVD-3DGS [114] Ours 16.60 19.93 20.00 0.710 0.759 0. 0.313 0.297 0.268 1m32s 21.71 3h20m 22.98 23.41 6m21s 0.808 0.815 0.835 0.186 0.204 0.181 1m54s 24.55 3h27m 24.65 25.17 8m38s 0.845 0.843 0. 0.155 0.159 0.152 2m01s 3h54m 9m21s Figure 10. Qualitative comparison on Replica [63] and ScanNet++ [102] with 3, 6, and 9 sparse views. Our method produces better coverage, geometric consistency, and fewer artifacts compared to baselines. White boxes highlight challenging regions. Best viewed zoomed in. FSGS [117], InstantSplat [12], DiFix3D [81], GenFusion [85], and GuidedVD-3DGS [114] using the same baseline configurations as described in the main paper. As illustrated in Fig 10, even with extremely sparse inputs (3 views), our method produces reasonable content and geometry while maintaining consistency. Compared to baselines, our approach demonstrates better scene coverage with fewer missing regions (black holes), improved geometric consistency with reduced ghosting artifacts, and overall higher visual quality. These improvements are particularly evident in challenging regions highlighted by white boxes. 15 Figure 11. Comparison of outpainting using adapted multi-view diffusion models. Top: input views. Middle: outpainted views generated by adapted SEVA [116], MVGenMaster [3], and our GaMO. Bottom: novel views after 3DGS refinement using the generated outpainted views. Adapted multi-view diffusion models suffer from multi-view inconsistency, resulting in noisy reconstructions, while our method produces consistent outpainted views that improve reconstruction quality. E. Evaluation on Mip-NeRF 360 We evaluate the generalization ability of our method on the Mip-NeRF 360 [2] dataset, which contains nine large-scale scenes, including both outdoor and indoor environments with diverse camera trajectories. Following the evaluation protocol of GenFusion [85], we report the averaged performance across all nine scenes. All methods are initialized with DUSt3R for fair comparison, and we use the same reconstruction pipeline as described in the main paper. Results. As shown in Tab. 7, our method achieves the highest average performance across all metrics, outperforming 3DGS, GenFusion, and GuidedVD-3DGS in PSNR, SSIM, and LPIPS. Although GenFusion benefits from largescale diffusion priors and sometimes generates smooth structures, it often suffers from texture flattening and degraded geometric fidelity. Qualitative results in Fig. 12 further illustrate these differences. GuidedVD-3DGS frequently produces floating splats and large holes due to view-coverage gaps in 360-degree scenes. GenFusion can inpaint missing areas but tends to oversmooth background textures. In contrast, our method maintains background completeness and structural plausibility, preserves high-frequency details across both outdoor and indoor scenes, and remains consistent even for far-distance regions, demonstrating strong robustness to large-scale 360degree environments. F. Outpainting Comparison Using Multi-View"
        },
        {
            "title": "Diffusion Models",
            "content": "We compare our method against adapted multi-view diffusion models for outpainting. Specifically, we adapt SEVA [116] and MVGenMaster [3] by modifying the camTable 7. Quantitative comparison on Mip-NeRF 360 [2] (9 scenes). Results are averaged over all nine scenes."
        },
        {
            "title": "Method",
            "content": "PSNR SSIM LPIPS 3DGS [28] GenFusion [85] GuidedVD-3DGS [114] GaMO (Ours) 15.30 16.40 13. 16.80 0.342 0.384 0.273 0.393 0.459 0.497 0.640 0.436 Figure 12. Qualitative results on Mip-NeRF 360. 360-degree scenes pose significant challenges for GuidedVD-3DGS due to wide view coverage and large unobserved regions. In contrast, our method consistently produces more complete and geometrically coherent reconstructions across both outdoor and indoor scenes, demonstrating stronger robustness to diverse scene layouts and scales. era intrinsics to generate outpainted versions of the input views with extended FOV. These outpainted input views are 16 Table 8. Outpainting Comparison Using Multi-View Diffusion Models. 3D reconstruction quality on Replica [63] dataset (6 views) after refinement with outpainted views from different methods. PSNR LPIPS SSIM"
        },
        {
            "title": "Method",
            "content": "Figure 13. Failure cases in heavily occluded regions. Due to severe occlusions in the scene, certain regions are never observed across all input views. Both outpainting (ours) and novel view generation methods struggle to reconstruct these completely unobserved areas. Red boxes highlight the occluded regions where reconstruction fails. lenging for both geometry-aware outpainting and multi-view diffusion methods. Potential Solutions. promising direction to address viewpoint-specific occlusions is to generate outpainted views from alternative camera perspectives with geometry-aware mechanisms, such as birds-eye or top-down views with larger FOV. By generating content from drastically different viewing angles, these views could potentially observe regions that are occluded from the original camera poses, thereby providing complementary supervision for the occluded areas. 3DGS [28] SEVA [116] MVGenMaster [3] GaMO (Ours) 24.41 22.11 23.37 25. 0.862 0.799 0.820 0.877 0.124 0.247 0.175 0.109 Table 9. Runtime breakdown on Replica 6 / office 2. All timings are measured on single NVIDIA RTX 4090 GPU. Stage Time (s) Time (min) Coarse 3DGS init. & render Multi-view outpainting 3DGS refine (train + render) Total 118 93 280 1.97 1.55 4.67 8.18 then used to train 3DGS for improved novel view synthesis. As shown in Fig. 11, SEVA produces highly noisy novel views after 3DGS refinement due to severe multi-view inconsistency caused by lack of geometric constraints. While MVGenMaster incorporates additional geometric mechanisms (e.g., multi-view conditioning), it still suffers from inconsistency issues that introduce artifacts in the refined reconstruction. In contrast, our GaMO effectively addresses the multi-view inconsistency problem, providing consistent outpainted views across multiple viewpoints that successfully refine 3DGS quality without introducing additional noise or artifacts. Quantitative results are provided in Tab. 8. G. Runtime Analysis We report the end-to-end runtime of our pipeline on representative indoor scene (Replica 6, office 2) with 6 input views at 512384 resolution, evaluated on single NVIDIA RTX 4090 GPU. The pipeline consists of three main stages: coarse 3DGS reconstruction and rendering, multi-view diffusion outpainting, and the final 3DGS refinement stage that incorporates DUSt3R point cloud initialization and refined 3DGS training. detailed runtime breakdown is summarized in Tab. 9. H. Failure Cases While our method demonstrates strong performance across wide range of scenarios, it remains limited in scenes containing severe occlusions. This limitation is inherent to all methods that generate novel views for 3D reconstruction; current multi-view diffusion models face the same challenge, as even densely sampled novel viewpoints struggle to reconstruct regions that are severely occluded by obstacles. As shown in Fig. 13, such heavily occluded areas remain chal-"
        }
    ],
    "affiliations": []
}