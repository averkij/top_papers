{
    "paper_title": "Benchmarks Saturate When The Model Gets Smarter Than The Judge",
    "authors": [
        "Marthe Ballon",
        "Andres Algaba",
        "Brecht Verbeken",
        "Vincent Ginis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, a manually revised version of the Omni-MATH dataset comprising a clean, exact-answer subset ($n{=}4181$) and a tagged, non-standard subset ($n{=}247$). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring a proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing a more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in $96.4\\%$ of the judge disagreements, indicating its inability to differentiate between models' abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 7 2 ] . [ 1 2 3 5 9 1 . 1 0 6 2 : r a"
        },
        {
            "title": "BENCHMARKS SATURATE WHEN THE MODEL GETS SMARTER\nTHAN THE JUDGE",
            "content": "Marthe Ballon1,2, 0009-0000-4586-234X Andres Algaba1,2 0000-0002-0532-3066 Brecht Verbeken1,2 0000-0002-7506-3298 Vincent Ginis1,2,3 0000-0003-0063-9608 1Data Analytics Lab, Vrije Universiteit Brussel, Pleinlaan 5, 1050 Brussel, Belgium 2imec-SMIT, Vrije Universiteit Brussel, Pleinlaan 9, 1050 Brussels, Belgium 3School of Engineering and Applied Sciences, Harvard University, Cambridge, Massachusetts 02138, USA January 28,"
        },
        {
            "title": "ABSTRACT",
            "content": "Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, manually revised version of the Omni-MATH dataset comprising clean, exact-answer subset (n=4181) and tagged, non-standard subset (n=247). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in 96.4% of the judge disagreements, indicating its inability to differentiate between models abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance. Keywords benchmarks datasets large language models LLM-as-a-judge LLM evaluation"
        },
        {
            "title": "Introduction",
            "content": "Evaluating Large Language Models (LLMs) accurately is becoming increasingly challenging as benchmarks shift toward open-ended formats and more difficult tasks, such as Olympiad-level mathematics [1, 2]. Open-response formats better probe deeper reasoning skills and avoid some inherent biases of fixed-answer formats [3]. However, they introduce additional complexities like answer extraction and equivalence judgments, creating new sources of errors [4, 5, 6, 7]. In this paper, we specifically examine two major sources of error [8] that compromise evaluation reliability in current LLM benchmarks. Dataset-induced errors, including ambiguous problem statements and unsolvable items, are widespread issue that prevents models from becoming fully reliable for the task at hand [9]. These errors often persist even in widely adopted benchmarks: for example, audits of MMLU report nontrivial percentage of item/ground-truth errors [10], and analyses of HellaSwag identify substantial proportion of problematic instances [11]. Judge-induced errors primarily arise when automated judges must extract and assess equivalence between free-form answers and reference solutions, common requirement in open-response evaluation pipelines [12]. LLM-based judges can exhibit systematic biases, prompt Corresponding author: marthe.ballon@vub.be 1 January 28, 2026 Figure 1: Overview of the cleaning process of the Omni-MATH dataset [1]. First, we check the compilability of the 4,428 problem statements in LaTeX and convert them to valid LaTeX code with python. Next, PhD-level mathematician manually went through the compiled pdf files twice, checking for the solvability and verifiability of each problem. In this process, missing information was added when available through browsing manually or through GPT-5.1, and images where added to the data folder. Furthermore, tag was added to each problem containing an image, proof or estimation. Degenerate problems received the should delete tag. We refer to the resulting dataset as Omni-MATH-2, which contains the same number of entries as the original Omni-MATH dataset (647 edited problems (14.6%), 247 tagged as non-standard (5.6%), see Table 1). The Omni-MATH-2-Filtered dataset (n=4,181) is the subset of cleaned questions, excluding the tagged ones. This makes it suitable for exact-answer judges. sensitivity, and inconsistencies that influence final outcomes [13, 14]. Although methods such as judge ensembles or committees have been proposed to improve robustness [15, 16, 17, 18], judge reliability remains largely task-dependent, which does not necessarily guarantee universally dependable improvements [19, 20]. In mathematical evaluation, scoring usually depends entirely on the correctness of the final answer, causing datasetand judge-induced errors to rapidly become bottleneck, particularly when the accuracy of the models approaches saturation. Standard math benchmarks such as GSM8K [21] and MATH [22] are already considered saturated, which prompted the development of more challenging datasets like FrontierMath [23] and Humanitys Last Exam [3]. Omni-MATH [1] is widely-used math benchmark consisting of 4,428 Olympiad-level problems across numerous subdomains and difficulty levels, on which current state-of-the-art models achieve around 85% (see Table A1). Therefore, Omni-MATH is ideal to investigate how residual errors are partitioned between dataset-induced issues, real model mistakes and judge-induced failures. Prior cleaning largely focused on formatting-level issues [24], hereby overlooking content-level solvability and verifiability constraints. To address this interaction, we introduce Omni-MATH-2 (see Figure 1), manually revised version of Omni-MATH [1], preserving the original dataset size (n=4,428) while significantly improving LaTeX compilability, interpretability, and suitability for automated judging. In total, 647 problems were edited (14.6%) and 247 were tagged as non-standard (5.6%). We release multiple evaluation-ready subsets, notably Omni-MATH-2-Filtered (n=4,181), from which we exclude the tagged non-standard questions to ensure suitability for exact-answer judging. Using Omni-MATH-2-Filtered, we benchmark five state-of-the-art models and explicitly measure judge-induced noise by comparing evaluations from two judges: Omni-Judge [1] and GPT-5 mini. Our results demonstrate that judge choice significantly alters both absolute accuracy and model rankings (Figure 3, Table A1). targeted human audit reveals Omni-Judge errors in 96.4% of the judge disagreements on clean questions, confirming that judge competence can limit evaluation reliability well before model accuracy saturates. Furthermore, we found that judge disagreement increases as problems become more difficult, highlighting the importance of capable judges for current and future benchmarks. On the subset of tagged problems, both judges did not pick up on evaluation incompatibility, counting both strong estimates and model abstentions, i.e. cases where models correctly stated they did not have sufficient information, as incorrect. In summary, as models near saturation, benchmark outcomes are increasingly shaped by the evaluation pipeline itself rather than by true differences in model capability. Practically, this motivates treating benchmarks explicitly as triplets (dataset, model, judge), and additionally, investing in dataset audits, rigorous judge calibration via ensembles [16, 18], and statistically robust uncertainty reporting [25, 26]. Note that statistical fragilityuncertainty in estimates, prompt sensitivity, and dataset characteristicscan also inflate model differences [27, 25, 28, 29], an issue we do not fully address here. II January 28, Figure 2: Example of the evaluation process for question labelled as image. The original problem in Omni-MATH misses the corresponding image, rendering the problem unsolvable. The model to be evaluated, GPT-5, correctly identifies that there is missing information, but Omni-Judge counts this as an incorrect answer."
        },
        {
            "title": "2 Results",
            "content": "2.1 Dataset-induced errors propagate through the evaluation pipeline Dataset errors include not only degenerate questions, but also questions with missing images and multiple-choice options, and questions that ask for proof or estimation which are subsequently verified against an exact reference answer. These errors influence every step in the LLM evaluation pipeline. In the cleaning process (see Figure 1), 14.6% of all problems statements were edited to some extent, consisting of converting to valid LaTeX code, adding missing information, and tagging problems with the labels image, proof, estimation and should delete when relevant. The set of tagged problems consists out of 247 problems (5.6% of the entire dataset) that are not solvable with the given data or not verifiable with Omni-Judge (or any other judge that assesses equivalence against reference answer). We discuss an example of each category, demonstrating how these errors can propagate through the evaluation pipeline. Fix Edited Tagged Image Proof Estimation Should delete Nr of problems 647 61 115 54 25 Image There are 61 problems in Omni-MATH that contain an image in the mathematical olympiad they were crawled from. Some problems had code to generate an image in their statement but for most problems it was absent. Figure 2 shows an example of problem statement that refers to an unattached image and relies on it to be solvable. Although the model correctly identifies that there is insufficient information to solve the problem, the judge marks this as incorrect. Table 1: Overview of Omni-MATH revisions resulting from the cleaning process in Figure 1. Note that one problem can have multiple tags. Proof large part of the tagged questions consists of problems that ask to prove given statement. The reference answer, however, does not contain the proof but rather short final answer like \"Yes, proven\". There is usually sketch of the proof available in the metadata field \"solution\" but this is, in general, not used for model evaluations. In Figure A1, GPT-5 correctly states that the claim is true, and provides corresponding proof. The judge evaluates this answer as incorrect because the final answer does not match the reference answer exactly, and does not take the proof into account (most judges are prompted to only look at the final answer). Estimation Omni-MATH also contains 54 problems that ask to estimate certain number, quantity, series sum, ... The grade of the student is then function of their estimate and the exact final answer. As Omni-Judge (like most judges) is prompted to assess equivalence with the reference answer, the evaluation of estimates by LLMs often goes 3 January 28, 2026 Figure 3: The judge-induced difference in accuracy on Omni-MATH-2-Filtered is not uniformly distributed across disciplines, difficulty tiers or models, indicating structural evaluation noise rather than i.i.d. label noise. Evaluating five state-of-the-art models on Omni-MATH-2-FilteredClaude Sonnet 4.5, DeepSeek v3.2, Gemini 3 Pro, GPT-5 and Kimi K2 Thinkingproduced different ranking of their mathematical abilities when evaluated with GPT-5 Mini rather than with Omni-Judge, which calls into question the interpretability of the inter-model differences. Furthermore, the right-hand panels show that, as questions become more difficult, judge disagreement increases, indicating that the judges conclusion is more important for challenging problems. The difference between judges answers is also modeland domain-dependent, with some disciplines and models producing larger differences than others (e.g. Claude and Deepseek, Calculus domain). For numerical performance scores with Bayesian confidence intervals consult Table A1. wrong. In Figure A2, GPT-5 provides an estimate that should, according to the scoring rule in the problem statement, earn 18.44 out of the 20 points. However, the judge evaluates this answer as incorrect. Should delete The fourth category of tagged problems are the degenerate problems, i.e. problems where the solution is in the problems statement, empty problems, duplicates, ... The problem in Figure A3 asks to compute the smallest positive integer that does not appear in any problem statement on any round at HMMT November 2023. This is question that anyone who did not have access to the question sheet could impossibly know. However, GPT-5 still gives the correct answer and the judge also marks this as correct. All four examples show that the incorrect/correct answers do not necessarily reflect model abilities. Note that, in this study, we did not account for wrong reference answers, which could add another unknown discrepancy to the model performance scores. 2.2 Judge-induced noise is not uniform across models, domains and difficulty tiers The previous section demonstrated that judges are sensitive to quality, type and format of problem. Therefore, we will not only evaluate language models on the cleaned Omni-MATH dataset, but also the judges. We ask five state-of-the-art models, Claude Sonnet 4.5, DeepSeek v3.2, Gemini 3 Pro, GPT-5 and Kimi K2 thinking, to answer the cleaned Omni-MATH questions, excluding the tagged ones. We will refer to this subset as Omni-MATH-2-Filtered (n=4,181). Then, we evaluated their answers with both Omni-Judge and GPT-5 mini, using similar prompt. Consult Section for details on the evaluation process. The evaluation of the mathematics abilities of the five models differs for Omni-Judge and GPT-5 mini, as Gemini 3 Pro jumps to first place (see left panel of Figure 3). The performance scores of Claude Sonnet 4.5 and DeepSeek v3.2 also differ substantially between the two judges, even when taking into account 95% confidence intervals (see Table A1). For GPT-5 and Kimi K2 Thinking, the difference in performance between the two judges is negligible. The inter-judge noise is not uniformly distributed across mathematical domains and difficulty tiers (Tier 4 consists of the hardest problems), which where extracted from the original Omni-MATH metadata. Omni-Judge and GPT-5 mini differ the most when it comes to evaluating Calculus (n=203) problems, followed by Algebra (n=1,876) problems (see 4 January 28, 2026 Figure 4: Example of judge disagreement between Omni-Judge and GPT-5 mini. Here, it is not straightforward to see whether the models answer is equivalent to the reference answer. Two expert mathematicians, with the help of an LLM council, building on Claude Opus 4.5, DeepSeek v3.2 speciale, DeepSeek v3.2, GPT-5, and Gemini 3 pro, verified that the models answer is in fact correct and equivalent to the reference answer. GPT-5 mini thus correctly assesses equivalence. left heatmap of Figure 3). For all models, the inter-judge difference is the largest for Tier 4 problems, indicating that as questions increase in difficulty, the conclusion of the judge becomes more important (see right heatmap of Figure 3). 2.3 Benchmark saturation is not only model-dependent, but also judge-dependent To investigate the reason for the inter-judge differences observed in Figure 3, two PhD-level mathematicians annotated the disagreements between Omni-Judge and GPT-5 mini by evaluating which judge was wrong in terms of mathematical correctness (ground-truth target). We sampled 100 of the 338 disagreements on model answers by GPT-5. Next, we categorised the reason for the judge being wrong into five categories: failed to assess equivalence (the judge fails to see that the final model answer is equivalent with the reference answer, or wrongly states that the final answer is equivalent to the reference answer), didnt follow instructions (the judge is either too obedient or disobedient with respect to its prompt, see Section A), dataset error (in grading the judges, we discovered several wrong reference answers and few issues with problem statements that were missed in the initial cleaning process), wrong extraction (judge fails to extract the final answer correctly) and unclear (it is not clear why the model fails). Excluding the datasets error cases, Omni-Judge was wrong in 96.4% of the sampled disagreements (see Table A2), primarily because it failed to assess equivalence. This shows that the judge-difference is not just noisy, but that Omni-Judge is fundamentally miscalibrated, and as result not able to differentiate model capabilities for this dataset. We present three examples of the annotation process to illustrate how judges can differ in opinion. 5 January 28, 2026 Easy equivalence In Figure A4, the models final answer differs from the reference answer only in the simplification of fraction. Omni-Judge fails to assess the equivalence between the two fractions and incorrectly labels the models answer as wrong. GPT-5 mini correctly infers that both fractions are equal. Hard equivalence In Figure 4, it is not straightforward to see whether the models answer is equivalent to the reference answer. Two expert mathematicians together with the help of an LLM council, building on Claude Opus 4.5, DeepSeek v3.2 speciale, DeepSeek v3.2, GPT-5, and Gemini 3 pro, proved that n(n+1) is in fact equal to 3 (cid:80)n i=1 min(n + 1 i, 2i 1). GPT-5 mini thus correctly judges the models answer as correct. In 14 out of 100 sampled disagreements, we discovered that there was an error in the Incomplete reference answer dataset (11 wrong/incomplete reference answers and 3 ill-posed problem statements, whose ambiguity only became clear when trying to solve the problem). In Figure A5, the model gives more general expression for all the pairs of positive integers (m, n) that satisfy mn 1 m2 + n2 than the reference answer. The reference answer states (2, 1), (3, 1), (1, 2), (1, 3), while GPT-5s final answer is all (m, n) Z>0 with m2 + n2 = 5(mn 1). First of all, note that for example the pair (9, 2) also satisfies the condition in the problem statement, indicating that the reference answer is incomplete. Then, we verified together with an LLM council, that the models answer is in fact the correct and complete answer to the problem. As result, GPT-5 mini wrongly grades the models answer by following its instructions to always regard the reference answer as correct. Omni-Judge does regard the models answer as correct, although for the wrong reasons. GPT-5 mini is also the stronger judge on the subset of tagged questions. To verify how the judges operate in flawed evaluation pipelines, we also annotated the disagreements between Omni-Judge and GPT-5 mini on Omni-MATH-2Tagged across the five models, resulting in 176 disagreements in total. Here, Omni-Judge was wrong in 64, 8% of the disagreements and GPT-5 mini in 6.8%. The remaining problems were too ambiguous to judge or did not contain reference answer. 2.4 Interaction between dataset and judge errors explains saturation We analyze judge behavior on Omni-MATH-2-Tagged. Since proofs cannot yet be reliably verified in our pipeline, we focus on missing-image and estimation problems. We select GPT-5 as the model to be evaluated by Omni-Judge and GPT-5 mini. Out of 61 problems that contain an image in their original problem statement, GPT-5 states for 28 out of 61 problems that there is not enough information to solve them, and asks for the missing diagram, figure, table etc (E.g. Figure 2). Manual inspection confirms that the information required to determine the answer is indeed not present in the text. However, GPT-5 mini judges this abstention as incorrect in all cases and Omni-Judge in 27 out of the 28 cases. In the remaining missing-image problems, the model makes extra assumptions that allows it to solve the problem, or the figure does not contain critical problem information. For the subset of estimation problems, we evaluate the estimation provided by GPT-5 using the scoring rule stated in the problem statement (E.g Figure A1). For small number of problems, the scoring rule is stated in the written out solution. Figure 5 shows that both judges frequently label strong estimates as incorrect without using the scoring rule present in the problem statement. Both inspections show that current judges do not pick up on ill-posed/incompatible problems, causing them to persist as model abilities grow. This implies that benchmark saturation below 100% is caused not only by dataset errors, but also by judge behaviour. Consequently, saturation can occur at even lower accuracy levels than expected."
        },
        {
            "title": "3 Discussion",
            "content": "Figure 5: On the subset of estimation problems in Omni-MATH-2, OmniJudge and GPT-5 mini evaluate substantial portion of strong estimates as incorrect. We compute the contest score by using the scoring rule present in the problem statement or solution. Our work shows that, when model accuracy increases to the point where evaluation becomes the bottleneck, benchmark saturation is no longer primarily property of model capability, but rather of the triplet (dataset, model, judge). The Omni-MATH dataset was designed specifically to benchmark the mathematical reasoning skills of LLMs. It provides large set of Olympiad-level math problems and uses an LLM verifier (Omni-Judge), which is trained to evaluate open-ended answers based on the problem statement and the reference answer. However, upon auditing both the dataset 6 January 28, 2026 and the judge, we found that nontrivial fraction of problems were unsolvable with the given data or structurally incompatible with exact-answer verification. Even on the cleaned exact-answer subset, judge competence was found to dominate the measured performance differences between frontier models. Together, these effects make benchmark saturation an interaction phenomenon that can occur for lower accuracies than expected. Omni-MATH-2 demonstrates that dataset-induced errors are not merely cosmetic (e.g. LaTeX issues or clutter) but propagate throughout the entire evaluation pipeline. Our cleaning process (see Figure 1) identified substantial amount of problems14.6% was edited in totalwith missing images, missing multiple-choice options, requests for estimations or proofs while verifying against an exact reference answer etc. This indicates that, as models improve, this kind of intensive auditing work will determine whether benchmark measures true competence or mostly pipeline idiosyncrasies. In some cases, like Figure A3, the model answered problem correctly that it could impossibly know without being at that specific Olympiad, implying risk of data leakage for Omni-MATH. We observe that judge disagreement on the cleaned exact-answer subset (n=4,181) increases with problem difficulty for all evaluated models. This suggests an emerging evaluator gap: as judge competence is not reliably above that of the models being evaluated, this has consequences for the interpretability of LLM benchmarks. In this setting, Omni-Judge proved to be wrong in 96.4% of the cases, rendering it not able to differentiate model abilities. However, for the missing image and estimation problems, we found that both judges, Omni-Judge and GPT-5 mini, did not pick up on evaluation incompatibility, counting model abstentions and good estimates as incorrect. Our results show that judge quality is increasingly important in this era of rapid benchmark saturation on challenging tasks, making it necessary to maintain an evaluator margin or add redundancy when it cannot be guaranteed. We therefore recommend evaluation designs that allow non-binary outcomes (e.g., partial credit, uncertainty, abstention), multi-judge frameworks on contested items [16, 18], and we explicitly encourage people to look at their benchmark data as systematic checks did not catch most failure modes presented in this paper. Limitations While Omni-MATH-2 reduces several high-impact failure modes in Omni-MATH and makes judge limitations more visible, our study has some important constraints. First, we did not revise the solutions and reference answers of Omni-MATH explicitly as this would require ground truth information that is not available in some cases, e.g. proofs or Tier 4 difficulty items. We did report the incomplete reference answers that became visible through annotating the judge disagreements in Section 2.3. Secondly, we focus on two judges under specific prompts and settings. Our analyses therefore cannot fully characterize the space of judge behaviors, nor can they claim GPT-5 mini is universally correct. 7 January 28,"
        },
        {
            "title": "Acknowledgements",
            "content": "We thank the authors and contributors of the original Omni-MATH dataset [1]. Their release of the benchmark and verifier Omni-Judge enabled the analyses in this paper. This research was supported by funding from the Flemish Government under the Onderzoeksprogramma Artificiële Intelligentie (AI) Vlaanderen program. Andres Algaba acknowledges support from the Francqui Foundation (Belgium) through Francqui Start-Up Grant and fellowship from the Research Foundation Flanders under Grant No.1286924N. Vincent Ginis acknowledges support from Research Foundation Flanders under Grant No.G032822N and G0K9322N."
        },
        {
            "title": "Author contributions",
            "content": "VG and MB were responsible for the main idea of the study. BV and MB performed the annotations. MB revised the Omni-MATH dataset, conducted the analyses and made the figures. AA and MB drafted the manuscript. All authors collaboratively revised the manuscript and provided critical feedback."
        },
        {
            "title": "Dataset and code availability",
            "content": "Omni-MATH-2 comprising subset of cleaned, exact-answer problems and set of tagged, non-standard problems, is publicly available on Hugging Face (https://huggingface.co/datasets/martheballon/Omni-MATH-2). data other All (https://doi.org/10.5281/zenodo.18380309) Benchmarks-saturate-when-the-model-gets-smarter-than-the-judge). our and GitHub necessary replicate study code and to are publicly at Zenodo (https://github.com/MartheBallon/ available"
        },
        {
            "title": "References",
            "content": "[1] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-MATH: Universal Olympiad Level Mathematic Benchmark For Large Language Models. arXiv preprint arXiv:2410.07985, 2024. [2] Aidar Myrzakhan, Sondos Mahmoud Bsharat, and Zhiqiang Shen. Open-LLM-Leaderboard: From Multi-Choice to Open-Style Questions for LLMs Evaluation, Benchmark, and Arena. arXiv preprint arXiv:2406.07545, 2024. [3] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys Last Exam. arXiv preprint arXiv:2501.14249, 2025. [4] Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, et al. Lessons from the Trenches on Reproducible Evaluation of Language Models. arXiv preprint arXiv:2405.14782, 2024. [5] Anka Reuel, Amelia Hardy, Chandler Smith, Max Lamparth, Malcolm Hardy, and Mykel Kochenderfer. BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices. Advances in Neural Information Processing Systems, 37:2176321813, 2024. [6] Qingchen Yu, Zifan Zheng, Shichao Song, Zhiyu Li, Feiyu Xiong, Bo Tang, and Ding Chen. xFinder: Large Language Models as Automated Evaluators for Reliable Evaluation. arXiv preprint arXiv:2405.11874, 2024. [7] Ine Gevers, Victor De Marez, Jens Van Nooten, Jens Lemmens, Andriy Kosar, Ehsan Lotfi, Nikolay Banar, Pieter Fivez, Luna De Bruyne, and Walter Daelemans. In Benchmarks We Trust ... Or Not? In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng, editors, Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2366223676, Suzhou, China, November 2025. Association for Computational Linguistics. [8] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. Survey on Evaluation of Large Language Models. ACM transactions on intelligent systems and technology, 15(3):145, 2024. [9] Joshua Vendrow, Edward Vendrow, Sara Beery, and Aleksander Madry. Do Large Language Model Benchmarks Test Reliability? arXiv preprint arXiv:2502.03461, 2025. [10] Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, et al. Are We Done with MMLU? In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the 8 January 28, Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 50695096, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. [11] Edwin Chen. HellaSwag or HellaBad? 36% of this popular LLM benchmark contains errors. https://surgehq. ai/blog/hellaswag-or-hellabad-36-of-this-popular-llm-benchmark-contains-errors, 2022. Accessed on 13/11/2025. [12] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Advances in neural information processing systems, 36:4659546623, 2023. [13] Kayla Schroeder and Zach Wood-Doughty. Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge. arXiv preprint arXiv:2412.12509, 2024. [14] Joachim Baumann, Paul Röttger, Aleksandra Urman, Albert Wendsjö, Flor Miriam Plaza-del Arco, Johannes Gruber, and Dirk Hovy. Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation. arXiv preprint arXiv:2509.08825, 2025. [15] Nitay Calderon, Roi Reichart, and Rotem Dror. The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs. arXiv preprint arXiv:2501.10970, 2025. [16] Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. Replacing Judges with Juries: Evaluating LLM Generations with Panel of Diverse Models. arXiv preprint arXiv:2404.18796, 2024. [17] Hossein Rahmani, Emine Yilmaz, Nick Craswell, and Bhaskar Mitra. JudgeBlender: Ensembling Judgments for Automatic Relevance Assessment. arXiv preprint arXiv:2412.13268, 2024. [18] Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Weiwen Xu, Deli Zhao, and Lidong Bing. Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee Discussions. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 44404463, 2025. [19] Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu. LLMs-as-Judges: Comprehensive Survey on LLM-based Evaluation Methods. arXiv preprint arXiv:2412.05579, 2024. [20] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. From Generation to Judgment: Opportunities and Challenges of LLM-as-a-Judge. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 27572791, 2025. [21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168, 2021. [22] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. arXiv preprint arXiv:2103.03874, 2021. [23] Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, et al. FrontierMath: Benchmark for Evaluating Advanced Mathematical Reasoning in AI. arXiv preprint arXiv:2411.04872, 2024. [24] LLMTeamAkiyama. cleaned_kbsdjames_omni-math: dataset card and repository on the Hugging Face Hub, 2025. Accessed: 2025-11-12; Dataset generated: 2025-07-29. [25] Evan Miller. Adding Error Bars to Evals: Statistical Approach to Language Model Evaluations. arXiv preprint arXiv:2411.00640, 2024. [26] Sam Bowyer, Laurence Aitchison, and Desi Ivanova. Position: Dont use the CLT in LLM Evals with fewer than few hundred datapoints. arXiv preprint arXiv:2503.01747, 2025. [27] Desi R. Ivanova. Towards more rigorous evaluations of language models. https://open.substack.com/pub/ probapproxincorrect/p/towards-more-rigorous-evaluations?, 2024. Accessed on 12/11/2025. [28] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying Language Models Sensitivity to Spurious Features in Prompt Design or: How learned to start worrying about prompt formatting. arXiv preprint arXiv:2310.11324, 2023. [29] Riccardo Lunardi, Vincenzo Della Mea, Stefano Mizzaro, and Kevin Roitero. On Robustness and Reliability of Benchmark-Based Evaluation of LLMs. arXiv preprint arXiv:2509.04013, 2025. 9 January 28,"
        },
        {
            "title": "A Methods",
            "content": "A.1 The original Omni-MATH dataset The Omni-MATH benchmark [1] contains 4,428 Olympiad-level math problems crawled from contest pages, AoPS Wiki and the AoPS forum. The problems extracted from the AoPS forum were reformatted with GPT-4o. Each entry in the dataset consists of problem, an exact answer, and written out solution together with the following metadata fields: domain, difficulty, and source (see Figures A1 to A3). To create Figure 3, we only use the primary domains, and we joined the Calculus and Pre Calculus domain to obtain sufficient number of data points. Furthermore, we divided the data into four difficulty tiers based on the quartiles of the difficulty distribution, with Tier 1 consisting of the easiest problems and Tier 4 of the hardest. A.2 Omni-MATH-2 Omni-MATH-2 is an extensively revised version of the original Omni-MATH dataset. When using the Omni-MATH dataset, we discovered that even high-quality, human verified datasets can have problem errors propagating through the evaluation pipeline. Apart from obvious degenerate questions and missing images, the cleaning process also made more subtle problem errors visible, such as missing multiple choice options, asking for proof/estimation while using an exact-answer verifier, latex clutter etc. The whole cleaning process was performed by PhD-level mathematician. Cleaning the problem statements of Omni-MATH The cleaning process, described in Figure 1, had three main aims: ensuring compilability in LaTeX, ensuring solvability given the data in the problem statement, and ensuring verifiability with Omni-Jugde or another final answer-based verifier. First, we checked compilability in LaTeX by attempting to compile the problem statements using XeLaTeX in Texmaker (the code also works with pdfLaTeX). We then converted the problem statements to valid LaTeX code using python. It was at this stage that the first cases of missing images became apparent, as some questions contained code to generate an image or to load an image that was not attached (Figure 2). We searched the internet for the original Olympiad question, added the missing image to the data folder, and edited the code to load the image into the problem statement when necessary. If we could not find the original problem by browsing, we used GPT-5.1 via the chat interface. Once compilation was successful, we read the PDFs of the problem statements to check whether they contained \"a question\" and whether they had all the necessary information to be solved. For example, sometimes parts a, and of problem were spread out over three dataset entries, such that parts and did not have sufficient background information. Adding tags indicating solvability and verifiability We also noticed that substantial proportion of problems asks to prove given claim (Figure A1) or to estimate quantity, number, series sum etc (Figure A2). This is problem as the verifier proposed by [1] only compares the models final answer to the reference answer, and the reference answer contains the exact final answer only (E.g Proven., 3.1415875473). Omni-MATH does contain field \"solution\", consisting of written out solution or sketch of the proof, but Omni-Judge is not trained to evaluate model/student reasoning. We indicate this failure mode by adding the tags proof and estimation. Finally, we also add tag to all problems containing an image because they require multimodal language models to be solved and evaluated. The small percentage of degenerate problems (empty, contains the solution, not question, duplicate) are tagged should delete. In this manner, one can exclude all tagged problems for straightforward benchmarking. Omni-MATH-2-Filtered is then the subset of all cleaned entries that are solvable and verifiable with Omni-Judge. Omni-MATH-2-Tagged is the subset of proof, estimation, image, should delete questions, which should not be taken into account for benchmarking LLMs with the current evaluation strategies. A.3 Models To solve Omni-MATH-2-Filtered and Omni-MATH-2-Tagged, we use the following state-of-the art language models: Claude Sonnet 4.5 (claude-sonnet-4-5-20250929, 64,000 output tokens, 25, 000 thinking budget, Claude Batch API) DeepSeek V3.2 (deepseek-reasoner, 64,000 output tokens (shared), implicit thinking budget, DeepSeek API) Gemini 3 Pro (gemini-3-pro-preview, 64,000 output tokens, high thinking level, no thinking budget, Gemini Batch API) GPT-5 (gpt-5-2025-08-07, medium reasoning effort, no token limit, OpenAI Batch API) 10 January 28, 2026 Kimi K2 Thinking (kimi-k2-thinking, 128,000 output tokens via max_tokens, 256,000 context window, Moonshot API) We use the same prompt for each model: Instructions: Solve the following problem. Enclose the final answer in boxed{{}} environment. Input: {problem} A.4 Judges To correct the responses of the five state-of-the art language models on the filtered Omni-MATH-2 dataset, we employ Omni-Judge (KbsdJames/Omni-Judge) and OpenAIs GPT-5 mini (gpt-5-mini-2025-08-07). Omni-Judge Omni-Judge is an efficient and low cost open-source math-evaluation model developed by the authors of Omni-MATH [1]. The model is trained to assess the correctness of an answer generated by an LLM, given the problem and reference answer (see Figures A1 to A3). We make requests to the chat completions endpoint of the kbsdjames.omni-judge API by running the model in LM Studio. We use the same few-shot prompt as in [1] and set the max_new_tokens parameter to 300, and context length to 4,096. GPT-5 mini GPT-5 mini is faster, more cost-efficient version of GPT-5, designed for well-defined tasks and precise prompts. In Humanities Last Exam [3], they use o3-mini for evaluating language models on frontier math, physics and other science problems. We adopt their evaluation prompt for GPT-5 mini, where we excluded the instruction to give confidence score. We ask the model to provide its answer in json schema with fields extracted final answer, reasoning, and correct. 11 January 28, 2026 Figure A1: Example of the evaluation process for question labelled as proof. 12 January 28, 2026 Figure A2: Example of the evaluation process for question labelled as estimation. 13 January 28, 2026 Figure A3: Example of the evaluation process for question labelled as should delete. 14 January 28, 2026 Figure A4: Example of judge disagreement between Omni-Judge and GPT-5 mini. Omni-Judge fails to assess that the fraction in the reference answer is equal to the fraction in the models final answer. 15 January 28, 2026 Figure A5: Example of judge disagreement between Omni-Judge and GPT-5 mini. The reference answer is incomplete, causing GPT-5 mini to judge the models answer as wrong, while it is actually the correct and complete answer. 16 January 28, 2026 GPT-5 mini Omni-Judge Claude Sonnet 4.5 DeepSeek v3. 79.29 [78.03,80.49] 74.93 [73.65,76.28] 82.95 [81.78,84.06] 77.45 [76.16,78.69] Gemini 3 Pro (preview) 89.93 [88.98,90.81] 83.43 [82.29,84.53] GPT-5 84.53 [83.40,85.59] 83.47 [82.35,84.60] Kimi K2 Thinking 86.87 [85.81, 87.86] 86.99[86.00,88.03] Table A1: Accuracy of Claude 4.5 Sonnet, DeepSeek v3.2, Gemini 3, GPT-5 and Kimi K2 Thinking on Omni-MATH-2Filtered with Bayesian iid confidence intervals [26]. Total wrong Failed to assess equivalence Didnt follow instructions Dataset error Wrong extraction Unclear Judge disagreements (n=100): Omni-Judge GPT-5 mini 86 12 1 75 1 10 1 9 2 0 0 1 Table A2: Excluding the dataset errors, Omni-Judge is wrong in 96.4% of the subsampled judge disagreements with GPT-5 mini for Omni-MATH-2-Filtered. This is mainly because Omni-Judge is not able to assess equivalence between the models final answer and the reference answer."
        }
    ],
    "affiliations": [
        "Data Analytics Lab, Vrije Universiteit Brussel",
        "School of Engineering and Applied Sciences, Harvard University",
        "imec-SMIT, Vrije Universiteit Brussel"
    ]
}