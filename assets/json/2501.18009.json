{
    "paper_title": "Large Language Models Think Too Fast To Explore Effectively",
    "authors": [
        "Lan Pan",
        "Hanbo Xie",
        "Robert C. Wilson"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models have emerged many intellectual capacities. While numerous benchmarks assess their intelligence, limited attention has been given to their ability to explore, an essential capacity for discovering new information and adapting to novel environments in both natural and artificial systems. The extent to which LLMs can effectively explore, particularly in open-ended tasks, remains unclear. This study investigates whether LLMs can surpass humans in exploration during an open-ended task, using Little Alchemy 2 as a paradigm, where agents combine elements to discover new ones. Results show most LLMs underperform compared to humans, except for the o1 model, with those traditional LLMs relying primarily on uncertainty driven strategies, unlike humans who balance uncertainty and empowerment. Representational analysis of the models with Sparse Autoencoders revealed that uncertainty and choices are represented at earlier transformer blocks, while empowerment values are processed later, causing LLMs to think too fast and make premature decisions, hindering effective exploration. These findings shed light on the limitations of LLM exploration and suggest directions for improving their adaptability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 9 0 0 8 1 . 1 0 5 2 : r Lan Pan School of Psychology Georgia Institute of Technology Atlanta, USA louannapan@gmail.com Hanbo Xie School of Psychology Georgia Institute of Technology Atlanta, USA hanboxie1997@gatech.edu Robert C. Wilson School of Psychology Georgia Institute of Technology Atlanta, USA bob.wilson@gatech.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Models (LLMs) have emerged many intellectual capacities. While numerous benchmarks assess their intelligence, limited attention has been given to their ability to explorean essential capacity for discovering new information and adapting to novel environments in both natural and artificial systems. The extent to which LLMs can effectively explore, particularly in open-ended tasks, remains unclear. This study investigates whether LLMs can surpass humans in exploration during an open-ended task, using Little Alchemy 2 as paradigm, where agents combine elements to discover new ones. Results show most LLMs underperform compared to humans, except for the o1 model, with those traditional LLMs relying primarily on uncertainty-driven strategies, unlike humans who balance uncertainty and empowerment. Representational analysis of the models with Sparse Autoencoders (SAE) revealed that uncertainty and choices are represented at earlier transformer blocks, while empowerment values are processed later, causing LLMs to think too fast and make premature decisions, hindering effective exploration. These findings shed light on the limitations of LLM exploration and suggest directions for improving their adaptability. Keywords Large Language Models Exploration Empowerment Uncertainty Reasoning"
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have become landmarks of modern Artificial Intelligence, showcasing remarkable human-like cognitive capacities through their ability to predict and generate text recursively [1, 16, 17]. The question of whether LLMs have reached or will achieve Artificial General Intelligence (AGI) continues to spark debate, fueled by an ever-growing body of empirical evaluations. While extensive benchmarks have been developed to assess how LLMs perceive, think, reason, and act across diverse environments, limited attention has been given to their capacity for exploration. Explorationdefined as behaviors aimed at discovering new information, possibilities, or strategies, often at the expense of immediate rewardsplays crucial role in intelligence, enhancing long-term understanding, adaptability, and performance. This behavior stands in contrast to exploitation, which focuses on leveraging known information for immediate benefits. Exploration has been extensively studied in the fields of Reinforcement Learning [8, 15] and human learning [4, 19, 7]. In human learning, exploration strategies are typically categorized into three types: random exploration, uncertaintydriven exploration, and empowerment. Random exploration introduces stochastic noise into behaviors, enabling agents to stumble upon new information. Uncertainty-driven exploration prioritizes sampling actions with uncertain outcomes to reduce ambiguity and improve decision-making confidence. Empowerment, on the other hand, emphasizes intrinsic rewards and open-ended discovery, driving agents to maximize possibilities rather than optimizing specific outcomes. This type of exploration aligns closely with behaviors observed in tasks like scientific research, where the goal is to uncover as many novel possibilities as possible. These authors contributed equally to this work. Corresponding author. LLM Empowerment Exploration (Under Review) Although preliminary research suggests that LLMs exhibit limited exploratory behavior compared to humans [1], current investigations are narrow in scope, often focusing on bandit tasks [9, 12]. These studies provide an incomplete understanding, neglecting the diverse forms of exploration, particularly empowerment. To bridge this gap, our study investigates LLMs exploration capacities in broader context, examining both uncertainty-driven exploration and empowerment. We address three key research questions in this work: Can Large Language Models explore effectively in an open-ended task, comparable to humans? What exploration strategies do LLMs employ, and how do these compare to human strategies? Why do LLMs succeed or fail in exploratory tasks, and what mechanisms underpin their performance? To explore these questions, we adopt the experimental paradigm introduced by Brändle et al. [2], using the video game Little Alchemy 2 (see methods 2.1). In this game, participants aim to create as many elements as possible by combining known elements, task that closely aligns with the concept of empowerment and offers robust framework for evaluating open-ended exploration. We then apply regression models to analyze the exploration strategies of humans and LLMs, focusing on both uncertainty-driven and empowerment-based behaviors (see methods 2.3). Finally, we deploy Sparse AutoEncoders (SAE) to probe the latent representations of exploration-related values, providing insights into how LLMs process information and generate exploratory behavior. This study not only enhances our understanding of LLMs exploratory abilities but also highlights exploration as key element for building more adaptive and intelligent AI systems."
        },
        {
            "title": "2 Methods",
            "content": "2.1 Task Description: Little Alchemy 2 Little Alchemy 2 involves discovering new elements by combining predefined set of basic elements: water, fire, earth, and air. These elements serve as the initial inventory, and players (humans or LLMs) attempt to discover new combinations based on deterministic rules(see Figure.1). The total is 720 elements and the elements range across categories including nature, space, animals, plants, food, inventions, technology, science, tools, buildings, lore, and myths. Among these elements, only 3,452 combinations (out of 259,560) can successfully create other elements and therefore, it requires semantic understanding of the elements (i.e., empowerment) to explore effectively. This framework mimics creative combinatorial space exploration task, challenging participants to explore patterns to expand their inventory. 2.2 Experimental Setup Data from 29,493 human participants across 4,691,033 trials establish the benchmark. The players were instructed in the rules of the game and tasked with discovering new elements. Performance was measured by the average number of new elements discovered. We evaluated the performance of four LLMs: gpt-4o-2024-08-06(gpt-4o, OpenAI Team [13]), o1-2024-12-17(o1, OpenAI Team [14]), Meta-Llama-3.1-8B-Instruct(LLaMA3.1-8B, Llama Team [10]), and Meta-Llama-3.1-70BInstruct(LLaMA3.1-70B, Llama Team [10]). These models were selected to represent range of model sizes and architectures, closed source as well as open source, allowing us to analyze impacts from model variations on exploration and discovery. Each model was prompted with game rules, current inventory, and trial history to contextual reasoning (Figure. 1). Their outputs were constrained to valid game actions, in the format of element + element (for complete prompts, see Figure. 7A). To investigate the impact of randomness on exploration, we varied the sampling temperature across four settings: 0.0, 0.3, 0.7, and 1.0 (o1 is not available to set parameters and defaults as 1), and under each temperature, there are five repetitions for running the experiment. Lower temperatures encourage deterministic outputs, favoring exploitation, while higher temperatures introduce stochasticity, promoting exploration. These settings allowed us to examine the trade-offs between exploring uncertain combinations and leveraging known strategies. 2.3 Regression: Empowerment vs. Uncertainty-Driven Strategies To analyze exploration dynamics in Little Alchemy 2, we assessed the roles of empowerment and uncertainty in decision-making: 2 LLM Empowerment Exploration (Under Review) 2.3.1 Empowerment Empowerment: In the context of Little Alchemy 2, empowerment translates into the players intrinsic desire to create elements that offer many new successful combinations.Selecting combinations that maximize future potential (e.g., unlocking paths to more elements). For example, the element human in combination with other elements leads to 83 new elements, while alien leads to only 1 new element. Thus, the human element is more empowering than the alien element. Brändle et al. [2] uses neural network to predict empowerment because it effectively models the complex combinatorial relationships and potential outcomes within Little Alchemy 2. By leveraging the neural network, the method incorporates multiple factors, including the probabilities of successful combinations, the likelihood of specific results, and the intrinsic empowerment values of resulting elements. This approach ensures an accurate estimation of the empowerment value by capturing both the immediate and future potential combinations. To align with the original methodology, we use the same empowerment value of each combination from the neural network model in our regression. Empowerment E(ecA,B ) for combination cA,B is modeled as: E(ecA,B ) = (linkcA,B ) 720 (cid:88) i=0 (resultcA,B = i) E(ei) where: (linkcA,B ): Probability of successfully combining and B. (resultcA,B = i): Probability that cA,B results in element i. E(ei): Empowerment of i, based on future combinations. As new trials occur and outcomes are observed (e.g., combining water and fire leads to novel inventory steam), these outcomes provide evidence to update the empowerment values of elements used in the combination. The success or failure of attempts refines the empowerment scores, which directly influence choices made by the LLMs in the following trials. The empowerment value for each trials elements is updated using dynamic updates, based on the empowerment values predicted by neural network. Empowerment is updated as follows: when successful combination creates novel result, the empowerment values of the involved elements are increased. If the combination is repeated the successful combination and no new elements are created, empowerment remains unchanged. On failure, the empowerment values are slightly decreased. This method captures the intrinsic motivation of selecting combinations with higher future potential, refining element values dynamically as the game progresses. Empowerment scores are updated via dynamic updating based on trial outcomes. L(E(ei)) = E(ei) increase_factor, E(ei) decrease_factor, E(ei), if success if fail if repeat 2.3.2 Uncertainty-Driven Exploration Uncertainty reflects the novelty of element use, defined as: Ue = (cid:115) log(T ) te + 1 where is the total trials, and te is the count of element being chosen. Higher Ue encourages exploration of less-used elements. 2.3.3 Statistical Analysis To examine the relationship between temperature, empowerment, uncertainty, and performance in Little Alchemy 2, we employed generalized linear mixed-effects models (GLMMs) with varying configurations tailored to different aspects of the task. This approach allowed us to assess how LLMs and humans adapt their exploration strategies under different conditions. Model 1: We modeled the decision-making process to explore the influence of empowerment and uncertainty on element selection. Model 2: To investigate how sampling temperature interacts with empowerment and uncertainty, we extended the above model, Interaction terms (temperature*empowerment, temperature*uncertainty) to assess how temperature impacts empowermentand uncertainty-driven strategies. LLM Empowerment Exploration (Under Review) 2.4 Sparse Autoencoder (SAE) Analysis SAE is type of auto-encoder structure that can reconstruct inputs with L2 norms in the latent space [11]: ˆx = g(f (x; We, be); Wd, bd), where: = (x; We, be) = σ(Wex + be) is the encoder output (latent representation). ˆx = g(z; Wd, bd) = Wdz + bd is the decoder output (reconstructed input). We Rmn and Wd Rnm are the encoder and decoder weights, with Wd = . be Rm and bd Rn are the encoder and decoder biases. σ() is the activation function (i.e., ReLU). The goal of training SAE is to minimize the reconstruction loss, as well as the L2 norm which forces the latent space to be sparsity: LSAE = 1 (cid:88) i=1 x(i) ˆx(i) 2 + λ (cid:88) j=1 zjwj2, where: 2 is the mean squared reconstruction error for sample i. x(i) ˆx(i)2 λ is the weight for the sparsity regularization term. zj is the activation of the j-th neuron in the latent representation z. wj2 is the L2 norm of the corresponding encoder weight vector wj. Recently, SAE has been proposed to understand the latent representation in language models [3, 6]. To explore how Large Language Models (LLMs) represent cognitive variables such as empowerment and uncertainty in the context of the alchemy game, we used Sparse Auto-Encoders (SAEs) to learn the latent representations of elements within the model. For each layer, we extracted embeddings from each trials available element in the choice set and train them in the SAE. Then we correlate each neuron in the hidden layer in SAEs with our target cognitive variables (i.e., choices, uncertainty values, and empowerment values) and find out the most correlated neuron, where we suppose the relevant variable is most strongly represented. This analysis will help us understand how the model represents and processes cognitive information through the transformer blocks. Finally, we conduct an intervention to examine whether ablating the most correlated neuron causally reduces the corresponding exploration strategy employed by the LLM in the task. 2.5 Most LLMs Performed Worse Than Humans, Except o1 From 29,493 human players, 90% completed fewer than 500 trials. Experiments were set up with 500 trials for the LLMs. On average, LLaMA3.1-8B discovered 9 elements, LLaMA3.1-70B discovered 25 elements, gpt-4o discovered 35 elements, and o1 discouvered 177 elements (Figure.1). In comparison, humans discovered 42 elements on average within 500 trials and 51 elements across all trials."
        },
        {
            "title": "3 Results",
            "content": "o1 significantly outperformed humans (t = 9.71, < 0.001), while the other LLMs performed worse (gpt-4o: = 5.48, < 0.001; LLaMA3.1-70B: = 6.39, < 0.001; LLaMA3.1-8B: = 28.12, < 0.001). Performance improved with larger model sizes, with LLaMA3.1-70B outperforming LLaMA3.1-8B (t = 6.02, < 0.0001) and gpt-4o slightly surpassing LLaMA3.1-70B (t = 3.27, = 0.003). Exploration success declines in later trials as the inventory grows, and it becomes much harder as the probability of success decreases (see Appendix A). Therefore, different exploration strategies could yield very different performances in the task in different phases. Effective strategies in the latter phase rely on understanding latent game structures (empowerment) rather than uncertainty-driven exploration. Sampling temperatures were manipulated to assess their impact on exploration strategies. Higher temperatures moderately improved performance (β = 0.124, = 5.060, < 0.001). 4 LLM Empowerment Exploration (Under Review) Figure 1: A: LLMs Game Process. LLMs select two elements per trial based on the inventory and trial history. B: Human Game Interface. Players select two elements to discover new elements, added to the inventory. C: LLMs and Human Performance. Behavioral patterns (Figure.2) highlight o1s superior strategy, achieving more successful outcomes with new combinations and avoiding repetition of failed or already-successful pairings. This underscores o1s strong exploratory capacity and innovative approach. For other LLMs, Our result shows that for even larger models, as temperature increases, the percentage of choosing existing combinations decreases. This reveals diminishing number of redundant behaviors among LLMs. More importantly, majority of these new combinations do not generate new elements, suggesting that the high temperatures only alter the uncertainty-driven exploration strategies but not empowerment, since in larger spaces, only random combinations are not sufficient to perform the task effectively(see Appendix B). This also explains why higher temperatures can moderately improve the model performance but are still distant from human behaviors. 3.1 LLMs Primarily Use Uncertainty-driven Strategies but Not Empowerment To examine the exact strategies that the models are using, we calculated uncertainty and empowerment values for each element (see methods 2.3). Then, based on each LLMs choices of combinations, each of the elements is encoded as chosen or not chosen. We also used random sampling to balance the proportion of chosen elements and not-chosen elements. Then we use trial numbers, uncertainty values, and empowerment values to predict whether an element is chosen or not. Notably, we used linear mixed effect model with the consideration of random slope on each of these variables, so that we could get individual estimates of each sample. Most LLMs show near-zero empowerment weights, significantly lower than humans (Figure.3, left panel). This suggests LLMs rarely use empowerment for decision-making. In contrast, o1 demonstrates the highest empowerment weights, surpassing humans, indicating human-like strategy of focusing on actions that expand future possibilities. Higher temperatures lead to increased reliance on uncertainty-driven strategies (temperature uncertainty : β = 2.911, = 7.440, < 0.001), but empowerment remains unaffected (β = 0.196, = 1.067, = 0.286). Among all models, only o1, with fixed temperature of 1, balances uncertainty and empowerment effectively, enabling robust exploration in later task stages. 3.2 Uncertainty and Choices are Processed Much Earlier Than Empowerment in LLMs This unbalanced strategy used in traditional LLMs makes us wonder why LLMs could not use empowerment in the game. Theoretically, LLMs should be able to represent the semantic meaning of these elements. Are they really representing such information as empowerment but not using it or do they lack of ability to understand empowerment? To investigate this question, we employed Sparse Auto-Encoders(SAE)(see methods 2.4 to decompose the latent representation of elements in LLMs to figure out whether both empowerment and uncertainty are properly represented during the computation. Our results suggest that, in LLaMA3.1-70B, the uncertainty value is highly correlated at layer 2(r = 0.73, Figure.4A). This suggests that in LLaMA3.1-70B, the uncertainty value is strongly represented in the hidden states. We also discover moderate correlation with empowerment value at layer 72 (r = 0.55, Figure.4A), indicating LLaMA3.1-70B 5 LLM Empowerment Exploration (Under Review) Figure 2: A: Human and LLMs different Temperatures Performance.LLM and Human Performance Across Temperatures. For LLMs, we set four temperatures(0, 0.3, 0.7, 1). LLMs (gpt-4o, LLaMA3.1-8B, LLaMA3.1-70B) achieve their best performance at temperature = 1. B: Human and LLMs Best Temperatures Behaviors. According to whether the combination selected by each trial is repeated, successful, and initial, the behavior of each LLM trial is divided into 5 categories. Compare the temperature at which LLM performs best with humans and o1 behavior. C: LLM Inventory Performance Relative to Human Percentiles. also represents empowerment values in the middle layer. Despite both values being represented in the hidden states of the model, we found that when we run logistic regression models for each neuron to predict the choices, the highest beta weights also occur at layer 1(beta = 1.08, Figure.4A), aligning with the representation of uncertainty values. This explains why the model mainly deploys uncertainty-driven exploration strategies but not empowerment. Interestingly, both choices and uncertainty values are strongly represented at the begining layers, which may indicate that the model already decides before processing empowerment values of the elements in later layers. We additionally intervene in those most correlated neurons to investigate casual relationships between them and model behaviors. We ablated the most correlated neuron (zeroing the latent activation) identified by uncertainty values and empowerment values in the experiment with other identical settings. The regression analysis of intervened model behavior suggests after ablating the empowerment neuron in the SAE, the models empowerment strategy use is even smaller (Figure.4B). This establishes casual relationship that the neuron we identified through SAE can control the models empowerment strategy. On the other hand, when we ablated the uncertainty value neuron, the models performance catastrophically dropped, with most of the trials invalid (Figure.4C). The valid data are not even sufficient for regression analysis. This suggests that the earlier layer of uncertainty is very sensitive in the in-context learning for understanding the contexts of the task and history, showing fundamental role in this task. The ablation results further confirm the role of these two representations in the model. Notably, beyond ablation (simply zeroing out the activation) of the neurons, we also tried other intervention factors to see whether they would help improve model performance through neuroscience approach. Our results showed that both slightly weakening and strengthening neuron activity will hurt the model performance (see appendix C.3). This could suggest deeper issue that the current infrastructure of LLMs may fail to do such open-ended exploration tasks."
        },
        {
            "title": "4 Discussion and Conclusion",
            "content": "Paper Summary. Exploration is essential for discovering new opportunities and understanding complex environments. Our study reveals that most LLMs struggle to achieve human-level exploration in the open-ended task. They heavily rely on uncertainty-driven strategies, which provide short-term gains but fail to support long-term success. While 6 LLM Empowerment Exploration (Under Review) Figure 3: Regression Estimates by Temperature and Model. All models show lower empowerment weights than humans, except o1. As temperature increases, uncertainty weights rise, with o1 showing the highest weights across all models and humans. both uncertainty and empowerment are present in their latent spaces, LLMs fail to balance these strategies effectively, resulting in suboptimal performance and limited adaptability to broader decision spaces. However, we also find exceptional models like o1, which surpass humans in performance and indicate stronger uncertainty-driven and empowerment exploration strategy usage. This suggests that LLMs with reasoning training may be essential to perform in open-ended tasks, which requires variety of exploration strategies. Fast Thinking in Traditional LLMs. key issue lies in LLMs thinking too fast during exploratory tasks. In LLaMA3.1-70B, uncertainty values dominate early transformer blocks and the activations from early transformer blocks correlate strongly with immediate choices, while empowerment values emerge in middle blocks. This temporal mismatch leads to premature decision-making that prioritizes short-term utility over deeper exploration. This predominant information processing by uncertainty values and choices can weaken the role of empowerment in the exploratory decision-making process. We also replicate similar finding in LLaMA3.1-8B (Figure.13 in the appendix). We believe that the traditional inference paradigms mainly drive this premature information processing in traditional LLMs. The autoregressive way of generating tokens based on conditional probabilities of existing context may diminish the potential exploration capacity of LLMs. We additionally tried several common ways to improve the model performance, such as prompt engineering, intervention (feature steering), and seeking alternative models (appendix.C). The additional results show that both prompt engineering and intervention did not help to improve the model performance. In the model alternative, we experiment on most recently released open-source reasoning model, DeepSeek-R1 [5], which claims to match o1-level performance in multiple benchmarks, with its reasoning process visible. This reasoning model outperforms other traditional LLMs and reaches human-level performance in this task (Figure.9 in the appendix). This superior performance provides more evidence that model infrastructure may be the main reason that limits their performance in this type of task. Lmitations and Future Directions. Despite these findings, the underlying cause of LLMs thinking too fast remains unclear and requires further investigation. Future research could explore the interaction between model architecture and processing dynamics, as well as how LLMs weigh uncertainty and empowerment during decision-making. Interventions such as integrating extended reasoning frameworks like CoT, optimizing transformer block interactions, or training with explicit exploratory objectives could enhance LLMs exploratory abilities. These efforts would not only improve performance but also advance our understanding of creating AI systems capable of more human-like exploration. 7 LLM Empowerment Exploration (Under Review) Figure 4: A: SAE Correlation Analysis. Maximum correlation of uncertainty values across layers, peaking at layer 2. Maximum correlation of empowerment values across layers, peaking at layer 72. Maximum correlation of choices across layers, peaking at layer 1. B: LLaMA3.1-70B Intervention Regression Results. The regression estimates for empowerment, and uncertainty under the original condition(LLaMA3.1-70B, temperature = 1), empowerment intervention (set to 0), and uncertainty intervention (set to 0). C: LLaMA3.1-70B Average Inventory of Interventions. Uncertainty intervention significantly reduces the average inventory, indicating its essential role in model performance."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Franziska Brändle for helping make up the source data and codes for the human empowerment study. We also thank Can Demircan and Huadong Xiong for the helpful discussion in training and interpreting Sparse AutoEncoders in the paper. This work was funded by SCIALOG Award #29079 from the Research Corporation for Scientific Advancement (to RCW) and the OpenAI Researcher Access Program (to HX). This research was also supported in part through research cyberinfrastructure resources and services provided by the Partnership for an Advanced Computing Environment (PACE) at the Georgia Institute of Technology, Atlanta, Georgia, USA. Codes and data will not be publicly available until the paper is peer-reviewed. However, they can be available upon special requests to the authors."
        },
        {
            "title": "References",
            "content": "[1] Marcel Binz and Eric Schulz. Using cognitive psychology to understand gpt-3. Proceedings of the National Academy of Sciences, 120(6):e2218523120, 2023. [2] Franziska Brändle, Lena Stocks, Joshua Tenenbaum, Samuel Gershman, and Eric Schulz. Empowerment contributes to exploration behaviour in creative video game. Nature Human Behaviour, 7(9):14811489, 2023. [3] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. https://transformercircuits.pub/2023/monosemantic-features/index.html. 8 LLM Empowerment Exploration (Under Review) [4] Nathaniel Daw, John Odoherty, Peter Dayan, Ben Seymour, and Raymond Dolan. Cortical substrates for exploratory decisions in humans. Nature, 441(7095):876879, 2006. [5] DeepSeek-AI Team. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [6] Can Demircan, Tankred Saanum, Akshay Jagadish, Marcel Binz, and Eric Schulz. Sparse autoencoders reveal temporal difference learning in large language models. arXiv preprint arXiv:2410.01280, 2024. [7] Samuel Gershman. Deconstructing the human algorithms for exploration. Cognition, 173:3442, 2018. [8] Leslie Pack Kaelbling, Michael Littman, and Andrew Moore. Reinforcement learning: survey. Journal of artificial intelligence research, 4:237285, 1996. [9] Akshay Krishnamurthy, Keegan Harris, Dylan Foster, Cyril Zhang, and Aleksandrs Slivkins. Can large language models explore in-context? arXiv preprint arXiv:2403.15371, 2024. [10] Llama Team. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [11] Andrew Ng et al. Sparse autoencoder. CS294A Lecture notes, 72(2011):119, 2011. [12] Allen Nie, Yi Su, Bo Chang, Jonathan Lee, Ed Chi, Quoc Le, and Minmin Chen. Evolve: Evaluating and optimizing llms for exploration. arXiv preprint arXiv:2410.06238, 2024. [13] OpenAI Team. Gpt-4o system card. https://openai.com/index/gpt-4o-system-card/, 2024. [14] OpenAI Team. Openai o1 system card. https://openai.com/index/openai-o1-system-card/, 2024. [15] Richard Sutton and Andrew Barto. Reinforcement learning: An introduction. MIT press, 2018. [16] Ala Tak and Jonathan Gratch. Gpt-4 emulates average-human emotional cognition from third-person perspective. arXiv preprint arXiv:2408.13718, 2024. [17] Taylor Webb, Keith Holyoak, and Hongjing Lu. Emergent analogical reasoning in large language models. Nature Human Behaviour, 7(9):15261541, 2023. [18] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [19] Robert Wilson, Andra Geana, John White, Elliot Ludvig, and Jonathan Cohen. Humans use directed and random exploration to solve the exploreexploit dilemma. Journal of experimental psychology: General, 143 (6):2074, 2014. [20] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. 9 LLM Empowerment Exploration (Under Review)"
        },
        {
            "title": "A The Game Difficulty",
            "content": "We use real game tree to calculate the probability of each player succeeding as the inventory size increases. The simulation incorporates random setting for selecting elements and combinations, ensuring variability across trials. At each step, new elements are added to the inventory based on the successful combinations, and the success probability is recalculated dynamically. The success rate decrease in Figure 5 aligns with the convergence of inventory growth trends in Figure 2. As the inventory size grows, the success rate decreases, making further growth increasingly difficult. For example, when the inventory size = 4, that is, the initial state, there are four elements: water, fire, air, and earth. There are 10 combinations between two elements (including the same element), and each combination can succeed. Therefore, when the inventory size is = 4, the players success rate is 100%. The success probability (Ps) is given by: Where: Ps = Cn n: Current inventory size. Cn: The total number of possible combinations given the inventory size n, Cn = n(n+1) . This includes 2 combinations with repetition (e.g., + A). S: Number of successful combinations for the current inventory size. Figure 5: Game Difficulty vs. Inventory Size. Based on the real game tree, each inventory size has different success probability. LLM Empowerment Exploration (Under Review)"
        },
        {
            "title": "B The LLM Behavior Across Different Temperatures",
            "content": "To better understand the exact changing behaviors of LLMs under different temperatures, we categorized all combinations into four types: whether this generates new element, and whether these two combinations have been used before. Our result shows that for even larger models, as temperature increases, the percentage of choosing existing failed combinations decreases. This reveals diminishing number of redundant behaviors among LLMs. In the meantime, the percentage of choosing new but failed combinations increases significantly, which suggests that the model tends to choose new combinations more often in higher temperatures than in lower temperatures. More importantly, majority of these new combinations do not generate new elements, suggesting that the high temperatures only alter the uncertainty-driven exploration strategies but not empowerment, since in larger spaces, only random combinations are not sufficient to perform the task effectively. This also explains why higher temperatures can moderately improve the model performance but are still distant from human behaviors. Figure 6: Behavioral Categories of LLMs at Different Temperatures. Each trial is categorized into five conditions:(1)Failure with Existing Combination: The trial repeats previous combination that does not generate new element. (2)Failure with New Combination: The trial uses new combination for the first time, but it fails to generate new element. (3)Success with New Combination: The trial uses new combination for the first time, successfully generating new element. (4)Success with Existing Combination: The trial repeats previous combination that successfully generates an element. (5)Invalid Trial: The chosen one or two elements are not present in the current inventory. 11 LLM Empowerment Exploration (Under Review)"
        },
        {
            "title": "C Attempts for Model Improvements",
            "content": "To investigate any potential general way to improve the performance of models, we had several attempts including prompt engineering, interventions, and experiments on alternative open-source models. C.1 Prompt Engineering Because the model exhibited repeated behaviors and did not fully utilize empowerment, we introduced more direct, guiding prompts (including the steps highlighted in Figure 7) to help gpt-4o(temperature = 1) make more diverse and forward-looking choices, including Chain-of-Thought (CoT, Wei et al. [18]), self-reflection [20] and even explicit strategy hint. However, the result shows that simply updating the prompteven with explicit instructions and reasoning strategiesdidnt have significant improvement in performance(average inventory = 43, = 1.17, = 0.304). The model continued to repeat combinations and did not significantly increase its ability to discover new elements. It averages 43 elements. This suggests that while prompt engineering can help shape models outputs, it may not be enough on its own to overcome certain ingrained tendencies such as repeating prior actions or failing to maximize exploratory behavior. Figure 7: A: LLMs Game Original Prompt. The prompt for each trial consists of three parts: the system prompt, which provides the game rule guide; the current inventory including those from the beginning and discoveries during the game; and the trial history. B: LLMs Game Prompt Engineering. Each colored section highlights specific goal: The green section encourages models to explore more creative combinations by reminding them that wider variety of elements can unlock new possibilities. The blue section emphasizes avoiding repeated behavior by explicitly instructing the model to check past attempts. Figure 8: A: Best Temperature of Each Model and Human Performance. B: Best Temperature of Each Model and Human Behaviors. Choose the LLM models(gpt-4o, LLaMA3.1-8B, LLaMA3.1-70B) best performance at temperature = 1, and compare it with human and o1, gpt-4o prompt-engineering(temperature = 1). Compare each models performance and behaviors. LLM Empowerment Exploration (Under Review) C.2 Open-Source Reasoning Model - DeepSeek-R1 To investigate whether reasoning model, which is known as trained with RL algorithms in the inference phase, would generate better result than traditional LLMs, we also experimented with the most recently publicly released opensource reasoning model, DeepSeek-R1[5]. This model like o1 can do deep chain-of-thought reasoning automatically and the reasoning process is visible. Therefore, we quickly investigated this model to see how the reasoning models perform and how their reasons can relate to the actual thinking process in this task. The result shows that DeepSeek-R1 reached near human-level task performance (Figure.9A), but still underperforms than o1. In the behavioral patterns (Figure.9B), DeepSeek-R1, compared to traditional LLMs have fewer attempts on existing combinations, showing stronger exploration strategy usage. However, compared to humans and o1, DeepSeekR1 tried more on failed new element combinations but not successful ones, suggesting DeepSeek-R1 may explore less effectively than humans and o1. Our regression results confirm furtherly that DeepSeek-R1 exhibit stronger/weaker uncertainty-driven exploration stratgies and stronger/weaker empowerment, echoing its underperformance than o1. Figure 9: A: Best Temperature of Each Model and Human Performance. B: Best Temperature of Each Model and Human Behaviors. Choose the LLM models(gpt-4o, LLaMA3.1-8B, LLaMA3.1-70B) best performance at temperature = 1, and compare it with human and o1, deepseek-reasoner(temperature = 1). Compare each models performance and behaviors.C: Regression estimates by temperature and model. To conduct deeper investigation, we also collected the models reasoning process along with the experiment. Here, we use qualitative analysis of some pieces of its reasoning process. For example, in trial 200, the model discovered 48 elements. The specific reasoning process is in Figure. 10. The model engages in systematic multi-stage reasoning process to propose new combination from its inventory. It begins by reviewing the inventory and cross-referencing prior failed attempts to avoid redundancy. The model revisits successful paths (e.g., how village was created from house + field) and evaluates logical relationships(empowerment) 13 LLM Empowerment Exploration (Under Review) between elements, such as exploring combinations involving larger structures like continent. But the model doesnt choose combination containing continent. It discards irrelevant or infeasible paths, such as those requiring missing elements (ash, fruit). After confirming the presence of both city and water in the inventory, it identifies that city + water has not been tested and hypothesizes that this combination could yield new element, such as port or harbor. This iterative and logic-driven process highlights the models ability to balance memory, deduction, and validation in problem-solving. However, the shortcoming is obvious. Although there are many logical expressions, the models reasoning process is not straightforward to its final choice, where we only see the last sentence in red can somehow explain the underlying motivation. This could probably suggest that the models reasoning process is somehow redundant and may needs further refinement to make efficient decisions. Figure 10: The reasoning process of DeepSeek-R1 in Trial 200: The model explores possible combinations(blue color part) for discovering new element in the game by systematically reviewing inventory elements(orange color part), prior attempts(blue color part), and logical inferences to make the decision(red color part). It also found some more empowerment elements(green color part) but didnt choose them. C.3 Intervention Analysis on LLaMA3.1-70B Empowerment and Uncertainty Layers To investigate the role of empowerment and uncertainty representations in LLaMA3.1-70B during in-context learning, we analyzed the most correlated neurons with these values and performed targeted interventions to assess their impact on model performance. Given that both empowerment and uncertainty were highly demanding in the task (shown by the example of o1s superior performance), we are wondering whether intervention to strengthen both representations of uncertainty and empowerment could generate better performance. We found that interventions in the uncertainty layer, which is located relatively early in the model (similar to the choice layer), caused severe performance degradation, with even minor adjustments rendering LLaMA3.1-70B unable to perform the task effectively. This suggests that uncertainty representation plays fundamental role in processing task context and history in in-context learning. Conversely, enhancing the empowerment layer also cant improve the performance. Even when set to an intervention factor of 1.5, which brought the model closer to the original level, other intervention values resulted in decreased performance. Additionally, ablation experiments, in which the most correlated neuron activations were zeroed out, further confirmed the critical role of these representations. Therefore, as highlighted before, we argue the limit in infrastructure in traditional LLMs is the main reason for their below-human-level performance in open-ended exploration tasks. 14 LLM Empowerment Exploration (Under Review) Figure 11: LLaMA3.1-70B Average Inventory of Uncertainty Intervention. Set 5 different levels of uncertainty intervention(0.0, 0.5, 0.7, 1.0, 2.0). Increasing the uncertainty intervention progressively disrupts the models ability to complete the task, indicating the critical role of early uncertainty layers in processing task history and context. LLaMA3.1-70B Average Inventory of Empowerment Intervention. Performance remains closer to the original level when the intervention is set to 1.5, whereas other levels of intervention result in performance degradation."
        },
        {
            "title": "D SAE Setup",
            "content": "We train all layers in LLaMA3.1-70B with the same set of hyper-parameters. Those hyper-parameters are tuned to ensure reconstruction is satisfying as well as with good sparsity representation. We set the hidden size of latent as 8192, the same as the dimensions of the model embeddings. We set the learning rate as 1e-4, with batch size of 256. The L2 norm is only 1e-6. L2 norm above this value will significantly amplify the reconstruction loss. sanity check for this parameter. Same way for LLaMA3.1-8B, we set the hidden size of latent as 4096, the same as the dimensions of the model embeddings. We set the learning rate as 1e-4, with batch size of 256. The L2 norm is only 1e-6. Figure 12: Sparse Autoencoder (SAE) Training Metrics. Each row represents different model architectures. From left to right, the panels illustrate the layer-wise test L2 norm, test reconstruction loss, and the number of active neurons during training. The top row corresponds to smaller model(LLaMA3.1-8B), and the bottom row corresponds to larger model(LLaMA3.1-70B) with more layers. Test Accuracy Between Original and Reconstructed Data. In both cases, reconstructed data achieves higher accuracy across layers, demonstrating the SAEs ability to preserve essential features during encoding and reconstruction. 15 LLM Empowerment Exploration (Under Review) Replicated SAE Result in LLaMA3.1-8B We investigated the role of the empowerment and uncertainty layers in LLaMA3.1-8B (temperature = 1) by training Sparse Autoencoder (SAE) to identify and interpret their activations, followed by targeted interventions where each layers activation was set to zero. The results, summarized in Figure.13A and Figure.13B, show that setting the empowerment layer to zero had minimal effect on regression estimates and only slightly reduced model performance, suggesting that the empowerment layer has limited role in sustaining task performance. In contrast, setting the uncertainty layer to zero led to substantial reduction in regression estimates for uncertainty, accompanied by marked decline in model performance. This highlights the critical importance of the uncertainty layer in facilitating exploration and maintaining robust decision-making capabilities within the model. Figure 13: A: SAE Correlation Analysis. Maximum correlation of uncertainty values across layers, peaking at layer 7. Maximum correlation of empowerment values across layers, peaking at layer 22. Maximum correlation of choices across layers, peaking at layer 15. B: LLaMA3.1-8B Intervention Regression Results.The bars represent the regression estimates for empowerment, and uncertainty under the original condition(LLaMA3.1-8B, temperature = 1), empowerment intervention (set to zero), and uncertainty intervention (set to zero). C: LLaMA3.1-8B Average Inventory of Interventions. Uncertainty intervention leads to significant reduction in the average inventory, indicating its essential role in model performance."
        }
    ],
    "affiliations": [
        "School of Psychology Georgia Institute of Technology Atlanta, USA"
    ]
}