{
    "paper_title": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation",
    "authors": [
        "Vladan Stojnić",
        "Yannis Kalantidis",
        "Jiří Matas",
        "Giorgos Tolias"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose a training-free method for open-vocabulary semantic segmentation using Vision-and-Language Models (VLMs). Our approach enhances the initial per-patch predictions of VLMs through label propagation, which jointly optimizes predictions by incorporating patch-to-patch relationships. Since VLMs are primarily optimized for cross-modal alignment and not for intra-modal similarity, we use a Vision Model (VM) that is observed to better capture these relationships. We address resolution limitations inherent to patch-based encoders by applying label propagation at the pixel level as a refinement step, significantly improving segmentation accuracy near class boundaries. Our method, called LPOSS+, performs inference over the entire image, avoiding window-based processing and thereby capturing contextual interactions across the full image. LPOSS+ achieves state-of-the-art performance among training-free methods, across a diverse set of datasets. Code: https://github.com/vladan-stojnic/LPOSS"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 7 7 7 9 1 . 3 0 5 2 : r LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation Vladan Stojnic1 Yannis Kalantidis2 Jiˇrı Matas1 Giorgos Tolias1 1 VRG, FEE, Czech Technical University in Prague 2 NAVER LABS Europe"
        },
        {
            "title": "Abstract",
            "content": "53.8/35.9 66.1/48.4 We propose training-free method for open-vocabulary semantic segmentation using Vision-and-Language Models (VLMs). Our approach enhances the initial per-patch predictions of VLMs through label propagation, which jointly optimizes predictions by incorporating patch-to-patch relationships. Since VLMs are primarily optimized for crossmodal alignment and not for intra-modal similarity, we use Vision Model (VM) that is observed to better capture these relationships. We address resolution limitations inherent to patch-based encoders by applying label propagation at the pixel level as refinement step, significantly improving segmentation accuracy near class boundaries. Our method, called LPOSS+, performs inference over the entire image, avoiding window-based processing and thereby capturing contextual interactions across the full image. LPOSS+ achieves state-of-the-art performance among training-free methods, across diverse set of datasets. Code: https: //github.com/vladan-stojnic/LPOSS 1. Introduction Semantic segmentation models are typically trained in supervised manner, requiring pixel-level annotations based on predefined set of categories or classes. These approaches face significant scalability challenges due to the high cost of annotation, and their applicability is limited by the fixed set of classes they are trained to recognize. Open-vocabulary semantic segmentation aims to generalize and allow the segmentation of an image into any given list of classes provided at inference time. This task is usually solved with the use of Vision-and-Language Model (VLM) [10, 22, 33]. VLMs produce aligned textual and visual features and enable matching textual descriptions, such as class names, to visual features. While highly effective for image-level classification tasks, VLMs are less suited for dense prediction tasks like semantic segmentation out-of-the-box, as they are trained to align global image representations 82.1/53.6 84.0/59.1 69.4/54.5 70.3/60.0 ground-truth"
        },
        {
            "title": "LPOSS",
            "content": "LPOSS+ Figure 1. Open-vocabulary semantic segmentation with LPOSS (patch-level LP) and LPOSS+ (pixel-level LP). Performance reported via mIoU / Boundary IoU. Images from the Context and COCO-Stuff datasets. with textual data. Our work addresses this gap by propostraining-free VLM enhancement for opening novel, vocabulary semantic segmentation. Although segmentation is defined as pixel-wise labeling, recent approaches, that rely on VLMs [26, 42, 46] utilizing ViT [15] architecture, form predictions at the patch level and simply up-sample the predictions. ViT-based VLM typically provides class probability distribution independently for each patch, yielding an initial prediction. Our approach incorporates the initial prediction together with patch relationships by making joint predictions across all patches. This is achieved through minimizing quadratic cost function with unary and binary terms. This approach ensures that final predictions remain close to initial ones while nearby patches or patches with similar appearances produce similar predictions, reflecting common prior in semantic segmentation. Optimization is performed through 1 either an iterative Label Propagation (LP) [54] operation on patch graph or, equivalently, by solving linear system. The latter is computationally efficient due to the high sparsity of the coefficient matrix and the use of GPU-based solvers. Following prior work [23, 26, 46], and due to the fact that VLMs optimize inter-modal rather than intramodal similarity, we leverage strong Vision Model (VM), specifically DINO [5], to effectively capture patch-to-patch appearance and relationships. Using such an encoder, instead of the VLMs visual encoder, significantly improves predictions at patch level. While operating at the patch level provides efficiency benefits, it constrains pixel-level accuracy. In an experiment where semantic segmentation ground truth is downsampled to patch-level resolution, then upsampled and evaluated as prediction, we show that the resulting performance measured via mIoU averages approximately 85% across eight datasets. To address this limitation, we perform second stage of LP on pixel-level graph, using the patch-based LP result as the initial prediction set. This refinement consistently improves segmentation accuracy, particularly near class boundaries, as we show by measuring the Boundary IoU metric [9]. Figure 1 demonstrates examples of such improvement using VLM and VM with ViT backbone. The strength of ViT architectures comes with limitations. There is strong bias introduced by their pretraining settings, i.e. processing fixed-resolution square images. Consequently, current segmentation methods perform feature extraction and prediction in sliding window fashion with overlapping windows. In contrast, we move beyond this standard practice by performing predictions jointly for the entire image, accounting for interactions across all patches. However, we show that while joint predictions are possible, joint feature extraction across the whole image is not feasible without noticeable performance decrease. Our proposed method, called Label Propagation over Patches (LPOSS) and Pixels (LPOSS+), achieves state-of-the-art results across various datasets, outperforming existing training-free methods on eight datasets. Our contributions are summarized as follows: We establish connection between classical label propagation methods and vision-language models (VLMs) in the context of dense prediction tasks. We introduce training-free approach for semantic segmentation that predicts and propagates labels at both patch and pixel levels. This strategy leverages the pretraining of foundation models and addresses the performance limitations of coarse patch-level predictions. We propose to decouple feature extraction from class prediction, enabling image-level inference and capturing contextual interactions across the entire image. 2. Related work It The introduction of vision-language models [10, 22, 33] opened up the possibilities of performing many recognition tasks in the open-vocabulary setting [43], e.g., zero-shot classification [33, 38, 41], semantic segmentation [17, 53], object detection [44], etc. However, considering these models are usually trained only to optimize global image representation, they do not excel in dense recognition tasks such as segmentation [3, 53]. Because of this, considerable work is being done to improve their performance. This is achieved by training specialized VLM for such tasks, changing the VLM architecture during inference, and employing other vision models to aid VLMs. Training VLMs for segmentation. One line of work trains from scratch VLMs specialized for semantic segmentation using varying levels of supervision: using segmentation masks without class annotations [14, 17] and only imagecaption pairs [29, 34, 47, 48]. On the other hand, another line of work trains additional modules on top of pre-trained VLM to improve their localization abilities [6, 28, 31]. Training-free segmentation with VLMs. is observed [27, 42] that different VLM components are responsible for VLMs bad localization performance and that the performance can be substantially improved by removing or changing them during inference. MaskCLIP [53] revisits the global attention pooling of CLIP [33] and proposes to remove its query and key embeddings while reformulating the value embedding layer and the last linear layer into two 11 convolutions. SCLIP [42] proposes to replace the selfattention layer of the ViT with correlative self-attention. ClearCLIP [25] identifies residual connections as the primary source of VLMs bad localization performance. Because of this, it proposes to remove them from the last layer. It additionally replaces self-attention with self-selfattention and removes the feed-forward network from the last layer. GEM [3] improves the semantic segmentation abilities of VLMs by adding self-self-attention blocks parallel to self-attention layers. We consider these methods complementary to ours as we apply our method on top of MaskCLIP [53]. VLMs and VMs for segmentation. Vision models trained in self-supervised fashion show remarkable object localization abilities [5, 13, 32, 36, 37]. Because of this, the growing line of work investigates how to leverage them to improve the ability of VLMs to perform open-vocabulary semantic segmentation. CLIP-DIY [45] uses an unsupervised object discovery method FOUND [37] to clear and improve the open-vocabulary segmentation maps of CLIP [33]. CLIP-DINOiser [46] proposes training-free variant that uses patch-to-patch affinity from DINO [5] to refine CLIPs image features. Additionally, it shows that good localization capabilities can be extracted from CLIP by training two small convolutional layers. ProxyCLIP [26] 2 proposes replacing the attention matrix of the last layer of CLIP [33] with an affinity matrix from vision model such as DINO [5]. LaVG performs panoptic segmentation using the normalized cut on top of DINO [5] features. The discovered segments are then classified into class using VLM. Our method falls into this group of methods and is most similar to CLIP-DINOiser [46]. Compared to CLIPDINOiser, which uses affinity based on Euclidean similarities between DINO patches, we propose to use geodesic similarities obtained through label propagation. Additionally, compared to all other works, we perform the prediction jointly across all sliding windows, which are usually used in such methods. Finally, we depart from performing predictions only at the patch level and refine the predictions at the pixel level by applying the same process on top of pixels. Relation to CRFs. Conditional Random Fields (CRFs) [24, 40] have long tradition in dense prediction tasks with In early deep models classical approaches [18, 24, 35]. for semantic segmentation, CRFs are used as refinement step to increase the accuracy and the resolution of the predictions [8]. There are also attempts to integrate similar objective function during end-to-end training [7, 50]. CRFs and LP both leverage pairwise connections to achieve smooth label assignments. LP directly propagates labels based on neighbors, while CRFs explicitly model these relationships in their energy function. We opt for an LP variant that has theoretical convergence proofs and corresponds to an intuitive quadratic cost function, which does not hold for all existing LP variants [2]. LP is typically easy to implement, computationally efficient [54], and compatible with GPU-based software implementations [38]. Consequently, it is used in various computer vision and machine learning tasks [19, 21, 38, 39], while in this work, LP is tailored for semantic segmentation in the era of VLMs and ViT encoders. 3. Label propagation over patches and pixels 3.1. Problem formulation In the task of open-vocabulary semantic segmentation we are given an image RHW 3 and list of class names = {c1, . . . , cC}. Our goal is to assign one of the classes to every pixel or, conversely, tensor RHW that contains segmentation mask for each of the classes. To solve such task, one can utilize Vision Language Model (VLM), i.e. model that is able to encode both the class names as well as the image in the compatible feature spaces. Let : RdNP denote the vision encoder, i.e. ViT [15] with patch size . The vision encoder extracts feature vectors for all NP = H/P W/P patches for an image in 1. Also, let : Rd represent the text encoder of the VLM, with representing the space of the textual input. One can, therefore, perform open-vocabulary semantic segmentation by obtaining the patch features for an image Zvlm = (X), class name features = g(C) RdC, and calculating per patch class similarities Yvlm = vlmF, (1) which are further upsampled to pixel-wise class similarities at the original image size. The class with the highest similarity is then assigned to each pixel. 3.2. Preliminaries Label propagation (LP). Let {n1, . . . , nN } be set of graph nodes and RN be symmetric, typically very sparse, adjacency matrix with zero diagonal. We obtain symmetrically normalized adjacency matrix by ˆS = 1 2 , where = diag(S1N ) is degree matrix and 1N is the all-ones -dimensional vector. Given ˆS, label propagation [54] is an iterative process given by 2 SD 1 ˆY (t+1) = α ˆS ˆY (t) + (1 α)Y (2) until convergence, that refines initial node predictions RN across classes based on the graph structure2. Hyper-parameter α (0, 1) controls the amount of propagation and refinement. It is possible to show [54] that this iterative solution is equivalent to solving -variable linear systems L(S) ˆY = Y, (3) 2 = α ˆS is the graph where L(S) = αD 1 2 SD 1 Laplacian. The closed-form solution of these linear systems ˆY = L(S)1Y = Linv(S)Y, (4) is not practical for large graphs as Linv(S) is dense RN matrix. However, given that is positive-definite, it is usual [20, 21] to solve (3) using the conjugate-gradient method which is known to be faster than running the iterative solution [20]. Matrix Linv provides geodesic similarities between all pairs of nodes, as these are captured based on the graph structure. Additionally, label propagation corresponds to minimizing the quadratic criterion Q( ˆY ) = (1α) (cid:88) i=1 ˆYiYi2+α (cid:88) i,j=1 SijD 1 2 ˆYiD 2 ˆYj2. (5) The criterion (5) consists of two parts: the first one keeps node predictions close to the initial ones, and the second one pushes neighboring nodes in the graph to have the same predictions. 1We discard the representation corresponding to the CLS token usually 2A common use of LP is to propagate from labeled to unlabeled nodes; present in ViTs. instead we refine predictions for all nodes which are already labeled. 3 Exploiting Vision Model (VM). VLMs are trained by minimizing cross-modal similarities between corresponding visual and textual features. This training approach results in suboptimal performance on dense prediction tasks, such as open vocabulary semantic segmentation [53], for two main reasons. First, VLMs are typically trained using loss on global image representations, which means they do not explicitly optimize patch-level features. Second, VLM training focuses solely on cross-modal similarities and does not account for relationships across patches, making the resulting representations less effective for capturing the interpatch relationships essential for semantic segmentation. For these reasons, recent works [23, 26, 46] further utilize separate Vision Model (VM) to provide improved patch-level representations. Let Zvm = h(X) denote patchlevel features from such model, where : RdNP is an encoder with ViT architecture. The usual choice for the vision model is DINO [5], an encoder trained with self-supervised learning that exhibits strong performance for dense prediction tasks [5, 36, 37]. The training-free variant of the CLIP-DINOiser [46] improves VLM predictions Yvlm by refining VLM image features Zvlm using patch relations from VM. They use image features Zvm and compute an affinity matrix = vmZvm. This affinity is then used to refine the VLM predictions as YDINOiser = AZ vlmF = AYvlm. (6) This process propagates the patch features, or, equivalently, due to linearity, the VLM predictions, over the affinity matrix3. This is smoothening operation, which we also perform in our method but in more principled way. Class prediction using sliding window approach. Current approaches [23, 26, 42, 46] perform feature extraction and prediction in sliding-window fashion to handle test images of arbitrary resolution. Let denote set of sliding windows. Prediction typically proceeds in 3 steps: vlm and obtain predictions (wi) 1. Extract features (wi) vlm = independently for each window wi W. (wi) vlm 2. Upsample patch-level predictions to the original window resolution using function up(Y (wi) vlm ). 3. Obtain per-pixel predictions by averaging predictions over all sliding windows that contain each pixel and combine them into an class confidence map vlm = combine({up(Y (w1) (img) vlm )}) . The sliding window approach presented above is result of the poor generalization of ViTs to resolutions and aspect ratios different from the ones used during training. All existing approaches couple feature extraction with class prediction and perform both jointly, and independently in each window before averaging predictions across windows. vlm ), . . . , up(Y (wK ) In the following sections, we decouple the two for our approach and perform predictions by looking across windows. 3.3. LPOSS: LP over patches We observe that (6) and (4) can both be seen as different forms of label refinement. In (6), the VLM-based labels Yvlm are refined using matrix of Euclidean similarities across patches, while in (4), graph labels are refined via the geodesic similarities Linv, computed over the graph. This motivates us to utilize label propagation to refine the VLM predictions. Not only is (4) optimizing (5), which is principled and intuitive objective function, but we also expect geodesic similarities to capture more complex relations and contextual information, as they are computed by considering the entire set of patch features together. We assume patch pi = (zvmi, pi) is represented as tuple by its features zvmi from the VM, i.e. the i-th column of Zvm, and the location of its center pi in the image. We then construct graph with nodes {p1, . . . , pNP } and adjacency matrix SP = Sa Sp, where denotes element-wise multiplication. Sa is an appearance-based adjacency matrix zvmj )γ if zvmj is in the k-nearest neighbors and saij = (z vmi of zvmi, and 0 otherwise. Sp is position-based adjacency matrix that depends on the spatial distance between patches pipj 2 with spij = exp . Having such graph, we σ apply label propagation by (cid:16) (cid:17) Ylposs = Linv(SP)Yvlm (7) to refine the VLM predictions. This process effectively refines the predictions such that neighboring patches (patches with high appearance similarity or spatial proximity) have similar predictions, while keeping the final prediction not far from the VLM predictions. 3.4. LPOSS across all windows We propose to decouple the feature extraction process from class prediction. First, we perform feature extraction on per-window basis to be consistent with the encoders training resolution. Unlike existing methods, however, we then carry out predictions across windows, i.e., jointly for the entire image. This approach enables us to explicitly account for interactions across all patches. , = 1, . . . , NP , which we merge in (W) Sliding-window extraction provides us with baseline VLM predictions (wi) for window wi with patches vlm p(wi) vlm = , . . . , (wK ) [Y (w1) ], across all windows. We convlm vlm struct affinity matrix S(W) over , . . . , p(wK ) nodes {p(w1) } contain1 ing patches from all sliding windows, and perform LP jointly for the whole image by RKNP KNP , . . . , p(wK ) , . . . , p(w1) NP NP 1 3Their implementation ℓ2-normalizes features after propagation, i.e. ℓ2(AX)F , which breaks the equivalence but performs slightly better. lposs = Linv(SW )Y (W) (W) vlm . (8) 4 Figure 2. Overview of LPOSS and LPOSS+. Processing steps: (1) input image window cropping (2) feature extraction with the VLM and patch-level predictions per window (3) patch-level predictions jointly across all windows with LPOSS (4) upsampling of patch-level predictions (5) combining window predictions into single pixel-level image prediction (6) pixel-level prediction with LPOSS+. The graphs of LPOSS and LPOSS+ are generated based on patch features extracted with the VM and pixel color values, respectively. 79.0/67.8 32.8/6.9 ground-truth ground-truth at patch-level res. Figure 3. Visualization of artifacts caused by patch-level predictions. Ground-truth at patch-level resolution produced by downsampling the ground-truth by the patch size = 16 and then upsampling to the original size. Performance reported via mIoU / Boundary IoU. We decompose the output prediction into parts associated with each window by (W) ], and, similar to other approaches, upsample and combine by lposs = [Y (w1) , . . . , (wK ) lposs lposs lposs = combine({up(Y (w1) (img) lposs ), . . . , up(Y (wK ) lposs )}), (9) lposs RHW C. We show graphical represenwith (img) tation of this process in Figure 2. 3.5. LPOSS+: LP over pixels Similar to related approaches, LPOSS performs predictions at patch resolution, which are subsequently upsampled to the original image resolution. However, we observe that this process results in block-like artifacts in the predictions, as shown in Figure 3. To assess the impact of these artifacts on performance, we conducted the following experiment on the ground-truth maps: we downsampled the ground-truth map to patch-level resolution by factor corresponding to the patch size (e.g., = 16), then upsampled it back to the original image resolution. We treat the result as prediction and evaluate performance using mIoU and Boundary IoU [9] relative to the ground truth. The average performance across eight datasets was approximately 85% for mIoU and 70% for Boundary IoU, as shown in Table 1 and Table 2, indicating that patch-level predictions inherently limit performance. To address this, we propose to apply label propagation at the pixel level to further refine LPOSS predictions (img) . We assume pixel pi = (zi, pi) to be represented as tuple by its location pi within the image and its features zi, which may be based on the RGB values or any encoder. Similar to the patch-level case, we construct graph with pixels as nodes and adjacency matrix SP = Sa p. The position-based affinity is binary, with non-zero elements within the neighborhood of pixel, and the appearance-based affinity Sa has elements equal to saij = exp lposs (cid:16) (cid:17) zizj 2 τ . We apply label propagation on top of the pixel-level graph by , lposs Ylposs+ = Linv(SP)Y (img) (10) with (img) lposs+ RHW C, which, after argmax, becomes the final segmentation mask. This process refines the predictions such that neighboring pixels have similar predictions, while keeping the final prediction not far from the LPOSS predictions. We call this approach LPOSS+ and show its visualization in Figure 2. 4. Experiments 4.1. Datasets and evaluation metrics We evaluate our method on eight datasets: PASCAL VOC [16] (VOC), COCO Object [4] (Object), PASCAL Context [30] (Context), PASCAL Context59 [30] (C59), COCO Stuff [4] (Stuff), PASCAL VOC20 [16] (VOC20), ADE20k [51, 52], and Cityscapes [12] (City), that are commonly used [23, 26, 42, 46] to evaluate open-vocabulary 5 Method VM VOC Object Context Oracle (patch-level res.) MaskCLIP* [53] GEM [3] SCLIP [42] ClearCLIP [25] CLIP-DIY [45] CLIP-DINOiser* [46] ProxyCLIP* [26] LaVG* [23] LPOSS LPOSS+ - 91.5 32.9 46.8 59.1 51.8 59.9 62.2 59.3 61.8 61.1 62.4 83.8 16.3 - 30.5 33.0 31.0 34.7 36.3 33. 33.4 34.3 86.6 22.9 34.5 30.4 32.6 19.7 32.5 34.4 31.5 34.6 35.4 C59 86. 25.5 - 34.2 35.9 19.8 36.0 38.0 34.6 37.8 38.6 Stuff VOC20 ADE20k 86. 17.5 - 22.4 23.9 13.3 24.6 25.7 22.8 25.9 26.5 92.8 61.8 - 80.4 80.9 79.7 80.8 79.7 81.9 78.8 79.3 80. 14.2 17.1 16.1 16.7 9.9 20.1 19.4 14.8 21.8 22.3 City 73.6 25.0 - 32.2 30.0 11.6 31.1 36.0 25.0 37.3 37. Avg 85.2 27.0 - 38.2 38.1 30.6 40.2 41.1 38.2 41.3 42.1 Table 1. Performance comparison in terms of mIoU on 8 datasets using ViT-B/16 backbone for both VLM and VM. * denotes the methods for which we reproduce the performance. denotes numbers taken from Wysoczanska et al. [46]. Method VM VOC Object Context Oracle (patch-level res.) MaskCLIP [53] CLIP-DINOiser [46] ProxyCLIP [26] LaVG [23] LPOSS LPOSS+ - 80.1 17.2 47.4 45.0 49.5 48.5 51. 61.3 7.0 17.5 19.7 19.1 18.4 20.4 62.6 9.2 15.7 17.4 18.3 19.1 21. C59 68.0 12.0 22.4 23.0 23.6 24.8 27.1 Stuff VOC ADE20k 69.0 9.3 15.6 15.9 15.4 17.9 19.2 83.5 38.6 71.1 69.4 74. 70.1 71.7 65.0 8.2 12.3 12.9 9.2 14.9 16.0 City 66. 17.2 22.1 28.4 15.7 28.4 29.5 Avg 69.5 14.8 28.0 29.0 28.1 30.3 32. Table 2. Performance comparison in terms of Boundary IoU on 8 datasets using ViT-B/16 backbone both for VLM and VM. semantic segmentation. Following standard practice, we report mean Intersection over Union (mIoU) metric and additionally Boundary IoU [9] that measures performance only near the ground-truth mask boundaries. 4.2. Experimental setup Backbones and textual prompts. We report the results using ViT-B/16 backbones and use DINO [5] as visual model and OpenCLIP [10] as VLM (f, g). We follow the setup of CLIP-DINOiser [46] and use the output of DINOs last layers value embedding as the visual model features Zvm, while we use the output of MaskCLIP [53] as the VLM features Zvlm and . We use ImageNet prompt templates from CLIP [33] as templates for the VLM, following standard practice [23, 26, 46, 53]. For datasets that have class background, we use text expansion for it and expand background into, e.g. sky, wall, etc., following SCLIP [42], ProxyCLIP [26], and LaVG [23]. Compared methods We compare our method with methods that use only VLM, by making small changes during inference to the general ViT architecture, for open-vocabulary semantic segmentation: MaskCLIP [53], GEM [3], SCLIP [42], and ClearCLIP [25]. Additionally, we compare with methods that utilize vision models to aid VLMs: CLIP-DIY [45], CLIP-DINOiser [46]4, 4We use their trained model in our evaluation because it is publicly ProxyCLIP [26], and LaVG [23]. Implementation details We reproduce results for CLIPDINOiser5, ProxyCLIP6, and LaVG7 following their official implementations, while we reproduce MaskCLIP using the implementation provided by CLIP-DINOiser. We report the performance of CLIP-DIY [45] as reported in CLIP-DINOiser [46], while for GEM [3], SCLIP [42], and ClearCLIP [25] we report the numbers provided by the original papers. We implement our method using MMSegmentation [11] using the sliding-window approach with window size 224 224, window stride 112 112, and resizing the input image to have the shorter side of 448 for all datasets. We also fix the values of hyper-parameters α, k, γ, σ, r, and τ for LPOSS and LPOSS+ to 0.95, 400, 3.0, 100, 13, and 0.01, respectively, across all datasets. For pixel features of LPOSS+, we use image pixels converted to Lab color space. We discuss the design choices and show the impact of different hyper-parameters of LPOSS and LPOSS+ in Section 7 of the supplementary. 4.3. Results We present the quantitative results in Table 1 given by the conventionally used mIoU metric. LPOSS achieves stateavailable and performs similarly to the training-free variant. 5https://github.com/wysoczanska/clip_dinoiser 6https://github.com/mc-lan/ProxyCLIP 7https://github.com/dahyun-kang/lavg 6 Image GT MaskCLIP CLIP-DINOiser ProxyCLIP LaVG LPOSS+ 9.7/4.0 85.6/42.5 56.2/25.8 36.3/19.7 87.5/49.0 3.2/1. 75.3/41.7 30.4/18.5 30.9/25.2 72.7/47.7 5.9/3.9 28.1/17. 25.2/19.0 43.6/31.2 45.3/36.2 5.7/2.2 41.2/20.2 13.9/8. 15.3/8.2 42.7/23.1 19.4/13.2 30.6/19.5 38.8/25.5 18.1/10. 34.5/25.0 Figure 4. Qualitative comparison of open-vocabulary semantic segmentation. comparison of LPOSS+ with the best performing methods: MaskCLIP [53], CLIP-DINOiser [46], ProxyCLIP [26], and LaVG [23]. On top of each segmentation map we show its mIoU/Boundary IoU. Pixels shown in white are pixels that do not have class in the ground-truth. of-the-art performance on average, with competitive performance across most datasets with only few notable exceptions. One such exception happens on the VOC20 dataset where LPOSS has low performance compared to others. We observe this is the case because of the choice of the window size, something that we discuss in other experiments. LPOSS+ further improves the results of LPOSS by 0.8% on average and obtains state-of-the-art performance on all datasets besides VOC20 and Object. ProxyCLIP is the only method that significantly outperforms LPOSS+ on the Objects dataset, something that we attribute to the fact that it employs dataset-specific thresholds to filter the background class in this case, as well as for the VOC dataset. Boundary IoU metric. Based on Table 1 we can already see that going beyond patch-level predictions in LPOSS to pixel-level predictions in LPOSS+ improves the results. Improvements are even more visible when we look into the Boundary IoU [9] metric, presented in Table 2, which is representative of how well the predicted object boundaries match the ground truth. LPOSS+ outperforms all other methods and achieves state-of-the-art on almost all datasets. Interestingly, even LPOSS outperforms competitors in this metric by large margin. We attribute this to our choice of the window size, as LPOSS achieves Boundary IoU of 28.9% on average when we apply it with window size 448 448 and window stride 224 224, which is comparable to the other approaches. Additionally, in Table 1 and Table 2, we report mIoU and Boundary IoU, respectively, for the oracle-based experiment for patch-level resolution prediction. In this experiment, we downsample the ground-truth map to the patchlevel resolution by factor equal to the patch size and upsample it to the original image resolution, which we treat as predictions. Both mIoU and especially Boundary IoU are significantly impacted by this process, which shows that patch-level predictions are significantly limiting the segmentation performance. Qualitative results. We present the qualitative results of LPOSS+ and the most important competitors in Figure 4. We observe that LPOSS+ obtains good results across vaMethod No sliding windows CLIP-DINOiser [46] ProxyCLIP [26] LaVG [23] LPOSS LPOSS+ Window size: 448 448, window stride: 224 224 CLIP-DINOiser [46] ProxyCLIP [26] LaVG [23] LPOSS LPOSS+ Window size: 224 224, window stride: 112 112 CLIP-DINOiser [46] ProxyCLIP [26] LaVG [23] LPOSS LPOSS+ Ensemble of two setups above CLIP-DINOiser [46] ProxyCLIP [26] LaVG [23] LPOSS LPOSS+ VOC Object Context C59 Stuff VOC20 ADE20k City Avg 60.2 43.1 24.3 59.2 59.7 62.2 59.9 36.7 61.3 62.0 55.4 54.9 62.0 61.1 62.4 63.0 59.0 51.4 62.9 63. 32.4 24.2 16.1 31.2 31.7 34.7 36.0 19.6 32.5 33.1 33.0 32.9 32.3 33.4 34.3 35.9 35.1 27.3 34.1 35.0 31.3 25.9 12.7 31.4 32.0 32.5 34.6 16.4 32.9 33. 32.2 33.2 31.8 34.6 35.4 34.1 35.3 24.3 35.1 35.8 34.6 28.8 13.7 34.4 35.0 36.0 38.2 18.7 36.3 36.9 35.7 36.4 34.7 37.8 38.6 37.8 38.9 28.4 38.6 39. 23.2 18.7 10.0 23.3 23.6 24.6 25.6 13.8 25.2 25.5 24.8 24.3 22.8 25.9 26.5 26.3 26.1 20.1 26.5 27.0 79.1 73.1 32.5 80.7 81.3 80.8 78.9 44.3 82.8 83. 73.1 73.0 79.8 78.8 79.3 79.6 78.1 63.7 82.1 82.5 18.7 13.6 8.2 19.0 19.3 20.1 18.9 10.2 20.4 20.7 21.1 18.6 15.4 21.8 22.3 22.1 19.8 14.7 22.2 22. 28.2 13.6 6.8 28.4 28.5 31.1 33.6 10.8 31.8 32.1 36.3 36.0 23.1 37.3 37.9 35.9 36.8 20.3 36.5 37.0 38.5 30.1 15.6 38.4 38.9 40.2 40.7 21.3 40.4 40. 38.9 38.7 37.7 41.3 42.1 41.8 41.1 31.3 42.3 42.9 Table 3. Impact of sliding windows for feature extraction. Feature extraction is performed with windows of different size/stride or no windows at all. Following the original design choices, other methods perform window-based prediction, while LPOSS and LPOSS+ perform prediction jointly across all windows. Image resize of the smaller side to 448 is used and the reported metric is mIoU. riety of images coming from different benchmark datasets. Qualitatively, our output is closer to that of CLIP-DINOiser due to the similarity of the approach. Nevertheless, ours varies in smoother way, boosting the performance, which indicates that our LP-based formulation is key ingredient. Additionally, in Figure 1, we show the qualitative comparison of LPOSS and LPOSS+. We observe that predictions of LPOSS+ follow the object boundary much better than the predictions from LPOSS. Impact of sliding windows. In Table 3, we show the impact of sliding windows. All methods are negatively impacted when we depart from the use of sliding windows, with ProxyCLIP and LaVG being significantly more impacted than others. The largest drop in the performance happens on the Cityscapes dataset, which has an aspect ratio that is furthest from the square aspect ratio that was used to train VLM and VM. Besides that, CLIP-DINOiser and ProxyCLIP perform better when used with larger sliding windows, while LaVG, LPOSS, and LPOSS+ prefer smaller windows. We attribute LPOSSs and LPOSS+s preference for smaller windows to the fact that we decouple prediction and feature extraction. We are thus able to select (smaller) window size close to the training resolution to get stronger features, but are still able to propagate labels over the full image. Related methods lose performance in this setting because the prediction happens with the smaller context. Additionally, based on the analysis in Section 6 of the supplementary we propose to ensemble the two different window sizes. We observe that all methods, besides LaVG, benefit from the ensemble. However, LPOSS and LPOSS+ benefit more than others and obtain state-of-the-art results. LPOSS++ as refinement step for linear semantic segmentation. LPOSS+ refines pixel-level predictions and, as such, could potentially be applied to other dense prediction tasks. To test this, we apply it on the predictions obtained by training linear segmentation head on top of DINOv2 [32] features on the ADE20k dataset [51, 52]. Applying LPOSS+ out of the box, we are able to improve DINOv2 predictions and increase mIoU from 47.3% to 49.5%. 5. Conclusion This work proposes training-free method for openvocabulary segmentation, called LPOSS, that uses ViTbased VLMs and VMs. We highlight the limitations of lowresolution patch-based predictions and window-based processing. LPOSS targets both issues and provides improvements through an LP-based predictor that operates jointly over patches of all windows, and over pixels. In comprehensive benchmark across variety of datasets and domains, LPOSS surpasses the previous state-of-the-art, especially when evaluating near class boundaries. 8 Acknowledgements. This work was supported by the Junior Star GACR GM 21-28830M and the Czech Technical University in Prague grant No. SGS23/173/OHK3/3T/13. We acknowledge VSB Technical University of Ostrava, IT4Innovations National Supercomputing Center, Czech Republic, for awarding this project access to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium through the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (grant ID: 90254)."
        },
        {
            "title": "References",
            "content": "[1] Nikita Araslanov and Stefan Roth. Single-stage semantic segmentation from image labels. In CVPR, 2020. 14 [2] Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux. Label propagation and quadratic criterion. In Semi-Supervised Learning, pages 192216. The MIT Press, 2006. 3 [3] Walid Bousselham, Felix Petersen, Vittorio Ferrari, and Hilde Kuehne. Grounding everything: Emerging localizaIn CVPR, tion properties in vision-language transformers. 2024. 2, 6 [4] Holger Caesar, Jasper R. R. Uijlings, and Vittorio Ferrari. In CVPR, Coco-stuff: Thing and stuff classes in context. 2018. [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. EmergIn ing properties in self-supervised vision transformers. ICCV, 2021. 2, 3, 4, 6, 14 [6] Junbum Cha, Jonghwan Mun, and Byungseok Roh. Learning to generate text-grounded mask for open-world semantic segmentation from only image-text pairs. In CVPR, 2023. 2 [7] Siddhartha Chandra and Iasonas Kokkinos. Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs. In ECCV, 2016. 3 [8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE TPAMI, 2018. 3 [9] Bowen Cheng, Ross Girshick, Piotr Dollar, Alexander C. Berg, and Alexander Kirillov. Boundary IoU: Improving In CVPR, object-centric image segmentation evaluation. 2021. 2, 5, 6, 7 [10] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In CVPR, 2023. 1, 2, 6 [11] MMSegmentation Contributors. MMSegmentation: and https : / / github . com / open - segmentation toolbox semantic Openmmlab benchmark. mmlab/mmsegmentation, 2020. 6 [12] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 5 [13] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In ICLR, 2024. [14] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. Decoupling zero-shot semantic segmentation. In CVPR, 2022. 2 [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1, 3 [16] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. 5 [17] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In ECCV, 2022. 2 [18] Xuming He, Richard S. Zemel, and Miguel A. CarreiraPerpinan. Multiscale conditional random fields for image labeling. In CVPR, 2004. 3 [19] Xuefeng Hu, Ke Zhang, Lu Xia, Albert Chen, Jiajia Luo, Yuyin Sun, Ken Wang, Nan Qiao, Xiao Zeng, Min Sun, Cheng-Hao Kuo, and Ram Nevatia. Reclip: Refine contrastive language image pre-training with source free domain adaptation. In WACV, 2024. [20] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon, and Ondrej Chum. Efficient diffusion on region manifolds: Recovering small objects with compact CNN representations. In CVPR, 2017. 3 [21] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-supervised learning. In CVPR, 2019. 3, 11 [22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. 1, 2 [23] Dahyun Kang and Minsu Cho. grounding for open-vocabulary semantic segmentation. ECCV, 2024. 2, 4, 5, 6, 7, 8, 12, 14,"
        },
        {
            "title": "In defense of lazy visual\nIn",
            "content": "[24] Philipp Krahenbuhl and Vladlen Koltun. Efficient inference in fully connected crfs with gaussian edge potentials. In NeurIPS, 2011. 3, 14 [25] Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang. Clearclip: Decomposing clip representations for dense vision-language inference. In ECCV, 2024. 2, 6 [26] Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang. Proxyclip: Proxy attention improves clip for open-vocabulary segmentation. In ECCV, 2024. 1, 2, 4, 5, 6, 7, 8, 12, 14, 15 [27] Yi Li, Hualiang Wang, Yiqun Duan, Hang Xu, and Xiinterpretability for conarXiv preprint aomeng Li. trastive language-image pre-training. arXiv:2209.07046, 2022."
        },
        {
            "title": "Exploring visual",
            "content": "[28] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana 9 [42] Feng Wang, Jieru Mei, and Alan Yuille. Sclip: Rethinking self-attention for dense vision-language inference. In ECCV, 2024. 1, 2, 4, 5, 6 [43] Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, Henghui Ding, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong, Xudong Jiang, Bernard Ghanem, and Dacheng Tao. Towards IEEE Trans. Pattern open vocabulary learning: survey. Anal. Mach. Intell., 46(7):50925113, 2024. 2 [44] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy. Aligning bag of regions for openvocabulary object detection. In CVPR, 2023. 2 [45] Monika Wysoczanska, Michael Ramamonjisoa, Tomasz Trzcinski, and Oriane Simeoni. CLIP-DIY: CLIP dense inference yields open-vocabulary semantic segmentation forfree. In WACV, 2024. 2, [46] Monika Wysoczanska, Oriane Simeoni, Michael Ramamonjisoa, Andrei Bursuc, Tomasz Trzcinski, and Patrick Perez. Clip-dinoiser: Teaching clip few dino tricks for openIn ECCV, 2024. 1, 2, vocabulary semantic segmentation. 3, 4, 5, 6, 7, 8, 12, 14, 15 [47] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas M. Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In CVPR, 2022. 2 [48] Jilan Xu, Junlin Hou, Yuejie Zhang, Rui Feng, Yi Wang, Yu Qiao, and Weidi Xie. Learning open-vocabulary semantic segmentation models from natural language supervision. In CVPR, 2023. 2 [49] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything V2. arXiv preprint arXiv:2406.09414, 2024. 12 [50] Shuai Zheng, Sadeep Jayasumana, Bernardino RomeraParedes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip H. S. Torr. Conditional random fields as recurrent neural networks. In ICCV, 2015. 3 [51] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Scene parsing through Barriuso, and Antonio Torralba. ADE20K dataset. In CVPR, 2017. 5, [52] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ADE20K dataset. IJCV, 2019. 5, 8 [53] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from CLIP. In ECCV, 2022. 2, 4, 6, 7, 15 [54] Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Scholkopf. Learning with local and global consistency. In NeurIPS, 2003. 2, 3 Marculescu. Open-vocabulary semantic segmentation with mask-adapted CLIP. In CVPR, 2023. 2 [29] Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He, and Tianrui Li. Segclip: Patch aggregation with learnable centers for open-vocabulary semantic segmentation. In ICML, 2023. 2 [30] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan L. Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, 2014. 5 [31] Jishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang, Ashish Shah, Philip H. S. Torr, and Ser-Nam Lim. Open vocabulary semantic segmentation with patch aligned contrastive learning. In CVPR, 2023. 2 [32] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. TMLR, 2024. 2, [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 2, 3, 6 [34] Kanchana Ranasinghe, Brandon McKinzie, Sachin Ravi, Yinfei Yang, Alexander Toshev, and Jonathon Shlens. Perceptual grouping in contrastive vision-language models. In ICCV, 2023. 2 [35] Jamie Shotton, John M. Winn, Carsten Rother, and Antonio Criminisi. TextonBoost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation. In ECCV, 2006. 3 [36] Oriane Simeoni, Gilles Puy, Huy V. Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Perez, Renaud Marlet, and Jean Ponce. Localizing objects with self-supervised transformers and no labels. In BMVC, 2021. 2, 4 [37] Oriane Simeoni, Chloe Sekkat, Gilles Puy, Antonın Vobecky, Eloi Zablocki, and Patrick Perez. Unsupervised object localization: Observing the background to discover objects. In CVPR, 2023. 2, 4 [38] Vladan Stojnic, Yannis Kalantidis, and Giorgos Tolias. Label propagation for zero-shot classification with vision-language models. In CVPR, 2024. 2, 3, [39] Boyuan Sun, Yuqi Yang, Le Zhang, Ming-Ming Cheng, and Qibin Hou. Corrmatch: Label propagation via correlation In matching for semi-supervised semantic segmentation. CVPR, 2024. 3 [40] Charles Sutton and Andrew McCallum. An introduction to conditional random fields. Found. Trends Mach. Learn., 2012. 3 [41] Vishaal Udandarao, Ankush Gupta, and Samuel Albanie. Sus-x: Training-free name-only transfer of vision-language models. In ICCV, 2023. 2 10 LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Analysis of performance 6.3. The impact of window size 6.1. Quantitative analysis per image In Figure 5a and Figure 5b, we present the comparison of mIoU and Boundary IoU performance of LPOSS and LPOSS+. We observe that LPOSS+ consistently improves the performance of LPOSS with respect to both metrics. Figure 5c presents the comparison of mIoU and Boundary IoU performance of LPOSS+. We see that although the two metrics are correlated to some extent, they are still complementary to each other. Boundary IoU performance is usually lower than mIoU performance, which is consistent with the definition that Boundary IoU measures the finegrained performance at segment borders, which consist of the hardest pixels to classify. We present the comparison of the mIoU performance of the oracle experiment and LPOSS in Figure 5d. We observe that oracle performance varies lot across images, which we attribute to the differences in object size and shape. Additionally, the oracle performance acts as an upper bound in the majority of cases. Exceptions are justified by the effect of bilinear interpolation and combining predictions across many windows. 6.2. Per image/class comparison with MaskCLIP LPOSS refines MaskCLIP predictions, so we look at how successful LPOSS is in improving these predictions. Figure 6 shows the comparison of the mIoU for LPOSS and MaskCLIP on the image and class level. We observe that LPOSS successfully improves MaskCLIP predictions in the vast majority of cases. Rarely, an image, with already very low performance, is further harmed if the MaskCLIP output is spatially very noisy, as shown in Figure 7. Furthermore, we look into the impact of window size on CLIP-DINOiser and LPOSS. In Figure 8, we visualize the results of both methods as well as MaskCLIP using two different window sizes. We observe that there are some cases when MaskCLIP has better output for the large windows, used by CLIP-DINOiser, vs the small ones, used by LPOSS. In these cases, CLIP-DINOiser outperforms LPOSS. We also observe that there are cases where LPOSS performs well for both large and small window sizes, while CLIPDINOiser performs better for the larger window size. Based on this, we propose to run our methods using an ensemble of large and small window sizes, and observe that this further improves the performance of LPOSS and LPOSS+, as presented in Section 4.3. 7. Ablation study Design of Sa and Sp. We construct Sa as k-nearest neighbor graph with edge weights in the form sγ, as it is common choice for the adjacency matrix in the label propagation literature [21, 38]. Another choice could be exp ( 1s σ ), which we found to perform slightly worse for LPOSS (0.1%). For Sp, we chose the RBF kernel as in CRFs and bilateral filters. We further tested linear kernel, which performs bit worse (0.2%). LPOSS and LPOSS+ construct adjacency matrices SP and SP in different fashion. The primary difference comes from the way they control the sparsity of the graph. LPOSS uses k-nearest neighbors on top of the appearance-based adjacency Sa. On the other hand, LPOSS+ controls the sparsity using binary spatial affinity that has nonzero elements only within the neighborhood of the pixel. This difference is motivated by two reasons. First, (a) mIoU comparison of LPOSS and LPOSS+ (b) Boundary IoU comparison of LPOSS and LPOSS+ (c) Comparison between mIoU and Boundary IoU for LPOSS+ (d) mIoU comparison of Oracle (patch-level res.) and LPOSS Figure 5. Analysis of LPOSS and LPOSS+ performance per image. The plot shows 5000 randomly selected test images from all eight datasets used in the paper. Method Sa Sp LPOSS DINO CLIP LPOSS LPOSS DINO LPOSS DINO LPOSS+ DINO LPOSS+ CLIP LPOSS+ DINO LPOSS+ DINO LPOSS+ DINO LPOSS+ DINO Coupled feat. extraction and prediction Pix. feature Avg - - - - 41.3 38.3 40.6 39.2 Lab Lab Lab Lab depth 42.1 39.0 41.4 39.8 42.2 Lab+depth 42. Table 4. Ablations for LPOSS and LPOSS+. We report mIoU averaged across 8 datasets. Default setups used in the paper are marked with . the proposed method (achieving the same performance as the proposed method when the RBF kernel converges to the proposed binary values). Features used in adjacency Sa. We replace DINO features for the construction of Sa with CLIP features and observe significant drop in performance for both proposed methods, as shown in Table 4. Impact of spatial adjacency Sp. We remove the use of Sp in the construction of SP , i.e. set it to matrix full of ones, and observe, as shown in Table 4, that using Sp improves the results both for LPOSS and LPOSS+. Additionally, in Figure 9, we visualize the impact of Sp on the predictions. We observe that the use of Sp cleans the predictions. Impact of window-based predictions. We perform an experiment where we do feature extraction per window, as always, but also prediction per sliding window, as other methods do too [23, 26, 46]. We observe, as shown in Table 4 that our choice of performing prediction jointly across all windows is indeed beneficial to the performance of LPOSS and LPOSS+. Pixel features. We switch pixel features from the default choice of Lab color space to depth predictions from DepthAnythingV2 [49] and their combination. We observe, as shown in Table 4, that using predicted depth marginally improves the results, but we opt not to use it in our default setup as it introduces another model during inference. We conclude that the use of different pixel-level features is worth future exploration. Impact of hyper-parameters k, σ, and r. In Figure 10, we show the impact of hyper-parameters k, σ, and on the performance. Good performance is achieved over wide range of values. For LPOSS+, controls the performance/speed trade-off (via sparsity), and we found = 13 to be good compromise. Figure 6. Comparison of MaskCLIP and LPOSS performance per image and per class. The plot shows 5000 randomly selected test images and all classes from all eight datasets in our experiments. with LPOSS+, we aim to refine the predictions around the boundaries, which can be accomplished by looking at the neighborhood of each pixel. Second, the color-based features used in LPOSS+ can be very noisy, which can create issues for k-nearest neighbor search. To validate this, we try implementing LPOSS+ using the functions of LPOSS. However, we have found that this makes LPOSS+ ineffective and actually produces performance results worse than LPOSS. Additionally, we also experiment with using an RBF kernel in and find that it performs bit worse than Image(GT)"
        },
        {
            "title": "LPOSS",
            "content": "Figure 7. Examples of the LPOSS failure cases that are due to the MaskCLIP noisy predictions. MaskCLIP produces very spatially noisy predictions in some cases, which then translates to the bad performance of LPOSS. Pixels shown in white are pixels that do not have class in the ground-truth. 12 Image(GT) MaskCLIP@448 MaskCLIP@224 CLIP-DINOiser@448 CLIP-DINOiser@ LPOSS@448 LPOSS@224 Figure 8. Impact of the window size on different methods. The top rows show examples where both CLIP-DINOiser and LPOSS benefit from the large window size. The bottom rows show examples where LPOSS works well for both window sizes while CLIP-DINOiser performs better for the larger window size. Pixels shown in white are pixels that do not have class in the ground-truth. Default setup of each method is shown in bold. Image(GT)"
        },
        {
            "title": "MaskCLIP",
            "content": "LPOSS w/o Sp"
        },
        {
            "title": "LPOSS w Sp",
            "content": "Figure 9. Impact of the spatial adjacency Sp. comparison of segmentation maps of LPOSS when applied without or with Sp. Pixels shown in white are pixels that do not have class in the ground-truth. 13 8. Additional results 8.1. Method comparison using different VM. Throughout the paper, we use DINO [5] with ViT-B/16 backbone as VM. However, here we show that LPOSS and LPOSS+ can be applied with other VM backbones as well. Concretely, we use DINO [5] with ViT-B/8 backbone and DINOv2 [5] with ViT-B/14 backbone. Considering that the VM now uses patch size different from that of VLM, the feature vectors coming from the VM and VLM are of different size. To apply LPOSS in such situation, we upsample the VLM features to match the size of VM features. We present the results of this experiment in Table 5 and compare the results of LPOSS and LPOSS+ with ProxyCLIP [26] and LaVG [23], which also report the performance of such experiments. DINO with ViT-B/8. For DINO with ViT-B/8, we observe that both LPOSS and LPOSS+ outperform LaVG, while ProxyCLIP outperforms LPOSS and performs on par with LPOSS+. However, ProxyCLIP again significantly outperforms LPOSS and LPOSS+ only on Object and VOC20 datasets, for which we provide an explanation in Section 4.3. We also observe that going to pixel-level predictions in LPOSS+ improves the results even when the patch size is as small as 8. Additionally, we observe that after using ViT-B/8 backbone for the VM, the graph defined by S(W) in (8) has four times as many nodes compared to the default setup of using ViT-B/16 backbone, with the increase coming from the fact that there are four times more feature vectors for each sliding-window. So we propose to just increase the value of the hyper-parameter from 400 to 800, while keeping all other hyper-parameters fixed, to allow better connectivity between nodes coming from different sliding windows. With this change, we observe that LPOSS performs on par with ProxyCLIP, while LPOSS+ outperforms it, as shown in Table 5. DINOv2 with ViT-B/14. For DINOv2, we observe that LPOSS and LPOSS+ perform slightly worse than for the case of using DINO with ViT-B/16. However, LPOSS+ still outperforms ProxyCLIP, while LPOSS performs on par with it. We note that due to the very different distribution of similarities coming from DINOv2, compared with DINO, we use value of γ = 7.0 for LPOSS and LPOSS+. 8.2. Complementarity with other methods. We build LPOSS and LPOSS+ by applying them on top of initial predictions coming from the VLM; in particular as MaskCLIP computes them. However, these initial predictions can come from any model, so here we show that we can apply them on top of CLIP-DINOiser [46], by simply using YDINOiser instead of Yvlm. Compared to the main paper experiments, we also use the sliding window setup used in CLIP-DINOiser, as it significantly improves CLIPDINOiser performance as shown in Table 3. We present the results of these experiments in Table 6. We observe that LPOSS and LPOSS+ are complementary to CLIP-DINOiser and that they can further improve its performance. Additionally, we show that using better initialization of CLIP-DINOiser compared to MaskCLIP improves LPOSS and LPOSS+ results as well. 8.3. LPOSS+ vs. other post-processing methods. We compare LPOSS+ with two other post-processing pixel refinement methods, PAMR [1] and DenseCRF [24] by applying them on top of LPOSS predictions. PAMR, with 5 iterations and dilations 8 and 16, achieves 41.8 mIoU (vs. 42.1 of LPOSS+) while DenseCRF was unable to improve LPOSS at all. 9. Computation requirements We measure the average time necessary to perform the inference per image on the VOC20 dataset using an NVIDIA A100 GPU. LPOSS processes each image in under 0.1s, which is comparable to all methods except LAVG (6.5s). LPOSS+ for pixel-level post-processing takes 0.5s/image, comparable to or faster than pixel-level post-processing methods PAMR [1] (0.5s) or DenseCRF [24] (1.6sec). Note that LPOSS+ speed can be controlled via hyper-parameter (see Figure 10). 41 40 m 38 LPOSS 800 50 200 400 41 40 39 38 LPOSS LPOSS+ LPOSS+ time 42 40 LPOSS ] [ t 400 200 50 100 200 σ 400 3 5 7 11 13 15 Figure 10. Impact of hyper-parameters k, σ, and r. We report mIoU averaged across 8 datasets. Default setups in the paper are marked with . 14 VM VOC Object Context C59 Stuff VOC20 ADE20k Method ProxyCLIP* [26] LaVG* [23] LPOSS LPOSS+ ProxyCLIP [26] LaVG [23] LPOSS LPOSS (k = 800) LPOSS+ LPOSS+ (k = 800) DINO(ViT-B/16) DINO(ViT-B/16) DINO(ViT-B/16) DINO(ViT-B/16) DINO(ViT-B/8) DINO(ViT-B/8) DINO(ViT-B/8) DINO(ViT-B/8) DINO(ViT-B/8) DINO(ViT-B/8) ProxyCLIP [26] DINOv2(ViT-B/14) LPOSS (γ = 7.0) LPOSS+ (γ = 7.0) DINOv2(ViT-B/14) DINOv2(ViT-B/14) 59.3 61. 61.1 62.4 61.3 62.1 61.4 62.2 62.2 63.0 58.6 59.7 60.8 36.3 33. 33.4 34.3 37.5 34.2 33.5 34.1 34.2 34.8 37.4 33.3 34.3 34.4 31. 34.6 35.4 35.3 31.6 34.9 35.2 35.5 35.8 33.8 34.3 35.1 38.0 34. 37.8 38.6 39.1 34.7 38.2 38.5 38.9 39.1 37.2 37.5 38.3 25.7 22. 25.9 26.5 26.5 23.2 26.3 26.4 26.8 26.8 25.4 25.6 26.2 79.7 81. 78.8 79.3 80.3 82.5 77.6 78.7 78.0 79.0 83.0 80.0 80.4 19.4 14. 21.8 22.3 20.2 15.8 22.6 22.5 23.0 22.8 19.7 21.9 22.4 City 36.0 25.0 37.3 37.9 38.1 26.2 40.2 39.4 40.2 39.3 33.9 36.0 36. Avg 41.1 38.2 41.3 42.1 42.3 38.8 41.8 42.1 42.3 42.6 41. 41.0 41.8 Table 5. Performance comparison in terms of mIoU on 8 datasets using ViT-B/16 backbone for VLM and DINO ViT-B/8, DINO . * denotes ViT-B/16, or DINOv2 ViT-B/14 backbone for VM. Default setups of hyper-parameters used in the paper are marked with the methods for which we reproduce the performance. Method VOC Object Context C59 Stuff VOC20 ADE20k MaskCLIP* [53] LPOSS (MaskCLIP) LPOSS+ (MaskCLIP) CLIP-DINOiser* [46] LPOSS (CLIP-DINOiser) LPOSS+ (CLIP-DINOiser) 32.9 61.1 61.8 62.2 65.0 66.1 16.3 32.5 33.2 34.7 36.3 36.8 22.9 32.9 33.4 32.5 32.9 33. 25.5 36.3 36.9 36.0 36.6 37.2 17.5 25.2 25.6 24.6 25.1 25.4 61.8 82.8 83.1 80.8 84.0 84. 14.2 20.4 20.7 20.1 19.7 19.9 City 25.0 31.7 31.9 31.1 29.3 29.5 Avg 27.0 40.4 40.8 40.2 41.1 41.6 Table 6. Performance comparison in terms of mIoU on 8 datasets applying LPOSS and LPOSS+ on top of MaskCLIP (default choice in the main paper) or CLIP-DINOiser [46]. ViT-B/16 backbone is used both for VLM and VM. Sliding-window size 448 448 and stride 224 224 as used in CLIP-DINOiser. * denotes the methods for which we reproduce the performance."
        }
    ],
    "affiliations": [
        "NAVER LABS Europe",
        "VRG, FEE, Czech Technical University in Prague"
    ]
}