{
    "paper_title": "Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation",
    "authors": [
        "Quanting Xie",
        "So Yeon Min",
        "Tianyi Zhang",
        "Kedi Xu",
        "Aarav Bajaj",
        "Ruslan Salakhutdinov",
        "Matthew Johnson-Roberson",
        "Yonatan Bisk"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "There is no limit to how much a robot might explore and learn, but all of that knowledge needs to be searchable and actionable. Within language research, retrieval augmented generation (RAG) has become the workhouse of large-scale non-parametric knowledge, however existing techniques do not directly transfer to the embodied domain, which is multimodal, data is highly correlated, and perception requires abstraction. To address these challenges, we introduce Embodied-RAG, a framework that enhances the foundational model of an embodied agent with a non-parametric memory system capable of autonomously constructing hierarchical knowledge for both navigation and language generation. Embodied-RAG handles a full range of spatial and semantic resolutions across diverse environments and query types, whether for a specific object or a holistic description of ambiance. At its core, Embodied-RAG's memory is structured as a semantic forest, storing language descriptions at varying levels of detail. This hierarchical organization allows the system to efficiently generate context-sensitive outputs across different robotic platforms. We demonstrate that Embodied-RAG effectively bridges RAG to the robotics domain, successfully handling over 200 explanation and navigation queries across 19 environments, highlighting its promise for general-purpose non-parametric system for embodied agents."
        },
        {
            "title": "Start",
            "content": "Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation Quanting Xie1,, So Yeon Min1,, Tianyi Zhang1, Kedi Xu1, Aarav Bajaj1, Ruslan Salakhutdinov1, Matthew Johnson-Roberson1, and Yonatan Bisk1 1 4 2 0 2 8 ] . [ 4 3 1 3 8 1 . 9 0 4 2 : r AbstractThere is no limit to how much robot might explore and learn, but all of that knowledge needs to be searchable and actionable. Within language research, retrieval augmented generation (RAG) has become the workhouse of large-scale non-parametric knowledge, however existing techniques do not directly transfer to the embodied domain, which is multimodal, data is highly correlated, and perception requires abstraction. To address these challenges, we introduce Embodied-RAG, framework that enhances the foundational model of an embodied agent with non-parametric memory system capable of autonomously constructing hierarchical knowledge for both navigation and language generation. Embodied-RAG handles full range of spatial and semantic resolutions across diverse environments and query types, whether for specific object or holistic description of ambiance. At its core, Embodied-RAGs memory is structured as semantic forest, storing language descriptions at varying levels of detail. This hierarchical organization allows the system to efficiently generate context-sensitive outputs across different robotic platforms. We demonstrate that Embodied-RAG effectively bridges RAG to the robotics domain, successfully handling over 200 explanation and navigation queries across 19 environments, highlighting its promise for generalpurpose non-parametric system for embodied agents. Index TermsAutonomous Agents, Autonomous Vehicle Navigation, AI-Enabled Robotics I. INTRODUCTION Humans excel as generalist embodied agents in part due to our ability to build, abstract, and reason over rich memories. We seamlessly log experiences at appropriate levels of detail and retrieve information ranging from specific facts to holistic impressions, allowing us to respond to diverse requests across different contexts. In contrast, current embodied agents [1] [4] lack such versatile memory capabilities, limiting their ability to operate effectively in unbounded and complex realworld environments. While existing methods such as semantic mapping [1], [2] and scene graphs [5], [6] attempt to capture spatial and contextual relationships, they largely fall short of the dynamic and flexible memory, retrieval, and generative abilities exhibited by humans. In the language domain, foundation models combined with non-parametric memory mechanisms have achieved near This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Agreement No. HR00112490375 and partially supported by funding from Lockheed Martin Corporation. 1Quanting Xie, So Yeon Min, Tianyi Zhang, Kedi Xu, Aarav Bajaj, Ruslan Salakhutdinov, Matthew Johnson-Roberson, and Yonatan Bisk are with Carnegie Mellon University, Pittsburgh, PA 15213, USA quantinx@andrew.cmu.edu, soyeonm@andrew.cmu.edu * Equal contribution. https://quanting-xie.github.io/Embodied-RAG-web/ human-level performance across various tasks. RetrievalAugmented Generation (RAG) [7][9] has been widely adopted in the field of Natural Language Processing (NLP) as non-parametric memory mechanism over large document corpora, enhancing the accuracy and relevance of responses generated by Large Language Models (LLMs). Similarly, the continuous stream of experiences gathered by embodied agents forms vast databases that exceed the context window limitations of LLMs. To address this, approaches like RAG are essential for enabling human-like embodied agents to operate effectively in large, dynamic environments. By integrating non-parametric memory, foundation models within robots can store and retrieve diverse range of experiences, enhancing their ability to navigate and respond in real-world scenarios. However, applying RAG to embodied scenarios presents unique challenges due to key differences between textual data and embodied experiences. First, while RAG relies on existing documents, building memory from embodied experiences is itself core research challenge. Current methods, such as dense point clouds or scene graphs, fail to capture the full range of experiences beyond object-level attributes, without relying on human-engineered schemas or exceeding memory budgets. Second, unlike documents, embodied experiences have inherent correlated structure semantically similar objects are often spatially correlated and hierarchically organized so embodied experiences should not be treated as independent samples. Finally, embodied observations vary in granularity and structure: outdoor scenes might be sparse, while indoor environments are cluttered, and repeated objects across frames can confuse LLMs, complicating retrieval. To bridge this gap, we present Embodied-RAG. EmbodiedRAG has two components, Memory Construction (Fig. 2(a)) and Retrieval and Generation (Fig. 2(b c)). During Memory Construction, the system autonomously builds topological map for low-level navigation and hierarchical semantic forest without relying on hand-crafted constraints or features. This forest is organized based on spatial correlations between hierarchical nodes, each containing language descriptions of observations, and can be expanded to handle temporal or multi-modal inputs. Root nodes represent global explanations, leaf nodes capture specific object arrangements, and intermediate nodes reflect various mid-level scales. EmbodiedRAG allows retrieval at various levels of abstraction in the language query (explicit, implicit, global), matching it with the spatial/semantic resolution (local, intermediate, global) of the memory (Fig. 2(b)/(c), Fig. 3). In the Retrieval and Generation process, to mitigate perceptual hallucinations from 2 Fig. 1: Overview: Our goal is for robots to navigate and communicate effectively in any environment where humans are present. We introduce Embodied-RAG, framework for automatically building hierarchical spatial memory and providing both explanations and navigation across multiple levels of query abstraction. Embodied-RAG supports robotic operations regardless of the querys abstraction level, the platform, or the environment. semantic similarity searches, Embodied-RAG incorporates robust reasoning component. This involves parallelized tree traversals scored by language model, with retrieved results structured and used as context for generating explanations or navigational actions via an LLM. To evaluate the performance of Embodied-RAG, we developed an Embodied-RAG Benchmark, which consists of queries that require multimodal outputs (navigational waypoints and text responses) and reasoning (implicit questions and global summaries). Across over 200 benchmark tasks, we compared Embodied-RAG with two other non-parametric memory baselines: Semantic Match and vanilla RAG. We found that our method serves as an initial step toward solving the problems mentioned above in applying non-parametric memory to embodied agents, showing superior performance against these baselines on the Embodied-RAG Benchmark in the following aspects: (1) More robust against object detection errors on explicit queries (direct object retrieval) since it leverages hierarchical spatial relevancyfor example, recognizing that toothbrush is more likely found in bathroom; (2) Improved reasoning on implicit queries (indirect object retrieval), achieving 220% improvement over Semantic Match and 30% relative improvement over RAG; (3) Generating more accurate global summarization and trend analysis within the environment, where Semantic Match is unsupported and RAG shows poor quality. Furthermore, our experiments demonstrate that this pipeline is versatile and applicable across various practical forms of embodiment (drones, locobots, quadrupeds) and can be seamlessly integrate with existing low-level autonomous navigation pipelines. This highlights Embodied-RAGs potential as general system capable of task-, environment-, and platformagnostic operation, enabling robots to effectively navigate and communicate in any environment where humans are present. The key contributions and implications of this paper include: Method We introduce the system of Embodied-RAG. This method addresses problems of naively apply nonparametric memories like RAG to embodied setting. Task We introduce the general task of Embodied-RAG benchmark, formulating semantic navigation and question answering under single paradigm (Table I, Figure 1). Implications Our results and discussion provide basis for rethinking approaches to generalist robot agents based on non-parameteric memories. II. TASK: EMBODIED-RAG BENCHMARK The Embodied-RAG benchmark contains queries from the cross-product of {explicit, implicit, global} questions with potential {navigational action, language} generation outputs. task consists of: Query: The content can be explicit (e.g. particular object instance), implicit (e.g. looking for adequacy, instruction with more pragmatic understanding required), or global. The request might pertain to location or general vibe. Experience: The experience is sequence of egocentric visual perception and odometry, occurring in indoor, outdoor, or mixed environments. Output: The expected output can be both navigation actions with language descriptions (Fig 4 top, Fig. 2 c1), or language explanations (Fig 4 bottom, Fig. 2 c-2). Example tasks are shown in Fig. 4, with instances of explicit, implicit, and global queries in Fig. 1. Spatially, the TABLE I: Comparison of related tasks and datasets. Scope of Query Output Format Experience Task Dataset Explicit Implicit Global Navigational Free-form Indoor Outdoor 3 Semantic Navigation Embodied QA VideoQA ObjectNav [10] ImageNav [3], [11] VLN [12][15] OpenEQA [16] EQA [16] Excalibur [17] VideoQA [18] OVQA [19] Embodied-RAG queries range from specific regions small enough to contain certain objects to global regions encompassing the entire scene. Linguistically, global queries are closer to retrievalaugmented generation tasks, while explicit/implicit ones are more retrieval-focused. Explicit and implicit queries are navigational tasks that expect navigation actions and text descriptions of the retrieved location. Global queries are explanation tasks requiring text generation at more holistic level; there are no global navigation tasks since they pertain to large areas, sometimes the entire environment. who carefully went through the simulated or real environment. Queries were collected by four human annotators with teleoperated robots through two real outdoor/mixed environments, three real indoor environments, and fourteen simulated environments. These diverse environments include residential neighborhood, deserted theme park, and college campus. The environments convey different atmospheres through the activities of people or remnants of their activitiessuch as people lined up, chatting, or jackets left in office spaces. Navigation was performed by quadruped, drone, and locobot. III. RELATED WORKS a) Nonparametric Methods Outside the Embodied Domain: In the text and multimodal domain, RAG [7][9] augments LLMs by incorporating retrieval component from vector database, enhancing the accuracy and relevance of generated content. However, these methods assume pre-existing memory and focus on retrieval and generation rather than the construction of the memory. Active agents operating in space require dynamic approach where memory construction and retrieval/generation are coupled and simultaneous. Furthermore, embodiment introduces the challenge of connecting spatial resolution with language abstraction. GraphRAG [20] addresses the level of abstraction between language queries and document scope. In contrast, our method, Embodied-RAG, extends this resolution problem to the multimodal and spatial domains, simultaneously tackling all of memory construction, retrieval, and generation of language and navigational actions. b) Parametric Use of Foundation Models in the Embodied Domain: Current approaches in embodied AI often rely on the parametric use of foundation models to perceive environments and plan [21][23]. Systems like PIVOT [24] and NavGPT [4] employ large language models (LLMs) in Markovian manner, where decisions are made based on the current state without incorporating external memory or past experiences. This reliance on pretrained LLMs or vision-language models (VLMs) can lead to hallucinations, as these models depend on internal knowledge for action generation, often misinterpreting the world state [25]. Furthermore, agents may suffer from extrinsic visual hallucinations, which negatively affect decision-making [26]. In the textual and multimodal domain, it is known that RAG [7][9], [27] mitigates hallucinations. In our work, we show that EmbodiedRAG reduces hallucinations with the use of nonparametric components. While embodied foundation models such as RT-x models [28] and OpenVLA [29] have been introduced, their nonparametric components have not been explored. c) Existing Methods of Semantic Memory and Retrieval: Several methods have been proposed for storing and querying semantic memory in spatial environments, but they remain limited and task-specific compared to the potential of foundation models. Approaches like [2], [30], [31] associate voxels with predefined object categories, enabling fixed vocabulary retrieval, while methods such as [32], [33] map voxels to image embeddings, allowing for open vocabulary queries. Systems like [34] store images per voxel, supporting queries about people, language/image inputs, and object categories. However, common challenge across these approaches is aligning the semantic abstraction with the spatial resolution. Queries such as cup, red cup, or want to heat my lunch are object-level, but methods like [35], [36] focus primarily on local retrieval during exploration, using structured frontiers based on object layouts. Scene graphs [5], [6], while free from dense memory issues, rely on human-engineered schemas (e.g. floor room object asset), making them unsuitable for novel or outdoor environments. Other approaches, such as OCTREE maps [37] and their semantic versions [38][40], organize occupancy data efficiently but still limit semantics to the object level. Methods like Semantic OCTREE [38], [40] and GENMos [39] use fixed object categories, lacking support for free-form language queries or varying levels of spatial and semantic resolution needed for holistic understanding. d) Semantic Navigation and Question Answering: Tasks like ObjectNav [2], [10], [36], ImageNav [3], [11], [41], and Visual Language Navigation [13] assess robots ability to navigate towards semantic targets based on object categories, images, or language descriptions. While recent efforts like GOATBench [1] combine multiple input types, these tasks still focus on object-level queries and lack the flexibility to han4 Fig. 2: Embodied-RAG method overview. (a) Memory is constructed by hierarchically organizing the nodes of the topological map into semantic forest. (b) The memory in (a) can be retrieved for query, with parallelized tree traversals. (c) Navigation actions with text outputs, or global explanations can be generated for the query, with using the retrieval results as LLM contexts. dle broader, more abstract user requests. Embodied Question Answering (EQA) [16], [42][44] and Video Question Answering (VideoQA) [45][48] extend navigation by requiring text-based answers within actionable or video environments, though EQA is limited to indoor settings and VideoQA lacks active navigation. Our approach expands these paradigms by integrating action-based and question-answering capabilities across wider range of environments and user queries. IV. METHOD: EMBODIED RETRIEVAL AND GENERATION A. Memory Construction The memory construction process of Embodied-RAG consists of two parts: topological map and semantic forest."
        },
        {
            "title": "Topological map We employ a topological graph composed",
            "content": "of nodes with the following attributes: Position Information: The allocentric coordinates (x, y, z) and the yaw angle θ. Image Path: Each node contains path to an associated ego-centric image. Captions: Generated by vision-language model, these captions provide object-level natural language textual descriptions of the image. The nodes form topological map (blue nodes in Fig. 2), eliminating the need for specific control parameters like velocity and yaw, which often vary across different drive systems. This abstraction enables compatibility with any local planner, regardless of the robots embodiment. Furthermore, the topological structure is far more memory-efficient than traditional metric maps [2], [30], [32], allowing for efficient scaling in both large outdoor and complex indoor environments. Our experiments show that this approach successfully navigates kilometer-scale simulated environments. Semantic Forest We use separate tree structure, referred to as semantic forest, to capture meaning at various spatial resolutions. The nodes of this forest are those of the topological map, with the non-leaf nodes capturing larger spaces at thinner density of semantic specificity. First, we create the forest through hierarchical clustering. Since spatially approximate leaf nodes (blue nodes in Fig. 3(a)) exhibit semantic correlations, we employ an agglomerative clustering mechanism [49] to group nodes based on their physical positions assigning the mean position of the leaves. This iterative process continues until root node is formed, stopping when no further relevance is found based on threshold set by the algorithm. Once we have complete forest with one or more root nodes, each non-leaf node receives language description. We achieve this by prompting large language model (LLM, e.g., GPT-4) to generate abstraction that encompasses the descriptions of its direct child nodes (see website for the prompting). This process is conducted bottomup, starting from the leaf nodes and moving up to the parent nodes. We parallelized this process across all nodes at the same hierarchical level. B. Retrieval To address perception hallucinations and improve reasoning capabilities over hierarchies of abstraction constructed for given environment, we modified RAGs relevancy scoring mechanism from semantic similarity to LLM selections at each level, following strategy similar to Tree-of-Thoughts [50]. The input to this retrieval process consists of semantic trees, and the output is the top chains, which represent node paths from selected leaf nodes to the root (e.g., the concatenation of green, yellow, and red nodes in Fig. 2(c)). We run the following process, which takes single tree as input and outputs single leaf node. Starting by visiting the root node, we run BFS with LLM selection; we ask LLM Selector to choose the best child node of the currently visited node based on compatibility with the given query. For example, if the query is find me place that is bright and Fig. 3: We illustrate three retrieval methods: (a) Semantic Match, (b) Retrieval-Augmented Generation (RAG), and (c) our proposed method, Embodied-RAG. Semantic Match retrieves the node in the topological map with the highest cosine similarity with the query, while RAG outputs top nodes. In contrast, Embodied-RAG retrieves the best chains of the semantic forest. quiet but has some presence of people, we prompt the LLM to select the best description among the children of the currently visited node. We then visit the selected best child node and iterate this process until we reach leaf node. Once we obtain leaf nodes ( nodes from each tree) by running this process times for each of the trees, we obtain the chain from the selected node to the root node. The processes are parallelized across the trees. The set of these best chains is the retrieval output, containing semantics at all scales for any specific location that corresponds to the leaf scale. Embodied-RAG unifies the retrieval process to handle explicit, implicit, and global queries, producing both explanations and navigational actions as outputs. Note, these hierarchies and corresponding trees allow for querying automatically created semantic regions, something particularly useful for outdoor navigation where walls and structures cannot be used to determine function. C. Generation We pass the retrieved best chains as part of context, for the LLM to generate navigation and text description (Fig. 4 top) or global explanations (Fig. 4 bottom). Given the query and the chains, we prompt the LLM to select waypoint with reasoning, or to explain (prompt in our project website). Navigation We select waypoint (a leaf node of the semantic forest) and use planner to generate navigational actionssequences of (torque, velocity) pairs to reach the waypoint. To select this waypoint, we ask the LLM to choose the best single leaf node, togehter with textual reasoning, using the query and the chain as input. Again, including the entire chain as input ensures that waypoint can be generated for implicit navigation tasks as well. Text Answers As depicted in Figure 2 (c), we concatenate the chains as part of the prompt to the LLM. We ask it to generate an answer to the query based on the retrieved chains. The spatial scale of attention in each node of the chain facilitate the LLM to generate responses at any semantic scale (explicit, implicit, general) based on the retrieved result. V. EXPERIMENTS Task To assess the efficacy of our approach and ensure statistical robustness, we collected data across 19 diverse including both indoor and outdoor settings. environments, These environments span simulated settings (AirSim [51], Habitat Matterport [52]) and real-world locations, comprising 7 small and 12 large environments. The dataset contains 250 distinct queries, categorized by their nature and complexity. Embodiment For the real-world robotic configuration, we utilized Unitree Go2, equipped with three Realsense cameras to capture 180-degree field of view. Positional data was acquired using the Go2s integrated lidar and SLAM algorithms. For simulations, we use the default drone setup with 210degree panoramic view for and APIs for drone manipulation and positional data acquisition for AirSim. For Habitat, we use the default locobot setup. To construct the experience, human annotators teleoperated and mapped each environment. However, our methodology is adaptable to any frontier-based exploration with minimal modifications [35], [53], [54], since the problem distills into retrieving frontier (potential leaf nodes of the topological map) under Embodied-RAG. Evaluation Before evaluation, users familiarized themselves with the collected dataset to understand the environment. The four human annotators who generated the queries cross-evaluated, with each query receiving three evaluations, excluding the one from its author. For navigation output, participants chose binary success or fail, and we calculated the Success Rate (SR) from the average across evaluators and tasks. For text output, participants rated the relevance and correctness of the response on Likert scale of 1 to 5. a) Baselines: To establish the comparative performance of our Embodied-RAG approach, we benchmarked it against two baseline methodologies. The first baseline, Semantic Match, follows existing methods by computing cosine similarities between the query and the semantic embeddings of captions from nodes in the topological map, which are also leaf nodes of the semantic forest [33], [55], [56]. The second baseline, we employs an naive RAG [7] in embodied setting, identifying the = 10 semantically closest nodes in the topological map and using their scene captions to augment LLMs prompt, enhancing retrieval accuracy. We use the same = 10 for the retrieval of chains in Embodied-RAG. VI. RESULTS A. Quantitative Result We first present quantitative results that demonstrate the effectiveness of our approach in Table II. As outlined in Sec6 TABLE II: Comparison of Methods on different Embodied-RAG Benchmarks. Explicit and Implicit queries are evaluated using Success Rate (SR), while Global queries use Likert Scale of 1 to 5. Env. Small Large Total Explicit Implicit Global Embodied-RAG RAG Sem. Embodied-RAG RAG Sem. Embodied-RAG RAG Sem. 0.955 0.977 0.969 0.955 0.947 0.955 0.895 0.949 0. 1.000 0.914 0.926 0.818 0.695 0.364 0.426 0.706 0. 4.88 4.86 4.87 3.67 2.43 2.68 - - - tion II, we categorize the Embodied-RAG benchmark queries into three major types: explicit retrieval, implicit retrieval, and global retrieval. Additionally, we classify environments as either small or large based on the number of topological nodes mapped. Our results indicate that Embodied-RAG consistently outperforms RAG and Semantic Match across all tasks and environments. Crucially, all approaches yield expected strong results for explicit queries where single node is being extracted. RAGs multi-hypothesis approach outperforms Semantic Similarity, and the hierarchy of Embodied-RAG provides small further boost. The story changes dramatically as we move to implicit queries where the lack of structure causes RAG and and Semantic search performance to drop dramatically, while Embodied-RAG maintains robustness even in large environments. similar result is seen in the likert scale for Global questions. Note, Semantic Match cannot be applied for Global as it lacks summarizing and reasoning. B. Qualitative Result We further conduct qualitative comparison on the reasoning generated by Embodied-RAG and the baseline models before they select retrieval goal. Embodied-RAG consistently demonstrated superior reasoning, especially in handling implicit requests and global queries. This is likely because relying solely on retrieving semantically similar objects, as the baselines do, is insufficient for addressing queries that require global context and understanding relationships between different parts of the environment  (Fig. 4)  . Implicit Query: Find where can buy some drinks? From the figure, we see that Embodied-RAG correctly identifies food service area, while the baselines provide incorrect answers. For RAG and direct semantic match, the most relevant results retrieved are those with direct semantic associations, such as refrigerator or water fountain. However, there is clear mismatch between the users intention and the retrieved objects. The goal is to buy water, which typically requires counter or vending machine for the transaction, rather than simply grabbing it from refrigerator or drinking from water fountain. Embodied-RAG performs multi-step reasoning from the top of the tree to the bottom, and retrieves more diverse and plausible locations. It successfully identifies counters as the most appropriate locations for the users intention. Global Query: As illustrated in Figure 4, Embodied-RAG demonstrates comprehensive understanding of the environment by accurately describing it as suburban neighborhood intertwined with park. This holistic perception is attributed to Embodied-RAGs hierarchical organization of information, Figure 3, enables it to pseudo-attend to every node in the map. In contrast, RAG retrieves only the most similar nodes, resulting in fragmented view characterized by redundant items and failure to integrate observations into cohesive environmental context. This limitation of RAG, where subareas are treated as independent rather than interconnected components of the whole, aligns with findings from previous work [20]. Specifically, Embodied-RAG recognizes the tree-dominated landscape as an integral part of the park, understanding it in relation to other elements such as bushes and shrubs, rather than as an isolated area. In contrast, RAG treats this landscape as distinct entity, detached from its broader park context. As result, RAG presents fragmented collection of local observations, while Embodied-RAG generates outputs that are both spatially and semantically coherent, reflecting humanlike understanding of the environment. C. Computational Efficiency Both memory construction and retrieval have computational complexity of O(log ), where represents the number of nodes in the environment. This choice allows us to efficiently scale to larger environments, as the time complexity only increases logarithmically with the number of nodes. Additionally, when performing the retrievals, we execute them in parallel to minimize the overall time cost. In our real-life experiments, the time costs are demonstrated in the supplementary video, which is 8x fast-forwarded. On average, single retrieval takes around 20 seconds in most of our environments, and the travel time depends on the speed of the specific embodiment in use. D. Ablation We investigate the impact of {1, GPT4 Token Limit} on Embodied-RAG and RAG in Figure 5. total of 15 experiments were conducted for each in each environment. We observe that with larger k, both RAG and Embodied-RAG show improved performance, but this improvement plateaus at higher values. RAG still fails to capture the larger holistic resolution with just more object-level nodes and cannot adequately solve the implicit/general queries, further justifying our hierarchy and tree selection approach. VII. LIMITATIONS AND FUTURE WORK We primarily focused on semantic forests rather than topological map. Therefore, we may not be robust in obstacle avoidance involving dynamic objects and people. Furthermore, Embodied-RAG currently struggles with requests that require precise counting of objects at small scale (e.g., How many 7 Fig. 4: Example reasoning of Embodied-RAG and RAG for generation tasks are highlighted in blue and pink boxes, respectively. semantic forest with learned or pre-trained mechanism to cluster with positional information (e.g. utilizing LLM). VIII. CONCLUSIONS We present Embodied-RAG, system capable of capturing spatial memory at any spatial and semantic resolution in both indoor and outdoor environments, and retrieving and generating responses for navigation and explanation requests. Additionally, we introduce the task of Embodied-RAG benchmark, unifying semantic navigation and question answering. Our findings demonstrate that Embodied-RAG can robustly handle implicit and global queries, as well as ambiguously phrased requests from human annotators. Our results indicate that Embodied-RAG shows potential as the basis for incorporating large non-parameteric memories into robotics foundation models. We are excited for future extensions to manipulation and dynamic environments that enable robotics tasks out of reach for current memory/context constrained approaches. Fig. 5: Effect of total number of searches or retrievals chairs are there around the red table?). This limitation arises because the agglomerative clustering of the semantic forest does not consider multi-view consistency. Future work could incorporate multi-view consistency in the hierarchies of the"
        },
        {
            "title": "REFERENCES",
            "content": "[1] Khanna, Roozbeh et al., Goat-bench: benchmark for multi-modal lifelong navigation, arXiv:2404.06609, 2024. [2] Chaplot, R. et al., Object goal navigation using goal-oriented semantic exploration, NeurIPS, vol. 33, 2020. [3] J. Krantz, S. Lee, J. Malik, D. Batra, and D. S. Chaplot, Instancespecific image goal navigation: Training embodied agents to find object instances, CVPR, 2022. [4] G. Zhou, Y. Hong, and Q. Wu, Navgpt: Explicit reasoning in vision-and-language navigation with large language models, arXiv:2305.16986, 2023. [5] K. Rana et al., Sayplan: Grounding large language models using 3d scene graphs for scalable task planning, in CoRL, 2023. [6] Li, Fuchun et al., Embodied semantic scene graph generation, in CoRL, A. Faust, D. Hsu, and G. Neumann, Eds. PMLR, 2022. [7] P. Lewis, D. Kiela et al., Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021. [8] A. Asai, S. Min, Z. Zhong, and D. Chen, Acl 2023 tutorial: Retrievalbased language models and applications, ACL 2023, 2023. [9] J. Chen, H. Lin, X. Han, and L. Sun, Benchmarking large language models in retrieval-augmented generation, 2023. [10] P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva et al., On evaluation of embodied navigation agents, arXiv:1807.06757, 2018. [11] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi, Target-driven visual navigation in indoor scenes using deep reinforcement learning, in 2017 ICRA. IEEE, 2017, pp. 33573364. [12] Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Y. Wang, C. Shen, and A. v. d. Hengel, Reverie: Remote embodied visual referring expression in real indoor environments, in ICCV, 2020, pp. 99829991. [13] J. Gu, E. Stefani, Q. Wu, J. Thomason, and X. E. Wang, Vision-andlanguage navigation: survey of tasks, methods, and future directions, arXiv:2203.12667, 2022. [14] Ku, Anderson et al., Room-across-room: Multilingual vision-andlanguage navigation with dense spatiotemporal grounding, EMNLP, 2020. [15] Anderson et al., Vision-and-language navigation:interpreting visuallygrounded navigation instructions in real environments, in CVPR, 2018. [16] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra, Embodied question answering, in CVPR, 2018, pp. 110. [17] H. Zhu, R. Kapoor, S. Y. Min, W. Han, J. Li, K. Geng, G. Neubig, Y. Bisk, A. Kembhavi, and L. Weihs, Excalibur: Encouraging and evaluating embodied exploration, in ICCV, 2023, pp. 14 93114 942. [18] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox, Alfred: benchmark for interpreting grounded instructions for everyday tasks, in CVPR, 2020, p. 10740. [19] Gao, G. et al., Dialfred: Dialogue-enabled agents for embodied instruction following, RA-L, 2022. [20] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt, and J. Larson, From local to global: graph rag approach to queryfocused summarization, arXiv preprint arXiv:2404.16130, 2024. [21] Z. Durante, J. Gao et al., Agent ai: Surveying the horizons of multimodal interaction, 2024. [22] Y. Hu, Q. Xie, V. Jain, F. Xia, Y. Bisk et al., Toward general-purpose robots via foundation models: survey and meta-analysis, 2023. [23] Wang, Yankai et al., survey on large language model based autonomous agents, arXiv:2308.11432, 2023. [24] Nasiriany, Brian et al., Pivot: Iterative visual prompting elicits actionable knowledge for vlms, 2024. [32] N. M. M. Shafiullah, C. Paxton, L. Pinto, S. Chintala, and A. Szlam, Clip-fields: Weakly supervised semantic fields for robotic memory, arXiv: Arxiv-2210.05663, 2022. [33] C. Huang, O. Mees, A. Zeng, and W. Burgard, Visual language maps for robot navigation, in Proceedings of the ICRA, London, UK, 2023. [34] M. Chang, T. Gervet, M. Khanna, S. Yenamandra, D. Shah, S. Y. Min, K. Shah, C. Paxton, S. Gupta, D. Batra et al., Goat: Go to any thing, arXiv:2311.06430, 2023. [35] S. K. Ramakrishnan, D. S. Chaplot, Z. Al-Halah, J. Malik, and K. Grauman, Poni: Potential functions for objectgoal navigation with interaction-free learning, in ICCV, 2022, pp. 18 89018 900. [36] S. Y. Min, Y.-H. H. Tsai, W. Ding, A. Farhadi, R. Salakhutdinov, Y. Bisk, and J. Zhang, Self-supervised object goal navigation with insitu finetuning, in 2023 IROS. IEEE, 2023, pp. 71197126. [37] A. Hornung, K. M. Wurm, M. Bennewitz, C. Stachniss, and W. Burgard, Octomap: An efficient probabilistic 3d mapping framework based on octrees, Autonomous robots, vol. 34, pp. 189206, 2013. [38] L. Zhang, L. Wei, P. Shen, W. Wei, G. Zhu, and J. Song, Semantic slam based on object detection and improved octomap, IEEE Access, vol. 6, pp. 75 54575 559, 2018. [39] K. Zheng, A. Paul, and S. Tellex, Asystem for generalized 3d multiobject search, in 2023 ICRA. IEEE, 2023, pp. 16381644. [40] K. Liu, Z. Fan, M. Liu, and S. Zhang, Object-aware semantic mapping of indoor scenes using octomap, in 2019 Chinese Control Conference (CCC). IEEE, 2019, pp. 86718676. [41] L. Mezghan, S. Sukhbaatar, T. Lavril, O. Maksymets, D. Batra, P. Bojanowski, and K. Alahari, Memory-augmented reinforcement learning for image-goal navigation, in 2022 IROS. IEEE, 2022, pp. 33163323. [42] Padalkar et al., Open x-embodiment: Robotic learning datasets and rt-x models, arXiv:2310.08864, 2023. [43] L. Yu, X. Chen, G. Gkioxari, M. Bansal, T. L. Berg, and D. Batra, Multi-target embodied question answering, in ICCV, 2019, p. 6309. [44] S. Tan, M. Ge, D. Guo, H. Liu, and F. Sun, Knowledge-based embodied question answering, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [45] Zhong, Tat-Seng et al., Video question answering: Datasets, algorithms and challenges, arXiv:2203.01225, 2022. [46] H. Yang, L. Chaisorn, Y. Zhao, S.-Y. Neo, and T.-S. Chua, Videoqa: question answering on news video, in Proceedings of the eleventh ACM international conference on Multimedia, 2003, pp. 632641. [47] Castro, Rada et al., Lifeqa: real-life dataset for video question answering, in LREC, 2020. [48] J. Xiao, X. Shang, A. Yao, and T.-S. Chua, Next-qa: Next phase of question-answering to explaining temporal actions, in ICCV, 2021. [49] P. H. Sneath and R. R. Sokal, Numerical Taxonomy: The Principles and Practice of Numerical Classification. W.H. Freeman, 1973. [50] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan, Tree of thoughts: Deliberate problem solving with large language models, NeurIPS, vol. 36, 2024. [51] S. Shah, D. Dey, C. Lovett, and A. Kapoor, Airsim: High-fidelity visual and physical simulation for autonomous vehicles, in FSR, 2018. [52] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang et al., Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai, arXiv:2109.08238, 2021. [53] B. Yamauchi, frontier-based approach for autonomous exploration, in Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA97. Towards New Computational Principles for Robotics and Automation, 1997, pp. 146151. [54] H. Zhu et al., Dsvp: Dual-stage viewpoint planner for rapid exploration [25] Ji, Pascale et al., Survey of hallucination in natural language generaby dynamic expansion, IROS, pp. 76237630, 2021. tion, ACM Computing Surveys, vol. 55, no. 12, 2023. [55] N. Hughes et al., Hydra: real-time spatial perception system for 3d [26] Zhou, Huaxiu et al., Analyzing and mitigating object hallucination in scene graph construction and optimization, RSS, 2022. large vision-language models, arXiv:2310.00754, 2023. [56] B. Chen et al., Open-vocabulary queryable scene representations for [27] Shuster, Jason et al., Retrieval augmentation reduces hallucination in real world planning, in ICRA, 2022. conversation, arXiv:2104.07567. [28] A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky, A. Rai, A. Singh, A. Brohan et al., Open x-embodiment: Robotic learning datasets and rt-x models, arXiv:2310.08864, 2023. [29] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi et al., Openvla: An opensource vision-language-action model, arXiv:2406.09246, 2024. [30] S. Y. Min, D. S. Chaplot, P. Ravikumar, Y. Bisk, and R. Salakhutdinov, Film: Following instructions in language with modular methods, ICLR, 2021. [31] S. Y. Min, Yonatan et al., Dont copy the teacher: Data and model challenges in embodied dialogue, EMNLP, 2022."
        }
    ],
    "affiliations": []
}