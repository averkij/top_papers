{
    "paper_title": "Pay Less Attention to Function Words for Free Robustness of Vision-Language Models",
    "authors": [
        "Qiwei Tian",
        "Chenhao Lin",
        "Zhengyu Zhao",
        "Chao Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 2 2 2 2 7 0 . 2 1 5 2 : r Pay Less Attention to Function Words for Free Robustness of Vision-Language Models"
        },
        {
            "title": "Chao Shen",
            "content": "Xian Jiaotong University michaeltqw@stu.xjtu.edu.cn, {linchenhao, zhengyu.zhao}@xjtu.edu.cn, chaoshen@mail.xjtu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and 90% ASR drop with 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly available. 1. Introduction Building robust vision-language models (VLMs) has gathered profound academic focus because of the necessity of defending VLMs against various adversarial attacks. To this end, many works [13, 17] have been proposed to enhance model robustness, purify perturbations, or detect potential adversaries. Among them, adversarial training (AT) shows superior performance in enhancing the robustness of VLMs. However, AT-based methods incur significant performance drops compared to vanilla models and high computational costs. To resolve the trade-off mentioned above, we propose to enhance VLM robustness by further refining visionlanguage alignment (VLA). Rather than perturbing images during fine-tuning, we break texts into finer grains: content words, i.e., nouns/verbs, and function words, i.e., am/is/are. Specifically, we hypothesize that function words could incur vulnerability of VLMs against cross-modal adversarial attacks because of their ubiquity and lack of specificity. To verify our hypothesis, we record the white-box similarity between function-/content-words and images during targeted (image) attacks1, and find that 80.3% of images show higher similarity scores towards the function words than content words after attacks, while 0% of the images exhibit this pattern before attacks. We also provide visual demonstration using Grad-CAM[18] from successful untargeted attack to demonstrate the impact of function words. As shown in Fig.1, removing all function words greatly mitigates the distractions from adversarial perturbations. Lastly, to qualitatively validate the impact of function words in adversarial examples, we record the performance variation after removing nouns, adjectives, verbs, and function words on both clean and adversarial examples for Image-to-Text/Text-to-Image Retrieval (T2IR/I2TR), as presented in Table.1. Results show that function words are the only words that reduce ASR without causing significant performance drop. These results confirm our hypothesis on function words, implying that proper removal of function words could potentially defend VLMs against attacks. inspired by the setting of differential transformers [22] and differential amplifiers, we propose Function-word De-Attention (FDA) as the first method to build robust VLMs by refining vision-language alignment. Specifically, our FDA works by deploying parallel pipeline on multi-attention heads within fusion-encoders, calculating the cross-attention between function words and the input images, i.e., distractions. We further softmax along the dimensions of visual and textual tokens to highlight the most misleading textual/visual tokens. Finally, we subtract the above distractions from the original attention for the output. To validate the effectiveness of FDA, we conduct comprehensive experiments on two SOTA baseConsequently, 1We tested on the 1k testset of Flickr30k retrieval dataset, using PGD attack with ϵ = 4/255. 1 Figure 1. Grad-CAM of attention maps of VLM under white-box untargeted attacks through perturbed images. The texts are given at the bottom of the figure, with function words highlighted. Left: The VLM correctly recognizes the female student on the clean image given the token her. Mid: The VLM is distracted by the adversarial perturbation and partially looks at the male coach. Right: The distraction is mitigated by simply applying masks to remove all function words: the VLM successfully looks back at the female student. Table 1. Effectiveness of removing different words when testing on clean and adversarial examples. Adversarial examples use AutoAttack, and ASR is presented using the average results for all epsilons (ϵ = 2, 4). / indicates increased/decreased ASR (higher values preferred). Removing function words can lower ASR without significantly harming clean performance. robustness while preserving performance. We provide in-depth ablation studies to show qualitative analysis, the insensitivity of our FDA towards hyperparameters, generalization across backbones, and enhancement on zero-shot performance. Removed Clean (R@1) () Avg ASR Drop ASR () 2. Related Work"
        },
        {
            "title": "Words",
            "content": "N/A"
        },
        {
            "title": "NOUN\nADJ\nVERB\nFUNC",
            "content": "T2IR 95.90 58.90 91.60 93.80 94.30 I2TR 85.60 32.83 78.36 79.36 81. - 25.27 0.42 0.38 0.54 lines, 3 models, 2 tasks, 3 datasets, and 6 attacks. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models for retrieval, and 90% ASR drop with better clean performance on grounding. Our FDA is also verified to enhance the zero-shot performance of VLMs. Overall, our contributions are summarized as follows: We identify that function words are distractions for vision-language alignment and subsequently propose Function-word De-Attentioning (FDA) to pay less attention to function words for more aligned vision-language models with free robustness. We conduct comprehensive experiments on two SOTA baselines, 3 models, 2 tasks, and 3 datasets, under 6 attacks, and validate the effectiveness of FDA in enhancing Adversarial attacks on vision-language models. In light of the advancement in VLMs, adversarial attacks on VLMs have also emerged to fool VLMs into incorrect or misleading outputs. Recent studies on white-box attacks [1] have exhibited impressive results. Besides, black-box attacks[5, 11, 20, 23, 26] have also demonstrated significant effectiveness against pre-trained VLMs through transferable attacks. Adversarial Defense on vision-language models For defenses, adversarial training (AT) [16, 19, 25] has exhibited significant effectiveness in defending models against various adversarial attacks against classification, retrieval, etc. Several AT-based methods [13, 17] have demonstrated impressive robustness boost on CLIP models. However, AT is notoriously well-known for downgrading performance significantly due to the inclusion of adversarial examples into training. Although [17] proposed FARE to use the visual embeddings of vanilla models as supervision to balance the trade-off between clean performance and robustness, the performance drops remain considerably noticeable. Besides, the high computational costs also hinder broader applications in practice. specifically acquire the cross-attention between function words and the input images, namely the distraction, and then subtract them from the original attention. An illustration of our FDA is given in Fig.2. We first parallelly extract the features of all function words (denoted as Tf ) within the input texts by masking all other tokens, excluding [CLS]: FTf = (T, MTf ), Tf (3) where Mtf is the function word mask, and is the function word dictionary. Here, we use dictionary shortlisted from the stopwords list in [9]. Subsequently, we adopt parallel pipeline to calculate function words attention scores: SL,H Tf = Q(FTf )K(FV )T dk (4) With the function words attention scores, we then conduct softmax along the dimensions of visual tokens and textual tokens, respectively. In this way, we highlight the visual tokens with false activation under token words or the most misleading tokens with the largest visual activation: AttL,H AttL,H = sof tmax(cid:0)SL,H = sof tmax(cid:0)SL,H Tf Tf , dim = 1(cid:1)V, , dim = 2(cid:1)V (5) Afterwards, we subtract both distractions from Att individually and take the minimum value as the final attention scores, with being control gate for automatically adjusting the subtraction. ˆAttL,H = min(cid:0)AttL,H G( AttL,H )(cid:1) (6) Finally, after complete calculation of FDA, denoted as DA(), we concatenate the attention scores from all attention heads for outputs: ), AttL,H G( AttL,H L,H ˆAtt = (cid:18)"
        },
        {
            "title": "F DA",
            "content": "(cid:16) Q(cid:0)FT , FTf (cid:1), K(cid:0)FV (cid:1), (FV (7) (cid:1)(cid:17) (cid:19) ; ... FDA can be flexibly implemented on any number of fusion layers/attention heads, as each may specialize differently [6]. general intuition is to remove these distractions in the early layers instead of the later ones to avoid possible absorption of distractions, but not so exquisitely upfront that it may undermine the contextual integrity of the original inputs. We provide ablation and analysis in Sec.4.3. 4. Experiments Vision-Language Tasks&Datasets. To thoroughly evaluate the performance and robustness of FDA, we incorporate Figure 2. Left: An illustration of our Function-word De-Attention (FDA) method. On the existing process of attention calculation, which uses FV and FT , we add parallel pipeline to calculate the attentions between function words FTf and the images FV . Afterwards, the function-attention passes control gate before entering the FDA module (triangle) differentially to subtract distractions as presented in Eq.6. Right: We speculate that attacks can easily cross the boundary for misalignments for less aligned models (top), and by removing function-word distractions, models can learn robust embedding (bottom), preventing misalignments. 3. Methodology In this section, we first provide brief preliminary for the original calculation pipeline of cross-attention and introduce our Function-words De-Attention (FDA). 3.1. Preliminary For given textual/visual encoder /V, input images and texts are fed into repsective encoders with their attention masks MT /MI for the embeddings FT , FV Rdk : FT = (T, MT ), FV = V(I, MI ) (1) Then, cross-attention scores are calculated by inputting these hidden states into the fusion encoder: AttL,H = sof tmax (cid:1)K(FV (cid:16) Q(cid:0)FT dk (cid:1) , dim = (cid:17) (FV (cid:1) (2) where Q/K/V is the query/key/value layers, and L, is the index of layers and attention heads. 3.2. Function-word De-Attention (FDA) Built upon our previous observation, we hypothesize that function words are potential distractions in vision-language To remove such distractions, we propose alignment. Function-word De-Attention (FDA): we add parallel pipeline upon the existing cross-attention calculation to 3 several downstream tasks, including Text-to-Image/Imageto-Text Retrieval (T2IR/I2TR) and Visual Grounding (VG). For datasets, we use the Flickr30k[14] and MSCOCO [10] dataset for retrieval, and RefCOCO+ [24] for VG. Models. For T2IR/I2TR, we test our method on the ALBEF [7], TCL [21], and BLIP[8], using 14M/14M/124M pretrained images, respectively. All models use the ViT-B/32 [3] as visual encoders and BERT [2] as textual encoders. Specifically, TCL shares the same backbones as ALBEF but uses different training strategy (triplet contrastive learning), while BLIP uses larger pre-trained encoder with an extra decoder. CLIP [15] is not included due to the absence of fusion encoders. Baselines. As for baselines, we adopt the two SOTA methods for robust CLIP, i.e., TeCoA [13] and FARE [17], on all the models as adversarial fine-tuning baselines. To account for the robustness and accuracy trade-off, we lower the perturbation strength of each method to ensure similar clean performance as our FDA, such that TeCoA and FARE serve as reference to compare robustness. Specifically, we use ϵ = 1 for TeCoA and FARE. For Text-to-Image/Image-toText Retrieval, we adversarially fine-tune using TeCoA and FARE for 4/1/1 epochs for ALBEF/TCL/BLIP. For Visual Grounding, we adversarially train models with both TeCoA and FARE for only 1 epoch, as both methods incur significant performance drops on ALBEFs. Attacks. To thoroughly evaluate the robustness of our models, we test all models with three adversarial attacks and use the average of all attacks for robustness evaluations. Specifically, we use Projected Gradient Descent [12] and AutoAttack [1], denoted as PGD and APGD. As for the adaptive attack, we apply function word masks to the input texts for APGD to evade our FDA, denoted Masked APGD (MAPGD). All attacks are fully white-box, i.e., the attackers are aware of and can access the extra FDA operations. For each attack, we follow the settings in [13] and [17] to attack images with perturbation bounded by = 2 255 . Specifically, for targeted attacks in T2IR/I2TR, we apply circular shift targeted to ensure non-overlapping unmatched targets. For targeted attacks in VG, we follow the settings of [4] and do not apply patched attack. Metrics. We use the common metric, Attack Success Rate (ASR), to indicate the efficacy of all adversarial attacks. For an ASR on given model and the baseline model, denoted as ASRM /ASRB, respectively, we calculate the relative ASR change in percentage using ASR = ASRB ASRM 100%. Consequently, positive/larger ASRB ASR indicates improved/stronger robustness, while negative/lower ASR implies decreased/weaker robustness, with 0% (100%) meaning no robustness gain (completely defended). Details are given in the Sec.A of the Appendix. Implementation Details. Since our FDA parallelly computes distracted attention for subtraction, finetuning models 255 , 4 with FDA is identical to downstream finetuning without extra modifications or parameters. Following the settings in [7], we finetune the model by 10 epochs and use the lastepoch model for all tasks/models. For the layer index, we use L01 and 05 for all models/tasks/datasets, with corresponding ablation studies on the selection of the layer and attention head indices for all models and tasks in Sec.4.3. 4.1. Main Results In this section, we compare the robustness of FDA and other baselines on T2IR/I2TR and VG tasks. T2IR/I2TR. Results of T2IR/I2TR on ALBEF, TCL, and BLIP for all methods are given in Table.2. Overall on Flickr, our FDA consistently exhibits the best robustness with the best clean performance on all models over all other baselines, yielding 22.26/14.69%, 14.29/13.55%, and 51.60/56.36% average ASR drop over all 3 attacks on ALBEF/TCL/BLIP for ϵ = 2/4, with negligible 0.30/0.10%, 0.50/0.22%, and 0.70/46% performance drops in R@1 for T2IR/I2TR on each model, respectively. On MSCOCO, similar patterns exist as our FDA boosts the ASR drop by 9/14% for ϵ = 2/4 with 0.1% clean performance boost. i. Attack-wise, on Flickr, our FDA exhibits the best defense against PGD and MAPGD in 22 out of 24 results, leading TeCoA/FARE by 60/65% on the BLIP model, demonstrating the effectiveness of FDA in enhancing robustness against various attacks. For the strongest adaptive attack, MAPGD, our FDA maintains its lead over TeCoA and FARE on ABLEF and BLIP, with an average lead by over 10%. Although our FDA shows more vulnerability against APGD on the TCL model, it retains the best comprehensive robustness of the other two baselines, yielding 10-20% lead for ϵ = 2/4. It is noticeable that TeCoA/FARE becomes ineffective for all attacks with ϵ = 4, while our FDA retains its effectiveness facing stronger attacks. Similar trends also exist on MSCOCO. ii. Performance-wise, all baseline methods suffered from performance drop by an average4/3/9% on ALBEF/TCL/BLIP. Nevertheless, our FDA only causes minor or little drops of less than 1% for all models, yielding lead of TeCoA and FARE by approximately 4%, 3%, and %/7 on average, demonstrating the feasibility of paying less attention to function words for free robustness. iii. Scalability-wise, we find that the effectiveness of FDA benefits significantly as the model scales: on ALBEF/TCL, which uses 14M pre-trained images, FDA enhances robustness of each model by roughly 15%; while on BLIP, which uses 124M pre-trained images, FDA achieves an impressive 54% overall increase in ASR. We attribute the enhancement to the capability of the backbone model, which enables the encoders to better capture visual clues. Grounding. Similar patterns persist for VG as shown in Table.3. Our FDA achieves almost complete defense for all 4 Attack success rate (ASR) of PGD/APGD/MAPGD (masked APGD) against for Text-to-Image/Image-to-Text Retrieval Table 2. (T2IR/I2TR) on Flickr30k and COCO. Results are presented in percentage (%). / indicates increased/decreased ASR (higher values preferred). indicates higher performance than clean models. (Full results are given in Sec.B of the Appendix.) Our FDA consistently shows the best performance and overall robustness on ALL models. Dataset VLM Defense Text-to-Image Retrieval Image-to-Text Retrieval ASR drop Clean (R@1) ASR () Clean (R@1) ASR () PGD APGD MAPGD PGD APGD MAPGD ASR c F A P O C B 5 5 2 / 2 5 5 2 / 4 5 5 2 / 2 5 5 2 / 4 5 5 2 / 5 5 2 / 4 5 5 2 / 2 5 5 2 / 4 No Defense TeCoA FARE FDA No Defense TeCoA FARE FDA No Defense TeCoA FARE FDA No Defense TeCoA FARE FDA No Defense TeCoA FARE FDA No Defense TeCoA FARE FDA No Defense TeCoA FARE FDA No Defense TeCoA FARE FDA 95.90 91.20 91.10 95.60 95.90 91.20 91.10 95.60 94. 92.10 91.70 94.40 94.90 92.10 91.70 94.40 97.20 90.30 89.70 96.50 97. 90.30 89.70 96.50 77.60 68.04 69.28 77.70 77.60 68.04 69.28 77.70 3. 2.56 2.46 3.37 8.72 9.13 9.27 7.90 14.68 19.39 17.29 12.44 16. 19.34 18.60 13.70 10.29 70.55 11.08 11.72 8.52 37.66 44.29 46.21 30. 25.10 19.28 20.24 7.66 66.31 67.47 48.38 81.11 80.62 81.03 58.64 63. 59.38 66.53 18.96 61.18 86.39 62.39 66.29 15.86 0.95 0.72 0.26 0. 4.71 1.57 1.40 3.82 88.85 92.35 28.37 11.01 18.56 22.68 9.65 14. 25.69 32.34 11.87 65.88 73.12 70.15 58.66 80.92 85.48 86.25 75.80 66. 59.11 60.98 68.48 81.63 80.08 79.64 86.24 50.19 48.67 54.92 40.98 71. 75.69 80.49 60.64 30.47 34.23 32.71 27.60 51.20 59.90 63.45 44.92 85. 81.44 81.48 85.50 85.60 81.44 81.48 85.50 84.02 80.40 78.22 83.82 84. 80.40 78.22 83.82 87.30 78.04 77.72 86.84 87.30 78.04 77.72 86.84 60. 53.07 53.58 60.63 60.70 53.07 53.58 60.63 0.71 0.55 0.55 0.35 7. 4.60 5.20 4.90 4.11 4.10 4.60 3.30 14.98 17.45 16.55 12.55 15. 18.45 18.35 13.70 65.58 70.80 61.25 44.50 29.72 78.36 35.40 38.05 24. 76.60 76.95 56.50 11.83 60.08 8.85 10.00 5.50 47.80 58.00 13.75 67. 86.08 62.35 67.30 14.45 0.35 0.15 0.02 0.28 2.41 0.36 0.35 2. 87.35 90.50 16.30 8.86 13.05 14.59 8.03 12.18 20.57 24.35 10.57 58. 61.30 65.90 51.35 77.14 79.60 80.60 71.00 60.79 46.85 47.85 57.50 73. 67.95 67.35 77.80 44.35 37.15 46.65 35.00 71.60 72.30 82.50 55.50 19. 18.89 16.76 18.02 36.17 40.37 39.61 32.83 - 3.02 5.36 22.26 - 2.15 2.48 14.69 - 2.78 1.09 14.29 - 4.16 6.42 13.55 - 15.70 3.09 51.60 - 1.09 7.04 56.36 - 2.87 0.53 9.28 - 3.25 16.08 14.43 attacks, yielding an over 90% ASR drop while performing better on clean examples than the vanilla model. Specifically, FDA shows 93.16/91.50% ASR drop for ϵ = 2/4. While TeCoA and FARE show comparative clean performance, they achieve 21.21/21.63% and 12.08/13.28% ASR drop, respectively, with an over 1% drop on clean examples. These results confirm the efficacy of FDA in enhancing robustness for similar/better clean performance. 4.2. Untargeted Attacks Apart from targeted attacks, we further evaluate the robustness against untargeted attacks. Thus, we retrained all models using TeCoA/FARE and their combination with our FDA to validate the effectiveness of FDA in defending against untargeted attacks. For T2IR/I2TR, as presented in Table.4. Overall, we find FDA consistently boosts the robustness of TeCoA and FARE for all untargeted attacks on all models. Specifically, the scalability of FDA also applies after combining with TeCoA/FARE: both methods benefit more from FDA on the larger backbone of BLIP, yielding 4/3% robustness boost. Furthermore, we notice that FDA also boosts the clean performance of both methods on ALBEF considerably, besides the improvement in robustness. For VG, we observe identical patterns: implementing FDA yields solid robustness gain. For example, FARE experiences 5 Table 3. Attack success rate (ASR) of PGD/APGD/MAPGD (masked APGD) against for Visual Grounding (VG) on RefCOCO+. Results are presented in percentage (%). / indicates increased/decreased ASR (higher values preferred). indicates higher performance than (Full results are given in Sec.B of the Appendix.) Our FDA consistently shows the best performance and overall clean models. robustness on ALL models. l"
        },
        {
            "title": "Defense",
            "content": "Clean (Acc) ASR on Test Split () ASR on Test Split ()"
        },
        {
            "title": "PGD APGD MAPGD PGD APGD MAPGD",
            "content": "ASR 5 5 2 / 2 5 5 2 /"
        },
        {
            "title": "No Defense",
            "content": "58.50 65."
        },
        {
            "title": "TeCoA\nFARE\nFDA",
            "content": "57.20 56.40 58.10 64.70 64.20 66."
        },
        {
            "title": "No Defense",
            "content": "58.50 65."
        },
        {
            "title": "TeCoA\nFARE\nFDA",
            "content": "57.20 56.40 58.10 64.70 64.20 66.80 46.30 45.00 44.70 46.10 46.30 45.00 44.70 46. 6.70 6.81 6.13 1.36 7.89 6.57 6.74 1.50 11.16 7.72 10.42 2. 11.16 8.17 9.66 2.10 11.16 8.01 9.96 1.80 11.75 8.46 10.27 2. 6.07 3.39 4.54 0.00 4.39 3.56 4.03 0.34 7.08 6.28 6.72 0. 8.06 6.10 7.06 0.00 7.42 6.10 6.21 0.00 8.06 6.44 6.55 0. - 21.21 12.08 93.16 - 21.63 13.28 91.50 Table 4. Robustness evaluations on ALBEF using FDA as plug-and-play tool with TeCoA and FARE against targeted and untargeted attacks for Text-to-Image/Image-to-Text Retrieval. Results are averaged over T2IR and I2TR. Full results are provided in Sec.C of the Appendix. FDA consistently boosts clean performance and/or robustness against all attacks."
        },
        {
            "title": "Defense",
            "content": "Clean (R@1) Average ASR 2/255 () Average ASR 4/255 ()"
        },
        {
            "title": "Avg ASR drop",
            "content": "T2IR I2TR"
        },
        {
            "title": "PGD APGD MAPGD PGD APGD MAPGD",
            "content": "Robustness ()"
        },
        {
            "title": "No Defense",
            "content": "95.90 85.60 72.67 68."
        },
        {
            "title": "91.20\nFARE + FDA 91.40",
            "content": "81.40 81.86 80.76 80.80 75.84 75.52 69.87 70.70 64.09 63.22 48.18 47."
        },
        {
            "title": "No Defense",
            "content": "97.20 87.30 78.17 77."
        },
        {
            "title": "89.70\nFARE + FDA 89.80",
            "content": "68.00 67.78 77.72 77.72 48.01 43.80 47.51 45.07 41.23 38.20 53.62 49. 63.19 61.65 60.44 44.00 44.54 67.65 38.16 35.63 51.45 46. 94.71 97.49 97.57 96.43 96.42 83.81 81.41 80.84 75.79 74. 99.80 94.01 95.37 94.26 90.37 89.96 75.41 72.20 78.71 76. 81.75 82.69 82.01 75.79 73.57 89.82 72.39 69.67 77.01 74. - 0.47 1.31 13.09 13.46 - 28.23 32.18 22.36 25. Table 5. Robustness evaluations of FDA as plug-and-play tool with TeCoA and FARE against targeted and untargeted attacks for Visual Grounding. Results are averaged over Test-A and Test-B. Full results are provided in Sec.C of the Appendix. FDA consistently boosts clean performance and robustness against all attacks."
        },
        {
            "title": "Defense",
            "content": "Clean (Acc) Average ASR 2/255 () Average ASR 4/255 ()"
        },
        {
            "title": "No Defense",
            "content": "58.50 65."
        },
        {
            "title": "56.40\nFARE + FDA 56.10",
            "content": "64.70 64.90 64.20 63.70 46.30 45.00 45.30 44.70 44.70 27. 9.67 10.37 10.56 11.25 20.06 12.68 12.85 14.45 13.02 19. 12.80 12.01 14.49 13.05 32.33 23.48 9.64 9.84 10.79 10. 16.22 15.22 16.70 15.20 23.31 16.43 15.67 16.97 15.50 ASR - 39.68 40.30 34.69 39.35 significant robustness enhancement regarding untargeted attacks by 5%. In sum, our FDA compatibility works with both TeCoA and FARE to further boost their robustness against untargeted attacks. 4.3. Ablation Study We now provide comprehensive studies on untargeted attacks, hyperparameters of FDA: encoder, dictionary, layer/head, and zero-shot performance. (See full results in 6 Table 6. Comparison bewteen fine-tuning ALBEF by directly removing content words (CONT), nouns (NOUN), function words (FUNC), Determiner DA (DDA), and Adjective DA (ADA). ASR is presented using the average results for PGD and APGD. De-Attention shows significan advantages over directly masking and de-attentioning other words."
        },
        {
            "title": "Maksd",
            "content": "Clean (R@1) ()"
        },
        {
            "title": "Words",
            "content": "T2IR N/A"
        },
        {
            "title": "FDA",
            "content": "95.90 21.50 68.60 94.00 95.60 95.50 95.60 I2TR 85. 11.10 44.62 80.86 85.42 85.38 85.50 ASR () - - - 1. 9.28 15.10 23.07 Sec.D of the Appendix.) 4.3.1. FDA v.s. Masking & Other DA We start by providing comparisons of our FDA and finetuning models by directly masking function words. We also include content words and nouns for thorough evaluation. As for variant of FDA, we further compare our FDA with Adjective DA (ADA) and Determiner DA (DDA). Specifically, we choose determiners (DET) and adjectives because DET indicates using small subset (i.e., a/an/the) of function words, while ADJ adopts completely different set of words. Results are presented in Table.6. Note: We only test on PGD and APGD since MAPGD is not applicable for nouns and content words. 4.3.2. FDA v.s. Adaptive Wwords selection We further qualitatively verify FDA with 3 adaptive selection methods (i.e., using per-token similarity of text and image features to choose tokens with low similarity): i) setting threshold of µ δ; ii) setting threshold of µ 2δ, with µ, δ being the mean and std of the text-image similarity. We further choose the lowest tokens, with being the number of function words in the texts. We denote them as SIM-δ, SIM-2δ, and SIM-N, respectively. Furthermore, we record the % of selected words that are in our shortlisted function words dictionary. Results are shown in Table.7. In summary, we find that the gained robustness of VLMs increased as the proportion of function words increased. While we cannot design an adaptive mechanism that perfectly aligns with using the function words dictionary, we find that while the vulnerability of VLMs does not necessarily come from low-similarity (or low semantic) Table 7. Comparison between FDA and adaptive selection, i.e., using image-text similarity to choose less informative tokens. SIM-δ/-2δ indicates using µ δ and µ 2δ as the de-attention threshold, with µ, δ being the mean and std of the text-image similarity. SIM-N refers to choosing the lowest tokens, with being the number of function words in the text. % of words means the percentage of function words in the selected ones. Results confirm the correlation between the proportion of function words and the gained robustness. Defense SIM-N SIM-2δ SIM-δ FDA % of Words Clean R@1 () ASR Drop ASR () in Dictionary T2IR 25.95 74.53 79. 95.90 95.60 95.30 I2TR 85.50 85.32 85.38 2/255 4/255 Avg 2.44 9.38 12.85 12.81 13.86 11.54 7.62 8.39 12.41 100.00 95.90 85. 27.61 18.53 23.07 words, there is an evident correlation between the percentage of function words and the gained robustness. First of all, masking content words and nouns yields the largest performance drop, making it unviable for robustness evaluation. This aligns with the intuition that these words carry extensive semantic information crucial for VLM tasks. Furthermore, masking function words leads to evident performance drop (3%) and brings negligible robustness (1%).As for DDA, as subset of FDA, it shows almost identical clean performance, with significant drop in robustness, indicating insufficient de-attentioning. ADA also shows subpar performance compared with FDA. Overall, we find that FDA leads the clean and adversarial performance among other variants. 4.3.3. Hyperparameters The implementation of FDA, especially the macrohypaerparameters influencing where to implement, would largely impact the subsequent performance of models. We first provide relative ablation studies to help understand the mechanics and design of our FDA. Encoders&Dictionary. We start by comparing three implementations: FDA on text encoders, fusion encoders, and both, denoted as , H, and &H. As presented in the top rows of Table.8, we find that performs the worst among all, indicating that an early subtraction is insufficient for removing such subtraction. Although &H provides significant robustness boost, it costs an evident 2% performance drop on performance, implying that subtraction on both encoders is too strong and potentially causes contextual distortion. performs the best as it helps models concentrate while preserving the contextual meaning. For the dictionary, we use the off-the-shelf stopwords dictionary in [9], containing 208 words/symbols, denoted as Full Dict. Furthermore, we use shortlisted dictionary, by only using the most commonly used function words, containing 93 crucial function words, denoted as Shortlisted 7 Table 8. Ablation studies on the encoders and dictionary of FDA. We use T2IR/I2TR for evaluation."
        },
        {
            "title": "Defense",
            "content": "Clean (R@1) Average ASR 2/255 () Average ASR 4/255 ()"
        },
        {
            "title": "Avg ASR drop",
            "content": "T2IR I2TR"
        },
        {
            "title": "PGD APGD MAPGD PGD APGD MAPGD",
            "content": "w/o FDA 95.90 85.60 90.75 & H"
        },
        {
            "title": "Full Dict\nShortlisted Dict",
            "content": "95.10 93.80 95.60 95.10 95.40 85.28 85.00 85.50 84.46 85.40 90.19 89.40 90.55 89.78 90. 2.04 2.10 2.01 1.86 2.03 1.71 14.83 21.86 17.61 12.50 13.54 13. 62.37 8.15 15.82 55.00 56.60 56.78 7.96 10.66 9.06 6.40 6.46 6. 15.99 24.70 21.91 13.70 14.47 14.15 79.03 17.30 20.99 73.12 74.65 75. ASR - 2.54 15.61 18.48 4.22 6.45 Table 9. Ablation studies on the layer/head index L/H of FDA on Text-to-Image/Image-to-Text Retreival on ALBEF, TCL and BLIP. Results are averaged over T2IR/I2TR. Shallower layers/heads (smaller L/H) consistently outperform over others on retrieval tasks."
        },
        {
            "title": "Defense",
            "content": "Clean (R@1) Average ASR 2/255 () Average ASR 4/255 ()"
        },
        {
            "title": "Avg ASR drop",
            "content": "T2IR I2TR"
        },
        {
            "title": "PGD APGD MAPGD PGD APGD MAPGD",
            "content": "ASR"
        },
        {
            "title": "BLIP",
            "content": "w/o FDA Lall, all Lall, 611 Lall, 05 L0 , 05 L01, 05 w/o FDA Lall, 05 L0 , 05 L01, 05 w/o FDA Lall, 05 L0 , 05 L01, 05 95.90 85.60 90. 95.50 95.00 95.40 95.60 95.40 85.54 84.96 85.40 85.50 85.32 90.52 89.98 90.40 90.55 90.36 94.90 84.02 89. 94.10 94.40 94.20 83.98 83.82 83.96 89.04 89.11 89.08 2.04 2.06 2.31 1.71 1.86 1.81 7. 6.17 6.06 6.42 14.83 14.82 17.76 13.60 12.50 12.30 68.07 54.34 46.44 48.64 97. 87.30 92.25 18.46 61.67 96.50 96.80 96.70 86.94 86.86 86. 91.72 91.83 91.77 16.60 6.43 6.58 22.74 17.41 16.36 62.37 60.98 65.95 56.78 55.00 54.87 62. 65.59 62.99 64.82 47.27 43.06 39.79 37.99 7.96 7.82 7.91 6.92 6.40 6.17 15. 16.15 19.46 14.15 13.70 13.45 33.69 79.74 29.24 27.31 28.22 66.24 57.57 61.30 64. 86.23 61.09 15.85 15.15 31.38 23.51 22.34 79.03 80.70 83.70 75.07 73.12 72.71 79. 84.47 82.02 83.44 71.44 65.19 59.32 58.07 - 1.56 8.70 6.45 18.48 16.91 - 8.52 13.92 11.14 - 26.46 52.51 53.98 Table 10. Ablation studies on the layer/head index L/H of FDA on Visual Grounding Retrieval on ALBEF, TCL, and BLIP. Results are averaged over Test A/B splits."
        },
        {
            "title": "Defense",
            "content": "Clean (Acc) Average ASR 2/255 () Average ASR 4/255 ()"
        },
        {
            "title": "PGD APGD MAPGD PGD APGD MAPGD",
            "content": "w/o FDA Lall, 05 L0 , 05 L01, 05 58.50 65.90 46.30 56.90 57.90 58.00 58. 65.80 65.90 66.80 46.40 46.40 46.10 56.70 56.77 57.00 6.38 1.02 1.72 0.59 9. 2.06 3.09 0.87 9.29 2.27 2.93 0.73 6.14 0.77 1.85 0.92 9. 2.38 2.60 0.72 9.90 2.22 2.76 0.88 ASR - 81.83 71.43 92. Dict. Both dictionary settings are trained with FDA Lall to maximize their impacts on training. As presented in the lower row of Table.8, there are no significant performance gaps between the two settings, with Full Dict performing slightly worse regarding both clean and adversarial examples. We attribute the minor degradation to the length of the stopwords dictionary, which could unnecessarily skim words and distort the context. We provide the shortlisted dictionary in Sec.E of the Appendix. Attention Head &Layer. We then investigate the index of the layers and attention heads for retrieval and grounding, as presented in Table.9 and Table.10. Specifically, we train series models using FDA but using different and H: for layers, we use all, 0-1, and 0 layers, denoted as Lall, L01, L0; for attention heads, we use all heads, 1st half (0-5) and the second half (6-11), denoted 8 Table 11. Zero-shot performance by applying FDA as plugand-play tool on T2IR/I2TR on ALBEF/BLIP and VG on ALBEF. T2IR/I2TR uses R@1/5/10, while VG uses accuracies."
        },
        {
            "title": "Avg Performance",
            "content": "l i R E P F A w/o FDA 92. - L0 L01 Lall w/o FDA L0 L01 Lall 92.02 92.41 92.17 92.24 92.19 92.22 92. 0.01 0.40 0.16 - 0.05 0.02 0.47 w/o FDA 53.12 - L0 L01 Lall 52.72 52.68 53.34 0.40 0.44 0.22 as all, 05, 611. For T2IR/I2TR, as shown in Table.9, we find that the shallow implementations of FDA, i.e., L0/L01, 05 consistently yield the best performance on robustness on all models. Specifically, L0, 05 constantly achieves the best clean performance, leading other counterparts by 0.1-0.2%. We further test the leading 3 settings on retrieval tasks, i.e., Lall/L0/L01, 05 on VG. As shown in 10, we find the shallow L01, 05 settings still top w.r.t. both adversarial and clean examples, leading other settings by 1020%/0.3-0.4%, respectively. Overall, while FDA behaves slightly differently across settings/tasks, its effectiveness remains solid and insensitive to the head/layer parameters. 4.3.4. Zero-shot Performance Finally, we adopt the three settings of FDA without finetuning to evaluate the zero-shot performance on different tasks (T2IR/I2TR/VG) on ALBEF and BLIP (H 05 is omitted and unchanged for all FDA). Results are presented in Table.11. We find that Lall performs the best for all tasks and all models. This not only suggests that Lall serves as the most generalizable setting for multiple VL tasks and models, but also implies the feasibility of FDA for performance boost on zero-shot tasks. 4.4. Analysis and Visualization We notice that the results of APGD and MAPGD somewhat worsen after adversarial finetuning, e.g., TeCoA and FARE on ALBEF, FARE on BLIP in Table.2, etc. As previously illustrated in Fig.2, defending against targeted attacks requires more aligned vision-language embedding. Consequently, we hypothesize that such abnormality potentially Figure 3. Left: T-SNE of the vision-language embedding of vanilla VLM, FDA, FARE, and TeCoA. Our FDA is the most aligned model. Right: Comparison of text-image similarity for vanilla VLM versus VLM + FDA. Our FDA yields better alignment with larger similarities and smaller variances. originates from the disruption in vision-language alignment brought by adversarial noise for enhanced robustness. To validate our speculation, we visualize the visionlanguage distribution of ALBEF together with TeCoA, FARE, and FDA, as shown in the left graph Fig.3. From the left graph, we find that both FARE and TeCoA (left column) yield severely disrupted embedding, where images and texts sparsely scatter away from each other. On the other hand, our FDA (lower right) has the most aligned crossmodal embedding, as all images and texts remain tightly aligned with each other. To numerically compare the alignment of FDA and the vanilla model, we record the top 200 average white-box text-image similarity scores. As shown in the right figure of Fig.3, applying FDA generates higher average text-image similarity scores and lower variations. 4.5. Limitaiton Besides subtraction, FDA could be potentially improved through modular or algorithmic approach for more refined removal. Furthermore, we did not implement FDA to finetune larger VLM or verify the effectiveness of FDA using LoRa due to the hardware limitation. Finally, our FDA is designed for backbones with fusion encoder and thus not directly implementable for CLIP and other similar backbones. However, we believe implementation on CLIP-like models would be valuable exploration for future work. 5. Conclusion We propose Function-word De-Attention (FDA) calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Specifically, we tested the FDA on 2 downstream tasks, 3 datasets, and 3 models, and evaluated all methods under 6 attacks. By comparing with existing SOTA defenses, our FDA shows superiority of FDA in boosting robustness."
        },
        {
            "title": "References",
            "content": "[1] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pages 22062216. PMLR, 2020. 2, 4 [2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. 4 [3] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. 4 [4] Kuofeng Gao, Yang Bai, Jiawang Bai, Yong Yang, and Shu-Tao Xia. Adversarial robustness for visual grounding of multimodal large language models. arXiv preprint arXiv:2405.09981, 2024. 4 [5] Bangyan He, Xiaojun Jia, Siyuan Liang, Tianrui Lou, Yang Liu, and Xiaochun Cao. Sa-attack: Improving adversarial transferability of vision-language pre-training models via self-augmentation, 2023. [6] Seil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. Your large vision-language model only needs few attention heads for visual grounding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 93399350, 2025. 3 [7] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In Advances in Neural Information Processing Systems, 2021. 4 [8] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. 4 [9] Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. Bert-attack: Adversarial attack against bert using bert. arXiv preprint arXiv:2004.09984, 2020. 3, 7 [10] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. 4 [11] Dong Lu, Zhiqiang Wang, Teng Wang, Weili Guan, Hongchang Gao, and Feng Zheng. Set-level guidance attack: Boosting adversarial transferability of vision-language In Proceedings of the IEEE/CVF Inpre-training models. ternational Conference on Computer Vision (ICCV), pages 102111, 2023. 2 [12] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. [13] Chengzhi Mao, Scott Geng, Junfeng Yang, Xin Wang, and Carl Vondrick. Understanding zero-shot adversarial robustness for large-scale models. arXiv:2212.07016, 2022. 1, 2, 4 arXiv preprint [14] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correIn Prospondences for richer image-to-sentence models. ceedings of the IEEE International Conference on Computer Vision (ICCV), 2015. 4 [15] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 4 [16] Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In International conference on machine learning, pages 80938104. PMLR, 2020. 2 [17] Christian Schlarmann, Naman Deep Singh, Francesco Croce, and Matthias Hein. Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models. arXiv preprint arXiv:2402.12336, 2024. 1, 2, [18] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017. 1 [19] Qiwei Tian, Chenhao Lin, Zhengyu Zhao, Qian Li, and Chao Shen. Collapse-aware triplet decoupling for adversarially robust image retrieval. arXiv preprint arXiv:2312.07364, 2023. 2 [20] Qiwei Tian, Chenhao Lin, Zhengyu Zhao, Qian Li, Shuai Liu, and Chao Shen. Adversarial video promotion against text-to-video retrieval, 2025. 2 [21] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang. Vision-language pre-training with triple contrastive the IEEE/CVF conference learning. on computer vision and pattern recognition, pages 15671 15680, 2022."
        },
        {
            "title": "In Proceedings of",
            "content": "[22] Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, and Furu Wei. Differential transformer. arXiv preprint arXiv:2410.05258, 2024. 1 [23] Ziyi Yin, Muchao Ye, Tianrong Zhang, Tianyu Du, Jinguo Zhu, Han Liu, Jinghui Chen, Ting Wang, and Fenglong Ma. Vlattack: Multimodal adversarial attacks on vision-language tasks via pre-trained models. In Advances in Neural Information Processing Systems, pages 5293652956. Curran Associates, Inc., 2023. 2 [24] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expresIn European conference on computer vision, pages sions. 6985. Springer, 2016. 4 [25] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International 10 conference on machine learning, pages 74727482. PMLR, 2019. [26] Jiaming Zhang, Qi Yi, and Jitao Sang. Towards adversarial attack on vision-language pre-training models. In Proceedings of the 30th ACM International Conference on Multimedia, page 50055013, New York, NY, USA, 2022. Association for Computing Machinery. 2 11 E. Details for function word dictionary We provide the function word dictionary we used as follows: am, is, are, was, were, be, been, being, have, has, had, do, does, did, will, would, shall, should, may, might, must, can, could, ought, dare, need, used, to, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very. F. Visualization of attention scores Finally, we provide an illustration of original attention, FDA with one subtraction, and FDA, as shown in Fig.4."
        },
        {
            "title": "Appendix",
            "content": "A. Details for attacks and evaluation metrics We first introduce the attacks and evaluation metrics for each VL task, including the scenarios where targeted attack is considered successful and the corresponding metrics. Text-to-Image/Image-to-Text Retrieval. For T2IR, successful targeted attack is only when the manipulated images emerge in the Top 1/5 position given the targeted text queries; for I2TR, successful attack is only when the targeted texts emerge in the Top 1/5 position given the manipulated images as the query. Consequently, the ASR of T2IR/I2TR would be the hit rate at the top 1/5, i.e., the probability of appearance in the top 1/5 position, denoted as ASR@1/5. In the main paper, we use the average of ASR@1/5 as the overall ASR. Untargeted attacks follow the identical setting of existing works, i.e., lowering the R@1/5 of the victim models. Visual Grounding. For visual grounding, we choose to obfuscate the model by fooling it into recognizing other objects as the target, or, if there is only one object in the image, locating the position of the object incorrectly (top-left corner). successful attack is when the IOU of the targeted bounding box and the model bounding box is larger than 0.5, i.e., the model locates the object within the targeted bounding box. As for untargeted attacks, we follow existing settings to lower the accuracy of the victim model and calculate the drops as ASR. B. Full results for targeted attacks In this section, we provide full results for all targeted attacks on all models and tasks. Specifically, for T2IR and I2TR, results on ALBEF is given in Table.12 and Table.13, results on TCL is given in Table.14 and Table.15, and results on BLIP is given in Table.16 and Table.17, respectively. Targeted attacks for visual grounding on ALBEF are given in Table.18. C. Full results for untargeted attacks In this section, we provide full results for all untargeted attacks on all models and tasks. Specifically, for T2IR and I2TR, results on ALBEF is given in Table.19 and Table.20, and results on BLIP is given in Table.21 and Table.22, respectively. Untargeted attacks for visual grounding on ALBEF are given in Table.23. D. Full results for ablation studies In this section, we provide full results for all ablation studies. T2IR and I2TR results are given in Table.24 and Table.25. Zero-shot performance is given in Table.26. 12 Table 12. ASR of white-box targeted attacks against Text-to-Image Retrieval on Flickr30k and COCO. The model is ALBEF. Changes over unattacked values are presented in parentheses. All results are in percentage (%). ASR@1/5 indicates the attack success rate of the adversarial image showing up in the top-1/5 position of the targeted text queries. Dataset Defense Clean No Defense TeCoA FARE FDA-L0 FDA-L01 FDA-Lall No Defense TeCoA FARE FDA-L0 FDA-L01 FDA-Lall No Defense TeCoA FARE FDA-L01 No Defense TeCoA FARE FDA-L01 5 5 2 / 2 5 5 2 / 4 5 5 2 / 5 5 2 / 4 95.90 91.20 91.10 95.60 95.40 95.40 95.90 91.20 91.10 95.60 95.40 95. 77.60 68.04 69.28 77.70 77.60 68.04 69.28 77. c O PGD APGD MAPGD ASR@1 ASR@5 ASR@1 ASR@5 ASR@1 ASR@5 0.30 (+0.20) 7.50 (+6.50) 14.60 (+14.50) 15.70 (+14.70) 50.10 (+50.00) 81.90 (+80.90) 0.20 (+0.20) 0.10 (+0.10) 0.10 (+0.10) 0.20 (+0.20) 0.40 (+0.40) 4.30 (+4.20) 3.90 (+3.90) 4.00 (+4.00) 2.90 (+2.90) 3.00 (+3.00) 3.00 (+3.00) 5.30 (+4.90) 5.10 (+4.80) 7.30 (+6.60) 6.90 (+6.10) 5.90 (+5.20) 14.10 (+13.10) 14.70 (+14.30) 14.80 (+14.50) 13.50 (+12.80) 12.40 (+11.60) 13.50 (+12.80) 18.70 (+18.70) 17.20 (+17.20) 20.40 (+20.00) 17.80 (+17.30) 59.90 (+59.90) 58.10 (+57.90) 86.40 (+86.00) 80.50 (+80.00) 12.10 (+12.10) 12.00 (+12.00) 12.80 (+12.80) 16.50 (+16.40) 19.40 (+19.40) 18.80 (+18.80) 13.90 (+13.90) 13.90 (+13.90) 14.60 (+14.60) 13.40 (+12.70) 12.80 (+12.00) 14.20 (+13.50) 16.60 (+15.60) 19.60 (+19.20) 18.80 (+18.30) 14.10 (+13.40) 14.00 (+13.20) 14.80 (+14.10) 43.60 (+43.60) 43.30 (+43.30) 43.50 (+43.50) 75.00 (+74.90) 81.00 (+81.00) 79.70 (+79.70) 69.00 (+69.00) 68.10 (+68.10) 68.90 (+68.90) 73.90 (+73.20) 73.60 (+72.80) 77.30 (+76.60) 87.00 (+86.00) 90.00 (+89.60) 85.90 (+85.40) 82.40 (+81.70) 81.30 (+80.50) 84.10 (+83.40) 0.22 (+0.18) 1.80 (+1.72) 10.18 (+10.14) 11.94 (+11.86) 20.14 (+20.14) 40.88 (+0.80) 0.10 (+0.08) 0.08 (+0.06) 0.66 (+0.56) 0.55 (+0.45) 16.42 (+16.40) 19.64 (+19.62) 17.40 (+17.34) 25.82 (+25.72) 24.52 (+24.50) 21.76 (+21.74) 44.02 (+43.92) 43.74 (+43.64) 0.26 (+0.22) 1.58 (+1.46) 9.00 (+ 8.98) 10.40 (+10.30) 18.28 (+18.26) 37.00 (+36.90) 2.10 (+2.06) 7.44 (+7.36) 14.26 (+14.22) 14.80 (+14.72) 43.74 (+43.70) 58.72 (+58.64) 0.52 (+0.50) 0.40 (+0.38) 2.74 (+2.64) 2.52 (+2.42) 25.00 (+24.98) 30.94 (+30.92) 26.46 (+26.36) 33.82 (+33.72) 53.56 (+53.54) 54.46 (+54.44) 66.28 (+66.18) 72.48 (+72.38) 1.74 (+1.70) 6.06 (+5.94) 11.76 (+11.74) 12.08 (+11.98) 37.92 (+37.90) 51.98 (+51.88) Figure 4. heatmap of attention probabilities given the same image and text inputs. Left: Original attention probabilities are relatively noisy and have several visible stripes with very low probabilities, implying the existence of some less relevant visual tokens that are activated, with negligible contributions. Mid: Attention probabilities with one FDA subtraction show much less aforementioned stripes, with much cleaner and more focused attentions. However, some distractions still exist and remain visible. Right: Attention probabilities with two subtractions show the cleanest attention maps and have the most negligible distractions, with only strong activations on the most relevant visual tokens, i.e., with higher probabilities. 13 Table 13. ASR of white-box targeted attacks against Image-to-Text Retrieval on Flickr30k and COCO. The model is ALBEF. Changes over unattacked values are presented in parentheses. All results are in percentage (%). ASR@1/5 indicates the attack success rate of the targeted text queries showing up in the top-1/5 position of the adversarial image. Dataset Defense Clean PGD APGD MAPGD ASR@1 ASR@5 ASR@1 ASR@5 ASR@1 ASR@5 No Defense TeCoA FARE FDA-L0 FDA-L01 FDA-Lall 85.60 81.44 81.48 85.50 85.32 85.40 0.30 (+0.30) 1.10 (+1.10) 14.40 (+14.40) 15.40 (+15.40) 53.50 (+53.50) 63.50 (+63.50) 0.10 (+0.08) 0.10 (+0.10) 1.00 (+1.00) 1.00 (+1.00) 16.42 (+16.40) 19.64 (+19.62) 17.40 (+17.34) 25.82 (+25.72) 56.90 (+56.90) 56.30 (+56.30) 65.70 (+65.70) 65.90 (+65.90) 0.10 (+0.10) 0.10 (+0.10) 0.20 (+0.20) 0.60 (+0.60) 0.80 (+0.80) 1.00 (+1.00) 12.30 (+12.30) 12.10 (+12.10) 13.50 (+13.50) 12.80 (+12.80) 12.50 (+12.50) 13.70 (+13.70) 46.50 (+46.50) 46.90 (+46.90) 48.50(+48.50) 56.20 (+56.20) 55.90 (+55.90) 58.00 (+58.00) No Defense 85.60 4.50 (+4.50) 9.80 (+9.80) 15.70 (+15.70) 15.90 (+15.90) 74.40 (+74.40) 79.00 (+79.00) TeCoA FARE FDA-L0 FDA-L01 FDA-Lall No Defense TeCoA FARE FDA-L01 81.44 81.48 85.50 85.32 85.40 60.70 53.07 53.58 60.63 2.80 (+2.80) 3.40 (+3.40) 6.40 (+6.40) 7.00 (+7.00) 18.30 (+18.30) 18.30 (+18.30) 18.60 (+18.60) 18.40 (+18.40) 78.10 (+78.10) 77.00 (+77.00) 81.10 (+81.10) 80.60 (+80.60) 3.30 (+3.30) 3.10 (+3.10) 4.00 (+4.00) 0.22 (+0.22) 0.02 (+0.02) 0.00 (+0.00) 6.50 (+6.50) 6.90 (+6.90) 7.80 (+7.80) 0.50 (+0.48) 0.12 (+0.12) 0.06 (+0.04) 13.70 (+13.70) 13.40 (+13.40) 14.10 (+14.10) 7.68 (+7.68) 10.82 (+10.82) 11.80 (+11.80) 13.70 (+13.70) 13.50 (+13.50) 14.20 (+14.20) 10.04 (+10.02) 13.58 (+13.54) 17.40 (+17.38) 68.80 (+68.80) 69.30 (+69.30) 72.20 (+72.20) 14.64 (+14.64) 14.32 (+14.32) 21.62 (+12.62) 72.40 (+72.40) 72.30 (+72.30) 75.20 (+75.20) 24.16 (+21.14) 33.74 (+33.74) 20.92 (+20.90) 0.16 (+0.10) 0.42 (+0.40) 7.14 (+ 7.14) 12.34 (+12.32) 16.72 (+16.72) 26.66 (+26.64) No Defense 60.70 1.46 (+1.46) 3.38 (+3.36) 11.11 (+11.10) 13.26 (+13.24) 50.10 (+50.00) 81.90 (+80.90) TeCoA FARE FDA-L01 53.07 53.58 60.63 0.16 (+0.16) 0.22 (+0.22) 0.56 (+0.56) 0.50 (+0.48) 18.58 (+18.58) 21.84 (+21.84) 25.56 (+25.56) 26.88 (+26.86) 38.42 (+38.42) 31.94 (+31.94) 52.00 (+51.96) 47.30 (+47.28) 1.20 (+1.20) 2.92 (+2.90) 9.88 (+ 9.88) 11.28 (+11.26) 27.74(+27.74) 37.94 (+37.92) 5 5 2 / 2 5 5 2 / 5 5 2 / 2 5 5 2 / 4 i C Table 14. ASR of white-box targeted attacks against Text-to-Image Retrieval on Flickr30k. The model is TCL. Changes over unattacked values are presented in parentheses. All results are in percentage (%). ASR@1/5 indicates the attack success rate of the adversarial image showing up in the top-1/5 position of the targeted text queries. l"
        },
        {
            "title": "Defense",
            "content": "Clean 5 5 2 / 2 5 5 2 /"
        },
        {
            "title": "No Defense",
            "content": "TeCoA FARE FDA-L0 FDA-L01 FDA-Lall"
        },
        {
            "title": "No Defense",
            "content": "TeCoA FARE FDA-L0 FDA-L01 FDA-Lall 94.90 92.10 91.70 94.40 94.20 94.10 94.90 92.10 91. 94.40 94.20 94."
        },
        {
            "title": "MAPGD",
            "content": "ASR@1 ASR@5 ASR@1 ASR@5 ASR@1 ASR@5 2.10 (+2.10) 18.80 (+18.40) 66.00 (+66.00) 75.20 (+74.80) 50.90 (+50.90) 82.50 (+82.10) 3.10 (+3.10) 3.20 (+3.20) 1.90 (+1.90) 2.40 (+2.40) 2.30 (+2.30) 19.30 (+19.00) 20.40 (+20.20) 61.00 (+61.00) 63.10 (+63.10) 71.70 (+71.40) 71.90 (+71.70) 44.20 (+44.20) 46.80 (+46.80) 74.10 (+73.80) 75.20 (+75.00) 15.40 (+15.10) 17.00 (+16.60) 16.80 (+16.40) 44.90 (+44.90) 46.30 (+46.30) 50.80 (+50.80) 52.00 (+51.70) 55.80 (+55.00) 60.90 (+60.50) 52.40 (+52.40) 54.40 (+54.40) 54.90 (+54.90) 84.60 (+84.30) 86.70 (+85.90) 87.10 (+86.70) 21.50 (+21.50) 54.00 (+53.60) 80.30 (+80.30) 82.00 (+81.60) 75.20 (+75.20) 88.10 (+87.70) 27.80 (+27.80) 30.20 (+30.20) 60.90 (+60.60) 62.30 (+62.10) 79.70 (+79.70) 80.30 (+80.30) 81.60 (+81.30) 81.80 (+81.60) 74.10 (+74.10) 73.20 (+73.20) 86.10 (+85.80) 86.10 (+85.90) 17.90 (+17.90) 18.80 (+18.80) 19.00 (+19.00) 43.00 (+42.70) 44.90 (+44.50) 44.90 (+44.50) 56.80 (+56.80) 60.20 (+60.20) 66.10 (+66.10) 60.60 (+60.30) 64.50 (+63.70) 69.10 (+68.70) 80.20 (+80.20) 80.40 (+80.40) 79.80 (+79.80) 92.30 (+92.00) 92.90 (+92.10) 94.50 (+94.10) 14 Table 15. ASR of white-box targeted attacks against Image-to-Text Retrieval on Flickr30k. The model is TCL. Changes over unattacked values are presented in parentheses. All results are in percentage (%). ASR@1/5 indicates the attack success rate of the targeted text queries showing up in the top-1/5 position of the adversarial image. l"
        },
        {
            "title": "Defense",
            "content": "Clean 5 5 2 / 2 5 5 2 /"
        },
        {
            "title": "No Defense",
            "content": "TeCoA FARE FDA-L0 FDA-L01 FDA-Lall"
        },
        {
            "title": "No Defense",
            "content": "TeCoA FARE FDA-L0 FDA-L01 FDA-Lall 84.02 80.40 78.22 83.82 83.96 83.98 84.02 80.40 78. 83.82 83.96 83."
        },
        {
            "title": "MAPGD",
            "content": "ASR@1 ASR@5 ASR@1 ASR@5 ASR@1 ASR@5 2.50 (+2.50) 5.70 (+5.70) 64.10 (+64.10) 66.80 (+66.80) 50.70 (+50.70) 58.90 (+58.90) 2.10 (+2.10) 3.10 (+3.10) 1.90 (+1.90) 1.80 (+1.80) 1.30 (+1.30) 6.10 (+6.10) 6.10 (+6.10) 5.30 (+5.30) 4.80 (+4.80) 4.60 (+4.60) 69.60 (+69.60) 59.70 (+59.70) 72.00 (+72.00) 62.80 (+62.80) 65.50 (+65.50) 65.20 (+65.20) 70.40 (+70.40) 69.50 (+69.50) 43.40 (+43.40) 45.30 (+45.30) 51.50 (+51.50) 45.60 (+45.60) 47.50 (+47.50) 54.30 (+54.30) 52.90 (+52.90) 53.90 (+53.90) 56.00 (+56.00) 62.10 (+62.10) 64.40 (+64.40) 64.40 (+64.40) 24.60 (+24.60) 34.70 (+34.70) 78.80 (+77.80) 78.60 (+78.60) 70.40 (+70.40) 75.50 (+75.50) 29.60 (+29.60) 33.30 (+33.30) 34.70 (+34.70) 39.50 (+39.50) 76.00 (+76.00) 76.20 (+76.20) 77.20 (+77.20) 77.70 (+77.70) 65.50 (+65.50) 65.20 (+65.20) 70.40 (+70.40) 69.50 (+69.50) 20.00 (+20.00) 20.10 (+20.10) 22.20 (+22.20) 28.50 (+28.50) 29.30 (+29.30) 31.10 (+31.10) 56.10 (+56.10) 60.00 (+60.00) 64.40 (+64.40) 56.90 (+56.90) 60.80 (+60.80) 65.50 (+65.50) 75.50 (+75.50) 77.80 (+77.80) 79.50 (+79.50) 80.10 (+80.10) 82.70 (+82.70) 84.10 (+84.10) Table 16. ASR of white-box targeted attacks against Text-to-Image Retrieval on Flickr30k. The model is BLIP. Changes over unattacked values are presented in parentheses. All results are in percentage (%). ASR@1/5 indicates the attack success rate of the adversarial image showing up in the top-1/5 position of the targeted text queries. l"
        },
        {
            "title": "Defense",
            "content": "Clean 5 5 2 / 2 5 5 2 /"
        },
        {
            "title": "No Defense",
            "content": "TeCoA FARE FDA-L0 FDA-L01 FDA-Lall"
        },
        {
            "title": "No Defense",
            "content": "TeCoA FARE FDA-L0 FDA-L01 FDA-Lall 97.20 81.50 79.40 96.80 96.50 96.50 97.20 81.50 79. 96.80 96.50 96."
        },
        {
            "title": "MAPGD",
            "content": "ASR@1 ASR@5 ASR@1 ASR@5 ASR@1 ASR@5 2.50 (+2.50) 46.10 (+46.10) 80.50 (+81.10) 75.20 (+74.80) 57.90 (+57.90) 84.70 (+84.30) 4.30 (+4.30) 1.00 (+1.00) 3.10 (+3.10) 3.00 (+3.00) 3.20 (+3.20) 16.20 (+16.20) 10.30 (+10.10) 19.30 (+19.30) 12.90 (+12.90) 45.70 (+45.60) 46.20 (+46.20) 14.80 (+14.80) 9.70 (+ 9.70) 39.20 (+39.10) 38.00 (+37.90) 12.00 (+11.90) 12.40 (+12.30) 42.00 (+41.80) 13.60 (+13.60) 12.30 (+12.30) 16.00 (+16.00) 26.30 (+26.20) 25.60 (+25.70) 39.60 (+39.40) 24.20 (+24.20) 22.10 (+22.10) 24.50 (+24.50) 61.70 (+61.60) 59.90 (+59.80) 66.30 (+66.30) 31.80 (+31.80) 90.60 (+90.20) 79.80 (+79.80) 93.00 (+92.60) 57.90 (+57.90) 84.70 (+84.30) 46.20 (+46.20) 23.90 (+23.90) 73.00 (+72.90) 61.50 (+61.30) 60.40 (+60.40) 42.30 (+42.30) 83.10 (+83.00) 82.70 (+82.60) 49.50 (+49.50) 33.90 (+33.90) 74.80 (+74.80) 74.70 (+74.70) 13.60 (+13.60) 13.20 (+13.20) 31.60 (+31.60) 19.80 (+19.70) 18.60 (+18.50) 86.80 (+86.60) 16.40 (+16.40) 15.10 (+15.10) 21.40 (+21.40) 43.60 (+43.50) 41.60 (+41.70) 62.00 (+61.80) 44.70 (+44.70) 43.40 (+43.40) 50.00 (+50.00) 78.50 (+78.40) 77.90 (+77.80) 81.90 (+81.70) 15 Table 17. ASR of white-box targeted attacks against Image-to-Text Retrieval on Flickr30k. The model is BLIP. Changes over unattacked values are presented in parentheses. All results are in percentage (%). ASR@1/5 indicates the attack success rate of the targeted text queries showing up in the top-1/5 position of the adversarial image. Defense Clean No Defense TeCoA TeCoA + FDA-L01 FARE FARE + FDA-L01 FDA-L0 FDA-L01 FDA-Lall No Defense TeCoA TeCoA + FDA-L01 FARE FARE + FDA-L01 FDA-L0 FDA-L01 FDA-Lall 5 5 2 / 5 5 2 / 4 87.30 68.00 67.78 65.64 66.22 86.86 86.86 86.94 84. 68.00 67.78 65.64 66.22 86.86 86.86 86.94 PGD APGD MAPGD ASR@1 ASR@5 ASR@1 ASR@5 ASR@1 ASR@5 7.00 (+7.00) 16.60 (+16.60) 54.90 (+54.90) 65.00 (+65.00) 37.60 (+37.60) 50.90 (+50.90) 2.40 (+2.40) 2.20 (+2.20) 0.40 (+0.40) 0.40 (+0.40) 4.40 (+4.40) 4.60 (+4.60) 6.70 (+3.20) 5.90 (+ 5.90) 6.20 (+ 6.20) 13.90 (+13.90) 12.90 (+12.90) 25.10 (+25.10) 25.40 (+25.40) 9.30 (+ 9.30) 8.90 (+ 8.90) 19.80 (+19.80) 19.60 (+19.60) 1.90 (+ 1.90) 2.10 (+ 2.10) 11.30 (+11.30) 9.40 (+ 9.40) 19.60 (+19.60) 17.50 (+17.50) 8.00 (+ 8.00) 6.00 (+ 6.00) 15.10 (+15.10) 13.60 (+13.60) 5.30 (+ 5.30) 4.80 (+ 4.80) 14.60 (+14.60) 14.10 (+14.10) 13.10 (+13.10) 16.90 (+16.90) 15.70 (+15.70) 14.40 (+14.40) 18.60 (+18.60) 31.70 (+31.70) 30.60 (+30.60) 35.00 (+35.00) 41.60 (+41.60) 39.40 (+39.40) 46.50 (+46.50) 58.80 (+58.80) 74.90 (+74.90) 83.70 (+83.70) 88.10 (+88.10) 67.00 (+67.00) 75.90 (+75.90) 46.60 (+46.60) 44.50 (+44.50) 58.10 (+58.10) 59.10 (+59.10) 58.90 (+58.90) 59.10 (+59.10) 69.00 (+69.00) 70.30 (+70.30) 47.10 (+47.10) 47.40 (+47.40) 60.10 (+60.10) 61.10 (+61.10) 23.90 (+23.90) 24.70 (+24.70) 37.30 (+37.30) 37.10 (+37.10) 45.70 (+45.70) 43.90 (+43.90) 60.90 (+60.90) 58.60 (+58.60) 34.00 (+34.00) 31.90 (+31.90) 50.90 (+50.90) 48.20 (+48.20) 14.80 (+14.80) 14.20 (+14.20) 55.20 (+52.20) 15.30 (+15.30) 14.70 (+14.70) 70.80 (+70.80) 16.80 (+16.80) 16.20 (+16.20) 21.80 (+21.80) 17.30 (+17.30) 16.40 (+16.40) 22.10 (+22.10) 53.40 (+53.40) 52.00 (+52.00) 60.20 (+60.20) 60.70 (+60.70) 59.00 (+59.00) 68.70 (+68.70) Table 18. Attack success rate (ASR) of targeted PGD/APGD/MAPGD (masked APGD) against for Visual Grounding (VG) on RefCOCO+. All results are presented in percentage (%). Changes over unattacked values are presented in parentheses. Defense Clean Performance Test Split () Test Split () Val Test Test PGD APGD MAPGD PGD APGD MAPGD 5 5 2 / 2 5 5 2 / 4 No Defense 58. 65.90 TeCoA FARE FDA-L0 FDA-L01 FDA-Lall 57.20 56.40 58.00 58.10 57.90 64.70 64.20 65.90 66.80 65. No Defense 58.50 65.90 TeCoA FARE FDA-L0 FDA-L01 FDA-Lall 57.20 56.40 58.00 58.10 57. 64.70 64.20 65.90 66.80 65.80 46.30 45.00 44.70 46.40 46.10 46.40 46. 45.00 44.70 46.40 46.10 46.40 16.40 (+6.00) 20.40 (+10.00) 20.40 (+10.00) 25.73 (+4.80) 26.53 (+5.60) 27.30 (+6.37) 17.87 (+6.00) 18.40 (+5.33) 18.67 (+ 6.80) 22.13 (+ 9.06) 18.93 (+ 7.06) 22.00 (+ 8.93) 24.00 (+2.67) 24.27 (+3.60) 26.27 (+4.94) 26.00 (+5.33) 26.13 (+4.80) 25.87 (+5.20) 12.53 (+1.73) 12.67 (+1.20) 12.27 (+2.14) 14.40 (+ 3.60) 13.60 (+ 2.13) 12.93 (+ 2.80) 14.40 (+ 3.60) 13.06 (+ 1.59) 13.30 (+ 3.17) 20.80 (+1.20) 20.26 (-0.14) 20.53 (-0.27) 21.33 (+1.73) 19.87 (-0.53) 21.60 (+0.80) 21.07 (+1.47) 20.13 (-0.27) 21.60 (+0.80) 17.47 (+7.07) 20.40 (+10.00) 20.40 (+10.00) 24.40 (+3.47) 26.53 (+5.60) 27.30 (+6.37) 18.00 (+6.13) 18.93 (+5.86) 19.07 (+ 7.20) 21.47 (+ 8.40) 19.33 (+ 7.46) 22.00 (+ 8.93) 24.13 (+2.80) 24.27 (+3.60) 26.13 (+4.80) 26.27 (+5.60) 26.13 (+4.80) 25.87 (+5.20) 13.07 (+2.27) 12.80 (+1.33) 12.27 (+2.14) 14.40 (+ 3.60) 13.33 (+ 1.86) 13.20 (+ 3.07) 14.40 (+ 3.60) 13.33 (+ 1.86) 13.47 (+ 2.67) 20.53 (+0.93) 20.67 (+0.27) 20.13 (-0.67) 20.53 (+0.93) 19.87 (-0.53) 21.87 (+1.07) 20.80 (+1.20) 20.13 (-0.27) 21.73 (+0.93) 16 Table 19. ASR of white-box untargeted attacks against Text-to-Image Retrieval on Flickr30k. The model is ALBEF. After-attack R@k values are presented in parentheses. All results are in percentage (%). ASR@1/5 indicates the drop of R@1/5 after attacks. Defense Clean PGD APGD MAPGD ASR@1 ASR@5 ASR@1 ASR@5 ASR@1 ASR@5 No Defense TeCoA TeCoA + FDA-L0 FARE FARE + FDA-L01 No Defense TeCoA TeCoA + FDA-L01 FARE FARE + FDA-L01 5 5 2 / 2 5 5 2 / 4 95. 91.20 91.60 91.10 90.60 95.90 91.20 91.60 91.10 90.60 78.54 (21.46) 57.39 (42.61) 74.70 (25.30) 55.89 (44.11) 70.93 (29.07) 48.17 (51.83) 81.22 (18.78) 80.73 (19.27) 60.20 (39.80) 58.72 (41.28) 76.02 (23.98) 68.87 (31.13) 58.13 (41.87) 50.20 (49.80) 67.95 (32.05) 67.75 (32.25) 46.01 (53.99) 44.12 (55.88) 74.39 (25.61) 76.73 (23.27) 96.15 (3.85) 51.52 (48.48) 53.46 (46.54) 92.00 (8.00) 54.35 (45.65) 52.95 (47.05) 88.11 (11.89) 70.95 (29.05) 69.31 (30.69) 76.73 (23.27) 49.29 (50.71) 49.19 (50.81) 87.80 (12.20) 23.79 (76.21) 26.32 (73.68) 72.97 (27.03) 98.98 (1.02) 98.68 (1.32) 98.08 (1.92) 98.17 (1.83) 95.33 (4.67) 94.73 (5.27) 85.44 (14.56) 85.09 (14.91) 73.91 (26.09) 72.41 (27.59) 87.36 (12.64) 87.02 (12.98) 71.49 (28.51) 69.47 (30.53) 93.93 (6.07) 93.90 (6.10) 80.57 (19.43) 79.67 (20.33) 63.56 (36.44) 62.60 (37.40) 80.57 (19.43) 80.18 (19.82) 63.56 (36.44) 58.33 (41.67) Table 20. ASR of white-box untargeted attacks against Image-to-Text Retrieval on Flickr30k. The model is ALBEF. After-attack R@k values are presented in parentheses. All results are in percentage (%). ASR@1/5 indicates the drop of R@1/5 after attacks. Defense Clean No Defense TeCoA TeCoA + FDA-L01 FARE FARE + FDA-L01 No Defense TeCoA TeCoA + FDA-L01 FARE FARE + FDA-L01 5 5 2 / 2 5 5 2 / 4 85.60 81.44 81. 81.48 80.14 85.60 81.44 81.80 81.48 80.14 PGD APGD MAPGD ASR@1 ASR@5 ASR@1 ASR@5 ASR@1 ASR@5 84.85 (15.15) 69.91 (30.09) 77.34 (22.66) 64.58 (35.42) 76.02 (23.98) 57.65 (42.35) 87.68 (12.32) 88.59 (11.41) 74.26 (25.74) 74.02 (25.98) 74.70 (25.30) 73.91 (26.09) 61.62 (38.38) 59.89 (40.11) 74.49 (25.51) 73.26 (26.74) 58.16 (41.84) 56.63 (43.37) 84.09 (15.91) 83.61 (16.39) 69.48 (30.52) 68.98 (31.02) 64.18 (35.82) 63.70 (36.30) 74.06 (25.94) 73.27 (26.73) 61.80 (38.20) 61.39 (38.61) 41.13 (58.87) 61.25 (58.75) 97.08 (2.92) 93.61 (6.39) 89.11 (10.89) 81.30 (18.70) 88.34 (11.66) 77.89 (22.11) 99.13 (0.87) 99.72 (0.76) 98.27 (1.73) 98.35 (1.65) 96.51 (3.49) 97.61 (2.39) 86.92 (13.08) 86.96 (13.04) 79.35 (20.65) 78.91 (21.09) 90.81 ( 9.19) 85.82 (14.18) 81.08 (18.92) 80.33 (19.67) 95.45 (4.55) 95.27 (4.73) 84.96 (15.04) 83.83 (16.17) 74.06 (25.94) 73.27 (26.73) 84.96 (15.04) 84.93 (15.07) 74.06 (25.94) 70.85 (29.15) Table 21. ASR of white-box untargeted attacks against Text-to-Image Retrieval on Flickr30k. The model is BLIP. After-attack R@k values are presented in parentheses. All results are in percentage (%). ASR@1/5 indicates the drop of R@1/5 after attacks. Defense Clean No Defense TeCoA TeCoA + FDA-L01 FARE FARE + FDA-L01 No Defense TeCoA TeCoA + FDA-L01 FARE FARE + FDA-L01 5 5 2 / 5 5 2 / 4 97.20 81.50 80.40 79.40 79.30 97.20 81.50 80. 79.40 79.30 PGD APGD MAPGD ASR@1 ASR@5 ASR@1 ASR@5 ASR@1 ASR@5 82.45(17.55) 61.79 (38.21) 80.94 (19.06) 67.40 (32.60) 72.22 (27.78) 52.96 (47.04) 55.33 (44.70) 51.30 (48.70) 32.79 (67.21) 28.74 (71.26) 47.73 (52.77) 44.25 (55.75) 29.22 (70.78) 28.09 (71.91) 44.16 (55.84) 40.56 (59.44) 26.40 (73.60) 24.08 (75.92) 54.19 (45.81) 51.41 (48.59) 28.29 (71.71) 29.18 (70.82) 60.72 (+39.28) 56.62 (+43.38) 36.13 (63.87) 33.08 (66.92) 58.65 (41.35) 54.01 (45.99) 34.93 (65.07) 30.59 (69.41) 99.90 (0.10) 99.60 ( 0.40) 95.69 (4.31) 90.57 (9.43) 93.18 (6.82) 83.75 (16.25) 96.97 (3.03) 96.64 (3.36) 93.94 ( 6.06) 92.73 ( 7.27) 80.95 (19.05) 79.07 (20.93) 68.40 (31.60) 65.08 (31.92) 80.98 (19.02) 77.99 (22.01) 66.38 (33.62) 63.81 (36.19) 94.23 (5.77) 94.14 (5.86) 84.87 (15.13) 82.86 (17.14) 85.09 (14.91) 82.43 (17.57) 66.27 (33.73) 63.12 (36.88) 83.46 (16.54) 80.04 (19.96) 63.76 (36.42) 60.95 (39.05) 17 Table 22. ASR of white-box untargeted attacks against Image-to-Text Retrieval on Flickr30k. The model is BLIP. After-attack R@k values are presented in parentheses. All results are in percentage (%). ASR@1/5 indicates the drop of R@1/5 after attacks. Defense Clean No Defense TeCoA TeCoA + FDA-L01 FARE FARE + FDA-L0 No Defense TeCoA TeCoA + FDA-L01 FARE FARE + FDA-L01 5 5 2 / 2 5 5 2 / 4 87. 68.00 67.78 65.64 66.22 84.02 68.00 67.78 65.64 66.22 PGD APGD MAPGD ASR@1 ASR@5 ASR@1 ASR@5 ASR@1 ASR@5 89.68 (10.32) 78.74 (21.26) 85.45 (14.55) 74.51 (25.49) 80.19 (19.81) 65.22 (34.78) 62.58 (37.42) 58.08 (41.92) 41.35 (58.65) 37.06 (62.94) 53.87 (46.13) 50.37 (49.63) 34.11 (65.89) 30.10 (69.90) 50.67 (49.33) 49.50 (50.50) 31.41 (68.59) 28.36 (71.64) 63.35 (36.65) 59.73 (40.27) 44.21 (55.79) 39.97 (60.03) 68.64 (31.36) 64.06 (35.94) 48.99 (51.01) 44.39 (55.61) 66.25 (33.75) 61.54 (38.46) 45.97 (54.03) 41.74 (58.26) 99.90 (0.10) 99.79 (0.21) 96.70 ( 3.30) 93.09 (6.91) 94.01 (5.99) 88.34 (11.66) 97.42 (2.58) 96.64 (3.36) 93.13 (6.87) 91.04 (8.96) 82.94 (17.06) 80.22 (19.78) 69.33 (30.67) 64.43 (35.57) 80.98 (19.02) 77.99 (22.01) 66.38 (33.62) 63.81 (36.19) 94.21 (5.79) 94.45 (5.55) 88.16 (11.84) 88.40 (11.60) 87.41 (12.59) 86.00 (14.00) 76.07 (23.93) 72.89 (27.11) 86.52 (13.48) 84.24 (15.76) 74.31 (25.69) 71.37 (28.63) Table 23. Attack success rate (ASR) of untargeted PGD/APGD/MAPGD (masked APGD) against for Visual Grounding (VG) on RefCOCO+. After-attack accuracies are presented in parentheses. All results are in percentage (%). ASR indicates an accuracy drop after attacks. Defense Clean Performance Test Split () Test Split () Val Test Test PGD APGD MAPGD PGD APGD MAPGD 5 5 2 / 2 5 5 2 / No Defense 58.50 65.90 TeCoA TeCoA + FDA-Lall FARE FARE + FDA-Lall 57.20 57. 56.40 56.10 64.70 64.90 64.20 63.70 No Defense 58.50 65. TeCoA TeCoA + FDA-Lall FARE FARE + FDA-Lall 57.20 57.10 56.40 56.10 64.70 64.90 64.20 63. 46.30 45.00 45.30 44.70 44.70 46.30 45.00 45.30 44.70 44. 17.40 (48.50) 15.90 (50.00) 15.30 (50.60) 13.20 (33.10) 7.40 (38.90) 7.60 (38.70) 8.20 (56.50) 8.30 (56.60) 9.80 (54.90) 9.80 (55.10) 9.80 (54.90) 9.70 (55.20) 3.00 (42.00) 3.60 (41.70) 4.60 (40.40) 4.80 (40.50) 4.70 (40.30) 5.00 (40.30) 9.40 (54.80) 9.50 (54.50) 11.80 (52.40) 10.90 (53.10) 12.00 (52.20) 10.80 (53.20) 2.90 (41.80) 3.40 (41.00) 4.70 (40.00) 4.00 (40.40) 4.60 (40.10) 4.10 (40.30) 21.40 (44.50) 18.70 (47.20) 18.20 (47.70) 14.90 (31.40) 8.60 (37.70) 8.80 (37.50) 8.30 (56.40) 7.90 (57.00) 12.50 (52.20) 11.30 (53.60) 12.20 (52.50) 11.60 (53.30) 2.90 (42.10) 3.40 (41.90) 5.90 (39.10) 5.90 (39.40) 6.30 (38.70) 6.10 (39.20) 9.40 (54.80) 9.50 (54.50) 11.80 (52.40) 10.90 (53.10) 13.60 (50.60) 12.20 (51.80) 3.10 (41.60) 2.70 (41.70) 5.60 (39.10) 5.10 (39.30) 5.70 (39.00) 5.30 (39.10) 18 Table 24. ASR of ablation studies against Text-to-Image Retrieval on Flickr30k. The model is ALBEF. Changes over unattacked values are presented in parentheses. All results are in percentage (%). ASR@1/5 indicates the attack success rate of the adversarial image showing up in the top-1/5 position of the targeted text queries. Defense Clean No Defense FDA - FDA - & FDA - Lall, all Lall, 611 Lall, 05 L0, 05 L01, 05 Full Dict Shortlisted Dict No Defense FDA - FDA - & FDA - Lall, all Lall, 611 Lall, 05 L0, 05 L01, 05 Full Dict Shortlisted Dict 95.90 95.10 93.80 95. 95.50 95.00 95.40 95.60 95.40 95.40 95.10 95.90 95.10 93.80 95.60 95.50 95.00 95.40 95.60 95.40 95.40 95.10 5 5 2 / 5 5 2 / 4 PGD APGD MAPGD ASR@1 ASR@5 ASR@1 ASR@5 ASR@1 ASR@5 0.30 (+0.20) 7.50 (+6.50) 14.60 (+14.50) 15.70 (+14.70) 50.10 (+50.00) 81.90 (+80.90) 0.40 (+0.40) 0.50 (+0.50) 0.10 (+0.10) 0.10 (+0.10) 0.20 (+0.20) 0.40 (+0.40) 0.10 (+0.10) 0.20 (+0.20) 0.40 (+0.40) 0.30 (+0.30) 4.30 (+4.20) 5.10 (+0.20) 4.70 (+4.70) 2.90 (+2.90) 3.90 (+3.90) 3.30 (+3.30) 3.00 (+3.00) 2.90 (+2.90) 3.00 (+3.00) 6.40 (+6.20) 6.90 (+6.60) 7.30 (+6.60) 7.30 (+6.80) 8.00 (+7.70) 5.90 (+5.20) 7.30 (+6.60) 6.90 (+6.10) 5.90 (+5.20) 6.60 (+6.10) 20.70 (+20.70) 16.80 (+20.90) 12.10 (+12.10) 23.70 (+23.50) 19.30 (+19.30) 13.40 (+12.70) 4.80 (+ 4.80) 13.60 (+13.60) 43.60 (+43.60) 28.30 (+28.10) 21.80 (+21.80) 73.90 (+73.20) 14.40 (+14.40) 16.70 (+16.70) 12.80 (+12.80) 12.10 (+12.10) 12.00 (+12.00) 15.90 (+15.40) 19.50 (+19.20) 14.20 (+13.50) 13.40 (+12.70) 12.80 (+12.00) 46.50 (+46.50) 48.90 (+48.90) 43.50 (+43.50) 43.60 (+43.60) 43.30 (+43.30) 84.90 (+84.40) 85.60 (+85.30) 77.30 (+76.60) 73.90 (+73.20) 73.60 (+72.80) 12.80 (+12.80) 12.80 (+12.80) 14.20 (+13.50) 14.20 (+13.50) 43.60 (+43.60) 43.50 (+43.50) 77.50 (+77.00) 77.30 (+76.60) 14.10 (+13.10) 20.00 (+19.80) 16.70 (+16.40) 13.50 (+12.80) 14.70 (+14.70) 15.70 (+15.40) 13.50 (+12.80) 13.50 (+12.80) 12.40 (+11.60) 16.50 (+16.40) 24.80 (+24.80) 20.90 (+20.90) 13.90 (+13.90) 16.30 (+16.30) 19.60 (+19.60) 14.60 (+14.60) 13.90 (+13.90) 13.90 (+13.90) 16.60 (+15.60) 25.50 (+25.30) 22.20 (+21.90) 14.10 (+13.40) 16.70 (+16.20) 20.10 (+19.80) 14.80 (+14.10) 14.10 (+13.40) 14.00 (+13.20) 75.00 (+74.90) 12.10 (+12.10) 19.90 (+19.90) 69.00 (+69.00) 76.70 (+76.70) 77.70 (+77.70) 68.90 (+68.90) 69.00 (+69.00) 68.10 (+68.10) 87.00 (+86.00) 40.00 (+39.80) 23.50 (+23.20) 82.40 (+81.70) 92.10 (+91.60) 91.70 (+91.40) 84.10 (+83.40) 82.40 (+81.70) 81.30 (+80.50) 3.00 (+3.00) 3.70 (+3.70) 13.50 (+12.80) 11.60 (+11.10) 14.90 (+14.90) 12.80 (+12.80) 15.20 (+14.70) 14.20 (+13.50) 69.60 (+69.60) 43.50 (+43.50) 83.90 (+83.40) 77.30 (+76.60) 19 Table 25. ASR of ablation studies against Image-to-Text Retrieval on Flickr30k. The model is ALBEF. Changes over unattacked values are presented in parentheses. All results are in percentage (%). ASR@1/5 indicates the attack success rate of the adversarial image showing up in the top-1/5 position of the targeted text queries. Defense Clean No Defense FDA - FDA - & FDA - Lall, all Lall, 611 Lall, 05 L0, 05 L01, 05 Lall, 05 - Full Dict Lall, 05 - Shortlisted Dict No Defense FDA - FDA - & FDA - Lall, all Lall, 611 Lall, 05 L0, 05 L01, 05 Lall, 05 - Full Dict Lall, 05 - Shortlisted Dict 5 5 2 / 2 5 5 2 / 4 85.60 85.28 93.80 85.50 85.54 84.96 85.40 85.50 85.32 84.46 85. 85.60 95.10 93.80 85.50 85.54 84.96 85.40 85.50 85.32 84.46 85.40 PGD APGD MAPGD ASR@1 ASR@5 ASR@1 ASR@5 ASR@1 ASR@5 0.30 (+0.30) 1.10 (+1.10) 14.40 (+14.50) 15.40 (+15.40) 53.50 (+53.50) 63.50 (+63.50) 0.30 (+0.30) 0.10 (+0.10) 0.10 (+0.10) 0.30 (+0.30) 0.30 (+0.30) 0.20 (+0.20) 0.10 (+0.10) 0.10 (+0.10) 0.40 (+0.40) 0.20 (+0.20) 4.50 (+4.50) 6.40 (+6.40) 5.10 (+5.10) 3.30 (+3.30) 5.20 (+5.20) 4.10 (+4.10) 4.00 (+4.00) 3.30 (+3.30) 3.10 (+3.10) 3.60 (+3.60) 4.00 (+4.00) 1.50 (+1.50) 1.10 (+1.10) 0.60 (+0.60) 1.00 (+1.00) 1.10 (+1.10) 1.00 (+1.00) 0.60 (+0.60) 0.80 (+0.80) 5.90 (+5.20) 1.00 (+1.00) 9.80 (+9.80) 11.30 (+11.30) 10.00 (+10.00) 6.50 (+ 6.50) 7.90 (+7.90) 8.80 (+8.80) 7.80 (+7.80) 6.50 (+6.50) 6.90 (+6.90) 7.40 (+7.40) 7.80 (+7.80) 20.90 (+20.90) 16.80 (+20.90) 12.30 (+12.30) 22.30 (+22.30) 17.80 (+17.80) 12.80 (+12.80) 5.90 (+ 5.90) 13.00 (+13.00) 46.50 (+46.50) 10.40 (+10.40) 15.10 (+15.10) 56.20 (+56.20) 14.40 (+14.40) 17.30 (+17.30) 13.50 (+13.50) 12.30 (+12.30) 12.10 (+12.10) 15.00 (+15.00) 17.80 (+17.80) 13.70 (+13.70) 12.80 (+12.80) 12.50 (+12.50) 51.70 (+51.70) 55.60 (+48.90) 48.50 (+48.50) 46.50 (+46.50) 46.90 (+46.90) 60.90 (+60.90) 72.30 (+72.30) 58.00 (+58.00) 56.20 (+56.20) 55.90 (+55.90) 13.50 (+13.70) 13.50 (+13.50) 13.70 (+13.70) 13.70 (+13.70) 48.00 (+48.00) 48.50 (+48.50) 57.40 (+57.40) 58.00 (+58.00) 15.70 (+15.70) 24.50 (+24.50) 20.80 (+20.80) 13.70 (+13.70) 15.90 (+15.90) 19.10 (+19.10) 14.10 (+14.10) 13.70 (+13.70) 13.40 (+13.40) 15.90 (+15.90) 24.90 (+24.90) 21.10 (+21.10) 13.70 (+13.70) 16.10 (+16.10) 19.30 (+19.30) 14.20 (+14.20) 13.70 (+13.70) 13.50 (+13.50) 74.40 (+74.40) 15.50 (+15.50) 19.80 (+19.80) 68.80 (+68.80) 78.90 (+78.90) 82.00 (+82.00) 72.20 (+72.20) 68.80 (+68.80) 69.30 (+69.30) 79.00 (+79.00) 19.10 (+19.10) 21.00 (+21.00) 72.40 (+72.40) 82.50 (+82.50) 85.40 (+85.40) 75.20 (+75.20) 72.40 (+72.40) 72.30 (+72.30) 14.10 (+14.10) 14.10 (+14.10) 14.10 (+14.10) 14.20 (+14.20) 70.80 (+70.80) 72.20 (+72.20) 74.40 (+74.40) 75.20 (+75.20) Table 26. Zero-shot performance by applying FDA as plug-and-play tool on T2IR, I2TR on ALBEF/BLIP and VG on ALBEF."
        },
        {
            "title": "Models Method",
            "content": "Zero-shot Performance()"
        },
        {
            "title": "Average",
            "content": "w/o FDA 88.50 98.50 99.20 75.88 93.34 88. 92.01 -"
        },
        {
            "title": "ALBEF",
            "content": "Lall L0 L01 89.10 89.00 89.60 98.60 98.50 98.80 99.40 99.30 99.40 75.56 75.38 76.16 93.70 93.20 93. 96.66 96.72 96.80 92.17 92.02 92.41 0.16 0.01 0.40 w/o FDA 87.20 98.00 99. 78.20 94.08 96.88 92.24 - T2IR/I2TR"
        },
        {
            "title": "BLIP",
            "content": "VG"
        },
        {
            "title": "ALBEF",
            "content": "Lall L0 L01 w/o FDA Lall L0 L01 88.70 87.00 87.10 98.40 98.00 98.00 99.30 99.10 99.10 78.80 78.14 78. 94.20 94.10 94.16 96.88 96.82 96.82 54.50 54.73 54.10 54.14 61.77 62.17 61.71 61. 43.10 43.11 42.34 42.42 92.71 92.19 92.22 53.12 53.34 52.72 52.68 0.47 0.05 0. - 0.22 0.40 0."
        }
    ],
    "affiliations": [
        "Xian Jiaotong University"
    ]
}