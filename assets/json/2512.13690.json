{
    "paper_title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
    "authors": [
        "Susung Hong",
        "Chongjian Ge",
        "Zhifei Zhang",
        "Jui-Hsien Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process."
        },
        {
            "title": "Start",
            "content": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders Susung Hong1, Chongjian Ge2 1 University of Washington Zhifei Zhang2 Jui-Hsien Wang2 2 Adobe Research https://susunghong.github.io/DiffusionBrowser 5 2 0 2 5 1 ] . [ 1 0 9 6 3 1 . 2 1 5 2 : r Figure 1. DiffusionBrowser is plug-and-play model that enables interactive previews anywhere in the multi-step diffusion process, which allows users to make decisions about whether to continue denoising or modify prompts. In addition, DiffusionBrowser provides multiple variation generation mechanisms that guide users to explore the creative generation space in tree-like structure. Our novel, efficient multibranch decoder architecture preserves the full capacity of the base model, and can generate rich multi-modal previews for each timestep in < 1s, adding negligible overhead at inference time."
        },
        {
            "title": "Abstract",
            "content": "Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generationkeeping users in the dark for prolonged period. In this work, we propose DiffusionBrowser, model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4 real-time speed (less than 1 second for 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process. * Work done during an internship at Adobe. 1. Introduction Modern video diffusion models possess remarkable capabilities to generate vivid depictions of diverse scenes. However, two fundamental challenges remain for practical deployment: 1) limited controllability, which results from the inherent stochasticity of the diffusion processes, and 2) slow generation speed, which restricts iterative creation and efficient workflows. Recent work has explored adding various control mechanisms such as camera or object controls, and conditional input modalities such as edge or depth to make video generation more predictable. Another body of work focuses on improving training and inference efficiency, for example, via distillation, mixture-of-experts, autoregressive models, sparse attention, etc. However, while these methods mitigate the two issues, they come with consequences. For example, distillation can cause mode collapse and quality degradation, and adding extra input conditioning like depth can change base model quality and complicate training and inference setups. Even if these techniques work 1 perfectly, diffusion models are still naturally stochastic, and hence some amount of uncertainty remains. To address these limitations, we propose DiffusionBrowser, model-agnostic, lightweight framework to provide users with consistent previews at any given point in the denoising process (block-wise or denoising step-wise), and do so without compromising the models full capacity and with negligible overhead. The previews allow users to terminate irrelevant generations early and save inference resources. Drawing inspiration from traditional graphics rendering pipelines, we designed DiffusionBrowser to be able to preview auxiliary intrinsic channels such as albedo, depth, and surface normals on top of RGB pixels. We show that these intrinsics emerge early in the generation process and can be decoded using carefully designed multi-branch, modalityoptimized decoder. Our preview heads provide more complete 3D preview of the final generation and can be used to understand the inner workings of diffusion models to provide insights on various blocks and timesteps. An additional benefit of the preview decoders is to unlock new form of generation control by steering the denoising trajectories at sample time, allowing users to interact with the vast diversity provided by the generation model. Because DiffusionBrowser surfaces semantically meaningful signals such as coarse layout, motion direction, and appearance at early timesteps, users can intervene before the model commits to non-ideal path. We demonstrate this by showing examples of color, depth, and normal steering at various branching points, providing decision tree-like, interactive generation capability (see Figure 1). Our main contributions are summarized below: We introduce DiffusionBrowser, lightweight framework that provides previews during video diffusion, enabling early termination without degrading fidelity. Our method preserves full-generation quality, supports rapid iteration, and is fully plug-and-play. Inspired by classical rendering pipelines and by the emergence of intrinsics early in denoising, we produce rich previews of RGB and intrinsic channels (albedo, depth, normals) through multi-branch, multi-loss predictor. The preview heads enable novel, interactive generation control by steering denoising trajectories using early semantic signals (layout, motion, appearance), allowing users to guide outcomes at branching points. 2. Related Work Efficiency-Based Methods. Many studies have improved the efficiency of video diffusion models by reducing sampling steps or simplifying model architectures. Distillationbased approaches [33, 47, 48] compress multi-step denoising into fewer steps but often suffer from quality degradation and reduced output diversity. Cascaded diffusion models, such as FlashVideo [55], adopt coarse-to-fine generation strategy; however, each stage still requires full-step inference, limiting practical speedup. Autoregressive frameworks [10, 24, 34] generate frames sequentially to mitigate long-horizon error accumulation but are not suitable for real-time preview during generation. Other directions explore efficiency through techniques such as mixture-ofexperts [14] and sparse attention [7, 50, 54], aiming to reduce computational cost. Despite these advances, existing methods often involve complex training pipelines or modify the models capacity. In contrast, our approach remains model-agnostic and preserves full generation fidelity while enabling fast previews and user steering without altering the underlying diffusion model. Diffusion Features. Diffusion models generate highquality images and videos through iterative denoising [19, 39, 41]. Past research has focused on their internal representations to understand how semantics, structure, and style are encoded, from older U-Net-based architectures [31] to more recent transformer-based ones [2]. Cross-attention maps reveal how text tokens align with visual features, enabling semantic control and editing [6, 17, 18], though they do not fully explain objects spontaneously included in the scene. Meanwhile, studies of self-attention and intermediate states show these components carry rich structural information independent of text conditioning [1, 15, 20, 21, 25]. Applications of diffusion features include image-to-image translation [52], correspondence [37, 38, 43, 44, 51], and zero-shot video generation [22, 23, 27]. In this paper, we define the novel task of preview generation and steering that can serve as new tool to analyze video diffusion features. Generative Models and Scene Intrinsics. Prior work has shown that image generative models, such as GANs and diffusion models, encode geometry and shading cues, enabling applications such as depth estimation and relighting [9, 11, 12, 28, 49, 53]. These findings suggest that, despite being trained solely on 2D images, generative models implicitly learn both inverse and forward rendering processes. Our paper builds on this work to predict intrinsics from intermediate diffusion features. Post-Training Alignment and Reinforcement Learning. Previous work has explored training-time finetuning [5, 13] or inference-time adaptation of diffusion models [16, 26] using reinforcement learning. In particular, Jain et al. [26] recently proposed casting generation as tree search problem and showed how reward function can be used to guide sampling. However, the reward function used in that work, such as the aesthetic score, is accessible only after roll-out to obtain clean sample. In contrast, our work focuses on efficiently generating multi-modal previews at intermediate nodes, with which users can then steer the generation. In other words, we circumvent the expensive reward evaluation and directly align with user preference by design, and therefore can complement this existing work. Figure 2. Linear probing results for scene intrinsics (base color, depth, and normals) and RGB with respect to timesteps and blocks. We use single linear layer with MSE loss (cosine loss for normals). Predictive power from features to scene intrinsics saturates quickly both across blocks and across timestepsaround the 5th15th of 50 timesteps and the 10th20th of 30 blocks. Depth and normals are more easily predicted at earlier stages, whereas RGB prediction quality increases monotonically with both layer depth and timestep. Similar patterns are observed in nonlinear analyses (see the supplementary material), confirming that scene intrinsics can be captured quickly. 3. Background 3.1. Diffusion Models Diffusion models [19, 41] learn to synthesize data by reversing gradual noising process. The noising process for an image over time [0, 1] is: dx = (x, t)dt + g(t)dw, (1) where and are drift and diffusion functions, and dw Instead of sampling from is standard Wiener process. the reverse-time stochastic differential equations, one can equivalently solve their associated deterministic ordinary differential equations, which yield the same marginal distributions under suitable conditions. This perspective enables flow-matching approaches [35, 36]. Flow-matching samplers frame generative modeling as learning vector field vθ(xt, t) whose trajectories satisfy: dxt dt = vθ(xt, t), (2) and whose induced flow matches the data distribution at = 0. In practice, we simulate this continuous dynamics via discrete sequence of steps, updating xt iteratively according to the learned vector field. 3.2. Scene Intrinsics Scene intrinsics is multi-channel representation used to describe the set of geometric, shading, and lighting information from images, which has been long studied in computer vision and graphics communities [4, 40]. Since the decomposition is mathematically under-constrained, recent studies have turned to diffusion models to leverage their stochasticity to sample the possible solution space [29, 30, 32, 49]; given an image or video, these diffusion models are trained to predict scene intrinsics, including geometric channels such as depth and surface normals, and appearance channels such as base color (albedo), roughness, and metallic. 4. Interactive Preview Generation We discuss the appropriate preview representation and introduce our framework, DiffusionBrowser, that enables interactive preview generation. 4.1. Previewing With Scene Intrinsics We seek efficient and semantically meaningful preview representations that satisfy the following two conditions: (1) Human users with the preview can determine what will be generated in the full-fidelity generation. (2) Diffusion models can generate the representation at earlier stages of denoising in order to make previews efficient. Note that the two goals are contradictory: the best, most perceptually consistent preview for humans is the final output, which is the last stage of reasonable diffusion model (otherwise, the model is unnecessarily large). We therefore approach the problem by finding representation that compromises (1) as long as key perceptual factors such as appearance and motion can be depicted. Scene intrinsics offer an appealing choice in that if we discard irradiance (lighting), then the rest of the channels such as albedo and depth are lower frequency signals consisting of larger, colorful patches, yet object boundaries and scene compositions are still visible (see Figure 1 for an example). In addition, previous work has suggested early emergence of low frequency structures in RGB [46], which offers good prospect for satisfying (2). 3 Indeed, intrinsic scene representations emerge early in the denoising process. We demonstrate this by training set of linear probes for the scene intrinsics. Specifically, given transformer-based diffusion model with Nb blocks and denoising schedule that involves Nt steps, we attach linear projection layers, each at distinct block and timestep t, to predict target intrinsic maps yt,b {b, d, n, r, m, c}, for base color, depth, surface normals, material roughness, material metallicity, and color RGB, respectively. The results in Figure 2 clearly show that intrinsics emerge early blockwise, but especially stepwise, supporting our thesis that these semantic features can be useful in early-step preview generation. More training details can be found in 6.1. 4.2. Multi-Branch Preview Predictor We leverage the results from the linear probing experiment to determine the semantically meaningful diffusion features and aim to build better predictor in this section. Naive Predictor. One simple approach to improve the linear probing results is to use deeper model. Since we want lightweight decoder and are limited by data scale, we choose 3D convolutional layers with channelSpecifically, given prediction ˆy = specific losses. [ˆb, ˆd, ˆn, ˆm, ˆr, ˆc], the per-channel losses are written as Lo = ˆo o1, Ls = ˆs s1, {b, c}, {d, m, r}, Ln = 1 ˆn ˆn2n2 . (3) (4) (5) The complete loss function for the naive predictor is the sum of all channels, Ln = (cid:80) jI Lj, where = {b, d, n, m, r, c}. Superposition Problem. We noticed that with the naive predictor above, the results can contain certain blurry parts, especially at high motion or spatially complex patches (e.g., around fast-moving hands). We hypothesize that this is similar to the hallucination problem (e.g., generated hand images containing unseen six fingers [3]), but one that happens at intermediate parts of the denoising trajectory caused by superimposed spatiotemporal uncertainty. Specifically, the estimated posterior mean ˆx0 = E[x0xt] generated by models trained with mean squared error learns smoothed approximation of the true score function and can temporarily push samples toward low-density regions between modes of the data distribution. This is especially problematic at noisy timesteps because, intuitively, the conditional likelihood p(xtx0) = ((1 σt)x0, σ2 I) broadens significantly for large t, and thus the conditional posterior p(x0xt) can be In other words, the blurred parts are highly multimodal. likely superpositions of plausible nearby trajectories supported by multimodal data (e.g., one trajectory with hand moving up, the other down, and the sample is superimposed to manifest as blurry patch at high t). If this does not get sufficiently resolved near = 0, it becomes hallucinated samples (e.g., six-finger hands). Figure 3. Diffusion models trained on toy 4-frame tri-modal dataset reveal severe hallucination and superposition problems at low NFE settings (low total number of steps, distilled few-step model, or early preview) for models trained with MSE. In contrast, our multi-branch decoder architecture correctly produces clean tri-modal distribution and remains artifact-free. For our results, the samples are randomly extracted from different branches, which learned to favor different modes in the data. Toy problem illustrates the superposition problem. To test our analysis, we constructed tri-modal dataset containing 4-frame videos of single white dot moving left, right, or remaining stationary (see Figure 3). We then trained diffusion model on this dataset using 4-layer DiT (with roughly 0.3M parameters) with standard ϵparameterization [19]. Once we trained the model, we additionally distilled it with consistency distillation [42]. Running inference on these toy models leads to the following observations supporting our hypothesis: 1) the superposition problem occurs in ˆx0 at earlier timesteps such as 200 of 1,000 steps (see the third panel in Figure 3); 2) using coarser timestep discretization such as 20-step DDPM and consistency-distilled models with 1 step exhibits more severe superposition problems, which create multiple highintensity dots at the same time or completely remove the dots, which never occurs in the toy training videos (see the first and fourth panels in Figure 3). This motivates us to address states in superposition when predicting clean signals. Multi-Branch Multi-Loss Predictor We mitigate the superposition problem with multi-branch decoding architecture (MB); see Figure 4 for an illustration. Instead of single deterministic head, we introduce independent decoders {Dk}K k=1, each predicting intrinsic maps ˆyk = Dk(ft,b). Their ensemble average ˆyens. ="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) k=1 Dk(ft,b) (6) is trained jointly with individual heads. The total loss combines individual branch losses L(k) (reusing the loss from 5. Generating Variations Using Previews Multi-step diffusion as traversing tree. Our multibranch decoder can generate multi-channel video previews at any timestep in less than 1 second of wall-clock time that are consistent with the 4-second final videos (see Table 1). These properties make it practical for users to interact with the generation system in novel tree structure (see Figure 1) where they can move up/down the denoising levels. To enrich this, we propose two variation generation methods to steer between siblings within the same noise level. 5.1. Variations Through Stochastic Renoising The first way to introduce variation is simply by renoising clean latent prediction ˆz0 using different random noise: zt = (1 σtp )ˆz0 + σtp ϵ, ϵ (0, I). (9) Here, tp is the timestep immediately after previewing occurs. The noise scale σtp is consistent with the original schedule, preserving the image structure generated so far while introducing stochastic finer-scale variations. Multiple denoised samples from the perturbed latents yield set of plausible scene variations without repeating the full diffusion process. 5.2. Variation Through Latent Steering We also introduce an incremental method to steer the later denoising timesteps using the trained preview decoder, generating purposeful variations. Specifically, with trained, frozen preview decoder, steering involves solving the optimization problem L(cid:0)D(ft,b), y(cid:1), min ft,b (10) where : maps features ft,b to the intrinsic map space P. The details of the optimization can be found in the supplementary material S4. 6. Experiments We conduct extensive experiments to validate our hypotheses and demonstrate the effectiveness of the proposed framework. 6.1. Implementation Details In this paper, we use Wan 2.1 [45], although our framework is model-agnostic. We constructed synthetic video dataset with cached intermediate diffusion features. total of 1,000 videos were generated with unique prompts using DiffusionRenderer [32], which provided scene intrinsic channels along with RGB pixels. Our MB decoder is implemented with four 3D convolutional layers followed by two upscaling 3D convolutional layers for each branch, Figure 4. Our multi-branch multi-loss decoder is trained with intermediate diffusion features. Grounded by branch-wise loss and an aggregated ensemble loss, it is designed to reduce the superposition problem. the naive predictor) with an ensemble loss: Lens. = (cid:20)(cid:13) (cid:13)ˆyn (cid:13) ens. yn(cid:13) 2 (cid:13) (cid:13) 2 (cid:21) + 1 ˆnens. ˆnens.2n2 , Ltotal = λens.Lens. + (cid:88) k= L(k) , (7) (8) where yn = [b, d, m, r, c] denotes all intrinsic components except normals n, which require directional loss. With proper branch-wise loss, each branch prediction represents possible mode in the data distribution. We find that using mode-seeking loss (e.g., LPIPS) for Ln together with multi-branching resolves the superposition problem even with single NFE (see Figure 3 and S3) and also ensures that the mean of the branches closely matches the ground-truth mean of the data. Using the ensemble prediction from finite number of branches during inference, the multi-branch model achieves higher-quality results and clearer edges while also encouraging diversity across branches, compared to the naive predictor, which tends to produce sharp edges but misaligns with the final video output, as shown in Figure 5. Figure 5. Qualitatively, the proposed MB decoder improves mode selection and reduces artifacts due to multimodal ambiguity. Red boxes highlight high-uncertainty regions that caused blurred patches in the naive single-branch decoder. 5 Figure 6. Timestep-wise evolution of base color, normal, and albedo. Coarse geometry and recognizable structures appear around the 5th timestep, with details refined progressively thereafter. with = 4 branches. This results in resolution of roughly 208 120; therefore, we use linear interpolation to downsample RGB and pseudo-ground-truth. Temporally, we subsampled every fourth frame to match the temporal size of the features. Ensemble weighting λens. = 10.0. predictive power for intrinsic properties like depth and normals decreases. These observations also align with the linear probing analysis in Figure 2, showing that intrinsic information saturates in mid-to-late blocks, after which the representation primarily supports appearance refinement. 6.2. Baseline Comparison 6.4. Rubber-Like 4D Previsualization We compare MB-based preview generation with other methods. Since our model is uniquely multi-modal, we compare channels with separate baselines when possible; these include x0 prediction (x0-pred) that uses Tweedies formula to estimate the clean latent and then passes it through the pretrained VAE decoder to obtain clean RGB video. We also compare to the state-of-the-art Video Depth Anything [8] for depth estimation and intrinsics with DiffusionRenderer [32], both using x0-pred as input. We selected 10% of the total denoising steps and report PSNR as the metric. The results are shown in Table 1. Our predictor outperforms the other baselines, suggesting the effectiveness of our feature-level predictor. Also, we measured the overhead using wall-clock time, which shows that our decoder is significantly more efficient compared to the baselines. 6.3. Stepwise and Blockwise Preview Evolution Representative examples of how previews evolve stepwise and blockwise are shown in Figure 7. We found consistent convergence behaviors as shown by the linear probing analysis in Figure 2. In addition, qualitatively, rough geometry and scene structure appear as early as 10% of the denoising steps, which are well captured by the scene intrinsics. These material previews remain stable throughout the denoising process, consistent with the final generation. Figure 6 shows the blockwise evolution. We found that lower blocks contain coarse geometry and color distributions, while mid-level blocks (around the 15th20th of the 30-layer model) capture detailed spatial structure with stable base color and depth predictions. At the last block, the We show that the MB decoder preview at only 10% of the denoising steps can be used to create 4D visualization of the video being generated (Figure 8). Despite being computed from highly noisy intermediate features, the previews reveal smooth object motion, spatial composition, and overall color palette, resembling rubber-like low-frequency representation of the scene, which can be useful for interactive exploration. More results can be seen in the supplementary material S1. 6.5. Variation Generation Stochastic variation generation introduced in 5.1 renoises latents using the appropriate scale based on noise level. At intermediate steps, users can preview the coarse scene structures using our MB decoder and then experiment with alternative finer details by sampling the base model at that level. An example is shown in Figure 9. Steered variation generation introduced in 5.2 allows for channel-targeted steering. Figures 10 and 11 show separate color and geometry steering, and more results are provided in the supplementary material S4. Note that lighting and texture remain consistent across steered variants, likely because they are handled by later stages of the denoising process. Also note that steering is different task from video editing. The latter comprises myriad different techniques and is aimed at changing the final output with precision, whereas the steering proposed in this work is meaningful way of generating variations during generation and is meant to be complementary to video editing methods (e.g., steer-then-edit or preview-while-editing). 6 Decoder Type x0-pred Video Depth Anything* Diffusion Renderer* Linear Probing Ours RGB 16.98 17.96 18.03 Base Color 14.81 15.51 16. Depth 11.64 5.17 15.52 16.95 Normal Metallicity 18.45 19.54 20.04 17.22 11.22 16.42 Roughness 14.72 15.75 17.03 Runtime 4.69s 9.50s 222.87s 0.47s 0.53s Speedup 8.85x 17.92x 420.51x 0.89x 1x Table 1. Comparison across different models and modalities using PSNR and wall-clock time at 10% of the total denoising steps shows that our model produces the best results for most of the channels while being significantly resource-efficient. Speedup is computed using our approach as the baseline. Figure 7. Block-wise evolution of base color, normal, and albedo. Intrinsics are best predicted from mid-level features, slightly degrading in the final layers. Figure 8. Rubber-like 4D visualization can be derived from the intrinsic previews from our model. Interestingly, at only 10% of the denoising schedule, clear structural representation of the scene has already emerged. Figure 9. Examples of variation generation via stochasticity injection show coarse details being preserved at lower noise levels, while the injected stochasticity changes several details in the video highlighted by the red boxes. Model Albedo Depth Normal Metallicity Roughness Naive Shallow Deep Ours 0.121 0.120 0.121 0.117 0.121 0.115 0.118 0.115 0.167 0.156 0.160 0. 0.249 0.238 0.240 0.241 0.152 0.140 0.146 0.142 Table 2. Ablation study comparing decoder variants. We report L1 error on the validation set. Both the naive and our (MB) decoders are 6 layers deep; the shallow is 4 layers, and the deep is 8. 6.6. Ablation Study We analyze the impact of key architectural choices in Table 2. The MB decoder achieves lower MSE and L1 errors across most intrinsic properties compared to single-branch 7 counterpart, confirming that modeling multiple hypotheses improves robustness and interpretability (Figure 5). Shallower or deeper variants show marginal differences, suggesting that our six-layer multi-branch configuration provides balanced trade-off between accuracy and computational cost. 6.7. User Study To evaluate the perceptual quality of our representations, we conducted user study with 35 participants comparing our method against the x0-pred baseline. Participants were shown two representations alongside reference video and asked to judge which better predicted video content, exhibited fewer visual artifacts, and more clearly conveyed scene composition. Previews generated by DiffusionBrowser were preferred 74.6%, 72.9%, and 76.9% of the time for content predictability, visual fidelity, and scene clarity, respectively, when compared to the x0-pred baseline. More details can be found in the supplementary material S6. 7. Discussion Utilizing Diffusion Features Video diffusion models must resolve greater ambiguity than image diffusion models and are expected to learn more informative features. However, their learned representations remain relatively underexplored. To the best of our knowledge, our work is the first to utilize video diffusion transformer features to predict multiple scene intrinsics simultaneously, providing rich analysis of diffusion features. In video diffusion, these feaFigure 10. Steering base color at 10% of the total denoising steps allows users to generate variations in the same context. The text prompt is car driving on sunny road. Figure 11. Examples of variation generation via steering show meaningful steered base color, depth, and normal results. tures correlate strongly with physical scene attributes such as depth and albedo, supporting the hypothesis that diffusion implicitly performs form of inverse rendering. Figure 12. Failure case in intrinsic steering. The sphere added at the 20th layer gradually dissolves and deforms in subsequent timesteps. Superposition Problem The superposition problem was introduced and empirically verified in 4.2, which describes the observation of blurred predictions at intermediate denoising states for high-motion and high-complexity regions. We attributed this to diffusion features encoding multiple possible future states simultaneously, and reason that it is superset of the notorious hallucination problem such as the implausible 6-finger hand generated by diffusion model. We provided theoretical reasoning for the source of this problem and empirically verified it via small-scale toy problem. We showed that an explicit, multi-headed architecture like our MB decoder can mitigate this issue. We believe there is potential to extend this approach to related problems, such as few-step distillation, where models are also constrained by limited NFEs and can lead to exacerbated hallucination and quality degradation. We leave this as an exciting direction for future work. Limitations and Future Work While our framework enables fast and semantically meaningful previews for video diffusion models, several limitations remain. We deliberately limit our scope to scene intrinsics, and text prompts are not considered; the interaction between intrinsic previews and text-driven conditioning can be explored in future work. Additionally, there are failure cases in steering, as shown in Figure 12, where the steered intrinsics dissipate as denoising progresses, which we attribute to an out-ofdistribution issue for our shallow decoder. For future work, we aim to explore alternative decoder architectures to improve mode separation and produce clearer, more coherent outputs at higher resolution, as well as expand the intrinsic representations to include additional modalities. 8. Conclusion DiffusionBrowser offers new perspective on interacting with video diffusion models by making their coarseto-fine internal evolution visible, actionable, and efficient. By decoding stable intrinsic signals that emerge early in the denoising process, our lightweight, plug-andplay preview framework enables users to terminate unpromising generations, iterate rapidly, and steer trajectories without sacrificing final quality. Beyond practical speedups, these previews serve as window into the geometry, layout, and appearance dynamics that govern diffusion behavior, opening new opportunities for interpretability and user-driven control. We believe DiffusionBrowser lays the groundwork for more interactive, transparent, and resource-efficient video generation pipelines, and provides foundation for future research into controllable diffusion and the structure of generative processes."
        },
        {
            "title": "References",
            "content": "[1] Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, and Seungryong Kim. Self-rectifying diffusion sampling with perturbed-attention guidance. In European Conference on Computer Vision, pages 117. Springer, 2024. 2 [2] Donghoon Ahn, Jiwon Kang, Sanghyun Lee, Minjae Kim, Jaewon Min, Wooseok Jang, Saungwu Lee, Sayak Paul, Susung Hong, and Seungryong Kim. Fine-grained perturbation guidance via attention head selection. arXiv preprint arXiv:2506.10978, 2025. 2 [3] Sumukh Aithal, Pratyush Maini, Zachary C. Lipton, and J. Zico Kolter. Understanding hallucinations in diffusion models through mode interpolation, 2024. 4 [4] Sean Bell, Kavita Bala, and Noah Snavely. Intrinsic images in the wild. ACM Trans. on Graphics (SIGGRAPH), 33(4), 2014. 3 [5] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning, 2024. 2 [6] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM transactions on Graphics (TOG), 42(4):110, 2023. 2 [7] Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, et al. Sana-video: Efficient video generation with block linear diffusion transformer. arXiv preprint arXiv:2509.24695, 2025. [8] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. arXiv:2501.12375, 2025. 6 [9] Yida Chen, Fernanda Viegas, and Martin Wattenberg. Beyond surface statistics: Scene representations in latent diffusion model. arXiv preprint arXiv:2306.05720, 2023. 2 [10] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. 2 [11] Xiaodan Du, Nicholas Kolkin, Greg Shakhnarovich, and Anand Bhattad. Generative models: What do they know? arXiv preprint do they know things? arXiv:2311.17137, 2023. 2 lets find out! [12] Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2179521806, 2024. 2 [13] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models, 2023. 2 [14] Alireza Ganjdanesh, Yan Kang, Yuchen Liu, Richard Zhang, Zhe Lin, and Heng Huang. Mixture of efficient diffusion experts through automatic interval and sub-network selection. In European Conference on Computer Vision, pages 5471. Springer, 2024. [15] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arXiv:2307.10373, 2023. 2 [16] Yutong He, Naoki Murata, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Dongjun Kim, Wei-Hsiang Liao, Yuki Mitsufuji, J. Zico Kolter, Ruslan Salakhutdinov, and Stefano Ermon. Manifold preserving guided diffusion, 2023. 2 [17] Alec Helbling, Tuna Han Salih Meral, Ben Hoover, Pinar Yanardag, and Duen Horng Chau. Conceptattention: Diffusion transformers learn highly interpretable features, 2025. 2 [18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. 2022. 2 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. 2, 3, 4 [20] Susung Hong. Smoothed energy guidance: Guiding diffusion models with reduced energy curvature of attention. Advances in Neural Information Processing Systems, 37: 6674366772, 2024. 2 [21] Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. Improving sample quality of diffusion models using self-attention guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7462 7471, 2023. 2 [22] Susung Hong, Junyoung Seo, Heeseong Shin, Sunghwan Hong, and Seungryong Kim. Direct2v: Large language models are frame-level directors for zero-shot text-to-video generation. arXiv preprint arXiv:2305.14330, 2023. [23] Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi Yu, and Sibei Yang. Free-bloom: Zero-shot text-to-video generator with llm director and ldm animator. arXiv preprint arXiv:2309.14494, 2023. 2 [24] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the traintest gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 2 [25] Junha Hyung, Kinam Kim, Susung Hong, Min-Jung Kim, and Jaegul Choo. Spatiotemporal skip guidance for enhanced video diffusion sampling. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 11006 11015, 2025. 2 [26] Vineet Jain, Kusha Sareen, Mohammad Pedramfar, and Siamak Ravanbakhsh. Diffusion tree sampling: Scalable inference-time alignment of diffusion models, 2025. 2 [27] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-toimage diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. 2 [28] Gyeongnyeon Kim, Wooseok Jang, Gyuseong Lee, Susung Hong, Junyoung Seo, and Seungryong Kim. Depth-aware guidance with self-estimated depth representations of diffusion models. Pattern Recognition, 153:110474, 2024. 2 [29] Peter Kocsis, Vincent Sitzmann, and Matthias Nießner. Intrinsic image diffusion for indoor single-view material estimation, 2024. 3 [30] Peter Kocsis, Lukas Hollein, and Matthias Nießner. Intrinsix: High-quality pbr generation using image priors, 2025. 3 [31] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have semantic latent space. arXiv preprint arXiv:2210.10960, 2022. 2 [32] Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Chih-Hao Lin, Jun Gao, Alexander Keller, Nandita Vijaykumar, Sanja Fidler, et al. Diffusion renderer: Neural inverse and forward rendering with video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2606926080, 2025. 3, 5, 6, 12 [33] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316, 2025. 2 [34] Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, and Lu Jiang. Autoregressive adversarial post-training for real-time interactive video generation. arXiv preprint arXiv:2506.09350, 2025. 2 [35] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [36] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [37] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. Advances in Neural Information Processing Systems, 36: 4750047510, 2023. 2 [38] Grace Luo, Trevor Darrell, Oliver Wang, Dan Goldman, and Aleksander Holynski. Readout guidance: Learning control from diffusion features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82178227, 2024. 2 [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 2 [40] Takafumi Saito and Tokiichiro Takahashi. Comprehensible In Proceedings of the 17th anrendering of 3-d shapes. nual conference on Computer graphics and interactive techniques, pages 197206, 1990. 3 [41] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2020. 2, 3 [42] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023. 4 [43] Nick Stracke, Stefan Andreas Baumann, Kolja Bauer, Frank Fundel, and Bjorn Ommer. Cleandift: Diffusion features without noise. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 117127, 2025. 2 [44] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems, 36:13631389, 2023. 2 [45] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 5 [46] Tianxing Wu, Chenyang Si, Yuming Jiang, Ziqi Huang, and Ziwei Liu. Freeinit: Bridging initialization gap in video diffusion models. In European Conference on Computer Vision, pages 378394. Springer, 2024. 3 [47] Yongqi Yang, Huayang Huang, Xu Peng, Xiaobin Hu, Donghao Luo, Jiangning Zhang, Chengjie Wang, and Yu Wu. Towards one-step causal video generation via adversarial selfdistillation. arXiv preprint arXiv:2511.01419, 2025. 2 [48] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion modIn Proceedings of the Computer Vision and Pattern els. Recognition Conference, pages 2296322974, 2025. 2 [49] Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, and Miloˇs Haˇsan. Rgbx: Image decomposition and synthesis using materialand lighting-aware diffusion models. In ACM SIGGRAPH 2024 Conference Papers, New York, NY, USA, 2024. Association for Computing Machinery. 2, [50] Chenlu Zhan, Wen Li, Chuyu Shen, Jun Zhang, Suhui Wu, and Hao Zhang. Bidirectional sparse attention for faster video diffusion training. arXiv preprint arXiv:2509.01085, 2025. 2 [51] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. Advances in Neural Information Processing Systems, 36:4553345547, 2023. 2 [52] Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Van Gool, and Radu Timofte. Plug-and-play image restoration with deep denoiser prior. TPAMI, 44(10):63606376, 2021. 2 10 [53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Scaling in-the-wild training for diffusion-based illumination harmonization and editing by imposing consistent light transport. In The Thirteenth International Conference on Learning Representations, 2025. 2 [54] Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Vsa: Faster video diffusion with trainable sparse attention. arXiv preprint arXiv:2505.13389, 2025. [55] Shilong Zhang, Wenbo Li, Shoufa Chen, Chongjian Ge, Peize Sun, Yida Zhang, Yi Jiang, Zehuan Yuan, Binyue Peng, and Ping Luo. Flashvideo: Flowing fidelity to detail for efficient high-resolution video generation. arXiv preprint arXiv:2502.05179, 2025. 2 11 DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders Supplementary Material S1. Analysis: Diffusion Features and Intrinsic"
        },
        {
            "title": "Previews",
            "content": "In this section, we provide further evidence that intermediate diffusion features contain cleaner intrinsic structure than the pseudo-ground-truth supervision. Although the pseudoground-truth intrinsics used for training may contain geometric inaccuracies, Figure 13 shows that our decoder can produce more stable and coherent geometry than the supervision itself. This highlights that diffusion features encode intrinsic scene structure reliably using their strong prior and that our decoder manages to extract this information even when training labels are imperfect. To further quantify the decodability of intrinsic signals, we retrain series of nonlinear decoders across all blocks and timesteps, shown in Figure 14. These experiments extend the linear-probing analysis from the main paper (Figure 2) and confirm that predictive power saturates early across the network hierarchy. While deeper decoders improve performance relative to linear predictors, the overall trend remains the same; the most reliably decodable structure appears in early or mid-level layers. Finally, Tables 69 and Figure 16 compare decoder performance across timesteps. The multi-branch architecture becomes increasingly beneficial as noise decreases, widening the gap over linear predictors. Importantly, x0prediction performs significantly worse than both feature decoders at early timesteps, even for RGB, and only surpasses them at approximately 16% of the denoising process. Although this is relatively early stage of denoising, it reveals much of the geometry of the dynamic scene and allows us to reconstruct rubber-like results (Figure 15). This supports our claim that early previews substantially benefit from decoding features rather than relying on the VAE decoder. S2. Synthetic Training Data Generation To train the intrinsic decoders, we constructed dataset designed to cover broad range of scene types. Table 5 lists the 40 scene categories used for prompt generation, spanning human activities, animals, natural environments, indoor and outdoor scenes, motion types, fantasy concepts, and more. For each category, we generated 25 prompts, yielding 1,000 prompts in total. We then ran DiffusionRenderer [32] to predict all intrinsics for decoder training, as described in 6.1. The proposed dataset exposes the decoder to the diversity of structures and appearance variations encountered Figure 13. Even when pseudo-GT data predicted with DiffusionRenderer [32] contains incorrect geometry, our decoder predicts plausible and consistent structure from diffusion features. during video diffusion. Training details for the decoders are provided in the main paper. S3. Analysis: More on the Toy Problem In the toy experiment demonstrating the superposition phenomenon in the main paper (4.2), we leverage controlled toy environment consisting of 4-frame sequences at 7 7 resolution for each frame. single white dot moves left, right, or remains stationary. We examine two variants: (1) motion-only uncertainty and (2) motion+position uncertainty, where the starting location is slightly jittered to induce multimodal clean states. This controlled setting reveals how diffusion models trained with MSE behave when the clean posterior is multimodal: at high-noise timesteps, the model predicts the posterior mean, producing in-between states that never occur in the data. Figure 17 illustrates typical artifacts in this setting, e.g., duplicated or faded dots, when using low number of function evaluations (NFE). For our toy multi-branch architecture, we use simpler mode-seeking objective based on the available dataset. Because we have direct access to the full ground-truth distribution, we can explicitly encourage each branch to predict specific data mode. This pushes each branch toward the data instead of toward the average, achieving similar effect to ℓ1 + perceptual loss in our final decoder model. Therefore, the final training loss selects the closest data point in terms of ℓ1 distance (mode-seeking loss), while an ensemble term 12 Figure 14. Nonlinear probing comparisons across timesteps and blocks. The reported loss is the last-epoch validation loss, ℓ1 + perceptual. The results show similar trend to linear decoding. Figure 15. Rubber-like 4D reconstruction results. Even at 10% of the timestep, each reconstruction represents the composition, geometry, and dynamics of the scene, while at the 20% timestep, refined reconstruction result is produced. This structure encourages each branch to specialize in one plausible mode, yielding clean and mode-consistent predictions. The ensemble prediction helps each branch maintain diversity by regularizing their average toward the mean. When the ensemble loss is removed, as shown in Figure 18, the branches collapse toward fewer modes and fail to capture the accurate multimodal distribution. Table 4 summarizes the numerical results. With 4 frames, the correct number of dots (of intensity exceeding 0.5) is 4 in expectation for motion-only sequences. The multi-branch decoder is the only method that consistently reproduces the correct mode structure under 1-NFE sampling. Distilled and few-step DDPM baselines produce artifacts under motion and position uncertainty, resulting in more dots. In contrast, the average number of boxes with motion and position uncertainty is approximately 3.8 (due to moving out of the frame at the last frame in two of nine cases), resulting in significantly fewer dots than expected. Our method alleviates this effect. Figure 16. PSNR comparison between x0-pred (the VAE decoder), Linear, and our method. In the high-noise regime, the linear and our decoders perform similarly, with the gap increasing as the denoising process progresses. The PSNR of the x0-pred decoder and our method crosses at 16% of the denoising steps, suggesting that early previews benefit substantially from our decoder. uses standard MSE objective: L(k) branch = min x0dataset x(k) 0 x01, Lens ="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) x(k) 0 x02 2. k=1 (11) (12) 13 Figure 19. Failure modes for different modalities. We steered each of the maps at 10% of the denoising steps. gradient update is applied in feature space using the Jacobian of D, the learned multi-branch decoder. Normals are steered using cosine loss, while other modalities use ℓ1 distance. Our goal is not to formalize new optimization framework but to demonstrate that preview-level edits can be propagated back into diffusion features. We explore simple proof-of-concept targets for different modalities: (1) Base color: cluster the predicted colors via K-means++ and shift toward another cluster. (2) Depth edges: enhance depth gradients using Sobel operator. (3) Normal flipping: invert the y-axis of the predicted normal map. These are intentionally minimal examples; more sophisticated target-construction methods could be used. Complementary to more traditional video editing methods, our latent steering method provides simple yet efficient way of steering the denoising trajectory toward more favorable directions with minimal waste of compute. We believe that steering during denoising presents brand-new avenue for controllable generation. The results shown in our paper are far from perfect and have several failure modes. Precise color steering is not always possible, and dramatic geometric editing such as completely removing one half of the depth map or flipping normals to point the surface in physically implausible orientation are some of the major failure cases, illustrated in Figure 19. We attribute these to: 1) the small base model we used might not have sufficient 3D understanding capability; 2) limited capacity of the trained decoder; 3) out-of-distribution problems; and 4) simplistic steering methodology and imperfect execution. We leave the improvements and more careful examination of the steering results to future work. S5. Benefits of Multi-Modal Previews Multi-modal previews provide several advantages over latent-space visualizations. First, intrinsic modalities, particularly depth and normals, reveal coarse scene geometry earlier in the denoising process than RGB or latents. SecFigure 17. Toy experiment. Multi-branch predictions recover separate modes without superposition. The green boxes represent the prediction of each branch, and the red box represents the averaged prediction of the branches. Figure 18. Without ensemble loss, the reduced diversity results in collapse to fewer number of modes and causes artifacts. Metric x0-pred Ours Content Predictability Visual Fidelity Scene Clarity 25.4% 27.1% 23.1% 74.6% 72.9% 76.9% Table 3. User study comparing our intrinsic preview method against the x0-pred baseline. S4. Details on Latent Steering We implement preview steering by applying small gradientbased modifications to intermediate diffusion features to guide the decoded intrinsic map toward chosen target. 14 that intrinsic modalities provide more informative and reliable early-stage previews than the x0-pred baseline. Data Method NFE Mean #Boxes Std. #Boxes Only Motion Motion+Position MB-Avg. CD DDPM DDPM DDPM GT MB-Avg. CD DDPM DDPM GT 1 1 20 200 Prev. 1K - 1 1 200 Prev. 1K - 4.0 8.2 7.5 5.0 4.0 4. 3.5 0.4 0.7 3.8 3.8 0.2 1.2 1.1 2.0 0.0 0.0 0.2 0.5 0.8 0.4 0.2 Table 4. Toy experiment results across configurations. MB-Avg. denotes the mean across branches of the average and standard deviation of the number of boxes. ond, base color previews offer simplified appearance information without lighting, making scene layout clearer. Third, our method produces all previews simultaneously from the same features, allowing users to cross-reference modalities at any timestep. See Figures 2123 for the timestep-wise evolution of the intrinsic modalities. Figure 20 compares latent renderings with our base color predictions. Intrinsic previews exhibit fewer lighting artifacts and present cleaner structural representation during early timesteps. S6. User Study Setup Here we provide additional details on the user study, specifically regarding the experimental setup and the questions participants were asked. The goal of the study was to evaluate the perceptual usefulness of intrinsic-based previews compared to the x0-pred baseline. Each participant was shown two preview representations for given reference video: (1) standard x0-pred preview and (2) our intrinsic-based preview, where participants could additionally consult predicted modalities (e.g., base color, depth, normals) alongside the RGB preview. For each trial, participants answered three questions designed to measure complementary aspects of preview quality: Content Predictability: Which representation allows you to predict the content of the reference video? This measures how well preview communicates the expected outcome of the diffusion process. Visual Fidelity: Which representation has fewer artifacts or errors (such as noise, flickering, etc.)? This assesses perceived stability and cleanliness. Scene Clarity: Which video more clearly shows the scene composition (objects, motion, layout, etc.)? This evaluates structural interpretability. total of 35 participants each evaluated 10 examples, yielding 350 responses for each question. As summarized in the main paper, participants consistently favored our intrinsic-based previews across all three criteria, indicating Figure 20. Comparison between RGB (from x0-pred) and base color (from our decoder) previews at 4% and 10% of the denoising steps. Our base color preview reveals structural components and color layout more clearly than the latent. 16 No. Category No. Category Human-Focused Categories 1 3 5 Human Portraits and Expressions Sports and Physical Activities Human Emotions and Reactions Animal Categories 7 Wild Animals in Natural Habitats 9 11 Marine Life and Underwater Scenes Insects and Microscopic Life Forests and Tree Scenes Nature and Vegetation Categories 12 14 Weather Phenomena 16 Seasonal Transformations Indoor Environment Categories 17 Home Interior Scenes Offices and Workspaces Outdoor Environment Categories Urban Cityscapes 21 Beach and Coastal Environments 23 2 4 6 8 10 13 15 18 Human Daily Activities Professions and Work Environments Celebrations and Social Events Domestic Animals and Pets Birds and Flying Creatures Flowers and Garden Beauty Natural Landscapes Restaurants and Dining Cultural and Educational Spaces Rural and Countryside 22 24 Mountain and Adventure Scenes Motion and Movement Categories Transportation and Vehicles 25 Flowing Elements (Water, Smoke, Particles) 27 26 Dance and Choreography 28 Mechanical and Industrial Motion Fantasy and Creative Categories 29 31 Fantasy Creatures and Magic Abstract and Surreal Concepts Complex Scene Categories 33 35 Crowd Scenes and Gatherings Time-lapse and Slow Motion Specialized Categories 37 39 Artistic and Stylized Visuals Technology and Modern Gadgets 30 32 Science Fiction and Futuristic Historical and Period Scenes Action and Adventure 34 36 Microscopic and Macro Worlds 38 40 Cooking and Food Preparation Transformations and Metamorphosis Table 5. 40 categories with 25 prompts each (1,000 total prompts) 17 Decoder Type RGB Base Color Depth Normal Metallic Roughness 4% Denoising Steps x0-pred Linear Ours 12.07 14.74 14.75 6% Denoising Steps x0-pred Linear Ours 14.15 16.17 16.05 8% Denoising Steps x0-pred Linear Ours 15.65 17.19 17. 10% Denoising Steps x0-pred Linear Ours 16.98 17.96 18.03 12% Denoising Steps x0-pred Linear Ours 18.01 18.50 18.70 14% Denoising Steps x0-pred Linear Ours 18.83 18.88 19. 16% Denoising Steps x0-pred Linear Ours 19.59 19.18 19.58 18% Denoising Steps x0-pred Linear Ours 20.16 19.41 19.92 20% Denoising Steps x0-pred Linear Ours 20.69 19.60 20. - 13.88 14.16 - 14.74 15.22 - 15.20 15.89 - 15.51 16.38 - 15.68 16.79 - 15.80 17. - 15.88 17.08 - 15.94 17.29 - 15.97 17.34 - 14.52 14.86 - 15.10 15.74 - 15.35 16. - 15.52 16.95 - 15.62 17.28 - 15.68 17.59 - 15.69 17.70 - 15.69 17.71 - 15.69 17. - 18.92 19.02 - 19.17 19.39 - 19.39 19.66 - 19.54 20.04 - 19.63 20.11 - 19.70 20. - 19.76 20.44 - 19.79 20.56 - 19.82 20.52 - 10.98 15.53 - 11.02 16.43 - 11.19 16. - 11.22 16.42 - 11.23 15.95 - 11.25 17.10 - 11.25 16.83 - 11.25 17.14 - 11.25 16. - 15.56 16.15 - 15.69 16.60 - 15.67 16.74 - 15.75 17.03 - 15.75 17.35 - 15.75 17. - 15.75 17.49 - 15.73 17.47 - 15.71 17.73 Table 6. PSNR results for different decoder types across denoising timesteps. Higher is better. 18 Decoder Type RGB Base Color Depth Normal Metallic Roughness 4% Denoising Steps x0-pred Linear Ours 0.069 0.037 0.037 6% Denoising Steps x0-pred Linear Ours 0.043 0.027 0.029 8% Denoising Steps x0-pred Linear Ours 0.031 0.022 0.022 10% Denoising Steps x0-pred Linear Ours 0.024 0.018 0.019 12% Denoising Steps x0-pred Linear Ours 0.019 0.016 0.016 14% Denoising Steps x0-pred Linear Ours 0.016 0.015 0.014 16% Denoising Steps x0-pred Linear Ours 0.014 0.014 0.013 18% Denoising Steps x0-pred Linear Ours 0.012 0.013 0.012 20% Denoising Steps x0-pred Linear Ours 0.011 0.013 0.012 - 0.047 0.046 - 0.038 0.037 - 0.035 0.031 - 0.032 0.028 - 0.031 0. - 0.031 0.025 - 0.030 0.024 - 0.030 0.024 - 0.030 0.023 - 0.044 0.046 - 0.038 0. - 0.036 0.032 - 0.034 0.029 - 0.034 0.027 - 0.033 0.025 - 0.033 0.025 - 0.033 0. - 0.033 0.024 - 0.017 0.016 - 0.016 0.015 - 0.015 0.014 - 0.014 0.013 - 0.014 0. - 0.014 0.012 - 0.013 0.011 - 0.013 0.011 - 0.013 0.011 - 0.143 0.155 - 0.132 0. - 0.130 0.127 - 0.128 0.131 - 0.128 0.133 - 0.128 0.126 - 0.127 0.126 - 0.127 0. - 0.127 0.123 - 0.042 0.042 - 0.040 0.039 - 0.040 0.038 - 0.039 0.034 - 0.038 0. - 0.038 0.034 - 0.038 0.032 - 0.038 0.032 - 0.038 0.033 Table 7. MSE results for different decoder types across denoising timesteps. Lower is better. Decoder Type RGB Base Color Depth Normal Metallic Roughness 4% Denoising Steps x0-pred Linear Ours 0.186 0.141 0.135 6% Denoising Steps x0-pred Linear Ours 0.140 0.115 0. 8% Denoising Steps x0-pred Linear Ours 0.114 0.099 0.095 10% Denoising Steps x0-pred Linear Ours 0.093 0.090 0.084 12% Denoising Steps x0-pred Linear Ours 0.080 0.084 0. 14% Denoising Steps x0-pred Linear Ours 0.072 0.080 0.072 16% Denoising Steps x0-pred Linear Ours 0.065 0.077 0.069 18% Denoising Steps x0-pred Linear Ours 0.060 0.075 0. 20% Denoising Steps x0-pred Linear Ours 0.056 0.073 0.063 - 0.166 0.156 - 0.148 0.136 - 0.140 0.124 - 0.134 0. - 0.132 0.111 - 0.130 0.109 - 0.129 0.108 - 0.129 0.106 - 0.128 0.105 - 0.161 0. - 0.149 0.133 - 0.143 0.121 - 0.140 0.115 - 0.139 0.111 - 0.138 0.107 - 0.138 0. - 0.138 0.106 - 0.138 0.104 - 0.091 0.090 - 0.088 0.085 - 0.085 0.081 - 0.084 0. - 0.083 0.077 - 0.082 0.075 - 0.082 0.074 - 0.081 0.073 - 0.081 0.073 - 0.309 0. - 0.297 0.241 - 0.293 0.242 - 0.291 0.241 - 0.291 0.243 - 0.290 0.233 - 0.290 0. - 0.290 0.230 - 0.290 0.230 - 0.168 0.160 - 0.163 0.152 - 0.163 0.149 - 0.161 0. - 0.161 0.139 - 0.160 0.140 - 0.160 0.135 - 0.160 0.136 - 0.160 0.135 Table 8. L1 error results for different decoder types across denoising timesteps. Lower is better. 20 Decoder Type RGB Base Color Depth Normal Metallic Roughness 4% Denoising Steps x0-pred Linear Ours 0.751 0.706 0.689 6% Denoising Steps x0-pred Linear Ours 0.634 0.628 0.601 8% Denoising Steps x0-pred Linear Ours 0.522 0.571 0.523 10% Denoising Steps x0-pred Linear Ours 0.433 0.535 0.466 12% Denoising Steps x0-pred Linear Ours 0.369 0.511 0.426 14% Denoising Steps x0-pred Linear Ours 0.319 0.494 0.397 16% Denoising Steps x0-pred Linear Ours 0.279 0.482 0.373 18% Denoising Steps x0-pred Linear Ours 0.249 0.472 0.354 20% Denoising Steps x0-pred Linear Ours 0.224 0.464 0.343 - 0.681 0.601 - 0.623 0.528 - 0.582 0. - 0.554 0.422 - 0.537 0.386 - 0.525 0.366 - 0.517 0.350 - 0.510 0.338 - 0.505 0. - 0.665 0.501 - 0.651 0.467 - 0.641 0.435 - 0.632 0.411 - 0.627 0.392 - 0.624 0. - 0.621 0.376 - 0.620 0.368 - 0.619 0.362 - 0.505 0.456 - 0.495 0.423 - 0.483 0. - 0.469 0.362 - 0.460 0.346 - 0.453 0.333 - 0.447 0.325 - 0.444 0.316 - 0.441 0. - 0.791 0.607 - 0.789 0.561 - 0.787 0.556 - 0.785 0.538 - 0.785 0.536 - 0.784 0. - 0.783 0.517 - 0.783 0.508 - 0.782 0.511 - 0.683 0.545 - 0.689 0.515 - 0.692 0. - 0.692 0.477 - 0.692 0.465 - 0.693 0.462 - 0.692 0.453 - 0.691 0.451 - 0.690 0. Table 9. LPIPS results for different decoder types across denoising timesteps. Lower is better. 21 Figure 21. Timestep-wise evolution of base color, normal, and albedo. Coarse geometry appears early and refines through denoising. 22 Figure 22. Timestep-wise evolution of base color, normal, and albedo. Coarse geometry appears early and refines through denoising. Figure 23. Timestep-wise evolution of base color, normal, and albedo. Coarse geometry appears early and refines through denoising."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University of Washington"
    ]
}