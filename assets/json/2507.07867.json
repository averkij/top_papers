{
    "paper_title": "Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders",
    "authors": [
        "Dimitrios Bralios",
        "Jonah Casebeer",
        "Paris Smaragdis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Neural audio codecs and autoencoders have emerged as versatile models for audio compression, transmission, feature-extraction, and latent-space generation. However, a key limitation is that most are trained to maximize reconstruction fidelity, often neglecting the specific latent structure necessary for optimal performance in diverse downstream applications. We propose a simple, post-hoc framework to address this by modifying the bottleneck of a pre-trained autoencoder. Our method introduces a \"Re-Bottleneck\", an inner bottleneck trained exclusively through latent space losses to instill user-defined structure. We demonstrate the framework's effectiveness in three experiments. First, we enforce an ordering on latent channels without sacrificing reconstruction quality. Second, we align latents with semantic embeddings, analyzing the impact on downstream diffusion modeling. Third, we introduce equivariance, ensuring that a filtering operation on the input waveform directly corresponds to a specific transformation in the latent space. Ultimately, our Re-Bottleneck framework offers a flexible and efficient way to tailor representations of neural audio models, enabling them to seamlessly meet the varied demands of different applications with minimal additional training."
        },
        {
            "title": "Start",
            "content": "RE-BOTTLENECK: LATENT RE-STRUCTURING FOR NEURAL AUDIO AUTOENCODERS Dimitrios Bralios1 Jonah Casebeer2 Paris Smaragdis1 1 University of Illinois Urbana-Champaign 2 Adobe Research 5 2 0 2 0 1 ] . [ 1 7 6 8 7 0 . 7 0 5 2 : r ABSTRACT Neural audio codecs and autoencoders have emerged as versatile models for audio compression, transmission, feature-extraction, and latent-space generation. However, key limitation is that most are trained to maximize reconstruction fidelity, often neglecting the specific latent structure necessary for optimal performance in diverse downstream applications. We propose simple, post-hoc framework to address this by modifying the bottleneck of pre-trained autoencoder. Our method introduces Re-Bottleneck, an inner bottleneck trained exclusively through latent space losses to instill userdefined structure. We demonstrate the frameworks effectiveness in three experiments. First, we enforce an ordering on latent channels without sacrificing reconstruction quality. Second, we align latents with semantic embeddings, analyzing the impact on downstream diffusion modeling. Third, we introduce equivariance, ensuring that filtering operation on the input waveform directly corresponds to specific transformation in the latent space. Ultimately, our ReBottleneck framework offers flexible and efficient way to tailor representations of neural audio models, enabling them to seamlessly meet the varied demands of different applications with minimal additional training. Index Terms autoencoders, latent modeling, structured latent spaces, equivariance, generative modeling 1. INTRODUCTION Neural audio autoencoders and codecs have become foundational in modern audio processing, enabling high-fidelity reconstruction from compact latent representations [1, 2, 3, 4]. These models are powerful components, driving innovation in areas like audio generation via next-token prediction [5, 6, 7], latent diffusion [4] and supporting tasks such as classification, enhancement, and source separation [8, 9]. However, their primary training objective, optimizing reconstruction, often results in latent spaces that lack the specific structure required for optimal performance in many downstream applications. This creates crucial disconnect: general-purpose latents from reconstruction-focused autoencoders are not inherently aligned with the needs of specialized audio tasks. Addressing this structural deficit is challenging. Current practice typically involves either adapting downstream models to accommodate the autoencoders arbitrary latent structure (e.g., requiring specialized token-prediction orders [5], or coarse-to-fine diffusion [10]), or, more commonly, undertaking the significant effort of redesigning and retraining the autoencoder from scratch to embed desired properties. Examples of this costly retraining include developing taskspecific tokenizers for improved prediction [11] or designing models for semantic alignment directly in the latent space [12, 13, 14]. While effective, these tailored autoencoders demand substantial architectural modifications and retraining on vast datasets, representing major computational and development burden. The value of explicitly structured latent spaces is particularly evident in generative models like latent diffusion, where structured representations have been shown to significantly improve training efficiency and generation quality in other domains [15, 16]. Applying similar principles to audio, such as incorporating semantic alignment or equivariance, holds great promise but would require modifying and retraining the core autoencoder. Given the increasing availability of powerful audio autoencoders pre-trained on large datasets, retraining from scratch to impose new structures is often impractical. Researchers are faced with the dilemma of using suboptimal existing latents or incurring the prohibitive cost of full autoencoder training. In this work, we ask: Can we efficiently impose desired structural properties onto the latent spaces of existing, off-the-shelf audio autoencoders without the prohibitive cost of full model retraining? To address this, we introduce Re-Bottleneck, novel, lightweight framework. Re-Bottleneck is inspired by the Re-Encoder [17] and operates post-hoc by training compact inner autoencoder, combined with an adversarial discriminator, within the latent bottleneck of pre-trained model. This approach allows us to restructure the latent space using only latent-domain losses, circumventing complex waveform-level objectives and avoiding the extensive computation and tuning required for end-to-end autoencoder retraining. Re-Bottleneck offers fast and flexible path to obtaining structured latents from pre-trained models, enabling their more effective use across diverse downstream audio tasks. We demonstrate ReBottlenecks capabilities through three distinct studies: Ordered Channels: Enforcing monotonic ordering across latent channels to capture progressively finer detail, while maintaining near-full reconstruction fidelity. Semantic Alignment: Aligning latent vectors with embeddings from pre-trained audio (e.g., BEATs [18]) and text (e.g., T5 [19]) models, and analyzing its impact on downstream diffusion-based audio generation. Equivariance Constraints: Introducing transformationequivariance in the latent domain, ensuring predictable latent transformations correspond to given filtering operations. that Collectively, lightweight, these experiments highlight latent-only adaptations via the Re-Bottleneck framework can effectively yield structured and application-aligned representations without requiring modifications or retraining of the base autoencoder. Our framework thus provides an efficient, flexible, and rapid paradigm for tailoring the latent spaces of neural audio models, enabling researchers to seamlessly adapt powerful pre-trained models to meet the varied demands of different applications and supporting fast, iterative experimentation. In the spirit of reproducible science and to enable use of this tool, training code is available here1. 1https://github.com/dbralios/rebottleneck 2. PROPOSED METHOD 2.1. Overview Our framework employs pre-trained and frozen neural audio autoencoder, comprising an encoder AE and decoder AD. The encoder transforms potentially multi-channel input waveform RN to compact latent representation = AE(x) RCT . The decoder reconstructs the waveform from the latent representation, ˆx = AD(z). As is common in audio autoencoders, the autoencoders bottleneck is parameterized as variational autoencoder to facilitate learning within the latent space. Operating exclusively within the latent domain of the base autoencoder, the Re-Encoder(R), consisting of an encoder RE and decoder RD, maps the original latent representation to an inner latent representation, the Re-Bottleneck((cid:101)z): (cid:101)z = RE(z) RCT where are the dimensions of this inner latent space. The Re-Bottleneck, (cid:101)z, is specifically designed to exhibit desired properties relevant to the downstream task. For instance, these properties could include encouraging specific channels to represent distinct semantic features, enforcing particular ordering of information, or promoting invariance/equivariance to certain input transformations. The Re-Encoder then reconstructs an approximation of the orig- (1) inal latent representation z, denoted as ˆz: ˆz = R(z) = RD (cid:0)RE(z)(cid:1). (2) Crucially, only the Re-Encoder components (RE and RD) are trained, while the pre-trained audio autoencoder (AE, AD) remains frozen. This strategy preserves the high-quality audio reconstruction capabilities of the original autoencoder while allowing targeted manipulation and structuring of the latent bottleneck for the specific downstream task. This approach also sidesteps the significant challenge of training high-quality audio decoder from scratch, which is known to be notoriously difficult task, particularly in achieving perceptual fidelity. 2.2. Latent Space Training Training the Re-Encoder primarily within the latent space offers significant advantages in terms of computational efficiency and training stability compared to methods that require computationally expensive forward and backward passes through the high-dimensional audio decoder AD. Objectives defined in the waveform or spectral domains, often combined with discriminators operating on reconstructed audio, necessitate the materialization of waveforms and can introduce complex interactions with the structural regularization terms we apply to (cid:101)z. The fundamental objective for training the ReEncoder is latent space reconstruction loss, encouraging ˆz to be close to the original latent z. LR (cid:2)AE(x) R(AE(x))2 rec = ExD (cid:3) , (3) The loss terms for the Re-Encoder include an adversarial term and feature matching term, designed to encourage to produce latents that fool the discriminator and match the feature statistics of real latents: LR adv = ExD (cid:2)(1 D(R(AE(x)))2(cid:3) , LR fm = ExD (cid:34) (cid:88) i=1 Di(AE(x)) Di(R(AE(x)))1 Di(R(AE(x)))1 (5) (cid:35) , (6) where Di denotes the feature map at the i-th layer of D. Concurrently, the Re-Bottleneck (cid:101)z can optionally be regularized with KL-divergence term LR kl to encourage its distribution towards desired prior, similar to the base autoencoders bottleneck regularization. Finally, and critically, we apply task-specific structural losses to the Re-Bottleneck (cid:101)z . These losses, denoted collectively as LR task, are designed to enforce the desired properties on (cid:101)z that are relevant to the downstream task (e.g., disentanglement losses, channel-wise sparsity, equivariance penalties). The specific form of LR task depends on the particular downstream objective. The total training objective for the Re-Encoder is weighted sum of these loss components: LR = λrecLR rec + λadvLR adv + λfmLR fm + λklLR kl + λtaskLR task, (7) where the λ are hyperparameters balancing the contribution of each loss term. Empirically, we find these weights easy to balance due to the well-behaved nature of training on VAE latents. The discriminator is trained using LD adv. To demonstrate Re-Bottleneck, we manipulate the base autoencoders bottleneck in three distinct ways. 2.3. Re-Bottleneck Types 2.3.1. Ordered Re-Bottleneck We induce monotonic, hierarchical ordering of the channels within the inner bottleneck, (cid:101)z, by implementing nested dropout during training [20]. At each training iteration, we randomly sample prefix length U{1, }. This value, effectively sets the number of active channels in (cid:101)z. Based on the sampled (cid:101)z, we construct binary mask (m) of size , where (m) c,t = 1 if the channel index is less than or equal to and 0 otherwise. This mask is then applied element-wise to the inner latent representation (cid:101)z. Consequently, the forward pass through the rest of the model becomes (cid:16) (cid:17) R(m)(x) = RD (m) RE(x) . (8) rec, LR fm on R(m)(x), and sample an in evWe optimize LR ery batch. This strategy forces to encode the most salient information into the earliest channels, thereby producing an ordered latent. adv, LR where is the training dataset and = AE(x) serves as both the input to and the target for reconstruction. To ensure that the ReEncoders output latents ˆz remain within the distribution of latents produced by the base encoder AE, we employ single latent discriminator D. The role of is to distinguish between real latents (those directly from AE(x) and fake latents (those reconstructed by the Re-Encoder, R(AE(x)). The discriminators training objective is standard adversarial loss: LD adv = ExD (cid:2)(1 (AE(x)))2 + (R (AE(x)))2(cid:3) . (4) 2.3.2. Semantically-Aligned Re-Bottleneck We induce semantic structure in the inner bottleneck, (cid:101)z, by defining the task loss LR task as contrastive loss. This loss encourages similarity between (cid:101)z and representations from pretrained semantic encoder, Fsem (e.g. self-supervised audio model). Specifically, we first obtain sequence of semantic embeddings = Fsem(x) RHK from the input x. These are then temporally-pooled to produce semantic vector RH . Similarly, we take the the inner latent representation (cid:101)z apply linear transform (linear probe) to map its dimensions to dimensions, and then temporally-pool to get RH . The constrastive loss is then calculated as: LR task = 1 (cid:88) i=1 log exp(cid:0)sim( zi, si)/τ (cid:1) j=1 exp(cid:0)sim( zi, sj)/τ (cid:1) , (cid:80)B (9) where sim(u, v) = uv/uv, τ is temperature hyperparameter, and is the number of training samples in the batch. This objective encourages to capture the semantic content of the input in straightforward and aligned manner within (cid:101)z. 2.3.3. Equivariant Re-Bottleneck We induce equivariance of the encoder RE with respect to transformation g() by (with abuse of notation) training it such that g(RE(x)) is equal to RE(g(x)). This objective encourages RE to commute with g(). Specifically, we consider parametric Gaussian low-pass filter gα() with cutoff frequency α, and its counterpart operating in the latent space, hα(). The design of hα() mimics the behavior of Gaussian filter in the frequency domain. To achieve equivariance with respect to this filtering operation, we employ two strategies. First, an explicit loss: LR task = Ex,α (cid:2)hα((cid:101)z) RE(AE(gα(x)))2 (cid:3) . (10) This loss explicitly trains the filtered latent representation hα((cid:101)z) to match the latent representation of the filtered input RE(AE(gα(x))). Inspired by [21], we modify our reconstruction term, LR rec = Ex,α (cid:2)AE(gα(x)) RD(hα(RE(z)))2 (cid:3) , (11) and apply analogous modifications to all discriminator terms. This loss trains the full autoencoder R. Jointly, these losses train to commute with filtering, enforcing unique regularization on (cid:101)z. General hyperparameters include optimizing the encoderdecoder with Adam using learning rate of 5 104 and the discriminator with 1 104. The InfoNCE temperature τ was set to 0.07. In the ordered variant, the mask is only applied to 75% of the batch. We trained the ordered and equivariant variants on 3 chunks with batch size of 64 for 12 epochs. Due to GPU memory limitations, for the semantic variant we halved the batch size and doubled the chunk length. We train on single NVIDIA L40S GPU for < 48 hours. 3.2. Diffusion Experiments We evaluate the downstream impact of Re-Bottleneck variants on text-to-audio diffusion by integrating them into the Stable Audio Open (SAO) pipeline [4]. Latent representations from all variants are first standardized to zero mean and unit variance based on training set statistics. We adopt the default SAO diffusion hyperparameters, omitting only weight decay. We train on 6-second chunks with batch size of 72 across three NVIDIA L40S GPUs. Each variant is trained for 100K steps, requiring about four days per experiment. 3.3. Data For training, we use the Jamendo-FMA-captions dataset [10], which comprises synthetically generated captions for the MTGJamendo [23] and FMA [24] collections, totaling approximately 122K stereo files at 44.1 KHz ( 8, 000 hours). We do not use Freesound due to being unable to locate stereo version. For evaluation, we use the SongDescriber (no-vocals) [25, 4] benchmark derived from MTG-Jamendo. To prevent data leakage, the training split is sanitized by removing duplicate track IDs, exact audio-hash duplicates, and similar samples based on mel-spectrogram descriptors, following [26]. 3. EXPERIMENTAL SETUP 3.4. Evaluation To validate the efficacy of Re-Bottlenecks, we conduct three experiments corresponding to the three Re-Bottleneck types above. 3.1. Re-Bottleneck Experiments For all experiments, we use the publicly released Stable Audio Open (SAO) VAE [4] as our frozen A. This VAE compresses stereo 44.1 KHz audio into 64 channel latents at 21.5 Hz. We use the provided checkpoint, originally trained for over 19 days on 32 GPUs. In the semantic alignment experiments we use BEATs iter3+ [18] and T5-base [19]. The Re-Encoder model, comprising symmetric encoder (RE) and decoder (RD) uses ConvNeXt-V2 backbone [22]. Both encoder and decoder blocks are constructed from 4 sequential ConvNeXt-V2 units. These units have hidden dimension 768 and are placed between linear layers that match channel dimensions to the input and inner bottleneck. The complete model has 19.1 In most cases, the inner-bottleneck is parameterized parameters. as VAE. The latent discriminator follows the architecture of the multi-band discriminator [3]. The discriminator takes the input inner latents, structured as single-channel, 3-dimensional tensor. It is built from sequence of Conv2d layers with 256 hidden channels, applying LeakyReLU activation after every layer except the last. For the base model variant, the loss weights are set as follows: λrec = 1.0, λkl = 104, λadv = 0.5, and λfm = 1. In the semantic variant, λtask = 2.5, and in the equivariant variant λtask = 0.5. We evaluate reconstruction fidelity using STFT, mel distance, and SISDR, all computed with the auraloss library [27]. In the case of the diffusion model, generation quality is measured by the FAD via the fadtk toolkit [28] configured with the CLAP-LAIONMusic model. To assess prompt alignment, we report the CLAP score, which quantifies audiotext similarity [4]. 4. EXPERIMENTS & RESULTS This section demonstrates the flexibility of the Re-Bottleneck framework by showcasing various latent space modifications. Training each Re-Bottleneck for these demonstrations required less than 48 GPU hours, representing under 0.33% of the 14.5K GPU hours used for the base autoencoder. 4.1. Ordered Re-Bottleneck In this experiment, we evaluate the ability of the Re-Bottleneck to impose structured importance on latent channels and to induce decorrelation among them. We compared the reconstruction performance of pre-trained SAO when latent channels were progressively removed, using different channel ordering strategies: random dropout, PCA, and three configurations of the Re-Bottleneck. As illustrated in Fig. 1, random dropout (pink) showed rapid decline in performance as channels were removed. PCA (grey) provided linear benchmark for performance degradation. The Re-Bottleneck Fig. 1. Reconstruction fidelity metrics (lower is better) as function of the number of retained latent channels. We compare our Re-Bottlenecks against PCA baseline (gray crosses), and random dropout baseline (pink diamonds) which randomly selects channels to set to zero. The dashed horizontal line denotes the SAO full-channel VAE baseline, and inset zooms highlight performance in the high-channel regime. Fig. 2. Pearson correlation matrices of latent channels comparing the SAO VAE baseline (left) and our Re-Bottleneck (right, VAE, discriminator, MSE). Lower off-diagonal correlation in our model indicates more decorrelated, less redundant latent representation. trained with only an MSE loss (blue) achieved significantly better reconstruction performance than both random dropout and PCA across most channel counts. However, for the highest channel counts (i.e., when few channels were removed), random dropout and PCA slightly outperformed the MSE-only Re-Bottleneck. To improve performance, we introduced latent discriminator loss, resulting in the Re-Bottleneck variant shown in green. This configuration improved performance over the MSE-only model, particularly for larger channel counts. These first results demonstrate ordering but not orthogonality. To achieve this, we trained Re-Bottleneck variant (shown in purple) using combination of latent MSE, latent discriminator, and latent KL divergence loss. As demonstrated by the cross-channel correlation matrix in Fig 2, the original VAE latents exhibit significant off-diagonal correlation. In contrast, the latents produced by this Re-Bottleneck variant display pronounced diagonal structure, indicating successful decorrelation. This experiment highlights the Re-Bottleneck frameworks power in post-hoc tailoring of pretrained latent spaces, demonstrating its ability to learn both an importance ordering and decorrelated, potentially normalized, representation. This effectively positions the Re-Bottleneck as learned, non-linear modern day PCA for neural audio codecs. Fig. 3. 2D PCA visualization of mean-pooled audio embeddings. Each point represents one audio file, colored by K-means clusters computed in the BEATs embedding space. This comparison highlights how semantic cluster structure is preserved or altered across different embedding spaces. 4.2. Semantically Aligned Re-Bottleneck We evaluate the ability of the Re-Bottleneck to align pre-trained autoencoder latent space with that of semantic model, while preserving invertibility. Unlike autoencoders focused on reconstruction, semantic models capture meaningful data attributes. We train the Re-Bottleneck using reconstruction, discriminator, and contrastive InfoNCE losses (aligning to BEATs [18]/T5 [19] embeddings) to instill semantic structure in the autoencoder latent space. First, we visualize semantic structure via PCA of mean-pooled latents from the SongDescriber dataset, clustered by BEATs embeddings  (Fig. 3)  . The baseline VAE, trained for reconstruction, shows poor semantic clustering. The Re-Bottleneck trained with semantic alignment shows notably improved cluster separation compared to the VAE, while largely retaining reconstruction quality (Tab. 1). We measure alignment using Centered Kernel Alignment (CKA) [29, 30] and Projection Weighted Canonical Correlation Analysis (PWCCA) [31] (Tab. 2). The baseline SAO bottleneck yielded 0.43 CKA / 0.63 PWCCA. non-reconstructive upper bound (semantic loss only) achieved 0.69 CKA / 0.83 PWCCA. Our model including reconstruction loss alongside semantic objectives, achieved 0.70 CKA / 0.78 PWCCA. This shows that the model recovers nearly the upper bounds semantic alignment, indicating strong alignment and invertibility are largely compatible in the case of BEATs. Linear probe metrics (Sec. 2.3.2) confirm that our linear probe effectively transfers semantic structure into the latent space. Reconstruction performance (Tab. 1) confirms invertibility. The semantically-aligned Re-Bottleneck showed 5% degradation versus Table 1. Reconstruction performance on 10 chunks from SongDescriber (no vocals). Model STFT mel SISDR T5 aligned ReBot BEATs aligned ReBot Equivariant ReBot SAO Baseline 1.29 1.29 1.39 1.27 0.79 0.81 0. 0.77 6.2 dB 6.2 dB 6.3 dB 6.5 dB Table 2. Alignment metrics (CKA / PWCCA) with mean-pooled T5 and BEATs embedding targets. vs compares the meanpooled bottleneck representation to the target, LP vs compares the linear probe to the target, and vs LP compares the bottleneck to the linear probe. Model Target vs LP vs vs LP T5 aligned ReBot 0.19 / 0.71 0.18 / 0.73 0.98 / 1.00 BEATs aligned ReBot BEATs - λrec, λadv, λfm = 0 BEATs 0.70 / 0.78 0.69 / 0.83 0.65 / 0.82 0.66 / 0. 0.96 / 0.98 0.97 / 1.00 SAO Baseline T5 BEATs 0.15 / 0.68 0.43 / 0.63 - - - - the baseline autoencoder, trading this for 20-60% gain in semantic capture (CKA/PWCCA). This promising trade-off is achieved with efficient training (< 48 GPU hours). 4.3. Equivariant Re-Bottleneck We evaluate the Re-Bottleneck on its ability to enable equivariance, the property where known transformation of the input audio corresponds to another known transform of its latent representation. Enforcing equivariance creates structured latent space that reflects these input-domain changes. To demonstrate this, we apply corresponding filters: Gaussian filter Gα[k] = exp(0.5 k2 α2) in the STFT domain, and hα(c) = exp(0.5 mel(c)1.4 α1.4) in the latent space. This latent filter hα is designed to mirror the STFT filter Gα by assuming latent channel index maps to mel-scaled frequency, mel(c). During training, we sample the cutoff with minimum value of α corresponding to 1.4 KHz. Table 3 reports STFT and mel distances (STFT / mel) comparing the output of each method to the low-pass-filtered target audio at three cutoff frequencies. Our Re-Bottleneck applies the filter in its latent space before decoding. Its decoded output quality is comparable to that of the filtered target autoencoded by the base SAO VAE (labeled AE Target in the table), confirming effective filtering in the latent domain. In contrast, applying the same latent filter to standard VAEs latent space (even after channel ordering attempts) yields substantially higher errors. Qualitatively, we verified that the spectral envelope of the Re-Bottleneck output matched the target. Next, we verify that the Re-Bottleneck learns meaningfully different latent structure compared to the original VAE. In toy experiment, we created 0 dB mixtures of clean audio and chirp signal. We then attempted chirp removal by applying magnitude-based mask to the latent representation before decoding. This resulted in 1.3 dB SISDR for the Re-Bottleneck, significantly outperforming the original VAE which yielded -1.3 dB SISDR with the same masking. This demonstrates Re-Bottleneck structures the latent space to better separate components, confirming the learned structural difference. Finally, we ablate the explicit loss (Eq. 10) to confirm its conTable 3. (STFT / mel) distances to low-pass filtered target."
        },
        {
            "title": "AE Target",
            "content": "SAO + Latent Filtering"
        },
        {
            "title": "1.4 KHz\n2.8 KHz\n5.5 KHz",
            "content": "0.98 / 0.91 1.08 / 0.85 1.17 / 0.81 0.88 / 0.78 1.05 / 0.81 1.16 / 0.79 5.08 / 3.76 4.10 / 2.77 2.60 / 1.81 Fig. 4. FAD scores versus training steps, using the clap-laion-music backbone. The standard SAO (orange squares) starts best but is surpassed by the semantic Re-Bottleneck (blue circles). The ordered and T5-aligned Re-Bottlenecks do not improve downstream performance, while the equivariant matches baseline performance. tribution to learning the desired latent transformation. Omitting this term causes the equivariance error to increase by more than an order of magnitude compared to the model trained with it. This confirms the explicit loss is crucial for learning the equivariance property. 4.4. Investigating Diffusability Leveraging Re-Bottlenecks to simplify latent space design, we evaluate their impact on downstream diffusion using the standard SAO pipeline. Fig. 4 shows FAD scores over training iterations. After 100,000 steps, the baseline SAO scored 0.435 FAD and 0.185 CLAP Score. Our BEATs Re-Bottleneck improved performance to 0.411 FAD and 0.191 CLAP. The equivariant Re-Bottleneck (0.428 FAD, 0.176 CLAP) was closer to the baseline in terms of FAD. However, neither the T5-aligned (0.440 FAD, 0.181 CLAP) nor the ordered (0.443 FAD, 0.177 CLAP) variants showed improvement. This demonstrates the Re-Bottlenecks value as versatile framework for latent prototyping. 5. CONCLUSION While pre-trained neural audio codecs excel at reconstruction, their default latent spaces often lack the specific structure needed for optimal performance across diverse downstream tasks. Our proposed Re-Bottleneck framework offers flexible, post-hoc solution to this limitation. By applying targeted latent-space losses, demonstrated through experiments in achieving ordered channel importance, semantic alignment (while largely preserving reconstruction), and latent space equivariance, the Re-Bottleneck effectively instills userdefined structure into pre-trained representations. This approach is remarkably efficient, requiring minimal additional trainingusing less than third of percent of the compute needed for the original VAE. This efficiency allows neural audio models to readily and cost-effectively adapt to varying task demands without requiring expensive retraining of the base model. 6. REFERENCES [1] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi, Soundstream: An endto-end neural audio codec, IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 30, pp. 495507, 2021. [2] Alexandre Defossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi, High fidelity neural audio compression, Transactions on Machine Learning Research, 2023. [3] Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar, High-fidelity audio compression with improved rvqgan, Proc. NeurIPS, 2024. [4] Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons, Stable audio open, in Proc. ICASSP. IEEE, 2025, pp. 15. [5] Andrea Agostinelli, Timo Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al., arXiv preprint Musiclm: Generating music from text, arXiv:2301.11325, 2023. [6] Flores Garcia, Seetharaman, Kumar, and Pardo, Vampnet: Music generation via masked acoustic token modeling, 24th Proc. ISMIR, 2023. [7] Zalan Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, and Marco Tagliasacchi, SoundarXiv preprint storm: Efficient parallel audio generation, arXiv:2305.09636, 2023. [8] Ansh Mishra, Jia Qi Yip, and Eng Siong Chng, Time-domain heart sound classification using neural audio codecs, in Proc. ICAICTA. IEEE, 2024, pp. 15. [9] Jia Qi Yip, Chin Yuen Kwok, Bin Ma, and Eng Siong Chng, Speech separation using neural audio codecs with embedding loss, in Proc. APSIPA ASC. IEEE, 2024, pp. 16. [10] Luca Lanzendorfer, Tongyu Lu, Nathanael Perraudin, Dorien Herremans, and Roger Wattenhofer, Coarse-to-fine text-to-music latent diffusion, in Proc. ICASSP. IEEE, 2025, pp. 15. [11] Jean-Marie Lemercier, Simon Rouard, Jade Copet, Yossi Adi, and Alexandre Defossez, An independence-promoting loss for music generation with language models, in Proc. ICML, 2024. [16] Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj, Masked autoencoders are effective tokenizers for diffusion models, in Proc. ICML, 2025. [17] Dimitrios Bralios, Paris Smaragdis, and Jonah Casebeer, Learning to upsample and upmix audio in the latent domain, arXiv preprint arXiv:2506.00681, 2025. [18] Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, Wanxiang Che, Xiangzhan Yu, and Furu Wei, Beats: Audio pre-training with acoustic tokenizers, in Proc. ICML, 2023, pp. 51785193. [19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu, Exploring the limits of transfer learning with unified text-to-text transformer, JMLR, 2020. [20] Oren Rippel, Michael Gelbart, and Ryan Adams, Learning ordered representations with nested dropout, in Proc. ICML, 2014, pp. 17461754. [21] Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis, Eq-vae: Equivariance regularized latent space for improved generative image modeling, arXiv preprint arXiv:2502.09509, 2025. [22] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie, Convnext v2: Co-designing and scaling convnets with masked autoencoders, in Proc. CVPR, 2023, pp. 1613316142. [23] Dmitry Bogdanov, Minz Won, Philip Tovstogan, Alastair Porter, and Xavier Serra, The mtg-jamendo dataset for automatic music tagging, in Machine Learning for Music Discovery Workshop, ICML, 2019. [24] Michael Defferrard, Kirell Benzi, Pierre Vandergheynst, and Xavier Bresson, Fma: dataset for music analysis, in Proc. ISMIR, 2017. [25] Manco, Weck, Doh, Won, Zhang, Bodganov, Wu, Chen, Tovstogan, Benetos, et al., The song describer dataset: corpus of audio captions for music-andlanguage evaluation, NeurIPS Machine Learning for Audio Workshop, 2023. [26] Dimitrios Bralios, Gordon Wichern, Francois Germain, Zexu Pan, Sameer Khurana, Chiori Hori, and Jonathan Le Roux, Generation or replication: Auscultating audio latent diffusion models, in Proc. ICASSP. IEEE, 2024, pp. 1156 1160. [12] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu, Speechtokenizer: Unified speech tokenizer for speech language models, in Proc. ICLR, 2024. [27] Christian Steinmetz and Joshua Reiss, auraloss: Audio focused loss functions in pytorch, in Digital music research network one-day workshop (DMRN+ 15), 2020. [13] Alexandre Defossez, Laurent Mazare, Manu Orsini, Amelie Royer, Patrick Perez, Herve Jegou, Edouard Grave, and Neil Zeghidour, Moshi: speech-text foundation model for realtime dialogue, Tech. Rep., 2024. [14] Haibin Wu, Ho-Lam Chung, Yi-Cheng Lin, Yuan-Kuei Wu, Xuanjun Chen, Yu-Chi Pai, Hsiu-Hsuan Wang, Kai-Wei Chang, Alex Liu, and Hung-Yi Lee, Codec-superb: An indepth analysis of sound codec models, in Findings of ACL, 2024, pp. 1033010348. [15] Jingfeng Yao, Bin Yang, and Xinggang Wang, Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models, in Proc. CVPR, 2025, pp. 1570315712. [28] Azalea Gui, Hannes Gamper, Sebastian Braun, and Dimitra Emmanouilidou, Adapting frechet audio distance for generative music evaluation, in Proc. ICASSP. IEEE, 2024, pp. 13311335. [29] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton, Similarity of neural network representations revisited, in Proc. ICML, 2019, pp. 35193529. [30] Nello Cristianini, John Shawe-Taylor, Andre Elisseeff, and Jaz Kandola, On kernel-target alignment, Proc. NeurIPS, 2001. [31] Ari Morcos, Maithra Raghu, and Samy Bengio, Insights on representational similarity in neural networks with canonical correlation, Proc. NeurIPS, 2018."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University of Illinois Urbana-Champaign"
    ]
}