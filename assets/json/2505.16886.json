{
    "paper_title": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?",
    "authors": [
        "Nour Jedidi",
        "Yung-Sung Chuang",
        "James Glass",
        "Jimmy Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the growing success of reasoning models across complex natural language tasks, researchers in the Information Retrieval (IR) community have begun exploring how similar reasoning capabilities can be integrated into passage rerankers built on Large Language Models (LLMs). These methods typically employ an LLM to produce an explicit, step-by-step reasoning process before arriving at a final relevance prediction. But, does reasoning actually improve reranking accuracy? In this paper, we dive deeper into this question, studying the impact of the reasoning process by comparing reasoning-based pointwise rerankers (ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under identical training conditions, and observe that StandardRR generally outperforms ReasonRR. Building on this observation, we then study the importance of reasoning to ReasonRR by disabling its reasoning process (ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more effective than ReasonRR. Examining the cause of this result, our findings reveal that reasoning-based rerankers are limited by the LLM's reasoning process, which pushes it toward polarized relevance scores and thus fails to consider the partial relevance of passages, a key factor for the accuracy of pointwise rerankers."
        },
        {
            "title": "Start",
            "content": "Dont Overthink Passage Reranking: Is Reasoning Truly Necessary? 1MIT Lincoln Laboratory Nour Jedidi1 Yung-Sung Chuang2 James Glass2 2Massachusetts Institute of Technology nour.jedidi@ll.mit.edu Jimmy Lin3 3University of Waterloo 5 2 0 M 2 2 ] . [ 1 6 8 8 6 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "With the growing success of reasoning models across complex natural language tasks, researchers in the Information Retrieval (IR) community have begun exploring how similar reasoning capabilities can be integrated into passage rerankers built on Large Language Models (LLMs). These methods typically employ an LLM to produce an explicit, step-by-step reasoning process before arriving at final relevance prediction. But, does reasoning actually improve reranking accuracy? In this paper, we dive deeper into this question, studying the impact of the reasoning process by comparing reasoning-based pointwise rerankers (ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under identical training conditions, and observe that StandardRR generally outperforms ReasonRR. Building on this observation, we then study the importance of reasoning to ReasonRR by disabling its reasoning process (ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more effective than ReasonRR. Examining the cause of this result, our findings reveal that reasoning-based rerankers are limited by the LLMs reasoning process, which pushes it toward polarized relevance scores and thus fails to consider the partial relevance of passages, key factor for the accuracy of pointwise rerankers."
        },
        {
            "title": "Introduction",
            "content": "Recently, there has been surge of interest in reasoning models such as DeepSeek-R1 (Guo et al., 2025), OpenAIs o3, Qwen3 (Yang et al., 2025), and others. By generating an explicit reasoning process i.e., chain-of-thought (CoT) prior to producing its final response, reasoning models have shown strong performance across wide range of complex natural language tasks such as mathematics (Yang et al., 2024). Following the success of reasoning models, researchers in the Information Retrieval (IR) com1 Figure 1: Average NDCG@10 of reasoning pointwise rerankers (ReasonRR) compared to their non-reasoning variants (StandardRR and ReasonRR-NoReason) on MS MARCO and BRIGHT. munity have begun to explore how incorporating reasoning process can improve Large Language Model (LLM) based retrieval systems (Yan et al., 2025; Ji et al., 2025; Shao et al., 2025; Weller et al., 2025; Zhuang et al., 2025), especially with the introduction of reasoning-intensive retrieval benchmarks such as BRIGHT (Su et al., 2025). In particular, recent work has explored incorporating reasoning process to improve LLM-based pointwise (Weller et al., 2025) and setwise (Zhuang et al., 2025) rerankers, showing promising results on reasoning-intensive retrieval tasks. However, the importance of the explicit reasoning processes in rerankers when compared to standard, non-reasoning rerankers under identical training conditions remains an open question. In this paper, we study the necessity of this explicit reasoning processes and ask: Does scaling test-time compute via generation of reasoning tokens prior to making relevance prediction actually improve reranking accuracy? To investigate this, we examine two perspectives: RQ1: Under identical training setups (e.g., training data and backbone LLM), how do reasoning rerankers compare against standard, non-reasoning rerankers? RQ2: How does disabling the reasoning process of reasoning reranker affect its reranking accuracy? To answer these research questions, we train and evaluate three reranker variants: (1) StandardRR, standard LLM-based pointwise reranker that directly classifies query-passage pairs as relevant or non-relevant (Nogueira et al., 2020; Ma et al., 2024); (2) ReasonRR, our reproduction of Rank1 (Weller et al., 2025), which generates reasoning chain prior to making relevance prediction; and (3) ReasonRR-NoReason, modified version of ReasonRR, in which the explicit reasoning process is disabled at inference time by providing forced reasoning process, effectively transforming ReasonRR into standard pointwise reranker. The central findings of our experiments can be summarized as follows and are shown in Figure 1: Under identical training setups, we find no general advantage of the reasoning process for pointwise reranking. While certain domains and LLM scales benefit from reasoning, on average, StandardRR did better on in-domain and out-of-domain datasets versus ReasonRR. In fact, we find that reasoning can even degrade effectiveness for rerankers explicitly trained to reason (i.e., ReasonRR). As shown in Figure 1, ReasonRR-NoReason outperforms ReasonRR by 1.4 points in-domain (MS MARCO) and 3 points out-of-domain (BRIGHT) in terms of NDCG@10. Further investigation suggests that this likely stems from the reasoning process forcing the model towards polarized relevance scores which does not account for the partial relevance of passages. Our results show that while ReasonRR is better relevance classifier than ReasonRR-NoReason, ReasonRR-NoReason placed more emphasis on partial relevance scores, contributing to its better reranking accuracy. While we find that self-consistency (Wang et al., 2023) can bring about improvements to ReasonRR, it is still outperformed by StandardRR. Given this, our results suggest that practitioners are better served by simpler methods like StandardRR, which is more accurate and cost-effective. 2 Our findings build upon recent research in the NLP community which question the necessity of the reasoning process for LLMs (Ma et al., 2025). We hope our work not only encourages future work that can improve reasoning for reranking tasks, but also highlights the importance of comparing against strong, simple baselines when developing new methodologies."
        },
        {
            "title": "2 Background",
            "content": "In this section, we provide brief background on pointwise rerankers (StandardRR) and reasoning pointwise rerankers (ReasonRR), as they form the basis of our study. We emphasize that ReasonRR is our reproduction of Rank1 (Weller et al., 2025); however, we refer to it as ReasonRR to maintain consistency with the usage of StandardRR throughout the paper. Preliminaries. The goal of information retrieval (IR) is to identify relevant passages from large collection of texts, denoted by = {P1, P2, . . . , Pn}, given user-issued query, q. Current IR systems typically employ multi-stage pipeline where first-stage retriever fetches an initial set of passages from and reranker reorders the top-k passages {P1, P2, . . . , Pk}, where n, to produce more accurate ranking. StandardRR. StandardRR is trained as pointwise reranker, independently producing relevance score, R, for given query-passage pair. To train StandardRR, the simplest approach is to directly fine-tune an LLM to produce the tokens true or false given dataset of (query, passage, relevance label) triples, where true and false denote relevant or not relevant, respectively. At inference, for each query-passage pair (q, Pi) in the top-k, the probability of relevance, R, is computed by applying softmax exclusively to the logits corresponding to the tokens true and false: = softmax (cid:0)ztrue(q, Pi), zfalse(q, Pi)(cid:1) true (1) Here, ztrue(q, Pi) and zfalse(q, Pi) denote the logits assigned by the LLM for the true and false tokens, given input (q, Pi). The subscript true after the softmax normalization indicates that only the probability assigned to the token true is considered for R. The passages are then sorted in descending order of R. We note that recent methods, such as RankLLaMA (Ma et al., 2024), train pointwise rerankers using hard negatives sampled from the top-ranking results of first-stage retriever. However, as our goal is to keep the training setup identical to that of ReasonRR, which we describe next, we do not consider hard negatives. ReasonRR. ReasonRR builds upon the setup described for StandardRR by fine-tuning an LLM to first generate reasoning process, r, before producing the tokens true or false. To do so, ReasonRR is fine-tuned with dataset of (query, passage, r, relevance label) quadruples. Following Equation 1, is again computed by considering the softmax over the logits of the true and false tokens, but in this case, also considers the LLMs generated reasoning process, r: = softmax (cid:0)ztrue(q, Pi, ri), zfalse(q, Pi, ri)(cid:1) true (2) where ri is the reasoning process generated for input (q, Pi). The passages are then reordered as described for StandardRR."
        },
        {
            "title": "3 Does Reasoning Improve Rerankers?",
            "content": "In this section, we study the impact of reasoning on pointwise rerankers through two different lenses: (1) how does StandardRR compare to ReasonRR when trained under the same settings? And, (2) how is ReasonRRs reranking accuracy affected if we forcefully remove its reasoning process (ReasonRR-NoReason)? Through these two perspectives, we hope to shed light on different ways reasoning may influence reranking accuracy."
        },
        {
            "title": "3.1 RQ1: StandardRR vs. ReasonRR",
            "content": "Our first experiment aims to understand the importance of reasoning from the training perspective. Specifically, if we train StandardRR on the exact same data as ReasonRR, but omit the reasoning chain, how does performance compare? To answer this research question, we train pointwise rerankers of varying sizes, with and without reasoning chains. Experiment Setup. To train the rerankers, we leverage the training data provided by Weller et al. (2025).1 The dataset augments MS MARCO (Bajaj et al., 2016) with reasoning chains generated by Deepseek R1 (Guo et al., 2025), which include 1https://huggingface.co/datasets/jhu-clsp/ rank1-training-data MS MARCO v1 MS MARCO DL19 DL20 DL21 DL22 DL23 BM25 50.6 48. 44.6 26.9 26.3 + Qwen2.5-1.5B StandardRR ReasonRR + Qwen2.5-3B StandardRR ReasonRR + Qwen2.5-7B StandardRR ReasonRR 73.1 68.7 72.5 70.4 74.6 70.3 69.4 63.1 68.9 66.4 70.0 64. 68.9 65.7 69.4 65.9 70.9 65.9 50.7 43.3 51.4 45.2 50.3 45. 44.2 38.8 45.5 41.3 46.3 41.1 Table 1: In-domain performance of StandardRR versus ReasonRR. Each Qwen2.5 model reranks the top-100 passages from BM25. R1s final relevance predictions. The dataset consists of approximately 386K quadruples in the following format: (query, passage, R1s reasoning chain, relevance label)."
        },
        {
            "title": "For",
            "content": "the backbone LLM, we leverage the Qwen2.5 base models (Yang et al., 2024) ranging from 1.5B to 7B model sizes. To train ReasonRR we fine-tune using LoRA (Hu et al., 2022) for one epoch with rank 32 and alpha 64. To train StandardRR we follow the same setup, but only use the (query, passage, relevance label) triples, omitting R1s reasoning chain. We evaluate StandardRR and ReasonRR on indomain and out-of-domain retrieval datasets. For in-domain evaluation, we leverage passage ranking datasets based on MS MARCO v1TREC DL19 and TREC DL20 (Craswell et al., 2020, 2021b)and based on MS MARCO v2TREC DL21, TREC DL22, and TREC DL23 (Craswell et al., 2021a, 2022, 2023). For out-of-domain evaluation, we focus on BRIGHT (Su et al., 2025), reasoning-intensive retrieval benchmark. We report NDCG@10, the official metric for both the MS MARCO and BRIGHT datasets. At inference, models rerank the top-100 passages retrieved by BM25. For BRIGHT, models rerank passages retrieved by BM25 using queries expanded with GPT-4 CoT; however, following Weller et al. (2025), the rerankers are not provided the GPT-4 CoT. For MS MARCO, we implement BM25 using Pyserini (Lin et al., 2021) and for BRIGHT, we follow the implementation from the BRIGHT codebase. LLM training was performed using HuggingFace (Wolf et al., 2019) and inference with vLLM (Kwon et al., 2023)."
        },
        {
            "title": "Coding",
            "content": "Theorem-based Avg. Bio. Earth. Econ. Psy. Rob. Stack. Sus. Leet. Pony AoPS TheoQ. TheoT. BM25 + GPT-4 CoT 53. 54.1 24.3 38.7 18.9 27.7 26. 19.3 17.6 3.9 19.2 20.8 27. + Qwen2.5-1.5B StandardRR ReasonRR + Qwen2.5-3B StandardRR ReasonRR + Qwen2.5-7B StandardRR ReasonRR 37.0 32.5 41.6 37.3 47.1 47. 21.7 20.3 27.1 27.8 38.0 35.4 16.8 12.3 23.1 25.5 16.1 11. 10.0 15.3 26.3 23.5 2.6 6.6 30.6 12.3 20.9 20.7 31.9 33. 22.2 18.3 16.9 24.3 30.3 25.2 13.2 11.3 42.0 26.2 28.1 24. 44.1 35.2 26.1 20.0 29.5 25.2 36.5 31.0 19.3 15.1 37.5 36. 1.8 3.4 2.7 4.7 4.6 5.9 16.1 10.6 16.2 20.7 22.4 22. 26.1 13.7 19.0 15.6 30.6 34.0 24.6 23.6 39.4 36.6 31.0 27. Table 2: Out-of-domain performance of StandardRR versus ReasonRR. Each Qwen2.5 model reranks the top-100 passages from BM25 + GPT-4 CoT. Results. In Tables 1 and 2 we present the evaluation results for both in-domain and out-of-domain retrieval tasks. As shown in Appendix E, ReasonRR is comparable to Rank1, achieving similar NDCG@10 (27.8 versus 27.5, respectively), confirming our implementation is valid. On MS MARCO, we find that StandardRR outperforms ReasonRR by an average of 5.3, 3.7, and 5 points across the 1.5B, 3B, and 7B model sizes, respectively. Surprisingly, on BRIGHT, we find similar story: StandardRR outperforms ReasonRR, achieving 3.4, 1, and 3.2 points higher average NDCG@10 across the 1.5B, 3B, and 7B model sizes. However, while in-domain StandardRR always outperformed ReasonRR, out-of-domain, the results suggest that reasoning can be beneficial depending on the model scale and domain. For example, at the smaller model scales (1.5B and 3B), ReasonRR achieves higher NDCG@10 versus StandardRR on the Psychology (Psy.), Stack Overflow (Stack.), and AoPS datasets. At the 7B scale, while StandardRR begins to consistently outperform ReasonRR, we find that ReasonRR still performs better on the AoPs dataset. All in all, these results suggest that while reasoning can improve rerankers for certain model sizes and domains, training ReasonRR-style pointwise reranker does not provide any general advantage versus StandardRR."
        },
        {
            "title": "3.2 RQ2: How Important is the Reasoning",
            "content": "Process to ReasonRR? Our results up to this point demonstrated that, under the exact same training regime, rerankers that are trained to simply output relevance prediction (StandardRR) outperform rerankers trained to reaQwen2.5 Method"
        },
        {
            "title": "MS MARCO BRIGHT",
            "content": "1.5B 3B 7B StandardRR ReasonRR ReasonRR-NoReason StandardRR ReasonRR ReasonRR-NoReason StandardRR ReasonRR ReasonRR-NoReason 61.3 55.9 56.7 61.5 57.8 58.3 62.4 57.4 58.8 19.0 15.6 11.6 24.6 23.6 23.4 31.0 27.8 30. Table 3: Studying the effect of removing the reasoning process from pointwise rerankers with reasoning. Results on MS MARCO and BRIGHT represent an average across the corresponding datasets. Bold results denote best between ReasonRR and ReasonRR-NoReason. See Appendix for results on individual datasets. son prior to making the relevance prediction (ReasonRR), on average. But, what if we disable the reasoning for ReasonRR? We hypothesize that if the reasoning is crucial to ReasonRRs relevance prediction, its reranking accuracy should drop if it does not reason. Experiment Setup. In order to disable the reasoning for ReasonRR (ReasonRR-NoReason), we pre-fill the LLMs reasoning with \"forced\" reasoning process: <think> Okay, think have finished thinking. </think>, following the setup from Ma et al. (2025). We then follow the same evaluation setup as in Section 3.1. Note that this, in essence, turns ReasonRR into standard pointwise reranker as it only needs to output the relevance label. Results. The results of this experiment can be found in Table 3. For MS MARCO, ReasonRRNoReason is consistently more effective than Rea4 MS MARCO MS MARCO v2 DL19 DL20 DL21 DL22 DL F1 F F1 F F"
        },
        {
            "title": "ReasonRR",
            "content": "+ Self-Consistency ReasonRR-NoReason 71.4 80.3 75.6 54.5 79. 64.6 56.4 87.1 68.5 49.1 70. 57.6 39.7 66.9 49.8 65.9 65.7 60.2 82.4 85.5 84. 73.2 74.3 70.2 49.2 49.1 44.7 82.1 84.0 84.8 61.5 62.0 58.6 54.0 53.5 52.0 89.2 90.1 92. 67.3 67.2 66.6 41.6 43.0 36.6 73.2 76.0 79.2 53.1 54.9 50.1 35.6 36.0 34.1 61.9 66.5 73. 45.2 46.8 46.7 Table 4: Comparison of relevance classification performance (Precision, Recall, and F1-score) across Qwen2.5-7B reranker variants. sonRR, improving by an average of 0.8, 0.5, and 1.4 points across the 1.5B, 3B, and 7B model sizes, respectively. On BRIGHT, ReasonRR-NoReason is less effective for smaller models (1.5B and 3B), but as model size increases, ReasonRR-NoReason begins outperform ReasonRR. In fact, at the 7B scale, ReasonRR-NoReason is able to improve by 3 points versus ReasonRR on BRIGHT, closing the gap between StandardRR and ReasonRR. These findings are remarkably concordant with those of Section 3.1: (1) For in-domain datasets, reasoning reduces reranking effectiveness across all model sizes and (2) reasoning appears to be more beneficial out-of-domain for smaller rerankers, but as the LLM size increases, any benefits diminish, and reasoning actually hurts reranking accuracy even for ReasonRR, which was trained to reason prior to making relevance prediction."
        },
        {
            "title": "4 Why Does Reasoning Hurt Rerankers?",
            "content": "One reason why ReasonRR may perform worse than StandardRR is that ReasonRR has poorly calibrated and polarized probabilities for ranking due to the conclusions made by its reasoning process. For example, ReasonRR will almost always assign very high probabilities when its reasoning concludes that passage is relevant, and thus may not be able to reflect that one passage may be more relevant than another passage. On the other hand, as StandardRR is trained to only output true or false, it may implicitly learn to output scores that account for one passage being more relevant than another passage. Due to this, we hypothesize that StandardRR can better model the partial relevance of query-passage pairs, making the outputs less polarized and preserving the uncertainty of scores which can be essential for the effectiveness of pointwise rerankers. In this section, we dive deeper into this hypothesis. First, we investigate how ReasonRR compares to StandardRR and ReasonRR-NoReason as simple binary relevance classifier. Then, we compare the relevance score distributions for StandardRR, ReasonRR, and ReasonRR-NoReason and examine qualitative example of ReasonRRs reasoning process. Finally, we discuss the results and propose potential improvements for ReasonRR."
        },
        {
            "title": "4.1 Relevance Classification Comparison",
            "content": "We first study how different reranking methods compare as simple relevance classifiers, ignoring their reranking accuracy measured by metrics like NDCG@10, which, ultimately, is what we really care about. Doing so will allow us to better understand how much we can attribute differences in effectiveness to simply being worse relevance classifiers. In most cases, better relevance classification should result in better reranking accuracy. For this experiment, we set ypred = 1 if > 0.5, and ypred = 0 otherwise. For the ground truth relevance judgements, we set judgments > 2 (corresponding to highly relevant and perfectly relevant) as positive labels, and the rest as negative labels, following standard practice used for binary measures in IR (MacAvaney et al., 2022). The results for the Qwen2.5-7B models are in Table 4. Comparing StandardRR to ReasonRR, we find that in terms of F1-score and precision, StandardRR is consistently stronger than ReasonRR. However, ReasonRR generally has higher recall than StandardRR, indicating that ReasonRR is classifying passages as relevant more frequently. We note that this is further confirmed in Figure 2, which we discuss in the next subsection. Surprisingly, ReasonRR-NoReason is generally worse at relevance classification than ReasonRR (in terms of F1 and precision), yet outperforms it in terms of retrieval metrics, as discussed in Section 3.2. Over the next two subsections, we provide potential explanations for this observation. 5 Figure 2: Relevance Scores Distribution across Qwen2.5-7B reranker variants on DL19."
        },
        {
            "title": "4.2 Relevance Scores Distribution",
            "content": "Our observations in Section 4.1 revealed mismatch between relevance classification precision and reranking accuracy metrics (i.e., NDCG@10) for ReasonRR versus ReasonRR-NoReason. To better understand why this may be the case, we plot the distribution of the relevance scores across the Qwen2.5-7B rerankers, shown in Figure 2. We find that StandardRR and ReasonRR place similar proportion of their predictions in the lowrelevance bin (00.1) for around 70% of its scores. However, while StandardRR spreads its remaining scores across both partial-relevance (0.10.9) regions (11.4%) and high-relevance (0.91.0) regions (19.7%), ReasonRR exhibits more extreme distribution. It places almost no scores in intermediate regions, assigning essentially any passage with partial relevance exclusively into the very high relevance bins (29.0%). This demonstrates that the reasoning process leads the model to make polarized decisions, either relevant or not relevant, and rarely assigning partial relevance, matching our hypothesis. Interestingly, if we take ReasonRR and forcefully remove its reasoning process as done in Section 3.2 (i.e., ReasonRR-NoReason), this no longer becomes the case. While the majority of the relevance scores are still at the tails of the distribution, ReasonRR begins to place more of its predictions across the partial-relevance bins (21.4%), while placing less predictions in the low-relevance and high-relevance bins."
        },
        {
            "title": "4.3 Qualitative Study",
            "content": "tive example can be found in Table 5. Looking at the reasoning process, we find that ReasonRR explicitly mentions that the passage is \"somewhat relevant\", but because it has to select between two binary options (relevant or not relevant), it reasonably selects the passage as relevant. This subsequently makes high (0.999) as ReasonRR finishes its reasoning stating \"Therefore, the answer is true\". This example shows how the reasoning process can bottleneck the ability for ReasonRR to measure any concept of partial relevance, even when the model states that the passage is not fully addressing the query."
        },
        {
            "title": "4.4 Discussion",
            "content": "The findings from Sections 4.1 and 4.2 suggest that the ability to assign partial relevance is critical to the effectiveness of pointwise rerankers, not just the ability to predict if passage is relevant or not. While ReasonRR achieves higher effectiveness on binary relevance classification metrics compared to ReasonRR-NoReason, it still falls behind ReasonRR-NoReason in terms of NDCG@10. This observation suggests that the improvements of StandardRR over ReasonRR arise not only from (1) StandardRR being better relevance classifier, but also from (2) StandardRR having stronger ability to capture partial relevance. As ReasonRR and ReasonRR-NoReason only differ in their use of reasoning and ReasonRR-NoReason is more effective in reranking, it would appear that (2) is the more crucial factor for reranking accuracy, highlighting the importance of partial relevance modeling for pointwise rerankers. Finally, to better understand how ReasonRR handles partially relevant passages, we examine the reasoning process of ReasonRR for random example in which StandardRR produces partially relevant prediction (i.e., in the 0.1-0.9 bin). The qualitaCan injecting concept of partial relevance into ReasonRR help? We explore simple method to incorporate partial relevance into ReasonRR: self-consistency (Wang et al., 2023), which we denote as ReasonRR + Self-Consistency. Unlike"
        },
        {
            "title": "Reasoning",
            "content": "how to help jammed finger Jammed finger vs. broken finger. Share on Pinterest. broken finger is often more painful than jammed finger, although both may be treated using splint. broken finger will cause severe pain and swelling that lasts for hours or even days. While jammed finger is painful, it is not usually severe. Okay, lets see. The user is asking how to help jammed finger. The passage provided talks about the difference between jammed finger and broken finger. It mentions that both might be treated with splint, but broken finger is more painful and has more swelling. First, the query is about treatment for jammed finger. The passage does mention that jammed finger can be treated with splint, which is direct answer to the query. However, the main focus of the passage is comparing jammed and broken fingers, not giving detailed treatment steps. It does say that both might use splint, which is relevant. But the passage doesnt go into other treatment methods like icing, elevation, or when to see doctor. So, the passage is somewhat relevant because it mentions treatment (splint) for jammed finger. But its not comprehensive. The user might need more information, but the passage does provide basic answer. However, the question is whether the passage is relevant, not if its complete. Since it does address treatment (splint) for jammed finger, even briefly, its relevant. The comparison to broken finger might also help in understanding when jammed finger isnt severe. Therefore, the answer is true. StandardRR: 0.374 ReasonRR: 0.999 ReasonRR-NoReason: 0.810 ReasonRR + Self-Consistency: 0.877 Table 5: An example of ReasonRRs (Qwen2.5-7B) reasoning process where it scores partially relevant passage as highly relevant. As reference, we additionally provide the probability of relevance, R, for all reranker variants for the provided query-passage pair."
        },
        {
            "title": "MS MARCO BRIGHT",
            "content": "StandardRR ReasonRR-NoReason"
        },
        {
            "title": "ReasonRR",
            "content": "+ Self-Consistency 62.4 58.8 57.4 59.2 31.0 30.8 27.8 30.7 Table 6: Influence of Self-Consistency on ReasonRR. For this experiment, the base LLM is Qwen2.5-7B. Bold results denote best between ReasonRR and ReasonRR + Self-Consistency. See Appendix for results on individual datasets. the majority vote approach used by Wang et al. (2023), we average the predicted values across eight sampled outputs from ReasonRR to produce continuous score suitable for reranking. The results of this experiment can be found in Table 6 and its relevance distribution is shown in Figure 3. By leveraging self-consistency decoding, ReasonRR begins to distribute its relevance scores away from the low-relevance (00.1) and high-relevance (0.91.0) bins and distributes 20% of its predictions into partial-relevance (0.10.9) bins. By doing so, its NDCG@10 improves by 1.8 points on MS MARCO and 2.9 points on BRIGHT, even though the relevance classification metrics presented in Table 4, particularly precision, is generally on-par with ReasonRR. So, is reasoning truly necessary for pointwise rerankers? Even with the improvements from ReasonRR + Self-Consistency, ReasonRR still falls behind StandardRR, suggesting that, at least in their current state, reasoning may not be best suited for pointwise reranking schemes. This is especially true when taking into account the lower inference costs of StandardRR versus ReasonRR. Potential Solutions. Given our results, we believe that to fully realize the benefits of reasoning in pointwise reranking, it is essential to re-design ReasonRR to explicitly consider partial relevance, and the promising directions are: Training with non-binary relevance scores: Instead of predicting binary relevance, ReasonRR can be trained to generate graded scores (e.g., from 1 to 5). However, the current Rank1 training data (Weller et al., 2025) only provides binary labels, so it will be necessary to develop methods to synthesize realistic data that can accurately reflect partial relevance. We leave this to future work. Leveraging reasoning signals: When ReasonRR explicitly indicates partial relevance, through phrases like somewhat relevant, these signals could be extracted to produce more accurate intermediate scores. Score calibration through loss function de7 Effcient Reasoning. Another line of work parallel to ours has focused on making reasoning models more efficient, studying if the reasoning chain can be made more concise. Most closely related to our work is Ma et al. (2025), who demonstrated that the reasoning process of current reasoning models is not required for high performance. Another line of work, as summarized in Sui et al. (2025), has focused on fine-tuning LLMs to reason more efficiently by leveraging variable length chain-ofthought data (Munkhbat et al., 2025; Xia et al., 2025; Yu et al., 2024; Kang et al., 2025). We highlight that rather than trying to make reasoning models more efficient, our work is primarily focused on questioning the necessity of reasoning for passage reranking, and not making reasoning rerankers more efficient."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we study whether scaling test-time computevia generation of reasoning tokens prior to making relevance predictionactually improves the accuracy of pointwise rerankers. To do so, we train and evaluate three pointwise rerankers, StandardRR, ReasonRR, and ReasonRRNoReason. Through experiments across in-domain and out-of-domain datasets, we find that the reasoning process consistently harms the accuracy of pointwise rerankers, especially as LLM size increases. Investigating the root cause of this result, we observe that the reasoning process restricts ReasonRRs ability to capture partial relevance between query-document pairs, which is an important factor for pointwise reranking accuracy. While we explored self-consistency as potential remedy for this restriction, StandardRR still outperformed ReasonRR + Self-Consistency. Our findings suggest that the reasoning process is unnecessary for pointwise rerankers and that practitioners are better served with simpler methods like StandardRR. We believe that in order to fully realize the benefits of reasoning, it is essential to re-design how reasoning is utilized by ReasonRR. Some promising directions we discuss include training with loss functions that encourage calibrated scoring or generating synthetic data that elicit relevance scores beyond binary labels. However, we emphasize that any improvements in the training of ReasonRR should also be properly compared against strong and simple baselines. Figure 3: Relevance Scores Distribution for ReasonRR + Self-Consistency on DL19 sign: Another approach is to directly train ReasonRR to produce calibrated scores using tailored loss functions, encouraging outputs that reflect various degrees of relevance. While each of these directions is worth further exploration, they remain open research problems to unlock the full potential of ReasonRR. Until then, our results suggest that practitioners are better served by standard pointwise rerankers, which are simpler, more efficient, and currently more accurate."
        },
        {
            "title": "5 Related Work",
            "content": "Reasoning and Retrieval. Recent advances in LLMs have motivated exploration into how reasoning processes can be integrated into retrieval systems. O1 Embedder (Yan et al., 2025) trained an LLM to generate intermediate \"thoughts\" based on the user query which were then used to enrich the query representations for dense retrieval. DEBATER (Ji et al., 2025), on the other hand, leveraged an iterative step-by-step reasoning process to learn more effective document representations. Rank1 (Weller et al., 2025) and RankR1 (Zhuang et al., 2025) instead focused on reasoning for rerankers, with Rank1 focusing on pointwise rerankers and Rank-R1 on Setwise (Zhuang et al., 2024) rerankers, where the LLM reranker generates reasoning steps before selecting the most relevant document among set of candidate documents. More recently, ReasonIR (Shao et al., 2025) explored the use of synthetic data to train retrievers for reasoning-intensive retrieval tasks."
        },
        {
            "title": "Limitations",
            "content": "Other Reranking Methods. While we demonstrate that reasoning hurts pointwise rerankers, it remains an open question what influence reasoning may have for other reranking approaches such as listwise (Sun et al., 2023) and setwise (Zhuang et al., 2024). However, as shown in Zhuang et al. (2024), pointwise rerankers are much more efficient as they can rerank candidate passages in parallel (Ma et al., 2024) and thus, our study covers very popular and commonly used approach for passage reranking. LLM Models and Scales. We limit our study to the Qwen2.5 family of models as it was the primary model used in related work (Zhuang et al., 2025; Weller et al., 2025) and allowed us to control for factors such as LLM scale. However, as future work, it would be interesting to study the impact of reasoning across different model families. Additionally, our experiments were also limited to LLMs with 7B parameters. While our results showed that increasing LLM size benefited StandardRR more than ReasonRR, the influence at larger scales remains an open question. We note that our StandardRR at 7B scale still outperforms or is competitive with the reported results for Rank1-14B and Rank1-32B on BRIGHT, which we believe can mitigate these concerns. Improvements to ReasonRR. Even though ReasonRR + Self-Consistency which was grounded in observations from our analysis makes strong improvements on ReasonRR, it still is less effective than our StandardRR. While we propose potential solutions to improve ReasonRR, we leave the implementation of these methods as future work. We hope our results and analysis can help in the development of new reasoning pointwise rerankers."
        },
        {
            "title": "Ethics Statement",
            "content": "Our research solely uses publicly available datasets, and no personal information is collected. All datasets and models are used in accordance with its intended use and licenses. The goal of our study is to better understand the factors that influence the accuracy of LLM rerankers, which we hope can have positive impact on building better search engines and other applications built on retrieval systems. While our results showed that standard pointwise rerankers, which minimize the output tokens generated by an LLM, outperform more verbose reasoning pointwise rerankers, we do recognize that such systems still rely on LLMs, which means that there is risk that the LLM can produce biased, harmful, or offensive output."
        },
        {
            "title": "Acknowledgments",
            "content": "DISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited. This material is based upon work supported by the MIT Comp Sci & Artificial Intelligence under Air Force Contract No. FA8702-15-D-0001 or FA870225-D-B002. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the MIT Comp Sci & Artificial Intelligence L. 2025 Massachusetts Institute of Technology. Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than as specifically authorized by the U.S. Government may violate any copyrights that exist in this work."
        },
        {
            "title": "References",
            "content": "Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. 2016. Ms marco: human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, Jimmy Lin, Ellen M. Voorhees, and Ian Soboroff. 2022. Overview of the trec 2022 deep learning track. In Text Retrieval Conference. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Jimmy J. Lin. 2021a. Overview of the trec 2021 deep learning track. In Text Retrieval Conference. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. Voorhees. 2020. Overview of the trec 2019 deep learning track. ArXiv, abs/2003.07820. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. Voorhees. 2021b. Overview of the trec 2020 deep learning track. ArXiv, abs/2102.07662. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Hossein A. Rahmani, Daniel Campos, Jimmy Lin, Ellen M. Voorhees, and Ian Soboroff. 2023. Overview of the trec 2023 deep learning track. In Text Retrieval Conference. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Yifan Ji, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan Li, Zhiyuan Liu, Yu Gu, Ge Yu, and Maosong Sun. 2025. Learning more effective representations for dense retrieval through deliberate thinking before search. arXiv preprint arXiv:2502.12974. Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou. 2025. C3ot: Generating shorter chain-of-thought without compromising effectiveness. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2431224320. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: python toolkit for reproducible information retrieval research with sparse and dense In Proceedings of the 44th Interrepresentations. national ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2356 2362. Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. 2025. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858. Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. Fine-tuning llama for multi-stage In Proceedings of the 47th Intertext retrieval. national ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2421 2425. Sean MacAvaney, Nicola Tonellotto, and Craig Macdonald. 2022. Adaptive re-ranking with corpus graph. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 14911500. Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun. 2025. Selftraining elicits concise reasoning in large language models. arXiv preprint arXiv:2502.20122. Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020. Document ranking with preIn Findings trained sequence-to-sequence model. of the Association for Computational Linguistics: EMNLP 2020, pages 708718, Online. Association for Computational Linguistics. Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih, Pang Wei Koh, et al. 2025. Reasonir: Training retrievers for reasoning tasks. arXiv preprint arXiv:2504.20595. Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han yu Wang, Liu Haisu, Quan Shi, Zachary Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan Arik, Danqi Chen, and Tao Yu. 2025. BRIGHT: realistic and challenging benchmark for reasoning-intensive retrieval. In The Thirteenth International Conference on Learning Representations. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, et al. 2025. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. Is chatgpt good at search? investigating large language models as re-ranking agents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1491814937. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Orion Weller, Kathryn Ricci, Eugene Yang, Andrew Yates, Dawn Lawrie, and Benjamin Van Durme. 2025. Rank1: Test-time compute for reranking in information retrieval. arXiv preprint arXiv:2502.18418. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, et al. 2019. Huggingfaces transformers: State-ofthe-art natural language processing. arXiv preprint arXiv:1910.03771. Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. 2025. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067. 10 Ruiran Yan, Zheng Liu, and Defu Lian. 2025. O1 embedder: Let retrievers think before action. arXiv preprint arXiv:2502.07555. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. 2024. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023. Shengyao Zhuang, Xueguang Ma, Bevan Koopman, RankJimmy Lin, and Guido Zuccon. 2025. r1: Enhancing reasoning in llm-based document rerankers via reinforcement learning. arXiv preprint arXiv:2503.06034. Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon. 2024. setwise approach for effective and highly efficient zero-shot ranking with large language models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3847."
        },
        {
            "title": "A Dataset Details",
            "content": "We show the number of test queries for each dataset used for evaluation in Table 7. Dataset TREC DL19 TREC DL20 TREC DL21 TREC DL22 TREC DL23 Biology Earth Science Economics Psychology Robotics Stackoverflow Sustainable Living Leetcode Pony AoPs TheoremQA Questions TheoremQA Theorems # Queries 43 54 53 76 82 103 116 103 101 101 117 108 142 112 111 194 76 Table 7: Dataset Details The above datasets have the following licenses. The MS MARCO datasets are intended for non-commercial research purposes. BRIGHT is under CC BY 4.0 license. Rank1 training data, described in Section 3.1, is under MIT License."
        },
        {
            "title": "B Model Details",
            "content": "Qwen2.5-1.5B: 1.5B base model. Huggingface ID: Qwen/Qwen2.5-1.5B Qwen2.5-3B: 3B base model. Huggingface ID: Qwen/Qwen2.5-3B Qwen2.5-7B: 7B base model. Huggingface ID: Qwen/Qwen2.5-7B The above models have the following licenses. Qwen2.5-1.5B is under the Apache 2.0 License. Qwen2.5-3B is under the Qwen Research License Agreement. Qwen2.5-7B is under the Apache 2.0 License. 12 MS MARCO v1 MS MARCO v2 DL19 DL20 DL21 DL22 DL BM25 50.6 48.0 44.6 26.9 26. + Qwen2.5-1.5B StandardRR ReasonRR ReasonRR-NoReason + Qwen2.5-3B StandardRR ReasonRR ReasonRR-NoReason + Qwen2.5-7B StandardRR ReasonRR ReasonRR-NoReason ReasonRR + Self-Consistency 73.1 68.7 69.9 72.5 70.4 71.8 74.6 70.3 73.3 71. 69.4 63.1 61.7 68.9 66.4 63.7 70.0 64.3 65.0 66.7 68.9 65.7 67.3 69.4 65.9 66.8 70.9 65.9 69.1 68. 50.7 43.3 44.9 51.4 45.2 47.1 50.3 45.6 46.1 46.0 44.2 38.8 39.5 45.5 41.3 41.9 46.3 41.1 40.5 42. Table 8: Full results for StandardRR, ReasonRR, ReasonRR-NoReason, and ReasonRR + SelfConsistency. All models rerank the top-100 passages from BM25. We also leverage Pyserini (Lin et al., 2021) and vLLM (Kwon et al., 2023) which are under the Apache 2.0 License."
        },
        {
            "title": "C Training and Inference Details for\nStandardRR and ReasonRR",
            "content": "To train StandardRR and ReasonRR we fine-tune Qwen2.5 using LoRA (Hu et al., 2022) for one epoch with rank 32 and alpha 64, using batch size of 128 and learning rate of 2e-4. We apply LoRA to all the linear layers of the transformer model. Note, to train the StandardRR we leverage the same dataset as ReasonRR, but only use the (query, passage, relevance label) triples, ignoring the R1 reasoning process. Training for each reranker took less than day and was done on an A100 GPU. Due to limited computational resources, each model is only trained once. For inference, we run all models on NVIDIA A6000 (48GB) and A100 (80GB) GPUs. As the StandardRR and ReasonRR outputs are run with greedy decoding, all the scores in the paper are from single run. Full Results for ReasonRR-NoReason and ReasonRR + Self-Consistency In this section, we provide the full results for ReasonRR-NoReason and ReasonRR + SelfConsistency across MS MARCO and BRIGHT datasets. These results can be found Table 8 and Table 9. StackExchange Coding Theorem-based Avg. Bio. Earth. Econ. Psy. Rob. Stack. Sus. Leet. Pony AoPS TheoQ. TheoT. BM25 + GPT-4 CoT 53.6 54.1 24.3 38.7 18.9 27. 26.3 19.3 17.6 3.9 19.2 20. 27.0 + Qwen2.5-1.5B StandardRR ReasonRR ReasonRR-NoReason + Qwen2.5-3B StandardRR ReasonRR ReasonRR-NoReason + Qwen2.5-7B StandardRR ReasonRR ReasonRR-NoReason ReasonRR + Self-Consistency + Rank1-7B (Our Results) + Rank1-7B (Reported Results) 37.0 32.5 23. 41.6 37.3 40.8 47.1 47.0 56.0 49.6 48.0 48.8 21.7 20.3 15.3 27.1 27.8 20.5 38.0 35.4 41.9 38. 37.2 36.7 16.8 12.3 10.4 20.9 20.7 20.3 28.1 24.0 27.5 27.4 21.8 20.8 23.1 25.5 13. 31.9 33.1 31.9 44.1 35.2 38.5 40.9 35.1 35.0 16.1 11.1 10.4 22.2 18.3 14.0 26.1 20.0 23.2 23. 19.9 22.0 10.0 15.3 6.2 16.9 24.3 15.3 29.5 25.2 21.6 29.3 22.6 18.7 26.3 23.5 7. 30.3 25.2 23.3 36.5 31.0 32.7 33.2 31.0 36.2 2.6 6.6 4.4 13.2 11.3 18.7 19.3 15.1 16.3 14. 12.7 12.7 30.6 12.3 11.3 42.0 26.2 37.3 37.5 36.0 39.4 38.4 30.8 31.2 1.8 3.4 3. 2.7 4.7 3.7 4.6 5.9 7.2 8.1 6.8 6.3 16.1 10.6 12.0 16.2 20.7 24.6 22.4 22.2 27.2 25. 26.0 23.7 26.1 13.7 23.2 30.6 34.0 31.1 39.4 36.6 38.0 39.1 38.2 37.8 19.0 15.6 11. 24.6 23.6 23.4 31.0 27.8 30.8 30.7 27.5 27.5 Table 9: Full results for StandardRR, ReasonRR, ReasonRR-NoReason, and ReasonRR + Self-Consistency. All models rerank the top-100 passages from BM25 + GPT-4 CoT . more reasoning chains is not more effective for ReasonRR."
        },
        {
            "title": "G Prompts",
            "content": "For training and evaluation of StandardRR, ReasonRR, ReasonRR-NoReason, and ReasonRR + Self-Consistency, we leverage the same exact prompts used in the Rank1 (Weller et al., 2025) paper, but apply the Qwen2.5 (Yang et al., 2024) chat template. Below we repeat the baseline prompt. For the dataset specific prompts we used, please refer to Weller et al. (2025). StandardRR Standard Prompt: <im_start>system Determine if the following passage is relevant to the query. Answer only with true or false. <im_end> <im_start>user Query: {} Passage: {} <im_end> <im_start>assistant"
        },
        {
            "title": "MS MARCO BRIGHT",
            "content": "StandardRR ReasonRR-NoReason"
        },
        {
            "title": "ReasonRR",
            "content": "+ Self-Consistency (3 samples) + Self-Consistency (8 samples) 62.4 58.8 57.4 59.1 59.2 31.0 30.8 27.8 30.6 30.7 Table 10: Influence of the number of sampled chains for ReasonRR + Self-Consistency. Comparison with Rank1 (Weller et al., 2025) We also provide the results for Rank1 to ensure that our reproduction, ReasonRR, is valid. These results are in Table 9. We found that the Rank1 paper used an earlier edition of BRIGHT, which had minor differences in queries and judged documents. Thus, we report Rank1 results on the new BRIGHT, Rank1 (Our Results) as well as the original papers reported results, Rank1 (Reported Results). Comparing ReasonRR for Qwen2.5-7B to Rank1-7B, both are comparable in terms of NDCG@10 (27.8 versus 27.5). Number of Sampled Outputs for ReasonRR + Self-Consistency In this section, we present the results of ReasonRR + Self-Consistency when we only sample 3 reasoning chains from ReasonRR. The results are shown in Table 10. We find that ReasonRR + SelfConsistency (n=3) is as effective as ReasonRR + Self-Consistency (n=8), suggesting that sampling ReasonRR Standard Prompt: <im_start>system Determine if the following passage is relevant to the query. Answer only with true or false. <im_end> <im_start>user Query: {} Passage: {} <im_end> <im_start>assistant <think> ReasonRR-NoReason Standard Prompt: <im_start>system Determine if the following passage is relevant to the query. Answer only with true or false. <im_end> <im_start>user Query: {} Passage: {} <im_end> <im_start>assistant <think> Okay, have finished thinking. </think>"
        }
    ],
    "affiliations": [
        "MIT Lincoln Laboratory",
        "Massachusetts Institute of Technology",
        "University of Waterloo"
    ]
}