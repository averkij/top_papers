{
    "paper_title": "Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models",
    "authors": [
        "Kai Sun",
        "Yushi Bai",
        "Zhen Yang",
        "Jiajie Zhang",
        "Ji Qi",
        "Lei Hou",
        "Juanzi Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Benefiting from contrastively trained visual encoders on large-scale natural scene images, Large Multimodal Models (LMMs) have achieved remarkable performance across various visual perception tasks. However, the inherent limitations of contrastive learning upon summarized descriptions fundamentally restrict the capabilities of models in meticulous reasoning, particularly in crucial scenarios of geometric problem-solving. To enhance geometric understanding, we propose a novel hard negative contrastive learning framework for the vision encoder, which combines image-based contrastive learning using generation-based hard negatives created by perturbing diagram generation code, and text-based contrastive learning using rule-based negatives derived from modified geometric descriptions and retrieval-based negatives selected based on caption similarity. We train CLIP using our strong negative learning method, namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for geometric problem-solving. Experiments show that our trained model, MMGeoLM, significantly outperforms other open-source models on three geometric reasoning benchmarks. Even with a size of 7B, it can rival powerful closed-source models like GPT-4o. We further study the impact of different negative sample construction methods and the number of negative samples on the geometric reasoning performance of LMM, yielding fruitful conclusions. The code and dataset are available at https://github.com/THU-KEG/MMGeoLM."
        },
        {
            "title": "Start",
            "content": "Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models Kai Sun, Yushi Bai, Zhen Yang, Jiajie Zhang, Ji Qi, Lei Hou, Juanzi Li Tsinghua University 5 2 0 2 6 2 ] . [ 1 2 5 1 0 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Benefiting from contrastively trained visual encoders on large-scale natural scene images, Large Multimodal Models (LMMs) have achieved remarkable performance across various visual perception tasks. However, the inherent limitations of contrastive learning upon summarized descriptions fundamentally restrict the capabilities of models in meticulous reasoning, particularly in crucial scenarios of geometric problem-solving. To enhance geometric understanding, we propose novel hard negative contrastive learning framework for the vision encoder, which combines image-based contrastive learning using generation-based hard negatives created by perturbing diagram generation code, and text-based contrastive learning using rule-based negatives derived from modified geometric descriptions and retrievalbased negatives selected based on caption similarity. We train CLIP using our strong negative learning method, namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for geometric problem-solving. Experiments show that our trained model, MMGeoLM, significantly outperforms other open-source models on three geometric reasoning benchmarks. Even with size of 7B, it can rival powerful closed-source models like GPT-4o. We further study the impact of different negative sample construction methods and the number of negative samples on the geometric reasoning performance of LMM, yielding fruitful conclusions. The code and dataset are available at https://github.com/THU-KEG/MMGeoLM."
        },
        {
            "title": "Introduction",
            "content": "Geometric mathematical reasoning has garnered significant attention as an essential capability for large multimodal models (LMMs) (Anthropic, 2024; OpenAI, 2023; Bai et al., 2023). It requires fine-grained identification of visual elements (Lu *Equal contribution 1 Figure 1: Showcasing geometric problem incorrectly solved by GPT-4o, Claude-3 and Qwen2.5-VL. GPT-4o and Claude-3 generate non-existent geometric elements, such as ABC and ABE, while Qwen2.5-VL misinterprets geometric knowledge. et al., 2023) within the given images, such as geometric shapes, spatial configurations, and the relationships between them (He et al., 2024). However, the eyes of most existing LMMs, i.e., their pretrained vision encoders such as CLIP (Patel et al., 2024; Yang et al., 2023; Goel et al., 2022), are primarily trained on general visual datasets that do not emphasize the intricate features necessary for specialized mathematical reasoning. Therefore, these models often fail to understand the nuanced geometric information accurately and produce incorrect reasoning and answers. As shown in Figure 1, facing simple parallel line problem, the leading LMMs such as GPT-4o (OpenAI, 2024a), Claude-3 (Anthropic, 2024), and Qwen2.5-VL (Bai et al., 2025) all hallucinate non-existent elements or misinterpret spatial relationships (e.g., ABC, ABE, and the concept of similar triangles), exModel Training Training Data MM-MATH AltCLIP No AltCLIP In-Batch Randomly-sampled Negative (400K) No AltCLIP MMCLIP 1: Retrieval-based Negative (100K) AltCLIP MMCLIP 2: Rule-based Negative (100K) AltCLIP MMCLIP 3: Image Negative (4K) AltCLIP MMCLIP 1+ 2 AltCLIP MMCLIP 1+ 2+ 3 23.8 24.6 26.6 28.1 28.2 28.4 30.1 Table 1: Comparison of geometric reasoning performance on MM-MATH across LMMs with vision encoders trained with different hard negative data. hibiting notable deficiencies in capturing the intricate geometric details. To address these shortcomings, recent research has focused strategies such as fine-tuning on specialized mathematical datasets (Gao et al., 2023; Zhang et al., 2024c; Peng et al., 2024a,b) or utilizing massive image-caption pairs to enhance the visual perception ability of LMMs (Qi et al., 2024; Wang et al., 2024) by aligning images with corresponding captions. However, the construction of image-caption pairs is inherently susceptible to hallucinations, as many captions are generated by existing LMMs. As result, training with only these positive samples may lead to spurious alignment (Zhang et al., 2024b). To achieve more robust and semantically meaningful alignment, it is crucial to incorporate hard negative samplessemantically similar but mismatched pairswhich push the vision encoder to learn finer distinctions beyond shallow correlations. Therefore, to further improve LMMs abilities for capturing geometric information, we investigate key question: How can we systematically construct hard negatives tailored for geometric reasoning? In this work, we propose two types of hard negative sample construction methods, i.e., imagebased and text-based, to enhance fine-grained geometric element recognition in the vision encoder. For text-based contrastive learning, we design two strategies to create negative captions for geometry image: (1) retrieval-based method that employs dense retrieval in geometric-domain text corpus to select captions with high lexical similarity but differing content as negative samples; and (2) rule-based method that modifies key geometric attributes in the captions, such as shapes, angles, and lengths, while keeping other elements unchanged, thereby producing negative samples that bear similar appearance but with key distinct information from the positives. For image-based contrastive learning, we introduce novel method that leverages large language model (LLM) to generate detailed caption and corresponding diagram generation code for given geometry problem, forming the positive image sample. The LLM then perturbs the code to create visually similar but geometrically incorrect diagrams, which serve as hard negative samples. Additionally, we modify the original CLIP training framework and propose MMCLIP, method designed to handle an arbitrary number of hard negative samples centered around single image or caption. Table 1 shows the effectiveness of MMCLIP training on different hard negative sets. We conduct extensive experiments to evaluate the effectiveness of our CLIP training for LMMs. Specifically, we assess our trained MMGeoLM across four geometric benchmarks spanning two categories: choice-based questions (GeoQA, MathVISTA, and WE-MATH) and open-ended questions (MM-MATH). Our results show that the proposed model outperforms all existing open-source models on GeoQA and MathVISTA, and achieves state-of-the-art performance on MM-MATH, surpassing GPT-4o by 7.5%. We further analyze the impact of different types of hard negative samples. Notably, we find that negative samples constructed from authentic, exam-based images significantly boost model performanceusing just 4K image negatives yields better results than over 100K text negatives. Lastly, we observe that increasing the number of hard negatives improves performance up to certain threshold, beyond which it leads to diminishing returns and eventual degradation."
        },
        {
            "title": "2 Method",
            "content": "In this section, we first introduce our proposed geometric diagram strong negative construction method in 2.1. Subsequently, we present the corresponding negative captions approach for geometric elements in 2.2. Finally, we describe training strategy of handling an arbitrary number of negative examples in 2.3, followed by our newly collected multimodal mathematical dataset in 2.4."
        },
        {
            "title": "2.1 Negative Images Construction",
            "content": "We construct negative geometric examples by generating geometric diagram codes for positive images and perturbing them to create negatives. Existing generation methods (Zhang et al., 2024c; Zou et al., 2024; Wei et al., 2024) rely on handcrafted rules that cover limited elements and over2 Figure 2: Image-based and text-based hard negative construction and the corresponding MMCLIP training method. look inter-element relationships, resulting in gap from real-world problems. In contrast, authentic exam questions, created by educators using professional geometry software (McClintock et al., 2002), produce diagrams that are more diverse and accurate. Specifically, in these authentic geometry problems (Sun et al., 2024), geometric elements are often described explicitly (e.g., AB is 8 cm) or implicitly through reasoning (e.g., using the Pythagorean theorem). Benefiting from advancements in mathematical reasoning and code-generation capabilities of LLMs (OpenAI, 2024b), which can effectively parse geometric conditions, perform necessary reasoning, and generate executable code to produce geometric diagrams closely matching the original images. As illustrated in Figure 2, providing the LLM with both the problem and the answer yields code and caption. Executing the code produces diagrams that not only replicate the original figures but may also include additional informative annotations (see Appendix for prompt details). To handle occasional errors in model-generated code, we adopt lightweight model (GLM et al., 2024) to correct syntax issues when the code fails. Subsequently, we generate detailed captions directly from code using Gemini 2.5 Pro (DeepMind, 2025), avoiding noise from question-based captions. Upon evaluating 123 generated captions, we find that the accuracy of captions in representing geometric elements reaches 100%. To create the negative images, We design prompts (Appendix A) for LLMs to generate 10 negative captions for each positive caption, differing subtly but critically. We then employ Gemini 2.5 Pro to modify the Python scripts based on these perturbed captions. These modified scripts produce diagrams that closely resemble the originals while aligning with the intended constraints."
        },
        {
            "title": "2.2 Negative Captions Construction",
            "content": "We propose two methods for constructing textbased hard negative samples. The first method retrieves captions similar to each positive example from large image-caption dataset. The second method generates targeted hard negative samples based on reasoning errors observed during LMM inference. Some examples of these hard negative samples are shown in Figure 1. Retrieval-based Hard Negative. Many previous works in open-domain QA take the topranked instances recalled by the retriever as negative examples to further improve retriever performance (Karpukhin et al., 2020; Xiong et al., 2020; 3 Huang et al., 2020). However, they suffer from the risks of introducing false negatives, which means some related instances are incorrectly treated as negatives (Xiong et al., 2020; Zhou et al., 2022; Yang et al., 2024). To address this issue, we build upon the recently proposed MAVIS dataset (Zhang et al., 2024c), which mitigates false negatives by ensuring one-to-one correspondence between each image and its caption. We use SimANS (Zhou et al., 2022) to encode all captions, compute pairwise similarities, and retrieve the top 100 most similar captions per image as hard negatives. Rule-based Hard Negative. Existing studies modify positive captions by randomly altering nouns, attributes, and relationships (Yuksekgonul et al., 2022; Zhang et al., 2024a), while others leverage in-context learning to generate meaningful modifications (Patel et al., 2025). However, these methods either introduce semantic ambiguity or focus solely on the vision encoder level without considering downstream reasoning errors. For example, as shown in Figure 1, LMMs incorrectly interpret the parallel line AB and CD as forming triangle ABC, leading to errors in solving the geometric problem. Simply altering positive captions without considering LMM reasoning errors provides limited benefits in improving geometric element recognition. To address this issue, we analyze the evaluation results of LMMs on MM-MATH (Sun et al., 2024) and identify four major types of image element recognition errors in their responses. Based on these error types, we design corresponding rulebased strategies to construct hard negative samples: 1. Geometric element ordering: Modifying the sequence of alphabetical order in geometric diagrams while ensuring the new order does not match the original cyclically (e.g., ABCD changing to CDAB are invalid). 2. Shape attributes: Altering properties such as changing squares to rectangles or right triangles to isosceles triangles. 3. Geometric relationships: Modifying relationships such as parallelism between two lines or similarity between two triangles. 4. Numerical values: Adjusting numerical values in captions, such as modifying angles or segment lengths. We use GLM-4 (GLM et al., 2024) to generate 10 rule-based hard negatives for each positive example. An example is shown in Figure 2."
        },
        {
            "title": "2.3 Hard Negative CLIP Training",
            "content": "Though CLIP-trained vision encoders are widely used in LMMs (Radford et al., 2021; Doveh et al., 2023b), they typically adopt in-batch negative sampling during training, which limits their capacity for fine-grained image understanding. While methods like NegCLIP (Yuksekgonul et al., 2022) introduce hard negatives, they still rely heavily on in-batch samples. Considering the methods in Section 2.2 have allowed us to construct large number of hard negatives for each image, we propose MMCLIP training strategy, which focuses on each single image and only uses its corresponding hard negatives for training, entirely abandoning traditional in-batch negatives. In this way, the vision encoder is forced to distinguish subtle differences in all cases, thereby enhancing its fine-grained geometry understanding capacities. }N j=1)}M , {T Formally, given training dataset = {(Ii, + i=1, where Ii represents the image, + is the negative samples conducted around the image, using the methods in Section 2.2. Our loss function is formulated as denotes the positive example and = 1 (cid:88) i=1 log exp (cid:0)s(Ii, + exp (cid:0)s(Ii, + )(cid:1)+ )(cid:1) exp (cid:0)s(Ii, ) (cid:80) j=1 (cid:1) (1) where s(, ) denotes the similarity function over the feature encodings of the input instances. Since negative samples are constructed around elements within the image, their score s(Ii, ) are expected to be higher than traditional in-batch negatives. From the above equation, we calculate the gradient for positive and negative samples: s+ = (cid:80)N ) + (cid:80)N j=1 exp(s ) j=1 exp(s ) exp(s+ (2) = (3) exp(s+ s i = s(Ii, + exp(s ) ) + (cid:80)N j=1 exp(s ) = s(Ii, ) and where s+ ). Since the number of hard negatives is unrestricted, the gradient function guided by Equation 3 can be effectively optimized, leading to improved imagecaption alignment. The training strategy is illustrated in Figure 2. We further validate the effectiveness of hard negatives and analyze the impact of their quantity in our experiments in Section 4.2."
        },
        {
            "title": "2.4 Supervised Fine-tuning Data",
            "content": "To enhance the mathematical reasoning of LMMs, it is crucial not only to improve their understanding of image elements but also to strengthen their problem-solving ability. To achieve this, we collaborate with the 21st Century Education Network to construct high-quality training dataset of 12K geometry problems. This dataset aligns with middle school curricula and serves as an effective evaluation of students learning capabilities. The initial data is collected in MathML format, and we convert them into standard LaTeX using GLM-4. Each problem includes detailed analysis, from which we extract the core solution to create structured step-by-step reasoning process. The final answers are enclosed in boxed{}, enabling precise evaluation and targeted learning."
        },
        {
            "title": "3.1 Architecture",
            "content": "We adopt the LLaVA architecture (Liu et al., 2024), which comprises three components: (1) vision encoder, (2) 2-layer MLP adapter, and (3) an LLM backbone. For the LLM backbone, we use Mammoth2-7B (Yue et al., 2024) and Qwen2.5-7BInstruct (Qwen et al., 2025). The vision encoder is based on AltCLIP (Chen et al., 2022b), configured with maximum length of 512 tokens and model size of 0.5B parameters."
        },
        {
            "title": "3.2 Training Details",
            "content": "In the vision-text alignment stage, we first pretrain AltCLIP on the 400K MAVIS image-caption alignment dataset (Zhang et al., 2024c) to improve the models generalization for geometric problems. Next, we fine-tune the model following the MMCLIP training strategy, using 100K hard caption negative samples, each consisting of 10 rule-based and 30 retrieval-based negatives to enhance the models understanding of geometric elements. In addition, we fine-tune the model on 4K hard image negative samples, where each example includes 10 negative geometric diagrams. In the second stage, we tune the MLP adapter using 67K G-LLAVA (Gao et al., 2023) image-text alignment data. In the third stage, we perform supervised fine-tuning on combined dataset comprising 300K MAVIS instruction data (Zhang et al., 2024c), 117K G-LLaVA instruction data (Gao et al., 2023), and 17K open-ended geometric problems. The open-ended dataset includes 12K problems from our collected dataset and 5K randomly sampled MM-MATH (Sun et al., 2024) geometric problems, with the rest 700 problems reserved for evaluation. We name the trained model as MMGeoLM."
        },
        {
            "title": "4 Experiments",
            "content": "To quantitatively evaluate the effectiveness of our MMGeoLM in geometric problem-solving, we conduct comparative experiments on four geometric benchmarks using various LMMs. Additionally, we analyze the impact of different vision encoder types and training strategies, as well as the number of hard negatives in MMGeoLM."
        },
        {
            "title": "4.1.1 Datasets and Metrics",
            "content": "We assess the geometric reasoning capabilities of various LMMs across two types of benchmarks: multiple-choice and open-ended. The multiplechoice benchmark includes GeoQA (Chen et al., 2021), geometric question-answering task based on plane geometry; We-Math (Qiao et al., 2024), visual mathematical reasoning task with questions of varying difficulty, solvable in two or three steps; and MathVista (Lu et al., 2023), widely used for evaluating LMMs performance. The open-ended benchmark is MM-MATH (Sun et al., 2024), which features high discriminative difficulty sourced from secondary school-level problems. We use accuracy (ACC) as the metric. For We-Math, we assess multi-step problems (S2 and S3), while for MMMATH, we evaluate models across easy, medium, and hard categories using test set of 700 problems excluded from training. For MathVista, we evaluate geometry problem-solving (GEO) and algebraic (ALG) categories."
        },
        {
            "title": "4.1.2 Baselines",
            "content": "The evaluated LMMs are categorized into two groups: Closed-source APIs: Claude-3-Opus (Anthropic, 2024), Claude-3.5-Sonnet, GPT-4o (OpenAI, 2024a), and GPT-4V. Open-source LMMs: MAVIS-7B (Zhang et al., 2024c), G-LLaVAInternVL2-8B (Chen 7B (Gao et al., 2023), et al., 2024), LLaVA-OneVision-7B (Li et al., 2024), Qwen2.5-VL-7B-Instruct (Bai et al., 2025), Chimera-Reasoner-8B (Peng et al., 2024b), InternLM-XComposer2-7B (Dong et al., 2024), Phi-3-Vision-128K-Instruct (Abdin et al., 2024), and Math-LLaVA-13B (Shi et al., 2024). For the open-source category, we choose LMMs that 5 have achieved strong results on geometric benchmarks (Chen et al., 2021; Lu et al., 2023) since 2023. Additionally, we include human evaluation baselines, using scores from prior studies (Zhang et al., 2024c; Sun et al., 2024)."
        },
        {
            "title": "4.1.3 Overall Results",
            "content": "As shown in Table 2, our proposed MMGeoLM achieves state-of-the-art performance on the MathVista and MM-MATH benchmarks. On the GeoQA benchmark, MMGeoLMQwen2.5-7B lags 0.4% behind Chimera-Reasoner8B. As Chimera-Reasoner-8B was trained on GeoQA (Peng et al., 2024b), MMGeoLMQwen2.5-7B achieves the best performance among other models that were not trained on this dataset. The improved performance in geometric problemsolving demonstrates the effectiveness of our training approach. For the We-Math benchmark, MMGeoLM-Mammoth2-7B underperforms GPT-4o, Claude-3.5-Sonnet, and the open-source Qwen2.5-VL-7B-Instruct model. We attribute this to We-Maths emphasis on recognition from visual elements rather than geometric mathematical reasoning, which limits the effectiveness of MMGeoLM-Mammoth2-7Bs geometric reasoning enhancement strategies. On the open-ended MMMATH benchmark, MMGeoLM-Qwen2.5-7B performs poorly on hard problems, with less than 10% accuracy, but achieves over 55% accuracy on easy problems. Easy problems require fewer reasoning steps, allowing geometric element recognition to significantly enhance accuracy. In contrast, hard problems involve multi-step reasoning, where its impact is more limited. Compared to using the original AltCLIP and without MMCLIP (AltCLIP trained only with in-batch learning on MAVIS), the vision encoder trained with our constructed hard negative samples and the MMCLIP training method significantly enhances MMGeoLMs geometric understanding capabilities."
        },
        {
            "title": "4.2 Experiments on Vision Encoder",
            "content": "We further quantitatively evaluate the performance of various vision encoders trained with different datasets and strategies. The experiments are divided into three parts: (1) performance evaluation of six LMMs trained with different AltCLIP variants across two benchmarks, (2) top-1 positive example retrieval performance evaluation of six AltCLIP models, and (3) varying number of hard caption negatives."
        },
        {
            "title": "4.2.1 Setup\nWe compare the following six types of AltCLIP\nmodels are as follows:",
            "content": "Original AltCLIP: No additional data training. Random AltCLIP: Trained with randomly selected negative samples within batch (batch size = 10) using 400K MAVIS data. Retrieved AltCLIP: Trained with 100K samples, each with 10 retrieval-based caption negatives. Rule-based AltCLIP: Trained with 100K samples, each with 10 rule-based caption negatives. Negative-images AltCLIP: Trained with 4K samples, each with 10 hard image negatives. Retrieved + Rule-based + Negative-images: Trained on all three datasets combined. The Random AltCLIP follows OpenAIs training approach (Radford et al., 2021), while the Retrieved, Rule-based, Negatives-images and Retrieved + Rule-based + Negatives-images AltCLIP models employ our proposed MMCLIP training approach. In the second and third stages, LMMs are trained on 67K G-LLaVA alignment data and 17K open-ended geometric reasoning problems. For the retrieval experiments, we assess the retrieval performance of trained AltCLIP models by selecting positive samples from large pool of negative samples. Retrieval performance is evaluated using the Hit@1 metric. We construct four types of negative datasets: Random Negative: Caption negatives randomly selected in training batch. Retrieval Negative: Caption negatives retrieved using the SimANS (Zhou et al., 2022) model, selecting the top 100 most similar captions to the positive sample. Rule-based Negative: Caption negatives generated according to the rules defined in section 2.2. Image Negative: Image negatives generated from real-world geometric problems by MM-MATH. For the number of negative experiments, we investigate the impact of hard caption negative samples on MMGeoLM varying from 5 to 50."
        },
        {
            "title": "4.2.2 Results\nAs shown in Figure 3, LMMs trained with hard neg-\native samples consistently improve performance\non the GeoQA and MM-MATH benchmarks. No-\ntably, on MM-MATH, Negative-images AltCLIP,\ntrained with only 4K image negative samples, sur-\npasses Retrieved AltCLIP and Rule-based AltCLIP",
            "content": "6 Model Human* Claude-3-Opus* Claude-3.5-Sonnet GPT-4V* GPT-4o MAVIS-7B* G-LLaVA-7B* Math-LLaVA-13B InternVL2-8B LLaVA-OneVision-7B Chimera-Reasoner-8B Phi-3-Vision-128K-Instruct Qwen2.5-VL-7B-Instruct InternLM-XComposer2-7B MMGeoLM-Mammoth2-7B w/o MMCLIP Original AltCLIP MMGeoLM-Qwen2.5-7B w/o MMCLIP Original AltCLIP GeoQA MathVista We-Math MM-MATH GPS ALG S2 S3 Easy Med. Hard Avg 92.3 44.5 65.1 - 58. 68.3 67.0 48.4 56.4 64.6 69.6 29.5 59.0 38.8 68.5 55.4 46.7 69.2 56.3 50.7 48.4 50.9 - - 46.3 66.3 50.5 62.7 64.1 56.7 55.6 60.5 54.6 48.5 38.8 67.7 63.0 68.7 52.2 45.8 69.8 54.1 47.6 46.6 68.4 53.0 65.4 59.2 - 55.0 60.8 53.1 42.6 40.0 66.9 56.6 69.5 51.3 46.8 68.1 50.3 48. 32.9 64.7 49.2 58.0 37.9 30.1 31.7 41.1 28.7 29.9 33.3 58.1 33.1 41.5 34.1 30.1 39.8 35.2 33.1 23.0 62.1 38.2 43.6 34.6 32.7 23.0 37.1 22.3 31.6 30.1 50.6 33.0 37.5 35.3 29.2 36.4 35.1 32. 90.7 29.5 34.4 37.8 45.8 - - - 33.6 40.5 36.2 12.9 50.7 18.9 52.5 42.5 38.2 55.3 50.5 52.5 81.9 19.3 31.9 21.2 30. - - - 21.7 24.8 22.2 7.1 32.2 12.2 32.4 21.8 20.4 36.9 28.8 30.8 47.6 80.4 3.6 13.6 1.8 10.9 - - - 9.0 10.5 9.0 0.0 14.2 4. 4.5 4.5 1.6 9.0 4.5 4.5 20.3 31.7 23.1 31.8 - - - 23.3 27.0 24.1 7.8 34.8 13.1 36.9 26.8 25.9 39.2 34.3 33.7 Table 2: Accuracy (%) of various LMMs on GeoQA, MathVista, We-Math, and MM-MATH. S2/S3 denote the two-step settings in We-Math. Results marked * are taken from their original papers. Figure 3: Performance evaluation of MMGeoLMs built with six AltCLIP trained on different contrastive datasets. Figure 4: Comparison of two image negative strategies: Mavis uses 100K samples from the Mavis dataset, while MM-MATH-Align uses our 4K constructed samples. by 0.2% and 1.7%, respectively. On MM-MATH, Rule-based, Retrieved, and Image-negative AltCLIP consistently outperform Random AltCLIP on easy and medium-level questions. These results confirm the effectiveness of both hard negative construction methods. Furthermore, AltCLIP trained with combination of caption and image negatives achieves the best performance on both GeoQA and MM-MATH, demonstrating their complementary effect in improving model performance. Further experiments on image negatives (Figure 4) reveal that Negative-image-MM-MATHAlign, constructed from real-world images, outperforms Negative-image-Mavis. This suggests that high-quality, realistic image negatives have more pronounced impact on model performance than large-scale synthetic counterparts. The results of the retrieval performance experiments are shown in Figure 5. Under the negativeimages setting, Negative-images AltCLIP achieves retrieval score of 23%, while other AltCLIP variants remain below 15%. These results highlight the difficulty of retrieving realistic images. Notably, the dataset includes only 4,021 image 7 shown in Table 1), demonstrating that hard negatives enhance multimodal mathematical reasoning more effectively."
        },
        {
            "title": "5 Related Work",
            "content": "Mathematics-related research has recently received significant attention in LLMs and LMMs. For textonly mathematical reasoning, several works employ external tools like Tora (Gou et al., 2023) or intermediate step decomposition methods such as MAmmoTH (Yue et al., 2023), Metamath (Yu et al., 2023) and Math-Shepherd (Yue et al., 2023). For multimodal mathematics tasks involving images, benchmarks have been proposed, including mathematical competition-oriented OlympiadBench (He et al., 2024), geometry-focused datasets like Geometry3K (Lu et al., 2021), VisScience (Jiang et al., 2024), UniGeo (Chen et al., 2022a) and GeoQA (Chen et al., 2022a). Many works take effort on training multimodel math, including Gllava (Gao et al., 2023), Meta-LLaVA (Shi et al., 2024) and MAVIS (Zhang et al., 2024c). However, these multimodal approaches typically suffer from inaccuracies in recognizing geometric elements, where our paper specifically addresses the enhancement of visual recognition accuracy. Vision encoders play crucial role in understanding image content. While previous works (Zhang et al., 2024a; Doveh et al., 2023a,b; Singh et al., 2023) have leveraged negative captions to enhance image comprehension, but not address the recognition of geometric elements within images. Methods such as NegCLIP (Yuksekgonul et al., 2022) and TriCLIP (Yang et al., 2024) focus on learning from negative samples, but their applicability is limited by the constrained number of negative samples. In this work, we propose robust negative learning strategy that explicitly constructs negative samples for geometric elements while enabling arbitrary expansion, thereby improving geometric understanding in multimodal tasks."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduced MMCLIP, novel hard negative contrastive learning framework to enhance LMMs geometric reasoning by refining their vision encoders with systematically generated image and text-based negatives. Our resulting model, MMGeoLM, significantly outperformed existing models, even surpassing GPT-4o on key benchmarks. This work demonstrates the power of taiFigure 5: Retrieval performance of AltCLIP variants trained with different strategies. Random + means sequential training with random negatives followed by negatives created with the strategy. Figure 6: MMGeoLM Performance with AltCLIPs trained with varying numbers of hard negative samples. negative samples, which is likely insufficient to fully exploit the models performance potential. In the retrieval-based and rule-based negative settings, Random AltCLIP outperforms Rule-based AltCLIP in retrieval-based settings and exceeds Retrieved AltCLIP in rule-based settings. These results indicate that while hard negative training enhances performance within specific domains, it may limit generalization. Furthermore, pre-training on random negatives followed by hard negatives improves both generalization and domain-specific performance, ultimately enhancing overall model effectiveness. As shown in Figure 6, MMGeoLMs performance on MM-MATH improves with increasing hard negative samples from 5 to 30, but slightly declines beyond this point. These results indicate that hard caption negative samples have diminishing returns beyond certain threshold, with excessive examples reducing model performance. Training with only five hard negative samples outperforms training with ten randomly selected negatives (as lored hard negative sampling for developing LMMs with fine-grained visual understanding in specialized domains such as geometric understanding and reasoning."
        },
        {
            "title": "Limitations",
            "content": "Despite the effectiveness of the proposed image hard negatives constructed by perturbing Python scripts, the method heavily relies on the accuracy of LLM-generated code and captions. Any systematic biases in these synthetic constructions may introduce artifacts that differ from human-designed geometric problems. As result, the models performance on real-world visual inputs remains uncertain and requires further validation on diverse real-world datasets."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, and 1 others. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Anthropic. 2024. Claude3 system card. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. 2022a. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. arXiv preprint arXiv:2212.02746. Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. 2021. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. arXiv preprint arXiv:2105.14517. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, and 1 others. 2024. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271. Google DeepMind. 2025. intelligent Our most //blog.google/technology/google-deepmind/ gemini-model-thinking-updates-march-2025/. Accessed: 2025-05-19. ai model. Gemini 2.5: https: Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, and 1 others. 2024. Internlm-xcomposer2: Mastering freeform text-image composition and comprehension arXiv preprint in vision-language large model. arXiv:2401.16420. Sivan Doveh, Assaf Arbelle, Sivan Harary, Roei Herzig, Donghyun Kim, Paola Cascante-Bonilla, Amit Alfassy, Rameswar Panda, Raja Giryes, Rogerio Feris, and 1 others. 2023a. Dense and aligned captions (dac) promote compositional reasoning in vl models. Advances in Neural Information Processing Systems, 36:7613776150. Sivan Doveh, Assaf Arbelle, Sivan Harary, Eli Schwartz, Roei Herzig, Raja Giryes, Rogerio Feris, Rameswar Panda, Shimon Ullman, and Leonid Karlinsky. 2023b. Teaching structured vision & language concepts to vision & language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26572668. Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, and 1 others. 2023. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, and 1 others. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793. Shashank Goel, Hritik Bansal, Sumit Bhatia, Ryan Rossi, Vishwa Vinay, and Aditya Grover. 2022. Cyclip: Cyclic contrastive language-image pretraining. Advances in Neural Information Processing Systems, 35:67046719. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Tora: tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, and 1 others. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008. Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, and Ledell Wu. 2022b. Altclip: Altering the language encoder in clip for extended language capabilities. arXiv preprint arXiv:2211.06679. Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020. Embedding-based retrieval in facebook search. In 9 Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 25532561. Zhihuan Jiang, Zhen Yang, Jinhao Chen, Zhengxiao Du, Weihan Wang, Bin Xu, and Jie Tang. 2024. Visscience: An extensive benchmark for evaluating k12 educational multi-modal scientific reasoning. arXiv preprint arXiv:2409.13730. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and 1 others. 2024. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. 2023. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. 2021. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165. Edwin McClintock, Zhonghong Jiang, and Raquel July. 2002. Students development of three-dimensional visualization in the geometers sketchpad environment. OpenAI. 2023. GPT-4V(ision) system card. OpenAI. 2024a. Hello gpt-4o. https://openai.com/ index/hello-gpt-4o/. OpenAI. 2024b. Openai o1 system card. https: //openai.com/index/openai-o1-system-card/. Accessed: 2025-05-19. Maitreya Patel, Abhiram Kusumba, Sheng Cheng, Changhoon Kim, Tejas Gokhale, Chitta Baral, and Yezhou Yang. 2024. Tripletclip: Improving compositional reasoning of clip via synthetic vision-language negatives. arXiv preprint arXiv:2411.02545. Maitreya Patel, Naga Sai Abhiram Kusumba, Sheng Cheng, Changhoon Kim, Tejas Gokhale, Chitta Baral, and 1 others. 2025. Tripletclip: Improving compositional reasoning of clip via synthetic vision-language negatives. Advances in Neural Information Processing Systems, 37:3273132760. Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, and Zhi Tang. 2024a. Multimath: Bridging visual and mathematical reasoning for large language models. arXiv preprint arXiv:2409.00147. Tianshuo Peng, Mingsheng Li, Hongbin Zhou, Renqiu Xia, Renrui Zhang, Lei Bai, Song Mao, Bin Wang, Conghui He, Aojun Zhou, and 1 others. 2024b. Chimera: Improving generalist model with domainspecific experts. arXiv preprint arXiv:2412.05983. Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, and 1 others. 2024. Cogcom: Train large vision-language models diving into details through chain of manipulations. arXiv preprint arXiv:2402.04236. Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, and 1 others. 2024. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and 1 others. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. 2024. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294. Harman Singh, Pengchuan Zhang, Qifan Wang, Mengjiao Wang, Wenhan Xiong, Jingfei Du, and Yu Chen. 2023. Coarse-to-fine contrastive learning in image-text-graph space for improved vision-language compositionality. arXiv preprint arXiv:2305.13812. Kai Sun, Yushi Bai, Ji Qi, Lei Hou, and Juanzi Li. 2024. Mm-math: Advancing multimodal math evaluation with process evaluation and fine-grained classification. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 13581375. Haoxiang Wang, Pavan Kumar Anasosalu Vasu, Fartash Faghri, Raviteja Vemulapalli, Mehrdad Farajtabar, Sachin Mehta, Mohammad Rastegari, Oncel Tuzel, and Hadi Pouransari. 2024. Sam-clip: Merging vision foundation models towards semantic and spatial understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36353647. 10 Haoran Wei, Youyang Yin, Yumeng Li, Jia Wang, Liang Zhao, Jianjian Sun, Zheng Ge, Xiangyu Zhang, and Daxin Jiang. 2024. Slow perception: Lets perceive geometric figures step-by-step. arXiv preprint arXiv:2412.20631. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808. Kaicheng Yang, Jiankang Deng, Xiang An, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, and Tongliang Liu. 2023. Alip: Adaptive language-image pretraining with synthetic caption. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 29222931. Zhen Yang, Zhou Shao, Yuxiao Dong, and Jie Tang. 2024. Trisampler: better negative sampling principle for dense retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 92699277. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653. Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. 2024. Mammoth2: Scaling instructions from the web. arXiv preprint arXiv:2405.03548. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. 2022. When and why vision-language models behave like bags-ofarXiv preprint words, and what to do about it? arXiv:2210.01936. Le Zhang, Rabiul Awal, and Aishwarya Agrawal. 2024a. Contrasting intra-modal and ranking crossmodal hard negatives to enhance visio-linguistic comIn Proceedings of the positional understanding. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1377413784. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, and 1 others. 2024b. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer. Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, and 1 others. 2024c. Mavis: Mathematical visual instruction tuning with an automatic data engine. arXiv preprint arXiv:2407.08739. Kun Zhou, Yeyun Gong, Xiao Liu, Wayne Xin Zhao, Yelong Shen, Anlei Dong, Jingwen Lu, Rangan Majumder, Ji-Rong Wen, Nan Duan, and 1 others. 2022. Simans: Simple ambiguous negatives sampling for dense text retrieval. arXiv preprint arXiv:2210.11773. Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. 2024. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836."
        },
        {
            "title": "A Prompts",
            "content": "After multiple experimental iterations, we constructed two different types of prompts for our experiments, as illustrated in Figures 7 and 8. Specifically, Figure 7 demonstrates the prompt constructed by inputting the textual description and answer of realistic geometry problems to obtain the corresponding geometric diagram Python script. Meanwhile, Figure 8 illustrates the prompt obtained by modifying captions from positive examples to create slightly perturbed negative samples. Figure 7: Python script prompt from question and true answer in geometric problems Figure 8: Negative caption generation prompt from positive captions"
        },
        {
            "title": "B Figure Comparison",
            "content": "Figure 9 presents illustrative examples of image negative sample, covering analytical geometry and planar geometry. Geometric elements include triangles, quadrilaterals, as well as various relational properties such as perpendicularity and intersection. Additionally, certain metric properties absent from the original diagrams are explicitly indicated. Specifically, the first column shows the original diagrams, and the second column depicts diagrams generated from the given questions and the true answer, which, despite slight discrepancies, accurately capture the overall outlines of the original diagrams. The third and fourth columns display the constructed negative diagram instancesgenerated based upon their corresponding positive examplesdemonstrating notable similarities but also clear and meaningful distinctions."
        },
        {
            "title": "C Case Study",
            "content": "C.1 Case Study In Figures 10 and 11, we compare the solutions generated by GPT-4o and our MMGeoLM. In Figure 10, the red text highlights GPT-4os misrecognition of two triangles in the image, leading to an incorrect final result. In contrast, MMGeoLM produces correct solution, albeit through reasoning process different from the ground-truth answer. This highlights MMGeoLMs capability to generate diverse problem-solving approaches. Figure 11 illustrates errors in image-based reasoning for both MMGeoLM and GPT-4o. While both models make mistakes, MMGeoLMs reasoning aligns more closely with the correct solution, demonstrating improved mathematical reasoning. Overall, our trained MMGeoLM model demonstrates greater reasoning diversity and enhanced geometric problem-solving performance compared to current mainstream models. 12 Figure 9: Comparison of generated geometric figures. The first column shows the original images, the second column presents the positive samples, and the last two columns illustrate the constructed negative samples. Figure 10: Comparison of GPT-4o and MMGeoLM in geometric problem-solving. MMGeoLM correctly answers the problem using different solution compared to True Answer, while GPT-4o generates non-existent element ABD. Figure 11: Comparison of GPT-4o and MMGeoLM in geometric problem-solving. Both models produce incorrect answers, but MMGeoLMs solution is closer to the True Answer."
        }
    ],
    "affiliations": [
        "Tsinghua University"
    ]
}