{
    "paper_title": "Waver: Wave Your Way to Lifelike Video Generation",
    "authors": [
        "Yifu Zhang",
        "Hao Yang",
        "Yuqi Zhang",
        "Yifei Hu",
        "Fengda Zhu",
        "Chuang Lin",
        "Xiaofeng Mei",
        "Yi Jiang",
        "Zehuan Yuan",
        "Bingyue Peng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https://github.com/FoundationVision/Waver."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 1 6 7 5 1 . 8 0 5 2 : r WAVER: WAVE YOUR WAY TO LIFELIKE VIDEO GENERATION"
        },
        {
            "title": "ABSTRACT",
            "content": "We present Waver, high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), imageto-video (I2V), and text-to-image (T2I) generation within single, integrated framework. We introduce Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https: //github.com/FoundationVision/Waver. Figure 1: Left: Human evaluation win rates (GSB) of Waver compared to Veo3, Kling2.0, and Wan2.1 on the Waver-bench 1.0 Text-to-Video (T2V) dataset across three dimensions: motion quality, visual quality, and prompt following. Waver-bench 1.0 covers wide range of scenarios, including sports, daily activities, landscapes, animals, animations, and more. Right: Human evaluation win rates (GSB) on the Hermes Motion Testset across three dimensions: motion quality, visual quality, and prompt following. The Hermes Motion Testset encompasses complex motion scenarios such as tennis, basketball, gymnastics, and others. We can observe that, compared to general scenarios, Waver demonstrates more pronounced advantage in complex motion scenarios."
        },
        {
            "title": "Contents",
            "content": ""
        },
        {
            "title": "2 Model Architecture",
            "content": "2.1 Task-Unified DiT . 2.2 Cascade Refiner . . . . ."
        },
        {
            "title": "3 Training Data",
            "content": "3.1 Data Pre-Processing . 3.2 Quality Model 3.3 Caption Model . . . . . . . . 3. Semantic Balancing . . . . . . . . . . . . . . . . . . . 3.5 Hierarchical Data Filtering ."
        },
        {
            "title": "4 Training / Inference Recipe",
            "content": "4.1 Multi-stage Training . . . 4.2 Representation Alignment"
        },
        {
            "title": "4.3 Motion Optimization .",
            "content": ". ."
        },
        {
            "title": "4.5 Model Balancing .",
            "content": "4.6 Prompt Rewriting . . . . . . . . . . . . . . ."
        },
        {
            "title": "6.1 Artificial Analysis Arena .",
            "content": "."
        },
        {
            "title": "7 Discussion",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "7.1 DiT Model Sparsity .",
            "content": ". . . . . . . ."
        },
        {
            "title": "7.2 VAE Impact on Video Generation .",
            "content": ". . . . . . . . . . . . . . . . ."
        },
        {
            "title": "9 Contributions and Acknowledgments",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 5 5 7 9 10 10 11 11 12 13 14 16 17 19 22 22 22 24 24 28 29 30 2 Figure 2: T2V samples generated by Waver. Waver is capable of generating 1080p videos at arbitrary aspect ratios, delivering high levels of aesthetic quality, realism, and motion fidelity, while simultaneously supporting both T2V and I2V tasks."
        },
        {
            "title": "INTRODUCTION",
            "content": "Video generation (Singer et al., 2022; Blattmann et al., 2023; OpenAI, 2024) is currently highly It not only fulfills the limitless creative active area of interest in both academia and industry. aspirations people have for video content but is also gradually being applied to range of commercial scenarios, such as digital human live streaming for e-commerce, product display, and virtual try-on applications. Existing video generation models, most of which are based on the DiT (Peebles & Xie, 2023) architecture, have achieved remarkable progress in short period of time. Open-source models such as Wan (Wan et al., 2025), StepVideo (Ma et al., 2025), HunyuanVideo (Kong et al., 2024), and CogVideoX (Yang et al., 2025) have greatly advanced the development of the video generation community. Proprietary models such as Veo (DeepMind, 2025.05), Kling (Kuaishou, 2024.06), Hailuo (MiniMax, 2024.09), and Seedance (Gao et al., 2025) demonstrate better performance. However, the current video generation community still faces numerous challenges: (1) The quality of generated videos remains unsatisfactory, often lacking aesthetic appeal and realism, with poor performance in complex motion scenarios such as gymnastics and basketball; (2) Existing public technical reports rarely discuss the technical details required for generating high-resolution videos, such as those at 1080p, and the rationale behind certain model architecture designs is often unclear; (3) Most video generation models (Kong et al., 2024; Wan et al., 2025) implement text-to-video (T2V) and image-to-video (I2V) tasks using two separate models, resulting in substantial training resource overhead; (4) There is limited information regarding data processing pipelines and model training procedures, making it difficult to fully reproduce the entire training process. To overcome these problems and challenges, we propose Waver, model based on Rectified Flow Transformer (Esser et al., 2024b) and meticulously designed to achieve industry-grade performance. Waver consists of two modules: Task-Unified DiT and Cascade Refiner. We first employ Task-Unified DiT to generate videos with resolution of 720p and durations ranging from 5 to 10 seconds, and subsequently apply Cascade Refiner to upscale them to 1080p resolution. Task-Unified DiT: Joint modeling of multiple tasks allows effective learning of task-specific features and mutual promotion. Hybrid Stream enhances modality alignment and accelerates convergence. In Task-Unified DiT, we employ flexible input conditioning mechanism to unify text-to-image (T2I), text-to-video (T2V), and image-to-video (I2V) tasks within single network. It only needs to modify the channel dimension of the model input. We further design Hybrid Stream DiT architecture to optimize the trade-off between modality alignment and parameter efficiency, while also accelerating training convergence. At this stage, we have achieved high-quality video generation at native resolution of 720p, alongside image generation at resolutions up to 1080p. Cascade Refiner: With reduced computational overhead, videos are upscaled to 1080p, significantly enhancing visual clarity. In the second stage, the Cascade Refiner takes the 720p videos generated in the first stage as input and outputs 1080p videos with identical content but significantly enhanced clarity. This process is also implemented via flow matching with fewer inference steps, enabling the model to learn the transition from low-resolution to high-resolution videos. Compared to single-stage methods that directly generate 1080p videos, our two-stage approach achieves 40% acceleration. Comprehensive data pipelines and detailed training recipes reveal the full details of video generation model training. We establish comprehensive data curation pipeline, which includes manually annotated and trained caption and quality models to obtain high-quality training data and captions. In total, our model has seen more than 200 million video clips. We also provide detailed recipes outlining how to optimize the models semantic representation, aesthetics, motion, and realism, as well as how to balance these different aspects. Additionally, we present the data volume used at each training stage, multiple hyperparameters and noise scheduling for both training and inference, and infrastructure optimization strategies. Superior generation quality in both general scenes and complex motion scenarios. We conduct comprehensive evaluation of Waver across multiple dimensions and benchmarks. On the public leaderboard of the Artificial Analysis Arena, Waver ranks third in both the T2V and I2V tracks, with data as of 2025-07-30 10:00 (GMT+8). On our internal Waver-bench 1.0, Waver outperformed Kling2.0 and Wan2.1 (Wan et al., 2025) in terms of motion quality, visual quality, and prompt following; it also surpasses Veo3 in motion quality and visual quality, while its prompt following 4 was slightly weaker than Veo3. On our challenging Hermes Motion Testset, specifically designed for complex motion evaluation, Waver demonstrates significant improvements over Veo3, Kling2.0, and Wan2.1 in both motion quality and prompt following. We hope that the optimization details presented in this technical report can help the community further advance the performance of current video generation models."
        },
        {
            "title": "2 MODEL ARCHITECTURE",
            "content": "We use Wan2.1-VAE (Wan et al., 2025) for efficient video latent compression and powerful dualencoder system, combining flan-t5-xxl (Raffel et al., 2020) and Qwen2.5-32B-Instruct (QwenTeam, 2024), for superior text understanding. Benefiting from the strong performance of Qwen, this dualencoder setup demonstrably improves prompt following in the text-to-image (T2I) task over the single-encoder (flan-t5-xxl) baseline  (Fig. 4)  . The architecture of Waver primarily consists of two components: the Task-Unified DiT and the Cascade Refiner. The Task-Unified DiT is built upon rectified flow Transformers (Esser et al., 2024b), serves as the core generation model. It fuses video and text modalities through Dual Stream + Single Stream architecture. The Cascade Refiner is also based on rectified flow Transformers, functions as super-resolution module. It takes the 480p or 720p videos generated by the Task-Unified DiT as input and upscales them to final resolution of 1080p. This design enables efficient, high-fidelity video super-resolution with fewer inference steps. Figure 3: Architecture of Task-Unified DiT."
        },
        {
            "title": "2.1 TASK-UNIFIED DIT",
            "content": "Unified Input Formulation. To unify diverse generative tasks (T2I, T2V, I2V) within single framework, we employ flexible input conditioning mechanism based on three-part input tensor. This input is formed by concatenating primary noisy latent (V ), conditional frames tensor (I), and binary condition mask (M ask) along the channel dimension. The tensor contains the VAEencoded latents of any known conditioning frames, with other frames filled by black image latents. The ask tensor then specifies which frames are conditions (value 1) and which are to be generated (value 0), providing unique binary indicator for each frame in the sequence. This formulation is highly flexible. It not only allows us to mix tasks and adjust their proportions during training, but also enables straightforward extension to other tasks, such as video interpolation. Hybrid Stream Structure. Our Task-Unified DiT combines Dual Stream and Single Stream blocks to optimize the trade-off between modality alignment and parameter efficiency. The Dual Stream block processes video and text modalities with separate parameters, merging them only during self-attention. This allows for co-adaptation of text and video features, fostering strong alignment, but at the cost of parameter efficiency; in text-to-video generation, the large number of 5 Figure 4: Comparison of 512p T2I results using different text encoders. For each case, image on the left is only using flan-t5-xxl and that on the right is using flan-t5-xxl and Qwen2.5-32B-Instruct. Figure 5: Loss comparison between Hybrid Stream, Dual Stream, and Single Stream structures. Hybrid Streams loss converges faster. video tokens means the text-specific parameters are underutilized. Conversely, the Single Stream block uses shared set of parameters for the combined modalities, maximizing efficiency. However, this shared approach can slow convergence due to the differing statistical distributions of video and text data. Therefore, we adopt hybrid strategy: the first layers use the Dual Stream design to effectively align the modalities, while the subsequent layers switch to the Single Stream design for computational efficiency. This Hybrid Stream structure demonstrably achieves faster convergence than either pure approach, as evidenced in Fig.5. Some key parameters can be found in Tab. 1. Hybrid Position Embedding. To effectively encode spatio-temporal information, we employ hybrid positional encoding scheme that synergizes relative and absolute position signals. This scheme combines 3D Rotary Position Embedding (RoPE) (Su et al., 2024) with factorized learnable position embedding (Zhai et al., 2022). The first component, 3D RoPE, provides relative positional 6 Model Size Input Dim. Output Dim. Num of Heads Head Dim 12B 16 36 16 24 128 Table 1: Key parameter selections in our Task-Unified DiT. information across three dimensions: temporal, height, and width. In contrast to the sinusoidal encoding used in ViT (Dosovitskiy et al., 2020), 3D RoPE offers stronger extrapolation capability for varying video durations and resolutions and reduces distortion and deformation in generated videos. The second component, the factorized learnable positional embedding, supplies absolute position information. It independently encodes each dimension (temporal, height, width) and then sums the resulting embeddings. By providing an explicit positional anchor, this method has been shown to accelerate model convergence during training."
        },
        {
            "title": "2.2 CASCADE REFINER",
            "content": "Training and inference on 1080p videos are computationally expensive. To address this, we propose two-stage approach inspired by (Zhang et al., 2025c). In the first stage, we generate low-resolution video. In the second stage, this video is upscaled to 1080p using dedicated refiner model. This hierarchical process can potentially accelerate the generation of 1080p videos, as the refiner is conditioned on the strong priors provided by the initial low-resolution video. To this end, we trained video refiner that takes 480p or 720p video generated in the first stage as input and outputs 1080p video with identical content but significantly enhanced clarity. Architecturally, the refiner is based on Waver1.0, but we have replaced the standard attention mechanism with window attention (Seawead et al., 2025). For training this refiner, we constructed low-resolution and high-resolution video pairs by applying degradation process to our high-definition training data. We then employ flow matching technique to learn the transition from the low-resolution video distribution to the high-resolution video distribution. At inference time, the output video from the first stage undergoes this same degradation process. This degraded video is then fed into the refiner to generate the final, polished 1080p high-definition result. complete overview of this two-stage pipeline is depicted in Fig. 6. Figure 6: Pipeline of Cascade Refiner. Window Attention We improve the efficiency of 1080p video generation by leveraging window attention to reduce computational costs. This mechanism partitions video tokens into local Wh Ww Wt windows and restricts attention calculations to within these boundaries. To enable crosswindow communication, we alternate between spatial (2x2x1 window) and temporal (1x1x2 window) attention schemes. Furthermore, we observed that attention patterns in the DiT architecture are densest in the shallowest and deepest layers. We therefore adopt hybrid approach, applying full 7 Figure 7: Fig.(a) illustrates the Refiners output, where it has upscaled 480p video from the first stage to 1080p. Besides improving the sharpness, it has also fixed the visual artifacts. Fig.(b) showcases the Refiners video editing ability by transforming the woman in the source video into man. (zoom in for better view.) attention to the first and last eight layers while retaining window attention for the remaining middle layers. This method effectively balances high-fidelity video synthesis with computational tractability. Pixel and Latent Degradation To train our super-resolution model, we synthesize low-quality videos from high-quality sources using two-part degradation process: pixel degradation and latent degradation. First, for pixel degradation, we downsample 1080p videos to 360p and then upscale them back to 1080p. This process simulates the low-resolution output of our first-stage generator. Second, first-stage outputs often contain generative artifacts and distortions not found in simple downsampled videos. To bridge this domain gap, we perform latent degradation by injecting noise into the VAE latent space. The noisy latent, xn, is computed as: xn = (1 wd) + wd n. Here, is the VAE latent of the video with pixel degradation, is Gaussian noise, and wd is weight sampled from predefined range in each training step. In our experiments, we found that sampling wd from the range [0.85, 0.95] yielded good performance. This strategy forces the model to learn robustness against both low resolution and generative distortions, reducing the train-inference distribution mismatch. Refiner Training and Inference We train the refiner model using flow matching objective. The model input is linear interpolation between the degraded and clean latents: The model is trained to predict the degraded latent towards the clean latent: xinput = xn + (1 w) x0. xtarget = x0 xn. Here, xn is the degraded VAE latent from our pixel and latent degradation process, x0 is the groundtruth VAE latent of the high-resolution video, and the weight is sampled from [0, 1] during training. During inference, we first generate low-resolution video using the first stage model. This video is then upscaled to 1080p, and its VAE latent undergoes the same noise degradation process described 8 previously. This final degraded latent is then fed into the refiner, which outputs the enhanced 1080p video. Thanks to our comprehensive degradation strategy, the refiner not only enhances video resolution but also corrects generative artifacts and distortions. Furthermore, we observed that the refiner can perform video editing tasks, such as object modification, when the weight wd is set to large value, as illustrated in Fig.7."
        },
        {
            "title": "3 TRAINING DATA",
            "content": "The effectiveness of video generation models fundamentally depends on the scale, diversity, and quality of the training data. This process begins with systematic preprocessing and segmentation, which extracts relevant clips from multiple sources to maximize content coverage. This is followed by hierarchical filtering process that rigorously removes low-quality and unsafe samples. To further enhance data diversity and address potential gaps in real-world content, we also incorporate synthetic data generated through advanced augmentation techniques. This integrated and automated workflow, as illustrated in Figure 8, enables us to efficiently construct robust dataset, laying solid foundation for training high-performing and generalizable video generation models. Figure 8: An overview of our proposed data processing pipeline. The process consists of five main stages: multi-source acquisition, pre-processing, quality filtering, captioning, and semantic balancing."
        },
        {
            "title": "3.1 DATA PRE-PROCESSING",
            "content": "The data preprocessing stage is critical for transforming raw video data into high-quality samples suitable for model training. Through this structured preprocessing workflow, we ensure that only high-quality, diverse, and representative video clips are included in the final dataset, laying strong foundation for subsequent filtering and augmentation steps. This stage consists of several key steps: Multi-source Video Acquisition. We collect raw video data from variety of sources to ensure diversity in content, style, and scenarios. In addition, for video categories that present greater challenges for generative modeling, such as complex ball sports or scenes with intricate motion, we perform targeted data collection and supplementation to address these specific difficulties. This multisource and targeted acquisition strategy helps to mitigate dataset bias and enhances the generalization capacity of the model. Video Segmentation. The raw videos collected vary in length and are not directly suitable for model training. To address this, we systematically segment them into shorter, manageable clips, following the methodology in (Chen et al., 2025). Specifically, we first leverage PySceneDetect (Developers) to perform initial scene detection. We then refine these preliminary segments by extracting DINOv2 (Oquab et al., 2023) features from each frame and computing the cosine similarity between adjacent frames. When similarity falls below set threshold, we mark shot change and further divide the clip. Finally, our selection criteria are twofold: (1) we retain all clips with durations between 2 and 10 seconds. (2) For clips exceeding 10 seconds, we subsequently sample segments with the lowest internal DINOv2 feature similarity, as these tend to capture significant motion and highlight moments, ensuring both clip coherence and suitability for training. Clip Quality Scoring. Each video clip undergoes comprehensive evaluation in multiple dimensions to ensure its suitability for model training. This systematic scoring framework allows us to build high-quality, balanced dataset and supports flexible filtering and selection of clips at different stages of training. The evaluation consists of several key steps: 9 (a) Video quality issue distribution in over 1M annotated clips used for training the quality model. (b) Action label distribution in the pre-training data, dominated by Social Interactions and Daily Activities. Figure 9: Overview of the data filtering process and dataset characteristics. For basic attributes, we first assess fundamental video properties, such as frame rate (FPS), resolution, and bitrate. Clips that do not meet minimum technical standards are excluded, ensuring that only videos with sufficient clarity and smoothness are retained. For static visual quality, we use pretrained aesthetic model to score each frame and employ OCR to measure the proportion of overlaid text, along with watermark detection, to provide quantitative indicators for subsequent filtering. To assess the dynamic quality of each clip, we analyze its motion magnitude using RAFT (Teed & Deng, 2021) for optical flow computation. To disentangle subject motion from camera movement, we first perform foreground segmentation on the clip, and then computed the optical flow separately for the foreground and background regions. The motion magnitude and diversity are subsequently calculated based primarily on the foreground flow, enabling more precise quantification of subjectspecific dynamics."
        },
        {
            "title": "3.2 QUALITY MODEL",
            "content": "Quality scoring has certain limitations. For example, it is difficult for the motion score to accurately evaluate slow-motion videos; the aesthetics score may not be reliable within certain score ranges; and OCR filtering can miss some watermarks or small text. To obtain higher-quality training data after quality scoring, we train quality model based on MLLM to further filter the data. We select over 1 million video clips with relatively high quality after quality scoring for manual annotation, labeling high-quality samples, and 13 different dimensions of low quality (see Fig. 9a for specific dimensions). We use this annotated data to perform SFT on VideoLLaMA3 (Zhang et al., 2025a), supporting both landscape and portrait videos. During 480p training, we used this model to filter the training data. We also conduct manual evaluation of the quality models predictions on the validation set, finding that the accuracy for samples predicted as high quality was 78%."
        },
        {
            "title": "3.3 CAPTION MODEL",
            "content": "The caption comprehensively describes various elements, including actions, subjects, backgrounds, camera motions, style, spatial relationships, etc. Notably, the description of actions is articulated with exceptional detail. Precise and fine-grained action descriptions play vital role in improving both motion fidelity and the instruction-following capabilities of video generation models with respect to detailed actions. Enhancing action temporal understanding. For each complete sub-action described in the annotated captions, we further annotate its start and end timestamps. We prompt the model with questions regarding the specific actions occurring within given start and end time, and ask the model to respond by describing the sequence of sub-actions that take place during this interval. Through this data construction approach, the model is able to better align fine-grained action descriptions with 10 corresponding video segments during training, thereby enhancing its capability to understand the temporal order of actions. Model Training. Based on the Qwen2.5-VL (Bai et al., 2025), we conduct joint training with both captioning and sub-action description tasks. For the captioning task, the model takes the Tariser2 (Yuan et al., 2025b) caption as input and generates detailed descriptive captions as output. The visual encoder is kept unfrozen during this stage. The training conducted during the DPO stage is aimed at mitigating hallucination. On the DPO (Rafailov et al., 2024) side, DPOP (Pal et al., 2024) loss is employed to maintain the stability of long-form outputs. In this stage, the visual encoder is frozen. The captioning model is trained on English-language data."
        },
        {
            "title": "3.4 SEMANTIC BALANCING",
            "content": "A balanced training dataset is crucial for model performance. However, in practice, certain categories are severely underrepresented in our training data; for example, as shown in Figure 9b, data for the sports category is particularly scarce. To address this, we constructed an action labeling system, annotated the training data according to this taxonomy, and balanced the data based on the distribution of these action labels. Specifically, we use Qwen2.5-32B (QwenTeam, 2024) to perform multi-level action label classification on the caption of the training videos, which includes 12 first-level action labels, 100 second-level action labels, and over 6,000 third-level action labels. We then analyzed the distribution of each level of action labels in the data. If an action labels proportion was too low, we addressed this imbalance by oversampling existing data and generating additional synthetic samples."
        },
        {
            "title": "3.5 HIERARCHICAL DATA FILTERING",
            "content": "Our data filtering strategy consists of several hierarchical stages, each applying stricter and more comprehensive quality criteria. Early stages focus on keeping diverse video content and basic semantic consistency. In later stages, we add more advanced checks for visual quality, composition, and data balance. By the final stage, only the highest-quality and high-resolution samples are kept. This process ensures the dataset is gradually refined to meet the needs of each training phase and supports effective model development. Figure 10: The hierarchical data filtering funnel, which progressively refines the dataset through increasingly strict quality criteria. 192p Pre-training. At this initial stage, we retain as much video data as possible to expose the model to diverse semantics and video types at low resolution. Filtering mainly removes clips with excessive overlaid text to avoid meaningless text displays, as well as static videos with minimal motion. Here, we focus on maintaining motion consistency, semantic alignment between text and video, and temporal coherence, without strict requirements on high-level visual quality. 480p Pre-training. Building on the previous stage, this phase introduces aesthetic scoring, stricter thresholds for overlaid text and motion, and an 8-dimensional quality model. Clips are now filtered for overall visual quality, artifacts such as black borders, texts or watermarks, playback speed anomalies, perspective changes, and motion stability, enabling more effective removal of low-quality or problematic videos. 11 720p Pre-training. While basic scoring and filtering remain similar to the previous stage, this stage expands the quality model to 12 dimensions by adding four new dimensions: subject cutoff, overexposed, dart or blurry, and image animation to better capture composition and color issues. This enables more refined selection of high-quality clips. 720p Continue Training (CT). At this stage, we further enhance video quality by adding two new criteria: video definition and color balance. Additionally, we enforce balance across different data sources and semantic categories to ensure diversity and representativeness, even as the dataset size is further reduced through stricter filtering. 720p Supervised Fine-Tuning (SFT). At this stage, we introduce synthetic data, including both highly aesthetic and surreal video samples. Although the training target remains 720p, only highresolution (2k/4k) real video clips are retained to ensure maximum clarity. For complex motion videos, manual selection and detailed caption annotation are performed to further improve data quality. 1080p Refinement. At this final stage, we focus on maximizing video clarity. Only the 720p CT data and the highest-quality 2k/4k high-resolution data are retained, providing the best possible samples for advanced model training."
        },
        {
            "title": "4 TRAINING / INFERENCE RECIPE",
            "content": "In this section, we provide detailed introduction to the entire training and inference process of the model, as well as how to improve the generated videos motion, aesthetics, realism, and color from the perspectives of data, training, noise scheduling, hyperparameter tuning, and inference."
        },
        {
            "title": "4.1 MULTI-STAGE TRAINING",
            "content": "Our entire multi-stage training schedule is shown in Tab. 2, which contains the tasks of T2I, T2V, I2V, and Refiner. We also present the data volume of each stage and the corresponding training hyperparameters. T2I. We first train the T2I task, starting with 256p images and progressively increasing the resolution up to 1024p, covering multiple aspect ratios such as 1:1, 16:9, 9:16, 4:3, and 3:4. The T2I task enables the model to learn the correspondence between text and images and enhance the models instruction following capability. T2V & I2V. After T2I, we begin training the T2V task, where the video aspect ratio is consistently maintained at 16:9 and 9:16. We also include T2I in joint training, starting from 192p, with the video resolution set to 192352, and both images and videos kept at the same resolution. We first train at 192p at frame rate of 12fps, followed by 192p 16fps. We find that using lower fps helps the model better learn motion in videos. Starting from 480p, we jointly train the T2V and I2V tasks by controlling whether to include the first-frame condition, with 20% probability of training I2V. The video resolution is 480864 at frame rate of 16fps. Training at 720p is divided into three stages: Pretraining, Continue Training (CT), and Supervised Finetuning (SFT), with gradually improving data quality. For T2I, in addition to 7201280, we also add 10801920 image data for training. Our experiments show that including 1080p images in training can improve visual quality. Refiner. The 1080p training for the Refiner is based on the checkpoint after 720p CT, using both 10801920 video and image data, and is also divided into Refine and SFT stages. During the SFT stage, we use only images and videos with resolutions above 2K as training data to improve the overall visual quality. Training hyperparameters. Throughout the entire training process, we ensure that the data from each stage is seen for at least one epoch, and the overall learning rate shows decreasing trend. Following SD3 (Esser et al., 2024a), the sigma shift in flow matching increases as the resolution increases."
        },
        {
            "title": "Task",
            "content": "T2I T2I T2I T2V / T2I T2V / T2I T2V+I2V / T2I T2V+I2V / T2I T2V+I2V / T2I T2V+I2V / T2I (T+V)2V / T2I (T+V)2V / T2I"
        },
        {
            "title": "Refine\nSFT",
            "content": "256p 512p 1024p 192p 12fps 192p 16fps 480p 16fps 720p 24fps 720p 24fps 720p 24fps 170M 100M 60M 185M / 40M 185M / 40M 41M / 40M 22M / 15M 4M / 15M 0.1M / 15M 1080p 24fps 1080p 24fps 3M / 3M 0.2M / 3M 24 3 1 2 1.5 3 1 2 2 1 1 1e-4 6e-5 3e-5 1e-5 1e-5 5e-5 3e-5 1e-5 5e-6 1e-5 5e1.0 2.0 3.0 1.5 1.5 2.5 3.0 3.0 3.0 4.5 4.5 Table 2: The entire training process for the four tasks: T2I, T2V, I2V, and Refiner. In the Data volume column, the value before the / refers to the amount of video data, while the value after the / refers to the amount of image data. refers to million."
        },
        {
            "title": "4.2 REPRESENTATION ALIGNMENT",
            "content": "Motivated by recent advances in image generation, where studies like REPA (Yu et al., 2024) and VAVAE (Yao et al., 2025) have established that leveraging semantic information significantly improves model convergence, we investigate parallel approach for video generation. We hypothesize that integrating video-level semantic information can similarly accelerate the training process of video generation models. To this end, we employ Qwen2.5-VL (Bai et al., 2025) to extract high-level semantic features from videos, model chosen for its capability to process inputs of arbitrary length. These semantic features are then aligned with the intermediate features of the DiT model by applying constraint based on cosine similarity. For computational efficiency, this constraint is selectively applied only during the 480p training stage for both video and image data. This avoids impeding the fast-iterating 192p stage and circumvents the prohibitive storage overhead required for 720p features. Concretely, the alignment operates on two feature sets. The first, denoted as RHhWhThCh , are latent features extracted from the 16th layer of our Task-Unified DiT model, where H, W, and represent the height, width, time, and channel dimensions, respectively. The second, RHf Wf Tf Cf , are the corresponding high-level semantic features from Qwen2.5-VL. To optimize training throughput, are pre-computed and stored offline. To harmonize these features for alignment, we perform two preprocessing steps on the DiT features h. First, to mitigate storage pressure and match spatial-temporal dimensions, we downsample to produce hd = d(h) RHf Wf Tf Ch . This involves downsampling factor of 2 in spatial dimensions and 4 in the temporal dimension (i.e., Hh = 2Hf , Wh = 2Wf , Th = 4Tf ). Second, we employ lightweight MLP, gϕ, to project the channel dimension of the downsampled features, yielding hg = gϕ(hd), which now shares the same channel dimension as f. The alignment is then achieved by maximizing the cosine similarity between the processed DiT features hg and the semantic features f. This is formulated as the following minimization objective: Lalign = Ex (cid:34) (cid:35)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 hg,i fi hg,ifi where represents the training data, and the sum is over all = Hf Wf Tf feature vectors in the tensor. Finally, this alignment loss is incorporated into our final training objective: Ltrain = Lfm + λLalign, where Lfm is the primary flow matching objective and λ is hyperparameter, set to 0.5 in our experiments. The inclusion of representation alignment constraint yields significant enhancement in the semantic quality of the generated videos. We demonstrate this through an ablation study comparing our method 13 Figure 11: Qualitative comparison of generated videos with and without our representation alignment method. For each of the four cases shown, the frame on the left is from the baseline model (no alignment), while the frame on the right is from our model with the alignment constraint. Figure 12: Top: 480p T2V results without 192p pretrain. Bottom: 480p T2V results with 192p pretrain. Results for prompt Two people are paddling two kayaks vigorously in the river. against baseline trained without the constraint. Both models were initialized from an identical 192p checkpoint and trained for 12,000 steps at 480p. Fig.11 provides side-by-side comparison, where the outputs from our method clearly display more organized and meaningful semantic structures."
        },
        {
            "title": "4.3 MOTION OPTIMIZATION",
            "content": "We optimized motion for the T2V and I2V tasks from three aspects: low-resolution video training, noise scheduling, mixed training of T2V & I2V, and poor motion data filtering. These improvements enable Waver to generate videos with large-scale, smooth motion. Low resolution video training. We find that training with low-resolution videos (such as 192p) is crucial for the model to learn motion. Waver learns motion at 192p, dividing the process into two stages: 12fps and 16fps. This approach helps to partially decouple motion learning from visual quality and semantics. Compared to directly switching from the T2I training to 480p T2V training, we find that introducing 192p T2V training as an intermediate step enables faster convergence during 480p training and results in generated videos with larger motion, as is shown in Fig. 12. Therefore, we invest considerable data and computational resources in 192p T2V training. Noise scheduling. We follow the flow matching schedule proposed in SD3 (Esser et al., 2024a). It highlights that timestep sampling is crucial for flow matching training. Specifically, timesteps near the middle of the [0, 1] interval are more challenging for the model to learn. Therefore, probability density functions that assign higher probability to the middle and lower probability to the ends, such as the logit-normal and mode functions, are used for timestep sampling, as is shown in Eq. 1 and Eq. 2, respectively. 14 Figure 13: Top: 720p T2V results with logit-normal sampling. Bottom: 720p T2V results with mode sampling. Results for prompt In the center of the boxing ring, two male boxers are fighting each other. Figure 14: Probability density functions for timestep sampling. While Text-to-Image uses broad logit-normal distribution (Eq. 1), Text-to-Video and Image-to-Video tasks employ sharply peaked mode distribution (Eq. 1) to generate larger motion amplitudes. πln(t; m, s) = 1 2π 1 t(1 t) (cid:16) exp (logit(t) m)2 2s2 (cid:17) , fmode(u; s) = 1 (cid:16) cos2(cid:0) π 2 (cid:17) u(cid:1) 1 + . (1) (2) We conduct ablation studies on different timestep sampling functions on 192p, 480p, and 720p T2V tasks, and the results are consistent across settings. We find that mode sampling enables the model to generate videos with larger motion amplitudes, as is shown in Fig. 13. Both the top and bottom models were trained for 10,000 steps under the 720p setting. The top model uses logit-normal sampling, while the bottom model uses mode sampling. We observe that the model with mode sampling generates videos in which the two boxers are engaged in more intense fight, with more frequent and faster actions. So we adopt Lognorm(0.5, 1) for the T2I task, Mode(1.29) for the T2V and I2V tasks. The distribution of timestep sampling is shown in Fig. 14 with sigma shift = 3. T2V & I2V joint training. Motion is crucial optimization objective for the I2V task, as all generated sequences share the same initial frame, and the quality of the subsequent motion largely determines the overall performance of the model. The I2V task often suffers from the issue of limited motion, as the strong conditioning on the initial frame leads the model to generate subsequent frames that are overly similar to the first frame. Some studies (Liu et al., 2025; Tian et al., 2025; Zhang et al., 2025d) address this problem by employing sophisticated designs, such as incorporating external supervision, representation, or conditioning, to enhance the motion generated by the model. We adopt simpler approach to enhance motion in the I2V task by jointly training I2V and T2V. Specifically, we introduce the initial frame as condition for the I2V task with probability of 20%. This strategy prevents the initial frame conditioning from becoming overly dominant and helps maintain comparable motion magnitudes between I2V and T2V. In the early stages of training, we observe that the initial frame consistency in I2V is relatively poor; however, as training converges, 15 Figure 15: Top: 720p I2V results without T2V joint training. Bottom: 720p I2V results with T2V joint training. Results for prompt man is skiing. Figure 16: Raw training video clips and the foreground optical flow. Top: Foreground motion score is 11.0. Bottom: Foreground motion score is 0.75. both the initial frame consistency and the magnitude of motion in I2V improve significantly. We compare the motion generated by models jointly trained on T2V and I2V tasks with that trained solely on I2V. As shown in Fig.15, the results clearly demonstrate that joint training leads to significantly larger motion amplitudes in the I2V task. Poor motion data filtering. The motion amplitude in the training data has critical impact on the motion quality of generated videos. We primarily filter out static videos and those with excessive motion using foreground motion score mentioned in Sec. 3.5. Fig. 16 presents examples of videos with different foreground motion scores. However, we found that some slow-motion and image-effect videos cannot be effectively filtered by the motion score alone. These two types of videos can also significantly degrade the models motion quality. Therefore, we further filter out slow-motion and image-animation videos using the video quality model described in Sec.3.2. After these two filtering steps, the remaining videos generally exhibit motion within normal range and are suitable for training models to achieve better motion performance."
        },
        {
            "title": "4.4 AESTHETICS OPTIMIZATION",
            "content": "Obtaining high-aesthetic video data is significantly more challenging than collecting high-quality images. This gap is particularly pronounced for dynamic, high-motion scenes, as real-world footage is often sourced from documentary-style recordings which prioritize capturing an event over artistic composition and are susceptible to technical flaws like motion blur. The challenge of generating coherent, high-fidelity motion is primary focus of state-of-the-art video generation research. To address this critical data gap, we introduce dedicated synthetic data enhancement stage Girdhar 16 et al. (2023), aimed at further boosting both the visual quality and creative capacity of our model by leveraging high-quality image assets. Step 1: Synthetic Data Generation. Leveraging our unified multi-task model, we utilize high-quality image datasets and synthesized surreal images, along with our I2V capability, to construct diverse collection of synthetic video samples. These videos are meticulously generated to emphasize aesthetic value, featuring visually striking scenes, advanced composition, and sophisticated use of color and lighting. This strategy also enhances the models ability to follow creative instructions and handle surreal concepts. As illustrated in Figure 17, our synthetic data exhibits significant improvement in aesthetic quality over typical real-world footage. Figure 17: Aesthetic comparison for running scene. Top: Typical real-world video. Bottom: Our synthetic data shows superior visual quality, including better lighting and composition. Step 2: Rigorous Quality Control. To ensure the highest quality, all synthetic videos undergo rigorous manual review. Samples exhibiting distortions, excessive AI-ness (such as loss of fine details or unnatural saturation), or insufficient motion are filtered out. Only samples that pass this review are included in the training set. Each video clip is independently reviewed by at least two annotators, and only those with unanimous approval are retained. The statistics of this review process, including the pass rate and breakdown of failure reasons, are detailed in Figure 18a. Step 3: Balanced Data Diet. We carefully balance the proportion of synthetic and real video data to preserve realism and prevent overfitting to synthetic styles. Our experiments indicate that relying solely on synthetic data can lead to loss of visual detail and limited semantic alignment, particularly given the limited scale of manually reviewed synthetic samples. Finetuning Results. We conduct high-aesthetic finetuning using this curated data. This aesthetics optimization strategy yields significant improvements in visual quality for the T2V task, without compromising temporal coherence. This is validated through both quantitative human evaluations and qualitative analysis. Quantitative improvements are demonstrated in side-by-side human evaluation on 304-prompt general benchmark (Figure 18b). The fine-tuned model achieved 39.5% win rate in visual quality, substantially outperforming the base models 32.9%. Crucially, the evaluation also confirmed that motion quality remains comparable between the two models, indicating our approach successfully isolates and enhances aesthetic performance. Qualitative enhancements are clearly visible in generated samples (Figure 19). After fine-tuning, videos exhibit noticeable improvements in color vibrancy, brightness, clarity, and overall chromatic harmony."
        },
        {
            "title": "4.5 MODEL BALANCING",
            "content": "The performance of video generation models is often characterized by various trade-offs. For example, from the perspective of model behavior, increased motion typically leads to greater blurriness, resulting in lower visual quality, while videos with high visual fidelity often exhibit limited motion. Moreover, larger motion is frequently accompanied by more artifacts, and there is also trade-off between realism and the presence of artifacts. In terms of scene generalization, we observe that, especially in the early stages of training, our model often struggles to distinguish between cartoon and realistic styles, or it may perform well on realistic scenes but poorly on non-realistic ones, or vice versagenerating plausible non-realistic scenes that nevertheless exhibit an artificial AI-like appearance. These issues are particularly pronounced in 17 (a) Manual review statistics for synthetic video samples (N=8381), where distortion is the primary failure reason (b) Human evaluation (SBS) results before and after high-aesthetic finetuning, which shows 7% improvement in visual quality after finetuning on the curated synthetic data, with motion quality preserved. Figure 18: Statistics and impact of our synthetic data enhancement stage. Figure 19: Comparison of four videos generated by models before and after high-aesthetic finetuning. For each case, the frame on the left is before finetuning, and that on the right is after finetuning. the T2V task. Balancing different aspects of model performance remains significant challenge. To address this, we employ strategies including prompt tagging, video APG, and model averaging. Prompt tagging. We employ prompt tagging approach to distinguish between different types of training data. Our training dataset is sourced from wide variety of origins, often exhibiting diverse styles and quality levels. We assign distinct tags to the training data based on both the data source and the quality model described in Sec. 3.2. In terms of style, we differentiate between categories such as anime (2D, 3D, etc.), game, CG, real, and synthetic data. During training, we prepend the caption with prompt describing the videos style. Regarding quality, we distinguish between high-definition, slow-motion data and low-definition, fast-motion data, appending prompt describing video quality to the end of the training caption. During inference, we incorporate prompts describing undesirable qualities, such as low definition or slow motion, into the negative prompt. For specific style requirements (e.g., anime style), we prepend the corresponding descriptive prompt to the overall prompt using prompt rewriting techniques. Video APG. We extend APG (Sadat et al., 2024) to video generation to enhance realism and reduce artifacts. APG decomposes the update term in CFG into parallel and orthogonal components and down-weights the parallel component to achieve high-quality generations without oversaturation. We find that normalizing the latent from [C, H, W] dimension achieves fewer artifacts than from [C, T, H, W] dimension. For the hyperparameters, we find the normalization threshold 27 and guidance scale 8 achieve good balance between realism and artifacts. Fig. 21 shows the comparison between CFG and video APG during inference. It is evident that video APG achieves greater realism without introducing additional artifacts. Model averaging. We observe that models trained on different datasets, at various training stages, or with different timestep sampling probabilities exhibit distinct characteristics. For example, some models generate videos with excellent visual quality but slower motion, while others produce faster motion but with more artifacts. We employ model averaging strategy (Li et al., 2025) to integrate these models with diverse attributes. Empirically, we find that merging 7 to 10 models yields favorable results. Comprehensive human evaluations presented in Fig. 22 indicate that the averaged model outperforms the individual model in visual quality, motion quality, and instruction following. Figure 20: Six different styles of videos generated by Waver T2V after prompt tagging. Figure 21: Top: 720p T2V results with CFG=5. Bottom: 720p T2V results with the optimal APG hyperparameters (CFG=8, norm threshold=27). Results for prompt An off-road motorcyclist speeds along dusty motocross track, with the tires kicking up dirt and gravel."
        },
        {
            "title": "4.6 PROMPT REWRITING",
            "content": "The purpose of prompt rewriting is to align diverse user inputs as closely as possible with the captions used during model training, thereby achieving results that are more consistent with the models training performance. We do not train dedicated prompt rewriting model; instead, we directly utilize existing LLMs such as GPT-4.1. The overall strategy involves enriching the prompts with additional details related to camera perspective, appearance, background, and actions. When the action duration is too short, we supplement the prompt with additional action descriptions. Furthermore, we provide several examples of training captions to guide the rewriting model, ensuring that the rewritten prompts are as closely aligned as possible with the distribution of training captions. Fig. 23 shows the generated videos before and after rewriting. We observe that the generation results after prompt rewriting exhibit clear advantages in terms of visual richness and aesthetic quality. For the two different generation durations, 5 seconds and 10 seconds, we designed two distinct system prompts. The rewrite for the 10-second duration, compared to that for 5 seconds, conditionally incorporates more action descriptions and results in longer overall prompt. 19 Figure 22: Human evaluation (SBS) results before and after model merging. Motion quality improves by 5% after merging, while visual quality and prompt following improves by 2% and 1%. Figure 23: Top: 720p T2V results generated with the original prompt. Bottom: 720p T2V results generated with the rewritten prompt."
        },
        {
            "title": "INFRASTRUCTURE",
            "content": "Hybrid sharded mode of FSDP. Since Fully Sharded Data Parallel (FSDP) is recognized for its simplicity and flexibility in large-scale distributed training, we employ the hybrid sharded mode of FSDP (Zhao et al., 2023) as our primary distributed training solution. Considering that communication bandwidth scales inversely with the total number of GPUs, we adopt hybrid sharding strategy with inner shard sizes of 64 or 128 to balance per-GPU model memory consumption against communication overhead. Torch compile. To maximize performance gains with minimal engineer effort, we leveraged PyTorchs torch.compile to compile individual transformer layers into fullgraphs, which enables PyTorchs inductor to automatically fuse underlying CUDA operators together during compilation. Ulysses sequence parallelism. When processing video samples at 720p or 1080p resolution with durations of 810 seconds, the input sequence length can reach several hundred thousand tokens, placing considerable demands on GPU memory. To address this, we adopt the Ulysses (Jacobs et al., 2023) sequence parallelism approach, which distributes the computation of single input sample along the sequence dimension across sequence-parallel process groups. This strategy significantly alleviates memory pressure and enables support for longer sequences within the constraints of available hardware memory. Bucket Dataloader. To support training on videos of arbitrary lengths, we adopted bucket dataloader to ensure that the token lengths of videos in each sampled batch are identical. Given the prevalence of short-duration videos in the training dataset and their low training efficiency, 20 Figure 24: Illustrative diagrams of Ulysses sequence parallelism, Selective Activation Checkpointing, and Activation Offloading. In the Activation Offloading diagram, fop denotes forward operation, bop denotes backward operation, h2d refers to host-to-device memory copy, and d2h refers to device-to-host memory copy. we combined the Sequence Packing algorithm to enhance the training efficiency of short videos. Specifically, we used SPFHP (Krell et al., 2021) to pack data based on token length, thereby reducing the number of short videos in the training data. We then bucketed the data according to the computational load of each sequence and loaded it using the bucket dataloader. Selective Activation Checkpointing. To achieve optimal computational performance within the constraints of available hardware memory, we employ torch Selective Activation Checkpointing (SAC) to enable fine-grained selection of operators for recomputation. In our SAC policy, operators with lower recomputation costs, such as element-wise addition and multiplication, are selectively recomputed during the backward pass, while activations of more computationally expensive operators, such as attention and GEMM operations, are retained in memory whenever feasible. This approach effectively balances memory savings and computational overhead. Activation offloading. We further employ an activation offloading strategy to reduce activation memory usage. In this approach, activations produced on the GPU during the forward pass are copied to CPU memory, and subsequently prefetched back to GPU memory several steps ahead of their consumption during the backward pass. The offloading and prefetching processes are executed in dedicated CUDA stream that overlaps with the main computation stream, resulting in negligible overhead. The memory savings achieved through this strategy enable us to retain greater number of operators in memory under the SAC policy during training. MFU stands for Model FLOPs Utilization. It is metric that measures the proportion of the theoretical peak floating-point operations per second (FLOPs) of hardware accelerator (such as GPU) that is actually utilized by model during training or inference. Finally, based on the aforementioned optimizations, the MFU achieved during training at different resolutions is summarized in Tab. 3."
        },
        {
            "title": "MFU",
            "content": "192p T2V / T2I 480p T2V+I2V / T2I 720p T2V+I2V / T2I 1080p (T+V)2V / T2I 0.32 0. 0.39 0.40 Table 3: MFU at different training stages."
        },
        {
            "title": "6.1 ARTIFICIAL ANALYSIS ARENA",
            "content": "Artificial Analysis is popular platform for benchmarking image and video generation models. It allows the public to compare results from different generative models in an open setting. Based on large number of user comparisons, the platform uses Elo scores to rank models according to user preferences. The ranking results are sourced from the website of the official Artificial Analysis text-to-video (T2V)1 and image-to-video (I2V)2 leaderboard, with data as of 2025-07-22 10:00 (GMT+8). Waver ranks third in both the T2V and I2V tracks, as is shown in Fig. 25. Figure 25: On the left and right are the official T2V and I2V ranking results from Artificial Analysis (as of 2025-08-05 12:00 (GMT+8)), with Waver ranking third on both lists."
        },
        {
            "title": "6.2 COMPREHENSIVE EVALUATION",
            "content": "Evaluation Metrics. We define comprehensive set of metrics for our human evaluation, organized into three main categories. 1) Motion Quality. This category assesses the physical realism and coherence of movement. This includes: Action, which evaluates if the motion is natural and smooth, conforming to the inherent laws of movement; Interaction, which checks if interactions (between subjects, subject-object, or between objects) adhere to physical laws; and Distortion, which determines if the subject exhibits artifacts like noise, frame loss, or blurring, and ensures the subject remains consistent throughout. 2) Visual Quality. This category evaluates the aesthetic and technical quality of the image itself. Key aspects are: Image Quality, covering texture, lighting, and the presence of visual artifacts such as flickering or overexposure; Color, for the appropriateness and appeal of the color scheme; Clarity, measuring image sharpness; Realism, determining if the visual output is sufficiently realistic (or stylistically consistent for non-realistic styles); and Aesthetics, for the overall visual appeal. 3) Prompt Following. This category measures how faithfully the generated video adheres to the users text prompt. This is broken down into: Subject, ensuring the specified subject(s) (e.g., humans, animals, objects) are present; Subject Description, for the accuracy of attributes like number and color; Action Accuracy, checking the correctness of actions, temporal sequence, and direction; Action Magnitude, assessing the amplitude and speed of actions; and Camera Movement, evaluating camera operations like panning, zooming, and tilting. Evaluation Benchmarks. To apply these metrics comprehensively and explore the upper limits of our models motion generation, we introduce two specialized benchmarks. The first, Waver-Bench 1.0, is broad, general-purpose benchmark consisting of 304 samples that cover wide range of 1https://artificialanalysis.ai/text-to-video/arena?tab=leaderboard&input=text 2https://artificialanalysis.ai/text-to-video/arena?tab=leaderboard&input=image 22 Figure 26: Some video examples of the Waver-Bench 1.0 generated by Waver. Figure 27: Some video examples of the Hermes Motion Testset generated by Waver. scenarios, including sports, daily activities, and surreal scenes. The second, the Hermes Motion Testset, is specifically designed to challenge motion generation. It comprises 96 prompts focused on 32 distinct types of sports activities, such as tennis, basketball, and gymnastics. Figure 26 and Figure 27 present several generated examples for each benchmark. Human Evaluation. On these benchmarks, we conducted extensive human evaluations to assess our models performance against leading competitors (Veo3, Kling 2.0, and Wan 2.1 14B). In side-by-side comparison format, human raters were shown videos generated by our model (Waver) and competitor, then asked to judge which video was superior. The judgment was based on the comprehensive criteria defined above, including motion quality, visual quality, and prompt following. 23 For the Hermes Motion Testset, we placed particular emphasis on motion-related sub-dimensions, including Action Accuracy and Action Magnitude. On the general-purpose Waver-bench 1.0 (Figure 28a), Wavers performance was notably strong. It demonstrated significantly superior motion quality, visual quality, and prompt following when compared to Wan 2.1 14B. Against Kling 2.0, Waver also achieved better visual quality and prompt following, and slightly outperformed it in motion quality. Its performance relative to Veo3 was also competitive, exhibiting higher visual and slightly better motion quality, though its prompt following was marginally weaker. On the demanding Hermes Motion Testset (Figure 28b), Waver substantially outperforms all baselines in both motion quality and prompt following. This dominant performance, particularly on motion-focused benchmark, highlights our models exceptional capability in generating high-fidelity, dynamic motion that accurately adheres to textual descriptions. (a) Evaluation on Waver-bench 1.0 (b) Evaluation on the Hermes Motion Testset Figure 28: Human evaluation results comparing our model (Waver) with leading competitors. Users were presented with side-by-side video comparisons and asked to choose which was better or if they were tied. The stacked bar charts show the proportion of user preference votes across different quality dimensions."
        },
        {
            "title": "7.1 DIT MODEL SPARSITY",
            "content": "In DiT, the computational complexity of standard self-attention scales quadratically with the number of input tokens, i.e., O(N 2). When processing high-resolution, long-duration videos, the number of tokens can easily reach hundreds of thousands, making full self-attention computationally prohibitive and major performance bottleneck. For instance, when generating 5-second 1080p video, attention operations can constitute over 90% of the total computational cost. Therefore, reducing computational overhead via sparse attention mechanisms becomes crucial for efficient video generation. While some approaches (Seawead et al., 2025)including the one used in our Cascaded Refiner in Sec.2.2leverage window attention to improve efficiency, this method suffers from inherent limitations. By relying on manually defined, fixed, and non-overlapping windows to constrain the attention range, it fails to capture the natural and content-adaptive patterns typically present in learned attention maps. 24 Figure 29: An illustration of attention maps is presented, with the query on the vertical axis and the key on the horizontal axis. The visualization reveals three central characteristics: (1) Heterogeneity across attention heads, reflecting functional diversity among heads; (2) Layer-wise sparsity dynamics, showing how attention sparsity shifts across layers; and (3) Timestep-wise sparsity evolution, highlighting changes in attention patterns throughout the denoise process. To develop sparse attention patterns that better align with video generation dynamics, we conducted detailed visual analysis of attention maps across different heads, layers, and timesteps. This analysis revealed three key patterns, as illustrated in Fig.29: 1) Heterogeneity Across Attention Heads: Different attention heads exhibit diverse focus patterns, including spatial-local (concentrating on local regions), temporal-local (attending to adjacent frames), global (capturing long-range dependencies), and fixed-column (consistently focusing on specific token columns). 2) Layer-wise Sparsity Dynamics: Attention sparsity varies significantly across layers, following densesparsedense pattern throughout the network. 3) Timestep-wise Sparsity Evolution: The sparsity of attention also evolves over the denoising timesteps, generally increasing as the generation process advances. These observations suggest that sparse attention strategy adaptive to the intrinsic structure of attention patterns would outperform fixed-window approaches. Based on this insight, we explore two potential directions: 1) Spatial-Temporal Sliding Window Attention. This method offers better alignment with spatio-temporally local attention patterns compared to conventional window attention. Although it has been explored in prior work (Xi et al., 2025; Zhang et al., 2025b), it remains limited in flexibility and is ineffective at capturing non-local dependencies such as fixed-column patterns. 2) NSA-Based Adaptive Sparse Attention. Native Sparse Attention (NSA) (Yuan et al., 2025a) provides greater flexibility in designing attention patterns. However, existing NSA methods are primarily designed for one-dimensional text data. Effectively adapting them to videoa 3D spatio-temporal mediumrequires careful redesign to incorporate its structural properties. 25 Building on these findings, our future work will focus on developing advanced sparse attention mechanisms that not only enhance computational efficiency, but also enable high-performance real-time video generation, thereby paving the way for broader practical adoption. (a) An overview of our VAE and ShuffleDown/Up blocks. (b) Comparison of the text fidelity between Wan 2.1 VAE and our VAE on I2V generation task. Figure 30: Overview of our proposed video VAE and its effectiveness in improving text fidelity."
        },
        {
            "title": "7.2 VAE IMPACT ON VIDEO GENERATION",
            "content": "Video VAE not only encodes videos into latent tokens to accelerate diffusion training, but also has significant impact on video generation quality. In this section, we introduce our VAE training recipes for addressing visual quality issues in video generation, including text fidelity, grainy textures, and background distortion. Improving Text Fidelity with ShuffleDown/Up Blocks We build causal video VAE that compresses the temporal dimension by 4 times and each spatial dimension by 8 times. Different from previous works (Yang et al., 2025; Wan et al., 2025), we introduce pixel-shuffle and pixel-unshuffle operations (Shi et al., 2016) to develop the 3D downsample blocks and 3D upsample blocks. During downsampling, pixel unshuffle can directly map spatial information to the channel dimension and reduce information loss. During upsampling, pixel shuffle can better restore the spatial structure. An overview of our VAE is shown in Fig.30a. The yellow block is the ShuffleDown block for downsampling, and the green block is the ShuffleUp block for upsampling. , H, , and indicates the input tensor shape. Cin denotes the number of input channels of each block, and Cout denotes the number of output channels of each block. The proposed ShuffleDown and ShuffleUp blocks significantly enhance the video reconstruction capability of our VAE, thus improving the text fidelity of the I2V task. As shown in Fig. 30b, the I2V model with our VAE preserves the text details in the conditional image, whereas the Wan 2.1 VAE loses these details and generates videos with distorted text. Mitigating Grainy Textures and Background Distortion with KL Loss common practice when training Video VAE is to use weighted sum of several loss functions, including L1 reconstruction loss, LPIPS perceptual loss, GAN loss, and KL loss (Yang et al., 2025; Wan et al., 2025; Polyak et al.). LV AE = λ1L1 + λ2LLP IP + λ3LGAN + λ4LKL. (3) The KL divergence loss in our VAE is computed in closed form for the Gaussian case as follows: LKL = KL (cid:0)N (µ, σ2I) (0, I)(cid:1) = 1 2DT HW (cid:88) d,t,h,w (cid:0)σ dthw + µ2 dthw 1 log σ2 dthw (cid:1) , (4) where D, , H, and denote the latent dimension, temporal length, height, and width of the latent tokens, respectively. And µdthw and σdthw are the mean and standard deviation at each position. 26 Figure 31: Comparison of generated video quality with different VAEs on the I2V task. Unlike (Esser et al., 2021), which computes the KL loss as sum, we take the mean over all positions to ensure compatibility with multi-resolution training. The KL loss serves as regularization objective, encouraging the learned latent distribution to approximate standard normal distribution. The weight of the KL loss, denoted as λ4 in Eq. 4, controls the strength of the regularization. Our findings indicate that the KL loss in VAE training has greater impact on the video generation task than on the VAE reconstruction task. The KL loss affects the generated video quality in two ways: (1) if the KL loss weight is too small, the decoder may generate grainy textures; (2) if the KL loss weight is too large, the background can become easily distorted. We investigate this problem within the I2V (Image-to-Video) task to enable precise control over the generated scenes We explore this problem in the context of the I2V (Image-to-Video) task, as I2V allows us to consistently control the generated scenes and ensure fair comparison. Specifically, we finetune the Waver I2V model using different VAEs, and use the same conditional image and prompt as inputs across all I2V models. We then compare the videos generated by our VAE with those produced by the original Wan 2.1 VAE (Wan et al., 2025) and the CogvideoX VAE (Yang et al., 2025), as illustrated in Fig. 31. The results indicate that when the KL loss weight is set too low, videos generated by the Waver I2V model with Wan VAE display obvious grainy textures. Note that the tokens predicted by the diffusion model are generally less accurate than those generated by the VAE encoder. Consequently, prediction errors may lead to mode collapse if the VAE decoder lacks robustness. larger KL loss weight encourages VAE to predict higher values of σdthw, resulting in noisier sampling process. The reparameterization-based sampling in the VAE acts as an augmentation mechanism for the decoder, thereby enhancing its robustness in video generation. In contrast, if the KL loss weight is too high, the Waver I2V model with CogvideoX VAE can cause the background to become obviously distorted in some cases. In the second row of Fig. 31, the ground appears swollen in the videos generated by CogvideoX VAE, whereas it is flat in the corresponding condition image. Next, we examine how the KL loss weight influences the VAE latent distribution and, in turn, affects the learning of the diffusion model. We visualize the latent distribution of CogvideoX VAE and its KL=0 variant, as shown in Fig. 32. We find that even though the two variants of VAEs have similar reconstruction results, their latent distributions are largely different. When the KL loss weight is set to 0, the VAE is able to learn smooth and well-structured latent space. In contrast, as the KL loss weight increases, the KL term begins to dominate the training objective. The learned latent representation begins to collapse to the prior distribution (0, I). As result, the latent representations become noisier, thereby increasing the learning difficulty for the diffusion model. Based on these observations, we finally set the KL loss weight to 3e 4 to train our VAE. Decrease LPIPS Loss Weight to Avoid Grid-like Artifacts We find that although the LPIPS loss (Zhang et al., 2018) helps stabilize VAE training, it can also introduce grid-like artifacts in video generation, particularly in detailed regions with high-motion dynamics. As illustrated in Fig. 33, the VAE trained with higher LPIPS loss weight exhibits obvious grid-like artifacts on the ground, 27 Figure 32: Comparison of the latent distributions and reconstructed images between CogVideoX VAE and its KL=0 variant. Figure 33: Ablation study of video generation results using VAEs trained with different LPIPS losses. whereas the VAE trained with an appropriately reduced LPIPS loss produces smooth surface. Based on these observations, we set the LPIPS loss weight to 0.1 in our experiments."
        },
        {
            "title": "7.3 CAPTION IMPACT ON VIDEO GENERATION",
            "content": "The use of detailed caption models facilitates more precise alignment between video content and its corresponding captions, thereby enabling more effective training of Text-to-Video (T2V) models. This improved alignment during the pretraining phase contributes to reduction in motion distortion. Furthermore, because detailed captions describe actions with greater specificity and adhere strictly to temporal order, T2V models exhibit enhanced instruction-following capabilities with respect to action sequences. In addition to actions, detailed captions provide richer descriptions of visual elements, such as the appearance and quantity of subjects, which further improves the models ability to generate visuals that are more consistent with real-world scenes. Fig. 34 illustrates the advantages of our T2V model trained with more detailed captions compared to that trained with Tarsier2 (Yuan et al., 2025b) captions. Due to the lack of explicit indication regarding whether spatial relationships are described from the viewers perspective or from the perspective of the subjects within the video in publicly available Multimodal Large Language Model (MLLM) pretraining datasets, MLLMs often make errors when generating captions that reference spatial relations such as left or right. Specifically, the models may either incorrectly specify the spatial relation, omit the relevant perspective, or entirely fail to describe the spatial relationship. This ambiguity negatively impacts the alignment between the video content and the generated caption. To address this issue in subsequent optimization of the captioning model, we place particular emphasis on the accurate representation of spatial relationships. We construct comprehensive set of question-answer pairs focused on spatial relationships, with each question explicitly requiring temporal/sequential localization, subject/object localization, and action localization. This design ensures that each question refers to unique event or moment within the video. Furthermore, we increase the complexity of sub-action description task by incorporating queries spanning multiple temporal intervals and expand the volume of caption training data accordingly. The three tasks are jointed trained. In the evaluation conducted on the sports caption benchmark dataset, the proposed model demonstrated notable improvements over the Tarsier2 model across three key metrics. Specifically, the new model achieves more accurate and detailed descriptions, exhibits less action description hallucinations, and provides more precise spatial relation descriptions. The win rates (GSB) of these three indicators are 53.4%, 20.5%, and 39.2%. 28 Figure 34: Top: The results of the T2V model trained using the Tarsier2 caption model. Bottom: The outcomes of the T2V model trained with our proposed caption model. In case (a), there is noticeable distortion in the legs in the top row, which is absent in the bottom row. In case (b), the strongman repeatedly lifts and lowers the stone in the top row, whereas in the bottom row, the actions strictly follow the given instructions. In case (c), the character is depicted with three hands, and the knife held in one hand is incorrectly rendered as fork in the top row. In contrast, the bottom row accurately shows the chef holding knife in one hand and fork in the other."
        },
        {
            "title": "8 CONCLUSION AND LIMITATION",
            "content": "Conclusion. We introduce Waver, rectified flow Transformer framework that unifies multiple video generation tasks and achieves high-performance generation results. Through robust data curation pipeline and detailed optimization strategies, Waver demonstrates superior performance across public and internal benchmarks, especially in motion quality. We hope that the insights and practical recipes provided in this work will support the community in further advancing the state of video generation technology. Limitation. There also exhibits certain limitations. For instance, in high-motion scenarios, human body details such as hands and legs are prone to distortion. Additionally, the generated videos sometimes lack rich visual details, resulting in limited expressiveness. In future work, we will employ reinforcement learning (RL) techniques to mitigate these distortions and enhance visual details, thereby further improving the generation quality."
        },
        {
            "title": "9 CONTRIBUTIONS AND ACKNOWLEDGMENTS",
            "content": "Core Contributors: Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei Yi Jiang, Zehuan Yuan, Bingyue Peng Contributors: Ruibiao Lu, Prasanna Raghav, Yi Fu, Zeyu Zhang, Hui Wu, Xing Wang, Chongxi Wang, Xibin Wu, Hongxiang Hao, Heng Zhang, Yanghua Peng Acknowledgments: Fangzhou Ai, Jinlai Liu, Xugang Ye, Xiaoran Xu, Bobo Zeng, Aaron Shen, Mao Hu, Xiaoxiao Qin, Tingxuan Li, Wanxing Wang, Puke Zhang, Yufei Wu, Ruoyu Guo, Ge Bai, Dongyang Wang, Tiger Li, Shoufa Chen, Chongjian Ge, Shilong Zhang, Shukai Wang, Dingyuan Xu, Chetan Velivela, Kairong Sun, Kaihua Jiang, Junru Zheng"
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, et al. Goku: Flow based video generative foundation models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2351623527, 2025. Google DeepMind. Veo 3. https://deepmind.google/technologies/veo/veo-3/, 2025.05. PySceneDetect Developers. Pyscenedetect. URL https://www.scenedetect.com. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024a. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Int. Conf. Mach. Learn., 2024b. Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Mario Michael Krell, Matej Kosec, Sergio Perez, and Andrew Fitzgibbon. Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance. arXiv preprint arXiv:2107.02027, 2021. Kuaishou. Kling ai. https://klingai.kuaishou.com/, 2024.06. Yunshui Li, Yiyuan Ma, Shen Yan, Chaoyi Zhang, Jing Liu, Jianqiao Lu, Ziwen Xu, Mengzhao Chen, Minrui Wang, Shiyi Zhan, et al. Model merging in pre-training of large language models. arXiv preprint arXiv:2505.12082, 2025. Peng Liu, Xiaoming Ren, Fengkai Liu, Qingsong Xie, Quanlong Zheng, Yanhao Zhang, Haonan Lu, and Yujiu Yang. Dynamic-i2v: Exploring image-to-video generaion models via multimodal llm. arXiv preprint arXiv:2505.19901, 2025. Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. MiniMax. Hailuo ai. https://hailuoai.com/video, 2024.09. OpenAI. Video generation models as world simulators, 2024. URL https://openai.com/ index/video-generation-models-as-world-simulators/. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Int. Conf. Comput. Vis., pp. 41954205, 2023. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, WeiNing Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720. QwenTeam. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. arXiv preprint arXiv:2305.18290, 2024. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):54855551, 2020. Seyedmorteza Sadat, Otmar Hilliges, and Romann Weber. Eliminating oversaturation and artifacts In The Thirteenth International Conference on of high guidance scales in diffusion models. Learning Representations, 2024. Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 18741883, 2016. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 32 Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow (extended abstract). In Assoc. Adv. Artif. Intell., 2021. Jie Tian, Xiaoye Qu, Zhenyi Lu, Wei Wei, Sichen Liu, and Yu Cheng. Extrapolating and decoupling image-to-video generation models: Motion modeling is easier than you think. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1251212521, 2025. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. In Int. Conf. Learn. Represent., 2025. Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1570315712, 2025. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025a. Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, and Yuan Lin. Tarsier2: Advancing large vision-language models from detailed video description to comprehensive video understanding. arXiv preprint arXiv:2501.07888, 2025b. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1210412113, 2022. Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025a. Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhengzhong Liu, and Hao Zhang. Fast video generation with sliding tile attention. arXiv preprint arXiv:2502.04507, 2025b. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Shilong Zhang, Wenbo Li, Shoufa Chen, Chongjian Ge, Peize Sun, Yida Zhang, Yi Jiang, Zehuan Yuan, Binyue Peng, and Ping Luo. Flashvideo: Flowing fidelity to detail for efficient high-resolution video generation. arXiv preprint arXiv:2502.05179, 2025c. Zhongwei Zhang, Fuchen Long, Zhaofan Qiu, Yingwei Pan, Wu Liu, Ting Yao, and Tao Mei. Motionpro: precise motion controller for image-to-video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2795727967, 2025d. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023."
        }
    ],
    "affiliations": []
}