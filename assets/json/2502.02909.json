{
    "paper_title": "SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in LLMs",
    "authors": [
        "Dinithi Jayasuriya",
        "Sina Tayebati",
        "Davide Ettori",
        "Ranganath Krishnan",
        "Amit Ranjan Trivedi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose SPARC, a lightweight continual learning framework for large language models (LLMs) that enables efficient task adaptation through prompt tuning in a lower-dimensional space. By leveraging principal component analysis (PCA), we identify a compact subspace of the training data. Optimizing prompts in this lower-dimensional space enhances training efficiency, as it focuses updates on the most relevant features while reducing computational overhead. Furthermore, since the model's internal structure remains unaltered, the extensive knowledge gained from pretraining is fully preserved, ensuring that previously learned information is not compromised during adaptation. Our method achieves high knowledge retention in both task-incremental and domain-incremental continual learning setups while fine-tuning only 0.04% of the model's parameters. Additionally, by integrating LoRA, we enhance adaptability to computational constraints, allowing for a tradeoff between accuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate that our PCA-based prompt tuning combined with LoRA maintains full knowledge retention while improving accuracy, utilizing only 1% of the model's parameters. These results establish our approach as a scalable and resource-efficient solution for continual learning in LLMs."
        },
        {
            "title": "Start",
            "content": "SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in LLMs Dinithi Jayasuriya, Sina Tayebati, Davide Ettori, Ranganath Krishnan, Amit Ranjan Trivedi Department of Electrical and Computer Engineering, University of Illinois Chicago, Chicago, IL Email: {dkasth2, stayeb3, detto3, amitrt}@uic.edu Intel Labs, Oregon Email: ranganath.krishnan@intel.com 5 2 0 2 5 ] . [ 1 9 0 9 2 0 . 2 0 5 2 : r AbstractWe propose SPARC, lightweight continual learning framework for large language models (LLMs) that enables efficient task adaptation through prompt tuning in lowerdimensional space. By leveraging principal component analysis (PCA), we identify compact subspace of the training data. Optimizing prompts in this lower-dimensional space enhances training efficiency, as it focuses updates on the most relevant features while reducing computational overhead. Furthermore, since the models internal structure remains unaltered, the extensive knowledge gained from pretraining is fully preserved, ensuring that previously learned information is not compromised during adaptation. Our method achieves high knowledge retention in both task-incremental and domain-incremental continual learning setups while fine-tuning only 0.04% of the models parameters. Additionally, by integrating LoRA, we enhance adaptability to computational constraints, allowing for tradeoff between accuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate that our PCA-based prompt tuning combined with LoRA maintains full knowledge retention while improving accuracy, utilizing only 1% of the models parameters. These results establish our approach as scalable and resourceefficient solution for continual learning in LLMs. I. INTRODUCTION Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing, enabling transfer learning across diverse tasks and domains. Their ability to encode rich semantic representations allows them to generalize effectively and adapt to new tasks with minimal supervision. These characteristics, along with their proficiency in few-shot learning, have established LLMs as indispensable tools for applications such as text generation [1], question answering [2], summarization [3], and reasoning [4]. While pretrained LLMs excel in handling static datasets and fixed tasks, many real-world applications require dynamic adaptability to evolving environments. For instance, LLMbased autonomous navigation systems must respond to unseen driving conditions, terrains, and tasks, such as understanding new traffic laws or adapting to varying weather conditions. Retraining new model for each task is computationally prohibitive, especially for large-scale systems, and the assumption that data from earlier tasks is always available for retraining is often unrealistic due to storage limitations, regulatory constraints, or privacy concerns. These challenges necessitate methods that enable models to integrate new information incrementally while retaining the prior knowledge. However, LLMs also face critical challenges in continual learningthe ability to acquire new knowledge without overwriting previously learned information. Naive fine-tuning on new tasks often leads to catastrophic forgetting [5], where updates disrupt prior knowledge. The high computational cost of fine-tuning all parameters for each task is impractical, given that most of the models parameters already encode transferable knowledge. Thus, continual learning in LLMs requires approaches that can facilitate efficient task-specific adaptation while preserving general-purpose capabilities. Several strategies have been proposed to address these challenges: Replay-based methods [6] mitigate forgetting by replaying data from previous tasks during training. However, these methods incur significant memory overhead and may be unsuitable for sensitive domains where storing and replaying data raises privacy concerns [7]. Regularization-based approaches, such as Elastic Weight Consolidation (EWC) [8] and Synaptic Intelligence (SI) [9], attempt to preserve to knowledge by penalizing updates to parameters critical previous tasks. However, these methods are less effective in the high-dimensional parameter spaces of LLMs. Parameterefficient fine-tuning (PEFT) methods, including LoRA [10] [12], and adapter tuning [13], address scalability and efficiency by introducing lightweight task-specific modules that reduce trainable parameters. While PEFT methods are computationally efficient, they often require architectural modifications, complicating deployment and risking interference with pretrained representations. Prompt tuning has emerged as an alternative that is wellsuited for task-specific adaptation in LLMs. By introducing small, trainable embeddings (soft prompts) appended to input tuning enables task-specific learning while tokens, prompt keeping the base model frozen. This approach significantly reduces the number of trainable parameters, making it computationally efficient and scalable for large models. However, tuning methods typically train separate existing prompt prompt for each task, resulting in inefficiencies and redundancy when tasks share underlying representations. Moreover, these methods lack mechanisms to address catastrophic forgetting, limiting their applicability to continual learning scenarios where sequential task adaptation is required. To address these limitations, as in Fig. 1(a), we propose SPARC, novel framework for continual learning in LLMs that leverages subspace-guided prompt tuning. At its core, the framework represents each task as subspace in the input embedding space using principal component analysis (PCA). By quantifying subspace overlap, the framework determines whether new task can reuse an existing prompt or requires new one. This reduces computational overhead by reusing prompts for tasks with overlapping representations, thereby facilitating knowledge transfer across related tasks. For tasks with minimal overlap, new prompts are initialized in orthogonal subspaces to preserve task-specific independence and mitigate interference. This alignment prevents catastrophic forgetting by ensuring that new knowledge does not overwrite previously learned representations. key advantage of the proposed framework is its parameter efficiency. By limiting updates to soft prompts, the framework minimizes the number of trainable parameters without requiring architectural modifications to the base model, ensuring scalability to large LLMs. This distinguishes it from approaches like LoRA and adapter tuning, which require changes to the models structure and may disrupt its pretrained capabilities. The framework's architecture-agnostic design and lightweight computational requirements make it particularly suitable for real-world applications where resource efficiency and model integrity are critical. We validate the framework through extensive experiments in both domain-incremental learning, where the model adapts across datasets from distinct domains, and task-incremental learning, where it sequentially learns tasks with different objectives. Our results demonstrate that the framework effectively mitigates catastrophic forgetting while achieving strong forward transfer, enabling efficient learning of new tasks, and backward transfer, ensuring 97% prior knowledge retention. In domain-incremental learning, the framework maintains an average forgetting ratio of 3%, while in task-incremental learning, it achieves no forgetting, demonstrating its robustness in continual learning scenarios. Notably, these results are obtained while fine-tuning only 0.04% of the models parameters, significantly reducing computational overhead compared to baseline approaches. By combining prompt reusability and orthogonal subspace alignment, our method provides scalable and resource-efficient solution for continual learning in LLMs, ensuring both adaptability and knowledge retention. II. BACKGROUND A. Soft Prompts and Soft Embeddings Prompt tuning is parameter-efficient approach for adapting frozen pretrained LLMs to specific tasks without modifying their internal weights. It introduces trainable embeddings, called soft prompts, which are prepended to the input token embeddings and optimized while keeping the LLM parameters fixed [14]. These learnable vectors, with dimensions matching the models embedding space, act as lightweight adapters to guide the models behaviour toward task-specific objectives while preserving its general-purpose knowledge. These prompts are concatenated with the input token embeddings and passed through the frozen LLM. During training, only the soft prompts are updated using task-specific gradients, while the pretrained weights remain unchanged. This selective optimization significantly reduces computational overhead. For instance, to the the trainable parameters are proportional product of the number of soft prompt tokens (T ) and the embedding dimensionality (d), typically amounting to few thousand parameters for large-scale models. Soft prompts are particularly advantageous for large LLMs as they enable task-specific adaptation without requiring extensive computational resources or architectural modifications. Moreover, task-specific prompts can be stored independently and reused across tasks, making this approach ideal for multitask learning and frequent updates. However, conventional prompt tuning methods lack mechanisms to mitigate catastrophic forgetting, where updates for new tasks overwrite knowledge from earlier ones. Additionally, they often train redundant prompts for tasks with overlapping representations, limiting efficiency in continual learning scenarios. B. PCA-based Subspace Identification Principal Component Analysis (PCA) is dimensionality reduction technique that transforms high-dimensional data into smaller set of orthogonal axes, called principal components, which capture the maximum variance in the data [15]. By projecting data onto these components, PCA identifies the most significant patterns while preserving as much information as possible, making it particularly useful for analyzing and comparing embedding spaces across tasks. (cid:80)N Given dataset RN D, where is the number of samples and is the feature dimension, PCA projects into lower-dimensional subspace Rk, where < D. The process begins by centering the data as Xc = µ, where i=1 xi. Next, the covariance matrix = 1 µ = 1 Xc is computed, and eigenvalue decomposition is performed: Cwi = λiwi, where wi are eigenvectors (principal components) and λi are eigenvalues. The top eigenvectors corresponding to the largest eigenvalues are selected to form the transformation matrix RkD, ensuring orthogonality (WW = I, where is the identity matrix). These components define tasks subspace in the embedding space. The projection of the original data into this subspace is given by Xk = XcW. PCA is particularly learning as it enables tasks to be effective for continual represented in lower-dimensional subspaces that capture their dominant features. These subspaces can then be compared to assess similarity across tasks, facilitating prompt reuse for tasks with shared representations or initializing orthogonal subspaces for independent tasks. By focusing on the most informative components, PCA reduces dimensionality while maintaining the core structure of task-specific knowledge. C. Subspace Overlap using Cosine Similarity The process begins with the initialization of soft prompts, which can be random or informed by task-specific heuristics. To enable efficient knowledge transfer across tasks, subspace overlap is quantified using cosine similarity [16]. This Fig. 1: Overview of SPARC: (a) The subspace of the new dataset is computed using PCA. By measuring the cosine similarity between this new subspace and previously learned prompt subspaces, the framework determines whether similar prompt already exists. If match is found, the existing prompt is reused for initialization, enhancing knowledge retention. Otherwise, new prompt is initialized in an orthogonal subspace to the existing prompts, ensuring differentiation and efficient adaptation. (b) The prompt embeddings consist of two key components: tunable soft tokens and PCA-based transformation matrix. This design significantly reduces the number of trainable parameters compared to traditional prompt tuning methods, making the approach more efficient while preserving model adaptability. technique measures the alignment between principal components of embedding subspaces from different tasks, helping determine whether knowledge from previous task can be reused for new one. For two datasets D1 and D2, with principal components P1 = {p1 k}, the cosine similarity between components p1 P2 is computed as: 1, . . . , p2 P1 and p2 k} and P2 = {p 1, . . . , p1 independent [12]. This is particularly important for tasks with minimal overlap in their embedding subspaces, where reusing previously learned prompt may lead to performance degradation or catastrophic forgetting. Two subspaces, Pt+1 (new task) and Pi (previous task), are orthogonal if the inner product of all basis vectors between them is zero. Mathematically, this is expressed as: Sij = p1 p2 p2 p1 . (1) Pt+1, Pi = 0, = 1, 2, . . . , t. principal component p1 is considered aligned with P2 if its similarity with any p2 exceeds predefined threshold τ . This alignment is summarized as the overlap percentage, which quantifies how many components of P1 match with P2: In practice, orthogonality is enforced by projecting the new subspace Pt+1 onto the complement of the span of the previously learned subspaces {P1, P2, . . . , Pt}. This ensures: Overlap Percentage = Number of Aligned Components Total Components in P1 (2) By identifying subspace overlap, the framework determines whether prompt for new task can reuse an existing one, minimizing redundancy. For tasks with substantial overlap, the corresponding prompt can be reused with minimal fine-tuning. For tasks with minimal overlap, new prompt is initialized to capture the independent features of the new task. This enables the framework to reuse learned knowledge for related tasks while isolating representations for unique task characteristics. D. Orthogonal Subspaces for Task Separation Orthogonal subspaces are employed to prevent interference between tasks, ensuring that task-specific knowledge remains . ProjPi (u) = 0, Pt+1, Pi {P1, P2, . . . , Pt}. Orthogonal subspaces allow the framework to isolate new task-specific knowledge while preserving prior representations. By aligning new prompts with orthogonal directions in the embedding space, the model effectively mitigates catastrophic forgetting and interference [17]. This approach also ensures that existing prompts remain intact and reusable for earlier tasks. Combining subspace overlap analysis with orthogonal initialization allows the framework to balance knowledge retention and efficient adaptation. Overlapping subspaces enable prompt reuse for related tasks, while orthogonal subspaces ensure task-specific independence for novel features, making the framework robust for continual learning of LLMs. III. SPARC: SUBSPACE-AWARE PROMPT TUNING FOR CONTINUAL LEARNING similarity between the subspace of the new dataset dt+1 and those of previously encountered datasets {d1, d2, . . . , dt}."
        },
        {
            "title": "The proposed framework leverages prompt",
            "content": "tuning with subspace-guided initialization and orthogonal alignment to enable efficient and scalable continual learning of LLMs. Soft prompt embeddings are appended to the input tokens of pretrained language model, allowing task-specific adaptation without modifying the models internal weights. This section outlines the processes of prompt initialization, subspace overlap analysis, and orthogonal prompt generation to effectively balance knowledge transfer and task-specific independence. A. Prompt Initialization and Subspace Representation To adapt LLMs to new tasks, trainable soft prompts are initialized and optimized to represent the core characteristics of dataset. The initialization process relies on PCA to capture the dominant features of the datasets embedding space. These principal components are leveraged for prompt initialization for parameter efficiency and task-specific adaptation (Fig. 1(b)). 1) Embedding Principal Components via PCA: Given dataset di, its embeddings RN are generated by passing input tokens through pretrained LLM or sentence transformer, where is the number of samples, and is the embedding dimension. PCA is applied to to extract the top components forming reduced subspace Rk, where D. The transformation matrix RkD is computed using the eigenvectors of the covariance matrix: = 1 Xc, where Xc = µ, µ = 1 (cid:88) i= xi. (3) The top eigenvectors of define the principal components in W, capturing the directions of maximum variance. Embeddings are projected into the subspace as Xk = XcW, retaining the most informative features of the dataset while reducing noise and dimensionality. This subspace forms the initialization space for task-specific prompts. 2) Learnable Prompt Parameters: Soft prompts are initialized as trainable parameter matrix RT K, where is the number of soft tokens, and is the dimensionality of the principal subspace (k). Each row of corresponds to trainable embedding vector, which is prepended to the input token embeddings of di. During training, only is updated to minimize the task-specific loss, while the pretrained models weights remain frozen. This selective optimization enables efficient task adaptation with minimal computational overhead. For example, with = 10 and = 100, the number of trainable parameters is reduced to 1,000, which is orders of magnitude smaller than fine-tuning the entire LLM. By decoupling prompt updates from the model, the framework ensures modularity and scalability across tasks. B. Subspace Overlap Analysis The framework determines whether new task can reuse an existing prompt or requires new one by evaluating the 1 , . . . , pt+1 1) Quantifying Overlap Using Cosine Similarity: Let Pt+1 = {pt+1 } represent the top principal components of dt+1, and Pi the components of an existing dataset di. Cosine similarity is used to measure alignment between the principal components: Sjk = pt+1 pt+1 pi pi . (4) For each pt+1 , the maximum similarity score across all components in Pi is recorded. The overlap percentage quantifies the proportion of components in Pt+1 that align with Pi above similarity threshold τ : Overlap Percentage = Number of Aligned Components Total Components in Pt+ . (5) If the overlap percentage exceeds τ (e.g., 50%), the corresponding prompt pi is reused with minimal fine-tuning. For tasks with low overlap, new prompt is initialized to capture the unique characteristics of dt+1. C. Orthogonal Prompt Initialization For datasets with minimal subspace overlap, the framework initializes new prompt in an orthogonal subspace to ensure task-specific independence and prevent interference with previously learned prompts. Projection: 1) Orthogonal subspaces {P1, P2, . . . , Pt} of previously learned tasks, the orthonormal basis is constructed by concatenating their principal components. The embeddings of dt+1, denoted Xt+1, are projected onto to compute the aligned components: Given the ProjV(Xt+1) = Xt+1VV. The orthogonal component is derived as: Xorth = Xt+1 ProjV(Xt+1). (6) (7) PCA is then applied to Xorth to identify its principal components, forming subspace that captures the unique features of dt+1. The new prompt is initialized in this orthogonal subspace to ensure independence from existing tasks. 2) Training and Adaptation: The orthogonally initialized prompt Pt+1 is optimized to minimize the task-specific loss. Existing prompts remain frozen and accessible for inference, allowing the framework to leverage previously acquired knowledge while integrating new information. This ensures robust continual learning with strong forward and backward transfer. By combining subspace overlap analysis and orthogonal initialization, the framework balances knowledge reuse for related tasks and independence for novel features, addressing the core challenges of catastrophic forgetting and task-specific adaptation in LLMs. Fig. 2: Per-token accuracy with varying numbers of soft tokens and principal components: (a) 100 principal components, 20 soft tokens. (b) 300 principal components, 20 soft tokens. (c) 100 principal components, 40 soft tokens. (d) Accuracy with different number of PCA components IV. RESULTS A. Evaluation Setup We evaluated our framework in domain-incremental and task-incremental learning settings using GPT-2 and DeBERTabase as base models. For domain-incremental learning, we use five datasets spanning healthcare, scientific literature, and general knowledge, sequentially introducing them to simulate real-world incremental learning. Per-token accuracy measures the frameworks ability to generate outputs aligned with taskspecific prompts while retaining previously learned knowledge. For task-incremental learning, accuracy is determined by selecting the most likely class from model logits and assessing the models ability to classify tasks incrementally. B. Impact of Principal Components and Soft Tokens To analyze the effect of principal components in PCA-based subspace selection, we conducted experiments with 100 and 300 components. The results in Fig. 2(d) show that increasing components improves accuracy for domains like healthcare and scientific literature, where richer representations are beneficial. However, for general knowledge tasks, performance remains stable, indicating that fewer components suffice. This underscores the importance of selecting an optimal number of PCA components to balance accuracy and computational efficiency. We also examine the impact of soft tokens. While Fig. 3: Accuracy in Domain Incremental Learning. (a) Non-Continual learning Accuracy is calculated by finetuning the model individually on each dataset, while final accuracy is obtained by sequentially fine-tuning the model across datasets. (b) Forgetting ratio for each dataset after the completion of sequential training. increasing tokens enhances task-specific performance by introducing more trainable parameters, it slightly reduces forward transfer due to overfitting to domain-specific features. These findings highlight the trade-off between specialization and generalization in continual learning. C. Domain-Incremental Learning Performance The domain-incremental evaluation includes datasets from diverse fields. HealthcareMagic and PubMedQA [18] cover medical question-answering, requiring models to infer responses based on patient queries and biomedical literature. SciQ [19] focuses on scientific reasoning, testing the models ability to answer science-related multiple-choice questions. OceanBench [20] introduces real-world oceanographic data analysis, challenging the models adaptability to highly specialized knowledge and TriviaQA [21] evaluates general knowledge comprehension by requiring the model to answer fact-based questions across broad range of topics. This diverse dataset selection ensures rigorous assessment of the frameworks capacity to adapt across distinct domains training while preserving prior knowledge. The sequential was conducted in the order of the x-axis in Fig. 2(a). We evaluated our framework on forward and backward transfers. Forward transfer in continual learning refers to the models ability to leverage previously learned knowledge to improve performance on new tasks without additional retraining [22]. Conversely, backward transfer assesses whether learning new task enhances or preserves performance on previously encountered tasks, ensuring knowledge retention and mitigating catastrophic forgetting. The results in Fig. 2 exhibit strong forward transfer, as demonstrated by improved accuracy on unseen datasets immediately after training on new domain. For example, fine-tuning on healthcare dataset significantly enhances performance on scientific literature datasets, leveraging shared semantic structures via subspace-guided prompt initialization. Backward transfer is also robust, with minimal accuracy degradation on earlier datasets after sequential training. Fig. 3(b) shows the forgetting ratio, measured as the percentage drop in accuracy on prior datasets, consistently remains below 5%. This highlights the frameworks effectiveness in mitigating catastrophic forgetting. We compare our frameworks final accuracy after completing sequential training with the baseline accuracy obtained by fine-tuning the model individually on each dataset. Fig. 3(a) shows that our approach consistently outperforms baseline fine-tuning, demonstrating superior knowledge retention and transfer. For instance, after training on healthcare, scientific, and general knowledge datasets, final accuracy exceeds the baseline by significant margin. These results validate the efficiency of subspace-guided prompt reuse and orthogonal prompt initialization in balancing task-specific learning and generalization. D. Task-Incremental Learning Performance The framework was also evaluated on diverse NLP tasks, using five SuperGLUE benchmark datasets such as CB (CommitmentBank), RTE (Recognizing Textual Entailment), WiC (Word-in-Context), BoolQ (Boolean Question Answering), and MultiRC (Multi-Sentence Reading Comprehension) [23]. These datasets cover diverse range of NLP tasks including reasoning over short texts, contextual word sense disambiguation, answering yes/no questions based on unstructured Fig. 4: Performance Comparison of Task-Incremental Learning Methods: Accuracy results for three approaches, PCA-Based Learning, Full Finetuning, and LoRA-Integrated Prompt-Based Learning. text, and extracting relevant information from multi-sentence passages. This diversity ensures comprehensive evaluation of the frameworks ability to generalize across different linguistic challenges in continual learning setting. Sequential training is performed to assess both forward transfer (efficient adaptation to new tasks) and backward transfer (retention of previously learned knowledge). Results confirm the frameworks flexibility and robustness across varying task objectives. Fig. 4 presents evaluation results for Full Fine-Tuning, PCA-based Continual Learning, and LoRA-integrated Continual Learning. During training, datasets are introduced sequentially, with accuracy measured on both previously trained and unseen datasets after each step. The diagonal values indicate accuracy on the current dataset, while other values represent performance on past and unseen datasets. Full Fine-Tuning achieves the highest accuracy but requires substantial computational resources, fine-tuning 100% of model parameters, making it impractical for scalable continual learning. In contrast, our PCA-based method achieves competitive accuracy while finetuning only 0.04% of parameters, ensuring efficiency. Moreover, it fully retains previously learned knowledge, demonstrating robustness in continual learning. E. Low Rank Adaptation with Prompt Tuning We also evaluate hybrid method integrating PCA-based tuning with LoRA. This tests adaptability to efficient learning strategies while enhancing accuracy without sacrificing knowledge retention. By fine-tuning only 1% of parameters, the PCA + LoRA approach preserves all prior knowledge while improving accuracy, further reinforcing our frameworks efficiency. Fig. 5 presents high-level comparison of task-incremental learning results, showing that PCA-based continual learning retains prior knowledge while achieving superior accuracy compared to the zero-shot performance of the base model. [3] A. Mullick, S. Bose, R. Saha, A. K. Bhowmick, A. Vempaty, P. Goyal, N. Ganguly, P. Dey, and R. Kokku, Leveraging the power of llms: fine-tuning approach for high-quality aspect-based summarization, arXiv preprint arXiv:2408.02584, 2024. [4] J. Huang and K. C.-C. Chang, Towards reasoning in large language models: survey, arXiv preprint arXiv:2212.10403, 2022. [5] Y. Luo, Z. Yang, F. Meng, Y. Li, J. Zhou, and Y. Zhang, An empirical study of catastrophic forgetting in large language models during continual fine-tuning, arXiv preprint arXiv:2308.08747, 2023. [6] B. Bagus and A. Gepperth, An investigation of replay-based approaches learning, in 2021 International Joint Conference on for continual Neural Networks (IJCNN). IEEE, 2021, pp. 19. Fig. 5: Task Incremental Learning: Accuracy comparison of PCA-Based Continual Learning, PCA-Based Non-Continual Learning, Full Finetuning, and Zero-Shot Inference. This underscores the robustness of our method in adapting to new tasks without forgetting previously learned information. Across all experiments, our framework maintains constant training cost, regardless of task count or model size. This is achieved by limiting trainable parameters to the product of soft tokens and PCA components, typically resulting in only few thousand parameters. For instance, with ten soft tokens and 300 PCA components, trainable parameters account for less than 0.002% of GPT-2s total parameters. This highlights our frameworks computational efficiency, making it wellsuited for large-scale models and resource-constrained environments. Overall, our results demonstrate that the proposed framework effectively mitigates catastrophic forgetting while achieving strong forward and backward transfer. By combining subspace-guided prompt reuse with orthogonal prompt initialization, it ensures task-specific adaptation, knowledge retention, and scalabilityaddressing key challenges in continual learning for large language models. V. CONCLUSIONS We presented subspace-guided prompt tuning framework for efficient continual learning in large language models by leveraging PCA-based subspace identification. This approach minimizes catastrophic forgetting, facilitates knowledge transfer, and significantly reduces computational overhead. Furthermore, its seamless integration with LoRA showcases its adaptability to hybrid parameter-efficient tuning strategies, achieving improved performance with minimal number of trainable parameters. REFERENCES [1] Z. Kasner and O. Duˇsek, Beyond traditional benchmarks: Analyzing behaviors of open llms on data-to-text generation, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024, pp. 12 04512 072. [2] K. Saito, K. Sohn, C.-Y. Lee, and Y. Ushiku, Unsupervised llm adaptation for question answering, arXiv preprint arXiv:2402.12170, 2024. [7] M. De Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. Slabaugh, and T. Tuytelaars, continual learning survey: Defying forgetting in classification tasks, IEEE transactions on pattern analysis and machine intelligence, vol. 44, no. 7, pp. 33663385, 2021. [8] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska et al., Overcoming catastrophic forgetting in neural networks, Proceedings of the national academy of sciences, vol. 114, no. 13, pp. 35213526, 2017. [9] F. Zenke, B. Poole, and S. Ganguli, Continual learning through synaptic intelligence, in International conference on machine learning. PMLR, 2017, pp. 39873995. [10] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, Lora: Low-rank adaptation of large language models, arXiv preprint arXiv:2106.09685, 2021. [11] S. Dou, E. Zhou, Y. Liu, S. Gao, J. Zhao, W. Shen, Y. Zhou, Z. Xi, X. Wang, X. Fan et al., Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment, arXiv preprint arXiv:2312.09979, vol. 4, no. 7, 2023. [12] Y.-S. Liang and W.-J. Li, Inflora: Interference-free low-rank adaptation for continual learning, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 23 63823 647. [13] Q. Gao, C. Zhao, Y. Sun, T. Xi, G. Zhang, B. Ghanem, and J. Zhang, unified continual learning framework with general parameter-efficient tuning, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 11 48311 493. [14] B. Lester, R. Al-Rfou, and N. Constant, The power of scale for parameter-efficient prompt tuning, arXiv preprint arXiv:2104.08691, 2021. [15] H. Abdi and L. J. Williams, Principal component analysis, Wiley interdisciplinary reviews: computational statistics, vol. 2, no. 4, pp. 433 459, 2010. [16] N. Passalis and A. Tefas, Unsupervised knowledge transfer using similarity embeddings, IEEE transactions on neural networks and learning systems, vol. 30, no. 3, pp. 946950, 2018. [17] C.-I. Chang, Orthogonal subspace projection (osp) revisited: comprehensive study and analysis, IEEE transactions on geoscience and remote sensing, vol. 43, no. 3, pp. 502518, 2005. [18] Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, and X. Lu, Pubmedqa: dataset for biomedical research question answering, arXiv preprint arXiv:1909.06146, 2019. [19] M. G. Johannes Welbl, Nelson F. Liu, Crowdsourcing multiple choice science questions, 2017. [20] Z. Bi, N. Zhang, Y. Xue, Y. Ou, D. Ji, G. Zheng, and H. Chen, Oceangpt: large language model for ocean science tasks, arXiv preprint arXiv:2310.02031, 2023. [21] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, Triviaqa: large scale distantly supervised challenge dataset for reading comprehension, arXiv preprint arXiv:1705.03551, 2017. [22] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, Continual lifelong learning with neural networks: review, Neural networks, vol. 113, pp. 5471, 2019. [23] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman, Superglue: stickier benchmark for generalpurpose language understanding systems, Advances in neural information processing systems, vol. 32, 2019."
        }
    ],
    "affiliations": [
        "Department of Electrical and Computer Engineering, University of Illinois Chicago, Chicago, IL",
        "Intel Labs, Oregon"
    ]
}