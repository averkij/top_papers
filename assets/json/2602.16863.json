{
    "paper_title": "SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation",
    "authors": [
        "Kushal Kedia",
        "Tyler Ga Wei Lum",
        "Jeannette Bohg",
        "C. Karen Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories."
        },
        {
            "title": "Start",
            "content": "SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation Kushal Kedia1 Tyler Ga Wei Lum2 Jeannette Bohg2 C. Karen Liu2 1Cornell University 2Stanford University Equal contribution Equal advising simtoolreal.github.io 6 2 0 2 8 ] . [ 1 3 6 8 6 1 . 2 0 6 2 : r Fig. 1: SimToolReal is framework for training single general-purpose, object-centric RL policy in simulation and transferring it to real-world tool use. (Top) Zero-shot deployment on novel real tools and tasks, spanning thin markers to thick hammers. (Bottom) Tool use typically involves grasping objects from flat surfaces, reorienting them in-hand, and performing the task. AbstractThe ability to manipulate tools significantly expands the set of tasks robot can perform. Yet, tool manipulation represents challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on single object and task, we procedurally generate large variety of tool-like object primitives in simulation and train single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories. Website: simtoolreal.github.io. I. INTRODUCTION From household chores to industrial work, tool use expands robots capacity to act on its environment. However, tool use represents particularly challenging class of dexterous manipulation tasks. Success requires grasping tools lying flat on surface, reorienting them into functional poses, and maintaining control during potentially forceful interactions with the environment (see Fig. 1). Consider hammering nail: the robot must grasp the hammer by its thin handle, in-hand into striking configuration, and deliver rotate it impact without losing the grasp. Such interactions expose fundamental limitation of parallel-jaw grippers: with only two opposing contacts along single grasp axis, they provide limited resistance against externally induced torques, making grasp stability primarily rely on friction and grip force. This motivates multi-fingered dexterous hands for stable tool use. Imitation learning from teleoperated demonstrations is common paradigm for learning manipulation policies [20, 90], but teleoperation is poor fit for collecting high-quality dexterous tool-use data [13]. Human and robot hands differ in kinematics and actuation, creating human-to-robot correFig. 2: Overview of SimToolReal. (Top) Training in Simulation: We train goal-conditioned RL policy in simulation that manipulates wide variety of procedurally-generated objects to randomly sampled goal poses. (Bottom) Inference in Real: We deploy this policy zero-shot on real-world tools from DexToolBench, following tool trajectories from human videos. Sim-to-real reinforcement spondence gap [68, 7] that makes precise dexterous control unintuitive for the operator [81]. At the same time, operators receive limited to no force and tactile feedback, preventing reliable regulation of contacts during the task [53, 55, 51, 73]. learning (RL) is promising alternative that has successfully demonstrated agile, dynamic, and dexterous behaviors in the real world [3, 15]. Although effective for specific skills, extending its success to diverse tools and tasks is bottlenecked by substantial per-object simulation setup and task-specific reward engineering [56]. Consequently, existing approaches are limited to sub-problems of dexterous tool-use, such as grasping [45, 1, 18], in-hand object reorientation [14, 54, 27], or spinning objects [78, 40]. Our goal is to learn dexterous manipulation policy that can be applied zero-shot to novel tools and tasks at test-time. Our key insight is to view dexterous tool use from an object-centric lens, framing task as moving tool through sequence of goal poses. Thus, we focus on training single goalconditioned policy that manipulates diverse objects to random goal poses. In simulation, we procedurally generate large set of primitive objects and train one unified goal-conditioned policy. At test time, we compose this policy for zero-shot tool use by sequentially conditioning it on goal pose trajectory extracted from human video [21, 46]. This provides dense guidance from the initial grasp, through in-hand reorientation into functional configuration, and into the ensuing tool-use motion. Our approach avoids per-task reward shaping while allowing generalization to novel tools and trajectories at testtime. Intuitively, mastering the ability of manipulating objects to any random pose induces the core skills required for tool use: establishing an initial grasp, in-hand object reorientation, and maintaining stable contact. We instantiate this approach in SimToolReal, framework for training single general-purpose RL policy in simulation and deploying it for real-world tool use  (Fig. 2)  . To enable generalization to novel objects, the policy is conditioned on the tools current 6D pose and coarse 3D bounding box over its graspable region (e.g., the handle of hammer). This abstraction enables zero-shot transfer to the real-world by effectively bypassing the sim-to-real visual gap. At deployment, we recover this representation on real tools using perception pipeline based on vision foundation models, combining SAM 3D [16] for segmentation and mesh extraction with FoundationPose [80] for 6D pose tracking. To evaluate generalization, we introduce DexToolBench, benchmark of daily tool-use behaviors, both in simulation and in the real-world. Each task is paired with human video demonstration, and the robot is evaluated on its ability to follow the demonstrated tool trajectory. Notably, SimToolReal is trained exclusively on procedurally-generated primitives with randomly sampled goal pose trajectories. Yet, it transfers zero-shot to the trajectory following tasks in DexToolBench and matches or outperforms specialist policies trained on single object instance and task. Our contributions: 1) We propose SimToolReal, unified RL framework that frames dexterous tool use as manipulating tool through sequence of desired poses. Training single policy under this objective induces dexterous skills, such as stable grasping and in-hand object reorientation. This policy is used zero-shot for tool-use without task-specific engineering. 2) We develop an object-centric perception pipeline that estimates the tools 6D pose and coarse 3D grasp bounding box using vision foundation models, enabling zero-shot sim-to-real deployment of the RL policy. 3) We introduce DexToolBench, benchmark of dexterous tool-use tasks. We show that SimToolReal achieves strong zero-shot performance on this benchmark over 120 realworld rollouts spanning 24 tasks, 12 object instances, and 6 tool categories. We outperform prior methods using fixed grasps and motion retargeting by 37% in task progress while matching the performance of specialist policies. II. RELATED WORK Imitation Learning from Teleoperated Data. Imitation learning (IL) is the predominant approach to train robot manipulation policies [88, 41, 19]. However, IL relies on highquality demonstrations, typically collected via human teleoperation. Many systems have been proposed to teleoperate multifingered dexterous hands via motion capture gloves [67, 77], VR devices [22, 30, 42, 8], or even direct camera input [63, 26, 72]. However, directly mapping hand motion to robot actions can lead to imprecise, unintuitive control [13, 68, 7]. Further, operators receive limited haptic and tactile feedback during teleoperation, which limits the collected demonstrations to simple manipulation tasks [53, 55, 51, 73]. Recent works propose wearable exoskeleton systems that physically couple the robot hand to the operator during data collection [74, 82, 24]. Still, these bulky exoskeleton systems carry considerable burden on the operator, constrain the range of human motion, and fail to demonstrate complex skills, such as in-hand reorientations. Another line of work treats the problem of teleoperation as shared autonomy. These approaches learn structured action-spaces from human hand motion data [52] or RL in simulation [28, 44], which are used by the human operator to control the robot. Recently, DexterityGen [86] trains RL policies conditioned on target grasp poses across diverse objects. However, unlike our approach, it trains object-specific policies and distills them into model that only conditions on hand-motion inputs. Largely, shared-autonomy methods are object-blind at execution time and rely on skilled operators to bridge the gap between perception and control. In contrast to all these approaches, we focus on autonomous manipulation policies without any human teleoperation effort. Learning Dexterity from Human Video Demonstrations. Learning dexterous manipulation directly from human video demonstrations is promising alternative to teleoperation. Kinematic motion retargeting pipelines can translate human hand motions into robot actions by reconstructing handobject interaction [38, 35]. More recently, these pipelines have been extended to convert demonstrations synthesized by videogeneration models into robot-executable trajectories [57, 37, 12]. However, because these methods primarily rely on kinematic hand-motion references, they struggle to produce actions that reliably grasp and maintain contact with tools. To address this limitation, another line of work performs functional retargeting, focusing on matching the demonstrated task outcome rather than precisely matching human hand motion. For instance, handobject 3D motion-capture can be used to train dexterous RL policies in simulation that reproduce the object motion while discovering robot-specific contact strategies [49, 17, 39]. More recently, functional retargeting has been extended to RGB-D human video demonstrations [46, 21] and even AI-generated videos [50]. Despite their success, these approaches typically require time-intensive RL training where the resulting policy is limited to the demonstrated manipulation task and object. In contrast, our approach can imitate object motion directly from single RGB-D human demonstration without any additional demonstration-specific training. We can use the same policy across diverse tasks. Sim-to-Real Learning. Policies learned in simulation can acquire dexterous manipulation behaviors without relying on human demonstrations, but transferring these skills to the real world often requires substantial environment shaping [56]. This includes closing the sim-to-real gap, building accurate task-specific object models, and carefully designing and tuning rewards for each behavior. Domain randomization over physical and visual parameters [27, 3, 6] and adaptation methods that infer real-world dynamics online [34, 62, 75] can reduce the sim-to-real gap significantly. Still, sim-to-real dexterous manipulation requires per-task engineering, which has led many systems to focus on narrower skill subsets such as grasping [1, 84, 45, 70, 69], in-hand object reorientation [14, 44], or dynamic object spinning [78, 40]. Recent research in locomotion has begun to unify RL policies across embodiments by leveraging large-scale simulation with procedurally generated robots and unified reward functions [43, 2]. Inspired by this direction, we train single RL policy across procedurally generated object primitives and the universal reward function of reaching random goal poses. We show that this formulation induces highly dexterous skills (e.g., in-hand reorientation and dynamic spinning) and transfers to diverse real-world tools without task-specific engineering effort. III. SIMTOOLREAL We propose SimToolReal, sim-to-real RL framework for zero-shot, dexterous tool manipulation that generalizes to unseen tools and manipulation sequences. Our key insight is to view dexterous tool-use tasks through an object-centric lens: wide range of tasks can be specified as manipulating tool through sequence of goal poses. This reduces the tool-use problem to learning single goal-reaching RL policy in simulation that manipulates procedurally generated tool primitives toward random goal poses. This training objective induces dexterous behaviors, such as stable grasping and inhand reorientation, that are essential for tool use. At test time, the policy takes as input sequence of object goal poses (e.g., from human video) and executes actions to track this sequence, enabling diverse zero-shot tool-use without any realworld object modeling in simulation or task-specific training. A. Problem Formulation We consider the problem of training dexterous policies that can manipulate any tool to arbitrary goal poses. Let ot SE(3) denote the current object pose at time t, st denote robot proprioception, and ϕ denote coarse object descriptor (e.g., approximate geometry or physical properties). Given goal pose SE(3), we learn policy at = πθ(st, ot, ϕ, g) that outputs joint position targets for the robots arm and hand. Task execution then reduces to repeatedly reaching the current goal pose and advancing to the next goal. This abstraction lets single goal-reaching controller support diverse tool-use tasks, without task-specific policy training or reward design. Fig. 3: Real-World Deployment. (Left) Human Video Processing: We collect an RGB-D human video and process it using vision foundation models. We use SAM 3D [16] to generate metric-scale object mesh and segment 3D grasp bounding box. Then, we use FoundationPose [80] to extract sequence of 6D goal poses. (Right) Inference-Time Pipeline: Our LSTM policy takes in proprioception, object pose, grasp bounding box, and goal pose, and it outputs joint position targets for the 29-DoF dexterous robot (arm + hand). To perform tool-use task, we assume access to sequence of goal object poses {gk}K k=1 with gk SE(3), extracted from human demonstration video. For example, in Fig. 2, we visualize the goal sequence of reorienting the hammer, aligning it above the nail, and swinging. During execution, we condition the policy on the current goal pose gk and advance to the next goal gk+1 when the object pose ot is sufficiently close, i.e., when d(ot, gk) < ϵ. Here, is pose distance function and ϵ is the success threshold. In the following, we describe how we train our goalconditioned RL policy in simulation, and the sim-to-real pipeline used for real-world deployment. B. Training General-Purpose Goal-Reaching Policy Environment Setup. We design general environment to model tool-use tasks. At the start of each episode, we place randomly selected object on the table in random pose and initialize the robot in randomized joint configuration. We then sample sequence of goal object poses {gk}K k=1. The robot must first learn to grasp the object from the flat table surface and subsequently manipulate it to reach each goal. If the object is dropped, the episode terminates and is reset. We sample the first goal randomly in the robots reachable workspace to expose the robot to wide range of object poses and large reorientations. Subsequent goals are sampled close to the previous goal to encourage smooth, trajectory-like motion. Reward Function. Our primary objective is to train goalpose reaching RL policy. Practically, we optimize [59]: = rsmooth + rgrasp + Igraspedrgoal. (1) rsmooth is regularization term that encourages smooth actions, rgrasp is shaping term that encourages grasping the object at the start of the episode, and Igrasped indicates whether the object has been grasped. After the object is grasped, rgoal is the dominant reward term, driving goal-pose reaching. We define: rgoal = max(d d(ot, g), 0) + Bsucc I[d(ot, g) < ϵ] , (2) which consists of (i) dense term that minimizes the distance d(ot, g) to the goal, and (ii) large, sparse success bonus Bsucc. is stateful variable that tracks the minimum distance achieved so far, which is reset when new goal is sampled. Positive reward is only provided for progress towards the goal, preventing the agent from exploiting the reward function by maintaining static proximity. Whenever the goal pose is reached (i.e., d(ot, g) < ϵ), the agent receives the success bonus and new goal is sampled. Following [4, 59], we define the distance to the goal pose as d(ot, g) = maxi ot,i gi, where we represent each pose using = 4 keypoints in its local frame. ot,i and gi denote the positions of the ith keypoint of the current and goal poses, respectively. See Appendix for more details about the reward terms. Procedural Generation of Tools in Simulation. We generate large diversity of tool-like primitive objects in simulation such that they span the variation seen in real-world tools. Each tool primitive is generated as combination of handle and head: we sample their geometries as cylinders and cuboids with varying dimensions. While simple, this design captures the key structure of many everyday tools, such as brushes, spatulas, markers, and hammers. random subset of our generated primitives is visualized in Fig 2. In addition to geometric variation, we randomize mass distribution by assigning different densities to the handle and head, representative of many real-world tools (e.g., hammer heads are typically much denser than handles). Additional generation details are provided in Appendix B. training, enabling zero-shot generalization to novel objects using demonstrations collected entirely after training. Object-Centric Policy Inputs. To deploy single policy across diverse real-world tools, we need an object representation that makes learning efficient while also being feasible to extract in the real-world. Real-world tools are diverse in their geometry and physical properties, and accurately estimating detailed geometric or physical parameters from visual sensing alone is impractical. Instead, we provide the policy only the object inputs that are reliably available at deployment: the current 6D tool pose and coarse 3D grasp-region bounding box. The bounding box encodes the intended graspable region (center + extents in the object frame; Fig. 3) and is held fixed during an episode. We use an LSTM backbone so the policy can integrate interaction history and implicitly infer latent physical and geometric properties that are not directly observed. This design follows the spirit of prior work such as DexFunc [1] and RMA [62], which demonstrate dexterous manipulation behaviors with limited object perception to reduce the sim-to-real observation gap. RL Training Details. We highlight key RL design choices essential for policy learning and sim-to-real transfer. a) SAPG [71]: We train our policy using SAPG, variant of PPO [66]. We find that standard PPO encounters exploration bottlenecks in massively parallel simulations. SAPG mitigates this by maintaining population of policies to promote exploration diversity and updating the leader policy using their collective experience. b) Domain Randomization: We apply targeted domain randomization during training to aid sim-to-real transfer. This includes observation delays, action-execution latency, noise and delay in object-pose estimates, and perturbations to the grasp-region bounding box. We additionally apply random force and torque perturbations to the object, encouraging the policy to learn strong, stable grasps. Randomization ranges and other details are provided in Appendix D. c) Asymmetric Critic [60]: While the actor receives only the minimal object representation available at test time, we train with an asymmetric actorcritic setup in which the critic has access to privileged states in simulation. We also provide the critic with clean (noise-free, undelayed) observations to improve value estimation and stabilize training. Additional details are included in Appendix D. C. Real-World Deployment In this section, we describe how we deploy our objectcentric policy in the real world to perform wide range of tool-use tasks. Fig. 3 provides an overview. Given thirdperson RGB-D view of human demonstration, we extract (i) 3D metric-scale object mesh and grasp region bounding box, and (ii) sequence of target object poses that serves as the goal trajectory. During execution, we run the policy in closed loop to reach the goal poses sequentially. In contrast to Lum et al. [46], our approach does not require trajectory during access to the real-world object or target Human Video Processing. From the first RGB-D frame, we reconstruct metric-scale 3D mesh of the object using SAM 3D [16]. We then segment the intended grasp region using SAM 2 [65], and convert this region into coarse 3D bounding box that is provided as part of the policy input. Finally, we run FoundationPose [80] on the RGB-D video conditioned on the extracted mesh to obtain the 6D object-pose trajectory. We process this trajectory by downsampling to 3 Hz. We use this processed trajectory as the goal sequence at test time. See Appendix for implementation details. Inference-time Object Tracking. During inference, we estimate the current 6D object pose with an update frequency of 30 Hz. For live pose tracking, we use FoundationPose [80] with RGB-D observations from third-person camera view combined with the 3D mesh extracted from the human video. At each control step, the policy is conditioned on proprioception, the current object pose, the (fixed) grasp-region bounding box, and the current goal pose from the demonstration trajectory. We initialize the goal as the first pose in the sequence, switching it to the next goal whenever the pose error d(ot, g) falls within tolerance ϵ. The episode ends when the final goal is reached or upon failure (e.g., loss of grasp or tracking). D. DexToolBench Existing robot manipulation benchmarks primarily focus on robots with parallel-jaw grippers [47, 31, 9, 61, 32, 23, 36] or dexterous grasping [84, 79, 89, 83, 76]. We introduce DexToolBench: real-world, dexterous manipulation benchmark of challenging tool-use tasks paired with digital-twin simulation environments. These tasks require grasping tools from flat surfaces, in-hand rotations to functional configurations, and environment interactions. This benchmark consists of 24 daily tool-use tasks, 12 unique object instances across 6 categories. Each task is defined by an RGB-D human video, captured via ZED 1 stereo camera. We process each video with our perception pipeline  (Fig. 3)  to extract 3D object mesh and 6D object pose trajectory. To facilitate reproducibility, we provide the raw RGB-D videos, processed data, simulation environments, and purchase links for the real-world tools. Each tool instance is visualized in Fig 4. We briefly describe the skills required in each tool-use category in the benchmark: a) Hammer: Grasp, 90 in-hand rotation, swing down or side. b) Marker: Grasp thin object, write on whiteboard. c) Eraser: Grasp, wipe marker ink on whiteboard. d) Brush: Grasp, 90 in-hand rotation, sweep forward or right. e) Spatula: Grasp, serve or 180 in-hand rotation and flip. f) Screwdriver: Grasp, 90 in-hand rotation, spin in free space along vertical or horizontal axis. See Appendix for more details about DexToolBench. IV. EXPERIMENTS Experimental Setup: We evaluate SimToolReal on robot consisting of 22-DoF Sharpa five-fingered left hand mounted on 7-DoF KUKA iiwa 14 arm. We evaluate our policies Fig. 4: Generalization to Unseen DexToolBench Tools and Tasks in the Real World. We evaluate our policy in the real world on unseen tool-use tasks in DexToolBench. Our evaluations span 24 unique task trajectories across 6 different object categories and 12 object instances. Each bar corresponds to 1 task trajectory on 1 object instance. We report the average Task Progress across 5 rollouts. Despite not being trained on these objects or trajectories, our policy demonstrates strong generalization to diverse tools of varying masses and geometries. on DexToolBench (Sec.III-D). For each task, we evaluate Task Progress, measuring the percentage of demonstrated goal poses tracked successfully. We focus on trajectory following of fixed goal sequence rather than evaluating functional task completion. We consider goal pose to be reached if the distance between the object pose and goal pose d(ot, g) is below our defined success tolerance ϵ = 2cm. We design experiments to answer the following questions: A. What is the zero-shot transfer performance of SimToolReal on unseen objects and trajectories in the real world? B. How does SimToolReal compare to prior methods that use kinematic motion retargeting or fixed-grasps? C. How does SimToolReal compare to task-specific specialists trained on single object and trajectory? D. To what extent does our training objective correlate with downstream performance on unseen tools and tasks? E. Which RL design decisions are important for maximizing training performance? A. Zero-Shot Real-World Tool-Use Setup. We evaluate whether our single policy trained in simulation transfers zero-shot to real-world tool use, without any objector task-specific fine-tuning. As shown in Fig. 4, our evaluation spans 24 task trajectories across 6 tool categories and 12 object instances in DexToolBench (Sec. III-D), all of which were unseen during training. We report average Task Progress over 5 trials, totaling 120 real-world rollouts. Results. Overall, the policy demonstrates strong zero-shot generalization across tools with diverse masses and geometries. We observe the highest Task Progress on eraser trajectories, which rely primarily on translation motion rather than in-hand object rotation. Similarly, marker trajectories dont require in-hand rotations, but because the tool is much thinner, the grasps are less reliable. Further, the markers small size makes it prone to pose tracking loss during occlusion. The four categories require rotation to reach functional (hammer, brush, remaining spatula, screwdriver) substantial in-hand object tool configurations. While the policy still achieves strong Task Progress across these tasks, performance degrades on thinner tools (e.g., performs better on the 3cm thick spoon spatula than the 1cm thick flat spatula) and heavier tools (e.g., performs better on the 36g claw hammer than the 331g mallet hammer). We also note that tool geometry the red brushs impacts task difficulty. For sideways head makes sweep forward easier, but the blue brushs frontal head makes sweep right easier. The screwdriver is the most challenging category, as the task requires both functional reorientation and continuous spinning. instance, Failure Analysis. The most common failure mode was pose tracking loss (43.7% of failures), followed by object drops (34.5%), failure to reach the goal pose due to incomplete in-hand rotation (18.2%), and grasp failure (3.6%). See Appendix for additional quantitative and qualitative analysis of our results. Notably, we observe strong recovery behavior when the policy makes mistakes. For example, when the object is dropped, the policy consistently attempts to re-grasp the object, provided the object remains within the workspace and pose tracking is not lost. B. Comparisons to Retargeting and Fixed Grasp Baselines Setup. In this section, we compare SimToolReal with: (i) Kinematic Retargeting: Following prior works [38, 46, 25, 64, 58], we retarget hand motion from human video into Fig. 5: Comparison against Baselines in the Real World. We compare SimToolReal against baselines on two variations of sweeping table with brush: with and without requiring tool rotation based on the initial states shown on the left. Average Task Progress is indicated in parentheses. SimToolReal succeeds on both variations, performing dexterous in-hand tool rotations in the harder variation. Fixed Grasp succeeds on the simpler variation of this task without tool rotation. However, when rotation is required, enforcing fixed grasp causes the arm to collide with the table while tracking the target trajectory. Kinematic Retargeting fails to reason about contact forces, and is unable to grasp the brush in both variations. dexterous robot actions. From an RGB-D video, we estimate 3D hand finger positions and then solve for arm and hand joint positions using an Inverse Kinematics (IK) solver. Although this method bypasses the need for RL, the resulting motion is purely kinematic and fails to account for contact interactions. See Appendix for implementation details. (ii) Fixed Grasp: Recent works [1, 57, 29, 91, 5] frame manipulation tasks as establishing grasp, then subsequently fixing the grasp while moving the object. To perform any object rotations, the robot must strictly rely on arm motion. This baseline isolates the necessity of in-hand manipulation; failure here demonstrates that the arm lacks the kinematic workspace to rotate tools arbitrarily without the assistance of in-hand rotation. See Appendix for implementation details. To systematically evaluate the baselines, we set up two variations of sweep forward with the red brush in the real-world (see Fig. 5): (i) Variation #1 starts with the brush top-down and does not require tool rotation, and (ii) Variation #2 starts with the brush sideways, which necessitates 90 tool rotation. We evaluate average Task Progress across 5 rollouts. Results. Kinematic Retargeting fails to grasp the brush in both variations as kinematic motion alone does not establish stable contacts. Fixed Grasp succeeds on Variation #1 as tool rotation is not required and arm motion alone is sufficient it has lower Task Progress than to finish the task. Still, SimToolReal since it is an open-loop method and cannot react to small errors. On Variation #2, enforcing fixed grasp causes the robot arm to collide with the table while tracking the target trajectory, as the optimizer is unable to find an arm motion trajectory that satisfies the tool rotation while avoiding table collision. In contrast, SimToolReal performs efficient in-hand object rotations to complete the task. Fig. 6: Comparison against Specialists. We compare SimToolReal in simulation against specialist policies trained on single object (Obj A) and trajectory (Traj A). We train one specialist policy for each of the 6 object categories in DexToolBench and report the average Task Progress across these categories. While the specialists succeed on their training setup (Obj / Traj A), performance degrades under deviation in the trajectory (Obj / Traj B) or the object (Obj / Traj A). SimToolReal has high zero-shot performance across all variants, despite not being trained on these objects or trajectories. C. Comparisons to Specialist Baseline Setup. In simulation, we compare SimToolReal against taskspecific specialist RL baselines trained on single tool and task. Following [46], we train 6 specialist policies, one for each of the 6 tool-use categories in DexToolBench. Each specialist policy trains on single object instance (Obj A) and task trajectory (Traj A). We evaluate both methods in simulation on (i) the training setup for the specialist: (Obj / Traj A), (ii) the training object with novel trajectory (Obj / Traj B), and (iii) novel object instance with the same trajectory (Obj / Traj A). For each variation, we perform 10 rollouts for each of the 6 specialist policies, reporting the average Task Progress across all trials. Results. Fig. 6 summarizes these results. On the setup used for training the specialist (Obj / Traj A), SimToolReal matches the specialist without training on that object or trajectory. However, the specialists performance drops significantly under any variation from the training setup. When the object stays the same, with only change in trajectory (Obj / Traj B), the specialist can only track the first few goals that lift the object. The performance is lowest when the object instance is changed (e.g., from red brush to blue brush) even if the trajectory remains the same (Obj / Traj A). This indicates that the specialist overfits to the training conditions. In contrast, SimToolReal shows strong zero-shot Task Progress across both object and trajectory variations. D. Training Objective Predicts Generalization Performance Setup. We study how improving on our training objective: reaching random goal poses across procedurally generated primitive objects, relates to performance on our test objective: executing human-demonstrated tool-use trajectories in DexToolBench. Fig. 7 reports (i) training rewards on the goalpose reaching objective on primitive objects and (ii) average Task Progress on DexToolBench trajectory following tasks in simulation, plotted against the number of environment steps. Fig. 7: Training Objective Drives Generalization. In simulation, we evaluate (Left) the episode reward on procedurallygenerated objects and (Right) the zero-shot Task Progress on unseen DexToolBench tools on different policy checkpoints throughout training. The strong correlation between the two curves validates our core hypothesis: improving random goalpose reaching performance on diverse object primitives drives corresponding gains in generalization to unseen tool-use tasks. Results. As training proceeds, the policys reward on the goal-pose reaching objective increases steadily. In parallel, the policys average Task Progress on DexToolBench trajectories also improves consistently. The correlation between these curves indicates that optimizing our training objective is reliable predictor of downstream generalization to tool-use behaviors, despite the mismatch between random goal-pose reaching on primitive objects during training and trajectory following on real tool objects at test time. This strong correlation suggests that our randomized primitive training effectively covers the space of skills required for real-world tool use. E. Ablations on RL Training Setup. We investigate which RL design decisions are critical for maximizing training performance. We focus on two key components of our RL pipeline: the choice of optimization algorithm (SAPG [71] vs. PPO [66]) and the use of privileged information (Asymmetric Critic [60] vs. standard critic). We train these variants with identical hyperparameters and environment settings, averaging results across 5 random seeds. Fig. 8: Ablation of RL Training Components. We compare the training reward across environment steps of SimToolReal against ablations averaged across 5 seeds. Replacing SAPG [71] with PPO [66] or not using Asymmetric Critic [60] results in significant performance drop, highlighting their importance. Results. Fig. 8 compares the training reward curves of our full method against these ablations. Importance of SAPG. We observe that replacing SAPG [71] with PPO [66] leads to significant drop in performance. While PPO suffers from exploration saturation at scale, SAPG mitigates this by training separate policies across environment chunks to increase data diversity, and then fusing gradients via importance sampling. Our results confirm that this is essential for learning complex dexterous tool-use. Importance of Asymmetric Critic. We observe that removing the Asymmetric Critic [60] and forcing the critic to rely solely on the same partial observations as the actor severely hinders learning. By accessing privileged simulation states, the critic learns more accurate value function to guide policy learning. This guidance is essential for overcoming the tasks partial observability. V. DISCUSSION AND LIMITATIONS We introduced SimToolReal, an RL framework for zero-shot dexterous tool manipulation that generalizes to unseen tool-use tasks. Our key insight is an object-centric reduction: tool-use can be specified as manipulating tool through sequence of goal poses. This shifts the problem from per-task reward design to universal objective of training goal-pose reaching policies. Training with this objective over procedurally generated primitives induces dexterous skills, such as stable grasping and in-hand reorientation, critical for tool-use. Limitations. While our approach can track tool-use goal sequences, it does not guarantee functional task completion, especially for high-force interactions. Conditioning on object pose goals alone is environment-blind, which can lead to collisions in cluttered scenes. We also currently assume tools are rigid. Pose alone can be insufficient to describe the state of non-rigid tools (e.g., scissors). Finally, our high-level goal sequence is fixed and is not replanned dynamically."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "This work is supported by Stanford Human-Centered Artificial Intelligence (HAI), ONR Young Investigator Award, the National Science Foundation (NSF) under Grant Numbers 2153854, 2327974, 2312956, 2327973, and 2342246, and the Natural Sciences and Engineering Research Council of Canada (NSERC) under Award Number 526541680. We thank Sharpa for the donation of the Sharpa hand and for the technical support provided by their team, specifically Kaifeng Zhang, Wenjie Mei, Yi Zhou, Yunfang Yang, Jie Yin, and Jason Lee. REFERENCES [1] Ananye Agarwal, Shagun Uppal, Kenneth Shaw, and Deepak Pathak. Dexterous functional grasping, 2023. URL https://arxiv.org/abs/2312.02975. [2] Bo Ai, Liu Dai, Nico Bohlinger, Dichen Li, Tongzhou Mu, Zhanxin Wu, Fay, Henrik Christensen, Jan Peters, and Hao Su. Towards embodiment scaling laws in robot locomotion. arXiv preprint arXiv:2505.05753, 2025. [3] Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubiks cube with robot hand. arXiv preprint arXiv:1910.07113, 2019. [4] Arthur Allshire, Mayank MittaI, Varun Lodaya, Viktor Makoviychuk, Denys Makoviichuk, Felix Widmaier, Manuel Wuthrich, Stefan Bauer, Ankur Handa, and Animesh Garg. Transferring dexterous manipulation from gpu simulation to remote real-world trifinger. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1180211809. IEEE, 2022. [5] Sai Haneesh Allu, Jishnu Jaykumar P, Ninad Khargonkar, Tyler Summers, Jian Yao, and Yu Xiang. Hrt1: One-shot human-to-robot trajectory transfer for mobile manipulation, 2025. URL https://arxiv.org/abs/2510.21026. [6] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39 (1):320, 2020. [7] Miguel Arduengo, Ana Arduengo, Adri`a Colome, Joan Human to robot Lobo-Prat, and Carme Torras. In 2020 IEEE-RAS whole-body motion transfer. 20th International Conference on Humanoid Robots (Humanoids), pages 299305, 2021. HUMANOIDS47582.2021.9555769. doi: 10.1109/ [8] Sridhar Pandian Arunachalam, Irmak Guzey, Soumith Chintala, and Lerrel Pinto. Holo-dex: Teaching dexarXiv preprint terity with immersive mixed reality. arXiv:2210.06463, 2022. [9] Pranav Atreya, Karl Pertsch, Tony Lee, Moo Jin Kim, Arhan Jain, Artur Kuramshin, Clemens Eppner, Cyrus Neary, Edward Hu, Fabio Ramos, et al. Roboarena: Distributed real-world evaluation of generalist robot policies. In Proceedings of the Conference on Robot Learning (CoRL 2025), 2025. [11] Samuel Buss. [10] Genesis Authors. Genesis: generative and universal physics engine for robotics and beyond, December 2024. URL https://github.com/Genesis-Embodied-AI/Genesis. Introduction to inverse kinematics with jacobian transpose, pseudoinverse and damped least squares methods. Unpublished manuscript, UniAvailversity of California, San Diego, 2004. able at https://www.math.ucsd.edu/sbuss/ResearchWeb/ ikmethods/iksurvey.pdf. [12] Boyuan Chen, Tianyuan Zhang, Haoran Geng, Kiwhan Song, Caiyi Zhang, Peihao Li, William Freeman, Jitendra Malik, Pieter Abbeel, Russ Tedrake, et al. Large video planner enables generalizable robot control. arXiv preprint arXiv:2512.15840, 2025. [13] Claire Chen, Zhongchun Yu, Hojung Choi, Mark Cutkosky, and Jeannette Bohg. Dexforce: Extracting force-informed actions from kinesthetic demonstrations for dexterous manipulation. IEEE Robotics and Automation Letters, 10(6):64166423, 2025. [14] Tao Chen, Megha Tippur, Siyang Wu, Vikash Kumar, Edward Adelson, and Pulkit Agrawal. Visual dexterity: In-hand reorientation of novel and complex object Science Robotics, 8(84), 2023. shapes. ISSN 2470doi: 10.1126/scirobotics.adc9244. URL http: 9476. //dx.doi.org/10.1126/scirobotics.adc9244. [15] Tao Chen, Eric Cousineau, Naveen Kuppuswamy, and Pulkit Agrawal. Vegetable peeling: case study in constrained dexterous manipulation. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 45424550, 2025. doi: 10.1109/ICRA55743.2025. 11127224. [16] Xingyu Chen, Fu-Jen Chu, Pierre Gleize, Kevin Liang, Alexander Sax, Hao Tang, Weiyao Wang, Michelle Guo, Thibaut Hardin, Xiang Li, et al. Sam 3d: 3dfy anything in images. arXiv preprint arXiv:2511.16624, 2025. [17] Yuanpei Chen, Chen Wang, Yaodong Yang, and Karen Liu. Object-centric dexterous manipulation from human motion data. In 8th Annual Conference on Robot Learning, 2024. [18] Zeyuan Chen, Qiyang Yan, Yuanpei Chen, Tianhao Wu, Jiyao Zhang, Zihan Ding, Jinzhou Li, Yaodong Yang, and Hao Dong. Clutterdexgrasp: sim-to-real system for general dexterous grasping in cluttered scenes. arXiv preprint arXiv:2506.14317, 2025. [19] Baiye Cheng, Tianhai Liang, Suning Huang, Maanping Shao, Feihong Zhang, Botian Xu, Zhengrong Xue, and Huazhe Xu. Moe-dp: An moe-enhanced diffusion policy for robust long-horizon robotic manipulation with skill arXiv preprint decomposition and failure recovery. arXiv:2511.05007, 2025. [20] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Robotics: Science and Systems, 2023. [21] Prithwish Dan, Kushal Kedia, Angela Chao, Edward Weiyi Duan, Maximus Adrian Pace, WeiChiu Ma, and Sanjiban Choudhury. X-sim: CrossarXiv embodiment preprint arXiv:2505.07096, 2025. learning via real-to-sim-to-real. [22] Runyu Ding, Yuzhe Qin, Jiyue Zhu, Chengzhe Jia, Shiqi Yang, Ruihan Yang, Xiaojuan Qi, and Xiaolong Wang. Bunny-visionpro: Real-time bimanual dexterous teleoperation for imitation learning. 2024. [23] Hao-Shu Fang, Chenxi Wang, Minghao Gou, and Cewu Lu. Graspnet-1billion: large-scale benchmark for general object grasping. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1144111450, 2020. doi: 10.1109/CVPR42600. 2020.01146. [24] Hao-Shu Fang, Branden Romero, Yichen Xie, Arthur Hu, Bo-Ruei Huang, Juan Alvarez, Matthew Kim, Gabriel Margolis, Kavya Anbarasu, Masayoshi Tomizuka, Edward Adelson, and Pulkit Agrawal. Dexop: device for robotic transfer of dexterous human manipulation. arXiv preprint arXiv:2509.04441, 2025. [25] Irmak Guzey, Yinlong Dai, Georgy Savva, Raunaq Bhirangi, and Lerrel Pinto. Bridging the human to robot dexterity gap through object-oriented rewards, 2024. URL https://arxiv.org/abs/2410.23289. [26] Ankur Handa, Karl Van Wyk, Wei Yang, Jacky Liang, Yu-Wei Chao, Qian Wan, Stan Birchfield, Nathan Ratliff, and Dieter Fox. Dexpilot: Vision based teleoperation of dexterous robotic hand-arm system, 2019. URL https: //arxiv.org/abs/1910.03135. [27] Ankur Handa, Arthur Allshire, Viktor Makoviychuk, Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sundaralingam, and Yashraj Narang. Dextreme: Transfer of agile in-hand manipulation from simulation to reality. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 59775984, 2023. doi: 10.1109/ICRA48891.2023.10160216. [28] Elvis Hsieh, Wen-Han Hsieh, Yen-Jen Wang, Toru Lin, Jitendra Malik, Koushil Sreenath, and Haozhi Qi. Learning dexterous manipulation skills from imperfect simulations. arXiv:2512.02011, 2025. [29] Cheng-Chun Hsu, Bowen Wen, Jie Xu, Yashraj Narang, Xiaolong Wang, Yuke Zhu, Joydeep Biswas, and Stan Birchfield. Spot: Se(3) pose trajectory diffusion for object-centric manipulation, 2025. URL https://arxiv.org/ abs/2411.00965. [30] Aadhithya Iyer, Zhuoran Peng, Yinlong Dai, Irmak Guzey, Siddhant Haldar, Soumith Chintala, and Lerrel Pinto. Open teach: versatile teleoperation system for robotic manipulation. arXiv preprint arXiv:2403.07870, 2024. [31] Arhan Jain, Mingtong Zhang, Kanav Arora, William Chen, Marcel Torne, Muhammad Zubair Irshad, Sergey Zakharov, Yue Wang, Sergey Levine, Chelsea Finn, Wei-Chiu Ma, Dhruv Shah, Abhishek Gupta, and Karl Polaris: Scalable real-to-sim evaluations for Pertsch. generalist robot policies, 2025. URL https://arxiv.org/ abs/2512.16881. [32] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J. Davison. Rlbench: The robot learning benchIEEE Robotics and mark & learning environment. Automation Letters, 2020. [33] Chung Min Kim*, Brent Yi*, Hongsuk Choi, Yi Ma, Ken Goldberg, and Angjoo Kanazawa. Pyroki: modIn 2025 ular toolkit for robot kinematic optimization. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025. URL https://arxiv.org/abs/ 2505.03728. [34] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. Rma: Rapid motor adaptation for legged robots. arXiv preprint arXiv:2107.04034, 2021. [35] Arjun Lakshmipathy, Jessica Hodgins, and Nancy Pollard. Kinematic motion retargeting for contact-rich anthropomorphic manipulations. ACM Transactions on Graphics, 44(2):120, 2025. [36] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martın-Martın, Chen Wang, Gabrael Levine, Wensi Ai, Benjamin Martinez, Hang Yin, Michael Lingelbach, Minjune Hwang, Ayano Hiranaka, Sujay Garlanka, Arman Aydin, Sharon Lee, Jiankai Sun, Mona Anvari, Manasi Sharma, Dhruva Bansal, Samuel Hunter, Kyu-Young Kim, Alan Lou, Caleb Matthews, Ivan Villa-Renteria, Jerry Huayang Tang, Claire Tang, Fei Xia, Yunzhu Li, Silvio Savarese, Hyowon Gweon, C. Karen Liu, Jiajun Wu, and Li FeiFei. Behavior-1k: human-centered, embodied ai benchmark with 1,000 everyday activities and realistic simulation. arXiv preprint arXiv:2403.09227, 2024. [37] Hongyu Li, Lingfeng Sun, Yafei Hu, Duy Ta, Jennifer Barry, George Konidaris, and Jiahui Fu. Novaflow: Zeroshot manipulation via actionable flow from generated videos. arXiv preprint arXiv:2510.08568, 2025. [38] Jinhan Li, Yifeng Zhu, Yuqi Xie, Zhenyu Jiang, Mingyo Seo, Georgios Pavlakos, and Yuke Zhu. Okami: Teaching humanoid robots manipulation skills through single video imitation. In 8th Annual Conference on Robot Learning (CoRL), 2024. [39] Kailin Li, Puhao Li, Tengyu Liu, Yuyang Li, and Siyuan Huang. Maniptrans: Efficient dexterous bimanual manipulation transfer via residual learning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 69917003, 2025. [40] Toru Lin, Zhao-Heng Yin, Haozhi Qi, Pieter Abbeel, and Jitendra Malik. Twisting lids off with two hands. arXiv:2403.02338, 2024. [41] Toru Lin, Yu Zhang, Qiyang Li, Haozhi Qi, Brent Yi, Sergey Levine, and Jitendra Malik. Learning visuotactile skills with two multifingered hands. arXiv:2404.16823, 2024. [42] Toru Lin, Yu Zhang, Qiyang Li, Haozhi Qi, Brent Yi, Sergey Levine, and Jitendra Malik. Learning visuotactile In 2025 IEEE skills with two multifingered hands. International Conference on Robotics and Automation (ICRA), pages 56375643. IEEE, 2025. [43] Min Liu, Deepak Pathak, and Ananye Agarwal. Locoformer: Generalist locomotion via long-context adaptation. arXiv preprint arXiv:2509.23745, 2025. [44] Xueyi Liu, He Wang, and Li Yi. Dexndm: Closing the reality gap for dexterous in-hand rotation via joint-wise neural dynamics model, 2025. URL https://arxiv.org/abs/ 2510.08556. [45] Tyler Ga Wei Lum, Martin Matak, Viktor Makoviychuk, Ankur Handa, Arthur Allshire, Tucker Hermans, Nathan D. Ratliff, and Karl Van Wyk. DextrAH-g: Pixels-to-action dexterous arm-hand grasping with geIn 8th Annual Conference on Robot ometric fabrics. Learning, 2024. URL https://openreview.net/forum?id= S2Jwb0i7HN. [46] Tyler Ga Wei Lum, Olivia Y. Lee, C. Karen Liu, and Jeannette Bohg. Crossing the human-robot embodiment gap with sim-to-real rl using one human demonstration, 2025. URL https://arxiv.org/abs/2504.12609. [47] Jianlan Luo, Charles Xu, Fangchen Liu, Liam Tan, Zipeng Lin, Jeffrey Wu, Pieter Abbeel, and Sergey Levine. Fmb: functional manipulation benchmark for generalizable robotic learning, 2024. URL https: //arxiv.org/abs/2401.08553. [48] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, Isaac gym: High performance gpuand Gavriel State. based physics simulation for robot learning, 2021. URL https://arxiv.org/abs/2108.10470. [49] Zhao Mandi, Yifan Hou, Dieter Fox, Yashraj Narang, Ajay Mandlekar, and Shuran Song. Dexmachina: Functional retargeting for bimanual dexterous manipulation, 2025. URL https://arxiv.org/abs/2505.24853. [50] Jiageng Mao, Sicheng He, Hao-Ning Wu, Yang You, Shuyang Sun, Zhicheng Wang, Yanan Bao, Huizhong Chen, Leonidas Guibas, Vitor Guizilini, et al. Robot learning from physical world model. arXiv preprint arXiv:2511.07416, 2025. [51] Dan Morris, Hong Tan, Federico Barbagli, Timothy Chang, and Kenneth Salisbury. Haptic feedback enhances force skill learning. In Second Joint EuroHaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems (WHC07), pages 2126, 2007. doi: 10.1109/WHC.2007.65. [52] Patrick Naughton, Jinda Cui, Karankumar Patel, and Soshi Iba. Respilot: Teleoperated finger gaiting via In Conference on gaussian process residual learning. Robot Learning (CoRL), 2024. [53] Allison Okamura. Haptic feedback in robot-assisted minimally invasive surgery. Current opinion in urology, 19(1):102107, 2009. [54] OpenAI, Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider, Szymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba. Learning dexterous in-hand manipulation, 2019. URL https://arxiv.org/abs/1808.00177. [55] Claudio Pacchierotti and Domenico Prattichizzo. Cutaneous/tactile haptic feedback in robotic teleoperation: Motivation, survey, and perspectives. IEEE Transactions on Robotics, 40:978998, 2023. [56] Younghyo Park, Gabriel Margolis, and Pulkit Agrawal. Position: Automatic environment shaping is the next frontier in rl. In Forty-first International Conference on Machine Learning, 2024. [57] Shivansh Patel, Shraddhaa Mohan, Hanlin Mai, Unnat Jain, Svetlana Lazebnik, and Yunzhu Li. Robotic manipulation by imitating generated videos without physical demonstrations. arXiv preprint arXiv:2507.00990, 2025. [58] Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, and Jitendra Malik. Reconstructing hands in 3D with transformers. In CVPR, 2024. [59] Aleksei Petrenko, Arthur Allshire, Gavriel State, Ankur Handa, and Viktor Makoviychuk. Dexpbt: Scaling up dexterous manipulation for hand-arm systems with population based training. CoRR, abs/2305.12127, 2023. URL https://doi.org/10.48550/arXiv.2305.12127. [60] Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asymmetric actor critic for image-based robot learning. In Hadas KressGazit, Siddhartha S. Srinivasa, Tom Howard, and Nikolay Atanasov, editors, Robotics: Science and Systems XIV, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA, June 26-30, 2018, 2018. doi: 10.15607/RSS. 2018.XIV.008. URL http://www.roboticsproceedings. org/rss14/p08.html. [61] Wilbert Pumacay, Ishika Singh, Jiafei Duan, Ranjay Krishna, Jesse Thomason, and Dieter Fox. The colosseum: benchmark for evaluating generalization for robotic manipulation. 2024. [62] Haozhi Qi, Ashish Kumar, Roberto Calandra, Yi Ma, and Jitendra Malik. In-Hand Object Rotation via Rapid Motor Adaptation. In Conference on Robot Learning (CoRL), 2022. [63] Yuzhe Qin, Wei Yang, Binghao Huang, Karl Van Wyk, Hao Su, Xiaolong Wang, Yu-Wei Chao, and Dieter Fox. Anyteleop: general vision-based dexterous robot armhand teleoperation system, 2024. URL https://arxiv.org/ abs/2307.04577. [64] Ri-Zhao Qiu, Shiqi Yang, Xuxin Cheng, Chaitanya Chawla, Jialong Li, Tairan He, Ge Yan, David J. Yoon, Ryan Hoque, Lars Paulsen, Ge Yang, Jian Zhang, Sha Yi, Guanya Shi, and Xiaolong Wang. Humanoid policy human policy. arXiv preprint arXiv:2503.13441, 2025. [65] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. URL https://arxiv.org/abs/2408.00714. [66] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [67] Kenneth Shaw, Yulong Li, Jiahui Yang, Mohan Kumar Srirama, Ray Liu, Haoyu Xiong, Russell Mendonca, and Deepak Pathak. Bimanual dexterity for complex tasks. In 8th Annual Conference on Robot Learning, 2024. [68] Zilin Si, Kevin Lee Zhang, Zeynep Temel, and Oliver Kroemer. Tilde: Teleoperation for Dexterous In-Hand Manipulation Learning with DeltaHand. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, July 2024. doi: 10.15607/RSS.2024.XX.128. [69] Ritvik Singh, Arthur Allshire, Ankur Handa, Nathan Ratliff, and Karl Van Wyk. Dextrah-rgb: Visuomotor policies to grasp anything with dexterous hands. arXiv preprint arXiv:2412.01791, 2024. [70] Ritvik Singh, Karl Van Wyk, Pieter Abbeel, Jitendra Malik, Nathan Ratliff, and Ankur Handa. End-to-end rl improves dexterous grasping policies. arXiv preprint arXiv:2509.16434, 2025. [71] Jayesh Singla, Ananye Agarwal, and Deepak Pathak. Sapg: Split and aggregate policy gradients. In Proceedings of the 41st International Conference on Machine Learning (ICML 2024), Proceedings of Machine Learning Research, Vienna, Austria, July 2024. PMLR. [72] Aravind Sivakumar, Kenneth Shaw, and Deepak Pathak. Robotic telekinesis: Learning robotic hand imitaarXiv preprint tor by watching humans on youtube. arXiv:2202.10448, 2022. [73] Strom, Leif Hedman, Sarna, Ann Kjellin, Wredmark, and Li Fellander-Tsai. Early exposure to haptic feedback enhances performance in surgical simulator training: prospective randomized crossover study in surgical residents. Surgical endoscopy, 20:13838, 10 2006. doi: 10.1007/s00464-005-0545-3. [74] Tony Tao, Mohan Kumar Srirama, Jason Jingzhou Liu, Kenneth Shaw, and Deepak Pathak. Dexwild: Dexterous human interactions for in-the-wild robot policies. arXiv preprint arXiv:2505.07813, 2025. [75] Shagun Uppal, Ananye Agarwal, Haoyu Xiong, Kenneth Shaw, and Deepak Pathak. Spin: Simultaneous percepIn Proceedings of the tion interaction and navigation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1813318142, 2024. [76] Weikang Wan, Haoran Geng, Yun Liu, Zikang Shan, Yaodong Yang, Li Yi, and He Wang. Unidexgrasp++: Improving dexterous grasping policy learning via geometryaware curriculum and iterative generalist-specialist learning. arXiv preprint arXiv:2304.00464, 2023. [77] Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, and Karen Liu. Dexcap: Scalable and portable mocap data collection system for dexterous manipulation. arXiv preprint arXiv:2403.07788, 2024. [78] Jun Wang, Ying Yuan, Haichuan Che, Haozhi Qi, Yi Ma, Lessons from Jitendra Malik, and Xiaolong Wang. learning to spin pens. In CoRL, 2024. [79] Youzhuo Wang, Jiayi Ye, Chuyang Xiao, Yiming Zhong, Heng Tao, Hang Yu, Yumeng Liu, Jingyi Yu, and Yuexin Ma. Dexh2r: benchmark for dynamic dexterous grasping in human-to-robot handover, 2025. URL https: //arxiv.org/abs/2506.23152. [80] Bowen Wen, Wei Yang, Jan Kautz, and Stan Birchfield. Foundationpose: Unified 6d pose estimation and tracking the IEEE/CVF of novel objects. Conference on Computer Vision and Pattern Recognition (CVPR), pages 1786817879, June 2024. In Proceedings of [81] Philipp Wu, Yide Shentu, Zhongke Yi, Xingyu Lin, and Pieter Abbeel. Gello: general, low-cost, and intuitive teleoperation framework for robot manipulators. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1215612163, 2024. doi: 10. 1109/IROS58592.2024.10801581. [82] Mengda Xu, Han Zhang, Yifan Hou, Zhenjia Xu, Linxi Fan, Manuela Veloso, and Shuran Song. Dexumi: Using human hand as the universal manipulation inarXiv preprint terface for dexterous manipulation. arXiv:2505.21864, 2025. [83] Yinzhen Xu, Weikang Wan, Jialiang Zhang, Haoran Liu, Zikang Shan, Hao Shen, Ruicheng Wang, Haoran Geng, Yijia Weng, Jiayi Chen, et al. Unidexgrasp: Universal robotic dexterous grasping via learning diverse proposal generation and goal-conditioned policy. arXiv preprint arXiv:2303.00938, 2023. [84] Jianglong Ye, Keyi Wang, Chengjing Yuan, Ruihan Yang, Yiquan Li, Jiyue Zhu, Yuzhe Qin, Xueyan Zou, and Xiaolong Wang. Dex1b: Learning with 1b demonstrations In Robotics: Science and for dexterous manipulation. Systems (RSS), 2025. [85] Brent Yi, Chung Min Kim, Justin Kerr, Gina Wu, Rebecca Feng, Anthony Zhang, Jonas Kulhanek, Hongsuk Choi, Yi Ma, Matthew Tancik, and Angjoo Kanazawa. Viser: Imperative, web-based 3d visualization in python. arXiv preprint arXiv:2507.22885, 2025. [86] Zhao-Heng Yin, Changhao Wang, Luis Pineda, Francois Hogan, Krishna Bodduluri, Akash Sharma, Patrick Lancaster, Ishita Prasad, Mrinal Kalakrishnan, Jitendra Malik, Mike Lambeta, Tingfan Wu, Pieter Abbeel, and Mustafa Mukadam. Dexteritygen: Foundation controller for unprecedented dexterity, 2025. URL https://arxiv.org/ abs/2502.04307. [87] Kevin Zakka, Baruch Tabanpour, Qiayuan Liao, Mustafa Haiderbhai, Samuel Holt, Jing Yuan Luo, Arthur Allshire, Erik Frey, Koushil Sreenath, Lueder A. Kahrs, Carlo Sferrazza, Yuval Tassa, and Pieter Abbeel. Mujoco playground: An open-source framework for gpu-accelerated robot transfer., 2025. URL https://github.com/google-deepmind/ mujoco playground. learning and sim-to-real [88] Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations. In Robotics: Science and Systems, 2024. [89] Jialiang Zhang, Haoran Liu, Danshi Li, XinQiang Yu, Haoran Geng, Yufei Ding, Jiayi Chen, and He Wang. Dexgraspnet 2.0: Learning generative dexterous grasping in large-scale synthetic cluttered scenes. In 8th Annual Conference on Robot Learning. [90] Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with arXiv preprint arXiv:2304.13705, low-cost hardware. 2023. [91] Yifeng Zhu, Arisrei Lim, Peter Stone, and Yuke Zhu. Vision-based manipulation from single human arXiv preprint video with open-world object graphs. arXiv:2405.20321, 2024. A. Reward Function Details"
        },
        {
            "title": "APPENDIX",
            "content": "Goal-Pose Reward (rgoal). Once Igrasped = 1, the goalreaching term rgoal becomes the dominant signal to train our RL policy. As Section III-B describes, we define: rgoal = max(d d(ot, g), 0) + Bsucc I[d(ot, g) < ϵ] , (8) Our goal-reaching reward has (i) dense progress term based on the goal distance d(ot, g) and (ii) sparse success bonus Bsucc. We maintain stateful variable storing the smallest distance achieved for the current goal (reset when new goal is sampled), and only reward improvements in distance to avoid incentivizing static proximity. When the goal is reached, i.e., d(ot, g) < ϵ, the agent receives Bsucc and we resample new goal. Fig. 9: Object Keypoints. (Left) Visualization of 4 pose keypoints in the local object frame. (Right) Visualization of the keypoint distances used to compute the distance d(ot, g). The reward function has three primary components: = rsmooth + rgrasp + Igraspedrgoal (3) where Igrasped is an indicator function that triggers once the object has been successfully grasped. We describe each reward term in further detail below: Smoothness Reward (rsmooth). To promote physically plausible control and reduce hardware wear, we penalize the L1 norm of joint velocities: rsmooth = λarm qarm1 λhand qhand1 (4) where qarm and qhand correspond to the current velocities of the 7-DOF Kuka arm and the 22-DoF Sharpa hand, respectively. Grasp Reward (rgrasp). The term rgrasp facilitates the transition from neutral pose to stable grasp: rgrasp = rapproach + (1 Igrasped)rlift (5) rapproach encourages the agent to reduce the distance between the robots hand and the object. It rewards the agent for approaching the object rather than simply maintaining fixed close distance. We define: rapproach = λapproach max( ft dft, 0) (6) where dft is the current mean distance between the fingertips and the object, and ft is stateful variable that stores the minimum mean distance achieved so far in the episode. rlift encourages the agent to grasp and lift the object. We define: rlift = λlift max(z zinit, 0) + I[z zlifted]Blifted (7) where is the vertical position of the object, zinit is the initial position of the object, zlifted is the lifted threshold, and Blifted is bonus that is awarded at most once per episode when the object has been lifted. Igrasped turns true once zlifted. This ensures that this lifting reward only applies at the start of the episode before the object has been lifted, and then is 0 for the remainder of the episode. At this point, rgoal takes over. Keypoint Distance Formulation: Following [4, 59], we measure distance using D=4 object-frame keypoints d(ot, g) = maxi ot,i gi , where ot,i and gi are the world-frame positions of the i-th keypoint under the current and goal poses, respectively. We define the following scales srew = , srew [srew ] = [0.14, 0.03, 0.03] (in metres), and we define the keypoints {ki}4 i=1 at the following offsets: , srew , , , srew /2 srew /2 srew /2 srew /2 srew /2 srew / srew /2 srew /2 srew /2 srew srew srew /2 /2 /2 (9) Fig. 9 visualizes our keypoint-based pose representation. Defining pose distance in this space yields single, interpretable metric that jointly captures translation and rotation error. Since many tools are elongated, we set srew , srew so the reward is more sensitive to pitch and yaw errors (rotations about the and axes) than to roll about the long x-axis. This biases the policy toward aligning the tools principal axis with the target pose, which is typically the dominant requirement for tool use. Using fixed relative scales across objects also ensures consistent trade-off between translational and rotational progress for all tasks. > srew B. Procedural Asset Generation Details To promote robustness to wide range of inertial properties, we procedurally generate tool-like objects using simple handlehead abstraction. This minimal design still spans broad family of real-world handheld tools (e.g., brushes, markers, spatulas, screwdrivers, and hammers). The goal is to expose the policy to large variations in geometry and physics without relying on complex meshes that slow down simulation and training. Each tool is composed of cuboids and cylinder primitives, which ensures stable physics and fast simulation. Tool parameterization. Each asset consists of (i) handle and (ii) head rigidly attached to one end of the handle. We randomize both geometry and density. For each part, we sample its length and cross-sectional dimensions. For cuboids we use width and height H; for cylindrical/capsule variants, denotes the diameter and we set = . Geometric randomizations. Each tool is rigid union of two parts: handle and head. For each part, we randomly choose one of two primitive shapes: cuboid (box) or Fig. 10: Representative Examples of DexToolBench Tasks. Visual breakdown of representative tasks in DexToolBench across the 6 tool categories, highlighting the diversity of objects and manipulation tasks. capsule (cylinder with rounded ends). We then sample the part dimensions uniformly from ranges chosen to span common handheld tools (from small brushes/markers to larger hammers). The head is attached to one end of the handle, and the heads long axis is rotated by 90 relative to the handles long axis. For cuboid, we sample length, width, and height. For capsule, we sample length and diameter: Handle dimensions: length is sampled in [5, 30] cm; width/height (or diameter) is sampled in [1, 4] cm. Head dimensions: length is sampled in [1, 15] cm; width/height (or diameter) is sampled in [0.5, 12] cm. Physics randomizations. To systematically induce physics randomizations, we assign different densities to the handle and head. Handles are sampled with lower density, ρlow U[300, 600] kg/m3, approximating wood or plastics. Heads are sampled with higher density range ρhigh U[300, 2000] kg/m3, approximating metals or dense rubber. Together with geometric variation, this density variation results in diverse center-of-mass locations (often shifted toward the head) and broad range of rotational inertias, encouraging the policy to learn to adapt under varying physics in the realworld. C. Simulation Training Details 1) Simulation Details: We use for massively-parallel GPU-accelerated simulation. We build from IsaacGym [48] Fig. 11: Visualization of Policy Observations during Real-World Deployment. (Top) Image frames from real-world rollout of the brush manipulation task. (Bottom) The corresponding visualization of the policy observations at each timestep, consisting of the robot state, estimated object pose, and goal pose (green). When the distance between the current pose and goal pose is sufficiently small, the goal pose is updated to the next pose in the goal sequence. Note that this visualization is rendering of the policys inputs, not physics-based simulation. the DexPBT Kuka Allegro Reorientation environment [59], and add substantial changes to the environment to improve dexterous tool-use, enable sim-to-real transfer, and improve generalization to unseen objects and trajectories. Notably, many other GPU-accelerated simulators, such as Genesis [10] and MJWarp [87], could not be used because they do not currently support simulation of parallel environments with different object geometries per scene. 2) Policy Action: The RL policy outputs an action at RJ , where = 29 represents the total number of actuated joints. We first clip the actions to the range [1, 1]. We partition [1, 1]7 and hand this action into arm components aarm [1, 1]22. These are processed into joint components ahand position targets qtarget as follows: t Arm Control (Delta). For the 7-DoF arm, we interpret the action as relative displacement (delta) from the previous target. The intermediate target is computed as: = qtarget, arm ˆqarm t1 + karmaarm (10) where karm = 0.025 is scaling factor derived from the control frequency and speed limits. We clip ˆqarm to the joint limits, and then apply an exponential moving average (EMA) filter with smoothing factor αarm to obtain the final qtarget, arm . Hand Control (Absolute). For the 22-DoF hand, we interpret the action as an absolute target within the joint limits. We map the action range [1, 1] to the physical limits [qhand upper] via an affine transformation: lower, qhand ˆqhand = ahand + 1 2 (qhand upper qhand lower) + qhand lower (11) We then apply an EMA filter with smoothing factor αhand to ˆqhand , and finally clip the result to the joint limits to ensure safety, yielding qtarget, hand . 3) Policy Observation: The policy observation space comprises robot proprioception st, object state ot, goal information g, and the object descriptor ϕ. We typically provide relative representations (e.g., keypoints expressed relative to the palm) over absolute world positions to facilitate generalization across tool geometries and improve invariance to absolute world coordinates. The specific observations are: Proprioception (st). Joint positions qt R29 and velocities qt R29. Previous joint position targets qtarget Palm pose, consisting of world position xpalm R3 and t1 R29. orientation quaternion rpalm R4. Fingertip positions relative to the palm center: {xtip,j xpalm}5 j=1 R15. Object Pose, Goal Pose, and Descriptor (ot, g, ϕ). Instead of directly inputting raw pose matrices, we represent the object and goal states using = 4 keypoints. These observation keypoints are computed using the objects grasp bounding box R3 (length, width, height), effectively conditioning the policy on the specific tool geometry. Object orientation quaternion robj R4. Object keypoints relative to the palm: {ot,i xpalm}4 i=1 R12. Keypoint errors to goal: {ot,i gi}4 i=1 R12. Here, ot,i and gi correspond to the i-th keypoint of the current and goal poses, respectively. Object scales R3, representing the dimensions of the tools graspable region (handle). We note that while the reward function (Appendix A) uses fixed keypoint offsets to maintain consistent success metric across objects, the policy observations use the instance-specific offsets defined by the object scales = [sx, sy, sz]: sx/2 sy/2 sz/2 , sx/2 sy/2 sz/2 , sx/2 sy/2 sz/ , sx/2 sy/2 sz/2 (12) In our experiments, we use αarm = αhand = 0.1, where kobs α = 1.0 means no smoothing. 4) Asymmetric Critic State: We utilize an asymmetric actor-critic setup where the critic is trained on privileged state space S, while the policy acts on the restricted observation space O. The critic state contains the exact, noise-free, and instantaneous ground-truth state of the system, whereas the policy observation is subjected to noise and delays to bridge the sim-to-real gap. In addition to the standard observations, the critic state includes: Ground-truth Velocities: Exact linear and angular velocities for the palm (vpalm, ωpalm) and the object (vobj, ωobj). Reward Signals: The instantaneous reward rt and the number of successes achieved this episode, which help to improve value function estimation. Stateful Progress Features: Auxiliary features including the minimum fingertip-to-object distance achieved since the last reset, the minimum keypoint distance achieved since last reset, the number of environment steps that have occurred since last reset, and binary signal Igrasped indicating if the object has been lifted. Noise-Free and Undelayed Object Pose: Exact object pose without noise or delays. 5) Reset and Initialization: Next, we describe the termination conditions and initialization of the environment. Episode Termination Conditions. An environment is reset if any of the following conditions are met: Object Fallen Off Table: The object height drops below the table surface. Object Drop (Hysteresis): To penalize dropping the object after grasping, we trigger reset if the object returns to the table surface (z < zinit) after the grasped flag Igrasped has been activated. Hand Wander: The distance between the hand and the object exceeds 1.5 m. Table Force Limits: The net force measured by the table sensor exceeds 100 N, preventing aggressive collisions. Timeout: The episode duration exceeds the maximum step limit. Max Consecutive Successes: The agent achieves the maximum number of consecutive successes for an episode. Scene Initialization. At the robot and object are reset to randomized states to encourage robustness: the start of each episode, Robot: Joint positions are initialized to default position with additive uniform noise. Object: The object is spawned above the table surface with randomized planar position (x, y) and random rotation quaternion SO(3). We also apply small random perturbations to the table height to improve robustness to geometric calibration errors. Goal Sampling Strategy. 1) Initial Goal (g0): The first goal is sampled uniformly from 3D workspace volume centered above the table. The sampling ranges relative to the table center are [0.35, 0.35], [0.1, 0.2], and [0.15, 0.52], where is up, is forward (wrt the robot), and is left (wrt the robot). 2) Subsequent Goals (gk+1): Upon successfully reaching goal gk, we sample the next goal relative to the previous goal. We apply random perturbation of up to 0.1 in position and 90 in rotation. 6) Hyperparameters: Table shows the important hyperparameters used for the simulation environment and SAPG training. D. Sim-to-Real Details Observation and action delay: We model system latency by passing observations, actions, and object states through FIFO queues and randomly sampling delayed values. To reflect the uncertainty and inference latency of visionbased object pose tracking, object pose estimates are assigned significantly larger delay than other signals. Accurate robot modeling: We explicitly match the simulations joint gains, armature, and damping to those of the physical robot. Sensor noise: We inject zero-mean Gaussian noise into joint velocities and object pose observations, as we found these signals to exhibit the highest measurement noise in the real world. The noise magnitudes are calibrated to match empirical levels, robustifying the policy against realistic sensor noise. Smooth actions: We apply exponential moving average (EMA) smoothing to joint position targets to prevent high-frequency control artifacts. Table height randomization: We randomize the table height at the start of each episode to improve robustness to geometric variations in the physical workspace. External force and torque perturbations: We apply random external forces and torques to the object during training to help the policy generalize to unmodeled contact dynamics and unexpected disturbances. Finger self-collisions: We restrict the range of the finger abduction/adduction joints to conservative subset of their physical limits. This prevents the policy from reaching configurations that could result in potentially damaging self-collisions on the real hardware. E. Human Video Processing Details 1) Metric-Scale Mesh and Grasp Bounding Box Acquisition: Fig. 12 illustrates our pipeline, which leverages SAM 3D [16] to extract metric-scale mesh and grasp bounding box from single RGB-D video demonstration. a) Initial Object Segmentation: We first obtain segmentation mask of the object from the first frame of the video. We use the first frame because it typically minimizes occlusion, as this is prior to human interaction. Although segmentation could be conditioned on text prompt, we found that prompting SAM 2 [65] with two user-specified points on the object provided superior reliability. b) Metric-Scale Mesh Extraction: Next, we generate 3D mesh using SAM 3D [16]. Although the default SAM TABLE I: Simulation environment and SAPG training hyperparameters. Parameter Value Parameter Value Environment & Control Simulation / Control Frequency Num. Environments Episode Length Obj. Pos. Range (x, y) Table Height Range (z) Robot Joint Pos. Range Success Tolerance (ϵ) Initial Height (zinit) Lift Threshold (zlifted) Domain Randomization Obj. Pose Noise (Trans.) Obj. Pose Noise (Rot.) Obj. Pose Delay Max Action/Obs Delay Max Joint Vel. Obs Noise (σ) Perturb. Force Scale Perturb. Torque Scale 120 / 60 Hz 24576 600 steps 10 cm 1 cm 0.1 rad 1 cm 0.63 0.73 1 cm 5.0 10 steps 3 steps 0.1 rad/s 5.0 0.5 Nm SAPG Hyperparameters Actor Network Critic Network Learning Rate Minibatch Size SAPG Block Size Entropy Bonus Scale Discount Factor (γ) GAE Parameter (τ ) Clip Range Reward Coefficients Arm Action Penalty (λarm) Hand Action Penalty (λhand) Approach Scale (λapproach) Lifting Scale (λlift) Lifting Bonus (Blifted) Goal Pose Scale (λgoal) Success Bonus (Bsucc) LSTM[1024] + MLP[1024,1024,512,512] MLP[1024,1024,512,512] 1 104 98,304 4096 0.005 0.99 0.95 0.1 0.03 0.003 50.0 20.0 300.0 200.0 1000.0 Fig. 12: Metric-Scale Mesh and Grasp Bounding Box Acquisition. We present semi-automated pipeline to extract metric-scale mesh and grasp bounding box from single RGB-D video scan. (1) We first segment the target object in the initial frame and reconstruct 3D mesh using SAM 3D [16], injecting the captured depth map to ensure metric accuracy. (2) To establish canonical coordinate frame, we virtually render the mesh from multiple views and use SAM 2 [65] to segment the geometry into handle and head regions based on user prompts. (3) The final grasp bounding box is derived from the handles geometry: it is centered on the handle, with the positive x-axis oriented toward the head, ensuring consistent alignment with policy training. 3D pipeline relies on monocular depth estimation from RGB input, the resulting mesh is not metrically accurate. Since we require accurate scale for both pose tracking and grasp bounding box estimation, we modify the pipeline to directly utilize the captured depth image instead of the predicted depth. c) Part Segmentation (Handle vs. Head): To define the objects origin, we segment the mesh into handle (graspable region) and head (non-graspable region). This decomposition allows us to center the grasp bounding box on the handle and align the objects x-axis along the handles primary axis (x-axis pointing from handle to head). We automate this by rendering video of the mesh from virtual camera circling the object. The user provides point prompts on the first frame to define the handle and head, and we use SAM 2 [65] to propagate these masks across the rendered sequence. d) Grasp Bounding Box Definition: Using the rendered camera extrinsics and intrinsics, we back-project the masked depth maps to 3D to obtain separate point clouds for the handle and head. We crop the full mesh based on these point clouds. Finally, we define the grasp bounding box using the handles geometry: the box is centered at the handles centroid, with its x-axis oriented along the vector pointing from the handle centroid toward the head centroid. 2) Goal Pose Sequence: We use FoundationPose to extract the raw object pose trajectory from the RGB-D human video demonstration, which is collected at the cameras native frequency of 30Hz. Directly tracking this raw sequence is suboptimal for two reasons: (1) frame-to-frame pose estimation jitter can make the goal pose sequence shaky, and (2) the demonstration often includes pre-grasp phase where the object remains stationary on the table while the human approaches it. To address these issues, we apply the following preprocessing steps. Temporal Downsampling. We downsample the trajectory from 30Hz to 3Hz. This acts as low-pass filter, removing high-frequency perception noise and ensuring the policy tracks the underlying smooth motion profile rather than attempting to replicate spurious artifacts in the pose estimation. Lift-off Truncation. We automatically trim the start of the trajectory to remove the static phase where the object rests on the table. We calculate the objects vertical position zt relative to the table surface at each timestep. The goal sequence is defined to start at the first frame tstart where the objects height exceeds threshold zthresh = 10cm. This ensures that the robot immediately attempts to lift and manipulate the tool, rather than trying to maintain static pose on the table surface. F. DexToolBench Details Fig. 10 visualizes tasks from DexToolBench. Fig. 13 visualizes the real-world objects and SAM 3D [16] generated objects in DexToolBench. Fig. 13: DexToolBench Objects. (Left): Real-world objects. (Right): SAM 3D [16] generated meshes of these objects. Objects. Here, we describe the 12 object instances across the 6 object categories: Hammer: Claw Hammer: 3D printed hammer consisting of thin black handle and gray head. Mallet Hammer: rubber mallet consisting of wooden handle and heavy, cylindrical black head. Marker: Sharpie Marker: standard Sharpie permanent marker with black cap. Staples Marker: dry-erase Staples marker with white barrel. It is slightly thinner than the Sharpie. Eraser: Handle Eraser: An eraser with thin handle and yellow bristles. Flat Eraser: An Expo eraser consisting of rectangular foam block without handle. Brush: Blue Brush: blue brush with thick black handle and yellow bristles along the same direction as the handle. Red Brush: red brush with long black handle and black bristles at 90 angle with respect to the handle. It is lighter than the Blue brush. Spatula: Spoon Spatula: spoonula with long, cylindrical black handle and shallow oval spoon. Flat Spatula: black spatula with flat rectangular blade and thin, rectangular handle. Screwdriver: Long Screwdriver: screwdriver with long shaft and wooden handle. Alternating yellow, black, and red pieces of tape are added to the handle to reduce rotational symmetry. Short Screwdriver: screwdriver with short shaft and bulbous handle with red and black pattern. Tasks. Here, we describe the 12 tool-use task trajectories instances across the 6 object categories: Hammer: Swing Down: Grasp the hammer from flat on the table, rotate it by 90 into striking configuration, swing down onto nail 3 times. Swing Side: Grasp the hammer from flat on the table, rotate it by 90 into striking configuration, swing sideways onto nail 3 times. Marker: Draw Smile: Grasp the marker from flat on the table, move to the whiteboard, draw two dots and smile. Write C: Grasp the marker from flat on the table, move to the whiteboard (different location from smile), draw C. Eraser: Wipe Smile: Grasp the eraser from flat on the table, move to the whiteboard, erase the smile. TABLE II: Detailed Real-World Evaluation Results. We report the Task Progress (%) for each of the 5 rollouts across all 24 object-task variations. The specific tools correspond to the instances shown in Fig. 4. Category Instance Trajectory Hammer Marker Eraser Brush Spatula Screwdriver Claw Mallet Sharpie Staples Handle Flat Blue Red Spoon Flat Long Short Swing Down Swing Side Swing Down Swing Side Draw Smile Write Draw Smile Write Wipe Smile Wipe Wipe Smile Wipe Sweep Fwd Sweep Right Sweep Fwd Sweep Right Serve Plate Flip Over Serve Plate Flip Over Spin Vert Spin Horiz Spin Vert Spin Horiz 100 100 100 65.6 100 100 100 31.0 100 100 100 47.8 100 100 43.2 100 77.5 100 0.0 20.9 48.4 15.4 R2 100 100 100 81.3 100 100 53.1 0.0 100 100 100 47.8 100 100 62.2 76.5 100 75.0 52.2 72.1 86. 61.5 100 R3 100 77.5 33.3 100 100 100 100 100 100 100 100 40.0 100 13.5 100 100 100 78.1 0. 53.5 83.3 12.8 100 R4 100 100 100 78.1 100 32. 100 100 100 100 100 100 60.0 100 100 42.2 48.5 56.3 78.3 74.4 3.3 100 77.8 R5 100 100 88.9 62. 0.0 56.0 100 100 100 100 100 100 60.0 100 100 59. 100 100 78.1 100 58.1 86.7 0.0 0.0 Avg 100 95. 84.4 77.5 80.0 77.6 90.6 66.2 100 100 100 100 51.1 82.7 61.4 85.0 95.5 77.5 46.1 55.8 61.7 37.9 75.6 Wipe C: Grasp the eraser from flat on the table, move to the whiteboard (different location from smile), erase the C. Brush: Sweep Forward: Grasp the brush from flat on the table, rotate it by 90 into sweeping configuration, sweep forward 3 times to sweep paper balls into trash. Sweep Right: Grasp the brush from flat on the table, rotate it by 90 into sweeping configuration, sweep rightwards 3 times to sweep paper balls into trash. Spatula: Serve Plate: Grasp the spatula from flat on the plate. Move to the other bowl/plate, perform scooping motion, perform serving motion onto the original plate. Flip Over: Grasp the spatula upside-down with the spatula head on the pan/bowl. Rotate it by 180, perform scooping motion, perform flipping motion. Screwdriver: Spin Vertical: Grasp the screwdriver from flat on the table, rotate it by 90 into vertical configuration, spin it by 360 along long axis. Spin Horizontal: Grasp the screwdriver from flat on the table, keep it at horizontal configuration, spin it by 360 along long axis. Processed Data. Each task is defined by an RGB-D human video, captured via ZED 1 stereo camera. We process each in video with our perception pipeline, described in detail Appendix E. The processed data includes: Raw RGB-D video Object segmentation masks Metric-scale object meshes 6D object pose trajectories Additionally, we provide: Visualization scripts to visualize the data in 3D space using Viser [85]. Simulation scripts to evaluate policies on these tasks. This will support multiple robot arms and hands. Links to purchase the real-world objects Task Progress. For each task, we evaluate Task Progress, defined as the percentage of object pose waypoints in the demonstrated trajectory that the robot successfully reached. We consider goal pose to be reached if the distance between the object pose and goal pose d(ot, g) is below our defined success tolerance ϵ = 2cm. This evaluation is performed closed-loop: the goal advances to the next waypoint only after the current waypoint is reached. This contrasts with open-loop playback, where goals advance at fixed frequency regardless of the objects state. Our closedloop protocol decouples execution speed from spatial accuracy, allowing the policy to utilize retry behaviors and execute as fast or slow as it needs to without being penalized for failing to match the exact timing profile of the demonstration. G. Real-World Experiment Additional Analysis Table II shows the per-rollout Task Progress of the results shown in Fig. 4. Fig. 11 visualizes the policy observations at each timestep during real-world deployment. Perception Failures. We observe that object pose estimation is the most common failure mode of our system (43.7% of failures). Specifically, FoundationPose [80] struggles under three conditions common in dexterous manipulation: (1) heavy object occlusion, which is exacerbated when the robot hand manipulates small objects (e.g., the small screwdriver); (2) visual ambiguity due to rotational symmetry (e.g., the cylindrical body of the marker); and (3) low visual contrast between the object and the environment (e.g., dark bristles on black surface). While our policy is trained to be robust to significant pose noise, it cannot recover from catastrophic tracking failures where the estimator loses the object entirely. Future work could address this by incorporating additional camera views or leveraging temporal consistency in 2D tracking to improve state estimation stability. Manipulation Failures. Manipulation failures primarily stem from two sources: object drops (34.5%) and failure to reach the goal pose due to incomplete in-hand rotation (18.2%). Object drops were most common on heavy objects (e.g., the 331g mallet hammer and the 325g blue brush), typically occurring during reorientation or environment contact. Failure to reach the goal pose due to incomplete in-hand rotation was most common on thin objects (e.g., the 1cm thick flat spatula), as the policy would repeatedly attempt in-hand rotation but fail to manipulate the thin geometry. Grasp failures were rare (3.6%) but occasionally occurred with the marker due to its thin shape. In these instances, the markers cylindrical shape often caused it to roll off of the table, so the policy was unable to recover. H. Kinematic Retargeting Baseline Details Fig. 14 visualizes the kinematic retargeting pipeline. Following Lum et al. [46], we perform hand pose estimation on each frame of the human video and then retarget these hand poses to dexterous robot. Specifically, we first use HaMeR [58] to predict initial hand keypoints and mesh vertices from RGB images. Next, we refine this prediction by using the corresponding depth image and mask of the hand (from SAM 2 [65]) to extract 3D hand point cloud, and then aligning the HaMeR mesh to this point cloud via Iterative Closest Point (ICP) registration. Finally, we retarget the robot to these poses in two stages: we first compute the arm configuration using Damped Least Squares [11] to place the palm, and subsequently optimize the hand finger joints to reach the fingertip targets relative to the palm. I. Fixed-Grasp Baseline Details To run this baseline, we first use our SimToolReal policy to grasp and lift the object to the initial goal pose. Once the first goal pose is reached, we stop running the policy and begin planning fixed grasp trajectory to perform the objecttrajectory following task. For the hand, we maintain fixed grasp by storing the current hand joint position targets and continuing to apply these joint position targets to the hand. For the arm, we plan motion that moves the object along the goal trajectory, assuming its relative pose to the end-effector remains constant. Problem Formulation. Specifically, we assume rigid grasp where the transformation TEO between the end-effector frame and the object frame remains constant throughout the trajectory. Let (i) BO denote the i-th target object pose in the robot base frame from goal sequence of length . The corresponding target end-effector pose (i) BE is derived as: BE = (i) (i) BO (TEO) (13) We test two methods for computing the arm joint trajectory q1:N that reach these target end-effector poses. Damped Least Squares. For each waypoint, we compute the arm Jacobian J(q) and the pose error vector (concatenating translation and axis-angle rotation error). We compute the iterative joint update using the Damped Least Squares method [11]: = (JJ + λ2I)1e (14) where the damping factor λ smooths the motion and prevents discontinuities. While this method is computationally efficient (solving in seconds), it is strictly local and does not account for environmental obstacles (e.g., the table surface or whiteboard), often leading to collisions. Collision-Free Trajectory Optimization. To address environment collisions, we utilize PyRoki [33], which uses JAX-based Levenberg-Marquardt solver for trajectory optimization. We extend its standard trajectory optimization formulation to optimize trajectory with waypoints instead of 1. This method incurs higher computation time (40 60 seconds per trajectory) and requires time-consuming process of modeling the robot as set of collision spheres. 1) Initialization: We warm-start the optimization by sequentially solving collision-free Inverse Kinematics (IK) for each waypoint, seeding each step with the solution from the previous waypoint to encourage temporal consistency. 2) Optimization: The solver minimizes cost function composed of target pose error and smoothness (joint velocity) terms, subject to constraints: joint position/velocity limits, self-collision avoidance, and environment collision avoidance. Fig. 14: Kinematic Retargeting Pipeline. From the RGB-D human video, we use SAM 2 [65] for hand masks and HaMeR [58] for hand pose prediction. Next, we use ICP registration to align the hand pose prediction with the segmented hand point cloud to obtain accurate 3D hand poses. Lastly, we perform IK-based retargeting of the arm and hand to match the human wrist pose and fingertip positions. Fig. 15 visualizes the trajectories produced by both methods. We use the Collision-Free Trajectory Optimization for all reported baseline experiments, as the simple Damped Least Squares method frequently fails due to significant table collisions during complex object rotations. J. Specialist Details Fig. 16 presents granular breakdown of the aggregate results shown in Fig. 6. While the main text reports the average performance across all categories, this figure details the specific generalization capabilities of the specialist policy trained for each of the six tool-use categories individually (e.g., Hammer, Spatula, Brush). Each specialist policy uses the same architecture, observation space, and action space as used by SimToolReal. It uses the same reward function, termination criteria, and episode initialization settings. The only changes are (i) Fixed Object Geometry: instead of training on diverse, procedurally generated primitive objects, the specialist is trained exclusively on single object instance (Obj A), and Fig. 15: Fixed Grasp Baselines. We first use our SimToolReal policy to grasp and lift the object. We then attempt to follow the trajectory using fixed grasp. Option 1 (Damped Least Squares): Tracks the target poses but causes severe collisions with the table. Option 2 (Collision-Free Trajectory Optimization): Avoids collisions but fails to reach the target poses. Fig. 16: Detailed Comparison against Specialists. We provide breakdown of the results in Fig. 6. Each plot compares SimToolReal to specialist policy trained on single object and trajectory (Obj / Traj A) for single object category. (ii) Fixed Task Trajectory: instead of training on randomly sampled goal poses, the specialist is trained on single, fixed sequence of goal poses (Traj A) extracted from specific human demonstration. Table III details the specific object instances and task trajectories that define Obj A, Traj A, Obj B, and Traj for each tool category. TABLE III: Specialist Objects and Trajectories. Objects and trajectories used for specialist policy training and evaluation. Category Obj Traj Obj Brush Eraser Hammer Marker Red Brush Flat Eraser Mallet Hammer Staples Marker Sweep Forward Blue Brush Wipe Smile Swing Down Draw Smile Handle Eraser Claw Hammer Sharpie Marker Traj Sweep Right Wipe Swing Side Write Screwdriver Long Screwdriver Spin Vertical Short Screwdriver Spin Horizontal Spatula Flat Spatula Serve Plate Spoon Spatula Flip Over"
        }
    ],
    "affiliations": [
        "Cornell University",
        "Stanford University"
    ]
}