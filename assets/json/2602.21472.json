{
    "paper_title": "The Design Space of Tri-Modal Masked Diffusion Models",
    "authors": [
        "Louis Bethune",
        "Victor Turrisi",
        "Bruno Kacper Mlodozeniec",
        "Pau Rodriguez Lopez",
        "Lokesh Boominathan",
        "Nikhil Bhendawade",
        "Amitis Shidani",
        "Joris Pelemans",
        "Theo X. Olausson",
        "Devon Hjelm",
        "Paul Dixon",
        "Joao Monteiro",
        "Pierre Ablin",
        "Vishnu Banna",
        "Arno Blaas",
        "Nick Henderson",
        "Kari Noriy",
        "Dan Busbridge",
        "Josh Susskind",
        "Marco Cuturi",
        "Irina Belousova",
        "Luca Zappella",
        "Russ Webb",
        "Jason Ramapuram"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Discrete diffusion models have emerged as strong alternatives to autoregressive language models, with recent work initializing and fine-tuning a base unimodal model for bimodal generation. Diverging from previous approaches, we introduce the first tri-modal masked diffusion model pretrained from scratch on text, image-text, and audio-text data. We systematically analyze multimodal scaling laws, modality mixing ratios, noise schedules, and batch-size effects, and we provide optimized inference sampling defaults. Our batch-size analysis yields a novel stochastic differential equation (SDE)-based reparameterization that eliminates the need for tuning the optimal batch size as reported in recent work. This reparameterization decouples the physical batch size, often chosen based on compute constraints (GPU saturation, FLOP efficiency, wall-clock time), from the logical batch size, chosen to balance gradient variance during stochastic optimization. Finally, we pretrain a preliminary 3B-parameter tri-modal model on 6.4T tokens, demonstrating the capabilities of a unified design and achieving strong results in text generation, text-to-image tasks, and text-to-speech tasks. Our work represents the largest-scale systematic open study of multimodal discrete diffusion models conducted to date, providing insights into scaling behaviors across multiple modalities."
        },
        {
            "title": "Start",
            "content": "The Design Space of Tri-Modal Masked Diffusion Models Louis Bethune, Victor Turrisi, Bruno Kacper Mlodozeniec Bhendawade, Amitis Shidani, Joris Pelemans, Theo X. Olausson Pierre Ablin, Vishnu Banna, Arno Blaas, Nick Henderson, Kari Noriy, Dan Busbridge, Josh Susskind, Marco Cuturi, Irina Belousova, Luca Zappella, Russ Webb, Jason Ramapuram 4 , Devon Hjelm, Paul Dixon, JoÃ£o Monteiro, 3 , Pau Rodriguez Lopez, Lokesh Boominathan, Nikhil 2 6 2 0 F 5 2 ] . [ 1 2 7 4 1 2 . 2 0 6 2 : r 2 Apple, Google Deepmind (work done at Apple), 3 University of Cambridge, 4 MIT, Work done during Apple internship. Discrete diffusion models have emerged as strong alternatives to autoregressive language models, with recent work initializing and finetuning base unimodal model for bi-modal generation. Diverging from previous approaches, we introduce the first tri-modal Masked Diffusion Models (MDM) pretrained from scratch on text, image-text, and audio-text data. We systematically analyze multimodal scaling laws, modality mixing ratios, noise schedules, batch-size effects and provide optimized inference sampling defaults. Our batch-size analysis yields novel stochastic differential equation (SDE) based reparameterization, eliminating the need for tuning the optimal batch size as reported in recent work. This reparameterization decouples the physical batch size, often chosen based on compute (GPU saturation, FLOP-efficiency, wall-clock time) from the logical batch size, chosen to balance the variance of gradients during stochastic optimization. Finally, we pretrain preliminary model showcasing the capabilities of unified design, achieving strong results at 3B model scale (6.4T tokens), in both text generation, T2I tasks, and T2S tasks. Our work represents the largest scale systematic open study of multimodal discrete diffusion models conducted to date, providing valuable insights into scaling behaviors across multiple modalities. Correspondence: Louis Bethune: louisbethune@apple.com; Victor Turrisi: v_turrisi@apple.com; Jason Ramapuram: jramapuram@ google.com. For detailed breakdown of author contributions see Section H."
        },
        {
            "title": "Introduction",
            "content": "A recurring theme in sequence modeling is that, whenever the full context is available, bidirectional information tends to perform better. Early work on bidirectional RNNs (Schuster and Paliwal, 1997) and LSTMs (Graves and Schmidhuber, 2005) demonstrated clear gains over purely forward recurrent models when both past and future states were accessible during training. This makes the dominance of causal transformers (Vaswani et al., 2017) in modern language modeling slightly surprising: the strongest transformer-based language models (DeepSeek-AI, 2024; Gemini Team, 2023; Singh et al., 2025; Anthropic, 2024) are trained with strictly left-to-right factorization (auto-regressively). The causal constraint is undeniably convenient (simple likelihood factorization, efficient per-token learning signal and fast streaming decoding via KV cache), but it is not evidently the best fit for conditional generation problems where the observed evidence may be scattered across positions and modalities. Discrete diffusion revisits the bidirectional viewpoint by replacing fixed generation order with iterative refinement: rather than predicting the next token, the model repeatedly denoises partially corrupted sequence. Recent progress in diffusion based language modeling (Nie et al., 2025) has narrowed much of the quality gap to strong causal baselines, including widely used models such as LLaMA 3 (Llama Team, 2024), strengthening the case that order agnostic, globally conditioned generation can be competitive at scale. Despite narrowing the performance gap at equivalent pretraining FLOP budgets, naive implementations still exhibit substantial latency compared to autoregressive baselines and further work is required to improve sampling efficiency in MDM (Jazbec et al., 2025; Wu et al., 2025). This refinement perspective is particularly appealing in multimodal settings. MDMs train on simple corruption process (masking) and learn to reconstruct missing tokens. With multimodal tokenization, text, image, and audio 1 (a) (b) (c) (d) Figure 1 High-fidelity generation. The pretrain-only 3B MDM demonstrates strong prompt adherence alongside high-quality visual rendering of texture, lighting, and composition. Samples show: (a) natural daylight and depth of field (\"egg in field of crocuses\"); (b) fine-grained fur texture in B&W (\"lions face\"); (c) soft, warm lighting with vintage color tones (\"preparing bread dough\"); and (d) complex multi-object arrangement (\"noodle soup with toppings\"). Extended generations in Appendix Section G. tokens can be concatenated, partially masked, and jointly denoised. This naturally supports infilling and arbitrary conditioning without re-deriving new factorization for every task. Although image and audio tokens could be independently modeled in continuous domains as in Li et al. (2025), we instead adopt discrete modeling to streamline optimization and substantially reduce complexity by employing unified embedding space and loss function. While much of the current multimodal MDM literature emphasizes adapting existing pretrained models, either by performing supervised finetuning on discrete diffusion bases like LLaDA (You et al., 2025; Yang et al., 2025) or by distilling and repurposing autoregressive backbones such as Qwen 2.5-Coder or other large AR models (Hui et al., 2024; Gong et al., 2025; Zheng et al., 2025; Zhang et al., 2025; Bie et al., 2025), our work targets the pretraining regime, where the dominant compute is spent and where the latent spaces are shaped. Moving from unimodal to multimodal MDM introduces large and underexplored design space. Choices that may appear secondary in isolation can dominate stability and compute efficiency. Because exhaustive large-scale sweeps are often infeasible, progress depends on reliable transfer rules, i.e. rules on how to transfer hyperparameters from small models to larger models. In this work, we take step toward sound pretraining recipes for native multimodal MDMs. We extend MDM to tri-modal setting (text, image, audio) via unified discrete token space with modality-specific boundary and mask tokens. The single resulting model supports multiple conditional generation queries, including text-to-image generation, image captioning, text-to-speech (TTS) and automatic speech recognition (ASR). Our main contribution is an empirical study of the pretraining and inference choices that govern scaling behavior and efficiency in this regime, together with controlled inference-time ablations that reveal modality-specific sensitivities: 1. Unified Multimodal MDM. Introduce the first tri-modal MDM capable of generating text, image, and audio from each other in any direction via single transformer backbone and unified vocabulary, eliminating the need for modality-specific heads, adapters or unimodal conditioning (Liu et al., 2023). 2. Elimination of Optimal Batch Size (ğµopt) and per-module transfer. We leverage SDE-based reparameterization to render training loss invariant to batch size up to critical threshold (ğµcrit), eliminating the need to search for an optimal batch size, ğµopt (Bergsma et al., 2025). We also validate the effectiveness of per-module (e.g., MLP, attention weights) hyperparameter scaling using CompleteP + SDE scaling (Mlodozeniec et al., 2025) within the multimodal MDM regime (Appendix D). 3. Multimodal Scaling Laws. We derive empirical scaling laws for validation loss as function of model size (ğ‘ ) and token budget (ğ·), providing prescriptive guidance for compute optimal tri-modal MDMs. We find the seminal formula ğ¿(ğ‘ , ğ·) = ğ¸ + (ğ´ğ‘ ğ‘/ğ‘ + ğµğ· 1)ğ‘ from Kaplan et al. (2020) to be better fit than the additive form of Hoffmann et al. (2022). In particular, we find these models to be asymptotically more data efficient than their auto-regressive counterpart, with the compute optimal frontier of ğ·(ğ‘ ) 7754 ğ‘ 0.84. 4. Modality Dependent Design Space. We characterize the distinct inference tradeoffs for each modality, identifying that optimal noise schedules and sampling parameters (guidance, temperature) differ significantly between text, image, and audio generation."
        },
        {
            "title": "2.1 Masked Diffusion Models",
            "content": "Although diffusion models first gained prominence through their success in continuous settings such as image generation (Song et al., 2021), the original formulation by Sohl-Dickstein et al. (2015) provided unified framework encompassing both continuous and discrete domains. One form of diffusion on discrete data, MDMs were first proposed by Austin et al. (2021), and generalized in earlier discrete diffusion work by Hoogeboom et al. (2021). In their formulation, the diffusion forward steps, ğ‘(ğ‘¥ğ‘¡ ğ‘¥ğ‘¡ 1), progressively noise the data ğ‘¥0 with mask tokens [MASK], turning the data distribution ğ‘0 := ğ‘ğ‘‘ğ‘ğ‘¡ğ‘ into stationary distribution ğ‘ğ‘‡ := ğ‘(ğ‘¥ğ‘‡ ) in which every token is masked. masked diffusion model ğ‘ğœƒ (ğ‘¥ğ‘¡ 1ğ‘¥ğ‘¡ ) with parameters ğœƒ then learns the reverse process such that starting from the stationary masked distribution ğ‘ğ‘‡ , the reverse process (cid:206)ğ‘¡ ğ‘ğœƒ (ğ‘¥ğ‘¡ 1ğ‘¥ğ‘¡ ) reconstructs the original text from such noised sequences. This approach for masked diffusion with fixed timesteps ğ‘¡ 0, 1, . . . ,ğ‘‡ was extended to continuous-time framework by Campbell et al. (2022), who formulated the forward noising process and corresponding reverse process as Continuous Time Markov Chains (CTMCs), and by Dieleman et al. (2022), who propose embedding discrete inputs into continuous space before learning the diffusion model. Below, we provide an overview of applications of MDMs to the three modalities we tackle in this work. Text. Austin et al. (2021) first applied MDMs to relatively small-scale text datasets like LM1B (Chelba et al., 2014). Then, recent adaptations of the continuous-time framework by Campbell et al. (2022) have enabled training of larger language diffusion models. LLaDA (Nie et al., 2025), for instance, trained an 8B-parameter MDM on 2.3T token corpus, obtaining strong performance on benchmarks such as MMLU (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021). In contrast, Dream (Ye et al., 2025) finetuned pretrained autoregressive Qwen2.5-7B model using 580B token corpus (Yang et al., 2024a), without accounting for the initial AR models pretraining budget. Both methods employ the same training objective, representing an upper bound on the negative log-likelihood, or variational evidence lower bound (ELBO), of the continuous-time masked diffusion process: Eğ‘¥0ğ‘ğ‘‘ğ‘ğ‘¡ğ‘, ğ‘¡ ğ‘ˆ (0,1) ğ‘¥ğ‘¡ ğ‘ (ğ‘¥ğ‘¡ ğ‘¥0 ) (cid:34) ğ‘¤ (ğ‘¡) ğ¿ â„“=1 1ğ‘¥ â„“ ğ‘¡ =MASK ğ‘ğœƒ (ğ‘¥ â„“ 0ğ‘¥ğ‘¡ ) (cid:35) . (2.1) Here ğ¿ is the sequence length and ğ‘¥ â„“ denotes the â„“th token of ğ‘¥. The indicator function 1ğ‘¥ â„“ ğ‘¡ =MASK makes sure the loss is computed only over masked tokens. The weights ğ‘¤ (ğ‘¡) depend on the form of the forward noise ğ‘(ğ‘¥ğ‘¡ ğ‘¥ğ‘  ); for LLaDA, it is ğ‘¤ (ğ‘¡) = 1/ğ‘¡, while Dream uses more complex schedule. Follow up works such as Zhu et al. (2025a) and Zhu et al. (2025b) improve performance and efficiency by introducing variance reduction and mixture-of-experts (MoE) methods to large language diffusion. We provide principled exposition of weighting in Appendix A. Image. Austin et al. (2021) and Shi et al. (2024) apply masked diffusion directly to pixel values by modeling them as categorical variables. However, their experiments are restricted to low resolution image datasets, such as CIFAR10 and downsampled ImageNet 64x64 (Deng et al., 2009; van den Oord et al., 2016). MaskGIT and VQ-Diffusion (Chang et al., 2022; Gu et al., 2022) instead use pretrained image tokenizers such as VQ-GAN (Esser et al., 2021; ai-forever, 2023; Zheng et al., 2022) or VQ-VAE (van den Oord et al., 2017) to downsample the full set of image pixels into patch-wise grid of discrete tokens, enabling stable high-resolution discrete image diffusion training. Audio. Literature around speech and audio generation is generally scarcer. Yang et al. (2023) apply discrete diffusion to audio generation. Borsos et al. (2023) combine the SoundStream audio tokenizer (Zeghidour et al., 2022) with masking-unmasking approach similar to MaskGIT (Chang et al., 2022) for audio generation."
        },
        {
            "title": "2.2 Multimodal Masked Diffusion Models",
            "content": "Some elements of multimodality were introduced by works such as Gu et al. (2022) (text-to-image generation) and You et al. (2025) (visual question answering). However, these models are still restricted to generating only one modality. In contrast, by applying unified probabilistic formulation that represents heterogeneous data as single stream of concatenated discrete tokens, MMaDA (Yang et al., 2025) unifies language modeling, image understanding, and image generation as multimodal MDM. MMaDA is initialized from LLaDAs weights and subsequently trained 3 Figure 2 Tri-Modal masked diffusion model architecture. Pure text is packed. Image-caption and audio-transcription pairs are padded to maximum length. Padding is ignored by attention and loss computation. Figure 3 Token-optimal curve ğ·(ğ‘ ) for different model families. In tri-modal MDM, token count growth sub-linearly with model size, suggesting diminishing returns of additional data. We use identical methodology to report all curves. with the same objective as LLaDA and Dream in Equation (2.1) but on the joint token stream of image and text tokens. Swerdlow et al. (2025) and Hu et al. (2023) train unified discrete diffusion model for both image and text at smaller scale, with the latter using mix of masked and uniform-state diffusion. We move beyond existing bimodal (textimage) MDMs by introducing audio as novel third modality, addressing new multimodal challenges. Unlike Yang et al. (2025), we pretrain from scratch, properly account for the total token budget, and jointly optimize representations across all modalities. We encourage the community to jointly report model and data size, (ğ‘ , ğ·total), to make fair comparisons between hypothesis classes."
        },
        {
            "title": "3 Method",
            "content": "We consider three modalities ğ‘š := {text, audio, image}. Each training sample belongs to one of three categories: text-only, image-text pairs or audio-text pairs, where each modality is represented as sequence of discrete tokens drawn from modality-specific vocabulary, ğ‘¥ğ‘š = (ğ‘¥ 1 ğ‘š, . . . , ğ‘¥ ğ¿ğ‘š ğ‘š ), ğ‘¥ğ‘– ğ‘š Vğ‘š, where Vğ‘š = ğ‘‰ğ‘š. To enable unified modeling, we construct shared vocabulary = Vtext Vaudio Vimage, where denotes disjoint union. We further introduce modality-specific special tokens Vspec = {BOSğ‘š, EOSğ‘š, MASKğ‘š : ğ‘š M}, as well as padding token PADtext that is added after the text prompt of multimodal sequences that are shorter than our target sequence length. Lastly, we introduce three special task tokens Vtask = {TASKtext, TASKimage-text, TASKaudio-text}, that allows us to signal to the model which task it needs to perform. This is especially useful if one wishes to add new tasks to the model through mid-training or supervised finetuning. The resulting unified vocabulary has size ğ‘‰ = (cid:205)ğ‘š ğ‘‰ğ‘š + ğ‘‰spec + ğ‘‰task + 1. 4 To avoid any confusion regarding modalities and diffusion time, we switch our notation from the generic signal ğ‘¥ to the full training sequence ğ‘ . Throughout the paper, superscripts denote position indices (i.e., token positions), while subscripts denote diffusion time or modality indices. Training. Training sequences are constructed by wrapping modality tokens with their boundary tokens. For example, an audio-text sample is represented as: ğ‘  = [TASKaudio-text, BOSaudio, ğ‘¥ 1 audio, . . . , ğ‘¥ ğ¿audio audio , EOSaudio, BOStext, ğ‘¥ 1 text, . . . , ğ‘¥ ğ¿text text , EOStext], with an image-text sample following the same format. Since we train with maximum sequence length ğ¿, text-only sequences are packed and truncated so that they always have exactly ğ¿ positions, i.e., no padding is necessary. On the other hand, mixed-modality sequences whose total length is shorter than ğ¿ are right-padded after EOStext using the token PADtext to match ğ¿. See Figure 2 for demonstration. Given sequence ğ‘  = (ğ‘ 1, . . . , ğ‘ ğ¿ ) Vğ¿, we define continuous-time forward masking process indexed by ğ‘¡ [0, 1]. Each position is independently corrupted according to Bernoulli masking mechanism with probability ğ›½ğ‘¡ , where ğ›½ denotes monotonic function with ğ›½0 = 0 and ğ›½1 = 1. Let ğ‘š(ğ‘–) denote the modality associated with position ğ‘–. The corrupted token ğ‘ ğ‘– ğ‘¡ is sampled as ğ‘¡ ğ‘ ğ‘– ğ‘ ğ‘– ğ‘¡ 1 (cid:40)MASKğ‘š (ğ‘– ) with probability ğ›½ğ‘¡, ğ‘ ğ‘– ğ‘¡ 1 ğ‘¡ , . . . , ğ‘ ğ¿ ğ‘¡ with probability 1 ğ›½ğ‘¡ . This defines corrupted sequence ğ‘ ğ‘¡ = (ğ‘ 1 ) with ğ‘ 0 = ğ‘ . Masking is applied independently across positions and modalities, with each modality using its own dedicated mask token MASKğ‘š. Note that the task tokens are never masked. The parameter ğ‘¡ controls the overall corruption level, smoothly interpolating between the original sequence at ğ‘¡ = 0 and fully masked sequence at ğ‘¡ = 1. For position ğ‘– associated with modality ğ‘š(ğ‘–), the forward noising kernel is given by ğ‘(ğ‘ ğ‘– ğ‘¡ ğ‘ ğ‘– ) = ğ›½ğ‘¡ ğ›¿MASKğ‘š (ğ‘– ) (s ğ‘– ğ‘¡ ) + (1 ğ›½ğ‘¡ ) ğ›¿sğ‘– (s ğ‘– ğ‘¡ ) , where ğ›¿ğ‘£ () denotes the Dirac measure centered at ğ‘£, and ğ›½ğ‘¡ = (cid:206)ğ‘¡ ğ‘¡ ğ›½ğ‘¡ . Since masking is applied independently across positions, the forward process factorizes as ğ‘(ğ‘ ğ‘¡ ğ‘ ) = ğ¿ (cid:214) ğ‘–=1 ğ‘(ğ‘ ğ‘– ğ‘¡ ğ‘ ğ‘– ). Equivalently, this induces Markov kernel between successive noise levels ğ‘(ğ‘ ğ‘¡ ğ‘ ğ‘¡ 1) = ğ¿ (cid:214) ğ‘–=1 (cid:104) ğ›¼ğ‘¡ ğ›¿ğ‘ ğ‘– ğ‘¡ 1 ğ‘¡ ) + (1 ğ›¼ğ‘¡ ) ğ›¿MASKğ‘š (ğ‘– ) (ğ‘ ğ‘– (ğ‘ ğ‘– ğ‘¡ ) (cid:105) , where ğ›¼ğ‘¡ = 1 ğ›½ğ‘¡ is chosen such that the marginal distribution of ğ‘ ğ‘¡ matches ğ‘(ğ‘ ğ‘¡ ğ‘ ) (see Section for more details). The monotonic nature of the masking process is clear: once token is masked, it stays masked. Denoising model. We parameterize the reverse process using denoising model ğ‘“ğœƒ : Vğ¿ Rğ¿ğ‘‰ which predicts logits over the unified vocabulary at each position. Given corrupted sequence ğ‘ ğ‘¡ , the model outputs â„ = ğ‘“ğœƒ (ğ‘ ğ‘¡ ), ğ‘£ denotes the logit assigned to token ğ‘£ at position ğ‘–, and the ground-truth token ğ‘ ğ‘– , we define the where â„ğ‘– per-token loss as â„“ğ‘– (ğœƒ, ğ‘ ) = log exp(cid:0)â„ğ‘– (cid:1) ğ‘ ğ‘– ğ‘£ (cid:1) (cid:205)ğ‘£ exp(cid:0)â„ğ‘– := log ğ‘ğœƒ (ğ‘ ğ‘– ğ‘ ğ‘¡ ). For memory efficiency and to enforce modality constraints, we implement this loss using cut-cross-entropy (CCE) (Wijmans et al., 2024), which avoids materializing the full probability distribution. Let Iğ‘¡ := { ğ‘– ğ‘ ğ‘– ğ‘¡ = MASKğ‘š (ğ‘– ) } denote the set of masked, non-padding positions at time ğ‘¡. The training objective is (ğœƒ ) = ğ‘ D, ğ‘¡ U(ğœ–,1) (cid:34)ğ‘¤ (ğ‘¡) Iğ‘¡ ğ‘– Iğ‘¡ (cid:35) â„“ğ‘– (ğœƒ, ğ‘ ) + ğœ† Lğ‘§, where ğœ–, ğœ† > 0 are small constants for numerical stability, and Lğ‘§ denotes the z-loss regularizer (de BrÃ©bisson and Vincent, 2016). We follow prior work (Nie et al., 2025; Yang et al., 2025) on masked diffusion models, and choose ğ‘¤ (ğ‘¡) = 1/ğ‘¡ which yields an unbiased estimator of the ELBO under Bernoulli masking. See Section for the effect of this weighting scheme and more general formulation. Inference. At generation time, we iteratively unmask tokens according to predefined linear schedule. Multimodal generation is conditioned on prompt, e.g., text, and target modality ğ‘š M. We start the process from fully masked sequence of the form ğ‘ ğ¾ = [TASKtask, BOSğ‘š, MASKğ‘š, . . . , MASKğ‘š, EOSğ‘š, BOStext, ğ‘¥ 1 text, . . . , ğ‘¥ ğ¿text text , EOStext]. For text-only generation, we instead construct the fully masked sequence of the form ğ‘ ğ¾ = [TASKtext, BOStext, ğ‘¥ 1 text, . . . , ğ‘¥ ğ¿text text , MASKtext, ..., MASKtext, EOStext]. At each reverse diffusion step ğ‘˜ [ğ¾], where ğ¾ denotes the number of generation steps, the denoising model produces â„ğ‘˜ = ğ‘“ğœƒ (ğ‘ ğ‘˜ 1), where â„ğ‘– ğ‘˜ RV denotes the logits at position ğ‘–. For each masked position ğ‘–, candidate token is sampled from the modality-constrained predictive distribution ğ‘˜ ğ‘ğœƒ ( ğ‘ ğ‘˜ 1) exp(cid:0)â„ğ‘– ğ‘ ğ‘– ğ‘£ Vğ‘š (ğ‘– ) . ğ‘˜,ğ‘£ (cid:1), Based on the unmasking schedule, subset of masked positions are revealed, updating the sequence ğ‘ ğ‘˜ . The process is repeated until no masked positions remain, producing the final generated sample."
        },
        {
            "title": "3.1 Architecture",
            "content": "For all experiments presented in this paper, we rely on standard bi-directional transformer architecture with prenormalization RMSNorm (Zhang and Sennrich, 2019), SwiGLU MLPs (Shazeer, 2020), rotary positional embeddings (RoPE) (Su et al., 2021) and QK-norm (Dehghani et al., 2023; Wortsman et al., 2024; Chameleon Team, 2024; Gemma Team, 2025). Our Tri-modal 3B model is pretrained from scratch for 1M steps with batch size of 3072 and sequence length of 3256. We tokenize modalities with specialized encoders: SBER-MoVQGAN (ai-forever, 2023) for images, Higgs Audio v2 (Boson AI, 2025) for audio, and Tiktoken (OpenAI, 2023) for text. To manage the large vocabulary efficiently, we employ cut-cross-entropy (Wijmans et al., 2025) and apply z-loss regularizer (de BrÃ©bisson and Vincent, 2016) to stabilize logits amplitudes. See Table 5 for full hyperparameter details."
        },
        {
            "title": "4 Hyperparameter Transfer",
            "content": "Selecting optimal hyperparameters is of paramount importance to the final performance of the model and conducting grid search at large scale is not feasible. In this work, we rely on hyperparameter transfer rules to transfer the optimal set found at small scale to larger scale. Several rules have been proposed in the literature: ğœ‡P (Yang et al., 2021) proposed scaling for width, later extending to depth with depth-ğœ‡P (Yang et al., 2024b), u-ğœ‡P (Blake et al., 2025) and CompleteP (Dey et al., 2025). Here, we rely on the work of Mlodozeniec et al. (2025), an extension of CompleteP (Dey et al., 2025). Additionally, Mlodozeniec et al. (2025) recently demonstrated performance gains from per-module hyperparameter optimization, adjusting multipliers for AdamW parameters (learning rate, weight decay, momenta ğ›½1, ğ›½2, and ğœ–) across distinct modules like MLP weights, attention projections, embeddings, and normalization layers. Our work provides the first empirical validation of this refined tuning in the context of multimodal MDMs, with preliminary results presented in Appendix D."
        },
        {
            "title": "4.1 Eliminating ğµopt with SDE Parametrization\nStochastic Optimization with AdamW operates like the discretization of a Stochastic Differential Equation (SDE) (Mal-\nladi et al., 2022; Mlodozeniec et al., 2025) whose timescale, noise, and drift, can be computed from AdamWâ€™s parame-\nters. According to these studies, AdamWâ€™s hyperparameters are redundant with batch size: for example, it is possible\nto reduce the noise in the gradient estimation, either by increasing the batch size or by decreasing the momentum\nweight. Similarly, lower noise allows for larger step sizes. Since batch size is typically constrained by the com-\npute budget and the memory available on the chip, it is desirable to make it a free hyperparameter whose value\ndoes not interfere with the performance of the model. We thus re-parametrize the hyperparameters in batch-size",
            "content": "6 with Mlodozeniec et al. (2025) to train the network with any batch size, guaranteeing similar performance across all compute budget, as long as the batch size is not larger than ğµcrit. Results are illustrated in Figure 4. SDE parametrization guarantees homogeneous behavior across batch sizes, including the smallest ones. This contrasts with the typical U-curve associated with non-SDE parametrization (Bergsma et al., 2025), where there is balance between total drift (when batch size is too big) and excessive noise (when batch size is too small)."
        },
        {
            "title": "4.2 Isonoise and Isohorizon Scaling Rules",
            "content": "The SDE is the continuous limit of the gradient flow elicited by AdamW. Intuitively, the SDE horizon corresponds to the trajectory length in parameter space (extending from origin), while SDE drift controls the scale of stochastic fluctuations induced by gradient noise. We propose new way to balance these two contributions, controlled by parameter ğ›¾ [0, 1]. When increasing the number of tokens ğ·, two quantities can be conserved: (a) we can conserve the SDE drift (isonoise curves) with ğ›¾ = 0, or (b) we can conserve the SDE horizon (isohorizon curves) with ğ›¾ = 1. We smoothly interpolate between these extremes for intermediate values (0 < ğ›¾ < 1) by defining the SDE-scaling factor ğœ… as: ğœ… = (cid:18) ğ· base ğ· (cid:19)ğ›¾ (cid:18) ğµ (cid:19) ğµbase , (4.1) where ğ· base is the base model size, ğ· is the target model size, ğµbase is the base batch size and ğµ the target batch size. Then, AdamWs hyperparameters are rescaled using ğœ… as: lr = lrbase ğœ…, ğ›½1 = (ğ›½ base )ğœ…, ğ›½2 = (ğ›½ base 2 )ğœ…, ğœ– = ğœ–base ğœ… . (4.2) We conduct an initial hyperparameter search of (lrbase, ğ›½ base cluding 240M embedding parameters), ğ· base = 13B tokens, and global batch size of ğµbase = 256 sequences. , ğœ–base) with 3k runs with N=320M model (in- , ğ›½ base 2 Figure 4 Below the critical batch size ğµcrit the SDE parametrization guarantees constant loss. In that regime, larger batch sizes allow fewer iterations. Above it, SDE discretization breaks and training ceases to be FLOP-efficient. Figure 5 Critical iteration count ğ‘†crit is constant w.r.t. model size under the SDE regime. This is compatible with the findings of Bergsma et al. (2025), but their study was done outside the SDE regime."
        },
        {
            "title": "5 Scaling Behavior of MDM under the SDE Transfer Rule",
            "content": "This section is devoted to the scaling properties of tri-modal MDM, under the CompleteP + SDE reparametrization regime. All experiments presented use cosine learning rate schedule with 1k steps of linear warmup, constrained so that warmup never exceeds 25% of the total iteration count. Following Busbridge et al. (2025), we set width proportional to depth, fixing ğœŒ = ğ‘‘emb/ğ‘›layers = 128 while scaling up models, ensuring consistent hyperparameter transfer and more stable scaling behavior. 7 Figure 6 Critical iteration count ğ‘†crit increases with token horizon ğ·. The increase is sub-linear, meaning that the critical batch size ğµcrit also increases with horizon ğ·. Figure 7 Critical batch-size ğµcrit as function of the token horizon. There is an intrinsic tension between wallclock time and FLOP-efficiency."
        },
        {
            "title": "5.1 Scaling Rules for Critical Batch Size\nCritical batch size without SDE. When not using SDE parametrization, the batch size impacts both the variance of the\nstochastic gradient estimation (the SDE drift), and the iteration count ğ‘† (the SDE horizon). Practically, it means that\nbeyond ğµcrit the marginal utility of each additional token in the batch decreases. Previous work in AR (Kaplan et al.,\n2020; DeepSeek-AI, 2024) modeling fit a power law to enable predicting critical batch size in tokens ğ· or compute\nbudget ğ¶. More recent work (Bergsma et al., 2025) suggests there exists a ğµopt: the batch size that minimizes the loss\nat a given token horizon ğ·. Under SDE parametrization, that notion disappears, as shown in Figure 4: all batch sizes\nunder ğ‘†crit yield identical results at fixed token budget ğ·. See Appendix Section C.2 for more details.",
            "content": "ğ‘†crit is estimated empirically as the minimum number of optimization steps required Critical batch size for SDE. to maintain FLOP-efficient training. When the number of integration steps ğ‘†crit is too low, the SDE approximation breaks and the performance at constant horizon ğ· plummets. This can be expressed in term of the critical batch size ğµcrit = ğ·/(ğ¿ğ‘†crit) with ğ¿ being the sequence length. Above ğ‘†crit the asymptotic loss depends mainly on the model size ğ‘ and token budget ğ·, irrespective of iteration count. We illustrate this phenomenon in Figure 4 with model of size 320M trained on 13B tokens. This means that below ğ‘†crit, all runs are FLOP-efficient: they minimize the loss for the token budget. Above that, runs are wall-clock inefficient: they trade faster training for wasteful usage of tokens."
        },
        {
            "title": "Takeaways",
            "content": "Under SDE parameterization, the global batch size can be chosen according to the available compute budget, to saturate the capacity of each GPU, without compromising the final loss, as long as it is less than ğµcrit. In Figure 5 we plot the final exp (ELBO) as function of the iteration Critical batch size as function of model size. count ğ‘† and models size ğ‘ , for constant token horizon ğ· of 13B tokens. We see that the critical iteration count ğ‘†crit is independent of model size. This is compliant with the findings of Bergsma et al. (2025), albeit in the non-SDE case. The maximum per-GPU batch size typically decreases with model size until it reaches 1, after which it requires more involved techniques (e.g., more fine-grained parallelization). Therefore, maintaining the same global batch size typically require more nodes as model size increases."
        },
        {
            "title": "Takeaways",
            "content": "Under SDE scaling, critical batch size is not impacted by model size. At equal token horizon, batch size less than ğµcrit for smaller model is also safe for bigger model. 8 In Figure 5 we plot the final exp (ELBO) as function of the Critical batch-size as function of the token horizon. iteration count ğ‘† and model size ğ·, for constant model size ğ‘ of 320M parameters. We see that ğ‘†crit grows with ğ·, at sub-linear speed, implying that the corresponding critical-batch size grows sub-linearly with the token horizon. Takeaways The number of GPUs cannot be scaled proportionally with ğ· forever: there is risk of reaching ğµcrit when ğ· is sufficiently large. Beyond that point, either the marginal utility of every new token drops, or total wall-clock time increases. Over-trained models cannot be both FLOP-efficient and fast to train."
        },
        {
            "title": "5.2 Optimal Driftâ€“horizon Tradeoffs",
            "content": "Normally, the physical batch size ğµ can be chosen freely to accommodate the number of nodes available, as long as ğµ ğµcrit. The SDE re-parametrization allows to re-map this to virtual batch size ğµ and corresponding virtual number of iterations ğ‘† such that ğ· = ğµ ğ‘†ğ¿. They correspond to the behavior of the same model trained without SDE parametrization, and physical batch size ğµ. This raises question: since the physical batch size ğµ is chosen based on compute considerations only, how should we chose the virtual batch size ğµ when ğ· is scaled-up? To answer the question, we design an experiment in which they evolve jointly as: ğ‘† = ğº (ğ·/ğ¿)1ğ›¾ and ğµ = ğº 1 (ğ·/ğ¿)ğ›¾, with ğ›¾ = ğ›¼/(ğ›¼ + ğ›½) and ğº = (cid:19) 1/(ğ›¼+ğ›½ ) . (cid:18) ğ›¼ğ´ ğ›½ğµ The behavior ğ›¾ = 0 correspond to the default setup of the literature in training LLMs (Kaplan et al., 2020), when more tokens simply correspond to more iterations, whereas the setup ğ›¾ = 1 is the choice made in the work of Mlodozeniec et al. (2025) that instead modulates the optimization hyperparameters (subsection 4.2). By sweeping over [0, 1] we decide how many of these extra-tokens are assigned to reducing the drift, or to extending the horizon. Results are given in Figure 8 and Figure 9. Surprisingly, neither setting used in the literature are optimal. By fitting power law of the form ğ¸ + ğ´ ğ‘† ğ›¼ + ğµ ğµ ğ›½ we find coefficients ğ›¼ = 0.18, ğ›½ = 0.23. Minimizing this parametric equation under the constraint ğ· = ğµ ğ‘†ğ¿, we find that ğ›¾ = 0.44. Figure 8 Drifthorizon tradeoff ğ›¾. When increasing token horizon ğ·, the optimal choice lies between reducing the drift (increasing the virtual batch size) and increasing the SDE horizon (increasing the virtual number of iterations). Figure 9 Isotoken curves at various virtual batch sizes. The optimal allocation between virtual batch size and virtual iteration count corresponds to factor ğ›¾ 0.44. The bottom of the U-curve would be ğµopt in non-SDE parametrization. For ğ›¾ < 1, the effective learning rate rises with the physical batch size, leveraging Link with learning rate schedule. the lower variance in gradient estimates for larger updates. Conversely, when ğ›¾ > 0, longer token horizon reduces the effective learning rate, permitting smaller incremental steps and deeper exploration of loss basins. Thus, ğ›¾ plays role akin to learning rate scheduler, determining the allocation of larger versus smaller updates. Consequently, the optimal ğ›¾ could shift if, for example, warmup-stable-decay schedule (HÃ¤gele et al., 2024; Schaipp et al., 2025) were used instead of the current cosine schedule. 9 Figure 10 Training curves for tri-modal MDM. We select 24 log-distributed isoFLOPS between 5e18 and 1e22 to cover the (ğ‘ , ğ·) grid. The performance is dominated by the total compute budget ğ¶, following formula in appendix H.1 of Busbridge et al. (2025). The loss is computed on an independent validation set with identical mixture weights."
        },
        {
            "title": "5.3 Scaling Laws for Tri-modal MDM",
            "content": "Scaling laws provide prescriptive mechanism to decide the model size (ğ‘ ) and the number of tokens (ğ·) in compute optimal manner. They are obtained by fitting power laws from raw training curves, as in Figure 10. One of the core contributions of our work is in establishing this for tri-modal MDMs that have been scaled under CompleteP with SDE re-parametrization. We train 262 different tri-modal MDM models with Token Per Parameter (TPP) ratios between 1 and 2000. We sample the (ğ‘ , ğ·) pairs along 24 different isoFLOPs logarithmically distributed between 5 1018 and 1 1022. The models size here does not account for embedding parameters (Pearce and Song, 2024), which strongly impacts smaller models as shown in Figure 12. FLOPs per token are computed using the formula in Appendix H.1 of Busbridge et al. (2025). Modality tokenizers are not taken into account for the total FLOP budget. Following Shukor et al. (2025), we fit power law using basin hopping and LBFGS, with 20 bootstrap samples with 90%- 10% cross-validation, to ensure stability of coefficients estimation. We found that the additive form was insufficient to explain the measurements, and had to rely on the form introduced in the seminal work of Kaplan et al. (2020), with an additional ğ¸ term to avoid loss degeneracy in the ğ‘ , ğ· limit. ğ¿ = ğ¸ + (cid:18) ğ´ ğ‘ ğ‘/ğ‘ + ğµ ğ· (cid:19)ğ‘ . (5.1) We report scaling coefficients of ğ‘ 0.14 and ğ‘ 0.17 in Figure 11, using the method of Section E. We measure an ğ‘…2 score of 99.3% and an MRE of 0.5%. We report isoloss contours in Figure 13 and isoFLOP curves in Figure 14. Finally, we compute optimal number of tokens per parameter as: ğ·(ğ‘ ) = 7754 ğ‘ ğ›¼ with ğ›¼ = ğ‘/ğ‘ = 0.84. (5.2) We report the compute-optimal token count ğ·(ğ‘ ) as function of model size against other popular model families in Figure 3. For all compute-optimal curves, we use the same methodology: we plot token horizon ğ· as function of total parameter count, and rely on approach 3 of Chinchilla with corrected coefficients from Besiroglu et al. (2024). We find that 3B model requires at least 480B tokens, in sharp contrast with 60B tokens reported by Chinchilla (Hoffmann et al., 2022) for autoregressive language models. This gap is maintained at all realistically reachable model sizes. Analysis. These scaling laws suggest that as ğ‘ grows, the TPP ratio ğ·/ğ‘ ğ‘ 0.16 decreases, i.e., MDMs become asymptotically more data-efficient per parameter. This is in contrast to AR language models, for which the rule of thumb ğ· 20ğ‘ popularized by Hoffmann et al. (2022) typically holds. The value of ğ‘ 0.17 is compatible with the one found in experiment of Section 4.1, on iteration count ğ‘†. The optimal compute allocation between tokens and parameters is then given by: ğ‘ (ğ¶) ğ¶0.55 and ğ·(ğ¶) ğ¶0.45. (5.3) Our coefficients are slightly higher for ğ‘ than they are for ğ·, suggesting diminishing returns of additional data when increasing model size. However, this asymptotic trend is offset at practical scales by the large leading constant (ğ· = 7754 ğ‘ 0.84): 3B model still requires 480B tokens, far exceeding the 60B implied by Chinchilla for autoregressive Figure 11 Scaling law fit for tri-modal MDMs using Equation 5.1 inspired by Kaplan et al. (2020). ğ‘…2 score of 99.3% and an MRE of 0.5%. ğ‘ and ğ· are expressed in billions units. Figure 12 Percentage of transformer-block parameters in the total parameter count. Tri-modality forces larger vocabulary, yielding ratio below 50% for small models. models. The crossing with Quokka (Ni et al., 2025) compute-optimal curves happens around the 20B scale ( 2T tokens). Below that crossing point, at equal FLOPs, models from our tri-modal MDM family should be smaller and trained for longer than the ones of Quokka. Above that crossing point, the trend reverses. Interpretation. While ğ‘ and ğ‘ coefficients inform on the relative effectiveness of parameter and data scaling within the same family of methods, they are not sufficient in isolation to conclude superiority of method over another. For example, some models such as logistic regression can still exhibit favorable exponents in some regimes (Lin et al., 2024) despite low expressiveness. In general, the asymptotics are better characterized by the value of ğ¸, corresponding to the incompressible error rate: the intrinsic entropy of the dataset, plus an additional error term coming from the bias of the family of models considered. Moreover, the ELBO is informative within diffusion family but can be misleading across families, since different forward processes induce different likelihood bounds (Sahoo et al., 2026); similarly, the data-coefficient ğ‘ is sensitive to data composition, as repetitions reduce the effective token count and thus deflate scaling efficiency (Muennighoff et al., 2023). Figure 13 Isoloss contours for tri-modal MDM. Dashed line indicates direction in which the 0-shot hyperparameter transfer is done using CompleteP + SDE. Figure 14 IsoFLOPs for tri-modal MDMs. Solid lines indicate scaling law predictions. Points represent measurements and indicates the lowest loss achievable at each isoFLOP."
        },
        {
            "title": "6 Data",
            "content": "To further parallelize and simply our analysis we conduct all experiments (with the exception of the modality mixing ablation of Section 7.2) using 33% (pure text), 33% (image-text), 33% (audio-text) mixing ratio. Inside each modality, we use predetermined reweighting of each mixture component, balancing quality and diversity. For all experiments, the token horizon ğ· is smaller than the total dataset size, ensuring that we operate in the regime of single global epoch. Nonetheless, some small, high-quality sub-mixture components are repeated up to 4 times, well within estimated repetition tolerance (Prabhudesai et al., 2025). Text data. Our text corpus is an aggregation of Nemotron-CC (Su et al., 2025); DCLM (Li et al., 2024); some subsets of The Pile (Gao et al., 2020) including Wikipedia, HackerNews, Ubuntu IRC, Arxiv, DM-mathematics, Openwebtext; licensed StackOverflow data; and various high-quality synthetic data obtained from reasoning traces of Qwen-32B; and other licensed datasets. All corpora go through an additional level of cleaning and filtering to remove PII (Personally Identifiable Information) and other problematic content linked to licensing issues. Different splits of the same datasets with identical mixture ratios are used to compute the validation loss. Audio-text data. Our audio training data consist of 2M hours of audio scraped from the web and transcribed by Whisper (Radford et al., 2023). The data was extracted from larger dataset that was PII filtered to remove private information and underwent series of quality filters based on speech activity detection, dialogue detection, production quality, and production complexity (Tjandra et al., 2025). Image-text data. Our image training data consists of an aggregation of multiple image and recaptioned text datasets such as CC3M (Sharma et al., 2018), CC12M (Changpinyo et al., 2021), COYO (Byeon et al., 2022), and other licensed datasets from Manzano (Li et al., 2025). All samples are also PII filtered to remove private information."
        },
        {
            "title": "7 Results",
            "content": "First, we detail the benefits of this unified design at large scale in Section 7.1. Then, we systematically ablate key design choices for our tri-modal discrete diffusion model. We evaluate the impact of different modality mixing ratios (Section 7.2) and masking schedules during training (Appendix F). We examine inference-time hyperparameters for text-to-image generation and text-to-speech generation (Section 7.3). Lastly, we explore the usage of anti-masking during training, which consists of augmenting the batch by generating two masked samples per sample, where one is the opposite of the other (Section 7.4). All ablations are conducted independently, starting from the same setup."
        },
        {
            "title": "7.1 Unified 3B Tri-modal MDM",
            "content": "Modality Dataset and Metric Image generation & composition Train (eval seed) FID-Inception 10.41, FID-DINOv2 112.12. FID-Inception 10.06, FID-DINOv2 107.61. CC12M Single Obj 93.12, Two Obj 63.38, Counting 33.44, Colors 64.89, Position 11.50, GenEval Color Attr. 27.00, Overall 48.89. Language (LM Harness) Harness OpenBookQA 37.40, TruthfulQA MC2 40.76, BBH 24.97, MMLU 41.57, WinoGrande 64.69, ARC-Easy (norm) 72.52, HellaSwag (norm) 65.88, LogiQA2 (norm) 30.66, PIQA (norm) 71.55, ARC-Challenge (norm) 43.09. Audio generation Train (eval seed) FAD 0.218, WER 0.124, PQ 6.89, CU 6.20, CE 5.45, PC 1.89. LibriSpeech-PC FAD 0.368, WER 0.164, PQ 6.01, CU 5.34, CE 5.07, PC 3.01. Figure 15 Tri-modal 3B overview across image, text, and audio. Left: radar summary (larger radius indicates better normalized score; vertex labels are raw values). Normalization is mixed by metric type: bounded metrics use natural bounds (GenEval/LM percentages to 100, WER to 1, Audiobox to 10), while unbounded Frechet metrics (FID-Incept, FID-DINOv2, FAD) use ECDF calibration from references, with lower-is-better inversion ğ‘  = 1 ğ¹ (ğ‘¥), where ğ¹ is the ECDF. Right: raw unnormalized metrics. 12 We evaluate our pretrained-only 3B tri-modal MDM (see Table 5 for the complete list of hyperparameters) on different settings for each specific modality. For text benchmarks we use LM evaluation harness (LM harness) (Gao et al., 2023). For image benchmarks, we evaluate generation FID (Heusel et al., 2017), using both DINOv2-L (Oquab et al., 2024) and Inception-v3 (Szegedy et al., 2016) as feature extractors, computed on CC12M (Changpinyo et al., 2021) and the training data sampled with different evaluation seed (hereafter train (eval seed)), and the GenEval (Ghosh et al., 2023) evaluation suite. For audio benchmarks, we evaluate text-to-speech generation conditioned on groundtruth prompts and durations from the train (eval seed) and LibriSpeech-PC (Meister et al., 2023) using generation FAD (Kilgour et al., 2019; Gui et al., 2024), WER, and Audiobox Aesthetics (Tjandra et al., 2025) scores measuring four perceptual dimensions: Production Quality (PQ), Content Usefulness (CU), Content Enjoyment (CE), and Production Complexity (PC), as metrics. We present the broken down results for the individual modalities in the radar plot and table in Figure 15."
        },
        {
            "title": "7.2 Modality Mixing Ratios",
            "content": "Figure 16 Loss contours for tri-modal mixture coefficients, taking [1/3, 1/3, 1/3] as the reference point for the 0-level contour. We do not observe synergies between modalities at that model and data scale: they all compete for capacity and tokens. Understanding how to combine data is of critical importance for multimodal models. We take an empirical stride towards quantifying this by carefully constructing an experiment where we vary the global modality mixing ratios, {ğ‘¤text, ğ‘¤image, ğ‘¤audio}, which respectively control the amount of text, image-text and audio-text data that is present within the pretraining mixture. We launch total of 15 experiments with model of size 320M (including. 80M nonembedding ones) and 13B tokens. We set minimum of 20% per modality to avoid degeneracy with out-of-distribution modalities. Results are shown in Figure 16. Unsurprisingly, we find that the loss decreases as the corresponding mixture weight increases. However, we do not witness synergies between modalities at these scales: the average loss on modality is independent on the relative ratio of the two other modalities. Therefore, the default choice of [1/3, 1/3, 1/3] appears reasonable. While there exists prescriptive mechanisms to determine mixing ratios, they either fail to tackle multimodal models (Chen et al., 2026) or to account for data-repetition (Shukor et al., 2025)."
        },
        {
            "title": "7.3 Best Generation Hyperparameters",
            "content": "We evaluate the impact of four different inference hyperparameters on generation quality: classifier-free guidance (CFG) scale, temperature, top-ğ‘ sampling, and number of generation steps. Unconditional generation for CFG is achieved by setting the text prompt to fully masked state. Text-to-image generation. We generate 10,000 images conditioned on text prompts from CC12M and train (eval seed), using as default configuration of CFG=6.0, temperature=1.0, top-p=1.0, and 1024 generation steps, while ablating one hyperparameter at time. As shown in Figure 17, FID improves as the number of steps increases but with diminishing returns at higher step counts. For CFG, temperature, and top-p, optimal FID is achieved at intermediate values, with the exception of top-p which shows preference for higher values. Text-to-speech generation. We evaluate audio generation based on text prompts from our audio-text train (eval seed) dataset and LibriSpeech-PC. Here, we select 10,000 samples from each dataset and filter to retain only samples with duration 30 seconds, retaining approximately 70% of train (eval seed) samples and 99% of LibriSpeech-PC samples. This filtering ensures consistent evaluation, as audio samples were truncated to maximum token budget during training. Then, we use ground-truth durations for variable-length generation, allowing us to focus on the effect of sampling hyperparameters independent of duration prediction. Lastly, we ablated one hyperparameter at 13 Figure 17 Text-to-image hyperparameter ablations. We generate images from text prompts and compute FID against reference images on two datasets: CC12M in blue and train (eval seed) in orange. Top row (ad) shows FID computed using DINOv2-L features, bottom row (eh) shows FID using Inception-v3 features. Panels (a,e) vary generation steps; (b,f) vary CFG scale; (c,g) vary temperature; (d,h) vary top-ğ‘. Stars indicate optimal values for each metric-dataset combination. Figure 18 Text-to-speech hyperparameter ablations. We evaluate audio generation from text prompts on two datasets (train set, LibriSpeech-PC) using ground-truth durations, with three metrics: FAD, WER, and Audiobox Aesthetics. Top row shows (a) steps sweep and (b) CFG sweep. Bottom row shows (c) temperature sweep and (d) top-ğ‘ sweep. indicate the best hyperparameter for each metric. For Production Complexity, lower values indicate simpler audio and are considered as preferable. time while relying on default configuration of CFG=3.0, temperature=1.2, top-p=0.9, and 1000 generation steps. As shown in Figure 18, quality improves with more generation steps but with diminishing returns at higher step counts, similarly to image generation. CFG scale shows an interesting trade-off where increasing CFG strengthens text conditioning, improving transcription accuracy (WER), but degrades audio fidelity as captured by FAD. Stars indicate optimal hyperparameter values for each metric, where lower values are preferred for FAD, WER, and Production Complexity (simpler audio) (Tjandra et al., 2025), while higher values are preferred for the other aesthetics metrics. CFG, temperature, and top-p exhibit varying optimal values across different metrics. Trends broadly stay consistent between the two datasets, demonstrating good generalization to the external LibriSpeech-PC dataset."
        },
        {
            "title": "7.4 Anti-Masking",
            "content": "The stochastic nature of the MDM training strategy is known to induce high variance (von RÃ¼tte et al., 2025). To mitigate this, recent work introduced anti-masking (Jia et al., 2025; Gong et al., 2025), which stabilizes training by applying decorrelated masks to each batch input. More specifically, one samples standard mask and subsequently applies its negation to the same input, resulting in two masked versions of the same input sample. While this reduces variance in batch gradient estimates (Jia et al., 2025; Gong et al., 2025), it doubles the computational cost of training 14 as each sample in batch is masked twice. We therefore compare models under unique fixed token horizon ğ·. To ensure compute matching, the baselines are trained with regular masking for two epochs, while the anti-mask variations are trained for single epoch. To exemplify this setup, consider dataset where ğ· consists of 8 samples, and batch size of 4 is used. The anti-masking model would process the following sequence of batches: [1, 1, 2, 2], [3, 3, 4, 4], [5, 5, 6, 6], [7, 7, 8, 8], where denotes the repeated sample with complementary masking patterns. In contrast, standard training would iterate through the unique samples twice: [1, 2, 3, 4], [5, 6, 7, 8], [1, 2, 3, 4], [5, 6, 7, 8]. For simplicity, we show the same order of samples across two epochs, but in practice samples are re-ordered. We then conduct ablation studies on both text-only and multimodal architectures. The text models are approximately 7B parameters and are trained with budget of 100 unique tokens per parameter. The multimodal models, with roughly 1.3B parameters, are trained on horizon of 50 unique tokens per parameter. We evaluate the impact of anti-masking by comparing these models against their standard baselines, using subset of the LM Harness for text tasks and assessing generation quality for audio and images in the multimodal setting. Table 1 Multimodal ablation results comparing standard MDM training versus anti-masking. Model FID (Inception) Train Data CC12M Train Data CC12M Train Data FID (DINOv2) FAD LibriSpeech Base Anti-mask 18.69 17. 26.77 21.04 306.19 302.61 395.73 361.00 0.24 0.22 0.79 0.55 Results for the multimodal experiments are presented in Table 1, reporting FID for images, and FAD for audio. We observe that anti-masking yields positive impact on performance across modalities, with the most significant gains observed in audio generation quality. Results for the text-only models are provided in Table 2. We see consistent improvements across most the tasks. Table 2 Anti-masking results on the evaluation harness. We report mean accuracy standard deviation. OpenBookQA TruthfulQA BBH MMLU Winogrande ARC-Easy HellaSwag LogiQA2 PIQA ARC-Challenge Base Anti-mask 32.80 2.10 33.00 2. 43.12 1.45 36.97 1.41 21.52 0.46 27.05 0.50 30.20 0.39 32.86 0.39 52.01 1.40 54.06 1.40 63.72 0.99 66.04 0.97 52.20 0.50 55.15 0. 24.75 1.09 26.27 1.11 67.19 1.10 67.41 1.09 31.66 1.36 34.90 1."
        },
        {
            "title": "Takeaways",
            "content": "Anti-masking is simple yet effective approach to improve MDM pre-training in the multiple-epochs setup. Crucially, it improves benchmark performance without incurring additional computational cost."
        },
        {
            "title": "8 Conclusion",
            "content": "This work reframes multimodal generation as order agnostic iterative refinement by extending masked discrete diffusion from language to unified tri-modal setting, where text, images, and audio share single token stream and single transformer backbone, enabling flexible conditioning (captioning, text to image, ASR, TTS) without modality specific heads or bespoke factorizations. Beyond demonstrating feasibility, we chart the practical design space that governs stability and efficiency at scale: we show how SDE-based reparameterization reduce expensive tuning, we derive empirical scaling behavior to guide compute optimal training, and we surface strongly modality dependent inference landscape where sampling hyperparameters sampling (guidance, temperature, steps) must be chosen differently for different modalities. Lastly, targeted training interventions such as anti-masking yield consistent improvements under compute matched comparisons."
        },
        {
            "title": "References",
            "content": "Mike Schuster and Kuldip K. Paliwal. Bidirectional recurrent neural networks. IEEE Trans. Signal Process., 45(11):26732681, 1997. doi: 10.1109/78.650093. URL https://doi.org/10.1109/78.650093. Alex Graves and JÃ¼rgen Schmidhuber. Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks, 18(5-6):602610, 2005. doi: 10.1016/J.NEUNET.2005.06.042. URL https://doi.org/10.1016/j.neunet. 2005.06.042. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008, 2017. URL https://proceedings. neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. DeepSeek-AI. Deepseek-v3 technical report. CoRR, abs/2412.19437, 2024. doi: 10.48550/ARXIV.2412.19437. URL https://doi.org/10. 48550/arXiv.2412.19437. Gemini Team. Gemini: family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312. 11805. URL https://doi.org/10.48550/arXiv.2312.11805. Aaditya Singh, Adam Fry, Adam Perelman, Adam Tart, Adi Ganesh, Ahmed El-Kishky, Aidan McLaughlin, Aiden Low, AJ Ostrow, Akhila Ananthram, et al. Openai gpt-5 system card. arXiv preprint arXiv:2601.03267, 2025. Anthropic. Introducing the next generation of claude, 2024. URL https://www.anthropic.com/news/claude-3-family. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. CoRR, abs/2502.09992, 2025. doi: 10.48550/ARXIV.2502.09992. URL https://doi.org/10.48550/ arXiv.2502.09992. Llama Team. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10. 48550/arXiv.2407.21783. Metod Jazbec, Theo Olausson, Louis BÃ©thune, Pierre Ablin, Michael Kirchhof, Joao Monterio, Victor Turrisi, Jason Ramapuram, and Marco Cuturi. Learning unmasking policies for diffusion language models. arXiv preprint arXiv:2512.09106, 2025. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion LLM by enabling KV cache and parallel decoding. CoRR, abs/2505.22618, 2025. doi: 10.48550/ARXIV.2505.22618. URL https://doi.org/10.48550/arXiv.2505.22618. Yanghao Li, Rui Qian, Bowen Pan, Haotian Zhang, Haoshuo Huang, Bowen Zhang, Jialing Tong, Haoxuan You, Xianzhi Du, Zhe Gan, Hyunjik Kim, Chao Jia, Zhenbang Wang, Yinfei Yang, Mingfei Gao, Zi-Yi Dou, Wenze Hu, Chang Gao, Dongxu Li, Philipp Dufter, Zirui Wang, Guoli Yin, Zhengdong Zhang, Chen Chen, Yang Zhao, Ruoming Pang, and Zhifeng Chen. MANZANO: simple and scalable unified multimodal model with hybrid vision tokenizer. CoRR, abs/2509.16197, 2025. doi: 10.48550/ARXIV. 2509.16197. URL https://doi.org/10.48550/arXiv.2509.16197. Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. Llada-v: Large language diffusion models with visual instruction tuning. CoRR, abs/2505.16933, 2025. doi: 10.48550/ARXIV.2505.16933. URL https: //doi.org/10.48550/arXiv.2505.16933. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. CoRR, abs/2505.15809, 2025. doi: 10.48550/ARXIV.2505.15809. URL https://doi.org/10.48550/arXiv.2505.15809. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report. CoRR, abs/2409.12186, 2024. doi: 10.48550/ARXIV.2409.12186. URL https://doi.org/10.48550/arXiv.2409.12186. Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, and Yizhe Zhang. Diffucoder: Understanding and improving masked diffusion models for code generation. CoRR, abs/2506.20639, 2025. doi: 10.48550/ARXIV.2506. 20639. URL https://doi.org/10.48550/arXiv.2506.20639. Huangjie Zheng, Shansan Gong, Ruixiang Zhang, Tianrong Chen, Jiatao Gu, Mingyuan Zhou, Navdeep Jaitly, and Yizhe Zhang. Continuously augmented discrete diffusion model for categorical generative modeling. CoRR, abs/2510.01329, 2025. doi: 10. 48550/ARXIV.2510.01329. URL https://doi.org/10.48550/arXiv.2510.01329. Ruixiang Zhang, Shuangfei Zhai, Yizhe Zhang, James Thornton, Zijing Ou, Joshua M. Susskind, and Navdeep Jaitly. Target concrete score matching: holistic framework for discrete diffusion. In Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=ZMrdvSm7xi. Tiwei Bie, Maosong Cao, Kun Chen, Lun Du, Mingliang Gong, Zhuochen Gong, Yanmei Gu, Jiaqi Hu, Zenan Huang, Zhenzhong Lan, Chengxi Li, Chongxuan Li, Jianguo Li, Zehuan Li, Huabin Liu, Lin Liu, Guoshan Lu, Xiaocheng Lu, Yuxin Ma, Jianfeng Tan, Lanning Wei, Ji-Rong Wen, Yipeng Xing, Xiaolu Zhang, Junbo Zhao, Da Zheng, Jun Zhou, Junlin Zhou, Zhanchao Zhou, Liwang Zhu, and Yihong Zhuang. Llada2.0: Scaling up diffusion language models to 100b. CoRR, abs/2512.15745, 2025. doi: 10.48550/ARXIV.2512.15745. URL https://doi.org/10.48550/arXiv.2512.15745. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html. Shane Bergsma, Nolan Dey, Gurpreet Gosal, Gavia Gray, Daria Soboleva, and Joel Hestness. Power lines: Scaling laws for weight decay and batch size in LLM pre-training. CoRR, abs/2505.13738, 2025. doi: 10.48550/ARXIV.2505.13738. URL https://doi.org/10. 48550/arXiv.2505.13738. Bruno Mlodozeniec, Pierre Ablin, Louis BÃ©thune, Dan Busbridge, Michal Klein, Jason Ramapuram, and Marco Cuturi. Completed hyperparameter transfer across modules, width, depth, batch and duration. arXiv preprint arXiv:2512.22382, 2025. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. CoRR, abs/2203.15556, 2022. doi: 10.48550/ARXIV.2203.15556. URL https: //doi.org/10.48550/arXiv.2203.15556. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id= St1giarCHLP. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 22562265. JMLR.org, 2015. URL http://proceedings.mlr.press/v37/sohl-dickstein15.html. Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 1798117993, 2021. URL https://proceedings.neurips.cc/paper/2021/ hash/958c530554f78bcd8e97125b70e6973d-Abstract.html. Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick ForrÃ©, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 1245412465, 2021. URL https://proceedings.neurips.cc/paper/2021/ hash/67d96d458abdef21792e6d8e590244e7-Abstract.html. Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. continuous time framework for discrete denoising models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b5b528767aa35f5b1a60fe0aaeca0563-Abstract-Conference.html. Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H. Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, Curtis Hawthorne, RÃ©mi Leblond, Will Grathwohl, and Jonas Adler. Continuous diffusion for categorical data. CoRR, abs/2211.15089, 2022. doi: 10.48550/ARXIV.2211.15089. URL https://doi.org/10.48550/arXiv. 2211.15089. Ciprian Chelba, TomÃ¡s Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion In Haizhou Li, Helen M. Meng, Bin Ma, Enword benchmark for measuring progress in statistical language modeling. gsiong Chng, and Lei Xie, editors, 15th Annual Conference of the International Speech Communication Association, INTERSPEECH 2014, Singapore, September 14-18, 2014, pages 26352639. ISCA, 2014. doi: 10.21437/INTERSPEECH.2014-564. URL https://doi.org/10.21437/Interspeech.2014-564. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. CoRR, abs/2508.15487, 2025. doi: 10.48550/ARXIV.2508.15487. URL https://doi.org/10.48550/arXiv.2508.15487. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024a. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10. 48550/arXiv.2412.15115. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. CoRR, abs/2505.19223, 2025a. doi: 10.48550/ARXIV.2505.19223. URL https://doi.org/10.48550/arXiv.2505.19223. Fengqi Zhu, Zebin You, Yipeng Xing, Zenan Huang, Lin Liu, Yihong Zhuang, Guoshan Lu, Kangyu Wang, Xudong Wang, Lanning Wei, Hongrui Guo, Jiaqi Hu, Wentao Ye, Tieyuan Chen, Chenchen Li, Chengfu Tang, Haibo Feng, Jun Hu, Jun Zhou, Xiaolu Zhang, Zhenzhong Lan, Junbo Zhao, Da Zheng, Chongxuan Li, Jianguo Li, and Ji-Rong Wen. Llada-moe: sparse moe diffusion language model. CoRR, abs/2509.24389, 2025b. doi: 10.48550/ARXIV.2509.24389. URL https://doi.org/10.48550/arXiv.2509.24389. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis K. Titsias. Simplified and generalized masked diffusion for discrete data. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/ hash/bad233b9849f019aead5e5cc60cef70f-Abstract-Conference.html. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: large-scale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pages 248255. IEEE Computer Society, 2009. doi: 10.1109/CVPR.2009.5206848. URL https://doi.org/10.1109/CVPR.2009.5206848. AÃ¤ron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In Maria-Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 17471756. JMLR.org, 2016. URL http://proceedings.mlr.press/v48/oord16.html. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1130511315. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01103. URL https://doi.org/10.1109/CVPR52688.2022.01103. Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1068610696. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01043. URL https://doi. org/10.1109/CVPR52688.2022.01043. Patrick Esser, Robin Rombach, and BjÃ¶rn Ommer. Taming transformers for high-resolution image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 1287312883. Computer Vision Foundation / IEEE, 2021. doi: 10.1109/CVPR46437.2021.01268. URL https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_ Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html. ai-forever. Sber-movqgan. https://github.com/ai-forever/MoVQGAN, 2023. Chuanxia Zheng, Long Tung Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation, 2022. URL https://arxiv.org/abs/2209.09002. AÃ¤ron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 63066315, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html. Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete diffusion IEEE ACM Trans. Audio Speech Lang. Process., 31:17201733, 2023. doi: 10.1109/TASLP. model for text-to-sound generation. 2023.3268730. URL https://doi.org/10.1109/TASLP.2023.3268730. ZalÃ¡n Borsos, Matthew Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, and Marco Tagliasacchi. Soundstorm: Efficient parallel audio generation. CoRR, abs/2305.09636, 2023. doi: 10.48550/ARXIV.2305.09636. URL https://doi.org/10.48550/arXiv. 2305.09636. Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An end-to-end neural IEEE ACM Trans. Audio Speech Lang. Process., 30:495507, 2022. doi: 10.1109/TASLP.2021.3129994. URL https: audio codec. //doi.org/10.1109/TASLP.2021.3129994. Alexander Swerdlow, Mihir Prabhudesai, Siddharth Gandhi, Deepak Pathak, and Katerina Fragkiadaki. Unified multimodal discrete diffusion. CoRR, abs/2503.20853, 2025. doi: 10.48550/ARXIV.2503.20853. URL https://doi.org/10.48550/arXiv.2503.20853. Minghui Hu, Chuanxia Zheng, Zuopeng Yang, Tat-Jen Cham, Heliang Zheng, Chaoyue Wang, Dacheng Tao, and Ponnuthurai N. Suganthan. Unified discrete diffusion for simultaneous vision-language generation. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id= 8JqINxA-2a. Erik Wijmans, Brody Huval, Alexander Hertzberg, Vladlen Koltun, and Philipp KrÃ¤henbÃ¼hl. Cut your losses in large-vocabulary language models. CoRR, abs/2411.09009, 2024. doi: 10.48550/ARXIV.2411.09009. URL https://doi.org/10.48550/arXiv.2411.09009. Alexandre de BrÃ©bisson and Pascal Vincent. The z-loss: shift and scale invariant classification loss belonging to the spherical family. CoRR, abs/1604.08859, 2016. URL http://arxiv.org/abs/1604.08859. Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlchÃ©-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 1236012371, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/1e8a19426224ca89e83cef47f1e7f53b-Abstract.html. Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv.org/abs/2002.05202. Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. CoRR, abs/2104.09864, 2021. URL https://arxiv.org/abs/2104.09864. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 74807512. PMLR, 2023. URL https://proceedings.mlr.press/v202/dehghani23a.html. Mitchell Wortsman, Peter J. Liu, Lechao Xiao, Katie E. Everett, Alexander A. Alemi, Ben Adlam, John D. Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Small-scale proxies for large-scale transformer training instabilities. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum? id=d8w0pmvXbZ. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. CoRR, abs/2405.09818, 2024. doi: 10.48550/ARXIV. 2405.09818. URL https://doi.org/10.48550/arXiv.2405.09818. Gemma Team. Gemma 3 technical report. CoRR, abs/2503.19786, 2025. doi: 10.48550/ARXIV.2503.19786. URL https://doi.org/10. 48550/arXiv.2503.19786. Boson AI. Higgs Audio V2: Redefining Expressiveness in Audio Generation. https://github.com/boson-ai/higgs-audio, 2025. GitHub repository. Release blog available at https://www.boson.ai/blog/higgs-audio-v2. OpenAI. tiktoken: fast byte-pair encoding tokenizer for openai models. https://github.com/openai/tiktoken, 2023. Erik Wijmans, Brody Huval, Alexander Hertzberg, Vladlen Koltun, and Philipp KrÃ¤henbÃ¼hl. Cut your losses in large-vocabulary language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=E4Fk3YuG56. Ge Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tuning large neural networks via zero-shot hyperparameter transfer. Advances in Neural Information Processing Systems, 34:1708417097, 2021. Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Tensor programs vi: Feature learning in infinite depth neural networks. In The Twelfth International Conference on Learning Representations, 2024b. Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Yuri Prince, BjÃ¶rn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach, and Douglas Orr. u-mu p: The unit-scaled maximal update parametrization. In The Thirteenth International Conference on Learning Representations, 2025. Nolan Dey, Bin Claire Zhang, Lorenzo Noci, Mufan Li, Blake Bordelon, Shane Bergsma, Cengiz Pehlevan, Boris Hanin, and Joel Hestness. Dont be lazy: Completep enables compute-efficient deep transformers. arXiv preprint arXiv:2505.01618, 2025. Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, and Sanjeev Arora. On the sdes and scaling rules for adaptive gradient algorithms. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 32ac710102f0620d0f28d5d05a44fe08-Abstract-Conference.html. Dan Busbridge, Amitis Shidani, Floris Weers, Jason Ramapuram, Etai Littwin, and Russell Webb. Distillation scaling laws. In Fortysecond International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=1nEBAkpfb9. Alexander HÃ¤gele, Elie Bakouch, Atli Kosson, Loubna Ben Allal, Leandro von Werra, and Martin Jaggi. Scaling laws and computeoptimal training beyond fixed training durations. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http: //papers.nips.cc/paper_files/paper/2024/hash/8b970e15a89bf5d12542810df8eae8fc-Abstract-Conference.html. Fabian Schaipp, Alexander HÃ¤gele, Adrien B. Taylor, Umut Simsekli, and Francis Bach. The surprising agreement between convex optimization theory and learning-rate scheduling for large model training. In Aarti Singh, Maryam Fazel, Daniel Hsu, Simon Lacoste-Julien, Felix Berkenkamp, Tegan Maharaj, Kiri Wagstaff, and Jerry Zhu, editors, Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025, volume 267 of Proceedings of Machine Learning Research. PMLR / OpenReview.net, 2025. URL https://proceedings.mlr.press/v267/schaipp25a.html. Tim Pearce and Jinyeop Song. Reconciling kaplan and chinchilla scaling laws. Trans. Mach. Learn. Res., 2024, 2024. URL https: //openreview.net/forum?id=NLoaLyuUUF. Mustafa Shukor, Louis BÃ©thune, Dan Busbridge, David Grangier, Enrico Fini, Alaaeldin El-Nouby, and Pierre Ablin. Scaling laws for optimal data mixtures. CoRR, abs/2507.09404, 2025. doi: 10.48550/ARXIV.2507.09404. URL https://doi.org/10.48550/arXiv.2507. 09404. Tamay Besiroglu, Ege Erdil, Matthew Barnett, and Josh You. Chinchilla scaling: replication attempt. CoRR, abs/2404.10102, 2024. doi: 10.48550/ARXIV.2404.10102. URL https://doi.org/10.48550/arXiv.2404.10102. Jinjie Ni, Qian Liu, Chao Du, Longxu Dou, Hang Yan, Zili Wang, Tianyu Pang, and Michael Qizhe Shieh. Training optimal large diffusion language models. CoRR, abs/2510.03280, 2025. doi: 10.48550/ARXIV.2510.03280. URL https://doi.org/10.48550/arXiv. 2510.03280. Licong Lin, Jingfeng Wu, Sham Kakade, Peter Bartlett, and Jason Lee. Scaling laws in linear regression: Compute, parameters, and data. Advances in Neural Information Processing Systems, 37:6055660606, 2024. Subham Sekhar Sahoo, Jean-Marie Lemercier, Zhihan Yang, Justin Deschenaux, Jingyu Liu, John Thickstun, and Ante Jukic. Scaling beyond masked diffusion language models. arXiv preprint arXiv:2602.15014, 2026. 20 Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36: 5035850376, 2023. Mihir Prabhudesai, Mengning Wu, Amir Zadeh, Katerina Fragkiadaki, and Deepak Pathak. Diffusion beats autoregressive in data-constrained settings. CoRR, abs/2507.15857, 2025. doi: 10.48550/ARXIV.2507.15857. URL https://doi.org/10.48550/arXiv.2507. 15857. Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 24592475, 2025. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:1420014282, 2024. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2849228518. PMLR, 2023. URL https://proceedings.mlr.press/ v202/radford23a.html. Andros Tjandra, Yi-Chiao Wu, Baishan Guo, John Hoffman, Brian Ellis, Apoorv Vyas, Bowen Shi, Sanyuan Chen, Matt Le, Nick Zacharov, Carleigh Wood, Ann Lee, and Wei-Ning Hsu. Meta audiobox aesthetics: Unified automatic quality assessment for speech, music, and sound. CoRR, abs/2502.05139, 2025. doi: 10.48550/ARXIV.2502.05139. URL https://doi.org/10.48550/arXiv.2502. 05139. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 25562565. Association for Computational Linguistics, 2018. doi: 10.18653/V1/P18-1238. URL https://aclanthology.org/P18-1238/. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 35583568. Computer Vision Foundation / IEEE, 2021. doi: 10.1109/CVPR46437.2021. 00356. URL https://openaccess.thecvf.com/content/CVPR2021/html/Changpinyo_Conceptual_12M_Pushing_Web-Scale_Image-Text_ Pre-Training_To_Recognize_Long-Tail_Visual_CVPR_2021_paper.html. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 66266637, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html. Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, ShangWen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, HervÃ© JÃ©gou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. Trans. Mach. Learn. Res., 2024, 2024. URL https://openreview.net/forum?id=a68SUt6zFt. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 2730, 2016, pages 28182826. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.308. URL https://doi.org/10.1109/CVPR.2016.308. 21 Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-toIn Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, edimage alignment. itors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ a3bf71c7c63f0c3bcb7ff67c67b1e7b1-Abstract-Datasets_and_Benchmarks.html. Aleksandr Meister, Matvei Novikov, Nikolay Karpov, Evelina Bakhturina, Vitaly Lavrukhin, and Boris Ginsburg. Librispeechpc: Benchmark for evaluation of punctuation and capitalization capabilities of end-to-end ASR models. In IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2023, Taipei, Taiwan, December 16-20, 2023, pages 17. IEEE, 2023. doi: 10.1109/ASRU57964.2023.10389666. URL https://doi.org/10.1109/ASRU57964.2023.10389666. Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. FrÃ©chet audio distance: reference-free metric for evaluating music enhancement algorithms. In Gernot Kubin and Zdravko Kacic, editors, 20th Annual Conference of the International Speech Communication Association, Interspeech 2019, Graz, Austria, September 15-19, 2019, pages 23502354. ISCA, 2019. doi: 10.21437/INTERSPEECH.2019-2219. URL https://doi.org/10.21437/Interspeech.2019-2219. Azalea Gui, Hannes Gamper, Sebastian Braun, and Dimitra Emmanouilidou. Adapting frechet audio distance for generative music evaluation. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2024, Seoul, Republic of Korea, April 14-19, 2024, pages 13311335. IEEE, 2024. doi: 10.1109/ICASSP48485.2024.10446663. URL https://doi.org/10.1109/ ICASSP48485.2024.10446663. Mayee Chen, Tyler Murray, David Heineman, Matt Jordan, Hannaneh Hajishirzi, Christopher RÃ©, Luca Soldaini, and Kyle Lo. Olmix: framework for data mixing throughout lm development. arXiv preprint arXiv:2602.12237, 2026. Dimitri von RÃ¼tte, Janis Fluri, Yuhui Ding, Antonio Orvieto, Bernhard SchÃ¶lkopf, and Thomas Hofmann. Generalized interpolating discrete diffusion. In Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=rvZv7sDPV9. Mengni Jia, Mengyu Zhou, Yihao Liu, Xiaoxi Jiang, and Guanjun Jiang. Bringing stability to diffusion: Decomposing and reducing variance of training masked diffusion models. CoRR, abs/2511.18159, 2025. doi: 10.48550/ARXIV.2511.18159. URL https://doi.org/ 10.48550/arXiv.2511.18159. Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-fidelity audio compression with improved RVQGAN. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 58d0e78cf042af5876e12661087bea12-Abstract-Conference.html. Antony W. Rix, John G. Beerends, Michael P. Hollier, and Andries P. Hekstra. Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs. In IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2001, 7-11 May, 2001, Salt Palace Convention Center, Salt Lake City, Utah, USA, Proceedings, pages 749752. IEEE, 2001. doi: 10.1109/ICASSP.2001.941023. URL https://doi.org/10.1109/ICASSP.2001.941023. Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Samuel Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely KlÃ¡r, Grace Lam, Shiyi Lan, Laura Leal-TaixÃ©, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, MingYu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Yen-Chen Lin, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, and Artur ZÃ³lkowski. Cosmos world foundation model platform for physical AI. CoRR, abs/2501.03575, 2025. doi: 10.48550/ARXIV.2501. 03575. URL https://doi.org/10.48550/arXiv.2501.03575. Fengyuan Shi, Zhuoyan Luo, Yixiao Ge, Yujiu Yang, Ying Shan, and Limin Wang. Scalable image tokenization with index backpropagation quantization, 2025. URL https://arxiv.org/abs/2412.02692. Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation, 2025. URL https://arxiv.org/abs/2409.04410. Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025."
        },
        {
            "title": "Appendices",
            "content": "A General formulation of weighting and the masking process A.1 Connection of Loss Weighting and Cumulative Corruption . A.2 Unbiasedness of the Importance Weighting . . . . . . . . . . Tokenizer Ablations B.1 Audio Tokenizer Ablations . B.2 Image Tokenizer Ablations . . . . . . . . . . . . . . . . . Training details C.1 Optimal Global Hyper-Parameter Search . C.2 Runtime as Function of Batch Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Hyperparameters for the Unified 3B Tri-modal MDM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MDM with Per-module Hyperparameters"
        },
        {
            "title": "I Acknowledgments",
            "content": "24 24 25 25 25 27 27 27 27 29 31 31 41"
        },
        {
            "title": "A General formulation of weighting and the masking process",
            "content": "In this section, we expand the MDM forward process explained in Section 3 and make it consistent with the previously introduced notation in the literature, particularly following Shi et al. (2024). In the general case of MDM, we progressively corrupt the original data ğ‘ 0 Vğ¿ into masked version ğ‘ ğ‘¡ over ğ‘‡ discrete time steps. The forward process defines Markov chain that transforms an original data ğ‘ 0 into corrupted version ğ‘ ğ‘¡ at time step ğ‘¡ [ğ‘‡ ]. This process is governed by sequence of transition matrices for each position ğ‘– denoted by ğ‘„ğ‘– ğ‘¡ 1). We define the masking probability at time ğ‘¡ as ğ›½ğ‘¡ [0, 1]. Consequently, the probability of token not being masked is ğ›¼ğ‘¡ = 1 ğ›½ğ‘¡ . The single-step transition matrix ğ‘„ğ‘– ğ‘¡ Rğ‘‰ ğ‘‰ . At each step ğ‘¡, token ğ‘ ğ‘– ğ‘¡ according to the probability ğ‘(ğ‘ ğ‘– ğ‘¡ ğ‘¡ 1 is transformed into ğ‘ ğ‘– ğ‘ ğ‘– ğ‘¡ is typically defined as: ğ‘„ğ‘– ğ‘¡ (ğ‘£ ğ‘Ÿ ) = ğ‘(ğ‘ ğ‘– ğ‘¡ = ğ‘£ ğ‘ ğ‘– ğ‘¡ 1 = ğ‘Ÿ ) = 1 ğ›½ğ‘¡ ğ›½ğ‘¡ 1 0 if ğ‘Ÿ MASKğ‘š (ğ‘– ) and ğ‘£ = ğ‘Ÿ if ğ‘Ÿ MASKğ‘š (ğ‘– ) and ğ‘£ = MASKğ‘š (ğ‘– ) if ğ‘Ÿ = ğ‘£ = MASKğ‘š (ğ‘– ) otherwise This formulation implies that token either remains unchanged or is replaced by the MASKğ‘š (ğ‘– ) token. More general formulations might allow transitions to any other token with small probability. The cumulative transition probability from ğ‘ 0 to ğ‘ ğ‘¡ is crucial for training and is given by the product of individual transition matrices: ğ‘„ğ‘– ğœ=1(1 ğ›½ğœ ). The probability of token ğ‘ ğ‘– 0 remaining unchanged until time ğ‘¡ is ğ›¼ğ‘¡ . Conversely, the probability of it having been masked at least once and therefore being MASKğ‘š (ğ‘– ) token at time ğ‘¡ is 1 ğ›¼ğ‘¡ . Therefore, the marginal distribution of ğ‘ ğ‘¡ given ğ‘ 0 for single token ğ‘– is: 0 = ğ‘Ÿ ). This can be simplified by defining ğ›¼ğ‘¡ = (cid:206)ğ‘¡ ğ‘¡ (ğ‘£ ğ‘Ÿ ) = ğ‘(ğ‘ ğ‘– ğœ=1 ğ›¼ğœ = (cid:206)ğ‘¡ ğ‘¡ = ğ‘£ ğ‘ ğ‘– ğ‘(ğ‘ ğ‘– ğ‘¡ = ğ‘£ ğ‘ ğ‘– 0 = ğ‘Ÿ ) = ğ›¼ğ‘¡ 1 ğ›¼ğ‘¡ if ğ‘£ = ğ‘Ÿ if ğ‘£ = MASKğ‘š (ğ‘– ) otherwise , ğ‘(ğ‘ ğ‘¡ ğ‘ 0) = ğ¿ (cid:214) ğ‘–= ğ‘(ğ‘ ğ‘– ğ‘¡ ğ‘ ğ‘– 0). This distribution ğ‘(ğ‘ ğ‘¡ ğ‘ 0) allows for direct sampling of ğ‘ ğ‘¡ from ğ‘ 0 at any time step ğ‘¡. A.1 Connection of Loss Weighting and Cumulative Corruption The weighting function ğ‘¤ (ğ‘¡) plays critical role in balancing the contribution of different time steps to the total loss, with the choice of ğ‘¤ (ğ‘¡) being intimately connected to the masking schedule defined by ğ›¼ğ‘¡ or equivalently ğ›½ğ‘¡ . Recall that ğ›¼ğ‘¡ represents the cumulative probability that token has not been masked up to time ğ‘¡. Conversely, 1 ğ›¼ğ‘¡ is the probability that token has been masked by time ğ‘¡. This results in two opposite scenarios: Early time steps (small ğ‘¡, ğ›¼ğ‘¡ 1): Few tokens are masked. Predicting the original ğ‘ 0 for these few masked tokens is relatively easy, as most of the context (unmasked tokens) is available. Late time steps (large ğ‘¡, ğ›¼ğ‘¡ 0): Most tokens are masked. Predicting the original ğ‘ 0 becomes very challenging, requiring the model to infer from minimal context. common motivation for weighting comes from ELBO in continuous diffusion, which often leads to weighting terms that compensate for varying noise levels. Based on Section A, the forward marginal distribution is ğ‘(ğ‘ ğ‘¡ ğ‘ 0) = ğ¿ (cid:214) ğ‘–=1 ğ›¼ğ‘¡ ğ›¿ğ‘ ğ‘– 0 ğ‘¡ ) + (cid:0)1 ğ›¼ğ‘¡ (cid:1) ğ›¿MASKğ‘š (ğ‘– ) (ğ‘ ğ‘– (ğ‘ ğ‘– ğ‘¡ ) . Training maximizes the ELBO, which decomposes into timestep-wise Kullback-Leibler Divergence (KLD): = (cid:104)KL(cid:0)ğ‘(ğ‘ ğ‘¡ 1 ğ‘ ğ‘¡, ğ‘ 0) ğ‘ğœƒ (ğ‘ ğ‘¡ 1 ğ‘ ğ‘¡ )(cid:1)(cid:105) . Eğ‘ ğ‘¡ 24 In MDM, each KL term is non-zero only when ğ‘ ğ‘– ğ‘¡ 1 ğ‘ ğ‘– ğ‘¡ = MASKğ‘š (ğ‘– ), ğ‘ 0) is categorical distribution whose parameters depend on the incremental masking rate. More precisely, let us denote ğœ‹ğ‘¡ the probability that masking occurred at time ğ‘¡ rather than earlier, so that ğ‘¡ = MASKğ‘š (ğ‘– ) . Conditioning on this event, the posterior ğ‘(ğ‘ ğ‘– ğ‘(ğ‘ ğ‘– ğ‘¡ 1 ğ‘ ğ‘– ğ‘¡ = MASKğ‘š (ğ‘– ), ğ‘ 0) = ğœ‹ğ‘¡ğ›¿ğ‘ ğ‘– 0 (ğ‘ ğ‘– ğ‘¡ 1) + (1 ğœ‹ğ‘¡ )ğ›¿MASKğ‘š (ğ‘– ) (ğ‘ ğ‘– ğ‘¡ 1) . Using Bayes rule, we have: ğœ‹ğ‘¡ = P(ğ‘ ğ‘– ğ‘¡ 1 = ğ‘ ğ‘– 0)P(ğ‘ ğ‘– P(ğ‘ ğ‘– ğ‘¡ = MASKğ‘š (ğ‘– ) ğ‘ ğ‘– ğ‘¡ = MASKğ‘š (ğ‘– ) ) ğ‘¡ 1 = ğ‘ ğ‘– 0) , where P(ğ‘ ğ‘– have: ğ‘¡ 1 = ğ‘ ğ‘– 0) = ğ›¼ğ‘¡ 1, P(ğ‘ ğ‘– ğ‘¡ = MASKğ‘š (ğ‘– ) ğ‘ ğ‘– ğ‘¡ 1 = ğ‘ ğ‘– 0) = 1 ğ›¼ğ‘¡ , and P(ğ‘ ğ‘– ğ‘¡ = MASKğ‘š (ğ‘– ) ) = 1 ğ›¼ğ‘¡ . Therefore, we ğœ‹ğ‘¡ = ğ›¼ğ‘¡ 1(1 ğ›¼ğ‘¡ ) 1 ğ›¼ğ‘¡ = ğ›¼ğ‘¡ 1 ğ›¼ğ‘¡ 1 ğ›¼ğ‘¡ . In the continuous-time limit, ğœ‹ğ‘¡ = ğ›¼ ğ‘¡ 1 ğ›¼ğ‘¡ objective . As result, the ELBO is equivalent up to constants to minimizing the Eğ‘¡ U(0,1) (cid:34) ğ‘¤ (ğ‘¡) ğ¿ ğ‘–=1 Eğ‘ (ğ‘ ğ‘– ğ‘¡ ğ‘ ğ‘– 0 ) (cid:2)â„“ğ‘– (ğœƒ, ğ‘ 0) (cid:12) (cid:12) ğ‘ ğ‘– ğ‘¡ = MASKğ‘š (ğ‘– ) (cid:35) (cid:3) , ğ‘¤ (ğ‘¡) = ğ›¼ ğ‘¡ 1 ğ›¼ğ‘¡ . This weighting ensures that each timestep contributes proportionally to the rate at which information about ğ‘ 0 is destroyed by masking. A.2 Unbiasedness of the Importance Weighting At fixed timestep ğ‘¡, each token position is independently masked with probability ğ‘¡, so that 1{ğ‘– Iğ‘¡ } Bernoulli(ğ‘¡). Averaging the reconstruction loss only over masked positions therefore corresponds to random subsampling of tokens. Without reweighting, the expected contribution of token ğ‘– to the loss in linear scheduling is which underweights tokens at small ğ‘¡ and biases the objective toward high-noise regimes. Multiplying by 1/ğ‘¡ yields an unbiased estimator: E[1{ğ‘– Iğ‘¡ } â„“ğ‘– (ğœƒ, ğ‘ )] = ğ‘¡ â„“ğ‘– (ğœƒ, ğ‘ ) , (cid:20) 1 ğ‘¡ 1{ğ‘– Iğ‘¡ } â„“ğ‘– (ğœƒ, ğ‘ ) (cid:21) = â„“ğ‘– (ğœƒ, ğ‘ ) . Therefore, the 1/ğ‘¡ factor corrects for the subsampling induced by masking, ensuring that each token contributes equally in expectation across timesteps. This corresponds to inverse-probability weighting and is analogous to the time-dependent weighting used in denoising score matching objectives for diffusion models, where losses are rescaled to normalize signal-to-noise ratio across noise levels."
        },
        {
            "title": "B Tokenizer Ablations",
            "content": "B.1 Audio Tokenizer Ablations The audio tokenizer determines the audio token rate and, therefore, the sequence length corresponding to the audio stream. Since we use fixed context length (ğ¿ = 3256) and clips with at most 30 seconds, we need low-rate codec that still preserves perceptual quality. Here, we compare several RVQ-based codecs: pretrained 24 kHz DAC (Kumar et al., 2023), pretrained Higgs Audio v2 (Boson AI, 2025), and DAC-style tokenizer trained on the same data as the main model. In Table 3, we present the reconstruction evaluation on LibriTTS-clean using PESQ (Rix et al., 2001) and Audiobox. Among the options, only two configurations fit our token budget for 30 audio, which we display in 25 Model # Codebooks PESQ Content Enjoy Content Useful. Prod. Complex. Prob. Quality Down. Factor Higgs pretrained DAC pretrained DAC retrained 8 4 9 4 9 4 3.168 2.544 3.658 2.396 2.969 2.433 5.670 5.577 5.555 5.069 5.691 5.585 6.192 6.087 5.994 5.415 6.234 6. 1.559 1.561 1.567 1.603 1.556 1.566 6.510 6.390 6.255 5.597 6.550 6.434 960 960 512 512 1024 1024 Table 3 Reconstruction metrics for different audio tokenizers. bold. Based on these results, we chose the Higgs Audio v2 tokenizer with 4 codebooks decoding as the default audio tokenizer as it provides strong and convenient ratedistortion trade-off. As expected, increasing the number of codebooks improves reconstruction but quickly becomes impractical under our sequence-length constraint. Consistent with prior observations for RVQ codecs (Kumar et al., 2023), we also find that training with more codebooks and decoding with fewer can preserve perceptual quality substantially better than training the low-rate model directly. Model Type Latent / Code Dim Vocab Size Tokens Per Image rFID cosmos-ci8x8 (Agarwal et al., 2025) cosmos-di16x16 (Agarwal et al., 2025) cosmos-di8x8-360p (Agarwal et al., 2025) ibq-262144 (Shi et al., 2025) movqgan-270m (ai-forever, 2023) openmagvitv2 (Luo et al., 2025) unitok (Ma et al., 2025) cosmos-ci8x8 (Agarwal et al., 2025) cosmos-di16x16 (Agarwal et al., 2025) cosmos-di8x8-360p (Agarwal et al., 2025) ibq-262144 (Shi et al., 2025) movqgan-270m (ai-forever, 2023) openmagvitv2 (Luo et al., 2025) unitok (Ma et al., 2025) Continuous FSQ FSQ IBQ MoVQ LFQ MCQ Continuous FSQ FSQ IBQ MoVQ LFQ MCQ CC12M 16 6 6 256 256 18 64 ImageNet 16 6 6 256 256 18 64 ImageNet - 512 cosmos-ci8x8 (Agarwal et al., 2025) cosmos-di16x16 (Agarwal et al., 2025) cosmos-di8x8-360p (Agarwal et al., 2025) ibq-262144 (Shi et al., 2025) movqgan-270m (ai-forever, 2023) openmagvitv2 (Luo et al., 2025) unitok (Ma et al., 2025) Continuous FSQ FSQ IBQ MoVQ LFQ MCQ 16 6 6 256 256 18 64 - 65536 65536 262144 16384 262144 4096 (x8)* - 65536 65536 262144 16384 262144 4096 (x8)* - 65536 65536 262144 16384 262144 4096 (x8)* 1024 256 1024 256 1024 256 256 (x8) 1024 256 1024 256 1024 256 256 (x8) 4096 1024 4096 1024 4096 1024 1024 (x8) 1.37 3.50 1.80 0.89 0.50 0.82 0.54 1.02 4.38 0.95 1.55 0.55 1.67 0.36 0.07 1.33 0.51 0.50 0.17 0.53 0. Table 4 Reconstruction FID for different image tokenizers. ImageNet consists of 50k validation examples and CC12M (Changpinyo et al., 2021) consists of 50k samples from the full dataset. Best FID per section is shown in bold. When continuous model achieves the best FID, the best discrete model is also bolded. *UniTok uses 8 categorical predictions of size 4096 (x8 underlying codes that are merged into single token). B.2 Image Tokenizer Ablations The same sequence length constrains apply to the image tokenizer. We want discrete image tokenizer that maps an image to as few tokens as possible while maintaining good image representation. Here, we compare discrete versions of Cosmos (Agarwal et al., 2025), IBQ (Shi et al., 2025), OpenMagVIT (Luo et al., 2025), Unitok (Ma et al., 2025), and MoVQGAN (ai-forever, 2023). We evaluate the reconstruction FID on ImageNet at 256 and 512 resolution as well as CC12M (Changpinyo et al., 2021) and present these results in Table 4. Based on these results, we chose MoVQGAN as the default image tokenizer as it provided good balance between high reconstruction performance, high compression and small vocabulary size."
        },
        {
            "title": "C Training details",
            "content": "C.1 Optimal Global Hyper-Parameter Search As highlighted in Section 4, most ablations in this work rely on optimal global hyperparameters scaled up with CompleteP (Dey et al., 2025). In Figure 19 we present Gaussian Process fit on 2900 trial runs at small scale (320M parameters in total, including 80M non-embedding ones, 13B tokens) to determine optimal global hyperparameters for tri-modal MDM. The optimal values are highlighted in red. We initialize the per-module multiplier search from this optimum to better seed the search process. Figure 19 Small-scale hyper-parameter search for 0-shot transfer. Final average cross-entropy loss for sweep of 5 AdamW hyperparameters (learning rate, ğ›½1, ğ›½2, weight-decay, ğœ–) on 350M model trained for 13B tokens. The plot shows cross-sections through the hyperparameter landscape through the best found hyperparameters. C.2 Runtime as Function of Batch Size As explored in Section 5, the SDE parametrization allows wide range of batch sizes to be used. Typically, bigger batch sizes allow less iterations, which can reduce runtime. Two knobs are available to increase the batch size: increasing the number of nodes, or modifying the per-GPU batch size. In practice, overall throughput grows sub-linearly with number of nodes because of communications on the cluster. Furthermore, for smaller batch sizes, hosted on single node, another phenomenon plays role: once the batch size is too small, the GPU becomes idle because small batch sizes do not benefit as much from parallelism. This is measured in Figure 20. The runtime diminishes slowly as the per-GPU batch size increases, and diminishes sharply as the number of nodes increases. Highest node count reduces wall-clock time significantly, but also reduces FLOP-efficiency because of sub-linear scaling. a) b) c) Figure 20 Performance as function of physical batch size.. In a) we see impact of batch size on total runtime, with the effect of the node-count clearly visible: more nodes reduces wall-clock time faster than simply saturating GPUs. In b) we see the runtime with 1, 2, 4, 8 and 16 nodes respectively. Doubling node count approximately halves the runtime. Finally, in c), we see that saturated GPUs have much more efficient FLOP usage than non-saturated ones. Doubling node count never allow to recover training efficiency of fewer nodes, because of communication overhead. C.3 Hyperparameters for the Unified 3B Tri-modal MDM Table 5 highlights all training details for the 3B model. ğ‘ blocks Dimension ğ‘ attention heads QK Norm Normalization Pre-norm Post-norm MLP style SwiGLU hidden dimension factor Positional embedding Weight initialization (base_width) Model Training parameters Batch size Sequence length Optimizer Base LR Base AdamW ğœ– Base AdamW ğ›½1 Base AdamW ğ›½2 Base weight decay LR warmup LR schedule Min LR Z-loss weight Training duration Hyperparameter Transfer Strategy Modality sampling rate (text-only, image-text, audio-text) Text tokens seen during training Image samples seen during training Audio samples seen during training 24 3072 24 Yes RMSNorm Yes No SwiGLU 2.75 RoPE trunc_normal(std=0.02) 3072 3256 AdamW 9ğ‘’ 4 1ğ‘’ 8 0.9 0.95 0.1 2, 000 steps Cosine 1ğ‘’ 6 1ğ‘’ 5 1, 000, 000 steps CompleteP (Dey et al., 2025)] [0.33, 0.33, 0.33] 3.4T 1B 1B Text Image Audio Total Text Image Audio ğ‘ƒ (Token packing) ğ‘ƒ (Random sequence subsample) Target resolution RandomResizedCrop Resize with white padding ğ‘ƒ (RandomResizedCrop) ğ‘ƒ (Resize with white padding) Normalization ğœ‡ and ğœ Max duration Number of frames Tokenizers Tiktoken (OpenAI, 2023) SBER-MoVQGAN (ai-forever, 2023) Higgs Audio Tokenizer v2 (4 codebooks) (Boson AI, 2025) Vocabulary (incl. special tokens) Text transformations Image transformations Audio transformations 117, 698 100, 281 16, 387 1, 0.95 0.05 256ğ‘¥256 [0.8, 1.0] - 0.5 0.5 [0.5, 0.5] 30 seconds 25 Table 5 Model and training details for the 3B multimodal MDM. 28 MDM with Per-module Hyperparameters To simplify and parallelize our analysis, we rely on CompleteP (Dey et al., 2025) for width and depth transfer using global hyperparameters (Appendix Section C.1) for all ablations done in this work, however, recent insights from Mlodozeniec et al. (2025) highlight that we can further improve performance by optimizing per-module hyperparameter multipliers for AdamW (learning rate, weight decay, ğ›½1, ğ›½2, and ğœ–). Training Details. Each hyperparameter for each weight gets unique multiplier. The multipliers for all depthrepeated blocks are parameterized as product of depth-dependent factor, and module-type factor all weights at the same depth get the same depth factor, and all weights of the same type (e.g., all ğ‘„ğ¾ğ‘‰ weights) get the same module-type factor. We initialize the local random search method from (Mlodozeniec et al., 2025) at the best global hyperparameters found using random search, shown in Figure 19. We conduct the search using transformer with 8 blocks of width 1024, totaling 320M parameters (including 80M non-embed parameters) trained at horizon of 13B multimodal tokens. The results of the search, and the speed-up obtained at this base model size, is reported in Figure 21. Figure 21 Per-module hyperparameter search on small scale model for 0-shot transfer. Average final ELBO for sweep of AdamWs per-module hyperparameters as well as initialization scales for 350M model (including 80M non-embedding parameters) trained for 13B tokens, using batch size of 256. Per-module tuning yields 1.81 reduction in token count to achieve an equivalent loss. In Table 6, we highlight the results of the search, listing the per module multipliers. Notably, Multiplier analysis. the resulting multipliers are highly structured rather than uniform: embedding and unembedding weights favor substantially larger effective learning rates (up to 4), while attention projections and MLP gates are tuned more conservatively, often with increased ğœ– for numerical damping. The learned depth factors further indicate smaller steps and stronger stabilization in later blocks, consistent with increasing sensitivity of deep representations to update noise. 29 Table 6 Optimal per-module hyperparameter multipliers found for the LLaDA Multimodal model. Depth factors apply to all layers where the block depth total depth falls within the highlighted fraction (as counted from the network input towards output). Category Standalone Blocks Module / Depth Embedding weights (Audio) Embedding weights (Image) Embedding weight (Text) LR WD 1.009 2.192 0.864 1.013 1.593 3. Unembedding weights (Audio) Unembedding weights (Image) Unembedding weights (Text) Unembedding norm weights attn_qkv_weight attn_proj_weight attn_q_norm_weight attn_k_norm_weight mlp_gate_weight mlp_fc1_weight mlp_fc2_weight ğ›¼1 0.962 2.108 1.421 3.442 1.929 1.346 4.508 0.173 0.256 1.530 1. 1.171 1.590 2.684 0.282 1.533 1.030 0.911 ğ›¼2 1.493 0.685 1.791 0.594 1.042 0.955 2.740 0.557 0.339 0.902 0. 1.870 3.309 0.655 1.161 1.789 3.053 1.149 ğœ– 1.494 0.734 0.317 0.742 0.635 1.206 1. 0.391 1.627 0.848 0.368 4.913 1.415 1.790 1.477 0.712 0.663 2.645 Init Scale 1.826 0.554 0.379 3.422 1.524 0.341 2. 2.498 4.732 1.344 0.436 0.643 1.944 0.878 2.171 1.189 0.997 0.485 1.633 1.655 3.008 2.305 1.714 0.630 0.535 0. 0.489 1.271 1.405 1.311 0.899 1.102 0.877 1.510 1.213 0.737 0.817 0.821 0.354 0.731 0.497 0.634 1.295 1. 1.105 0.525 0.725 1.018 norm1_weight norm2_weight 0 50% 50 100%"
        },
        {
            "title": "E Extended Scaling Laws Results",
            "content": "Computation of FLOPs for experiments. We rely on the formula in appendix H.1 of Busbridge et al. (2025) to compute the FLOPs and the model size. In particular, we do not account for input/output embedding, as the size of our multimodal vocabulary (117k) makes these embedding matrices bigger than the transformer backbone for small models. The ratio between transformer parameters and embedding parameters is illustrated in Figure 12. The value of ğ‘ reported in scaling laws fit always use the non-embedding parameters only (which are up-to 5 times smaller than the total models size). The FLOPs ğ¶ reported in scaling law account for everything. We compute the optimal ğ‘ (ğ¶) and ğ·(ğ¶) by minimizing the parametric loss under the constraint ğ¶ = FLOP per token(ğ‘ ) ğ·. Since we do not account for embeddings parameters in total model size, the popular FLOP per token(ğ‘ ) = 6ğ‘ does not apply out-of-the-box. Instead, we minimize the parametric loss ğ¿(ğ‘ , ğ· = ğ¶/FLOP per token(ğ‘ )) via linear-search over ğ‘ to plot compute-optimal curves in Figure 3 and Figure 14. This effect is more striking for small models where the embedding size is significant. For larger models, we found that the approximation FLOP per token(ğ‘ ) 6 holds. In that case, minimizing the parametric form ğ¿(ğ‘ , ğ·) = ğ¸ + ğ´ğ‘ ğ‘/ğ‘ + ğµğ· 1(cid:17)ğ‘ (cid:16) , under constraint ğ¶ = 6ğ‘ ğ· works well. By monotonicity, this is equivalent to minimizing ğ´ğ‘ ğ‘/ğ‘ + ğµğ· 1, which admits the following minimizers: ğ‘ (ğ¶) = ğº 1(ğ¶/6)ğœ, and ğ·(ğ¶) = ğº (ğ¶/6)1ğœ, with ğº = ğ‘ğµ ğ‘ğ´ and ğœ = ğ‘ ğ‘ + ğ‘ . Scaling laws for Uni-Modal Text MDM. We also perform scaling laws run on uni-modal text models, using CompleteP parametrization, but without SDE scaling rules. Every model size relies on different batch size to maximize GPU occupancy. The total sequence length is 4,096 with packing and truncation (no padding). Training curves as function of (ğ‘ , ğ·) FLOP budget are given in Figure 22 and the scaling laws predictions are reported in Figure 23. 30 Figure 22 Training curves for uni-modal text MDM models trained under CompleteP. a) b) Figure 23 Scaling laws for uni-modal text MDMs. a) Scaling law predictions for text-only MDM models, using CompleteP. b) Iso-FLOP curves for text-only MDM models under CompleteP parameterization (no SDE scaling)."
        },
        {
            "title": "F Masking Schedules for Image and Audio Generation",
            "content": "To determine the impact of the masking schedule on multimodal MDM training and generation quality, we evaluate four distinct schedules linear, cosine, polynomial, and geometric implemented using the continuous-time ELBO weighting in Shi et al. (2024). We train 1B model for 100k steps under each schedule, keeping all other hyperparameters the same. First, we evaluate image generation quality at 256 256 resolution using 1024 diffusion steps with CFG scales ranging from 1.0 to 10.0, temperature ğ‘‡ = 0.9, and nucleus sampling (top-ğ‘ = 0.9). Image quality is measured using both FIDInception and FID-DINOv2, computed over 8,192 generated samples on two datasets, CC12M and our train mixture (eval seed). Figure 25 shows that the polynomial schedule consistently achieves the best image quality across both metrics and datasets among the four schedules tested. Both metrics agree that polynomial yields superior generation quality, with optimal performance in the CFG range of 7 to 9. Then, we evaluate audio generation quality with ground-truth durations using 512 diffusion steps with CFG scales ranging from 1.0 to 10.0, temperature ğ‘‡ = 1.0, and nucleus sampling (top-ğ‘ = 0.9). We measure audio quality using FAD, WER, and AudioBox Aesthetics computed over 10,000 generated samples from the dataset. Figure 24 shows that the polynomial schedule also consistently achieves the best audio generation quality across all six metrics evaluated. Unlike in image generation, the optimal CFG range is between 1 and 3."
        },
        {
            "title": "G Extended generations",
            "content": "We present extended generations in Figure 26, Figure 27, Figure 28, and Figure 29 and their respective complete list of prompts in Table 7, Table 8, Table 9, and Table 10. Note that the prompts come from synthetic captions and were 31 Figure 24 Masking schedule ablation for audio generation across guidance scales. We evaluate four masking schedules (linear, cosine, polynomial, geometric) on ground-truth length audio generation quality using FAD, WER, and AudioBox Aesthetics metrics on our train mixture. Models are evaluated across CFG scales 1.0-10.0. Figure 25 Masking schedule ablation across guidance scales. We evaluate the generation quality of four masking schedules (linear, cosine, polynomial, geometric) in CC12M and our train mixture (eval seed). Models are evaluated across CFG scales 1.010.0. selected among larger set of generations based mix of quality filtering and diversity. (a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l) (m) (n) (o) (p) (q) (r) (s) (t) Figure 26 Samples generated by our model with different prompts. See Table 7 for the extensive list of prompts. 33 (u) (v) (w) (x) (y) (z) (aa) (ab) (ac) (ad) (ae) (af) (ag) (ah) (ai) (aj) (ak) (al) (am) (an) Figure 27 Samples generated by our model with different prompts. 34 (ao) (ap) (aq) (ar) (as) (at) (au) (av) (aw) (ax) (ay) (az) (ba) (bb) (bc) (bd) (be) (bf) (bg) (bh) Figure 28 Samples generated by our model with different prompts. (bi) (bj) (bk) (bl) (bm) (bn) (bo) (bp) (bq) (br) (bs) (bt) (bu) (bv) (bw) (bx) (by) (bz) (ca) (cb) Figure 29 Samples generated by our model with different prompts. 36 (a) Lagerstroemia macrocarpa Wall, Queens flower, Queens crape myrtle, Lythraceae. Close-up of vibrant pink flower with yellow center in full bloom, sharply focused against blurred green outdoor background. Numerous small green buds at different stages surround the flower on thin green stems. (b) cozy living room with plush brown sofa and patterned pillows. wooden coffee table sits in front, framed wall art above, rustic wooden ceiling, and bed with white comforter in the background. Decorative lighting and wall hangings add warmth. (c) photograph of rainbow trout in body of water. The trout is facing the left side of the image. The water is clear and there are some small rocks at the bottom of the water. There are also some small fish in the water. (d) close-up photograph of painting with variety of colors. The colors are bright and vibrant. The painting is made up of small squares of color that are all the same size. (e) Watercolor drawing of lady with punk makeup. (f) teddy bear lies alone among the snow. (g) close-up photo of dried white flowers with brown centers, surrounded by purple and green dried flowers. The arrangement is set against plain white wall, emphasizing texture and muted tones. (h) digital illustration of various food items rendered in black and white, arranged in grid pattern on white background. (i) grand palace, likely the Palace of Versailles, with gold doors, statues, columns, and clock tower. formal garden with trimmed hedges and colorful flowers stands in front under clear, sunny sky. (j) Fresh red cherries with stems in white bowl with blue rim on teal surface. few cherries and green leaf lie outside the bowl, with softly blurred background emphasizing freshness. (k) wide shot of fast-flowing blue river with white-water rapids, surrounded by steep rocky cliffs covered in dense green trees. (l) close-up photo of black and white dog looking up to the right. The dog wears brown collar with small bronze pendant against dark brown background. (m) tree in full bloom with deep lavender lilac flowers and vibrant green leaves, set against clear blue sky with no clouds. (n) Composition with bread and rolls on kitchen table. (o) These windows and those in the next photo depict scenes from the Old Testament. (p) Brown boots with white heart designs on the toes, resting on moss-covered ring. blurred tree trunk and branches appear in the background. (q) white bowl of mashed sweet potatoes garnished with sage, black pepper, and sea salt. In the background are gravy boat, bread, green beans, and pumpkin. (r) golden retriever wearing crown of yellow and white flowers, looking to the right, set in green grassy field. (s) Three pink cocktails on circular wooden tray, each garnished with lemon slices and mint, set against white wall and green table. (t) painting of young girl in pink dress and red coral necklace holding flowers in lush landscape with forest and stream, conveying innocence and childhood beauty. Table 7 Prompts used to generate images in Figure 26. 37 (u) serene landscape with snow-capped mountain beneath clear blue sky. Tall golden grass fills the foreground, distant trees add contrast, and warm lighting suggests early morning or late afternoon. (v) close-up photo of circular apple pie on wooden surface, with apples arranged in circular pattern and dusted with powdered sugar. (w) cartoon parrot wearing blue-and-white sailor hat with an anchor, smiling while holding guitar. The parrot has green and yellow feathers, an orange beak, blue eyes, and stands on white background. (x) Five bowls of chia pudding arranged in grid on white surface, topped with peanut butter, berries, kiwi with chocolate chips, nuts with honey, and banana with granola under bright lighting. (y) close-up photo of hand holding rolled up 20 Euro note. The background is out of focus, but there are green leaves and what appears to be tree. (z) close-up photo of small pizza topped with red sauce, cheese, and herbs on white plate placed on wooden table. (aa) Fresh silver fish with pinkish tones displayed on crushed ice in supermarket. The fish overlap slightly, with finely shaved white ice keeping them fresh and the background cropped tightly. (ab) An abandoned brick building with sloped roof and chimney. Three doors are boarded up, one is open, and overgrown grass and trees suggest long-term neglect. (ac) Metallic bowls arranged in semi-circle, each filled with vibrant powdered pigments including pink, blue, yellow, and green, photographed in close-up on dark surface. (ad) blue illustration of skeleton running. The skeleton is highlighted in orange. The background is black. (ae) Expanded horse nostrils with rime on facial hairs (af) Rustic pumpkin soup in white bowl on woven mat, garnished with black pepper. Pumpkin seeds and whole pumpkin appear nearby, creating warm autumnal setting. (ag) black and white illustration of shaggy dog with pointed ears, wide eyes looking forward, and slightly open mouth against white background. (ah) traditional Andalusian white village in Casares, Spain, with tightly packed white buildings and terracotta roofs built along hillside, surrounded by greenery and lacking modern infrastructure. (ai) Watercolor botanical illustrations of pink lotus and rose with soft brushstrokes and detailed petals, isolated on white background in delicate hand-painted style. (aj) colorful Day of the Dead illustration featuring skull wearing sombrero and holding maracas, surrounded by flowers on clean white background. (ak) handshake between two men in suits, one seated at desk with papers and pen, the other standing and smiling, with bookshelf visible behind them. (al) Cactus plants with pink flowers in brown pots placed on wooden table, set against tiled flooring and brownand-yellow tiled wall. (am) Stones and shells arranged in the shape of flower on plain white background. (an) Vintage hand drawn rustic wreath with cute spring flowers and hand written text Happy Mothers Day Table 8 Prompts used to generate images in Figure 27. 38 (ao) Skewers with charred meat, roasted golden potatoes, and roasted red tomatoes served on white plate with brown rim. The background is softly blurred with table elements visible. (ap) Blossom fruit. beautiful spring. (aq) An adult cow and calf lying on straw inside wooden barn. The cow is brown with white patches, the calf mostly white with brown markings, lit by soft natural light. (ar) Slow Down Tours on the train. (as) calm beach scene with large wet boulder in the foreground, sandy shore, gentle blue waves, and clear sky creating tranquil atmosphere. (at) forest landscape with green, red, and yellow foliage in the foreground and mountain covered in autumn leaves in the background. (au) Gorgeous portrait of blue peacock with silky blue feathers. (av) large rock formation covered in green plants with light blue water in front and cloudy sky above. (aw) tall glass of green beverage with purple lid on wooden surface, surrounded by colorful signage with Chinese characters, suggesting cafÃ© or food stall setting. (ax) An ancient stone wall with Mayan carvings depicting figures, animals, and geometric shapes, weathered and textured in light gray stone. (ay) Abandoned barn in Sauk County, Wisconsin - LOCs Public Domain Archive Public Domain Search. (az) Wrapped Up In Wool Penguin. (ba) utility pole with multiple electrical wires and street lamp in the foreground, with dark wooden building and partly cloudy blue sky behind it. (bb) cozy bedroom with large bed, white comforter, beige walls, framed pictures, seating furniture, rug, and vintage chandelier with candle-style lights. (bc) golden porcelain Turkish coffee cup and saucer with ornate detailing and lion-shaped handle, isolated on reflective surface against white background. (bd) person in white dress holding bouquet of white and pink roses with green foliage, set against natural green background. (be) detailed Carcassonne Mini board game tile depicting medieval village with church, temple, river, and trees, rendered in realistic illustrated style. (bf) carved Halloween pumpkin with triangular eyes and jagged mouth, wearing black witch hat, resting on dark leaves in spooky setting. (bg) white horse standing inside fenced area with red-and-white striped barrier, facing the camera with alert ears and trees in the background. (bh) collage of contemporary furniture pieces including beds, shelving, dressers, mirrors, and chandeliers displayed across modern bedroom interiors. Table 9 Prompts used to generate images in Figure 28. 39 (bi) wide shot of valley surrounded by snow-covered mountains under cloudy sky, evenly lit by natural daylight. (bj) white lighthouse on rocky cliff surrounded by green vegetation, overlooking the ocean beneath clear blue sky with scattered clouds. (bk) creeping thistle flower with vibrant pink center on sepia-toned old paper background, softly lit with blank space for text. (bl) dimly lit dining table set with plates of food and wine glasses. Three wine bottles are visible, chairs surround the table, and wooden wall adds cozy mood. (bm) Streets of Kanazawa - japanese, japan, kanazawa, street, building, oriental, lantern. (bn) white bowl of soup with meat, carrots, and greens on wooden table, with salt and pepper shakers, tomato, and beige napkin nearby. (bo) close-up view of stalactites hanging from cave ceiling, with rugged textured walls and contrasting light and shadow highlighting natural formations. (bp) rocky shoreline at Nha Trang beach, Vietnam, with turquoise water creating white foam, calm sea beyond, and clear sky with scattered clouds. (bq) spider positioned at the center of its web, facing the camera, surrounded by green leaves with softly blurred background and light specks. (br) Ship wreck \"Superior Producer\" in turquoise water of coral reef in Caribbean Sea. (bs) bright cityscape with numerous beige buildings of varying roof styles, trees on the left, and clear sunny weather. (bt) tree with thick trunk near dirt path leading into canyon filled with orange rock formations and scattered pine trees. (bu) cappuccino in white porcelain cup and saucer on wooden table, topped with floral chocolate design and photographed with soft blur. (bv) quiet winter forest with snow-covered path winding through densely packed trees under an overcast sky. (bw) four-image collage showing stone buildings, glass structure with purple light, and buildings near large bodies of water. (bx) collage of 36 cat photos arranged in uniform grid, each cat looking at the camera against bright background. (by) lush hillside with green grass, trees, and large boulder in the foreground, with cloudy skies over the Bulbul hills in the background. (bz) panoramic cityscape showing colorful residential buildings in the foreground and an industrial area with smokestacks under clear blue sky. (ca) vibrant garden with tree full of ripe oranges near swimming pool, featuring potted plants and an umbrella in bright light. (cb) Different spring blossoms in little bottle with nature background. Table 10 Prompts used to generate images in Figure 29."
        },
        {
            "title": "H Contributions",
            "content": "All authors contributed to writing this paper, designing the experiments and discussing results at each stage of the project. Code. General training code was written by Jason Ramapuram, Victor Turrisi, Louis BÃ©thune and Vishnu Banna. Bruno Mlodozeniec and Dan Busbridge extended the baseline functional optimizers to support MuP and CompleteP. Evaluators were written by Pau Rodriguez Lopez in collaboration with Louis BÃ©thune and Arno Blaas. Scaling laws experiments were sculpted and executed by Louis BÃ©thune in discussions with AmiExperiments. tis Shidani, Pierre Ablin and Jason Ramapuram. Main model (Section 3.1) was trained by Victor Turrisi in discussions with Jason Ramapuram and Louis BÃ©thune. Bruno Kacper Mlodozeniec wrote and executed the per-module hyper-parameter search (Section D) and crafted ğµcrit experimental procedure that was executed by Louis BÃ©thune (Section 4.1). Inference ablations were crafted and executed by Lokesh Boominathan and Nikhil Bhendawade in discussions with Theo X. Olausson (Section 7.3). Data mixtures experiments (Section 7.2) were designed by Pierre Ablin and Louis BÃ©thune, and executed by Nikhil Bhendawade and Louis BÃ©thune. JoÃ£o Monteiro crafted and executed the anti-masking experiments (Section 7.4) in discussions with Victor Turrisi, Jason Ramapuram, Louis BÃ©thune and Amitis Shidani. Tokenizers were trained and benchmarked by Paul Dixon (Section B.1) and Devon Hjelm (Section B.2) in discussions with Jason Ramapuram and Victor Turrisi. Data. The data loading library was built by Victor Turrisi in collaboration with Louis BÃ©thune. Data collection and pre-processing was done by Louis BÃ©thune, Victor Turrisi, Joris Pelemans, Kari Noriy, Jason Ramapuram, Luca Zappella and Nikhil Bhendawade. General Infrastructure. Nick Henderson built the pipeline to build docker containers with all optimizations for networking and high-performance training. Theoretical formulation and situating work. The theoretical framework (Sections and 3) was crafted by Amitis Shidani in discussions with Pierre Ablin, Devon Hjelm and Arno Blaas. Grounding work with respect to relevant literature executed by Arno Blaas (Section 2) in discussions with Amitis Shidani and Jason Ramapuram. Project Organization and Tech Lead. Overall project organization and guidance enabled by Irina Belousova, Luca Zappella, Russ Webb and Jason Ramapuram. Jason Ramapuram organized, setup scientific objectives, provided technical leadership and setup the preliminary fault tolerant, distributed scalable code-base for the project."
        },
        {
            "title": "I Acknowledgments",
            "content": "We thank Samy Bengio, Jerremy Holland, Erik Wijmans, David Koski, Miguel Sarabia del Castillo, for their helpful feedback and critical discussions throughout the process of writing this paper; Michael Brooks, Denise Hui, Li Li, Rajat Phull, Evan Samanas, Guillaume Seguin, and the wider Apple infrastructure team for assistance with developing and running scalable, fault tolerant code. Names are in alphabetical order by last name within group."
        }
    ],
    "affiliations": [
        "Apple",
        "Google Deepmind",
        "MIT",
        "University of Cambridge"
    ]
}