{
    "paper_title": "WonderZoom: Multi-Scale 3D World Generation",
    "authors": [
        "Jin Cao",
        "Hong-Xing Yu",
        "Jiajun Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to \"zoom into\" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/"
        },
        {
            "title": "Start",
            "content": "WonderZoom: Multi-Scale 3D World Generation Jin Cao* Hong-Xing Yu*"
        },
        {
            "title": "Stanford University",
            "content": "https://wonderzoom.github.io/ 5 2 0 2 9 ] . [ 1 4 6 1 9 0 . 2 1 5 2 : r Figure 1. Multi-scale 3D world generation from single image. WonderZoom enables interactive exploration across spatial scales. Users can zoom into any region and specify prompts to generate new fine-scale content while maintaining cross-scale consistency. Here we show three zoom-in sequences. We attach an interactive viewer in the supplmentary material."
        },
        {
            "title": "Abstract",
            "content": "We present WonderZoom, novel approach to generating 3D scenes with contents across multiple spatial scales from single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users *Equal contribution. to zoom into 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multiscale 3D world creation from single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/. 1. Introduction 3D world generation has emerged as transformative capability in computer vision, enabling the synthesis of immersive environments from minimal input [7, 9, 13, 24, 50, 51]. However, despite the inherently multi-scale nature of realworld scenes, existing approaches remain fundamentally constrained to single-scale generation. They can produce landscape-level environments and room-scale scenes, but fail to synthesize coherent content across multiple spatial scales, e.g., tiny ladybug lying on sunflower in vast field. This limitation prevents existing approaches from generating rich, detailed worlds that span from panoramic vistas down to microscopic surface details, restricting their applicability for interactive exploration and content creation. The fundamental challenge underlying this limitation is the absence of scale-adaptive 3D representation suitable for scene generation. Traditional Level-of-Detail (LoD) representations [26] were designed for efficiently rendering pre-existing graphics content, where all geometric details are known in advance. Recent hierarchical representations like Hierarchical 3D Gaussian Splatting [17] and Mip-NeRF [1] extend these principles to neural reconstruction, efficiently encoding scenes at multiple scales. But critically, they still assume access to complete multi-scale image data upfront for one-pass optimization. Both paradigms, rendering and reconstruction, fundamentally conflict with generation, where images do not exist priori and must be synthesized progressively. In generation, we must create coarse-scale content first, then iteratively synthesize finer details conditioned on both the coarser structure and user-specified prompt and regions of interest. This requires representations that can grow dynamically as new fine-scale content is generated, not static hierarchies optimized with complete supervision. Current generation methods [50, 51] sidestep this challenge entirely by restricting themselves to single scales, while naive application of existing hierarchical representations would demand generating all scales simultaneously, which is computationally intractable approach that violates the inherent coarse-to-fine nature of multi-scale synthesis. To address this challenge, we propose WonderZoom, novel framework for multi-scale 3D world generation from single image. Our approach introduces two key technical innovations: (1) scale-adaptive Gaussian surfels, dynamically updatable hierarchical representation that, unlike existing multi-scale methods, supports incremental refinement as new content is generated. It allows adding arbitrary levels of detail without re-optimization, and (2) progressive detail synthesizer that iteratively generates fine-grained 3D structures conditioned on both coarser scales and userspecified regions and viewpoints. These components work synergistically: the scale-adaptive representation provides persistent 3D canvas that grows in detail over time, while the synthesizer produces coherent multi-scale content through controlled coarse-to-fine generation process. By enabling dynamic updates to the 3D representation as new scales are synthesized, WonderZoom fundamentally shifts from the reconstruction paradigm to multi-scale generation, overcoming the computational and architectural barriers that constrain existing methods to single scales. Our approach enables users to interactively zoom into any region of the generated 3D scene, triggering autoregressive synthesis of previously non-existent details, e.g., from an entire landscape down to microscopic surface features. Unlike traditional multi-resolution rendering that simply reveals pre-existing details, WonderZoom generates new content on-demand, creating coherent structures that were never part of the original input or coarse generation. This capability allows infinite exploration of generated worlds at arbitrary levels of detail. In summary, our contributions are threefold: We propose WonderZoom, the first approach to enable multi-scale 3D world generation from single image, supporting seamless transitions from macro to micro scales. We introduce scale-adaptive Gaussian surfels, dynamically updatable representation that grows incrementally with newly generated finer-scale content, while maintaining real-time rendering performance. We demonstrate and evaluate multi-scale 3D generation across diverse scenarios including natural environments, villages, and urban scenes, achieving consistent quality across scale transitions while significantly outperforming state-of-the-art video and 3D generation models in both perceptual quality and prompt alignment. 2. Related Work 3D World Generation. Early 3D scene generation methods focused on novel view synthesis from single image, constructing renderable representations like layered depth images [33, 38], radiance fields [34, 36, 49], multi-plane images [37, 56], and point features [28, 43], though these only supported small viewpoint changes from the input. Later works explored generating more significant viewpoint changes and multiple connected scenes. Infinite Nature [24] and its follow-ups [4, 5, 21] pioneered perpetual view generation for natural scenes with neural renderer. Recent methods [20, 22, 35, 48, 55] expanded this capability to explicit 3D, e.g., SceneScape [9] and Text2Room [13] generate meshes from text prompts, WonderJourney [50] and WonderWorld [51] creates diverse connected 3D scenes using LLMs and point-based representations, LucidDreamer [7] and CAT3D [10] focus on room-scale environments with 3D Gaussian splatting. Another line of work specializes in city-scale generation [8, 23, 45, 46], producing largescale 3D Gaussian splatting representations of urban environments. However, these methods operate at single spatial scale aligned with their inputgenerating either landscapes, rooms, or cities, but not content across scales. In contrast, we enable multi-scale 3D generation where users can progressively zoom into any region to synthesize entirely new content at finer scales, creating details that were never visible or implied in the original input image. 2 Multi-scale 3D Representations. Classical computer graphics has long addressed multi-scale rendering through Levelof-Detail (LoD) techniques [26], which adaptively select geometric complexity based on viewing distance, and mipmapping, which precomputes texture pyramids for efficient antialiased rendering. These traditional methods assume all geometric and texture details exist upfront, making them suitable only for rendering pre-authored content, not for progressive generation. Recent neural 3D reconstruction methods have incorporated similar multi-scale principles, e.g., MipNeRF [1] introduces integrated positional encoding to handle scale ambiguity, with extensions like Mip-NeRF 360 [2] and Zip-NeRF [3] improving unbounded scene representation. In the Gaussian splatting [16] domain, Mip-Splatting [52] addresses aliasing through 3D smoothing filters, while Hierarchical 3D Gaussian Splatting [17] builds explicit LoD hierarchies for efficient rendering. Octree-GS [30] and ScaffoldGS [25] use spatial hierarchies to manage primitives across scales. However, both traditional LoD and these neural hierarchical representations share critical limitation: they are fundamentally designed for scenarios where content at all scales is known: either pre-authored (traditional LoD) or reconstructed from complete multi-scale image supervision (neural methods). This paradigm is incompatible with generation, where fine-scale content must be synthesized progressively without pre-existing data. Our approach addresses this gap by organically integrating scale-adaptive representation that can be dynamically refined with progressive generation pipeline. Controllable Content Synthesis. Controllable video generation methods have made significant strides in conditional synthesis, accepting camera trajectories [11, 32], depth maps [53], or semantic masks as inputs to guide generation. However, these approaches cannot perform multi-scale generation due to the absence of training data that captures coherent content across vastly different spatial scales. Superresolution techniques have evolved from 2D image enhancement to 3D domains, including mesh refinement, point cloud upsampling [54], and neural field super-resolution [39]. Yet these methods focus on sharpening and refining pre-existing content rather than generating entirely new cross-scale structures. recent work, Generative Powers of Ten [42], demonstrates infinite zoom generation by jointly sampling multiple scales through coordinated diffusion processes, though this remains limited to 2D images. Hierarchical generation approaches like Progressive GANs [15] and cascaded diffusion models [12] synthesize content at increasing resolutions through staged refinement. Our approach uniquely extends these capabilities to 3D, combining controllable generation with true multi-scale synthesisenabling users to interactively zoom into any region and generate coherent new content across vastly different spatial scales, from environmental to microscopic levels that never existed in the original input. 3. Approach Formulation. We target multi-scale 3D world generation from single image. Given an input image I0 and sequence of user-specified prompts {U1, . . . , Un} with corresponding camera viewpoints {C0, . . . , Cn}, Ci R44 that progressively zoom into regions of interest, our goal is to generate sequence of 3D scenes {E0, E1, . . . , En} at increasing spatial granularities. Here, E0 represents the initial 3D scene reconstructed from the input image I0, while each subsequent scene Ei (i > 0) represents finer-scale content that is spatially contained within the previous scene Ei1, creating nested hierarchy where zooming reveals newly synthesized details rather than pre-existing geometry. This process can be repeated multiple times from the same initial image I0 with different camera sequences and prompt sequences. Figure 1 illustrates this capability, where we demonstrate three distinct zoom sequences from single input. Challenges. The major technical bottleneck preventing multi-scale generation is the lack of scale-adaptive 3D representations suitable for generation. Existing multi-scale representations, from classical Level-of-Detail techniques to recent hierarchical methods like Hierarchical 3D Gaussian Splatting [17], are designed for either rendering pre-existing graphics content or reconstruction with complete multi-scale image supervision available upfront. However, generation imposes fundamentally different requirements: we need to create coarse-scale content Ei1 first, then iteratively synthesize finer details Ei conditioned on both the coarser structure Ei1 and user-specified prompts Ui and regions of interest defined by Ci. This demands representations that can grow dynamically as new scales are generated while maintaining real-time rendering capability, capability absent in existing methods that assume static, pre-optimized hierarchies. Another challenge lies in synthesizing semantically meaningful content that follows user prompts Ui while maintaining geometric and appearance consistency with previous scales Ei1. Unlike simple super-resolution that merely enhances existing details, we may need to generate entirely new structures (e.g., bird or lizard as in Figure 1) that were not implied in the coarser representation. Overview. We propose WonderZoom to enable multi-scale 3D world generation through two key technical innovations. To address the first challenge, we introduce scale-adaptive Gaussian surfels (Sec. 3.1) that allow dynamic updates without re-optimization. This representation enables adding arbitrarily many scales Ei while maintaining real-time rendering capability at any scale, as new finer-scale surfels can be seamlessly integrated into the existing hierarchy without modifying coarser levels. To address the second challenge, we design progressive detail synthesizer (Sec. 3.2) that gen3 Figure 2. WonderZoom overview. From an input image, we first reconstruct an initialized 3D scene. Users specify prompts and camera viewpoints to generate finer-scale content. Our progressive detail synthesizer creates new-scale images, registers depth to maintain geometric consistency, and synthesizes auxiliary views for complete 3D scene creation. Our scale-adaptive Gaussian surfels enable dynamic updates without re-optimization, seamlessly integrating new content while preserving real-time rendering. erates new fine-grained 3D structures Ei from user prompts Ui while ensuring consistency with the previous scale Ei1. The synthesizer leverages the coarse geometry as spatial conditioning to guide the generation of coherent fine-scale content, going beyond simple super-resolution to create semantically meaningful details. We show an illustration of our framework in Figure 2. We summarize the complete multi-scale generation control loop in Algorithm 1 in supplementary material. 3.1. Scale-adaptive Gaussian Surfels Definition. We introduce scale-adaptive Gaussian surfels to represent our multi-scale scenes {E0, . . . , En}. Formally, we model the scenes as radiance field represented by set of Gaussian surfels {gj}N j=1. Each surfel is parameterized as = {p, q, s, o, c, snative}, where denotes the 3D spatial position, denotes the orientation quaternion, = [sx, sy] denotes the scales of the x-axis and y-axis, denotes the opacity, and denotes the view-independent RGB color. The Gaussian kernel follows the same formulation as in prior work [51], with covariance matrix Σ = Qdiag(s2 y, ϵ2)QT where is the rotation matrix obtained from and ϵ is small thickness parameter. The key addition is snative, the native scale at which the surfel was created, which enables scale-aware rendering as we describe later. In WonderZoom, we sequentially generate each scene, starting from E0 and progressively adding finer-scale content through En. This demands our representation to satisfy two requirements: (1) capable of dynamic updates given new scale images Ii at viewpoints Ci without re-optimizing exx, s2 isting surfels, and (2) supporting real-time rendering at any observation scale. Dynamic updating. The core idea of our dynamic representation is that we incrementally add new surfels to represent each new scale without modifying existing ones. When we create the initial scene E0 from the input image I0, we generate N0 surfels to represent the coarse-scale geometry and appearance. When we subsequently generate the finer-scale scene E1 from zoomed-in view I1 at camera C1, we add N1 new surfels to the representation, resulting in total of = N0 + N1 surfels. This process continues: when generating Ei, we add Ni new surfels, bringing the total to = (cid:80)i k=0 Nk. Crucially, the surfels from previous scales remain unchanged: we only append new surfels that capture the finer details visible at the current scale. This additive mechanism naturally enables dynamic updates: each new scale simply extends the existing representation rather than requiring global re-optimization, allowing the multi-scale world to grow organically as users explore different regions at increasing levels of detail. Scale-aware opacity modulation for real-time rendering of multi-scale scenes. Since we represent multi-scale content with surfels across different scales, the same surface may be covered by multiple layers of surfels from E0 through Ei. Directly rendering all surfels would cause aliasing and reduce rendering speed. To address this, we introduce scaleaware opacity modulation based on each surfels native scale: snative = (cid:113) dnative native native (1) 4 (cid:113) render render where dnative is the surfels depth relative to Ci (the camera view where the surfel was created) and native , native are the focal lengths of Ci. During rendering at camera Crender, we compute the current rendering scale srender = drender/ for each surfel. For surfels at intermediate scales (0 < < n), we define parent and child scale bounds: sparent = dparent/ where dparent and parent are defined relative to Ci1, and schild = where dchild and child are defined reladchild/ child tive to Ci+1. The rendered opacity is then modulated as: parent parent child (cid:113) (cid:113) = α, (2) where α = 1 log(sparent)log(srender) log(sparent)log(snative) log(srender)log(schild) log(snative)log(schild) 1 if no parent and srender snative if sparent srender snative if snative srender schild if no child and srender snative otherwise. (3) This design ensures surfels are most visible at their native scale (α = 1 when srender = snative) and fade smoothly when viewed at different scales. Notably, surfels at the coarsest scale (i = 0) remain fully visible when zoomed out, while surfels at the finest scale (i = n) remain fully visible when zoomed in, ensuring complete scene coverage at all observation scales. Proposition 1 (Seamless Scale Transition). Our scaleaware opacity modulation ensures smooth visual transitions between adjacent scales without discontinuities. Specifically, consider two surfels gj and gk located at the same 3D position but created at adjacent scales Ei1 and Ei respectively. When the rendering scale srender transitions between their native scales, i.e., when srender [snative , snative ], the sum of their modulated opacj ity weights satisfies: αk(srender) + αj(srender) = 1. (4) This property holds because the linear interpolation in log space for gk decreasing from its native scale matches exactly with the complementary interpolation for gj increasing toward its child scale bound. As result, the total contribution from overlapping surfels at different scales remains constant during zoom operations, eliminating popping artifacts and ensuring visually continuous scale transitions. This partition of unity is fundamental to maintaining coherent appearance as users navigate across the multi-scale hierarchy. 5 Optimization. Our scale-aware opacity modulation preserves the differentiability of the rendering pipeline, thereby we use gradient-based optimization for surfel parameters. When creating surfels for new scale Ei from image Ii, we generate pixel-aligned surfels following the same approach as prior work [51], where each surfel corresponds to pixel in Ii. We also follow the same geometry-based initialization: each surfels position is initialized using the estimated depth map via back-projection, orientation from the estimated surface normal, and scales according to the Nyquist sampling theorem to ensure appropriate coverage without excessive overlap. The color is initialized from the corresponding pixel RGB values, the native scale snative is computed based on the creation viewpoint Ci, and opacity is initialized to = 0.1 for stable optimization. We then optimize the opacity, orientation, and scales (while keeping positions, colors, and native scales fixed) using Adam [19] with photometric loss = 0.8L1+0.2LD-SSIM [16] against the input image Ii. This lightweight optimization refines the surfel geometry while preserving the multi-scale structure. 3.2. Progressive Detail Synthesizer }K Goal. Given the coarse-scale scene Ei1, target camera viewpoint Ci, and user prompt Ui, our goal is to generate an image Ii and its corresponding depth map Di that are geometrically consistent with Ei1 while incorporating the content specified in Ui. Note that Ui may describe entirely new structures not visible or implied in Ei1 (e.g., ladybug on sunflower), requiring our approach to go beyond simple super-resolution to synthesize semantically meaningful content. Since we aim to generate complete 3D scene Ei that can be rendered from varying viewpoints, we additionally generate set of auxiliary images {Ik k=1 from neighboring viewpoints to augment Ii, enabling optimization of more complete 3D structure that extends beyond the single input view. This subsection describes our three-stage pipeline: new scale image generation from the coarse scene and prompt, scale-consistent depth registration to maintain geometric coherence, and auxiliary view synthesis for complete 3D reconstruction. New scale image synthesis. To generate the finer-scale image Ii, we begin by rendering coarse observation from the previous scale: Oi = render(Ei1, Ci), where Ci has larger focal length than Ci1 to zoom into the region of interest. Since Oi is obtained through direct zoom-in rendering and thus lacks fine details, we apply extreme superresolution to synthesize high-frequency content. However, extreme zoom ratios require additional semantic guidance beyond what is visible in Oi. We therefore extract semantic context from the previous scale using vision-language model (VLM): = VLM(Oi1), where Oi1 is the rendered image at the previous scale. The super-resolved image is then generated as = SR(Oi, S), conditioned on both the coarse observation and semantic context. To incorporate user-specified content Ui that may include entirely new structures absent in Ei1, we apply controllable image editing model: Ii = Edit(I i, Ui). This two-stage approach super-resolution followed by semantic editingenables both faithful detail enhancement of existing structures and insertion of novel content specified by the user. Scale-consistent depth registration. To estimate depth map Di that maintains geometric consistency with Ei1, we employ multi-stage registration approach. First, we render target depth map from the existing geometry: Dtarget = render depth(Ei1, Ci), which provides sparse depth values for regions visible in the previous scale. We then fine-tune monocular depth estimator Dθ to align its predictions with this target depth by minimizing: (cid:80) u,v Dtarget Ldepth = (u, v) Dθ(Ii)(u, v) m(u, v) , (cid:80) u,v m(u, v) (5) where m(u, v) = 1 if Dtarget (u, v) is defined, and m(u, v) = 0 for undefined regions due to zoom-in effect. This finetuning ensures that the estimated depth Di = Dθ(Ii) aligns with the coarse geometry while still predicting reasonable depths for newly visible regions. To further refine the registration, we apply segment-wise depth alignment using SAMgenerated masks to correct for local depth inconsistencies as in prior work [50, 51], and for any newly added structures from the editing stage (e.g., the ladybug in Figure 2), we use Grounded SAM [31] to isolate these regions and estimate their depth while maintaining consistency with surrounding geometry. Auxiliary view synthesis. While Ii provides detailed content at the target viewpoint Ci, single image is insufficient to reconstruct complete 3D scene that can be rendered from arbitrary viewpoints. To address this, we synthesize auxiliary views {Ik k=1 from neighboring camera positions using camera-controlled video diffusion model. We first render conditioning frames from the current partial scene: {Ok )}K is the inii tial scene constructed from Ii alone, and {Ck } are camera viewpoints sampled around Ci. Along with these frames, we generate corresponding masks {Mk } indicating regions requiring synthesis (e.g., occluded areas not visible in Ii). The video diffusion model then generates temporally consistent frames: {Ik }), conditioned on the partial observations and masks. We then leverage video depth model to estimated depth {Dk } for these generated frames, and the resulting image-depth pairs are used to optimize more complete 3D scene following the same optimization procedural as described in Sec. 3.1. This auxiliary view synthesis enables us to construct complete 3D scenes Ei that extend beyond the single input view while maintaining coherence with the generated content. In practice, we also } = {render(E partial } = VideoDiff({Ok k=1, where partial }, {Mk }K , Ck apply it to help generate the coarsest-scale scene E0. 4. Experiments In our experiments, we evaluate WonderZoom on multi-scale world generation and compare it to existing methods. We also perform ablation studies to analyze WonderZoom. Baselines. We are not aware of any prior method that allows multi-scale 3D scene generation. Therefore, we consider state-of-the-art methods in general-purpose 3D scene generation including WonderWorld [51] and HunyuanWorld [35]. Besides 3D-based approaches, we further include state-ofthe-art camera-controlled video generation models, including Gen3C [32] and Voyager [14]. We use these baselines official codes for comparison. Test examples. For comparison with the baselines, we collect publicly available real images and generate synthetic images as our testing examples, and we also use examples from Wang et al. [42]. We evaluate on 32 generated scenes from 8 test input images, spanning diverse scene types such as field, city, forest, and underwater. Among them, sunflower image and coral image are synthetic, and all others are real images. For each test example, we generate 4 new-scale scenes in additional to the input scale, i.e., we generate {E0, , E4}. For fair comparison, we use fixed camera paths and the same text prompts for all methods. Metrics. For quantitative comparison, we adopt the following evaluation metrics: (1) We collect 200 human study two-alternative force choice (2AFC) results on the rendering of new scale scenes, i.e., {E1, , E4}. (3) To evaluate the alignment of generated scenes w.r.t. text prompts, we render 9 sudoku-like novel views around each generated scene Ei, 1 4, and compute the CLIP [29] scores of the prompt versus the rendered images. (4) We evaluate rendered novel view image quality with CLIP-IQA+ [40], Q-align IQA [44], and NIQE [27]. (4) We also measure the aesthetics of novel views by the Q-align IAA [44]. We leave more details in the supplementary material. Implementation details. In our implementation, we use Chain-of-Zoom [18] as our super-resolution model. We use Gen3C [32] as the camera-controlled video diffusion model in auxiliary view synthesis. We estimate image depth by MoGe [41] and video depth by GeometryCrafter [47]. We leave more details in the supplementary material. We will release the full code and software for reproducibility. 4.1. Comparison Qualitative showcase. We show qualitative comparison in Figure 3 and in the appendix. Meanwhile we show more examples generated by our method in Figures 4. We also strongly encourage the reader to see video results and to interactively view generated worlds in the HTML in our supplementary materials. From the qualitative comparison, Figure 3. Comparison of WonderZoom with baselines on multi-scale 3D world generation. Figure 4. Qualitative results of WonderZoom on multi-scale 3D world generation. we find that the state-of-the-art 3D scene generation methods and the controllable video generation methods are not able to create multi-scale scenes. In particular, 3D methods always generate blurry zoom-in views as their 3D scene representations (i.e., Gaussian surfels in WonderWorld [51] and meshes in HunyuanWorld [35]) do not support dynamic updating 7 Method CS CIQA QIQA NIQE QIAA Time/s WonderWorld [51] HunyuanWorld [35] Gen3C [32] Voyager [14] WonderZoom (Ours) 0.2687 0.2510 0.3004 0.2609 0.3432 0.5064 0.2827 0.5489 0.5746 0. 1.081 1.058 2.992 3.148 3.926 21.74 15.21 4.924 4.913 3.695 1.339 1.302 2.018 2.929 2.986 9.3 704.2 306.7 596.6 62.1 Table 1. Quantitative comparison. CS denotes CLIP score, CIQA denotes CLIP-IQA+, QIQA denotes Q-align IQA, QIAA denotes Q-align IAA, and Time measures the time used in generating new-scale scene. views are not aligned with the prompts. In contrast, WonderZoom allows creating new scale structures that are closely aligned with the prompts, and generates high-quality novel views at any new scale. Quantitative comparison. We show the quantitative metrics in Table 1 and 2. WonderZoom outperforms all baseline methods in terms of alignment, novel view quality, aesthetics metrics, as well as humans preferences. This further validates our observations through visual comparison. Zoom-in Accuracy Visual Quality Prompt Match 4.2. Ablation study Over WonderWorld [51] Over HunyuanWorld [35] Over Gen3C [32] Over Voyager [14] 80.7% 83.2% 77.8% 76.1% 98.3% 98.7% 83.8% 81.7% 98.2% 98.9% 96.1% 90.9% Table 2. Human study 2AFC results of favor rate of WonderZoom (Ours) over baseline methods. Figure 5. Ablation on the opacity modulation. Metrics Methods Ours w/o mod. Ours GPU memory FPS 7.96G 3.40G 1.4 97.2 Table 3. Comparison of computational cost for variants about scaleadaptive opacity modulation. Figure 6. Ablation study on our depth registration. Figure 7. Ablation study on auxiliary view synthesis. when new scale images are generated. Camera-controlled video models are able to zoom in, yet their control is imprecise compared to explicit 3D methods, and their generated 8 We evaluate how the key technical components affect the multi-scale generation performances. We focus on the scaleaware opacity modulation, depth registration, and auxiliary view synthesis. Scale-aware opacity modulation. We consider variant Ours w/o mod. which removes our scale-aware opacity modulation. We show visual comparison in Figure 5 and quantitative comparison on computational cost in Table 3. From the table, we can see that without our scale-aware opacity modulation, the computational burden makes it intractable for multi-scale real-time rendering. Furthermore, we observe from the visual result that it creates blurry renderings due to the lack of an appropriate mechanism for rendering multi-scale surfels. In contrast, ours maintains high-quality rendering while requiring lower GPU memory and providing much faster rendering speed. Depth registration. We consider variant Ours w/o depth registration that removes the scale-consistent depth registration from WonderZoom. We show visual comparison in Figure 6. As we can see in the comparison, removing our depth registration creates significant shape distortion on the new detail depth estimation, i.e., the newly generated beetle is distorted when observed from novel views. Our depth registration significantly alleviates this artifact. Auxiliary view synthesis. We compare our model with Ours w/o auxiliary view. As shown in Figure 7, our auxiliary view synthesis is critical in generating complete 3D scene, while removing it leads to missing regions as revealed by the grey areas. 5. Conclusion WonderZoom allows multi-scale 3D world generation from single image. Through the scale-adaptive Gaussian surfels and progressive detail synthesizer, we enable users to interactively zoom into any region and synthesize entirely new details while maintaining cross-scale consistency and real-time rendering. Our experiments demonstrate significant improvements over existing 3D-based and video-based methods in both visual quality and prompt alignment. WonderZoom opens new possibilities for interactive content creation and virtual world exploration across multiple scales. Limitations. WonderZoom can struggle in extreme zooming into pure texture regions (we show failure cases in the Appendix in the supplementary materials) because it relies on semantic cues to inform what to generate in the next scale. Future work may explore texture-specific priors or procedural generation that can hallucinate plausible micro-structures when semantic cues are insufficient."
        },
        {
            "title": "References",
            "content": "[1] Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 58555864, 2021. 2, 3 [2] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded In Proceedings of the anti-aliased neural radiance fields. IEEE/CVF conference on computer vision and pattern recognition, pages 54705479, 2022. 3 [3] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased gridbased neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19697 19705, 2023. 3 [4] Shengqu Cai, Eric Ryan Chan, Songyou Peng, Mohamad Shahbazi, Anton Obukhov, Luc Van Gool, and Gordon Wetzstein. DiffDreamer: Towards consistent unsupervised singleview scene extrapolation with conditional diffusion models. In ICCV, 2023. 2 [5] Lucy Chai, Richard Tucker, Zhengqi Li, Phillip Isola, and Noah Snavely. Persistent nature: generative model of In Proceedings of the IEEE/CVF unbounded 3d worlds. Conference on Computer Vision and Pattern Recognition, pages 2086320874, 2023. 2 [6] Jianqi Chen, Yilan Zhang, Zhengxia Zou, Keyan Chen, and Zhenwei Shi. Dense pixel-to-pixel harmonization via continuous image representation. IEEE Transactions on Circuits and Systems for Video Technology, pages 11, 2023. 1 [7] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. 1, 2 [8] Paul Engstler, Aleksandar Shtedritski, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Syncity: Training-free generation of 3d worlds. arXiv preprint arXiv:2503.16420, 2025. [9] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. Scenescape: Text-driven consistent scene generation. arXiv preprint arXiv:2302.01133, 2023. 1, 2 [10] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view diffusion models. arXiv preprint arXiv:2405.10314, 2024. 2 [11] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 3 [12] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. 3 [13] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d meshes from 2d text-to-image models. arXiv preprint arXiv:2303.11989, 2023. 1, [14] Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu, Zhenwei Wang, Junta Wu, Jie Jiang, Hui Li, Rynson WH Lau, Wangmeng Zuo, and Chunchao Guo. Voyager: Longrange and world-consistent video diffusion for explorable 3d scene generation. arXiv preprint arXiv:2506.04225, 2025. 6, 8 [15] Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In Proc. NeurIPS, 2021. 3 [16] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):114, 2023. 3, 5 [17] Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer, Alexandre Lanvin, and George Drettakis. hierarchical 3d gaussian representation for real-time rendering of very large datasets. ACM Transactions on Graphics, 43(4), 2024. 2, 3 [18] Bryan Sangwoo Kim, Jeongsol Kim, and Jong Chul Ye. Chain-of-zoom: Extreme super-resolution via scale autoregression and preference alignment, 2025. 6 [19] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5 [20] Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik-hang Lee, and Pengyuan Zhou. Dreamscene: 3d gaussian-based text-to-3d scene generation via formation pattern sampling. arXiv:2404.03575, 2024. [21] Zhengqi Li, Qianqian Wang, Noah Snavely, and Angjoo Kanazawa. Infinitenature-zero: Learning perpetual view generation of natural scenes from single images. In European Conference on Computer Vision, pages 515534. Springer, 2022. 2 [22] Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos Plataniotis, Sergey Tulyakov, and Jian Ren. Wonderland: Navigating 3d scenes from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 798810, 2025. 2 [23] Chieh Hubert Lin, Hsin-Ying Lee, Willi Menapace, Menglei Chai, Aliaksandr Siarohin, Ming-Hsuan Yang, and Sergey Tulyakov. Infinicity: Infinite-scale city synthesis. In ICCV, 2023. 2 [24] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from single image. In Proceedings of the IEEE/CVF International Con9 ference on Computer Vision, pages 1445814467, 2021. 1, [25] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d gaussians for view-adaptive rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2065420664, 2024. 3 [26] David Luebke, Martin Reddy, Jonathan Cohen, Amitabh Varshney, Benjamin Watson, and Robert Huebner. Level of detail for 3D graphics. Elsevier, 2002. 2, 3 [27] Anish Mittal, Rajiv Soundararajan, and Alan Conrad Bovik. Making completely blind image quality analyzer. IEEE Signal Processing Letters, 20:209212, 2013. 6 [28] Simon Niklaus, Long Mai, Jimei Yang, and Feng Liu. 3d ken burns effect from single image. ACM Transactions on Graphics (ToG), 38(6):115, 2019. 2 [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 6 [30] Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, and Bo Dai. Octree-gs: Towards consistent real-time rendering with lod-structured 3d gaussians. arXiv preprint arXiv:2403.17898, 2024. [31] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 6 [32] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Muller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed worldconsistent video generation with precise camera control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 3, 6, 8 [33] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 3d photography using context-aware layered depth inpainting. In CVPR, 2020. 2 [34] Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, Joao Henriques, Christian Rupprecht, and Andrea Vedaldi. Flash3d: Feed-forward generalisable 3d scene reconstruction from single image. arXiv:2406.04343, 2024. 2 [35] HunyuanWorld Team, Zhenwei Wang, Yuhao Liu, Junta Wu, Zixiao Gu, Haoyuan Wang, Xuhui Zuo, Tianyu Huang, Wenhuan Li, Sheng Zhang, et al. Hunyuanworld 1.0: Generating immersive, explorable, and interactive 3d worlds from words or pixels. arXiv preprint arXiv:2507.21809, 2025. 2, 6, 7, 8 [36] Alex Trevithick and Bo Yang. Grf: Learning general radiance field for 3d scene representation and rendering. In arXiv:2010.04595, 2020. 2 [37] Richard Tucker and Noah Snavely. Single-view view synthesis with multiplane images. In CVPR, 2020. 2 [38] Shubham Tulsiani, Richard Tucker, and Noah Snavely. Layerstructured 3d scene inference via view synthesis. In ECCV, 2018. 2 [39] Chen Wang, Xian Wu, Yuan-Chen Guo, Song-Hai Zhang, Yu-Wing Tai, and Shi-Min Hu. Nerf-sr: High quality neural radiance fields using supersampling. In Proceedings of the 30th ACM International Conference on Multimedia, pages 64456454, 2022. 3 [40] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In AAAI, 2023. 6 [41] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking accurate monocular geometry estimation for open-domain images with optimal training supervision, 2024. 6 [42] Xiaojuan Wang, Janne Kontkanen, Brian Curless, Steven Seitz, Ira Kemelmacher-Shlizerman, Ben Mildenhall, Pratul Srinivasan, Dor Verbin, and Aleksander Holynski. Generative powers of ten. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7173 7182, 2024. 3, [43] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. SynSin: End-to-end view synthesis from single image. In CVPR, 2020. 2 [44] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Chunyi Li, Liang Liao, Annan Wang, Erli Zhang, Wenxiu Sun, Qiong Yan, Xiongkuo Min, Guangtao Zhai, and Weisi Lin. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. In ICML, 2024. 6 [45] Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu. Citydreamer: Compositional generative model of unbounded 3d cities. In CVPR, 2024. 2 [46] Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu. GaussianCity: Generative gaussian splatting for unbounded 3D city generation. arXiv 2406.06526, 2024. 2 [47] Tian-Xing Xu, Xiangjun Gao, Wenbo Hu, Xiaoyu Li, SongHai Zhang, and Ying Shan. Geometrycrafter: Consistent geometry estimation for open-world videos with diffusion priors. arXiv preprint arXiv:2504.01016, 2025. 6 [48] Shuai Yang, Jing Tan, Mengchen Zhang, Tong Wu, Gordon Wetzstein, Ziwei Liu, and Dahua Lin. Layerpano3d: Layered 3d panorama for hyper-immersive scene generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 110, 2025. [49] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. Pixelnerf: Neural radiance fields from one or few images. arXiv:2012.02190, 2020. 2 [50] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: In Proceedings of Going from anywhere to everywhere. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 1, 2, 6 [51] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William T. Freeman, and Jiajun Wu. Wonderworld: Interactive 3d scene generation from single image. In CVPR, 2025. 1, 2, 4, 5, 6, 7, 8 [52] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. arXiv preprint arXiv:2311.16493, 2023. 3 10 [53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [54] Yan Zhang, Wenhan Zhao, Bo Sun, Ying Zhang, and Wen Wen. Point cloud upsampling algorithm: systematic review. Algorithms, 15(4):124, 2022. 3 [55] Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Suya Bharadwaj, Tejas You, Zhangyang Wang, and Achuta Kadambi. Dreamscene360: Unconstrained text-to-3d scene generation with panoramic gaussian splatting. arXiv preprint arXiv:2404.06903, 2024. 2 [56] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv:1805.09817, 2018. 2 11 WonderZoom: Multi-Scale 3D World Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Algorithm We provide an algorithm of WonderZoom in Alg. 1 B. Additional Results We provide additional visual results in Figures 9,10, 11 and 12 to show that WonderZoom significantly outperforms other baselines in terms of visual quality. C. Failure Cases As shown in Fig. 13, when zooming repeatedly into the cluster of branches, the scene eventually collapses into pure texture patterns with no remaining semantic cues (e.g., individual branches or leaves). Since WonderZoom relies on the semantics of the current-scale image to infer what should appear at the next scale, such texture-only regions become under-constrained, making further refinement in more new scales unreliable, and finally fail to generate multi-scale 3D world. This failure does not occur when recognizable structure is still present, but represents an inherent limitation when the input region no longer contains semantic information. D. Additional Details Additional implementation details. All images are processed at resolution of 720 1088. We use GPT-4V as our VLM for semantic context extraction and editing prompt generation. The initial camera focal length is set to fx = fy = 1024, with progressive zoom-in operations increasing the focal length for finer scales, typically we multiply the current focal length by 8 for new scale. We use INR-Harmonization [6] after image editing for improved shading consistency. Human study details. We use Prolific to recruit participants for our human preference evaluation. For each comparison, we collect responses from around 200 participants worldwide. The survey is implemented using Google Forms, and all responses are fully anonymized for both the participants and the authors. Each question presents two zoom-in sequences of the same scene generated by two different methods. Participants are shown the images in leftright layout: each side contains (1) global view of the scene and (2) zoomed-in view of the same region, indicated by red bounding box and connecting lines. The leftright order of methods is randomized for every participant and every question. Participants are instructed to carefully compare the two sides and make two-alternative forced choice (2AFC). 1 Figure 8. An example of our user study. For each comparison, we ask three questions: (i) Which one looks like the camera is moving closer? (ii) Which one looks better to your eyes? and (iii) Which one fits the prompt better? We compare our method with four baselines across six scenes, this yields 24 comparison pairs and 72 questions in total. Each participant answers all 72 questions. screenshot of the survey interface is provided in Figure 8. Algorithm 1 Multi-Scale 3D World Generation Control Loop Input: Initial image I0, initial camera C0 R44 Output: Multi-scale scene hierarchy {E0, E1, . . . , En} Runtime output: Real-time rendered observation Orender Runtime user control: Camera viewpoint Crender, zoom region Ci+1, (optional) edit prompt Ui+1 1: Initialize: E0 ReconstructScene(I0, C0) 2: Crender C0 3: 0 4: Thread 1: Real-time Scale-Adaptive Rendering 5: while true do 6: srender drender/ render Orender RenderWithOpacityModulation((cid:83)i Crender UserCameraControl() render (cid:113) 7: 8: 9: end while k=0 Ek, Crender) Initial 3D scene from input image Initialize rendering camera Current scale index Continuous rendering loop Compute rendering scale Sec. 3.1 Interactive camera update 10: Thread 2: Progressive Detail Synthesis Triggered by user zooming into region of interest with prompt Ui+1 at camera Ci+1 Ii+1 ControlledEdit(I i+1 SuperResolution(Oi+1, S) 11: // Stage 1: New Scale Image Synthesis 12: Oi+1 Render(Ei, Ci+1) 13: VLM(Render(Ei, Ci)) 14: 15: if Ui+1 = then 16: 17: else 18: 19: end if 20: // Stage 2: Scale-Consistent Depth Registration 21: Dtarget i+1 RenderDepth(Ei, Ci+1) 22: Di+1 DepthRegistration(Ii+1, Dtarget i+1 ) 23: // Stage 3: Scale-Adaptive Surfel Generation 24: partial i+1 InitializeSurfels(Ii+1, Di+1, Ci+1) i+1, Ui+1) Ii+1 i+1 i+1}K i+1, Dk 25: 26: // Stage 4: Auxiliary View Synthesis 27: {Ck 28: {Ik 29: // Stage 5: Optimization 30: Ei+1 OptimizeSurfels(E partial 31: 32: + 1 k=1 SampleNeighboringViews(Ci+1) i+1} AuxiliaryViewSynthesis(E partial i+1 , {Ck i+1}) i+1 , {Ii+1, I1 i+1, . . . , IK i+1}) Coarse observation at zoomed view Extract semantic context Extreme super-resolution Insert user-specified content Target depth from coarse scale Fine-tune depth estimator Create surfels with snative = dnative/ (cid:113) native native Optimize {q, s, o} with = 0.8L1 + 0.2LD-SSIM Increment scale index 2 Figure 9. Visual comparison of multi-scale 3D world generation results. 3 Figure 10. Visual comparison of multi-scale 3D world generation results. 4 Figure 11. Visual comparison of multi-scale 3D world generation results. 5 Figure 12. Visual comparison of multi-scale 3D world generation results. 6 Figure 13. failure case of WonderZoom. When zooming too deeply into the tree region, the view collapses into texture-like patterns instead of meaningful branch structures."
        }
    ],
    "affiliations": []
}