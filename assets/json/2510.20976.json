{
    "paper_title": "L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks",
    "authors": [
        "Jiyu Cui",
        "Fang Wu",
        "Haokai Zhao",
        "Minggao Feng",
        "Xenophon Evangelopoulos",
        "Andrew I. Cooper",
        "Yejin Choi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models have demonstrated remarkable reasoning capabilities across diverse natural language tasks. However, comparable breakthroughs in scientific discovery are more limited, because understanding complex physical phenomena demands multifaceted representations far beyond language alone. A compelling example is the design of functional materials such as MOFs-critical for a range of impactful applications like carbon capture and hydrogen storage. Navigating their vast and intricate design space in language-based representations interpretable by LLMs is challenging due to the numerous possible three-dimensional atomic arrangements and strict reticular rules of coordination geometry and topology. Despite promising early results in LLM-assisted discovery for simpler materials systems, MOF design remains heavily reliant on tacit human expertise rarely codified in textual information alone. To overcome this barrier, we introduce L2M3OF, the first multimodal LLM for MOFs. L2M3OF integrates crystal representation learning with language understanding to process structural, textual, and knowledge modalities jointly. L2M3OF employs a pre-trained crystal encoder with a lightweight projection layer to compress structural information into a token space, enabling efficient alignment with language instructions. To facilitate training and evaluation, we curate a structure-property-knowledge database of crystalline materials and benchmark L2M3OF against state-of-the-art closed-source LLMs such as GPT-5, Gemini-2.5-Pro and DeepSeek-R1. Experiments show that L2M3OF outperforms leading text-based closed-source LLMs in property prediction and knowledge generation tasks, despite using far fewer parameters. These results highlight the importance of multimodal approaches for porous material understanding and establish L2M3OF as a foundation for next-generation AI systems in materials discovery."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 6 7 9 0 2 . 0 1 5 2 : r Preprint L2M3OF: LARGE LANGUAGE MULTIMODAL MODEL FOR METAL-ORGANIC FRAMEWORKS Jiyu Cui1,2 Fang Wu3 Haokai Zhao4 Minggao Feng1 Xenophon Evangelopoulos1,2 Andrew I. Cooper1,2 Yejin Choi3 1Department of Chemistry, University of Liverpool 2Leverhulme Research Centre for Functional Materials Design, University of Liverpool 3Department of Computer Science, University of Stanford 4School of Computer Science and Engineering, University of New South Wales cuijy123@liverpool.ac.uk"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse natural language tasks. However, comparable breakthroughs in scientific discovery are more limited, because understanding complex physical phenomena demands multifaceted representations far beyond language alone. compelling example is the design of functional materials such as metal-organic frameworks (MOFs) critical for range of impactful applications like carbon capture and hydrogen storage. Navigating their vast and intricate design space in language-based representations interpretable by LLMs is challenging due to the numerous possible three-dimensional atomic arrangements and strict reticular rules of coordination geometry and topology. Despite promising early results in LLM-assisted discovery for simpler materials systems, MOF design remains heavily reliant on tacit human expertise rarely codified in textual information alone. To overcome this barrier, we introduce L2M3OF, the first multimodal LLM for MOFs. L2M3OF integrates crystal representation learning with language understanding to process structural, textual, and knowledge modalities jointly. L2M3OF employs pre-trained crystal encoder with lightweight projection layer to compress structural information into token space, enabling efficient alignment with language instructions. To facilitate training and evaluation, we curate structurepropertyknowledge database of crystalline materials and benchmark L2M3OF against state-of-the-art (SOTA) closed-source LLMs such as GPT-5, Gemini-2.5-Pro, and DeepSeek-R1. Experiments show that L2M3OF outperforms leading text-based closed-source LLMs in property prediction and knowledge generation tasks, despite using far fewer parameters. These results highlight the importance of multimodal approaches for porous crystalline material understanding and establish L2M3OF as foundation for next-generation AI systems in materials discovery."
        },
        {
            "title": "INTRODUCTION",
            "content": "Metal-organic frameworks represent versatile class of porous crystalline materials with high tunability and broad physical properties that promise transformative applications in direct carbon capture (Rohde et al., 2024), clean hydrogen storage (Chen et al., 2020), water harvesting (Alawadhi et al., 2024), and controlled drug delivery (Wu & Yang, 2017). MOF functional design involves intricate reticular synthesis procedures by linking metal atoms and organic molecules into repeating patterns, akin to LEGO building at the nanoscale. Scaling-up their design is nevertheless non-trivial, even with machine learning, both due to the large number of possible building-block combinations that give rise to an enormous design space, and because of the expertise-driven nature of the design which heavily relies on domain knowledge (Yaghi et al., 2003). Equal contribution. 1 Preprint Large language models have recently emerged as powerful AI assistants for chemists, demonstrating strong reasoning capabilities in language-related chemistry tasks, such as chemical knowledge integration and tool orchestration, offering promising potential in accelerating the exploration of large design spaces (Mirza et al., 2025). The discovery of new functional materials such as MOFs however, is fundamentally more challenging because unimodal textual representations typically fail to capture complex, high-dimensional reticular phenomena that give rise to different functionalities. Unlike molecules (Zhu et al., 2024) or proteins (Wu et al., 2023), which can be expressed as textual sequences of relatively small range of elements, MOFs inhabit three-dimensional, periodic structures that resist straightforward representation. In addition to being compositionally much broader, the structure-function problem for MOFs is inherently more complex because the 3-dimensional atomic sequence does not encode function alone; rather, it emerges from combination of factors, including local bonding environments, long-range crystallographic symmetry, pore connectivity, and other topological features (Luo et al., 2024). Despite the emerging line of research on LLMs for accelerated materials design (Kang et al., 2025; Duan et al., 2025) spanning broad range of downstream tasks, including property prediction (Niyongabo Rubungo et al., 2025) and de-novo structure generation (Wang et al., 2025a), existing approaches remain restricted to text-centric or file-based representations, such as crystallographic information files (CIFs) and text-based property descriptions (Tang et al., 2025). While effective for sequential reasoning, such encodings fail to capture three-dimensional symmetries, periodicity, and long-range structural correlations that underpin crystalline behavior, often underperforming when compared with geometryor symmetry-aware models (Alampara et al., 2025b). collection of existing LLMs and their modeling capacity for crystalline materials is presented in Table 1. The challenge here extends beyond structural representation; it lies in the machine understanding of materials functionality. Multimodal integration in learning strategies, that is, leveraging atomic information as well as literature knowledge to interlink structure with function, is therefore key to enable holistic understanding of materials applicability. Whereas molecular modeling has seen initial success in coupling LLMs with graph neural networks or generative models (Jablonka et al., 2024), analogous strategies for crystalline systems remain rare, owing to system and design complexity, as well as the lack of standardized datasets and benchmarks tailored to crystalline materials, rendering rigorous evaluation and reproducibility quite challenging. This work proposes L2M3OF, the first multimodal LLM for MOF design that combines multimodal MOFs representations (Park et al., 2023) with curated domain-knowledge from MOFs literature. L2M3OF is versatile and inherently designed to be lightweight to allow for an efficient alignment with language instructions, demonstrating SOTA performance on diverse design-critical tasks including property prediction and material application recommendation, rendering it an indispensable AI-assistant for chemists and materials scientists. To train and test L2M3OF, we curate the first-ever structure-property-knowledge MOFs database, namely MOF-SPK, featuring structural, property and domain-knowledge information for more than 100,000 MOFs materials. L2M3OF outperforms leading commercially-available LLMs such as DeepSeek, GPT-4o, and Gemini-2.5-Pro, demonstrating SOTA capabilities not only in capturing essential representational aspects of complex MOFs systems, but also holistic understanding of their broader functional role and potential applicability. Fig. 1 illustrates the core architectural features of L2M3OF."
        },
        {
            "title": "2 BACKGROUND AND RELATED WORK",
            "content": "Crystal representation. crystal structure is defined by the geometric arrangement of its atoms within unit cell. The unit cell represents the smallest repeating block that captures and maintains the complete symmetry and structure of the crystal. crystal can therefore be uniquely represented as = (A, X, L) using the following three parameters that characterize its unit cell: i) atom identities = {a0, ..., aN } AN , where denotes the set of all chemical elements, ii) Cartesian coordinates of atoms = [x0, ..., xN ]T RN 3 and iii) the lattice matrix that describes the periodicity of the crystal = [l1, l2, l3]T R33. Crystal information is traditionally encoded in standardized text format in Crystallographic Information Files (CIFs), which have been the keystone of systematically curated databases of both predicted and experimentally found crystals (Boyd et al., 2019; Zhao et al., 2025) and actively used for materials discovery over the last decades. Fig. 10 exemplifies the structure of CIF file. CIF databases come with two-fold challenge: they 2 Preprint Model CrystalType LLM Model Input Downstream Tasks LLM-Prop (Niyongabo Rubungo et al., 2025) CrystLLM (Antunes et al., 2024) CrysText (Mohanty et al., 2024) Mat2Seq (Yan et al., 2025) MatText (Alampara et al., 2025a) CrystalICL (Wang et al., 2025b) CSLLM (Song et al., 2025) deCIFer (Johansen et al., 2025) MatterGPT (Wang et al., 2025a) Text2Struc (Baibakova, 2025) Matterchat (Tang et al., 2025) Chemeleon (Park et al., 2025) MOFGPT (Badrinarayanan et al., 2025) ChatMOF (Kang & Kim, 2024) L2M2OF L2M3OF Inorganic Inorganic Inorganic Inorganic Inorganic Inorganic Inorganic Inorganic Inorganic Inorganic Inorganic Inorganic MOFs MOFs MOFs MOFs T5 Llama-2 Llama-3.1 GPT Llama-2 Llama-2 LlamaText Multimodal Transformer GPT CodeGen Mistral BERT GPT GPT Qwen2.5 Qwen2.5 Structure Property Knowledge Q&A Table 1: Model features of LLMs for crystalline materials. Structure corresponds to structure prediction or structure extraction; Property means property prediction; Knowledge means knowledge generation, and Q&A stands for question and answering. describe materials structures and isolated properties but lack holistic information on their functionality, which is present in published papers. Importantly, the textual format of CIFs is less amenable to typical ML pipelines posing barriers to streamlining data-driven materials discovery Tian et al. (2022). While this still remains grand challenge in materials science, recent literature has increasingly focused on the development of either hand-crafted or machine-learned crystal representations that are well suited for machine learning algorithms. Crystal representation learning. Recent progress in crystal representation learning spans wide range of representations and modalities, ranging from graph-based to structural and foundation model approaches. CGCNN (Xie & Grossman, 2018) pioneered interpretable crystal graph convolutional networks for property prediction directly from atomic connections, while iCGCNN (Cheng et al., 2021) further enhances this by incorporating Voronoi tessellation and three-body interactions. Physics-guided generative models like PGCGM (Zhao et al., 2023) leverage symmetry-affine transformations to generate diverse, structurally valid crystals, significantly outperforming previous generators. MOFTransformer (Kang et al., 2023) was the first inherently multimodal architecture, combining atom-based and energy-grid embeddings to capture local and global features, achieving SOTA property prediction for MOFs. Similarly, DeepSorption (Cui et al., 2023) integrates global structural awareness via transformer for highly accurate adsorption predictions in porous materials. More recently, the emergence of foundation models allowed the extension of these practises to broader crystalline systems. MCRT (Feng et al., 2025) multimodally integrates local atomic information with global persistence-image based views of organic molecular crystals, while CLOUD (Xu et al., 2025) employs symmetry-aware, physic-informed string representations for the development of scalable foundation model, pre-trained on millions of inorganic crystals. Together, these approaches illustrate the shift from local graph-based modeling to multimodal, geometry-aware methodologies, which enable few-shot learning and hold significant promise for leveraging their representations in LLMs. LLMs for crystalline materials. LLMs have recently drawn much interest from the chemistry and materials community due to their unique capabilities in text generation, chemical knowledge integration, and characterization tool utilization (Zheng et al., 2025). Recent efforts have demonstrated the LLMs capacity to process raw CIF files of inorganic crystals to generate textual descriptions for further language-based training exploitation (Alampara et al., 2025a). Beyond text generation, LLMs have demonstrated promising performance in plausible structure generation, such as CrystaLLM (Antunes et al., 2024), which was trained on millions of inorganic crystal CIF files and validated via ab initio simulations on de-novo generated structures. Chemeleon (Park et al., 2025) proposed the integration of text descriptions with 3D structural data using cross-modal contrastive learning and diffusion models, enabling natural languageguided generation of chemical compositions and structures of inorganic crystals. CSLLM (Song et al., 2025), framework of three fine-tuned LLMs, use textual representation for crystal material to predict the synthesizability, synthesis method, and precursors of 3D inorganic crystals. Finally, Matterchat (Tang et al., 2025) is 3 Preprint Figure 1: An overview of the L2M3OF framework and its applicability in MOFs design. structure-aware LLM for inorganic materials, trained on more than 140,000 structures, capable of ingesting textual information from atomic structures to reason answers on material description and property-prediction. While these approaches have demonstrated success to small-scale systems, they struggle to generalise to larger, complex systems, such as MOFs, with hundreds or even thousands of atoms per unit cell and latent functionality cues concealed in their CIF representations (Xiao et al., 2023). MOFGPT Badrinarayanan et al. (2025) is the first LLM for de-novo generation of MOFs, utilizing GPT generator trained on MOFid sequences and reinforcement learning framework that steers generation toward target propertiesas predicted via MOFormer Kang et al. (2023))using reward functions. Nevertheless, MOFGPTs generator relies on 2D MOFid strings, missing thus intrinsic functional information encoded in their 3D structure of MOFs. Our work, further complements this expressivity by not only enabling the use of 3D representation of large MOFs structures, but also enhancing the functionality prediction and comprehension of the LLM via jointly training on domain-specific information from the literature."
        },
        {
            "title": "3 DATASET AND MODEL ARCHITECTURE",
            "content": "3.1 MOF-SPK DATASET CONSTRUCTION To train and validate the performance of our model we construct structurepropertyknowledge database for MOFs, namely MOF-SPK. We specifically couple experimentally found MOF structures in CIF format together with curated literature information directly related to these structures. We selected 133,737 CIFs from the Cambridge Crystallographic Data Centre (CCDC) database (Groom et al., 2016) together with the corresponding publications that reported those structures. To ensure data consistency, we removed guest molecules from the MOFs structures using the CSD Python API and applied the StructureMatcher module from the pymatgen Python package (Ong et al., 2013) to identify and eliminate duplicate entries with equivalent crystal structures. We further augmented this data with property calculations and knowledge, leveraging high-throughput computational tools, Python packages and LLMs into structurepropertyknowledge database which formed the test-bench for this study. Based on MOF-SPK we designed four assessment tasks which we then use to evaluate the utility of our LLM as an assistive tool for MOF design. These tasks represent critical and time-consuming characterization procedures in the design of functional materials that can offer valuable acceleration and assistance. The tasks include: i) property prediction, ii) structure extraction, iii) description generation, and iv) general question & answering, and form the basis to facilitate model training and evaluation. Property prediction assesses the LLMs complex physical perception ability on the absolute and relative positions of atoms in the 3D space. Here we computed key physical characteristics of crystal materials density, pore limiting diameter (PLD), largest cavity diameter (LCD), accessible 4 Preprint surface area (ASA), and void fraction (VF) using high-throughput computational tools such as ZEO++ (Willems et al., 2012). The structure extraction task assesses the LLMs capacity to learn and understand the materials reticular composition of building units, i.e., by extracting the SMILES representation of its molecular building blocks. The MOFid Python toolbox (Bucior et al., 2019) was used to generate the data for this task. For the description generation task, we curated domain knowledge from more than 35,000 scientific publications on experimentally discovered MOFs, extracting information on applications, characterization methods, stability, and structural features with the assistance of DeepSeek-R1. Application refers to the uses of the material, such as adsorption or catalysis. Characterization method specifies the experimental techniques that should be employed to analyze the material, for example, Powder X-ray Diffraction or X-ray Photoelectron Spectroscopy. Stability describes how stable the material is, for instance, whether it remains stable in water. Structural features summarize the materials structural characteristics, such as forming 1D chain or 3D open framework. The description generation task assesses whether the crystal LLM can learn and establish the relationship between crystal structures and crystal knowledge. Finally, the question & answering task assesses the LLMs ability to answer materials-related questions on MOFs. We generated five questions and answers for each scientific publication based on their abstract. First, we prompted DeepSeek-R1 to identify five keywords from each abstract and then extract relevant questions and answers pairs around those. Examples of the above tasks are included in Section A.3 of the Appendix. To ensure that the MOF-SPK dataset is chemically well-balanced in terms of material representation we perform comprehensive statistical analyses included in Section A.2 of the Appendix. Figure 2: An overview of model architecture and model training methods. (A) The architecture differences between L2M2OF and L2M3OF. (B) The schematic diagram of the group training method. 3.2 MODEL ARCHITECTURE We design two complementary models; apart from our main contribution, L2M3OF multimodal LLM that incorporates structural information through MOF 3D structure encoder, we further develop language-only variant, namely L2M2OF, which instead represents MOFs in their textual CIF format. The juxtaposition of the two variants against our expertly designed MOF tasks, helps better understand the role of multimodal material representations in LLM-guided discovery. Fig. 2 shows the architectures of the two proposed models. L2M2OF processes crystal material by converting its CIF into textual sequence SM. This sequence contains the unit cell parameters, space group symmetry, and atomic coordinates with their respective element types. This material representation is concatenated with task-specific natural 5 Preprint language instruction IT to form the complete input prompt as XLLM = [SM; IT ]. The instruction IT embeds expert domain knowledge to guide the model. For instance, for VF prediction, IT defines the property and its physical significance. The model then generates the target prediction autoregressively. The probability of generating the output sequence of tokens is given by: (Y XLLM) = (cid:89) i=1 (yi y<i, XLLM; ΘLLM), (1) where ΘLLM represents the parameters of pre-trained LLM. key advantage of this text-only paradigm is its ability to perform inference with SOTA commercial LLMs (e.g., GPT-5, GeminiPro, Deepseek-R1) without additional training. In this study, we also fine-tuned an open-source LLM on the MOF-SPK database to serve as strong text-only baseline for comparison against our multimodal approach. L2M3OF conversely fuses textual instructions with non-textual, geometric structural data. The model consists of three core components: Crystal Structure Encoder: We employ PMTransformer (Park et al., 2023) as the crystal encoder, which is GNN pre-trained on 1.9 million hypothetical porous materials. It takes the crystal structure and outputs fixed-dimensional latent representation, or embedding, zstruct Rd: zstruct = PMTransformer(M; ΘPMT), (2) where ΘPMT represents the frozen, pre-trained parameters of the encoder. Multimodal Projection Bridge: This component transforms and compresses the structural embedding for seamless integration with the language model through compression and projection network. The compression network, MLPtoken, then compresses this sequence along the token dimension from length to shorter, fixed length (M < ). We empirically found that this compression accelerates training significantly without loss of performance. The projection network, MLPfeat, projects the encoders output from its native dimension denc to sequence of tokens in LLMs embedding space RdLLM . The entire process is defined as: Hstruct = MLPtoken(zstruct; Θtoken), Hproj = MLPfeat(Hstruct; Θfeat), where Θbridge = (Θfeat, Θtoken) denotes the combined parameters of the projection and compression MLPs, and Hstruct RM denc, Hproj RM dLLM . Large Language Model: The compressed structural token sequence Hstruct is prepended to the tokenized instruction sequence Tokenize(IT ) to form the combined input for the LLM. The LLM then generates the output conditioned on this multimodal input: (3) (Y M, IT ) = (cid:89) i=1 (yi y<i, Hstruct, IT ; ΘLLM, Θbridge). (4) During training, the encoder parameters ΘPMT are kept frozen to preserve its pre-trained knowledge and stabilize training. Only ΘBridge and ΘLLM are updated. 3.3 TRAINING OBJECTIVE We trained our models using an instruction-tuning paradigm, tailoring them for property prediction tasks in materials science. The objective is to minimize the negative log-likelihood of the target sequence (e.g., the numerical or classification values) given the input instruction and material data. For dataset of examples, each containing an instruction, material, and target response (I (i) , M(i), (i))i = 1N , the loss function for L2M3OF is defined as: L(ΘLLM, Θbridge) = 1 (cid:88) i=1 log (Y (i) M(i), (i) ; ΘLLM, Θbridge). (5) This supervised fine-tuning (SFT) process teaches the model to follow instructions and reason about material properties based on the provided textual and structural information, enabling it to generalize to new, unseen materials and tasks. We further implement group training strategy to enhance 6 Preprint context diversity during SFT. For each mini-batch, we randomly concatenate multiple instructionanswer pairs to form extended conversational contexts. The loss is computed only on the answer tokens of each question within the group, while the preceding Q&A pairs serve as contextual background. This approach effectively increases the diversity of training contexts without substantially increasing computational costs, as the total number of tokens per batch remains unchanged. The method acts as an efficient form of data augmentation, exposing the model to richer contextual patterns during training."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL RESULTS Training models on past data and evaluating them on future discoveries is crucial because it mirrors real-world deployment scenarios. To this end, we partition the dataset by material deposition year, i.e., crystal structures deposited on or before 2020 were used for training, while those from 2021 onwards formed the validation set. We further sample 500 crystal structures deposited after 2022 and use as the test set. We evaluate the performance of our models against leading commercial LLMs Table 2: Performance comparison of commercial LLMs, L2M2OF, and L2M3OF on property prediction and structure extraction. The best performances are in bold, the second best underlined. Metric DeepSeek-V3 DeepSeek-R1 GPT-4o GPT-5 mini GPT-5 Gemini-2.5-pro L2M2OF L2M3OF PLD () LCD () Density () ASA () VF () BLEU () EXACT () MACCS () RDK () MORGAN () VALIDITY () 1.99 2.27 0.41 762.7 0.21 0.27 0.00 0.50 0.32 0.22 0.34 Property Prediction (MAE) 2.94 4.14 9.86 745.3 0.13 2.93 4.59 0.31 1317.6 9. Structure Extraction 0.20 0.00 0.49 0.27 0.18 0.35 0.38 0.02 0.53 0.37 0.24 0.44 3.24 4.37 0.31 726.2 0.08 0.38 0.02 0.56 0.43 0.28 0.60 1.97 3.10 0.35 1481.6 0. 0.28 0.01 0.52 0.40 0.25 0.44 2.09 2.28 0.31 805.9 0.88 0.38 0.00 0.57 0.46 0.29 0.80 1.21 1.00 0.55 497.8 0.04 0.41 0.29 0.63 0.44 0.36 0.78 0.55 0.53 0.17 253.7 0. 0.34 0.18 0.50 0.25 0.23 0.75 by Google (Comanici et al., 2025), DeepSeek Guo et al. (2025) and OpenAI (OpenAI et al., 2024) on the four tasks introduced in Section 3.1 to assess the learning and comprehensive capabilities of LLMs for MOFs. Here we do not compare against other crystal LLMs from the literature as these are not suitable for MOF materials or do not support knowledge generation capabilities which is one of the main scopes of this study. Property prediction. This task assesses the LLMs performance (in terms of mean absolute error) in accurately predicting wide range of MOF properties. As shown in the top section of Table 2, L2M3OF attains the lowest MAE on all five targets, while commercial LLMs consistently underperform, especially on geometry-sensitive metrics such as PLD, LCD, and ASA, which require robust grounding in 3D pore topology. Even on the easier task of density prediction, the best commercial systems (Gemini-2.5-Pro, GPT-5, GPT-5-mini) still perform worse against L2M3OF. Importantly, we further observe clear failure modes suggestive of hallucination or unit/normalization errors: GPT-4o reaches MAE = 9.86 on density and GPT-5 mini reaches MAE = 9.63 on void fraction. The L2M2OF variant also outperforms the commercial LLMs on property prediction, albeit losing against the multimodal L2M3OF. Under same number of training steps however, L2M2OF is substantially slower because textual crystal descriptions require far more tokens. These findings demonstrate the utility of literature injected domain knowledge in the training of scientific LLMs and further indicate that multimodal training enhances an LLMs ability to perceive and reason about the 3D spatial information of porous crystalline materials significantly, yielding superior accuracy as well as efficiency. Structure extraction. Extracting molecular building blocks from MOFs requires fine-grained perception of local chemical information and structural features. Here we compare the SMILES of 7 Preprint LLM-extracted units against ground-truth (as computed in Section 3.1 to assess accuracy and validity according to the BLEU, EXACT and VALIDITY normalized scores as in (Zhuang et al., 2025)1. We further assess structural similarity between the extracted molecular units and the ground-truth. We test three different molecular fingerprinting methods, namely MACCS, RDKit and Morgan and use the Tanimoto similarity metric (Szafarczyk et al., 2024). Interestingly, LLMs that use CIFs as textual input achieve even stronger results on this task. In particular, L2M2OF performs best on BLEU, EXACT, MACCS, and MORGAN, while Gemini-2.5-pro leads on RDK and VALIDITY, with high SMILES VALIDITY score of 0.8, which highlights the strong capabilities of advanced commercially available LLMs in chemical tasks. The success of CIF-based models is not entirely surprising however; the explicit textual representation of CIF directly encodes the elemental composition of materials, which facilitates the inference of constituent molecules and metallic units putting more emphasis on local environment. In contrast, vectorized crystal representations make such local compositional information less transparent. Figure 3: Performance comparison of Gemini-2.5-pro and L2M3OF on the tasks of description generation and case study of application recommendation task. Description generation. The description generation task is evaluated across four subtasks: application recommendation, characterization method, stability description, and structural feature. Among these, application recommendation is both the most important and the most challenging, as it requires not only an accurate perception of crystal structures and properties but also sufficient domain knowledge to map materials to plausible use cases. This is an indispensable tool to support scientists in making informed, application-oriented decisions rather than treating structure analysis in isolation. To ensure reliable evaluation and reduce reliance on manual judgment, we employ GPT o4-mini as an impartial chemistry knowledge judge. Following (Wang et al., 2023), we adopt calibration strategy where o4-mini compares the outputs of two LLMs for each test question. Since LLMs are sensitive to response order, we mitigate positional bias by swapping the order of the outputs and re-evaluating. The final score is computed by aggregating results from both prompt orders. Each evaluation includes the question, ground-truth answer and two candidate responses. During assessment, the o4-mini is instructed to select best LLM responses based on strict scientific accuracy and factual correctness. Given its strong preliminary performance on property prediction and structure extraction, we adopt Gemini-2.5-Pro as baseline for comparisons. As shown in in the left-most side of Fig. 3, L2M3OF outperforms Gemini-2.5-Pro across the description generation task. Averaged over the four subtasks, it achieves 61.8% win rate with 13.2% ties. Notably, for application recommendation L2M3OF wins 47.6% of the head-to-head comparisons versus 40.0% for Gemini-2.5-Pro, indicating stronger grounding from structure to function. The right-most side of Fig. 3 shows an application recommendation example case for material called NAYBEY (that efficiently separates carbon dioxide 1The SMILES BLEU score measures the overlap between the LLM-generated and ground-truth SMILES strings. EXACT assesses exact SMILES matches. VALIDITY evaluates the percentage of the LLM-generated molecules that conform to chemical syntax rules (Zhuang et al., 2025). Preprint from nitrogen via molecular sieving (You et al., 2022)), where both DeepSeek-R1 and Gemini2.5-Pro correctly suggested gas adsorption and separation, whereas GPT-5 failed to do so. L2M3OF went further by explicitly identifying the materials molecular sieving potential. Generally, L2M3OF demonstrates decisive gains in characterization method and stability description, while Geminis responses often remain overly generic. On the structural feature subtask, Gemini-2.5-Pro maintains edge, consistent with earlier structure-extraction results. This again reflects the advantage of textbased CIF representation in accurately capturing local structural information that are pertinent to structural feature description task. Figure 4: Performance comparison of Gemini-2.5-pro and L2M3OF in the tasks of question&answer. Question & Answering. The question & answering task not only relies on understanding the crystal structure but also emphasizes the models mastery of MOF-specific domain knowledge. Figure 4 illustrates an example test of Q&A as well average performance comparisons, highlighting L2M3OFs clear win rate of 42.4% against Gemini-2.5-pro (40.8%). We empirically observe that overall, Gemini-2.5-pro often produces overly verbose responses and fails to reason toward the correct answer due to its lack of domain knowledge, whereas L2M3OF is able to reason and respond concisely and accurately. Table 3: Ablation study performance comparison. Property Prediction Structure Extraction PLD () LCD () Density () ASA() VF () BLEU () EXACT () MACCS () RDK () MORGAN() VALIDITY() L2M3OF w/o joint training w/o group training 0.55 3.41 0.73 0.53 6.13 0.72 0.17 1.34 0. 253.7 1616.3 275.1 0.01 0.21 0.02 0.34 0.19 0.27 0.18 0.01 0.17 0.50 0.32 0.48 0.25 0.14 0. 0.23 0.11 0.21 0.75 0.63 0.77 4.2 ABLATION STUDY To probe cross-task interactions, we compare joint training and separate training across different tasks using the same data budget and model size (Table 3 and Fig 11). The results show clear, consistent gains from joint training. On property prediction, the jointly trained model achieves substantially lower MAE on geometry-dependent targets. There is also significant improvement in the structure extraction task. In the description generation task, the head-to-head win rate against Gemini-2.5-Pro rises from 53.3% to 61.8%. The three tasks capture complementary facets of the same underlying material representation. Property prediction forces the model to be numerically faithful to pore geometry; structure extraction sharpens the models awareness of local chemistry; description generation ties these cues to functional outcomes. Optimizing them together encourages holistic, structure-aware embedding that captures both global topology and local chemical context. We also investigate the effect of group training which, without introducing additional training overhead, primarily improves the predictive accuracy of the model on the property prediction task. The above experimental results further demonstrate the importance of enabling the model to jointly learn and capture crystal structures, properties, and knowledge. 9 Preprint"
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "We proposed L2M3OF, the first multimodal large language model designed specifically for MOFs. By integrating geometric structure encoding with language-based domain knowledge, L2M3OF outperforms state-of-the-art commercial LLMs across property prediction, description generation, and question answering tasksdespite using fewer parameters. These results highlight the importance of multimodal architectures in capturing the intricate interplay between structure and function in crystalline materials. L2M3OFs success demonstrates how grounding LLMs in 3D representations and curated literature can bridge gaps in automated materials discovery. As lightweight and versatile tool, it offers chemists scalable AI assistant for navigating complex design spaces."
        },
        {
            "title": "6 REPRODUCIBILITY STATEMENT",
            "content": "To ensure our findings are reproducible, well make all code and processed data publicly available upon paper acceptance. The dataset construction is detailed in Section 3.1, and we will share the processed data to facilitate its use by others. The model architecture is fully described in Section 3.2, and training specifics are provided in Section 3.3. The evaluation protocols are laid out in detail in the Section 4.1. We commit to making both our training and inference code accessible, allowing for full replication of our experiments. This comprehensive approach ensures that our results can be validated and built upon by the research community."
        },
        {
            "title": "7 ETHICS STATEMENT",
            "content": "While our training data is confined to scientific literature on MOFs, the underlying base model carries potential societal biases and inherent safety risks. Consequently, our models outputs do not ensure 100% accuracy and may not comprehensively cover the full spectrum of safety and honesty. The model should, therefore, only be used under professional guidance to prevent the generation of biased, inaccurate, or harmful content in real-world applications. To mitigate these risks and ensure responsible future development, we recommend series of safeguards. The deployment of this model in real-world scientific research should be accompanied by professional guidance. Future research should prioritize more comprehensive evaluation of the base models safety and explore integrating formal safety and honesty constraints directly into the trained L2M3OF architecture. These crucial steps can help ensure that further advancements in this field are built upon strong foundation of ethical responsibility."
        },
        {
            "title": "8 CODE AVAILABILITY",
            "content": "The code will be made available online after the article is accepted for publication. ACKNOWLEDGMENTS The authors acknowledge financial support from the Leverhulme Trust via the Leverhulme Research Centre for Functional Materials Design. The authors also acknowledge the AI for Chemistry: AIchemy hub for funding (EPSRC grant EP/Y028775/1 and EP/Y028759/1). This project has received funding from the European Research Council under the European Unions Horizon 2020 research and innovation program (grant agreement no. 856405). AIC thanks the Royal Society for Research Professorship (RSRPS2232003)."
        },
        {
            "title": "REFERENCES",
            "content": "Nawaf Alampara, Santiago Miret, and Kevin Maik Jablonka. Less can be more for predicting properties with large language models, 2025a. URL https://arxiv.org/abs/2406.17295. Nawaf Alampara, Mara Schilling-Wilhelmi, Martino Rıos-Garcıa, Indrajeet Mandal, Pranav Khetarpal, Hargun Singh Grover, N. M. Anoop Krishnan, and Kevin Maik Jablonka. Probing the limitations of multimodal language models for chemistry and materials research, February 2025b. 10 Preprint Ali Alawadhi, Saumil Chheda, Gautam Stroscio, Zichao Rong, Daria Kurandina, Ha Nguyen, Nakul Rampal, Zhiling Zheng, Laura Gagliardi, and Omar Yaghi. Harvesting water from air with high-capacity, stable furan-based metalorganic frameworks. Journal of the American Chemical Society, 146(3):21602166, 2024. Luis M. Antunes, Keith T. Butler, and Ricardo Grau-Crespo. Crystal structure generation with autoregressive large language modeling. Nature Communications, 15(1):10570, 2024. Srivathsan Badrinarayanan, Rishikesh Magar, Akshay Antony, Radheesh Sharma Meda, and Amir Barati Farimani. MOFGPT: Generative Design of MetalOrganic Frameworks using Language Models. Journal of Chemical Information and Modeling, 65(17):90499060, 2025. Viktoriia Baibakova. Text2Struc: Programmatic Crystal Structure Generation with Fine-Tuned Large Language Models, 2025. URL https://chemrxiv.org/engage/chemrxiv/ article-details/67ad5b2981d2151a023ae346. Peter Boyd, Arunraj Chidambaram, Enrique Garcıa-Dıez, Christopher Ireland, Thomas Daff, Richard Bounds, Andrzej Gładysiak, Pascal Schouwink, Seyed Mohamad Moosavi, Mercedes Maroto-Valer, et al. Data-driven design of metalorganic frameworks for wet flue gas co2 capture. Nature, 576(7786):253256, 2019. Benjamin J. Bucior, Andrew S. Rosen, Maciej Haranczyk, Zhenpeng Yao, Michael E. Ziebel, Omar K. Farha, Joseph T. Hupp, J. Ilja Siepmann, Alan Aspuru-Guzik, and Randall Q. Snurr. Identification Schemes for MetalOrganic Frameworks To Enable Rapid Search and Cheminformatics Analysis. Crystal Growth & Design, 19(11):66826697, 2019. Zhijie Chen, Penghao Li, Ryther Anderson, Xingjie Wang, Xuan Zhang, Lee Robison, Louis R. Redfern, Shinya Moribe, Timur Islamoglu, Diego A. Gomez-Gualdron, Taner Yildirim, J. Fraser Stoddart, and Omar K. Farha. Balancing volumetric and gravimetric uptake in highly porous materials for clean energy. Science, 368(6488):297303, 2020. Jiucheng Cheng, Chunkai Zhang, and Lifeng Dong. geometric-information-enhanced crystal graph network for predicting properties of materials. Communications Materials, 2(1):92, 2021. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Jiyu Cui, Fang Wu, Wen Zhang, Lifeng Yang, Jianbo Hu, Yin Fang, Peng Ye, Qiang Zhang, Xian Suo, Yiming Mo, Xili Cui, Huajun Chen, and Huabin Xing. Direct prediction of gas adsorption via spatial atom interaction learning. Nature Communications, 14(1):7043, 2023. Chenru Duan, Aditya Nandy, Shyam Chand Pal, Xin Yang, Wenhao Gao, Yuanqi Du, Hendrik Kraß, Yeonghun Kang, Varinia Bernales, Zuyang Ye, et al. The rise of generative ai for metal-organic framework design and synthesis. arXiv preprint arXiv:2508.13197, 2025. Minggao Feng, Chengxi Zhao, Graeme M. Day, Xenophon Evangelopoulos, and Andrew I. Cooper. universal foundation model for transfer learning in molecular crystals. Chemical Science, 16 (28):1284412859, 2025. C. R. Groom, I. J. Bruno, M. P. Lightfoot, and S. C. Ward. The Cambridge Structural Database. Acta Crystallographica Section B: Structural Science, Crystal Engineering and Materials, 72(2): 171179, 2016. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. Leveraging large language models for predictive chemistry. Nature Machine Intelligence, 6(2):161169, 2024. 11 Preprint Frederik Lizak Johansen, Ulrik Friis-Jensen, Erik Bjørnager Dam, Kirsten Marie Ørnsbjerg Jensen, Rocıo Mercado, and Raghavendra Selvan. decifer: Crystal structure prediction from powder diffraction data using autoregressive language models, 2025. URL https://arxiv.org/ abs/2502.02189. Yeonghun Kang and Jihan Kim. Chatmof: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models. Nature communications, 15(1): 4705, 2024. Yeonghun Kang, Hyunsoo Park, Berend Smit, and Jihan Kim. multi-modal pre-training transformer for universal transfer learning in metalorganic frameworks. Nature Machine Intelligence, 5(3):309318, 2023. Yeonghun Kang, Wonseok Lee, Taeun Bae, Seunghee Han, Huiwon Jang, and Jihan Kim. Harnessing Large Language Models to Collect and Analyze MetalOrganic Framework Property Data Set. Journal of the American Chemical Society, 147(5):39433958, 2025. Jun Luo, Omar Ben Said, Peigen Xie, Marco Gibaldi, Jake Burner, Cecile Pereira, and Tom Woo. Mepo-ml: robust graph attention network model for rapid generation of partial atomic charges in metal-organic frameworks. npj Computational Materials, 10(1):224, 2024. Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Martino Rıos-Garcıa, Benedict Emoekabu, Aswanth Krishnan, Tanya Gupta, Mara Schilling-Wilhelmi, Macjonathan Okereke, Anagha Aneesh, Mehrdad Asgari, Juliane Eberhardt, Amir Mohammad Elahi, Hani M. Elbeheiry, Marıa Victoria Gil, Christina Glaubitz, Maximilian Greiner, Caroline T. Holick, Tim Hoffmann, Abdelrahman Ibrahim, Lea C. Klepsch, Yannik Koster, Fabian Alexander Kreth, Jakob Meyer, Santiago Miret, Jan Matthias Peschel, Michael Ringleb, Nicole C. Roesner, Johanna Schreiber, Ulrich S. Schubert, Leanne M. Stafast, A. D. Dinga Wonanke, Michael Pieler, Philippe Schwaller, and Kevin Maik Jablonka. framework for evaluating the chemical knowledge and reasoning abilities of large language models against the expertise of chemists. Nature Chemistry, 17(7): 10271034, July 2025. ISSN 1755-4330, 1755-4349. doi: 10.1038/s41557-025-01815-x. Trupti Mohanty, Maitrey Mehta, Hasan M. Sayeed, Vivek Srikumar, and Taylor D. Sparks. CrysText: Generative AI Approach for Text-Conditioned Crystal Structure Generation using LLM, December 2024. URL https://chemrxiv.org/engage/chemrxiv/ article-details/6753874c7be152b1d02eecb5. Andre Niyongabo Rubungo, Craig Arnold, Barry P. Rand, and Adji Bousso Dieng. LLM-Prop: Predicting the properties of crystalline materials using large language models. npj Computational Materials, 11(1):186, 2025. Shyue Ping Ong, William Davidson Richards, Anubhav Jain, Geoffroy Hautier, Michael Kocher, Shreyas Cholia, Dan Gunter, Vincent L. Chevrier, Kristin A. Persson, and Gerbrand Ceder. Python Materials Genomics (pymatgen): robust, open-source python library for materials analysis. Computational Materials Science, 68:314319, 2013. OpenAI, Josh Achiam, and et al. GPT-4 technical report, 2024. URL https://arxiv.org/ abs/2303.08774. Hyunsoo Park, Yeonghun Kang, and Jihan Kim. Enhancing StructureProperty Relationships in Porous Materials through Transfer Learning and Cross-Material Few-Shot Learning. ACS Applied Materials & Interfaces, 15(48):5637556385, 2023. Hyunsoo Park, Anthony Onwuli, and Aron Walsh. Exploration of crystal chemical space using text-guided generative artificial intelligence. Nature Communications, 16(1):4379, 2025. Rachel C. Rohde, Kurtis M. Carsch, Matthew N. Dods, Henry Z. H. Jiang, Alexandra R. McIsaac, Ryan A. Klein, Hyunchul Kwon, Sarah L. Karstens, Yang Wang, Adrian J. Huang, Jordan W. Taylor, Yuto Yabuuchi, Nikolay V. Tkachenko, Katie R. Meihaus, Hiroyasu Furukawa, Danielle R. Yahne, Kaitlyn E. Engler, Karen C. Bustillo, Andrew M. Minor, Jeffrey A. Reimer, Martin HeadGordon, Craig M. Brown, and Jeffrey R. Long. High-temperature carbon dioxide capture in porous material with terminal zinc hydride sites. Science, 386(6723):814819, 2024. 12 Preprint Zhilong Song, Shuaihua Lu, Minggang Ju, Qionghua Zhou, and Jinlan Wang. Accurate prediction of synthesizability and precursors of 3D crystal structures via large language models. Nature Communications, 16(1):6530, 2025. Micha Szafarczyk, Piotr Ludynia, Przemys Kukla, et al. python library for efficient computation of molecular fingerprints. arXiv preprint arXiv:2403.19718, 2024. Yingheng Tang, Wenbin Xu, Jie Cao, Weilu Gao, Steve Farrell, Benjamin Erichson, Michael W. Mahoney, Andy Nonaka, and Zhi Yao. Matterchat: multi-modal llm for material science, 2025. URL https://arxiv.org/abs/2502.13107. Siyu Isaac Parker Tian, Aron Walsh, Zekun Ren, Qianxiao Li, and Tonio Buonassisi. What information is necessary and sufficient to predict materials properties using machine learning?, 2022. URL https://arxiv.org/abs/2206.04968. Baoning Wang, Zhiyuan Xu, Zhiyu Han, Qiwen Nie, Xi Chen, Hang Xiao, and Gang Yan. SLICESPLUS: crystal representation leveraging spatial symmetry. Materials & Design, 253:113856, 2025a. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators, 2023. URL https: //arxiv.org/abs/2305.17926. Ruobing Wang, Qiaoyu Tan, Yili Wang, Ying Wang, and Xin Wang. Crystalicl: Enabling in-context learning for crystal generation, 2025b. URL https://arxiv.org/abs/2508.20143. Thomas F. Willems, Chris H. Rycroft, Michaeel Kazi, Juan C. Meza, and Maciej Haranczyk. Algorithms and tools for high-throughput geometry-based analysis of crystalline porous materials. Microporous and Mesoporous Materials, 149(1):134141, 2012. Fang Wu, Lirong Wu, Dragomir Radev, Jinbo Xu, and Stan Li. Integration of pre-trained protein language models into geometric deep learning networks. Communications Biology, 6(1):876, 2023. Ming-Xue Wu and Ying-Wei Yang. MetalOrganic Framework (MOF)-Based Drug/Cargo Delivery and Cancer Therapy. Advanced Materials, 29(23):1606134, 2017. Hang Xiao, Rong Li, Xiaoyang Shi, Yan Chen, Liangliang Zhu, Xi Chen, and Lei Wang. An invertible, invariant crystal representation for inverse design of solid-state materials using generative deep learning. Nature Communications, 14(1):7027, 2023. Tian Xie and Jeffrey C. Grossman. Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties. Physical Review Letters, 120(14):145301, 2018. Changwen Xu, Shang Zhu, and Venkatasubramanian Viswanathan. Cloud: scalable and physicsinformed foundation model for crystal representation learning, 2025. URL https://arxiv. org/abs/2506.17345. Omar Yaghi, Michael OKeeffe, Nathan Ockwig, Hee Chae, Mohamed Eddaoudi, and Jaheon Kim. Reticular synthesis and the design of new materials. Nature, 423(6941), 2003. Keqiang Yan, Xiner Li, Hongyi Ling, Kenna Ashen, Carl Edwards, Raymundo Arroyave, Marinka Zitnik, Heng Ji, Xiaofeng Qian, Xiaoning Qian, and Shuiwang Ji. Invariant tokenization of crystalline materials for language model enabled generation. In Proceedings of the 38th International Conference on Neural Information Processing Systems, NIPS 24, Red Hook, NY, USA, 2025. Curran Associates Inc. ISBN 9798331314385. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. 13 Preprint Jian-Jun You, Fang-Fang Li, Xia-Yun Zeng, Yu-Peng Liu, Si-Han Lin, Neng-Bin Hua, Qian-Ting Wang, Li-An Ma, and Lei Zhang. cage-based metal-organic framework with unique tetrahedral node for size-selective CO2 capture. Journal of Solid State Chemistry, 311:123140, 2022. Guobin Zhao, Logan Brabson, Saumil Chheda, Ju Huang, Haewon Kim, Kunhuan Liu, Kenji Mochida, Thang Pham, Gianmarco Terrones, Sunghyun Yoon, et al. Core mof db: curated experimental metal-organic framework database with machine-learned properties for integrated material-process screening. Matter, 8(6), 2025. Yong Zhao, Edirisuriya M. Dilanga Siriwardane, Zhenyao Wu, Nihang Fu, Mohammed Al-Fahdi, Ming Hu, and Jianjun Hu. Physics guided deep learning for generative design of crystal materials with symmetry constraints. npj Computational Materials, 9(1):38, 2023. Zhiling Zheng, Nakul Rampal, Theo Jaffrelot Inizan, Christian Borgs, Jennifer Chayes, and Omar Yaghi. Large language models for reticular chemistry. Nature Reviews Materials, 10(5): 369381, 2025. Yanqiao Zhu, Jeehyun Hwang, Keir Adams, Zhen Liu, Bozhao Nan, Brock Stenfors, Yuanqi Du, Jatin Chauhan, Olaf Wiest, Olexandr Isayev, Connor W. Coley, Yizhou Sun, and Wei Wang. Learning over molecular conformer ensembles: Datasets and benchmarks, 2024. URL https: //arxiv.org/abs/2310.00115. Xiang Zhuang, Keyan Ding, Tianwen Lyu, Yinuo Jiang, Xiaotong Li, Zhuoyi Xiang, Zeyuan Wang, Ming Qin, Kehua Feng, Jike Wang, Qiang Zhang, and Huajun Chen. Advancing biomolecular understanding and design following human instructions. Nature Machine Intelligence, 7(7):1154 1167, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 THE USE OF LARGE LANGUAGE MODELS In this study, large language models were employed in several aspects of our work. During manuscript preparation, we used LLMs for polishing the language. In the research process, LLMs were applied to literature corpus processing, benchmark testing on the MOF-SPK database, and serving as evaluators of experimental results. The specific usage details and the models adopted are provided in the main text. All intellectual contributions such as research ideas, experimental designs, analyses, and conclusions were developed solely by the authors, who take full responsibility for the content of this paper. A.2 MOF-SPK STATISTICAL ANALYSIS In this section we examine the underlying chemical balance of the MOF-SPK dataset in its three main aspects, structure, property and knowledge. In terms of elemental diversity, MOF-SPK is quite diverse: excluding noble gases that do not form coordination bonds, the database contains up to 81 chemical elements (Fig. 5A). We further examine the distribution of MOF sizes in the database, expressed in number of LLM tokens, to assess the representational bias of diverse materials when processed by LLMs. For this we used the Qwen2.5-7B (Yang et al., 2025) tokenizer to quantify the token-length distribution of CIF representations for MOFs. The dataset exhibits unimodal distribution with peak between 103 and 104 tokens, indicating that textual serialization of crystal structures typically requires thousands to tens of thousands of tokens (Fig. 5B). Notably, the most verbose case, the CIF of material LELMEW, reaches 94,000 tokens while the structure contains only 3216 atoms, underscoring the substantial redundancy introduced by purely text-based encodings of complex crystalline materials. These observations motivate more compact representations, such as structured symbols or multimodal embeddings, that capture geometric and compositional information without incurring excessive sequence length. We further analyzed the distributions of five key MOF properties, namely LCD, PLD, density, ASA, and VF, crucial for understanding MOFs physical characteristics. Their distributions exhibit long-tailed behavior, highlighting the inherent challenges in predicting these properties (Fig. 5C). We use Qwen2.5-7B to analyze and summarize the application landscape of MOFs. The results show substantial breadth: beyond uses such 14 Preprint as gas adsorption and separation, catalysis, and chemical sensing, MOFs also function as luminescent materials and as crystalline sponges for hostguest chemistry (Fig. 5D). This diversity creates an opportunity for LLMs to exceed the capabilities of individual human experts, who are typically specialized in single application area and may overlook cross-domain potential. For example, framework designed by an adsorption specialist might underperform on uptake targets yet be an excellent catalyst; domain boundaries can mask such good alternative uses. Figure 5: Data analysis of structurepropertyknowledge database for crystal materials. (A) Elemental distribution in the dataset. (B) The token quantity density distribution of the CIFs in the dataset. (C) The distribution of properties of crystal material in the dataset. (D) The application distribution of the crystal material in the dataset. A.3 DATASET EXAMPLE 15 Preprint Prompt The structure of the crystal material is <crys>. Can you provide the pore limiting diameter of this crystal material according to the structure of this crystal material? The pore limiting diameter (PLD) in crystal materials refers to the smallest diameter of sphere that can pass through the pore structure of the material. This diameter represents the smallest void space in the pore system, and its crucial for understanding the materials size-selective capabilities, especially in applications like gas separation. The output format is [[pore limiting diameter]] and the unit of the output is A. The probe molecule is the Nitrogen molecule, and the radius of the probe molecule is 1.82 A. Ground-truth [[1.5 A]] Figure 6: Example of prompt and ground truth for property prediction task. Prompt The structure of the crystal material is <crys> .The Simplified Molecular Input Line Entry System (smiles) is specification in the form of line notation for describing the structure of chemical species using short ASCII strings. SMILES strings can be imported by most molecule editors for conversion back into two-dimensional drawings or three-dimensional models of the molecules.Can you provide the smiles of this crystal material according to the structure of this crystal material.The format of the output is {{SMILES}}. Ground-truth {{O=C1N=NC(=O)c2cnccc21.[Zn]}} Figure 7: Example of prompt and ground truth for structure extraction task. Prompt The structure of the crystal material is <crys>. Please provide the Application of this crystal material and provide the reason in 100 words according to the structure of this crystal material. Ground-truth Anti-corrosion coatings for electrogalvanized steel The reason is: The study demonstrates that binary zinc carboxylate MOFs (e.g., ZnC12C16) form compact, hydrophobic layers on metal surfaces. These coatings act as barriers against corrosion by inhibiting zinc dissolution and reducing solubility. Their performance surpasses single carboxylates (e.g., Zn(C12)2 or Zn(C16)2) in electrochemical and climatic tests, aligning with industrial requirements for eco-friendly alternatives to phosphating/chromating processes. Figure 8: Example of prompt and ground truth for description generation task. 16 Preprint Prompt The structure of the crystal material is <crys>. Please answer this question in 100 words according to the structure of this crystal material. Question: What advantages do the new zinc-based metal-organic frameworks (MOFs) offer in terms of environmental impact and synthesis? Ground-truth The new zinc-based MOFs can be easily synthesized in water, providing an environmentally friendly anti-corrosion treatment for metals, which aligns with sustainable practices in material science. Figure 9: Example of prompt and ground truth for question&answer task. Figure 10: Example of crystallographic information file. 17 Preprint Figure 11: Performance comparison of Gemini-2.5-pro and L2M3OF in the ablation study."
        }
    ],
    "affiliations": [
        "Department of Chemistry, University of Liverpool",
        "Department of Computer Science, University of Stanford",
        "Leverhulme Research Centre for Functional Materials Design, University of Liverpool",
        "School of Computer Science and Engineering, University of New South Wales"
    ]
}