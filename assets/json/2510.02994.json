{
    "paper_title": "Towards Scalable and Consistent 3D Editing",
    "authors": [
        "Ruihao Xia",
        "Yang Tang",
        "Pan Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D editing - the task of locally modifying the geometry or appearance of a 3D asset - has wide applications in immersive content creation, digital entertainment, and AR/VR. However, unlike 2D editing, it remains challenging due to the need for cross-view consistency, structural fidelity, and fine-grained controllability. Existing approaches are often slow, prone to geometric distortions, or dependent on manual and accurate 3D masks that are error-prone and impractical. To address these challenges, we advance both the data and model fronts. On the data side, we introduce 3DEditVerse, the largest paired 3D editing benchmark to date, comprising 116,309 high-quality training pairs and 1,500 curated test pairs. Built through complementary pipelines of pose-driven geometric edits and foundation model-guided appearance edits, 3DEditVerse ensures edit locality, multi-view consistency, and semantic alignment. On the model side, we propose 3DEditFormer, a 3D-structure-preserving conditional transformer. By enhancing image-to-3D generation with dual-guidance attention and time-adaptive gating, 3DEditFormer disentangles editable regions from preserved structure, enabling precise and consistent edits without requiring auxiliary 3D masks. Extensive experiments demonstrate that our framework outperforms state-of-the-art baselines both quantitatively and qualitatively, establishing a new standard for practical and scalable 3D editing. Dataset and code will be released. Project: https://www.lv-lab.org/3DEditFormer/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 4 9 9 2 0 . 0 1 5 2 : r TOWARDS SCALABLE AND CONSISTENT 3D EDITING Ruihao Xia1, Yang Tang1, Pan Zhou2 1East China University of Science and Technology, 2Singapore Management University"
        },
        {
            "title": "ABSTRACT",
            "content": "3D editingthe task of locally modifying the geometry or appearance of 3D assethas wide applications in immersive content creation, digital entertainment, and AR/VR. However, unlike 2D editing, it remains challenging due to the need for cross-view consistency, structural fidelity, and fine-grained controllability. Existing approaches are often slow, prone to geometric distortions, or dependent on manual and accurate 3D masks that are error-prone and impractical. To address these challenges, we advance both the data and model fronts. On the data side, we introduce 3DEditVerse, the largest paired 3D editing benchmark to date, comprising 116,309 high-quality training pairs and 1,500 curated test pairs. Built through complementary pipelines of pose-driven geometric edits and foundation model-guided appearance edits, 3DEditVerse ensures edit locality, multi-view consistency, and semantic alignment. On the model side, we propose 3DEditFormer, 3D-structure-preserving conditional transformer. By enhancing image-to-3D generation with dual-guidance attention and time-adaptive gating, 3DEditFormer disentangles editable regions from preserved structure, enabling precise and consistent edits without requiring auxiliary 3D masks. Extensive experiments demonstrate that our framework outperforms state-of-the-art baselines both quantitatively and qualitatively, establishing new standard for practical and scalable 3D editing. Dataset and code will be released. Project: https://www.lv-lab.org/3DEditFormer/"
        },
        {
            "title": "INTRODUCTION",
            "content": "3D editing aims to locally manipulate the geometry or appearance of 3D object in controllable and efficient manner, and is essential for both professional workflows and everyday creative tasks. It has become critical capability across applications such as immersive content creation (Chen et al., 2018), digital entertainment (Zhan et al., 2024), AR/VR (Madhavaram et al., 2025), and product design (Huang et al., 2024a). Despite its importance, 3D editing remains far more challenging than 2D editing (Brooks et al., 2023; Kawar et al., 2023; Liu et al., 2025; Wang et al., 2025; Huang et al., 2025a). Unlike 2D editing, which modifies single image, 3D editing must simultaneously ensure cross-view geometric consistency, global structural fidelity, and fine-grained controllability. These challenges have prevented 3D editing from reaching the ease and accessibility of modern 2D tools. Existing approaches fall into three categories. First, Score Distillation Sampling (SDS) methods (Dong et al., 2024; Liu et al., 2024; Zhuang et al., 2024) distill guidance from 2D diffusion priors (Rombach et al., 2022), but are prohibitively slow, often taking tens of minutes per edit. Second, multi-view image editing methods (Chen et al., 2024a;b; Qi et al., 2024; Li et al., 2025b; Bar-On et al., 2025; Erkoc et al., 2025) modify multiple rendered views and reconstruct them into 3D (Xu et al., 2024a), but struggle with cross-view consistency, leading to distortions and misalignments. Third, end-to-end 3D generative models such as Trellis (Xiang et al., 2025) and VoxHammer (Li et al., 2025a) operate in latent spaces, but depend on manually created 3D masks that are coarse and error-prone, often causing unintended changes. For example, in Fig. 4 (4th column), adding hat to dog also alters its body. These limitations make existing methods impractical for real-world use. Ideally, practical 3D editing system should match the intuitiveness of modern 2D editing tools (Liu et al., 2025; Wang et al., 2025): allowing users to specify edits with simple prompts while producing fast, precise, localized, and structure-preserving modifications, without manual mask creation. The Corresponding Author 1 Table 1: 3D editing dataset comparison of 3D-Alpaca-Editing (Ye et al., 2025), CMD (Li et al., 2025b), and Edit3D-Bench (Li et al., 2025a) across key criteria. Consistency refers to the preservation of unedited regions. 3D-Alpaca-Editing lacks this property as it independently generates the before/after assets. Harmony denotes whether the edit appears natural and semantically coherent. CMD falls short in this regard since it constructs new objects by concatenating unrelated 3D assets. 3D Editing Datasets Training Size Testing Size Edit Region Training & Test Consistency Harmony 3D-Alpaca-Editing CMD Edit3D-Bench 52,532 40,000 3DEditVerse (Ours) 116,309 50 1,500 Figure 1: Some examples of our 3DEditVerse dataset. See more examples in Appendix A. key challenge, then, is: How can we enable precise, localized 3D edits with intuitive prompts while maintaining structural fidelity across views? This is the problem we tackle in this work. Contributions. To address this challenge, we focus on two fundamental bottlenecks: the scarcity of paired 3D editing datasets and the difficulty of achieving controllable and structure-preserving edits. First, we introduce 3DEditVerse, the first large-scale high-fidelity 3D editing benchmark, comprising 116,309 paired original and edited 3D assets for training and 1,500 for testing. Unlike prior datasets (see Tab. 1) which are limited in scale, edit diversity, or annotation granularity, 3DEditVerse meets four essential criteria: localized edit regions, scalability for large-scale training, multi-view consistency, and semantic harmony. It is constructed through two complementary pipelines: (i) pose-driven geometric edits, which generate beforeafter assets capturing diverse articulations and geometric variations of animated characters; (ii) appearance-driven edits, guided by textual instructions and leveraging cascade of foundation modelsDeepSeek-R1 (Guo et al., 2025) for prompt diversification, Flux (Labs, 2024; Labs et al., 2025) for source-target image synthesis, QwenVL (Bai et al., 2025) for automated edit instruction generation and region localization, and Trellis (Xiang et al., 2025) for 3D liftingaugmented with multi-view mask projection and latent-space repainting (Lugmayr et al., 2022). Finally, 1,500 test samples are manually evaluated and carefully curated through human assessment. This design ensures edits are localized, consistent across views, and harmonious with unedited regions, providing the first scalable high-quality resource for training and evaluating end-to-end 3D editing models. Second, we propose the 3D-structure-preserving conditional transformer (3DEditFormer), novel extension of image-to-3D Trellis (Xiang et al., 2025) tailored for 3D editing. Existing imageto-3D diffusion models (Xiang et al., 2025; Yang et al., 2024) can generate plausible assets but struggle to preserve structure: unedited regions often drift, and source-target image guidance alone is insufficient to maintain geometric and textural fidelity. 3DEditFormer addresses this by injecting multi-stage features from the source asset into target generation. Specifically, we design DualGuidance Attention Block with two cross-attention pathways: one attends to fine-grained structural features at late diffusion steps, while the other attends to semantic transition features at early steps. Time-Adaptive Gating mechanism balances their influence to emphasize semantic edits early and structural fidelity later. This enables localized, consistent, and structure-preserving 3D edits without manual 3D masks (Barda et al., 2025) or external constraints (Li et al., 2025a). Finally, training 3DEditFormer on 3DEditVerse achieves state-of-the-art (SoTA) 3D editing performance. Our approach produces edits that are both faithful to user intent and consistent across views. As shown in Fig. 4, our approach enables high-quality local modifications while maintaining structural fidelity, outperforming existing baselines by significant margins. Moreover, unlike VoxHammer (Li et al., 2025a), which relies on precise 3D masks as auxiliary input, our 3DEditFormer achieves superior results without requiring any mask, yielding an average +13% improvement on 3D metrics and demonstrating both higher fidelity and greater practicality. 2 Figure 2: Overview of our data generation pipeline for text-guided 3D editing. Starting from largescale Vocabulary Set, we employ multiple foundation models in carefully orchestrated manner and construct the text-to-image-to-3D lifting pipeline."
        },
        {
            "title": "2 RELATED WORK",
            "content": "3D Generation. Early 3D generation relied on GANs (Goodfellow et al., 2020) but struggled with diversity and fidelity (Chan et al., 2022; Gao et al., 2022). Diffusion models (Ho et al., 2020) later improved quality across different representations such as multi-view images (Liu et al., 2023; Huang et al., 2024c), triplanes (Wu et al., 2024a; Shue et al., 2023), and 3D Gaussians (Chen et al., 2024c; Xu et al., 2024b), yet efficiency and accurate appearance modeling remain challenging issues. Recent advances (Xiang et al., 2025; Yang et al., 2024) have moved to latent spaces for more compact and scalable generation. We build on this foundation and extend it to localized editing, introducing structural priors and edit-aware mechanisms that enable faithful, structure-preserving modifications. 3D Editing. Existing approaches fall into three main paradigms. SDS-based methods (Miao et al., 2025; Huang et al., 2025b) leverage 2D diffusion priors to optimize 3D assets, but are prohibitively slow and unsuitable for interactive use. Multi-view editing methods (Chen et al., 2024b; Qi et al., 2024) modify rendered images before reconstructing them into 3D, offering efficiency but often suffering from cross-view inconsistencies and distorted geometry. End-to-end generative models directly edit assets in latent space, achieving better integration of shape and texture, but they still depend on manually annotated 3D masks (Xiang et al., 2025; Li et al., 2025a), which are coarse, labor-intensive, and prone to unintended modifications. In contrast, our 3DEditFormer eliminates the need for such manual masks while still achieving localized and consistent 3D edits. 3 3DEDITVERSE DATASET critical obstacle in advancing 3D editing is the absence of large-scale paired datasets of original and edited 3D assets. Such pairs are essential for training models that can faithfully learn how local edits affect geometry and appearance while preserving unedited regions. Without them, models either overfit to synthetic toy edits or rely on weak supervision, limiting generalization and practical use. Existing datasets (Li et al., 2025b;a; Ye et al., 2025), summarized in Tab. 1, are insufficient due to small scale, missing edit correspondences, or unrealistic scenarios. This gap prevents 3D editing methods from achieving the same accessibility and precision as their 2D counterparts. To address this, we present 3DEditVerse, the first large-scale dataset of paired 3D assets for local editing. It is designed to (i) provide sufficient scale and diversity, covering both geometric and appearance edits, and (ii) ensure edits are realistic, localized, and structure-preserving. To this end, 3 we introduce two automated pipelinespose-driven geometric edits and text-guided appearance editsthat generate high-quality paired assets at scale. In addition, 1,500 test samples are manually evaluated and curated to guarantee the reliability of evaluation benchmark."
        },
        {
            "title": "3.1 CHARACTER–ANIMATION COMPOSITIONS FOR GEOMETRIC EDITS",
            "content": "The first pipeline targets pose-driven geometric edits, where the same object undergoes articulation or structural variation while maintaining identity. We leverage publicly available 3D characters and animation sequences (Inc., 2021), exploiting the fact that different poses of the same character naturally form valid beforeafter edit pairs. The generation process proceeds in two steps. (1) Candidate Pose Generation. Animation sequences are sampled at fixed intervals to extract candidate frames. key challenge is redundancy: many poses across or within sequences are visually similar. To ensure diversity, we render each pose from canonical view, extract embeddings using DINOv2 (Oquab et al., 2024), and prune near-duplicates based on cosine similarity. This yields curated pool of 4,998 unique candidate poses spanning wide variety of articulations. (2) Data Assembly. From our collection of 108 distinct characters, we pair each one with 500 poses randomly selected from the candidate pool. This procedure results in total of 108 500 = 54,000 unique 3D assets. Paired data is then formed by associating different pose-renders of the same character, providing rich source for training models on pose and shape alterations. The visualization of characteranimation compositions is shown in Fig. 1. 3.2 TEXT-GUIDED GENERATIVE PIPELINE FOR APPEARANCE EDITS The second pipeline focuses on appearance-driven edits, which modify textures, colors, or fine details while preserving overall geometry. Unlike geometry edits, these require sophisticated generative pipelines. We design fully automated text-to-image-to-3D lifting pipeline via cascade of foundation models. In Fig. 2, each model is annotated with numbered label (e.g., ①), and we follow the same numbering in the description for clarity. Details of the instructional prompts for each model are provided in Appendix B. Source and Target Images, and Edit Prompt Generation. As shown in Fig. 2 (upper), we start with the 4,585-word vocabulary from Huang et al. (2024b). For each word, ① DeepSeek-R1 (Guo et al., 2025) generates diverse descriptive prompts, which are fed into ② Flux.1-Dev (Labs, 2024) to synthesize high-quality source image src. This serves as the before-edit state. Then, ③ QwenVL (Bai et al., 2025) analyzes src and generates edit instructions pedit, each describing plausible and semantically coherent modification (e.g., add vase to the table). Finally, src, pedit is provided to ④ Flux.1-Kontext (Labs et al., 2025), which executes the edit and outputs the target image tgt. This produces large-scale, semantically rich image editing pairs for lifting into 3D. In addition, we leverage samples from (Ye et al., 2025), where each sample provides source image rendered from existing 3D assets in the Objaverse-XL dataset (Deitke et al., 2023) and an accompanying edit prompt generated from predefined template. We take these sourceprompt pairs and apply the Flux.1-Kontext model (Labs et al., 2025) to execute the edits. This augmentation not only diversifies the distribution of editing instructions but also ensures alignment with real 3D geometries, strengthening the robustness of our paired data. 3D Lifting with Consistency Preservation. straightforward approach of independently lifting the source and target images to 3D using models such as Trellis (Xiang et al., 2025) often produces severe geometric distortions and identity mismatches. To address this, we propose consistencypreserving 3D lifting pipeline that explicitly localizes the edit region in 3D and applies maskguided repainting strategy (Lugmayr et al., 2022) to ensure fidelity. (1) Edited-Region Identification. As shown in Fig. 2 (lower-left), we first generate initial 3D assets ˆSsrc 3D and ˆStgt 3D from the source and target images with the Trellis model. To localize the edit without manual annotation, we employ ⑤ Qwen-VL as an open-set detector: given rendered view of the 3D asset and the edit instruction, it outputs 2D bounding box B2D to highlight the edited region. 4 3D }N Figure 3: Overview of our proposed 3DEditFormer. i=1 and {f (2,i) i=1 are extracted from the frozen Trellis model (Xiang et al., 2025) at different denoising timesteps, capturing fine-grained structural priors and semantic transition cues, respectively. (b) These features are injected into each transformer layer via (c) Dual-Guidance Attention Block, where their contributions are modulated by (d) Time-Adaptive Gating mechanism. (a) Multi-stage features {f (1,i) 3D }N (2) Multi-View 3D Mask Projection. As shown in Fig. 2 (lower), to obtain robust 3D mask, we render the rotating asset ˆSsrc/tgt into sequence of views and apply ⑥ SAM2 (Ravi et al., 2025) to segment and track the target region across frames. The resulting 2D masks {M 2D} are then backprojected into 3D space using the pinhole camera model. Formally, given the intrinsic and extrinsic parameters Ki and [Ri ti] of camera i, one can project voxel = (x, y, z, 1) onto the i-th view: 3D pi = Ki[Ri ti]v, pi = (cid:16) pi,x pi,z , pi,y pi,z (cid:17) . We check whether pi lies inside the 2D mask (cid:88)N c(v) = 1[pi 2D], 2D, and accumulate evidence across all views: i=1 where is the number of views, and is set to 70 in this work. The final 3D mask is defined as (1) (2) M3D = {v c(v) τ }, (3) retaining voxels consistently supported by at least fraction τ of views. This ensures that the mask is geometrically consistent and resilient to segmentation noise. (3) Localized 3D Editing. With M3D in place, we perform localized 3D editing using the Repaint strategy (Lugmayr et al., 2022) within Trellis (Xiang et al., 2025), as shown in Fig. 2 (lower-right). During denoising, the latent representation of the source asset is selectively fused with that of the target, but only within the masked region. This ensures precise modifications while preserving the rest of the geometry, yielding high-fidelity structure-preserving edits. See details in Appendix C. (4) Post-Editing Consistency Filtering. To ensure global consistency, in Fig. 2 (lower-right), we render both the edited asset Stgt 3D into multiple views and compare them using DINOv2 (Oquab et al., 2024) feature similarity. Samples with mean cosine similarity below threshold are discarded, effectively removing artifacts from incomplete mask coverage and enhancing the robustness of the pipeline. 3D and the initial target prediction ˆStgt Together, the two pipelines produce about 118K paired 3D assets, covering both 54,000 structural pose-driven and 64,123 appearance-based edits. Crucially, all pairs are localized, consistent across views, and semantically coherent, enabling robust supervised training of 3D editing models. Then, 1,500 test samples are manually assessed and carefully curated to further guarantee the reliability of the evaluation benchmark. As shown in Tab. 1, unlike existing datasets that are either too small or weakly annotated, 3DEditVerse is the first benchmark to combine scale, diversity, and fidelity. This resource lays the foundation for systematic progress in 3D editing research. 3D-STRUCTURE-PRESERVING CONDITIONAL TRANSFORMER While SoTA image-to-3D generation models (Xiang et al., 2025; Yang et al., 2024; Wu et al., 2024c) can synthesize plausible 3D assets from single image, they struggle to preserve structural consistency in editing scenarios. In particular, providing only the source or target images is insufficient 5 for the model to determine which regions of the original geometry and texture should remain unchanged, often leading to unintended distortions in unedited areas. To address this problem, we introduce the 3D-Structure-Preserving Conditional Transformer (3DEditFormer), new framework explicitly designed to inject structural priors from the source asset into the generation of the edited asset. Unlike prior approaches that treat editing as re-synthesis from scratch, 3DEditFormer enforces principled coupling between source and target through dual structural guidance. As shown in Fig. 3, our framework consists of three key innovations: (i) Dual-Guidance Attention Block that integrates source-aware cross-attention at multiple levels, (ii) Multi-Stage Feature Extraction module that disentangles fine-grained structural fidelity from semantic transition cues, and (iii) Time-Adaptive Gating mechanism that dynamically balances these signals across denoising stages. Together, these components resolve the inconsistency problem of prior methods and enable edits that are both localized and structure-preserving in 3D space."
        },
        {
            "title": "4.1 DUAL-GUIDANCE ATTENTION BLOCK",
            "content": "3DEditFormer builds upon the Trellis (Xiang et al., 2025), an image-to-3D framework which stacks transformer attention layers consisting of self-attention, cross-attention, and Feed-Forward Networks (FFN). We freeze the Trellis backbone to retain its generative strength and augment the original self-attention with our proposed Dual-Guidance Attention Block (DualAttn), as shown in Fig. 3 (b). This block introduces two parallel cross-attention branches, while keeping the other pathways untouched. These two cross-attention branches interact with the multi-stage features described in Sec. 4.2, which encode complementary structural information from the source 3D asset, as shown in Fig. 3 (c). Accordingly, 3DEditFormer directly injects source-aware priors into every layer, constraining the editing process to remain faithful to the original structure of the source 3D asset. Formally, let be the input feature of the i-th dual-guidance attention block. Each block first computes: h1 = SelfAttn(Norm(x)), (4) representing the original frozen self-attention pathway. To integrate source 3D structural priors, we introduce two additional feature sets, {f (1,i) 3D }, extracted from the frozen Trellis at distinct timesteps (see details in Sec. 4.2). Then, the corresponding cross-attention branches are: 3D , (2,i) h2 = CrossAttn1(Norm(x), (1,i) 3D ), h3 = CrossAttn2(Norm(x), (2,i) 3D ). (5) The outputs are adaptively gated using timestep-dependent coefficients (g1, g2), which will be elaborated on Sec. 4.3: = h1 + g1 h2 + g2 h3, (6) where denotes element-wise scaling. This fused representation is then passed through the original cross-attention with image context tgt and the FFN, completing the attention layer computation. 4.2 MULTI-STAGE FEATURE EXTRACTION central novelty of 3DEditFormer lies in its dual feature design, which captures complementary signals from different diffusion stages, as shown in Fig. 3 (a). Fine-grained structural features {f (1,i) 3D at late diffusion timestep 0. Since the denoising network at late timesteps emphasizes structural refinement, these features encode detailed structural information necessary for preserving unedited regions. i=1 are extracted from the source 3D ˆSsrc 3D }N Semantic transition features {f (2,i) i=1 are derived by conditioning the frozen network on both the source 3D asset ˆSsrc 3D and the target image tgt at an early timestep 1. Early denoising stages prioritize semantic alignment with conditioning signals, enabling these features to capture how the structure should evolve to reflect the edit. 3D }N Formally, for timesteps t1 0 and t2 1, we compute: {f (1,i) 3D }N i=1 = F(Ssrc 3D, t1, zero), {f (2,i) 3D }N i=1 = F(Ssrc 3D, t2, tgt), (7) where denotes the frozen Trellis transformer that produces set of block-wise features in single forward pass, zero is an empty image condition, and tgt is the target edited image."
        },
        {
            "title": "4.3 TIME-ADAPTIVE GATING",
            "content": "To balance the contribution of the two feature types throughout denoising, we introduce timeadaptive gating mechanism, as shown in Fig. 3 (d): (g1, g2) = MLPedit(tembedding). (8) The MLP generates dynamic weights depending on the current timestep embedding tembedding. At early timesteps, the model emphasizes (2,i) 3D to capture semantic transitions, while at later timesteps it prioritizes (1,i) 3D to ensure structural fidelity. By integrating dual guidance, multi-stage feature extraction, and adaptive gating, 3DEditFormer introduces the first framework that explicitly disentangles what should change from what should remain in 3D editing. This resolves fundamental bottleneck of existing methods, providing edits that are localized, consistent, and structure-preservingan essential step toward scalable 3D editing."
        },
        {
            "title": "4.4 TRAINING AND INFERENCE",
            "content": "In the first stage, transformer (1) θ1 Our 3DEditFormer follows the two-stage generation paradigm established in Trellis (Xiang et al., 2025). generates coarse voxelized shapes that capture the global structure. In the second stage, separate transformer (2) refines fine-grained texture and θ2 appearance features, which are subsequently decoded into explicit 3D representations such as 3D Gaussians (Kerbl et al., 2023) or meshes via VAE-based decoder (Kingma & Welling, 2013). The two transformers are parameterized independently but are both trained under the same Conditional Flow Matching (CFM) objective (Lipman et al., 2023): L(θk) = Et,x0,ϵT (k) θk (x, t) (ϵ x0)2 2, (9) where x(t) = (1 t)x0 + tϵ interpolates between clean sample x0 and noise ϵ with timestep t. Here, (k) for the corresponding training. θk is either (1) θ1 or (2) θ"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "With frozen Trellis, only 252M parameters are fine-tuned for 40k iterations across the voxel generation and texture refinement stages with batch size 16 using AdamW (Loshchilov & Hutter, 2017). For 3D metrics, we follow (Wu et al., 2024b), and uniformly sample 100,000 points from both (1) Chamfer Distance (CD) (Fan et al., 2017) the predicted mesh and the ground-truth mesh. computes the average closest-point distance between the two sets, while (2) Normal Consistency (NC) (Gkioxari et al., 2019) measures the alignment of surface normals, capturing geometric fidelity. (3) F10.01 (Knapitsch et al., 2017) reports the harmonic mean of precision and recall under strict distance threshold of 0.01, reflecting preservation of fine geometric details. For 2D metrics, following (Li et al., 2025a), each mesh is rendered from 10 fixed camera viewpoints. (1) PSNR quantifies pixel-level reconstruction accuracy, and (2) SSIM (Wang et al., 2004) evaluates structural similarity in luminance, contrast, and texture. (3) LPIPS (Zhang et al., 2018), based on deep perceptual features, reflects perceptual similarity, with lower values indicating better quality. Finally, (4) DINO-I computes cosine similarity between DINOv2 (Oquab et al., 2024) image embeddings, assessing semantic consistency between rendered outputs and reference images. 5.1 MAIN RESULTS Qualitative Comparison. We present qualitative comparisons with SoTA methods in Fig. 4. By comparison, one can observe that EditP23 (Bar-On et al., 2025) fails to preserve geometry and texture fidelity, often yielding over-smoothed or incomplete results (e.g., the ship losing structural detail, the soldiers uniform becoming blurred). Instant3dit (Barda et al., 2025) can generate edited variants but frequently introduces severe artifacts, such as broken geometry in the ship and collapsed textures in the soldier, indicating instability under localized edits. Figure 4: Qualitative comparison among our proposed 3DEditFormer and SoTAs, including EditP23 (Bar-On et al., 2025), Instant3dit (Barda et al., 2025), and VoxHammer (Li et al., 2025a) on our proposed 3DEditVerse test set. More visualizations are provided in appendix and E. VoxHammer (Li et al., 2025a) demonstrates stronger geometric fidelity than EditP23 and Instant3dit but is highly sensitive to mask accuracy. When 3D masks are imprecise, its editing consistency deteriorates rapidly, as illustrated by the red circles in Fig. 4. In contrast, our 3DEditFormer does not require any 3D masks: edits are guided solely by the target image, greatly simplifying the pipeline while preserving both structure and consistency. For instance, it successfully adds secondary ship without distorting the original vessel and removes the soldiers rifle while maintaining uniform integrity. These results show that 3DEditFormer achieves faithful localized edits and preserves unedited regions, outperforming prior methods in both accuracy and usability. Quantitative Comparison. Tab. 2 reports quantitative results against SoTA methods on our 3DEditVerse test set. Our 3DEditFormer consistently outperforms existing methods across both 3D and 2D metrics. EditP23 (Bar-On et al., 2025) exhibits the weakest performance, with high CD and low NC, reflecting poor geometric fidelity. Instant3dit (Barda et al., 2025) achieves moderate improvements but suffers from unstable quality, as indicated by low F10.01 and SSIM. VoxHammer (Li et al., 2025a) achieves strong results and the best SSIM when accurate 3D masks are available, highlighting its ability to preserve low-level structural similarity. However, its reliance on precise masks severely limits its practicality: with even small perturbations (e.g., increasing the 3D masks by 9%, 18%, or 27%), VoxHammer suffers from severe performance degradation, as shown in Tab. 2. In contrast, our 3DEditFormer attains the best overall performance without any auxiliary 3D mask, achieving +13% improvement of 3D Metrics than VoxHammer. It surpasses prior methods on CD, NC, F1, PSNR, LPIPS, and DINO-I, while remaining competitive on SSIM. These results demonstrate that 3DEditFormer achieves state-of-the-art fidelity and consistency through simpler, more robust pipeline, removing the need for external mask supervision. 8 Table 2: Quantitative test performance on 3DEditVerse. 3D Mask indicates whether method requires 3D mask of the editing region at inference. Impro. denotes the average relative improvement on 2D/3D metrics compared with the baseline EditP23 or Instant3dit. Full denotes all the test data of our 3DEditVerse, and w/o Char-Anim means excluding test characteranimation compositions from 3DEditVerse since this subset lacks 3D editing masks and thus cannot be used to evaluate methods requiring explicit masks. VoxHammer uses the ground-truth masks, while the subscript in VoxHammer+9% denotes an increase of the ground-truth mask radius by 9%. Method EditP23 3DEditFormer 3D Mask Test Data Full Instant3dit VoxHammer VoxHammer+9% VoxHammer+18% VoxHammer+27% 3DEditFormer w/o Char-Anim 3D Metrics 2D Metrics CD NC F10.01 Impro. PSNR SSIM LPIPS DINO-I Impro. 46.19 0.689 13.84 0.830 29.34 0.734 9.84 0.885 10.27 0.880 10.95 0.873 11.42 0.867 7.04 0. 32.33 64.30 32.84 77.22 75.56 73.72 72.02 86.05 - 18.32 63.1% 24.40 - 20.16 74.1% 26.11 71.7% 25.83 68.7% 25.53 66.2% 25.19 87.1% 26. 0.870 0.918 0.868 0.942 0.939 0.936 0.933 0.938 0.158 0.068 0.132 0.052 0.055 0.058 0.060 0.045 0.785 0.963 0.840 0.959 0.958 0.956 0.955 0. - 39.5% - 37.6% 36.2% 34.8% 33.6% 39.9% Table 3: Ablation study on the effectiveness of the Dual-Guidance Attention Block (Fine-Grained Structural Features (1) 3D + Semantic Transition Features (2) 3D ) and Time-Adaptive Gating. Methods Baseline + Fine-Grained Feat. (1) 3D + Semantic Feat. (2) 3D + Time-Adaptive Gating CD 16.230 14.586 14.084 13.843 3D Metrics NC F10.01 2D Metrics PSNR SSIM LPIPS DINO-I 0.814 0.825 0.828 0.830 60.183 63.701 64.023 64.297 23.656 24.021 24.252 24.395 0.910 0.913 0.916 0. 0.0784 0.0699 0.0687 0.0682 0.956 0.960 0.962 0.963 5.2 ABLATION STUDY Tab. 3 reports the contribution of each component in 3DEditFormer. We begin with baseline model that uses vanilla cross-attention with Ssrc 3D but lacks our multi-stage structural guidance. The results show that this baseline yields relatively weak geometric fidelity and perceptual quality. Introducing fine-grained structural features (1) 3D leads to clear improvements in CD, NC, and F1, indicating that late-stage features help preserve unedited geometric details. Building on this, adding semantic transition features (2) 3D further enhances both 3D and 2D metrics, showing that early-stage features provide complementary cues that guide structural adaptation toward the target edits. Finally, incorporating the proposed time-adaptive gating mechanism delivers the strongest overall performance. By dynamically balancing the contributions of (1) 3D across denoising steps, the model consistently improves upon all metrics. These results highlight the importance of adaptive feature fusion for achieving edits that are both localized and structure-preserving. 3D and (2)"
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduce 3DEditVerse, the first large-scale benchmark for paired 3D editing, comprising about 118K asset pairs with diverse geometryand appearance-driven edits, designed for scalability, consistency, and semantic alignment. Built upon this resource, we propose 3DEditFormer, structure-preserving conditional transformer that employs dual-guidance attention and time-adaptive gating to disentangle editable regions from preserved structure, enabling precise and consistent edits without relying on 3D masks. Extensive experiments show that our framework achieves SoTA performance in 3D editing, combining high fidelity with strong practicality. Limitation Discussion. Our 3DEditFormer relies on latent-space editing, which, while efficient, may introduce precision loss when handling high-resolution 3D assets. Fine geometric details can be degraded during the latent transformation. Future work could explore lossless editing directly in the original 3D domain to better preserve fine-grained mesh fidelity."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025. Roi Bar-On, Dana Cohen-Bar, and Daniel Cohen-Or. EditP23: 3D editing via propagation of image prompts to multi-view. arXiv preprint arXiv:2506.20652, 2025. Amir Barda, Matheus Gadelha, Vladimir Kim, Noam Aigerman, Amit Bermano, and Thibault Groueix. Instant3dit: Multiview inpainting for fast editing of 3D objects. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1627316282, 2025. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1839218402, 2023. Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3D generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1612316133, 2022. Chien-Wen Chen, Jain-Wei Peng, Chia-Ming Kuo, Min-Chun Hu, and Yuan-Chi Tseng. Ontlus: 3D content collaborative creation via virtual reality. In International Conference on Multimedia Modeling, pp. 386389. Springer, 2018. Hansheng Chen, Ruoxi Shi, Yulin Liu, Bokui Shen, Jiayuan Gu, Gordon Wetzstein, Hao Su, and Leonidas Guibas. Generic 3D diffusion adapter using controlled multi-view editing. arXiv preprint arXiv:2403.12032, 2024a. Minghao Chen, Iro Laina, and Andrea Vedaldi. DGE: Direct gaussian 3D editing by consistent multi-view editing. In European Conference on Computer Vision, pp. 7492. Springer, 2024b. Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu. Text-to-3D using gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2140121412, 2024c. Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-XL: universe of 10M+ 3D objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. Shaocong Dong, Lihe Ding, Zhanpeng Huang, Zibin Wang, Tianfan Xue, and Dan Xu. Interactive3D: Create what you want by interactive 3D generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 49995008, 2024. Ziya Erkoc, Can Gumeli, Chaoyang Wang, Matthias Nießner, Angela Dai, Peter Wonka, Hsin-Ying Lee, and Peiye Zhuang. PrEditor3D: Fast and precise 3D shape editing. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 640649, 2025. Haoqiang Fan, Hao Su, and Leonidas Guibas. point set generation network for 3D object reconstruction from single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 605613, 2017. Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. GET3D: generative model of high quality 3D textured shapes learned from images. Advances in Neural Information Processing Systems, 35:3184131854, 2022. Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh R-CNN. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 97859795, 2019. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. 10 Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. Ian Huang, Guandao Yang, and Leonidas Guibas. Blenderalchemy: Editing 3D graphics with visionlanguage models. In European Conference on Computer Vision, pp. 297314. Springer, 2024a. Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong Guo, and Lei Zhang. Tag2Text: Guiding vision-language model via image tagging. In International Conference on Learning Representations, 2024b. Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Liangliang Cao, and Shifeng Chen. Diffusion model-based image editing: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025a. Yufei Huang, Bangyan Liao, Yuqi Hu, Haitao Lin, Lirong Wu, Siyuan Li, Cheng Tan, Zicheng Liu, Yunfan Liu, Zelin Zang, et al. Dacapo: Score distillation as stacked bridge for fast and highquality 3d editing. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1630416313, 2025b. Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. MV-Adapter: Multi-view consistent image generation made easy. arXiv preprint arXiv:2412.03632, 2024c. Adobe Systems Inc. Mixamo. https://www.mixamo.com/, 2021. Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 60076017, 2023. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3D gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):1391, 2023. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics, 36(4):113, 2017. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. FLUX. 1 Kontext: arXiv preprint Flow matching for in-context image generation and editing in latent space. arXiv:2506.15742, 2025. Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, and Lu Sheng. Voxhammer: Training-free precise and coherent 3D editing in native 3D space. arXiv preprint arXiv:2508.19247, 2025a. Peng Li, Suizhi Ma, Jialiang Chen, Yuan Liu, Congyi Zhang, Wei Xue, Wenhan Luo, Alla Sheffer, Wenping Wang, and Yike Guo. CMD: Controllable multiview diffusion for 3D editing and progressive generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pp. 110, 2025b. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. Fangfu Liu, Hanyang Wang, Weiliang Chen, Haowen Sun, and Yueqi Duan. Make-your-3D: Fast and consistent subject-driven 3D content generation. In European Conference on Computer Vision, pp. 389406. Springer, 2024. 11 Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3D object. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 92989309, 2023. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. In Proceedings of the Repaint: IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1146111471, 2022. Inpainting using denoising diffusion probabilistic models. Vivek Madhavaram, Shivangana Rawat, Chaitanya Devaguptapu, Charu Sharma, and Manohar Kaul. In IEEE/CVF Winter Conference on Towards training free approach for 3D scene editing. Applications of Computer Vision, pp. 28902899. IEEE, 2025. Xingyu Miao, Haoran Duan, Yang Long, and Jungong Han. Rethinking score distilling sampling for 3d editing and generation. arXiv preprint arXiv:2505.01888, 2025. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. Zhangyang Qi, Yunhan Yang, Mengchen Zhang, Long Xing, Xiaoyang Wu, Tong Wu, Dahua Lin, Xihui Liu, Jiaqi Wang, and Hengshuang Zhao. Tailor3D: Customized 3D assets editing and generation with dual-side images. arXiv preprint arXiv:2407.06191, 2024. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. SAM 2: Segment anything in images and videos. In The Thirteenth International Conference on Learning Representations, 2025. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3D neural field generation using triplane diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2087520886, 2023. Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, and Jianchao Yang. Seededit 3.0: Fast and high-quality generative image editing. arXiv preprint arXiv:2506.05083, 2025. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600 612, 2004. Bin-Shih Wu, Hong-En Chen, Sheng-Yu Huang, and Yu-Chiang Frank Wang. TPA3D: Triplane attention for fast text-to-3D generation. In European Conference on Computer Vision, pp. 438 455, 2024a. Qirui Wu, Daniel Ritchie, Manolis Savva, and Angel Chang. Generalizing single-view 3D shape retrieval to occlusions and unseen objects. In 2024 International Conference on 3D Vision, pp. 893902. IEEE, 2024b. Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3D: Scalable image-to-3D generation via 3D latent diffusion transformer. Advances in Neural Information Processing Systems, 37:121859121881, 2024c. 12 Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3D latents for scalable and versatile 3D generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2146921480, 2025. Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3D mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024a. Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein. GRM: Large gaussian reconstruction model for efficient 3D reconstruction and generation. In European Conference on Computer Vision, pp. 120, 2024b. Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, et al. Hunyuan3D 1.0: unified framework for text-to3D and image-to-3D generation. arXiv preprint arXiv:2411.02293, 2024. Junliang Ye, Zhengyi Wang, Ruowen Zhao, Shenghao Xie, and Jun Zhu. Shapellm-omni: native multimodal LLM for 3D generation and understanding. arXiv preprint arXiv:2506.01853, 2025. Chaobi Zhan, Chul-Soo Kim, and Xin Wei. 3D image processing technology based on interactive entertainment application in cultural and creative product design. Entertainment Computing, 50: 100701, 2024. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 586595, 2018. Jingyu Zhuang, Di Kang, Yan-Pei Cao, Guanbin Li, Liang Lin, and Ying Shan. Tip-editor: An accurate 3D editor following both text-prompts and image-prompts. ACM Transactions on Graphics, 43(4):112, 2024."
        },
        {
            "title": "APPENDIX",
            "content": "A VISUALIZATION OF OUR 3DEDITVERSE DATASET Figure 5: More examples of (a) CharacterAnimation Compositions and (b) generative data from text-guided editing in our proposed 3DEditVerse dataset."
        },
        {
            "title": "B DETAILS OF INSTRUCTIONAL PROMPTS",
            "content": "B.1 PROMPT EXAMPLE OF DEEPSEEK-R1 FOR VOCABULARY TO TEXT Prompt Example of DeepSeek-R1 for Vocabulary to Text Instruction: You are an expert creative assistant for text-to-image AI pipeline. Your task is to generate descriptive text prompts for creating an initial image. Specifically, for the object plane: Generate numbered list of 20 text prompts (around 20 words each). Each prompt should contain some attributes or adjectives that describe the object. **Constraint:** Do not describe any background or environment or action, as the object will be rendered on pure white background. **Output Format:** Text Prompts: 1. [Text Prompt 1] 2. ... 20. [Text Prompt 20] Output: Text Prompts: 1. sleek silver jet plane with swept-back wings and twin rear engines on white background. 2. vintage propeller plane featuring rounded wings and radial engine in polished chrome. 3. military stealth plane with angular edges and matte black coating, no visible seams. 4. small white private plane with high-mounted wings and single nose propeller. 5. cargo plane showcasing boxy fuselage and high tail section with four turbofans. 6. paper airplane with sharp creases and pointed wings folded from blue paper. 7. futuristic concept plane with smooth curves and retractable wingtips in metallic teal. 8. seaplane with large pontoons and reinforced wing struts in faded yellow. 9. biplane with dual red fabric wings and exposed cockpit wires. 10. toy plane with chunky plastic wings and oversized propellers in primary colors. 11. drone plane featuring multiple rotors and compact carbon-fiber body. 12. glider plane with extra-long tapered wings and minimalist cockpit framing. 13. fighter jet with delta wings and afterburner nozzles in camouflage gray. 14. crop duster plane with chemical tanks and sturdy landing gear in olive green. 15. stunt plane with checkerboard patterns and reinforced wing supports. 16. commercial airliner with elongated body and blended winglets in navy blue. 17. spaceplane with heat-shield tiles and retractable landing legs. 18. flying wing design with no tail section and embedded engines in charcoal gray. 19. micro-light plane with fabric-covered frame and open cockpit in bright orange. 20. turbo-prop plane with wing-mounted engines and triple-blade propellers. In the above example, we illustrate the case of the vocabulary plane, where the prompt provided to DeepSeek-R1 (Guo et al., 2025) and its corresponding outputs are shown. Specifically, for each vocabulary word, we generate 20 diverse textual descriptions to enhance edit diversity. B.2 PROMPT EXAMPLE OF FLUX.1-DEV FOR SOURCE IMAGE GENERATION Prompt Example of Flux.1-Dev for Source Image Generation Instruction: glider plane with extra-long tapered wings and minimalist cockpit framing in the form of an object in the middle of pure white background, no shadow For each textual description, we append specific template that places the object at the center of pure white background, ensuring clean and unambiguous layout generated by Flux.1-Dev (Labs, 2024). This design facilitates reliable image-to-3D lifting in subsequent stages. The resulting source images are illustrated in Fig. 6. 15 B.3 PROMPT EXAMPLE OF QWEN-VL FOR EDIT PROMPT GENERATION Prompt Example of Qwen-VL for Edit Prompt Generation Instruction: You are an expert visual analyst for an advanced image editing pipeline. Your task is to analyze the provided image of an object and generate high-quality editing prompts. **Analysis & Goal:** Look closely at the object in the image. Your goal is to suggest list of 5 specific **substantial, and obvious shape transformations**. These edits will be used by another AI to modify this exact image. **Core Instructions:** Each suggested edit prompt must fall into one of the following three categories: 1. **Add Part: Propose adding new, structurally significant component that is logically related to the main object. * *Example based on cup image: Add thick, rectangular handle to its side. 2. **Remove Part: Identify visually distinct and major part of the object and propose its removal. * *Example based on chair image: Remove the entire backrest. 3. **Replace Part: Identify specific part of the object and propose replacing it with new part of distinctly different shape. * *Example based on lamp image: Replace its slender cylindrical base with wide pyramid base. **Strict Constraints:** * **Focus ONLY on Shape:** Do not suggest changes related to color, texture, material (e.g., make it wooden), or lighting. The edits must alter the objects geometry. * **Be Specific:** Refer to parts you can actually see in the image (e.g., the base, the top, the legs, the handle). Output: 1. Remove the vertical stabilizer from the tail section. 2. Add small, rounded winglet to the tip of each wing. 3. Replace the nose cone with more pointed, aerodynamic design. 4. Add set of flaps to the trailing edge of the wings. 5. Replace the horizontal stabilizers with larger, swept-back surfaces. We feed each generated source image into the Qwen2.5-VL-7B model (Bai et al., 2025) to produce diverse editing instructions. The example above corresponds to the airplane image shown in the topleft corner of Fig. 6. Each resulting edit prompt is then paired with its source image and passed to the Flux.1-Kontext model (Labs et al., 2025) for image editing. Figure 6 illustrates several examples of edited results alongside their corresponding prompts. B.4 PROMPT EXAMPLE OF QWEN-VL FOR EDIT REGION LOCALIZATION Prompt Example of Qwen-VL for Edit Prompt Generation Instruction: Given the original image and the edited image concatenated side-by-side: text instruction describes the editing area: Remove the canopy above the pump Your task: 1. Compare the two images and locate only the regions in the left image corresponding to the given editing area: canopy 2. 2. Output the bounding box(es) for the regions in the format: [[xmin, ymin, xmax, ymax]] (values in pixels, only in the 512x512 left image). 3. If you cannot find the edited region (i.e. canopy), output an empty list: [] **Directly output the bounding box(es)** Output: [103, 52, 396, 104] 16 We concatenate the source and target images side by side and provide them to the Qwen-VL-Max model (Bai et al., 2025) together with the above prompt, which enables the model to localize the edited region by predicting bounding boxes corresponding to the modification. Figure 6: Examples of sourcetarget image pairs. Text below the source (left) shows the generation prompt, while text below the target (right) shows the editing instruction. DETAILS OF 3D EDITING VIA REPAINT Here we provide the detailed derivation of the Repaint-based (Lugmayr et al., 2022) 3D editing. During the denoising process of generating the target 3D asset Stgt controlled noise into the source asset ˆSsrc 3D, at each timestep we inject 3D to obtain noisy latent representation: (cid:1), = (cid:0) ˆSsrc zsrc 3D, σt (10) where σt denotes the noise variance at timestep determined by the diffusion schedule. To ensure editing is spatially confined, we fuse the noisy latent of the source with that of the target using the binary 3D mask M3D: ˆzt = M3D ztgt where denotes element-wise multiplication. + (1 M3D) zsrc , (11) 17 In this process, voxels inside the editing region are updated according to the evolving target latent ztgt , while voxels outside remain anchored to the source latent zsrc ."
        },
        {
            "title": "D VISUALIZATION OF COMPARISON WITH SOTA METHODS",
            "content": "Figure 7: More qualitative results compared with SoTA methods. 18 VISUALIZATION ON CHARACTERANIMATION TEST SET Figure 8: Qualitative results of our 3DEditVerse on characteranimation test set. Fig. 8 presents qualitative results of our 3DEditFormer on the CharacterAnimation Test Set. Given source 3D asset and target image depicting new pose, our method successfully generates target 3D assets that accurately capture the articulated geometry and maintain texture fidelity. The results demonstrate that 3DEditFormer is able to produce realistic, pose-driven edits while preserving consistency across views, highlighting its effectiveness for complex characteranimation scenarios."
        }
    ],
    "affiliations": [
        "East China University of Science and Technology",
        "Singapore Management University"
    ]
}