{
    "paper_title": "Self-Reflective Generation at Test Time",
    "authors": [
        "Jian Mu",
        "Qixin Zhang",
        "Zhiyong Wang",
        "Menglin Yang",
        "Shuang Qiu",
        "Chengwei Qin",
        "Zhongxiang Dai",
        "Yao Shu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) increasingly solve complex reasoning tasks via long chain-of-thought, but their forward-only autoregressive generation process is fragile; early token errors can cascade, which creates a clear need for self-reflection mechanisms. However, existing self-reflection either performs revisions over full drafts or learns self-correction via expensive training, both fundamentally reactive and inefficient. To address this, we propose Self-Reflective Generation at Test Time (SRGen), a lightweight test-time framework that reflects before generating at uncertain points. During token generation, SRGen utilizes dynamic entropy thresholding to identify high-uncertainty tokens. For each identified token, it trains a specific corrective vector, which fully exploits the already generated context for a self-reflective generation to correct the token probability distribution. By retrospectively analyzing the partial output, this self-reflection enables more trustworthy decisions, thereby significantly reducing the probability of errors at highly uncertain points. Evaluated on challenging mathematical reasoning benchmarks and a diverse set of LLMs, SRGen can consistently strengthen model reasoning: improvements in single-pass quality also translate into stronger self-consistency voting. Especially, on AIME2024 with DeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on Pass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a plug-and-play method that integrates reflection into the generation process for reliable LLM reasoning, achieving consistent gains with bounded overhead and broad composability with other training-time (e.g., RLHF) and test-time (e.g., SLOT) techniques."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 9 1 9 2 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "SELF-REFLECTIVE GENERATION AT TEST TIME Jian Mu1, Qixin Zhang2, Zhiyong Wang3, Menglin Yang1, Shuang Qiu4, Chengwei Qin1, Zhongxiang Dai5, Yao Shu1 1Hong Kong University of Science and Technology (Guangzhou), 2Nanyang Technological University, 3University of Edinburgh, 4City University of Hong Kong, 5The Chinese University of Hong Kong, Shenzhen jianmu@hkust-gz.edu.cn, yaoshu@hkust-gz.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) increasingly solve complex reasoning tasks via long chain-of-thought, but their forward-only autoregressive generation process is fragile; early token errors can cascade, which creates clear need for selfreflection mechanisms. However, existing self-reflection either performs revisions over full drafts or learns self-correction via expensive training, both fundamentally reactive and inefficient. To address this, we propose Self-Reflective Generation at Test Time (SRGen), lightweight test-time framework that reflects before generating at uncertain points. During token generation, SRGen utilizes dynamic entropy thresholding to identify high-uncertainty tokens. For each identified token, it trains specific corrective vector, which fully exploits the already generated context for self-reflective generation to correct the token probability distribution. By retrospectively analyzing the partial output, this self-reflection enables more trustworthy decisions, thereby significantly reducing the probability of errors at highly uncertain points. Evaluated on challenging mathematical reasoning benchmarks and diverse set of LLMs, SRGen can consistently strengthen model reasoning: improvements in single-pass quality also translate into stronger self-consistency voting. Especially, on AIME2024 with DeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on Pass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as plug-and-play method that integrates reflection into the generation process for reliable LLM reasoning, achieving consistent gains with bounded overhead and broad composability with other training-time (e.g., RLHF) and test-time (e.g., SLOT) techniques. The code is available at https://github.com/2020-qqtcg/SRGen."
        },
        {
            "title": "INTRODUCTION",
            "content": "The ability to execute complex multi-step reasoning remains central frontier in advancing large language models (LLMs). LLMs generate step-by-step reasoning traces, often called chain-of-thought (CoT) (Wei et al., 2022). This capability has enabled substantial progress in mathematics, program synthesis, and other domains (Yao et al., 2023; Plaat et al., 2024). The fidelity of these traces often determines whether the final answer is correct (Paul et al., 2024; Hammoud et al., 2025). Thus, improving the reliability of the reasoning process is critical to realizing the full potential of LLMs. fundamental tension persists between the fluid, self-corrective character of human problem solving and the rigid, forward-only dynamics of standard LLM decoding. Humans iterate: they pause, re-evaluate premises, and change course. In contrast, LLMs perform autoregressive decoding (Vaswani et al., 2017): each token depends on all preceding tokens, and prior outputs cannot be revised. As result, early errors can propagate and compound, derailing the entire trajectory (Jain et al., 2025). This brittleness in forward-only decoding is major obstacle to reliable reasoning. Many prior works tackles this fragility via error correction. One line pursues post hoc iterative refinement: the model critiques and revises complete draft in subsequent passes (Madaan et al., 2023; Yuksekgonul et al., 2024), incurring substantial latency and computational cost. Another line Corresponding author"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Conceptual comparison of self-reflection paradigms. Feature Paradigm SRGen (Ours) Post-hoc Refinement RL Self-Correction Intervention timing Operating mode Training cost Inference latency Composability During generation Proactive Zero Low (bounded) High (plug-and-play) Post-generation Reactive Zero High (multiplicative) Medium During training Reactive High Near-zero Low (model-dependent) trains models for intrinsic self-correction, for example via reinforcement learning (Bensal et al., 2025; Ma et al., 2025). This enables mid-reasoning fixes but still requires that an erroneous segment be produced before intervention. Crucially, both approaches are reactive; they address errors only after they have occurred. The challenge of proactive error prevention, namely steering the model away from mistake before it is committed, remains research gap. To this end, we introduce Self-Reflective Generation at Test Time (SRGen), lightweight inferencetime framework for proactive error prevention. The key premise is that tokens differ in informativeness: recent work identifies critical tokens via high predictive entropy (Wang et al., 2025), low confidence (Fu et al., 2025), or spikes in mutual information (Qian et al., 2025). Instead of using these signals solely to adjust sampling or apply post hoc filtering, SRGen intervenes at the moment of risk: during decoding it detects critical points, briefly pauses, and optimizes small corrective vector δ with token-level reflection loss; this vector is injected into the hidden state before emitting the next token. The intervention is local and transient, steering the model away from early errors without additional full passes. Table 1 positions SRGen relative to existing self-reflection approaches and highlights new paradigm of proactive, test-time self-reflection. SRGen requires no additional training (unlike RLbased methods), avoids the latency of post hoc iterative refinement, and prevents errors before they compound. Its plug-and-play nature makes it broadly applicable to pre-trained language models and compatible with other reasoning-enhancement techniques, including SFT, RL, and distillation trained models and with test-time methods using similar mechanisms (e.g., SLOT (Hu et al., 2025)). We evaluate SRGen on challenging mathematical reasoning benchmarks (AIME2024/2025, HMMT 2025, AMC) across diverse model families. SRGen delivers consistent gains in most settings. It raises single-pass accuracy and, by improving the quality of individual reasoning paths, increases the effectiveness and sample efficiency of self-consistency. On AIME2024, for example, DeepSeek-R1-Distill-Qwen-7B improves Avg@5 by 12% and Cons@5 by 13.3%. For Qwen3-32B, Avg@5 increases by 6% and Cons@5 by 10%, rising from 80% to 90%. SRGen also composes favorably with peer test-time methods such as SLOT (Hu et al., 2025), yielding additional gains and outperforming either component in isolation."
        },
        {
            "title": "2 PROBLEM SETUP",
            "content": "In autoregressive language models, the generation of token sequence = (y1, y2, . . . , yT ) is modeled by the product of conditional probabilities: (yx0) = (cid:89) t=1 (yty<t, x0; θ), (1) where x0 is the input prompt and θ denotes the model parameters. Such forward-only decoding process, unfortunately, exhibits key vulnerability: fragility. An early error in reasoning chain can propagate and amplify, leading to catastrophic failures in the final output (Jain et al., 2025). Existing solutions predominantly fall into two categories. (1) Post-hoc Iterative Refinement. Methods such as Self-Refine (Madaan et al., 2023) employ multi-stage pipeline where the model critiques and revises complete draft (Shinn et al., 2023; Yu et al., 2024; Yuksekgonul et al., 2024). Although often effective, this approach incurs substantial computational overhead and latency. (2) Training for Intrinsic Self-Correction. This line of work embeds correction capabilities directly"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: An overview of the Self-Reflective Generation (SRGen) framework. This framework consists of two main stages. (1) Uncertainty Monitoring. threshold is dynamically computed from the mean and standard deviation of token entropies within recent history window of size N. (2) Self-Reflective Optimization. If the current tokens entropy exceeds the threshold, correction vector, δ, is optimized on-the-fly using joint loss of cross-entropy and entropy minimization. This δ is then added to the tokens hidden state to steer the final decision towards more reliable outcome. into the model parameters, typically via techniques like reinforcement learning (Bai et al., 2022; Hu et al., 2024; Kumar et al., 2024; Moskvoretskii et al., 2025; Bensal et al., 2025). These methods require expensive, resource-intensive training and can only intervene after an error has already been generated. common thread unites these approaches: their reactive nature, as they correct errors only after they have occurred. This limitation motivates our central research question: Can we design proactive error prevention mechanism that identifies and intervenes at potential error points in real-time during generation, thereby enhancing reasoning reliability within single decoding pass and at minimal additional cost?"
        },
        {
            "title": "3 SELF-REFLECTION GENERATION PROCESS",
            "content": "3.1 OVERVIEW OF SRGEN To mitigate error propagation during autoregressive generation, we introduce novel Self-Reflective Generation at Test Time (SRGen) framework. SRGen embeds lightweight monitor-reflect-optimize loop into the autoregressive decoding process. This loop enables the model to identify and correct potential errors at intermediate steps, thereby mitigating their propagation throughout the generated sequence. As illustrated in Figure 1, the process at each generation step consists of two stages: Stage 1: Dynamic Uncertainty Monitoring. At each step, the framework assesses the model predictive uncertainty for the next token. We quantify this uncertainty using token entropy. An intervention is triggered if this entropy exceeds dynamic threshold that adapts to the local context of the generated sequence. Stage 2: Self-Reflective Optimization. If the uncertainty exceeds the threshold, the standard decoding process is paused to initiate self-reflective optimization. This optimization computes transient correction vector δ by minimizing self-reflection loss function. The correction vector δ"
        },
        {
            "title": "Preprint",
            "content": "is then applied to the current hidden state to refine the next-token probability distribution, guiding it toward more confident and contextually coherent output. If the threshold is not met, the model proceeds with standard decoding. The complete workflow is formalized in Algorithm 1. By selectively applying this real-time intervention, SRGen enhances output reliability with minimal and bounded computational overhead."
        },
        {
            "title": "3.2 STAGE 1: DYNAMIC UNCERTAINTY MONITORING",
            "content": "A primary challenge in identifying critical reasoning junctures is that fixed uncertainty threshold is suboptimal. Models with different architectures, training paradigms, or scales have distinct entropy profiles, even on the same task, as shown in Appendix F. static threshold would therefore fail to reliably detect moments of high uncertainty across diverse contexts. To address this issue, we propose dynamic thresholding strategy that adapts to the recent generation history of model. Formally, at each decoding step t, before sampling the next token yt, we first compute the predictive entropy of the next-token distribution given the prefix y<t = (y0, . . . , yt1): Ht = H(p(y<t)). (2) We maintain sliding window Ht containing the most recent entropy values, {HtN , . . . , Ht1}. From this history, we compute the running mean µ(Ht) and standard deviation σ(Ht). The selfreflection process is triggered if the current entropy Ht represents statistically significant deviation from the recent trend. Formally, reflection is activated if: Ht > µ(Ht) + σ(Ht), (3) where is sensitivity hyperparameter. This adaptive approach enables our method to distinguish between naturally high-entropy passages and anomalous uncertainty spikes that warrant intervention. We present in the Appendix the tokens identified by this method. 3.3 STAGE 2: SELF-REFLECTIVE OPTIMIZATION Once high-uncertainty juncture is identified at step t, our goal is to compute transient correction to steer the generative process of model. This correction must balance two competing objectives: sharpening the predictive distribution to reduce uncertainty while preserving the semantic coherence established by the preceding context y<t. Blindly minimizing entropy can cause the distribution to collapse onto high-frequency but contextually inappropriate tokens. We therefore require principled objective to navigate this trade-off. Inspired by the work of (Hu et al., 2025), we introduce transient correction vector δ Rd, where is the dimension of the model hidden state. This vector is initialized to zero and optimized only when the uncertainty monitor is triggered. The correction is applied to the final hidden state ht1 before the vocabulary projection head W, yielding modified logits vector: logits = W(ht1 + δ). The optimization of δ is guided by hybrid loss function LSRGen defined over the prefix y<t: LSRGen(δ; λ, y<t) = (1 λ)LCE(y<t; δ) + λLAEM(y<t; δ). This loss comprises two components: (4) (5) Retrospective Context Loss (LCE): This term ensures contextual fidelity by penalizing corrections δ that disrupt the model predictions for the already-generated prefix. It is the negative log-likelihood of the prefix, where the same correction δ is applied to all historical hidden states: LCE(y<t; δ) = t2 (cid:88) i=0 log p(yi+1yi, δ), (6) where p(yi+1yi, δ) = softmax(W(hi + δ))yi+1 and hi is the hidden state corresponding to the prefix yi."
        },
        {
            "title": "Preprint",
            "content": "Anticipatory Entropy Minimization (LAEM): This term directly targets the high uncertainty at the current step t. By minimizing the entropy of the next-token predictive distribution, it encourages the model to make more confident prediction: LAEM(y<t; δ) = H(p(y<t, δ)), (7) where the perturbed distribution is p(y<t, δ) = softmax(W(ht1 + δ)). After optimizing δ for few gradient steps to find δ, we use this correction to generate the token yt. The vector is then discarded, ensuring that each intervention is localized to its specific context."
        },
        {
            "title": "4 METHOD ANALYSIS AND INSIGHTS",
            "content": "This section provides deeper analysis of the SRGen framework, justifying its core design principles and theoretical underpinnings."
        },
        {
            "title": "4.1 RATIONALE FOR DYNAMIC AND SELECTIVE INTERVENTION",
            "content": "The design of our uncertainty monitor is based on the principle of targeted intervention, which is crucial for two reasons: (1) Efficiency: Limiting the intervention to few key tokens significantly reduces computational overhead. Applying reflection at every step would be computationally prohibitive, whereas our targeted approach adds only modest, bounded overhead by focusing resources on the most critical junctures. (2) Quality: Excessive self-reflection can be counterproductive. Unnecessary intervention on low-uncertainty tokens may disrupt the fluency of the generated text. Our selective strategy, guided by the dynamic threshold, ensures that intervention is not only efficient but also beneficial to the final output quality. 4.2 THEORETICAL BASIS OF THE HYBRID LOSS Our central theoretical result establishes that the hybrid loss in SRGen is not heuristic but emerges directly from principled optimization problem. We formalize this by showing that our loss function is equivalent to the Lagrangian of constrained objective that seeks to minimize uncertainty while preserving contextual fidelity. This is stated formally in Theorem 1 and detailed proof is provided in Appendix B.1. Theorem 1 (Hybrid Loss as Principled Constrained Optimization). Given trade-off parameter λ (0, 1), the minimizer δ of the hybrid loss objective LSRGen(δ; λ) = (1 λ) LCE(δ) + λ LAEM(δ), is also the solution to the constrained optimization problem min δ LAEM(δ) s.t. LCE(δ) ϵ. (8) (9) The choice of λ implicitly defines the constraint boundary ϵ = LCE(δ), establishing formal equivalence between tuning the loss weight and setting fidelity tolerance. Remark. This theorem provides strong theoretical grounding for our method. The most powerful insight is that the SRGen objective is not an arbitrary blend of losses but principled, tractable solution to well-defined constrained optimization problem. This reframes the intuitive goal of cautious generation, i.e., reducing future uncertainty (LAEM) without sacrificing fidelity to the current context (LCE ϵ), in the rigorous language of optimization theory. The parameter λ is thus revealed to be more than simple weighting factor; it implicitly controls the price of violating the contextual fidelity constraint. small λ (corresponding to large Lagrange multiplier α) enforces strict fidelity requirement, heavily penalizing deviations. Conversely, large λ prioritizes uncertainty reduction, effectively relaxing the constraint. This perspective provides formal basis for tuning λ. Furthermore, this result justifies our use of simple weighted sum for the loss function. It demonstrates that this common practical approach is, in this case, equivalent to the more complex but formally correct Lagrangian relaxation, making the objective both theoretically sound and easily optimizable via standard gradient-based methods."
        },
        {
            "title": "4.3 COMPUTATIONAL OVERHEAD ANALYSIS",
            "content": "The computational overhead of SRGen comprises two components: negligible monitoring stage and more substantial, on-demand optimization stage. The monitoring stage, which computes predictive entropy at each token, incurs minimal cost because its inputs (logits and softmax distributions) are already computed during standard autoregressive generation. Consequently, the primary overhead stems from the on-the-fly optimization of the correction vector δ, which is triggered only at sparse, high-uncertainty junctures. This cost can be approximated as: Overhead Nact Cbp, where Nact is the number of reflection activations, is the number of inner optimization steps, and Cbp is the cost of single backpropagation pass. This design is inherently efficient. Unlike post-hoc refinement methods, whose costs scale linearly with the full sequence length, the overhead of our SRGen scales only with the number of critical interventions. Our experiments in Section 5.3 showing that the total overhead remains bounded, stabilizing at approximately 50%. This makes SRGen practical solution for enhancing model reasoning without prohibitive computational expense. (10)"
        },
        {
            "title": "5 EXPERIMENT",
            "content": "5.1 EXPERIMENTAL SETUP Models. We assess generality on open-weight models spanning scales, architectures, and posttraining regimes: Qwen2.5-Math-7B (Team, 2024), DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Llama-8B (Guo et al., 2025), and Qwen3-32B (Yang et al., 2025). This set covers two architecture families (Qwen, Llama), sizes from 7B to 32B, and heterogeneous post-training pipelines (distillation, SFT, RL) that yield distinct entropy profiles. This diversity probes whether SRGen remains effective across decoding behaviors, tokenizers, and training regimes, rather than overfitting to any single family or size. Benchmarks. We evaluate on AIME2024, AIME2025 (Art of Problem Solving, 2024), HMMT2025 (Balunovic et al., 2025), and AMC (LI et al., 2024). These benchmarks contain highdifficulty mathematical reasoning problems that often require long chains of thought. As the reasoning path lengthens, decision points proliferate and the likelihood of error increases. The tasks also demand exact answers, so small early slips tend to propagate and can overturn the final result. Experimental Settings. For inference, we cap the maximum generation length at 4,096 tokens for Qwen2.5-Math-7B and 32,768 for all other models. Decoding uses temperature 0.6 with nucleus sampling at top-p = 0.95, following common recommendations for reasoning models. We also report Qwen2.5-Math-7B performance at temperature 0 in subsequent analyses. All experiments were conducted on NVIDIA A800-80G GPUs. Unless otherwise noted, our method uses the following hyperparameters: training epochs = 3, learning rate η = 0.01, entropy-detection window = 25, and standard-deviation multiplier = 4. We discuss our baseline scope in Appendix D. Metrics. We report three primary metrics: (1) Avg@k: the average of Pass@1 over independent decodes; this reduces variance due to sampling. (2) Cons@k (Consistency@k): accuracy after self-consistency voting over the final answers; this measures whether multiple high-quality reasoning paths converge to the same result. (3) Pass@k: the probability that at least one of attempts is correct; this reflects the exploration breadth of the model. 5.2 MAIN RESULTS Table 2 summarizes the effects of SRGen across four long chain-of-thought math benchmarks and multiple post-training regimes. We organize the results by metric to make the connection between the design of SRGen and the observed gains clear. Avg@k. Avg@k is the mean Pass@1 over independent decodes, so increases directly reflect stronger reasoning. We set = 5 to balance effectiveness and computation. After applying SRGen, nearly all models improve on challenging mathematical tasks. On AIME2024, for example, Qwen2.5-Math-7B rises by 7.4%, DeepSeek-R1-Distill-Qwen-7B by 12%, and Qwen3-32B from 76.7% to 82.7%. Other benchmarks show similar gains. These results indicate"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Performance of SRGen on the benchmark, reporting Avg@5, Cons@5, and Pass@5. Inline marks show absolute change vs. Base. Model Benchmark Avg@5 Cons@5 Pass@ Base w/ SRGen Base w/ SRGen Base w/ SRGen Qwen2.5-Math-7B Distill-Qwen-7B Distill-Llama-8B Qwen3-32B AIME2024 AIME2025 HMMT2025 AMC AIME2024 AIME2025 HMMT2025 AMC AIME2024 AIME2025 HMMT2025 AMC AIME2024 AIME2025 HMMT2025 AMC 14.6 6.0 1.3 34. 49.3 35.3 15.3 51.0 48.0 30.7 14.0 50.0 76.7 70.7 23.3 54.0 22.0 ( 7.4) 9.3 ( 3.3) 3.3 ( 2.0) 41.2 ( 7.2) 61.3 ( 12.0) 42.7 ( 7.4) 18.0 ( 2.7) 51.2 ( 0.2) 52.7 ( 4.7) 32.7 ( 2) 16.0 ( 2.0) 50.6 ( 0.6) 82.7 ( 6.0) 76.0 ( 5.3) 28.0 ( 4.7) 56.8 ( 2.8) 6.7 6.7 0.0 34.0 50.0 33.0 16.7 51.0 46.7 26.7 10.0 53.0 80.0 73.0 26.7 54.0 23.3 ( 16.6) 6.7 ( 0.0) 0.0 ( 0.0) 41.0 ( 7.0) 63.3 ( 13.3) 46.7 ( 13.7) 16.7 ( 0.0) 51.0 ( 0.0) 63.3 ( 16.6) 33.3 ( 6.6) 13.0 ( 3.0) 53.0 ( 0.0) 90.0 ( 10.0) 76.7 ( 3.7) 26.7 ( 0.0) 57.0 ( 3.0) 40.0 13.0 6.0 49.0 73.0 53.0 23.3 64.0 70.0 50.0 20.0 57. 90.0 86.7 33.0 60.0 40.0 ( 0.0) 26.7 ( 13.7) 13.3 ( 7.3) 52.0 ( 3.0) 80.0 ( 7.0) 60.0 ( 7.0) 33.0 ( 9.7) 64.0 ( 0.0) 76.7 ( 6.7) 50.0 ( 0.0) 33.0 ( 13.0) 57.0 ( 0.0) 93.0 ( 3.0) 86.7 ( 0.0) 43.3 ( 10.3) 61.0 ( 1.0) that SRGen, via self-reflective generation, lowers error probability at critical points and strengthens reasoning across model sizes, training paradigms, and architecture families. Cons@k. As single-pass quality improves, self-consistency voting over higher-quality candidates attains higher accuracy even for small k. We report = 5 to balance accuracy and cost, and later analyze how Cons@k varies with for single model. On AIME2024, Qwen2.5-Math-7B improves by 16.6%, DeepSeek-R1-Qwen-7B by 13.3%, and Qwen3-32B by 10% to reach 90%; only DeepSeek-R1-Llama-7B shows decrease. Other settings also exhibit strong gains. These results suggest that adding SRGen to single pass, with modest extra cost, can reduce the number of candidates needed for self-consistency and achieve strong accuracy at small k, thereby lowering overall inference cost. Pass@k. One concern is that improving single-pass accuracy might narrow exploration and harm Pass@k as grows. In our experiments, Pass@k is comparable to the base model in some cases and higher in most. This pattern suggests that SRGen primarily converts low-quality, incorrect traces into high-quality, correct ones, without degrading the accuracy of existing correct traces or the ability of the model to explore correct solutions. 5.3 EFFICIENCY ANALYSIS We evaluate inference latency after integrating SRGen to confirm that the added components incur only minimal overhead. Experiments use Qwen2.5-Math-7B. To estimate average latency, we run the full AIME2024 benchmark and compute the mean per-task runtime. To eliminate the effect of randomness, all timing uses greedy decoding. Detailed results appear in Figure 2. Iterations denotes the number of optimization steps taken per SRGen activation; Time is the average runtime per task; and Activations is the number of times SRGen is triggered within task. An Iterations value of 0 corresponds to the baseline without SRGen. With SRGen triggered about six times per task on average, the additional runFigure 2: Activations and Time Increase."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Cons@k and Pass@k accuracy of Qwen2.5-Math-7B on the AMC benchmark. time plateaus at roughly 50% as the number of iterations increases, indicating that SRGen does not introduce multiplicative or larger latency. 5.4 CONS@K AND PASS@K Using Qwen2.5-Math-7B on the AMC benchmark, we further examine how Cons@k and Pass@k change with increasing after applying SRGen. Detailed results are shown in Figure 3. When SRGen-enhanced samples are used for self-consistency voting, Cons@k remains consistently higher than the base model as grows, whereas Pass@k gradually converges to the base model. These trends indicate that self-reflective generation in SRGen reduces the probability of reasoning errors and improves single-pass accuracy, thereby producing higher-quality candidates for self-consistency and boosting Cons@k. Importantly, the reflection mechanism in SRGen does not diminish the exploratory capacity of the model: it primarily reduces mistakes by reweighting uncertain tokens, correcting erroneous traces while leaving correct ones intact. Taken together, these results suggest that SRGen is promising plug-in for achieving higher single-pass accuracy or more sample-efficient self-consistency. 5.5 ORTHOGONALITY: INTEGRATION WITH RELATED METHODS Building on our initial results showing that SRGen can pair with diverse training paradigms and further improve models post-training, we next provide stronger evidence of its orthogonality and potential synergy by combining it with method from the same family. Specifically, we adopt SLOT, representative test-time optimization approach. During inference, SLOT optimizes sample-specific vector δSLOT over the prompt-processing stage and injects it into the hidden states of the model to steer generation. All experiments are conducted on the Qwen2.5-Math-7B model, and we use greedy decoding throughout to ensure reproducibility. Figure 4: Performance of SLOT, SRGen, and their combination for Qwen2.5-Math-7B across the AMC, MATH500, and AIME2024 benchmarks. As shown in Figure 4, combining SLOT with SRGen further strengthens the reasoning ability of the model, with particularly pronounced gains on mathematical tasks. On MATH500, the joint approach"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Ablation analysis of the balancing parameter λ, window size , and standard-deviation multiplier k. lifts Qwen2.5-Math-7B from 63.8% to 70.6%, outperforming either SLOT or SRGen used in isolation. These results provide additional evidence that SRGen is an orthogonal test-time method that consistently enhances reasoning performance and exhibits clear synergy with related techniques. 5.6 HYPERPARAMETER ABLATION STUDY We perform an ablation study to examine the key hyperparameters of SRGen. Specifically, we vary iterations (the number of gradient steps used to optimize δ), learning rate (the step size for updating δ), the balancing coefficient λ, and the two parameters of our dynamic-entropy monitoring module: history window size and standard-deviation multiplier k. To clearly reveal performance trends while keeping computation manageable, we evaluate on the first 100 instances of the MATH500 benchmark. All experiments use the Qwen2.5-Math-7B with greedy decoding. Table 3: Effects of iterations and learning rate. Iterations = 0 corresponds to the baseline (original model). The balancing hyperparameter λ is fixed to 0.05. Learning rate 0.01 0.05 0.10 0 64.0 64.0 64.0 1 71.0 72.0 71.0 Iterations 3 70.0 71.0 71.0 5 72.0 71.0 71.0 7 70.0 71.0 71. 9 71.0 71.0 71.0 As shown in Table 3, varying the number of optimization iterations and the learning rate within reasonable ranges yields only minor changes in performance. Across all hyperparameter settings in our ablation, accuracy remains stable at roughly 71% ( 1%), indicating that SRGen is relatively insensitive to these choices. Figure 5 shows even at the extremes (λ = 0, disabling entropy minimization; λ = 1, disabling cross-entropy), SRGen yields substantial gains over the baseline, reinforcing our claim that targeted intervention on critical tokens is effective. The strongest performance arises when both losses are used, indicating synergy and motivating careful calibration of λ. In practice, small λ values that place greater weight on the cross-entropy term work best; for example, λ = 0.05 performs consistently well across our tests. Varying and k, we observe that small fails to capture the recent entropy trend, whereas an overly large becomes unrepresentative by incorporating too many outliers. Choosing [25, 40] better tracks short-horizon entropy dynamics. For k, smaller values flag more tokens as uncertain, leading to abnormally high trigger counts and reduced efficiency and reflecting on too many tokens yields little additional gain while risking disruption of correct reasoning by perturbing high-confidence tokens. Conversely, very large misses many critical tokens. Values around [2.5, 4] strike balance: they identify critical tokens broadly while keeping triggers modest, yielding larger improvements."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce Self-Reflective Generation at Test Time (SRGen), lightweight, plug-and-play framework that performs token-level self-reflection only at critical tokens detected by dynamic uncertainty monitoring. When triggered, brief self-reflective optimization learns an on-the-fly correction vector δ and injects it into the hidden state under hybrid loss LSRGen to reduce predictive uncertainty while preserving contextual fidelity. Across challenging mathematical benchmarks, SRGen improves accuracy with about 50% additional inference time and yields more effective selfconsistency voting without harming exploration. SRGen is broadly practical: it strengthens reasoning across model families and training paradigms and composes with other test-time methods, making it promising inference-stage plug-in for reliable LLM reasoning."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work complies with the ICLR Code of Ethics and the authors institutional policies. SRGen is training-free, test-time method evaluated only on public mathematical reasoning benchmarks and open-weight models; no human subjects or animal experiments were involved. All datasets were used strictly under their licenses and contain no personally identifiable information; we did not attempt re-identification. We took care to mitigate risks such as overconfidence and bias amplification by restricting evaluation to factual math tasks and recommending safety filters and uncertainty calibration for any open-domain use. To support transparency and reproducibility, we detail models, hyperparameters, and evaluation protocols in the paper and will release anonymized code and configurations after the review process. The authors declare no competing interests; any largelanguage-model assistance was limited to language polishing and did not affect the research design, experiments, or conclusions."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We have made every effort to ensure that the results in this paper are reproducible. An anonymized companion artifact accompanying the submission contains code, configuration files, evaluation scripts, fixed random seeds, and dataset-access instructions; the repository will be open-sourced after review. The experimental setup (models, hyperparameters, and hardware) is documented in the Experiment section with results in Table 2; efficiency measurements are given in Section 5.3 (Figure 2); ablations over iterations, learning rate, λ, , and are provided in Section 5.6 with Table 3 and Figure 5. We also give complete description of our contributionSRGenincluding the method overview (Figure 1), pseudocode (Algorithm 1), and theoretical guarantees (Theorem 1, Appendix B.1) to assist re-implementation. Additionally, all benchmarks used in this paper are publicly available mathematical reasoning datasets. Our artifact provides scripts and instructions to obtain them. Baseline scope and prompts are summarized in Appendices and E. We also provide our implementation in the supplementary materials and will open-source the code in the future. We believe these measures will enable other researchers to reproduce our results and further advance the field."
        },
        {
            "title": "REFERENCES",
            "content": "Art of Problem Solving. Aime problems and solutions. https://artofproblemsolving. Accessed: com/wiki/index.php/AIME_Problems_and_Solutions, 2025-09-14. Jun 2024. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Mislav Balunovic, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovic, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions, February 2025. URL https: //matharena.ai/."
        },
        {
            "title": "Preprint",
            "content": "Shelly Bensal, Umar Jamil, Christopher Bryant, Melisa Russak, Kiran Kamble, Dmytro Mozolevskyi, Muayad Ali, and Waseem AlShikh. Reflect, retry, reward: Self-improving llms via reinforcement learning. arXiv preprint arXiv:2505.24726, 2025. Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, and Yunhe Wang. Forest-of-thought: Scaling test-time compute for enhancing llm reasoning. arXiv preprint arXiv:2412.09078, 2024. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: In The Twelfth Decoding by contrasting layers improves factuality in large language models. International Conference on Learning Representations, 2023. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hasan Abed Al Kader Hammoud, Hani Itani, and Bernard Ghanem. Beyond the last answer: Your reasoning trace uncovers more than you think. arXiv preprint arXiv:2504.20708, 2025. Chi Hu, Yimin Hu, Hang Cao, Tong Xiao, and Jingbo Zhu. Teaching language models to selfimprove by learning from language feedback. arXiv preprint arXiv:2406.07168, 2024. Yang Hu, Xingyu Zhang, Xueji Fang, Zhiyang Chen, Xiao Wang, Huatian Zhang, and GuoarXiv preprint Slot: Sample-specific language model optimization at test-time. jun Qi. arXiv:2505.12392, 2025. Kushal Jain, Moritz Miller, Niket Tandon, and Kumar Shridhar. First-step advantage: Importance of starting right in multi-step math reasoning. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 766778, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.42. URL https://aclanthology.org/2025.findings-acl.42/. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface. co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/ aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. Yafu Li, Xuyang Hu, Xiaoye Qu, Linjie Li, and Yu Cheng. Test-time preference optimization: On-the-fly alignment via iterative textual feedback. arXiv preprint arXiv:2501.12895, 2025. Zicheng Lin, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, and Zhaopeng Tu. Critical tokens matter: Token-level contrastive estimation enhances llms reasoning capability. In Forty-second International Conference on Machine Learning, 2025. Ruotian Ma, Peisong Wang, Cheng Liu, Xingyan Liu, Jiaqi Chen, Bang Zhang, Xin Zhou, Nan Du, and Jia Li. 2 r: Teaching llms to self-verify and self-correct via reinforcement learning. arXiv preprint arXiv:2502.12853, 2025. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. Viktor Moskvoretskii, Chris Biemann, and Irina Nikishina. Self-taught self-correction for small language models. arXiv preprint arXiv:2503.08681, 2025."
        },
        {
            "title": "Preprint",
            "content": "Debjit Paul, Robert West, Antoine Bosselut, and Boi Faltings. Making reasoning matter: Measuring and improving faithfulness of chain-of-thought reasoning. arXiv preprint arXiv:2402.13950, 2024. Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas Back. Reasoning with large language models, survey. CoRR, 2024. Chen Qian, Dongrui Liu, Haochen Wen, Zhen Bai, Yong Liu, and Jing Shao. Demystifying reasoning dynamics with mutual information: Thinking tokens are information peaks in llm reasoning. arXiv preprint arXiv:2506.02867, 2025. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya Grover, Kai-Wei Chang, Marcus Rohrbach, and Anna Rohrbach. When to solve, when to verify: Compute-optimal problem solving and generative verification for llm reasoning. arXiv preprint arXiv:2504.01005, 2025. Amir Taubenfeld, Tom Sheffer, Eran Ofek, Amir Feder, Ariel Goldstein, Zorik Gekhman, and Gal Yona. Confidence improves self-consistency in llms. arXiv preprint arXiv:2502.06233, 2025. Qwen Team. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, and Yuyu Luo. Atom of thoughts for markov llm test-time scaling. arXiv preprint arXiv:2502.12018, 2025. Jean Vassoyan, Nathanael Beau, and Roman Plaud. Ignore the kl penalty! boosting exploration on critical tokens to enhance rl fine-tuning. arXiv preprint arXiv:2502.06533, 2025. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Xiao Yu, Baolin Peng, Michel Galley, Jianfeng Gao, and Zhou Yu. Teaching language models to self-improve through interactive demonstrations. In NAACL-HLT, 2024. Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. Textgrad: Automatic differentiation via text. arXiv preprint arXiv:2406.07496, 2024. Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu Wen, Chenghua Lin, Wenhao Huang, et al. First return, entropy-eliciting explore. arXiv preprint arXiv:2507.07017, 2025."
        },
        {
            "title": "Preprint",
            "content": "Zhi Zhou, Tan Yuhao, Zenan Li, Yuan Yao, Lan-Zhe Guo, Xiaoxing Ma, and Yu-Feng Li. Bridging internal probability and self-consistency for effective and efficient llm reasoning. arXiv preprint arXiv:2502.00511, 2025. Yuqi Zhu, Ge Li, Xue Jiang, Jia Li, Hong Mei, Zhi Jin, and Yihong Dong. Uncertainty-guided chain-of-thought for code generation with llms. arXiv preprint arXiv:2503.15341, 2025."
        },
        {
            "title": "A RELATED WORK",
            "content": "Self-Reflection in LLMs. Self-reflection seeks to move LLMs from impulsive first-pass outputs to more deliberative and accurate responses. Existing approaches largely fall into two categories. (1) Post hoc iterative refinement. These methods use multi-stage inference pipelines: the model first drafts an answer, then critiques it, and finally revises it based on its own feedback (Shinn et al., 2023; Yu et al., 2024; Yuksekgonul et al., 2024; Li et al., 2025). Frameworks such as Self-Refine (Madaan et al., 2023) formalize the generate, critique, and refine loop. While effective, they incur substantial latency and computational overhead because they require multiple full forward passes. (2) Training for intrinsic self-correction. This line embeds self-correction directly in the parameters, typically via fine-tuning on corrective data or reinforcement learning (RL) (Bai et al., 2022; Hu et al., 2024; Kumar et al., 2024; Moskvoretskii et al., 2025). Such models can produce refined outputs in single pass but demand extensive and costly training. SRGen offers distinct alternative that operates at test time. It avoids the high overhead of iterative methods by requiring neither auxiliary LLM calls for feedback nor the generation of multiple complete outputs. Operating at the token level. SRGen is lightweight method and can be synergistically combined with both training-based and post-hoc self-reflection approaches. Identifying and Leveraging Critical Tokens. Recent work rests on the observation that tokens are not equally informative (Lin et al., 2025). Studies identify critical or pivotal tokens that mark decision points along reasoning path and leverage them in several ways. (1) Guiding training: policy gradients are applied selectively at high-entropy positions to focus learning (Wang et al., 2025; Vassoyan et al., 2025). (2) Triggering exploration: critical tokens act as branching points for sampling diverse reasoning paths (Zheng et al., 2025; Zhu et al., 2025) or for localized iterative refinement that probes the solution space more deeply (Qian et al., 2025). (3) Pruning search: lowconfidence tokens trigger the removal of less promising paths within self-consistency frameworks (Fu et al., 2025; Taubenfeld et al., 2025; Zhou et al., 2025). We introduce new paradigm for the use of critical tokens. SRGen employs them as real-time triggers for corrective intervention directly on the models hidden state. This allows for an on-the-fly steering of single generation path, fundamentally different and more direct mechanism than prior art. Test-Time Scaling. To improve performance without costly retraining, range of methods increase computation at test time. These approaches fall into two broad strategies. The first generates multiple reasoning paths and selects an outcome via voting or scoring. This includes producing multiple complete solutions, as in self-consistency (Wang et al., 2022; Singhi et al., 2025), or exploring tree or graph of intermediate steps, as in Tree-of-Thoughts and its variants (Yao et al., 2023; Bi et al., 2024; Teng et al., 2025). The second intervenes within single decoding process. Prompt-based techniques such as chain of thought (Wei et al., 2022) elicit more deliberative reasoning. More directly, methods adjust the models internal computations during single pass, e.g., DoLa (Chuang et al., 2023) contrasts layer logits to steer decoding and SLOT (Hu et al., 2025) injects samplespecific vector into the hidden states to steer generation globally and indirectly encourages longer reasoning by suppressing the EOS token. SRGen advances the second strategy with fine-grained, dynamic intervention. Whereas SLOT applies static, sample-level vector throughout decoding, SRGen computes token-level corrective vector δ on-the-fly at detected critical junctures. This targeted adjustment adapts to the immediate context and steers the reasoning process without branching or multiple full passes."
        },
        {
            "title": "B THEOREMS AND PROOFS",
            "content": "B.1 PROOF OF THM. 1 Statement. Fix trade-off parameter λ (0, 1). Let Fλ(δ) (1 λ) LCE(δ) + λ LAEM(δ), where LCE and LAEM are defined in the main text ( 3.3) on the current prefix y<t with the same correction vector δ injected as in Eq. 4. Let δ arg minδ Fλ(δ) be any minimizer. Then δ also solves the constrained problem min δ LAEM(δ) s.t. LCE(δ) ε, (11)"
        },
        {
            "title": "Preprint",
            "content": "with the implicitly induced tolerance ε LCE(δ). Proof. Let ε = LCE(δ). By construction, δ is feasible for (11) with the constraint held at equality. Suppose, for contradiction, that δ is not an optimizer of (11); then there exists feasible (cid:98)δ with LCE((cid:98)δ) ε and LAEM((cid:98)δ) < LAEM(δ). Consider the hybrid objective values: Fλ((cid:98)δ) = (1λ)LCE((cid:98)δ)+λLAEM((cid:98)δ) (1λ)ε+λLAEM((cid:98)δ) < (1λ)ε+λLAEM(δ) = Fλ(δ), which contradicts the optimality of δ for Fλ. Hence no feasible point can achieve strictly smaller LAEM under the tolerance LCE ε, and δ solves (11). Lagrangian view and the λα mapping. The constrained problem (11) has Lagrangian L(δ, α) = LAEM(δ) + α(cid:0)LCE(δ) ε(cid:1), α 0. For any fixed α 0, minimizing over δ is, up to an additive constant αε, equivalent to minimizing the weighted sum LAEM(δ) + αLCE(δ). Identifying weights gives the bijection λ = 1 1 + α , 1 λ = α 1 + α , α = 1 λ λ . Therefore, the hybrid loss Fλ is exactly rescaled Lagrangian with dual variable α = 1λ λ . Under standard regularity ensuring KKT optimality (e.g., existence of primal optimum and either convexity with Slaters condition or other sufficient conditions for strong duality), any primal-dual optimal pair (δ, α) of (11) also minimizes weighted sum, and the mapping above recovers λ from α. This provides the converse direction under mild assumptions: for given active tolerance ε, an appropriate choice of λ (equivalently, α) recovers the same optimizer δ. Pareto-optimality interpretation. Consider the bi-objective vector G(δ) (cid:0)LCE(δ), LAEM(δ)(cid:1). By the proof above, any minimizer δ of Fλ with λ (0, 1) is Pareto-optimal: if there existed (cid:98)δ with G((cid:98)δ) G(δ) and one component strictly smaller, it would violate the optimality of δ for Fλ. Hence the hybrid loss selects points on the Pareto front of the two desiderata contextual fidelity and uncertainty reduction. In particular, the induced tolerance ε = LCE(δ) characterizes the specific frontier point attained. On the λ ε trade-off. Intuitively, larger λ increases the relative price on LAEM and relaxes the pressure on LCE, thus tending to yield solutions with lower LAEM and (weakly) higher LCE (i.e., looser fidelity tolerance). Formally, consider any 0 < λ1 < λ2 < 1 with corresponding minimizers δ1, δ2. Optimality implies Fλ1(δ1) Fλ1 (δ2), Fλ2(δ2) Fλ2 (δ1). Writing Fλ = (1 λ)LCE + λLAEM and rearranging yields the weighted trade-off bounds: 1 λ1 λ1 (cid:2)LCE(δ1) LCE(δ2)(cid:3) LAEM(δ2) LAEM(δ1) 1 λ2 λ2 (cid:2)LCE(δ1) LCE(δ2)(cid:3). (12) Thus the relative weights (1 λ)/λ govern the paired improvements: as λ increases (placing more emphasis on LAEM), the achievable decrease in LAEM per unit increase in LCE becomes tighter. In strictly convex or uniqueness regimes this typically induces monotone path ε(λ) = LCE(δλ) that is nondecreasing in λ.1 Boundary cases and feasibility. When λ 1 (α 0), Fλ approaches LAEM, i.e., the unconstrained entropy-minimization objective. The induced tolerance becomes ε = LCE(δ) for an LAEM-minimizer δ, so the constraint is tight (active at equality) in this mapping. When λ 0 (α ), Fλ emphasizes LCE and the solution tends to minimize contextual distortion subject to making any progress on LAEM; operationally this corresponds to nearly hard fidelity constraint. Existence of minimizers. Both LCE and LAEM in our setting are nonnegative and continuous in δ (they are compositions of smooth maps: affine shift in logits, softmax, entropy, and prefix NLL). Thus Fλ is lower-bounded by 0 and continuous. If arg min Fλ fails to exist on Rd due to lack of coercivity, it suffices (and is standard at test time) to either: (i) restrict δ to compact trust region 1We avoid global convexity claims for LAEM; the sufficiency result above does not require convexity. In practice, unique local minimizer selected by deterministic inner optimizer makes the λ-path stable."
        },
        {
            "title": "Preprint",
            "content": "{δ R}, or (ii) add tiny quadratic regularizer γ 2 δ2 (this does not affect the equivalence to (11) because the same regularizer can be added to both the constrained and weighted formulations). Under either modification, minimizer exists and the above arguments apply verbatim. Joint-Descent Lemma. If the gradients form an acute angle, i.e., LCE(δ), LAEM(δ) > 0, then for any λ (0, 1) and sufficiently small η > 0, the update δ+ = δ η(cid:2)(1 λ)LCE(δ) + λLAEM(δ)(cid:3) strictly decreases both objectives to first order. Proof. Directional derivatives give dη LCE(δ+)(cid:12) (cid:12)η=0 = (cid:2)(1 λ)LCE2 + λLCE, LAEM(cid:3) < 0, dη LAEM(δ+)(cid:12) (cid:12)η=0 = (cid:2)λLAEM2 + (1 λ)LAEM, LCE(cid:3) < 0, where both inequalities use the acute-angle assumption. Takeaway for SRGen. The proof establishes that the SRGen loss is not an ad-hoc blend: it is precisely Lagrangian relaxation of the constrained goal reduce uncertainty while keeping contextual fidelity within tolerance. Therefore λ is an interpretable knob: it implicitly sets the fidelity tolerance ε = LCE(δ) and moves SRGen along the fidelityconfidence Pareto frontier. In practice (cf. Fig. 5 in the main text), small but nonzero λ often works well, reflecting regime where fidelity is enforced strongly while still reaping entropy reductions at high-uncertainty points. B.2 PRACTICAL GUIDANCE. The Lagrangian view above makes λ an interpretable knob that implicitly sets fidelity tolerance. In practice, we recommend: 1. Choosing λ. Start from small but nonzero values (e.g., λ [0.05, 0.20]) so that contextual fidelity remains strong while LAEM still improves at high-uncertainty positions. Increase λ only if LAEM plateaus while LCE stays well below the desired tolerance. 2. Optional adaptive schedule. If target tolerance ε is available (e.g., from validation), adjust λ online by simple proportional rule: (cid:16) λ clip λ exp (cid:0)ηλ [ LCE(δ)/ε 1 ](cid:1), λmin, λmax (cid:17) , so that λ increases when LCE is below the target (allowing stronger LAEM minimization) and decreases when it is above the target. 3. Inner optimization for δ. Use few (K) steps of first-order updates (e.g., Adam or clipped gradient descent) with backtracking line search. Early-stop the inner loop as soon as (i) Fλ stops decreasing, or (ii) the tolerance criterion LCE(δ) ε is met (if using the constrained view). To ensure existence/stability, either enforce trust region δ or add tiny quadratic penalty γ 2 δ2. In practice, pick (or γ) so that the induced logit shift remains moderate (e.g., within few units). 4. Numerical stability. Compute losses with log-sum-exp stabilization; clip gradient norms for both δ and the logits; avoid mixing training-time dropout into the inner loop; and cache reusable quantities along the prefix to reduce variance across inner iterations. 5. Diagnostics and stopping. Log the pairs (cid:0)LCE(δt), LAEM(δt)(cid:1) over timesteps and visualize the empirical Pareto curve. healthy run shows (on average) nonincreasing LAEM with modest, controlled increases in LCE. If LCE spikes, lower λ or shrink the trust region; if LAEM barely moves, raise λ slightly or increase the inner optimization budget. 6. Common failure modes & remedies. (i) Over-aggressive λ: fidelity drops abruptly; fix by reducing λ, tightening δ R, or increasing γ. (ii) Under-aggressive λ: negligible LAEM improvement; fix by raising λ or allowing few extra inner steps. (iii) Cycling/instability: use smaller step sizes, enable gradient clipping, and adopt line search or momentum dampening."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 SRGen: Self-Reflective Generation 1: Input: pre-trained model with head ; prompt x0 Hyperparameters: (sensitivity), (window size), λ (loss weight), (steps), η (lr), τ (temperature) δ 0 for = 1 to do ht1 (x0:t) ht1; Et Entropy(softmax(z/τ )) if = and Et > µ(E) + σ(E) then 2: Output: generated sequence 3: (), 1, empty ring buffer of size 4: while EOS not generated and < MAX LENGTH do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: end while 20: return LCE (cid:80)t2 LAEM (cid:80) (1 λ) LCE + λ LAEM δ δ η δL end if yt softmax(z/τ ); push Et into and keep the most recent end for (ht1 + δ) j=0 log p(xj+1 x0:j, δ) yt; vV p(v x0:t, δ) log p(v x0:t, δ) last hidden state for current context dynamic trigger inner optimization of δ modify only the last state at sampling x0:t+1 x0:t yt; +"
        },
        {
            "title": "C ALGORITHM OF SRGEN",
            "content": "Explanation Alg. 1. L12 Inputs/outputs and hyperparameters. controls trigger sensitivity; is the entropy history window; λ balances contextual fidelity vs. entropy minimization; T, η set the inner-loop budget; τ is the decoding temperature. L3 Initialize the sequence and ring buffer to maintain recent entropies for on-the-fly calibration. This enables model/temperature/positionagnostic triggering. L46 At each step, obtain the last hidden state ht1, project to logits z, and compute predictive entropy Et=H(softmax(z/τ )). L7 Dynamic trigger: activate reflection iff Et significantly exceeds the local baseline via Et > µ(E) + σ(E), where µ, σ are rolling stats over the last steps. L89 Enter short inner optimization while freezing M, and optimizing transient correction vector δ only when needed. L10 Retrospective context loss LCE preserves prefix fidelity by applying the same δ to historical states when computing teacher-forced likelihood of xj+1. L11 Anticipatory entropy minimization LAEM sharpens the current predictive distribution to reduce uncertainty at the flagged token. L12 Hybrid objective (1λ)LCE + λLAEM trades off stability and decisiveness; small λ avoids collapse while still decreasing entropy. L13 Update δ with few small steps (T typically 5), keeping overhead bounded. L15 Inject δ only at the current step for sampling; historical injection appears only inside the loss terms, so past tokens are not altered. L1720 Sample, append, update the context and entropy buffer, and continue until EOS or length limit."
        },
        {
            "title": "D BASELINE SCOPE",
            "content": "SRGen performs token-level updates during decoding; most post hoc self-reflection acts at the answer/trajectory level (generatecritiquerevise) after draft is produced. These interventions occur at different stages and granularities, so they do not interfere and can be layered; SRGen is meant to complement, not replace, outer-loop reflection. Our goal in the main results is to isolate what SRGen itself contributes: how much improvement can we obtain by adding only SRGen to given base model? Direct, head-to-head comparisons with post hoc reflection would introduce extra prompt design and pipeline choices (e.g., critique/rewrite templates, voting rules, number of drafts). Keeping these prompts strictly consistent across methods is difficult, and small template changes can dominate the outcome, shifting the focus from the"
        },
        {
            "title": "Preprint",
            "content": "method to prompt engineering. To avoid this confound, we do not include such comparisons in the main tables and instead measure the SRGen-only effect. Concretely, we compare each base model with and without SRGen under the same decoding setup (temperature, top-p, maximum tokens, and stopping criteria). The evaluation spans multiple architectures and training regimes (e.g., distillation, SFT, RL) to show that SRGen consistently improves reasoning across diverse settings. Although we do not treat post hoc reflection as competing baseline, SRGen is designed to be composable with outer-loop methods that operate during the reasoning process. In the main paper we show two kinds of evidence: (i) gains when adding SRGen to models trained under different paradigms, and (ii) gains when combining SRGen with test-time method such as SLOT, indicating that SRGen and reflection-style approaches address different layers of the reasoning stack and work well together (see Section 5.5)."
        },
        {
            "title": "E PROMPT USED",
            "content": "To facilitate reproducibility, we provide the system prompt used in our benchmark evaluations. AIME2024 and AIME2025 You are helpful assistant. Solve the following math problem efficiently and clearly. The last line of your response should be of the following format: Therefore, the final answer is: $boxed{{ANSWER}}$ hope it is correct (without quotes) where ANSWER is just the final number that solves the problem. Think step by step before answering. MATH500 and HMMT2025 and AMC Solve the following math problem efficiently and clearly. The last line of your response should be of the following format: Therefore, the final answer is: $boxed{{ANSWER}}$ hope it is correct (without quotes) where ANSWER is just the final number or expression that solves the problem. Think step by step before answering."
        },
        {
            "title": "F ENTROPY ANALYSIS",
            "content": "A fixed entropy threshold does not generalize across models, temperatures, or positions in the same sequence. Figures 6a (T =0) and 6b (T =0.6) show large differences in the scale and variance of token-level entropy across architectures and post-training regimes. For example, at =0 the finalstep entropy ranges from 2 104 for Qwen2.5-Math-7B to 0.66 for Qwen3-32B. At =0.6 the final-step entropy is 0.0002 (Qwen2.5-Math-7B), 0.3145 (DeepSeek-R1-Distill-Llama8B), 0.0918 (DeepSeek-R1-Distill-Qwen-7B), and 0.0011 (Qwen3-32B). Sequence lengths also vary widely (e.g., 11,174 steps for DeepSeek-R1-Distill-Llama-8B versus 780 for Qwen332B at =0.6), and within sequence the baseline entropy drifts while sharp local spikes persist. Under any fixed threshold τ , low-entropy models would rarely trigger (missed high-risk segments), whereas high-entropy models would trigger excessively (many false positives); temperature changes further skew the trigger rate. To handle these distribution shifts, SRGen uses dynamic threshold based on rolling estimate of the local entropy distribution. At step t, with predictive entropy Ht, we compute the mean µt and standard deviation σt over history window of length , and set τt = µt + σt, trigger if Ht τt. (13) This adaptive rule calibrates to each model, temperature, and stage of decoding: it detects relative spikes in low-entropy models, avoids always-on firing in high-entropy models, and tracks non-"
        },
        {
            "title": "Preprint",
            "content": "(a) Entropy trajectories of different models (temperature = 0) (b) Entropy trajectories of different models (temperature = 0.6) Figure 6: Entropy trajectories of different models (temperature = 0 and 0.6). stationary drift along the reasoning trajectory. The trajectories in Figures 6a and 6b illustrate that the rule consistently activates on local high-risk segments, enabling proactive test-time intervention without extra decoding passes."
        },
        {
            "title": "G CRITICAL TOKENS",
            "content": "We ran suite of mathreasoning tasks with DEEPSEEK-R1-DISTILL-QWEN-7B and, using our dynamic entropy monitor, extracted tokens whose next-token uncertainty exceeded the adaptive threshold. The aggregated frequencies are summarized in Figure 7. The head of the distribution is dominated by function words and discourse connectives: the, so, but, that, since, which, if, then, for, together with stance/hedging markers (e.g., wait, perhaps, maybe) and referential anchors (e.g., i, we, this, it). These items typically occur at clause boundaries and reasoning junctions where the model must decide among competing con-"
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Tokens with high uncertainty above the dynamic threshold. tinuations (introducing premise, switching polarity, or committing to next step). Consequently, uncertainty spikes concentrate on tokens that steer the reasoning trajectory, rather than on content tokens that merely elaborate it. This observation directly supports our design: simple dynamic-threshold policy preferentially surfaces precisely these connective, high-impact tokens, yielding semantically meaningful intervention points without spending budget on uninformative positions. In practice, focusing the inner updates on this compact, high-coverage set keeps SRGen longer in the joint-descent regime of our hybrid objective, where the cross-entropy and entropy-minimization gradients align and both confidence and contextual fidelity can improve simultaneously (cf. Appendix B.1). As optimization progresses and the two gradients begin to oppose each other, the Lagrangian weight λ provides principled knob to regulate how aggressively we trade context fit for uncertainty reduction at each triggered step. Together, Figure 7 substantiates that entropy-guided, token-aware intervention offers precise and proactive control of the generation at the very junctures that determine the downstream path. This mechanism explains the consistent single-pass gains and more sample-efficient self-consistency we observe across models and benchmarks, while keeping overhead modest and bounded."
        },
        {
            "title": "H LOSS IN TRAINING",
            "content": "We analyze the loss-reduction dynamics observed during optimization, providing empirical guidance for selecting the learning rate. SRGen is method for rapid, on-the-fly optimization at test time. Using Qwen2.5-Math-7B and DeepSeek-R1-Distill-Qwen, we perform up to 1000 inner-loop updates on the correction vector δ at single uncertainty trigger, and report the resulting loss curves in Figure 8a and 8b. The curves show that larger learning rates are well suited to our on-the-fly procedure: they drive the objective down quickly and reach stable plateau, whereas smaller learning rates converge slowly (or stall), making them impractical for real-time adaptation at inference. With properly chosen learning rate, only handful of inner steps is required to achieve substantial loss reduction, which justifies our choice of few-step updates and preserves the efficiency of test-time optimization without introducing noticeable latency. We further plot the trajectories of the CE loss and the entropy-minimization loss as function of inner steps (Figures 9a and 9b). For small step counts, both losses drop together and the points lie roughly along the diagonal. This is the joint-descent regime predicted by our analysis: when LCE (cid:3) and LAEM form an acute angle, step along the hybrid direction (cid:2)(1 λ)LCE + λLAEM"
        },
        {
            "title": "Preprint",
            "content": "(a) Loss curves of Qwen2.5-Math-7B. (b) Loss curves of DeepSeek-R1-Distill-Qwen. Figure 8: Loss curves of different models. decreases both objectives (Appendix B.1, Joint-Descent Lemma). As optimization proceeds, the trajectories bend and spread, indicating that the gradients become increasingly antagonistic and the iterates approach the Pareto frontier described by Theorem 1, where improving one objective requires paying with the other. The role of λ also matches the Lagrangian view: larger λ corresponds to smaller dual weight on CE, α = (1 λ)/λ, hence looser implicit tolerance ε. Empirically, this shifts the early trajectory toward the entropy axis, updates first prioritize reducing LAEM while LCE stalls, so more inner steps are required before contextual fitting starts to reduce CE. In the extreme, an over-aggressive λ can drive rapid entropy reduction that distorts the token probability distribution, sometimes causing entropy collapse and ultimately harming efficiency and final quality. Taken together, these plots align with the theory and motivate using small but nonzero λ (or an adaptive schedule) to remain in the joint-descent regime before trading off along the frontier."
        },
        {
            "title": "I CASE STUDY",
            "content": "We provide two examples to illustrate how SRGen affects the generation process. To ensure that any differences are attributable solely to SRGen, we run Qwen2.5-Math-7B with greedy decoding and highlight the first divergence point. The blue-shaded span marks tokens identical to the baseline (without SRGen); the first token immediately after the blue region is the token altered by SRGen. In both examples, the reasoning path before the change leads to an incorrect answer, whereas the path after the changed token ultimately yields the correct result. Example 1 Query The sum of the first positive odd numbers is 121. What is the value of ? Original Response"
        },
        {
            "title": "Preprint",
            "content": "The sum of the first positive even numbers is 110. What is the value of ? (Hint: The sum of the first positive even numbers is (N + 1).) 10 . Self-Reflective Response The sum of the first positive odd numbers is given by the formula 2. So, we need to find such that 2 = 121. Taking the square root of both sides, we get = 121 = 11. Therefore, the value of is 11 . Example 2 Query There exist constants a, b, c, and such that (sin x)7 = sin 7x + sin 5x + sin 3x + sin for all angles x. Find d. Original Response To solve the problem, we need to express (sin x)7 in terms of sin 7x, sin 5x, sin 3x, and sin x. We can use the multiple angle formulas and the binomial theorem to expand (sin x)7. The steps are as follows: 1. Use the identity sin = eixeix to express (sin x)7. 2. Expand the expression using the binomial theorem. 3. Simplify the expression to find the coefficients of sin 7x, sin 5x, sin 3x, and sin x. Lets implement this in Python using sympy: ... The coefficient of sin in the expansion of (sin x)7 is 0. Therefore, the value of is 0 . 2i Self-Reflective Response To solve the problem, we need to express (sin x)7 in terms of sin 7x, sin 5x, sin 3x, and sin x. We can use the multiple angle formulas and the binomial theorem to expand (sin x)7. First, we use the identity sin = eixeix to express (sin x)7: 2i (cid:18) eix eix 2i Expanding this using the binomial theorem, we get: (sin x)7 = (cid:19) . (sin x)7 = 1 (2i)7 7 (cid:88) k=0 (cid:18)7 (cid:19) (eix)7k(eix)k. Simplifying the powers of eix and eix, we get: (sin x)7 = 1 (2i)7 7 (cid:88) k=0 (cid:18)7 (cid:19) (1)kei(72k)x. This can be written as: (sin x)7 = 1 (2i)7 (cid:0)e7ix 7e5ix + 21e3ix 35eix + 35eix 21e3ix + 7e5ix e7ix(cid:1) . Using the identity eix + eix = 2 cos x, we can rewrite the expression in terms of sines:"
        },
        {
            "title": "Preprint",
            "content": "(sin x)7 = 1 (2i)7 (cid:0)e7ix e7ix 7(e5ix e5ix) + 21(e3ix e3ix) 35(eix eix)(cid:1) . This simplifies to: (sin x)7 = 1 (2i)7 (2i sin 7x 7 2i sin 5x + 21 2i sin 3x 35 2i sin x) . Factoring out 2i and simplifying, we get: (sin x)7 = 1 (2i)6 (sin 7x 7 sin 5x + 21 sin 3x 35 sin x) . Since (2i)6 = 64, we have: (sin x)7 = 1 Thus, the coefficients are: (sin 7x 7 sin 5x + 21 sin 3x 35 sin x) . = 1 64 , = 7 64 , = 21 64 , = 35 64 . Therefore, the final answer is:"
        },
        {
            "title": "J LLM USAGE",
            "content": "Large Language Models (LLMs) were used to aid in the writing and polishing of the manuscript. Specifically, we used an LLM to assist in refining the language, improving readability, and ensuring clarity in various sections of the paper. The model helped with tasks such as sentence rephrasing, grammar checking, and enhancing the overall flow of the text. It is important to note that the LLM was not involved in the ideation, research methodology, or experimental design. All research concepts, ideas, and analyses were developed and conducted by the authors. The contributions of the LLM were solely focused on improving the linguistic quality of the paper, with no involvement in the scientific content or data analysis. The authors take full responsibility for the content of the manuscript, including any text generated or polished by the LLM. We have ensured that the LLM-generated text adheres to ethical guidelines and does not contribute to plagiarism or scientific misconduct."
        },
        {
            "title": "Preprint",
            "content": "(a) Cross-entropy loss and entropy-minimization loss vs. steps under different learning rates (Qwen2.5-Math7B) (b) Cross-entropy loss and entropy-minimization loss vs. steps under different learning rates (DeepSeek-R1Distill-Qwen) Figure 9: Cross-entropy/entropy-minimization losses vs. steps under different learning rates."
        }
    ],
    "affiliations": [
        "City University of Hong Kong",
        "Hong Kong University of Science and Technology (Guangzhou)",
        "Nanyang Technological University",
        "The Chinese University of Hong Kong, Shenzhen",
        "University of Edinburgh"
    ]
}