{
    "paper_title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning",
    "authors": [
        "Qikai Chang",
        "Zhenrong Zhang",
        "Pengfei Hu",
        "Jiefeng Ma",
        "Yicheng Pan",
        "Jianshu Zhang",
        "Jun Du",
        "Quan Liu",
        "Jianqing Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as a promising approach to bridge this gap. Despite recent advances, existing methods struggle with three key challenges: constructing tool-integrated reasoning data, performing fine-grained optimization, and enhancing inference. To overcome these limitations, we propose THOR (Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen, a multi-agent actor-critic-based pipeline for constructing high-quality datasets of tool-integrated reasoning paths, aligning with the policy and generalizing well across diverse models. Second, to perform fine-grained hierarchical optimization, we introduce an RL strategy that jointly optimizes for both trajectory-level problem solving and step-level code generation. This is motivated by our key insight that the success of an intermediate tool call is a strong predictor of the final answer's correctness. Finally, THOR incorporates a self-correction mechanism that leverages immediate tool feedback to dynamically revise erroneous reasoning paths during inference. Our approach demonstrates strong generalization across diverse models, performing effectively in both reasoning and non-reasoning models. It further achieves state-of-the-art performance for models of a similar scale on multiple mathematical benchmarks, while also delivering consistent improvements on code benchmarks. Our code will be publicly available at https://github.com/JingMog/THOR."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 1 6 7 3 1 . 9 0 5 2 : r Preprint. THOR: TOOL-INTEGRATED HIERARCHICAL OPTIMIZATION VIA RL FOR MATHEMATICAL REASONING Qikai Chang1, Zhenrong Zhang2, Pengfei Hu1, Jiefeng Ma2, Yicheng Pan1, Jianshu Zhang2, Jun Du1, Quan Liu2, Jianqing Gao2 1University of Science and Technology of China, 2iFLYTEK Research"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as promising approach to bridge this gap. Despite recent advances, existing methods struggle with three key challenges: constructing toolintegrated reasoning data, performing fine-grained optimization, and enhancing inference. To overcome these limitations, we propose THOR (Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen, multi-agent actor-critic-based pipeline for constructing high-quality datasets of tool-integrated reasoning paths, aligning with the policy and generalizing well across diverse models. Second, to perform fine-grained hierarchical optimization, we introduce an RL strategy that jointly optimizes for both trajectory-level problem solving and step-level code generation. This is motivated by our key insight that the success of an intermediate tool call is strong predictor of the final answers correctness. Finally, THOR incorporates self-correction mechanism that leverages immediate tool feedback to dynamically revise erroneous reasoning paths during inference. Our approach demonstrates strong generalization across diverse models, performing effectively in both reasoning and non-reasoning models. It further achieves state-of-the-art performance for models of similar scale on multiple mathematical benchmarks, while also delivering consistent improvements on code benchmarks. Our code will be publicly available at https://github.com/JingMog/THOR. Figure 1: An overview of our method. The left panel depicts the motivation and challenges, the middle highlights our proposed solution with the TIRGen data construction pipeline and the THOR hierarchical RL framework, and the right panel reports experimental results. Corresponding author: Jun Du (jundu@ustc.edu.cn). 1 Preprint."
        },
        {
            "title": "1\nLarge Language Models (LLMs) have achieved remarkable progress,\nincreasingly exhibiting\nhuman-like capabilities such as thinking, reflection, and self-correction. They have shown signif-\nicant improvements in mathematical reasoning, code generation, and autonomous agent tasking\n(Jaech et al., 2024; Guo et al., 2025; Yang et al., 2025; Team et al., 2025).",
            "content": "Recent methods for enhancing LLMs mathematical reasoning can be broadly categorized into search-based methods (Besta et al., 2024; Zhang et al., 2025; Hu et al., 2025) and Reinforcement Learning (RL) paradigms (Yu et al., 2025; Yue et al., 2025b). Despite notable progress, both are limited by fundamental weakness of LLMs. As probabilistic, next-token predictors, they inherently struggle with high-precision tasks (Chen et al., 2022), such as numerical computation, equation solving, symbolic manipulation (Pan et al., 2025), and formal proofs (Lewkowycz et al., 2022), often leading to factual hallucinations (Li et al., 2025b). Programmatic reasoning, however, excels in these domains. Therefore, integrating the semantic reasoning of LLMs with the precise, verifiable execution of external code-based tools offers crucial pathway to overcome these limitations. Tool-Integrated Reasoning (TIR) has emerged as powerful paradigm for enhancing LLM reasoning by enabling them to leverage external tools to augment reasoning (Gou et al., 2023; Li et al., 2025a). Despite considerable efforts, three core challenges remain: constructing TIR data, performing fine-grained optimization, and enhancing inference. (1) For constructing TIR data, current methods synthesize tool-use data via prompting (Gou et al., 2023; Yang et al., 2024). However, for reasoning models such as DeepSeek-R1 (Guo et al., 2025) and QwQ (Team, 2025), prompting alone often fails to elicit effective tool use (Li et al., 2025a). While techniques like START (Li et al., 2025a) explicitly inject code prompts into the thinking process, purely rule-based approaches are difficult to locate suitable insertion positions. Therefore, existing TIR data construction methods suffer from style mismatches and poor applicability to reasoning models. (2) For performing finegrained optimization, current research primarily employs either SFT or RL. SFT-based methods, like Toolformer (Schick et al., 2023) and Aimo-2 (Moshkov et al., 2025), require large-scale, highquality demonstration data and often suffers from poor generalization. Existing RL methods (Mai et al., 2025; Li et al., 2025c; Feng et al., 2025) typically optimize at the trajectory-level, overlooking fine-grained updates on specific error-prone steps. Although RL is more scalable alternative, it (3) For enhancing faces severe sparse reward problems, particularly in long reasoning chains. inference, existing methods typically interleave tool calls directly with natural language reasoning in single pass, thereby overlooking the role of immediate tool feedback in reasoning. To address these challenges, we propose THOR, tool-integrated framework designed to enhance (1) For constructing TIR data, in order to efficiently generthe reasoning ability of LLMs. ate policy-aligned TIR data, we propose TIRGen, an actor-critic-based data construction pipeline. The actor is responsible for generating natural language reasoning steps, while the critic evaluates whether steps can be transformed into executable code and interacts with an external executor to refine the reasoning. This iterative process yields TIR dataset that is naturally aligned with the actors policy and broadly applicable across diverse models and tools. (2) For performing finegrained optimization, we are motivated by the key insight that the success of an intermediate tool call is strong predictor of the final answers correctness. Our experiments later confirm this insight. Based on this, we introduce hierarchical RL strategy that combines trajectory-level and step-level optimization. At the trajectory-level, we directly optimize for the correctness of the final answer. Concurrently, at the step-level, we apply fine-grained optimization to execution failure steps, specifically enhancing the models code generation ability. (3) For enhancing inference, we propose self-correction mechanism that leverages immediate feedback from tools to dynamically revise its CoT during inference. When code invocation fails, it backtracks and explores alternative reasoning paths, thereby significantly enhancing the models reasoning robustness and overall performance. We evaluate our method on diverse challenging and widely-used benchmarks, including MATH500 (Hendrycks et al., 2021), AIME 2024 & 2025, AMC, Minerva Math (Lewkowycz et al., 2022), and Olympiad Bench (He et al., 2024). THOR establishes new state-of-the-art (SOTA) result among models of comparable size across architectures and scales, while reducing inference overhead. It further improves performance on code generation benchmarks HumanEval, MBPP (Liu et al., 2023), and LiveCodeBench, validating the effectiveness and generalizability of our approach. Our primary contributions are as follows: 1) Tool-Integrated Data Construction Pipeline. We introduce TIRGen, pipeline for generating TIR data, applicable across diverse models, and better Preprint. aligned with the preferences of the policy model. 2) Hierarchical Optimization. We propose hierarchical reinforcement learning approach that combines trajectory-level and step-level optimization. 3) Self-correction Inference Enhancement. We introduce self-correction mechanism that leverages immediate tool feedback to revise reasoning steps during inference. 4) Superior Performance and Broad Generalization. Our approach generalizes across reasoning and non-reasoning models, achieving SOTA on mathematical benchmarks and consistent gains on code tasks."
        },
        {
            "title": "2.1 PROBLEM FORMULATION",
            "content": "In the context of tool-integrated reasoning, an LLM solves mathematical problems by interleaving natural language reasoning with tool invocations. Specifically, we formulate an LLM, parameterized by θ, as policy πθ. Given problem and corresponding instruction I, this policy πθ autoregressively generates an entire interaction trajectory τ , which is an alternating sequence of thoughts, actions, and observations: τ = (r1, a1, o1, ..., rt, at, ot, ..., rn1, an1, on1, rn), (1) where rt is step of natural language reasoning, at is an action of tool call, ot is the observation returned by the external execution environment after executing action at, and is the number of reasoning steps. This process is formulated as an iterative think-act-observe loop. The model incorporates the new observation ot into its context to inform the generation of the subsequent thought rt+1 and action at+1. This cycle continues until the model produces the final answer within its last thought rn, thereby concluding the trajectory. Formally, the likelihood of generating specific trajectory τ is factorized as: Pπθ (τ q, I) = Pπθ (rn q, I, H1:n1) n1 (cid:89) t=1 Pπθ (rt q, I, H1:t1) (cid:124) (cid:123)(cid:122) (cid:125) Thought Pπθ (at rt, q, I, H1:t1) , (cid:123)(cid:122) (cid:125) (cid:124) Action (2) where H1:t1 = (cid:8)r1, a1, o1, . . . , rt1, at1, ot1(cid:9) denotes the history of the previous interactions. Each term is modeled as product of token-level probabilities generated by the LLM. 2.2 TIRGEN: TIR DATA GENERATION PIPELINE Figure 2: The TIR data construction pipeline. In this pipeline, the Actor agent generates reasoning steps. The Critic agent identifies tool-executable steps and converts them into tool-augmented reasoning steps. After multi-stage filtering, we obtain the cold start dataset DSF . Existing methods for TIR highlight significant need for high-quality training data. Most approaches rely on simple prompting or powerful external large models to synthesize TIR data for non-reasoning models (Gou et al., 2023; Li et al., 2025c), but these approaches fail to extend effectively to reasoning models such as R1-Distill-Qwen. Although START (Li et al., 2025a) constructs long-CoT TIR data using rule-based prompt-hint approach, the resulting trajectories often contain redundant code invocations. Therefore, existing TIR data construction methods face critical shortcomings, including style mismatches between the generated data and policy models, as well as 3 Preprint. Initialize trajectory τ (q) while not IsSolved(τ ) do Algorithm 1 TIRGen: TIR Data Generation Pipeline 1: Input: Actor model πactor, Critic model πcritic, Dataset Dq, Code interpreter sandbox S. 2: Initialize: Raw cold start dataset Draw 3: for question Dq do 4: 5: 6: 7: 8: 9: 10: 11: 12: τ τ (rt) 13: Draw Draw {τ } 14: 15: DSF MultiStageFilter(Draw) 16: Return DSF rt πactor( τ ), rt Lstep if JudgeCodeSolvable(rt) then rt logic ExtractLogicπcritic at ConvertToCodeπcritic (rt, rt ot S(at) τ τ (rt Actor generates reasoning step Identify operation solvable with code by πcritic Step 1: Extract the pure reasoning part Step 2: Convert calculation part to code Step 3: Execute code to get observation Filter for format, code quality and difficulty logic, at, ot) logic) (rt) else limited applicability to reasoning models. To overcome these limitations, we introduce TIRGen, an automated TIR data synthesis pipeline leveraging an actor-critic framework, as shown in Figure 2. In this framework, an Actor agent generates natural language reasoning step with maximum length of Lstep. Critic agent then evaluates this step to identify operations that can be solved by code easily, such as numerical calculations or equation solving. Upon identifying such an operation, the Critic transforms it into an executable Python code snippet, ensuring consistency with the Actors original reasoning logic. After obtaining the code, the Critic then interacts with code interpreter to obtain precise execution result, which is used to replace the original operations and formulate new code-augmented reasoning step. The Actor subsequently continues reasoning using this new step. This iterative cycle continues until complete solution is derived, as shown in Algorithm 1. This design yields two key advantages: Reduced Reliance on Large-scale Models. The Actor handles the core mathematical reasoning, while the Critic only needs basic instruction-following and code-generation skills. In this way, the complex task is divided and solved by two agents. Policy Alignment. Since the Critic accesses only isolated reasoning steps without problem and answer, the synthesized data reflects the Actors intrinsic abilities. Consequently, this data remains in-distribution, mitigating the performance degradation caused by training on out-of-distribution data (Gudibande et al., 2023; Chen et al., 2024). After sampling, we employ multi-stage filtering procedure: (1) Format Consistency. Remove samples with erroneous tool calls, code formats, or output formats; enforce final answers to be wrapped by boxed{}. (2) Code Quality. Discard candidates whose code either fails to execute or contains only simple operations. Concretely, require at least one of: (i) library invocation (e.g., sympy, numpy), (ii) control flow (loop/branch). (3) Difficulty & Call-round Balancing. To ensure diverse problem complexity and tool invocation rounds, we stratify the samples by the number of code calls and apply moderate down-sampling to each subset. Additionally, filter instances solvable by pure CoT baseline, ensuring that all retained instances require tool integration to be solved. This comprehensive process yields the final dataset for our cold start phase, denoted as DSF . 2.3 HIERARCHICAL RL TRAINING STRATEGY 2.3.1 COLD START We initialize our model with cold start procedure (Guo et al., 2025), utilizing the dataset DSF generated by TIRGen. This initial stage is designed to teach the fundamental patterns of tool invocation, which is particularly important for reasoning models, as they often struggle to invoke code before cold start. During this stage, we directly perform supervised fine-tuning on the base model 4 Preprint. Figure 3: hierarchical optimization framework comprising (a) trajectory-level RL for mathematical problem solving and (b) fine-grained step-level optimization for code generation. In addition, we introduce (c) self-correction mechanism for online error correction during inference. πθ with the loss function as follows: LSFT(θ) = E(q,I,y)DSF (cid:104) (cid:88) t=1 (cid:105) , log πθ(yt q, I, y1:t1) (3) where is the length of response. In this way, we obtained the policy model πθ after the cold start. 2.3.2 HIERARCHICAL REINFORCEMENT LEARNING In the hierarchical optimization phase, we employ reinforcement learning to refine the models toolcalling strategy for solving complex problems. Existing reinforcement learning methods optimize solely at the trajectory-level based on the final answers correctness and thus suffer from severe sparse reward problem (Mai et al., 2025; Li et al., 2025c; Feng et al., 2025). Our experiments reveal key observation: the success of an intermediate tool call is strong predictor of the final answers correctness. Motivated by this, we propose hierarchical reinforcement learning method that combines coarse-grained trajectory-level optimization to enhance the models problem-solving ability with fine-grained step-level optimization to improve the models code generation capability, as shown in Figure 3. The observation is verified in Appendix D.1. Trajectory-level Optimization. At the trajectory-level, we adopt GRPO (Shao et al., 2024) and use the correctness of the final answer as the reward to enhance the ability of mathematical reasoning, as shown in Figure 3(a). The complete trajectory-level objective is: Ltraj πθ (θ) =E[q DRL, {si}G i=1 πθ(Sq)] 1 (cid:88) (cid:16) i=1 1 t=1 I(si,t) (cid:80)si si (cid:88) t:I(si,t)=1 (cid:1)(cid:17) (4) min (cid:0) πθ(siq) πθold(siq) Ai, clip( πθ(siq) πθold(siq) , 1 εlow, 1 + εhigh)Ai + αLNLL(θ), where denotes the group size, is the set of all generated responses, and Tpos is the subset of responses with positive advantages. I(si,t) = 1 indicates that the token si,t is generated by πθ, rather than an observation from the executor. Additionally, to better leverage successful trajectories, we incorporate language modeling (LM) loss LNLL on positive examples with weighting coefficient 5 Preprint. α, which directly reinforces the likelihood of correct samples during RL training (Yue et al., 2025b). LNLL(θ) = 1 (cid:80) siTpos si (cid:88) siTpos log πθ(siq). (5) Step-level Optimization. After sampling full trajectories, we perform step-level optimization to correct code errors using execution feedback, as illustrated in Figure 3(b). This stage focuses exclusively on reasoning steps that resulted in failed actions Aerr. First, we construct step-level optimization dataset Dstep. For each failed step, we treat it as query and draw group rollouts. The execution correctness of the generated code serves as the reward signal. Consequently, it is crucial to ensure that the rollouts within groups cover diverse execution outcomes. Next, we describe the construction of Dstep and its corresponding optimization method. Within think-act-observe tuple (rt, at, ot), failed action at inherently indicates an error. Moreover, since at is conditioned on rt, keeping rt fixed fails to provide sufficient diversity. To address this, we backtrack to rt and partition it into prefix rt suf has length of Lsuf . We then condition the model on the history up to rt pre and regenerate the new reasoning suffix and action. This procedure yields the dataset Dstep for fine-grained step-level optimization: pre and suffix rt suf, where rt Dstep = {pref(τ, t) at Aerr, τ }, pref(τ, t) = (q, r1, a1, o1, ..., rt1, at1, ot1, rt pre), rt = rt pre rt suf, (6) (7) This formulation constructs fine-grained code generation task: given reasoning prefix pref(τ, t), the model must correctly regenerate the subsequent thought ˆrt suf and action ˆat. At this stage, each sample contains single think-act-observe loop. The reward is computed from execution correctness. We optimize the policy with the following step-level loss: Lstep πθ (θ) = E[q Dstep, {s i}G i=1 πθ(Sq)] 1 (cid:88) (cid:16) i=1 1 (cid:80)s t=1 I(s i,t) i (cid:88) t:I(s i,t)=1 (cid:1)(cid:17) (8) min (cid:0) πθ(s πθold(s iq) iq) i, clip( πθ(s πθold (s iq) iq) , 1 εlow, 1 + εhigh)A + αLNLL(θ). To stabilize training, we mask the external tool observation ot in the loss calculation, as it is not direct model output. Furthermore, since execution failures may arise not only from model-generated errors but also from environment issues or sandbox limitations, we apply filter to trajectories with failed code executions during trajectory-level optimization to prevent inappropriate penalties. The final training objective is the sum of the trajectory-level and step-level losses: L(θ) = Ltraj πθ (θ) + Lstep πθ (θ). (9) 2.4 SELF-CORRECTION DURING INFERENCE During inference, our model follows the think-act-observe loop. To exploit immediate feedback from tool execution, we introduce self-correction mechanism that dynamically rectifies erroneous reasoning steps, as shown in Figure 3(c). Specifically, when an action at fails to execute, it indicates that both the action at and its associated reasoning step rt are likely incorrect. To explore different solving paths, the model backtracks to rt and partitions it into prefix rt suf, as previously described. Conditioned on the history up to rt pre, the model then regenerates new reasoning suffix ˆrt suf and revised action ˆat, which achieves online error correction during inference. The correction procedure can be repeated for up to Ncorr attempts. Importantly, each attempt only requires regenerating the suffix ˆrt suf and the corresponding action ˆat, rather than the entire trajectory. Thus, the additional computational cost is minimal compared to the total cost. pre and suffix rt Preprint. Table 1: Comparison with state-of-the-art methods on mathematical benchmarks, the best results are in bold and the second-best are underlined. denotes our reimplementation results of Avg@4. indicates results from their official releases. Model Code MATH AIME AIME AMC Minerva Olympiad Use Bench Math 2024 2025 2023 500 Non-Reasoning Models (Lightweight) Qwen3-1.7B Qwen2.5-Math-1.5B THOR-1.5B Non-Reasoning Models (Standard-scale) GPT-4o-0513 rStar-Math-7B Eurus-2-PRIME-7B TORL-7B AutoTIR-7B ZTRL-7B Qwen2.5-Math-7B-Instruct Qwen2.5-Math-7B Qwen2.5-Math-7B THOR-7B Reasoning Models (Lightweight) DeepSeek-R1-Distill-Qwen-1.5B DeepScaleR-1.5B-Preview Qwen3-1.7B THOR-Thinking-1.7B Reasoning Models (Standard-scale) OpenAI o1 QwQ-32B-Preview START-32B OpenMath-Nemotron-7B DeepSeek-R1-Distill-Qwen-7B Qwen3-8B THOR-Thinking-8B 73.0 23.8 79. 74.6 78.4 79.2 82.2 62.6 80.2 79.8 51.5 64.7 87.5 82.8 87.8 91.0 92.8 94.8 90.6 94.4 - 92.8 95.5 96.8 13.4 5.8 36.7 9.3 26.7 26.7 43.3 33.3 50.0 10.8 8.3 20.6 50.0 28.9 43.1 45.0 60. 74.4 50.0 66.7 72.9 55.5 64.2 77.5 9.8 4.2 20.0 - - 13.3 30.0 16.7 26.7 11.7 5.8 13.1 33.3 23.3 30.0 31.7 33.3 - 40.0 47.1 57.5 40.6 54.2 62.5 - 21.9 67. - 47.5 57.4 75.0 - - 54.4 33.1 49.7 81.3 62.9 73.6 80.6 82.5 - 80.0 95.0 - - 91.2 96.8 - 16.5 36.0 - - 38.6 - - - 44.8 26.7 28.1 53.9 26.5 30.2 52.7 54. - - - - - 64.4 65.6 - 18.1 54.0 - 47.1 48.9 - - - 43.1 22.9 37.9 61.1 43.3 50.0 65.7 68.8 - - - - - 77.7 79.7 Avg. - 15.1 48.9 - - - - - - 40.8 24.7 35.7 61.2 44.6 52.5 61.1 65.3 - - - - - 74.5 79."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 DATASET We evaluate the effectiveness of THOR on diverse set of representative and challenging benchmarks for both mathematical reasoning and code generation. For mathematical reasoning, our evaluation covers the high school-level MATH 500 (Hendrycks et al., 2021), as well as competition-level benchmarks including AMC 2023, AIME 2024, AIME 2025, MinervaMath (Lewkowycz et al., 2022), and OlympiadBench (He et al., 2024). These benchmarks span geometry, algebra, and number theory, providing comprehensive evaluation of LLMs mathematical reasoning. To assess answer correctness, we use Qwen3-32B to compare model predictions with ground truth. For code generation, we adopt EvalPlus (Liu et al., 2023) on HumanEval+ and MBPP+ to assess basic programming skills, and LiveCodeBenchv6 (Jain et al.) for competition-level programming tasks. 3.2 COMPARISON WITH STATE-OF-THE-ART METHODS To assess THORs effectiveness and generalization, we conduct comprehensive experiments on both non-reasoning and reasoning models. For the non-reasoning setting, we use Qwen2.5-Math-7B (Yang et al., 2024) to obtain THOR-7B. For the reasoning setting, we adopt Qwen3-8B (Thinking Mode) (Yang et al., 2025) to obtain THOR-Thinking-8B. We further evaluate generalization on the corresponding lightweight models, Qwen2.5-1.5B-Math and Qwen3-1.7B. To reduce randomness, we adopt Avg@4 as the evaluation metric. For reasoning models, the maximum context length is 16,384 tokens, while for non-reasoning models it is 4,096 tokens. For comparison, we evaluate THOR against diverse set of TIR and CoT-based methods, including AutoTIR (Wei et al., 2025), TORL-7B (Li et al., 2025c), Eurus-2-PRIME-7B (Cui et al., 2025), rStart-Math-7B (Guan et al., 2025), ZTRL-7B (Mai et al., 2025), and GPT-4o (Hurst et al., 2024). We also include long CoT methods include START (Li et al., 2025a), DeepSeek-R1-Distill-Qwen Preprint. (Guo et al., 2025), DeepScaleR (Luo et al., 2025), QwQ (Team, 2024), Nemotron (Moshkov et al., 2025) and OpenAI o1. As shown in Table 1, THOR achieves substantial improvements on both nonreasoning and reasoning models, demonstrating its effectiveness in enhancing mathematical reasoning capabilities. Moreover, despite relying only on small policy models, THOR remain competitive with state-of-the-art systems and surpass many larger models, while maintaining low inference cost. The detailed inference cost is provided in Appendix D.2."
        },
        {
            "title": "3.3 SELF-REWARDED INFERENCE ENHANCEMENT",
            "content": "Traditional test-time scaling (TTS) search algorithms, such as Best-of-N (BoN), often rely on external Outcome Reward Models (ORMs) to evaluate the trajectory quality. We implement an ORM-free search method that exploits intermediate code execution feedback as self-contained reward signal. Specifically, our approach generates independent candidates and selects the best one with the highest execution pass rate, thereby eliminating the need for an external reward model. As shown in Table 2, self-rewarded BoN significantly outperforms single path reasoning by exploring larger search space. Furthermore, performance continues to improve as increases, indicating that code execution success can serve as reliable reward signal for assessing reasoning quality. Interestingly, on the more challenging problems from AIME 2024 and 2025, the gains are more substantial, indicating that difficult problems benefit more from precise code execution support. Table 2: Self-rewarded inference results, using execution pass rate as self-contained reward score. MATH AIME AIME AMC Minerva Olympiad 500 2024 2023 Math Bench Avg. 87.5 87.7 87.7 96.8 97.2 97. 50.0 51.7 53.3 77.5 84.2 86.7 33.3 35.0 38.3 62.5 68.3 70.0 81.3 81.3 83.8 96.8 97.5 97. 53.9 53.9 54.9 65.6 65.8 65.8 61.1 61.2 61.5 79.7 81.3 82.0 61.2 61.8 0.6% 63.3 2.1% 79.8 82.4 2.6% 83.2 3.4% Model Non-Reasoning Model THOR-7B THOR-7Bbon@4 THOR-7Bbon@8 Reasoning Model THOR-Thinking-8B THOR-Thinking-8Bbon@4 THOR-Thinking-8Bbon@8 3.4 ABLATION STUDY To analyze the contribution of each component in THOR, we conduct an ablation study by selectively removing key modules. This results in six system variants T1T6, built upon Qwen2.5-Math7B and Qwen3-8B, as summarized in Table 3. Impact of Cold Start. The cold start data generated by TIRGen provides foundation for subsequent RL. The goal of RL is to refine the models policy towards its capability frontier (Yue et al., 2025a), which can be estimated using pass@k (Chen et al., 2021). Consequently, we evaluate cold start by its impact on pass@16 and code invocation ratio. We further compare with other TIR dataset, including the Long CoT TIR data generated by Nemotron (Moshkov et al., 2025) and the Short CoT TIR data from ReTool (Feng et al., 2025). As shown in Figure 4, TIRGen substantially improves both metrics, effectively expanding the capability frontier. Compared with other datasets, the data generated by TIRGen effectively mitigates performance degradation arising from out-of-distribution samples. More critically, it actively encourages reasoning models to utilize tools and dramatically increases the code ratio, behavior rarely seen in the baseline. Impact of Tool-Integrated RL. To evaluate the effectiveness of tool-integrated RL, we apply two different RL strategies to the baseline model (T1): standard CoT-based RL and trajectorylevel TIR-based RL, which yield T2 and T4, respectively. While both outperform the baseline, T4 achieves substantially greater improvements than T2, validating the effectiveness of TIR RL. Impact of Hierarchical RL. By incorporating step-level optimization into trajectory-level RL (T4), we obtain T5. T5 achieves further performance gains across most datasets, underscoring the importance of fine-grained optimization for enhancing code generation capabilities in TIR setting. 8 Preprint. Table 3: Results of the ablation on each component. Cold start uses the data generated by TIRGen in Section 2.2. TrajRL and StepRL correspond to trajectory-level and step-level optimization defined in Section 2.3. SelfCorr denotes self-correction during inference in Section 2.4. Code Cold Traj RL Start Use Step RL Self MATH AIME AIME AMC Minerva Olympiad Corr Bench Math 2024 2025 500 Non-Reasoning Model T1 T2 T3 T4 T5 T6 Reasoning Model T1 T2 T3 T4 T5 T6 51.5 72.9 64.7 86.9 87.3 87.5 95.5 95.7 92.9 96.1 96.6 96.8 8.3 30.0 20.6 42.7 45.0 50. 64.2 65.8 60.8 71.7 74.2 77.5 5.8 11.7 13.3 30.8 31.7 33.3 54.2 52.5 51.7 60.0 60.0 62.5 33.1 53.8 49.7 77.5 80.0 81.3 91.2 93.1 88.8 95.0 95.6 96.8 26.7 41.5 28.1 52.2 53.4 53. 64.4 64.4 61.4 64.5 65.4 65.6 22.9 38.3 37.9 58.2 60.5 61.1 77.7 78.0 72.9 78.9 79.0 79.7 Avg. 24.7 41.4 35.7 58.1 59.7 61.2 74.5 74.9 71.4 77.7 78.5 79. Figure 4: Ablation on cold-start efficiency. We compare our TIRGen against other TIR datasets, including Long CoT from Nemotron and Short CoT from ReTool. Results are reported as code ratio in (a) and pass@16 in (b) and (c), demonstrating the effectiveness of TIRGen and cold start. Impact of Self-Correction. By leveraging step-level feedback from intermediate code, we construct self-correction mechanism, yielding variant T6. We set the maximum number of correction attempts Ncorr = 4, which leads to substantial performance gains. This result highlights the critical importance of successful code generation and execution for the final outcome. 3.5 GENERALIZATION ON CODE BENCHMARKS We also evaluate THORs code generation abilities using the pass@1 metric on HumanEval+, MBPP+ and LiveCodeBenchv6. As illustrated in Figure 5, our models achieve consistent improvements across all benchmarks. Notably, these gains are realized in zero-shot setting without any fine-tuning on code generation data. These results confirm that our method strengthens both mathematical reasoning and code generation, underscoring THORs robustness and versatility across distinct reasoning domains. Figure 5: Pass@1 accuracy on code generation benchmarks. Preprint."
        },
        {
            "title": "4 CONCLUSION",
            "content": "In this work, we address three core challenges in tool-integrated reasoning: construction of TIR data, hierarchical optimization, and inference enhancement. We propose THOR (Tool-Integrated Hierarchical Optimization via RL), novel hierarchical RL framework for TIR that fully leverages step-level feedback. First, to mitigate the scarcity of TIR data, we introduce TIRGen, an actorcritic-based TIR data construction pipeline. For model training, THOR integrates coarse-grained trajectory-level optimization for overall reasoning ability with fine-grained step-level optimization for code generation ability. During inference, tool feedback is used to dynamically adjust the reasoning and perform self-correction. Experiments demonstrate that THOR achieves substantial improvements across diverse models and benchmarks while maintaining low inference cost."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "For the reproducibility of our results, we have provided detailed description of our method in Section 2 and experimental setups in Appendix E. In addition, to further facilitate the reproduction, we will release our codes and datasets."
        },
        {
            "title": "REFERENCES",
            "content": "Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pp. 1768217690, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey arXiv preprint Levine, and Dawn Song. The false promise of imitating proprietary llms. arXiv:2305.15717, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Preprint. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, et al. Deepmath-103k: large-scale, challenging, dearXiv preprint contaminated, and verifiable mathematical dataset for advancing reasoning. arXiv:2504.11456, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Pengfei Hu, Zhenrong Zhang, Qikai Chang, Shuhang Liu, Jiefeng Ma, Jun Du, Jianshu Zhang, Quan Liu, Jianqing Gao, Feng Ma, et al. Prm-bas: Enhancing multimodal reasoning through prm-guided beam annealing search. arXiv preprint arXiv:2504.10222, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free In The Thirteenth International Conference on evaluation of large language models for code. Learning Representations. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen Yu, Binyuan Hui, Junyang Lin, and Dayiheng Liu. Start: Self-taught reasoner with tools. arXiv preprint arXiv:2503.04625, 2025a. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025b. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025c. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:2155821572, 2023. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. Xinji Mai, Haotian Xu, Weinong Wang, Yingying Zhang, Wenqiang Zhang, et al. Agent rl scaling law: Agent rl with spontaneous code execution for mathematical problem solving. arXiv preprint arXiv:2505.07773, 2025. 11 Preprint. Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025. Yicheng Pan, Zhenrong Zhang, Pengfei Hu, Jiefeng Ma, Jun Du, Jianshu Zhang, Quan Liu, Jianqing Gao, and Feng Ma. Enhancing the geometric problem-solving ability of multimodal llms via symbolic-neural integration. arXiv preprint arXiv:2504.12773, 2025. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:68539 68551, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown. Hugging Face, 2024. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845, 2024. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Yifan Wei, Xiaoyan Yu, Yixuan Weng, Tengfei Pan, Angsheng Li, and Li Du. Autotir: Autonomous tools integrated reasoning via reinforcement learning. arXiv preprint arXiv:2507.21836, 2025. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. Preprint. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023a. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. In International Conference on React: Synergizing reasoning and acting in language models. Learning Representations (ICLR), 2023b. Fei Yu, Anningzhe Gao, and Benyou Wang. Ovm, outcome-supervised value models for planning in mathematical reasoning. arXiv preprint arXiv:2311.09724, 2023. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, and Lin Yan. Whats behind ppos collapse in long-cot? value optimization holds the secret. arXiv preprint arXiv:2503.01491, 2025. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025a. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025b. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh arXiv preprint Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv:2408.15240, 2024. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint arXiv:2411.14405, 2024. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. 13 Preprint."
        },
        {
            "title": "A LLM USAGE",
            "content": "We used LLMs, including GPT-5 and Gemini 2.5 Pro, only to polish grammar and improve the clarity of the manuscript. All research ideas, experiments, analyses were conducted by the authors."
        },
        {
            "title": "B RELATED WORKS",
            "content": "LLMs have shown remarkable progress in mathematical problem solving. This section reviews relevant works, which we categorize into two main groups based on whether they integrate external tools: tool-free reasoning and tool-integrated reasoning. B.1 TOOL-FREE REASONING Search-based Methods. Search-based methods improve LLM reasoning by systematically exploring large space of potential solutions. Early approaches leveraged prompting strategies like Chainof-Thought (CoT) (Wei et al., 2022) and its search-oriented extensions, including Tree-of-Thought (ToT) (Yao et al., 2023a) and Graph-of-Thought (GoT) (Besta et al., 2024). To more effectively guide this exploration, prominent line of work integrates explicit Reward Models (RMs), including Outcome Reward Models (ORMs) (Yu et al., 2023; Zhang et al., 2024) and Process Reward Models (PRMs) (Wang et al., 2023b; 2024; Zhang et al., 2025) with search algorithms like BoN, step-level BoN, MCTS and beam search. For instance, Marco-o1 (Zhao et al., 2024) and rStar-Math (Guan et al., 2025) employ MCTS for systematically exploration, while PRM-BAS (Hu et al., 2025) uses Beam Annealing Search to balance search breadth and efficiency. Although these methods yield significant performance gains, they have two key limitations. First, the large-scale search incurs substantial computational overhead at inference time. Second, and more critically, they do not directly optimize the models internal reasoning policy, thereby constraining its ultimate capability. RL-based Methods. RL represents another dominant paradigm for enhancing LLM reasoning, where policy gradient (PG) methods have become core technical route (Guo et al., 2025; Team et al., 2025; Yang et al., 2025). These methods can be categorized into value-model-based and value-model-free approaches. Value-model-based approaches are exemplified by Proximal Policy Optimization (PPO) (Schulman et al., 2017), which stabilizes training via policy probability ratio clipping. Its variants include VC-PPO (Yuan et al., 2025) with decoupled-GAE to mitigate value bias and reward decay, and VAPO (Yue et al., 2025b) with length-adaptive GAE to address the bias-variance trade-off. Value-model-free methods bypass the need for an explicit value critic. For instance, GRPO (Shao et al., 2024) estimates the baseline from group scores. This is enhanced by DAPO (Yu et al., 2025) with techniques like dynamic sampling and token-level loss, while GSPO (Zheng et al., 2025) performs optimization based on sequence-level likelihood ratios. key limitation of these approaches is their primary reliance on sparse, trajectory-level reward signals. For tasks involving long reasoning chains, this reward sparsity impedes efficient policy learning. B.2 TOOL-INTEGRATED REASONING Integrating external tools, such as code executors, search engines, databases and external APIs has become prominent strategy for augmenting the reasoning capabilities of LLMs. Early approaches focused on prompting methods, without integrate tool use into model optimization. For example, ReAct (Yao et al., 2023b) demonstrated the use of prompting to invoke the Wikipedia API for question answering and fact verification. VOYAGER (Wang et al., 2023a) explored in-context learning to leverage predefined tools within Minecraft. Subsequent studies incorporated human-labeled or synthetic tool-integrated data during fine-tuning (Schick et al., 2023; Yang et al., 2024; Moshkov et al., 2025; Li et al., 2025a). However, while effective in specific domains, the generalization is often constrained by the scope and quality of the synthetic data. More recently, RL has been employed to learn dynamic tool-integrated policies for mathematics reasoning (Mai et al., 2025; Li et al., 2025c; Feng et al., 2025). Nevertheless, existing RL-based approaches often rely on prompt-based triggers to initiate tool invocation, which limits their applicability to models not previously exposed to tool-integrated training data, such as R1-Distill-Qwen (Guo et al., 2025) and QwQ (Team, 2025). Furthermore, the step-level feedback provided by tools remains unexplored. 14 Preprint."
        },
        {
            "title": "C LIMITATION AND SOCIAL IMPACT",
            "content": "C.1 LIMITATION AND FUTURE WORK In this work, we systematically investigate the effectiveness of tool-integrated reasoning, focusing specifically on code integration for mathematical problem solving. Although we have verified the effectiveness of code-integrated reasoning, other types of tools such as search engines, symbolic systems remain to be explored. Due to computational constraints, we did not experiment with largerscale models such as 32B or 72B. Nevertheless, we have validated the effectiveness of THOR across multiple model sizes ranging from 1.5B to 8B, which demonstrates that our approach generalizes well across different scales. In the future, we will explore larger models and conduct deeper investigation into multi-tool joint optimization. C.2 SOCIAL IMPACT By integrating precise tool execution with hierarchical reinforcement learning, THOR significantly enhances the mathematical reasoning capabilities of LLMs. This advancement holds substantial promise for education and scientific research by offering reliable, automated assistance for complex problems in mathematics, engineering, and the formal sciences. However, like any powerful LLMbased system, THOR carries risk of misuse, such as generating misleading or harmful content if deployed without proper oversight. Consequently, the development of robust ethical safeguards and responsible deployment protocols is important for its application in real-world scenarios."
        },
        {
            "title": "D MORE EXPERIMENTS",
            "content": "D.1 STATISTICAL VALIDATION To examine the relationship between code execution success and answer correctness, we analyzed their joint distribution on the test set, as shown in Table 4. We then applied chi-square test of independence, which yielded highly significant result (χ2 = 336.3, = 4.09 1075), thereby rejecting the null hypothesis of independence and confirming statistical association between the two variables. These findings verified our research motivation: the success of an intermediate tool call is strong predictor of the final answers correctness. Table 4: Joint distribution between code execution result and answer correctness. Code True Code False Answer True Answer False 3950 139 318 D.2 INFERENCE COST ANALYSIS We evaluate the inference efficiency of THOR by analyzing its token consumption. Our data construction process, guided by TIRGen, identifies redundant computational steps within reasoning chains and transforms them into executable code. By learning from this data, THOR is trained to generate more concise solutions, effectively leveraging tools to simplify computations at inference time. As reported in Table 5, our method reduces token consumption, demonstrating its computational efficiency. These results already include the overhead of self-correction."
        },
        {
            "title": "E IMPLEMENTATION DETAILS",
            "content": "E.1 TIR DATA CONSTRUCTION & COLD START. In the cold start stage, for data source construction, we collected large set of question-answer pairs from various public datasets, including DAPO17k (Yu et al., 2025), DeepMath103k (He et al., 2025), 15 Preprint. Table 5: The number of token consumption during inference across different benchmarks. Model Non-Reasoning Model Qwen2.5-Math-7B THOR-7B Reasoning Model Qwen3-8B THOR-Thinking-8B MATH AIME AIME AMC Minerva Olympiad 500 2024 2023 Math Bench Avg (#Tokens) 866 705 1283 1325 1420 1132 928 802 729 5102 4506 11986 10338 13022 7989 6749 6906 5444 1090 981 9238 8205 1083 1019 6% 9041 7841 13% and Deepscaler40k (Luo et al., 2025), which cover mathematical problems of diverse difficulty levels. After processing with TIRGen, we obtain 29,217 short CoT TIR samples from Qwen2.5Math-7B and 57,598 long CoT TIR samples from Qwen3-8B. The distribution of code invocation counts in the final cold start dataset DSF is shown in Figure 6. In TIRGen, the Actor agent uses the corresponding policy model, while the Critic agent uses Qwen3-32B (Non-thinking) for its strong instruction-following capability. For non-reasoning models, we set Lstep = 512, whereas for reasoning models we set Lstep = 4096. Our experiments utilize SandboxFusion1 as the external code execution environment. Figure 6: The distribution of code call rounds in the cold start dataset DSF . For cold start, models are full parameter fine-tuned for 1 epoch with global batch size of 256. We use AdamW optimizer (Loshchilov & Hutter, 2017) with fixed learning rate of 2 106. ZeRO (Rajbhandari et al., 2020) is adopted for memory-efficient training. For reasoning models, the maximum context length is 20,000 tokens, while for non-reasoning models it is 4,096 tokens. E.2 HIERARCHICAL REINFORCEMENT LEARNING. During the RL stage, we use the publicly available dataset ToRL28k (Li et al., 2025c). To stabilize training, we adopt an off-policy variant of GRPO and employ dynamic data filtering (Yu et al., 2025) to accelerate convergence. We set the group size = 16, the weight coefficient on the loss LNLL to α = 0.01, and the learning rate to 1 106. The KL-divergence term is omitted. Clipping coefficients are configured as εhigh = 0.28 to encourage diversity and εlow = 0.2. During rollout, we use the maximum sampling lengths of 4,096 tokens for non-reasoning models and 16,384 tokens for reasoning models, with up to 5 rounds of code interaction. rule-based reward function is used to mitigate reward hacking. All experiments are conducted on 16 NVIDIA H200 GPUs. E.3 PROMPT SETTING In this subsection, we provide the complete prompt settings used in our framework. Figures 7 and 8 illustrate the prompts designed for the Actor and Critic agents in the TIRGen data construction 1https://github.com/bytedance/SandboxFusion 16 Preprint. pipeline, respectively. Figures 9 and 10 present the prompts for tool-integrated reasoning in reasoning models and non-reasoning models."
        },
        {
            "title": "F CASE STUDY",
            "content": "In this section, we present case study to illustrate how THOR performs tool-integrated reasoning, including non-reasoning models in Figure 11, 12 and reasoning models in Figure 13. 17 Preprint. Figure 7: The prompt used by the Critic agent in our TIR data construction pipeline, TIRGen. 18 Preprint. Figure 8: The prompt used by the Actor agent in our TIR data construction pipeline, TIRGen. Figure 9: Prompt for tool-integrated reasoning in reasoning models. Figure 10: Prompt for tool-integrated reasoning in non-reasoning models. 19 Preprint. Figure 11: Example of tool-integrated reasoning by THOR-7B on an AIME 2024 problem, where external tools facilitate precise numerical computation. 20 Preprint. Figure 12: Example of tool-integrated reasoning by THOR-7B on an AIME 2025 problem, where complex equations are efficiently solved and verified with tool support. 21 Preprint. Figure 13: Example of tool-integrated reasoning by THOR-Thinking-8B on an AIME 2024 problem, where the use of tool enables the self-validation of reasoning steps."
        }
    ],
    "affiliations": [
        "University of Science and Technology of China",
        "iFLYTEK Research"
    ]
}