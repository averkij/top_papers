{
    "paper_title": "OmniGen2: Exploration to Advanced Multimodal Generation",
    "authors": [
        "Chenyuan Wu",
        "Pengfei Zheng",
        "Ruiran Yan",
        "Shitao Xiao",
        "Xin Luo",
        "Yueze Wang",
        "Wanli Li",
        "Xiyan Jiang",
        "Yexin Liu",
        "Junjie Zhou",
        "Ze Liu",
        "Ziyi Xia",
        "Chaofan Li",
        "Haoge Deng",
        "Jiahao Wang",
        "Kun Luo",
        "Bo Zhang",
        "Defu Lian",
        "Xinlong Wang",
        "Zhongyuan Wang",
        "Tiejun Huang",
        "Zheng Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 1 7 8 8 1 . 6 0 5 2 : r OmniGen2: Exploration to Advanced Multimodal Generation Chenyuan Wu*, Pengfei Zheng*, Ruiran Yan*, Shitao Xiao*, Xin Luo*, Yueze Wang* Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ziyi Xia, Ze Liu, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, Zheng Liu Beijing Academy of Artificial Intelligence {stxiao, yzwang}@baai.ac.cn zhengliu1026@gmail.com *Co-first Authors and Listed in Alphabetical Order. Core Contributor. Project Lead."
        },
        {
            "title": "Abstract",
            "content": "In this work, we introduce OmniGen2, versatile and open-source generative model designed to provide unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce reflection mechanism tailored for image generation tasks and curate dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen"
        },
        {
            "title": "Introduction",
            "content": "Unified image generation has attracted significant attention in recent years [80; 92; 73; 49]. For instance, OmniGen [80] employs streamlined Transformer architecture capable of addressing diverse image generation tasks without the need for additional plugins or preprocessors. The recent breakthroughs of models like Gemini-2.0-flash [23] and GPT-4o [54] have further demonstrated the vast potential of this field, and heralded new paradigm in multimodal intelligence, shifting the frontier from specialized models [3; 34] towards powerful, unified systems. Chameleon [72] and Emu3 [76] utilize discrete autoregressive methods across all modalities, while the Janus series [78; 12] introduces dual image encoders specifically designed for understanding and generation tasks. Transfusion integrates both autoregressive and diffusion processes within single Transformer framework. However, the range of image generation tasks supported by these models remains limited. In this work, we introduce OmniGen2, an open-source generative model that demonstrates competitive performance across wide range of generation tasks. Unlike its predecessor, OmniGen [80], our Technical Report Figure 1: Overview of versatile abilities in OmniGen2 and reflection model observations indicate that simple parameter sharing is insufficient for simultaneously handling autoregressive text modeling and diffusion-based image modeling. To address this, OmniGen2 adopts distinct architectural pathways for autoregressive and diffusion tasks. Although models like GPT-4o [54] exhibit strong prompt-following abilities, they often lack the consistency required for precise image editing and maintaining subject identity in in-context generation. We speculate that this observed lack of consistency is characteristic limitation of models that rely exclusively on high-level semantic encoders, which often struggle to capture fine-grained visual details necessary for such tasks. To address this gap, OmniGen2 enhances its predecessors core strategy of using VAE features for low-level visual information. Recent models such as Mogao [41] and BAGEL [14], which also employ unshared parameters for text and image modalities and utilize dual vision tokenizers (VAE [32] and ViT [16]) for image processing. In contrast. OmniGen2 feeds VAE-derived features exclusively into the diffusion model, rather than the multimodal large language model (MLLM). This design choice preserves the inherent multimodal understanding capabilities of the MLLM by avoiding the influence of VAE encodings, while also reducing redundant image representations. As result, OmniGen2 maintains the simplicity and strong text generation capabilities of the original MLLM. Beyond the model, we also explore to tackle the foundational data and evaluation challenges that impede progress in this domain. On the one hand, we started by collecting and reorganizing relevant open-source datasets. However, most of them are plagued by inherent quality limitations, especially in tasks such as image editing and in-context generation. This limitation has substantially contributed to the significant performance gap between open-source and commercial models. To address this issue, we developed comprehensive pipeline for generating image editing and in-context data from video sources, and we plan to release these datasets to the community. Additionally, we constructed reflective data for image generation through an iterative generative process, with the aim of incorporating the reasoning and reflection capabilities of LLMs into multimodal generation. On the other hand, our extensive evaluation of OmniGen2 reveals its competitive performance across various task domains, including text-to-image (T2I) generation, image editing, and in-context genera2 Figure 2: Architecture of OmniGen2. OmniGen2 employs separate transformer architectures for autoregressive and diffusion. Two distinct image encoders are utilized: ViT encodes images for input into the text transformer, while VAE encodes images for the diffusion transformer. tion. Notably, for the in-context generation task, there is currently lack of well-established public leaderboards to systematically assess and compare the key capabilities of different models. Existing resources, such as DreamBench [64], are insufficient to capture the complexity and requirements of real-world scenarios. To mitigate this limitation, we introduce the OmniContext benchmark, which includes eight task categories specifically designed to evaluate consistency with respect to individuals, objects, and scenes. Experimental results show that OmniGen2 achieves state-of-the-art consistency performance among open-source models. Our primary contributions can be summarized as follows: We open-source OmniGen2, powerful multimodal generative model that demonstrates exceptional performance in various image generation tasks. In addition to its robust image synthesis capabilities, OmniGen2 also maintains strong text generation capability. Building on this foundation, we further explore the application of multimodal reflection mechanisms in image generation. Novel data generation pipelines and suite of high-quality datasets derived from videos, specifically designed to address the data scarcity for advanced image editing and in-context learning tasks. The OmniContext Benchmark, comprehensive evaluation suite designed to rigorously assess and standardize the measurement of in-context visual generation capabilities across diverse scenarios. Furthermore, it is important to emphasize that the text generation capability in OmniGen2 primarily originates from the MLLM component within its decoupled architecture, rather than from explicit endto-end training of the MLLM in the OmniGen2 base modelapart from the incorporation of special token. End-to-end, full-parameter training is only performed in the reflection model. Therefore, OmniGen2 should be regarded as multimodal generative model that supports simultaneous output of both images and text, rather than native multimodal model."
        },
        {
            "title": "2 Model",
            "content": "2.1 Design Princeple In the original OmniGen [80] framework, we employed autoregressive modeling for text and diffusion-based approach for image generation, both implemented within transformer architecture initialized with phi-3 [1]. Following the release of OmniGen, we conducted series of further experiments. Firstly, we replaced phi-3 with the more powerful Qwen model. Surprisingly, despite leveraging stronger large language model (LLM), we observed decline in image generation quality. Secondly, we explored mixture-of-experts (MoE) strategy to independently route text and image parameters, akin to the approach used in LMfusion [68]. Our findings indicate that initializing the image branch with parameters derived from the text branch leads to inferior performance compared to 3 Figure 3: An illustration of Omni-RoPE. It decomposes position information into three components: (1) Sequence and Modality Identifier (idseq) that is constant for all tokens within single image (treating it as semantic unit) but unique across different images; and (2) and (3) 2D spatial coordinates (h, w) that are locally computed from (0,0) for each image entity. This dual mechanism enables the model to unambiguously distinguish different images via their unique idseq, while the shared local spatial coordinates enhance consistency for tasks like image editing. direct random initialization of the image pathway. These results suggest that parameters optimized for text are not well-suited for image modeling. Consequently, in OmniGen2, we decouple the diffusion process and randomly initialize its parameters. Recent approaches, such as MetaQuery [4] and BLIP-3o [8], employ learnable query tokens to encode conditional information for diffusion-based generation. These methods compress all condition information into fixed number of tokens, which inevitably constrains the representation capacity and incurs information loss. We also find that such token-based compression struggles to handle long text rendering. Therefore, OmniGen2 leverages the hidden states of the multi-modal interleaved conditions produced by the MLLM, instead of relying on fixed set of learnable query tokens, as input to the diffusion decoder. An alternative solution could involve integrating both the hidden states of condition and query token, which we leave as direction for future work. Another important consideration is the integration of the VAE encoder. Although existing MLLMs predominantly utilize ViTs for image modeling, ViTs often struggle to capture fine-grained visual details, resulting in diminished image fidelity in image generation tasks. Although end-to-end training of ViT features can alleviate this limitation, it introduces additional complexity in balancing image understanding and generation tasks. Recent works such as BAGEL [14] and Mogao [41] address this issue by encoding images twiceincorporating both VAE and ViT features within the model. However, this dual-encoding approach requires substantial architectural modifications and the introduction of intricate attention mechanisms, thereby increasing development complexity. Furthermore, adapting the model to this new architecture necessitates retraining to restore its image understanding capabilities. In light of these challenges, we opt to employ VAEs exclusively as inputs to the diffusion decoder, rather than integrating them into the MLLM itself. This strategy preserves the architectural simplicity of the MLLM and maintains its multimodal understanding abilities without the need for extensive retraining. 2.2 Multimodal Large Language Model As illustrated in Figure 2, OmniGen2 leverages foundational MLLM transformer to process both textual and visual inputs. For text generation tasks, an autoregressive language head is employed, whereas image generation is accomplished via dedicated diffusion module. The transformer backbone is initialized by Qwen2.5-VL-3B [3]. And special token, <img>, is introduced to explicitly indicate image generation within the output sequence. Upon encountering this token, the model triggers the diffusion decoder to synthesize the corresponding image. The hidden states produced by the MLLM serve as conditional inputs to the diffusion decoder. However, since these hidden states may lack detailed visual information, we further augment the decoder with VAE-derived features extracted from the input images. The diffusion decoder then generates images using rectified flow (RF) approach. 2.3 Diffusion Transformer As depicted in the figure 2, we employ simple diffusion transformer architecture that directly concatenates features from MLLM, VAE, and noise, allowing joint attention over these modalities. Following Lumina-Image 2.0 [57], multiple input conditions are first processed by refiner network Figure 4: Multimodal Reflection for image generation. to ensure alignment before being passed to the transformer layers. The diffusion decoder comprises 32 layers with hidden size of 2520, resulting in approximately 4 billion parameters. Owing to the explicit incorporation of VAE features, the hidden states corresponding to images within the MLLM become less critical. To reduce computational overhead, we discard the image-related hidden states in the MLLM and retain only those associated with text tokens. Additionally, we adopted 3D rotary position embedding in the diffusion transformer, which is modification of the Qwen mRoPE. Multimodal Rotary Position Embedding. Inspired by recent advances in multimodal position embedding designs [75; 57; 71], we introduce novel Omni-RoPE specifically designed to meet the demands of our diverse and complex tasks, particularly image editing and in-context generation. As illustrated in Figure 3, Our Omni-RoPE is decomposed into three distinct components: 1. Sequence and Modality Identifier (idseq): The primary role of this component is to distinguish tokens from different modalities and sequences. Crucially, we treat each image as single complete semantic unit. Consequently, all tokens belonging to the same image are assigned shared and constant ID. In contrast, for text tokens, this ID increases monotonically for each subsequent token, functioning as standard 1D positional index to preserve word order. This component is equal to the original mRoPE in Qwen2-VL [75]. 2. 2D Spatial Height Coordinate (h): Represents the normalized vertical position for the image tokens. 3. 2D Spatial Width Coordinate (w): Represents the normalized horizontal position for the image tokens. For all non-image tokens, both spatial coordinates (h, w) are set to zero. The key to our design lies in how these components work in concert. For each image entitywhether serving as source or target imageits spatial coordinates (h, w) are independently computed from (0, 0). This ensures that tokens at corresponding positions share identical spatial embeddings, which strongly encourages consistency and the preservation of unmodified regions during editing. While the spatial coordinates are locally defined, the unique Sequence and Modality Identifier idseq provides an unambiguous mechanism to distinguish between these different image entities. This holistic design seamlessly degrades to standard 1D positional embedding for text-only inputs, making our M-RoPE flexible and robust framework that effectively supports the full spectrum of multimodal operations. 2.4 Training Strategy The MLLM is initialized with Qwen2.5-VL1, with the majority of its parameters kept frozen during training to preserve its multimodal understanding capabilities. Only the newly introduced special tokens <img> are updated. The diffusion model is trained from scratch, initially on the T2I generation task, and subsequently using mixed-task training strategy to accommodate multiple objectives. During the reflection training phase, all model parameters are unfrozen (see Figure 13), allowing the model to generate reflective textual descriptions and iteratively refine image outputs. 1https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct 5 Figure 5: In-Context Generation Dataset Construction Pipeline. The final input images are outlined with red border and the target image is marked by blue boundary."
        },
        {
            "title": "3 Dataset Construction",
            "content": "For multimodal understanding tasks, we utilize the dataset provided by LLaVA-OneVision [36]. For T2I generation, our training corpus comprises approximately 140 million open-source images sourced from Recap-DataComp [38], SAM-LLaVA [9], ShareGPT4V [10], LAION-Aesthetic [65], ALLaVA-4V [7], DOCCI [51], DenseFusion [39], JourneyDB [69], and BLIP3-o [8]. Furthermore, we incorporate 10 million proprietary images, for which we generate synthetic annotations using the Qwen2.5-VL-72B [3]. For image editing tasks, we collect publicly available datasets, including SEED-Data-Edit [20], UltraEdit [91], OmniEdit [77], PromptFix [86], and ImgEdit [83]. However, these open-source resources often suffer from suboptimal image quality, limited instruction accuracy, and insufficient task diversity. To overcome these constraints and better serve our research objectives, we have meticulously constructed new comprehensive training dataset for this study. The subsequent sections provide detailed account of our data construction pipeline. 3.1 In-Context Data The in-context image generation task [79; 82; 35; 71] focuses on extracting visual conceptsuch as specific object, identiy or individualfrom input images and accurately reproducing it within newly generated images. This task, also known as subject-driven generation [64], parallels in-context learning in large language models: the image generation model produces personalized outputs in real time based solely on the provided context, without the need for additional fine-tuning. While in-context image generation has been extensively explored due to its broad range of applications, the community still faces notable shortage of high-quality datasets tailored to this task. 3.1.1 In-Context Generation In-context generation tasks require modeling the diverse appearances of an object across different scenarios. To address this, we leverage video data, which inherently capture the same subjects under varying conditions across frames. This temporal diversity enables the construction of training pairs in which subjects remain semantically consistent but exhibit differences in pose, viewpoint, and illumination. As illustrated in Figure 5, our data pipeline begins by extracting keyframes from each video and designating base frame. Using Qwen2.5-VL-7B-Instruct [3], we identify the primary subjects within the base frame, capitalizing on the models vision-language capabilities to focus on semantically salient entities while filtering out irrelevant background objects. The subject bounding boxes are then obtained via GroundingDINO [46], conditioned on the tags generated by the vision-language model. Subsequently, SAM2 [61] is employed to segment and track identified subjects in subsequent frames, with the last valid frame containing all subjects selected to maximize appearance variation. To mitigate tracking errorssuch as the inclusion of visually similar but incorrect objectswe introduce VLM-based filtering step to ensure subject consistency. To further enhance visual diversity, FLUX.1-Fill-dev2 is used to outpaint the subject with novel background in 2https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev 6 Figure 6: In-Context Editing Dataset Construction Pipeline. The final input and target images are outlined by red and blue consistent with Figure 5. the input frame. We apply DINO [6]-based similarity filtering to discard samples where the subjects appearance deviates significantly, and Qwen2.5-VL-7B-Instruct is leveraged to assess both the semantic quality and consistency of the generated samples. Additionally, Qwen2.5-VL-7B-Instruct is used to generate concise object descriptions and detailed captions for the base image, which are then integrated into natural language instructions. The final training triplet comprises the instruction, the repainted image as input, and the original image as output, providing semantically rich and visually diverse supervision for multi-subject generation tasks. 3.1.2 In-Context Edit We further extend the in-context generation paradigm to editing tasks, introducing new task termed in-context editing, as illustrated in Figure 6. Here, the model extracts relevant elements from context image and utilizes them to edit target input image. The data source for in-context editing mirrors that of in-context generation: two frames containing the same object are selected, with one serving as the context clip and the other as the target clip. Initially, object masks for both frames are obtained using SAM2 [61]. For the context image, FLUX.1-Fill-dev is applied to generate new background for the object via outpainting, encouraging the model to focus on object-specific features. Subsequently, FLUX.1-Fill-dev is used to inpaint the target clip, removing the object while preserving the original background to create the input clip. Finally, Qwen2.5-VL-72B-Instruct [3] generates natural language description of the transformation from the input clip to the target clip, which is combined with the object description from the context clip to produce comprehensive natural language instructions. 3.2 Image Editing Data 3.2.1 Inpaint Data Most existing editing datasets are constructed using inpainting methods; however, they exhibit certain drawbacks: (1) The quality of images used is low: both due to low resolution and degradation after inpainting. (2) Editing instructions are inaccurate: previous work predefines editing instructions and uses inpainting models to generate images based on these instructions, but the inpainting models have poor instruction-following capabilities, causing mismatch between editing instructions and original-inpainted image pairs. In this work, we select small set of high-quality images from text-to-image data as our data source, applying FLUX.1-Fill-dev for inpainting. We use the inpainted images as inputs and the original images as targets to ensure high-quality target images. Additionally, we do not input instructions to the inpainting model, allowing it to fill content randomly. After obtaining image pairs, we employ MLLM to write editing instructions based on these pairs. We find that the latest MLLM(e.g., Qwen2.5-VL) excels at writing editing instructions for original-inpainted image pairs, resulting in high-accuracy editing dataset. 7 Figure 7: Create image edit pairs from video dataset. We first filter out frames belonging to different scenes to ensure contextual consistency, and then remove frames that exhibit significant changes in viewpoint. 3.2.2 Video Data Traditional inpainting methods are inherently limited in their capacity to construct diverse types of data, rendering them inadequate for tasks such as action modification, object movement, or expression changes. To address these limitations, we additionally extract editing pairs from video sources. We show the pipeline in Figure 7. Image editing tasks typically require localized modifications while preserving the integrity of the surrounding context. To construct suitable image editing pairs from videos, it is essential to identify frame pairs that exhibit only local changes. We begin by segmenting videos into distinct scenes to avoid pairing frames across discontinuous contexts. Scene boundaries are detected by analyzing average RGB pixel intensities, while rolling average of differences in the HSV color space enhances robustness to rapid motion. Within each identified scene, we extract multiple frame pairs and evaluate their differences using both DINOv2 [55] and CLIP [59]. Pairs exhibiting substantial differencesindicative of viewpoint changesor negligible differences are filtered out. Since camera viewpoints in videos often change even within single scene, further refinement is necessary. Existing approaches, such as vision-language models, are computationally expensive and prone to inaccuracies, while methods based on color histograms or pixel-level similarity are either insensitive to spatial structure or overly susceptible to noise. To address these challenges, we divide each image into multiple blocks and compare the color histograms of corresponding blocks to assess their similarity, effectively reducing the impact of noise. The proportion of similar blocks is then computed to impose spatial constraints, serving as reliable indicator of viewpoint consistency. This strategy efficiently filters out frame pairs with viewpoint changes while maintaining computational efficiency. Finally, for each retained image pair with consistent camera viewpoint, we employ Qwen2.5VL-72B-Instruct [3] to generate precise editing instructions, thereby facilitating the construction of high-quality image editing datasets. 3.3 Interleave Data 3.3.1 Interleaved Frames We initially segment videos based on detected scene transitions and extract key frames from each segment. Subsequently, we construct two types of video frame sequences, each comprising up to five frames: intra-scene interleaved sequence composed of frames within identical scene, and interscene interleaved sequence composed of frames across different scenes. Following frame sequence extraction, we annotate each pair of consecutive frames with descriptive captions using MLLM to describe changes in object actions and behaviors, variations in environment and background, and differences in object appearances. Given the substantial volume of required annotations, we employ lightweight model, Qwen2.5-VL-7B-Instruct, for this process. Consequently, we obtain 0.8 million interleaved data samples from video sources, which serve to pretrain the models capacity for processing continuous multimodal sequences. Figure 8: Overview of OmniContext benchmark. Left: Image genres included in OmniContext. Right: Example images for each genre in OmniContext. 3.3.2 Reflection Data Inspired by previous advances in test-time scaling and self-reflection of large language models [24; 29; 42], we further explore the integration of reflection capabilities into multimodal generation models and demonstrate how test-time scaling can enhance the quality of image generation. In this section, we focus on describing the construction of the reflection data for subsequent model fine-tuning. The reflection data comprise an interleaved sequence of text and images, beginning with user instruction, followed by the multimodal models generated image and step-by-step reflections on the previous generated outputs. Each reflection addresses two key aspects: 1) an analysis of the deficiencies or unmet requirements in relation to the original instruction, and 2) proposed solutions to address the previous images limitation. To construct self-reflection data, we select small subset from the training data (in the current experiment, we only use data from the text-to-image task) and generate images through the model. Subsequently, we use an MLLM to assess whether the generated images meet the instruction requirements. If the images fail to adequately follow instructions or exhibit other quality issues, the model identifies specific errors and suggests modifications. Initially, we experimented with the DSG [13] evaluation framework to assess instruction-image alignment. However, this approach frequently led to hallucinations. Later, we discovered that powerful multimodal models could handle this task directly, so we employed Doubao-1.5-pro [17] to output issues and modification suggestions. After obtaining the first round of reflections, we append the generated images and corresponding reflections to the original instructions and fine-tune the model on these data. Once training is complete, we continue inferring data (using the first round of reflection data) to obtain second round of images and corresponding reflective data. This iterative process yields multiple rounds of self-reflection data. There is currently limited research on employing reflection mechanisms to enhance image generation tasks within multimodal generative models. We hope that our present work will contribute to advancing the development of reasoning capabilities in the field of multimodal generation. After the model acquires initial reflective capabilities through training with the current data, online reinforcement learning algorithms can further enhance these capabilities, which we leave for future exploration."
        },
        {
            "title": "4 OmniContext Benchmark",
            "content": "A key objective of OmniGen2 is to enable consistent generation of specific context images provided by user. To rigorously evaluate this crucial yet under-benchmarked capability, we introduce OmniContext, new benchmark designed to assess models ability to maintain subject fidelity across different contexts. Existing in-context image generation benchmarks fall short in capturing real-world application scenarios. They do not account for scenarios with multiple input images and are limited by small number of context images and task types. DreamBench [64] only contains 30 objects and 9 Figure 9: An illustrative example of evaluating the output image in the OmniContext benchmark. 25 prompt templates, and lacks coverage of human subjects and scene-level contexts. Meanwhile, prior benchmarks utilize CLIP-I and DINO metrics to assess the quality of in-context generated images. These metrics rely on image-level similarity between inputs and outputs, making them inadequate for scenarios involving multiple subjects and lacking explainability. To bridge these gaps, we construct OmniContext using large-scale, manually collected dataset of high-quality images including personal photos, open-source images, animation stills and AI-generated images. As illustrated in Figure 8, the benchmark encompasses three distinct categories of context images Character, Object, and Scene covering an extensive range of entities and environments.Through systematic combinations of these different input image types, we establish three primary task categories (SINGLE, MULTIPLE, and SCENE) and eight fine-grained subtasks, with each subtask comprising 50 examples. The SINGLE category refers to image generation tasks conditioned on single context image containing either character of an object. The MULTIPLE category involves compositional interactions between subjects derived from multiple context images. The SCENE category pertain to involves image generation tasks conditioned on specified environmental contexts provided within the reference image. The construction of image-prompt pairs adopts hybrid approach that integrates both multi-modal large language models (MLLMs) and manual annotation. Initially, image sources are classified and filtered by MLLMs to remove low-quality samples. Subsequently, human experts curate the remaining images according to three criteria: (1) clear depiction of the main subject, (2) aesthetic quality, and (3) diversity. Prompts are first generated using GPT-4o, and then systematically filtered and refined by annotators to ensure comprehensive diversity in both semantic content and syntactic structure. To effectively evaluate in-context image generation capability and enhance the explainability of the evaluation outcomes, we employ the state-of-the-art multi-modal large language model GPT-4.1 [53] to assess the generated outputs, as shown in Figure 9. OmniContext incorporates three metrics: Prompt Following (PF), Subject Consistency (SC) and an Overall Score, which is computed as the geometric mean of PF and SC scores. Following the established methodology of VIEScore [33], we utilize GPT-4.1 to generates scores on 010 scale while simultaneously provides detailed rationales to justify its evaluations. We believe the OmniContext benchmark will serve as valuable resource for driving future research in controllable, reference-based image generation."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we conduct comprehensive evaluation of OmniGen2 to demonstrate its unified capabilities across wide spectrum of generation tasks. We begin by presenting an overall comparison against state-of-the-art models in Table 1, which covers four key domains: visual understanding, text-to-image generation, image editing, and in-context generation. Our results show that OmniGen2 10 Model # Params Understanding MMB MMMU MM-Vet Image Generation Image Editing In-context Generation GenEval DPG-Bench ImgEdit-Bench GEdit-Bench-EN Single Multiple Scene LLaVA-1.5 [45] LLaVA-NeXT [44] SDXL [56] SD3-medium [2] FLUX.1-dev [34] Instruct-P2P [5] MagicBrush [88] AnyEdit [84] Step1X-Edit [47] IC-Edit [90] UNO [79] InfiniteYou [31] Show-o [81] Janus-Pro [12] Emu3 [76] MetaQuery-XL [4] BLIP3-o 4B [8] BLIP3-o 8B [8] BAGEL [14] UniWorld-V1 [43] OmniGen [80] OmniGen 36.4 79.3 67.8 51.1 36.3 57.4 - - - - - - - - - - 7B + 1.6B 3B + 1.4B 7B + 1.4B 7B + 7B 7B + 12B 3.8B 3B + 4B - - - - - - - - - - - 75.5 58.5 83.5 78.6 83.5 85.0 83. - 79.1 - - - - - - - - - - 27.4 36.3 31.6 58.6 46.6 58.6 55.3 58.6 - 53. - - - - - - - - - - - 39.8 37.2 66.6 60.1 66.6 67.2 67.1 - 61.8 - - 0.55 0.62 0.66 - - - - - - - 0.68 0.80 0.54 / 0.66 0.80 0.81 0.84 0.82 / 0.88 0.84 0.68 0.80 / 0.86 - - 74.7 84.1 84.0 - - - - - - - 67.27 84.19 80.60 82.05 79.36 81.60 85.07 81.38 81.16 83.57 - - - - - 1.88 1.90 2.45 3.06 3.05 - - - - - - - - 3.20 3.26 2.96 3.44 - - - - - 3.68 1.86 3.21 6.70 4.84 - - - - - - - - 6.52 4.85 5.06 6.42 - - - - - - - - - - 6.72 6.05 - - - - - - 6.25 - 6.46 7.81 - - - - - - - - - - 4.48 - - - - - - - 6.02 - 5.26 7.23 - - - - - - - - - - 3.59 - - - - - - - 5.08 - 4.34 6.71 Table 1: Comparison of different models across Understanding, Generation, Editing, and In-context Generation tasks. *: The first term represents the number of parameters for text generation, while the second term corresponds to the number of parameters allocated for image generation. refers to the methods using LLM rewriter. achieves remarkable balance of strong performance across all areas, particularly excelling in In-context generation. 5.1 Visual Understanding Our visual understanding capabilities are powered by frozen, pre-trained 3-billion-parameter MLLM, specifically Qwen2.5-VL-3B-Instruct [3]. As shown in Table 1, OmniGen2 achieves solid scores of 79.1 on MMBench [48], 53.1 on MMMU [87], and 61.8 on MM-Vet [85]. This architectural choice offers two significant advantages. First, by freezing the MLLM, we ensure its strong, native understanding abilities are fully preserved, avoiding any performance degradation from training on generative tasks. Second, the compact 3B model size makes OmniGen2 significantly more lightweight than unified models built upon larger MLLM. This results in substantially lower training overhead and makes our model more efficient and accessible for practical deployment. 5.2 Text-to-Image Generation We assess OmniGen2s T2I generation capabilities on two standard benchmarks: GenEval [21], which evaluates compositional understanding, and DPG-Bench [27], which measures long prompt following. Our model demonstrates highly competitive performance, particularly when considering its resource efficiency. Qualitative visualizations are provided in Figure 10. Evaluation on GenEval. As shown in Table 2, OmniGen2 excels at generating images from complex, compositional prompts. With an LLM rewriter (OmniGen2), our model achieves an impressive overall score of 0.86. This result surpasses other powerful unified models like UniWorld-V1 (0.84) and is remarkably close to the state-of-the-art BAGEL (0.88). It is crucial to note that this near-SOTA performance is achieved with exceptional efficiency. OmniGen2 utilizes only 4B trainable parameters and was trained on just 15M T2I pairs. This is in stark contrast to BAGEL, which leverages 14B trainable parameters and massive 1600M T2I dataset. Evaluation on DPG-Bench. On the DPG-Bench benchmark  (Table 3)  , OmniGen2 achieves an overall score of 83.57, again outperforming UniWorld-V1 (81.38) and rivaling leading specialized models like SD3-medium (84.08). These strong results across two distinct benchmarks confirm that OmniGen2 is highly capable and efficient generator for both complex compositional and general-purpose long prompts. 11 Method Single object Two object Counting Colors Position Color attribution Overall SDv2.1 [63] SDXL [56] IF-XL LUMINA-Next [94] SD3-medium [2] FLUX.1-dev [34] NOVA [15] OmniGen [80] TokenFlow-XL [58] Janus [78] Janus Pro [12] Emu3-Gen [76] Show-o [81] MetaQuery-XL [4] BLIP3-o 4B [8] BLIP3-o 8B [8] BAGEL [14] BAGEL [14] UniWorld-V1 [43] UniWorld-V1 [43] OmniGen2 OmniGen2 0.98 0.98 0.97 0.92 0.99 0.99 0.99 0.98 0.95 0.97 0.99 0.99 0.98 - - - 0.99 0.98 0.99 0.98 1 0.99 0.51 0.74 0.74 0.46 0.94 0.81 0.91 0.84 0.60 0.68 0.89 0.81 0.80 - - - 0.94 0.95 0.93 0.93 0.95 0.96 0.44 0.39 0.66 0.48 0.72 0.79 0.62 0. 0.41 0.30 0.59 0.42 0.66 - - - 0.81 0.84 0.79 0.81 0.64 0.74 0.85 0.85 0.81 0.70 0.89 0.74 0.85 0.74 0.81 0.84 0.90 0.80 0.84 - - - 0.88 0.95 0.89 0.89 0.88 0.98 0.07 0.15 0.13 0.09 0.33 0.20 0.33 0.40 0.16 0.46 0.79 0.49 0.31 - - - 0.64 0.78 0.49 0.74 0.55 0.71 0.17 0.23 0.35 0.13 0.60 0.47 0.56 0. 0.24 0.42 0.66 0.45 0.50 - - - 0.63 0.77 0.70 0.71 0.76 0.75 0.50 0.55 0.61 0.46 0.74 0.67 0.71 0.68 0.55 0.61 0.80 0.66 0.68 0.80 0.81 0.84 0.82 0.88 0.80 0.84 0.80 0.86 Table 2: Evaluation of text-to-image generation ability on GenEval [22] benchmark. refers to the methods using LLM rewriter. Method Global Entity Attribute Relation Other Overall LUMINA-Next [94] SDXL [56] PlayGroundv2.5 [37] Hunyuan-DiT [40] PixArt-Σ [9] DALLE3 [52] SD3-medium [2] FLUX.1-dev [34] OmniGen [80] Show-o [81] EMU3 [76] TokenFlow-XL [58] Janus [78] Janus Pro [12] BLIP3-o 4B [8] BLIP3-o 8B [8] BAGEL [14] UniWorld-V1 [43] OmniGen2 82.82 83.27 83.06 84.59 86.89 90.97 87.90 82.1 87.90 79.33 85.21 78.72 82.33 86.90 - - 88.94 83.64 88.81 88.65 82.43 82.59 80.59 82.89 89.61 91.01 89.5 88.97 75.44 86.68 79.22 87.38 88.90 - - 90.37 88.39 88. 86.44 80.91 81.20 88.01 88.94 88.39 88.83 88.7 88.47 78.02 86.84 81.29 87.70 89.40 - - 91.29 88.44 90.18 80.53 86.76 84.08 74.36 86.59 90.58 80.70 91.1 87.95 84.45 90.22 85.22 85.46 89.32 - - 90.82 89.27 89.37 81.82 80.41 83.50 86.41 87.68 89.83 88.68 89.4 83.56 60.80 83.15 71.20 86.41 89.48 - - 88.67 87.22 90. 74.63 74.65 75.47 78.87 80.54 83.50 84.08 84.0 81.16 67.27 80.60 73.38 79.68 84.19 79.36 81.60 85.07 81.38 83.57 Table 3: Evaluation of text-to-image generation ability on DPG-Bench [27] benchmark. 5.3 Image Editing Image editing is cornerstone of OmniGen2s capabilities. We rigorously evaluate its performance across three diverse benchmarks: Emu-Edit [66], GEdit-Bench-EN [47] and ImgEdit-Bench [83]. The results collectively demonstrate that OmniGen2 achieve strong performance in instruction-based image editing. Qualitative visualizations are provided in Figure 11. As shown in Table 4, OmniGen2 exhibits an exceptional balance between edit accuracy and image preservation. On Emu-Edit, our model achieves the highest CLIP-Out score (0.309), indicating it most effectively applies the requested edits among all compared models. Concurrently, it secures the second-best scores for CLIP-I (0.876) and DINO (0.822), which measure the preservation of unedited regions. This combination highlights OmniGen2s proficiency in making precise, localized changes without disturbing the rest of the image. This strong instruction-following capability is further 12 Figure 10: Qualitative text-to-image generation by OmniGen2. Examples showcasing the models high fidelity to various text prompts and its support for diverse aspect ratios. confirmed on GEdit-Bench, where OmniGen2 achieves the second-highest Semantic Consistency (SC) score of 7.16. This leads to strong overall score of 6.41, placing it among the top-tier models. We note that the overall score is slightly held back by lower performance in the subtasks of Portrait Beautification and Text Modification, where we obtain overall scores 5.608 and 5.141, indicating lack of specific data. Nonetheless, the high SC score across general tasks proves its robust capacity to understand and execute user commands accurately. As detailed in Table 5.3, OmniGen2 establishes itself as the new state-of-the-art among open-source models on the comprehensive ImgEdit-Bench. The superior performance of BAGEL and OmniGen2 on the Action benchmark demonstrates the advantage of learning from video, which provides abundant data on dynamic actions. 5.4 In-context Generation distinguishing feature of OmniGen2 is its capcity to perform in-context generation, and we show the Qualitative visualizations in Figure 12. We introduce the OmniContext benchmark to provide"
        },
        {
            "title": "Method",
            "content": "Emu-Edit CLIP-I CLIP-Out DINO GEdit-Bench-EN PQ SC Gemini-2.0-flash [23] GPT-4o [54] Instruct-Pix2Pix [5] MagicBrush [88] AnyEdit [84] OmniGen [80] ICEdit [90] Step1X-Edit [47] BAGEL [14] UniWorld-V1 [43] OmniGen2 - - 0.856 0.877 - - 0.907 0.860 0.839 - 0. - - 0.292 0.298 - - 0.305 0.304 0.307 - 0.309 - - 0.773 0.807 - - 0.866 0.782 0.753 - 0.822 6.73 7.85 3.58 4.68 3.18 5.96 5.11 7.09 7.36 4.93 7. 6.61 7.62 5.49 5.66 5.82 5.89 6.85 6.76 6.83 7.43 6.77 6.32 7.53 3.68 4.52 3.21 5.06 4.84 6.70 6.52 4.85 6.41 Table 4: Quantitative comparison on Emu-Edit [67] and GEdit-Bench-EN [47]. For Emu-Edit, CLIP-I/DINO measure consistency with the source image, while CLIP-Out measures alignment with the caption of target image, CLIP-B/32 [59] and DINO-S/16 [6] are leveraged for feature calculation. For GEdit-Bench, SC (Semantic Consistency) evaluates instruction following, and PQ (Perceptual Quality) assesses image naturalness and artifacts. Higher scores are better for all metrics. Model GPT-4o [54] MagicBrush [88] Instruct-P2P [5] AnyEdit [84] UltraEdit [91] OmniGen [80] Step1X-Edit [47] ICEdit [90] BAGEL [14] UniWorld-V1 [43] OmniGen2 Add Adjust Extract Replace Remove Background Style Hybrid Action Overall 4.61 2.84 2.45 3.18 3.44 3.47 3.88 3.58 3.56 3.82 3. 4.33 1.58 1.83 2.95 2.81 3.04 3.14 3.39 3.31 3.64 3.06 2.9 1.51 1.44 1.88 2.13 1.71 1.76 1.73 1.7 2.27 1.77 4.35 1.97 2.01 2.47 2.96 2.94 3.40 3.15 3.3 3.47 3. 3.66 1.58 1.50 2.23 1.45 2.43 2.41 2.93 2.62 3.24 3.2 4.57 1.75 1.44 2.24 2.83 3.21 3.16 3.08 3.24 2.99 3.57 4.93 2.38 3.55 2.85 3.76 4.19 4.63 3.84 4.49 4.21 4. 3.96 1.62 1.2 1.56 1.91 2.24 2.64 2.04 2.38 2.96 2.52 4.89 1.22 1.46 2.65 2.98 3.38 2.52 3.68 4.17 2.74 4.68 4.2 1.90 1.88 2.45 2.7 2.96 3.06 3.05 3.2 3.26 3. Table 5: Comparison results on ImgEdit-Bench [83]. Overall is calculated by averaging all scores across tasks. comprehensive evaluation of existing models performance in this domain. OmniContext comprises eight subtasks, with overall scores for each subtask presented in Table 6. As the inaugural model evaluated on this benchmark, OmniGen2 establishes strong baseline, achieving an overall score of 7.18. These results show OmniGen2s proficiency in disentangling the subjects identity from its original background and re-rendering it accurately according to new textual instructions. OmniGen2 exhibits significant improvements over competing models across all task types, demonstrating superior prompt-following ability and subject consistency. The detailed metrics for each subtask are presented in Tables 7, 8 and 9. Several noteworthy observations can be drawn from these results. OmniGen2 consistently outperforms all open-source baselines across every evaluation metric, irrespective of whether the input comprises single image or multiple images. Among closed-source models, GPT-4o [54] achieves the highest scores in both the Overall and Prompt Following metrics, while Flux.1 Kontext [35] demonstrates superior performance in Subject Consistency. In contrast, Gemini-2.0-Flash [23] exhibits comparatively weaker results across these benchmarks. 5.5 Reflection We fine-tune the OmniGen2 on the reflection dataset and present the reflection capabilities in Figure 13. In the successful cases, the model effectively reflects on the initial generated image, identifies its shortcomings, and makes appropriate corrections in the second round, ultimately producing an image that accurately satisfies the given instruction. Moreover, the model demonstrates the ability to terminate the generation process at an appropriate point. Our observations indicate that the fine-tuned Figure 11: Versatile image editing with OmniGen2. The model skillfully handles wide variety of instructions, from simple object modifications to complex motion change and stylistic alterations. Method Flux.1 Kontext max [35] Gemini-2.0-flash [23] GPT-4o [54] InfiniteYou [31] UNO [79] BAGEL [14] OmniGen [80] OmniGen2 SINGLE MULTIPLE SCENE Character Object Character Object Char. + Obj. Character Object Char. + Obj. 8.48 5.06 8.90 6.05 6.60 5.48 7.21 8.05 8.68 5.17 9.01 - 6.83 7.03 5.71 7. - 2.91 9.07 - 2.54 5.17 5.65 7.11 - 2.16 8.95 - 6.51 6.64 5.44 7.13 - 3.80 8.54 - 4.39 6.24 4.68 7. - 3.02 8.90 - 2.06 4.07 3.59 6.38 - 3.89 8.44 - 4.33 5.71 4.32 6.71 - 2.92 8.60 - 4.37 5.47 5.12 7. Average - 3.62 8.80 - 4.71 5.73 4.34 7.18 Table 6: Overall score comparison of existing image generation models on our proposed OmniContext benchmark. \"Char. + Obj.\" indicates Character + Object. Method Character SINGLE Object Average Flux.1 Kontext max [35] Gemini-2.0-flash [23] GPT-4o [54] InfiniteYou [31] UNO [79] BAGEL [14] OmniGen [80] OmniGen2 PF 7.98 5.54 8.89 7.81 7.56 7.72 7.12 8.04 SC Overall PF SC Overall PF SC Overall 9.24 5.98 9.03 5.15 6.48 4.86 7.58 8. 8.48 5.06 8.90 6.05 6.60 5.48 7.21 8.05 8.78 6.17 9.40 - 7.78 8.56 7.66 8.44 8.76 5.89 8.74 - 6.65 6.06 5.04 7. 8.68 5.17 9.01 - 6.83 7.03 5.71 7.58 8.38 5.86 9.14 - 7.67 8.14 7.39 8.24 9.00 5.93 8.88 - 6.56 5.46 6.31 7. 8.58 5.11 8.95 - 6.72 6.25 6.46 7.81 Table 7: Comparison on task type SINGLE from OmniContext. Prompt Following (PF), Subject Consistency (SC), and Overall scores are reported (higher is better, ). model performs particularly well in making corrections when the initial image contains issues related to object color, quantity, or shape. However, the reflection model still exhibits significant limitations in both the reflection and correction stages. The model may over-reflect, particularly when dealing with simple instructions, generating Figure 12: Qualitative results of in-context generation and in-context edit. MULTIPLE Method Gemini-2.0-flash [23] GPT-4o [54] UNO [79] BAGEL [14] OmniGen [80] OmniGen2 PF 3.65 9.17 3.88 6.14 5.92 7.70 Character Object Char. + Obj. Average SC Overall PF SC Overall PF SC Overall PF SC Overall 3.02 9. 2.38 4.86 6.18 6.96 2.91 9.07 2.54 5.17 5.65 7.11 2.50 9.06 7.46 7.54 5.60 7.62 5.02 8. 5.86 6.10 5.46 6.88 2.16 8.95 6.51 6.64 5.44 7.13 4.26 8.34 5.10 6.74 4.64 7.56 5.80 8. 4.10 6.28 4.96 7.56 3.80 8.54 4.39 6.24 4.68 7.45 3.47 8.86 5.48 6.81 5.39 7.63 4.62 8. 4.11 5.75 5.53 7.13 2.96 8.86 4.48 6.02 5.26 7.23 Table 8: Comparison on task type MULTIPLE from OmniContext. Prompt Following (PF), Subject Consistency (SC), and Overall scores are reported (higher is better, ). SCENE Method Gemini-2.0-flash [23] GPT-4o [54] UNO [79] BAGEL [14] OmniGen [80] OmniGen2 PF 3.76 9.05 2.74 4.56 4.14 7.06 Character Object Char. + Obj. Average SC Overall PF SC Overall PF SC Overall PF SC Overall 3.33 8.88 2.50 3.94 3.42 5.94 3.02 8.90 2.06 4.07 3.59 6. 4.02 8.33 5.62 6.12 5.24 7.20 5.22 8.62 3.52 5.50 3.72 6.38 3.89 8.44 4.33 5.71 4.32 6. 2.89 8.71 5.22 5.90 5.56 7.50 4.63 8.57 3.86 5.30 4.84 6.68 2.92 8.60 4.37 5.47 5.12 7. 3.56 8.70 4.53 5.53 4.98 7.25 4.39 8.69 3.29 4.91 3.99 6.33 3.28 8.65 3.59 5.08 4.34 6. Table 9: Comparison on task type SCENE from OmniContext. Prompt Following (PF), Subject Consistency (SC), and Overall scores are reported (higher is better, ). unnecessary requirements or drawing incorrect conclusions about the image. Conversely, the model sometimes fails to revise the image based on its reflections or follows erroneous reflection instructions, resulting in degraded outputs. These limitations stem from the constrained perceptual capabilities of the current 3B-scale MLLM and insufficient reflection training data, which can introduce biases. In future work, we plan to scale up the MLLM and explore reinforcement learning approaches to enhance the models reflection capabilities."
        },
        {
            "title": "6 Limitations",
            "content": "However, we also find several limitations of OmniGen2: 16 Figure 13: Example of generation with reflection using OmniGen2. Left and middle: Successful correction via one round of reflection. Right: an example of failed reflection, where the correct answer is incorrectly judged as wrong due to over-reflection. (1) Performance Disparity Between English and Chinese Prompts. As shown in the first row of Figure 14, prompts in English generally yield better results than those in Chinese. For instance, when using Chinese prompt, the generated image exhibits minor inconsistency between input image and edited image. (2) Limited Generalization to Certain Instructions. The second row highlights OmniGen2s difficulty in modifying human body shapes, likely due to the scarcity of real-world data capturing such variations. (3) Sensitivity to Input Image Quality. As illustrated in Figure 14, the quality of the generated output is highly sensitive to the quality of the input image. When we input low-quality image (generated by adding noise to the raw image), the resulting images exhibit significant degradation, with details becoming notably blurred. Furthermore, downsampling the input image to maximum dimension of 256 pixels leads to further loss of clarity and detail, and the models ability to accurately follow generation instructions is substantially reduced. (4) Ambiguity in Multi-Image Inputs. The third row of Figure 14 demonstrates that the models performance improves when the prompt explicitly specifies the correspondence between objects and their source images (e.g., the bird from image 1 and the desk from image 2), indicating sensitivity to ambiguous multi-source instructions."
        },
        {
            "title": "7 Related Works",
            "content": "7.1 Image Generation Diffusion models have achieved remarkable success in generative modeling, particularly for highfidelity image synthesis [18; 9; 15]. Pioneering systems such as the Stable Diffusion (SD) series [62; 56; 18], DALLE [60], and Imagen [28] have demonstrated strong text-to-image capabilities, laying the foundation for controllable and scalable generation. To enhance controllability, models like ControlNet [89] and T2I-Adapter [50] introduce external condition modules, while StyleShot [19], InstructPix2Pix [5], and EMU-Edit [67] focus on fine-grained editing and instruction-driven generation. These developments highlight the increasing flexibility of diffusion models in various conditional settings. Recently, growing body of work has shifted attention toward unified image generation, which aims to handle various image generation tasks within single model [80; 49; 11; 73]. OmniGen jointly models text and images within single transformer to achieve unified image generation. UniReal treats various tasks as video generation prediction, and designs different prompts for task differentiation. Recently, several studies have introduced Chain-of-Thought reasoning to enhance image generation tasks [25; 74; 30]. However, research on reflection mechanisms in this context remains limited. 17 Figure 14: Visualization of OmniGen2s Limitations. Line 1: The model performs poorly when processing Chinese prompts and low-quality images. Line 2: The model often struggles to modify human body shapes accurately. Line 3: The model is sensitive to ambiguous instructions involving multiple image sources. ReflectionFlow [95] proposes reflection framework that integrates multiple components, including the original text-to-image (T2I) model, fine-tuned multimodal large language model (MLLM) for instruction generation, and an image editing model. Notably, ReflectionFlow requires training only on single instruction-image pairs. In contrast, OmniGen2 explores an integrated approach, enabling reflection and subsequent image regeneration within single model. Moreover, OmniGen is fine-tuned on multi-turn reflection data rather than single-turn editing data, allowing it to better capture the iterative nature of real-world image editing scenarios. 7.2 Multimodal Generation The field of multimodal generation models has rapidly evolved from modality-specific systems to unified architectures capable of both comprehending and generating content across diverse modalities. Early approaches, exemplified by Chameleon [72], pioneered token-based, early-fusion autoregressive generation for text and images. Emu2 [70] and its successor Emu3 [76] further advanced this nexttoken prediction paradigm, with Emu3 asserting that pure autoregressive approach can achieve general multimodal intelligence across text, images, and video, even eliminating the need for diffusion models. In contrast, hybrid models like Show-o [81] and Transfusion [93] integrate autoregressive text generation with discrete or continuous diffusion for images within single transformer, demonstrating unification while grappling with efficiency challenges that necessitate acceleration techniques like consistency distillation. MetaMorph [26] introduces Visual-Predictive Instruction Tuning (VPiT) to adapt LLMs for generating text and visual tokens, leveraging their inherent world knowledge and reasoning abilities. Similarly, LMFusion [68] efficiently adapts text-only LLMs for multimodal generation by using parallel, modality-specific modules and freezing text components to prevent catastrophic forgetting. MetaQueries [4] offers an efficient interface between frozen Multimodal 18 LLMs (MLLMs) and diffusion models via learnable queries, enabling knowledge-augmented image generation while preserving understanding performance. BLIP3-o [8], an open-source family of models, employs diffusion transformer to generate semantically rich CLIP image features and uses sequential pretraining strategy to optimize both understanding and generation. MoGao [41] and BAGEL [14] both focus on large-scale pre-training on interleaved data, demonstrating powerful generalization capabilities in downstream tasks."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [2] Stability AI. Sd3-medium. stable-diffusion-3-medium, 2024. https://stability.ai/news/ [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Rachel Ben-Eliyahu-Zohary, Ehud Gudes, and Giovambattista Ianni. Metaqueries: Semantics, complexity, and efficient algorithms. Artificial Intelligence, 149(1):6187, 2003. [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [7] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4vsynthesized data for lite vision-language model, 2024. [8] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. [9] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [10] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. [11] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1250112511, 2025. [12] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [13] Jaemin Cho, Yushi Hu, Jason Baldridge, Roopal Garg, Peter Anderson, Ranjay Krishna, Mohit Bansal, Jordi Pont-Tuset, and Su Wang. Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-to-image generation. In ICLR, 2024. [14] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 19 [15] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [17] Doubao. Doubao-1.5-pro. https://seed.bytedance.com/zh/special/doubao_1_5_ pro, 2025. [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [19] Junyao Gao, Yanchen Liu, Yanan Sun, Yinhao Tang, Yanhong Zeng, Kai Chen, and Cairong Zhao. Styleshot: snapshot on any style. arXiv preprint arXiv:2407.01414, 2024. [20] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. [21] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [22] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024. [23] Google. Gemini 2.0 flash. https://developers.googleblog.com/en/ experiment-with-gemini-20-flash-native-image-generation, 2025. [24] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [25] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025. [26] Agrim Gupta, Linxi Fan, Surya Ganguli, and Li Fei-Fei. Metamorph: Learning universal controllers with transformers. arXiv preprint arXiv:2203.11931, 2022. [27] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. [28] Imagen-Team-Google. Imagen 3, 2024. URL https://arxiv.org/abs/2408.07009. [29] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [30] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, PhengAnn Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025. [31] Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, and Xin Lu. Infiniteyou: Flexible photo recrafting while preserving your identity. arXiv preprint arXiv:2503.16418, 2025. [32] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [33] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation, 2023. 20 [34] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [35] Black Forest Labs. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space. 2025. [36] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [37] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. [38] Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, and Cihang Xie. What if we recaption billions of web images with llama-3? arXiv preprint arXiv:2406.08478, 2024. [39] Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, and Ling-Yu Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. 2407.08303, 2024. [40] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. [41] Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. [42] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [43] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. [44] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/. [45] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [46] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. [47] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. [48] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [49] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instruction-based image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. [50] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models, 2023. URL https://arxiv.org/abs/2302.08453. 21 [51] Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, et al. Docci: Descriptions of connected and contrasting images. arXiv preprint arXiv:2404.19753, 2024. [52] OpenAI. Dalle 3. https://openai.com/index/dall-e-3, 2024. [53] OpenAI. Gpt-4-1. https://openai.com/index/gpt-4-1, 2025. [54] OpenAI. Gpt-4o. https://openai.com/index/introducing-4o-image-generation, 2025. [55] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [56] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [57] Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, et al. Lumina-image 2.0: unified and efficient image generative framework. arXiv preprint arXiv:2503.21758, 2025. [58] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25452555, 2025. [59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [60] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [61] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [62] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [63] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [64] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. [65] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. [66] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879, 2024. 22 [67] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879, 2024. [68] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Llamafusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. [69] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. Advances in Neural Information Processing Systems, 36, 2024. [70] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. [71] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. [72] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [73] Xueyun Tian, Wei Li, Bingbing Xu, Yige Yuan, Yuanzhuo Wang, and Huawei Shen. Mige: unified framework for multimodal instruction-based image generation and editing. arXiv preprint arXiv:2502.21291, 2025. [74] Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, and Pheng-Ann Heng. Delving into rl for image generation with cot: study on dpo vs. grpo. arXiv preprint arXiv:2505.17017, 2025. [75] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [76] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [77] Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In The Thirteenth International Conference on Learning Representations, 2024. [78] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1296612977, 2025. [79] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-tomore generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. [80] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1329413304, 2025. [81] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 23 [82] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [83] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, Imgedit: unified image editing dataset and benchmark. arXiv preprint and Li Yuan. arXiv:2505.20275, 2025. [84] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2612526135, 2025. [85] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [86] Yongsheng Yu, Ziyun Zeng, Hang Hua, Jianlong Fu, and Jiebo Luo. Promptfix: You prompt and we fix the photo. arXiv preprint arXiv:2405.16785, 2024. [87] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [88] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36, 2024. [89] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 38363847, October 2023. [90] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. [91] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. [92] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. [93] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [94] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Xiangyang Zhu, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. Advances in Neural Information Processing Systems, 37:131278131315, 2024. [95] Le Zhuo, Liangbing Zhao, Sayak Paul, Yue Liao, Renrui Zhang, Yi Xin, Peng Gao, Mohamed Elhoseiny, and Hongsheng Li. From reflection to perfection: Scaling inference-time optimization for text-to-image diffusion models via reflection tuning. arXiv preprint arXiv:2504.16080, 2025."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence"
    ]
}