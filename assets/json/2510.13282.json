{
    "paper_title": "Universal Image Restoration Pre-training via Masked Degradation Classification",
    "authors": [
        "JiaKui Hu",
        "Zhengjian Yao",
        "Lujia Jin",
        "Yinghao Chen",
        "Yanye Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This study introduces a Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct a corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in a generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and a 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at https://github.com/MILab-PKU/MaskDCPT."
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 Universal Image Restoration Pre-training via Masked Degradation Classification JiaKui Hu, Zhengjian Yao, Lujia Jin, Yinghao Chen, Yanye Lu 5 2 0 2 5 ] . [ 1 2 8 2 3 1 . 0 1 5 2 : r AbstractThis study introduces Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at https://github.com/MILab-PKU/MaskDCPT. Index TermsPre-training, Degradation classification, Universal image restoration. I. INTRODUCTION"
        },
        {
            "title": "U NIVERSAL image restoration is the process of employing",
            "content": "a single model to transform low-quality (LQ) images affected by variable, mixed, and real-world degradation into high-quality (HQ) images. In recent work, deep learningbased methods [1, 2, 3, 4, 5, 6] have demonstrated superior performance and efficiency in solving universal image restoration compared to traditional techniques [7, 8]. The prevalent approaches employ degradation representations of LQ images as discriminative prompts for universal image restoration tasks, utilizing elements such as gradients [9], frequency [10], supplementary parameters [2], and features JiaKui Hu, Zhengjian Yao and Yanye Lu are with Institute of Medical Technology, Peking University Health Science Center, Peking University, Beijing, China, and also with Biomedical Engineering Department, College of Future Technology, Peking University, Beijing, China, and also with National Biomedical Imaging Center, Peking University, Beijing, China. Lujia Jin is with JIUTIAN Research, Beijing, China. Yinghao Chen is with the College of Electronic Engineering, National University of Defense Technology, Changsha, China. Corresponding Authors: Yanye Lu (yanye.lu@pku.edu.cn). Fig. 1: MaskDCPT achieves the state-of-the-art fidelity and perception in multiple restoration tasks, including all-in-one and real-world scenarios. compressed through large neural networks [1, 3, 4, 5, 11]. These degradation representations subsequently function as prompts for base restoration models, which are either fine-tuned or specifically trained for universal image restoration. Despite achieving high performance through the implementation of precise and effective prompts, these methods do not exploit the latent prior information inherent within the restoration models. Pre-training methods [12, 13, 14, 15, 16, 17, 18, 19] are adept at exploiting the latent prior information inherent within the restoration models themselves. They can activate latent discriminant information within neural networks, thereby facilitating the acquisition of universal representations and rendering the pre-trained model suitable for downstream tasks. Contrastive learning [20, 21] discovers representations by maximizing agreement across multiple augmented views of the same sample using contrastive loss [22], thus obtaining features with fine-grained discriminant information [17]. Masked Image Modeling (MIM) [18, 19, 23] extends BERTs [12] success from language to vision transformers and CNNs. MIM introduces challenging image reconstruction task through substantially high mask ratio, which requires the model to uncover the intrinsic distribution of images. Following GPTs [13, 14] success in language generation, related methods [24] are utilized in image generation. PURE [25] also successfully used pre-trained autoregressive MLLM to adapt to real-world super-resolution. However, pre-training in image restoration [26, 27, 28] is mainly confined to single-task applications or requires carefully designed fine-tuning methods. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2 This suggests that current approaches do not fully arouse the universal representations provided by extensive pre-training. It is imperative to develop pre-training framework for restoration models that can handle universal restoration tasks. In this paper, we assert that the ability for degradation classification constitutes frequently overlooked, yet salient, in restoration models. We discriminative feature inherent validate the effectiveness and robustness of neural networks in this capability. First, we examine the degradation classification capabilities of the classical [29, 30, 31] and all-in-one [2] image restoration architectures. Models with random initialization possess preliminary aptitude for degradation classification, which is subsequently refined through all-in-one restoration training, thus enabling better identification of previously unobserved degradation types. Further investigation reveals that this ability remains intact even when images are randomly masked. This observation indicates that image distribution learning based on masked modeling and degradation distribution learning based on degradation classification can coexist. Drawing upon this finding, we leverage this potential during the pre-training for universal image restoration tasks. By integrating degradation classification, restoration, and reconstruction synergistically during the pre-training phase, the models proficiency in discerning degradation is significantly enhanced. This methodology not only maintains its efficacy in image restoration, but also fosters more comprehensive pre-training process. Building on these insights, we introduce Masked Degradation Classification Pre-training (MaskDCPT) framework designed for universal image restoration tasks. This approach provides the model with strong prior knowledge about degradation discrimination by simultaneously pre-training three tasks: degradation classification, image reconstruction, and restoration. This enhances the models ability to identify degradation, supporting the learning of universal restoration representations, and making the pre-trained model suitable for downstream restoration tasks. Specifically, MaskDCPT uses an encoder-decoder structure. The encoder includes an image restoration network without the restoration head. The decoder is divided into two parts: one for degradation classification and the other for image reconstruction and restoration. The encoder transforms the input image into refined latent features. The classification decoder identifies the degradation type of the input image, while the reconstruction decoder, following the MIM design, enables both reconstruction and restoration of the input image using these features. The pre-trained encoder serves as the initialization for the restoration model during fine-tuning, greatly improving restoration performance. Experimental results show that our MaskDCPT framework significantly enhances the effectiveness of various architectures in restoration tasks, including all-in-one, mixed, and real-world degradation scenarios. Moreover, to accommodate broad spectrum of degradations present in real-world application scenarios, we curate dataset consisting of 2.5 million samples, referred to as UIR-2.5M, tailored for the universal image restoration. This dataset covers 19 degradation types and over 200 degradation levels. Experiments indicate that the restoration model trained with the UIR-2.5M dataset demonstrates superior generalization when exposed to unseen degradation. In summary, our main contributions are as follows. We validate that degradation classification is an inherent prior ability of restoration networks. This inherent capability is rapidly enhanced in restoration training and persists even after the input image is masked. We serve the degradation classification as fundamental component of pre-training. By incorporating it with mask image modeling, we devise the MaskDCPT specifically tailored for universal image restoration. MaskDCPT offers substantial performance gains and can be applied to diverse architectures and tasks. Within the 5D all-in-one restoration task, MaskDCPT achieves PSNR gain of 4.17, 4.32, 4.38, and 3.77 dB for SwinIR, NAFNet, Restormer, and PromptIR, respectively. When restoring mixed and real-world degradations, MaskDCPT provides 34.8% reduction in PIQE to the baseline method. We curate and release the largest universal image restoration dataset, UIR-2.5M. The restoration model trained with UIR-2.5M shows enhanced generalization to unseen degradation types and levels. Compared to our conference paper [32] presented at ICLR 2025, several improvements have been made in this study. The conference version segmented the pre-training into two independent stages, each assigned to degradation classification and generation capacity preservation. In this study, we accomplish the parallelization of them by integrating degradation classification with MIM, thereby enhancing training efficiency and yielding superior pre-training results. This advancement allows us to achieve the State-of-the-art (SoTA) results in 5D all-in-one restoration and mixed degradation tasks. Furthermore, we collect the dataset, UIR-2.5M, for more intricate universal image restoration. Restoration model trained with UIR-2.5M can generalize better in real-world scenarios, unseen degradation types, and levels. Finally, we expand our conference version by incorporating more references and experiments. Compared with recent methods, MaskDCPT consistently delivers superior universal image restoration performance. We further analyze how to improve MaskDCPTs pre-training performance, supported by more ablation studies. II. RELATED WORK A. Image Restoration Recent advances in deep-learning based restoration models [29, 33, 34, 31, 35, 36, 30, 37, 38, 39] have consistently demonstrated superior performance and efficiency compared to traditional techniques in the realm of single task image restoration. The proposed neural networks primarily utilize convolutional neural networks (CNNs) [40] and Transformers [41]. CNNs [42, 43, 33, 34, 30] exhibit exceptional efficacy in processing localized information within images, while Transformers [29, 31, 36, 37, 39] are adept at exploiting the local self-similarity of images through the utilization of long-range dependencies. However, these methods construct specialized models tailored to individual tasks [30, 44, 39]. Consequently, significant subset of these techniques proves insufficient to address the inherent diversities associated with image restoration [45]. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3 Universal Image Restoration is conceived for this purpose, which requires single model to handle various degradations. In early universal restoration approaches, distinct tasks are managed by decoupled learning [46] or employing different encoders [45] or decoder heads [47]. These approaches require the model to explicitly assess degradation types and select distinct network branches to address varied degradations. In recent developments, AirNet [1] employs MoCo [21], while IDR [48] formulates various physical degradation models to acquire degradation representations for comprehensive image restoration. PromptIR [2] integrates additional parameters via dynamic convolutions to facilitate universal image restoration without recourse to embedded features. DACLIP [4], MPerceiver [3], and DiffUIR [5] harness large external models [49, 50, 51] or generative priors to achieve improved performance and accommodate more tasks. Furthermore, VLUNet [52] has advanced the field by developing deep unfolding network to achieve more stable restoration results. DFPIR [53] introduces degradation-related parameter perturbations. UniRestore [54] introduces considerations for task-oriented image restoration, while UniRes [55] focuses more on complex mixed degradation. These methods integrate the modulation of external parameters [2], physical models [48], human instructions [56, 6, 52], and the high-dimensional features derived from extensive neural networks [1, 3, 4, 5, 57, 58]. However, investigation into the intrinsic potential of the image restoration model and its performance ceiling has been largely overlooked. B. Pre-training in computer vision Pre-training is way in which intrinsics prior are concealed in input samples and used to improve the performance in downstream tasks. In computer vision, it is divided into two schools: Contrastive Learning (CL) [20, 21] and Mask Image Modeling (MIM) [18, 19]. CL aligns features from positive pairs and uniforms the induced distribution of features in the hypersphere [59]. MIM learns to create before learning to understand [19]. However, it is difficult to extend to other architectures [23, 60, 61] and discards the decoder during downstream tasks, resulting in inconsistent representations between pre-training and fine-tuning [62]. Recently, many pre-training methods [47, 37, 63] have been proposed for restoration. Unfortunately, these methods use larger datasets to train larger models in single-degradation settings for pretraining. The existing SSL method [26] for image restoration works well in high-cost tasks but is inappropriate for lowcost tasks such as image denoising. RAM [28] pioneered the integration of MIM in the context of all-in-one image restoration. In contrast to them, MaskDCPTs focus lies on examining the influence of masks on the models capability to discriminate degradation. Using this as bridge, we aim to effectively merge the learning of degradation classification with the learning of image reconstruction. III. PRELIMINARY STUDY inherently can differentiate between various degradations, (2) there is degradation classification step in the early training of the restoration model, (3) this ability can be generalized in masked images. These findings inspire us to believe that optimizing these two intrinsic capabilities may significantly enhance the performance of the model in downstream restoration tasks. We conduct preliminary experiments to verify these findings, using extracted output features before the restoration head and employing k-nearest neighbor (kNN) classifier to categorize five degradation types: haze, rain, Gaussian noise, motion blur, and low-light. We randomly select 5,000 images (100 per degradation type) from the datasets: Test1200 [64] for deraining, OTS-BETA [65] for dehazing, SIDD [66] for denoising, GoPro [67] for deblurring, and LOL [68] for low-light enhancement. The images are center-cropped to maintain uniform feature dimensionality, and then the features are flattened for kNN classification. The dataset is divided into training and test sets in 2:1 ratio, ensuring an equal distribution of degradation. A. Inherent degradation classification ability As presented in Figure 2 (left) at the next page, random initialized models can achieve the accuracy of the degradation classification of 52 60 %. These models inherently possess the capability to classify degradation, highlighting that this is an intrinsic aptitude of neural networks in restoration tasks. Drawing inspiration from self-supervised pretraining methods [18, 20, 21], it is posited that enhancing this intrinsic capability can lead to improved performance on downstream tasks. The question then arises: In what ways can this inherent capability of restoration models be optimized? B. Degradation classification in restoration training We perform an additional verification of the degradation classification capabilities of models trained in three distinct (3D) all-in-one restoration tasks (haze, rain, and Gaussian noise). It should be noted that the five target degradations for classification encompass the three types of degradation used during the training phase. The results, as illustrated in Figure 2 (left), indicate that after 3D all-in-one training, the models exhibit an accuracy of 94% or higher in degradation classification, encompassing degradation types not previously encountered. This result suggests that the all-in-one image restoration training significantly improves the models ability to classify degradation. Moreover, across four distinct architectures, an increase in the number of restoration training iterations corresponded to an improvement in the models degradation classification capability. Consequently, during the training of the all-in-one restoration model, while performing restoration tasks, it simultaneously acquires the capability to discern the type of degradation present in the input image. Beyond image reconstruction, we discern another significant and foundational ability of image restoration models: degradation classification. We verify that (1) image restoration models This experiment demonstrates that the restoration training can optimize the models capacity for degradation classification. It also offers partial clarification on the success of IPT [69]. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4 Fig. 2: Degradation classification accuracy changes with restoration training iterations (left) and image mask ratio (right). The results are averaged under five random seeds. The direct employment of the all-in-one restoration task for pretraining serves to enhance the models capability in degradation classification, thereby substantially augmenting the performance on downstream tasks. C. Degradation classification in masked images We further investigate the robustness of the degradation classification capability of the restoration network when subjected to corrupted input images. Given its prevalence and simplicity of implementation, random masking is employed to simulate image damage. The results are illustrated in Figure 2 (right). When the mask ratio is kept below 50%, the ability to classify degradation remains largely unchanged. However, higher mask ratio leads to reduced classification capability. This finding indicates that the degradation classification capability of the restoration model remains notably robust, even in scenarios where the input image is masked. Inspired by MIM [18], image reconstruction can be integrated through the application of high-ratio mask combined with degradation classification during pre-training. This approach facilitates the learning of the image distribution and concurrently increases the degradation classification for subsequent restoration tasks, thereby enhancing the models restoration performance. IV. METHOD Based on aforementioned analysis, we propose the Masked Degradation Classification Pre-Training (MaskDCPT). We first introduce its overall pipeline and then introduce its specific components. Finally, we introduce the UIR-2.5M dataset that we collected, which includes 19 degradation types commonly seen in real life and 2.5 million images. A. Overall Pipeline MaskDCPT consists of an encoder that comprises restoration models [29, 31, 30, 2] without their restoration heads, and decoder that classifies the degradation of input images based on the features of the encoder. We leverage Masked Image Modeling (MIM) to facilitate the parallel execution of both the degradation classification and image reconstruction stages. For given input image xdegrad with specified degradation Dgt, the image is masking according to predetermined ratio and then processed by the encoder to extract the encoders feature set . Our decoder is equipped with two distinct heads: the reconstruction head, which serves to reconstruct the original image from , and the degradation classification head, which is tasked with determining the type of degradation from . Figure 3 illustrates this overall pipeline. B. MaskDCPT In this section, we describe the detailed training process of MaskDCPT in components. 1) Masked Encoder: Given low quality image xlq, we randomly mask the degraded images (in patch size 16 16) with mask ratio r. is 50% by default. xlq = xlq, (1) where is the mask map and is the Hadamard product. To achieve more effective degradation classification, it is crucial to extract features from deeper layers that contain richer high-level semantic information [70]. However, image restoration models typically adhere to the residual learning design concept [71]. The sole reliance on features from the deepest layer for the loss function calculation may result in gradient vanishing in the shallower layers, due to the loss of the encoders long residual connections [71] during feature extraction. To achieve balance, the features are extracted from each block in the latter part of the encoder. We define these extracted features as {Fi}, ([ 2 ] + 1, , l), where is the number of blocks in the network, and [] is the integer symbol. {Fi} = Encoder(xlq). (2) When local operators, such as convolution, are used to process input images, these operators integrate surrounding data through the process. This causes the introduction of new signals where the signals should have been masked, leading to information aliasing. To reduce this aliasing [72], submanifold convolution [73] is employed in pretraining. Discussion. The existing MIM-based restoration pretraining method RAM [28] uses pixel-level mask modeling, which improves the models ability to capture the distribution of individual pixels. However, this focus on pixel-level details might cause lack of capturing local information within images. Ablation experiments IX show that, when applying the same fine-tuning strategy, our patch-level mask pretraining outperforms RAMs pixel-level masking approach. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 5 Fig. 3: MaskDCPTs overall pipeline. First, MaskDCPT receives degraded images and implements random patch-level masking to them. Subsequently, the restoration backbone processes the masked images. Throughout this phase, network features are masked to impede the local information leakage. These masked features are then directed towards the reconstruction head for the image restoration as well as the degradation classification head for the degradation classification. After MaskDCPT, the encoder is fine-tuned for downstream restoration tasks. 2) Classification Decoder: After extracting masked multilevel features, we feed {Fi} into the degradation classification decoder (DegCls-Dec) of the lightweight decoder to classify the degradation of the input images. The details of the decoder architecture are shown in Figure 3. To better aggregate the extracted features, it is necessary to scale up to the features {Fi}. The scaling coefficient {ωi} is learnable. Then, the scaled feature = ωiFi is plugged into the i-th block in ResNet18 to classify the degradation. For stabling the training process, we replace the normalization layers in the decoder from BatchNorm to LayerNorm. ˆDgt = DegCls-Dec({F }). (3) It is crucial to note that the challenge of obtaining image restoration data [74] results in an imbalance in the number of data sets that represent different types of degradation. For example, the deraining dataset Rain200L [75] comprises only 200 images, whereas the dehazing dataset RESIDE [65] encompasses 72,135 images. This imbalance poses significant long-tail challenge in classifying degradation. To address this issue, we employ Focal Loss [76] as the loss function for long-tail degradation classification. Lcls = Focal Loss(Dgt, ˆDgt). (4) 3) Reconstruction Decoder: Another MaskDCPTs task is to enable the restoration model to learn clean image distributions by reconstructing masked images. The reconstruction decoder (Recon-Dec) in the decoder allows the encoders feature Fl to reconstruct ˆxgt, as shown in Figure 3. The overall loss function of MaskDCPT is as follows: Ltotal = αLpix + Lcls = αxgt ˆxgt1 + Focal Loss(Dgt, ˆDgt), (5) where α is 1 by default, and ˆxgt = Recon-Dec(Fl). Discussion. Eq. 5 performs the simultaneous execution of three tasks: degradation classification, image reconstruction, and image restoration. This is because Lpix analyzes both the masked and unmasked regions. The former facilitates image reconstruction from unmasked regions, while the latter aids in the restoration of unblemished images from unmasked areas. As image restoration still enhances the degradation classification, it is posited that the losses in Eq. 5 are mutually reinforced, thus fostering an expedited and more robust pre-training. Furthermore, within masking, Lcls in Eq. 5 acts as bridge between MIM and CL. Unlike the ill-posed property of image restoration, degradation classification has clear and welldefined objective. Under the training objective defined by Eq. 5, the model learns to extract information from partially masked inputs. When images undergo the same degradation but are masked differently, they should still be classified into the same degradation category, indicating convergence of their learned representations. In contrast, images with different degradations, even if masked by the same mask, should be classified into JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Fig. 4: The structure of the UIR-2.5M dataset. It consists of two principal categories, namely single and mixed. Single degradation types contain: blur, weather, noise, compression, and others (suboptimal imaging conditions). Mixed degradation tasks comprise combined distortions resulting from adverse weather, JPEG artifacts, motion blur, and low-light. Our dataset encompasses both synthetic and real-world data instances. The comprehensive dataset includes 2.5 million samples. The detailed distribution of the UIR-2.5M dataset is presented in the Appendix. different degradation categories, reflecting divergent features. This behavior aligns with the core principles of CL. C. Data collection To address comprehensive range of degradations encountered in real-world scenarios, an ideal approach would involve training on large dataset that includes various degradations and features images rich in texture detail. However, since degraded images and their perfectly registered clean images cannot coexist in real environments, it is huge challenge to construct paired general image restoration dataset from real-world data. While generating simulated data is relatively straightforward task and allows for the creation of complex mixed degradations not easily captured in real-world scenarios, synthetic datasets lack the diversity and realism to effectively train models capable of generalizing to demanding realworld environments. practical strategy involves curating and filtering existing datasets, followed by preprocessing them into standardized format conducive to research applications. Therefore, we carefully selected the available training datasets to ensure maximum coverage of different types of degradation and image textures. Table 4 provides summary of our curated real and synthetic training datasets, categorized by degradation. Following the aforementioned operations, comprehensive collection of 2,482,988 pairs of universal image restoration datasets designated as UIR-2.5M has been assembled, encompassing single (1,774,975) and mixed (708,013) segments. it is noted that in To enhance applicability in practice, both segments, proportion of 3% of the data are sourced from the real-world. Fields such as face, remote sensing, medical imaging, and document remain unexplored and are thus earmarked for future work in the collation of image restoration data within those specific sub-fields. Additionally, local degradation, such as reflection, flare, and incompleteness, has yet to be addressed, with plans to focus on these challenges in future work. V. EXPERIMENTS AND RESULTS Our evaluation of MaskDCPT encompasses three distinct scenarios: (1) All-in-one. We fine-tune single model after MaskDCPT to facilitate image restoration across various degradations, assessing its performance on both 5D all-inone and 12D tasks. (2) Single-task. Following IDR [48], we assess the performance of all-in-one trained models in unseen or real-world degradations without fine-tuning. To elucidate the impact of MaskDCPT on single-task pretraining, we present the fine-tuning results of MaskDCPT pretrained models within particular single-task contexts. (3) Mixed degradation. We perform an evaluation of the fine-tuned model under mixed degradation conditions to determine the suitability of MaskDCPT to restore complex degraded images with mixed degradation. Metrics. Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index Metric (SSIM) within the sRGB color space JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7 Method AirNet [1] IDR [48] AdaIR [57] DA-CLIP [4] RCOT [77] DA-RCOT [78] MoceIR [79] DFPIR [53] SwinIR [29] + RAM [28] + DCPT [32] + MaskDCPT (Ours) NAFNet [30] + DCPT [32] + MaskDCPT (Ours) Dehazing Deraining Denoising Deblurring Low-Light 21.04 / 0.884 / 0.077 / 62.52 32.98 / 0.951 / 0.058 / 50.12 30.91 / 0.882 / 0.102 / 78.12 24.35 / 0.781 / 0.189 / 66.13 18.18 / 0.735 / 0.122 / 116.9 25.24 / 0.943 / 0.052 / 33.25 35.63 / 0.965 / 0.043 / 45.62 31.60 / 0.887 / 0.092 / 66.24 27.87 / 0.846 / 0.178 / 40.83 21.34 / 0.826 / 0.108 / 100.6 30.25 / 0.981 / 0.013 / 13.11 37.86 / 0.981 / 0.014 / 13.75 31.30 / 0.892 / 0.110 / 46.64 28.11 / 0.864 / 0.189 / 19.59 22.94 / 0.894 / 0.120 / 52.41 29.78 / 0.968 / 0.014 / 15.26 35.65 / 0.962 / 0.022 / 22.24 30.93 / 0.885 / 0.089 / 54.12 27.31 / 0.838 / 0.143 / 23.34 21.66 / 0.828 / 0.095 / 55.81 30.26 / 0.971 / 0.016 / 16.74 36.88 / 0.975 / 0.024 / 19.67 31.05 / 0.882 / 0.099 / 62.12 28.12 / 0.862 / 0.155 / 21.56 22.76 / 0.830 / 0.097 / 61.24 30.96 / 0.975 / 0.008 / 10.62 37.87 / 0.980 / 0.012 / 12.20 31.23 / 0.888 / 0.082 / 37.65 28.68 / 0.872 / 0.135 / 12.39 23.25 / 0.836 / 0.084 / 47.23 30.72 / 0.979 / 0.013 / 13.28 38.01 / 0.982 / 0.014 / 13.63 31.34 / 0.893 / 0.103 / 42.93 30.04 / 0.901 / 0.143 / 15.11 23.00 / 0.902 / 0.118 / 49.77 31.23 / 0.982 / 0.013 / 13.48 37.56 / 0.979 / 0.016 / 14.71 31.26 / 0.892 / 0.091 / 38.75 28.79 / 0.879 / 0.164 / 17.07 23.79 / 0.895 / 0.122 / 56.35 21.50 / 0.891 / 0.069 / 82.13 30.78 / 0.923 / 0.081 / 64.38 30.59 / 0.868 / 0.122 / 79.08 24.52 / 0.773 / 0.288 / 56.21 17.81 / 0.723 / 0.159 / 146.2 28.45 / 0.975 / 0.021 / 10.19 26.09 / 0.875 / 0.209 / 92.90 31.06 / 0.888 / 0.110 / 39.95 26.88 / 0.823 / 0.249 / 36.31 21.55 / 0.876 / 0.156 / 89.10 35.70 / 0.975 / 0.022 / 12.10 31.16 / 0.890 / 0.113 / 40.00 26.42 / 0.810 / 0.270 / 37.17 20.38 / 0.836 / 0.154 / 68.46 28.68 / 0.977 / 0.019 / 8.93 31.13 / 0.890 / 0.080 / 27.41 26.53 / 0.808 / 0.218 / 29.23 21.94 / 0.905 / 0.111 / 55.69 37.16 / 0.979 / 0.014 / 7.60 29.29 / 0.981 / 0.015 / 5. 25.23 / 0.939 / 0.053 / 32.68 35.56 / 0.967 / 0.050 / 43.57 31.02 / 0.883 / 0.139 / 49.57 26.53 / 0.808 / 0.206 / 49.12 20.49 / 0.809 / 0.141 / 127.9 35.68 / 0.973 / 0.021 / 12.73 31.31 / 0.886 / 0.106 / 41.88 29.22 / 0.886 / 0.153 / 15.54 23.52 / 0.855 / 0.113 / 44.57 29.47 / 0.976 / 0.015 / 4.26 26.31 / 0.888 / 0.071 / 25.88 39.92 / 0.986 / 0.008 / 4.21 31.40 / 0.978 / 0.012 / 3.39 31.41 / 0.894 / 0.076 / 26.55 31.40 / 0.920 / 0.092 / 7.61 Restormer [31] + DCPT [32] + MaskDCPT (Ours) 24.09 / 0.927 / 0.067 / 43.62 34.81 / 0.962 / 0.050 / 51.69 31.49 / 0.884 / 0.108 / 40.79 27.22 / 0.829 / 0.191 / 32.02 20.41 / 0.806 / 0.144 / 123.1 36.62 / 0.977 / 0.019 / 11.65 31.20 / 0.890 / 0.105 / 44.73 28.58 / 0.875 / 0.170 / 18.31 23.26 / 0.842 / 0.120 / 59.28 29.19 / 0.976 / 0.018 / 6.47 31.29 / 0.892 / 0.076 / 26.50 30.58 / 0.910 / 0.102 / 11.12 26.11 / 0.879 / 0.076 / 30.33 39.27 / 0.985 / 0.009 / 4.87 32.67 / 0.985 / 0.010 / 3.12 PromptIR [2] + RAM [28] + DCPT [32] + MaskDCPT (Ours) 25.20 / 0.931 / 0.034 / 28.13 35.94 / 0.964 / 0.049 / 40.42 31.17 / 0.882 / 0.120 / 43.71 27.32 / 0.842 / 0.133 / 36.29 20.94 / 0.799 / 0.148 / 118.3 28.11 / 0.888 / 0.178 / 79.38 31.08 / 0.889 / 0.109 / 42.79 28.00 / 0.862 / 0.183 / 18.36 24.45 / 0.907 / 0.120 / 51.45 29.63 / 0.975 / 0.014 / 5.64 31.27 / 0.891 / 0.110 / 46.10 28.86 / 0.880 / 0.164 / 17.61 23.09 / 0.840 / 0.128 / 60.42 37.18 / 0.979 / 0.016 / 9.75 30.93 / 0.982 / 0.012 / 3.89 31.30 / 0.892 / 0.079 / 27.67 29.99 / 0.900 / 0.111 / 11.81 26.30 / 0.881 / 0.078 / 29.53 39.12 / 0.985 / 0.009 / 4.94 32.71 / 0.985 / 0.009 / 3.07 TABLE I: 5D all-in-one image restoration results in terms of PSNR / SSIM / LPIPS / FID. Classic restoration models pre-trained with MaskDCPT outperform the methods that require all-in-one specific training and architecture. All methods are trained on widely used 5D all-in-one restoration dataset following IDR [48] to ensure fair comparison. are employed to quantify image distortions. In addition, Learned Perceptual Image Patch Similarity (LPIPS) and Frechet Inception Distance (FID) are utilized as perceptual metrics. For test sets lacking reference HQs, the Perception-based Image Quality Evaluator (PIQE) and the Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) serve as evaluation metrics. We use the pyiqa 1 to calculate them. A. All-in-one image restoration"
        },
        {
            "title": "We first assess the performance gain of MaskDCPT on",
            "content": "different architectures in all-in-one image restoration. 5D all-in-one dataset. To facilitate fair comparison, subset of UIR-2.5M was meticulously crafted for the 5D all-in-one task. This subset, termed as the UIR-2.5M-5D subset, comprises the following: Rain200L, consisting of 200 training images for the purpose of deraining; RESIDE, which includes 72,135 training images alongside 500 test images (SOTS) designated for dehazing; BSD400 and WED, collectively offering 5,144 training images for Gaussian denoising; GoPro, featuring 2,103 training images and 1,111 test images intended for single image motion deblurring; and LOL, which provides 485 training images accompanied by 15 test images for the low-light enhancement. 12D all-in-one dataset. For the 12D all-in-one task, we use the UIR-2.5M-single for training. In order to comprehensively assess the effectiveness of the restoration model across diverse simulated and real-world conditions, we employ the following datasets for evaluation: MSPFN 5 sets (Rain100L, Rain100H, Test100, Test1200, Test2800), SynRain-13k for deraining; SOTS for dehazing; Snow100K-L for desnowing; RainDS and RainDrop for raindrop removal; LoL v1, LoL 1https://github.com/chaofengc/IQA-PyTorch v2, LSRW for low-light enhancement; GoPro, HIDE, REDS for deblurring; DPDD for defocus deblurring; Urban100 for Gaussian denoising, deblurring, and demosaicing; SIDD for real-world captured denoising; and RDNet for demoire. Implementation details. During MaskDCPT, image restoration models are trained by AdamW optimizer with zero weight decay for 100k iters with batch-size 16 on 256 256 image patches on 4 NVIDIA L40S GPUs. Due to the heterogeneous encoder-decoder design, we employ different learning rates for the encoder and decoder. The learning rate is set to 3 104 for the encoder and to 1 104 for the decoder. The learning rate does not alter during MaskDCPT. After MaskDCPT, the encoder is used to initialize the image restoration models. Dataset sampler. For degradation with fewer training data, we use repeat sampler technology to ensure that there are enough training pairs for each degradation. For 5D all-in-one image restoration, the repetition ratio is [1H, 300RS, 15GN, 5MB, 60LL]. For 12D all-in-one image restoration, the repetition ratio is [4GN, 4RN, 1MB, 20DB, 1GB, 4J, 5H, 8RS, 180RD, 5S, 4MS, 30LL, 6MR]. The above abbreviations used to represent degradation can be found in the abbrev. in Figure 4. 5D all-in-one image restoration results are reported in Table and Figure 5. (1) The models pre-trained using MaskDCPT exhibit superior performance compared to the specifically designed all-in-one image restoration architecture across most tasks. In the low-light enhancement task, MaskDCPT-NAFNet achieves an improvement of 2.52 dB over DFPIR and 3.37 dB over AdaIR in terms of PSNR. Additionally, it outperforms IDR [48] by 3.53 dB and DA-RCOT [78] by 2.72 dB in the motion deblurring task. (2) Regarding the multi-stage training method (IDR), MaskDCPT consistently demonstrates performance improvements. When Restormer [31] serves as the baseline, the performance gain achieved by IDR is confined JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST"
        },
        {
            "title": "PromptIR",
            "content": "Dehazing (GT) MaskDCPT-SwinIR MaskDCPT-NAFNet MaskDCPT-Restormer MaskDCPT-PromptIR"
        },
        {
            "title": "PromptIR",
            "content": "Deraining (GT) MaskDCPT-SwinIR MaskDCPT-NAFNet MaskDCPT-Restormer MaskDCPT-PromptIR"
        },
        {
            "title": "PromptIR",
            "content": "Denoising (GT) MaskDCPT-SwinIR MaskDCPT-NAFNet MaskDCPT-Restormer MaskDCPT-PromptIR"
        },
        {
            "title": "PromptIR",
            "content": "Deblurring (GT) MaskDCPT-SwinIR MaskDCPT-NAFNet MaskDCPT-Restormer MaskDCPT-PromptIR"
        },
        {
            "title": "PromptIR",
            "content": "Low-Light (GT) MaskDCPT-SwinIR MaskDCPT-NAFNet MaskDCPT-Restormer MaskDCPT-PromptIR Fig. 5: Visual comparison on 5D all-in-one image restoration datasets. Zoom in for best view. to 0.74 dB relative to its base method, whereas MaskDCPT provides an average performance gain of 4.38 dB. (3) MaskDCPT exhibits adaptability to wide range of architectures. Observations indicate that regardless of whether the network employs CNN or Transformer architecture, and whether it follows linear [29] or UNet-like structure [30, 31, 2], MaskDCPT consistently delivers an average performance enhancement of 3.77 dB and above in the 5D all-in-one image restoration task. (4) MaskDCPT demonstrates superiority over existing pre-training methods. Compared to PromptIR [2] models pre-trained using the MaskDCPT and RAM [28] frameworks, those pre-trained with MaskDCPT show significant performance improvements. Specifically, in the dehazing task, MaskDCPT-PromptIR surpasses RAM-PromptIR by 3.08 dB and DCPT-PromptIR by 1.78 dB, respectively. 12D all-in-one image restoration results. Furthermore, the degradation types are scaled up to 12 to determine the efficacy of MaskDCPT in the presence of greater number of degradation types. Following DACLIP [4] and InstructIR [56], we use NAFNet as the basic restoration model due to its JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9 Type Type Method Method Deraining PSNR / SSIM LPIPS / FID Method Method Dehazing PSNR / SSIM LPIPS / FID Method Method Desnowing PSNR / SSIM LPIPS / FID Task Specific DGUNet Restormer 30.99 / 0.913 31.76 / 0.924 0.095 / 32.94 0.082 / 28.89 Dehamer MB-Talor 34.93 / 0.989 37.54 / 0.993 0.008 / 18.11 0.005 / 2. SFNet Focal-Net All in One InsturctIR [56] DA-CLIP [4] UniRestore [54] FoundIR [80] DCPT [32] MaskDCPT (Ours) 29.12 / 0.891 29.95 / 0.902 27.87 / 0.864 29.72 / 0.890 30.50 / 0.909 31.93 / 0.922 0.100 / 34.98 0.071 / 24.90 0.125 / 39.53 0.094 / 33.25 0.077 / 27.25 0.056 / 20.46 MaskDCPT (Ours) InsturctIR [56] DA-CLIP [4] UniRestore [54] FoundIR [80] DCPT [32] 26.90 / 0.952 28.19 / 0.965 27.91 / 0.904 30.12 / 0.974 30.40 / 0.976 31.82 / 0.982 0.178 / 44.03 0.069 / 8.12 0.093 / 10.35 0.013 / 4.94 0.015 / 4.29 0.011 / 3.19 MaskDCPT (Ours) InsturctIR [56] DA-CLIP [4] UniRestore [54] FoundIR [80] DCPT [32] 30.21 / 0.907 29.97 / 0.903 28.20 / 0.858 28.26 / 0.868 28.78 / 0.863 29.64 / 0.895 30.42 / 0.907 30.51 / 0.909 0.086 / 3.34 0.090 / 3. 0.108 / 4.83 0.088 / 2.10 0.099 / 3.89 0.082 / 3.36 0.089 / 3.08 0.073 / 1.84 Type Type Method Method Raindrop PSNR / SSIM LPIPS / FID Method Method Low-light Enhance PSNR / SSIM LPIPS / FID Method Method Motion Deblur PSNR / SSIM LPIPS / FID Task Specific IDT UDR-S2Former 24.55 / 0.803 27.33 / 0.815 0.198 / 56.04 0.340 / 44. GLARE LLFlow-SKF 19.57 / 0.766 22.60 / 0.660 0.183 / 50.12 0.194 / 58.53 Stripformer DiffIR 30.43 / 0.902 30.53 / 0.898 0.119 / 8.80 0.128 / 9. All in One InsturctIR [56] DA-CLIP [4] UniRestore [54] FoundIR [80] DCPT [32] MaskDCPT (Ours) 21.19 / 0.761 24.03 / 0.772 20.57 / 0.721 21.10 / 0.757 20.32 / 0.751 27.57 / 0.838 0.275 / 109.5 0.180 / 54.91 0.333 / 113.2 0.275 / 107.3 0.253 / 108.2 0.124 / 26.83 MaskDCPT (Ours) InsturctIR [56] DA-CLIP [4] UniRestore [54] FoundIR [80] DCPT [32] 21.42 / 0.752 19.91 / 0.709 9.55 / 0.276 19.67 / 0.688 19.54 / 0.646 24.35 / 0. 0.208 / 55.69 0.198 / 52.03 0.493 / 113.4 0.234 / 50.77 0.262 / 59.14 0.168 / 34.83 MaskDCPT (Ours) InsturctIR [56] DA-CLIP [4] UniRestore [54] FoundIR [80] DCPT [32] 27.85 / 0.847 26.25 / 0.822 26.29 / 0.807 27.10 / 0.827 27.68 / 0.856 29.83 / 0.884 0.194 / 15.43 0.177 / 15.45 0.194 / 20.95 0.169 / 17.15 0.199 / 16.46 0.127 / 8.68 Type Type Method Method Defocus Deblur PSNR / SSIM LPIPS / FID Method Method JPEG Removal PSNR / SSIM LPIPS / FID Method Method Real Denoising PSNR / SSIM LPIPS / FID Task Specific NRKNet DRBNet 26.11 / 0.817 25.72 / 0.806 0.223 / 43.96 0.182 / 39.37 SwinIR Restormer 29.83 / 0.897 32.71 / 0.960 0.084 / 8.20 0.043 / 2. Restormer Uformer 39.93 / 0.947 39.80 / 0.946 0.198 / 47.24 0.200 / 47.15 All in One InsturctIR [56] DA-CLIP [4] UniRestore [54] FoundIR [80] DCPT [32] MaskDCPT (Ours) 23.84 / 0.746 23.55 / 0.747 22.91 / 0.724 23.45 / 0.742 25.68 / 0.816 25.64 / 0. 0.329 / 84.88 0.288 / 67.54 0.364 / 91.59 0.358 / 89.21 0.216 / 42.59 0.183 / 38.49 MaskDCPT (Ours) InsturctIR [56] DA-CLIP [4] UniRestore [54] FoundIR [80] DCPT [32] 31.93 / 0.944 30.77 / 0.923 30.23 / 0.918 31.43 / 0.930 31.89 / 0.947 32.02 / 0.944 0.061 / 3.77 0.079 / 5.58 0.080 / 6.40 0.059 / 3.46 0.050 / 3.08 0.039 / 2.83 MaskDCPT (Ours) InsturctIR [56] DA-CLIP [4] UniRestore [54] FoundIR [80] DCPT [32] 35.45 / 0.881 34.18 / 0.838 35.41 / 0.835 37.12 / 0.888 37.07 / 0.881 38.68 / 0. 0.356 / 57.45 0.301 / 62.47 0.247 / 56.00 0.266 / 46.53 0.282 / 51.03 0.152 / 29.48 Type Type Method Method Gaussian Deblur PSNR / SSIM LPIPS / FID Task Specific SwinIR Restormer 32.91 / 0.918 33.47 / 0.930 0.077 / 2.34 0.064 / 2.21 Method Method SwinIR GRL-S Demosaic PSNR / SSIM LPIPS / FID 39.94 / 0.994 41.77 / 0.996 0.006 / 1.03 0.004 / 0.66 Method Method SwinIR RDNet Demoire PSNR / SSIM LPIPS / FID 24.89 / 0.888 26.16 / 0.941 0.100 / 28.73 0.091 / 23. All in One InsturctIR [56] DA-CLIP [4] UniRestore [54] FoundIR [80] DCPT [32] MaskDCPT (Ours) 31.37 / 0.884 30.89 / 0.867 30.77 / 0.871 32.90 / 0.915 32.06 / 0.906 33.28 / 0.927 0.113 / 6.04 0.128 / 6.45 0.130 / 6.11 0.073 / 2.59 0.087 / 2.81 0.057 / 2.13 MaskDCPT (Ours) InsturctIR [56] DA-CLIP [4] UniRestore [54] FoundIR [80] DCPT [32] 37.08 / 0.977 38.12 / 0.990 37.99 / 0.990 38.44 / 0.992 38.11 / 0.991 38.59 / 0. 0.011 / 2.33 0.006 / 1.07 0.006 / 1.21 0.005 / 0.84 0.006 / 1.05 0.005 / 0.72 MaskDCPT (Ours) InsturctIR [56] DA-CLIP [4] UniRestore [54] FoundIR [80] DCPT [32] 24.69 / 0.843 24.75 / 0.826 24.06 / 0.819 24.71 / 0.876 24.18 / 0.815 25.21 / 0.942 0.111 / 32.18 0.134 / 38.71 0.155 / 45.28 0.107 / 32.49 0.159 / 31.38 0.095 / 24.41 TABLE II: 12D all-in-one image restoration results in terms of PSNR / SSIM / LPIPS / FID. All-in-one network pre-trained with MaskDCPT outperforms task-specific methods in terms of fidelity for deraining, desnowing, raindrop removal, and low-light enhancement. In most restoration tasks, it surpasses task-specific methods in terms of perceptual metrics. All the all-in-one methods are trained on UIR-2.5M to ensure fair comparison. precision. The performance of the restoration model under 12 degradation is presented in Table II. It can be observed that, (1) compared to abstract CLIP embeddings [4], complex human instructions [56], and the large diffusion model [80, 54], the degradation classification prior to the MaskDCPT-trained model is more effective in addressing the complex all-in-one restoration task. In the context of motion deblurring, the MaskDCPT framework demonstrates an improvement of 1.98 dB, 3.58 dB, and 3.54 dB in PSNR metrics compared to InsturctIR, DA-CLIP, and UniRestore, respectively, while achieving reduction of 50% in FID metrics. Moreover, MaskDCPT achieves stateof-the-art performance across all other assessed all-in-one tasks. (2) The restoration model trained with MaskDCPT demonstrates superior performance over previous task-specific methods in terms of both fidelity and perceptual quality. For instance, in desnowing task, MaskDCPT surpasses FocalNet by 0.54 dB in PSNR; in low-light enhancement, it exceeds GLARE by 4.78 dB in PSNR. MaskDCPT also exhibits advances over task-specific approaches in perceptual assessments. For the real image denoising task, MaskDCPT achieves 37.4% reduction in the FID compared to Uformer; and in the raindrop removal task, it obtains 63.5% reduction in LPIPS relative to UDR-S2Former. (3) The universal restoration method performs similarly to task-specific approaches under global degradation. However, for non-uniform degradation such as haze, motion blur, or defocus blur, task-specific methods perform better. B. Single-task image restoration further analysis is conducted to determine the suitability of MaskDCPT for single-task image restoration pre-training from two perspectives. i. Zero-shot (ZS): This evaluates whether MaskDCPT trained models under 12D all-in-one fine-tuning are JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10 Method AdaIR [57] DA-RCOT [78] MoceIR [79] DFPIR [53] SwinIR-5D + RAM [28] + DCPT [32] + MaskDCPT (Ours) NAFNet-5D + DCPT [32] + MaskDCPT (Ours) + MaskDCPT-12D (Ours) Restormer-5D + DCPT [32] + MaskDCPT (Ours) PromptIR-5D + RAM [28] + DCPT [32] + MaskDCPT (Ours) Urban100 Kodak24 BSD ID OOD ID OOD ID OOD σ = 15 σ = 25 σ = 50 σ = 60 σ = 75 σ = 15 σ = 25 σ = 50 σ = 60 σ = 75 σ = 15 σ = 25 σ = 50 σ = 60 σ = 75 34.10 33.95 33.99 33.94 32.79 33.77 33.64 33.83 33.14 33.64 34.11 33.86 33.72 34.14 34.17 33.27 33.64 33.88 34. 31.68 31.29 31.58 31.59 30.18 31.41 31.14 31.39 30.64 31.23 31.80 31.49 31.26 31.79 31.81 30.85 31.27 31.49 31.79 28.28 26.36 28.21 28. 26.52 27.95 27.63 27.91 27.20 27.98 28.63 28.23 28.03 28.58 28.53 27.41 27.90 28.15 28.52 26.63 22.03 26.86 26.08 24.47 24.93 24.36 25. 25.74 26.30 27.03 27.28 25.98 26.31 26.94 25.74 25.94 26.71 26.91 22.60 16.83 23.45 21.45 19.80 20.56 20.04 21.31 19.93 20.13 20.79 25. 21.89 22.12 23.81 19.22 19.65 22.90 23.84 34.88 34.73 34.85 34.77 33.89 34.51 34.63 34.65 34.27 34.72 34.92 34.75 34.78 34.96 34. 34.44 34.46 34.78 34.81 32.39 31.96 32.37 32.32 31.32 32.10 32.11 32.15 31.80 32.28 32.49 32.31 32.37 32.49 32.36 31.95 32.01 32.30 32. 29.22 26.93 29.20 29.20 27.93 28.90 28.86 28.93 28.62 29.21 29.42 29.17 29.08 29.40 29.20 28.71 28.81 29.14 29.20 27.39 21.82 27.72 26. 25.36 26.03 25.98 26.15 25.92 27.09 27.49 28.28 26.91 27.34 27.57 26.53 26.96 27.58 27.56 23.15 16.23 23.84 21.38 20.01 20.85 20.54 21. 18.08 19.99 20.31 26.89 23.68 24.08 24.30 19.41 19.80 23.52 24.51 34.01 33.84 33.98 33.94 33.31 33.63 33.82 33.78 33.67 33.94 34.03 33. 34.03 34.09 33.91 33.85 33.68 33.96 33.92 31.34 30.91 31.34 31.29 30.59 31.06 31.16 31.13 31.02 31.31 31.41 31.29 31.49 31.46 31. 31.17 31.08 31.32 31.30 28.06 25.95 28.06 28.05 27.13 27.80 27.86 27.85 27.73 28.12 28.21 28.04 28.11 28.25 28.06 27.89 27.81 28.08 28. 26.47 21.62 26.65 25.85 24.39 24.95 24.49 25.51 25.90 26.32 26.58 27.16 25.31 26.33 26.60 24.49 24.96 25.93 26.62 22.83 16.29 23.26 21. 20.11 20.82 20.29 21.25 19.42 19.84 20.17 25.86 22.97 23.77 23.93 19.14 19.84 21.77 23.91 TABLE III: [ZS] Gaussian denoising results in five levels, including in-domain (σ = (15, 25, 50)) and out-of-domain (σ = (60, 75)) degradation levels. MaskDCPT improves performance for in-domain (ID) degradation levels. With scaling degradation types and levels in training data, the restoration model can generalize better to out-of-domain (OOD) degradation levels. Input (LQ) MoceIR [79] DFPIR [53] MaskDCPT-5D MaskDCPT-12D Target (HQ) (Ours) (Ours) Fig. 6: Visual comparison on out-of-domain (OOD) scenarios (Gaussian denoising, σ = 75). The MaskDCPT-12D is the only method that effectively removes noise while avoiding the introduction of extraneous artifacts. used to solve single tasks without optimization. ii. Fine-tuning (FT): This assesses whether the model weights pre-trained with MaskDCPT can be directly used for fine-tuning on single-task image restoration. [ZS] Implementation details. In zero-shot (ZS) settings, we evaluate the performance of the all-in-one models pre-trained with MaskDCPT on the following: (1) trained tasks at unseen degradation levels, specifically Gaussian denoising in Urban100, Kodak24, and BSD68 datasets. (2) Unseen degradation type within unseen real-world scenarios, including RealBlur-R for motion-debluring, CUHK and PixelDP for defocus-debluring, RealRain1K for deraining, Snow100k-real for desnowing, RTTS for dehazing, along with DICM, LIME, MEF, NPE, and VV for low-light enhancement. These real-world datasets have no reference HQ data. [FT] Implementation details. In fine-tuning (FT) configurations, we train the Restormer [31] model using the Rain13K dataset for image deraining and the GoPro dataset for single image motion deblurring, facilitating fair comparison with DegAE [26]. The training hyperparameters utilized remain consistent with those employed by Restormer [31]. The key variation lies in the utilization of MaskDCPT pre-trained parameters for model initialization. The fine-tuning process is executed on single NVIDIA A100 GPU. [ZS] Unseen degradation levels: Gaussian denoising. Table III and Figure 6 elucidates the Gaussian denoising results of the image restoration model pre-trained with MaskDCPT across various noise levels, including those degradations not encountered during the training phase. (1) The model pretrained with MaskDCPT evidences substantial improvements across all architectures and testsets, with particular emphasis on the high-resolution dataset Urban100 [81]. Specifically, MaskDCPT-SwinIR exhibits an enhancement of 1.39 dB over SwinIR in Gaussian denoising with σ = 50. (2) MaskDCPT displays marked superiority over existing pre-training methods. Compared to the PromptIR models pre-trained by MaskDCPT and RAM, those pre-trained with MaskDCPT exhibit significant performance enhancements. Notably, within the σ = 50 and the high-resolution dataset Urban100 [81], MaskDCPT-PromptIR exceeds RAM-PromptIR by 0.97 dB and DCPT-PromptIR by 0.2 dB, respectively. (3) Following exposure to broader spectrum of degradations, the model demonstrates considerable progress in addressing unseen synthesized levels. In particular, MaskDCPT-NAFNet-12D outperforms MaskDCPT-NAFNet5D by 5.69 dB in unseen Gaussian noise coefficients, e.g., 75. This performance is attributed to the diverse noise types included within UIR-2.5M, which augment the models ability to comprehend and mitigate complex noise phenomena. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 11 Degradation Method Motion Blur Defocus Blur Rain Snow Haze Low-light PSNR / SSIM / LPIPS / FID PIQE / BRISQUE PSNR / SSIM / LPIPS / FID PIQE / BRISQUE PIQE / BRISQUE PIQE / BRISQUE DA-CLIP [4] InstructIR [56] UniRestore [54] FoundIR [80] DCPT-NAFNet [32] MaskDCPT-NAFNet (Ours) 12.54 / 0.280 / 0.471 / 89.81 34.07 / 0.948 / 0.079 / 24.04 30.89 / 0.878 / 0.125 / 42.07 29.12 / 0.832 / 0.146 / 43.46 24.78 / 0.772 / 0.195 / 53.92 32.21 / 0.907 / 0.090 / 22. 39.48 / 34.98 35.31 / 32.21 31.15 / 27.53 47.40 / 41.74 38.75 / 37.71 28.61 / 30.19 33.24 / 0.939 / 0.102 / 63.87 35.80 / 0.964 / 0.096 / 63.83 32.54 / 0.905 / 0.141 / 91.49 36.02 / 0.967 / 0.093 / 64.36 32.82 / 0.914 / 0.159 / 79.27 37.02 / 0.978 / 0.070 / 57.37 31.34 / 24.45 33.35 / 24.41 32.69 / 27.16 33.18 / 26.20 32.59 / 25.02 30.06 / 23.27 47.67 / 34.90 50.97 / 31.45 46.88 / 30.95 61.14 / 42.26 52.40 / 37.97 33.98 / 33.21 37.64 / 27.45 36.08 / 26.31 34.63 / 27.05 44.17 / 33.51 35.48 / 26.97 28.50 / 24.76 TABLE IV: [ZS] Real-world restoration results in six real-world degradation types. Input (LQ) UniRestore [54] FoundIR [80] DCPT [32] MaskDCPT (Ours) Fig. 7: Visual comparison on real-world restoration scenarios. Zoom in for best view. Dataset Method DeblurGAN DeblurGANv2 SRN DMPHN Restormer DegAE-Restormer [26] DCPT-Restormer [32] MaskDCPT-Restormer (Ours) GoPro HIDE PSNR SSIM PSNR SSIM 28.70 0.858 24.51 0.871 29.55 0.934 26.62 0.875 30.26 0.934 28.36 0. 31.20 0.940 29.09 0.924 32.92 0.961 31.22 0.942 33.03 (+0.11) - 31.43 (+0.21) - 33.12 (+0.20) 0.962 31.47 (+0.25) 0.946 (+0.04) 33.29 (+0.37) 0.964 (+0.03) 31.55 (+0.33) 0.946 (+0.04) Dataset Method SIRR MSPFN LPNet AirNet Restormer DegAE-Restormer [26] DCPT-Restormer [32] MaskDCPT-Restormer (Ours) Test100 PSNR SSIM 32.37 0.926 33.50 0. 33.61 0.958 34.90 0.977 36.74 0.978 35.39 (-1.35) 0.972 (-0.06) 37.24 (+0.50) 0.980 (+0.02) 37.70 (+0.96) 0.984 (+0.06) TABLE V: [FT] Single Image Motion Deblurring results in the single-task setting on the GoPro dataset. Image Deraining results in the single-task setting on the Test100 dataset. [ZS] Unseen degradation types: real-world blur and weather. Table IV and Figure 7 illustrate the superior generalization capabilities of MaskDCPT in real-world scenarios, significantly outperforming all-in-one restoration methodologies. According to the quantitative metrics, MaskDCPT attained the majority of the state-of-the-art results, notably achieving the lowest Frechet Inception Distance (FID) in the motion blur task, and delivered superior performance across the other five real-world environments. The visual output delineates that (1) the methods grounded in degradation classification (DCPT [32] and our MaskDCPT) are adept at eliminating small disturbances, e.g., rain and snow from images, unlike the Diffusion-based methods [54, 80]. (2) The integration of mask processing enhances the models capacity to discern and ameliorate localized degradations. Although DCPT [32] is effective in globally removing rain and snow, it cannot address local low-light conditions. Our MaskDCPT adeptly resolves this problem by accurately illuminating these regions. More visual comparisons are shown in the supplementary. [FT] Single-task degradation: motion blur and rain. MaskDCPT is suitable for pre-training on single task. Table shows that Restormer pre-trained with MaskDCPT outperforms 0.37 dB on GoPro. MaskDCPT remains an appropriate approach for pre-training on image deraining tasks. In contrast, DegAE [26] exhibits reduced performance in image deraining. MaskDCPT exhibits greater universality. C. Image restoration on mixed degradation Dataset. We use the UIR-2.5M-mixed as training dataset, and test our model on CDD and LoL-Blur. The testset comprises prevalent degradation combinations, including low-light, haze, rain-streaks, and snow. We conduct evaluations exclusively on three-mixed degradation to illustrate the advantages of MaskDCPT in restoring intricate degradation mixtures. Implementation details. We use the NAFNet pre-trained by MaskDCPT on 5D all-in-one restoration datasets to initialize model. We use the AdamW optimizer with the initial learning rate 3 104 gradually reduced to 1 106 with the cosine annealing schedule to train our image restoration models. The training runs for 750k iters with batch size 32 on 4 NVIDIA L40 GPUs. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 12 Method + + RS + + + + 25.86 / 0.797 / 0.210 / 25.12 25.22 / 0.800 / 0.205 / 28.87 26.45 / 0.862 / 0.147 / 11.48 DACLIP [4] 24.84 / 0.777 / 0.233 / 28.71 24.32 / 0.760 / 0.279 / 40.33 26.33 / 0.860 / 0.163 / 17.31 InstructIR [56] OneRestore [6] 25.18 / 0.799 / 0.165 / 24.85 25.28 / 0.802 / 0.148 / 24.90 25.02 / 0.788 / 0.227 / 34.32 25.41 / 0.801 / 0.213 / 30.04 25.40 / 0.802 / 0.208 / 28.51 24.28 / 0.753 / 0.317 / 43.28 MoceIR [79] 25.76 / 0.817 / 0.188 / 23.01 25.90 / 0.819 / 0.174 / 23.56 25.56 / 0.829 / 0.181 / 23.69 DCPT-NAFNet [32] MaskDCPT-NAFNet (Ours) 26.24 / 0.811 / 0.151 / 22.80 26.38 / 0.814 / 0.143 / 22.90 27.13 / 0.881 / 0.131 / 10. TABLE VI: Mixed degraded image restoration results on CDD [6] and LoL-Blur [82] in terms of PSNR / SSIM / LPIPS / FID. All methods are trained on UIR-2.5M-mixed to ensure fair comparison. Input (LQ) MoceIR [79] DCPT [32] MaskDCPT (Ours) Target (HQ) Fig. 8: Visual comparison on mixed degradation scenarios. MaskDCPT can restore the illumination globally. Mask ratio (%) Dehazing Deraining Denoising Debluring Low-light Patch size Dehazing Deraining Denoising Debluring Low-light 0 25 75 30.93 31.93 32.71 32.66 37. 38.84 39.12 39.08 31.27 31.28 31. 31.24 28.86 29.10 29.99 29.84 23. 25.71 26.30 26.00 1 4 32 29.33 31.99 32.71 32.17 36. 39.03 39.12 38.88 31.09 31.28 31. 31.29 27.89 29.74 29.99 29.68 23. 26.11 26.30 25.75 TABLE VII: Ablations of the mask ratio. TABLE VIII: Ablations of masked patch size. Results of restoration on mixed degradation are displayed in Table VI. MaskDCPT can deliver substantial performance enhancements to the restoration model in mixed degradation scenarios. Compared with OneRestore [6], MaskDCPT demonstrates PSNR improvement of 1.06 dB for mixed degradations involving low-light, haze and rain degradation, and 2.11 dB for those involving low-light, blur, and noise degradation. For compelling evidence, Figure 8 provides visual comparison of image restoration in three composite degradation samples (low-light + haze + rain). NAFNet pre-trained with MaskDCPT can restore more natural result from mixed-degraded image and fully preserve image texture and detail such as lighting and building textures. D. Ablation studies Our conference paper [32] has demonstrated the necessity of decoder architecture, multi-scale feature extraction, training stages, and pre-training it self in DCPT through the performance of several ablation experiments. We hereby comprehensively analyze our newly added mask mechanism. The ablations are performed with PromptIR [2] in the 5D all-in-one image restoration task, in terms of PSNR . Impact of mask ratio. As shown in Table VII, it was observed that selecting mask ratio of 50% optimizes restoration performance. In contrast, when the mask ratio is reduced to 0, MaskDCPT reverts to DCPT [32], thus losing its ability to train simultaneously for degradation discrimination and image reconstruction, resulting in notable performance decline. Impact of masked patch size. Refer to Table VIII, patch size of 16 is optimal for MaskDCPT. In instances where the patch size is adjusted to 1, the masking approach aligns with RAM [28]. Our findings indicate that employing patch size of 1 during pre-training with degradation classification can notably diminish restoration performance. This occurs because pixellevel masks disrupt the distribution of degradation information throughout the image, thereby impeding the models ability to effectively detect degradation, which ultimately impacts the restoration performance. Masking method Dehazing Deraining Denoising Debluring Low-light square block-wise random 32.18 32.22 32.71 38.94 39. 39.12 30.84 30.98 31.30 29.07 29. 29.99 25.82 25.90 26.30 TABLE IX: Ablations of masking methods. Impact of masking methods. Following SimMIM [19], we conducted ablations involving variety of masking methods. As evidenced by the results presented in Table IX, the random masking strategy exerts optimal performance in the 5D all-inone image restoration task. This observation can be attributed to the inherently pixel-intensive nature of image restoration tasks, which necessitate the models proficiency in processing various image regions. The application of random masks improves the models ability to fit the distribution of pixels between disparate image regions, thus markedly increasing restoration performance. E. Discussions Restoration performance as the degradation classification accuracy changes. The results demonstrate direct correlation between enhancements in the accuracy of degradation classification during pre-training and subsequent improvements in the JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13 Method MaskDCPT iterations SwinIR [29] Initial DC Acc. (%) 0 54 25k 50k 75k 100k 69 82 89 94 PSNR (dB) 25.04 26.10 27.58 28.44 29.21 NAFNet [30] Restormer [31] PromptIR [2] Initial DC Acc. (%) 90 95 97 98 PSNR (dB) 27.77 29.40 30.95 31.88 32. Initial DC Acc. (%) 60 92 97 99 PSNR (dB) 27.60 29.34 30.76 31.63 31.98 Initial DC Acc. (%) 56 90 97 98 PSNR (dB) 28.11 29.77 30.43 31.50 31.88 TABLE X: All-in-one restoration performance improved as the degradation classification accuracy increased. The PSNR are averaged among 5 tasks in 5D all-in-one restoration. DC donates the degradation classification. networks all-in-one restoration performance. As delineated in Table X, there is notable improvement in the performance of restoration models, concomitant with an increase in the initial degradation classification accuracy. This correlation implies that the effectiveness of MaskDCPT is largely attributable to its facilitation of degradation classification prior to the initiation of restoration training. Types Methods w/o MaskDCPT MaskDCPT 5D 9D 12D Restormer [31] + instructs [56] + frequency [57] + MoE [79] Restormer [31] + instructs [56] + frequency [57] + MoE [79] Restormer [31] + instructs [56] + frequency [57] + MoE [79] 27.60 / 0.112 30.11 / 0.083 30.09 / 0.089 30.62 / 0. 27.14 / 0.139 29.67 / 0.094 29.20 / 0.118 29.46 / 0.101 26.88 / 0.196 29.03 / 0.140 28.26 / 0.173 28.71 / 0.159 31.98 / 0.055 31.93 / 0.056 31.99 / 0.055 32.07 / 0.050 31.37 / 0.058 31.40 / 0.059 31.32 / 0.061 31.43 / 0.056 30.79 / 0.061 30.80 / 0.061 30.74 / 0.063 30.91 / 0.058 TABLE XI: 5D (H, RS, GN, MB, LL) restoration performance in terms of PSNR / LPIPS of degradation-aware architectures as influenced by training methods and scaled degradation types. What do degradation-aware architectures bring to? MaskDCPT has been shown to be highly effective in improving model performance. Furthermore, baseline model trained with MaskDCPT demonstrates superior performance compared to degradation-aware architectures. We wonder: What do degradation-aware architectures bring to restoration performance? We examined variations in the restoration performance of three degradation-aware architectures: instruction [56], frequency [57], and Mixture of Experts (MoE) [79] as influenced by changes in training methods and increased degradation types. Restormer [31] serves as the baseline model. The experimental results are presented in Table XI. (1) When training models from scratch, degradation-aware architectures can provide specific performance enhancements. (2) However, as the types of degradation increase, the performance of the model experiences varying degrees of decline. Both MaskDCPT and degradationaware architectures can mitigate this performance decline. From network architecture perspective, the instruction-based methodology emerges as the most effective means of alleviating this performance decline. (3) After training with MaskDCPT, degradation-aware architectures consistently approximate the baseline performance. It suggests that such designs may not be able to increase the model performance ceiling after fully converged training. VI. CONCLUSION This paper first validates that randomly initialized restoration models achieve baseline degradation classification performance while preserving robustness with masked input images. Furthermore, models trained for all-in-one restoration exhibit superior classification accuracy. To enhance this efficacy and robustness, we introduce MaskDCPT and experimentally demonstrate its effectiveness in universal image restoration. By integrating degradation classification priors with image distribution learning, MaskDCPT enhances 3-4 dB PSNR gain in all-in-one restoration and 35 % less PIQE in real-world scenarios for restoration models. In addition, we gather an extensive dataset for universal image restoration, UIR-2.5M, which is able to improve the generalization of restoration models when addressing unseen degradation. In future work, efforts will be directed to improve the generalization of restoration models in the presence of unseen and complex degradation."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Boyun Li, Xiao Liu, Peng Hu, Zhongqin Wu, Jiancheng Lv, and Xi Peng. All-in-one image restoration for unknown corruption. In CVPR, 2022. [2] Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, and Fahad Shahbaz Khan. Promptir: Prompting for allin-one image restoration. NeurIPS, 2023. [3] Yuang Ai, Huaibo Huang, Xiaoqiang Zhou, Jiexiang Wang, and Ran He. Multimodal prompt perceiver: Empower adaptiveness generalizability and fidelity for allin-one image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2543225444, 2024. [4] Ziwei Luo, Fredrik Gustafsson, Zheng Zhao, Jens Sjolund, and Thomas Schon. Controlling visionlanguage models for universal image restoration. In The Twelfth International Conference on Learning Representations, 2023. [5] Dian Zheng, Xiao-Ming Wu, Shuzhou Yang, Jian Zhang, Jian-Fang Hu, and Wei-shi Zheng. Selective hourglass mapping for universal image restoration based on diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [6] Yu Guo, Yuan Gao, Yuxu Lu, Huilin Zhu, Ryan Wen Liu, and Shengfeng He. Onerestore: universal restoration In European framework for composite degradation. Conference on Computer Vision, pages 255272. Springer, 2025. [7] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-D transform-domain collaborative filtering. TIP, 2007. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14 [8] Jianchao Yang, John Wright, Thomas Huang, and Yi Ma. Image super-resolution via sparse representation. IEEE transactions on image processing, 19(11):2861 2873, 2010. [9] Cheng Ma, Yongming Rao, Yean Cheng, Ce Chen, Jiwen Lu, and Jie Zhou. Structure-preserving super resolution with gradient guidance. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 77697778, 2020. [10] Xiaozhong Ji, Guangpin Tao, Yun Cao, Ying Tai, Tong Lu, Chengjie Wang, Jilin Li, and Feiyue Huang. Frequency consistent adaptation for real world super resolution. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 16641672, 2021. [11] Cong Wang, Jinshan Pan, Wei Wang, Jiangxin Dong, Mengzhu Wang, Yakun Ju, and Junyang Chen. Promptrestorer: prompting image restoration method with degradation perception. Advances in Neural Information Processing Systems, 36, 2024. [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, 2019. [13] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [14] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [15] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:2127121284, 2020. [16] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. [17] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96409649, 2021. [18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1600016009, 2022. [19] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: In simple framework for masked image modeling. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 96539663, 2022. [20] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PMLR, 2020. [21] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. [22] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. [23] Keyu Tian, Yi Jiang, Chen Lin, Liwei Wang, Zehuan Yuan, et al. Designing bert for convolutional networks: Sparse and hierarchical masked modeling. In The Eleventh International Conference on Learning Representations, 2022. [24] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pages 16911703. PMLR, 2020. [25] Hongyang Wei, Shuaizheng Liu, Chun Yuan, and Lei Zhang. Perceive, understand and restore: Real-world image super-resolution with autoregressive multimodal generative models. arXiv preprint arXiv:2503.11073, 2025. [26] Yihao Liu, Jingwen He, Jinjin Gu, Xiangtao Kong, Yu Qiao, and Chao Dong. Degae: new pretraining In Proceedings of the paradigm for low-level vision. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2329223303, June 2023. [27] Haoyu Chen, Jinjin Gu, Yihao Liu, Salma Abdel Magid, Chao Dong, Qiong Wang, Hanspeter Pfister, and Lei Zhu. Masked image training for generalizable deep image denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1692 1703, 2023. [28] Chu-Jie Qin, Rui-Qi Wu, Zikun Liu, Xin Lin, Chun-Le Guo, Hyun Hee Park, and Chongyi Li. Restore anything with masks: Leveraging mask image modeling for blind all-in-one image restoration. In European Conference on Computer Vision, pages 364380. Springer, 2024. [29] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1833 1844, 2021. [30] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun. Simple baselines for image restoration. In Shai Avidan, Gabriel Brostow, Moustapha Cisse, Giovanni Maria Farinella, and Tal Hassner, editors, Computer Vision ECCV 2022, pages 1733, Cham, 2022. Springer Nature Switzerland. [31] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 restoration. In CVPR, 2022. [32] JiaKui Hu, Lujia Jin, Zhengjian Yao, and Yanye Lu. Universal image restoration pre-training via degradation classification. arXiv preprint arXiv:2501.15510, 2025. [33] Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, and Chengpeng Chen. Hinet: Half instance normalization In Proceedings of the network for image restoration. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 182192, 2021. [34] Yiqun Mei, Yuchen Fan, and Yuqian Zhou. Image superresolution with non-local sparse attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 35173526, June 2021. [35] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxim: Multi-axis mlp for image processing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 57695780, 2022. [36] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: general u-shaped transformer for image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 17683 17693, June 2022. [37] Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. Activating more pixels in image superresolution transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2236722377, June 2023. [38] Lujia Jin, Qing Guo, Shi Zhao, Lei Zhu, Qian Chen, Qiushi Ren, and Yanye Lu. One-pot multi-frame denoising. International Journal of Computer Vision, 132(2):515 536, 2024. [39] JiaKui Hu, Zhengjian Yao, Lujia Jin, Hangzhou He, and Yanye Lu. Enhancing image restoration transformer via adaptive translation equivariance. arXiv preprint arXiv:2506.18520, 2025. [40] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436444, 2015. [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [42] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 136144, 2017. [43] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In ECCV, 2018. [44] Xiaoyu Xiang Yawei Li, Yuchen Fan, Denis Demandolx, Rakesh Ranjan, Radu Timofte, and Luc Van Gool. Efficient and explicit modelling of image hierarchies for image restoration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. [45] Ruoteng Li, Robby Tan, and Loong-Fah Cheong. All in one bad weather removal using architectural search. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 31753185, 2020. [46] Qingnan Fan, Dongdong Chen, Lu Yuan, Gang Hua, Nenghai Yu, and Baoquan Chen. general decoupled learning framework for parameterized image operators. TPAMI, 2019. [47] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer, 2021. [48] Jinghao Zhang, Jie Huang, Mingde Yao, Zizheng Yang, Hu Yu, Man Zhou, and Feng Zhao. Ingredient-oriented multi-degradation learning for image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 58255835, 2023. [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language In International conference on machine supervision. learning, pages 87488763. PMLR, 2021. [50] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [51] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [52] Haijin Zeng, Xiangming Wang, Yongyong Chen, Jingyong Su, and Jie Liu. Vision-language gradient descent-driven all-in-one deep unfolding networks. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 75247533, 2025. [53] Xiangpeng Tian, Xiangyu Liao, Xiao Liu, Meng Li, and Chao Ren. Degradation-aware feature perturbation for allin-one image restoration. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 28165 28175, 2025. [54] Chen, Wei-Ting Chen, Yu-Wei Liu, Yuan-Chun Chiang, Sy-Yen Kuo, Ming-Hsuan Yang, et al. Unirestore: Unified perceptual and task-oriented image restoration model using diffusion prior. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17969 17979, 2025. [55] Mo Zhou, Keren Ye, Mauricio Delbracio, Peyman Milanfar, Vishal Patel, and Hossein Talebi. Unires: Universal image restoration for complex degradations. arXiv preprint arXiv:2506.05599, 2025. [56] Radu Timofte Marcos V. Conde, Gregor Geigle. Highquality image restoration following human instructions, 2024. [57] Yuning Cui, Syed Waqas Zamir, Salman Khan, Alois Knoll, Mubarak Shah, and Fahad Shahbaz Khan. Adair: Adaptive all-in-one image restoration via frequency mining and modulation. arXiv preprint arXiv:2403.14614, 2024. [58] Xu Zhang, Jiaqi Ma, Guoli Wang, Qian Zhang, Huan JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 16 Zhang, and Lefei Zhang. Perceive-ir: Learning to perceive degradation better for all-in-one image restoration. IEEE Transactions on Image Processing, 2025. [59] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and In International conuniformity on the hypersphere. ference on machine learning, pages 99299939. PMLR, 2020. [60] Peng Gao, Teli Ma, Hongsheng Li, Ziyi Lin, Jifeng Dai, and Yu Qiao. Mcmae: Masked convolution meets masked autoencoders. Advances in Neural Information Processing Systems, 35:3563235644, 2022. [61] Man Yao, Xuerui Qiu, Tianxiang Hu, Jiakui Hu, Yuhong Chou, Keyu Tian, Jianxing Liao, Luziwei Leng, Bo Xu, and Guoqi Li. Scaling spike-driven transformer with efficient spike firing approximation training. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [62] Qi Han, Yuxuan Cai, and Xiangyu Zhang. Revcolv2: Exploring disentangled representations in masked image In Thirty-seventh Conference on Neural modeling. Information Processing Systems, 2023. [63] Wenbo Li, Xin Lu, Shengju Qian, and Jiangbo Lu. On efficient transformer-based image pre-training for lowlevel vision. In Edith Elkind, editor, Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23, pages 10891097. International Joint Conferences on Artificial Intelligence Organization, 8 2023. Main Track. [64] Wenhan Yang, Robby Tan, Jiashi Feng, Zongming Guo, Shuicheng Yan, and Jiaying Liu. Joint rain detection and removal from single image with contextualized deep networks. TPAMI, 2019. [65] Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, and Zhangyang Wang. Benchmarking single-image dehazing and beyond. TIP, 2018. [66] Abdelrahman Abdelhamed, Stephen Lin, and Michael Brown. high-quality denoising dataset for smartphone cameras. In CVPR, 2018. [67] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In CVPR, 2017. [68] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. In British Machine Vision Conference, 2018. [69] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12299 12310, 2021. [70] Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiangwen Kong, Jun Li, and Xiangyu Zhang. Reversible column In International Conference on Learning networks. Representations, 2023. [71] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond Gaussian denoiser: Residual IEEE learning of deep CNN for image denoising. Transactions on Image Processing, 26(7):31423155, 2017. [72] Keyu Tian, Yi Jiang, Qishuai Diao, Chen Lin, Liwei Wang, and Zehuan Yuan. Designing bert for convolutional networks: Sparse and hierarchical masked modeling. arXiv preprint arXiv:2301.03580, 2023. [73] Benjamin Graham and Laurens Van der Maaten. Submanifold sparse convolutional networks. arXiv preprint arXiv:1706.01307, 2017. [74] Yawei Li, Kai Zhang, Jingyun Liang, Jiezhang Cao, Ce Liu, Rui Gong, Yulun Zhang, Hao Tang, Yun Liu, Denis Demandolx, et al. Lsdir: large scale dataset for image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17751787, 2023. [75] Wenhan Yang, Robby Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, and Shuicheng Yan. Deep joint rain detection and removal from single image. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 13571366, 2017. [76] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 29802988, 2017. [77] Xiaole Tang, Xin Hu, Xiang Gu, and Jian Sun. Residual-conditioned optimal transport: towards structurepreserving unpaired and paired image restoration. In International Conference on Machine Learning, pages 4775747777. PMLR, 2024. [78] Xiaole Tang, Xiang Gu, Xiaoyi He, Xin Hu, and Jian Sun. Degradation-aware residual-conditioned optimal transport IEEE Transactions on for unified image restoration. Pattern Analysis and Machine Intelligence, 2025. [79] Eduard Zamfir, Zongwei Wu, Nancy Mehta, Yuedong Tan, Danda Pani Paudel, Yulun Zhang, and Radu Timofte. Complexity experts are task-discriminative learners for any image restoration. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 12753 12763, 2025. [80] Hao Li, Xiang Chen, Jiangxin Dong, Jinhui Tang, and Jinshan Pan. Foundir: Unleashing million-scale training data to advance foundation models for image restoration. arXiv preprint arXiv:2412.01427, 2024. [81] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed selfexemplars. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 51975206, 2015. [82] Shangchen Zhou, Chongyi Li, and Chen Change Loy. Lednet: Joint low-light enhancement and deblurring in the dark. In European conference on computer vision, pages 573589. Springer, 2022."
        }
    ],
    "affiliations": [
        "Biomedical Engineering Department, College of Future Technology, Peking University, Beijing, China",
        "College of Electronic Engineering, National University of Defense Technology, Changsha, China",
        "Institute of Medical Technology, Peking University Health Science Center, Peking University, Beijing, China",
        "JIUTIAN Research, Beijing, China",
        "National Biomedical Imaging Center, Peking University, Beijing, China"
    ]
}