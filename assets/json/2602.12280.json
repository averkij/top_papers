{
    "paper_title": "Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching",
    "authors": [
        "Huai-Hsun Cheng",
        "Siang-Ling Zhang",
        "Yu-Lun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the \"dual-constraint\": initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a \"common structural subspace\" valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/"
        },
        {
            "title": "Start",
            "content": "Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching Huai-Hsun Cheng Siang-Ling Zhang Yu-Lun Liu"
        },
        {
            "title": "National Yang Ming Chiao Tung University",
            "content": "6 2 0 2 2 1 ] . [ 1 0 8 2 2 1 . 2 0 6 2 : r Figure 1. Progressive semantic illusions from text. Given pair of text prompts (a), our method generates vector sketch that evolves over time. The initial generated sketch (b) depicts the first concept (e.g., pig). By adding further generated strokes (c), the drawing is transformed into totally different object (e.g., angel). This creates Stroke of Surprise: the process subverts the viewers expectation of the initial concept, triggering dramatic semantic reversal as the final strokes re-contextualize the entire composition."
        },
        {
            "title": "Abstract",
            "content": "Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, novel vector sketching task where single sketch undergoes dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the dual-constraint: initial prefix strokes must form coherent object (e.g., duck) while simultaneously serving as the structural foundation for second concept (e.g., sheep) upon adding delta strokes. To address this, we propose sequence-aware joint optimization 1 framework driven by dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover common structural subspace valid for both targets. Furthermore, we introduce novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/ (e.g., SketchAgent) employ greedy strategy, optimizing strokes solely for A. This renders the fixed prefix as semantic noise when extending to B, resulting in clutter. Crucially, these baselines fail to find Common Subspace, which is shared geometric configuration valid for both semantic interpretations. To overcome these limitations, we propose Stroke of Surprise, sequence-aware joint optimization framework designed to discover this common structural subspace  (Fig. 3)  . Unlike sequential approaches, we optimize parameters for both the prefix (Object A) and full phase (Object B) simultaneously using dual-branch Score Distillation Sampling (SDS) mechanism. This guidance ensures prefix strokes are recognizable as the initial concept yet primed for re-interpretation. Furthermore, we introduce geometric Overlay Loss to enforce spatial complementarity and prevent occlusion. This enables delta strokes to structurally integrate with and re-contextualize the prefix. For example, it can transform pig ears into angel wings, creating seamless illusion. Our main contributions are summarized as follows: Task: We introduce Progressive Semantic Illusion, extending visual illusions from the spatial to the temporal dimension. This task requires single vector sketch to reveal distinct semantic interpretations through progressive stroke accumulation. Method: We propose sequence-aware joint optimization framework that optimizes shared stroke parameters under simultaneous semantic constraints and novel Overlay Loss, ensuring the prefix strokes form robust structural foundation for the final illusion. Insight & Scalability: We demonstrate that joint optimization identifies Common Subspace that resolves conflicts between early semantic clarity and later structural integration. Our method scales to multi-phase illusions (A C) and significantly outperforms baselines in recognizability and coherence. 2. Related Work Sketch Generation and Sequential Modeling. Vector sketch synthesis evolved from category-specific to openvocabulary generation, progressing from edge detection [16, 118] and sketch datasets [29, 39, 54, 99] through RNNs [39], Transformers [17, 70, 96], GANs [34, 71], and autoregressive models [120, 130]. CLIP enabled text-driven synthesis [32, 108, 109], while diffusion-based Score Distillation [90] was adapted for SVG [52, 93, 106, 125, 127, 136, 138], with recent feed-forward [4, 22] and LLM-based methods [89, 121, 128]. Stroke ordering encodes semantics, as shown in foundational rendering [41, 42] and sequential modeling via attention [37], VAEs [39], embeddings [1], completion [71, 104], transformers [11], Bezier curves [25], temporality [55], and diffusion [114]. Stroke semantics are Figure 2. Challenges in progressive illusion sketching. (a) Rasterbased methods (e.g., Nano Banana Pro) rely on destructive editing, modifying the initial structure to fit the final target and thus violating the progressive constraint. (b) Vector-based baselines (e.g., SketchDreamer [93] or SketchAgent [110]) employ greedy strategy, where specific Phase 1 details become semantic noise or clutter in Phase 2. (c) Ours achieves dual-semantic coherency by jointly optimizing for common structural subspace, ensuring the initial strokes are valid building blocks for both interpretations (e.g., rabbit elephant). 1. Introduction Visual illusions traditionally exploit spatial ambiguities, requiring viewers to change viewpoints to perceive hidden meanings (e.g., Visual Anagrams [36]). In this work, we introduce new dimension to this artistic interplay: time. We propose Progressive Semantic Illusions, novel vector sketching task where the drawing process itself drives semantic transformation. As illustrated in Fig. 1, our method generates coherent initial sketch (e.g., pig) that is subsequently re-contextualized by additional strokes into distinct concept (e.g., an angel). This Stroke of Surprise subverts expectations, achieving perceptual shift through sequential stroke accumulation rather than spatial manipulation. Sketch generation has evolved from category-specific RNNs [39] to open-vocabulary models leveraging CLIP [94] and diffusion priors [97]. Methods like CLIPasso [108] and VectorFusion [52] utilize differentiable rasterization for highfidelity sketching, while sequential approaches like SketchAgent [110] and SketchDreamer [93] mimic step-by-step human drawing. Regarding illusions, Visual Anagrams [36] and ShadowDraw [79] explore multi-view effects via diffusion. However, these prior works focus on static pixel representations or spatial rearrangements, leaving the challenge of temporal semantic transformation in vector graphics unexplored. Generating progressive illusions presents unique DualConstraint: early strokes must depict object while simultaneously functioning as the structural foundation for object  (Fig. 2)  . Existing methods fail to address this additive nature. Raster-based models (e.g., Gemini) rely on destructive editing, overwriting initial pixels and violating the progressive constraint. Conversely, sequential vector models 2 Figure 3. Pipeline overview. Our method optimizes set of learnable stroke parameters, which are divided into prefix strokes Sprefix and delta strokes Sdelta. The optimization process involves two parallel branches. In the top branch, only the prefix strokes are rendered by differentiable rasterizer to create partial sketch (e.g., rabbit). This sketch is then guided by pre-trained, frozen text-to-image diffusion model using prompt corresponding to the prefix (a rabbit), resulting in the prefix SDS loss Lprefix SDS . In the bottom branch, the full set of strokes is rendered to create the complete sketch (e.g., horse). This is guided by the same diffusion model using prompt for the full SDS + Lfull object (a horse), resulting in the full SDS loss Lfull SDS. Gradients from this total loss are backpropagated to update all learnable stroke parameters. SDS. The total SDS guidance loss is the sum of these two terms LSDS = Lprefix explored through RL [49], optimal transport [139], feedforward [73], canvas-aware [47], style [101], recognizability [10], and hierarchical methods [135]. Human-AI collaboration includes co-creative [26], turn-taking [86], creativity [57], synchronous [64], and LLM systems [50, 110]. All prior methods target single semantic goal. We introduce dual-constraint optimization where prefix strokes forming Object are re-contextualized into Object B. Sketch Perception and Visual Illusions. Gestalt principles [111, 117], Recognition-by-Components [13, 14], illusory contours [56], and cognitive studies [18, 30] explain how sparse strokes evoke recognition. Computational approaches surpassed human benchmarks [29, 134] via RL [84, 85], primitives [2], implicit [7], dynamic [61], grouping [66], saliency [12], graphs [132], transformers [113], explainability [92], and neuroscience [103]. Visual illusions encode multiple meanings via spatial transformations: frequency [87], shadows [83], wire art [46], camouflage [24], evolution [81], and morphing [3, 8, 100]. Diffusion expanded this through view averaging [36], frequency decomposition [35], multi-task [131], phase transfer [33], 3D [31], fabrication [15], ambigrams [137], anamorphic [19, 28], neural shadows [112], cross-modal [23], sculpture [91, 119], viewpoint-dependent [59], biases [88], and predictive coding [116]. These rely on spatial transformations. We introduce temporal concealment where semantics emerge through stroke accumulation. plicit [95, 107], latent diffusion [126], interpolation [77], simplification [102], typography [51], discrete stylization [48], and transformation [123]. Score Distillation [45, 90] was adapted for vectors [52, 125] and editing [40], with improvements via coarse-to-fine [69], variational [115], noise-free [58], interval [68], DDIM [78], bridge [82], collaborative [60], likelihood score matching [20], and posterior [62] formulations. Multi-concept methods use composition [72], cross-attention [63], neurons [76], attention [21], subject-driven [98], decomposition [5], fusion [38], trainingfree [124], and guidance [53]. Multi-view leverages correspondence [105], joint modeling [75], hybrid [6], satelliteto-ground synthesis [65], and Pareto [133]. These focus on spatial composition. Our dual-branch SDS addresses temporal revelation: prefix strokes receive gradients from both targets, discovering configurations valid for two interpretations. 3. Method Progressive illusions require prefix strokes to depict an initial object while forming the structural basis for final one. We propose joint optimization framework via multi-branch Score Distillation Sampling to discover common structural subspace valid for both interpretations. Prefix strokes receive simultaneous gradients to satisfy dual roles, while an overlay loss enforces spatial separation, ensuring structural integration rather than occlusion. 3.1. Progressive Semantic illusion in Vector Form Differentiable Rendering and Score Distillation. Bezier foundations [9, 27] enabled differentiable rasterization [67], improved by splatting [74], ranking [44], layers [80], imWe partition set of learnable Bezier strokes into disjoint subsets: prefix Sprefix = {s1, . . . , sk} and delta Sdelta = {sk+1, . . . , sN }. The progressive illusion requires Sprefix to 3 Figure 4. Motivation and formulation of the overlay loss. (Top) Motivation: Without constraints, redundant strokes (b) occlude the prefix. Hard intersection (c) allows strokes to be placed arbitrarily close, causing crowding. (Bottom) Formulation: We compute soft overlay loss (f) from blurred maps (d, e). The blur expands the penalty region to create spatial buffer, forcing new strokes to maintain sufficient distance from the prefix to ensure visual clarity and separation. Figure 5. VLM-based evaluation and ranking pipeline. We employ GPT-4o to assess the quality of illusion sketches. (a) For Phase 1, the model evaluates the recognizability of the prefix sketch (Sprefix). (b) For Phase 2, the model evaluates the full sketch (Sfull) while simultaneously comparing it against the delta strokes (Sdelta). This comparison ensures that the prefix strokes provide essential structural scaffolding for the second concept, rather than being merely overwritten. High scores are awarded only when Sfull is significantly more recognizable than Sdelta alone. depict the initial concept p1, while the full sketch Sfull = depicts the target p2, achieved by delta strokes recontextualizing the prefix. We optimize stroke parameters θ such that the rasterized outputs R(Sprefix; θ) and R(Sfull; θ) align with p1 and p2, respectively. The core challenge lies in discovering configurations where prefix strokes meaningfully serve both semantic interpretations. 3.2. Joint Optimization Pipeline We employ dual-branch strategy to simultaneously refine both stroke subsets  (Fig. 3)  . Unlike sequential methods, our pipeline coordinates semantic objectives via parallel guidance on shared learnable parameters θ. We initialize strokes near the canvas center, partitioning them into Sprefix (first k) and Sdelta (remaining). At each iteration, the prefix branch renders Iprefix = R(Sprefix; θ). We apply the gradient of the Score Distillation Sampling (SDS) loss conditioned on p1: θLprefix SDS = (cid:20) w(t) (ϵϕ(zt, t, p1) ϵ) (cid:21) , zt θ (1) where zt is the noised latent, ϵϕ the noise predictor, and w(t) weighting function. Simultaneously, the full branch renders Ifull = R(Sfull; θ) SDS. We combine these graconditioned on p2, yielding θLfull dients as optimize to complement them. To prevent delta strokes from merely occluding the prefix, which is common issue with pure semantic guidance, we introduce an overlay loss that penalizes spatial overlap to enforce structural integration. 3.3. Overlay Loss for Spatial Coordination Semantic guidance alone fails to prevent spatial redundancy, often causing delta strokes to clutter prefix strokes (Fig. 4(b)). We introduce an overlay loss to enforce spatial complementarity. We render stroke subsets separately and apply Gaussian blur Gσ to create soft spatial buffers ( Iprefix, Idelta), as shown in Fig. 4(d,e). We then compute the normalized overlap: Loverlay = 2 Iprefix, Idelta Iprefix1 + Idelta , (3) where , denotes the inner product over pixel space."
        },
        {
            "title": "This constraint promotes structural",
            "content": "integration and smoother semantic transitions, ensuring prefix strokes serve as essential components rather than being obscured. The final objective is: = LSDS + λoverlayLoverlay, (4) where λoverlay weights the penalty. Gradients are backpropagated via differentiable rasterization. θLSDS = θLprefix SDS + θLfull SDS. (2) 3.4. Filtering and Ranking This ensures prefix strokes receive simultaneous gradients from both targets, satisfying dual roles, while delta strokes To ensure quality, our systematic pipeline selects the best candidates using VLM assessment and quantitative metrics. 4 overlap between S1:i and the next subset Si+1: = (cid:88) i=1 Li SDS + K1 (cid:88) i=1 overlayLi λi overlay. (8) 4. Experiments 4.1. Experimental Setup Baseline. We adapt state-of-the-art methods: Nano Banana Pro (raster), SketchAgent [110], and SketchDreamer [93] (vector). We design two protocols: (1) Text-to-illusion: Baselines generate sketches sequentially (prefix from p1, then full from p2). For the raster-based Nano Banana Pro, we enforce the progressive constraint by overlaying the prefix onto the final output, whereas vector baselines natively support stroke addition. (2) Ours-to-illusion: We provide our optimized prefix sketches as input to evaluate whether baselines can complete the transformation given an ideal structural foundation. Data. Our evaluation dataset comprises 64 common objects spanning diverse categories. We randomly sample pairs to form (p1, p2) combinations, run multiple optimization iterations per pair, then apply filtering and ranking to select top-k results for evaluation. Implementation Details We implement our framework using Stable Diffusion v1.5 for Score Distillation Sampling guidance on an NVIDIA RTX 4090 GPU. We optimize stroke parameters θ for 2,000 iterations using Adam optimizer with guidance scale 100 and overlay loss weight λoverlay = 0.1. Generation requires approximately 13 minutes for two-phase and 15 minutes for three-phase illusions. Metrics. For quantitative evaluation, we employ both standard and specialized metrics to assess illusion quality. We use CLIP score computed as the minimum across all phases to measure semantic alignment. Beyond standard metrics, we define two illusion-specific measures. Structural concealment evaluates whether prefix strokes contribute substantively to the full sketch rather than being occluded by delta strokes. For any metric {CLIP, ImageReward, HPS} [43, 122, 129], we compute: CM struct = Mfull Mdelta. Higher scores indicate prefix strokes retain significant structural roles. Semantic concealment measures whether non-current phase semantics are effectively hidden. Following [36], we compute: Csemantic = tr(softmax(S/τ )), where is the CLIP image-text similarity matrix and τ is temperature. Higher scores indicate clear phase-specific semantics. (9) We further conduct two user studies with 143 participants for additional quantitative validation. The first compares our Figure 6. Multi-phase pipeline. We scale to phases (e.g., AppleSheepEinstein) using cumulative stroke subsets (S1, . . . , SK ). Parallel branches optimize each cumulative sketch I1:i against prompt pi. Joint optimization ensures early strokes receive gradients from all subsequent losses ((cid:80) Li SDS), creating structure primed for the entire evolutionary sequence. VLM-based Quality Assessment. We employ GPT-4o to evaluate four dimensions  (Fig. 5)  . Phase recognizability and Single-object integrity ensure semantic accuracy and coherence. Illusion quality validates the prefixs structural contribution by confirming Sfull is significantly more recognizable than Sdelta alone. Sketch quality penalizes visual clutter. Each phase receives individual scores across these dimensions, and candidates failing minimum thresholds are filtered. Ranking Strategies. GPT-based ranking  (Fig. 18)  favors semantic accuracy: RGPT = ScorePhase 1ScorePhase 2. Metricbased ranking  (Fig. 17)  emphasizes perceptual contrast [79] by penalizing independent delta stroke quality: SCLIP = (CLIPp1 CLIPp2)/CLIP2 delta, SIR = Φ(IRp1)2 + Φ(IRp2)2 Φ(IRdelta)2, SHPS = HPS2 p1 + HPS2 p2 HPS2 delta, (5) (6) (7) where Φ() is the standard Gaussian CDF. The final score = SCLIP SIR SHPS ensures the prefix provides substantial structural contribution. 3.5. Extension to Multi-Phase Illusions Our framework naturally scales to K-phase illusions (A1, . . . , AK) by partitioning strokes into disjoint subsets S1, . . . , SK. Each cumulative prefix S1:i = (cid:83)i j=1 Sj renders concept Ai. We employ parallel branches  (Fig. 6)  to jointly optimize all parameters, rendering I1:i conditioned on prompt pi. This ensures early strokes (e.g., S1) receive gradients from all subsequent branches, coordinating cumulative interpretations. We extend the overlay loss to penalize Figure 7. Qualitative comparisons. We compare against SketchDreamer [93], SketchAgent [110], and Nano Banana Pro. (a) SketchDreamer produces noisy strokes, causing severe visual clutter. (b) SketchAgent yields overly abstract results with low recognizability. (c) Nano Banana Pro relies on destructive editing (e.g., overwriting the pig structure to draw an angel), failing the progressive constraint despite high image quality. (d) Ours generates clean, structurally consistent sketches where prefix strokes are creatively repurposed (e.g., rabbit whiskers becoming elephant ear). We provide additional video progressive illusion results and visualization of the optimization process in an interactive HTML in the supplementary material. Figure 8. Phase 2 extension with fixed prefix (ours). We evaluate how methods extend fixed Phase 1 sketch generated by our method. Interestingly, baselines produce better Phase 2 results here than in Fig. 7 (where they generate Phase 1 themselves). This indicates that our Phase 1 strokes inherently embed structural cues for the second concept, validating that our joint optimization successfully finds versatile common subspace. However, comparing (a-c) with (d), our method still achieves the highest success rate and structural consistency, as Sdelta is jointly optimized with the prefix rather than sequentially appended. top-1 result against baselines across five prompt pairs. The second assesses our ranking pipeline by asking participants to select satisfactory results from our top-4 outputs across four prompt pairs, evaluating both technical performance and practical user satisfaction. 4.2. Results and Analysis As shown in Tab. 1(a,c), our method substantially outperforms baselines in CLIP and concealment scores, achieving 100% coverage compared to Nano Banana Pros 34.9%. Fig. 7 and Fig. 16 highlights baseline limitations: visual clutter (SketchDreamer), oversimplification (SketchAgent), and destructive editing (Nano Banana Pro). Tab. 1(b,c) and Fig. 8 show that in the fixed-prefix setting, baselines achieve improved recognizability, suggesting our prefix strokes embed implicit structural cues (common subspace). However, they remain substantially inferior to our method across all metrics, confirming that joint optimization of the complete stroke sequence is essential for seamless integration. User Studies. Our user studies strongly reinforce these findings. In comparisons against baselines, participants se6 Table 1. Quantitative comparison. (a) Vector baselines lack quality; Nano Banana fails coverage (35%). (b) Extending our Phase 1 helps, but still lags behind (c), validating joint optimization. (c) Ours achieves top metrics with 100% coverage. Phase 1 CLIP Concealment (structural) Csemantic Coverage Method Source Avg min CLIP IR HPS CLIP (%) (a) (b) (c) SketchDreamer SketchAgent Nano Banana Pro SketchDreamer SketchAgent Nano Banana Pro Ours (GPT-ranking) Ours (Metric-ranking) - - - Ours Ours Ours 24.803 24.393 26.821 28.148 24.019 28.903 29.873 30.044 -0.393 -2.544 -2.774 0.060 -2.778 -1. 1.668 3.282 0.338 0.095 -0.663 0.302 0.080 -0.426 0.839 1.237 0.011 0.000 -0.019 0.011 0.003 -0. 0.023 0.029 0.887 0.752 0.875 0.961 0.762 0.958 0.983 0.980 100.0% 100.0% 34.9% 100.0% 100.0% 35.2% 100.0% 100.0% Figure 9. User study. (Left) Preference: Participants overwhelmingly favor our method (green) over baselines across both ranking strategies. (Right) Reliability: high success rate (97%) confirms that our pipeline consistently yields valid illusions, ensuring robustness against the inherent stochasticity of the generation process. Figure 10. Ablation on optimization strategy. (a) Sequential generation yields rigid Phase 1, creating structural conflicts (e.g., the ducks beak) that fail Phase 2 repurposing. (b) Joint optimization (Ours) identifies common structural subspace, yielding versatile Phase 1 where features serve both interpretations (e.g., the beak doubles as the cows ear). lected our method in 67.7% of GPT-ranking and 87.1% of Metric-ranking cases (Fig. 9(a)). Our ranking pipeline demonstrates strong reliability with over 98% overall satisfaction rates (Fig. 9(b)), thoroughly validating our frameworks effectiveness. Figure 11. Ablation on stroke initialization. (a, d) Scattered fails to aggregate strokes, resulting in disconnected artifacts. (c, f) Shifted yields valid sketches, proving that spatial concentration is critical for convergence, though it risks boundary cropping. (b, e) Centered (Ours) offers the optimal balance, ensuring structural integrity without clipping. 4.3. Ablation Studies Optimization Strategy We evaluate our joint optimization approach against sequential alternative that first optimizes prefix strokes for the initial concept, then fixes these and optimizes only delta strokes. As shown in Fig. 10(a), this sequential approach produces rigid structures where features conflict with the final object, causing failed transitions. In contrast, our joint optimization (Fig. 10(b)) updates both stroke sets simultaneously, discovering common structural subspace where prefix strokes represent the initial concept while integrating naturally into the final representation. This enables improved visual consistency and smooth transitions, confirming that joint optimization is essential for high-quality progressive illusion sketches. Stroke Initialization. Since our objective function is highly non-convex, initialization is critical for convergence. Fig. 11 shows that spatial concentration is paramount; scattered initialization fails to capture essential semantic features. In contrast, both centered and shifted gathered configurations succeed, indicating that local stroke density outweighs absolute position. We therefore adopt centered gathered initialization to balance density with spatial coverage, avoiding potential boundary clipping. Overlay Loss. We validate the necessity of Loverlay. As shown in Fig. 12(a), without this component, semantic guidance alone fails to prevent spatial redundancy, resulting in delta strokes that clutter the prefix. Loverlay addresses this by penalizing overlap, enforcing spatial complementarity, and significantly reducing intersection artifacts (Fig. 12(b)). Crucially, this promotes structural coherence: prefix strokes are forced to integrate naturally into the subsequent concept rather than being obscured, confirming that geometric constraints are essential for generating clean progressive illusions. Stroke Count. Optimal stroke budget depends on concept complexity  (Fig. 13)  . Simple transformations (e.g., rabbit-tohorse) succeed with minimal strokes (816), whereas complex subjects like Einstein require 3264 strokes to capture essential details; insufficient budgets compromise recognizability. We therefore adopt default of 16 prefix strokes and 32 total strokes, robustly balancing structural simplicity with egy demonstrates that prefix strokes must be primed for future semantics. Greedy baselines do not have this ability. Meanwhile, the Overlay Loss ensures structural integration without obfuscation. Evaluations confirm our results are both semantically accurate and perceptually surprising. Limitations. Our method inherits limitations from pretrained diffusion priors; weak SDS guidance for complex structures (e.g., scissors) causes optimization failure. We provide visual examples in the supplementary material."
        },
        {
            "title": "References",
            "content": "[1] Emre Aksan, Thomas Deselaers, Andrea Tagliasacchi, and Otmar Hilliges. Cose: Compositional stroke embeddings. Advances in Neural Information Processing Systems, 33: 1004110052, 2020. 2 [2] Stephan Alaniz, Massimiliano Mancini, Anjan Dutta, Diego Marcos, and Zeynep Akata. Abstracting sketches through simple primitives. In European Conference on Computer Vision, pages 396412. Springer, 2022. 3 [3] Marc Alexa, Daniel Cohen-Or, and David Levin. As-rigidas-possible shape interpolation. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 165172. 2023. 3 [4] Ellie Arar, Yarden Frenkel, Daniel Cohen-Or, Ariel Shamir, and Yael Vinker. Swiftsketch: diffusion model for imageto-vector sketch generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 112, 2025. 2 [5] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel CohenOr, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from single image. In SIGGRAPH Asia 2023 Conference Papers, pages 112, 2023. 3 [6] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79968006, 2024. 3 [7] Hmrishav Bandyopadhyay, Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Aneeshan Sain, Tao Xiang, Timothy Hospedales, and Yi-Zhe Song. Sketchinr: first look into sketches as implicit neural representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1256512574, 2024. [8] Thaddeus Beier and Shawn Neely. Feature-based image metamorphosis. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 529536. 2023. 3 [9] Pierre Bezier. How renault uses numerical control for car body design and tooling. Technical report, SAE Technical Paper, 1968. 3 [10] Ayan Kumar Bhunia, Ayan Das, Umar Riaz Muhammad, Yongxin Yang, Timothy Hospedales, Tao Xiang, Yulia Gryaditskaya, and Yi-Zhe Song. Pixelor: competitive Figure 12. Ablation of overlay loss (Loverlay). (a) Without Loverlay, the model generates redundant strokes atop existing ones to satisfy the semantic target, resulting in visual clutter (red circle) and high intersection artifacts. (b) With Loverlay, the generated strokes (Sdelta) become spatially complementary to the prefix (Sprefix), avoiding collisions to produce clean, coherent line drawing. Figure 13. Analysis of stroke count. (Top) Simple concepts (horse) form recognizable silhouettes with minimal strokes (816). (Bottom) While complex concepts (Einstein) require larger budget (3264) to capture essential details. Fewer strokes result in abstraction. Our default (1632) balances structural simplicity and semantic fidelity. semantic fidelity. 4.4. Applications We demonstrate versatility beyond standard two-phase scenarios. Fig. 14 confirms robustness across diverse concept pairs, ranging from structurally similar to semantically distant. Fig. 15 extends this to three-phase illusions (e.g., appleto-rabbit-to-pig), showcasing effective multi-target coordination. Furthermore, our framework generalizes to alternative representations, including B-spline curves  (Fig. 19)  , vector graphics  (Fig. 20)  , and colored sketches  (Fig. 21)  , validating the broad applicability of our joint optimization principle. 5. Conclusion We present Stroke of Surprise, the first framework for progressive semantic illusions in vector sketching. By shifting from spatial to temporal dimensions, we enable real-time semantic re-contextualization. Our joint optimization strat8 Figure 14. Additional 2-phase progressive illusion results produced by our method. Figure 15. Additional 3-phase progressive illusion results produced by our method. 9 Figure 16. Additional qualitative comparisons. Figure 17. Metric-based ranking. Figure 18. GPT-based ranking. sketching ai agent. so you think you can sketch? ACM Transactions on Graphics (TOG), 39(6):115, 2020. [11] Ankan Kumar Bhunia, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer, Fahad Shahbaz Khan, Jorma Laaksonen, and Michael Felsberg. Doodleformer: Creative sketch In European Conference on drawing with transformers. Computer Vision, pages 338355. Springer, 2022. 2 [12] Ayan Kumar Bhunia, Subhadeep Koley, Amandeep Kumar, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. Sketch2saliency: Learning to detect salient objects from human drawings. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 27332743, 2023. 3 [13] Irving Biederman. Recognition-by-components: theory of human image understanding. Psychological review, 94(2): 115, 1987. 3 [14] Irving Biederman and Ginny Ju. Surface versus edge-based determinants of visual recognition. Cognitive psychology, 20(1):3864, 1988. 3 [15] Ryan Burgert, Xiang Li, Abe Leite, Kanchana Ranasinghe, and Michael Ryoo. Diffusion illusions: Hiding images in plain sight. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 3 [16] John Canny. computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelligence, (6):679698, 2009. 10 Figure 19. Extension on variable-width Bspline. Figure 20. Extension on vector graph. Figure 21. Extension on colored strokes. [17] Alexandre Carlier, Martin Danelljan, Alexandre Alahi, and Radu Timofte. Deepsvg: hierarchical generative network for vector graphics animation. Advances in Neural Information Processing Systems, 33:1635116361, 2020. 2 [18] Patrick Cavanagh. The artist as neuroscientist. Nature, 434 (7031):301307, 2005. [19] Pascal Chang, Sergio Sancho, Jingwei Tang, Markus Gross, and Vinicius Azevedo. Lookingglass: Generative anamorphoses via laplacian pyramid warping. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2433, 2025. 3 [20] Chen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, Yi-Chen Lo, Chia-Che Chang, Yu-Lun Liu, Yu-Lin Chang, ChiaPing Chen, and Chun-Yi Lee. Denoising likelihood score matching for conditional score-based data generation. arXiv preprint arXiv:2203.14206, 2022. 3 [21] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM transactions on Graphics (TOG), 42(4):110, 2023. 3 [22] Zehao Chen and Rong Pan. Svgbuilder: Component-based colored svg generation with text-guided autoregressive transformers. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 23582366, 2025. 2 [23] Ziyang Chen, Daniel Geng, and Andrew Owens. Images that sound: Composing images and sounds on single canvas. Advances in Neural Information Processing Systems, 37: 8504585073, 2024. 3 [24] Hung-Kuo Chu, Wei-Hsin Hsu, Niloy Mitra, Daniel CohenOr, Tien-Tsin Wong, and Tong-Yee Lee. Camouflage images. ACM Trans. Graph., 29(4):511, 2010. 3 [25] Ayan Das, Yongxin Yang, Timothy Hospedales, Tao Xiang, and Yi-Zhe Song. Beziersketch: generative model for scalable vector sketches. In European conference on computer vision, pages 632647. Springer, 2020. tion. In Proceedings of the 2015 ACM SIGCHI Conference on Creativity and Cognition, pages 185186, 2015. 3 [27] Paul De Casteljau. Outillages methodes calcul. Andr Citro en Automobiles SA, Paris, 4:25, 1959. 3 [28] Soumyaratna Debnath, Ashish Tiwari, Kaustubh Sadekar, and Shanmuganathan Raman. Rasp: Revisiting 3d anamorphic art for shadow-guided packing of irregular objects. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 58495858, 2025. 3 [29] Mathias Eitz, James Hays, and Marc Alexa. How do humans sketch objects? ACM Transactions on graphics (TOG), 31 (4):110, 2012. 2, 3 [30] Judith Fan, Wilma Bainbridge, Rebecca Chamberlain, and Jeffrey Wammes. Drawing as versatile cognitive tool. Nature Reviews Psychology, 2(9):556568, 2023. 3 [31] Yue Feng, Vaibhav Sanjay, Spencer Lutz, Badour AlBahar, Songwei Ge, and Jia-Bin Huang. Illusion3d: 3d multiview illusion with 2d diffusion priors. arXiv preprint arXiv:2412.09625, 2024. [32] Kevin Frans, Lisa Soros, and Olaf Witkowski. Clipdraw: Exploring text-to-drawing synthesis through language-image encoders. Advances in Neural Information Processing Systems, 35:52075218, 2022. 2 [33] Xiang Gao, Shuai Yang, and Jiaying Liu. Ptdiffusion: Free lunch for generating optical illusion hidden pictures with phase-transferred diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1824018249, 2025. 3 [34] Songwei Ge, Vedanuj Goswami, Lawrence Zitnick, and Devi Parikh. Creative sketch generation. arXiv preprint arXiv:2011.10039, 2020. 2 [35] Daniel Geng, Inbum Park, and Andrew Owens. Factorized diffusion: Perceptual illusions by noise decomposition. In European Conference on Computer Vision, pages 366384. Springer, 2024. 3 [26] Nicholas Davis, Chih-PIn Hsiao, Kunwar Yashraj Singh, Lisa Li, Sanat Moningi, and Brian Magerko. Drawing apprentice: An enactive co-creative agent for artistic collabora- [36] Daniel Geng, Inbum Park, and Andrew Owens. Visual anagrams: Generating multi-view optical illusions with diffusion models. In Proceedings of the IEEE/CVF Conference 11 on Computer Vision and Pattern Recognition, pages 24154 24163, 2024. 2, 3, 5 [37] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, and Daan Wierstra. Draw: recurrent neural network for image generation. In International conference on machine learning, pages 14621471. PMLR, 2015. 2 [38] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. Advances in Neural Information Processing Systems, 36:1589015902, 2023. 3 [39] David Ha and Douglas Eck. neural representation of sketch drawings. arXiv preprint arXiv:1704.03477, 2017. 2 [40] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23282337, 2023. 3 [41] Aaron Hertzmann. Painterly rendering with curved brush strokes of multiple sizes. In Proceedings of the 25th annual conference on Computer graphics and interactive techniques, pages 453460, 1998. [42] Aaron Hertzmann. survey of stroke-based rendering. Institute of Electrical and Electronics Engineers, 2003. 2 [43] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning, 2022. 5 [44] Or Hirschorn, Amir Jevnisek, and Shai Avidan. Optimize & reduce: top-down approach for image vectorization. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 21482156, 2024. 3 [45] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3 [46] Kai-Wen Hsiao, Jia-Bin Huang, and Hung-Kuo Chu. Multiview wire art. ACM Trans. Graph., 37(6):242, 2018. 3 [47] Teng Hu, Ran Yi, Haokun Zhu, Liang Liu, Jinlong Peng, Yabiao Wang, Chengjie Wang, and Lizhuang Ma. Strokebased neural painting and stylization with dynamically preIn Proceedings of the 31st ACM dicted painting region. International Conference on Multimedia, pages 74707480, 2023. 3 [48] Yi-Chuan Huang, Jiewen Chan, Hao-Jen Chien, and Yu-Lun Liu. Voxify3d: Pixel art meets volumetric rendering. arXiv preprint arXiv:2512.07834, 2025. 3 [49] Zhewei Huang, Wen Heng, and Shuchang Zhou. Learning to paint with model-based deep reinforcement learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 87098718, 2019. 3 [50] Francisco Ibarrola, Tomas Lawton, and Kazjon Grace. collaborative, interactive and context-aware drawing agent for co-creative design. IEEE Transactions on Visualization and Computer Graphics, 30(8):55255537, 2023. 3 [51] Shir Iluz, Yael Vinker, Amir Hertz, Daniel Berio, Daniel Cohen-Or, and Ariel Shamir. Word-as-image for semantic typography. ACM Transactions on Graphics (TOG), 42(4): 111, 2023. [52] Ajay Jain, Amber Xie, and Pieter Abbeel. Vectorfusion: Text-to-svg by abstracting pixel-based diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19111920, 2023. 2, 3 [53] Jiaxiu Jiang, Yabo Zhang, Kailai Feng, Xiaohe Wu, Wenbo Li, Renjing Pei, Fan Li, and Wangmeng Zuo. Mcˆ 2: Multiconcept guidance for customized multi-concept generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 28022812, 2025. 3 [54] Jonas Jongejan, Henry Rowley, Takashi Kawashima, Jongmin Kim, and Nick Fox-Gieg. Quick, draw! the data. dataset for online game Quick, Draw, 2016. 2 [55] Marcelo Isaias de Moraes Junior and Moacir Antonelli Ponti. On the temporality for sketch representation learning. arXiv preprint arXiv:2512.04007, 2025. 2 [56] Gaetano Kanizsa, Paolo Legrenzi, and Paolo Bozzi. Organization in vision: Essays on gestalt perception. (No Title), 1979. 3 [57] Pegah Karimi, Jeba Rezwana, Safat Siddiqui, Mary Lou Maher, and Nasrin Dehbozorgi. Creative sketching partner: an analysis of human-ai co-creativity. In Proceedings of the 25th international conference on intelligent user interfaces, pages 221230, 2020. [58] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani Lischinski. Noise-free score distillation. arXiv preprint arXiv:2310.17590, 2023. 3 [59] Bo-Hsu Ke, You-Zhe Xie, Yu-Lun Liu, and Wei-Chen Chiu. Stealthattack: Robust 3d gaussian splatting poisoning via density-guided illusions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 27400 27411, 2025. 3 [60] Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, and Jinwoo Shin. Collaborative score distillation for consistent visual synthesis. arXiv preprint arXiv:2307.04787, 2023. 3 [61] Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. How to handle sketch-abstraction in sketch-based image retrieval? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1685916869, 2024. 3 [62] Juil Koo, Chanho Park, and Minhyuk Sung. Posterior distillation sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1335213361, 2024. 3 [63] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of textto-image diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19311941, 2023. [64] Tomas Lawton, Francisco Ibarrola, Dan Ventura, and Kazjon Grace. Drawing with reframer: Emergence and control in co-creative ai. In Proceedings of the 28th International Conference on Intelligent User Interfaces, pages 264277, 2023. 3 [65] Jie-Ying Lee, Yi-Ruei Liu, Shr-Ruei Tsai, Wei-Cheng Chang, Chung-Ho Wu, Jiewen Chan, Zhenjun Zhao, 12 Chieh Hubert Lin, and Yu-Lun Liu. Skyfall-gs: Synthesizing immersive 3d urban scenes from satellite imagery. arXiv preprint arXiv:2510.15869, 2025. 3 [66] Ke Li, Kaiyue Pang, Jifei Song, Yi-Zhe Song, Tao Xiang, Timothy Hospedales, and Honggang Zhang. Universal sketch perceptual grouping. In Proceedings of the european conference on computer vision (ECCV), pages 582597, 2018. 3 [67] Tzu-Mao Li, Michal Lukaˇc, Michael Gharbi, and Jonathan Ragan-Kelley. Differentiable vector graphics rasterization for editing and learning. ACM Transactions on Graphics (TOG), 39(6):115, 2020. [68] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards highfidelity text-to-3d generation via interval score matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 65176526, 2024. 3 [69] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 300309, 2023. 3 [70] Hangyu Lin, Yanwei Fu, Xiangyang Xue, and Yu-Gang Jiang. Sketch-bert: Learning sketch bidirectional encoder representation from transformers by self-supervised learning of sketch gestalt. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67586767, 2020. 2 [71] Fang Liu, Xiaoming Deng, Yu-Kun Lai, Yong-Jin Liu, Cuixia Ma, and Hongan Wang. Sketchgan: Joint sketch completion and recognition with generative adversarial network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 58305839, 2019. 2 [72] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. Compositional visual generation with composable diffusion models. In European conference on computer vision, pages 423439. Springer, 2022. 3 [73] Songhua Liu, Tianwei Lin, Dongliang He, Fu Li, Ruifeng Deng, Xin Li, Errui Ding, and Hao Wang. Paint transformer: Feed forward neural painting with stroke prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 65986607, 2021. 3 [74] Xi Liu, Chaoyi Zhou, Nanxuan Zhao, and Siyu Huang. Bezier splatting for fast and differentiable vector graphics rendering. arXiv preprint arXiv:2503.16424, 2025. 3 [75] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. 3 [76] Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones: Concept neurons in diffusion models for customized generation. arXiv preprint arXiv:2303.05125, 2023. 3 [77] Raphael Gontijo Lopes, David Ha, Douglas Eck, and Jonathon Shlens. learned representation for scalable vector graphics. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 79307939, 2019. [78] Artem Lukoianov, Haitz Saez de Ocariz Borde, Kristjan Greenewald, Vitor Guizilini, Timur Bagautdinov, Vincent Sitzmann, and Justin Solomon. Score distillation via reparametrized ddim. Advances in Neural Information Processing Systems, 37:2601126044, 2024. 3 [79] Rundong Luo, Noah Snavely, and Wei-Chiu Ma. Shadowdraw: From any object to shadow-drawing compositional art. arXiv preprint arXiv:2512.05110, 2025. 2, 5 [80] Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev, Nikita Orlov, Yun Fu, and Humphrey Shi. Towards layerwise image vectorization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1631416323, 2022. 3 [81] Penousal Machado, Adriano Vinhas, Joao Correia, and Aniko Ekart. Evolving ambiguous images. AI Matters, 2(1):78, 2015. 3 [82] David McAllister, Songwei Ge, Jia-Bin Huang, David Jacobs, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Rethinking score distillation as bridge between image distributions. Advances in Neural Information Processing Systems, 37:3377933804, 2024. 3 [83] Niloy Mitra and Mark Pauly. Shadow art. ACM Trans. Graph., 28(5):156, 2009. 3 [84] Umar Riaz Muhammad, Yongxin Yang, Yi-Zhe Song, Tao Xiang, and Timothy Hospedales. Learning deep sketch In Proceedings of the IEEE Conference on abstraction. Computer Vision and Pattern Recognition, pages 80148023, 2018. 3 [85] Umar Riaz Muhammad, Yongxin Yang, Timothy Hospedales, Tao Xiang, and Yi-Zhe Song. Goal-driven sequential data abstraction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7180, 2019. 3 [86] Changhoon Oh, Jungwoo Song, Jinhan Choi, Seonghyeon Kim, Sungwoo Lee, and Bongwon Suh. lead, you help but only with enough details: Understanding user experience of co-creation with artificial intelligence. In Proceedings of the 2018 CHI conference on human factors in computing systems, pages 113, 2018. 3 [87] Aude Oliva, Antonio Torralba, and Philippe Schyns. Hybrid images. ACM Transactions on Graphics (TOG), 25(3): 527532, 2006. 3 [88] Artemis Panagopoulou, Coby Melkin, and Chris CallisonBurch. Evaluating vision-language models on bistable images. arXiv preprint arXiv:2405.19423, 2024. [89] Sagi Polaczek, Yuval Alaluf, Elad Richardson, Yael Vinker, and Daniel Cohen-Or. Neuralsvg: An implicit reprearXiv preprint sentation for text-to-vector generation. arXiv:2501.03992, 2025. 2 [90] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 2, 3 [91] Louis Pratt, Andrew Johnston, and Nico Pietroni. Bending the light: Next generation anamorphic sculptures. Computers & Graphics, 114:210218, 2023. 3 [92] Zhiyu Qu, Yulia Gryaditskaya, Ke Li, Kaiyue Pang, Tao Xiang, and Yi-Zhe Song. Sketchxai: first look at explainabil13 ity for human sketches. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2332723337, 2023. [93] Zhiyu Qu, Tao Xiang, and Yi-Zhe Song. Sketchdreamer: Interactive text-augmented creative sketch ideation. arXiv preprint arXiv:2308.14191, 2023. 2, 5, 6 [94] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 2 [95] Pradyumna Reddy, Michael Gharbi, Michal Lukac, and Niloy Mitra. Im2vec: Synthesizing vector graphics without vector supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73427351, 2021. 3 [96] Leo Sampaio Ferraz Ribeiro, Tu Bui, John Collomosse, and Moacir Ponti. Sketchformer: Transformer-based representation for sketched structure. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1415314162, 2020. 2 [97] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [98] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22500 22510, 2023. [99] Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James Hays. The sketchy database: learning to retrieve badly drawn bunnies. ACM Transactions on Graphics (TOG), 35(4):112, 2016. 2 [100] Steven Seitz and Charles Dyer. View morphing. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 2130, 1996. 3 [101] Maria Shugrina, Chin-Ying Li, and Sanja Fidler. Neural brushstroke engine: learning latent style space of interactive drawing tools. ACM Transactions on Graphics (TOG), 41(6):118, 2022. 3 [102] Edgar Simo-Serra, Satoshi Iizuka, Kazuma Sasaki, and Hiroshi Ishikawa. Learning to simplify: fully convolutional networks for rough sketch cleanup. ACM Transactions on Graphics (TOG), 35(4):111, 2016. 3 [103] Johannes JD Singer, Radoslaw Cichy, and Martin Hebart. The spatiotemporal neural dynamics of object recognition for natural images and line drawings. Journal of Neuroscience, 43(3):484500, 2023. 3 [104] Guoyao Su, Yonggang Qi, Kaiyue Pang, Jie Yang, and YiZhe Song. Sketchhealer graph-to-sequence network for recreating partial human sketches. In Proceedings of The 31st British Machine Vision Virtual Conference (BMVC 2020), pages 114. British Machine Vision Association, 2020. 2 [105] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and Yasutaka Furukawa. Mvdiffusion: Enabling holistic multiview image generation with correspondence-aware diffusion. Advances in Neural Information Processing Systems, 36: 5120251233, 2023. [106] Vikas Thamizharasan, Difan Liu, Shantanu Agarwal, Matthew Fisher, Michael Gharbi, Oliver Wang, Alec Jacobson, and Evangelos Kalogerakis. Vecfusion: Vector font generation with diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79437952, 2024. 2 [107] Vikas Thamizharasan, Difan Liu, Matthew Fisher, Nanxuan Zhao, Evangelos Kalogerakis, and Michal Lukac. Nivel: Neural implicit vector layers for text-to-vector generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 45894597, 2024. 3 [108] Yael Vinker, Ehsan Pajouheshgar, Jessica Bo, Roman Christian Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, and Ariel Shamir. Clipasso: Semantically-aware object sketching. ACM Transactions on Graphics (TOG), 41(4):111, 2022. 2 [109] Yael Vinker, Yuval Alaluf, Daniel Cohen-Or, and Ariel Shamir. Clipascene: Scene sketching with different types and levels of abstraction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4146 4156, 2023. 2 [110] Yael Vinker, Tamar Rott Shaham, Kristine Zheng, Alex Zhao, Judith Fan, and Antonio Torralba. Sketchagent: Language-driven sequential sketch generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2335523368, 2025. 2, 3, 5, 6 [111] Johan Wagemans, James Elder, Michael Kubovy, Stephen Palmer, Mary Peterson, Manish Singh, and Rudiger Von der Heydt. century of gestalt psychology in visual perception: I. perceptual grouping and figureground organization. Psychological bulletin, 138(6):1172, 2012. 3 [112] Caoliwen Wang, Bailin Deng, and Juyong Zhang. Neural shadow art. arXiv preprint arXiv:2411.19161, 2024. 3 [113] Jiawei Wang and Changjian Li. Contextseg: Sketch semantic segmentation by querying the context with attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36793688, 2024. 3 [114] Qiang Wang, Haoge Deng, Yonggang Qi, Da Li, and YiZhe Song. Sketchknitter: Vectorized sketch generation with diffusion models. In The eleventh international conference on learning representations, 2023. 2 [115] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in neural information processing systems, 36:84068441, 2023. [116] Veith Weilnhammer, Heiner Stuke, Guido Hesselmann, Philipp Sterzer, and Katharina Schmack. predictive coding account of bistable perception-a model-based fmri study. PLoS computational biology, 13(5):e1005536, 2017. 3 [117] Max Wertheimer. Laws of organization in perceptual forms. first published as untersuchungen zur lehre von der gestalt ii. Psychologische Forschung, 4:301350, 1923. 3 14 [131] Zhiyuan Xu, Yinhe Chen, Huan-ang Gao, Weiyan Zhao, Guiyu Zhang, and Hao Zhao. Diffusion-based visual anaIn 2025 IEEE/CVF Winter gram as multi-task learning. Conference on Applications of Computer Vision (WACV), pages 919928. IEEE, 2025. 3 [132] Lumin Yang, Jiajie Zhuang, Hongbo Fu, Xiangzhi Wei, Kun Zhou, and Youyi Zheng. Sketchgnn: Semantic sketch segmentation with graph neural networks. ACM Transactions on Graphics (TOG), 40(3):113, 2021. 3 [133] Yinghua Yao, Yuangang Pan, Jing Li, Ivor Tsang, and Xin Yao. Proud: Pareto-guided diffusion model for multiobjective generation. Machine Learning, 113(9):65116538, 2024. 3 [134] Qian Yu, Yongxin Yang, Feng Liu, Yi-Zhe Song, Tao Xiang, and Timothy Hospedales. Sketch-a-net: deep neural network that beats humans. International journal of computer vision, 122(3):411425, 2017. [135] Sicong Zang, Shuhui Gao, and Zhijun Fang. Generating sketches in hierarchical auto-regressive process for flexible sketch drawing manipulation at stroke-level. arXiv preprint arXiv:2511.07889, 2025. 3 [136] Peiying Zhang, Nanxuan Zhao, and Jing Liao. Text-tovector generation with neural path representation. ACM Transactions on Graphics (TOG), 43(4):113, 2024. 2 [137] Boheng Zhao, Rana Hanocka, and Raymond Yeh. Ambigen: Generating ambigrams from pre-trained diffusion model. arXiv preprint arXiv:2312.02967, 2023. 3 [138] Zhongyin Zhao, Ye Chen, Zhangli Hu, Xuanhong Chen, and Bingbing Ni. Vector graphics generation via mutuIn Proceedings of ally impulsed dual-domain diffusion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 44204428, 2024. 2 [139] Zhengxia Zou, Tianyang Shi, Shuang Qiu, Yi Yuan, and Zhenwei Shi. Stylized neural painting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1568915698, 2021. 3 [118] Holger Winnemoller, Jan Eric Kyprianidis, and Sven Olsen. Xdog: An extended difference-of-gaussians compendium including advanced image stylization. Computers & Graphics, 36(6):740753, 2012. 2 [119] Kang Wu, Renjie Chen, Xiao-Ming Fu, and Ligang Liu. Computational mirror cup and saucer art. ACM Transactions on Graphics (TOG), 41(5):115, 2022. [120] Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liao. Iconshop: Text-guided vector icon synthesis with autoregressive transformers. ACM Transactions on Graphics (TOG), 42(6): 114, 2023. 2 [121] Ronghuan Wu, Wanchao Su, and Jing Liao. Chat2svg: Vector graphics generation with large language models and image diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2369023700, 2025. 2 [122] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 5 [123] Yan Wu, Jeff Donahue, David Balduzzi, Karen Simonyan, and Timothy Lillicrap. Logan: Latent optimisation for generative adversarial networks. arXiv preprint arXiv:1912.00953, 2019. 3 [124] Guangxuan Xiao, Tianwei Yin, William Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuning-free multisubject image generation with localized attention. International Journal of Computer Vision, 133(3):11751194, 2025. 3 [125] Ximing Xing, Chuang Wang, Haitao Zhou, Jing Zhang, Qian Yu, and Dong Xu. Diffsketcher: Text guided vector sketch synthesis through latent diffusion models. Advances in Neural Information Processing Systems, 36:1586915889, 2023. 2, [126] Ximing Xing, Juncheng Hu, Jing Zhang, Dong Xu, and Qian Yu. Svgfusion: Scalable text-to-svg generation via vector space diffusion. arXiv preprint arXiv:2412.10437, 2024. 3 [127] Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, and Qian Yu. Svgdreamer: Text guided svg generation with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 45464555, 2024. 2 [128] Ximing Xing, Juncheng Hu, Guotao Liang, Jing Zhang, Dong Xu, and Qian Yu. Empowering llms to understand and generate complex vector graphics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1948719497, 2025. 2 [129] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 1590315935, 2023. 5 [130] Peng Xu, Timothy Hospedales, Qiyue Yin, Yi-Zhe Song, Tao Xiang, and Liang Wang. Deep learning for free-hand sketch: survey. IEEE transactions on pattern analysis and machine intelligence, 45(1):285312, 2022."
        }
    ],
    "affiliations": []
}