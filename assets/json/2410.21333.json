{
    "paper_title": "Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse",
    "authors": [
        "Ryan Liu",
        "Jiayi Geng",
        "Addison J. Wu",
        "Ilia Sucholutsky",
        "Tania Lombrozo",
        "Thomas L. Griffiths"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chain-of-thought (CoT) prompting has become a widely used strategy for working with large language and multimodal models. While CoT has been shown to improve performance across many tasks, determining the settings in which it is effective remains an ongoing effort. In particular, it is still an open question in what settings CoT systematically reduces model performance. In this paper, we seek to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, looking at cases where (i) verbal thinking or deliberation hurts performance in humans, and (ii) the constraints governing human performance generalize to language models. Three such cases are implicit statistical learning, visual recognition, and classifying with patterns containing exceptions. In extensive experiments across all three settings, we find that a diverse collection of state-of-the-art models exhibit significant drop-offs in performance (e.g., up to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using inference-time reasoning compared to zero-shot counterparts. We also identify three tasks that satisfy condition (i) but not (ii), and find that while verbal thinking reduces human performance in these tasks, CoT retains or increases model performance. Overall, our results show that while there is not an exact parallel between the cognitive processes of models and those of humans, considering cases where thinking has negative consequences for human performance can help us identify settings where it negatively impacts models. By connecting the literature on human deliberation with evaluations of CoT, we offer a new tool that can be used in understanding the impact of prompt choices and inference-time reasoning."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 ] . [ 3 3 3 3 1 2 . 0 1 4 2 : r MIND YOUR STEP (BY STEP): CHAIN-OF-THOUGHT CAN REDUCE PERFORMANCE ON TASKS WHERE THINKING MAKES HUMANS WORSE Ryan Liu*1, Jiayi Geng*1, Addison J. Wu1, Ilia Sucholutsky2, Tania Lombrozo3, Thomas L. Griffiths1,3 1Department of Computer Science, Princeton University 2Center for Data Science, New York University 3Department of Psychology, Princeton University {ryanliu, jiayig}@princeton.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Chain-of-thought (CoT) prompting has become widely used strategy for working with large language and multimodal models. While CoT has been shown to improve performance across many tasks, determining the settings in which it is effective remains an ongoing effort. In particular, it is still an open question in what settings CoT systematically reduces model performance. In this paper, we seek to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, looking at cases where (i) verbal thinking or deliberation hurts performance in humans, and (ii) the constraints governing human performance generalize to language models. Three such cases are implicit statistical learning, visual recognition, and classifying with patterns containing exceptions. In extensive experiments across all three settings, we find that diverse collection of state-of-the-art models exhibit significant drop-offs in performance (e.g., up to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using inference-time reasoning compared to zero-shot counterparts. We also identify three tasks that satisfy condition (i) but not (ii), and find that while verbal thinking reduces human performance in these tasks, CoT retains or increases model performance. Overall, our results show that while there is not an exact parallel between the cognitive processes of models and those of humans, considering cases where thinking has negative consequences for human performance can help us identify settings where it negatively impacts models. By connecting the literature on human deliberation with evaluations of CoT, we offer new tool that can be used in understanding the impact of prompt choices and inference-time reasoning."
        },
        {
            "title": "INTRODUCTION",
            "content": "Chain-of-thought (Wei et al., 2022; Nye et al., 2021) is widely used prompting technique for large language and multimodal models (LLMs and LMMs), instructing models to think step-by-step or providing other structure that should be incorporated into their response. Large meta-studies have shown that this technique improves the performance of models on many tasks, particularly those involving symbolic reasoning (Sprague et al., 2024). More generally, inference-time reasoning has become default component of the newest LLMs and LMMs such as OpenAI o1-preview (OpenAI, 2024a) and Claudes web interface and mobile apps (Anthropic, 2024). However, there also exist cases where CoT decreases performance, but there have not been any identified patterns as to when this happens. With the increasing use of inference-time reasoning in deployed models, it is imperative to understand and predict when CoT has negative effect on model performance. key challenge for determining the limits of CoT is the sheer variety of tasks for which LLMs and LMMs are used. While the machine learning community has dedicated great efforts towards developing large set of benchmarks for these models (e.g., Hendrycks et al., 2020; Suzgun et al., 2022), applications of models extend beyond benchmarks to diverse contexts and variations of tasks 1 Figure 1: Tasks evaluated for reductions in performance from CoT prompting. Implicit Statistal Learning (ISL): Classification of strings generated by an artificial grammar. Face Recognition (FR): Recognition of face from set that shares similar descriptions. Classification of Data with Exceptions (CDE): Learning labels in the presence of exceptions. Natural Language Inference (NLI): Recognizing logical inconsistency. Spatial intuitions (SI): Tilting water glasses. Working Memory (WM): Aggregating features for decision. Humans show reductions in performance when engaging in verbal thinking in all tasks, we show that the first three have similar effects on LLMs and VLMs, while the last three differ between humans and models in meaningful ways. that could all potentially affect performance. Exploring this enormous space to identify settings where CoT has negative effects is daunting problem. This motivates the need to develop heuristics to help us identify risky cases that could pose real and salient challenges for model inference-time reasoning. To narrow down the set of tasks to explore, we draw parallel between CoT prompting and humans engaging in verbal thought (Lombrozo, 2024). Specifically, we explore the heuristic that tasks for which thinking or deliberation decrease human performance can also be ones for which CoT harms model performance. An important caveat is that models and humans have fundamentally different capabilities and consequently have different constraints affecting their performance (Griffiths, 2020; Shiffrin & Mitchell, 2023; McCoy et al., 2024). For example, LLMs have long context lengths that far exceed human memory limitations. Despite this, we believe that for some failure cases the tasks themselves, in conjunction with traits shared between humans and models, are responsible for thinking having negative effect on performance. Thus, we anticipate that CoT will hurt model performance in cases where (i) deliberation hurts performance in humans, and (ii) the constraints governing human performance on the task generalize to LLMs. To explore this approach, we draw on the cognitive psychology literature on cases where engaging in verbal thinking hurts human task performance (Schooler & Engstler-Schooler, 1990; Dijksterhuis, 2004; Van den Bos & Poletiek, 2008, inter alia). We chose six well-studied types of tasks from psychology that satisfy this, selected the most representative task of each type, and adapted them to properly evaluate LLMs and LMMs (see Figure 1). In particular, three of these task types satisfy (ii): those which involve implicit statistical learning, those where language is ill-suited for representing stimuli, and those that involve learning labels that contain exceptions to generalizable rules. We identify another three types of tasks where (ii) is not satisfied, i.e., the constraints governing humans and models in these tasks differ. For these, we hypothesize that model performance will not necessarily reduce: tasks involving reasoning where models still significantly underperform humans 2 without CoT, those where we expect that human and model priors differ substantially, and those that involve limits of working memory. For each of these six types, based on whether conditions (i) and (ii) hold, we develop testable hypotheses about whether CoT will have negative effect on model performance and evaluate these hypotheses on set of state-of-the-art models. In representative tasks for each of the three categories where (i) and (ii) are satisfied, we find that CoT drastically decreases model performance across models. For implicit statistical learning, we observe an absolute performance drop of 36.3% in the performance of OpenAI o1-preview compared to GPT-4o zero-shot, as well as consistent reductions in accuracy across eight other state-of-the-art models. For tasks involving visual stimuli that are ill-represented by language, we find reductions in performance across all six vision-language models tested. And when learning data that contain exceptions to generalizable rules, CoT increased the number of iterations it took to learn the labels of data points by up to 331%. In contrast, for the three categories of tasks for which (i) is satisfied but (ii) is not, we observed no negative effects caused by chain-of-thought. basic prerequisite for seeing negative impact from CoT is that zero-shot prompting produces reasonable performance. Thus, logical reasoning task where human judgments are worse after deliberation was not candidate for negative effect of CoT because zero-shot prompting resulted in models that were unable to score above chance. In this case, performance improved using chain-of-thought, matching existing findings showing an advantage on tasks involving logic and mathematical reasoning (Sprague et al., 2024). When models lacked access to relevant priors, such as in task where motor simulation was responsible for improved performance (relative to verbal thinking) in humans, performance was roughly equal between conditions. On the other hand, having access to longer context windows than human working memory synergized with CoT to improve model performance in preference task involving aggregating lot of features described in text. whether psychological results translate to effects for model inference-time reasoning. Our findings confirm that CoT drastically harms LLM/LMM performance in cases where (i) overthinking hurts human performance and (ii) humans and models face similar constraints when performing the task. The remainder of the paper is structured as follows: We cover related work surrounding CoT and intersections between LLM/LMMs and psychology in Section 2. We ground our work within the psychology literature and identify six categories of tasks where thinking reduces human performance in Section 3. In Section 4, we cover the implementations of each task, how we adapt them to test models, and their corresponding results. We then discuss the potential limitations and implications of our work in Section 5."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 INFERENCE-TIME REASONING Chain-of-thought prompting aims to improve the performance of language-based models by encouraging them to generate an intervening string of tokens that increases the probability of producing the correct answer (Wei et al., 2022; Nye et al., 2021). This approach can result in significant performance improvements in language (Zhang et al., 2022) and vision (Zhang et al., 2023) tasks, hypothesized to be consequence of exploiting local structure in language (Prystawski et al., 2024). However, recent metastudy suggests that the gains from using CoT are primarily in mathematical and symbolic reasoning tasks, and that it is otherwise unclear when it improves performance (Sprague et al., 2024). In related settings such as planning, there is little benefit from CoT prompting (Kambhampati et al., 2024), and CoT can also increase harmful outputs (Shaikh et al., 2023). Despite these results, the default expectation seems to be that CoT improves performance. For example, recent update to language-understanding benchmark cited the fact that CoT results in an improvement on the new benchmark but decreased performance on the original benchmark as an indicator that the new benchmark is better (Wang et al., 2024). This expectation seems to have driven the tendency towards the default use of CoT in the latest LLMs and LMMs. 2.2 PSYCHOLOGICAL METHODS AS TOOL FOR STUDYING LLMS AND VLMS Since the introduction of LLMs, there has been growing interest in understanding the connections between models and human minds (Hardy et al., 2023). Human cognition is often studied using well-controlled tasks involving carefully curated datasets designed to test specific hypotheses. The availability of these datasets, and the fact that they often consist mainly of text and/or images, have led to these tasks from the psychology literature quickly becoming popular methods for evaluating and understanding LLMs and LMMs (e.g., Binz & Schulz, 2023; Coda-Forno et al., 2024). For example, recent studies that leverage insights or datasets from psychology have evaluated the representational capacity of LLMs (Frank, 2023), explored how RLHF and CoT lead to different outcomes when trying to make models both helpful and honest (Liu et al., 2024a), compared human and machine representations via similarity judgments (Peterson et al., 2018; Marjieh et al., 2023a;b; 2024a), determined that LLMs over-estimate human rationality in decisions (Liu et al., 2024b), identified incoherence in LLM probability judgments (Zhu & Griffiths, 2024), identified susceptibility to linguistic illusions in LLMs (Marjieh et al., 2024b), uncovered LLMs underlying social biases (Bai et al., 2024), used storytelling to understand episodic memory in LLMs (Cornell et al., 2023), constructed prompts using psychological theories of metaphor (Prystawski et al., 2022), discovered cross-linguistic variability in LLM representations (Niedermann et al., 2023), and probed the roles of language and vision for in-context learning in VLMs (Chen et al., 2024). Many of these studies start with well-studied phenomenon in human cognition and then explore whether there is an isomorphic analog to it in LLMs or LMMs. Our work follows this established line of literature by associating the well-studied impact of deliberation on human performance to effects that occur in models using CoT."
        },
        {
            "title": "3 APPROACH: WHEN THINKING REDUCES HUMAN PERFORMANCE",
            "content": "A large body of psychological research has investigated effects of verbal thinking (often explicit deliberation) on memory, learning, judgment, and decision-making. Very often these effects are positive. For example, people who spend more time deliberating are more likely to respond correctly on questions that initially trigger an intuitive but incorrect response (Travers et al., 2016). However, there are also cases in which deliberation can impair performance, often involving mismatch between the representations or types of processing induced by deliberation and those that best support task performance (Schooler, 2002). classic setting for such effects is in the domain of implicit statistical learning. For example, in studies of artificial grammar learning participants are presented with sequences of letters or phonemes that conform to some structure (such as finite state grammar) and asked to recognize well-formed sequences. Studies often find that participants can differentiate well-formed sequences from those that are not well-formed, but cannot verbalize the basis for their judgments (Aslin & Newport, 2012; Romberg & Saffran, 2010). Some (but not all) studies further find that receiving explicit instructions to identify rules in verbal form impairs performance (Reber, 1976). Another class of cases concerns phenomenon termed verbal overshadowing. In classic demonstration, instructions to verbalize face led to impaired facial recognition relative to condition in which participants did not verbalize (Schooler & Engstler-Schooler, 1990). Such effects have been found for other perceptual stimuli (Fiore & Schooler, 2002; Melcher & Schooler, 1996), but do not extend to stimuli that are easy to verbalize (such as spoken statement) (Schooler & EngstlerSchooler, 1990) or to logical problem solving (Schooler et al., 1993). As third example, studies find that asking people to generate verbal explanations for their observations supports the discovery of broad and simple patterns (Edwards et al., 2019; Walker et al., 2017; Williams & Lombrozo, 2010; 2013). But when the stimuli are designed such that these broad and simple patterns contain exceptions, participants who were prompted to explain learned more slowly and made more errors (Williams et al., 2013). These effects are thought to arise from the mismatch between the representations or processes induced by form of thinking (in this case, explaining) and the representations or processes that best support task performance (Lombrozo, 2016). The effects reviewed so far plausibly concern impairments that arise from the representational limitations of language and the generalization of patterns found in language: language is not well-suited to encoding fine-grained perceptual discriminations (as required for face recognition), and language readily encodes some kinds of relationships (such as deductive entailment, or simple and broad patterns) but is less well-suited or frequently employed for others (such as complex finite state grammars, or patterns with arbitrary exceptions). Given that LLMs are likely to share limitations that arise from language and generalization, we might expect LLMs to exhibit patterns of impairment that mirror those found for humans on these tasks. We test these predictions in Section 4. Prior work has documented additional impairments in humans from verbal reasoning, but for some it is less clear if they should generalize to LLMs. For example, explaining how inconsistent statements could be true makes participants less likely to recognize logical inconsistency (Khemlani & Johnson-Laird, 2012). However, this assumes reasonable baserate in recognizing logical inconsistencies something that can be challenge for LLMs with zero-shot prompting. Prior work has also found that verbal reasoning can be less accurate than visual or motor simulation (Schwartz & Black 1999; see also Aronowitz & Lombrozo 2020; Lombrozo 2019), but this is consequence of information encoded in visual and motor representations that are likely not available to models. Finally, humans sometimes make poor choices that they regret when they deliberate over complex, multidimensional problems (Dijksterhuis, 2004) plausibly consequence of memory limitations that are not faced by LLMs. We anticipate that for these tasks CoT is less likely to reduce performance."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Following the different failure settings of human deliberation described in Section 3, we select six tasks, each representative of class of failure settings from the psychological literature, and conduct an experiment to test the effect of CoT on LLMs and LMMs. Based on the considerations outlined at the end of Section 3, we identify two criteria under which we anticipate task might result in CoT reducing performance: (i) Verbal thinking or deliberation hurts performance in humans. (ii) The constraints governing human performance are the same as those governing AI models. We use these criteria to develop hypothesis for each task. We first cover three cases where both (i) and (ii) are satisfied, before discussing three cases where (ii) is not satisfied. In each, we scale up the classic psychology study that tested humans, while also adapting the task towards modern use-cases of large language or multimodal models. 4.1 IMPLICIT STATISTICAL LEARNING Task. The first class of tasks we examine are those involving implicit statistical learning. As described in Section 3, psychology studies have found that data that contain statistical patterns can be better generalized by humans when those patterns are not linguistically described. We explore this for LLMs by replicating the task of learning artificial grammars (Reber & Lewis, 1977; Whittlesea & Dorken, 1993; Van den Bos & Poletiek, 2008). In the task, artificial words are constructed using finite-state grammars (FSGs) and participants are tasked with identifying which words belong to the same category (i.e., are generated by the same FSG). In total, we constructed 4400 classification problems corresponding to 100 randomly sampled unique FSGs that were structurally similar to those used to test humans in Fallshore & Schooler (1993). Each classification problem consisted of 15 training examples generated from the grammar, and the model was given new example and asked to classify it. Models were asked to classify 44 words per FSG, where 22 words belonged to the FSG and 22 did not. Words not belonging to the grammar were generated by replacing one letter from an existing word in the grammar. Details on problem generation are provided in Appendix A.1. Hypothesis. The artificial grammar learning task satisfies (i) based on the findings of Fallshore & Schooler (1993), where humans who conducted verbal thinking did worse on the problem. The task also satisfies (ii): verbal reasoning is thought to impair human performance due to the constraints of using explicit language-based processing, which is something shared by humans and LLMs using CoT. Thus, following our hypothesis, we predict that CoT will reduce LLM performance on the artificial grammar learning task. Models and prompts. We evaluate this task on several openand closed-source models: OpenAI o1-preview, GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, Llama 3.1 70B & 8B Instruct, and Llama 3 70B & 8B Instruct. We considered two prompts, zero-shot and CoT (see Appendix A.2). 5 Table 1: Results contrasting zero-shot and CoT for artificial grammar learning. GPT-4o (subset) OpenAI o1-preview (subset) GPT-4o Claude 3 Opus Claude 3.5 Sonnet Gemini 1.5 Pro Llama 3 8B Instruct Llama 3 70B Instruct Llama 3.1 8B Instruct Llama 3.1 70B Instruct Zero-shot CoT 94.00% - - 57.70% 87.50% 64.40% 70.70% 62.70% 65.90% 67.70% 68.00% 61.95% 59.70% 57.90% 60.50% 58.30% 53.52% 51.54% 65.90% 57.10% Performance decrease p-value 36.30% < 0.0001 23.10% 8.00% -1.80% 6.05% 1.80% 2.20% 1.98% 8.80% < 0.0001 < 0.0001 0.969 < 0.0001 < 0.05 < 0.05 < 0.0001 < 0.0001 Results. Consistent with our hypothesis, we find large reductions in performance when using CoT prompting compared to zero-shot prompting, displayed in Table 1. We find that when run on randomly selected subset of 440 problems, OpenAI o1-preview, which has form of CoT built into its responses, has 36.3% absolute accuracy decrease compared to GPT-4o zero-shot on the same subset. Similarly, while there is limited performance change between conditions for Claude 3.5 Sonnet, we see that its performance is lower than the zero-shot accuracy of Claude 3 Opus. Across the other models, we find consistent decreases in performance when performing CoT: 23.1% in GPT-4o, 8.00% in Claude 3 Opus, 6.05% in Gemini 1.5 Pro, and 8.80% in Llama 3.1 70B Instruct. Weaker models such as Llama 3.1 8B Instruct and Llama 3 8B Instruct perform closer to chance (50%), but the reduction in performance caused by CoT remains statistically significant. 4.2 FACIAL RECOGNITION Task. Another class of tasks we identify in Section 3 where verbal thinking reduces performance involves verbal overshadowing, where verbal thinking affects interactions with perceptual stimuli. We study this case using classic human face recognition task, where participants are first shown face of person and then asked to select an image of the same person out of list of candidates (Schooler & Engstler-Schooler, 1990). While psychological studies often include distractor task between the initial person and the candidates to increase the difficulty, we did not use these for LMMs due to their weak performance. We scale this task from one recognition problem to novel synthetic dataset of 500 problems across 2500 unique faces. For each problem, all faces were given the same described attributes for seven features: race, gender, age group, eye color, hair length, hair color, and hair type. We then generated pair of images of the same person and four images of other people matching this description using stable-image-ultra (StabilityAI, 2024). We adjusted the generation process to ensure that the pair clearly consisted of the same person, while the others clearly did not (see Appendix B.1 for further details). One of the pair was selected to be the initial stimulus, while the other was shuffled with the four images of other people to create five candidate answers. Models were prompted to identify which candidate matched the person from the initial stimulus. Hypothesis. The facial recognition task satisfies (i), as people who verbally reasoned about faces performed worse in Schooler & Engstler-Schooler (1990). (ii) is satisfied as human failure is attributed to the lack of granularity and inadequacy of language in describing the visual stimuli, constraint relating to verbal thinking rather than people. Following our hypothesis, we predict that CoT will reduce performance on our facial recognition task in LMMs. Models and prompts. We evaluated this task on several openand closed-source state-of-the-art LMMs: GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, InternVL2 26B, and InternVL2 Llama3 76B. Other models such as Llama 3.2 90B Vision and Molmo 72B were not considered as they do not support multiple image input. We considered two prompts, zero-shot and CoT, available in Appendix B.2. 6 Table 2: Comparison of zero-shot and CoT prompts for facial recognition. Performance decrease (relative) Performance decrease (absolute) Zero-shot CoT GPT-4o Claude 3 Opus Claude 3.5 Sonnet Gemini 1.5 Pro InternVL2 26B InternVL2 Llama3 76B 64.00% 51.20% 44.00% 29.60% 97.80% 94.80% 66.00% 54.60% 9.20% 6.00% 15.77% 13.77% 12.80% 14.40% 3.00% 11.40% 3.20% 2.00% 20.00% 32.73% 3.07% 17.27% 34.78% 12.68% p-value < 0.01 < 0.0001 < 0.05 < 0.05 < 0.05 0.44 Results. We find results consistent with our hypothesis every LMM tested shows drop in performance when asked to perform CoT (see Table 2). Weaker models often answered that all images are of the same person, resulting in accuracies below the random chance rate of 20%. However, even under these conditions, we observe decreases in performance due to CoT. 4.3 CLASSIFYING DATA WITH PATTERNS THAT CONTAIN EXCEPTIONS Task. third class of tasks we identify where CoT may harm performance is learning to classify exemplars when there are exceptions to generalizable rules. As mentioned in Section 3, when humans try to explain the category membership of exemplars, they tend to hypothesize simple classification rules, which can lead to inefficient learning when data contain arbitrary exceptions to these rules. To study if this phenomenon extends to CoT, we replicate multi-turn vehicle classification task from Williams et al. (2013), where participants try to correctly assign binary labels to list of vehicles. Participants are given feedback after each prediction, and conduct multiple passes over the list until they label all vehicles correctly in single pass or exceed the maximum number of tries. Vehicles in the task contained one feature that was almost fully correlated (80%) with the classification label, three features with no relation to the label, and one feature (the unique color) that individually identified the vehicle. Thus, participants could either try to learn the generalizable rule from the highly correlated feature and fail due to the exceptions, or they could learn the individual mappings from the identifying feature to the corresponding label. Human participants who were prompted to explain the classification of exemplars performed worse because they tended to attempt the former strategy. Participants in the original study performed this explanation after receiving feedback. To more explicitly include inference-time reasoning, we modify the point at which verbal thinking is performed, asking the LLM to perform CoT before making each prediction instead. In total, we constructed 2400 vehicles split into 240 lists of ten vehicles each and measured LLMs abilities to learn the labels of each list across up to 15 passes (see Appendix C.1 for details). Memory was implemented by including previous problems, guesses, and feedback in context. Hypothesis. This learning with exceptions task fulfills (i) as people tended to reason about generalizable rules when conducting verbal thinking, which increased the amount of time needed to learn the labels for the entire list (Williams et al., 2013). (ii) is satisfied as we expect models to replicate the tendency that humans have to focus on generalizable features when performing such thinking. Models and prompts. We evaluated this task on GPT-4o, Claude 3.5 Sonnet, and Claude 3 Opus. We only evaluated these models as the task requires long-context conversation capabilities, which other models such as Llama 3.1 70B Instruct were not sufficiently good at. We varied the prompt between direct and CoT, where the former corresponded to directly asking the model to classify with the memory in context (see Appendix C.2 for details). Results. In our experiments we find that CoT drastically increases the number of passes needed for the model to learn all the labels correctly. Averaged across the 240 lists, GPT-4o with CoT needed more than four times the number of passes to learn the labels compared to direct prompting, while Claude 3.5 Sonnet and 3 Opus both needed more than double (see Table 3). 7 Table 3: Average number of rounds for models to learn labels using either direct or CoT prompting. Direct CoT # Rounds increase (absolute) # Rounds increase (relative) GPT-4o Claude 3.5 Sonnet Claude 3 Opus 2.9 2.3 2.4 12.5 6.4 5. 9.6 4.1 3.1 331% 178% 129% p-value < 0.0001 < 0.0001 < 0.05 We also investigated the per-round accuracy of GPT-4o and found that direct prompting consistently resulted in the model attaining perfect classification on the second or third iteration, while with CoT, the model was only able to correctly classify around 8/10 objects after 4 to 5 iterations (see Appendix C.3). The model was unable to surpass this degree of accuracy over the long run, likely due to CoT biasing the model to rely on the seemingly generalizable rules from the exemplars, while in the process, significantly down-weighing the usefulness of contextual tokens that explicitly contained all of the correct answers. 4.4 TASKS WITH MISMATCH BETWEEN HUMAN AND MODEL ABILITIES In the next three tasks, we consider cases where (i) is satisfied but (ii) is not. In other words, humans perform worse on the task after deliberation or verbal thinking, but we do not expect this to generalize to model CoT due to differences between humans and models. Reasons for this include models producing poor performance with zero-shot prompting providing no opportunity for decrease in performance, or humans and models possessing different limitations for task-relevant abilities, such as access to different kinds of information or memory resources. Explaining logical inconsistency. Studies have found that when human participants are shown pair of logically inconsistent statements and asked to explain their coexistence, they become worse at judging whether the statements are indeed logically inconsistent (Khemlani & Johnson-Laird, 2012). In the task, participants are provided with two sentences following the template: If then it is always the case that B, and either A, but it is not the case that or It is not the case that B. The former corresponds to logical inconsistency between the statements, while the latter does not. In one condition humans were first asked to explain why an inconsistent pair could coexist before providing judgement on their inconsistency, while in another they conducted the same explanation after providing judgement. The former condition saw its classification performance drop significantly compared to the latter, indicating that deliberation towards the incorrect class reduced human performance. The original human experiment contained 12 unique {A, B} pairs. To scale this task to evaluate LLMs, we leverage existing entailment pairs in natural language inference tasks, which we use to fill in and to form the sentences. We used combination of three datasets: The Stanford Natural Language Inference (SNLI) dataset, the Multi-Genre Natural Language Inference (MNLI) dataset, and synthetic LLM-generated dataset of 100 entailment pairs. We filtered the datasets for pairs which were labeled entailment (i.e., entails B). In addition, we limit the maximum length of and such that the template forms coherent sentences. In total, we evaluate on 1608 pairs of {A, B} pairs: 675 from SNLI, 833 from MNLI, and 100 synthetic. Each pair was used to construct two classification problems, one consistent and one inconsistent, for total of 3216 problems which we use to evaluate LLMs. For more details on problem generation see Appendix D.1. We evaluated suite of state-of-the-art LLMs on this task: OpenAI o1-preview (on subset of 30 synthetic questions), GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, and Llama 3.1 70B Instruct. We used zero-shot prompting and two conditions of CoT: one where the model is simply asked to reason before answering, and another that follows the original experiment by asking the model to explain the inconsistency directly (see Appendix D.2 for details). Results were very similar across the two CoT conditions, so we report an average over both. Zero-shot prompting resulted in poor performance on this task, with most models performing close to chance (see Table 4). CoT often improved this performance, attributable to both the low base performance and the logical reasoning component for which CoT is typically helpful. This was Table 4: Results comparing zero-shot and CoT across the logical inconsistency task using stimuli from MNLI, SNLI, and synthetic LLM generation. MNLI SNLI Synthetic Zero-shot CoT Zero-shot CoT Zero-shot CoT OpenAI o1-preview (subset) GPT-4o Claude 3.5 Sonnet Claude 3 Opus Gemini 1.5 Pro Llama 3.1 70B Instruct - 53.2% 65.2% 62.7% 73.2% 55.6% - 93.9% 67.5% 58.8% 68.2% 81.6% - 51.4% 67.4% 66.2% 68.8% 50.4% - 94.3% 69.8% 58.7% 63.9% 82.3% - 51.0% 56.7% 54.5% 60.5% 50.0% 86.5% 74.0% 57.8% 51.8% 61.5% 65.8% Table 5: Results comparing zero-shot and CoT on the spatial intuition task. Zero-shot CoT Performance change (absolute) Performance change (relative) p-value GPT-4o Claude 3.5 Sonnet Claude 3 Opus Gemini 1.5 Pro InternVL2 Llama3 76B 38% 42% 42% 35% 39% 40% 38% 38% 36% 31% +2% -4% -4% +1% -8% +5.00% -10.53% -10.53% +2.78% -25.81% 0.61 0.28 0.28 0.99 0.67 especially pronounced in GPT-4o, where CoT improved performance by over 40% on pairs from MNLI and SNLI. However, in the model that performed best with zero-shot prompting, Gemini 1.5 Pro, we did see decrease in performance with CoT. These results suggest that reasonable level of performance with zero-shot prompting is prerequisite for CoT to reduce performance. Spatial intuitions. Psychologists have documented cases involving spatial reasoning in which humans generate more accurate responses after visual or motor simulation compared to verbal reasoning. To investigate whether this applies to models, we replicate cup-tilting task from Schwartz & Black (1999). In the task, participants are shown an image of two rectangles with varying height and width, representing two cups one empty and one that contains some water. Participants are then asked to estimate the water that should be added to the empty cup so that when tilting both cups, water will reach the rim at the same angle (see Figure 1, SI). While the original task had participants draw the water level on the empty cup, LMMs were unable to do this consistently. Thus, we turned the task into multiple choice question by adding markings to the side of the empty cup and asking the model to choose one. Incorrect answer options were generated by adding Gaussian noise to the correct answer while satisfying the constraint that answer options must be certain distance apart. We scale up this task by varying the dimensions of cup sizes and water height, creating total of 100 problems, each with code-drawn image containing the cups and multiple choice answers (see Appendix E.1 for details). We evaluated with zero-shot and CoT prompts on several openand closed-source large multimodal models: GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, and InternVL2 Llama3 76B. In this setting, it is unlikely that large multimodal models would share the same motor simulation capabilities as humans due to lack of representations built from motor experience. As the improved performance in the non-verbal thinking condition requires spatial or motor intuition, (ii) is not satisfied. Consistent with this, we did not observe statistically significant differences between zero-shot and CoT prompts (see Table 5). Generally, we expect this to extend to other tasks where models lack task-relevant priors that humans possess. Aggregating features for decision. The final category of tasks we consider are complex, multidimensional tasks that exceed human working memory capacity. study conducted by Dijksterhuis (2004) showed that humans made poor choices when deliberating over apartment options when provided with large amount of information about various decision features. In the study, participants were shown 48 statements for one second each, where the statements described either positive, negative, or neutral aspect of one of four apartment choices. Afterwards, they were asked to select 9 Table 6: Results for apartment selection task across four models and three ranges of . [0.1, 0.3] Zero-shot CoT [0.3, 0.5] Zero-shot CoT [0.5, 1] Zero-shot CoT GPT-4o Claude 3.5 Sonnet Claude 3 Opus Llama 3.1 70B Instruct 47% 50% 35% 42% 45% 62% 50% 6% 57% 62% 57% 44% 56% 72% 58% 5% 80% 81% 72% 43% 87% 95% 84% 20% the best apartment after either deliberating or completing distractor task. The authors found that the distractor task condition actually improved performance over deliberating based on memory. To scale this task up to evaluate LLMs, we generated 80 unique apartment features with four statements per feature: One positive, one negative, and two in between. We then asked GPT-4o to rate the impact each statement would have on the impression of an average tenant from -5 to 5. We randomly sampled apartments by choosing one statement per feature, and constructed sets of four where the best apartment had per-feature average score {[0.1, 0.3], [0.3, 0.5], [0.5, 1]} higher than the next-best option. We sampled 300 such sets of four apartments (100 per range) to form choice tasks (see Appendix F.1). We tested several openand closed-source LLMs with zero-shot and CoT prompts: GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, and Llama 3.1 70B Instruct. Llama 3.1 70B Instruct was often unable to return an answer after deliberating in the CoT condition, resulting in lower performance. In this setting, the differences in working memory between humans and models led to (ii) being unsatisfied. Humans performing the task were forced to rely on their aggregate impressions of each apartment due to the large amount of information. However, even after scaling up the number of contextually relevant statements over six-fold, models were simply able to access all feature statements in-context. Consistent with this, we observed somewhat positive effects from CoT (see Table 6). Essentially, the availability of context turns the problem into summing up the importances of the features, which the model is able to leverage additional inference-time reasoning to conduct. This highlights fundamental differences in capabilities between models and humans for solving certain tasks, highlighting the need for (ii) to consider whether the constraints governing human performance in these failure cases generalize to LLMs."
        },
        {
            "title": "5 DISCUSSION",
            "content": "Chain-of-thought prompting is an effective way to expand the capacities of large language and multimodal models. However, knowing that CoT significantly decreases performance in specific settings is important in considering when it should be deployed, and in particular whether it should be deployed by default. By using cases where verbal thinking decreases human performance as heuristic, we successfully identify three such settings where CoT results in large decreases in model performance, contributing towards discourse regarding both these discussions. While we make connection between human cognition and large language and multimodal models, we do not claim that these systems operate in the same way or that models should be anthropomorphized. Rather, we see this connection as tool for identifying settings where either the structure of the task or shared limitations result in negative effects of verbal thinking for both humans and models. Our exploration was guided by considering not just (i) that deliberation reduces human performance but also (ii) that there are no other constraints at work that result in meaningful difference between humans and models. Across experiments on six diverse tasks, we find overwhelming evidence that CoT results in decrease in performance when both (i) and (ii) are satisfied, but not when (ii) is not satisfied. More generally, these results suggest that we can successfully leverage the existing literature in cognitive psychology on deliberation and thinking to find cases that are informative about the performance of CoT. In the remainder of this section, we consider the limitations of this work and some future directions."
        },
        {
            "title": "5.1 LIMITATIONS",
            "content": "Types of inference-time reasoning. Since the invention of chain-of-thought prompting, researchers have developed various prompts that are specifically suited to target application domains, as well as more elaborate general-purpose prompts with multiple forward passes such as tree-ofthought (Yao et al., 2024) and self-consistency (Wang et al., 2023). We test the effectiveness of tree-of-thought on GPT-4o for the implicit statistical learning task (see Appendix A.3), and find that while it improves classification accuracy (64.55% vs. 62.52%) , this is still far from the zero-shot performance of 94.00%, suggesting that our approach extends across inference-time reasoning techniques. However, future work is still required to determine whether this generalizes to other task domains and methods of eliciting verbal thinking in models. Scope of application. While our psychology-based heuristic offers strategy for identifying failure cases of CoT, it is unlikely to cover all cases where CoT decreases performance. Existing psychological research has been guided by variety of theoretical and practical considerations towards studying humans, but this does not offer an exhaustive or representative sample of all tasks, and will miss cases that are uniquely interesting to study in models but not humans. Thus, we envision our contribution to be complementary to existing evaluation methods in natural language processing. As weve seen across our six sets of experiments, knowledge of what drives the decrease of performance in humans can be leveraged to generate predictions about the effects of CoT for different tasks, but this remains an inferential step that requires careful reasoning and an understanding of model capabilities. Despite these limitations, our method can be used to identify large and consequential failures of CoT, as documented in our three failure tasks. It also offers valuable crossdomain insight that can help build intuition and contribute to our overall understanding of inferencetime reasoning. On the flipside, the existence of capable LLM/LMM systems also allows us to reflect upon the reasons why human performance can be degraded by deliberation. By considering when CoTs effects mirror humans and when they do not, we can distinguish when the task or mechanisms shared by humans and models are responsible for failures, versus when the issues are with uniquely human strategies or limitations. Alternative explanation for why CoT does not replicate human results. There exists an alternative explanation for why we do not see drops in performance in the latter three tasks that how we implemented the tasks for LLMs removed the failure effect. Its possible that with different implementations we might in fact see decreased performance, mirroring what we see in humans. While we explored large number of variations for the latter three tasks, these explorations were not exhaustive due to the endless changes that one could make to the prompts. In other words, because the tasks were inevitably changed to scale up the evaluation and match more realistic applications of models, its also possible that these changes are what explain the human-model mismatch. 5.2 FUTURE DIRECTIONS & BROADER IMPLICATIONS While we have focused on CoT reasoning, the framework presented here suggests more general strategy for leveraging empirical work on humans to characterize the performance of models: jointly considering (i) psychological results concerning humans and (ii) whether the relevant constraints that shape human performance extend to those models. For example, it could be fruitful to investigate effects of comparison or analogical prompting (Lombrozo, 2024) through this lens. Altogether, we visualize studying how to evaluate and improve models as collaborative effort between natural language processing methods (e.g., benchmarks), psychological insights, and the burgeoning literature of evaluations that compare human and model performance. By sharing knowledge and building strong collaboration between these different disciplines, we can utilize rich insights from decades of literature that studied humans to advance the domains intuitions about these models and analyze an even broader array of tasks and applications for AI. ACKNOWLEDGMENTS Experiments were supported by Azure credits from Microsoft AFMR grant. This work and related results were made possible with the support of the NOMIS Foundation. Special thanks to Howard Chen and Mengzhou Xia for providing invaluable feedback on this project."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. System prompts - release notes, 2024. URL https://docs.anthropic.com/ en/release-notes/system-prompts#sept-9th-2024. Accessed: 2024-09-28. Sara Aronowitz and Tania Lombrozo. Learning through simulation. Philosophers Imprint, 20, 2020. Richard Aslin and Elissa Newport. Statistical learning: From acquiring specific items to forming general rules. Current Directions in Psychological Science, 21(3):170176, 2012. Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, and Thomas Griffiths. Measuring implicit bias in explicitly unbiased large language models. arXiv preprint arXiv:2402.04105, 2024. Marcel Binz and Eric Schulz. Using cognitive psychology to understand GPT-3. Proceedings of the National Academy of Sciences, 120(6):e2218523120, 2023. Allison Chen, Ilia Sucholutsky, Olga Russakovsky, and Thomas L. Griffiths. Analyzing the roles of language and vision in learning from limited data. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 46, 2024. Julian Coda-Forno, Marcel Binz, Jane Wang, and Eric Schulz. CogBench: large language model walks into psychology lab. arXiv preprint arXiv:2402.18225, 2024. Charlotte Cornell, Shuning Jin, and Qiong Zhang. The role of episodic memory in storytelling: Comparing large language models with humans. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 46, 2023. Ap Dijksterhuis. Think different: the merits of unconscious thought in preference development and decision making. Journal of Personality and Social Psychology, 87(5):586, 2004. Brian Edwards, Joseph Williams, Dedre Gentner, and Tania Lombrozo. Explanation recruits comparison in category-learning task. Cognition, 185:2138, 2019. Marte Fallshore and Jonathan Schooler. Post-encoding verbalization impairs transfer on artificial In Proceedings of the Fifteenth Annual Conference of the Cognitive Science grammar tasks. Society Erlbaum: Hillsdale, NJ, pp. 412416, 1993. Stephen Fiore and Jonathan Schooler. How did you get here from there? Verbal overshadowing of spatial mental models. Applied Cognitive Psychology, 16(8):897910, 2002. Michael Frank. Baby steps in evaluating the capacities of large language models. Nature Reviews Psychology, 2(8):451452, 2023. Thomas Griffiths. Understanding human intelligence through human limitations. Trends in Cognitive Sciences, 24(11):873883, 2020. Mathew Hardy, Ilia Sucholutsky, and Bill Thompson. Large language models meet cognitive sciIn Proceedings of the Annual Meeting of the ence: LLMs as tools, models, and participants. Cognitive Science Society, 45 (45), 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, and Anil Murthy. LLMs cant plan, but can help planning in LLMmodulo frameworks. arXiv preprint arXiv:2402.01817, 2024. Sangeet Khemlani and Philip Johnson-Laird. Hidden conflicts: Explanations make inconsistencies harder to detect. Acta Psychologica, 139(3):486491, 2012. Wayne Kirchner. Age differences in short-term retention of rapidly changing information. Journal of Experimental Psychology, 55(4):352, 1958. 12 Ryan Liu, Jiayi Geng, Joshua Peterson, Ilia Sucholutsky, and Thomas Griffiths. Large language models assume people are more rational than we really are. arXiv preprint arXiv:2406.17055, 2024a. Ryan Liu, Theodore Sumers, Ishita Dasgupta, and Thomas Griffiths. How do large language models navigate conflicts between honesty and helpfulness? In Forty-first International Conference on Machine Learning, 2024b. Tania Lombrozo. Explanatory preferences shape learning and inference. Trends in Cognitive Sciences, 20(10):748759, 2016. Tania Lombrozo. Learning by thinking in science and in everyday life. The Scientific Imagination, pp. 230249, 2019. Tania Lombrozo. Learning by thinking in natural and artificial minds. Trends in Cognitive Sciences, 2024. Raja Marjieh, Pol Van Rijn, Ilia Sucholutsky, Theodore Sumers, Harin Lee, Thomas L. Griffiths, and Nori Jacoby. Words are all you need? Language as an approximation for human similarity judgments. In The Eleventh International Conference on Learning Representations, 2023a. Raja Marjieh, Ilia Sucholutsky, Pol van Rijn, Nori Jacoby, and Thomas L. Griffiths. What language reveals about perception: Distilling psychophysical knowledge from large language models. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 45, 2023b. Raja Marjieh, Ilia Sucholutsky, Pol van Rijn, Nori Jacoby, and Thomas Griffiths. Large language models predict human sensory judgments across six modalities. Scientific Reports, 14(1):21445, 2024a. Raja Marjieh, Pol van Rijn, Ilia Sucholutsky, Harin Lee, Thomas L. Griffiths, and Nori Jacoby. In Proceedings of the Annual Meeting of the rational analysis of the speech-to-song illusion. Cognitive Science Society, volume 46, 2024b. R. Thomas McCoy, Shunyu Yao, Dan Friedman, Mathew D. Hardy, and Thomas L. Griffiths. Embers of autoregression show how large language models are shaped by the problem they are trained to solve. Proceedings of the National Academy of Sciences, 121(41):e2322420121, 2024. Joseph Melcher and Jonathan Schooler. The misremembrance of wines past: Verbal and perceptual expertise differentially mediate verbal overshadowing of taste memory. Journal of Memory and Language, 35(2):231245, 1996. Jakob Pete Niedermann, Ilia Sucholutsky, Raja Marjieh, Elif Celen, Thomas L. Griffiths, Nori Jacoby, and Pol van Rijn. Studying the effect of globalization on color perception using multilingual online recruitment and large language models. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 46, 2023. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. OpenAI. Introducing OpenAI o1, 2024a. URL https://openai.com/o1/. Accessed: 202409-28. OpenAI. DALLE 2, 2024b. URL https://openai.com/index/dall-e-2/. Accessed: 2024-10-01. Joshua Peterson, Joshua Abbott, and Thomas Griffiths. Evaluating (and improving) the correspondence between deep neural networks and human representations. Cognitive Science, 42 (8):26482669, 2018. Ben Prystawski, Paul Thibodeau, Christopher Potts, and Noah Goodman. Psychologicallyinformed chain-of-thought prompts for metaphor understanding in large language models. arXiv preprint arXiv:2209.08141, 2022. 13 Ben Prystawski, Michael Li, and Noah Goodman. Why think step by step? Reasoning emerges from the locality of experience. Advances in Neural Information Processing Systems, 36, 2024. Arthur Reber. Implicit learning of synthetic languages: The role of instructional set. Journal of Experimental Psychology: Human Learning and Memory, 2(1):88, 1976. Arthur Reber and Selma Lewis. Implicit learning: An analysis of the form and structure of body of tacit knowledge. Cognition, 5(4):333361, 1977. Alexa Romberg and Jenny Saffran. Statistical learning and language acquisition. Wiley Interdisciplinary Reviews: Cognitive Science, 1(6):906914, 2010. Jonathan Schooler. Re-representing consciousness: Dissociations between experience and metaconsciousness. Trends in Cognitive Sciences, 6(8):339344, 2002. Jonathan Schooler and Tonya Engstler-Schooler. Verbal overshadowing of visual memories: Some things are better left unsaid. Cognitive Psychology, 22(1):3671, 1990. Jonathan Schooler, Stellan Ohlsson, and Kevin Brooks. Thoughts beyond words: When language overshadows insight. Journal of Experimental Psychology: General, 122(2):166, 1993. Daniel Schwartz and Tamara Black. Inferences through imagined actions: Knowing by simulated doing. Journal of Experimental Psychology: Learning, Memory, and Cognition, 25(1):116, 1999. Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang. On second thought, lets not think step by step! Bias and toxicity in zero-shot reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, pp. 44544470, 2023. Richard Shiffrin and Melanie Mitchell. Probing the psychology of ai models. Proceedings of the National Academy of Sciences, 120(10):e2300963120, 2023. Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. To CoT or not to CoT? Chainof-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. StabilityAI. Stability AI developer platform - API reference, 2024. URL https://platform. stability.ai/docs/api-reference. Accessed: 2024-09-28. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Eoin Travers, Jonathan Rolison, and Aidan Feeney. The time course of conflict on the cognitive reflection test. Cognition, 150:109118, 2016. Esther Van den Bos and Fenna Poletiek. Intentional artificial grammar learning: When does it work? European Journal of Cognitive Psychology, 20(4):793806, 2008. Caren Walker, Tania Lombrozo, Joseph Williams, Anna Rafferty, and Alison Gopnik. Explaining constrains causal learning in childhood. Child development, 88(1):229246, 2017. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. MMLU-Pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022. 14 Bruce Whittlesea and Michael Dorken. Incidentally, things in general are particularly determined: An episodic-processing account of implicit learning. Journal of Experimental Psychology: General, 122(2):227, 1993. Joseph Williams and Tania Lombrozo. The role of explanation in discovery and generalization: Evidence from category learning. Cognitive Science, 34(5):776806, 2010. Joseph Williams and Tania Lombrozo. Explanation and prior knowledge interact to guide learning. Cognitive Psychology, 66(1):5584, 2013. Joseph Jay Williams, Tania Lombrozo, and Bob Rehder. The hazards of explanation: Overgeneralization in the face of exceptions. Journal of Experimental Psychology: General, 142(4):1006, 2013. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2024. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. Jian-Qiao Zhu and Thomas L. Griffiths. Incoherent probability judgments in large language models. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 46, 2024."
        },
        {
            "title": "A IMPLICIT STATISTICAL LEARNING TASK",
            "content": "To study cases involving implicit statistical learning, we consider an artificial grammar learning task. In the task, LLMs are provided with letter string train examples that belong to the same category, and are tasked to classify whether new string belongs to the same category. A.1 GENERATION OF ARTIFICIAL GRAMMAR LEARNING DATASET In the original psychology experiments (Fallshore & Schooler, 1993; Reber & Lewis, 1977), participants performed the classification task on strings generated by fixed finite state grammar (FSG) constructed by the researchers (see Figure 2). string is generated by the FSG if it corresponds to valid path along the directed edges from the source node to the sink node t, where the letters on the path are appended together. In our experiments, we expand the experiments massively to 100 randomly sampled FSGs that follow the same rough structure of those used in the experiment. To scale up the dataset, we construct and sample from all possible FSGs that obey the following rules. For visual representation please see Figure 3. 6 nodes total, including source s, sink t, and four nodes x1, . . . , x4. Edges (s, x1), (s, x3), (x2, t) and (x4, t) are always present. Edge (x1, x2) is always present to avoid isomorphisms and the null case where no paths exist from to t. The remaining middle edges {(x1, x3), (x1, x4), (x2, x1), (x2, x3), (x2, x4), (x3, x1), (x3, x2), (x3, x4), (x4, x1), (x4, x2), (x4, x3)} can either exist or not, for total of 211 combinations. Each xi can have self-loops, e.g., (x1, x1), for total of 24 combinations. Letters on each edge are randomly selected from the capital alphabet, for total of 268 combinations. Each FSG should be able to generate at least 37 unique strings with length 8. The construction of the FSG is unique with respect to the three graphical isomorphisms that each FSG satisfying the rules could have. For each FSG, we sampled paths of up to length 8 and used them as stimuli for the experiment. Following Fallshore & Schooler (1993), we sampled 37 to use in the experiment, assigning 15 to be training examples and 22 to be positive test examples. We also constructed 22 negative test examples by sampling random string from the FSG, perturbing one letter in randomly selected position to another letter that exists on some edge of the FSG. We ensured that the negative examples did not belong to the FSG. In total, this yielded 4400 individual questions asked to the large language models. Each question was asked individually after the 15 training examples. See the next section for the specific prompts. Figure 2: The FSG used in Fallshore & Schooler (1993) and Reber & Lewis (1977), two classic studies on artificial grammar learning. This FSG was used the generate strings for all participants in both studies. We form our dataset using FSGs that follow similar structure. 16 Figure 3: The potential FSGs used in our dataset. Directed edges that always exist are in black, while the others that could exist are dashed and in gray. Bi-directional arrows denote two potential directed arrows. The letters on each edge represent random sample. A.2 PROMPTS For our experiments, we prompted the models using one zero-shot prompt and one CoT prompt. The zero shot prompt is shown in Table 7. Table 7: Example prompt for artificial grammar learning task, zero shot. Prompt: These strings were generated according to certain set of rules. Does the following string also follow the same set of rules? [test example] Please ONLY answer Yes or No. The CoT prompt uses one of the most common prompting methods for chain-of-thought, replacing the last line with, Please reason about your answer before answering Yes or No. When conducting pilot experiments, we also tried version of the prompts where we asked models to memorize the following letter strings in the first line of the prompt as this was more in-line with the original human experiment. We found that results were extremely similar to the more general version shown above, and thus discarded this more specialized case. A.3 TREE-OF-THOUGHT EXPERIMENTS To analyze whether our hypotheses about model chain-of-thought extend to other types of inferencetime reasoning, we evaluated the performance of GPT-4o with tree-of-thought (ToT) (Yao et al., 2024) on subset of 10 artificial grammars, totaling 440 examples. Given the input prompt, we asked the LLM to generate five different thoughts to explain whether the string also followed the same set of rules as the in-context examples. Then, five votes were conducted to select the best thought, which we parsed and compared to the ground truth label. We found that ToT resulted in small improvement over the task (64.55% vs. 62.52% accuracy, see Table 8), but this performance was still much worse compared to GPT-4o zero-shot accuracy (94.00%). This suggests that the reduction in performance is not only associated with CoT, but also other types of inference-time reasoning. Table 8: Results comparing zero-shot, CoT, and ToT on subset of the artificial grammar learning task. Zero-shot CoT ToT Performance decrease (CoT) Performance decrease (ToT) p-value (CoT) p-value (ToT) GPT-4o 94.00% 62.52% 64.55% 31.48% 29.45% < 0.0001 < 0."
        },
        {
            "title": "B FACIAL RECOGNITION TASK",
            "content": "To study tasks where language impairs the recognition of visual stimuli, we focus on facial recognition task, where VLMs are asked to select one of five candidate images that matches the face of provided image. The original experiment in Schooler & Engstler-Schooler (1990) had participants view 30-second video of an individual robbing bank and then perform 20-minute distractor task, before either writing down descriptions of the robbers face or doing distractor task for 5 minutes. Participants were then provided with 8 verbally similar faces to choose from, and those who performed the written description performed much worse (38% vs. 64% accuracy) at identifying the robber. B.1 GENERATION OF FACIAL RECOGNITION DATASET To adapt this task to testing models, we made few design decisions. First, we chose to replace the initial video stimuli with an image of the persons face to allow for the testing of vision language models. Next, we chose to remove the distractor tasks. This decision was based on pilot results indicating that common psychology distractor tasks such as the n-back task (Kirchner, 1958) resulted in large amounts of noise in model outputs, while other distractors were of limited effect on the model due to it being able to retrieve the earlier stimuli in-context. Furthermore, even without the distractor, models already showed large difference in performance across zero-shot and CoT conditions. Thus, our task was simplified to facial matching task, where model was given human face as input and responded with the index of the matching face image as its output. To generate the faces for the facial recognition dataset, we use stable-image-ultra (StabilityAI, 2024). We experimented with other models such as DALL-E 2 (OpenAI, 2024b) and DALL-E 3, but found generation capabilities were significantly less realistic than stable-image-ultra. This difference was especially pronounced in generating realistic facial images of people in racial minorities. To cover diverse set of human faces, we prompt models to generate faces with features age {young, middle-aged, old}, race/ethnicity {asian, black, hispanic, white}, gender {man, woman}, eye color {brown, blue, green}, hair color {brown, black, blonde, red, gray}, hair length {long, short}, and hair type {curly, wavy, straight}. We removed some low-probability combinations such as red hair with asian ethnicity due to poorer quality of image generation. Then, we randomly sampled combinations of features to form descriptor set. One issue with stable-image-ultra is that when asked naively to generate an image of the same person as another image, it would alter some details such as ear shape, nose shape, or other facial ratios that would make it impossible to be the exact same person. We addressed this issue by prompting the stable-image-ultra image generation model to Generate two realistic images of the same person, one on the left and one on the right. The person should have the following description: [description]. After doing so, we were able to manually check and verify that the faces shown in the two images is clearly the same to the naked eye. One of these images was assigned to be the initial stimuli shown, while the other would be shuffled into the list of answers. We also ensured that the other remaining images were 1) clearly not of the same person as the image, and 2) the pose of the person, which was often similar between the pair of generated images, was also replicated in the other fake answers. This was achieved using the following prompt with the edit structure task in the set of image control API calls from StabilityAI: Generate an image of unique person with the same pose and style as the image provided. The person should have the following description: description. image input: [correct answer image] Once all the answers were generated, we manually verified the quality of generated images, and ensured that each of 1) and 2) were satisfied. An example of the images generated for problem are shown in Figure 4. 18 Figure 4: An example of the six images generated for problem. The first row contains one of the pair of generated images. The first image in the second row contains the other image in the pair, and the remaining four images are incorrect answers generated from this image. B.2 PROMPTS To evaluate models on the facial recognition task, we used one zero-shot prompt and one CoT prompt. The zero shot prompt is shown in Table 9. Table 9: Example prompt for facial recognition task, zero shot. Prompt: Here is an image of person. [image of initial person] Select the image that contains the same person as the person in the first image. [five images of possible matching faces] The CoT prompt uses the most original chain-of-thought prompting method by appending Lets think step by step to the end of the zero-shot prompt, with no other changes."
        },
        {
            "title": "C DATA WITH EXCEPTIONS TASK",
            "content": "In this task, we analyze the effect that CoT prompting has on the ability of LLMs to learn classification of objects that appear to follow pattern, but with exceptions. In these types of settings, Williams et al. (2013) reveal that when humans are given opportunities to deliberate after receiving feedback, they learn more slowly and make more errors compared to those who do not deliberate. The active form of thought mentally ingrains incorrect patterns that shift when exposed to successive unexpected answers, altogether leading to the creation of many deceptively incoherent lines of reasoning throughout the learning process that hinder the ability to directly keep track of the correct labels even after multiple passes. C.1 VEHICLE DATASET GENERATION We build off of the experimental set-up in Williams et al. (2013) where in each trial, we first create list of objects (vehicles) that are either warmor cold-climate, which is the label which we want models to learn. Based on this label, we generate one feature that correlates with this target label completely (see Column 2 of Table 10), and flip this 20% of the time to create exceptions in the data. In addition to this discriminating feature, following Williams et al. (2013), we also include 1) one unique feature which is different for each object and 2) three additional features whose values are randomized and have no connection with the object class. The unique feature in the original experiment was vehicle color, which we replaced with the license plate for realism. An example setup is depicted in Table 10. 19 Unique features Pattern-related features Irrelevant features License Plate Cold (Class A)/Warm (Class B) climate Transmission Seat covers Doors A23BCD B34EFG C45HIJ D56KLM E67NOP F78QRS G89TUV H90WXY J12ZAB K23CDE Drives on glaciers Made in Norway Used in mountain climbing Drives in jungles Has treads Heavily insulated Made in Africa Has wheels Lightly insulated Used on safaris Manual Automatic Automatic Manual Manual Manual Manual Automatic Manual Automatic Cloth Vinyl Vinyl Vinyl Cloth Vinyl Cloth Cloth Vinyl Vinyl Two Two Four Four Two Four Four Two Two Two Table 10: Sample vehicle classification list. Boldened features indicate flipped labels that break the initial classification pattern. We sampled 240 sets of 10 vehicles each and prompt the model to learn the labels of the vehicles in multi-turn setting, which we detail below. C.2 PROMPTS Models are provided with text descriptions of vehicles features one vehicle at time, iterating through the full set of ten vehicles repeatedly up to 15 times. Each time the model is given set of features, it predicts the corresponding label and subsequently receives feedback for its answer. In contrast to previous experiments, the problems, the models previous guesses, and the feedback given to the model are all stored in-context and provided to the model in its next prediction. In each iteration, the vehicles order shown to the participant is shuffled. Prompting stopped when the model correctly classified all of the vehicles in one iteration, or reached 15 iterations without performing this successfully. We used one zero-shot prompt and one CoT prompt. The zero-shot prompt was as follows in Table 11. Table 11: Example prompt for vehicle classification task, zero shot. [Chat history including previous prompts, model predictions, and feedback] Prompt: The vehicle description is as follows: License plate: [license plate] Descriptor: [descriptor] Transmission: [transmission] Seat Cover: [seat cover] Doors: [doors] Is this vehicle more likely to be Class or Class vehicle? Only answer with or B. In the CoT condition, instead of replicating the human study and asking the model to deliberate after each piece of feedback, we modify the prompt asking the model to make prediction. Specifically, we replace Only answer with or B. with Lets think step by step and answer with either or B. If you are unsure, feel free to guess and explain your reasoning. We append the last sentence because we observed that sometimes the model would refuse to answer based on lack of information. While we could have also implemented deliberations after each feedback to stay more faithful to the human experiment, our ultimate goal is to inform chain-of-thought, and CoT is most often applied during the process of asking questions to the model rather than having it reflect by itself. Furthermore, we believe that these settings are approximately equivalent: Deliberation in human experiments would focus on explaining the feedback provided, but this is also the case in this paradigm because the model would perform reasoning on the previous feedback provided when performing CoT during the prediction of the next label. 20 Figure 5: Aggregate learning curve (number of correct objects classified out of 10) for GPT-4o prompted via direct prompting and chain-of-thought over 15 iterations. Direct prompting attains perfection very quickly, whereas chain-of-thought prompting results in stagnation. C.3 PER-ROUND ACCURACY ANALYSIS Figure 5 depicts the aggregate accuracy (correctly predicted examples out of 10) of GPT-4o with direct and CoT prompts over 15 iterations through the list. Although CoT performs better than direct on the first iteration of the list, direct prompting quickly surpasses the performance of CoT by attaining perfect classification ability on the third iteration. Chain-of-thought prompting stagnates in performance at an accuracy level equivalent to the percentage of exemplars whose class designation adheres to the corresponding first-glance generalizable rule (80%). This suggests that the verbal thinking of CoT biases the model towards predicting via generalizable rules, even when there are more useful features that map exactly to correct answers in context. It is worth noting that CoTs tendency towards generalizable rules is often very helpful in other settings. For example, CoT does benefit from this tendency in the predictions of the first pass when all stimuli are previously unseen. This is in line with our conclusion that different strategies for prompting should be chosen based on the task, and neither is always better than the other."
        },
        {
            "title": "D LOGICAL INCONSISTENCY TASK",
            "content": "Here, participants were tasked to evaluate whether set of two statements were logically inconsistent. Statement pairs followed two forms: The first statement was always of the form B, where denotes implication, and the second statement was either of the form or B, where denotes the boolean AND operation, and denotes boolean negation. If the second statement was of the form B, the pair is inconsistent, whereas if the second statement was of the form B, the pair is consistent. Khemlani & Johnson-Laird (2012) found that if you ask humans to deliberate specifically as to why was plausible, they would subsequently be less accurate at identifying logical inconsistencies between the statements. D.1 LOGIC DATASET GENERATION To construct the dataset for the task, we first assigned claims to and B, and then filled in the template to construct the actual statements. To do the first part, we took statements where made logical sense following Khemlani & Johnson-Laird (2012). While the original authors simply hand-constructed 12 pairs of claims, we use combination of natural language inference (NLI) 21 datasets where pairs of statements are filtered to be of the entailment condition: MNLI, SNLI, and synthetic datset generated by prompting GPT-4o using the prompt: Generate list of 100 true statements of the format if then B. For each statement generate the result in JSON format with separate fields for index, and B. To construct the actual statements, we fit and into the templates in Table 12. Table 12: Sentence template for logical inconsistency task. Statement 1: If [A], then it is always the case that [B]. Statement 2 (conflict): [A], but it is not the case that [B]. Statement 3 (no conflict): It is not the case that [B]. In addition, to avoid having entailment pairs where statements are more than one sentence long or contain multiple clauses, we limited the maximum amount of words per claim (A or B) to seven. This allowed the sentences in the problem to flow smoothly, while still maintaining large population of entailment pairs. In total, we conducted experiments on 675 pairs from SNLI, 833 pairs from MNLI, and 100 pairs of claims that were synthetically generated, for final sum of 1608 pairs of {A, B}. This corresponded to 3216 questions asked per model, over which we calculated model accuracy. D.2 PROMPTS We prompted models using one zero-shot prompt and two CoT prompts. The prompt in the zero-shot condition was as follows: Table 13: Example prompt for logical inconsistency task, zero shot. Prompt: The following are two statements: 1. [Statement 1] 2. [Statement 2] Can both of these statements, as explicitly stated, be true at the same time? Please ONLY answer with Yes or No. The two chain-of-thought prompts altered the last line in the prompt to the following two sentences, respectively: Can both of these statements, as explicitly stated, be true at the same time? Please reason about your answer and then answer Yes or No. Can both of these statements, as explicitly stated, be true at the same time? Please first explain why statement 2 could be true and then answer Yes or No. Here, the first prompt follows the standard reason about your answer before answering CoT request, whereas the latter is more specific request aimed at more closely replicating the human study."
        },
        {
            "title": "E SPATIAL INTUITION TASK",
            "content": "In this task, participants were given drawings of two drinking glasses, one filled with water and one empty. They were asked to estimate the level of water that the second glass would need to be filled to such that the two glasses, when tilted to certain degree, would have the water they contain reach the rim of the glass at the same angle (Schwartz & Black, 1999). To simplify the task for the model, we changed the task from drawing line (image manipulation) to multiple choice (text output) by marking four separate heights on the side of the empty glass, labeling them through D, and asking the model to select letter. 22 Figure 6: An example of the water problem presented to large multimodal models. The glass on the left is filled with water, and the task is to determine which letter choice the empty glass should be filled to such that when the two glasses tilt to the same angle, water reaches each of their rims at the same time. E.1 MOTOR SIMULATION TASK DATASET GENERATION To scale up our dataset, instead of fixing the dimensions of the glass that contains water, we varied the width and height in {2, 3, 4} and {4, 5, 6} respectively (units are per 100 pixels). Then, following Schwartz & Black (1999), we created scenarios where the width and height of the empty cup was {wider, less wide, same width} and {taller, less tall, same height}. We also varied the amount of water that was in the original glass between { 1 2 , 3 4 } of its total height. Altogether, this resulted in 243 unique combinations of problems compared to the original 9. 4 , 1 For each problem, we computed the exact height that the empty cup would need to be filled with water to in order to get the water to the rim at the desired angle. Then, we sampled from Gaussian noise xi (0, 2) in order to generate the other answer choices {ai = + xi, {1, 2, 3}}, where 2 is half the distance from the correct answer to the maximum height of the glass. Furthermore, we ensured that none of the answer choices ai provided were above the maximum height of the cup, below zero, or within distance  of each other.  was an empirically determined parameter that controlled the difficulty of the problem, while also having lower bound due to limit for how closely the multiple choice letter options could be to each other on the graphical representation of the empty glass. visual representation of the final problem setup is in Figure 6. E.2 PROMPTS We use one zero-shot prompt and one CoT prompt. The zero-shot prompt is shown in Table 14. For the CoT prompt, we replaced Do not include anything else with Lets think step by step. Table 14: Example prompt for spatial intuition task, zero shot. Prompt: On the left of the figure provided, there is rectangular glass with its water level indicated with the blue line. On the right, there is target glass. The target glass has four height markings (A, B, C, D), each indicating different water level. Which marking should the target glass be filled to so that both glasses pour at the same angle? Provide your answer using the letter that matches the correct height marking. Do not include anything else. [Prompt image]"
        },
        {
            "title": "F WORKING MEMORY PREFERENCE TASK",
            "content": "In this task, participants were shown individual statements about one of four apartments frequently in succession. Each statement describes different aspect of one apartment, and participants tasks were to determine which apartment was overall most favorable. However, due to limits in human working memory, their performance in identifying the most beneficial apartment decreased when they tried to reason about the features of each apartment. F.1 APARTMENT DATASET GENERATION To extend this task to LLMs, we scaled up the number of stimuli to hopefully induce an increased pressure on the long-context capabilities of the model. Towards this effort, we first tested the limit of the amount of different features an apartment could have with the help of GPT-4o. We found that the model started repeating aspects of apartments after around 80 unique features. Then, we asked the model to generate positive, negative, and more neutral versions of statements regarding these features: Generate 80 positive statements about different aspects of an apartment. None of the statements should be about the same aspect. The following are 80 positive statements about aspects of an apartment. For each, generate corresponding negative statement that is the exact opposite. Make sure that all of the negative statements can coexist with positive statements that are not its direct correspondent. [positive statements] The following are 80 positive [. . . ] For each, generate corresponding neutral statement that is about the same aspect, is worse than the positive version, but is not negative. Make sure that all of the neutral statements can coexist with positive statements that are not its direct correspondent. [positive statements] The following are 80 negative [. . . ] For each, generate corresponding neutral statement that is about the same aspect, is better than the negative version, but is not positive. Make sure that all of the neutral statements can coexist with negative statements that are not its direct correspondent. [negative statements] We then manually considered conflicts between pairs of statements that were not of the same feature, and manually replaced the only feature statement that had conflict with another feature. Thus, cohesive descriptions of apartments could be sampled by randomly selecting one of the four statements for each of the 80 features. Next, we asked GPT-4o to rate the importance of each statement based on how much the statement affects the desirability of the apartment for the average tenant, from -5 to 5, with 5 being most desirable. Based on this, we could estimate the ground truth quality of each apartment by making the assumption that the features utilities sum up linearly.1 We then randomly sampled apartments with one statement per feature, and computed the score of an apartment as the mean of the feature scores. We then constructed sets of four apartments where the best apartment had at least an average score {[0.1, 0.3], [0.3, 0.5], [0.5, 1]} higher than the next-best option. This was to ensure that there is clear best apartment for the average tenant while not making the task too simple, which Intuitively, can be were also requirements in the original human study (Dijksterhuis, 2004). considered as difficulty level, where apartments are closer in rating for lower problems and are thus harder to get correct. Sampling randomly, this led to total of three datasets corresponding to three ranges of , each containing 100 sets of four apartments. F.2 PROMPTS For this task, we used one zero-shot and one CoT prompt in our evaluations. The zero-shot prompt is shown in Table 15. The CoT prompt replaces Respond with only the number of the apartment, do not include anything else. with Lets think step by step. 1Note that this is sometimes untrue; e.g., close proximity to grocery store is much more meaningful when an apartment has kitchen. 24 In our pilot experiments, we also tried variety of prompts such as replicating the distractor task using verbal n-back task, setting time limit for the model (i.e., you have three minutes to think about the problem) or using phrases such as very carefully think that were present in the original experiment, but the first resulted in too much noise whereas the latter two did not change the results. Table 15: Example prompt for working memory apartments task, zero shot. Prompt: You are an AI assistant designed to evaluate the desirability of four apartments for potential tenant. You will be given list of statements about the apartment candidates and how much the tenant likes or dislikes an apartment with the quality described by the statement. Your task is to determine which apartment is the most desirable based on the given criteria. The statements are as follows: [statements] Which apartment is most desirable to the tenant? Respond with only the number of the apartment, do not include anything else."
        }
    ],
    "affiliations": [
        "Center for Data Science, New York University",
        "Department of Computer Science, Princeton University",
        "Department of Psychology, Princeton University"
    ]
}