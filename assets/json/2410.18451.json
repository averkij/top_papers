{
    "paper_title": "Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs",
    "authors": [
        "Chris Yuhao Liu",
        "Liang Zeng",
        "Jiacai Liu",
        "Rui Yan",
        "Jujie He",
        "Chaojie Wang",
        "Shuicheng Yan",
        "Yang Liu",
        "Yahui Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this report, we introduce a collection of methods to enhance reward modeling for LLMs, focusing specifically on data-centric techniques. We propose effective data selection and filtering strategies for curating high-quality open-source preference datasets, culminating in the Skywork-Reward data collection, which contains only 80K preference pairs -- significantly smaller than existing datasets. Using this curated dataset, we developed the Skywork-Reward model series -- Skywork-Reward-Gemma-27B and Skywork-Reward-Llama-3.1-8B -- with the former currently holding the top position on the RewardBench leaderboard. Notably, our techniques and datasets have directly enhanced the performance of many top-ranked models on RewardBench, highlighting the practical impact of our contributions in real-world preference learning applications."
        },
        {
            "title": "Start",
            "content": "Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, Yahui Zhou Equal contribution, {yuhao.liuu, liang.zeng}@kunlun-inc.com Skywork AI, Kunlun Inc."
        },
        {
            "title": "Abstract",
            "content": "In this report, we introduce collection of methods to enhance reward modeling for LLMs, focusing specifically on data-centric techniques. We propose effective data selection and filtering strategies for curating high-quality open-source preference datasets, culminating in the Skywork-Reward data collection, which contains only 80K preference pairssignificantly smaller than existing datasets. Using this curated dataset, we developed the Skywork-Reward model seriesSkywork-Reward-Gemma-27B and Skywork-Reward-Llama-3.1-8Bwith the former currently holding the top position on the RewardBench leaderboard. Notably, our techniques and datasets have directly enhanced the performance of many top-ranked models on RewardBench, highlighting the practical impact of our contributions in real-world preference learning applications 1. 1. Introduction Large language models (LLMs) have achieved unprecedented success, demonstrating capabilities that were previously unattainable in both scope and performance (Achiam et al., 2023; Dubey et al., 2024; Team, 2024; Team et al., 2023, 2024a,b). This rapid advancement has fueled extensive research into aligning LLM outputs with user preferences (Bai et al., 2022). Among the various alignment strategies, reward modeling has emerged as prominent and scalable approach for capturing these preferences (Lambert et al., 2024; Wang et al., 2024e). Reward models are explicitly trained to evaluate how well the LLM outputs align with the intended responses desired by users, effectively acting as evaluators during both fine-tuning and deployment (Cai et al., 2024; Dong et al., 2024; Wang et al., 2024a,c, 2023). Despite its potential, training reward models poses several significant challenges (Lambert et al., 2024), primarily due to the inherent complexity and variability of human preferences, which are difficult to represent exhaustively (Sanderson et al., 2010). Prior research has sought to address these challenges by improving model architectures (Wang et al., 2024a,b) and developing customized loss functions (Cai et al., 2024; Lou et al., 2024; Winata et al., 2024), enabling reward models to better differentiate between nuanced preference pairs. These methods enhance the models capacity to prioritize preferred responses while minimizing rejected ones, thereby improving alignment with user preferences. In addition to these efforts, the availability and quality of preference data play pivotal role in the success of reward modeling. Unfortunately, 1The models and datasets are publicly available at https://huggingface.co/collections/Skywork/sky work-reward-model-66d7fbdebae0e60d00a6b60d and https://huggingface.co/collections/Skywo rk/skywork-reward-data-collection-66d7fda6a5098dc77035336d 4 2 0 2 4 2 ] . [ 1 1 5 4 8 1 . 0 1 4 2 : r open-source preference datasets are often noisy, with differences between preferred and rejected responses either overly subtle or inconsistently labeled (Park et al., 2024; Wang et al., 2024e; Xu et al., 2024). Such inconsistencies can significantly degrade the performance of reward models, underscoring the importance of meticulous data selection and filtering to ensure robust and reliable modeling. In this paper, we propose comprehensive suite of techniques to enhance reward modeling in LLMs, with particular focus on the curation of high-quality preference data. Specifically, we introduce lightweight yet effective preference data collections, relying solely on publicly available sources to ensure transparency and reproducibility. Our data selection and filtering strategies are designed to prioritize preference pairs that contribute most effectively to improving model performance. Additionally, we conduct extensive ablation studies on various loss functions, focusing on optimizing the margin between preferred and rejected responses. Our experimental results demonstrate that the vanilla Bradley-Terry loss (Bradley and Terry, 1952; Ouyang et al., 2022) consistently outperforms alternative approaches, underscoring its robustness in reward modeling tasks. We collectively employ these advanced training techniques to develop the Skywork-Reward model series and rigorously validate their effectiveness on the RewardBench benchmark (Lambert et al., 2024), demonstrating significant performance improvements with our proposed training techniques. As of October 2024, the Skywork-Reward model series holds the first and seventh positions on the RewardBench leaderboard (Lambert et al., 2024). Furthermore, our curated Skywork-Reward preference data collection has been widely adopted in subsequent research efforts (Lou et al., 2024; Winata et al., 2024; Yang et al., 2024; Zhang et al., 2024), highlighting its value and applicability. To promote further research and innovation in reward modeling for LLMs, we publicly release both the Skywork-Reward model series and the corresponding preference data collection. We hope that these contributions will inspire the future development of more aligned and human-centered LLMs. 2. Related Work Recent advancements in applying reinforcement learning techniques (Schulman et al., 2017), particularly Reinforcement Learning from Human Feedback (RLHF) (Bai et al., 2022; Casper et al., 2023), have shown substantial potential for enhancing LLMs. key component of RLHF is the development of reward models (Dubey et al., 2024; Gao et al., 2023; Team, 2024), which learn reward function based on human preferences or task-specific objectives to guide LLMs toward desired behaviors. As discussed by Lambert et al. (2024), reward modeling techniques can be broadly categorized into three categories based on the underlying model types: discriminative models, generative models, and implicit reward models through Direct Preference Optimization (DPO). We briefly describe each of them as follows. Discriminative Models Discriminative reward models are commonly trained using the BradleyTerry (BT) (Bradley and Terry, 1952) loss, which aims to maximize the reward difference between pairwise comparisonsspecifically, between chosen responses and rejected responses. These models estimate the probability that given response is preferred over an alternative, making them well-suited for binary ranking tasks. While the core BT loss remains standard component, considerable research has focused on enhancing data quality and refining the modeling framework. For example, the InternLM2-Reward models (Cai et al., 2024), trained on 2.4 million human-annotated and AI-generated preference samples, are optimized to classify pairwise comparisons, ensuring careful balance between helpfulness and harmlessness. Yang et al. (2024) improve the generalization ability of reward models by introducing regularization in the hidden states, mitigating the risk of over-optimization on specific reward functions. In complementary effort, Park et al. (2024) address inherent biases in reward modelssuch as the tendency to favor longer responsesby proposing de-biasing strategies in dataset construction. To capture more nuanced and complex preferences, models like Nemotron-Reward (Wang et al., 2024e) leverage multi-dimensional reward signals, allowing for more granular understanding of user preferences. Other methods introduce architectural modifications to boost performance. For instance, other than multi-dimensional rewards, ArmoRM (Wang et al., 2024a,b) also utilizes gating network that adaptively selects the most relevant reward dimension based on contextual information. Similarly, Zhang et al. (2024) explore the use of latent spaces within LLMs to model preferences, relying on similarity scores between responses to inform preference-based decisions. These advancements collaboratively push the boundaries of discriminative reward modeling, improving the ability of LLMs to align with diverse and subtle human preferences. Generative Models While discriminative models are widely adopted, generative models offer an alternative approach by directly using LLM-generated outputs to evaluate preference data (Zheng et al., 2023). Generative models excel in providing nuanced, interpretable assessments, capturing subtle differences in language use, and offering deeper insights into the decisionmaking process. However, their performance in reward modeling tasks often lags behind discriminative models (Lambert et al., 2024), as they are not specifically optimized to rank or select between pairwise comparisons. To bridge this gap, Wang et al. (2024c) introduces an auxiliary taskresponse deductionto enhance generative models ability to judge pairwise comparisons effectively based on textual outputs. Similarly, Self-Taught (Wang et al., 2024d) improves generative models through contrastive learning (Khosla et al., 2020), enabling them to generate preference judgments without relying on human annotations. Additionally, stateof-the-art chat-based LLMs like Gemini (Team et al., 2023) and GPT-4o (Achiam et al., 2023) demonstrate the potential of generative models by directly producing textual rewards. These advanced models leverage their powerful generative abilities to showcase the versatility of generative reward modeling in complex scenarios. Implicit Rewards via DPO Models third category, Direct Preference Optimization (DPO) Rafailov et al. (2024b), enables RLHF without the requirement of an explicitly trained reward model. Instead, DPO derives reward signal directly from the current policy and an initial supervised fine-tuned policy (Rafailov et al., 2024a), effectively reparameterizing preference learning within the model itself. While DPO models are not able to assign reward signals like discriminative model or generative model in nature, implicit rewards can be computed when corresponding supervised fine-tuned version of the model is available (Bellagente et al., 2024; Ivison et al., 2023). However, these models generally underperform compared to discriminative and generative models, which are explicitly optimized for reward modeling tasks. Our Skywork-Reward model series belong to the Discriminative Models category and have achieved top rankings on the RewardBench leaderboard (Lambert et al., 2024). 3. Method In this section, we describe our approach within Skywork-Reward to constructing lightweight yet high-quality preference dataset tailored for reward modeling. We outline the specific datasets 3 Figure 1 The composition chart of the Skywork-Reward preference data selections before and after applying data selection and filtering operations. used in our data mixture (section 3.1), the data selection and filtering techniques employed to optimize its composition (section 3.2), and the training objective that guides the reward models learning process (section 3.3). Our methodology aims to enhance the effectiveness of reward modeling while maintaining transparency and accessibility by focusing on solely publicly available preference data. We visualize the composition chart of the Skywork-Reward preference data selections in fig. 1. 3.1. Dataset Mixture Existing research (Dong et al., 2024; Jiang et al., 2023; Touvron et al., 2023) frequently leverages mixture of preference datasets from multiple sources to train reward models. These datasets typically contain between several hundred thousand to over million samples. For instance, Llama 2 (Touvron et al., 2023) employs approximately 1.5 million publicly available preference data points, augmented with 1.4 million internally generated samples, for reward model training. substantial portion of the public data originates from StackExchange, with the remainder capturing attributes such as helpfulness, harmlessness, and general human preferences. In similar vein, Dong et al. (2024) assemble more diverse dataset by aggregating samples from eight distinct sources, producing collection of around 700K preference pairs. Notably, approximately 90% of the responses in this dataset are generated by various LLMs, with more than half of the annotations sourced from GPT-3.5 and GPT-4. This growing reliance on LLMgenerated data underscores the increasing trend toward using automated systems for large-scale preference labeling in reward model development. We present the statistics of the Skywork Reward Preference data collections in table 1. lightweight yet high-quality data composition Our objective is to construct more lightweight preference data collection that not only reduces the overall data requirements but also targets important abilities and domains that RLHF seeks to optimize, such as math and code. Additionally, we focus exclusively on publicly available data to ensure transparency, reproducibility, and to enable broader adoption of our methodologies without reliance on proprietary or internal datasets. This strategy has resulted in the creation of the following dataset mixture, which we introduce below with brief overview of each included dataset. HelpSteer2 (Wang et al., 2024e) is compact preference dataset comprising only 10K 4 Dataset # Pairs Avg. # Avg. # Tokens Avg. # Tokens (Prompt) (Response) Turns Completion Annotator HelpSteer2 OffsetBias WildGuardMix Magpie Ultra Magpie Pro (Llama 3) Magpie Pro (Llama 3.1) Magpie Air Total 7,221 8,504 6,709 27,785 2,030 29,682 81,973 3.9 2 2 2 2 2 2 2.2 21.3 69.1 164.3 76.7 34.2 118.8 66.6 96.3 690.0 222.1 349.9 670.0 621.5 584.3 240. 527.2 Human + 6 LLMsa GPT-3.5 + GPT-4 + Claude 3 Opus 8 LLMsb Llama 3.1 405B Instruct Llama 3 70B Instruct Llama 3.1 70B Instruct Llama 3 8B Instruct - Human GPT-4 Human ArmoRM ArmoRM ArmoRM ArmoRM - Nemotron-2 43B, Nemotron-3 8B and 22B, Nemotron-4 15B and 340B, and Mixtral-8x7B-Instruct-v0.1. OLMo-7B-Instruct, GPT-3.5, Vicuna-7b-v1.5, Llama3-8B-Instruct, Mistral-7B-Instruct-v0.2, dolphin-2.9.1-llama-3-8b, dolphin-2.8-gemma-7b, and dolphin-2.8-mistral-7b-v02. Table 1 Statistics of the Skywork Reward Preference 80K dataset for reward modeling. The Avg. # Tokens (Prompt) and Avg. # Tokens (Response) columns are calculated using the tokenizer of Llama 3.1 8B Instruct. The Completion and Annotator columns indicate the source of the chosen or rejected response and the judge of the pairwise label, respectively. preference pairs2. The prompts are predominantly sourced from ShareGPT (RyokoAI, 2023), with responses generated by both LLMs and human annotators. Each response is annotated with five attributes: helpfulness, correctness, coherence, complexity, and verbosity. Despite its small size, this dataset contributed to developing the previously strongest reward model on RewardBench (Adler et al., 2024). OffsetBias (Park et al., 2024) is preference dataset of over 8K pairs, which aim to address various forms of bias and spurious signals commonly present in preference data, such as the tendency for longer responses to be perceived as better. The dataset includes rejected responses generated by robust models that appear well-formed but contain specific errors. The authors demonstrate that training on this adversarial data can significantly mitigate biases encoded during reward modeling. WildGuardMix (Han et al., 2024) is safety moderation dataset comprising diverse set of 92K benign and adversarial prompts, paired with corresponding compliance and refusal responses. The dataset includes both synthetic (vanilla and adversarial) and human-written prompts. For our purposes, we focus on the adversarial subset, which is constructed using the WildTeaming framework (Jiang et al., 2024) to generate challenging scenarios from benign and harmful user prompts. We only consider the training set of 87K samples. The Magpie series (Xu et al., 2024) is collection of four fully synthetic datasets generated by LLMs. The Magpie method leverages the tendency of autoregressive LLMs to generate user queries and assistant responses when provided with only prefix. We use the DPO version of the dataset, where chosen and rejected responses are determined based on ArmoRM (Wang et al., 2024b) scores. We consider four datasets synthesized by Llama 3.1 405B Instruct (50K), Llama 3.1 70B Instruct (98K), Llama 3 70B Instruct (98K), and Llama 3 8B Instruct (98K) (Dubey et al., 2024), corresponding to the names Ultra3, Pro (Llama 3.1)4, Pro (Llama 3)5, and Air6, respectively. 2Following Wang et al. (2024e), we only take pairs where the helpfulness score for the chosen response is higher than that of the rejected response. 3https://huggingface.co/datasets/argilla/magpie-ultra-v0.1 4https://huggingface.co/datasets/Magpie-Align/Magpie-Llama-3.1-Pro-DPO-100K-v0.1 5https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-DPO-100K-v0.1 6https://huggingface.co/datasets/Magpie-Align/Magpie-Air-DPO-100K-v0.1 5 Task Count Percentage Math Coding & debugging Information seeking Advice seeking Reasoning Planning Brainstorming Creative writing Data analysis Editing Role playing Total 29,657 8,193 7,837 4,546 3,854 2,185 1,081 794 725 337 330 59,539 49.81% 13.76% 13.16% 7.64% 6.47% 3.67% 1.82% 1.33% 1.22% 0.57% 0.55% 100% Figure 3 Distribution of task category in the selected Magpie preference pairs. Based on the average ArmoRM score, we select the top 30% examples from each of the Math and Code & debugging task categories independently. We also take the top 10% data from the rest of the task categories combined. Figure 2 Adjusted score distribution of the Magpie datasets. We calculate the average ArmoRM score of the generated responses in the Magpie dataset to guide data selection. We also manually reduce the Air and Pro (Llama 3) subsets to prioritize data synthesized by stronger models. The dashed vertical lines in the plot represent the mean ArmoRM scores for each subset. 3.2. Data Selection and Filtering The original composition of the seven datasets described above consists of approximately 378K samples (referred to as Preference 378K), which is considerably smaller than Preference 700K (Dong et al., 2024). However, this composition introduces certain challenges. For instance, the Magpie collection constitutes about 93% of the preference pairs, which could result in dilution effect, diminishing the influence of the other datasets. Furthermore, since the Magpie datasets are synthesized by models with varying capabilities, we can strategically prioritize higher-quality preference pairs to enhance the training efficacy of the reward model. In the following subsections, we detail our filtering process for the Magpie and WildGuardMix datasets, which together yield the final Skywork Reward Preference 80K. For HelpSteer2, we follow the methodology outlined in their paper (Wang et al., 2024e), utilizing only pairs where the selected response demonstrates higher helpfulness score compared to the rejected response. 3.2.1. Curating Magpie For the Magpie series, we utilize two key pieces of information: (1) the model used to generate the dataset and (2) the ArmoRM (Wang et al., 2024b) score associated with each chosen-rejected pair. In the Magpie subsets, each chosen-rejected pair is accompanied by ArmoRM scores for five generated responses, with the highestand lowest-scoring responses selected as the chosen and rejected responses, respectively. We assign the average score of the chosen and rejected responses as the overall score for each pair. This selection strategy has proven effective in 6 practice, capturing diverse range of pairs with varying reward differences (i.e., the difference between chosen and rejected rewards), though we do not claim it to be the optimal data selection method. Prioritizing data synthesized by stronger models We prioritize data generated by stronger models, as these are generally associated with higher-quality outputs (Xu et al., 2024). However, upon reviewing the ArmoRM scores, we observed that responses from the Air subsetgenerated by Llama 3 8B Instructoften received higher ratings than those from the Pro subsets, which were generated by Llama 3 70B Instruct and Llama 3.1 70B Instruct. We attribute this discrepancy to potential rating bias in ArmoRM, likely caused by distribution shifts for larger models. To address this, we chose to prioritize samples generated by larger or stronger models in the following order: Pro (Llama-3.1-70B-Instruct), Pro (Llama-3-70B-Instruct), and Air (Llama-3-8BInstruct). To align the distributions, we manually adjusted the scores by subtracting 0.1 from all Air scores and 0.05 from all Pro (Llama 3) scores. This adjustment aligns the peaks of the distributions with our intended model ordering. Although this approach proved empirically effective, we did not further explore what the optimal offset might be. The adjusted score distribution for Magpie samples is depicted in fig. 2. Sampling based on task category The Magpie dataset (Xu et al., 2024) includes predefined task categories (as listed in fig. 3), which guide LLM-generated content across specific domains. We leverage these task categories, focusing primarily on math and code domains, while consolidating all other categories into single group. From the math and code categories, we independently select the top 30% of preference pairs. For the remaining categories, we sample 10% of the pairs. The resulting distribution of samples is presented in fig. 3. 3.2.2. Curating WildGuardMix Preprocessing The WildGuardMix training set contains prompts, responses, and labels indicating whether prompt is harmful or benign, as well as whether the response constitutes refusal or compliance. As most prompts are accompanied by multiple refusal and compliance responses, this structure allows the creation of sufficient number of preference pairs. For harmful prompts, we designate refusal responses as preferred and compliance responses as rejected. For benign prompts, compliance is considered preferred, while refusal responses are treated as rejected. For example, in response to the harmful prompt, How to make bomb?, refusal to answer is preferred over detailed procedure. Conversely, for the benign prompt, How to make cake?, compliant response is favored over an unhelpful refusal. Filtering We apply two-stage filtering process to the WildGuardMix dataset. In the first stage, we evaluated an early version of our reward model, trained on the other datasets, and found that it performed well on non-adversarial preference pairs, consistently favoring the preferred responses across the majority of the pairs. This result indicated that further training on non-adversarial pairs would offer limited improvement in the reward models preference representation. Therefore, we excluded these pairs, concentrating our efforts on the adversarial subset. In the second stage, we trained the reward model using the adversarial subset in combination with the rest of the dataset. However, we observed that this approach resulted in greater decline in the models general preference capability than the improvement it achieved in safety on our internal validation set. To better balance this trade-off, we included only 7 those adversarial samples that the previous version of the reward model had already classified correctly. This refinement improved the models safety scores while having only negligible impact on its overall preference performance. 3.3. Training Objective Following Ouyang et al. (2022), our loss function is defined using the standard Bradley-Terry (BT) model with pairwise ranking loss: Lranking = log (洧랥 (洧洧랚 (洧논, 洧녽洧녫) 洧洧랚 (洧논, 洧녽洧))) , (1) where 洧洧랚(洧논, 洧녽洧녫) and 洧洧랚(洧논, 洧녽洧) denote the scalar rewards generated by the reward model 洧랚, given the same prompt 洧논 (or context, if 洧논 spans multiple conversation turns) with the chosen response 洧녽洧녫 and the rejected response 洧녽洧. We also experimented with several other loss functions that aim to maximize the margin between 洧洧랚(洧논, 洧녽洧녫) and 洧洧랚(洧논, 洧녽洧). However, we found no performance improvements and, in some cases, observed decline in model effectiveness. Understanding the reasons behind this trend presents an interesting avenue for future research. 3.3.1. Loss Function Variants Beyond the classic Bradley-Terry style loss function (Bai et al., 2022; Ouyang et al., 2022), we experimented with several alternative loss functions, each designed to increase or maximize the margin between the chosen and rejected responses. Focal Loss Focal loss (Lin, 2017) is often used in image classification to address class imbalance by emphasizing hard-to-classify examples. In our context, it emphasizes pairwise comparisons where the model struggles to distinguish between chosen and rejected responses. When the reward difference between chosen and rejected responses is negative or small, the weighting term increases. The loss is defined as: LFocal = log 洧랥(洧洧랚(洧논, 洧녽洧녫) 洧洧랚(洧논, 洧녽洧)) (1 洧랥(洧洧랚(洧논, 洧녽洧녫) 洧洧랚(洧논, 洧녽洧)))洧 , (2) where 洧 is the focal loss parameter controlling the down-weighting of easier examples. Focal Loss with Penalty (Cai et al., 2024) This variant introduces an additional penalty to further discourage predictions close to tie (i.e., 洧랥(洧洧랚(洧논, 洧녽洧녫) 洧洧랚(洧논, 洧녽洧)) 0.5), encouraging the model to make more confident decisions. The loss function is given by: LFocal-Penalty = (1 2 max (洧랥(洧洧랚(洧논, 洧녽洧녫) 洧洧랚(洧논, 洧녽洧)) 0.5, 0))洧 log 洧랥(洧洧랚(洧논, 洧녽洧녫) 洧洧랚(洧논, 洧녽洧)), (3) where 洧 adjusts the emphasis on difficult comparisons. Hinge Loss Hinge loss (Sch칬lkopf et al., 2001) is widely used in classification problems, particularly with Support Vector Machines (SVMs), to enforce margin between classes. Here, it enforces margin between the reward scores of chosen and rejected responses: LHinge = max(0, 洧녴 (洧洧랚(洧논, 洧녽洧녫) 洧洧랚(洧논, 洧녽洧))), (4) where 洧녴 is the margin parameter, encouraging separation of at least 洧녴 between the reward scores. 8 Margin Mean Squared Error (MSE) (Friedman et al., 2001) This loss combines the concept of margin with mean squared error, enforcing that the reward for the chosen response exceeds that of the rejected response by specified margin: LMargin-MSE = (洧洧랚(洧논, 洧녽洧녫) (洧洧랚(洧논, 洧녽洧) + 洧녴))2 , (5) where 洧녴 is the margin parameter. Cross-Entropy (CE) (Goodfellow et al., 2016) Cross-entropy loss is standard approach in classification tasks. In this ranking context, it is treated as binary classification problem between the chosen response 洧녽洧녫 and the rejected response 洧녽洧, based on their reward scores: LCE = (cid:2)log 洧랥(洧洧랚(洧논, 洧녽洧녫)) + log(1 洧랥(洧洧랚(洧논, 洧녽洧)))(cid:3) . (6) Bradley-Terry with Tempered Log (Carvalho et al., 2010) We modify the log functions curvature from concave to convex as follows: Lranking = (cid:104) 1 1 洧노 (洧랥 (洧洧랚 (洧논, 洧녽洧녫) 洧洧랚 (洧논, 洧녽洧)))1洧노 1 (cid:105) , (7) where 洧노 is set to negative value. Bradley-Terry with Temperature (Bradley and Terry, 1952) We also explored tuning the sharpness of the distribution with temperature parameter 洧녢: Lranking = log (cid:18) 洧랥 (cid:18) 洧洧랚 (洧논, 洧녽洧녫) 洧洧랚 (洧논, 洧녽洧) 洧녢 (cid:19)(cid:19) . (8) We tested each of these loss functions in an attempt to improve upon the Bradley-Terry model. Despite the theoretical motivations behind these variants, none consistently outperformed the baseline in terms of overall model performance, as shown in table 3. 4. Experiment This section outlines the training setup, baseline methods for comparison, and evaluation criteria (section 4.1). We then present quantitative results and provide insights gained from the experiments (section 4.2). 4.1. Experimental Setup 4.1.1. Training Hyperparameters and Training We use existing aligned models, Meta-Llama-3.1-8B-Instruct (Dubey et al., 2024) and Gemma-2-27B-it (Team, 2024), as backbones, replacing the final layer with randomly initialized reward head. Both models are trained with global batch size of 128, using AdamW as the optimizer with weight decay of 1e-3 and cosine learning rate schedule. Training spans 2 epochs on the Skywork Reward Preference 80K dataset. The learning rate is set to 2e-6 for the 8B model and 1e-6 for the 27B model. 9 4.1.2. Baselines and Evaluation Preference Dataset Baselines To demonstrate the advantages of the Skywork Reward Preference 80K dataset, we compare it with the dataset mixture from RLHFlow (Dong et al., 2024), which serves as baseline. RLHFlow integrates data from several well-known preference sources, including HH-RLHF (Bai et al., 2022), SHP (Ethayarajh et al., 2022), HelpSteer (Wang et al., 2023), PKU-SafeRLHF (Ji et al., 2024), UltraFeedback (Cui et al., 2023), UltraInteract (Yuan et al., 2024), Distilabel-Capybara (Daniele and Suphavadeeprasit, 2023), and Distilabel-Orca (Lian et al., 2023). This dataset mixture comprises approximately 700K samples, which we denote as Preference 700K. We train both the 8B and 27B models following the approach outlined by Dong et al. (2024). Additionally, we perform an ablation study by using only the 378K samples from our full dataset to validate the effectiveness of our filtering process. For the 378K dataset, we train for 2 epochs to ensure the number of gradient updates matches those used for Preference 700K and Skywork Reward Preference 80K. Reward Model Baselines We compare the performance of our reward models, trained on Skywork Reward Preference 80K, with the top-performing models from the RewardBench leaderboard. As of this writing, the leading reward models include SFR-LLaMa-3.1-70B-Judge-I, Nemotron-4-340B-Reward (Wang et al., 2024e), ArmoRM (Wang et al., 2024b), SFR-nemo-12BJudge-r, and InternLM-20B-Reward (Cai et al., 2024). Evaluation on RewardBench Our models are evaluated on RewardBench (Lambert et al., 2024), benchmark designed to assess reward models across multiple tasks, such as chat, reasoning, and safety. RewardBench contains prompt-chosen-rejected trios that measure models ability to assign higher scores to the chosen response compared to the rejected one. These trios are derived from diverse datasets, covering general chat, safety, and reasoning domains. Successful performance on this benchmark requires reward models to exhibit balanced and robust capabilities across all categories, rather than excelling in only one area. 4.2. Experimental Results We present our main results in table 2. Below are key observations: Small but high-quality datasets yield the best reward models. Skywork-Reward-Gemma-227B ranks first on RewardBench, while Skywork-Reward-Llama-3.1-8B surpasses all models except SFR-LLaMa-3.1-70B-Judge-I. Despite the smaller model size, straightforward training approach, and limited training data, our models demonstrate robust performance across all four categories, excelling particularly in the adversarial preference category on Chat Hard. Notably, the 27B reward model is the only model to achieve score above 90 on Chat Hard, outperforming the next-best model, Nemotron-4-340B-Reward, by more than four points, with score of 87.1. Quality over quantity. As shown in table 2, Llama 3 trained on the complete 378K samples outperforms both reward models trained on Preference 700K, as well as most other models, with the exception of SFR-LLaMa-3.1-70B-Judge-I and Nemotron-4-340B-Reward. Compared 10 Type Avg. Score Chat Chat Hard Safety Reasoning Model SFR-LLaMa-3.1-70B-Judge-I (Wang et al., 2024c) Nemotron-4-340B-Reward (Wang et al., 2024e) ArmoRM-Llama3-8B-v0.1 (Wang et al., 2024b) SFR-nemo-12B-Judge-r (Wang et al., 2024c) InternLM-20B-Reward (Cai et al., 2024) Llama-3-OffsetBias-RM-8B (Park et al., 2024) gemini-1.5-pro-0924 (Team et al., 2024a) gpt-4o-2024-08-06 (Achiam et al., 2023) Generative Custom Custom Generative Discriminative Discriminative Generative Generative Llama-3.1-8B Dubey et al. (2024) + Preference 700K Discriminative Gemma-2-27B (Team et al., 2024b) + Preference 700K Discriminative Llama-3.1-8B Dubey et al. (2024) + Preference 378K Discriminative Gemma-2-27B Team et al. (2024b) + Preference 378K Discriminative Skywork-Reward-Llama-3.1-8B Skywork-Reward-Gemma-2-27B Discriminative Discriminative 92.7 92.2 90.8 90.3 90.2 89.4 86.8 86.7 86.9 88.1 91.8 92.6 92.5 93.8 96.9 95.8 96.9 97.2 98.9 97.2 94.1 96. 98.0 97.5 94.6 94.4 95.8 95.8 84.8 87.1 76.8 82.2 76.5 81.8 77.0 76.1 67.3 71.7 84.5 87.5 87.3 91.4 91.6 92.2 92.2 86.5 89.9 86.8 85.8 88. 89.4 90.0 91.5 91.9 90.6 92.0 97.6 93.6 97.3 95.1 95.8 91.9 90.2 86.6 93.0 93.4 96.5 96.7 96.2 96.1 Table 2 Performance comparison of different reward models on RewardBench. The first block of the table includes the top reward models on the RewardBench leaderboard. The superscript in this block indicates that the results have not been officially verified. The second block of the table corresponds to Llama-3.1-8B and Gemma-2-27B (both instruct version) trained on Preference 700K and Preference 378K data, respectively. The final block of the table showcases the performance of our Skywork-Reward model series, which are trained on the Skywork Reward Preference 80K dataset. Notably, Skywork-Reward-Gemma-2-27B achieves state-ofthe-art performance, outperforming several competitive models on RewardBench. The highest performance in each column is masked as bold. to Preference 700K, the 378K dataset provides competitive advantage in Chat Hard while maintaining balanced performance across all four categories. Further dataset filtering and selection. Following the release of our models, we conducted more detailed analysis of the Skywork Reward Preference 80K dataset, including manual inspections and additional filtering using multiple LLMs. From refined subset of 66K preference pairs, we achieved scores of 96.3 and 94.9 on the 27B and 8B reward models, respectively. We extended this process to include carefully selected samples from previously discarded Magpie data, adding 20K more samples. Incorporating these samples further boosted the RewardBench scores to 96.8 and 95.5 for the 27B and 8B models, respectively. However, we have opted not to release these enhanced models yet, as they require further testing within our RLHF pipeline. Additionally, it remains unclear whether the high RewardBench scores reflect overfitting or genuinely improved reward signals in RLHF. Bradley-Terry loss remains the best overall. As demonstrated in table 3, the Bradley-Terry loss achieves the highest average score of 93.8, outperforming other loss function variants. While certain alternatives, such as Focal loss and Bradley-Terry with temperature, show marginal improvements in areas like Chat Hard, Safety, and Reasoning, these gains come at the cost of performance in the Chat category. Overall, the Bradley-Terry loss strikes the most effective balance across all categoriesChat, Chat Hard, Safety, and Reasoningmaintaining its position as the best-performing loss function for our models. 11 Loss function Avg. Score Chat Chat Hard Safety Reasoning Focal (Lin, 2017) Focal with penalty (Cai et al., 2024) Hinge (Sch칬lkopf et al., 2001) Margin MSE (Friedman et al., 2001) Cross-entropy (Goodfellow et al., 2016) Tempered log (Carvalho et al., 2010) Temperature-adjusted Bradley-Terry(Bradley and Terry, 1952) Bradley-Terry (Bradley and Terry, 1952) 93.6 93.4 93.3 92.3 87.6 92.9 93.7 93.8 94.3 93.9 94.1 90.2 74.9 96.4 94.3 95.8 91.8 91.5 90.2 89.0 87.3 87.4 91.7 91.4 92.0 92.0 92.6 93.3 94.0 91.8 92.7 92.0 96.5 96.5 96.3 96.7 94.5 96.2 96.3 96. Table 3 Ablation studies of loss functions that optimize the margin between chosen and rejected responses on Gemma-2-27B. 4.3. Potential Prompt Contamination During the preparation of this manuscript, we were informed by the RewardBench (Lambert et al., 2024) team of potential contamination involving approximately 5K prompts from the Magpie Ultra7 (Xu et al., 2024) subset, which may overlap with prompts present in the RewardBench evaluation set. Although the root cause of the overlap remains unclear, the RewardBench team suspects that Llama-3.1-405B-Instruct (Dubey et al., 2024), which was used to generate the Magpie Ultra dataset, may have been trained on these prompts. RewardBench evaluations rely on external sources (e.g., LLMBar (Zeng et al., 2023)), some of which contain prompts derived from widely utilized training datasets, such as Alpaca (Taori et al., 2023). This overlap has inadvertently introduced contamination into the Skywork Reward Preference 80K v0.1 dataset8. To address this issue, we applied decontamination script9 provided by the RewardBench leaderboard maintainers to compute detailed contamination statistics, as presented in table 4. We subsequently removed all pairs containing contaminated prompts from the Magpie Ultra subset, resulting in the creation of the v0.2 version of the Skywork Reward Preference 80K dataset. It is worth noting that some minor contamination likely persists across other subsets. These instances are scattered and originate from various sources, making them challenging to detect, though they are likely benign. As we show in later sections, removing contamination leads to improved performance in our reward models. Pervasive Contamination in (Synthetic) Preference Data It is important to acknowledge that the contamination issue is not unique to Skywork Reward Preference 80K. Other widely used preference datasets, such as Preference 700K (Dong et al., 2024) and Nectar (Zhu et al., 2023), are similarly affected. These datasets are frequently employed to train many open-weight reward models on the RewardBench leaderboard, including several top-ranking models. In table 4, we show that Preference 700K contains considerable number of prompts matching those in the RewardBench test set, both in terms of coverage and absolute counts. This underscores the need for more comprehensive investigations into data contamination and stricter dataset selection criteria in evaluations. 7https://huggingface.co/datasets/argilla/magpie-ultra-v0.1 8We refer to the contaminated dataset as v0.1 and the decontaminated version as v0.2. 9https://gist.github.com/natolambert/1aed306000c13e0e8c5bc17c1a5dd300 12 Dataset Preference 700K Nectar Skywork Reward Preference 80K v0.1 Skywork Reward Preference 80K v0.2 # of RewardBench Prompts With >7-Gram Match # of Contaminated Prompts 800 381 673 460 15,349 2,394 5,402 445 Table 4 Contamination statistics calculated by the decontamination script provided by the maintainer of the RewardBench leaderboard. The number of RewardBench prompts with larger than 7-gram match refer to larger than 7-gram match between the RewardBench prompts and prompts from the target dataset. The decontamination script uses n-gram range from 7 to 13. The number of contaminated prompts indicates the number of prompts satisfying the matching criteria. Skywork Reward Preference 80K v0.2 is the decontaminated version of v0.1. Model Avg. Score Chat Chat Hard Safety Reasoning Skywork-Reward-Llama-3.1-8B Skywork-Reward-Gemma-2-27B 92.5 93.8 95.8 95. 87.3 91.4 90.6 92.0 96.2 96.1 Skywork-Reward-Llama-3.1-8B (Decontaminated) Skywork-Reward-Gemma-2-27B (Decontaminated) 93.1 ( 0.6) 94.3 ( 0.5) 94.7 ( 1.1) 96.1 ( 0.3) 88.4 ( 1.1) 89.9 ( 1.5) 92.7 ( 2.1) 93.0 ( 1.0) 96.7 ( 0.5) 98.1 ( 2.0) Table 5 Performance comparison between our original Skywork-Reward model series trained on the full 80K pairs and the retrained reward models on the decontaminated 77K pairs. Both models trained on data free from contamination not only did not experience drop in overall performance, but also demonstrated improvements in Safety and Reasoning. Removing Contamination Leads to Higher Scores We retrained our reward models using the decontaminated Skywork Reward Preference 80K v0.2 dataset, following the same hyperparameters as before. small validation set from the remaining (decontaminated) portion of the Magpie dataset was used for early stopping. As shown in table 5, models trained on the decontaminated dataset achieved higher scores across all categories except Chat. This raises questions about the impact of the original contamination, as genuine contamination would typically result in highernot lowerscores in the v0.1 version. We also experimented with retraining our reward models on an entirely clean v0.2 dataset by removing all pairs containing matched prompts. The results, however, remained virtually identical to those shown in table 5 with minimal hyperparameter tuning. Manual inspection of the contaminated prompts revealed no obvious differences compared to the uncontaminated ones, leading us to hypothesize that many of the removed pairs may represent preferences misaligned with those measured by RewardBench. However, definitive conclusion would require deeper examination of the specific selected and rejected pairs, which we leave for future work. 5. Closing Remarks In this report, we introduce the Skywork-Reward Preference 80K data collection and demonstrate that carefully curated smaller, high-quality datasets can outperform both the complete data composition and much larger counterparts. Despite using fewer samples and straightforward training setup, our modelsSkywork-Reward-Gemma-2-27B and Skywork-Reward-Llama-3.18Bhave achieved state-of-the-art performance on RewardBench, excelling across multiple 13 categories and setting new benchmark in the Chat Hard category. These results highlight the value of prioritizing data quality over quantity, as well as the importance of targeted filtering and selection in the construction of preference datasets. Our findings emphasize that careful curation not only reduces data redundancy but also improves overall performance. We also addressed the pervasive issue of prompt contamination by releasing decontaminated v0.2 version of the dataset, which further empirically improved scores across most categories. Furthermore, our experiments reaffirmed the Bradley-Terry loss as the most effective loss function in our setting, striking the optimal balance across various tasks. These findings underscore the necessity of precise alignment between datasets and evaluation criteria, providing valuable insights for the development and assessment of reward models."
        },
        {
            "title": "References",
            "content": "J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. B. Adler, N. Agarwal, A. Aithal, D. H. Anh, P. Bhattacharya, A. Brundyn, J. Casper, B. Catanzaro, S. Clay, J. Cohen, et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704, 2024. Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. M. Bellagente, J. Tow, D. Mahan, D. Phung, M. Zhuravinskyi, R. Adithyan, J. Baicoianu, B. Brooks, N. Cooper, A. Datta, et al. Stable lm 2 1.6 technical report. arXiv preprint arXiv:2402.17834, 2024. R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Z. Cai, M. Cao, H. Chen, K. Chen, K. Chen, X. Chen, X. Chen, Z. Chen, Z. Chen, P. Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. C. R. Carvalho, A. D. Polson, and J. G. Scott. The dangers of inference using the bradley-terry model. The Annals of Statistics, 38(3):14911514, 2010. S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando, R. Freedman, T. Korbak, D. Lindner, P. Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023. G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu, and M. Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023. L. Daniele and Suphavadeeprasit. Amplify-instruct: Synthetically generated diverse multiturn conversations for effecient llm training. arXiv preprint arXiv:(coming soon), 2023. URL https://huggingface.co/datasets/LDJnr/Capybara. H. Dong, W. Xiong, B. Pang, H. Wang, H. Zhao, Y. Zhou, N. Jiang, D. Sahoo, C. Xiong, and T. Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. K. Ethayarajh, Y. Choi, and S. Swayamdipta. Understanding dataset difficulty with V-usable In International Conference on Machine Learning, pages 59886008. PMLR, information. 2022. J. H. Friedman, T. Hastie, and R. Tibshirani. The Elements of Statistical Learning. Springer, 2001. L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT press, 2016. 15 S. Han, K. Rao, A. Ettinger, L. Jiang, B. Y. Lin, N. Lambert, Y. Choi, and N. Dziri. Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms. arXiv preprint arXiv:2406.18495, 2024. H. Ivison, Y. Wang, V. Pyatkin, N. Lambert, M. Peters, P. Dasigi, J. Jang, D. Wadden, N. A. Smith, I. Beltagy, and H. Hajishirzi. Camels in changing climate: Enhancing lm adaptation with tulu 2, 2023. J. Ji, M. Liu, J. Dai, X. Pan, C. Zhang, C. Bian, B. Chen, R. Sun, Y. Wang, and Y. Yang. Beavertails: Towards improved safety alignment of llm via human-preference dataset. Advances in Neural Information Processing Systems, 36, 2024. D. Jiang, X. Ren, and B. Y. Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023. L. Jiang, K. Rao, S. Han, A. Ettinger, F. Brahman, S. Kumar, N. Mireshghallah, X. Lu, M. Sap, Y. Choi, et al. Wildteaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models. arXiv preprint arXiv:2406.18510, 2024. P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot, C. Liu, and D. Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33: 1866118673, 2020. N. Lambert, V. Pyatkin, J. Morrison, L. Miranda, B. Y. Lin, K. Chandu, N. Dziri, S. Kumar, T. Zick, Y. Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. W. Lian, B. Goodson, E. Pentland, A. Cook, C. Vong, and \"Teknium\". Openorca: An open dataset of gpt augmented flan reasoning traces. https://https://huggingface.co/Open-Orc a/OpenOrca, 2023. T. Lin. Focal loss for dense object detection. arXiv preprint arXiv:1708.02002, 2017. X. Lou, D. Yan, W. Shen, Y. Yan, J. Xie, and J. Zhang. Uncertainty-aware reward model: Teaching reward models to know what is unknown. arXiv preprint arXiv:2410.00847, 2024. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. J. Park, S. Jwa, M. Ren, D. Kim, and S. Choi. Offsetbias: Leveraging debiased data for tuning evaluators. arXiv preprint arXiv:2407.06551, 2024. R. Rafailov, J. Hejna, R. Park, and C. Finn. From 洧 to 洧: Your language model is secretly q-function. arXiv preprint arXiv:2404.12358, 2024a. R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024b. RyokoAI. ShareGPT52K Dataset. https://huggingface.co/datasets/RyokoAI/ShareG PT52K, 2023. M. Sanderson, M. L. Paramita, P. Clough, and E. Kanoulas. Do user preferences and evaluation measures line up? In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 555562, 2010. 16 B. Sch칬lkopf, A. J. Smola, K. R. M칲ller, P. J. Bartlett, W. S. Davidson, D. C. P. J. M. Williamson, and R. C. R. Sch칬lkopf. Kernel methods for pattern analysis. In Proceedings of the IEEE, volume 12, pages 406417. IEEE, 2001. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab /stanford_alpaca, 2023. G. Team. Gemma. 2024. doi: 10.34740/KAGGLE/M/3301. URL https://www.kaggle.com /m/3301. G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. G. Team, M. Reid, N. Savinov, D. Teplyashin, L. Dmitry, T. Lillicrap, J. Alayrac, R. Soricut, A. Lazaridou, O. Firat, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. in arxiv [cs. cl]. arxiv, 2024a. G. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. Ram칠, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024b. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. H. Wang, Y. Lin, W. Xiong, R. Yang, S. Diao, S. Qiu, H. Zhao, and T. Zhang. Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards. In ACL, 2024a. H. Wang, W. Xiong, T. Xie, H. Zhao, and T. Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. In EMNLP, 2024b. P. Wang, A. Xu, Y. Zhou, C. Xiong, and S. Joty. Direct judgement preference optimization. arXiv preprint arXiv:2409.14664, 2024c. T. Wang, I. Kulikov, O. Golovneva, P. Yu, W. Yuan, J. Dwivedi-Yu, R. Y. Pang, M. Fazel-Zarandi, J. Weston, and X. Li. Self-taught evaluators. arXiv preprint arXiv:2408.02666, 2024d. Z. Wang, Y. Dong, J. Zeng, V. Adams, M. N. Sreedhar, D. Egert, O. Delalleau, J. P. Scowcroft, N. Kant, A. Swope, et al. Helpsteer: Multi-attribute helpfulness dataset for steerlm. arXiv preprint arXiv:2311.09528, 2023. Z. Wang, Y. Dong, O. Delalleau, J. Zeng, G. Shen, D. Egert, J. J. Zhang, M. N. Sreedhar, and O. Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint arXiv:2406.08673, 2024e. G. I. Winata, D. Anugraha, L. Susanto, G. Kuwanto, and D. T. Wijaya. Metametrics: Calibrating metrics for generation tasks using human preferences. arXiv preprint arXiv:2410.02381, 2024. Z. Xu, F. Jiang, L. Niu, Y. Deng, R. Poovendran, Y. Choi, and B. Y. Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. R. Yang, R. Ding, Y. Lin, H. Zhang, and T. Zhang. Regularizing hidden states enables learning generalizable reward model for llms. arXiv preprint arXiv:2406.10216, 2024. L. Yuan, G. Cui, H. Wang, N. Ding, X. Wang, J. Deng, B. Shan, H. Chen, R. Xie, Y. Lin, et al. Advancing llm reasoning generalists with preference trees. arXiv preprint arXiv:2404.02078, 2024. Z. Zeng, J. Yu, T. Gao, Y. Meng, T. Goyal, and D. Chen. Evaluating large language models at evaluating instruction following. arXiv preprint arXiv:2310.07641, 2023. Y. Zhang, G. Zhang, Y. Wu, K. Xu, and Q. Gu. General preference modeling with preference representations for aligning language models. arXiv preprint arXiv:2410.02197, 2024. L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. B. Zhu, E. Frick, T. Wu, H. Zhu, and J. Jiao. Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November 2023."
        }
    ],
    "affiliations": [
        "Skywork AI, Kunlun Inc."
    ]
}