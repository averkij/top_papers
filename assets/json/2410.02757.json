{
    "paper_title": "Loong: Generating Minute-level Long Videos with Autoregressive Language Models",
    "authors": [
        "Yuqing Wang",
        "Tianwei Xiong",
        "Daquan Zhou",
        "Zhijie Lin",
        "Yang Zhao",
        "Bingyi Kang",
        "Jiashi Feng",
        "Xihui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregressive LLMs for video generation is limited to generating short videos of several seconds. In this work, we conduct a deep analysis of the challenges that prevent autoregressive LLM-based video generators from generating long videos. Based on the observations and analysis, we propose Loong, a new autoregressive LLM-based video generator that can generate minute-long videos. Specifically, we model the text tokens and video tokens as a unified sequence for autoregressive LLMs and train the model from scratch. We propose progressive short-to-long training with a loss re-weighting scheme to mitigate the loss imbalance problem for long video training. We further investigate inference strategies, including video token re-encoding and sampling strategies, to diminish error accumulation during inference. Our proposed Loong can be trained on 10-second videos and be extended to generate minute-level long videos conditioned on text prompts, as demonstrated by the results. More samples are available at: https://epiphqny.github.io/Loong-video."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 1 7 5 7 2 0 . 0 1 4 2 : r Loong: Generating Minute-level Long Videos with Autoregressive Language Models Yuqing Wang1 Tianwei Xiong1 Daquan Zhou2 Zhijie Lin2 Yang Zhao2 Bingyi Kang2 Jiashi Feng2 Xihui Liu1 1University of Hong Kong 2ByteDance Figure 1: One-Minute Videos Generated by Loong Conditioned on Texts. Loong is an autoregressive LLM-based model that can generate minute-level long videos with consistent appearance, large motion dynamics, and natural scene transitions."
        },
        {
            "title": "Abstract",
            "content": "It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregressive LLMs for video generation is limited to generating short videos of several seconds. In this work, we conduct deep analysis of the challenges that prevent autoregressive LLM-based video generators from generating long videos. Based on the observations and analysis, we propose Loong, new autoregressive LLM-based video generator that can generate minute-long videos. Specifically, we model the text tokens and video tokens as unified sequence for autoregressive LLMs and train the model from scratch. We propose progressive short-to-long training with loss re-weighting scheme to mitigate the loss imbalance problem for long video training. We further investigate inference strategies, including video token re-encoding and sampling strategies, to diminish error accumulation during inference. Our proposed Loong can be trained on 10-second videos and be extended to generate minute-level long videos conditioned on text prompts, as demonstrated by the results. More samples are available at: https://epiphqny.github.io/Loong-video. Corresponding Author Preprint."
        },
        {
            "title": "Introduction",
            "content": "Over the past few years, video generation models, including diffusion-based ones [18] and language model based approaches [9, 10], have shown impressive results in generating short videos of few seconds. To capture more comprehensive content, it is desirable to generate long videos with consistent appearance, larger motion dynamics, and natural scene transitions. Despite recent works [1113] to generate long videos with diffusion-based video generators, generating content-rich long videos on the scale of minutes remains largely underexplored and challenging. Autoregressive large language models (LLMs) have shown remarkable success in generating long and coherent text sequences [1419], demonstrating their ability to capture long-range dependencies and complex temporal patterns. Inspired by the success of autoregressive LLMs in other modalities and their flexibility in unifying various modalities and tasks, recent works [9, 10] have explored autoregressive language models for video generation. Those approaches map videos into discrete tokens and use text tokens as conditioning to generate the video tokens by next-token prediction with decoder-only transformers. State-of-the-art autoregressive LLM-based video generator [10] can generate high-quality 2-second short video clips and iteratively extend to 10-second coherent videos. Despite demonstrating the ability of long sequence generation in NLP and being explored for short video generation, the potential of LLMs to generate minute-level, content-rich, and dynamic videos In natural language processing, LLMs can be trained on long sequences remains unexplored. and extended beyond the training length. However, we empirically observe that either training autoregressive LLMs on long video sequences or extending short video generators to generate long videos leads to unsatisfactory performance for minute-level video generation. question arises: What restricts the capability of autoregressive language models for generating long videos? We hypothesize that the main obstacles are the large redundancy and strong inter-frame dependency among video tokens. The video tokens of the current frame depend heavily on the tokens of the previous frames, leading to two challenges for long video generation: (1) Imbalanced loss during training. When trained with the next-token prediction objective, predicting early-frame tokens from text prompts is much more difficult than predicting late-frame tokens based on the ground-truth tokens of previous frames. The imbalanced difficulty levels of tokens lead to imbalanced loss during training. The issue becomes more severe as the video length increases, where the accumulated loss of many easy tokens largely surpasses the loss of few difficult tokens and dominates the gradient direction. (2) Error accumulation during inference. While the model predicts the next token conditioned on previous ground-truth tokens during training, it has to predict the next token conditioned on previous predicted tokens during inference. This training-inference discrepancy leads to error accumulation during inference. Because of the strong inter-frame dependency among video tokens and the large number of video tokens, such error accumulation is non-negligible and causes visual quality degradation for long video inference. In this work, we propose Loong, aiming to unleash the power of autoregressive language models to generate long videos in the scale of minutes. Our autoregressive LLM-based video generator consists of two components: video tokenizer that compresses videos into sequences of discrete video tokens, and an autoregressive LLM that models the unified sequence of text tokens followed by video tokens through next-token prediction. To mitigate the problem of imbalanced loss for long video training, we introduce progressive short-to-long training strategy that gradually increases the training video length. We further propose loss re-weighting for early frames to prevent the model from being dominated by many easy tokens in the late frames. Moreover, we investigate inference strategies, including the video token re-encoding and sampling strategy, to further extend the video length by iteratively generating the next frames conditioned on previously generated frames. In order to enable training and inference with longer videos, we adopt low-resolution videos for the LLM-based video generator, and leverage super-resolution and refinement module to further enhance the resolution and fine-grained details of the generated long videos. In summary, we propose Loong, novel autoregressive LLM-based video generator that can generate content-rich, coherent, and dynamic long videos in the scale of minutes. Based on our observations and analysis of the issues that limit the power of LLMs for long video generation, we propose progressive short-to-long training with loss weighting scheme to enable model training on 10second videos. We further investigate inference strategies to extend the 10-second videos to minutelevel videos by autoregressive generation strategies designed for long video inference. Our model demonstrates its ability in generating minute-level long videos through extensive experiments."
        },
        {
            "title": "2 Related Work",
            "content": "Video generation. The mainstream video generation methods can be categorized into GAN-based [20 22], Diffusion-based [23, 7, 2426, 3, 6, 27, 28, 13] and language-model-based [29, 10, 30, 31]. Among them, Diffusion-based methods have recently gained the most popularity. Most Diffusionbased methods encode videos into latent space [32] for efficient training and utilize progressive inference strategies [25, 33, 34] to generate videos with high spatial-temporal resolution. With new scalable Diffusion Transformer [35] architecture, Sora [13] has further pushed video generation to new stage. Different from diffusion-based video generation methods, our work aims to explore and unleash the potentiality of language models for long video generation, as their ability for modeling long sequence and scaling up have been proved in NLP. Image and video generation with language models. Language models have recently been explored for visual generation, with most works focusing on tokenizing visual data into form that can be processed by these models. Quantization techniques like VQ-VAE [36, 37] are commonly used, and transformers are employed to model the resulting tokens. For image generation, autoregressive or masked transformers are prevalent [3844]. In short video generation, image-level or video-level tokenizers are utilized, incorporating spatial-temporal compression and causal structures. Transformers model the spatial-temporal relationships, with various techniques proposed, such as sparse attention, spatial-temporal attention, large-scale pre-training, and improved tokenization [9, 4548]. VideoPoet [10] stands out as multimodal model using bidirectional attention for conditioning, while our method aligns better with the language model paradigm by using unidirectional attention for both text and video. However, these short video generation models focus on producing 1-5 second clips, limiting their ability to capture complex events and maintain consistency over longer durations. Long video generation. Previous works have explored long video generation using various approaches. LongVideoGAN [49], NUWA-XL [50], and GAIA-1 [51] utilized GAN-based methods, diffusion-over-diffusion techniques, or world models but were limited to specific domains. More recently, video diffusion models have been extended for longer video generation. FreeNoise [52] and Gen-L [11] focus on sampling noise vectors and aggregating overlapping short video segments, respectively, while StreamingT2V [12] proposes an autoregressive approach with memory blocks for consistency and appearance preservation. In the language model domain, Phenaki [30] generates variable-length videos using masked video transformer. Despite these advancements, generating long videos with rich motion dynamics, consistent appearance, and high visual quality in the open domain remains challenge."
        },
        {
            "title": "3 Method",
            "content": "We present Loong, an autoregressive LLM-based model for generating long videos in the scale of minutes. We introduce the overall framework, composed of the video tokenizer and the LLM-based video generator, in Sec. 3.1. We analyze the problem with long video training and propose the progressive short-to-long training with loss re-weighting scheme, enabling training on 10-second videos, in Sec. 3.2. We further investigate inference strategies to extend the generated video length to the minute level and post-processing techniques to enhance the spatial resolution of generated videos in Sec. 3.3. 3.1 Overall Framework Inspired by previous work in LLM-based image generation and video generation models [38, 41, 48, 31, 10], Loong is designed with two components: video tokenizer that efficiently compresses the videos into discrete tokens, and decoder-only transformer that autoregressively predicts next video tokens based on text tokens. Video Tokenizer. In order to enable spatial-temporal joint compression and joint modeling of images and videos, we leverage causal 3D CNN architecture for the tokenizer, inspired by MAGViT2 [31]. The encoded spatial-temporal features are quantized into discrete tokens with Clustering Vector Quantization (CVQ) [53], an improved version of VQGAN [37] designed to enhance codebook utilization. To extend the temporal coverage of videos within limited number of tokens, we work with low-resolution videos and leave super-resolution for the post-processing in Sec. 3.3. The 3 Figure 2: Overall Framework and the Training process of Loong . Given the input text tokens, the model predict video tokens autoregressively. All the text and video information is formulated into unidirectional discrete token sequence, where the model predicts the next token based on the previous tokens. Video Tokenizer is utlized to convert video frames into discrete tokens. We use different color to represent first frame, short clip and long clip separately. We follow progressive training pipeline to train on long videos. We omit the special tokens for simplicity. tokenizer can compress 10-second video (65 frames, 128 128 resolution for each frame) into sequence of 17 16 16 discrete tokens with vocabulary size of 8192. Autoregressive LLM-based Video Generation. With the video frames converted into discrete tokens, we can now model the text and video tokens as unified sequence and formulate text-to-video generation as autoregressively predicting video tokens conditioned on the text tokens with decoderonly Transformers. The process is illustrated in Fig. 2. For simplicity, we omit the special separate tokens in the following formulation. Let = {t1, t2, . . . , tN } represent the sequence of text tokens, where is the number of text tokens. Similarly, let = {v1, v2, . . . , vL} represent the sequence of video tokens, where is the number of video tokens. The autoregressive LLM models the unified token sequence = [t; v] and is trained with the next-token prediction loss for the video tokens. = (cid:88) i=1 log p(vi v<i, t) (1) where vi denotes the i-th token in the video sequence v, and v<i denotes all the video tokens preceding vi. Discussion. Different from VideoPoet [10], which encodes text with pretrained T5 text encoder [54] and applies bidirectional attention for the input condition tokens and causal attention for the video tokens, our approach does not rely on pretrained text encoder. Instead, we formulate the text tokens and video tokens as unified token sequence and apply causal attention to all tokens. Our unified autoregressive modeling of text tokens and video tokens provides simpler formulation that is consistent with modern GPT-style LLMs [16]. This design may lead to potential benefits in extending our model to multimodal LLMs that unify different modalities and different tasks for understanding and generation. 3.2 Progressive Short-to-Long Training with Loss Re-weighting Most video generation models are trained on short video clips, typically no more than 4 seconds, which limits their ability to capture long-term dependencies and complex dynamics in longer videos. To address this limitation, it is essential to train these models on videos with longer durations, enabling them to learn and generate more coherent and contextually rich video content. However, training directly on long videos leads to suboptimal performance, even when the model is trained for large number of iterations. We illustrate the loss curve of different frame ranges when training on 65-frame videos (with 4,356 tokens, covering 10 seconds) in Fig. 3. We empirically 4 observe that tokens from early frames (frames 1-17) have larger losses than those from later frames (tokens from frames 50-65 have the smallest average loss). During training, the model learns through next-token prediction, where it is much easier to predict tokens of later frames given the previous ground-truth video and text tokens. In comparison, predicting early-frame tokens with little visual cues from previous frames is more challenging. The imbalanced loss is severe problem for long-sequence training because the accumulated loss of the many easy-to-predict tokens from later frames (18-65) surpasses the loss of the few difficult-to-predict tokens from early frames (1-17) and dominates the gradient direction, leading to suboptimal visual quality in the generated videos. To mitigate the aforementioned challenge of imbalanced video token difficulties, we propose progressive short-to-long training strategy with loss reweighting, demonstrated in the following. Progressive short-to-long training. In order to allow the model to first learn the text-conditioned appearance and motion of short videos, and then smoothly adjust to longer-range dependencies and more complex motion patterns in longer videos, we factorize training into three stages which gradually increases the training video length, as illustrated in the Fig. 2: (1) In stage-1, we pretrain the model with text-toimage generation on large dataset of static images, which helps the model to establish strong foundation for modeling per-frame appearance and structure. (2) In stage-2, we continue to train the model jointly on images and short video clips of 17 frames, where the model learns to capture short-term temporal dependencies and motion patterns while preserving the per-frame visual quality. (3) In stage-3, we increase the number of video frames to 65, covering temporal range of 10 seconds, and continue joint training. Imbalanced Training Losses Figure 3: When Training Directly on Long Videos. The training loss for late frames (18-65) is smaller than that of early frames (1-17), and the loss for the first frame remains relatively high, leading to suboptimal visual quality in the early frames( despite the model being pretrained on text-to-image). Loss re-weighting for early frames. To further strengthen the supervision of early frames and to prevent the model from forgetting the stage-1 and stage-2 priors, we propose loss re-weighting scheme for stage-3. To be specific, we apply larger loss weights for the tokens of early frames, and the overall weighted loss is formulated as Lweighted = (1 + λ) (cid:88) i=1 log p(vi v<i, t) (cid:88) i=K+ log p(vi v<i, t), (2) where the first term denotes the loss for the tokens corresponding to the early frames (the first 17 frames), and the second term denotes the loss for the tokens corresponding to the later frames (frames 18-65). λ is positive value to strengthen the loss weight of early frames. With the loss weighting and progressive training strategy, our model effectively mitigates the issues of long video training. As the model is trained on temporal range of 10 seconds, it can generate videos of up to 10 seconds with improved temporal coherence and consistency while maintaining the strong appearance and motion priors learned from the image and short video clips. 3.3 Inference Strategies for Extending Video Length and Resolution Large language models are proven to be length-generalizable, so we expect the LLM-based video generator trained on 10-second videos to be extended to generate longer videos autoregressively. However, generalizing beyond the training video duration is non-trivial and may lead to error accumulation and quality degradation. For instance, one-minute video corresponds to approximately 26, 112 video tokens under our current settings, which is significantly longer than most text sequences typically encountered in language modeling tasks. The considerable length and the large interframe dependency among video tokens pose challenges for extending the LLM-based generator for long video generation. In this subsection, we investigate inference strategies to generate minute5 Figure 4: Inference process of Loong . Given the input text, the model first predicts video tokens (illustrated by v1-v9) for the first 10s. The tokens from the last frames of this clip are then decoded into video frames and re-encoded by the video tokenizer. These re-encoded tokens (v7-v9), along with the text tokens, serve as conditions to predict the video tokens (v10-v13) for the next clip. This iterative process of token prediction, partial decoding, and re-encoding enables extending videos beyond the training duration while mitigating quality degradation. This process is repeated until the generated video reaches the desired length. level videos and post-processing methods like video super-resolution and refinement to generate higher-quality videos. Video token re-encoding. natural way of extending videos beyond the training duration is to iteratively generate the tokens of the next video clip, conditioned on the text prompts and the previously generated tokens of the current video clip, exploiting the benefit of autoregressive language models. However, this strategy leads to severe video quality degradation for video frames beyond the training range. With further analysis, we find that this issue stems from the token misalignment caused by the causal video tokenizer. To be specific, the tokens from the last frames in video clip are derived based on the context of all previous frames, while the tokens from the first frames in new video clip are derived without the context of the previous video clip. Therefore, generating tokens for the new clip directly conditioned on previous tokens leads to distribution shift in the input features for LLMs. To address this issue, we decode the LLM-generated video tokens to the pixel-space videos and then re-encode the last frames with the video tokenizer. The re-encoded video tokens and the text tokens serve as the conditions to generate the tokens of the next video clip. The inference process is illustrated in Fig. 4. Sampling strategy. Decoding video tokens with autoregressive language models is prone to error accumulation because of the autoregressive nature of the model and the strong inter-frame dependencies of video tokens. Errors in predicting one token can propagate and influence the generation of subsequent tokens, leading to degradation in video quality as the length increases. To mitigate this issue, we draw inspiration from the Top-k sampling strategy commonly used in NLP tasks. During the token sampling process, we only sample from the Top-k most probable tokens, ensuring that the generated tokens are of high quality. By focusing on the most likely tokens, we reduce the influence of potential errors on subsequent token generation, effectively alleviating the error accumulation problem. On the other hand, we also observe that too small values of (k = 1 degrades to greedy decoding) lead to almost static videos with little motion. To balance dynamic motion and error accumulation, we choose = 50 for our model. Super-resolution and refinement. As introduced in Sec. 3.1, our video tokenizer and LLM-based video generator operates on the low-resolution 128 128 videos. This design trades off spatial resolution for longer video sequences during training and inference. We apply off-the-shelf superresolution and refinement models [5558] on the LLM-genereated low-resolution videos. This module serves as post-processing to enhance the spatial resolution and fine-grained visual details of videos, without affecting the main content and motion of the generated videos."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details Model Architecture. Our video generation model follows the same architecture as LLaMA [18], with the model size ranging from 700M to 7B parameters. We train the models from scratch, without using any text-pretrained weights. The vocabulary consists of 32,000 tokens for text, 8,192 tokens for video, and 10 special tokens, resulting in total vocabulary size of 40,202. For the video tokenizer, we attempt to reproduce the architecture of MAGVIT2 [31], which is causal 3D CNN structure that separately models the first frame of the video. The model compresses the spatial dimensions (width and height) by factor of 8 and the temporal dimension by factor of 4. We utilize the Clustering Vector Quantization (CVQ) [53] method for quantization, as it achieves higher codebook usage ratio compared to the original Vector Quantization (VQ)[37, 59] approach. The video tokenizer has total of 246M parameters. Training. Our models are trained on 100M text-image pairs filtered from the combination of the CC12M [60] and LAION-2B [61] datasets, as well as the WebVid-10M [62] video training set and 5.5M video clips filterd from HDVG [63]. The training process follows the progressive strategy described in Sec. 3.2. We first pre-train the model on the combined image dataset for 200k iterations, followed by joint training on images and 17-frame video clips from the combined video dataset for another 200k iterations with batch size of 512. We then jointly train on 65 frames (covering 10 seconds) for 100k iterations with batch size of 256. The λ is set to 1.0 for the weighted loss of Eq. (2). In each stage, we use AdamW optimizer with base learning rate of 1.0e-4. The learning rate is scheduled using linear warmup for the first 10,000 iterations, followed by cosine annealing decay until reaching the maximum iteration count. For the training of the tokenizer, we also use progressive approach on the same dataset, increasing the video length from 1 to 17 to 65 frames while maintaining resolution of 128 128, with batch size of 64 and training for 400k iterations. Inference. During inference, our model first generates the initial 65 frames based on the text prompt. We then use the last 5 predicted frames as conditions for video extension. The classifier-free guidance ratio is set to 7.5. 4.2 Ablation Study In this section, we conduct ablation studies to evaluate the effectiveness of our main design choices. Unless otherwise specified, we use the 3B model with an output spatial resolution of 128 128, without any super-resolution and refinement module. To reduce computational cost, we train the models for half the number of iterations compared to the full setting described in Sec. 4.1. Due to the lack of general long video generation benchmark, we build custom one by selecting the top-1000 longest clips from the WebVid [62] validation set and slicing each to 27 seconds, the duration of the shortest among them. We employ two commonly used video generation metrics on this benchmark: Fréchet Video Distance (FVD)[64] and Video-Text Matching (VTM) score calculated by CLIP (ViT-L/14)[65]. We use the text prompt sets from prior works [4, 6, 66, 3, 13] to generate videos for visualization. Model Scaling. Scalability is an important characteristic of LLMs. To study scaling behavior of our model, we evaluate performance of the models with different sizes. Tab. 1 presents the quantitative results of our models with 700M, 3B and 7B parameters using the same number of iterations on the custom benchmark. We observe that larger models achieve better FVD and VTM scores, demonstrating the scalability of model size for our approach. Progressive Training with Loss Re-weighting. To validate the effectiveness of our proposed training strategy, we compare the models trained with and without our proposed strategies. Both models are pre-trained on images and then trained on videos. Fig. 5 (top row) shows the generated frames of model trained by single training stage without our proposed strategy. It is clear that the videos generated by the directly-trained models suffer from significant object appearance degradation, losing much of the structure information. In contrast, videos generated Table 1: Model Size Scalability of Loong . The performance improves as the model size increases. FVDI3D VTMc 700M 3B 7B 633 572 432 21.5 22.8 24.1 7 Figure 5: Effectiveness of the Progressive Training with Loss Re-weighting. We sample 4 frames from the 17 earlier frames of the video generation results, to show the performance of models trained with or without our training strategy. The top row shows results of the model trained directly on long video, the appearance of objects degrades largely. The bottom row shows the results model trained with our proposed training approach, the appearance preserves effectively. Figure 6: Effectiveness of Token Re-encoding during Video Extension. For each sample, the left two images show the results before the extension process, and the right two images show the results after extension. Without token re-encoding, the extension fails to generate visually consistent content. by the model trained with the proposed approach effectively preserve the appearance details. Video Token Re-encoding. Fig. 6 illustrates the importance of token re-encoding during the video extension process. Without proper token re-encoding, the model fails to maintain visual consistency when extending the video, resulting in abrupt changes in appearance and content. In contrast, by employing our token re-encoding technique, the extended frames seamlessly continue the video with coherent visual style and content. Sampling Strategy for Inference. We compare three sampling strategies when predicting each token: greedy decoding (k = 1), top-k sampling, and multinomial sampling from the whole vocabulary (k equals video token vocabulary size). As shown in Fig. 7, greedy decoding generates stable results but lacks diversity, while multinomial sampling produces more dynamic content at the cost of quality. Top-k sampling (k = 50) balances stability and diversity. smaller value prioritizes stability, resulting in less diverse motion, while larger allows for more dynamic and varied content at the risk of introducing instability. In the process of video extension, selecting an appropriate value is crucial for maintaining consistency and mitigating error accumulation over longer sequences. 8 Figure 7: Study on Sampling Strategies. Results of three different inference sampling strategies. Greedy decoding produces stable results but lacks diversity between frames. Multinomial sampling generates more dynamic and diverse content but with lower quality. Top-k sampling achieves balance between stability and diversity. is set to 50 in this experiment. 4.3 Quantitative Results Table 2: Comparison of zero-shot text-to-short-video generation on the MSRVTT benchmark Model CogVideo[47] MagicVideo[7] ModelScopeT2V[67] Show-1[28] VideoPoet[10] Loong CLIPSIM FVD 0.2631 - 998 0.2930 550 0.3072 538 0.3049 213 0.2903 274 Zero-shot Text to Short Video Generation. Although our approach is not specifically designed for short video generation, we compare our performance on the MSR-VTT dataset [68] using CLIP similarity (CLIPSIM) [46] and FVD [64] metrics, evaluated on 16 frames. As shown in Tab. 2, our FVD score is the second-best, only slightly behind VideoPoet [10] (pretrained). However, our CLIPSIM score is lower compared to some other methods, which can be attributed to the fact that our approach is trained from scratch without utilizing any pre-trained text weights. In contrast, methods with higher CLIPSIM scores, such as VideoPoet, leverage pre-trained language models like T5 [54] for text encoding, while diffusion-based methods often employ CLIP [65] text embeddings, which are already trained on the CLIP dataset. Despite not using pre-trained text models, our method still achieves competitive performance, demonstrating its effectiveness in capturing the semantic relationship between text and video. User Study on Long Video Generation. We conduct user study to compare our method with StreamingT2V [12], stateof-the-art open-sourced long video generation method built on Stable Video Diffusion [26]. We use 50 text prompts from prior works [4, 6, 66, 3] to generate 1-min videos. In the study, users are presented with 2 videos generated by the two models, conditioned on the same text. They are asked to choose the preferred video based on visual text matching and content consistency. The videos are presented randomly, and users are not informed about the models. We collect 440 responses. As shown in Fig. 8, our model outperforms StreamingT2V in both content consistency (win rate 0.83 vs. 0.125) and visual text matching (win rate 0.65 vs. 0.19). 4.4 Visualization Results Figure 8: User Study on 1-min videos. Comparison with the StreamingT2V on SVD model. Our model is more preferred by human raters in terms of both visual text match and content consistency. Fig. 9 illustrates the video frames generated by our model under various text-to-video generation scenarios. Text to Short Video. In the top row of the figure, we show sample of short video generation. As shown in the figure, our approach has the capability to generate short videos with rich details and high fidelity while maintaining strong alignment with the given text descriptions. Text to Long Video. The second row shows frames sampled from long video generated by our model, conditioned on concise text description. This sample demonstrate that our approach can generate long videos containing diverse content and larger dynamic changes compared to short video generation, while maintaining semantic alignment with the given text. Dense Text to Long Video. Although not explicitly trained on dense captions, our model can effectively adapt to dense text video generation in zero-shot manner. As illustrated in the last row of Fig. 9, the generated long video depicts rich content that corresponds to the detailed descriptions, including multiple characters, weather, scenery, and building information. However, we observe that the generated images appear slightly blurry. We attribute this to the low resolution of our transformers output, which may result in blurriness when generating highly detailed content. In Fig. 10, we also present visualization of the videos reconstructed by our tokenizer. We use videos selected from the WebVid valiation dataset [62] (not used for training). The original video frames are in the top row of each group, and the reconstructed videos of our tokenizer are shown in the bottom row. Despite achieving high compression ratio of approximately 256 (8 8 4), our tokenizer effectively preserves the fine-grained details of the original frames, and also maintains natural and coherent motion along the temporal dimension. 9 Figure 9: Generated Videos from Loong across Various Text-to-video Scenarios. Our model demonstrates diversity and quality across various text-to-video tasks, including short video, long video, and dense text-to-long video generation. The results exhibit rich details, smooth transitions, and strong semantic alignment with input descriptions. Figure 10: Reconstructed Videos by Our Tokenizer. Each group represents distinct video sequence, with the top row displaying the original frames and the bottom row presenting the corresponding reconstructions. Despite high compression ratio of 256, our tokenizer effectively preserves fine details and natural, coherent motion in the reconstructed videos."
        },
        {
            "title": "5 Conclusion and Discussions",
            "content": "In conclusion, we propose Loong, an autoregressive LLM-based video generation model that can generate minute-level long videos with consistent appearance, large motion dynamics, and natural scene transitions. We choose to model the text tokens and video tokens in unified sequence, and overcome the challenges of long video training with the progressive short-to-long training scheme and loss re-weighting. Our experiments demonstrate the effectiveness of our approach in generating minute-level long videos. We hope our work can motivate research on long video generation and multimodal modeling in the future. Border impact. The model can be deployed to assist visual artists and film producers on video creation, enhancing their efficiency. It can also be deployed for entertainment purposes. On the other hand, it may be used for generating fake content and delivering misleading information. The community should be aware of the potential social impacts. It is necessary to develop techniques to detect and watermark the videos generated by machine learning models."
        },
        {
            "title": "References",
            "content": "[1] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022. [2] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proc. IEEE Int. Conf. Comp. Vis., pages 73467356, 2023. [3] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In Proc. Int. Conf. Learn. Representations, 2022. [4] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024. [5] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: High-dynamic video generation. arXiv:2311.10982, 2023. [6] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. [7] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022. [8] Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan, Shuo Chen, Chetwin Low, Tuyen Hoang, Jie Wu, Jun Hao Liew, Hanshu Yan, et al. Magicvideo-v2: Multi-stage high-aesthetic video generation. arXiv preprint arXiv:2401.04468, 2024. [9] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. [10] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [11] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264, 2023. [12] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. [13] OpenAI. Sora: Creating video from text. https://openai.com/sora, 2024. [14] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [15] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. 11 [16] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Sastry, et al. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Proc. Advances in Neural Inf. Process. Syst., 2020. [17] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [18] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [19] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. In Proc. Advances in Neural Inf. Process. Syst., volume 27, 2014. [21] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In Proc. Advances in Neural Inf. Process. Syst., 2016. [22] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2018. [23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Proc. Advances in Neural Inf. Process. Syst., 2020. [24] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022. [25] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [26] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [27] Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu, Qi Zhang, Fu Li, Haocheng Feng, Errui Ding, and Jingdong Wang. Videogen: reference-guided latent diffusion approach for high definition text-to-video generation. arXiv preprint arXiv:2309.00398, 2023. [28] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXiv preprint arXiv:2309.15818, 2023. [29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. Advances in Neural Inf. Process. Syst., 2017. [30] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In Proc. Int. Conf. Learn. Representations, 2022. [31] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusiontokenizer is key to visual generation. In Proc. Int. Conf. Learn. Representations, 2024. [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [33] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. arXiv:2211.13221, 2022. [34] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 2256322575, 2023. 12 [35] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proc. IEEE Int. Conf. Comp. Vis., pages 41954205, 2023. [36] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In Proc. Advances in Neural Inf. Process. Syst., 2017. [37] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 1287312883, 2021. [38] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Proc. Int. Conf. Mach. Learn., pages 88218831, 2021. [39] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. [40] Huiwen Chang, Han Zhang, Jarred Barber, Aaron Maschinot, Jose Lezama, Lu Jiang, MingHsuan Yang, Kevin Patrick Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-image generation via masked generative transformers. In Proc. Int. Conf. Mach. Learn., pages 40554075, 2023. [41] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., June 2022. [42] Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, et al. Spae: Semantic pyramid autoencoder for multimodal generation with frozen llms. In Proc. Advances in Neural Inf. Process. Syst., 2024. [43] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [44] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. [45] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer. In Proc. Eur. Conf. Comp. Vis., pages 102118, 2022. [46] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. arXiv:2104.14806, 2021. [47] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: LargeIn Proc. Int. Conf. Learn. scale pretraining for text-to-video generation via transformers. Representations, 2022. [48] Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2023. [49] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and Tero Karras. Generating long videos of dynamic scenes. In Proc. Advances in Neural Inf. Process. Syst., 2022. [50] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. [51] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. [52] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. In Proc. Int. Conf. Learn. Representations, 2024. [53] Chuanxia Zheng and Andrea Vedaldi. Online clustered codebook. In Proc. IEEE Int. Conf. Comp. Vis., 2023. [54] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. 13 [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 1068410695, 2022. [56] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In Proc. Int. Conf. Learn. Representations, 2023. [57] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 38363847, 2023. [58] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion In Proceedings of the IEEE/CVF for editing real images using guided diffusion models. Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. [59] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. volume 30, 2017. [60] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018. [61] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LaionIn Proc. 5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Inf. Process. Syst., volume 35, 2022. [62] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proc. IEEE Int. Conf. Comp. Vis., 2021. [63] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation. arXiv preprint arXiv:2305.10874, 2023. [64] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. [65] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proc. Int. Conf. Mach. Learn., pages 87488763. PMLR, 2021. [66] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [67] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. [68] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 52885296, 2016."
        }
    ],
    "affiliations": [
        "ByteDance",
        "University of Hong Kong"
    ]
}