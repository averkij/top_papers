{
    "paper_title": "NAVIG: Natural Language-guided Analysis with Vision Language Models for Image Geo-localization",
    "authors": [
        "Zheyuan Zhang",
        "Runze Li",
        "Tasnim Kabir",
        "Jordan Boyd-Graber"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image geo-localization is the task of predicting the specific location of an image and requires complex reasoning across visual, geographical, and cultural contexts. While prior Vision Language Models (VLMs) have the best accuracy at this task, there is a dearth of high-quality datasets and models for analytical reasoning. We first create NaviClues, a high-quality dataset derived from GeoGuessr, a popular geography game, to supply examples of expert reasoning from language. Using this dataset, we present Navig, a comprehensive image geo-localization framework integrating global and fine-grained image information. By reasoning with language, Navig reduces the average distance error by 14% compared to previous state-of-the-art models while requiring fewer than 1000 training samples. Our dataset and code are available at https://github.com/SparrowZheyuan18/Navig/."
        },
        {
            "title": "Start",
            "content": "NAVIG: Natural Language-guided Analysis with Vision Language Models for Image Geo-localization Zheyuan Zhang1 * Runze Li2 * Tasnim Kabir3 Jordan Boyd-Graber3 1Tsinghua University 2Nanjing University 3University of Maryland zheyuan-22@mails.tsinghua.edu.cn jbg@umiacs.umd.edu 5 2 0 2 0 ] . [ 1 8 3 6 4 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Image geo-localization is the task of predicting the specific location of an image and requires complex reasoning across visual, geographical, and cultural contexts. While prior Vision Language Models (VLMs) have the best accuracy at this task, there is dearth of highquality datasets and models for analytical reasoning. We first create NAVICLUES, highquality dataset derived from GeoGuessr, popular geography game, to supply examples of expert reasoning from language. Using this dataset, we present NAVIG, comprehensive image geo-localization framework integrating global and fine-grained image information. By reasoning with language, NAVIG reduces the average distance error by 14% compared to previous state-of-the-art models while requiring fewer than 1000 training samples. Our dataset and code will be available at https: //github.com/SparrowZheyuan18/Navig/."
        },
        {
            "title": "Introduction",
            "content": "Image geo-localizationthe task of predicting the location where an image was taken (Hays and Efros, 2008)remains challenging multimodal problem. For example, to say Figure 1 is picture from Darlington (in England) requires reading the name of the hotel to determine possible candidates and excludingfor instancethe Croft hotel in Ontario based on architecture. Directly predicting the exact location of an image (Weyand et al., 2016; Haas et al., 2023; Cepeda et al., 2023) is difficult for computer vision models and requires extensive training on large image-location datasets. In contrast, human experts infer locations by reasoning. For example, in GeoGuessr1 game video, an expert player, zi8gzag, explained how he identified location in Korea: the presence of single yellow road lines and the language on the road signs * Equal contribution. 1http://www.geoguessr.com Figure 1: In image geo-localization, models need to find both cultural and geographical clues to infer correct locations. External tools like maps and guidebooks can also be helpful by providing extra knowledge. suggest an Asian region; large spikes atop concrete poles narrow it down to Japan and Korea, and the black and yellow guardrails rule out Japan. While recent research integrates textual knowledge (Luo et al., 2022) and explicit clues (Zhang et al., 2024; Mendes et al., 2024; Li et al., 2024) with Vision Language Models (VLMs) to enhance accuracy, the reasoning in these models is often limited to few words related to landmarks and does not provide concrete analysis, as human experts would. To date, these models reasoning remains more superficial than humans for two reasons: (1) Lack of high-quality reasoning datasets: Existing geotagged datasets lack linguistic reasoning elements, 1 while constructing dataset that involves reasoning based on image details is resource-intensive. (2) Complexity of diverse information retrieval: Images often contain rich details, such as road signs, texts, and building styles, requiring additional tools for accurate retrieval and interpretation. To address these questions, we introduce NAVICLUES, detailed and high-quality reasoning dataset for image geo-localization, and NAVIG, framework that combines both visual analysis and external knowledge to perform analytical reasoning. Inspired by the popular game GeoGuessr, NAVICLUES has over 2000 instances from five experienced YouTubers, recording their process of analyzing image details to infer locations, which trains VLMs to generate reasoning that mimics professional human players. With tools like public maps and expert-written guidebooks, we design pipeline that dives into fine-grained details and retrieves relevant information to further enhance accuracy. We evaluate NAVIG against state-of-the-art models on two open benchmarks using five levels of prediction and ablate each component to investigate their contributions. NAVIG outperforms previous state-of-the-art models by 14% reduction in average distance error while using less than 1000 training samples. We further illustrate the reasoning of NAVIG by providing examples of both successful and challenging cases. We release our dataset and framework to advance the use of reasoning in the field of image geo-localization."
        },
        {
            "title": "2 Collecting NAVICLUES: Linking Places",
            "content": "to Images This section explains how we process the reasoning of GeoGuessr players to construct NAVICLUES. In addition, we analyze their reasoning and identify fifteen key clues humans use in geo-localization."
        },
        {
            "title": "2.1 Data Collection",
            "content": "Despite previous datasets containing image location pairs (Hays and Efros, 2008; Vo et al., 2017; Astruc et al., 2024) and reasoning insights from guidebooks (Luo et al., 2022; Li et al., 2024), there is still lack of datasets that capture the analytical reasoning process used to deduce locations from image details. To train NAVIG to generate reasoning (Section 3.1), we use the data from GeoGuessr, popular game where players infer locations from street views, which preserve experts knowledge and strategies for image geolocalization. We mine game data from play along videos of five popular YouTubers, along with transcripts of their reasoning during gameplay. Data Mining. In typical GeoGuessr game, there are multiple rounds of guessing the location from new image. To segment the video transcript, we identify the timestamps of each rounds result pages by using Qwen-VL (Bai et al., 2023) to match the buttons and extract the corresponding scores. For images, we retrieve images from Google StreetView (GSV) API based on the coordinates of each round, omitting any unavailable ones.2 Following Haas et al. (2024), we capture images from four different directions and combine them to create 360-degree panoramic views  (Table 1)  , which contain the same details as in the games. For reasoning data, we split the transcripts by round timestamps. The raw dataset contains 2637 images and respective locations. Data Processing. To ensure data quality, we apply several processing steps: (1) we manually review and remove games where the visual content differs between the players view and the GSV data, such as games that allow movement or feature satellite view; (2) we exclude games with poor reasoning quality, including those with time limit of less than 30 seconds, transcripts shorter than 100 words, or incorrect answers (where the GeoGuessr Score is lower than 3400, approximately 575 km); (3) answer-guided reasoning (Mendes et al., 2024) generates the step-by-step reasoning process from both the transcripts and images, guided by the correct locations and details mentioned by humans. After processing, our final dataset, NAVICLUES, consists of 1120 images, each associated with corresponding location (continent, country, and coordinates), reasoning, and scores. NAVICLUES is geographically well-distributed  (Table 6)  , covering diverse set of images in various countries to reduce potential data biases. NAVICLUES is useful for analyzing human geo-localization strategies and training models for related tasks. Prompts, examples, and other details are in the Appendix B. Guidebook Data. For SEARCHER (Section 3.2), we collect guidebook data from two popular GeoGuessr community sources,3 which include tips from top players of GeoGuessr. Following Luo 2As GSV updates the images from time to time, some coordinates are deleted. 3https://somerandomstuff1.wordpress.com/2019/ 02/08/geoguessr-the-top-tips-tricks-and-techniq ues and https://www.plonkit.net 2 Image Location Israel, Ashkelon. 31.66671, 34. Query <image> Given an image, craft brief and cohesive reasoning path that deduces this location based on the visual clues present in the image. Using tone of exploration ... Response The scene reveals blend of urban and natural features typical of Mediterranean climate. Notably, the presence of palm trees suggests warm, temperate region. The architecture of the buildings, characterized by flat roofs and modern, functional style, often indicates environments with minimal snowfall. ... the neatly paved roads and sidewalks with occasional dust align with urban areas in regions with dry summers. ... the collective visual attributes and environmental clues point to location consistent with Israels landscape and architectural style. Table 1: Fine-tuning data format. The reasoning process leverages visual information in the images to deduce the correct location, such as climate, vegetation, building, and infrastructure. Complete prompts and response are in the Appendix. with guidebooks or maps (the Paria Main Road is in Toco) to locate an image. Based on this observation, NAVIG (Figure 3) has three components: (1) REASONER, which focuses on generating reasoning process that analyzes the general information present in the image (Section 3.1); (2) SEARCHER, which uses additional knowledge sources and tools to explore the details (Section 3.2); and (3) GUESSER, which takes the concatenated outputs from both analyzers to determine the final location, which can be configured to any granularity of locations (Section 3.3)."
        },
        {
            "title": "Locations",
            "content": "Recent VLMs cansometimesreason about the location of an image. (Li et al., 2024). However, the reasoning is limited to only few words and does not help localization (Zhang et al., 2024). To enhance VLMs to reason location-relevant information in images, we create NAVICLUES and fine-tune VLMs using it to build REASONER. The reasoning includes geographical information such as climate, vegetation, building, and infrastructure  (Table 1)  . This approach enables models to deduce locations from geographically pertinent details, exFigure 2: Top clues in human reasoning. Humans identify roads, cars, poles, and linguistic cluesspecifically the languages on plates, signs and houses. et al. (2022) and Li et al. (2024), each instance in our dataset contains clue (e.g., The chevrons are black with white arrows in the U.K.) paired with an image. The final dataset contains 6227 image clue pairs that have distinguishable geographical features, which could be further integrated into geolocalization frameworks as additional knowledge (Examples are in Appendix B)."
        },
        {
            "title": "2.2 Data Analysis",
            "content": "We further analyze the reasoning of human players and investigate common patterns among human experts in geo-localization tasks. We use spaCy (Honnibal and Johnson, 2015) for noun extraction, allowing us to explore the specific information these experts focus on. We manually filter out irrelevant words and phrases, retaining only content pertinent to geographic reasoning. This process identifies fifteen core clues frequently mentioned by experts reflecting common analytical patterns and reasoning strategies (Figure 2). The keywords cover cultural clues (e.g., language, flag, road, house) and natural geographical features (e.g., mountain, island, tree). This distribution conforms to categories in the guidebooks and further guides the implementation of our framework in Section 3."
        },
        {
            "title": "Reasoning and Tools",
            "content": "section presents NAVIG: This image geolocalization with reasoning about cultural and geographical clues and using external tools. YouTube experts both reason with image elements (the driving is on the left) and interpret image details 3 Figure 3: The framework of NAVIG comprises three main components: the REASONER, which handles general reasoning; the SEARCHER, which leverage external knowledge for detail-specific analysis, and the GUESSER, which combines outputs from both analyzers to generate predictions. panding the depth and applicability. After training, REASONER can generate rationale for images, where given an image I, the fine-tuned VLM produces reasoning R. However, as the reasoning relies solely on VLMs constrained by their parameterized knowledge, it lacks the information to understand specific details. For instance, human experts can search maps for text on buildings or road signs and consult guidebooks to identify the house style of particular country, which goes beyond the intrinsic knowledge within VLMs. To emulate this process, an additional module, SEARCHER, integrates external tools, enabling more accurate interpretation of nuanced details."
        },
        {
            "title": "3.2 SEARCHING Image Details",
            "content": "The SEARCHER module extracts fine-grained details from images to enhance the reasoning by integrating relevant knowledge. It crops the image, generates queries, and retrieves external knowledge. Grounding Image Details. As highlighted in Section 2.2, human experts often concentrate on specific elements in images, such as signs, houses, and roads, which provide crucial location-based clues. precise grounding model generates highquality queries: given an image and predefined set of elements = {e1, e2, . . . , en}, SEARCHER uses GroundingDino (Liu et al., 2023) to crop the image according to E. Since each image may contain multiple instances of an element, the cropped images is defined as = {ci,j ei E, [1, mi]}, where mi is the count of element ei in I. Specifically, we select road sign, building sign, and house from Figure 2 as elements, which align well with GroundingDino, since alternatives could yield overly large figures or uninformative results. Each cropped image ci,j is query for specific tools. Additionally, if ci,j is sign that contains text, textbased queries are generated with Optical Character Recognition (OCR) from Qwen2-VL (Wang et al., 2024). Therefore, the query set is: = i,j {ci,j, OCR(ci,j) if ci,j signs} Tools. The query set is then fed into Tool Set , which retrieves relevant knowledge. We use three tools for information retrieval: (1) The GeoGuessr Guidebook contains rich information for locating images (Section 2.1). Following prior research (Luo et al., 2022; Zhou et al., 2024), we frame Guidebook using as Retrieval-Augmented Generation problem. Given an input image (e.g., house as in Figure 3), we retrieve the most similar images. (2) Map. The map is critical tool in image geo-localization: text in images (e.g., 4 name on sign) can pinpoint location. We use OpenStreetMap4 for location retrieval, providing the top three search results, with the place name and multi-level location details. (3) VLM. We use an additional VLM as tool by prompting it to identify details that might be overlooked in the REASONER. The VLM generates descriptions for details to narrow down potential locations (Figure 3). Each tool in the Tool Set contributes to the retrieval of additional knowledge K: = t(Q) tT Further implementation details in the Appendix A."
        },
        {
            "title": "3.3 GUESSING the Final Location",
            "content": "The GUESSER uses all prior information to generate the final prediction. It concatenates the reasoning from the REASONER with the external knowledge retrieved by the SEARCHER, forms them into prompt template along with the image i, and makes the location prediction with VLM: yˆloc = VLMp (I, concat(R, K)) where yˆloc is the models generated location. The prompt is configurable to flexibly adjust to specific output formats, such as various location levels (e.g., country, city, and coordinates)."
        },
        {
            "title": "4 How Well Does NAVIG Reason Image",
            "content": "Locations? We compare NAVIG against prior state-of-the-art image geo-localization models and other baseline approaches (Section 4.2), ablate each module to evaluate their contributions (Section 4.3), and provide qualitative examples to highlight successful and challenging cases (Section 4.4)."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Implementation. We use three open-source models in NAVIG: MiniCPM-V (Yao et al., 2024), LLaVA (Liu et al., 2024), and Qwen2-VL (Wang et al., 2024). These models serve as VLMs for REASONER, SEARCHER, and the GUESSER components within the NAVIG framework. (1) For REASONER, Low-Rank Adaptation (LoRA) (Hu et al., 2022) fine-tunes models using NAVICLUES. We use minicpm-v-2.6, llava-1.6-vicuna-7b, and qwen2-vl-7b for their advanced performance and 4https://www.openstreetmap.org/ mid-range size, which align with our cost constraints. (2) For SEARCHER, we select the top three cropped clues as the basis for generation (e.g., if multiple houses are cropped, only will the three with the highest similarity be analyzed). CLIP (Radford et al., 2021) encodes guidebook images and query images, retrieving guidebook data by the Euclidean distance between image embeddings (FAISS (Johnson et al., 2019)), returning associated text if is below threshold dt (set to 30). (3) We prompt the GUESSER to predict locations at the coordinates level. Training hyperparameters, model configurations, and prompts are in Appendix A. Baselines. We compare NAVIG with two base- (1) Geo-localization Models: we select lines: top-performing open-source models from prior research in image geo-localization: G3 (Luo et al., 2022), GeoCLIP (Cepeda et al., 2023), and StreetCLIP (Haas et al., 2023). (2) Vision Language Models: we select vanilla MiniCPM-V, LLaVA, Qwen2VL as baselines, consistent with the backbone models used in NAVIG. The prompts for these VLM baselines are identical to those in NAVIG but lack analyses. We do not include commercial closedsource models (as discussed in Limitations). Dataset and Metrics. Following previous work (Hays and Efros, 2008; Astruc et al., 2024; Haas et al., 2024), we evaluate our framework on two public datasets, including GWS5K sampled from GWS15K (Clark et al., 2023) due to cost constraints, and Im2GPS3k (Hays and Efros, 2008). We first computed the haversine distance between predicted and ground truth coordinates. For models limited to city level outputs, we use the coordinates of the predicted city as their predictions. Next, we evaluated the prediction accuracythe percentage of guesses that fall within distance threshold from the correct locationat five geographic levels: Street (1 km), City (25 km), Region (200 km), Country (750 km), and Continent (2,500 km). In addition, we calculated the average error distance and GeoGuessr Score, metric from the original GeoGuessr game that quantifies guess accuracy, with scoring range of 0 to 5000. Details about metric computation are in Appendix C."
        },
        {
            "title": "4.2 Main Experiments",
            "content": "Accuracy. We compare NAVIG with state-ofthe-art image geo-localization models and Vision Language Models (GWS5k results in Table 2). (1) NAVIG (Qwen2-VL) has the highest accuracy across all metrics, beating specialized geo5 Model G3 GeoCLIP StreetCLIP MiniCPM-V LLaVA Qwen2-VL NAVIG - MiniCPM-V - LLaVA - Qwen2-VL Continent 2,500 km Country 750 km Region 200 km City 25 km Street 1 km Distance Score 50.9 78.2 79.4 27.1 43.9 89.4 71.5 74.7 91.1 14.6 46.5 43. 15.9 23.1 66.7 44.1 39.4 66.9 2.3 17.1 13.4 6.7 7.0 31.8 16.9 12.0 31.9 0.1 3.5 1. 1.6 1.2 6.1 3.5 1.9 6.7 0.0 0.4 0.3 0.1 0.0 0.1 0.3 0.3 0.7 4,341 2,099 2, 7,320 5,096 1,124 2,956 2,243 965 1,304 2,613 2,543 909 1,418 3,344 2,413 2,354 3,389 Table 2: Accuracy and scores on GWS5k. The data from Continent to Street represents the accuracy (%) at each level. The three sections are geo-localization models, VLMs, and NAVIG. Bold font indicates the best performance. NAVIG (Qwen2-VL) achieves the highest accuracy across all metrics. Model REASONER (MiniCPM-V) MiniCPM-V REASONER (LLaVA) LLaVA REASONER (Qwen2-VL) Qwen2-VL ROUGE F1 R2 RL R1 51.0 46.4 49.8 44.7 51.4 45. 14.8 12.6 13.9 10.8 14.6 12.3 24.6 22.1 24.0 21.8 24.3 22.1 Table 3: ROUGE F1 scores for reasoning generated by models and humans (%). REASONER models reason more similarly to humans. localization models trained on domain-specific datasets, despite its relatively compact size of only 7 billion parameters. (2) All VLMs generate effective analytical reasoning trained with only around 1,000 samples and beat their vanilla models. These findings underscore the quality of training data and the efficacy of NAVIG. Similar results on Im2GPS3k are in Appendix D. Reasoning. We evaluate the linguistic reasoning quality generated by the model on reserved test set of 50 games. To measure the alignment between model and human reasoning, we compute their ROUGE scores (Lin, 2004) which illustrate whether the model simulates human reasoning. REASONER achieves higher ROUGE scores across all models and metrics after training  (Table 3)  . We apply GPT-4o to label the granularity and accuracy (measured by exact match) of the reasoning by NAVIG (Qwen2-VL) on GWS5k  (Table 4)  . REASONER predicts country with an accuracy of 79.6%, while its challenging (3.0%) when it makes finer-grained predictions (e.g., city, town, or street), as these predictions require additional information. For example, REASONER outputs the Level Frequency Accuracy Country Others 100.0 50.7 79.6 3.0 Table 4: Frequency and accuracy (exact match) of REASONER. Others indicates more detailed predictions, which are challenging. combination of these elements point towards city like Chaco, Argentina while the correct answer is Trelew, Argentina. This indicates the importance of SEARCHER for precise predictions. Comparison with Humans. We also compare NAVIGs performance against human players in fifty randomly sampled GeoGuessr games (collected in Appendix B), focusing on common metrics for country, city, and street level predictions. Filtered human player data have time and access to additional knowledge resources (e.g., maps and guidebooks) for fair comparison with NAVIG. NAVIG outperforms humans in overall scores  (Table 5)  , although humans excel at finer-grained predictions by iteratively cross-referencing maps and comparing terrain and features within the game. This highlights future direction to use non-textual features to refine map-based searches and enhance street-level accuracy of models."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "To illustrate the contributions of each component in NAVIG, we ablate the reasoning training, the impact of REASONER, and SEARCHER. Table 6 presents the three VLMs accuracy on GWS5k. In this setup, NAVIG represents our framework, w/o training denotes results with the same prompt but without training on NAVICLUES, w/o Macro and 6 Country City Street Score"
        },
        {
            "title": "4.4 Qualitative Analysis",
            "content": "Model NAVIG - MiniCPM-V - LLaVA - Qwen2-VL Human Players 56.0 48.0 86.0 76. 18.0 14.0 32.0 48.0 0.0 0.0 4.0 42.0 2,863 2,690 4,202 3, Table 5: Performance between humans and NAVIG. The data from City to Street represents accuracy (%). Our best model beats humans with higher overall score but still struggles to achieve fine-grained accuracy. Model Country City Street NAVIG (MiniCPM-V) - w/o training - w/o REASONER - w/o SEARCHER - MiniCPM-V NAVIG (LLaVA) - w/o training - w/o REASONER - w/o SEARCHER - LLaVA NAVIG (Qwen2-VL) - w/o training - w/o REASONER - w/o SEARCHER - Qwen2-VL 44.1 - 3.3 - 10.2 - 0.3 - 14.9 39.4 - 25.8 - 20.2 + 0.4 - 16.3 66.9 - 6.0 - 4.0 + 0.1 - 0. 3.5 - 0.4 - 0.7 - 0.3 - 0.5 1.9 - 1.2 - 0.8 - 0.2 - 0.7 6.7 - 0.9 - 0.6 - 0.9 - 0.6 0.3 - 0.2 - 0.0 - 0.2 - 0.2 0.3 - 0.3 - 0.0 - 0.2 - 0.3 0.7 - 0.5 - 0.2 - 0.5 - 0. Table 6: Ablation results of NAVIG on the GWS5k dataset. Each component contributes to model accuracy, with their removal leading to notable declines across Country, City, and Street levels. w/o Micro refer to the results without the REASONER and SEARCHER modules, respectively. Results. (1) Each module contributes to improving the models accuracy. (2) Surprisingly, when the model is prompted to zero-shot generate reasoning, it can be misleading and decrease final accuracy. This highlights the necessity of training with NAVICLUES. (3) REASONER plays critical role in coarse-grained localization, with improvements at the country level and decreases without it, as the reasoning in NAVICLUES is limited to the country and city level. (4) SEARCHER substantially enhances fine-grained reasoning. Precise street-level localization on the GWS dataset is challenging, but the SEARCHER narrows the scope within 1 km for images containing textual information using map searches  (Table 2)  . Results on Im2GPS3k are in Appendix D, which is consistent with GWS. 7 This section examines how the analytical reasoning derived from images contributes to NAVIG inference process. NAVIG closely examines details within the image (e.g., climate, orientation, and Lower Mill in Figure 4 (top)) to determine the location. This detailed reasoning narrows down the possible range, while integration with OpenStreetMap data further aids the model in finding the restaurant, with an error distance of under 1 meter, improving its estimate by 144 km. However, image elements can also mislead the model. In Figure 4 (middle), the model fixates on shop name in the image, KLICK, which can be interpreted as German word. This leads the reasoning process astray, resulting in an incorrect localization. OpenStreetMap can also lead to false predictions when there are places with the same name, such as Bradesco, well-known Brazilian bank (Figure 4, bottom). The reasoning makes image geo-localization models more interpretable by revealing how image elements influence decisions."
        },
        {
            "title": "5 Related Work",
            "content": "Image geo-localization. Image geo-localization falls into three methods: (1) Retrieval-based methods retrieves the most similar images (Hays and Efros, 2008; Zhu et al., 2023). Various retrievers (Vo et al., 2017; Pramanick et al., 2022; Haas et al., 2023) and gallery types (Cepeda et al., 2023) have been proposed. (2) Classification-based methods divide geographical maps into distinct classes and train models to classify the images into these categories with different model structures (Radford et al., 2021; Wu and Huang, 2022) and map division strategies (Weyand et al., 2016; Theiner et al., 2022; Haas et al., 2024). (3) Generationbased methods use visual understanding and generation in Vision Language Models (VLMs) to directly generate the location or coordinates for geolocalization. Aligning visual content with rich text descriptions and reasoning (Jia et al., 2024; Li et al., 2024; Zhang et al., 2024) and incorporating external knowledge through Retrieval-Augmented Generation (Luo et al., 2022; Zhou et al., 2024) are state-of-the-art. However, challenges persist in effectively using VLMs, including limited reasoning data and relying on constrained knowledge sources. Visual Reasoning. Unlike previous methods, we treat geo-localization as complex visual reasoning task that deduces the correct location with Figure 4: Top: The model uses visual details and OpenStreetMap to accurately determine the location. Middle: The model is misled by linguistic elementsthe shop name, resulting in an incorrect inference. Bottom: The model found namesake when using OpenStreetMap. language, requiring both visual understanding and reasoning capabilities (Hudson and Manning, 2019; Gupta and Kembhavi, 2023). As VLMs have demonstrated exceptional accuracy in visual reasoning tasks (Alayrac et al., 2022; Lu et al., 2023), methods enhancing the visual reasoning of VLMs in specific tasks include: (1) High-quality reasoning data, which researchers have shown to be particularly effective in improving the performance of VLMs (Du et al., 2023; Chen et al., 2023); (2) Vision grounding, which enables models to ground in the details of the image and perform step-by-step reasoning (Qi et al., 2024; Wu and Xie, 2023; Zhang et al., 2024); and (3) Tool using and retrieval-augmented generation, which aid the model by leveraging tools to retrieve additional knowledge (Yang et al., 2023; Marino et al., 2021; Chen et al., 2022) and reduce hallucinations. We integrate these insights to improve NAVIG."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce reasoning dataset NAVICLUES and novel framework NAVIG with detailed visual reasoning and knowledge retrieval for image geolocalization. Evaluated with relatively small backbone models, NAVIG is highly accurate by using the reasoning chains of human experts, providing 8 interpretable, verifiable evidence that downstream user can use to validate its predictions. Future work could include detailed annotations to images more than street views, expanding more tools to improve finer-grained predictions, and using interpretable reasoning to assist geo-localization applications."
        },
        {
            "title": "Dataset",
            "content": "(1) Limited data size. In this work, we utilize data from human players in the GeoGuessr game to train Vision Language Models for performing geographic reasoning on images. The copyright and usage rights of the images are subject to that of Google Street View. However, the size of NAVICLUES is limited due to the scarcity of available data on YouTube and the data noise. (2) Panoramic street view images. To simulate the perspective of players in the GeoGuessr game, we use stitched panoramic images as the input to the model. Furthermore, nearly all images in the data from GeoGuessr are street views, despite our efforts to ensure geographically balanced distribution of data across countries. This limits its distribution, as theres more weather, street, car, and vegetation information in street views than in other images. Models trained with NAVICLUES might be weak at images with less street-level information. (3) Future work could consider expanding the training dataset by incorporating images of different sizes and types, including more detailed annotations to create dataset more than street views, to further enhance the performance of image geolocalization tasks with better reasoning."
        },
        {
            "title": "Models",
            "content": "(1) Limited model sizes. Due to cost constraints, we are unable to train larger models and conduct our experiments using top-performing, mediumsized open-source models (around 7B parameters). While this choice may result in performance that is not as competitive as larger models, it ensures practical balance between computational feasibility and model efficacy. We also refrain from using closed-source models, as their lack of transparency regarding training data and inability to be trained on NAVICLUES make them unsuitable for fair comparison. (2) Limited tool sets. We evaluated only limited set of tools and grounding words in SEARCHER. Identifying more geographic features such as cars, road markings, and poles would require more precise recognition methods and more sophisticated model designs, which could potentially improve performance. (3) Complexity of subsystems. We employ pipeline approach to construct our model, aiming to maximize the performance of each component at every stage. However, this process introduces knowledge from different resources, which might conflict with each other. Currently, we implement Guesser to handle the potential conflict and show the contribution of each ablated subsystems. We also examine the reasoning from REASONER to show the necessity of SEARCHER. (4) Future works can focus on including larger backbone models to further improve the performance, adding more tools, and conduct end-to-end training to better integrate the information, or add another fact-checking module to better discern information."
        },
        {
            "title": "Data Collection",
            "content": "In this work, we use the data from GeoGuessr players on YouTube to train our models. We carefully process the data and remove the personal information of the players, using all data for academic and non-commercial purposes, and giving appropriate credit to them in this paper. We make sure the use of our data is acceptable under YouTubes copyright policies and the Fair Use guidelines."
        },
        {
            "title": "Model Usage",
            "content": "While the task of image geo-localization has the potential to enable innovative applications in fields such as navigation and tourism, the misuse of these models could also lead to risks such as privacy breaches and surveillance. In our work, we ensured that all training and testing data came from publicly available sources, with no involvement of private or personal images or location data. Currently, as shown in our experiments, these models have not yet reached level of precision to accurately predict coordinates-level locations. For the future development of this field, it is crucial for researchers to ensure that these models are used within appropriate boundaries to prevent the leakage of private information."
        },
        {
            "title": "References",
            "content": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. 2022. Flamingo: visual language model In Advances in Neural Infor few-shot learning. formation Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Guillaume Astruc, Nicolas Dufour, Ioannis Siglidis, Constantin Aronssohn, Nacim Bouia, Stephanie Fu, Romain Loiseau, Van Nguyen Nguyen, Charles Raude, Elliot Vincent, et al. 2024. Openstreetview5m: The many roads to global visual geolocation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21967 21977. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: versatile visionlanguage model for understanding, localization, text reading, and beyond. Vicente Vivanco Cepeda, Gaurav Kumar Nayak, and Mubarak Shah. 2023. Geoclip: Clip-inspired alignment between locations and images for effective worldwide geo-localization. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. 2023. Shikra: Unleashing multimodal llms referential dialogue magic. ArXiv preprint, abs/2306.15195. Zhuo Chen, Yufeng Huang, Jiaoyan Chen, Yuxia Geng, Yin Fang, Jeff Pan, Ningyu Zhang, and Wen Zhang. 2022. Lako: Knowledge-driven visual question answering via late knowledge-to-text injection. In Proceedings of the 11th International Joint Conference on Knowledge Graphs, pages 2029. Brandon Clark, Alec Kerrigan, Parth Parag Kulkarni, Vicente Vivanco Cepeda, and Mubarak Shah. 2023. Where we are and what were looking at: Query based worldwide image geo-localization using hierarchies and scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 2318223190. IEEE. Yifan Du, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Jinpeng Wang, Chuyuan Wang, Mingchen Cai, Ruihua Song, and Ji-Rong Wen. 2023. What makes for good visual instructions? synthesizing complex visual reasoning instructions for visual instruction tuning. ArXiv preprint, abs/2311.01487. Tanmay Gupta and Aniruddha Kembhavi. 2023. Visual programming: Compositional visual reasoning without training. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 1495314962. IEEE. Lukas Haas, Silas Alberti, and Michal Skreta. 2023. Learning generalized zero-shot learners for openArXiv preprint, domain image geolocalization. abs/2302.00275. Lukas Haas, Michal Skreta, Silas Alberti, and Chelsea Finn. 2024. Pigeon: Predicting image geolocations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12893 12902. James Hays and Alexei A. Efros. 2008. IM2GPS: estimating geographic information from single image. In 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2008), 24-26 June 2008, Anchorage, Alaska, USA. IEEE Computer Society. Matthew Honnibal and Mark Johnson. 2015. An improved non-monotonic transition system for dependency parsing. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 13731378, Lisbon, Portugal. Association for Computational Linguistics. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Drew A. Hudson and Christopher D. Manning. 2019. GQA: new dataset for real-world visual reasoning and compositional question answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 67006709. Computer Vision Foundation / IEEE. Pengyue Jia, Yiding Liu, Xiaopeng Li, Xiangyu Zhao, Yuhao Wang, Yantong Du, Xiao Han, Xuetao Wei, Shuaiqiang Wang, and Dawei Yin. 2024. G3: An effective and adaptive framework for worldwide geolocalization using large multi-modality models. ArXiv preprint, abs/2405.14702. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. IEEE Billion-scale similarity search with gpus. Transactions on Big Data, 7(3):535547. Ling Li, Yu Ye, Bingchuan Jiang, and Wei Zeng. 2024. GeoReasoner: Geo-localization with reasoning in street views using large vision-language model. In Proceedings of the 41st International Conference on 10 Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 2922229233. PMLR. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024. Llavanext: Improved reasoning, ocr, and world knowledge. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. 2023. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. ArXiv preprint, abs/2303.05499. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, KaiWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-play compositional reasoning with large language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Grace Luo, Giscard Biamby, Trevor Darrell, Daniel Fried, and Anna Rohrbach. 2022. G3: Geolocation via guidebook grounding. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 58415853, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach. 2021. KRISP: integrating implicit and symbolic knowledge for opendomain knowledge-based VQA. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 1411114121. Computer Vision Foundation / IEEE. Ethan Mendes, Yang Chen, James Hays, Sauvik Das, Wei Xu, and Alan Ritter. 2024. Granular privacy control for geolocation with vision language models. ArXiv preprint, abs/2407.04952. Shraman Pramanick, Ewa Nowara, Joshua Gleason, Carlos Castillo, and Rama Chellappa. 2022. Where in the world is this image? transformer-based geo-localization in the wild. In European Conference on Computer Vision, pages 196215. Springer. Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et al. 2024. Cogcom: Train large visionlanguage models diving into details through chain of manipulations. ArXiv preprint, abs/2402.04236. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR. Jonas Theiner, Eric Müller-Budack, and Ralph Ewerth. 2022. Interpretable semantic photo geolocation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 750760. Nam N. Vo, Nathan Jacobs, and James Hays. 2017. Revisiting IM2GPS in the deep learning era. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 2640 2649. IEEE Computer Society. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024. Qwen2vl: Enhancing vision-language models perception of the world at any resolution. ArXiv preprint, abs/2409.12191. Tobias Weyand, Ilya Kostrikov, and James Philbin. 2016. Planet-photo geolocation with convolutional neural networks. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14, pages 3755. Springer. Meiliu Wu and Qunying Huang. 2022. Im2city: image geo-localization via multi-modal learning. In Proceedings of the 5th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery, pages 5061. Penghao Wu and Saining Xie. 2023. v*: Guided visual search as core mechanism in multimodal llms. ArXiv preprint, abs/2312.14135. Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mmreact: Prompting chatgpt for multimodal reasoning and action. ArXiv preprint, abs/2303.11381. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. 2024. Minicpm-v: gpt-4v level mllm on your phone. ArXiv preprint, abs/2408.01800. Gengyuan Zhang, Yurui Zhang, Kerui Zhang, and Volker Tresp. 2024. Can vision-language models be good guesser? exploring vlms for times and location reasoning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 636645. Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan Hu, Ni Lao, Lan Mu, Sheng Li, and Gengchen Mai. Img2loc: Revisiting image geolocalization 2024. 11 using multi-modality foundation models and imagebased retrieval-augmented generation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 27492754. Sijie Zhu, Linjie Yang, Chen Chen, Mubarak Shah, Xiaohui Shen, and Heng Wang. 2023. R2former: Unified retrieval and reranking transformer for place recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1937019380."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Training Parameters We trained the REASONER on Nvidia RTX 6000 Ada (48G), with CUDA 12.4, Transformers 4.45.1, and Pytorch 2.1.2. Parameters Max Length LoRA Rank LoRA Alpha Optimizer Adam Beta1 Adam Beta2 Learning Rate Warmup Ratio LR Scheduler Type Batch Size Weight Decay Value 2048 8 32 AdamW 0.9 0.95 1e-4 0.05 cosine 1 0. Table 7: Training parameters for REASONER. A.2 Other Parameters. For reproducibility, we also provide the parameters used in other modules and VLMs within our framework. GroundingDino. We utilize GroundingDino to crop detailed information from the images, such as signs and houses. We observe variation in the features of images across different datasets. For instance, the GWS5k dataset focuses on street scenes, and other datasets contain considerable noise (e.g., animals). Consequently, to reduce noise that could potentially affect model performance, we empirically set the thresholds as follows: Box-Threshold = 0.5 and Text-Threshold = 0.5 for GWS5K, and Box-Threshold = 0.8 and Text-Threshold = 0.6 for Im2GPS3k. Retrieval-Augmented Generation. We employ CLIP as the image encoder for guidebook clues, using ViT-B-32 as the vision encoder. The guidebook database is deployed with FAISS, and similarity is calculated using Euclidean Distance. The number of most relevant retrieved images, k, is set to 3, with similarity threshold of 30. OpenStreetMap. We use the Nominatim Search API to process map searches, which takes text queries, and return the most relevant results along with the place name, address, and coordinates. Vision Language Models. We use VisionLanguage Models in our framework for reasoning and location inference. The three models are minicpm-v-2.6, llava-1.6-vicuna-7b, and 13 qwen2-vl-7b. Each model is configured with temperature of 0 and an output length of 2048. A.3 Prompts for VLMs. In Table 8 and Table 9, we present the prompts used in NAVIG for Vision Language Models. Four distinct prompts are employed: (1) the Data Processing Prompt, which employed an answer guided reasoning generation method to prompt VLMs in extracting step-by-step reasoning from YouTube transcripts; (2) the REASONER Prompt, which is the same as the query in the training data, prompting VLMs to generate coherent reasoning process to infer the location within an image; (3) the SEARCHER Prompt, which generates additional knowledge from image details, and (4) the GUESSER Prompt, which synthesizes all prior information to make final prediction. Data Processing Prompt <image> Given an image and the known location details (Country: country, Latitude: lat, Longitude: lon), and an experts analysis of the location (transcript), craft brief and cohesive reasoning path that deduces this location based on the visual clues present in the image. Begin your reasoning without revealing that you know the exact location, using tone of exploration and inference. Carefully analyze and link observations of natural features (climate, vegetation, terrain), man-made structures (roads, buildings, signage), and distinct landmarks. Allow these observations to naturally lead you to the correct country, enhancing the accuracy of your deductions. Ensure that while the narrative seems to be guessing, it aligns with the known country, confirming the reliability of your reasoning without stating the specific coordinates. Start the reasoning without any intro, and make sure to make it brief. Table 8: The prompts used in NAVIG. Data. In this section, we present the data processing workflows and provide more detailed information on the various types of data used in the system. B.1 Data Processing. YouTubers. We utilized the scripts of five professional GeoGuessr players YouTube videos as the starting data for our reasoning generation. We thank these five players for their contributions to knowledge dissemination and promotion of image geo-localization: zi8gzag, GeoWizard, GeoPeter, Geogasm, and RAINBOLT TWO. Data Processing. We used the Google Street View5 API to retrieve images for our dataset. We 5https://www.google.com/streetview/ selected resolution of 640640 pixels (the maximum resolution accepted by GSV), field of view (FOV) of 90, and headings of 0, 90, 180, and 270 degrees to obtain four images. Stitching them together produces complete street view image, providing the same amount of information that GeoGuessr player would see. Next, we split the videos for retrieving the transcripts or each round. After player submits their final guess, the game reveals the distance between their guessed location and the actual coordinates, where the player can choose to either proceed to the next round or end the challenge. We use precise pixel coordinates in conjunction with OCR technology to detect the presence of the Next or End buttons and split the videos. We sample frames at rate of 1/6 per second to ensure no scene is missed. Additionally, we extract the GeoGuessr Score displayed beside the button and collect human players scores. Next, due to the noise in the data (with many informal language from players), we provide GPT-4o with the correct locations for paraphrasing and generating higher quality and more coherent data. REASONER Prompt <image> Given an image, craft brief and cohesive reasoning path that deduces this location based on the visual clues present in the image. Using tone of exploration and inference. Carefully analyze and link observations of natural features (climate, vegetation, terrain), man-made structures (roads, buildings, signage), and distinct landmarks. Allow these observations to naturally lead you to the correct country, enhancing the accuracy of your deductions. Start the reasoning without any intro, and make sure to make it brief. SEARCHER Prompt <image> Analyze the {item} images to determine the region with the highest likelihood of finding this type of {item}. For each image, provide only the core reasoning in one sentence. Dont say you cant determine, try your best as its geo-localization game GUESSER Prompt <image> <information> Using the provided information as reference, estimate the location depicted in the image with as much accuracy and precision as possible. Generally, you might use the reasoning to roughly locate the coarse-grained location, and use other information to help you decide more precisely. Use your own knowledge as well. Aim to deduce the exact coordinates whenever feasible. Format your response strictly as JSON in the following structure:{country: <country_name>, city: <city_name>, latitude: <Latitude Coordinate>, longitude: <Longitude Coordinate>} Ensure the JSON output is correctly formatted. Provide well-informed estimate for each value, avoiding any empty fields. Do not include additional information or commentary. Table 9: The prompts used in NAVIG. B.2 Data Demonstration. In this section, we present examples and key statistics for both NAVICLUES and guidebook datasets. NAVICLUES. Each data includes panoramic image, the corresponding location, and highquality reasoning process that shows how geographical and cultural information is used to infer the location (Figure 10). To reduce hallucination and bias, the model is not required to generate specific street-level locations or coordinates directly, but carefully analysis about image elements (e.g., climate) that collectively lead to the prediction. NAVICLUES is geographically well-distributed, covering various countries across the globe (Figure 5). Figure 5: Location distribution of NAVICLUES, covering wide range of countries around the world. Guidebooks. As shown in Table 11, each guidebook entry consists of an image-text pair, where the text describes the location and features depicted in the image. We utilize RAG in NAVIG to identify the most similar image and use the accompanying text to support the reasoning process. Statistical Results. We present the statistical results of NAVICLUES in Figure 6. Most distances are relatively small, indicating that players are often successful in deducing the correct location. The reasoning process averages 842 words in length, demonstrating high quality with extensive details."
        },
        {
            "title": "C Metrics Definition",
            "content": "C.1 GeoGuessr Score The GeoGuessr Score is metrics introduced in the GeoGuessr game to quantify the accuracy of guess. It is defined as: GeoGuessr Score = 5000 1492.7 (1) where represents the distance between the guessed coordinates and the actual coordinates in kilometers. Therefore, more accurate guess yields GeoGuessr Score closer to 5,000. 14 Image Location Chile. -27.1265479, -109.2876917 Reasoning These imposing stone figures are situated on grassy hillside overlooking the ocean, suggesting coastal location. The sparse vegetation and the presence of unique megalithic structures indicate historical and possibly isolated environment. The statues, known as Moai, are characteristic of an island rich in archaeological history. Such distinct culture is associated with an island famous for these figures, which is set far away from mainland destinations. This points toward the Polynesian island in the southeastern Pacific known for these world-renowned statues, aligning closely with location associated with Chile. Image Location China. 22.27992557066081, 114. Reasoning Tall, sleek skyscrapers dominate the cityscape, suggesting prominent urban environment. The distinct yellow license plates on vehicles are key hint, typically associated with regions influenced by British rule. The street signs feature both English and Chinese characters, which points to bilingual city. The presence of distinctive red double-decker bus is highly characteristic of city with British influence. All these clues, combined with the modern architecture and dense urban vibes, strongly suggest this is Hong Kong, likely within its central business district. Image Location Italy. 42.71658113303754, 13.01051150781922 Reasoning The landscape features rolling hills and dense cover of greenery, indicative of temperate climate typically found in parts of Southern Europe. The road layout with its narrow curve and the types of vehicles suggest European setting. The presence of Italian-language signage such as Cascia and recognizable Italian road markers, like narrow front plates, strongly points to Italy. The architecture, with its rustic and modest buildings in the distance, complements the rural Italian countryside vibe. Hence, these visual cues collectively affirm the location as Italy, likely in more central to southern region given the sign for Cascia. Table 10: Demonstration of NAVICLUES. Figure 6: Distribution of Distance (left) and Length of Reasoning (right) in NAVICLUES. Image Image Text (Toptips) Houses in Brittany, western region of France, are coloured white with dark roofs. Image Text (Plonkit) Tunisia has fairly unique stop sign with Arabic and Latin script. Location Israel, Ashkelon. 31.66671, 34.59127 Query <image> Given an image, craft brief and cohesive reasoning path that deduces this location based on the visual clues present in the image. Using tone of exploration and inference. Carefully analyze and link observations of natural features (climate, vegetation, terrain), man-made structures (roads, buildings, signage), and distinct landmarks. Allow these observations to naturally lead you to the correct country, enhancing the accuracy of your deductions. Start the reasoning without any intro, and make sure to make it brief. Response The scene reveals blend of urban and natural features typical of Mediterranean climate. Notably, the presence of palm trees suggests warm, temperate region. The architecture of the buildings, characterized by flat roofs and modern, functional style, often indicates environments with minimal snowfall. Observing the infrastructure, the neatly paved roads and sidewalks with occasional dust align with urban areas in regions with dry summers. The use of earth-toned paint on buildings matches the aesthetic found in Middle Eastern locales. Further exploration of the layout reveals hints of both residential and possibly suburban planning, with open spaces that are common in balanced urban settings. Taking all these elements into consideration, the collective visual attributes and environmental clues point to location consistent with Israels landscape and architectural style. Table 11: Examples of clues in the two guidebooks, Toptips and Plonkit. The data is constructed as imagetext pairs in the guidebooks. Table 12: Fine-tuning data format. The reasoning process leverages visual information in the images to deduce the correct location, such as climate, vegetation, building, and infrastructure. Model G3 GeoCLIP StreetCLIP MiniCPM-V LLaVA Qwen2-VL NAVIG - MiniCPM-V - LLaVA - Qwen2-VL Continent 2, 500 km Country 750 km Region 200 km City 25 km Street 1 km Distance Score 47.3 82.3 68.2 33.2 61.2 75.0 68.5 70.4 84.0 23.9 66.5 51.2 27.8 43.2 65.0 51.7 47.8 68. 6.0 48.0 29.6 22.4 25.9 48.9 36.5 26.8 49.1 1.6 32.2 19.0 15.9 16.5 29.9 23.1 16.7 28. 0.0 13.0 4.2 2.3 2.6 5.3 3.0 2.8 5.5 Table 13: Performance on Im2GPS3k. 4,938 1,762 3,161 6,624 3,387 2, 3,149 2,851 1,631 1,451 3,402 2,640 1,433 2,338 3,237 2,726 2,592 3,482 C.2 Haversine Distance Model Country City Street We calculate the Haversine Distance of the models with the following formulas: = sin2 ) ( δlat 2 + cos(latcor) cos(latpred) sin2 = 2r arcsin() where: ) ( δlon 2 (2) (3) is the Earths radius, which we set as 6,371, δlat is the difference in latitude between the true and predicted coordinates, δlon is the difference in longitude between the true and predicted coordinates, latcor and loncor are the correct coordinates, latpred and lonpred are the predicted coordinates."
        },
        {
            "title": "D Supplementary Experiments",
            "content": "In this section, we present supplementary experiments, including results from the experiments on Im2GPS3k, and SEARCHER details. As shown in Table 13, NAVIG outperforms prior models on Im2GPS3k in terms of Average Distance and GeoGuessr Score. However, GeoCLIP achieves better performance at the City and Street level, likely due to its training on coordinates datasets. The ablation results demonstrated in Table 14 are consistent with those in Table 6. We also analyze the usage of each tool across the datasets and the number of grounding images. This analysis illustrates how frequently NAVIG leverages each tool and image detail to deduce locations. NAVIG (MiniCPM-V) - w/o training - w/o REASONER - w/o SEARCHER - MiniCPM-V NAVIG (LLaVA) - w/o training - w/o REASONER - w/o SEARCHER - LLaVA NAVIG (Qwen2-VL) - w/o training - w/o REASONER - w/o SEARCHER - Qwen2-VL 51.7 - 1.6 - 10.6 - 0.3 - 23.9 47.8 - 15.3 - 8.1 + 0.1 - 4.5 68.3 - 4.3 - 2.9 + 0.1 - 3.3 23.1 - 1.8 - 3.5 - 0.2 - 7. 16.7 - 4.7 - 1.3 - 0.2 - 0.2 28.9 - 1.2 + 0.5 - 0.0 + 1.0 3.0 - 0.1 - 0.2 - 0.0 - 0.7 2.8 - 0.8 - 0.1 - 0.2 - 0.1 5.5 - 0.3 - 0.1 - 0.2 - 0.2 Table 14: Ablation results on Im2GPS3k. As shown in Table 15 and Table 16, houses are the most frequently identified items in the testing dataset, as images often contain multiple houses. In contrast, signs, though less common, play critical role by generating queries for OSM. The distribution of items directly influences the frequency of tool usage for knowledge retrieval. Dataset house road sign building sign GWS5k Im2GPS3k 3,451 465 20 52 104 24 Table 15: The frequency of how each item is grounded. Dataset RAG MAP VLM GWS5k Im2GPS3k 5,000 2,997 128 213 70 1,978 493 Table 16: The usage of each tool in each dataset, where denotes the size of the dataset."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Tsinghua University",
        "University of Maryland"
    ]
}