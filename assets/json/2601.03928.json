{
    "paper_title": "FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection",
    "authors": [
        "Mingyu Ouyang",
        "Kevin Qinghong Lin",
        "Mike Zheng Shou",
        "Hwee Tou Ng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots. However, screenshots are tokenized into thousands of visual tokens (e.g., about 4700 for 2K resolution), incurring significant computational overhead and diluting attention. In contrast, humans typically focus on regions of interest when interacting with UI. In this work, we pioneer the task of efficient UI grounding. Guided by practical analysis of the task's characteristics and challenges, we propose FocusUI, an efficient UI grounding framework that selects patches most relevant to the instruction while preserving positional continuity for precise grounding. FocusUI addresses two key challenges: (1) Eliminating redundant tokens in visual encoding. We construct patch-level supervision by fusing an instruction-conditioned score with a rule-based UI-graph score that down-weights large homogeneous regions to select distinct and instruction-relevant visual tokens. (2) Preserving positional continuity during visual token selection. We find that general visual token pruning methods suffer from severe accuracy degradation on UI grounding tasks due to broken positional information. We introduce a novel PosPad strategy, which compresses each contiguous sequence of dropped visual tokens into a single special marker placed at the sequence's last index to preserve positional continuity. Comprehensive experiments on four grounding benchmarks demonstrate that FocusUI surpasses GUI-specific baselines. On the ScreenSpot-Pro benchmark, FocusUI-7B achieves a performance improvement of 3.7% over GUI-Actor-7B. Even with only 30% visual token retention, FocusUI-7B drops by only 3.2% while achieving up to 1.44x faster inference and 17% lower peak GPU memory."
        },
        {
            "title": "Start",
            "content": "FOCUSUI: Efficient UI Grounding via Position-Preserving Visual Token Selection 1Mingyu Ouyang, 2Kevin Qinghong Lin, 1Mike Zheng Shou, 1Hwee Tou Ng 1National University of Singapore 2University of Oxford ouyangmingyu04@u.nus.edu, {kevin.qh.lin, mike.zheng.shou}@gmail.com, dcsnght@nus.edu.sg https://showlab.github.io/FocusUI 6 2 0 J 7 ] . [ 1 8 2 9 3 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots. However, screenshots are tokenized into thousands of visual tokens (e.g., about 4700 for 2K resolution), incurring significant computational overhead and diluting attention. In contrast, humans typically focus on regions of interest when interacting with UI. In this work, we pioneer the task of efficient UI grounding. Guided by practical analysis of the tasks characteristics and challenges, we propose FOCUSUI, an efficient UI grounding framework that selects patches most relevant to the instruction, while preserving positional continuity for precise grounding. FOCUSUI addresses two key challenges: (1) Eliminating redundant tokens in visual encoding. We construct patchlevel supervision by fusing an instruction-conditioned and rule-based UI-graph score that down-weights large homogeneous regions to select distinct and instruction-relevant visual tokens. (2) Preserving positional continuity during visual token selection. We find that general visual token pruning methods suffer from severe accuracy degradation on UI grounding tasks due to breaking positional information. We introduce novel POSPAD strategy, which compresses each contiguous sequence of dropped visual tokens into single special marker placed at the sequences last index to preserve positional continuity. Comprehensive experiments on four grounding benchmarks demonstrate that FOCUSUI surpasses GUI-specific baselines. On the ScreenSpot-Pro benchmark, FOCUSUI-7B achieves performance improvement of 3.7% over GUI-Actor-7B. Also, even with only 30% visual token retention, the performance of FOCUSUI-7B only drops by 3.2%, while achieving up to 1.44 faster inference and 17% lower peak GPU memory. 1. Introduction User interface (UI) visual grounding asks model to locate target region in high-resolution screenshot given natCorresponding authors. (a) Comparison of vanilla UI grounding VLMs, VLMs with visual token pruning, and our FOCUSUI. (b) Study 1: The exceptionally high proportion of visual (screenshot) vs. text (instruction) tokens in UI grounding tasks. (c) Study 2: Our proposed positionpreserving visual token selection vs. general visual token pruning methods. Figure 1. FOCUSUI is an efficient UI grounding framework that selects instruction-relevant visual tokens while preserving positional continuity. Study 1 provides motivation to address visual redundancy in UI grounding tasks, and Study 2 demonstrates the effectiveness of the our position-preserving selection. ural language instruction. Modern vision-language models (VLMs) have shown strong performance in UI tasks, including navigation and grounding, mainly driven by their abilities in processing high-resolution visual information. However, UI screenshots are typically high-resolution, and patchified into thousands of visual tokens that dominate the sequence budget (Fig. 1b). This extreme visual token skew causes substantial computational overhead. Although accuracy has improved rapidly, efficiency has been underexplored: naıve visual token pruning designed for natural images breaks positional continuity in multimodal sequences and yields severe accuracy drops on precise UI grounding tasks. Recent studies in token pruning strategies aim to mitigate the rapidly growing computational cost by visual tokens. It is typically 1 Figure 2. Overview of our proposed FOCUSUI. (a) Illustration of how the Instruction-to-Patch saliency score is constructed. (b) Queryguided Saliency Scorer and token selection. (c) Overall UI grounding framework illustrating how POSPAD is applied to dropped sequences to preserve positional continuity. For clarity, we omit the system prompt in the token sequence. achieved by exploiting redundancy and importance variance, and applying selection in prefilling stage to reduce memory and computation costs during decoding. However, directly dropping visual tokens incurs position information loss, as sequence continuity is broken, leading to severe accuracy drops on precise UI grounding. We present FOCUSUI, an efficient UI grounding framework that selects instruction-relevant visual tokens while preserving positional continuity needed for precise localization. First, lightweight Query-Guided Saliency Scorer predicts per-patch relevance under dense supervision that fuses an instruction-conditioned bounding-box overlap signal with rule-based UI-graph prior that down-weights large homogeneous regions. Second, we apply POSPAD which compacts each dropped contiguous sequence into one learnable marker placed at the sequences last index, preserving its positional information. This design mitigates sequence fragmentation and stabilizes grounding at aggressive retention ratios. FOCUSUI integrates seamlessly with VLMs based on Qwen2.5-VL [3] and Qwen3-VL [2] of multiple sizes. Across experiments on four benchmarks, FOCUSUI substantially speeds up inference and lowers peak GPU memory, while maintaining high accuracy. The main contributions of this work include: Pioneering the task of efficient UI grounding. We study the task characteristics and challenges of efficient UI grounding, presenting dedicated approach that preserves accuracy while reducing visual tokens. Instruction-to-patch selection with dense supervision. We fuse rule-based UI-graph prior with instructionconditioned bounding-box overlap to train lightweight Query-Guided Saliency Scorer that predicts per-patch saliency and filters irrelevant tokens. Position-preserving transformation. We introduce POSPAD to preserve sequence continuity during token selection, addressing the failure of general pruning methods on precise UI grounding tasks. Practical integration and results. We implement FOCUSUI with Qwen2.5-VL and Qwen3-VL backbones of multiple sizes (2B, 3B, and 7B). Our models outperform the best previous state-of-the-art models and show good accuracy-efficiency trade-offs across four UI grounding benchmarks. 2. Efficient UI Grounding: Task Characteristics and Challenges We identify two key challenges in UI grounding: (1) extreme token skew and redundancy from high-resolution screenshots, and (2) accuracy collapse under naıve visual token pruning due to broken positional continuity. In this section, we provide comprehensive empirical analysis of these challenges, thereby elaborating on the motivation for our efficient UI grounding framework. 2.1. High-Resolution Visual Understanding The task of UI grounding differs from natural visual understanding mainly in input characteristics: UI screenshots are typically high resolution (e.g., 2K at 2560 1440 or 4K at 3840 2160), compositionally structured, and dominated by large homogeneous panes interspersed with small widgets. To quantify this skewness, Study 1 in Fig. 1b shows that visual (screenshot) tokens account for 85.4% of the tokens across two benchmarks and two grounding models, confirming severe imbalance in visual tokens that incurs significant computational overhead. This motivates an instruction-aware selection that prioritizes patches relevant to the instruction and de-emphasizes visually repetitive regions. We implement this with an Instruction-to-Patch saliency score (3.2) that fuses: (i) bounding-box overlap with ground-truth box and (ii) rulebased UI-graph prior that down-weights large connected components, to guide the selection. 2.2. Position Sensitivity in UI Grounding VLMs process multimodal inputs as an interleaved sequence of visual patch tokens and text tokens [26]. In particular, Multimodal Rotary Position Embedding (M-RoPE) [28] is designed for modeling spatial and temporal relationships. In practice, Qwen2-VLs M-RoPE decomposes rotary dimensions into temporal, height, and width components to encode (t, h, w) structure [14]. However, we find that precise UI grounding is sensitive to the positional information of visual embeddings, which makes token reduction more challenging. Direct pruning creates positional jumps in the (h, w) dimensions of M-RoPE sequence, leading to pronounced localization offsets on fine-grained targets. To investigate this sensitivity, in Study 2 of Fig. 1c, we evaluate UI grounding models applied with advanced visual token pruning methods. The sharp accuracy drop suggests that although these pruning methods work well for general visual understanding scenerios, performance degrades dramatically on precise localization. We address this with POSPAD (3.3) strategy: for each contiguous sequence of dropped visual tokens, we replace the sequence with single learnable marker placed at the sequences last index, inheriting that indexs (h, w) positional information. This special marker preserves positional continuity and mitigates the disruption to the models spatial understanding. Together, Study 1 motivates what to remove (instruction-irrelevant or homogeneous regions), and Study 2 dictates how to select (position-preserving rather than naıve dropping). These findings collectively form the motivation of our efficient UI grounding framework. 3. FOCUSUI We introduce FOCUSUI, query-guided efficient UI grounding framework that selects instruction-relevant visual tokens while preserving positional continuity. As illustrated in Fig. 2, FOCUSUI comprises the following key components designed for efficient UI grounding: (i) fused supervision of per-patch saliency score to identify instruction-relevant visual tokens, (ii) lightweight Query-Guided Saliency Scorer for visual token selection, and (iii) novel positionpreserving POSPAD strategy to preserve positional information during token selection. In the following sections, we introduce each component in detail. 3.1. Instruction-to-Patch Saliency Score Motivated by observations in 2, we first construct dense supervision of per-patch saliency scores to select relevant visual tokens. We fuse two complementary components: (i) instruction-conditioned bounding-box overlap and (ii) UI-graph prior via union-find that down-weights large homogeneous regions. Bounding-Box Saliency Score. As summarized in Alg. 1, we partition the image into Gh Gw patch grid with patch size p, and denote the patch cell by Ri,j = [jp, ip, (j+1)p, (i+1)p]. Given an element bounding box bgt, each patch cell receives score proportional to its overlap with bgt. We set Sbbox [0, 1] with normalized overlap area(Ri,j bgt)/p2 so that fully covered patches score 1 Algorithm 1: Building Bounding-Box Saliency Score Input: [0, 1]HW 3, patch size p, ground-truth bbox bgt = (x1, y1, x2, y2) Output: Sbbox [0, 1]GhGw Gh H/p, Gw W/p for 0 to Gh 1 do for 0 to Gw 1 do Ri,j [jp, ip, (j+1)p, (i+1)p]; Sbbox[i, j] area(Ri,j bgt)/p2 return Sbbox Algorithm 2: Building UI-Graph Saliency Score Input: [0, 1]HW 3, threshold τ , patch size Output: Suig [0, 1]GhGw Gh H/p, Gw W/p Form patch pixels i,j R3pp for 0 < Gh, 0 < Gw Union-Find on nodes (i, j) for 0 to Gh 1 do for 0 to Gw 1 do if + 1 < Gw and vec(P i,j) vec(P i,j+1)2 < τ then UNION(cid:0)(i, j), (i, + 1)(cid:1) if + 1 < Gh and vec(P i,j) vec(P i+1,j)2 < τ then UNION(cid:0)(i, j), (i + 1, j)(cid:1) (cid:12){(i, j) : ri,j = u}(cid:12) Obtain component ids ri,j FIND(i, j) Counts nu (cid:12) (cid:12) for each unique root Assigning Weights: wu (cid:0) max{1, ln(nu + 1)}(cid:1)1 Set Suig[i, j] wri,j for all i, return Suig 3 and disjoint patches score 0, inducing center-to-edge decay along the box boundary. UI-Graph Saliency Score. To further suppress background regions and enrich supervision on non-annotated regions, we propose UI-graph saliency score based on unionfind over connected components of visual patches, which is inspired by the UI-graph prior in ShowUI [17]. Specifically, we treat each patch (i, j) in Ri,j as node and connect 4-neighborhood pairs whose ℓ2 distance in the RGB space is below threshold τ . Such unionfind groups connected components whose size nu reflects how visually repetitive region is. We then assign weight wu = (max{1, ln(nu+1)})1 to each patch so that large homogeneous regions (e.g., empty backgrounds) receive lower weights. The UI-graph score Suig sets each patch to its component weight wu. Such design naturally suppresses background regions and enhances the saliency of distinctive elements. This score is instructionagnostic, annotation-free, and complements Sbbox for each patch. See Alg. 2 for the full procedure. Fuse Supervision. Finally, we fuse the two scores to obtain joint supervision SIns2Patch as Instruction-to-Patch saliency score: SIns2Patch = λ Sbbox + (1 λ) Suig (1) where λ [0, 1] is controllable weight and empirically set to 0.8 across experiments. Fig. 3 provides an illustration of the two components and the final fused supervision. Figure 3. Illustrative example of building the Instruction-to-Patch saliency score. (a) Screenshot with ground-truth bounding box bgt. (b) Bounding-box saliency score Sbbox. (c) Union-find results. (d) Size of each connected component nu. (e) UI-graph saliency score Suig. (f) Fused supervision SIns2Patch by combining (d) and (e). Brighter regions represent positive patches and darker regions represent negative patches. 4 3.2. Lightweight Query-Guided Saliency Scorer With the obtained per-patch supervision SIns2Patch from Eq. (1), we train lightweight module, Query-Guided Saliency Scorer, that predicts per-patch saliency from similarities between patch and query text embeddings in the VLM backbone, as shown in Fig. 2 (b). Concretely, let {vi}M i=1 be patch embeddings from the vision encoder and {ej}N j=1 be query text embeddings (only the part corresponding to the instruction) in the language model (LM) space. We use self-attention layer to enhance features in each modality, preserving the original embedding semantics while strengthening cross-modal interactions. tanh constraint followed by ℓ2 normalization is applied to each feature to bound the similarities. We then compute token-wise similarities RM by matrix product between patch and text embeddings. Finally, we aggregate the similarities over text query dimensions with mean pooling to get per-patch saliency scores si: = E RM , si = 1 (cid:88) j=1 Pi,j . (2) To train the Query-Guided Saliency Scorer, we convert scores to probabilities and optimize KL divergence objective. Given fused supervision from Eq. (1), we minimize: LIns2Patch = KL(softmax(SIns2Patch) softmax(s)) . (3) 3.3. POSPAD: Positional Continuity Preservation Token Selection Policy. We first apply top-K selection over predicted per-patch saliency scores {si}iI from Eq. (2). Given retention ratio (0, 1], the number of kept patches is set to = rM . Let γ be the K-th element of the sorted list {si}iI. We form the kept index set = {i si γ} and drop the remaining indices = {i si < γ}. Sequence Transformation. After selecting instructionrelevant visual tokens, we further refine the sequence to alleviate positional information loss in the models spatial understanding. We introduce POSPAD, position-preserving sequence transformation that replaces each contiguous sequence of dropped visual tokens with single learnable special token POSPAD placed at the last index of that sequence. The illustration of POSPAD is shown in Fig. 4. Specifically, given the original visual token sequence x1:M , the kept index set K, and the drop index set defined above, we partition into contiguous sequences (i.e., maximal consecutive sequences) R1, . . . , RU with respect to the 1D flattened sequence order. For each sequence Ru, we keep only its last index rend = max Ru and remove the others. Let Eseq-end = {rend }U u=1 denote the set of sequence-end indices, and define the preserved index set = Eseq-end. sequence without altering its original architecture. We apply our framework to Qwen2.5-VL and Qwen3-VL models. For the Qwen3-VL model with DeepStack [20] vision encoder, deep visual embeddings are gathered only for the kept image tokens K. Coordinate-free UI Grounding with Selected Patches. We find the coordinate-free UI grounding scheme from GUIActor [30] most compatible with our selection: the model grounds elements directly at the patch embeddings with an extra action head on top of the LM decoder, while our visual token selection reduces candidates by discarding instructionirrelevant regions. Specifically, the decoder LM outputs sequence of action tokens: LM(I, q) = {x1:i1, <ACTOR START>, <ACTOR>, <ACTOR END>, xi+3:N }. (5) Then the action head aligns hACTOR with visual patches to produce an attention map over patches. We first contextually refine selected patch features {vi}M i=1 with self-attention layer: v1, . . . , vM = SelfAttn(v1, . . . , vM ). Then we project hACTOR and each vi with separate MLPT and MLPV and compute attention scores: = MLPT (hACTOR), αi = zzi , ai = softmax(α)i. zi = MLPV (vi), (6) The distribution ai identifies the most relevant regions for executing the action. With selected visual tokens, such an action head benefits from fewer visual candidates and retained patches that are more relevant to the instruction. Training Objective. The Query-Guided Saliency Scorer is trained end-to-end with the downstream LM objective next-token prediction loss LNTP and an action-attention loss LAttn for grounding: LAttn = (cid:88) i=1 pi log pi ai , pi = yi j=1 yj +ϵ (cid:80)M , = 1,. . . ,M (7) where yi denotes the attention score label for the i-th patch (1 if it overlaps with the ground-truth bounding box, 0 otherwise) and ϵ is small constant for numerical stability. The overall training objective is: = LIns2Patch + LNTP + LAttn. (8) 4. Experiments 4.1. Experimental Setup Implementation Details We adopt the state-of-the-art VLMs Qwen2.5-VL [3] and Qwen3-VL [2] as our base models, with different sizes to demonstrate the generalizability of our approach. We conduct supervised fine-tuning to Illustration of POSPAD sequence transformation for Figure 4. positional continuity preservation via an example 2D image (23 patches) and its 1D sequence. learnable <pos pad> marker is placed at the last index of each contiguous sequence of dropped visual tokens, as illustrated by strategy (d). We then replace each contiguous sequence with single marker <pos pad> and keep all other tokens unchanged: = (cid:40) <pos pad> if Eseq-end, xj if K, (4) POSPAD(x1:M ) = {x j}jS . Thus, the final output length of visual tokens is = (D ), with the total number of <pos pad> tokens being . Each dropped sequence Ru reduces the sequence by Ru 1 while preserving positional continuity at the sequence end. Concrete examples of , , and under different retention ratios are investigated in Tab. 7c. Compared to direct dropping, POSPAD preserves positional continuity and empirically stabilizes the models spatial understanding. Alternative strategies are also studied in 4.2.5. Since POSPAD alters only sequence sparsity and not token indices or rotary bases, it is compatible with common M-RoPE implementations and requires no modifications to the downstream LM architecture. 3.4. Efficient UI Grounding Framework Integration with VLMs. We integrate our visual token selection strategy into existing VLMs before visual patch embeddings are fed into the LM decoder. Concretely, the QueryGuided Saliency Scorer takes the patch features {vi}M i=1 and the instruction token embeddings ej, computes scores si via Eq. (2), and selects the top-K indices for given retention ratio r. We then refine the sequence with POSPAD, yielding compact visual sequence of length that preserves positional continuity. The LM decoder processes this Model Operator [22] OS-Atlas-7B [31] Aguvis-7B [35] Tong-UI-7B [38] UGround-V1-7B [13] UI-TARS-7B [23] UI-TARS-72B [23] UI-TARS-1.5-7B [23] Qwen2.5-VL-3B [3] Qwen2.5-VL-7B [3] Qwen2.5-VL-32B [3] GUI-Actor-3B [30] GUI-Actor-7B [30] Jedi-3B [33] Jedi-7B [33] FOCUSUI-3B (r = 100%) FOCUSUI-3B (r = 50%) FOCUSUI-3B (r = 30%) FOCUSUI-7B (r = 100%) FOCUSUI-7B (r = 50%) FOCUSUI-7B (r = 30%) ScreenSpot-V2 ScreenSpot-Pro Mob.-T Mob.-I Des.-T Des.-I Web-T Web-I Avg Dev Cre. CAD Sci. Office OS Avg-T Avg-I Avg 47.3 95.2 95.5 93.1 95.0 96.9 94.8 - 93.4 97.6 97.9 97.6 97.6 96.6 96.9 99.2 98.8 98.5 98.8 98.8 98.8 41.5 75.8 77.3 81.5 83.3 89.1 86.3 - 73.5 87.2 88.2 83.4 88.2 81.5 87.2 85.9 86.9 85. 91.6 92.2 90.1 90.2 90.7 95.4 96.4 95.0 95.4 91.2 - 88.1 90.2 98.5 96.9 96.9 96.9 95.9 96.1 95.0 96.1 95.6 93.9 93.3 80.3 63.6 77.9 82.9 77.8 85.0 87.9 - 58.6 74.2 79.3 83.6 85.7 78.6 87.9 87.3 87.3 87. 92.1 87.3 85.7 92.8 90.6 91.0 90.2 92.1 93.6 91.5 - 88.0 93.2 91.2 94.0 93.2 88.5 94.4 95.4 95.4 94.3 95.0 95.0 93.9 - - 84.3 70.5 35.1 39.6 16.1 43.7 53.0 32.7 45.0 77.3 84.1 17.7 17.9 10.3 24.4 27.4 16.8 28.1 72.4 86.0 16.1 21.4 13.8 34.6 34.3 19.4 84.7 88.7 22.7 21.1 15.3 34.3 38.3 18.4 35.1 77.2 87.6 28.1 31.7 14.6 39.0 49.6 24.5 85.2 91.6 36.1 32.8 18.0 50.0 53.5 24.5 47.8 87.7 90.3 40.8 39.6 17.2 45.7 54.8 30.1 50.9 90.0 31.8 40.2 31.8 47.2 65.6 33.2 71.4 80.9 21.4 25.8 18.4 29.5 40.9 20.4 37.8 81.3 88.8 29.1 24.9 13.8 31.1 45.7 22.4 39.9 86.2 91.3 48.5 41.1 32.6 57.1 67.4 42.3 63.2 85.7 91.0 39.8 36.7 34.1 49.6 61.3 35.2 86.7 92.1 38.1 41.4 38.3 50.8 63.0 38.8 83.7 88.6 38.1 34.6 84.2 91.7 27.4 34 23 38.6 57.0 25.0 49.8 32.2 52.4 68.7 26.0 52.6 - - - - 81.9 91.5 43.1 37.0 37.6 48.4 61.7 38.3 59.3 81.9 91.4 42.1 37.0 36.4 46.9 58.3 35.2 56.7 81.9 91.0 38.1 35.8 33.3 44.5 57.8 37.2 55. 84.4 93.1 44.5 41.1 42.9 52.0 69.6 44.4 64.7 85.2 92.6 42.8 40.5 40.2 51.6 67.0 40.3 61.7 85.2 91.8 38.8 39.9 42.9 49.2 64.4 38.8 60.4 23.0 36.6 4.0 18.9 22.9 - 8.0 25.7 31.1 - 16.2 35.7 17.5 38.1 - 42.6 6.6 25.9 7.6 27.6 22.5 47.6 42.2 44.6 13.7 36.1 18.2 39.5 - - 18.9 43.8 19.0 42.3 17.4 40.6 21.9 48.3 21.9 46.5 20.4 45.1 Table 1. Performance comparison on ScreenSpot-V2 [31] and ScreenSpot-Pro [15]. Model Text Elem Layout Manip Refuse Avg Model Basic Functional Spatial Avg Gemini-2.5-Pro [12] Operator [22] UGround-V1-7B [13] Aguvis-7B [35] UI-TARS-7B [23] UI-TARS-1.5-7B [23] Qwen2.5-VL-3B [3] Qwen2.5-VL-7B [3] GUI-Actor-3B [30] GUI-Actor-7B [30] Jedi-3B [33] Jedi-7B [33] 59.8 45.5 51.3 42.4 51.3 40.3 55.9 41.2 60.2 51.8 70.1 57.9 41.4 28.8 45.6 32.7 60.5 56.1 60.2 54.2 67.4 53.0 65.9 55. FOCUSUI-3B (r = 100%) 65.9 57.6 FOCUSUI-3B (r = 50%) 64.8 59.4 FOCUSUI-3B (r = 30%) 62.5 56.7 FOCUSUI-7B (r = 100%) 63.6 61.2 FOCUSUI-7B (r = 50%) 64.0 62.1 FOCUSUI-7B (r = 30%) 63.6 60.9 49.0 46.6 43.5 43.9 54.9 59.7 34.8 41.9 58.5 58.1 53.8 57.7 59.7 63.6 62.9 63.6 63.6 64.4 33.6 31.5 24.8 28.2 35.6 51.7 13.4 18.1 32.2 30.9 44.3 46. 37.6 37.6 33.6 34.9 31.5 31.5 38.9 45.2 40.6 0.0 36.4 0.0 38.7 0.0 47.5 0.0 56.0 0.0 27.3 0.0 31.4 0.0 50.5 0.0 49.5 0.0 50.9 7.4 54.1 7.4 0.0 0.0 0.0 0.0 0.0 0.0 53.4 54.6 51. 54.4 54.1 53.9 Claude-3.7-Sonnet [1] ShowUI-2B [17] OSAtlas-7B [31] UGround-7B [13] UGround-V1-7B [13] Aguvis-7B [35] UI-TARS-7B [23] UI-TARS-72B [23] GUI-Actor-3B [30] GUI-Actor-7B [30] Jedi-3B [33] Jedi-7B [33] FOCUSUI-3B (r = 100%) FOCUSUI-3B (r = 50%) FOCUSUI-3B (r = 30%) FOCUSUI-7B (r = 100%) FOCUSUI-7B (r = 50%) FOCUSUI-7B (r = 30%) 9.48 8.07 12.2 11.5 15.4 17.8 20.1 31.4 27.4 30.1 22.3 32.3 30.0 29.7 29. 33.6 32.5 32.3 7.73 7.67 11.2 12.2 17.1 18.3 24.3 30.5 24.6 28.1 25.2 30.5 26.9 26.0 26.4 31.2 31.0 29.2 7.60 2.07 3.67 2.79 6.25 5.06 8.37 14.7 7.0 7.8 9.35 12.8 8.7 8.2 7. 11.2 11.3 11.0 8.27 5.94 9.02 8.83 12.9 13.7 17.6 25.5 19.3 21.6 18.7 24.8 21.5 20.9 20.6 24.9 24.5 23.8 Table 2. Performance comparison on OSWorld-G [33]. Table 3. Performance comparison on UI-Vision [21]. obtain the following variants: FOCUSUI-3B and FOCUSUI7B with Qwen2.5-VL and FOCUSUI-QWEN3-VL-2B with Qwen3-VL. For fair comparison, we align the training budget with the baseline method GUI-Actor [30], using approximately 1M screenshots collected from several public UI datasets. To ensure annotation quality, we follow V2P [6] to apply OmniParser [19] to filter samples whose IoU between ground-truth and detected boxes is below 0.3. The visual token retention ratio is sampled uniformly from (0.1, 1.0) during training. All models are trained with DeepSpeed [25] Zero-2 on 8NVIDIA H200 GPUs for 1 epoch. More training details are provided in the Appendix. Evaluation Benchmarks We conduct experiments on four UI grounding benchmarks, including ScreenSpot-V2 [31], 6 Model ScreenSpot-V2 ScreenSpot-Pro Avg-T Avg-I Avg Avg-T Avg-I Avg Model %Ret. + Pruning Method (Venue) Ratio SS-V2 Avg SS-Pro OSWorld-G Avg Avg Qwen3-VL-2B [3] 94.7 78.9 87. 52.8 16.7 39.0 FOCUSUI-QWEN3-VL-2B (r = 100%) 95.8 95.7 FOCUSUI-QWEN3-VL-2B (r = 50%) 93.5 FOCUSUI-QWEN3-VL-2B (r = 30%) 85.6 91.4 85.0 91.0 84.3 89.5 51.5 52.5 49.7 20.9 39.8 20.9 40.4 20.2 38. Table 4. Performance comparison of models based on the Qwen3VL backbone. indicates results obtained from our own evaluation of the official model on HuggingFace [29]. ScreenSpot-Pro [15], OS-World-G [33], and UI-Vision [21]. Among them, ScreenSpot-Pro features higher-resolution interfaces that simulate multi-source real-world applications, serving as practical testbed for evaluating the properties of efficiency and precise UI grounding. 4.2. Main Results We organize our main results with the following five research questions (RQs): RQ1 (4.2.1 Performance) : Can FOCUSUI effectively reduce visual tokens while preserving accuracy? RQ2 (4.2.2 Comparison to General Pruning Methods): How does FOCUSUI compare to general visual token pruning methods? RQ3 (4.2.3 Efficiency Analysis): What efficiency gains does FOCUSUI achieve under different settings? RQ4 (4.2.4 Qualitative Results): How does FOCUSUI select instruction-relevant visual tokens? RQ5 (4.2.5 Ablation Study): How do the components and retention settings affect performance? 4.2.1. RQ1: Performance Tables 1, 2 and 3 report grounding performance on ScreenSpot-V2 [31] & ScreenSpot-Pro [15], OS-WorldG [33], and UI-Vision [21], respectively. We test series of retention ratios {100%, 50%, 30%} to characterize degradation curves and compare to dense baselines that consume all visual tokens. Across all four benchmarks, FOCUSUI exceeds GUI-specific baselines with the same size even at 30 50% token retention, achieving state-of-the-art grounding performance. Additionally, we report the performance of FOCUSUI-QWEN3-VL-2B based on the more recent state-of-the-art Qwen3-VL [2] backbone in Tab. 4. More detailed breakdown and retention ratio results are provided in the Appendix. 4.2.2. RQ2: Comparison to General Pruning Methods Tab. 5 presents comparison with alternative general VLM visual token pruning methods. Specifically, we compare against Fast-V [7], HiPrune [18], and Vision-Zip [36]. Our FOCUSUI preserves near-baseline accuracy at 30% token retention (within 0.5/3.2/2.5 points on ScreenSpot-V2/Pro/OSWorld-G), while general pruning severely degrades performance. Notably, our method is natively compatible with Qwen2.5-VL-3B 100% 30% 38.6 (-52.7%) 4.8 (-81.6%) 14.4 (-47.4%) + Fast-V (ECCV24) [7] 30% 72.0 (-11.7%) 18.0 (-30.8%) 20.4 (-25.3%) + HiPrune (arXiv25) [18] + Vision-Zip (CVPR25) [36] 30% 75.4 (-7.5%) 18.9 (-27.4%) 23.0 (-15.6%) 81.5 26.1 27.3 Jedi-3B 100% 30% 51.0 (-42.6%) 14.1 (-60.9%) 23.9 (-51.0%) + Fast-V (ECCV24) [7] 30% 80.9 (-9.0%) 26.2 (-27.3%) 40.4 (-17.1%) + HiPrune (arXiv25) [18] + Vision-Zip (CVPR25) [36] 30% 82.8 (-6.9%) 28.8 (-20.3%) 41.5 (-14.9%) 88. 36.1 48.8 FOCUSUI-3B 100% 91.5 43. 53.4 + Saliency Scorer w/ POSPAD 30% 91.0 (-0.5%) 40.6 (-7.3%) 51.8 (-3.0%) Table 5. Comparison to general visual token pruning methods. Model %Ret. #Vis. per Sample Max GPU SS-Pro Ratio Token Time (sec) Mem. (MB) Acc Base Model: Qwen2.5-VL, max pixel = 6400 28 28 = 4816000 FOCUSUI-7B FOCUSUI-7B FOCUSUI-7B FOCUSUI-7B 100% 5319 1.75 (1.00) 20994 (1.00) 70% 3989 1.67 (1.05) 18334 (0.87) 50% 2659 1.49 (1.18) 17944 (0.85) 30% 1329 1.22 (1.44) 17392 (0.83) Base Model: Qwen3-VL, max pixel = 6000 32 32 = 6144000 FOCUSUI-QWEN3-VL-2B 100% 4627 0.97 (1.00) 6278 (1.00) FOCUSUI-QWEN3-VL-2B 70% 3470 0.90 (1.08) 6142 (0.98) FOCUSUI-QWEN3-VL-2B 50% 2313 0.85 (1.14) 5680 (0.91) FOCUSUI-QWEN3-VL-2B 30% 1156 0.71 (1.37) 5170 (0.82) 48.3 47.7 46.5 45. 39.8 40.1 40.4 38.5 Table 6. Efficiency analysis on ScreenSpot-Pro benchmark under different retention ratios and model backbones of FOCUSUI. The number of <pos pad> tokens is not included. FlashAttention [10] since it does not require any intermediate attention or activation information. 4.2.3. RQ3: Efficiency Analysis In Tab. 6, we evaluate FOCUSUI with different Qwen2.5VL and Qwen3-VL backbones to study efficiency gains and accuracy-efficiency trade-offs. Results show that reducing retention ratio from 100% to 30% yields up to 1.44 faster inference and about 1718% lower peak memory with only 3.2-point accuracy loss. 4.2.4. RQ4: Qualitative Results Fig. 5 shows qualitative examples of FOCUSUI. The predicted heatmaps show that the model effectively selects the relevant visual tokens for the instruction while suppressing background regions. 4.2.5. RQ5: Ablation Study We highlight the effectiveness of our proposed components in Tab. 7. Models evaluated in experiments in Tab. 7a and 7b use Qwen2.5-VL-3B as the base model and are trained with 30% of the full data. Visual Token Selection. We compare with the variants illustrated in Fig. 4: (a) Original visual sequence. (b) Direct Drop. (c) Full Padding which preserves continuity by inserting <pos pad> at every dropped position. We also test zero-shot CLIP [24] as the scoring strategy. The performance of these variants is shown in Tab. 7a. 7 Figure 5. Qualitative visualization of predicted saliency heatmaps and retained patches under retention ratio = 30%. Black regions denote dropped visual tokens that are not consumed by the LM during decoding. Examples are taken from the ScreenSpot-V2 and ScreenSpot-Pro benchmarks, spanning web, desktop, and mobile interfaces. %Ret. Ratio Token Len? Continuity? Reduce Preserve Pos. SS-Pro Scoring Variant Baseline Retain Strategy (a) N/A CLIP Score [24] Ins2Patch Score (b) Direct drop (c) Full padding (d) POSPAD (b) Direct drop (c) Full padding (d) POSPAD 100% 50% 50% 50% 50% 50% 50% Acc 40.9 28.5 38.7 38.2 29.2 42.1 42.3 (a) Different visual token selection methods and positional continuity retention strategies. Variant SS-Pro Acc w/ UI-Graph Labeling only w/ BBox-based Labeling only Full FOCUSUI 41.1 39.8 42. (b) Ins2Patch score ablation with reduction rate of 50%. %Ret. Ratio Tokens Tokens Tokens Acc #POSPAD #Total SS-Pro #Vis. 100% 6019 75% 4514 50% 3009 25% 1504 10% 601 0 435 433 315 6140 5070 3563 1941 915 43.8 43.3 42.3 40.6 36.6 (c) Different retention ratios and numbers of tokens. Table 7. Ablation of key components of FOCUSUI. Instruction-to-Patch Saliency Score Supervision. Results in Tab. 7b indicate that removing either the UI-graph prior or the bounding-box overlap score degrades accuracy relative to the fused supervision of FOCUSUI. Retention Ratio. Tab. 7c suggests smooth accuracyretention trade-off of our FOCUSUI: 100% matches the dense baseline, 50% still retains most performance, and further aggressive settings incur larger accuracy drops. 5. Related Work transitions to purely visual solutions for task planning, element grounding, and interface control [9, 17, 23, 33] that fully utilizes VLMs capability. UI Visual Grounding Given screenshot and natural language instruction, UI grounding locates the target region for interaction on the screen. With more advanced model design [6, 9, 17, 23, 27, 30, 31, 37, 38] and data scaling [8, 13, 33, 35], the performance of UI grounding improves rapidly in recent times. Visual Token Reduction Compared to information-dense text, visual tokens often exhibit substantial redundancy, motivating token reduction to lower computation cost [4, 7, 34, 40]. Recent work further explores training-free pruning based on token importance and redundancy [18, 41] or implement encoder-side compression [36, 39]. 6. Conclusion In this paper, we introduced FOCUSUI, query-guided framework for efficient UI grounding that selects instructionrelevant visual tokens while preserving positional continuity. Integrated with state-of-the-art VLMs, FOCUSUI achieves strong accuracy-efficiency trade-offs across four UI grounding benchmarks. VLM-Powered GUI Agents Recent advances in VLMs have accelerated progress on GUI agents that perceive, plan, and act in graphical interfaces [1, 3, 12, 22, 28]. Starting from text-dependent GUI agents [11, 42], it progressively Limitations and Future Work. FOCUSUI primarily gains efficiency from spatial visual token reduction. Future work may consider the temporal dimension, as UI interactions typically involve multi-round and sequential actions. 8 A. Implementation Details A.1. Training Data Raw Dataset Our training set compiles several public highquality GUI datasets, following GUI-Actor. To ensure fair evaluation, samples from Wave-UI that overlap with the test sets of downstream tasks are excluded. Refining Annotation Quality We apply OmniParser V2 [19] to filter samples whose IoU between ground-truth and OmniParser detected boxes is below 0.3. This results in reduction of 22.9% in the number of elements. The final training statistics are in Tab. 8. Dataset #Screenshots #Elements Platform UGround [13] GUI-Env [30] GUI-Act [30] AndroidControl [16] AMEX [5] Wave-UI 775K 70K 13K 47K 100K 7K Total (Raw Dataset) Total (After Filtering) 1012K 976K 8M 262K 42K 47K 1.2M 50K 9.6M 7.4M Web Web Web Android Android Hybrid Table 8. Statistics of training datasets used for FOCUSUI. A.2. Training Details We train FOCUSUI on 8 NVIDIA H200 GPUs using bfloat16 precision, DeepSpeed ZeRO-2 [25], and FlashAttention-2 [10]. The effective batch size per GPU is set to 32 (with gradient accumulation of 4), and the max pixels is set to 5720064, matching GUI-Actor. Training proceeds in two stages: Stage 1: Saliency Scorer Pre-training. We pretrain the randomly initialized Query-Guided Saliency Scorer for 1 epoch with learning rate of 1e 4. This takes about 12 hours for both 3B and 7B models. Stage 2: Full Model Fine-tuning. We fine-tune all parameters for 1 epoch with learning rate of 5e 6. This takes about 36 hours for the 3B model and about 48 hours for the 7B model. Hyperparameter Details. During training, we construct per-patch saliency score supervision with τ = 2 and λ = 0.8. Patch size is set to 14 and 16 for Qwen2.5-VL and Qwen3VL based models, respectively. The visual token retention ratio is uniformly sampled from (0.1, 1.0) for each training sample. To enable reproducibility, we use the final checkpoint for all obtained FOCUSUI models. We also provide the full Weights & Biases (WandB) logs for all trained models. The training loss and evaluation curves during the training of FOCUSUI-7B are shown in Fig. 6. (a) Total Loss curve during training. (b) Evaluation: ScreenSpot-Pro and UI-Vision with retention ratio = 100%. (c) Evaluation: ScreenSpot-Pro and UI-Vision with retention ratio = 50%. Figure 6. WandB loss and evaluation results of FOCUSUI-7B. A.3. UI Grounding Benchmarks We evaluate on four public benchmarks containing screenScreenSpot-V2 [31], shots paired with instructions: ScreenSpot-Pro [15], OS-World-G [33], and UIVision [21]. The statistics of these benchmarks are shown in Tab. 9. ScreenSpot-V2 [31]. refined version of ScreenSpot [9] with 1,272 samples across mobile, desktop, and web environments. ScreenSpot-Pro [15]. This benchmark contains 1,581 samples from 23 professional applications, targeting highresolution interfaces and complex layouts to test generalization. 9 OS-World-G [33]. Sampled from OSWorld [32], this benchmark includes 564 samples categorized by task type (text matching, element recognition, layout understanding, finegrained manipulation, and refusal). UI-Vision [21]. desktop-centric benchmark with 5,790 samples from 83 applications, evaluating element grounding, layout grounding, and action prediction. Benchmark #Samples Avg Res. Max Res. Platform ScreenSpot-V2 ScreenSpot-Pro OS-World-G UI-Vision 1272 1581 564 5790 17251657 28801800 Hybrid 32671727 60163384 Desktop 1696955 19201080 Desktop 18511034 33602036 Desktop Table 9. Overview of the evaluation benchmarks used in this work. B. Discussion B.1. Visual Redundancy Analysis Tab. 10 provides the token statistics of Study 1 (shown in Fig. 1(b) in the main paper). Using the default model settings on the ScreenSpot-Pro evaluation, we find that visual tokens occupy at least 84.3% of the sequence across the studied benchmarks, confirming significant visual redundancy in UI grounding tasks. Benchmark Model #Sys. #Vis. Tokens Tokens Tokens #Inst. Vis. Token ScreenSpot-V ScreenSpot-Pro OS-World-G UI-Vision Jedi-1080p GUI-Actor Jedi-1080p GUI-Actor Jedi-1080p GUI-Actor Jedi-1080p GUI-Actor 397 90 397 90 397 90 397 90 2348 2629 5801 2244 2244 2249 2566 4.5 4.5 5.2 5.2 21.3 21. 9.9 9.9 % 85.4% 97.1% 86.7% 98.1% 84.3% 95.3% 84.7% 96.3% Table 10. Token statistics of Study 1 shown in Fig. 1(b) in the main paper. B.2. Position Sensitivity Analysis Tab. 11 shows the detailed results of Study 2 (shown in FIg. 1(c) in the main paper), comparing FOCUSUI with UI grounding models integrated with advanced visual token pruning methods. %Ret. Model Ratio + Pruning Method Avg SS-V SS-Pro OSW-G 100% Qwen2.5-VL-3B 50% 30% + Fast-V + HiPrune + Vision-Zip + Fast-V + HiPrune + Vision-Zip 100% Jedi-3B 50% 30% + Fast-V + HiPrune + Vision-Zip + Fast-V + HiPrune + Vision-Zip 100% FOCUSUI-3B 50% 30% + Full Settings + Full Settings 81.5 43.5 80.4 81.0 38.6 72.0 75.4 88. 50.3 88.3 88.1 51.0 80.9 82.8 91.5 91.4 91.0 Avg 26. 13.9 20.3 21.0 4.8 18.0 18.9 36.1 20.4 32.8 32.9 14.1 26.2 28.8 43. 42.3 40.6 Avg 27.3 14.3 26.2 27.1 14.4 20.4 23.0 48. 25.3 46.4 46.6 23.9 40.4 41.5 53.4 54.6 51.8 Table 11. Detailed comparison with general visual token pruning methods for Study 2 shown in Fig. 1(c) in the main paper. ranked patches (i.e., the top-K visual tokens): Patch Recall@K% = GT positive regions in top K% Total GT positive regions where ground-truth positive regions correspond to the area of UI elements paired with the given instruction. We evaluate {5%, 10%, 25%, 50%}. Additionally, we report the Full Coverage Budget, the percentage of visual tokens needed to fully cover the ground-truth elements. Results are shown in Tab. 12. Patch Recall@K% @5% @10% @25% @50% Avg Full Coverage Budget Model Random CLIP Zero-shot Baselines 0.26 0.11 0.41 0.21 0.51 0.23 0.65 0.35 0.05 0.12 FOCUSUI-3B 0.39 FOCUSUI-7B 0.43 Our Query-Guided Saliency Scorer 0.96 0.69 0.83 0.97 0.71 0. 0.56 0.60 0.85 0.61 0.25 0.24 Table 12. Patch Recall@K% and Full Coverage Budget performance comparison on the ScreenSpot-Pro benchmark. C. More Experimental Results C.2. Analysis of POSPAD C.1. Effective Visual Selection: Patch Recall@K% We verify the effectiveness of our visual token selection using Patch Recall@K%, defined as the fraction of groundtruth (GT) regions captured within the top K% of saliencyTo better understand the effect of preserving the original spatial layout, we further analyze the placement of POSPAD within contiguous sequences of dropped visual tokens, comparing three variants: (i) sequence-first, (ii) sequence-middle, and (iii) sequence-end (our proposed POSPAD). 10 Sequence Type SS-Pro Avg. C.4. Qualitative Examples = 100% = 75% = 50% = 25% sequence-first sequence-middle sequence-end (POSPAD) 42.0 41.2 42.3 41.8 41.1 42.1 39.8 38.7 40. 36.8 33.5 37.7 Table 13. Different placement of the POSPAD token. Tab. 13 reports study that varies the visual token retention ratio and the location of the POSPAD token. Across all retention ratios, placing POSPAD at the end of each dropped sequence achieves the best performance, especially under low retention ratios. The empirical results confirm our intuition: placing POSPAD at the end of the sequence is more compatible with the raster-scan ordering used by the vision encoder and M-RoPE. In contrast, placing POSPAD at the beginning or in the middle of the sequence pulls the whole region toward earlier positions, making it harder for the LM decoder to align with the original spatial structure. C.3. Detailed Performance vs. Retention Ratio Fig. 7 presents the detailed performance of FOCUSUI on ScreenSpot-V2 and ScreenSpot-Pro across varying reduction ratios (1 retention ratio r). The results demonstrate that FOCUSUI maintains high UI grounding accuracy even with significant visual token reduction. (a) Performance vs. reduction ratio on ScreenSpot-V2. (b) Performance vs. reduction ratio on ScreenSpot-Pro. Figure 7. UI grounding accuracy under different token reduction ratios. 11 Fig. 8 visualizes saliency maps on ScreenSpot-V2 and ScreenSpot-Pro, showing that our Query-Guided Saliency Scorer effectively highlights instruction-relevant regions while suppressing the background. We find that for straightforward tasks (Fig. 8 (a, d)), saliency scores peak significantly at the ground-truth locations. In more complex scenarios (Fig. 8 (b, c)), while the scores may be less concentrated, the model still successfully distinguishes potential targets from irrelevant background elements. D. Prompt Templates FOCUSUI (with Qwen2.5-VL base model) Below is the system prompt for FOCUSUI-3B and FOCUSUI-7B. You are GUI agent. Given screenshot of the current GUI and human instruction, your task is to locate the screen element that corresponds to the instruction. You should output PyAutoGUI action that performs click on the correct position. To indicate the click location, we will use some special tokens, which is used to refer to visual patch later. For example, you can output: pyautogui.click(<your_special_token_here >). FOCUSUI (with Qwen3-VL base model) Below is the system prompt for FOCUSUI-QWEN3-VL-2B. You are GUI agent. Your task is to locate the screen element that corresponds to the instruction. You should not call any external tools. Output only the coordinate of one point in your response. Format: (x, y) Qwen2.5-VL Below is the system prompt for evaluating Qwen2.5-VL models. You are GUI agent. Your task is to locate the screen element that corresponds to the instruction. You should not call any external tools. Output only the coordinate of one point in your response. Format: (x, y) Jedi and Qwen3-VL Below is the system prompt for evaluating Jedi-3B, Jedi-7B, and Qwen3-VL models. # Tools You may call one or more functions to assist with the user query. You are provided with function signatures within <tools></tools> XML tags: <tools> {\"type\": \"function\", \"function\": {\"name\": \" computer_use\", \"description\": \"Use mouse to interact with computer.n* The screens resolution is {screen_width}x{screen_height}.n* Make sure to click any buttons, links, icons, etc Figure 8. Qualitative examples of predicted per-patch saliency. Left: original screenshot; Middle: predicted saliency map; and Right: visual token selection results with = 30%. with the cursor tip in the center of the element . Dont click boxes on their edges unless asked. n* you can only use the left_click and mouse_move action to interact with the computer. if you can find the element, you should terminate the task and report the failure.\", \"parameters\": {\" properties\": {\"action\": {\"description\": \"The action to perform. The available actions are:n* mouse_move: Move the cursor to specified (x, y) pixel coordinate on the screen.n* left_click : Click the left mouse button with coordinate (x , y).n* terminate: Terminate the current task and report its completion status.\", \"enum\": [\" mouse_move\", \"left_click\"], \"type\": \"string\"}, \" coordinate\": {\"description\": \"(x, y): The ( pixels from the left edge) and (pixels from the top edge) coordinates to move the mouse to. Required only by action=mouse_move and action= left_click.\", \"type\": \"array\"}, \"status\": {\" description\": \"The status of the task. Required only by action=terminate.\", \"type\": \"string\", \" enum\": [\"success\", \"failure\"]}}, \"required\": [\" action\"], \"type\": \"object\"}}} </tools> For each function call, return json object with function name and arguments within <tool_call></ tool_call> XML tags: <tool_call> {\"name\": <function-name>, \"arguments\": <args-json -object>} </tool_call>"
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Claude 3.7 sonnet system card, 2025. 6, 8 [2] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. 2, 5, 7 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 5, 6, 7, 8 [4] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your ViT but faster. In International Conference on Learning Representations, 2023. 8 [5] Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Peng Gao, Shuai Ren, and Hongsheng Li. Amex: Android multi-annotation expo dataset for mobile gui agents. arXiv preprint arXiv:2407.17490, 2024. 9 [6] Jikai Chen, Long Chen, Dong Wang, Leilei Gan, Chenyi Zhuang, and Jinjie Gu. V2P: From background suppression to center peaking for robust GUI grounding task, 2025. 6, 8 [7] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2024. 7, 8 [8] Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, Yuan Yao, Yankai Lin, Zhiyuan Liu, and Maosong Sun. GUICourse: From general vision language model to versatile GUI agent. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2193621959, 2025. 8 [9] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual GUI agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 93139332, 2024. 8, [10] Tri Dao, Daniel Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems, 2022. 7, 9 [11] Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, Hengxu Wang, Luowei Zhou, and Mike Zheng Shou. AssistGUI: Task-oriented desktop graphical user interface automation. arXiv preprint arXiv:2312.13108, 2023. 8 [12] Google. Introducing gemini 2.0. Available at: https://blog.google/technology/google-deepmind/googlegemini-ai-update-december-2024, 2024. 6, [13] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for GUI agents. In The Thirteenth International Conference on Learning Representations, 2025. 6, 8, 9 [14] Jie Huang, Xuejing Liu, Sibo Song, Ruibing Hou, Hong Chang, Junyang Lin, and Shuai Bai. Revisiting multimodal positional encoding in vision-language models, 2025. 3 [15] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional highresolution computer use. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 87788786, 2025. 6, 7, 9 [16] Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on ui control agents. Advances in Neural Information Processing Systems, 37:92130 92154, 2024. 9 [17] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Stan Weixian Lei, Lijuan Wang, and Mike Zheng Shou. ShowUI: One vision-language-action model for GUI visual agent. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 19498 19508, 2025. 4, 6, 8 [18] Jizhihui Liu, Feiyi Du, Guangdao Zhu, Niu Lian, Jun Li, and Bin Chen. HiPrune: Training-free visual token pruning via hierarchical attention in vision-language models, 2025. 7, 8 [19] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based GUI agent. arXiv preprint arXiv:2408.00203, 2024. 6, 9 [20] Lingchen Meng, Jianwei Yang, Rui Tian, Xiyang Dai, Zuxuan Wu, Jianfeng Gao, and Yu-Gang Jiang. DeepStack: Deeply stacking visual tokens is surprisingly simple and effective for LMMs. In Advances in Neural Information Processing Systems, 2024. [21] Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan Rodriguez, Montek Kalsi, Nicolas Chapados, Tamer Ozsu, Aishwarya Agrawal, David Vazquez, Christopher Pal, et al. UI-Vision: desktop-centric GUI benchmark for visual perception and interaction. In Forty-second International Conference on Machine Learning, 2025. 6, 7, 9, 10 [22] OpenAI. Computer-using agent. Available https://openai.com/index/computer-using-agent, 6, 8 at: 2025. [23] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. UI-TARS: Pioneering automated GUI interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. 6, 8 13 [34] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, and Dahua Lin. PyramidDrop: Accelerating your large vision-language models via pyramid visual redundancy reduction, 2025. 8 [35] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous GUI interaction. arXiv preprint arXiv:2412.04454, 2024. 6, 8 [36] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1979219802, 2025. 7, 8 [37] Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-UI: Visual grounding for GUI instructions. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2241822433, 2025. 8 [38] Bofei Zhang, Zirui Shang, Zhi Gao, Wang Zhang, Rui Xie, Xiaojian Ma, Tao Yuan, Xinxiao Wu, Song-Chun Zhu, and Qing Li. TongUI: Building generalized GUI agents by learning from multimodal web tutorials. arXiv preprint arXiv:2504.12679, 2025. 6, [39] Ce Zhang, Kaixin Ma, Tianqing Fang, Wenhao Yu, Hongming Zhang, Zhisong Zhang, Yaqi Xie, Katia Sycara, Haitao Mi, and Dong Yu. VScan: Rethinking visual token reduction for efficient large vision-language models, 2025. 8 [40] Qizhe Zhang, Aosong Cheng, Ming Lu, Zhiyong Zhuo, MinQi Wang, Jiajun Cao, Shaobo Guo, Qi She, and Shanghang Zhang. [cls] attention is all you need for training-free visual token pruning: Make vlm inference faster. arXiv preprint arXiv:2412.01818, 2024. 8 [41] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis A. Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, and Shanghang Zhang. SparseVLM: Visual token sparsification for efficient visionlanguage model inference. In International Conference on Machine Learning, 2025. 8 [42] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. 8 [24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763, 2021. 7, 8 [25] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020. 6, [26] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 3 [27] Fei Tang, Yongliang Shen, Hang Zhang, Siqi Chen, Guiyang Hou, Wenqi Zhang, Wenqiao Zhang, Kaitao Song, Weiming Lu, and Yueting Zhuang. Think twice, click once: Enhancing GUI grounding via fast and slow systems. arXiv preprint arXiv:2503.06470, 2025. 8 [28] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3, 8 [29] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingfaces transformers: State-of-the-art natural language processing, 2020. 7 [30] Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, et al. GUI-Actor: Coordinate-free visual grounding for GUI agents. arXiv preprint arXiv:2506.03143, 2025. 5, 6, 8, 9 [31] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. OS-ATLAS: Foundation action model for generalist GUI agents. In The Thirteenth International Conference on Learning Representations, 2024. 6, 7, 8, [32] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. 10 [33] Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, and Caiming Xiong. Scaling computer-use grounding via user interface decomposition and synthesis, 2025. 6, 7, 8, 9,"
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "University of Oxford"
    ]
}