{
    "paper_title": "Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities",
    "authors": [
        "Zhiwei Hao",
        "Jianyuan Guo",
        "Li Shen",
        "Yong Luo",
        "Han Hu",
        "Guoxia Wang",
        "Dianhai Yu",
        "Yonggang Wen",
        "Dacheng Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several components$\\unicode{x2013}$such as weights, activations, and gradients$\\unicode{x2013}$each of which can be represented in different numerical formats. The resulting diversity has created a fragmented landscape in low-precision training research, making it difficult for researchers to gain a unified overview of the field. This survey provides a comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is a key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. A collection of papers discussed in this survey is provided in https://github.com/Hao840/Awesome-Low-Precision-Training."
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities Zhiwei Hao, Jianyuan Guo, Li Shen, Yong Luo, Han Hu, Guoxia Wang, Dianhai Yu, Yonggang Wen, Dacheng Tao 5 2 0 M 2 ] . [ 1 3 4 0 1 0 . 5 0 5 2 : r AbstractLarge language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several componentssuch as weights, activations, and gradientseach of which can be represented in different numerical formats. The resulting diversity has created fragmented landscape in lowprecision training research, making it difficult for researchers to gain unified overview of the field. This survey provides comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. collection of papers discussed in this survey is provided in Awesome-LowPrecision-Training. Index Termslarge language models; low-precision training; quantization I. INTRODUCTION ARGE Language Models (LLMs) have emerged as foundational technology in modern artificial intelligence, driving breakthroughs in natural language processing, code generation, and multimodal reasoning [1][4]. Their capacity to model complex patterns across massive datasets has enabled diverse array of applications, ranging from conversational Zhiwei Hao and Han Hu are with the School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China Email: haozhw@bit.edu.cn; hhu@bit.edu.cn Jianyuan Guo of Computer Science, City University of Hong Kong, Hong Kong 999077, China. Email: jianyuan guo@outlook.com the Department is with Li Shen is with the School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen 518107, China. Email: shenli6@mail.sysu.edu.cn Yong Luo is with the School of Computer Science, Wuhan University, Wuhan 430072, China. Email: luoyong@whu.edu.cn Guoxia Wang and Dianhai Yu are with the Baidu Inc., Beijing 100000, China. Email: wangguoxia@baidu.com; yudianhai@baidu.com Yonggang Wen and Dacheng Tao are with the College of Computing and Data Science, Nanyang Technological University, 639798, Singapore. Email: ygwen@ntu.edu.sg; dacheng.tao@gmail.com Correspondence author: Li Shen and Han Hu agents to tools that accelerate scientific discovery. Despite their transformative potential, training LLMs remains prohibitively expensive, demanding extensive computational resources and incurring significant energy costs [5]. For instance, training GPT-3 is estimated to require approximately 355 GPU years and $4.6 million, raising critical concerns about both scalability and environmental sustainability. To address these challenges, low-precision training has emerged as promising solution. By reducing the numerical precision of weights, gradients, and activations during trainingsuch as switching from 32-bit floating-point (FP32) precision to 16-bit (FP16) or even 8-bit formatsresearchers can significantly lower memory usage, communication overhead, and computational costs, while maintaining competitive model performance. Low-precision training offers several compelling advantages over traditional full-precision approaches, making it particularly attractive for large-scale or resource-constrained model training. First, reducing numerical precision directly decreases the memory footprint of model parameters, gradients, and intermediate activations. For instance, switching from FP32 to FP16 halves memory usage, enabling the training of larger models or the use of larger batch sizes within existing hardware limits. This benefit is especially important for memory-intensive tasks like LLM training, where memory often becomes the limiting factor. Second, modern hardware accelerators are increasingly optimized for low-precision arithmetic, offering substantially higher throughput for 16-bit or 8-bit operations. For example, NVIDIA Tensor Cores provide significant speedups for low-precision matrix multiplications. Moreover, these operations consume less energy, as they require fewer transistor switches, enhancing both performance and energy efficiency. Third, in distributed training environments, low-precision training reduces the volume of data exchanged between nodes, mitigating communication bottlenecks. Techniques such as gradient quantization and weight compression can lower communication costs with minimal accuracy degradation. Collectively, these advantages enable faster iteration cycles, reduced infrastructure costs, and the practical training of state-of-the-art models on more accessible hardwarean increasingly critical capability in the era of LLMs. In parallel, GPU architectures have evolved to better support low-precision computation. Table summarizes the progression of supported numerical formats across NVIDIA gener00000000/00$00.00 2021 IEEE JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 TABLE COMPARISON OF SUPPORTED PRECISIONS ACROSS NVIDIA GPUS. GPU P4 V100 A100 B100 Architecture Pascal Volta Ampere Hopper Blackwell CUDA Core Tensor Core FP32 FP16 BF16 FP8 FP6/FP FP32 FP16 BF16 FP8 FP6/FP4 ations. Early architectures like Pascal support only FP32 in CUDA cores, lacking hardware acceleration for lower precisions. Beginning with Volta, FP16 is supported in both CUDA and Tensor Cores, enabling more efficient low-precision computation. The Ampere architecture extended support to BF16 in both core types, providing greater flexibility for mixedprecision training. The Hopper architecture introduced native FP8 support in Tensor Cores, marking major milestone for ultra-low-precision training. The latest Blackwell architecture continues this progression, extending Tensor Core support to FP6 and FP4. This hardware evolution reflects clear trend toward enabling scalable and efficient low-precision computation, crucial for training increasingly large and complex models. Despite its promise, the field currently lacks comprehensive overview that synthesizes existing low-precision training techniques, highlights open challenges, and identifies promising research directions. Existing approaches are fragmented across different number representations (e.g., fixed-point, floating-point, and custom formats), targeted components (e.g., weights, activations, gradients), and training scenarios (e.g., single-node versus distributed training). This fragmentation makes it difficult to track overall progress and hinders unified advancement in the field. While previous surveys on efficient LLMs touch on related topics, they differ significantly from our focus. For instance, the study by Duan et al. [6] primarily focuses on distributed training optimizations across computation, communication, and memory, while the work by Gong et al. [7] centers on inference-time quantization. In contrast, low-precision training encompasses unique challenges such as handling low-precision gradients and optimizer states that do not arise in inferenceonly contexts. In this survey, we focus on low-precision training of models and provide comprehensive, structured review of recent advances in the field. We organize the literature primarily based on number representation formats, as this dimension plays central role in shaping hardware design, determining computational performance, and offering clear, intuitive framework for readers to navigate. In addition, we also cover Quantization-Aware Training (QAT) techniques and systemlow-precision training. level solutions that enable efficient Specifically, the topics covered in this survey are organized as follows: Fixed-point and integer-based methods. Fixed-point and integer representations are the most widely adopted numerical formats in low-precision training. Early works primarily relied on fixed-point quantization with global scaling factors. More recent studies have shifted toward integer quantization with finer-grained scalers to better handle outliers and improve precision. Floating-point-based methods. Floating-point formats represent another mainstream approach in low-precision training. While they share similar quantization process with fixed-point methods, recent advances in hardware support have led to increased interest in leveraging lowprecision floating-point numbers, enabling both flexibility and improved performance. Customized format-based methods. Beyond standard numerical formats, various customized representations have been proposed to further optimize performance. These formats are often derived from standard types. Quantization-aware training methods. Unlike general low-precision training, which applies quantization to weights, activations, and gradients, QAT focuses primarily on simulating quantization effects during the forward pass. Some methods rely on fake quantization to mimic quantization noise without improving training efficiency, while others incorporate real quantization, offering partial gains in training efficiency. System support. The practical adoption of low-precision training heavily depends on system-level support. This section discusses infrastructures that enable or accelerate low-precision training workflows. We also identify key open challenges and highlight promising directions for future research. Overall, this survey aims to offer clear and comprehensive understanding of the field, laying the groundwork for more efficient and scalable training practices in the era of large language models. Figure 1 compares the papers reviewed in this survey with different low-precision representations, revealing clear upward trend in integerbased methods over the past decade, alongside recent rise in floating-point methods. In contrast, fixed-point representations have experienced steady decline, while customized formats have consistently garnered minimal interest. The main contributions of this survey are as follows: We provide detailed classification of low-precision training techniques, organized by numerical formats, integer, floating-point, and cusincluding fixed-point, tomized representations. For each, we highlight their design principles, advantages, and trade-offs. To the best of our knowledge, this is the first survey dedicated to the low-precision training of neural networks. We examine QAT techniques that closely resemble lowprecision training and provide an overview of typical works that apply QAT in the context of LLM training. Moreover, system supports that enable efficient lowprecision training are also introduced. We outline key challenges in the field and discuss promising directions for future research aimed at developing JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3 Fig. 1. Annual count of reviewed papers in this survey from 2015 to 2024, categorized by primary adopted numerical format. more efficient and environmentally sustainable training practices for large models. This survey is organized as follows. As shown in Figure 2, Section II provides background on low-precision training, including numerical representation formats and affected components during training. Section III discusses techniques based on fixed-point and integer formats, while Section IV covers floating-point approaches. Section presents customized lowprecision formats. Section VI discusses QAT methods, and Section VII introduces system-level support strategies. Section VIII outlines open problems and research opportunities, and Section IX concludes the survey. i r i e - Basic Knowledge ( II) Low-Precision Fixed-Point and Integer Training ( III) Low-Precision Floating-Point Training ( IV) Custom Numerical Formats ( V) Quantization-Aware Training Techniques ( VI) System Support ( VII) Reviewed Works Future Directions ( VIII) Conclusion ( IX) Fig. 2. Overview of the survey structure and key components. II. BASIC KNOWLEDGE Before investigating the detailed literature on low-precision training, we first provide basic knowledge in this field. In this section, we begin by introducing different numerical representation formats, which serve as the categorization principle for our survey. Figure 3 provides visualization of commonly used fixed-point, integer, and floating-point numbers. Then, we discuss various components of the training process that can be optimized using low-precision techniques. Fig. 3. Visualization of different number representation formats. A. Number Representation Formats Fixed-Point and Integer Formats. Fixed-point representation is numerical encoding method that expresses fractional numbers by assigning fixed number of bits to their integer and fractional components. When no bits are allocated to the fractional part, the format simplifies to conventional integer representation. signed fixed-point scalar with total bit-width of and binary representation = (a0, a1, . . . , aB1) {0, 1}B is given by Sakr et al. [8]: (cid:32) = a0 + (cid:33) 2iai , B1 (cid:88) i=1 (1) where denotes the predefined dynamic range of a. To simplify hardware implementation, is often restricted to constant power of two. When = 2B1, the expression reduces to the standard twos-complement integer representation: = 2B1a0 + B1 (cid:88) i= 2B1iai. (2) In this case, the fractional component vanishes, and the dynamic range scales the result to interval [2B1, 2B1 1]. Fixed-point arithmetic is widely used in embedded systems and digital signal processing due to its hardware efficiency, as it eliminates the need for floating-point units. However, its static precision necessitates careful design to prevent overflow or underflow errors. Floating-Point Formats. Modern computing systems predominantly rely on floating-point representations for numerical computations due to their ability to efficiently handle wide dynamic range and precision. floating-point number typically comprises three components: the sign bit s, which determines the sign of the number; the exponent e, which controls the dynamic range; and the mantissa m, which governs the precision. The general form of floating-point number is given by: = (1)s 2eb, (3) JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4 Fig. 4. Generalized model training pipeline, showcasing components that can be optimized with low-precision methods. where denotes the exponent bias. The IEEE 754 standard [9] defines several widely used formats, such as FP32 single precision (E8M23)1 and FP16 half precision (E5M10). To meet the increasing demands of deep learning workloads, newer specialized formats have been including BF16 (E8M7), FP8 (E4M3, E5M2), introduced, which offer favorable trade-offs between precision, range, and computational efficiency. In pursuit of even more compact representations, emerging hardware platforms are beginning to support ultra-low-precision formats. For instance, NVIDIA Blackwell GPU architecture introduces support for FP6 (E2M3, E3M2) and FP4 (E2M1), enabling further acceleration of inference workloads while reducing memory bandwidth and power consumption. Customized Formats. Beyond standard fixed-point and floating-point representations, there also exist customized numerical formats specifically designed for deep learning applications, such as Microscaling [10]. These specialized formats will be introduced in context as we discuss the corresponding works in Section V. B. Different Low-Precision Components Several components of the training process can be optimized using low-precision arithmetic. To better illustrate this, we abstract the model training dynamics as follows: θt = θt1 η st, st = S(st1, gt), gt = θL(fθ(x), y), (State update) (4) (Gradient computation) where θt denotes the model parameters at iteration t, η is the learning rate, and st represents internal state of the optimizer, such as momentum or adaptive estimates in methods like Adam [11]. The function defines the update rule for the optimizer. The gradient gt is computed with respect to loss function over mini-batch of training data (x, y). This abstraction is also visualized in Figure 4. 1FP32 denotes 32-bit floating-point number, with 8 bits allocated for the exponent and 23 bits for the mantissa m. Prior work has primarily focused on applying low-precision optimization to one or more components of the training process. straightforward approach involves quantizing the model weights θt, activations used in the forward pass fθ(x), and gradients gt computed during backpropagation. In addition, some studies aim to further reduce the memory footprint by representing the optimizer state st in low precision. III. LOW-PRECISION FIXED-POINT AND INTEGER TRAINING In this section, we present low-precision training techniques based on fixed-point and integer quantization. It is worth noting that earlier works often used the terms fixed-point quantization and integer quantization interchangeably. For conceptual clarity, this survey distinguishes between the two by refeeing to method as fixed-point quantization or integer quantization depending on whether the scaling factor is data-dependent. Early studies predominantly employed fixedpoint quantization, where predefined, constant is used. For B-bit representation with fractional bits (k < B), the quantized value of an input is computed as: = clip (cid:16) round (cid:17) (cid:16) , 2B1, 2B1 (cid:17) , = 2k. (5) While fixed-point quantization offers advantages in terms of computational speed and memory efficiency compared to floating-point representations, its limited numerical range can result in overflow or underflow, potentially degrading model accuracy. As modern processors and specialized accelerators are increasingly optimized for integer operations, recent research efforts have shifted toward integer quantization techniques for low-precision training. Integer quantization typically employs symmetric uniform quantization [91], which the scaling enhances hardware efficiency. In this scheme, factor is data-dependent. Given an input range (l, u), the JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 5 Fixed-Point Training n T e d n - i Integer Training Binary Training Ordinary Fixed-Point: Gupta et al. [12], Xia et al. [13], FxpNet [14], Sakr et al. [8], Chakrabarti et al. [15], SWALP [16], MuPPET [17], QFX [18] Dynamic Fixed-Point: Courbariaux et al. [19], Jo et al. [20], Das et al. [21] Overall Design: DoReFa-Net [22], WAGE [23], WAGEUBN [24], Zhang et al. [25], DQT [26] Quantization Methods: Miyashita et al. [27], HALP [28], Zhao et al. [29], In-hindsight [30], Q-GaLore [31], Xi et al. [32], HDQT [33], HLP [34] Fine-grained Quantization: Zhang et al. [35], NITI [36], AMPA [37], Shen et al. [38], FracTrain [39], Jetfire [40] Quantization Errors: Banner et al. [41], Park et al. [42], Zhu et al. [43], Guo et al. [44] Dynamic Precision: FracTrain [39], CPT [45] Normalization Layers: Banner et al. [41], WAGEUBN [24], Yang et al. [46] Alternative Objectives: Fei et al. [47], Jetfire [40] Extended Applications: Yang et al. [46], SwitchBack [48] Activation Compression: Park et al. [49], ActNN [50], GACT [51], Novikov et al. [52], EXACT [53], Eliassen et al. [54] Theory: Banner et al. [41], Li et al. [55], Li et al. [56], Zhu et al. [43], ZipML [57], Chen et al. [58] General methods Optimizer-targeted methods Adam: DALL-E [59], Dettmers et al. [60], Chitsaz et al. [61], MicroAdam [62], Li et al. [63] Other Optimizers: QFT [64], Li et al. [65], Wang et al. [66] Communication-targeted methods DDP: Dettmers et al. [67], QSGD [68], DoubleSqueeze [69], LoCo [70], Chen et al. [71], Efficient-Adam [72], AsyLPG [73], Faghri et al. [74], THC [75], ZeRO++ [76] Non-DDP: AQ-SGD [77], SDP4Bit [78], QSDP [79] Binary Neural Networks: BinaryConnect [80], XNOR-Net [81], Wang et al. [82], 1-Bit FQT [83] Communication Speedup: Seide et al. [84], EF-SGD [85], 1-bit Adam [86], 0/1 Adam [87], 1-bit LAMB [88], Wu et al. [89], Birder [90] Fig. 5. Overview of studies on training with low-precision fixed-point and integer formats. quantized value is computed as: = round (cid:18) clip(x, c, c) = 2B1 1 = max(l, u). , (cid:19) , Gupta et al. [12]. Their work primarily investigates rounding schemes and identifies that the conventional nearest rounding approach causes gradient information loss as small values are consistently rounded to zero. To address this issue, the authors propose stochastic rounding, mathematically expressed as: (6) In both quantization formats, the original input is approximated by reconstructing the value ˆx as: ˆx = . (7) During training, the forward and backward propagation processes in low-precision training largely mirror those in fullprecision training, with the key distinction being the reduced numerical precision of values. notable exception is 1bit quantized training, where the Straight-Through Estimator (STE) is commonly used during backpropagation. This technique bypasses the non-differentiability of the quantization function by approximating gradients as if the operation had preserved full precision. Although heuristic, this workaround enables effective gradient flow while maintaining significant computational efficiency. In line with technological advancements, we begin by analyzing fixed-point training methods, followed by discussion of integer-based training approaches. Finally, we examine the extreme case of 1-bit quantized training. The structure of this section is outlined in Figure 5. A. Fixed-Point Training Ordinary fixed-point. One early attempt to utilize fixedpoint formats for deep neural network training is introduced by R(x) = (cid:40) with probability p(x), x + δ with probability 1 p(x), (8) δ where the probability function p(x) is defined as 1 xx . This formulation represents an unbiased rounding scheme, satisfying E(R(x)) = x. The stochastic rounding method preserves gradient information by maintaining non-zero probability for small parameter updates. Their experiments demonstrate that models trained with 16-bit fixed-point numbers using this approach exhibit minimal performance degradation. Xia et al. [13] later enhance this rounding method specifically for binary classification tasks. They propose using constant probability p(x) = 0.5 for all values within the interval [x, + δ). This modification not only preserves more gradient information but also offers potential advantages for hardware implementation. Empirical results using 16-bit fixed-point numbers show that their technique achieves faster convergence rates and higher classification accuracy compared to models trained with the original stochastic rounding scheme. Subsequently, Chen et al. [14] identify key limitation in prior work under the binarized neural network framework, where primal parameters are preserved and updated with high precision. To overcome this challenge, they propose FxpNet, in which both primal parameters and gradients are represented using adaptive fixed-point formats. In FxpNet, during the forward pass, both fixed-point primal weights and activations are JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6 binarized prior to computation. In the backward pass, gradients are also quantized to low-precision fixed-point values before being accumulated into their corresponding fixed-point primal parameters. To accurately capture the variations in parameters and gradients, the authors propose an adaptive scaling factor update method. This approach dynamically adjusts the scaling factor for distinct variable groups across different layers by comparing the overflow rate with predefined threshold. Experimental results show that FxpNet, using 12-bit fixedpoint representations for both primal parameters and gradients, achieves accuracy comparable to state-of-the-art floating-point counterparts. Sakr et al. [8] further tackle the challenge of enabling true fixed-point training for DNNs by proposing an SGD algorithm in which all parameters and computations are performed using fixed-point arithmetic with minimal precision. Their method ensures both training convergence and model accuracy, while addressing several critical challenges, such as managing quantization noise, balancing precision trade-offs both within and across layers, handling dynamic range constraints, and maintaining overall training stability. The central objective of their work is to determine minimal or near-minimal precision configuration that ensures the probability of prediction mismatch between the fixed-point and floating-point networks remains bounded. This bounded mismatch probability, in turn, guarantees convergence behavior similar to that of the floatingpoint baseline. To this end, the authors propose five key criteria including equal noise contribution for fixed-point to label mismatch, use of clipped gradients, minimization of quantization bias, control of activation gradient noise, and sufficient precision in weight accumulators. Using this precision assignment framework, the authors demonstrate substantial gains in training efficiency. Compared to floatingpoint baselines, their approach achieves up to 6 reduction in representational cost, 8 in computational cost, and 4 in communication cost. training, To reduce memory usage during training for convolutional models, Chakrabarti et al. [15] focus on activation compression. Specifically, in the forward pass, exact activations are layers. Howused to compute the outputs of subsequent ever, instead of storing these full-precision activations, the method retains compressed, approximate copy with lower memory footprint for use during the backward pass. In their experiments, the authors employ fixed-point quantization to compress activations. Results demonstrate that this approach achieves accuracy comparable to full-precision training, while enabling compact storage of activations using as little as 4-bit precision. Motivated by the intuitive idea that averaging operations can effectively mitigate quantization noise arising from stochastic rounding, Yang et al. [16] propose an approach to enhance low-precision training by averaging the iterates generated by low-precision SGD while incorporating an adjusted learning rate schedule. The implementation quantizes all numerical values to 8-bit fixed-point numbers, including the gradient accumulators used in the optimization process. Theoretical analysis demonstrates that the proposed method, termed SWALP, achieves convergence to the optimal solution for quadratic objectives. Rajagopal et al. [17] advance the field of quantised training through their proposed multilevel optimisation framework called MuPPET. This method integrates multiple low precision fixed-point representation schemes with dynamic precision switching mechanism, which autonomously determines the transition points between different precision levels during runtime. Throughout the training process, the quantisation level progressively increases in gradual manner. Experimental evaluations conducted on various computer vision models show that MuPPET maintains comparable accuracy to conventional full-precision training while delivering significant computational efficiency improvements. In recent work, Dai et al. [18] introduce QFX, framework that automatically learns optimal binary point positions during neural network training, facilitating the efficient deployment of quantized models on FPGA hardware platforms. This approach also incorporates multiplier-free quantization strategy, which is specifically designed to reduce the utilization of digital signal processors. Unlike conventional approaches that finetune pre-trained models for fixed-point FPGA deployment at the risk of accuracy degradation, QFX employs fixed-point QAT and demonstrates superior accuracy compared to standard quantization methods while maintaining hardware efficiency. Dynamic fixed-point. variant known as dynamic fixedpoint numbers [92] employs multiple adjustable scaling factors rather than single global scaling factor. This approach represents middle ground between floating-point and fixedpoint number formats. Courbariaux et al. [19] demonstrate the effectiveness of this dynamic fixed-point format for training deep neural networks in their pioneering work. Building upon this foundation, Jo et al. [20] introduce additional techniques including weight clipping and progressive batch size scaling when training networks with 16-bit dynamic fixedpoint numbers. Their method further reduces the test accuracy degradation. Recognizing that most published results focus either on outdated network architectures such as AlexNet [93] for ImageNet-1K [94] classification, or restrict their evaluation to smaller benchmark datasets like CIFAR-10 [95], Das et al. [21] advance this research direction by successfully training state-of-the-art visual recognition models with dynamic fixedpoint numbers on general purpose hardware. Their implementation encompasses four modern CNN architectures including ResNet-50 [96], GoogLeNet-v1 [97], VGG-16 [98], and AlexNet, evaluated on the full ImageNet-1K dataset, where all networks maintain or surpass the original floatingpoint accuracy while preserving identical hyperparameters and training iteration counts. Unlike integer quantization, which typically requires storing one or more data-dependent floating-point scaling factors, fixed-point representation encodes numerical values directly using fixed number of fractional bits, thereby eliminating the need for explicit scaling factor storage. This characteristic makes fixed-point quantization particularly appealing for hardware implementations such as FPGAs, where simplicity and resource efficiency are critical. However, the adoption of globally shared scaling factor across entire tensors also limits adaptability of fixed-point quantization. Consequently, it may introduce higher quantization errors compared to more flexible JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 schemessuch as per-tensor or per-channel integer quantizationwhere scaling factors can be individually adjusted to better match the data distribution and reduce precision loss. B. Integer Training Most existing low-precision training methods primarily employ integer formats, with research efforts focusing on different aspects of the training pipeline. To provide structured overview, we categorize integer training approaches into three primary types: General integer training, which applies quantization to weights, activations, and gradients throughout the training process; Optimizer-targeted integer training, which focuses on quantizing optimizer states such as momentum to reduce memory consumption; Communication-targeted integer training, which aims to enhance the efficiency of distributed training by quantizing the weights or gradients exchanged between devices. 1) General integer training: Overall design. One of the pioneering works in the field of integer training is DoReFaNet [22]. In addition to using low-bitwidth weights and activations, it adopts stochastic quantization for parameter gradients during backpropagation. This enables the use of bitlevel convolution kernels for both training and inference across CPUs, FPGAs, and GPUs. As an early attempt, DoReFa-Net paves the way for accelerating the training of low-bitwidth neural networks. Building upon this, WAGE [23] establishes framework that discretizes both training and inference by quantizing weights, activations, gradients, and errors to lowbitwidth integers. Yang et al. [24] further quantizes optimizer momentum terms, achieving Fully Quantized Training (FQT) framework. In parallel, Zhang et al. [25] address the issue of sparse outlier activations using dynamic block fallback quantization. This method allocates higher bit-widths to outliers when they are detected within specific quantization blocks. Furthermore, Zhao et al. [26] reduce the bit-width to as low as 3, demonstrating the feasibility of ternary weight training. Quantization methods. Some studies improve integer quantized training by redesigning quantization schemes to address specific challenges in low-precision optimization. Miyashita et al. [27] observe that conventional linear quantization struggles with the non-uniform distributions of weights and activations. To address this, they replace fixed-point representation with base-2 logarithmic quantization, which more effectively captures large dynamic ranges using fewer bits. This method demonstrates superior classification accuracy compared to linear quantization. HALP [28] tackles the fundamental trade-off between bit-width reduction and quantization noise. It combines Stochastic Variance Reduced Gradient (SVRG) with dynamic re-centering and re-scaling technique tailored for low-precision numbers. HALP preserves the convergence rate of full-precision SGD despite the presence of quantization noise. Building upon gradient quantization research, Zhao et al. [29] identify previously overlooked phenomenon, where layer-wise gradients often exhibit multiple distributions along the channel dimensions. To address this, they introduce gradient vectorized quantization, along with magnitude-aware clipping strategy that minimizes quantization error weighted by gradient magnitudes. The quantization parameters are optimized for each distribution through theoretical derivations. The dynamic nature of gradients poses challenges for fixed quantization ranges. In-hindsight [30] reveals that conventional dynamic quantization incurs substantial memory overhead. Their hardware-friendly alternative employs historical quantization ranges updated via moving averages to quantize current tensors. This method leverages accumulator statistics for online range updates, eliminating the need for real-time range computation while maintaining accuracy. Additionally, Q-GaLore [31] exploits the resilience of gradient projection matrices to low-bit quantization, enabling INT4 quantization of projections while preserving INT8 weights. Stochastic rounding captures gradient accumulation effects, supporting memory-efficient pretraining. Notably, QGaLore enables training LLaMA-7B model from scratch with only 16 GB of GPU memory. To handle outliers in activation matrices during low-bit quantization, the Hadamard transform [99] offers an effective solution by distributing these outliers across all dimensions, making them easier to process. For example, if one dimension contains very large value while others remain small, applying the transform produces more balanced vector in which all values are of similar magnitude. This transformation improves the suitability of the data for efficient quantization. Building on this foundation, Xi et al. [32] introduce Hadamard quantizer to suppress activation outliers during forward propagation and exploit the structural sparsity of activation gradients in backpropagation via bit splitting. In this approach, the gradient of each token is separated into higher and lower 4-bit components to prioritize the computation of larger gradients. Subsequently, Schiemer et al. [33] implement backward-pass matrix multiplications in the Hadamard domain for continual learning scenarios. This enables training with 4-bit integer inputs while maintaining 8-bit accumulators to enhance numerical stability. Kim et al. [34] develop Hadamard low-rank quantization to optimize the computational cost of backpropagation. Through sensitivity analysis of gradient computations, they design pipeline that applies 4-bit Hadamard quantization to activation gradients, while combining Hadamard transforms with lowrank approximation techniques for weight gradients. Fine-grained quantization. While traditional approaches often use uniform quantization with global scaling factor, emerging research shows that applying layer-specific or blockspecific scaling strategies better accommodates varying data distributions across network components while maintaining model accuracy. Early work by Zhang et al. [35] establishes key observations about layer-wise data characteristics, such as significant inter-layer distribution variance, dynamic range shifts during training, and the correlation between variance magnitude and required bit-width. Building on these insights, the authors propose precision-adaptive quantization method that stabilizes data distributions through layer-wise bitwidth adjustment. This approach achieves significant training speedup with negligible accuracy loss and no need for hyperJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 parameter modifications. NITI [36] introduces per-layer block exponentiation for dynamic range adaptation, enabling discrete parameter updates for 8-bit integer storage throughout training. It also employs hardware-efficient pseudo-stochastic rounding using intermediate accumulation bits as random sources. By integrating these techniques with an integer-optimized cross-entropy backpropagation approximation, NITI achieves full integer training with minimal computational overhead. AMPA [37] proposes layer-wise sensitivity measurements for weights, activations, and gradients, dynamically adjusting bitwidths during training and achieving an average precision lower than INT8. Other complementary approaches explore alternative adaptation strategies. Shen et al. [38] propose capacity-aware method that directly assigns decreasing bitwidths to layers with large capacity, eliminating the need for exhaustive sensitivity analysis while maintaining stability across trials. Meanwhile, FracTrain [39] automatically adjusted layer precisions based on input characteristics, and Jetfire [40] demonstrates that per-block quantization could effectively preserve accuracy. Quantization errors. Several studies focus on mitigating quantization-induced errors through both theoretical and empirical approaches. Through theoretical analysis, Banner et al. [41] show that most training operations are robust to substantial quantization, with only specific gradient computations requiring higher precision. Specifically, they bifurcate the layer gradients, using 16-bit precision for computing the weight gradient, while keeping 8-bit precision for computing the nextlayer gradient. The work by Park et al. [42] introduces an auxiliary parameter to retain partial accumulations of small gradient values, thus addressing precision shortage issues in parameter updates. They also propose simple guideline for selecting the appropriate bit-width for the final fully connected layer followed by softmax nonlinearity. The study of Zhu et al. [43] identifies four distinct gradient distribution characteristicssharp and wide, evolutionary, depth-specific, and structure-specificthat contribute to greater quantization error. Their solution combines deviation-counteractive learning rate scaling with cosine-distance-based gradient clipping method. Guo et al. [44] find that proper channel grouping significantly reduces quantization errors. Based on this insight, they propose ShiftQuant, which mitigates gradient quantization errors through intelligent channel grouping. Dynamic precision. Some studies explore dynamic precision adjustment during training to balance computational efficiency with model accuracy. FracTrain [39] gradually increases the precision of activations, weights, and gradients, reaching standard low-precision levels only in the final stages of training. Their approach also includes automatic adaptation of layer-wise precision. The findings suggest that using lower precision in the early stages of training, followed by higher precision during final convergence, achieves an optimal tradeoff between exploratory learning and final model accuracy. CPT [45] extends this idea by introducing cyclic precision schedule, where precision levels vary periodically throughout training. The optimal bounds for these cycles are determined in the early training phase using simple precision range test. Normalization layers. Batch normalization (BN) [100] layers, which are an important component in early CNN architectures, require high-precision computation to avoid issues such as zero variance and large dynamic range. To mitigate this drawback, some works adopt full-precision computation for BN layers [22], while others remove BN layers entirely from the model [101]. Banner et al. [41] introduces Range BN, which shows significantly higher tolerance to quantization noise. WAGEUBN [24] replaces each BN layer with constant scaling factor. Guo et al. [44] develops L1 normalization layers, which are mathematically equivalent to L2-norm batch normalization but demonstrate stronger linearity. Meanwhile, Yang et al. [46] quantizes L1 normalization layers, enabling them to run on integer-based hardware, which typically lacks support for square root operations. Alternative objectives. Advances in quantization techniques extend beyond conventional targets such as weights, activations, and gradients to address more specialized challenges in low-precision training. notable gap in this area is the handling of full-precision latent weights during integerized training, as identified by Fei et al. [47]. These latent weights consume substantial memory to accumulate gradient updates for optimizing discrete parameters. To address this, the authors introduce residual quantization to suppress noise in latent weights, along with dual quantizer that employs optimal nonuniform codebooks to minimize training perturbations. Beyond weight representation, the computational pipeline of quantization methods presents another major challenge. As Xi et al. [40] observe, most quantized training methods adopt quantize-compute-dequantize paradigm, which proves particularly inefficient for transformer architectures. This frequent data conversion results in significant memory access overhead and degrades the performance of low-precision computation. To address these issues, the authors propose Jetfire, an INT8 training framework featuring redesigned linear and nonlinear operators to support direct INT8 data flow. In addition, they employ per-block quantization to preserve model accuracy. Experiments show that Jetfire achieves accuracy comparable to FP16 training baselines and outperforms existing INT8 training approaches for transformers. Extended applications. Some works extend integer training beyond classification tasks. Yang et al. [46] propose new quantization framework for the training and inference of segmentation networks, where both parameters and operations are constrained to 8-bit integer-based values. Wortsman et al. [48] accelerate the training of vision-language models (VLMs) by using 16-bit precision for weight gradients, while employing 8-bit integers for the forward pass and input gradients. Additionally, it adopted AdamW along with the update clipping technique introduced in AdaFactor [102], which tracks the average ratio of the squared gradients to the second moment estimator and reduces the learning rate when this ratio becomes large. Activation compression. While gradient compression techniques are well-studied, specialized line of research focuses on compressing full-precision stored activations specifically for backward passes to reduce memory usage during training, while preserving forward pass precision to maintain model accuracy. These approaches are typically referred to JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9 as activation-compressed training (ACT). Based on the observation that activation values are concentrated in narrow ranges with sparse outliers, Park et al. [49] apply reduced precision only to the dense regions, thereby minimizing quantization errors for the majority of data while handling outliers separately in high precision. This method achieves 3-bit quantization of activations. Subsequent works identify several fundamental limitations in ACT, including unclear convergence behavior and non-specific quantization designs [50], and architecturespecific restrictions [51]. The framework ActNN [50] addressed these issues by introducing adaptive quantization with theoretical guarantees. ActNN adopts per-group quantization with dynamic bit allocation, choosing the numerical precision adaptively for each sample and layer. Group range and zero points remain in BF16, and the method minimizes overall variance within given bit-budget by allocating more bits to sensitive layers. Building on these insights, Liu et al. [51] propose GACT, which extends ACT to diverse model architectures through runtime compression ratio adaptation. GACT estimates the impact of each tensor on the gradient and adjusts the compression ratio accordingly. Beyond tensor quantization, Novikov et al. [52] tackle memory overheads from nonlinearity operations by approximating the derivative of the activation function as piecewise-constant functions. This approximation allows the replacement of full-precision activation storage with bin indices. In addition to conventional model architectures, solutions for graph neural networks (GNNs) also emerge. Liu et al. [53] introduce EXACT, the first framework optimized for GPU support of GNN-specific activation compression. Eliassen et al. [54] further improve efficiency through block-wise quantization, demonstrating significant reductions in memory consumption. Theory. While many empirical studies exist, researchers also seek to establish convergence guarantees for integer quantized training. Theoretical analyses reveal an intrinsic robustness in neural network training towards precision reduction. Banner et al. [41] demonstrate that most training operations can tolerate substantial quantization, with only few components requiring higher precision. This finding is further supported by Li et al. [55]. The authors identify critical distinction between convex and non-convex regimes. While convex problems allow quantized training with accuracy guarantees, non-convex landscapes require high-precision representations for effective greedy search phases. Regarding convergence bounds for quantized training, earlier work suffers from dimension-dependent bounds, as noted by Li et al. [56], where the required bit-widths scaled with model dimensionality d. Their contribution establishes dimensionindependent bounds and extends the analysis to diverse quantization schemes. For specific integer quantization, Zhu et al. [43] provides convergence bounds for INT8 training, while Zhang et al. [57] achieve 16 precision reduction in linear models with rigorous guarantees. Moving beyond worst-case analyses, Chen et al. [58] introduce statistical framework for FQT. key theoretical contribution of this work proves that FQT gradients are unbiased estimators of QAT gradients, the same implying that FQT and QAT algorithms exhibit convergence behavior as the learning rate approaches zero. The study further addresses practical challenges by proposing two novel gradient quantizers that handle dynamic range variation and optimize dimension-wise signal distribution, enabling 5bit gradient encoding in ResNet50 without any accuracy degradation. 2) Optimizer-targeted integer training: While quantized training alleviates memory pressure during both forward and backward passes, the optimizer states remain significant bottleneck. With the rapid advancement of deep learning, stateful optimization methods become the de facto standard for training neural networks. Unlike traditional stateless optimizers such as SGD, modern adaptive optimizers like Adam [11] utilize historical gradient statistics to dynamically adjust learning rates for each parameter. Although these methods significantly improve convergence and generalization, they introduce substantial memory overhead by maintaining auxiliary variables such as first-order and second-order statistics. These statistics incur additional memory costs proportional to the number of trainable parameters, leading to memory is 23 times larger than that of footprint for Adam that this overhead becomes SGD. In the context of LLMs, critical bottleneck. Consequently, there is growing interest in developing efficient optimization techniques that compress optimizer states. information before it Adam. Early work by Ramesh et al. [59] demonstrates the feasibility of stable training using 16-bit Adam moments. Building upon this, Dettmers et al. [60] pioneers the use of 8bit statistics through block-wise quantization, in which tensors are divided into smaller, independently quantized blocks. Their approach combines dynamic quantization with stabilized embedding layer to mitigate gradient variance arising from the non-uniform token distributions in language models. An empirical study by Chitsaz et al. [61] systematically evaluates various quantization strategies, revealing that while the first-order moments in Adam tolerate 4-bit quantization, the second-order moments require more careful handling, even at 8-bit precision. These findings inform their recommended quantization recipe for pretraining. Further advancing the field, Modoranu et al. [62] propose MicroAdam, which comis incorporated into presses gradient optimizer states. By employing novel variant of error correction [84], and compressing the error feedback itself, their method preserves convergence guarantees while significantly reducing memory consumption. Pushing the boundaries of low-bit optimization, Li et al. [63] achieve 4-bit quantization of optimizer states through two key innovations. First, they introduce adaptive block sizing to effectively handle moment outliers across parameter tensors. Second, they apply linear quantizer that excludes problematic zero-point values in the second-order moments. Additionally, they incorporate rank-1 normalization, where the scaling factor of each tensor element is determined by the minimum value of the row or column it resides in, allowing for more accurate outlier approximation. Other optimizers. While Adam remains the dominant optimizer in model training, emerging research explores state compression for more advanced optimizers such as Lion [103] and Shampoo [104]. [64] propose QFT, quantized training framework that employs Lion as the optimizer. Lion tracks JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 only momentum and maintains consistent update magnitudes for each parameter, enabling robust integer-based storage, of all optimizer states. In their framework, both gradients and momentum undergo uniform quantization, while weights use Dense-and-Sparse Quantizer with specialized gradient flow mechanism for quantized weight updates. Second-order optimizers like Shampoo pose greater memory challenges due to their use of preconditioning matrices. Several approaches address this constraint through 4-bit quantization. Li et al. [65] tackle the quantization of nondiagonal preconditioners using Cholesky decomposition and quantization of Cholesky factors with error compensation. Their innovations include 4-bit exponential moving average (EMA) error estimator and an efficient matrix packing scheme that stores both quantized factors and error states in single triangular matrix. Theoretical analysis confirms convergence for both smooth and nonsmooth optimization. Meanwhile, Wang et al. [66] identify the eigenvector matrix as more quantization-friendly target than the preconditioner itself, thereby avoiding distortion of small singular values during inverse root calculations. Their method enhances precision via Bjorck orthonormalization and shows that linear square quantization yields better results for second-order optimizer states. 3) Communication-targeted integer training: As models continue to grow in scale and complexity, training them efficiently on large datasets has become increasingly dependent on distributed computation across multiple processors and machines. widely adopted strategy for this purpose is Distributed Data Parallel (DDP), where each node processes subset of data and synchronizes model updates across all the scalability of such systems is often nodes. However, hampered by communication bottlenecks, especially when full-precision gradients and activations need to be exchanged frequently between nodes. To address this issue, range of gradient compression techniques have been proposed, aiming to reduce communication overhead while preserving convergence guarantees and training accuracy. DDP. One early effort in this direction explores the use of 8-bit approximations for compressing both gradients and activations [67], achieving up to 2 speedup in data transfer compared to standard 32-bit approaches without compromising predictive performance. Quantized SGD (QSGD) [68] extends this idea by introducing family of lossy gradient quantization schemes that balance communication efficiency and convergence guarantees, enabling users to control the trade-off between gradient precision and iteration variance. It provides theoretical support for convergence under both convex and non-convex objectives. Some works develop error compensation mechanisms. The importance of proper error handling is emphasized by Li et al. [75], who show that naive bidirectional compression schemes incur significant computational overhead and accuracy degradation. To handle this, Tang et al. [69] propose DoubleSqueeze, which implements bidirectional compression between workers and parameter servers while maintaining theoretical convergence guarantees. Their analysis show that error-compensated approaches exhibit better tolerance to compression noise compared to naive quantization. Similarly, Xie et al. [70] introduce LoCo with refined error-feedback mechanism using moving averages of past compression errors, demonstrating improved training stability. In addition, Chen et al. [71] integrates both gradient quantization and weight quantization within the parameter-server model. To address the bias introduced by quantization, the authors propose an error-feedback technique and theoretically establish convergence to first-order stationary points in stochastic nonconvex settings. Efficient-Adam [72] further enhances communication efficiency by introducing novel two-way quantization scheme combined with tailored error-feedback strategy. Recent advances have introduced more sophisticated quantization strategies. Yu et al. [73] develope double quantization framework that compresses both parameters and gradients while maintaining the original convergence rate through careful variance analysis. They further combine quantization with sparsification, establishing relationships between sparsity budgets and convergence. Faghri et al. [74] take an adaptive approach, proposing ALQ and AMQ schemes that dynamically adjust to changing gradient statistics during training, where ALQ leverages estimated gradient distributions for optimization, while AMQ utilizes exponentially spaced levels to achieve similar objectives. At the system level, Tensor Homomorphic Compression (THC) [75] introduces preprocessing approach using randomized hadamard transform to enable direct aggregation of compressed values, eliminating decompression overhead. complementing this, Wang et al. [76] present ZeRO++, which integrates block quantization into the Zero Redundancy Optimizer framework, particularly focusing on optimizing allgather operations through weight quantization. Non-DDP. Recent research has extended communication optimization beyond traditional data-parallel settings to accommodate broader range of parallel training paradigms. While gradient compression has been extensively explored in data-parallel training, activation compression in pipelineparallel scenarios remains relatively under-investigated. Addressing this gap, Wang et al. [77] introduce AQ-SGD, novel approach that compresses activation changes rather than their absolute values. This approach demonstrates theoretical convergence guarantees for non-convex optimization while significantly reducing communication costs during pipeline parallel training. In the context of Sharded Data Parallelism (ShardedDP), Jia et al. [78] propose SDP4Bit with two key innovations. First, it compresses weight updates by quantizing temporal differences between weights, achieving 4-bit representation. Second, it introduces two-level gradient quantization framework that uses higher precision (8-bit) for intra-node communication and lower precision (4-bit) for inter-node synchronization. The design is further enhanced with Hadamard Transform to manage gradient outliers. For large-scale model training, Fully Sharded Data Parallelism (FSDP) has gained traction due to its memory and compute efficiency. Building on this, Markov et al. [79] develop QSDP, quantized variant of FSDP. Building on layer-wise parameter gathering in FSDP, QSDP introduces JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 11 quantization before all-to-all communications, where gradients are compressed using standard unbiased compressors, while weights employ novel unbiased estimator. C. Binary Training Binary neural network. As extreme case of integer training, Binary neural networks (BNNs) have emerged as promising approach for efficient deep learning, particularly in resource-constrained scenarios. By quantizing weights and activations to 1, BNNs achieve substantial computational savings by replacing multiply-accumulate operations with simpler additions and subtractions. The foundation of modern BNN methods is established by BinaryConnect [80], [105]. This technique maintains fullprecision weights during gradient accumulation while using binary values for both forward and backward propagation. It not only reduces computational complexity but also serves as an effective form of regularization. The binarization process can be implemented either deterministically using the sign function: = sign(x) = (cid:40) +1 1 if 0, otherwise, or stochastically as follows: (cid:40) = +1 with probability σ(x), 1 with probability 1 σ(x), (9) (10) where σ(x) denotes the computationally efficient hard sigmoid function [80], [105]. Building on this foundation, Rastegari et al. [81] introduce two notable convolutional architectures: Binary-WeightNetworks and XNOR-Networks. The former achieves 32 reduction in memory usage by binarizing convolutional filters. The latter extends this approach by also binarizing layer inputs, enabling up to 58 acceleration in convolutional operations via efficient binary approximations. traditional Despite these advancements, training BNNs continued to present memory-related challenges. As highlighted by Wang et al. [82], training methods require storing highprecision activations for all layers, which restricte their deployment on memory-limited devices. To address this, the authors demonstrate that backpropagation is robust to quantization, enabling memory-efficient training strategy that stores activations solely in binary format without incurring significant accuracy degradation. The theoretical understanding of binary training is further deepened by Gao et al. [83] by conducting convergence analysis for 1-bit FQT. Their study show that Adam outperforms SGD in low-bitwidth settings due to its reduced sensitivity to gradient variance. Based on these insights, they introduce two key techniques: Activation Gradient Pruning (AGP), which reduces variance through selective group quantization, and Sample Channel Joint Quantization (SCQ), hardware-friendly method for efficient gradient computation. Communication speedup. 1-bit gradient compression has also emerged as promising solution for reducing bandwidth overhead in distributed deep learning. Early work by Seide et al. [84] demonstrate that 1-bit gradient quantization, combined with error feedback by accumulating quantization residuals into subsequent mini-batches can achieve accuracy nearly equivalent to full-precision SGD while significantly lowering communication costs. This principle is generalized by EFSGD [85], which establishes convergence guarantees at the same rate as uncompressed SGD for arbitrary compression operators. However, these approaches are primarily designed for SGD-based optimizers, leaving gap in applicability to adaptive methods such as Adam or LAMB [106]. To address this limitation, 1-bit Adam [86] introduces twostage strategy: an initial warm-up phase using standard Adam to stabilize gradient variance, followed by error-compensated 1-bit compression of momentum while freezing the variance preconditioner. Similarly, Lu et al. [87] propose 0/1 Adam, offering provable convergence guarantees under 1-bit quantization constraints. Extending these ideas to large-batch training, 1-bit LAMB [88] adopts similar warm-up strategy with LAMB before switching to compressed momentum SGD. Despite these advances, practical challenges remain in distributed environments. Notably, Wu et al. [89] observe that multi-hop all-reduce architectures suffer from cascading compression errors. To counter this, they propose the Marsit framework, which employs unbiased sign aggregation and global compensation to preserve convergence rates in such settings. To further eliminate practical bottlenecks such as warm-up requirements and the computational overhead of quantization, Birder [90] provides solution that natively integrates 1-bit quantization with adaptive updates. It removes the need for full-precision warm-up and theoretically matches the convergence speed of Adam. IV. LOW-PRECISION FLOATING-POINT TRAINING In this section, we present low-precision training techniques based on floating-point quantization. Unlike integer quantization, converting high-precision floating-point number to lower-precision one involves different process. Specifically, for converting high-precision floating-point value (EeMm) to lower-precision representation (EeMm), We begin by copying the lower exponent bits from the source to the target. The mantissa is then truncated to bits by rounding to the nearest value. To better preserve information during quantization, scaling factor is typically applied to the source value prior to conversion. If an overflow occurs, the result is clipped directly to the maximum or minimum representable value. In the case of underflow, the value is divided by the smallest subnormal number in the low-precision format, rounded to the nearest integer, and then multiplied back by the same smallest subnormal number. Unlike binarized training, low-precision floating-point training does not require specialized backward propagation techniques. Instead, it simply mirrors the process used in full-precision training, with the key difference being the reduced numerical precision. Following the order from higher to lower precision, we begin by introducing widely used 16-bit floating-point training techniques. We then explore emerging lower-precision approaches that utilize 8-bit and even 4-bit floating-point JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12 n T o - t F 16-Bit Floating-Point Training FP16: Micikevicius et al. [107], Zhao et al. [108], Campo [109] BF16: Kalamkar et al. [110] Sub-8-Bit Floating-Point Training Overall Design: HFP8 [111], Noune et al. [112], Lee et al. [113], APS [114], Desrentes et al. [115], Lutz et al. [116], Luo et al. [117], Radix-4 FP4 [118], Wang et al. [119], Zhou et al. [120], Fishman et al. [121], Micikevicius et al. [122], FP8-LM [123], DeepSeek-V3 [3] Value Scaling: Mellempudi et al. [124], GradScale [118], S2FP8 [125], Chmiel et al. [126], Blake et al. [127], FP8-LM [123], Perez et al. [128], Scalify [129] Result Accumulation: Mellempudi et al. [124], Wang et al. [130], Sakr et al. [131], Ali et al. [132] Optimizer States: FP8-LM [123], Fishman et al. [121], COAT [133] Block Floating-Point: HBFP [134], Accuracy Booster [135] Fig. 6. Overview of studies on training with low-precision floating-point formats. formats. The overall structure of this section is illustrated in Figure 6. A. 16-Bit Floating-Point Training The adoption of 16-bit floating-point formats for deep learning training has gained widespread popularity, driven by their computational efficiency and memory savings. Modern deep learning frameworks support these formats extensively, with FP16 and BF16 emerging as the most prominent choices. Early work by Micikevicius et al. [107] pioneer FP16 training by addressing its limited dynamic range through three key techniques. First, an FP32 master copy of weights is maintained to ensure accurate gradient updates. Second, loss scaling is applied to prevent gradient underflow. Third, FP32 accumulation is used for partial products in arithmetic operations. Together, these strategies enable FP16 training to achieve accuracy comparable to FP32 across variety of architectures. However, subsequent study by Zhao et al. [108] observe that uniform loss scaling fails to adequately handle variations in gradient distributions across layers, often necessitating heuristic tuning. To address this, the authors propose gradient scaling method that dynamically adjusts per-tensor scales during backpropagation. This method ensures that the estimated underflow rate of the scaled gradients remains below predefined threshold while avoiding overflow, thus improving training stability. Beyond numerical stability, the performance implications of precision conversion have also been examined. He et al. [109] find that format casting overhead can negate the benefits of FP16, accounting for more than 21% of execution time in certain scenarios. To mitigate this, the authors introduce Campo, cost-aware graph rewriting framework that selectively applies FP16 only to operations where it yields net speedup. By incorporating performance modeling, Campo minimizes casting overhead and maximizes training efficiency. Meanwhile, BF16 has emerged as robust alternative with inherent advantages for training stability. As demonstrated by Kalamkar et al. [110], BF16 offers dynamic range equivalent to FP32, eliminating the need for hyperparameter tuning and simplifying precision conversion. Empirical results confirme that BF16 can achieve state-of-the-art convergence parity with FP32 across various domains. As result, BF16 has gradually replaced FP16 as the default format for 16-bit training. Nevertheless, with the advancement of modern GPUs, researchers are now exploring even more efficient training methods that leverage floating-point formats with lower precisions. B. Sub-8-Bit Floating-Point Training Overall design. The adoption of lower numerical precision for model training presents significant challenges due to the dramatically reduced representation range. Early attempts at FP8 training show substantial accuracy degradation in popular architectures such as MobileNet and Transformers, primarily due to differing precision requirements between the forward and backward passes [111]. To address this, Sun et al. [111] propose hybrid format approach, using E4M3 for the forward pass and E5M2 for the backward pass. This method enables successful training across various tasks without accuracy loss. Noune et al. [112] conduct systematic empirical study of 8bit formats, identifying optimal exponent and mantissa configurations that preserve accuracy while enhancing training speed and power efficiency across multiple domains. Lee et al. [113] investigate the robustness of reduced-precision training for LLMs, highlighting the instability of current FP8 methods and proposing new evaluation techniques and sharpness-based metric to assess training stability under varying precision levels. Their analysis aims to guide the development of more reliable and cost-effective low-precision training schemes. In terms of optimizing communication in distributed training, Han et al. [114] propose Auto Precision Scaling (APS), method that enables accurate and efficient distributed deep learning by communicating gradients in 8-bit floating-point values, achieving minimal or no accuracy loss and significant speedups. In the field of hardward design, Desrentes et al. [115] propose architectures for exact dot product accumulate operators tailored to 8-bit floating-point formats, enabling precise accumulation by expanding products to fixed-point and rounding from wide accumulators. Lutz et al. [116] present two novel microarchitectures for fused FP8 DOT4 operations with single rounding, targeting efficient GEMM in ML workloads by accumulating to FP32 with dynamic range scaling. Their designslate accumulation and early accumulationoptimize power and area efficiency. Additionally, HiFloat8 Format for Ascend architectures has also been studied by Luo et al. [117]. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13 Research in low-precision floating-point training progressively advances toward more aggressive quantization strategies. Sun et al. [118] pioneer 4-bit training using Radix-4 FP4 formats (E3M0) combined with specialized rounding schemes. These schemes use nearest rounding and select the midpoint between two neighboring exponent levels as the rounding threshold. Gradient scaling techniques are also employed. Since each output activation gradient is used twice during backpropagation, the authors quantize it to either the even or odd phase to compute both the input activation gradient and the weight activation gradient, mitigating expected quantization error through cancellation. Furthermore, they analyze the quantization bias effects on batch normalization, finding that aggressive quantization induces internal covariate shifts, leading to generalization issues. Wang et al. [119] extend this direction by introducing differentiable quantization estimators for weights. They derive function with correction terms for accurate gradient estimation together with outlier compensation strategies for activations, including clamping method and sparse auxiliary matrix, which help preserve quantization accuracy and maintain model performance. Their framework specifically targets LLMs. Recognizing the varying sensitivity across network components in transformer-based architectures, Zhou et al. [120] develop mixed-precision techniques. They adopt FP4 for most operations while retaining FP8 precision for QKV computations and output projections. Fishman et al. [121] identify prolonged training instabilities in FP8 implementations and attribute these to activation function behaviors. They propose Smooth-SwiGLU, which enables stable training of models with up to 7B parameters. At scale, Micikevicius et al. [122] empirically validate that FP8 training achieves accuracy comparable to FP16 and bfloat16 across model sizes up to 175B parameters, without requiring hyperparameter adjustments. The practical deployment of these methods is demonstrated by FP8-LM [123], which incorporates automatic tensor-wise scaling and supports distributed parallel training. This results in 39% memory reduction and 75% training speedup compared to BF16 when training model with 175B parameters. Recent advances in low-precision training have achieved significant breakthroughs with the introduction of DeepSeekV3 [3], which represents the first successful application of FP8 training in industrial-scale scenarios. This innovative approach incorporates several key technical contributions that collectively enable stable and efficient FP8 training. At the core of this methodology lies fine-grained quantization techniques. Specifically, activations are processed in 1128 tiles and weights in 128128 blocks, with online calculation of maximum absolute values for each block. Due to the fine-grained quantization scheme general matrix multiplication (GEMM) operations are executed entirely in FP8 precision using the E4M3 format for both forward and backward passes. Memory efficiency is further enhanced through careful management of cached activations stored in FP8 for backward computation, with specialized designs for specific components. Specifically, the Linear operator inputs following attention use customized E5M6 format, while SwiGLU operator inputs in MoE layers are cached in FP8 and recomputed during backward passes. To optimize communication in MoE architectures, the system quantizes activations before MoE up-projections and activation gradients before down-projections into FP8. Notably, certain critical components retain higher precision, including the embedding module, output head, MoE gating modules, normalization operators, and attention operators. The framework maintains master weights in FP32, weight gradients in FP32, and optimizer states in BF16, ensuring numerical stability while benefiting from reduced precision where applicable. Value scaling. Prior to the emergence of group scaling in DeepSeek-v3, researchers extensively investigate various scaling strategies for low-precision floating-point training. Early approaches primarily focus on loss scaling techniques. Mellempudi et al. [124] proposes an adaptive method that dynamically adjusts the scaling factor update frequency by monitoring loss progression, effectively compensating for the reduced subnormal range in 8-bit floating-point formats. Building on this, GradScale [118] introduces trainable per-layer gradient scaling, which learns optimal scaling during training. Cambier et al. [125] presents S2FP8, an innovative 8-bit format that employs shifted and squeezed factors to rescale tensor ranges before truncation, thereby eliminating the need loss scaling tuning. Following this, Chmiel et for manual al. [126] explores per-layer gradient scaling and identifies optimal values that enable successful quantization of gradients using 6-bit floating-point formats. Blake et al. [127] introduce unit scaling, method that enables stable low-precision training by ensuring unit variance across weights, activations, and gradients at initialization, eliminating the need for scale tuning or added computational cost. Recent research shifts toward automation and system-level optimization. FP8-LM [123] proposes global scaling strategy that coordinates tensor-wise scaling factors across GPUs using single shared scalar, streamlining distributed training. Perez et al. [128] develops dynamic per-tensor scaling methodology for linear layers, validated on LLMs with up to 70 billion parameters. Scalify [129] propagates scaling information throughout computational graphs. This framework unifies FP8 and FP16 techniques under an automated paradigm through specialized operations and propagation rules. Result accumulation. While low-precision representations for weights and activations show promise, maintaining gradient fidelity during backpropagation remains key challenge, particularly for accumulations in partial product computations and weight updates. Mellempudi et al. [124] reduces the precision of the master weight copy from 32-bit to 16-bit without compromising model performance. Wang et al. [130] demonstrates successful end-to-end 8-bit floating-point training through two key innovations: chunk-based accumulation, which decomposes long dot products into intra-chunk partial sums followed by inter-chunk aggregation, and floating-point stochastic rounding, which helps preserve gradient fidelity. Subsequent research shifts toward establishing theoretical foundations for precision requirements. Sakr et al. [131] develops statistical model that correlates accumulation length with minimum bit-width by analyzing variance preservation in ensembles of partial sums. This provides principled guidance JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 14 m l r N s Posit: Gustafson et al. [136], Dinechin et al. [137], Lu et al. [138] posit number consists of sign bit, dynamically sized regime bits, and optional exponent and fraction bits. Flexpoint: Koster et al. [139] Flexpoint uses tensors with dynamically adjusted shared exponent, balancing precision and dynamic range. FloatSD: Lin et al. [140] FloatSD reduces convolution operations to the addition of two shifted partial products by limiting non-zero digits to two during training. MLS: Zhong et al. [141] MLS uses element-wise and group scaling to optimize dynamic range and simplify integer accumulation. Microscaling: Rouhani et al. [10], Tseng et al. [142], Chen et al. [143] The MX format combines per-block scaling factors (typically in E8M0) with low-precision element-wise values (e.g., FP8, INT8) to create efficient variants like MXFP8 or MXINT8. Fig. 7. Overview of studies employing custom numerical formats for low-precision training. for allocating precision in low-bit training. More recently, Ali et al. [132] designs dedicated FP8 multiply-accumulate (MAC) unit that combines stochastic rounding with optimized 12-bit accumulations, striking balance between computational efficiency and numerical accuracy. Optimizer states. While recent frameworks accelerate training using low-precision floating-point numbers, they often overlook optimizer state compression, leaving substantial memory savings unrealized. To address this gap, FP8LM [123] proposes an FP8 mixed-precision optimization scheme that reduces memory consumption from 16 bytes to 6 bytes per parameter. This is achieved by allocating 2 bytes for master weights, 1 byte each for gradients and first-order states, and 2 bytes for second-order states. Fishman et al. [121] specifically targets moment tensors, introducing dedicated FP8 formats (E4M3 for first moment, E5M2 for second moment) to ensure numerical stability during quantization. However, naive FP8 quantization of optimizer states leads to under-utilization of the representation range, resulting in suboptimal compression. COAT [133] addresses this limitation through two key innovations. First, it applies dynamic range expansion to optimizer states, using an expand function to align state distributions with the dynamic range of the E4M3 format prior to quantization. Second, it employs mixedgranularity activation quantization, combining per-tensor and per-group strategies to achieve additional memory savings. This co-design of optimizer and activation compression enables significantly greater memory reduction. Block floating-point. Based on the standard floating-point format, block floating point (BFP) emerges as an alternative that shares exponents across tensor blocks. This design preserves wide dynamic range while enabling efficient fixedpoint logic for multiply-and-accumulate operations. Building on this concept, Drumond et al. [134] proposes Hybrid BFP (HBFP), an approach that uses BFP for dot products while retaining floating-point arithmetic for other operations. Extending this work, Accuracy Booster [135] investigates the precision requirements of HBFP and demonstrates that 6-bit mantissas are sufficient to achieve FP32level accuracy when applied consistently across all layers and training epochs. V. CUSTOM NUMERICAL FORMATS Besides commonly used fixed-point and floating-point representations, some works propose customized numerical formats for low-precision training. It is important to note that certain customized formats, such as NormalFloat [144], are specifically designed for pretrained fixed parameters, which only participate in the forward inference stage during training. As such, we exclude these works from consideration, as they do not directly contribute to low-precision training. Figure 6 presents an overview of this section. Posit. Posit [136] is an alternative to traditional floatingpoint representations, offering hardware-friendly fixed-bit operations along with several advantages such as enhanced dynamic range, improved accuracy, and bitwise reproducibility across computing platforms. Posit employs dynamic segmentation structure consisting of sign, regime, exponent, and fraction bits, where only the necessary components are encoded, effectively avoiding overflow and underflow issues. Dinechin et al. [137] systematically evaluates the trade-offs between posit and floating-point implementations, suggesting that using posit as storage format may offer an optimal balance for general-purpose computing by preserving the strengths of both numerical systems. Lu et al. [138] demonstrates the effectiveness of low-bit posit quantization in model training, showing that 8-bit posit representations can achieve accuracy comparable to higher-precision floating-point formats when combined with tensor-wise scaling scheme. Flexpoint. Flexpoint [139] combines the advantages of fixed-point and floating-point arithmetic by introducing tensors with dynamically adjusted shared exponent, which minimizes overflows while maximizing the available dynamic range. To ensure numerical stability, Flexpoint dynamically estimates the exponent value based on the historical maximum values of the tensor, thereby preventing overflow while maintaining precision. key benefit of Flexpoint is its efficiency in hardware implementations. Compared to 32-bit floating point, it reduces memory and bandwidth requirements by amortizing the exponent storage and communication across the entire tensor. FloatSD. floating-point signed digit (FloatSD) format [140] is designed for CNN weight representation. The key idea behind FloatSD is to reduce the number of non-zero digits in the JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15 weight representation. This reduction allows the convolution operation, typically involving multiplication, to be simplified to the addition of two shifted multiplicands. Besides the weight representation, the authors also quantize the mantissa and exponent fields of neuron activations and gradients during training. This leads to the use of 8-bit floating-point numbers for these parameters, further contributing to the reduction in computational complexity. MLS. Multi-level scaling (MLS) format [141] is designed to achieve an optimal balance between high representation capability and energy efficiency. The MLS format incorporates element-wise scaling to improve the dynamic range of the data, together with group scaling factor to reduce the bitwidth of the element-wise exponent, allowing the accumulation process to be simplified to integer accumulation with minimal overhead. Additionally, the authors integrate the MLS format into low-bit tensor convolution arithmetic unit to support our training framework efficiently, by which the MLS format can be manipulated effectively, providing significant improvement in both energy efficiency and computational performance during training. Microscaling. The Microscaling (MX) format [10] combines per-block scaling factors with low-precision floatingpoint or integer types for individual elements. typical MX block consists of vector of elements that share single scaling factor (typically represented in E8M0 format), along with scalar values stored in reduced-precision formats such as FP8, FP6, FP4, or INT8. This design yields corresponding MX variants like MXFP8 or MXINT8. Rouhani et al. [10] demonstrates the practical viability of MX formats as replacements for FP32 in both inference and training. Their comprehensive evaluation across range of benchmarks shows that even 4-bit MX formats can support the training of large transformer models with only marginal accuracy degradation. Building on this foundation, Tseng et al. [142] present the first near-lossless training recipe using MXFP4 GEMMs, achieving 2 speedup over FP8 on supported hardware. Their key innovation involves computing unbiased gradient estimates via stochastic rounding, combined with random Hadamard transform to reduce variance caused by block-level outliers. Further analysis by Chen et al. [143] identifies weight oscillation during the forward pass as the primary cause of accuracy degradation in MXFP4 training. To mitigate this issue, the authors propose two novel techniques: an EMA Quantizer (Q-EMA), which stabilizes rounding decisions by incorporating historical weight information through exponential moving averages, and an Adaptive Ramping Optimizer (QRamping), which dynamically adjusts the update frequency of oscillating weights. VI. QUANTIZATION-AWARE TRAINING TECHNIQUES In addition to low-precision training, the deployment of LLMs with low precision during inference has also been extensively studied. While low-precision training aims to reduce the precision of both the forward and backward passes during the training process, low-precision inference focuses solely on reducing the precision of weights and activations during q c A Binary and ternary QAT: BitNet [153], BitNet b1.58 [154], Nielsen et al. [155] QAT with KD: LLM-QAT [156], BitDistiller [157] Other QAT methods: EfficientQAT [158], QuEST [159], Pang et al. [160] Fig. 8. Overview of studies incorporating quantization-aware training techniques. the forward pass. This typically involves the application of quantization techniques. Works in this domain can be broadly divided into two categories: Post-Training Quantization (PTQ) and QAT. PTQ methods typically involve quantizing pre-trained model without any further training, relying on heuristics or optimization strategies to adapt the model to lower precision. In contrast, QAT incorporates quantization into the training process, allowing the model to learn to accommodate lower precision during both the forward pass and backpropagation. Although there is large body of work on PTQ techniques for LLMs [145] [152], these methods are beyond the scope of this survey, and thus will not be covered here. On the other hand, QAT methods share some similarities with low-precision training. In the remaining of this section, we provide brief introduction to key works in the QAT field. Structure of this section is presented in Figure 8. Binary and ternary QAT. QAT for LLMs has seen progressive advancements in extreme quantization, starting with the pioneering BitNet architecture [153]. As the first 1-bit Transformer for LLMs, BitNet employs binary weights and 8-bit activations while maintaining high-precision gradients during training. Its simple implementation, which replaces linear projections with signum-binarized weights and uses STE-based backpropagation, demonstrates the feasibility of extreme quantization. BitNet b1.58 [154] further introduce ternary parameters through absmean quantization, as follows: (cid:18) (cid:18) Wq = max 1, min 1, round (cid:18) (cid:19)(cid:19)(cid:19) γ + ϵ , (11) (cid:80) ij Wij where γ = 1 indicates absolute value of all nm elements in w. It retains the activation scheme from the original BitNet. Nielsen et al. [155] enhances this framework with progressive two-phase 16-to-1.58-bit training strategy. This approach combines high-precision pre-training followed by transition to lower-bit quantization, providing smoother shift for the model and helping it retain more of the knowledge acquired during full-precision training. QAT with KD. Several studies incorporate knowledge distillation [161] into the QAT process. LLM-QAT [156] adopts data-free distillation, which generates synthetic data from pretrained models to guide the 4-bit quantization of LLaMA architectures without the need for original training data. BitDistiller [157] leverages full-precision models as teachers and introduces tailored asymmetric quantization and clipping strategy to minimize error in sub-4-bit regimes. Other QAT methods. Additionally, several methods push the limits of quantization techniques. EfficientQAT [158] JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 16 TABLE II SUMMARY OF SOFTWARE FRAMEWORKS AND LIBRARIES FOR LOW-PRECISION TRAINING SUPPORT Framework/Library Key Low-Precision Features PyTorch TensorFlow JAX PaddlePaddle Native AMP, GradScaler Native Mixed Precision, Auto loss scaling Explicit precision control AMP with automatic operator casting (FP16 focus) Primary Focus / Notes General-purpose deep learning General-purpose deep learning General-purpose deep learning General-purpose deep learning DeepSpeed Megatron-LM/Core Enhanced AMP, ZeRO integration, Exp. 4/8-bit QAT, 1-bit optimizers Optimized kernels, FP8 support (Hopper), Parallelism APIs Large-scale model training (extends PyTorch) LLM training (PyTorch-based) bitsandbytes NVIDIA Apex Colossal-AI Transformer Engine 8-bit optimizers, 4/8-bit quantization functions Early AMP & distributed tools (now largely deprecated) Simplified AMP (FP16/BF16 planned), Exp. FP8 linear/communication FP8-optimized Transformer layers, Auto scaling factors, QAT support Lightweight CUDA wrappers, LLM memory reduction Historical PyTorch extension (use native PyTorch) Simplifying large model training Accelerating Transformer models (NVIDIA GPUs) addresses the memory challenges associated with traditional QAT methods. It employs two-stage training strategy. It first trains the model and quantization parameters block-by-block, followed by end-to-end training of the quantization parameters alone. QuEST [159] improves both accuracy and speed by introducing Hadamard normalization and MSE-optimal fitting, alongside novel gradient estimator called trust estimation, which ensures stable training at 1-bit precision. Theoretical insights from Pang et al. [160] reveal that QAT instability arises from loss landscape sharpness. To address this, they introduce Feature-Perturbed Quantization (FPQ), which smooths the loss landscape through implicit Hessian regularization. VII. SYSTEM SUPPORT FOR LOW-PRECISION TRAINING Low-precision training techniques offer significant benefits for model training, primarily by reducing memory footprint, decreasing memory bandwidth requirements, and potentially accelerating computations on compatible hardware. However, effectively leveraging these advantages requires sophisticated system-level support. Software frameworks and specialized libraries play pivotal role in managing the complexities of low-precision formats, such as ensuring numerical stability, performing efficient data type conversions, and optimizing hardware utilization. This section provides an overview of the software frameworks and libraries that offer essential system support for enabling and facilitating low-precision DNN training. Table II provides summary of introduced frameworks and libraries. Mainstream frameworks. Several widely adopted deep learning frameworks offer built-in capabilities for integrating low-precision techniques into standard training workflows. PyTorch [162] has become leading framework for low-precision training, praised for its flexible and intuitive design. It supports native Automatic Mixed Precision (AMP), which combines FP32 operations with lower-precision formats (typically FP16 or BF16) during training. This integration automates the casting of operations to the appropriate types and employs techniques like gradient scaling to maintain numerical stability with minimal changes to existing user code. TensorFlow [163] offers similar support for mixed-precision training, providing an easy-to-enable framework for using FP16 or BF16 precision in model training. It automatically handles loss scaling and performs optimizations to ensure efficient execution on hardware accelerators such as GPUs and TPUs. JAX [164], general-purpose library for high-performance numerical computation, offers fine-grained control over numerical precision. This flexibility allows researchers to specify the precision for individual operations or blocks of operations, making it valuable for detailed studies on the impact of precision on model behavior and convergence. PaddlePaddle [165] also integrates robust AMP support, allowing users to accelerate computation by casting operations to FP16, while ensuring that operations sensitive to precision remain in FP32 to preserve model accuracy. Frameworks for LLMs. Training extremely large models, particularly in the transformer family, often requires specialized frameworks that combine low-precision techniques with advanced memory optimization and distributed training strategies. DeepSpeed [166] extends capabilities of PyTorch for training large-scale models, incorporating advanced memory optimizations like ZeRO (Zero Redundancy Optimizer) [167] alongside highly optimized support for mixed-precision training. DeepSpeed also explores cutting-edge low-precision approaches, including support for 4-bit and 8-bit training through quantization-aware optimizers, and communication-efficient distributed optimizers that compress gradients during largescale training. Megatron-LM, initially research-oriented framework for LLM training on PyTorch, has evolved into Megatron-Core, modular and formally supported library. It includes optimizations like tensor, pipeline, and sequence parallelism for efficient it supports FP8 precision, taking advantage of modern hardware architectures like NVIDIA Hopper GPUs. low-precision training. Notably, Specialized libraries and extensions. Beyond core frameworks, growing ecosystem of specialized libraries provides targeted functionalities for low-precision operations, specific model architectures, or hardware acceleration. The bitsandbytes library offers lightweight Python wrapper around custom CUDA functions, focusing on memory-efficient representations and computations. It supports 8-bit optimizers [168] and 8-bit matrix multiplication kernels [169], useful for inference and quantization-aware training. NVIDIA Apex [170] has been an important tool for PyTorch users, particularly for mixed-precision and distributed training. Its was one of the first widely adopted solutions for automatic mixed precision. Colossal-AI [171] provides simplified interfaces for advanced JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 17 training techniques, including AMP, and aims to make large model training more accessible and efficient. It also includes experimental features for FP8 precision and FP8 communication compression to optimize network bandwidth usage in distributed training. The Transformer Engine, developed by NVIDIA, is an open-source library specifically designed to accelerate Transformer models on NVIDIA GPUs. It offers robust support for FP8 training, especially on NVIDIA Hopper architecture GPUs, and provides tools for managing scaling factors and maintaining numerical accuracy. VIII. FUTURE DIRECTIONS Despite impressive progress, low-precision training of foundation models still faces several open challenges. Addressing these issues is crucial for further scaling while preserving both performance and training stability. Below, we outline key challenges along with promising directions for future research. Advanced quantization methods. Linear quantization remains the dominant approach due to its simplicity and compatibility with existing hardware. However, to effectively capture the diverse statistical properties of weights, activations, and gradients. Future work should explore nonlinear quantization schemes, such as logarithmic quantization or learned quantization strategies, which can provide improved dynamic range coverage and better adapt to the distributional characteristics of different tensors. These methods hold promise for advancing low-precision training, particularly in regimes where linear quantization begins to break down. it may fail Ultra low-precision training. Most current research focuses on 8-bit precision, which has demonstrated strong performance but does not fully capitalize on the potential efficiency gains available at lower bit-widths. Pushing toward ultra low-precision training, such as 4-bit or even 2-bit, can significantly improve memory and computational efficiency. However, achieving competitive performance at these extreme levels requires deeper understanding of the theoretical underpinnings. Building stronger theoretical foundations on convergence behavior, generalization capacity, and robustness to quantization noise will be critical. Such theory-guided approaches can make ultra low-precision training both practical and reliable at scale. Fine-grained scaling strategies. As precision decreases, the distributions of weights, activations, and gradients tend to exhibit more outliers due to the reduced dynamic range. This can severely degrade training stability and accuracy. While existing methods such as per-channel or per-token scaling offer some mitigation, more fine-grained scaling schemes are needed to effectively handle outliers in ultra-low precision regimes. Adaptive or learnable scaling strategies could further enhance robustness. However, these finer-grained approaches introduce additional scaling factors, which in turn increase memory usage. Therefore, it is essential to carefully balance the trade-off between improved quantization granularity and memory efficiency. Optimizer state compression. While much of the focus in low-precision training has been on quantizing weights and activations, optimizer states remain in high precision and represent substantial memory bottleneck. This is particularly problematic in large-scale foundation models, where optimizer states often consume 23 the memory footprint of the model parameters themselves. Efficiently compressing or quantizing these states is thus critical. However, certain components, such as second-order statistics, are highly sensitive to reduced precision with less attentions have been paid on them. More research is needed to understand how to compress these elements without sacrificing performance. Additionally, the design of optimizers natively compatible with low-precision training presents promising direction. Unified training frameworks for low-precision training. notable gap remains in the availability of scalable, modular training frameworks that support ultra low-precision training throughout the entire pipeline. This lack of tooling hinders both reproducibility and broader adoption. Future work should focus on developing robust, hardware-aware toolkits and libraries specifically designed for low-bit training. Key features should include support for dynamic precision scheduling, customizable quantization schemes, and deployment simulation to facilitate practical use in diverse hardware environments. Standardized Benchmarks and Evaluation Protocols. The absence of consistent benchmarks and evaluation protocols makes it difficult to systematically compare different low-precision training methods across architectures and tasks. There is clear need to develop standardized benchmark suites that encompass wide range of models, datasets, and precision levels. Such comprehensive benchmarking will provide clearer picture of trade-offs and facilitate fair, reproducible comparisons. Extending to border architectures. Current research on low-precision training is predominantly focused on LLMs, while other important categories of foundation models remain relatively underexplored. Expanding this line of research to include VLMs, diffusion models, and speech transformers is promising direction. These architectures may exhibit different quantization sensitivities and training dynamics, presenting unique challenges and opportunities for innovation in lowprecision training methods. Integration with other efficient training paradigms. Most existing studies focus exclusively on quantization, overlooking the potential benefits of combining it with other efficiencyoriented techniques. Integrating low-precision training with complementary approaches such as pruning or low-rank approximation can further reduce compute, memory footprint, and hardware demands in synergistic manner. Future research should explore how these methods interact and how to jointly optimize them to maximize training efficiency without compromising model performance. IX. CONCLUSION This survey provides an in-depth summarization of existing low-precision training works for LLMs. We begin with preliminaries about different numerical formats, followed by different low-precision components during training and the benefits of low-precision training. Then, categorized by different numerical formats, we introduce existing works with JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 18 fixed-point and integer numbers, floating-point numbers, and customized formats. Following this, we briefly introduce QAT approaches, which share some similarities with low-precision training methods to some extent. Lastly, we provide several potential future research directions. Overall, low-precision training has emerged as critical technique for reducing the computational and memory costs of training LLMs without significantly compromising performance. As both hardware and algorithmic support continue to evolve, we anticipate low-precision training will become an integral part of mainstream LLM development. Future research may focus on advancing quantization techniques, improving training stability at ultra low precision, and developing unified frameworks that support diverse model architectures. Additionally, efforts toward standard benchmarks and integration with other efficient training paradigms will be crucial for broader adoption and fair evaluation. We hope this survey serves as valuable reference for researchers and practitioners working toward scalable and efficient training of nextgeneration LLMs."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. 1 [2] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang et al., Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, arXiv preprint arXiv:2403.05530, 2024. 1 [3] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan et al., Deepseek-v3 technical report, arXiv preprint arXiv:2412.19437, 2024. 1, 12, 13 [4] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei et al., Qwen2.5 technical report, arXiv preprint arXiv:2412.15115, 2024. 1 [5] L. Shen, Y. Sun, Z. Yu, L. Ding, X. Tian, and D. Tao, On efficient training of large-scale deep learning models, ACM Computing Surveys, vol. 57, no. 3, pp. 136, 2024. 1 [6] J. Duan, S. Zhang, Z. Wang, L. Jiang, W. Qu, Q. Hu, G. Wang, Q. Weng, H. Yan, X. Zhang et al., Efficient training of large language models on distributed infrastructures: survey, arXiv preprint arXiv:2407.20018, 2024. [7] R. Gong, Y. Ding, Z. Wang, C. Lv, X. Zheng, J. Du, H. Qin, J. Guo, M. Magno, and X. Liu, survey of low-bit large language models: Basics, systems, and algorithms, arXiv preprint arXiv:2409.16694, 2024. 2 [8] C. Sakr and N. R. Shanbhag, Per-tensor fixed-point quantization of the back-propagation algorithm, in International Conference on Learning Representations, 2019. 3, 5, 6 [9] Ieee standard for floating-point arithmetic, IEEE Std 754-2019 (Revision of IEEE 754-2008), pp. 184, 2019. 4 [10] B. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D. Choudhary, M. Cornea, E. Dellinger, K. Denolf et al., Microscaling data formats for deep learning, arXiv preprint arXiv:2310.10537, 2023. 4, 14, 15 [11] D. P. Kingma and J. Ba, Adam: method for stochastic optimization, in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2015. 4, [12] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, Deep learning with limited numerical precision, in International Conference on Machine Learning, F. R. Bach and D. M. Blei, Eds., 2015, pp. 17371746. 5 [13] L. Xia, M. Anthonissen, M. Hochstenbach, and B. Koren, simple and efficient stochastic rounding method for training neural networks in low precision, arXiv preprint arXiv:2103.13445, 2021. 5 [14] X. Chen, X. Hu, H. Zhou, and N. Xu, Fxpnet: Training deep convolutional neural network in fixed-point representation, in International Joint Conference on Neural Networks, 2017, pp. 24942501. 5 [15] A. Chakrabarti and B. Moseley, Backprop with approximate activations for memory-efficient network training, in Advances in Neural Information Processing Systems, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. dAlche-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 24262435. 5, 6 [16] G. Yang, T. Zhang, P. Kirichenko, J. Bai, A. G. Wilson, and C. D. Sa, SWALP : Stochastic weight averaging in low precision training, in International Conference on Machine Learning, K. Chaudhuri and R. Salakhutdinov, Eds., 2019, pp. 70157024. 5, 6 [17] A. Rajagopal, D. A. Vink, S. I. Venieris, and C. Bouganis, Multiprecision policy enforced training (muppet) : precision-switching strategy for quantised fixed-point training of cnns, in International Conference on Machine Learning, 2020, pp. 79437952. 5, [18] D. Dai, Y. Zhang, J. Zhang, Z. Hu, Y. Cai, Q. Sun, and Z. Zhang, Trainable fixed-point quantization for deep learning acceleration on fpgas, arXiv preprint arXiv:2401.17544, 2024. 5, 6 [19] M. Courbariaux, Y. Bengio, and J.-P. David, Training deep neural networks with low precision multiplications, arXiv preprint arXiv:1412.7024, 2014. 5, 6 [20] S. Jo, H. Park, G. Lee, and K. Choi, Training neural networks with low precision dynamic fixed-point, in International Conference on Computer Design, 2018, pp. 405408. 5, 6 [21] D. Das, N. Mellempudi, D. Mudigere, D. D. Kalamkar, S. Avancha, K. Banerjee, S. Sridharan, K. Vaidyanathan, B. Kaul, E. Georganas, A. Heinecke, P. Dubey, J. Corbal, N. Shustrov, R. Dubtsov, E. Fomenko, and V. O. Pirogov, Mixed precision training of convolutional neural networks using integer operations, in International Conference on Learning Representations, 2018. 5, 6 [22] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou, Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients, arXiv preprint arXiv:1606.06160, 2016. 5, 7, 8 [23] S. Wu, G. Li, F. Chen, and L. Shi, Training and inference with integers in deep neural networks, in International Conference on Learning Representations, 2018. 5, [24] Y. Yang, L. Deng, S. Wu, T. Yan, Y. Xie, and G. Li, Training high-performance and large-scale deep neural networks with full 8-bit integers, Neural Networks, vol. 125, pp. 7082, 2020. 5, 7, 8 [25] P. Zhang, J. Wei, J. Zhang, J. Zhu, and J. Chen, Accurate int8 training through dynamic block-level fallback, arXiv preprint arXiv:2503.08040, 2025. 5, 7 [26] K. Zhao, T. Tabaru, K. Kobayashi, T. Honda, M. Yamazaki, and Y. Tsuruoka, Direct quantized training of language models with stochastic rounding, arXiv preprint arXiv:2412.04787, 2024. 5, 7 [27] D. Miyashita, E. H. Lee, and B. Murmann, Convolutional neural networks using logarithmic data representation, arXiv preprint arXiv:1603.01025, 2016. 5, 7 [28] C. De Sa, M. Leszczynski, J. Zhang, A. Marzoev, C. R. Aberger, K. Olukotun, and C. Re, High-accuracy low-precision training, arXiv preprint arXiv:1803.03383, 2018. 5, 7 [29] K. Zhao, S. Huang, P. Pan, Y. Li, Y. Zhang, Z. Gu, and Y. Xu, Distribution adaptive INT8 quantization for training cnns, in AAAI Conference on Artificial Intelligence, 2021, pp. 34833491. 5, 7 [30] M. Fournarakis and M. Nagel, In-hindsight quantization range estimation for quantized training, in IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2021, pp. 30633070. 5, 7 [31] Z. Zhang, A. Jaiswal, L. Yin, S. Liu, J. Zhao, Y. Tian, and Z. Wang, Q-galore: Quantized galore with int4 projection and layer-adaptive low-rank gradients, arXiv preprint arXiv:2407.08296, 2024. 5, 7 [32] H. Xi, C. Li, J. Chen, and J. Zhu, Training transformers with 4-bit integers, in Advances in Neural Information Processing Systems, 2023. 5, 7 [33] M. Schiemer, C. J. Schaefer, J. P. Vap, M. J. Horeni, Y. E. Wang, J. Ye, and S. Joshi, Hadamard domain training with integers for class incremental quantized learning, arXiv preprint arXiv:2310.03675, 2023. 5, 7 [34] S. Kim and E. Park, Hlq: Fast and efficient backpropagation via hadamard low-rank quantization, arXiv preprint arXiv:2406.15102, 2024. 5, [35] X. Zhang, S. Liu, R. Zhang, C. Liu, D. Huang, S. Zhou, J. Guo, Q. Guo, Z. Du, T. Zhi, and Y. Chen, Fixed-point back-propagation training, in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 23272335. 5, 7 [36] M. Wang, S. Rasoulinezhad, P. H. W. Leong, and H. K. So, NITI: training integer neural networks using integer-only arithmetic, IEEE Transactions on Parallel and Distributed Systems, vol. 33, no. 11, pp. 32493261, 2022. 5, 8 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 19 [37] L. Ding, W. Fei, Y. Huang, S. Ding, W. Dai, C. Li, J. Zou, and H. Xiong, AMPA: adaptive mixed precision allocation for low-bit integer training, in International Conference on Machine Learning, 2024. 5, 8 [38] A. Shen, Z. Lai, T. Sun, S. Li, K. Ge, W. Liu, and D. Li, Efficient deep neural network training via decreasing precision with layer capacity, Frontiers of Computer Science, vol. 19, no. 10, p. 1910355, 2025. 5, [39] Y. Fu, H. You, Y. Zhao, Y. Wang, C. Li, K. Gopalakrishnan, Z. Wang, and Y. Lin, Fractrain: Fractionally squeezing bit savings both temporally and spatially for efficient DNN training, in Advances in Neural Information Processing Systems, 2020. 5, 8 [40] H. Xi, Y. Chen, K. Zhao, K. J. Teh, J. Chen, and J. Zhu, Jetfire: Efficient and accurate transformer pretraining with INT8 data flow and per-block quantization, in International Conference on Machine Learning, 2024. 5, 8 [41] R. Banner, I. Hubara, E. Hoffer, and D. Soudry, Scalable methods for 8-bit training of neural networks, in Advances in Neural Information Processing Systems, 2018, pp. 51515159. 5, 8, 9 [42] H. Park, J. H. Lee, Y. Oh, S. Ha, and S. Lee, Training deep neural network in limited precision, arXiv preprint arXiv:1810.05486, 2018. 5, 8 [43] F. Zhu, R. Gong, F. Yu, X. Liu, Y. Wang, Z. Li, X. Yang, and J. Yan, Towards unified INT8 training for convolutional neural network, in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 19661976. 5, 8, 9 [44] W. Guo, D. Liu, W. Xie, Y. Li, X. Ning, Z. Meng, S. Zeng, J. Lei, Z. Fang, and Y. Wang, Towards accurate and efficient sub-8-bit integer training, arXiv preprint arXiv:2411.10948, 2024. 5, [45] Y. Fu, H. Guo, M. Li, X. Yang, Y. Ding, V. Chandra, and Y. Lin, CPT: efficient deep neural network training via cyclic precision, in International Conference on Learning Representations, 2021. 5, 8 [46] J. Yang, L. Deng, Y. Yang, Y. Xie, and G. Li, Training and inference for integer-based semantic segmentation network, Neurocomputing, vol. 454, pp. 101112, 2021. 5, 8 [47] W. Fei, W. Dai, L. Zhang, L. Zhang, C. Li, J. Zou, and H. Xiong, Latent weight quantization for integerized training of deep neural networks, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 47, no. 4, pp. 28162832, 2025. 5, 8 [48] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt, Stable and low-precision training for large-scale visionlanguage models, in Advances in Neural Information Processing Systems, 2023. 5, 8 [49] E. Park, S. Yoo, and P. Vajda, Value-aware quantization for training and inference of neural networks, in European Conference on Computer Vision, V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss, Eds., 2018, pp. 608624. 5, 9 [50] J. Chen, L. Zheng, Z. Yao, D. Wang, I. Stoica, M. W. Mahoney, and J. Gonzalez, Actnn: Reducing training memory footprint via 2bit activation compressed training, in International Conference on Machine Learning, 2021, pp. 18031813. 5, 9 [51] X. Liu, L. Zheng, D. Wang, Y. Cen, W. Chen, X. Han, J. Chen, Z. Liu, J. Tang, J. Gonzalez, M. W. Mahoney, and A. Cheung, GACT: activation compressed training for generic network architectures, in International Conference on Machine Learning, 2022, pp. 14 139 14 152. 5, [52] G. S. Novikov, D. Bershatsky, J. Gusak, A. Shonenkov, D. V. Dimitrov, and I. V. Oseledets, Few-bit backward: Quantized gradients of activation functions for memory footprint reduction, in International Conference on Machine Learning, 2023, pp. 26 36326 381. 5, 9 [53] Z. Liu, K. Zhou, F. Yang, L. Li, R. Chen, and X. Hu, EXACT: scalable graph neural networks training via extreme activation compression, in International Conference on Learning Representations, 2022. 5, 9 [54] S. Eliassen and R. Selvan, Activation compression of graph neural networks using block-wise quantization with improved variance minimization, in IEEE International Conference on Acoustics, Speech and Signal Processing, 2024, pp. 74307434. 5, 9 [55] H. Li, S. De, Z. Xu, C. Studer, H. Samet, and T. Goldstein, Training quantized nets: deeper understanding, in Advances in Neural Information Processing Systems, 2017, pp. 58115821. 5, 9 [56] Z. Li and C. D. Sa, Dimension-free bounds for low-precision training, in Advances in Neural Information Processing Systems, 2019, pp. 11 72811 738. 5, 9 [57] H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, and C. Zhang, Zipml: Training linear models with end-to-end low precision, and little bit of deep learning, in International Conference on Machine Learning, 2017, pp. 40354043. 5, 9 [58] J. Chen, Y. Gai, Z. Yao, M. W. Mahoney, and J. E. Gonzalez, statistical framework for low-bitwidth training of deep neural networks, in Advances in Neural Information Processing Systems, 2020. 5, 9 [59] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, Zero-shot text-to-image generation, in International Conference on Machine Learning, 2021, pp. 88218831. 5, [60] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer, 8-bit optimizers via block-wise quantization, in International Conference on Learning Representations, 2022. 5, 9 [61] K. Chitsaz, Q. Fournier, G. Mordido, and S. Chandar, Exploring quantization for efficient pre-training of transformer language models, in Conference on Empirical Methods in Natural Language Processing, 2024, pp. 13 47313 487. 5, 9 [62] I. Modoranu, M. Safaryan, G. Malinovsky, E. Kurtic, T. Robert, P. Richtarik, and D. Alistarh, Microadam: Accurate adaptive optimization with low space overhead and provable convergence, in Advances in Neural Information Processing Systems, 2024. 5, 9 [63] B. Li, J. Chen, and J. Zhu, Memory efficient optimizers with 4-bit states, in Advances in Neural Information Processing Systems, 2023. 5, 9 [64] Z. Li, X. Liu, B. Zhu, Z. Dong, Q. Gu, and K. Keutzer, Qft: Quantized full-parameter tuning of llms with affordable resources, arXiv preprint arXiv:2310.07147, 2023. 5, 9 [65] J. Li, K. Ding, K.-C. Toh, and P. Zhou, Memory-efficient 4-bit preconditioned stochastic optimization, arXiv preprint arXiv:2412.10663, 2024. 5, [66] S. Wang, P. Zhou, J. Li, and H. Huang, 4-bit shampoo for memoryefficient network training, in Advances in Neural Information Processing Systems, 2024. 5, 10 [67] T. Dettmers, 8-bit approximations for parallelism in deep learning, arXiv preprint arXiv:1511.04561, 2015. 5, 10 [68] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic, QSGD: communication-efficient SGD via gradient quantization and encoding, in Advances in Neural Information Processing Systems, 2017, pp. 17091720. 5, 10 [69] H. Tang, C. Yu, X. Lian, T. Zhang, and J. Liu, Doublesqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression, in International Conference on Machine Learning, 2019, pp. 61556165. 5, 10 [70] X. Xie, Z. Lin, K.-C. Toh, and P. Zhou, Loco: Low-bit communication adaptor for large-scale model training, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 5, [71] C. Chen, L. Shen, H. Huang, and W. Liu, Quantized adam with error feedback, ACM Transactions on Intelligent Systems and Technology, vol. 12, no. 5, pp. 56:156:26, 2021. 5, 10 [72] C. Chen, L. Shen, W. Liu, Efficient-adam: Communication-efficient distributed adam, IEEE Transactions Signal Processing, vol. 71, pp. 32573266, 2023. 5, 10 and Z. Luo, [73] Y. Yu, J. Wu, and L. Huang, Double quantization for communicationefficient distributed optimization, in Advances in Neural Information Processing Systems, 2019, pp. 44404451. 5, 10 [74] F. Faghri, I. Tabrizian, I. Markov, D. Alistarh, D. M. Roy, and A. Ramezani-Kebrya, Adaptive gradient quantization for data-parallel SGD, in Advances in Neural Information Processing Systems, 2020. 5, [75] M. Li, R. B. Basat, S. Vargaftik, C. Lao, K. Xu, M. Mitzenmacher, and M. Yu, THC: accelerating distributed deep learning using tensor homomorphic compression, in USENIX Symposium on Networked Systems Design and Implementation, 2024. 5, 10 [76] G. Wang, H. Qin, S. A. Jacobs, C. Holmes, S. Rajbhandari, O. Ruwase, F. Yan, L. Yang, and Y. He, Zero++: Extremely efficient coltraining, arXiv preprint lective communication for giant model arXiv:2306.10209, 2023. 5, 10 [77] J. Wang, B. Yuan, L. Rimanic, Y. He, T. Dao, B. Chen, C. Re, and C. Zhang, Fine-tuning language models over slow networks using activation quantization with guarantees, in Advances in Neural Information Processing Systems, 2022. 5, 10 [78] J. Jia, C. Xie, H. Lu, D. Wang, H. Feng, C. Zhang, B. Sun, H. Lin, Z. Zhang, X. Liu, and D. Tao, Sdp4bit: Toward 4-bit communication quantization in sharded data parallelism for LLM training, in Advances in Neural Information Processing Systems, 2024. 5, 10 [79] I. Markov, A. Vladu, Q. Guo, and D. Alistarh, Quantized distributed training of large models with convergence guarantees, in International Conference on Machine Learning, 2023, pp. 24 02024 044. 5, 10 [80] M. Courbariaux, Y. Bengio, and J. David, Binaryconnect: Training deep neural networks with binary weights during propagations, in JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 20 Advances in Neural Information Processing Systems, 2015, pp. 3123 3131. 5, 11 [81] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, Xnor-net: Imagenet classification using binary convolutional neural networks, in European Conference on Computer Vision, 2016, pp. 525542. 5, 11 [82] E. Wang, J. J. Davis, D. Moro, P. Zielinski, J. J. Lim, C. Coelho, S. Chatterjee, P. Y. K. Cheung, and G. A. Constantinides, Enabling binary neural network training on the edge, ACM Transactions on Embedded Computing Systems, vol. 22, no. 6, pp. 105:1105:19, 2023. 5, 11 [83] C. Gao, J. Chen, K. Zhao, J. Wang, and L. Jing, 1-bit fqt: Pushing the limit of fully quantized training to 1-bit, arXiv preprint arXiv:2408.14267, 2024. 5, 11 [84] F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu, 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns, in Annual Conference of the International Speech Communication Association, 2014, pp. 10581062. 5, 9, [85] S. P. Karimireddy, Q. Rebjock, S. U. Stich, and M. Jaggi, Error feedback fixes signsgd and other gradient compression schemes, in International Conference on Machine Learning, 2019, pp. 32523261. 5, 11 [86] H. Tang, S. Gan, A. A. Awan, S. Rajbhandari, C. Li, X. Lian, J. Liu, C. Zhang, and Y. He, 1-bit adam: Communication efficient large-scale training with adams convergence speed, in International Conference on Machine Learning, 2021, pp. 10 11810 129. 5, 11 [87] Y. Lu, C. Li, M. Zhang, C. D. Sa, and Y. He, Maximizing communication efficiency for large-scale training via 0/1 adam, in International Conference on Learning Representations, 2023. 5, 11 [88] C. Li, A. A. Awan, H. Tang, S. Rajbhandari, and Y. He, 1-bit LAMB: communication efficient large-scale large-batch training with lambs convergence speed, in IEEE International Conference on High Performance Computing, Data, and Analytics, 2022, pp. 272281. 5, 11 [89] F. Wu, S. He, S. Guo, Z. Qu, H. Wang, W. Zhuang, and J. Zhang, Sign bit is enough: learning synchronization framework for multi-hop allreduce with ultimate compression, in ACM/IEEE Design Automation Conference, 2022, pp. 193198. 5, 11 [90] H. Peng, S. Qin, Y. Yu, J. Wang, H. Wang, and G. Li, Birder: Communication-efficient 1-bit adaptive optimizer for practical distributed DNN training, in Advances in Neural Information Processing Systems, 2023. 5, [91] R. Krishnamoorthi, Quantizing deep convolutional networks for efficient inference: whitepaper, arXiv preprint arXiv:1806.08342, 2018. 4 [92] D. Williamson, Dynamically scaled fixed point arithmetic, in IEEE Pacific Rim Conference on Communications, Computers and Signal Processing Conference Proceedings, 1991, pp. 315318. 6 [93] A. Krizhevsky, I. Sutskever, and G. E. Hinton, Imagenet classification with deep convolutional neural networks, in Advances in Neural Information Processing Systems, P. L. Bartlett, F. C. N. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds., 2012, pp. 11061114. 6 [94] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei, Imagenet: large-scale hierarchical image database, in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2009. 6 [95] A. Krizhevsky, G. Hinton et al., Learning multiple layers of features from tiny images, 2009. [96] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2016. 6 [97] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, Going deeper with convolutions, in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2015, pp. 19. 6 [98] K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, in International Conference on Learning Representations, Y. Bengio and Y. LeCun, Eds., 2015. 6 [99] J. J. Sylvester, Lx. thoughts on inverse orthogonal matrices, simultaneous signsuccessions, and tessellated pavements in two or more colours, with applications to newtons rule, ornamental tile-work, and the theory of numbers, The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, vol. 34, no. 232, pp. 461475, 1867. 7 [101] Y. Yang, X. Chi, L. Deng, T. Yan, F. Gao, and G. Li, Towards efficient full 8-bit integer DNN online training on resource-limited devices without batch normalization, Neurocomputing, vol. 511, pp. 175186, 2022. 8 [102] N. Shazeer and M. Stern, Adafactor: Adaptive learning rates with sublinear memory cost, in International Conference on Machine Learning, 2018, pp. 46034611. 8 [103] X. Chen, C. Liang, D. Huang, E. Real, K. Wang, H. Pham, X. Dong, T. Luong, C. Hsieh, Y. Lu, and Q. V. Le, Symbolic discovery of optimization algorithms, in Advances in Neural Information Processing Systems, 2023. [104] V. Gupta, T. Koren, and Y. Singer, Shampoo: Preconditioned stochastic tensor optimization, in International Conference on Machine Learning, J. G. Dy and A. Krause, Eds., 2018, pp. 18371845. 9 [105] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, Quantized neural networks: Training neural networks with low precision weights and activations, Journal of Machine Learning Research, vol. 18, pp. 187:1187:30, 2017. 11 [106] Y. You, J. Li, S. J. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song, J. Demmel, K. Keutzer, and C. Hsieh, Large batch optimization for deep learning: Training BERT in 76 minutes, in International Conference on Learning Representations, 2020. 11 [107] P. Micikevicius, S. Narang, J. Alben, G. F. Diamos, E. Elsen, D. Garcıa, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, and H. Wu, Mixed precision training, in International Conference on Learning Representations, 2018. 12 [108] R. Zhao, B. Vogel, T. Ahmed, and W. Luk, Reducing underflow in mixed precision training by gradient scaling, in International Joint Conference on Artificial Intelligence, 2020, pp. 29222928. 12 [109] X. He, J. Sun, H. Chen, and D. Li, Campo: Cost-aware performance optimization for mixed-precision neural network training, in USENIX Annual Technical Conference, 2022, pp. 505518. 12 [110] D. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee, S. Avancha, D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen et al., study of bfloat16 for deep learning training, arXiv preprint arXiv:1905.12322, 2019. [111] X. Sun, J. Choi, C. Chen, N. Wang, S. Venkataramani, V. Srinivasan, X. Cui, W. Zhang, and K. Gopalakrishnan, Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks, in Advances in Neural Information Processing Systems, 2019, pp. 49014910. 12 [112] B. Noune, P. Jones, D. Justus, D. Masters, and C. Luschi, 8bit numerical formats for deep neural networks, arXiv preprint arXiv:2206.02915, 2022. 12 [113] J. Lee, J. Bae, B. Kim, S. J. Kwon, and D. Lee, To fp8 and back again: Quantifying the effects of reducing precision on llm training stability, arXiv preprint arXiv:2405.18710, 2024. 12 [114] R. Han, J. Demmel, and Y. You, Auto-precision scaling for distributed deep learning, in High Performance Computing: 36th International Conference, ISC High Performance 2021, Virtual Event, June 24July 2, 2021, Proceedings 36. Springer, 2021, pp. 7997. 12 [115] O. Desrentes, B. D. de Dinechin, and J. Le Maire, Exact dot product accumulate operators for 8-bit floating-point deep learning, in 2023 26th Euromicro Conference on Digital System Design (DSD). IEEE, 2023, pp. 642649. 12 [116] D. R. Lutz, A. Saini, M. Kroes, T. Elmer, and H. Valsaraju, Fused fp8 4-way dot product with scaling and fp32 accumulation, in 2024 IEEE 31st Symposium on Computer Arithmetic (ARITH). IEEE, 2024, pp. 4047. 12 [117] Y. Luo, Z. Zhang, R. Wu, H. Liu, Y. Jin, K. Zheng, M. Wang, Z. He, G. Hu, L. Chen et al., Ascend hifloat8 format for deep learning, arXiv preprint arXiv:2409.16626, 2024. [118] X. Sun, N. Wang, C. Chen, J. Ni, A. Agrawal, X. Cui, S. Venkataramani, K. E. Maghraoui, V. Srinivasan, and K. Gopalakrishnan, Ultralow precision 4-bit training of deep neural networks, in Advances in Neural Information Processing Systems, 2020. 12, 13 [119] R. Wang, Y. Gong, X. Liu, G. Zhao, Z. Yang, B. Guo, Z. Zha, and P. Cheng, Optimizing large language model training using fp4 quantization, arXiv preprint arXiv:2501.17116, 2025. 12, 13 [120] J. Zhou, D. Tang, R. Fu, B. Hu, H. Xu, Y. Wang, Z. Pei, Z. Su, L. Liu, X. Zhang et al., Towards efficient pre-training: Exploring fp4 precision in large language models, arXiv preprint arXiv:2502.11458, 2025. 12, 13 [100] S. Ioffe and C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, in International Conference on Machine Learning, 2015, pp. 448456. 8 [121] M. Fishman, B. Chmiel, R. Banner, and D. Soudry, Scaling fp8 training to trillion-token llms, arXiv preprint arXiv:2409.12517, 2024. 12, 13, 14 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 [122] P. Micikevicius, D. Stosic, N. Burgess, M. Cornea, P. Dubey, R. Grisenthwaite, S. Ha, A. Heinecke, P. Judd, J. Kamalu et al., Fp8 formats for deep learning, arXiv preprint arXiv:2209.05433, 2022. 12, 13 [123] H. Peng, K. Wu, Y. Wei, G. Zhao, Y. Yang, Z. Liu, Y. Xiong, Z. Yang, B. Ni, J. Hu et al., Fp8-lm: Training fp8 large language models, arXiv preprint arXiv:2310.18313, 2023. 12, 13, 14 [124] N. Mellempudi, S. Srinivasan, D. Das, and B. Kaul, Mixed precision training with 8-bit floating point, arXiv preprint arXiv:1905.12334, 2019. 12, 13 [125] L. Cambier, A. Bhiwandiwalla, T. Gong, O. H. Elibol, M. Nekuii, and H. Tang, Shifted and squeezed 8-bit floating point format for lowprecision training of deep neural networks, in International Conference on Learning Representations, 2020. 12, 13 [126] B. Chmiel, L. Ben-Uri, M. Shkolnik, E. Hoffer, R. Banner, and D. Soudry, Neural gradients are near-lognormal: improved quantized and sparse training, in International Conference on Learning Representations, 2021. 12, 13 [127] C. Blake, D. Orr, and C. Luschi, Unit scaling: Out-of-the-box lowprecision training, in International Conference on Machine Learning. PMLR, 2023, pp. 25482576. 12, [128] S. P. Perez, Y. Zhang, J. Briggs, C. Blake, J. Levy-Kramer, P. Balanca, C. Luschi, S. Barlow, and A. W. Fitzgibbon, Training and inference of large language models using 8-bit floating point, arXiv preprint arXiv:2309.17224, 2023. 12, 13 [129] P. Balanca, S. Hosegood, C. Luschi, and A. Fitzgibbon, Scalify: scale propagation for efficient low-precision llm training, arXiv preprint arXiv:2407.17353, 2024. 12, 13 [130] N. Wang, J. Choi, D. Brand, C. Chen, and K. Gopalakrishnan, Training deep neural networks with 8-bit floating point numbers, in Advances in Neural Information Processing Systems, 2018, pp. 76867695. 12, 13 [131] C. Sakr, N. Wang, C. Chen, J. Choi, A. Agrawal, N. R. Shanbhag, and K. Gopalakrishnan, Accumulation bit-width scaling for ultra-low precision training of deep networks, in International Conference on Learning Representations, 2019. 12, 13 [132] S. B. Ali, S. Filip, and O. Sentieys, stochastic rounding-enabled low-precision floating-point MAC for DNN training, in Design, Automation & Test in Europe Conference & Exhibition, 2024, pp. 16. 12, 14 [133] H. Xi, H. Cai, L. Zhu, Y. Lu, K. Keutzer, J. Chen, and S. Han, Coat: Compressing optimizer states and activation for memory-efficient fp8 training, arXiv preprint arXiv:2410.19313, 2024. 12, [134] M. Drumond, T. Lin, M. Jaggi, and B. Falsafi, Training dnns with hybrid block floating point, in Advances in Neural Information Processing Systems, 2018, pp. 451461. 12, 14 [135] S. B. Harma, A. Chakraborty, N. Sperry, B. Falsafi, M. Jaggi, and Y. Oh, Accuracy booster: Enabling 4-bit fixed-point arithmetic for dnn training, arXiv preprint arXiv:2211.10737, 2022. 12, 14 [136] J. L. Gustafson and I. T. Yonemoto, Beating floating point at its own game: Posit arithmetic, Supercomputing Frontiers and Innovations, vol. 4, no. 2, pp. 7186, 2017. 14 [137] F. De Dinechin, L. Forget, J.-M. Muller, and Y. Uguen, Posits: the good, the bad and the ugly, in Proceedings of the Conference for Next Generation Arithmetic 2019, 2019, pp. 110. 14 [138] J. Lu, C. Fang, M. Xu, J. Lin, and Z. Wang, Evaluations on deep neural networks training using posit number system, IEEE Transactions Computers, vol. 70, no. 2, pp. 174187, 2021. 14 [139] U. Koster, T. Webb, X. Wang, M. Nassar, A. K. Bansal, W. Constable, O. Elibol, S. Hall, L. Hornof, A. Khosrowshahi, C. Kloss, R. J. Pai, and N. Rao, Flexpoint: An adaptive numerical format for efficient training of deep neural networks, in Advances in Neural Information Processing Systems, 2017, pp. 17421752. 14 [140] P. Lin, M. Sun, C. Kung, and T. Chiueh, Floatsd: new weight representation and associated update method for efficient convolutional neural network training, IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol. 9, no. 2, pp. 267279, 2019. 14 [141] K. Zhong, X. Ning, G. Dai, Z. Zhu, T. Zhao, S. Zeng, Y. Wang, and H. Yang, Exploring the potential of low-bit training of convolutional neural networks, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 41, no. 12, pp. 54215434, 2022. 14, [142] A. Tseng, T. Yu, and Y. Park, Training llms with mxfp4, arXiv preprint arXiv:2502.20586, 2025. 14, 15 [143] Y. Chen, H. Xi, J. Zhu, and J. Chen, Oscillation-reduced mxfp4 training for vision transformers, arXiv preprint arXiv:2502.20853, 2025. 14, 15 [144] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, Qlora: Efficient finetuning of quantized llms, in Advances in Neural Information Processing Systems, 2023. 14 [145] X. Wei, Y. Zhang, X. Zhang, R. Gong, S. Zhang, Q. Zhang, F. Yu, and X. Liu, Outlier suppression: Pushing the limit of low-bit transformer language models, Advances in Neural Information Processing Systems, pp. 17 40217 414, 2022. 15 [146] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, Smoothquant: Accurate and efficient post-training quantization for large language models, in International Conference on Machine Learning, 2023, pp. 38 08738 099. [147] Y. Yang, J. Gao, and W. Hu, Raana: fast, flexible, and data-efficient post-training quantization algorithm, arXiv preprint arXiv:2504.03717, 2025. 15 [148] J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han, Awq: Activation-aware weight quantization for on-device llm compression and acceleration, Proceedings of Machine Learning and Systems, pp. 87100, 2024. 15 [149] Y. Zhao, C.-Y. Lin, K. Zhu, Z. Ye, L. Chen, S. Zheng, L. Ceze, A. Krishnamurthy, T. Chen, and B. Kasikci, Atom: Low-bit quantization for efficient and accurate llm serving, Proceedings of Machine Learning and Systems, pp. 196209, 2024. 15 [150] H. Liu, H. Gao, X. Zhang, C. Li, F. Zhang, W. Wang, F. Ma, and H. Yu, Septq: simple and effective post-training quantization paradigm for large language models, in ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2025, pp. 812823. 15 [151] M. Zhu, Q. Zhong, L. Shen, L. Ding, J. Liu, B. Du, and D. Tao, Zeroshot sharpness-aware quantization for pre-trained language models, in Conference on Empirical Methods in Natural Language Processing, 2023, pp. 11 30511 327. [152] J. Li, T. Zhang, I. E.-H. Yen, and D. Xu, Fp8-bert: Post-training quantization for transformer, arXiv preprint arXiv:2312.05725, 2023. 15 [153] H. Wang, S. Ma, L. Dong, S. Huang, H. Wang, L. Ma, F. Yang, R. Wang, Y. Wu, and F. Wei, Bitnet: Scaling 1-bit transformers for large language models, arXiv preprint arXiv:2310.11453, 2023. 15 [154] S. Ma, H. Wang, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, R. Wang, J. Xue, and F. Wei, The era of 1-bit llms: All large language models are in 1.58 bits, arXiv preprint arXiv:2402.17764, vol. 1, 2024. 15 [155] J. Nielsen, P. Schneider-Kamp, and L. Galke, Continual quantizationaware pre-training: When to transition from 16-bit to 1.58-bit pretraining for bitnet language models? arXiv preprint arXiv:2502.11895, 2025. 15 [156] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi, and V. Chandra, LLM-QAT: data-free quantization aware training for large language models, in Annual Meeting of the Association for Computational Linguistics, 2024, pp. 467484. 15 [157] D. Du, Y. Zhang, S. Cao, J. Guo, T. Cao, X. Chu, and N. Xu, Bitdistiller: Unleashing the potential of sub-4-bit llms via self-distillation, in Annual Meeting of the Association for Computational Linguistics, 2024, pp. 102116. [158] M. Chen, W. Shao, P. Xu, J. Wang, P. Gao, K. Zhang, and P. Luo, Efficientqat: Efficient quantization-aware training for large language models, arXiv preprint arXiv:2407.11062, 2024. 15 [159] A. Panferov, J. Chen, S. Tabesh, R. L. Castro, M. Nikdan, and D. Alistarh, Quest: Stable training of llms with 1-bit weights and activations, arXiv preprint arXiv:2502.05003, 2025. 15, 16 [160] J. Pang and T. Cai, Stabilizing quantization-aware training by implicitregularization on hessian matrix, arXiv preprint arXiv:2503.11159, 2025. 15, 16 [161] G. E. Hinton, O. Vinyals, and J. Dean, Distilling the knowledge in neural network, arXiv preprint arXiv: 1503.02531, 2015. 15 [162] A. Paszke, Pytorch: An imperative style, high-performance deep learning library, arXiv preprint arXiv:1912.01703, 2019. 16 [163] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, J. Levenberg, D. Mane, R. Monga, L. Kaiser, M. Kudlur, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viegas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, TensorFlow: Large-scale machine learning on heterogeneous systems, 2015, software available from tensorflow.org. [Online]. Available: https://www.tensorflow.org/ 16 [164] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. WandermanJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 22 and Q. Zhang, Milne, transformations of Python+NumPy programs, 2018. [Online]. Available: http://github. com/jax-ml/jax 16 composable JAX: [165] Y. Ma, D. Yu, T. Wu, and H. Wang, Paddlepaddle: An open-source deep learning platform from industrial practice, Frontiers of Data and Domputing, vol. 1, no. 1, pp. 105115, 2019. [166] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He, Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters, in Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, 2020, pp. 35053506. 16 [167] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, Zero: Memory optimizations toward training trillion parameter models, in SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2020, pp. 116. 16 [168] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer, 8-bit optimizers via block-wise quantization, arXiv preprint arXiv:2110.02861, 2021. 16 [169] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, Llm.int8(): 8-bit matrix multiplication for transformers at scale, arXiv preprint arXiv:2208.07339, 2022. 16 [170] NVIDIA, Apex, 2022. NVIDIA/apex [Online]. Available: https://github.com/ [171] S. Li, H. Liu, Z. Bian, J. Fang, H. Huang, Y. Liu, B. Wang, and Y. You, Colossal-ai: unified deep learning system for large-scale parallel training, in Proceedings of the 52nd International Conference on Parallel Processing, 2023, pp. 766775."
        }
    ],
    "affiliations": [
        "Baidu Inc., Beijing 100000, China",
        "College of Computing and Data Science, Nanyang Technological University, 639798, Singapore",
        "Computer Science, City University of Hong Kong, Hong Kong 999077, China",
        "School of Computer Science, Wuhan University, Wuhan 430072, China",
        "School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen 518107, China",
        "School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China"
    ]
}