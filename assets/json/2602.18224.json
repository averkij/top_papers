{
    "paper_title": "SimVLA: A Simple VLA Baseline for Robotic Manipulation",
    "authors": [
        "Yuankai Luo",
        "Woping Chen",
        "Tong Liang",
        "Baiqiao Wang",
        "Zhenguo Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic manipulation, leveraging large-scale pre-training to achieve strong performance. The field has rapidly evolved with additional spatial priors and diverse architectural innovations. However, these advancements are often accompanied by varying training recipes and implementation details, which can make it challenging to disentangle the precise source of empirical gains. In this work, we introduce SimVLA, a streamlined baseline designed to establish a transparent reference point for VLA research. By strictly decoupling perception from control, using a standard vision-language backbone and a lightweight action head, and standardizing critical training dynamics, we demonstrate that a minimal design can achieve state-of-the-art performance. Despite having only 0.5B parameters, SimVLA outperforms multi-billion-parameter models on standard simulation benchmarks without robot pretraining. SimVLA also reaches on-par real-robot performance compared to pi0.5. Our results establish SimVLA as a robust, reproducible baseline that enables clear attribution of empirical gains to future architectural innovations. Website: https://frontierrobo.github.io/SimVLA"
        },
        {
            "title": "Start",
            "content": "SimVLA: Simple VLA Baseline for Robotic Manipulation Yuankai Luo Woping Chen Tong Liang Baiqiao Wang Zhenguo Li"
        },
        {
            "title": "Frontier Robotics",
            "content": "6 2 0 2 0 2 ] . [ 1 4 2 2 8 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models have emerged as promising paradigm for generalpurpose robotic manipulation, leveraging largescale pre-training to achieve strong performance. The field has rapidly evolved with additional spatial priors and diverse architectural innovations. However, these advancements are often accompanied by varying training recipes and implementation details, which can make it challenging to disentangle the precise source of empirical gains. In this work, we introduce SimVLA, streamlined baseline designed to establish transparent reference point for VLA research. By strictly decoupling perception from controlusing standard vision-language backbone and lightweight action headand standardizing critical training dynamics, we demonstrate that minimal design can achieve state-of-the-art performance. Despite having only 0.5B parameters, SimVLA outperforms multi-billion-parameter models on standard simulation benchmarks without robot pretraining. SimVLA also reaches on-par real-robot performance compared to π0.5. Our results establish SimVLA as robust, reproducible baseline that enables clear attribution of empirical gains to future architectural innovations. 1. Introduction The field of Vision-Language-Action (VLA) learning has advanced rapidly, driven by wave of architectural innovations. Recent methods have proposed increasingly sophisticated designs, ranging from mechanisms that enrich perception with temporal context, to modules that inject explicit 3D spatial awareness, to high-capacity decoders that model complex action distributions. While these contributions have pushed the boundaries of robot capabilities, they also introduce significant challenge for the research commuCorrespondence to: Zhenguo Li <zhenguol@gmail.com>, Project website: https://frontierrobo.github.io/SimVLA. Preprint. February 23, 2026. 1 Figure 1. Out-of-box real-robot task examples. We deploy SimVLA without any additional fine-tuning on our held-out scenes and evaluate it on set of multi-stage tasks that require both dexterous manipulation and semantic understanding. nity: attributing performance gains to specific components. Since architectural changes are frequently introduced alongside confounding variablessuch as varying pretraining datasets, differing backbone scales, or ad-hoc optimization schedulesit can be difficult to disentangle the impact of novel mechanism from the underlying training recipe. To facilitate clearer comparisons and accelerate progress, we introduce SimVLA, streamlined baseline designed to serve as transparent reference point for VLA research. Our goal is not to suggest that architectural complexity is unnecessary, but to establish high-performance lower bound of complexity against which future innovations can be measured. By providing clean, minimal design that achieves state-of-the-art results, we hope to help the community better quantify the true added value of sophisticated architectural components when they are introduced. SimVLA adopts modular philosophy that decouples perception from control: standard pretrained vision-language backbone produces fused representations, which are then processed by lightweight action head to predict continuous actions. This design offers critical advantage in future-proofing: as vision-language models (VLMs) evolve, SimVLA allows researchers to swap in the latest SOTA backSimVLA: Simple VLA Baseline for Robotic Manipulation Table 1. Performance and efficiency summary. We report LIBERO success (%) and peak training VRAM at Batch=8 (GB) under matched evaluation setup. Model Backbone LIBERO Avg VRAM (GB) OpenVLA-OFT π0.5 VLA-Adapter SimVLA (Ours) 7B 3B 0.5B 0.5B 97.1 96.9 97.3 98. 62.0 GB 51.3 GB 24.7 GB 9.3 GB bones (e.g., upgrading from 0.5B to 7B model) without redesigning complex cross-modal adapters. Furthermore, we rigorously standardize the often-overlooked training dynamicssuch as data shuffling strategies, action space normalization, and optimization schedulesdemonstrating that these implementation details often outweigh architectural differences in their impact on performance. Despite its simplicity, SimVLA is both effective and highly efficient. Figure 1 illustrates SimVLAs zero-shot scene generalization capability when deployed on the Galaxea R1 Lite, additional details about the robot platform and training data are provided in Section 4.1.2. Table 1 further presents representative example in which our model outperforms multi-billion-parameter baselines while maintaining compact memory footprint. Our main contributions are: We propose SimVLA, modular VLA baseline that decouples perception from control, enabling flexible and future-proof design that can easily adapt to new visionlanguage backbones. We identify and standardize the silent drivers of VLA performancespecifically data shuffling, normalization, and optimization dynamicsproviding rigorous training recipe that enables fair cross-model comparisons. We show that this minimal design achieves state-of-the-art performance, surpassing larger and more complex models on simulation benchmarks while enabling efficient realrobot transfer with zero-shot scene generalization. The rest of our paper is organized as follows: Section 2 reviews recent VLA advances. Section 3 introduces SimVLA and our standardized training recipe. Section 4 evaluates SimVLA in simulation and on real robots with detailed ablations. We draw our conclusion in Section 5. 2. Related Work The development of VLA models has accelerated rapidly, with numerous approaches proposing diverse strategies to improve robotic control through multimodal learning. In this section, we organize recent advances along three complementary axes: (1) visual and temporal augmentation 2 methods that enrich perceptual inputs with motion cues, predictive modeling, or memory mechanisms; (2) geometric and 3D prior integration that injects explicit spatial understanding into the VLA framework; and (3) complex action representations and architectural innovations that explore more expressive decoders and efficient designs. For more comprehensive review, readers are encouraged to consult recent surveys (Xu et al., 2025; Sapkota et al., 2025; Zhang et al., 2025b) that offer systematic analyses of the VLA research landscape. Visual and Temporal Augmentation. Early VLA models, such as OpenVLA (Kim et al., 2024), typically rely on static RGB inputs, which limits their ability to reason about finegrained physical dynamics or long-horizon dependencies. To bridge this gap, recent approaches have focused on augmenting the visual modality with explicit motion cues or temporal reasoning. For instance, FlowVLA (Zhong et al., 2025), CoT-VLA (Zhao et al., 2025), and TraceVLA (Zheng et al., 2024) introduce visual chain-of-thought by explicitly predicting optical flow, sub-goal image, or overlaying trajectory traces onto input images, while 4D-VLA (Zhang et al., 2025a) integrates 4D spatiotemporal information to mitigate state chaos. Additionally, PixelVLA (Liang et al., 2025a) and ReconVLA (Song et al., 2025b) enhance visual grounding via auxiliary segmentation or reconstruction tasks. Beyond immediate visual augmentation, several works incorporate predictive modeling to enhance planning. WorldVLA (Cen et al., 2025) and Dream-VLA (Zhang et al., 2025c) integrate world models to predict future states, while ThinkAct (Huang et al., 2025) and IntentionVLA (Chen et al., 2025) generate plans, or intention descriptions before acting. To handle long contexts, FPC-VLA (Yang et al., 2025), MemoryVLA (Shi et al., 2025), and HAMLET (Koo et al., 2025) propose dedicated memory modules or dual-stream architectures to make history-aware predictions. While these methods significantly improve state tracking and capture features at multiple temporal scales, they often incur substantial computational overhead and architectural complexity, requiring auxiliary estimators or multi-stage inference processes that can complicate realtime deployment. Geometric and 3D Priors. Recognizing that 2D visionlanguage backbones may lack precise spatial understanding, growing body of research explicitly injects 3D geometric priors into the VLA framework. Both 4D-VLA (Zhang et al., 2025a) and SpatialVLA (Qu et al., 2025a) apply positional encodings to enhance the spatial awareness. 4D-VLA fuses positional encoded 3D coordinates with visual tokens, whereas SpatialVLA introduces egocentric 3D position encodings that does not rely on camera extrinsics. GraspVLA (Deng et al., 2025) and MolmoAct SimVLA: Simple VLA Baseline for Robotic Manipulation (Lee et al., 2025) both introduce additional spatial priors to enhance chain-of-thought reasoning capabilities. Specifically, GraspVLA improves 3D awareness in its unified CoT progress through auxiliary training tasks such as detection and target grasp pose estimation, while MolmoAct (Lee et al., 2025) utilizes depth-aware perception tokens and visual trace to enable reasoning in space. Other approaches, such as GeoVLA (Sun et al., 2025), FALCON (Zhang et al., 2025d), and DepthVLA(Yuan et al., 2025) go step further by processing point clouds, geometric tokens, or depth maps alongside RGB data. Although these spatially-aware architectures demonstrate superior precision in geometric tasks, they often introduce dependencies on specific sensors or heavy 3D encoders. This can reduce the models generality across diverse embodiments compared to RGB-only baselines which are easier to scale and deploy. Action Representations and Architectures. To address the limitations of simple discrete action tokenization, recent work has explored two primary directions: optimizing architectural efficiency and enhancing the expressivity of action distributions. Focusing on efficiency and adaptation, several works modify the underlying architecture or tokenization scheme to reduce computational overhead. For instance, FAST (Pertsch et al., 2025) employs frequency-domain tokenization to compress trajectories, while PD-VLA (Song et al., 2025a) accelerates inference via parallel decoding. OpenVLA-OFT (Kim et al., 2025) forgoes action tokenization and directly regress continuous actions instead. On the architectural side, VLAAdapter (Wang et al., 2025a) and FLOWER (Reuss et al., 2025) introduce lightweight adapters or action head that lowers computational burden, and X-VLA (Zheng et al., 2025b) utilizes soft prompts for scalable cross-embodiment learning. Specialized efficient models like NORA (Hung et al., 2025), SmolVLA (Shukor et al., 2025), and GR00TN1 (Bjorck et al., 2025) further optimize performance on specific hardware or humanoid embodiments. Parallel to efficiency, significant body of work has investigated methods to model continuous multimodal distributions. Diffusion-based policies have emerged as dominant paradigm in this area, with models like Diffusion Policy (Chi et al., 2023), π0 (Black et al., 2024), π0.5 (Black et al., 2025), and DD-VLA (Liang et al., 2025b). Meanwhile, UnifiedVLA (Wang et al., 2025b) and UniVLA (Bu et al., 2025) explore unified tokenization and latent action spaces to capture causal dynamics, while UniAct (Zheng et al., 2025a) and VQ-VLA (Wang et al., 2025c) learn universal discrete action codebooks with vector quantization. Despite their performance gains, these models often introduce challenges such as increased inference latency (e.g., iterative diffusion steps) or training instability. Our work offers counterpoint to this trend. We demonstrate that simple but strong baseline, built on standard flow matching and validated training recipe, can achieve competitive performance without the need for additional visual cues, spatiotemporal priors, or major architectural change. 3. SimVLA: Simple VLA Baseline As introduced in Section 2, recent VLA models have rapidly evolved with increasingly complex architectural components and additional priors. While these additions often yield empirical improvements, they also complicate fair comparisons across methods, making it difficult to disentangle gains from architectural novelty versus optimization and implementation choices. In this work, we deliberately take conservative stance. Rather than introducing new architectural components, we ask more fundamental question: how strong can minimal VLA design be when core modeling and training choices are carefully standardized? Our goal is not to diminish the importance of richer inductive biases, but to establish clean and reproducible baseline that clarifies what performance is achievable without additional architectural complexity. To this end, we propose SimVLA, simple and modular VLA baseline that decouples perception and control via lightweight action head conditioned on vision-language representations. Despite its simplicity, SimVLA achieves competitiveand in several cases state-of-the-artperformance across standard benchmarks, while offering substantial improvements in training efficiency and inference throughput. We hope that this baseline can serve as strong reference point for future work, enabling more precise evaluation of architectural innovations in VLA systems. 3.1. Preliminaries the distribution"
        },
        {
            "title": "We model\nfuture",
            "content": "Problem Formulation. conditional chunk of At = [at, at+1, . . . , at+H1] RHda given an observation ot. The observation contains the respective multi-view RGB images 1 , the pairing language instruction ℓt, and the robot proprioception (state) st: ot = [I 1 , . . . , , . . . , , ℓt, st]. action VLM Backbone Encoder. Following the standard latefusion paradigm (Black et al., 2024), we employ pretrained VLM Eϕ to map multi-view RGB observations and the corresponding language instruction into shared token representation: Zt = Eϕ(I 1 , . . . , , ℓt). Importantly, we deliberately treat the VLM as perceptionlanguage encoder, rather than as an action-generating module. This design choice reflects principled separation of responsibilities: high-level semantic understanding is han3 SimVLA: Simple VLA Baseline for Robotic Manipulation dled by the VLM, while continuous control is delegated to lightweight action head. Such decoupling enables modularity, simplifies inference, and facilitates controlled analysis of downstream action modeling choices. Although the VLM is used in an encoder-only role, it is not frozen by default. We jointly fine-tune the backbone together with the action head, optionally using short warmup stage for training stability. This preserves adaptability to the target robotic domain while maintaining clear architectural boundary between perception-language representation and action generation. Action Head. SimVLA uses vanilla Transformer encoder (Vaswani et al., 2017) as an action head to model action chunks in continuous space. The action head consumes the fused VLM tokens Zt, proprioception st, timestep embedding, and noised action chunk, and predicts the corresponding denoising vector. Flow Matching. We model continuous action generation using conditional flow matching (Lipman et al., 2022; Black et al., 2024), which learns deterministic vector field that transforms noise into data under the conditioning of the current observation. Compared to discrete autoregressive decoding or stochastic diffusion-based formulations, flow matching offers lightweight and stable abstraction that is particularly well-suited for continuous control. Concretely, let denote normalized action chunk and ϵ (0, I) denote Gaussian noise. We sample noise level (0, 1] and construct noised action xt = tϵ + (1 t)x. The action head vθ(xt, ot, t) is trained to predict the corresponding denoising vector field using standard ℓ2 objective: L(θ) = (cid:2)vθ(xt, ot, t) (ϵ x)2 (cid:3) . We emphasize that our goal is not to capture highly multimodal action distributions, but rather to provide simple and reliable mechanism for generating smooth and temporally consistent action chunks. At inference time, we integrate the learned vector field from noise to data using small number of Euler steps, resulting in efficient and stable action generation suitable for real-time execution. 3.2. SimVLA Architecture Design Principle. SimVLA adopts an intentionally minimal architectural design as illustrated in Figure 2: visionlanguage encoder produces fused representations once per control step, and lightweight action transformer generates continuous action chunks via flow matching. Our goal is not to introduce new architectural mechanisms, but to establish clean and neutral baseline that isolates the effects of action modeling and training dynamics. At each timestep, the fused vision-language tokens Zt are first obtained from the VLM backbone. We then construct Figure 2. SimVLA overview. SimVLA is minimal baseline: VLM encoder produces fused vision-language tokens once per control step, and lightweight action transformer performs flowmatching denoising to generate continuous action chunk. the input sequence to the action head by concatenating projected VLM tokens, broadcasted proprioceptive state embedding, sinusoidal time embedding, and the noised action chunk. This unified token sequence is processed by vanilla Transformer encoder with pure self-attention (Vaswani et al., 2017), without cross-attention, memory modules, or modality-specific routing. Rationale. We intentionally rely on self-attention over concatenated tokens as neutral information integration mechanism. While more specialized architecturessuch as crossattention bridges or conditional normalizationmay offer additional inductive biases, they also introduce confounding factors that complicate fair comparison. By using single self-attention transformer, SimVLA allows the model to learn modality interactions directly from data, while keeping architectural assumptions minimal. Practical Advantages. This design further yields practical benefits for deployment. Since the vision-language backbone is executed only once per control step, all subsequent denoising iterations are handled by the lightweight action head, resulting in reduced latency and improved inference throughput. 3.3. Training and Inference Recipe central takeaway of this work is that strong VLA performance can often be achieved through careful standardization of training and inference details, even with minimal architectural design. In practice, we find that several seemingly minor choices can dominate performance differences if left under-specified. Accordingly, we explicitly control and report the following factors across all experiments. Action Representation and Normalization. We train the flow model in normalized continuous action space, using per-dimension statistics computed from the training set. Proprioceptive states are normalized when applicable to improve optimizer conditioning. We predict action chunks of 4 SimVLA: Simple VLA Baseline for Robotic Manipulation horizon and execute them in receding-horizon manner; we emphasize that the choice of is major performance knob and must be tuned per benchmark. Data Handling. Beyond action chunking, we carefully control data shuffling during training. Since demonstration trajectories exhibit strong temporal correlations, improper shuffling can lead to brittle optimization and poor longhorizon generalization. We find that consistent shuffling is critical for stable training and fair benchmarking. Optimization Dynamics. We systematically sweep learning rates, warm-up schedules, and learning rate schedulers while keeping batch size and total training steps fixed across comparisons. Notably, we observe that learning rate selection alone can overshadow architectural differences if not properly tuned, underscoring the importance of reporting optimization details for reproducibility. Architecture Configuration. While SimVLA employs minimal action head by default, we ablate action transformer scale, VLM backbone choice, and information injection mechanisms (token concatenation, cross-attention, and conditional normalization). We view these variations as implementation choices rather than architectural novelties, and we report them to contextualize performance differences. Inference. At inference time, SimVLA follows an encodeonce, denoise-in-the-head workflow. Given an observation, the VLM backbone is executed once to obtain fused tokens, after which the lightweight action head performs small number of Euler integration steps to generate clean action chunk. The resulting actions are post-processed and executed in receding-horizon fashion, enabling efficient real-time control. 4. Evaluation To empirically validate the effectiveness of SimVLA, we conduct comprehensive evaluation across standard simulation benchmarks and real-world robotic settings. 4.1. Experimental Setup 4.1.1. SIMULATION BENCHMARKS We follow standard evaluation protocols for each simulation benchmark and keep the comparison settings consistent to support fair and reproducible results. LIBERO (Liu et al., 2023). We utilize all four standard suites: LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long, comprising 10 tasks per suite with 500 expert demonstrations each. This serves as our primary testbed to evaluate the models long-horizon consistency and generalization performance. LIBERO-PRO (Zhou et al., 2025). To address the memorization issue in current VLA studies, we evaluate on LIBERO-PRO, robust benchmark that introduces systematic perturbations across four dimensions: object appearance (Obj), spatial layout (Pos), language instructions (Sem), and task goals (Task). SimplerEnv (Li et al., 2024). We evaluate on SimplerFractal (Google Robot) and Simpler-Bridge (WidowX) to assess the models performance in high-fidelity simulated environments. For Simpler-Fractal, we report variant aggregation scores to test the policys robustness against diverse scene variations. For Simpler-Bridge, the evaluation focuses on real-to-sim transfer across tasks. Training Setup. Following recent works (Zheng et al., 2025b; Liang et al., 2025b; Wang et al., 2025a), we train single generalist policy on the union of all standard LIBERO datasets (Spatial, Object, Goal, Long). For LIBERO-PRO, we directly evaluate this policy without additional finetuning to strictly test zero-shot robustness. For SimplerEnv, we train our model on the Fractal (Brohan et al., 2022) and BridgeData-V2 datasets (Walke et al., 2023), strictly following the recent work (Zheng et al., 2025b). Across all benchmarks, we train directly from pretrained VLM backbone, without any robotic data pre-training. Critical Hyperparameters. key finding of our research is that performance is highly sensitive to small set of training and design choices, which can easily overshadow architectural differences if left under-tuned. To make our comparisons transparent, we summarize the concrete settings we tried: Data & representation. We vary the action-chunk horizon {10, 20, 30}. We ablate data shuffling (shuffle vs. no shuffle). We ablate normalization (on/off) for both actions and proprioception; when enabled, we compute per-dimension statistics (mean/std, with robust quantile estimates) on the training set and normalize accordingly. Optimization dynamics. We sweep the learning rate over {5105, 104, 2104, 5104}, warm-up steps over {0, 1000}, and the scheduler over {cosine decay, none}. For all variants, we keep the training budget fixed (batch size and total steps/epochs); the exact budgets are reported in the appendix. Architecture configuration. Action transformer scale: we use two configurations (small vs. large), as in our training scripts, e.g., {768, 12, 12} (80M params) vs. {1024, 24, 16} (300M params) for {hidden size, depth, # heads}. VLM backbone: we compare Florence-2 (0.9B) (Xiao et al., 2024) and SmolVLM-0.5B (Marafioti et al., 2025). Information injection: we compare (i) token concatenation with pure self-attention (default), (ii) crossattention injection, and (iii) conditional AdaLN. Detailed hyperparameters are provided in Appendix A. 5 SimVLA: Simple VLA Baseline for Robotic Manipulation Table 2. Comparison on the LIBERO benchmark. We report the success rate (%) on the official test episodes for each suite (Spatial/Object/Goal/Long), and the overall average across the four suites. Bold denotes the best performance, and Bold denotes the second-best. indicates the backbone scale in billions. Table 3. Robustness evaluation on the LIBERO-PRO benchmark. We report the success rate (%) across five perturbation dimensions: Original (Ori), Object (Obj), Position (Pos), Semantic (Sem), and Task (Task). Bold indicates the best performance. Task Suite Method Ori Obj Pos Sem Task Model Large Models ( 4B) UniVLA (Bu et al., 2025) FlowVLA (Zhong et al., 2025) UnifiedVLA (Wang et al., 2025b) OpenVLA (Kim et al., 2024) OpenVLA-OFT (Kim et al., 2025) DD-VLA (Liang et al., 2025b) MemoryVLA (Shi et al., 2025) PD-VLA (Song et al., 2025a) MolmoAct (Lee et al., 2025) ThinkAct (Huang et al., 2025) CoT-VLA (Zhao et al., 2025) WorldVLA (Cen et al., 2025) TraceVLA (Zheng et al., 2024) FPC-VLA (Yang et al., 2025) 4D-VLA (Zhang et al., 2025a) SpatialVLA (Qu et al., 2025a) Small Models (< 4B) π0 (Black et al., 2024) π0-FAST (Pertsch et al., 2025) π0.5 (Black et al., 2025) NORA (Hung et al., 2025) SmolVLA (Shukor et al., 2025) GR00T-N1 (Bjorck et al., 2025) GraspVLA (Deng et al., 2025) FLOWER (Reuss et al., 2025) Tiny Models (< 1B) X-VLA (Zheng et al., 2025b) VLA-Adapter (Wang et al., 2025a) VLA-OS (Gao et al., 2025) UniAct (Zheng et al., 2025a) SimVLA Spatial Object Goal Long Avg 9 8.5 8.5 7 7 7 7 7 7 7 7 7 7 7 4 4 3 3 3 3 2.2 2 1.8 1 0.9 0.5 0.5 0.5 0. 96.5 93.2 95.4 84.7 97.6 97.2 98.4 95.5 87.0 88.3 87.5 87.6 84.6 86.2 88.9 88.2 96.8 96.4 98.8 92.2 93.0 94.4 - 97.1 96.8 95.0 98.8 88.4 98.4 98.6 98.4 96.7 95.4 91.4 91.6 96.2 85.2 87.0 95.2 89.9 98.8 96.8 98.2 95.4 94.0 97.6 94.1 96.7 95.6 91.6 93.6 79.2 97.9 97.4 96.4 94.9 87.6 87.1 87.6 83.4 75.1 92.0 90.9 78.6 95.8 88.6 98.0 89.4 91.0 93.0 91.2 95. 92.0 72.6 94.0 53.7 94.5 92.0 93.4 91.7 77.2 70.9 69.0 60.0 54.1 82.2 79.1 55.5 85.2 60.2 92.4 74.6 77.0 90.6 82.0 93.5 95.2 88.1 95.5 76.5 97.1 96.3 96.7 94.7 86.6 84.4 81.1 81.8 74.8 86.9 88.6 78.1 94.2 85.5 96.9 87.9 88.8 93.9 89.1 95.7 98.2 97.8 87.0 77.0 99.6 98.6 99.2 96.5 87.0 99.8 97.8 97.2 92.7 77.0 98.6 97.6 95.0 66.0 70.0 96.4 98.1 97.3 85.6 77.8 98.6 Baselines. We compare SimVLA against spectrum of representative policies, ranging from standard baselines to recent complex architectures: RT-1-X / RT-2-X (ONeill et al., 2024), OpenVLA (Kim et al., 2024), Octo-Small / OctoBase (Team et al., 2024), TraceVLA (Zheng et al., 2024), SpatialVLA (Qu et al., 2025b), UnifiedVLA (Wang et al., 2025b), UniVLA (Bu et al., 2025), X-VLA (Zheng et al., 2025b), VLA-Adapter (Wang et al., 2025a), VLA-OS (Gao et al., 2025), UniAct (Zheng et al., 2025a), NORA (Hung et al., 2025), MemoryVLA (Shi et al., 2025), ThinkAct (Huang et al., 2025), CoT-VLA (Zhao et al., 2025), WorldVLA (Cen et al., 2025), SmolVLA (Shukor et al., 2025), MolmoAct (Lee et al., 2025), π0 (Black et al., 2024), π0FAST (Pertsch et al., 2025), π0.5 (Black et al., 2025), DDVLA (Liang et al., 2025b), OpenVLA-OFT (Kim et al., 2025), RoboVLM (Liu et al., 2025), GR00T-N1 (Bjorck et al., 2025), FlowVLA (Zhong et al., 2025), PD-VLA (Song et al., 2025a), FPC-VLA (Yang et al., 2025), 4DVLA (Zhang et al., 2025a), GraspVLA (Deng et al., 2025), FLOWER (Reuss et al., 2025). Baseline results are either sourced directly from the original papers or reproduced using open-source implementations under the identical input modalities described above. 6 Spatial Object Goal Long OpenVLA (Kim et al., 2024) 98.0 97.0 0.0 π0.5 (Black et al., 2025) SimVLA 97.0 98.0 97.0 20.0 97.0 99.0 98.0 29.0 98.0 0.0 1.0 0.0 OpenVLA (Kim et al., 2024) 99.0 98.0 0.0 π0.5 (Black et al., 2025) SimVLA 0.0 98.0 98.0 98.0 17.0 96.0 1.0 100.0 85.0 1.0 100.0 4.0 OpenVLA (Kim et al., 2024) 98.0 96.0 0.0 π0.5 (Black et al., 2025) SimVLA 98.0 97.0 97.0 38.0 97.0 99.0 82.0 0. 0.0 0.0 99.0 10.0 OpenVLA (Kim et al., 2024) 93.0 81.0 0.0 π0.5 (Black et al., 2025) 93.0 92.0 8.0 96.0 61.0 3.0 SimVLA 96.0 0.0 1.0 93.0 98.0 10.0 4.1.2. REAL-ROBOT (GALAXEA R1 LITE) Beyond simulation, we evaluate zero-shot cross-scene generalization on real mobile bimanual robot. We train two policies on the 500 hour Galaxea Open-World Dataset collected with the same embodiment (Jiang et al., 2025): (i) our SimVLA initialized from Florence-2, and (ii) π0.5 baseline initialized from the publicly released π0.5 weights. We then deploy both policies in our own held-out scenes without additional fine-tuning. We evaluate on eight multi-stage manipulation tasks that emphasize dexterous, fine-grained manipulation under diverse scenes and objects: store the dolls, arrange eggs, put the flowers in the vase, put the pen into the pen holder, wipe the desktop, fold the clothes, pick up garbage on the ground, and open the drawer. For each task, we report task success under fixed time budget. Additional dataset/robot details and per-task rubrics are provided in Appendix A. 4.2. Simulation Results Results on LIBERO. Table 2 shows that SimVLA achieves the highest reported average success rate under our matched setup. Despite using compact 0.5B backbone, it surpasses large baselines ( 4B) like OpenVLA-OFT (97.1%) and MemoryVLA (96.7%), validating the efficacy of our simple, well-tuned architecture. SimVLA achieves nearperfect scores on Spatial (99.4%), Object (99.8%), and Goal (98.2%), ranking first overall. On the challenging Long suite, it attains robust 96.4%, demonstrating strong temporal consistency without explicit memory modules. Robustness on LIBERO-PRO. Table 3 presents our evaluation on the rigorous LIBERO-PRO benchmark. SimVLA demonstrates strong zero-shot generalization, with superior Semantic robustness and improved Task robustness, while positional robustness remains challenging. While SimVLA: Simple VLA Baseline for Robotic Manipulation Table 4. Comparison on WidowX robot tasks; success rates (%). Table 5. Comparison on Google Robot tasks; success rates (%). Method Spoon Carrot Stack Eggplant Avg Model (Variant Aggregation) Pick Move Open Avg Large Models ( 4B) RT-1-X (ONeill et al., 2024) Octo-Base (Team et al., 2024) OpenVLA (Kim et al., 2024) OpenVLA-OFT (Kim et al., 2025) DD-VLA (Liang et al., 2025b) RoboVLM (Liu et al., 2025) SpatialVLA (Qu et al., 2025a) MemoryVLA (Shi et al., 2025) ThinkAct (Huang et al., 2025) FPC-VLA (Yang et al., 2025) Small Models (< 4B) Octo-Small (Team et al., 2024) π0 (Black et al., 2024) π0-FAST (Pertsch et al., 2025) GR00T-N1 (Bjorck et al., 2025) FLOWER (Reuss et al., 2025) Tiny Models (< 1B) 0.0 12.5 0.0 12.5 29.2 29.2 16.7 75.0 58.3 58.3 47.2 29.1 29.1 62.5 71.0 4.2 8.3 0.0 4.2 29.2 25.0 25.0 75.0 37.5 45. 9.7 0.0 21.9 45.8 13.0 0.0 0.0 0.0 8.3 20.8 12.5 29.2 37.5 8.7 79.2 4.2 16.7 10.8 16.7 8.0 0.0 43.1 4.1 37.5 70.8 58.3 100.0 100.0 70.8 75.0 56.9 62.5 66.6 20.8 88.0 6.3 31.3 7.8 39.6 54.2 38.5 42.7 71.9 43.8 64. 43.9 40.1 48.3 49.5 45.0 X-VLA (Zheng et al., 2025b) SimVLA 100 100 91.7 91.7 95.8 91.7 95.8 95.8 95.8 baselines like OpenVLA and π0.5 collapse to near-zero performance on Task-level perturbations (indicating reliance on trajectory memorization), SimVLA reaches 10.0% success on both Goal and Long suites. SimVLA also ranks first across all suites on Semantic robustness (around 98 100% success). In contrast, position robustness is high only on Spatial (29.0%) and remains low on Object/Goal/Long, highlighting key direction for future work. Results on WidowX. As detailed in Table 4, SimVLA achieves state-of-the-art performance with an overall average of 95.8%, effectively tying with the heavily pre-trained X-VLA. Despite strictly adhering to no pre-training protocol, SimVLA matches X-VLAs average success rate and even secures perfect scores (100%) on the Put Spoon on Towel and Put Eggplant in Basket tasks. This result is particularly significant as SimVLA outperforms large-scale baselines by substantial marginssurpassing MemoryVLA (71.9%) and FPC-VLA (64.6%). Results on Google Robot. In the Google Robot variant aggregation tasks shown in Table 5, SimVLA achieves an average success rate of 76.1%, outperforming strong baselines such as SpatialVLA (67.5%), RT-2-X (65.6%), and ThinkAct (65.1%). SimVLA is also slightly higher than XVLA (75.7%) on the overall average. Together, these results suggest that simple, data-efficient baseline can remain competitive on challenging benchmarks without relying on extensive robotic pre-training. 4.3. Ablation Studies To examine which parts of our training recipe matter in practice (Sec. 3.3), we conduct controlled ablations on LIBERO by varying one knob at time while keeping the remaining Large Models ( 4B) RT-1-X (ONeill et al., 2024) RT-2-X (ONeill et al., 2024) Octo-Base (Team et al., 2024) OpenVLA (Kim et al., 2024) OpenVLA-OFT (Kim et al., 2025) RoboVLM (Liu et al., 2025) TraceVLA (Zheng et al., 2024) DD-VLA (Liang et al., 2025b) SpatialVLA (Qu et al., 2025a) ThinkAct (Huang et al., 2025) Small Models (< 4B) π0 (Black et al., 2024) π0-FAST (Pertsch et al., 2025) GR00T-N1 (Bjorck et al., 2025) Tiny Models (< 1B) 49.0 82.3 0.6 54.5 65.3 75.6 60.0 82.5 88.0 84. 75.2 77.6 78.8 32.3 79.2 3.1 47.7 59.0 60.0 56.4 64.6 72.7 63.8 63.7 68.2 62.5 29.4 35.3 1.1 17.7 12.2 10.6 31.0 23.6 41.8 47.6 25.6 31.3 13.2 36.9 65.6 1.6 40.0 45.5 48.7 49.1 56.9 67.5 65. 54.8 59.0 51.5 X-VLA (Zheng et al., 2025b) SimVLA 85.5 87.4 79.8 65.2 61.9 75.9 75.7 76. settings fixed  (Table 6)  . Overall, we find that several implementation details are indispensable: changing single knob can lead to substantial performance drop, often larger than the gains attributed to architectural changes. Key Findings. Table 6 highlights few dominant knobs that largely determine performance. Data shuffling and normalization are critical. Disabling either shuffling or action normalization causes near-collapse in performance, suggesting that stable optimization and consistent action scaling are prerequisites for strong baseline. Optimization dynamics dominate. The learning rate must be tuned: too large (5 104) degrades sharply, while too small (5 105) also underperforms. Likewise, removing the small VLM learning-rate multiplier (setting it to 1.0) hurts substantially, indicating that preserving the pretrained backbone while adapting the action head is important for stable end-to-end training. Some architecture choices matter, but are secondary to the above. Scaling down the action head (largesmall) only slightly reduces performance, whereas alternative conditioning mechanisms (AdaLN / cross-attention) are noticeably worse than simple token concatenation under our setup. Switching the VLM backbone to Florence-2 remains competitive, consistent with the modular VLM encoder + action head design. 4.4. Real-robot Results In the following section, we report real-robot evaluation results on Galaxea R1 Lite under the zero-shot, cross-scene protocol described above. We compare our SimVLA against π0.5 baseline on eight tasks: store the dolls, arrange eggs, 7 Table 6. Ablations on LIBERO. Each row corresponds to one ablation setting with the remaining knobs fixed to the default configuration. SimVLA: Simple VLA Baseline for Robotic Manipulation Knob SimVLA Value Spatial Object Goal Long Default settings 99.4 99.8 98. 96.4 Data & representation (default: H=10, shuffling on, normalization on) Action chunk horizon = 20 = 30 Data shuffling Action normalization off off 99.2 95.4 6.2 22.6 89.6 93. 0.0 3.2 92.4 80.6 13.6 23.2 88.4 79. 0.0 0.0 Avg 98.6 92.4 87.3 9. 12.3 Optimization dynamics (default: lr 2104, warm-up none, scheduler none, VLM LR multiplier 0.1) Learning rate Warm-up steps Scheduler 5105 1104 510 1000 cosine VLM LR multiplier 1.0 98.0 99.6 84.4 97. 99.2 41.2 97.6 98.2 91.8 99.6 99.4 80. 96.2 98.4 76.0 70.4 85.6 38.4 96.4 93.8 98.4 93. 46.4 8.2 90.6 95.5 72.7 96.8 97.5 44. Architecture configuration (default: large head (1024,24,16), concat injection, SmolVLM-0.5B) Action transformer scale small (768,12,12) Condition injection conditional AdaLN cross-attention VLM backbone Florence-2 98.8 99.2 98.4 99.8 99.6 98.0 96. 99.2 98.6 94.8 96.6 94.8 70.4 76.2 98. 93.8 98.0 91.1 91.5 97.7 put the flowers in the vase, put the pen into the pen holder, wipe the desktop, fold the clothes, pick up garbage on the ground, and open the drawer. Fig. 1 shows example outof-box deployments of SimVLA in our held-out real-world scenes. Results on Real Robot. Overall, SimVLA achieves performance that is broadly comparable to π0.5 under the same zero-shot protocol  (Fig. 3)  . Beyond fold the clothes, put the pen into the pen holder and put the flowers in the vase, which remain challenging, the other tasks are typically around 80% success. Notably, our SimVLA is trained end-to-end directly from pretrained VLM backbone (without any VLA/robotdata pre-training), whereas the π0.5 baseline uses its publicly released initialization. 5. Conclusion In this work, we introduced SimVLA, minimalist VLA baseline designed to address the challenge of performance attribution in the rapidly evolving VLA landscape. By strictly decoupling perception from control and adhering to standardized training recipe, we demonstrated that small backbone can rival or even outperform multi-billionFigure 3. Real-robot zero-shot results on Galaxea R1 Lite. parameter SOTA baselines. Our extensive evaluations on the simulation benchmarks and real-world robotic tasks confirm that SimVLA achieves superior performance and scene generalization while maintaining low memory footprint. Crucially, our findings highlight that silent implementation detailssuch as data shuffling strategies, action normalization, and optimization dynamicsare often as influential as architectural novelties. By isolating these factors, SimVLA provides the community with reproducible reference point. We hope that this baseline enables researchers to more rigorously quantify the added value of future architectural innovations. SimVLA: Simple VLA Baseline for Robotic Manipulation"
        },
        {
            "title": "Impact Statement",
            "content": "This work advances research on Vision-Language-Action (VLA) models for robotic manipulation by providing transparent and reproducible baseline. By standardizing training dynamics and simplifying architectural design, our work may facilitate fairer comparisons and more reliable scientific progress in embodied AI research. Potential positive impacts include improved accessibility to robotic manipulation systems, more efficient deployment of assistive robots in domestic or industrial settings, and reduced computational overhead compared to larger models. At the same time, advances in general-purpose robotic manipulation may introduce societal risks, including unintended automation of human labor, misuse in unsafe environments, or deployment without adequate safety validation. We emphasize that SimVLA is evaluated in controlled research settings, and real-world deployment should incorporate appropriate safety constraints, human oversight, and domain-specific validation."
        },
        {
            "title": "References",
            "content": "Bjorck, J., Castaneda, F., Cherniadev, N., Da, X., Ding, R., Fan, L., Fang, Y., Fox, D., Hu, F., Huang, S., et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. Black, K., Brown, N., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., Groom, L., Hausman, K., Ichter, B., Jakubczak, S., Jones, T., Ke, L., Levine, S., Li-Bell, A., Mothukuri, M., Nair, S., Pertsch, K., Shi, L. X., Tanner, J., Vuong, Q., Walling, A., Wang, H., and Zhilinsky, U. π0: Vision-Language-Action Flow Model for General Robot Control. arXiv preprint arXiv:2410.24164, 2024. Black, K., Brown, N., Darpinian, J., Dhabalia, K., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., Galliker, M. Y., Ghosh, D., Groom, L., Hausman, K., Ichter, B., Jakubczak, S., Jones, T., Ke, L., LeBlanc, D., Levine, S., Li-Bell, A., Mothukuri, M., Nair, S., Pertsch, K., Ren, A. Z., Shi, L. X., Smith, L., Springenberg, J. T., Stachowicz, K., Tanner, J., Vuong, Q., Walke, H., Walling, A., Wang, H., Yu, L., and Zhilinsky, U. π0.5: VisionLanguage-Action Model with Open-World Generalization. arXiv preprint arXiv:2504.16054, 2025. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Bu, Q., Yang, Y., Cai, J., Gao, S., Ren, G., Yao, M., Luo, P., and Li, H. Univla: Learning to act anywhere with taskcentric latent actions. arXiv preprint arXiv:2505.06111, 2025. Cen, J., Yu, C., Yuan, H., Jiang, Y., Huang, S., Guo, J., Li, X., Song, Y., Luo, H., Wang, F., Zhao, D., and Chen, H. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539, 2025. Chen, Y., Gu, K., Wen, Y., Zhao, Y., Wang, T., and Nie, L. Intentionvla: Generalizable and efficient embodied intention reasoning for human-robot interaction. arXiv preprint arXiv:2510.07778, 2025. Chi, C., Feng, S., Du, Y., Xu, Z., Cousineau, E., Burchfiel, B., and Song, S. Diffusion policy: Visuomotor policy learning via action diffusion. In Proceedings of Robotics: Science and Systems (RSS), 2023. Deng, S., Yan, M., Wei, S., Ma, H., Yang, Y., Chen, J., Zhang, Z., Yang, T., Zhang, X., Zhang, W., Cui, H., Zhang, Z., and Wang, H. Graspvla: grasping foundation model pre-trained on billion-scale synthetic action data. arXiv preprint arXiv:2505.03233, 2025. Gao, C., Liu, Z., Chi, Z., Huang, J., Fei, X., Hou, Y., Zhang, Y., Lin, Y., Fang, Z., Jiang, Z., and Shao, L. Vlaos: Structuring and dissecting planning representations and paradigms in vision-language-action models. arXiv preprint arXiv:2506.17561, 2025. Huang, C.-P., Wu, Y.-H., Chen, M.-H., Wang, Y.-C. F., and Yang, F.-E. Thinkact: Vision-language-action reasoning via reinforced visual latent planning. arXiv preprint arXiv:2507.16815, 2025. Hung, C.-Y., Sun, Q., Hong, P., Zadeh, A., Li, C., Tan, U.-X., Majumder, N., and Poria, S. Nora: small open-sourced generalist vision language action model for embodied tasks. arXiv preprint arXiv:2504.19854, 2025. Jiang, T., Yuan, T., Liu, Y., Lu, C., Cui, J., Liu, X., Cheng, S., Gao, J., Xu, H., and Zhao, H. Galaxea open-world dataset and g0 dual-system vla model. arXiv preprint arXiv:2509.00576, 2025. Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E., Lam, G., Sanketi, P., Vuong, Q., Kollar, T., Burchfiel, B., Tedrake, R., Sadigh, D., Levine, S., Liang, P., and Finn, C. Openvla: An open-source Vision-Language-Action model. arXiv preprint arXiv:2406.09246, 2024. Kim, M. J., Finn, C., and Liang, P. Fine-tuning visionlanguage-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. 9 SimVLA: Simple VLA Baseline for Robotic Manipulation Koo, M., Choi, D., Kim, T., Lee, K., Kim, C., Seo, Y., and Shin, J. Hamlet: Switch your vision-languageaction model into history-aware policy. arXiv preprint arXiv:2510.00695, 2025. Pertsch, K., Stachowicz, K., Ichter, B., Driess, D., Nair, S., Vuong, Q., Mees, O., Finn, C., and Levine, S. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. Lee, J., Duan, J., Fang, H., Deng, Y., Liu, S., Li, B., Fang, B., Zhang, J., Wang, Y. R., Lee, S., Han, W., Pumacay, W., Wu, A., Hendrix, R., Farley, K., VanderBilt, E., Farhadi, A., Fox, D., and Krishna, R. Molmoact: Action reasoning models that can reason in space. arXiv preprint arXiv:2508.07917, 2025. Li, X., Hsu, K., Gu, J., Mees, O., Pertsch, K., Walke, H. R., Fu, C., Lunawat, I., Sieh, I., Kirmani, S., Levine, S., Wu, J., Finn, C., Su, H., Vuong, Q., and Xiao, T. Evaluating real-world robot manipulation policies in simulation. In 8th Annual Conference on Robot Learning, 2024. URL https://openreview.net/forum?id=LZh48DTg71. Qu, D., Song, H., Chen, Q., Yao, Y., Ye, X., Ding, Y., Wang, Z., Gu, J., Zhao, B., Wang, D., and Li, X. Spatialvla: Exploring spatial representations for visual-language-action model, 2025a. URL https://arxiv.org/abs/2501.15830. Qu, D., Song, H., Chen, Q., Yao, Y., Ye, X., Ding, Y., Wang, Z., Gu, J., Zhao, B., Wang, D., and Li, X. Spatialvla: Exploring spatial representations for Vision-LanguageAction model. arXiv preprint arXiv:2501.15830, 2025b. Reuss, M., Zhou, H., Ruhle, M., Omer Erdinc Yagmurlu, Otto, F., and Lioutikov, R. Flower: Democratizing generalist robot policies with efficient vision-language-action flow policies. arXiv preprint arXiv:2509.04996, 2025. Liang, W., Sun, G., He, Y., Dong, J., Dai, S., Laptev, I., Khan, S., and Cong, Y. Pixelvla: Advancing pixel-level understanding in vision-language-action model. arXiv preprint arXiv:2511.01571, 2025a. Sapkota, R., Cao, Y., Roumeliotis, K. I., and Karkee, M. Vision-language-action models: Concepts, progress, applications and challenges, 2025. URL https://arxiv.org/ abs/2505.04769. Liang, Z., Li, Y., Yang, T., Wu, C., Mao, S., Nian, T., Pei, L., Zhou, S., Yang, X., Pang, J., Mu, Y., and Luo, P. Discrete diffusion vla: Bringing discrete diffusion to action decoding in vision-language-action policies. arXiv preprint arXiv:2508.20072, 2025b. Shi, H., Xie, B., Liu, Y., Sun, L., Liu, F., Wang, T., Zhou, E., Fan, H., Zhang, X., and Huang, G. Memoryvla: Perceptual-cognitive memory in vision-languageaction models for robotic manipulation. arXiv preprint arXiv:2508.19236, 2025. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Liu, B., Zhu, Y., Gao, C., Feng, Y., Liu, Q., Zhu, Y., and Stone, P. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:4477644791, 2023. Liu, H., Li, X., Li, P., Liu, M., Wang, D., Liu, J., Kang, B., Ma, X., Kong, T., and Zhang, H. Towards generalist robot policies: What matters in building vision-language-action models. 2025. Marafioti, A., Zohar, O., Farre, M., Noyan, M., Bakouch, E., Cuenca, P., Zakka, C., Allal, L. B., Lozhkov, A., Tazi, N., et al. Smolvlm: Redefining small and efficient multimodal models. arXiv preprint arXiv:2504.05299, 2025. ONeill, A., Rehman, A., Maddukuri, A., Gupta, A., Padalkar, A., Lee, A., Pooley, A., Gupta, A., Mandlekar, A., Jain, A., et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 68926903. IEEE, 2024. Shukor, M., Aubakirova, D., Capuano, F., Kooijmans, P., Palma, S., Zouitine, A., Aractingi, M., Pascal, C., Russi, M., Marafioti, A., Alibert, S., Cord, M., Wolf, T., and Cadene, R. Smolvla: vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. Song, W., Chen, J., Ding, P., Zhao, H., Zhao, W., Zhong, Z., Ge, Z., Ma, J., and Li, H. Accelerating vision-languageaction model integrated with action chunking via parallel decoding. arXiv preprint arXiv:2503.02310, 2025a. Song, W., Zhou, Z., Zhao, H., Chen, J., Ding, P., Yan, H., Huang, Y., Tang, F., Wang, D., and Li, H. Reconvla: Reconstructive vision-language-action model as effective robot perceiver. arXiv preprint arXiv:2508.10333, 2025b. Sun, L., Xie, B., Liu, Y., Shi, H., Wang, T., and Cao, J. Geovla: Empowering 3d representations arXiv preprint in vision-language-action models. arXiv:2508.09071, 2025. Team, O. M., Ghosh, D., Walke, H., Pertsch, K., Black, K., Mees, O., Dasari, S., Hejna, J., Kreiman, T., Xu, C., Luo, J., Tan, Y. L., Sanketi, P. R., Vuong, Q., Xiao, T., Sadigh, D., Finn, C., and Levine, S. Octo: An opensource generalist robot policy. ArXiv, abs/2405.12213, 2024. 10 SimVLA: Simple VLA Baseline for Robotic Manipulation Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017. Walke, H. R., Black, K., Zhao, T. Z., Vuong, Q., Zheng, C., Hansen-Estruch, P., He, A. W., Myers, V., Kim, M. J., Du, M., et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pp. 17231736. PMLR, 2023. Wang, Y., Ding, P., Li, L., Cui, C., Ge, Z., Tong, X., Song, W., Zhao, H., Zhao, W., Hou, P., Huang, S., Tang, Y., Wang, W., Zhang, R., Liu, J., and Wang, D. Vla-adapter: An effective paradigm for tiny-scale vision-languageaction model. arXiv preprint arXiv:2509.09372, 2025a. Wang, Y., Li, X., Wang, W., Zhang, J., Li, Y., Chen, Y., Wang, X., and Zhang, Z. Unified vision-language-action model. arXiv preprint arXiv:2506.19850, 2025b. in robotic manipulation: survey, 2025b. URL https: //arxiv.org/abs/2503.03464. Zhang, W., Liu, H., Qi, Z., Wang, Y., Yu, X., Zhang, J., Dong, R., He, J., Lu, F., Wang, H., Zhang, Z., Yi, L., Zeng, W., and Jin, X. Dreamvla: vision-language-action model dreamed with comprehensive world knowledge. arXiv preprint arXiv:2507.04447, 2025c. Zhang, Z., Li, H., Dai, Y., Zhu, Z., Zhou, L., Liu, C., Wang, D., Tay, F. E. H., Chen, S., Liu, Z., Liu, Y., Li, X., and Zhou, P. From spatial to actions: Grounding visionlanguage-action model in spatial foundation priors. arXiv preprint arXiv:2510.17439, 2025d. Zhao, Q., Lu, Y., Kim, M. J., Fu, Z., Zhang, Z., Wu, Y., Li, Z., Ma, Q., Han, S., Finn, C., Handa, A., Liu, M.-Y., Xiang, D., Wetzstein, G., and Lin, T.-Y. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. CVPR 2025, 2025. arXiv:2503.22020. Wang, Y., Zhu, H., Liu, M., Yang, J., Fang, H.-S., and He, T. Vq-vla: Improving vision-language-action models via scaling vector-quantized action tokenizers. arXiv preprint arXiv:2507.01016, 2025c. Zheng, J., Li, J., Liu, D., Zheng, Y., Wang, Z., Ou, Z., Liu, Y., Liu, J., Zhang, Y.-Q., and Zhan, X. Universal actions for enhanced embodied foundation models. arXiv preprint arXiv:2501.10105, 2025a. Xiao, B., Wu, H., Xu, W., Dai, X., Hu, H., Lu, Y., Zeng, M., Liu, C., and Yuan, L. Florence-2: Advancing unified representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 48184829, 2024. Xu, C., Zhang, S., Liu, Y., Sun, B., Chen, W., Xu, B., Liu, Q., Wang, J., Wang, S., Luo, S., Peters, J., Vasilakos, A. V., Zafeiriou, S., and Deng, J. An anatomy of visionlanguage-action models: From modules to milestones and challenges, 2025. URL https://arxiv.org/abs/2512.11362. Zheng, J., Li, J., Wang, Z., Liu, D., Kang, X., Feng, Y., Zheng, Y., Zou, J., Chen, Y., Zeng, J., Zhang, Y.-Q., Pang, J., Liu, J., Wang, T., and Zhan, X. X-vla: Soft-prompted transformer as scalable crossarXiv embodiment vision-language-action model. preprint arXiv:2510.10274, 2025b. Zheng, R., Liang, Y., Huang, S., Gao, J., Daume, H., Kolobov, A., Huang, F., and Yang, J. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. ArXiv, abs/2412.10345, 2024. Yang, Y., Duan, Z., Xie, T., Cao, F., Shen, P., Song, P., Jin, P., Sun, G., Xu, S., You, Y., and Liu, J. Fpc-vla: vision-language-action framework with supervisor for failure prediction and correction. arXiv preprint arXiv:2509.04018, 2025. Zhong, Z., Yan, H., Li, J., Liu, X., Gong, X., Zhang, T., Song, W., Chen, J., Zheng, X., Wang, H., and Li, H. Flowvla: Visual chain of thought-based motion reasoning for vision-language-action models. arXiv preprint arXiv:2508.18269, 2025. Yuan, T., Liu, Y., Lu, C., Chen, Z., Jiang, T., and Zhao, H. Depthvla: Enhancing vision-language-action models with depth-aware spatial reasoning. arXiv preprint arXiv:2510.13375, 2025. Zhou, X., Xu, Y., Tie, G., Chen, Y., Zhang, G., Chu, D., Zhou, P., and Sun, L. Libero-pro: Towards robust and fair evaluation of vision-language-action models beyond memorization. arXiv preprint arXiv:2510.03827, 2025. Zhang, J., Chen, Y., Xu, Y., Huang, Z., Zhou, Y., Yuan, Y.-J., Cai, X., Huang, G., Quan, X., Xu, H., and Zhang, L. 4d-vla: Spatiotemporal vision-language-action pretraining with cross-scene calibration. arXiv preprint arXiv:2506.22242, 2025a. Zhang, K., Yun, P., Cen, J., Cai, J., Zhu, D., Yuan, H., Zhao, C., Feng, T., Wang, M. Y., Chen, Q., Pan, J., Zhang, W., Yang, B., and Chen, H. Generative artificial intelligence 11 SimVLA: Simple VLA Baseline for Robotic Manipulation A. Datasets and Experimental Details A.1. Galaxea Open-World Dataset We train on the Galaxea Open-World Dataset (Jiang et al., 2025), large-scale real-world mobile-manipulation dataset with 500 hours of demonstrations (about 100K trajectories). The dataset spans 150 task categories across 50 real-world scenes and covers more than 1,600 objects and 58 skills. Importantly for our setting, data are collected under single consistent embodiment, so that perception streams, action/state signals, and language annotations are naturally aligned for end-to-end VLA training. Platform. All demonstrations are recorded on the Galaxea R1 Lite platform, which is also the real robot we use for our zero-shot evaluation in Sec. 4. R1 Lite is mobile bimanual robot with 23-DoF embodiment (two 6-DoF arms, 3-DoF torso for workspace extension, 6-DoF vector-drive omnidirectional base up to 1.5 m/s, and two 1-DoF grippers). The platform is equipped with head stereo RGB camera for scene-level context and dual Intel RealSense D405 RGB-D wrist cameras for close-range manipulation. A.2. Real-robot evaluation details For each real-robot task, we use simple rubric that measures whether the policy can complete the task within the fixed time budget used in Sec. 4. We report binary success/failure over 50 trials. Store the dolls. The robot must pick up three plush dolls placed on tabletop and put them into designated storage container. Success is defined as all three dolls being fully inside the container at the end of the episode. Arrange eggs. The robot must pick up one egg and place it into designated slot of an egg carton. Success is defined as the egg being seated in the target slot without being dropped. Put the flowers in the vase. The robot must grasp flower and insert it into the opening of vase. Success is defined as the flower being inside the vase and remaining stably placed at the end of the episode. Put the pen into the pen holder. The robot must pick up two pens and insert them into pen holder. Success is defined as both pens being inside the holder (stably contained) at the end of the episode. Wipe the desktop. The robot must wipe visible stain/dirty region on desktop using cloth rag. Success is defined as the target region being visibly cleaned (stain disappears or is substantially reduced) within the time budget. Fold the clothes. The robot must fold shirt starting from flat or mildly wrinkled state on the table. Success is defined as completing the target fold (folding in sleeves and making main fold) with the garment remaining on the working surface. Pick up garbage on the ground. The robot must pick up two plastic bottles from the ground and dispose them into trash bin. Success is defined as both bottles ending inside the bin. Open the drawer. The robot must grasp the drawer handle and pull to open the drawer to specified extent. Success is defined as the drawer being opened beyond threshold without damaging the mechanism. A.3. Training hyperparameters To facilitate reproducibility, we summarize the key hyperparameters used in our simulation training (LIBERO) and real-robot training (Galaxea-500h). All simulation runs are executed on 4H100 GPUs and are initialized from pretrained VLM backbone (without any VLA data pretraining). For complete implementation details, please refer to our codebase on the project website. Simulation. Table 7 reports the configuration. 12 SimVLA: Simple VLA Baseline for Robotic Manipulation Table 7. Hyperparameters for simulation datasets training. Configuration Libero WidowX Google Robot 64 644 = 256 Large (1024,24,16) Batch size (per GPU) Global batch size Action head Action chunk horizon 10 Image resize Action normalization Data shuffling Optimizer Betas Weight decay Learning rate VLM LR multiplier Warm-up steps Scheduler Training steps Precision 128128 on on AdamW (0.9, 0.95) 0.0 2e-4 0.1 0 none 150K bfloat16 80 804 = 320 Large (1024,24,16) 30 224224 on on AdamW (0.9, 0.95) 0.0 1e-4 0.1 0 none 50K bfloat 80 804 = 320 Large (1024,24,16) 30 224224 on on AdamW (0.9, 0.95) 0.0 1e-4 0.1 0 none 150K bfloat16 Real-robot training (Galaxea-500h). Table 8 summarizes the training setup on the Galaxea Open-World Dataset. For fair comparison, both our method and π0.5 are trained using the same Galaxea data and the same hyperparameters, with the same compute budget: 64H100 GPUs for 150K training steps. Table 8. Hyperparameters for Galaxea-500h training."
        },
        {
            "title": "Value",
            "content": "32 3264=2048 Large (1024,24,16) Batch size (per GPU) Global batch size Action head Action chunk horizon 30 Image resize Action normalization Data shuffling Optimizer Betas Weight decay Learning rate VLM LR multiplier Warm-up steps Scheduler Training steps Precision 224224 on on AdamW (0.9, 0.95) 0.0 1e-4 0.1 1000 none 150K bfloat"
        }
    ],
    "affiliations": []
}