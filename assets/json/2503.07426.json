{
    "paper_title": "RePO: ReLU-based Preference Optimization",
    "authors": [
        "Junkang Wu",
        "Kexin Huang",
        "Xue Wang",
        "Jinyang Gao",
        "Bolin Ding",
        "Jiancan Wu",
        "Xiangnan He",
        "Xiang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Aligning large language models (LLMs) with human preferences is critical for real-world deployment, yet existing methods like RLHF face computational and stability challenges. While DPO establishes an offline paradigm with single hyperparameter $\\beta$, subsequent methods like SimPO reintroduce complexity through dual parameters ($\\beta$, $\\gamma$). We propose {ReLU-based Preference Optimization (RePO)}, a streamlined algorithm that eliminates $\\beta$ via two advances: (1) retaining SimPO's reference-free margins but removing $\\beta$ through gradient analysis, and (2) adopting a ReLU-based max-margin loss that naturally filters trivial pairs. Theoretically, RePO is characterized as SimPO's limiting case ($\\beta \\to \\infty$), where the logistic weighting collapses to binary thresholding, forming a convex envelope of the 0-1 loss. Empirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO and SimPO across multiple base models, requiring only one hyperparameter to tune."
        },
        {
            "title": "Start",
            "content": "RePO: ReLU-based Preference Optimization Junkang Wu 1 Kexin Huang 1 Xue Wang 2 Jinyang Gao 2 Bolin Ding 2 Jiancan Wu 1 Xiangnan He 1 Xiang Wang 1 5 2 0 2 0 1 ] . [ 1 6 2 4 7 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Aligning large language models (LLMs) with human preferences is critical for real-world deployment, yet existing methods like RLHF face computational and stability challenges. While DPO establishes an offline paradigm with single hyperparameter β, subsequent methods like SimPO reintroduce complexity through dual parameters (β, γ). We propose ReLU-based Preference Optimization (RePO), streamlined algorithm that eliminates β via two advances: (1) retaining SimPOs reference-free margins but removing β through gradient analysis, and (2) adopting ReLU-based max-margin loss that naturally filters trivial pairs. Theoretically, RePO is characterized as SimPOs limiting case (β ), where the logistic weighting collapses to binary thresholding, forming convex envelope of the 0-1 loss. Empirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO and SimPO across multiple base models, requiring only one hyperparameter to tune. 1. Introduction Aligning large language models (LLMs) with human preferences is essential for their effective deployment in realworld applications (Anil et al., 2023; Touvron et al., 2023; OpenAI, 2023; Bubeck et al., 2023), ensuring that their outputs are beneficial, accurate, and adhere to human values (Ouyang et al., 2022) while minimizing potential safety risks. primary approach for achieving this alignment is Reinforcement Learning from Human Feedback (RLHF). It typically entails multi-stage process: initially, reward model is trained using ranked preference data; subsequently, the LLM is optimized with the guidance of reward model, using reinforcement learning algorithms such as Proximal Work done during internship at Alibaba 1University of Science and Technology of China, Hefei, China 2Alibaba Group, Hangzhou, China. Correspondence to: Xiang Wang <xiangwang1223@gmail.com>. Preprint. Policy Optimization (PPO) (Schulman et al., 2017). Despite its effectiveness, RLHF faces significant challenges, such as high computational costs and training instability (Rafailov et al., 2023; Zhao et al., 2023). Recent research has been exploring simpler offline alternatives. Direct Preference Optimization (DPO) (Rafailov et al., 2023) is one such approach. It reparameterizes the reward function in RLHF, bypassing the explicit learning of the reward model, and thus enabling direct training of LLMs with human preference data. Formally, the DPO objective is logistic-log loss, which applies log-sigmoid activation to the reward margin between the preferred and less-preferred responses to the same input. Here, the reward is derived from comparing the policy model of the LLM with reference model. Building on this, follow-up studies, especially SLiC-HF (Zhao et al., 2023) and SimPO (Meng et al., 2024), focus on further simplifications on the loss function and reward margins separately. Specifically, SLiC-HF replaces the logistic-log loss in DPO with hinge loss, achieving faster-decaying tail. SimPO, on the other hand, introduces reference-free reward margins with logistic-log loss, offering more streamlined and efficient approach. While advancing the field and achieving leading performance, they introduce additional complexity on hyperparameter tuning. SimPO inherits DPOs reward margin strength controller β and adds target reward margin γ. Similarly, SLiC-HF utilizes γ, but also introduces regularization weight λ. The necessity of dual-tuning these hyperparameters poses significant challenges in model training and deployment. This leads to natural question: Can we develop simpler offline preference optimization algorithm? In this paper, we propose simple idea: simultaneously simplifying the log-sigmoid activation and reward margins, to achieve comparable or even better performance. For reward margins, we adopt the reference-free approach of SimPO but eliminate the hyperparameter β. For the logsigmoid activation, we employ ReLU to the reference-free reward margins, following max-margin (support vector machine) paradigm (Boser et al., 1992; Cortes & Vapnik, 1995), similar to SLiC-HF. Consequently, our method can be interpreted as SimPO without β and SLiC-HF without RePO: ReLU-based Preference Optimization Figure 1. RePO, SimPO, and DPO primarily differ in their loss functions, as highlighted in the shaded box. RePO either outperforms or achieves comparable performance to DPO and SimPO across various settings on AlpacaEval 2, with only single hyperparameter γ. IT denotes pre-trained instruction-tuned models. e.g., Gemma2-IT 9B refers to Gemma2-9B (instruct setup). λ. Interestingly, it bridges these methods by omitting core component from each while maintaining competitive performance, demonstrating the effectiveness of this streamlined approach. We term this ReLU-based Preference Optimization RePO. Theoretically, we demonstrate that RePO emerges as the natural endpoint of SimPOs architectural evolution: when β , SimPOs logistic weighting collapses to RePOs binary thresholding (Lemma 3.1). This asymptotic behavior motivates our key theoretical insight the ReLU loss in RePO is precisely the convex envelope of the 0-1 loss (Theorem 4.2), inheriting its global optimality while enabling tractable gradient-based optimization. Practically, this formulation provides the following advantages: Simplified Hyperparameter Tuning: RePO removes the hyperparameter β, requiring only the tuning of γ within the range [0, 1]. Empirical results show that RePO can achieve performance comparable to SimPO on three models (Llama, Mistral, and Gemma) even with fixed γ = 0.5, significantly reducing the complexity of hyperparameter tuning. Effective Data Filtering: By using ReLU instead of continuous sigmoid function, RePO interprets the target reward margin γ as strict cutoff point. This mechanism zeroes out gradients for preference pairs whose reward margin exceeds γ (well-separated pairs), while assigning unit weight to less-separated pairs. This selective approach minimizes overfitting by focusing optimization on challenging data points. Controllable Over-Optimization: The cutoff point γ controls the mean of reward margin within batch, representing new metric for evaluating over-optimization. Experimental results suggest that this metric correlates with model behavior and can replace KL divergence as simpler alternative, potentially controlling optimization without requiring reference model. While RePO effectively filters out well-separated pairs by zeroing their gradients, it assigns unit gradient weight to all less-separated pairs. This implicit uniform weighting overlooks the varying importance of these pairs, potentially limiting its ability to prioritize more informative examples. To address this limitation, we extend the concept of RePO within SimPO and propose new method, RePO++, by leveraging SimPOs logistic-log loss to differentiate the importance of less-separated pairs. This approach assigns greater weights to less-separated pairs, while continuing to filter out well-separated pairs. Notably, only single line of code is required to seamlessly integrate RePO into SimPO, resulting in RePO++. In nutshell, our main contribution is ReLU-based Preference Optimization (RePO), simple but effective reference-free algorithm for training language models from preferences. With its straightforward motivation, RePO is grounded in principled theoretical framework and consistently outperforms DPO and SimPO across three base model settings (Mistral2-7B, Llama3-8B, and Gemma2-9B) on AlpacaEval 2 (cf. Figure 1). We hope our exploration will motivate people to rethink the fundamental roles of loss function design for preference optimization. 2. Preliminaries Offline Alignment. In the offline alignment setting, we utilize dataset = {(x, yw, yl)}, where each entry consists of prompt accompanied by two responses: yw is the preferred (winning) response, and yl is the less preferred (losing) response, both generated by reference policy πref. The primary challenge is to find latent reward function r(x, y) that governs these preference relationships. Al2 RePO: ReLU-based Preference Optimization though r(x, y) is not directly observable, the Bradley-Terry (BT) model (Bradley & Terry, 1952) provides probabilistic framework for modeling the probability of yw over yl given x: P(yw yl x) = exp(r(x, yw)) exp(r(x, yw)) + exp(r(x, yl)) . (1) Here, r(x, y) represents the latent reward for response in the context of prompt x. The aim of offline alignment is to develop policy πθ capable of accurately estimating this latent reward function using the dataset D. Reinforcement Learning from Human Feedback (RLHF). Traditional offline alignment methods often leverage reinforcement learning augmented with KLdivergence regularization term in the reward objective (Bai et al., 2022; Ziegler et al., 2019; Ouyang et al., 2022). The optimization objective, parameterized by regularization parameter β > 0, is expressed as: max πθ ExD,yπθ(yx)[rϕ(x, y)]βDKL(πθ(y x)πref(y x)), (2) where rϕ(x, y) represents the reward function trained using the BT model on the preference data, πθ is the policy being optimized, and πref is the fixed reference policy, typically derived from supervised fine-tuning. The KL-divergence term ensures that πθ remains close to πref, mitigating risks of excessive divergence during training. (cid:16) (cid:17)(cid:17) divergence from the SFT policy, avoiding the need for an explicit KL penalty. The SLiC-HF loss function is defined as: LSLiC-HF(πθ) =E(x,yw,yl)D (cid:104)"
        },
        {
            "title": "ReLU",
            "content": "(cid:16) log πθ(yw x) . log πθ(yl x) γ (cid:105) λ log πθ(yw x) (5) Simple Preference Optimization (SimPO). SimPO (Meng et al., 2024) advances preference optimization with two key innovations: (1) it normalizes the reward by the length of the response, calculating the average log-probability per token for response under the policy πθ, and (2) it incorporates target reward margin γ to ensure that the reward difference between the preferred and less preferred responses exceeds this margin. The SimPO loss function is defined as: LSimPO(πθ) = E(x,yw,yl)D (cid:18) log πθ(yw x) (cid:20) yw log σ (cid:18) β log πθ(yl x) yl (cid:19)(cid:19)(cid:21) γ , (6) where denotes the number of tokens in response y, ensuring length-aware scaling of rewards, and γ is the predefined margin that enforces minimum difference in rewards between yw and yl. To align with subsequent discussions, we modify the original SimPO formulation by setting γ to γ/β. 3. Analysis on RePO Directed Preference Optimization (DPO). DPO (Rafailov et al., 2023) stands out as leading method for offline preference optimization by eliminating the need for an explicit reward model. Instead, it reformulates the reward function r(x, y) as closed-form expression based on policy ratios: In this section, we first present the expression for RePO and compare its performance with that of SimPO through experiments. We then analyze the advantages and key factors behind the success of RePO from the perspectives of gradient behavior, data filtering, and over-optimization. r(x, y) = β log πθ(y x) πref(y x) + β log Z(x), (3) 3.1. RePO where Z(x) is partition function that does not depend on y. This leads to the DPO loss for given triplet (x, yw, yl) as: Method Formulation. We propose RePO by simplifying the SimPO loss function through two key modifications: (i) removing the hyperparameter β, and (ii) replacing the logsigmoid function with ReLU activation. First, we define the implicit reward margin Mθ as: LDPO(πθ; πref) = E(x,yw,yl)D (cid:20) (cid:18) (cid:18) log σ β log πθ(yw x) πθ(yl x) log πref(yw x) πref(yl x) (cid:19)(cid:19)(cid:21) (4) , Mθ = log πθ(yw x) yw log πθ(yl x) yl , (7) where σ() denotes the sigmoid function. This loss encourages the policy πθ to prefer yw over yl in alignment with the reference policy. Sequence Likelihood Calibration (SLiC-HF). SLiCHF (Zhao et al., 2023) advances preference optimization with two key innovations: (1) it employs sequence-level calibration loss that contrasts the log-probability difference between preferred and dispreferred responses using margin γ, and (2) it integrates regularization term to prevent which measures the policys relative preference between responses. Using Mθ, the proposed loss function is: LRePO(πθ) = E(x,yw,yl)D [ReLU ((Mθ γ))] , (8) where γ [0, 1] is the sole hyperparameter representing the target reward margin. Comparison between RePO and SimPO / SLiC-HF. As formalized in Equation 8, RePO simplifies the SimPO framework by replacing the log-sigmoid activation with ReLU 3 RePO: ReLU-based Preference Optimization operator and removing the need to tune the hyperparameter β. Simultaneously, it eliminates the explicit regularization term in SLiC-HF, thereby removing the associated hyperparameter λ. This dual simplification makes RePO unified specialization of both SimPO and SLiC-HF, reducing hyperparameter tuning complexity while maintaining competitive performance, as demonstrated in Section 3.2. Training Dynamics Monitoring. We track optimization progress with two metrics: Batch-level reward margin: mbatch = E(x,yw,yl)B[Mθ], (9) which reflects the average reward margin within each training batch B. Dataset-level reward margin: mD = E(x,yw,yl)D[Mθ], (10) which measures the overall reward margin on the entire training set D. Here, mbatch provides immediate feedback during each update, while mD captures the global preference strength of the policy over the complete dataset. Gradient Analysis. We analyze the gradient dynamics of RePO and SimPO to highlight their fundamental differences, the gradients are as follows: θLSimPO(πθ) = βED [sθ (θ,yw θ,yl )] , (11) θLRePO(πθ) = ED [I(Mθ < γ) (θ,yw θ,yl )] , (12) where sθ = σ(β(Mθ + γ)) is SimPOs sigmoid weighting function. The terms θ,yw = 1 yw θ log πθ(yw x) and θ,yl = 1 yl θ log πθ(yl x) correspond to the gradients that increase the probability of the winning response yw and decrease the probability of the losing response yl, respectively. The scaling factor β in Equation 11 linearly amplifies gradient magnitudes but does not alter the relative update directions in adaptive optimizers like Adam (Kingma & Ba, 2014), as the momentum terms automatically normalize scale variations. We therefore omit β in Figure 2 for clearer visualization of the weighting function shapes. RePOs gradient (Equation 12) applies uniform updates to pairs with Mθ < γ, while SimPO (Equation 11) employs β-scaled continuous weighting through sθ. Figure 2 visualizes this contrast, showing how RePOs binary thresholding emerges as limiting case of SimPO when β . Lemma 3.1 (Gradient Equivalence in the SimPO-to-RePO Limit). Under the same Mθ and γ definitions, the SimPO Figure 2. Gradient weighting functions of SimPO (sθ) and RePO (I(Mθ < γ)). As β , sθ converges to the binary indicator (dashed line), establishing RePO as the limit case of SimPO. gradient converges pointwise to the RePO gradient as β : lim β θLSimPO = θLRePO. (13) Sketch. The convergence follows from the pointwise limit of the sigmoid weighting: lim β sθ = lim β σ(β(Mθ + γ)) = I(Mθ < γ). Substituting this into Equation 11 yields Equation 12. Remark 3.2. Please check Appendix for all proofs. Lemma 3.1 establishes RePO as the asymptotic limit of SimPO with large β. This explains why RePO achieves comparable performance to SimPO (Section 3.2) while eliminating β tuning. The binary thresholding in RePO also induces implicit data filtering, which we analyze in Section 3.3. 3.2. Relationship between SimPO and RePO The previous section analyzes the relationship between SimPO and RePO from the perspective of gradient behavior. In this section, we compare their performance from an empirical standpoint. Experimental setup. We adopt the experimental setup used in SimPO (Meng et al., 2024), with Llama3-8B and Gemma2-9B as our base models. For comparison, we use pre-trained instruction-tuned models as reference policies. To ensure consistency, we employ the same training datasets as in SimPO: princeton-nlp/llama3-ultrafeedback-armorm1 for Llama3-8B and princeton-nlp/gemma2-ultrafeedbackarmorm2 for Gemma2-9B. In all experiments involving SimPO, we set β = 10.0 and γ = 0.4 for Gemma2-9B and β = 10.0 and γ = 0.3 for Llama3-8B, unless stated otherwise. 1https://huggingface.co/datasets/princeton-nlp/llama3ultrafeedback-armorm 2https://huggingface.co/datasets/princeton-nlp/gemma2ultrafeedback-armorm 4 RePO: ReLU-based Preference Optimization Figure 4, the performance of RePO is highly sensitive to γ. Increasing γ initially improves performance, but beyond certain threshold, performance degrades. Intuitively, small γ retains many zero-gradient data points, leading to under-filtering. Conversely, large γ causes many data points to have unit gradient weight, potentially leading to overfitting. Further discussion on γs role in data filtering is provided in subsequent sections. 3.3. Data Filtering by RePO Building upon the preceding discussion, we observe that adjusting the parameter γ defines cutoff point that determines which sample gradients are set to zero, effectively implementing data filtering. In this section, we analyze the training process of RePO through the lens of data filtering. Observation 4: γ controls the distribution of implicit reward margins. We also report mD (cf. Equation 10), the mean of implicit reward margins across all training pairs. As shown in the bar chart of Figure 4, as γ increases, mD rises. However, alignment performance w.r.t. LC improves initially but declines once γ exceeds certain threshold. This suggests that while larger γ increases the separation between training pairs, excessive separation can introduce the risk of reward hacking (Amodei et al., 2016), leading to performance drop. Observation 5: Less data, better alignment. Figure 5 illustrates the distribution of the implicit reward margin Mθ across different training stages, alongside the ratio of zero gradients (i.e., when Mθ exceeds γ). As training progresses, the models ability to discriminate between winning and losing samples improves, resulting in steady increase in both the implicit reward margin and the ratio of filtered data. Notably, the filtered data ratio rises from 48% to 58% between steps 200 and 400. Despite using only half of the samples for gradient updates, the model achieves optimal performance, indicating that RePO effectively utilizes smaller, more informative subset of training data. 3.4. Over-Optimization Analysis When larger γ is selected, RePO permits most data to bypass the filter, resulting in continuous increase in the reward margin between winning and losing samples. This phenomenon, manifestation of reward exploitation or hacking (Amodei et al., 2016), is often referred to as overoptimization in the RLHF literature, and has been empirically investigated in both controlled experiments (Gao et al., 2023) and user studies (Dubois et al., 2023). Model Over-Optimization: Building on Rafailov et al. (2024), we investigate over-optimization in RePO, by evaluating six different values of γ (0.0, 0.2, 0.4, 0.6, 0.8, 1.0), each corresponding to varying levels of data filtering. Figure 3. Performance of SimPO with varying β and RePO on AlpacaEval2 benchmark. Figure 4. Line plot illustrating the variation in RePOs performance with different values of γ, measured by the AlpacaEval 2 LC Win Rate. The accompanying bar chart shows the mean implicit reward margin, mD, computed across all training pairs. Evaluation benchmarks. We evaluate our models on two widely used open-ended instruction-following benchmarks: AlpacaEval 2 (Li et al., 2023) and Arena-Hard (Li et al., 2024). These benchmarks assess the models conversational abilities across various queries. For AlpacaEval 2, we report both the length-controlled win rate (LC) and raw win rate (WR). For Arena-Hard, we report the win rate (WR). Observation 1: Large β enhances SimPOs performance. We first experiment with SimPO by varying the parameter β over the set {1.0, 3.0, 5.0, 10.0}, while keeping γ fixed. As shown in Figure 3, increasing β consistently improves SimPOs performance,with convergence observed as β grows larger Observation 2: RePO achieves competitive performance. RePO achieves performance comparable to SimPO with large β. As shown in Figure 3, RePO achieves win rates of 51.1% on Llama3-8B and 66.6% on Gemma2-9B, comparable to SimPOs performance. This aligns with Lemma 3.1, which establishes that RePO can be interpreted as limiting case of SimPO as β . Observation 3: γ is crucial hyperparameter. In RePO, the gradients become zero when the implicit reward margin exceeds γ, indicating that the model πθ is confident in its predictions for these data pairs. These pairs, considered wellseparated, are excluded from updates, mitigating overfitting. Thus, γ serves as critical cutoff point. As shown in 5 RePO: ReLU-based Preference Optimization Figure 5. Implicit reward margin Mθ distribution across training steps (total: 467) for RePO at γ = 0.4. Dashed line: γ = 0.4. Green: samples below γ (gradient descent); gray: samples above γ (zero gradient). Numbers: fraction of samples above γ. samples. One potential solution is to apply non-linear activation function to weight the gradient of each pair. Specifically, we extend RePO to DPO and SimPO, introducing new method, RePO++. LRePO++(πθ) = ED [log σ (ReLU (β (Mθ γ)))] , (15) What does the RePO++ update do? We again consider analyzing the gradient of the loss function RePO++ for an intuitive understanding. The gradient w.r.t. the parameters θ is given by: θLRePO++(πθ) = βED [sθ Iθ (θ,yw θ,yl )] , (16) where sθ = σ (β (Mθ + γ)) and Iθ is an indicator function that is 1 if Mθ < γ and 0 otherwise. Intuitively, This gradient combines the strengths of SimPO and RePO: it scales updates by sθ and filters them using Iθ, ensuring that the model focuses on less-separated pairs, with smaller separations receiving higher weights. Adaptation with RePO++. The core contribution of RePO++ lies in leveraging ReLU to mitigate overoptimization while preserving the standard workflow of preference optimization. This makes RePO++ easily adaptable to existing DPO-like methods. For instance, as shown in πθ(ylx) log πref(ywx) Equation 15, replacing Mθ with log πθ(ywx) πref(ylx) seamlessly integrates RePO++ into DPO, forming ReLUenhanced version of DPO. 4. Theoretical Analysis of RePO Next, we relate preference optimization to supervised binary classification task and show how RePO leverages the convex envelope of the 0-1 loss to achieve both theoretical optimality and efficient optimization. Following Tang et al. (2024), we reformulate reward modeling as supervised binary classification task. Given data pairs (z, l) with Rk and labels {1, 1}, the goal is to learn predictor ˆℓ(z) such that sign(ˆℓ(z)) matches E[sign(ˆℓ(z) l. The classification accuracy is given by: 1 2 Figure 6. Over-optimization results for RePO on Llama3-8B-IT and Gemma2-9B-IT. Metrics include model LC win rate on AlpacaEval2 and raw win rate on Arena-Hard. Fitted dotted curves apply scaling laws from Gao et al. (2023) to direct alignment, with GPT-4 win rates substituting gold reward model scores. Across all cases, we observe distinct hump-shaped performance pattern: while moderate filtering improves alignment, excessive filtering causes performance to degrade, highlighting the over-optimization effect. Scaling Law Fits. Previous work (Gao et al., 2023; Rafailov et al., 2024) has established scaling laws for reward model scores as function of the KL divergence between the initial and optimized policies. In contrast, we eliminate the reference model and the associated computational cost of calculating KL divergence. Instead, we use the mean implicit reward margin during training as proxy metric. The reward function R(d) is given by: R(d) = d(α β log d), (14) where α and β are constants dependent on the reward models dataset size and parameter count, and = mbatch. Without training proxy reward model, we substitute GPT-4 win rates over dataset completions for the gold reward. Interestingly, we find that this scaling law provides an accurate relationship between and win rates for RePO. 3.5. RePO++ Building on the previous discussion, we identify the advantages of RePO in finding cutoff point to control over-optimization. However, limitation persists: for cases where the implicit reward margin is smaller than γ, their gradient weights become unit, failing to differentiate between 6 RePO: ReLU-based Preference Optimization l)] + 1 2 , and the corresponding loss function is: (cid:104) (cid:105) 1 sign(ˆℓ(z) l) L0-1(ˆℓ) := . The above loss, known as the 0-1 loss is non-convex. We therefore approximate it using convex functions : R: Lf (ˆℓ) := (cid:104) (ˆℓ(z) l) (cid:105) . In the context of reward modeling, given response pairs (yw, yl) with preference yw yl, we construct binary classification samples with ℓ = 1. For pointwise reward model rϕ : R, the natural parameterization is: ˆℓ(yw, yl) = rϕ(yw) rϕ(yl), leading to the loss: Lf (ˆℓ) := [f (rϕ(yw) rϕ(yl))] . Different choices of (x) correspond to distinct loss functions. Specifically, when (x) = I(x < 0), the 0-1 loss is obtained; when (x) = log σ(x), we obtain the logistic loss used in SimPO; and when (x) = ReLU(x), it corresponds to the loss function used in RePO. Definition 4.1. Let be closed convex set. The convex envelope of L0-1 over is: convDL0-1(x) := sup {h(x) is convex, L0-1 D} . (17) Theorem 4.2 (Convex Envelope of 0-1 Loss). For = [a, b] with a, > 0, the convex envelope of L0-1(x) = I(x < 0) is: convDL0-1(x) = 1 ReLU(x). Corollary 4.3 (Optimality Preservation). Let be convex. Then: 1. For binary classification with loss L0-1: arg min ˆℓ L0-1(ˆℓ) = arg min ˆℓ convDL0-1(ˆℓ). 2. When = [a, b]: arg min xD L0-1(x) = arg min xD 1 ReLU(x). Remark 4.4. Corollary 4.3 establishes: (i) equivalence between minimizing the 0-1 loss and its convex envelope, (ii) exact optimality preservation for the ReLU surrogate. This guarantees gradient-based optimization converges to solutions matching the theoretical optimum of the intractable 0-1 loss. 7 Corollary 4.5 (Logistic Loss is Not the Convex Envelope of 0-1 Loss). The logistic loss flogsigmoid(x) = log σ(x) is not the convex envelope of L0-1. Remark 4.6. Unlike SimPOs logistic loss (Corollary 4.5), the ReLU-based surrogate inherits the 0-1 losss global optimality. This explains RePOs robustness to suboptimal local minima during gradient-based training. 5. Experiments In this section, we present our main experimental findings, demonstrating that RePO consistently achieves better results than existing preference optimization methods across multiple benchmarks. 5.1. Experimental Setup The main experimental setup has been described in Section 3.2. We increase model: Mistral2-7B (Jiang et al., 2023). Additionally, the v0.2 Llama3-Instruct setup uses RLHFlow/ArmoRM-Llama3-8B-v0.1 (Wang et al., 2024b) as the reward model for ranking generated data, significantly enhancing performance. As for baselines, we compare RePO with several state-of-the-art preference optimization methods: DPO (Rafailov et al., 2023), SimPO (Meng et al., 2024), IPO (Azar et al., 2023), CPO (Xu et al., 2024), KTO (Ethayarajh et al., 2024), ORPO (Hong et al., 2024), and R-DPO (Park et al., 2024). We also include the SFT model as baseline. We thoroughly tune the hyperparameters for each baseline and report the best performance. Further details can be found in Appendix C.1. We also evaluate on downstream tasks from the Huggingface Open Leaderboard benchmarks (Beeching et al., 2023), with additional details in in Appendix C.2. The code is available at https://github.com/junkangwu/RePO. 5.2. Main Results RePO consistently and significantly outperforms existing preference optimization methods. As shown in Table 1, all preference optimization methods outperform the SFT model. Nevertheless, RePO despite its simplicity achieves the strongest overall performance across all benchmarks and settings. The consistent gains highlight its robustness and efficiency. Specifically, RePO surpasses the best-performing baseline by 0.2 to 2.8 points on the AlpacaEval 2 LC win rate under various configurations. On Arena-Hard, RePO generally performs better than competing methods, although CPO achieves slightly higher scores at times. We observe that CPO tends to produce responses that are approximately 50% longer. Because Arena-Hard does not penalize response length, longer outputs may occasionally yield higher evaluations. RePO: ReLU-based Preference Optimization Table 1. AlpacaEval 2 (AE2), Arena-Hard (AH) results across four settings. WR denotes the raw win rate,LC the length-controlled win rate. The best results are highlighted in bold, while the second-best are underlined. Method SFT SLiC-HF DPO IPO CPO KTO ORPO R-DPO SimPO RePO Llama3-Instruct (8B) Mistral-Instruct (7B) Llama3-Instruct v0.2 (8B) Gemma2-Instruct (9B) AE 2 AH AE 2 AH AE 2 AH AE 2 AH LC (%) WR (%) WR (%) LC (%) WR (%) WR (%) LC (%) WR (%) WR (%) LC (%) WR (%) WR (%) 24.0 26.9 40.2 35.9 29.6 38.3 31.6 40.3 43.8 46.7 23.6 27.5 38.1 34.4 34.4 34.1 29.8 37.3 38. 41.1 22.4 26.2 31.2 30.2 29.4 30.3 26.3 32.9 32.6 33.3 19.0 24.1 20.3 22.3 26.2 19.4 24.0 21.4 30. 30.4 15.4 24.6 17.9 18.6 31.7 20.3 23.0 22.2 32.1 33.6 12.9 18.9 13.4 16.2 23.8 16.8 18.6 13.8 20. 20.3 24.0 33.9 48.2 40.6 36.5 41.4 36.5 51.6 55.6 57.7 23.6 32.5 47.5 39.6 40.8 36.4 33.1 50.7 49. 51.1 22.4 29.3 35.2 34.9 34.2 28.9 30.4 35.0 33.6 35.2 48.7 65.1 70.4 62.6 56.4 61.7 56.2 68.3 72. 73.6 36.5 60.5 66.9 58.4 53.4 55.5 46.7 66.9 65.0 66.6 42.1 53.7 58.8 53.5 55.2 53.8 46.2 57.9 57. 59.1 Table 2. Performance improvements of RePO and RePO++ over DPO and SimPO. Results are present on AlpacaEval 2 (AE 2) and Arena-Hard (AH) with LC (%) and WR (%). Red numbers indicate relative improvements. Method DPO w. RePO w. RePO++ SimPO w. RePO w. RePO++ Llama3-Instruct v0.2 (8B) Gemma2-Instruct (9B) AE 2 AH AE 2 AH LC (%) WR (%) WR (%) LC (%) WR (%) WR (%) 47.5 48.2 50.3+4.4% 51.8+9.1% 38.2+8.5% 73.8+4.8% 71.0+6.1% 64.2+9.2% 50.8+5.4% 52.2+9.9% 37.2+5.7% 71.8+2.0% 69.5+3.9% 65.7+11.7% 58. 35.2 70.4 66.9 49.6 55.6 57.7+3.8% 51.1+3.0% 35.2+4.8% 73.6+1.7% 66.6+2.5% 59.1+2.2% 56.1+0.9% 50.1+1.0% 35.9+6.8% 74.1+2.3% 66.5+2.3% 59.8+3.5% 33. 57.8 72.4 65.0 5.3. Adaptations of RePO RePO is versatile approach that can be adapted to different optimization settings. In this work, we explore two adaptations: RePO and RePO++, applied to both DPO and SimPO. The detailed formulations are provided in Section 3.5. As shown in Table 2, integrating RePO with DPO and SimPO consistently enhances performance. Notably, RePO++ achieves superior results in most scenarios, which we attribute to its combination of ReLU activation and the original weighting function sθ. This design effectively mitigates overoptimization while preserving the benefits of Interestingly, applying RePO and the original scheme. RePO++ to DPO yields particularly significant improvements (5%12%), achieving high score of 65.7 on the Arena-Hard benchmark. 5.4. Dynamic Margin Scheduling To further investigate the impact of the target reward margin γ , we conduct an ablation study using dynamic values for γ . Specifically, we implement two strategies: (i) γ increases from small to large, and (ii) γ decreases from large to small. Using RePO, we control the initial and final 8 Figure 7. Performance of RePO with dynamic γ. The dashed line represents RePO with fixed γ. values of γ , adjusting it uniformly over the course of one training epoch. As shown in Figure 7, both excessively small and large values of γ lead to suboptimal performance (e.g., , 1.0 0.8 , 0.0 0.2 ). In contrast, starting with moderately large value of γ and gradually decreasing it during training (e.g., , 1.0 0.2 ) results in improved model performance. We interpret this behavior as follows: in the early stages of training, the model is underfitting and thus benefits from stronger updates. In later stages, reducing γ helps mitigate overfitting, effectively addressing this issue. 6. Discussion Conclusion. In this work, we propose RePO, simple preference optimization by replacing logistic weighting with ReLU-based max-margin loss, eliminating the need for hyperparameter β. Theoretically, RePO is characterized as SimPOs β limit, collapsing to binary thresholding for data filtering. Empirically, it matches or outperforms DPO and SimPO on AlpacaEval 2 and Arena-Hard with only single tunable parameter (γ). Its gradient dynamics prioritize challenging samples, mitigating over-optimization while maintaining training stability. This work establishes RePO as robust, hyperparameter-efficient alternative for offline alignment. Limitations and Future Work. One limitation of our apRePO: ReLU-based Preference Optimization proach is that it currently operates in an offline setting, and future research will focus on extending it to online reinforcement learning frameworks. Integrating repo with online RL techniques could enhance real-time adaptability and scalability. Another direction for future work is to explore how the cutoff point strategy can be further refined to sustain performance improvements in self-play scenarios (Chen et al., 2024b), particularly in highly dynamic environments."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Ahmadian, A., Cremer, C., Galle, M., Fadaee, M., Kreutzer, J., Pietquin, O., Ustun, A., and Hooker, S. Back to basics: Revisiting reinforce-style optimization for learning from human feedback in llms. In ACL (1), pp. 1224812267. Association for Computational Linguistics, 2024. Amodei, D., Olah, C., Steinhardt, J., Christiano, P. F., Schulman, J., and Mane, D. Concrete problems in AI safety. CoRR, abs/1606.06565, 2016. Anil, R., Borgeaud, S., Wu, Y., Alayrac, J., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., Silver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., Lillicrap, T. P., Lazaridou, A., Firat, O., Molloy, J., Isard, M., Barham, P. R., Hennigan, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Doherty, R., Collins, E., Meyer, C., Rutherford, E., Moreira, E., Ayoub, K., Goel, M., Tucker, G., Piqueras, E., Krikun, M., Barr, I., Savinov, N., Danihelka, I., Roelofs, B., White, A., Andreassen, A., von Glehn, T., Yagati, L., Kazemi, M., Gonzalez, L., Khalman, M., Sygnowski, J., and et al. Gemini: family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. N., Olsson, C., Amodei, D., Brown, T. B., Clark, J., McCandlish, S., Olah, C., Mann, B., and Kaplan, J. Training helpful and harmless assistant with reinforcement learning from human feedback. ArXiv, abs/2204.05862, 2022. Beeching, E., Fourrier, C., Habib, N., Han, S., Lambert, N., Rajani, N., Sanseviero, O., Tunand Wolf, T. stall, L., Open LLM leaderhttps://huggingface.co/spaces/ board. HuggingFaceH4/open_llm_leaderboard, 2023. Boser, B. E., Guyon, I., and Vapnik, V. training algorithm for optimal margin classifiers. In COLT, pp. 144152. ACM, 1992. Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 1952. Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S. M., Nori, H., Palangi, H., Ribeiro, M. T., and Zhang, Y. Sparks of artificial general intelligence: Early experiments with GPT-4. CoRR, abs/2303.12712, 2023. Calandriello, D., Guo, Z. D., Munos, R., Rowland, M., Tang, Y., Pires, B. A., Richemond, P. H., Lan, C. L., Valko, M., Liu, T., Joshi, R., Zheng, Z., and Piot, B. Human alignment of large language models through online preference optimisation. In ICML. OpenReview.net, 2024. Chen, L., Zhu, C., Soselia, D., Chen, J., Zhou, T., Goldstein, T., Huang, H., Shoeybi, M., and Catanzaro, B. ODIN: Disentangled reward mitigates hacking in RLHF. arXiv preprint arXiv:2402.07319, 2024a. Chen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Selfplay fine-tuning converts weak language models to strong language models. In ICML. OpenReview.net, 2024b. Chowdhury, S. R., Kini, A., and Natarajan, N. Provably robust DPO: aligning language models with noisy feedback. In ICML. OpenReview.net, 2024. Anthony, T., Tian, Z., and Barber, D. Thinking fast and slow with deep learning and tree search. Advances in neural information processing systems, 30, 2017. Christiano, P. F., Leike, J., Brown, T. B., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. In NIPS, pp. 42994307, 2017. Azar, M. G., Rowland, M., Piot, B., Guo, D., Calandriello, D., Valko, M., and Munos, R. general theoretical paradigm to understand learning from human preferences. ArXiv, abs/2310.12036, 2023. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try ARC, the AI2 reasoning challenge. ArXiv, abs/1803.05457, 2018. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T., Showk, S. E., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda, Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 9 RePO: ReLU-based Preference Optimization Conover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., and Xin, R. Free dolly: Introducing the worlds first truly open instruction-tuned LLM, 2023. URL https: //www.databricks.com/blog/2023/04/12/ dolly-first-open-commercially-viable -instruction-tuned-llm. Cortes, C. and Vapnik, V. Support-vector networks. Mach. Learn., 20(3):273297, 1995. Ding, N., Chen, Y., Xu, B., Qin, Y., Zheng, Z., Hu, S., Liu, Z., Sun, M., and Zhou, B. Enhancing chat language models by scaling high-quality instructional conversations. In EMNLP, 2023. Dong, H., Xiong, W., Pang, B., Wang, H., Zhao, H., Zhou, Y., Jiang, N., Sahoo, D., Xiong, C., and Zhang, T. RLHF workflow: From reward modeling to online RLHF. CoRR, abs/2405.07863, 2024. Dubois, Y., Li, C. X., Taori, R., Zhang, T., Gulrajani, I., Ba, J., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacafarm: simulation framework for methods that learn from human feedback. In NeurIPS, 2023. Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., and Kiela, D. KTO: Model alignment as prospect theoretic optimization. ArXiv, abs/2402.01306, 2024. Gao, L., Schulman, J., and Hilton, J. Scaling laws for reward model overoptimization. In ICML, volume 202 of Proceedings of Machine Learning Research, pp. 10835 10866. PMLR, 2023. Gao, Z., Chang, J. D., Zhan, W., Oertell, O., Swamy, G., Brantley, K., Joachims, T., Bagnell, J. A., Lee, J. D., and Sun, W. REBEL: reinforcement learning via regressing relative rewards. CoRR, abs/2404.16767, 2024. Geng, X., Gudibande, A., Liu, H., Wallace, E., Abbeel, P., Levine, S., and Song, D. Koala: dialogue model for academic research. Blog post, April, 1:6, 2023. Havrilla, A., Raparthy, S., Nalmpantis, C., Dwivedi-Yu, J., Zhuravinskyi, M., Hambro, E., and Railneau, R. GLoRe: When, where, and how to improve LLM reasoning via global and local refinements. arXiv preprint arXiv:2402.10963, 2024. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020. Huang, A., Zhan, W., Xie, T., Lee, J. D., Sun, W., Krishnamurthy, A., and Foster, D. J. Correcting the mythos of kl-regularization: Direct alignment without overoptimization via chi-squared preference optimization. CoRR, abs/2407.13399, 2024. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de Las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.- A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7B. ArXiv, abs/2310.06825, 2023. Khaki, S., Li, J., Ma, L., Yang, L., and Ramachandra, P. Rs-dpo: hybrid rejection sampling and direct preference optimization method for alignment of large language models. In Findings of the Association for Computational Linguistics: NAACL 2024, pp. 16651680, 2024. Kim, D., Kim, Y., Song, W., Kim, H., Kim, Y., Kim, S., and Park, C. sdpo: Dont use your data all at once. CoRR, abs/2403.19270, 2024. Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kopf, A., Kilcher, Y., von Rutte, D., Anagnostidis, S., Tam, Z. R., Stevens, K., Barhoum, A., Nguyen, D. M., Stanley, O., Nagyfi, R., et al. Openassistant conversationsdemocratizing large language model alignment. In Thirtyseventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. Lambert, N., Pyatkin, V., Morrison, J. D., Miranda, L. J. V., Lin, B. Y., Chandu, K. R., Dziri, N., Kumar, S., Zick, T., Choi, Y., Smith, N. A., and Hajishirzi, H. RewardBench: Evaluating reward models for language modeling. ArXiv, abs/2403.13787, 2024. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Levesque, H., Davis, E., and Morgenstern, L. The Winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012. Li, T., Chiang, W.-L., Frick, E., Dunlap, L., Zhu, B., From live data to Gonzalez, J. E., and Stoica, I. high-quality benchmarks: The Arena-Hard pipeline, April 2024. URL https://lmsys.org/blog/ 2024-04-19-arena-hard/. Hong, J., Lee, N., and Thorne, J. ORPO: Monolithic preference optimization without reference model. ArXiv, abs/2403.07691, 2024. Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. AlpacaEval: An automatic evaluator of instruction-following 10 RePO: ReLU-based Preference Optimization models. alpaca_eval, 2023. https://github.com/tatsu-lab/ Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Lin, S., Hilton, J., and Evans, O. TruthfulQA: Measuring how models mimic human falsehoods. In ACL, pp. 3214 3252, 2022. Luo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C., Geng, X., Lin, Q., Chen, S., and Zhang, D. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. Meng, Y., Xia, M., and Chen, D. Simpo: Simple preference optimization with reference-free reward. NeurIPS, 2024. Muldrew, W., Hayes, P., Zhang, M., and Barber, D. Active preference learning for large language models. In Forty-first International Conference on Machine Learning, 2024. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. CoRR, abs/2203.02155, 2022. Park, R., Rafailov, R., Ermon, S., and Finn, C. Disentangling length from quality in direct preference optimization. ArXiv, abs/2403.19159, 2024. Princeton University. ORF 523 Lecture 8: Online Convex Optimization, Accessed 2025. URL https: //www.princeton.edu/aaa/Public/ Teaching/ORF523/ORF523_Lec8.pdf. cessed: 31 Jan 2025. AcRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. Rafailov, R., Chittepu, Y., Park, R., Sikchi, H., Hejna, J., Knox, W. B., Finn, C., and Niekum, S. Scaling laws for reward model overoptimization in direct alignment algorithms. CoRR, abs/2406.02900, 2024. Rosset, C., Cheng, C., Mitra, A., Santacroce, M., Awadallah, A., and Xie, T. Direct nash optimization: Teaching language models to self-improve with general preferences. CoRR, abs/2404.03715, 2024. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. Tang, Y., Guo, Z. D., Zheng, Z., Calandriello, D., Munos, R., Rowland, M., Richemond, P. H., Valko, M., Pires, B. A., and Piot, B. Generalized preference optimization: unified approach to offline alignment. In ICML. OpenReview.net, 2024. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model, 2023. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Canton-Ferrer, C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. Wang, C., Jiang, Y., Yang, C., Liu, H., and Chen, Y. Beyond reverse KL: generalizing direct preference optimization with diverse divergence constraints. In ICLR. OpenReview.net, 2024a. Wang, H., Xiong, W., Xie, T., Zhao, H., and Zhang, T. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. ArXiv, abs/2406.12845, 2024b. Wang, S., Zhang, Z., Zhao, R., Tan, F., and Cam-Tu, N. Reward difference optimization for sample reweighting in offline rlhf. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 21092123, 2024c. 11 RePO: ReLU-based Preference Optimization Wu, J., Xie, Y., Yang, Z., Wu, J., Chen, J., Gao, J., Ding, B., Wang, X., and He, X. Towards robust alignment of language models: Distributionally robustifying direct preference optimization. CoRR, abs/2407.07880, 2024a. Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P. F., and Irving, G. Finetuning language models from human preferences. ArXiv, abs/1909.08593, 2019. Wu, J., Xie, Y., Yang, Z., Wu, J., Gao, J., Ding, B., Wang, X., and He, X. β-dpo: Direct preference optimization with dynamic β. CoRR, abs/2407.08639, 2024b. Wu, Y., Sun, Z., Yuan, H., Ji, K., Yang, Y., and Gu, Q. Self-play preference optimization for language model alignment. CoRR, abs/2405.00675, 2024c. Xiong, W., Dong, H., Ye, C., Wang, Z., Zhong, H., Ji, H., Jiang, N., and Zhang, T. Iterative preference learning from human feedback: Bridging theory and practice for RLHF under kl-constraint. In ICML. OpenReview.net, 2024. Xu, H., Sharaf, A., Chen, Y., Tan, W., Shen, L., Durme, B. V., Murray, K., and Kim, Y. J. Contrastive preference optimization: Pushing the boundaries of LLM performance in machine translation. ArXiv, abs/2401.08417, 2024. Yang, S., Cui, L., Cai, D., Huang, X., Shi, S., and Lam, W. Not all preference pairs are created equal: recipe for annotation-efficient iterative preference learning. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 65496561, 2024. Ye, Z., Agarwal, R., Liu, T., Joshi, R., Velury, S., Le, Q. V., Tan, Q., and Liu, Y. Evolving alignment via asymmetric self-play. arXiv preprint arXiv:2411.00062, 2024. Yuan, W., Pang, R. Y., Cho, K., Li, X., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. In ICML. OpenReview.net, 2024. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. HellaSwag: Can machine really finish your senIn Korhonen, A., Traum, D., and M`arquez, tence? L. (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791 4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472. Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P. J. Slic-hf: Sequence likelihood calibration with human feedback. CoRR, abs/2305.10425, 2023. Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et al. LIMA: Less is more for alignment. NeurIPS, 2023. 12 A. Related Works RePO: ReLU-based Preference Optimization Reinforcement learning from human feedback. RLHF is technique designed to align large language models with human preferences and values (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022; Azar et al., 2023). Traditional RLHF is typically structured in three stages: supervised fine-tuning (Zhou et al., 2023; Taori et al., 2023; Geng et al., 2023; Conover et al., 2023; Kopf et al., 2023; Ding et al., 2023), reward modeling (Gao et al., 2023; Luo et al., 2023; Chen et al., 2024a; Lightman et al., 2023; Havrilla et al., 2024; Lambert et al., 2024), and policy optimization (Schulman et al., 2017; Anthony et al., 2017). In the third stage, Proximal Policy Optimization (PPO) is widely adopted. In contrast, RLOO (Ahmadian et al., 2024) reduces the GPU memory footprint of RLHF by eliminating the Critic model and leverages Leave-One-Out strategy to achieve superior performance. GRPO (Shao et al., 2024), another variant of PPO, improves mathematical reasoning abilities while optimizing memory usage by replacing the Leave-One-Out method with direct subtraction of the mean of all samples for given prompt. Offline preference optimization. In addition to DPO, several alternative preference optimization objectives have been proposed. IPO (Azar et al., 2023) addresses overfitting issues inherent in DPO. ORPO (Hong et al., 2024) and SimPO (Meng et al., 2024) aim to remove reliance on reference model. R-DPO (Park et al., 2024) targets the reduction of exploitation due to sequence length, while KTO (Ethayarajh et al., 2024) handles preference optimization in the absence of pairwise data. CPO (Xu et al., 2024) and β-DPO (Wu et al., 2024b) focus on improving the quality of preference data. Another research direction addresses noise in offline alignment, which arises from the need to construct pairwise data. rDPO (Chowdhury et al., 2024), variant of DPO, mitigates preference noise and enhances policy robustness, while DrDPO (Wu et al., 2024a) applies distributed robust optimization to tackle this issue. Other works have approached the problem through divergence regularization (Wang et al., 2024a; Huang et al., 2024), selection of high-quality data (Khaki et al., 2024; Ye et al., 2024), or reweighting loss functions (Wang et al., 2024c; Muldrew et al., 2024; Yang et al., 2024). Iterative Preference Optimization. Offline preference optimization methods, such as DPO, face limitation due to the lack of an explicit reward model, which hinders their ability to sample preference pairs from the optimal policy. To address this, iterative preference optimization techniques have been proposed. These methods iteratively update the reference model using the most recent policy model or generate new preference pairs in each iteration (Dong et al., 2024; Kim et al., 2024; Rosset et al., 2024; Xiong et al., 2024; Yuan et al., 2024; Chen et al., 2024b; Wu et al., 2024c; Gao et al., 2024). For instance, SPIN (Chen et al., 2024b) employs self-play framework to fine-tune the model in supervised manner, while Yuan et al. (2024) annotate preferences throughout the iterative process. REBEL improves sample quality by regressing the relative reward. Additionally, (Calandriello et al., 2024) generates data using mixture policy, similar to the Nash-MD algorithm (Rosset et al., 2024). B. Proofs B.1. Proof of Lemma 3.1 Lemma 3.1 (Gradient Equivalence in the SimPO-to-RePO Limit). Under the same Mθ and γ definitions, the SimPO gradient converges pointwise to the RePO gradient as β : lim β θLSimPO = θLRePO. (13) Proof. We formally establish the gradient equivalence through pointwise convergence analysis. Let be the data distribution and θ denote model parameters. Recall the gradient expressions: SimPO Gradient: RePO Gradient: θLSimPO = βED (cid:2)σ(cid:0)β(Mθ + γ)(cid:1) (θ,yw θ,yl )(cid:3) θLRePO = ED [I(Mθ < γ) (θ,yw θ,yl )] (18) (19) where σ() is the sigmoid function. The equivalence hinges on the limiting behavior of the sigmoid weighting term sθ = σ(β(Mθ + γ)). We analyze three cases: 13 RePO: ReLU-based Preference Optimization Case 1: Mθ < γ Here, Mθ + γ > 0. As β , lim β σ(cid:0)β(Mθ + γ)(cid:1) = lim σ(z) = 1 = I(Mθ < γ). Case 2: Mθ > γ Here, Mθ + γ < 0. As β , lim β σ(cid:0)β(Mθ + γ)(cid:1) = lim σ(z) = 0 = I(Mθ < γ). Case 3: Mθ = γ This occurs on measure-zero set under continuous distributions. The limit becomes: lim β σ(0) = 1 2 = I(Mθ < γ), which is negligible in expectation. Thus, limβ sθ = I(Mθ < γ) almost everywhere. Substituting this into the SimPO gradient: lim β θLSimPO = lim β (cid:20) = ED βED [sθ (θ,yw θ,yl )] (cid:21) βsθ (θ,yw θ,yl ) lim β (Dominated Convergence Theorem) To resolve the β scaling, observe that for Mθ = γ: lim β βsθ = (cid:40) limβ β 1 = if Mθ < γ if Mθ > γ limβ β 0 = 0 (20) (21) (22) The divergence when Mθ < γ is mitigated by adaptive optimizers like Adam, which normalize gradient magnitudes through momentum terms. Formally, let gθ = θ,yw θ,yl . Under Adams update rule: where ˆmt and ˆvt are bias-corrected momentum estimates. The infinite gradient magnitude is absorbed into ˆmt/ effectively reducing to unit-scaled update. Hence, in normalized update space: ˆvt, θt+1 = θt η ˆmt ˆvt + ϵ , lim β βsθ gθ I(Mθ < γ) gθ. Combining these results: lim β θLSimPO = ED [I(Mθ < γ) (θ,yw θ,yl )] = θLRePO, (23) which completes the proof. B.2. Proof of Theorem 4.2 Theorem 4.2 (Convex Envelope of 0-1 Loss). For = [a, b] with a, > 0, the convex envelope of L0-1(x) = I(x < 0) is: convDL0-1(x) = ReLU(x). 1 Proof. We demonstrate that h(x) = 1 ments. ReLU(x) satisfies the convex envelope definition through three sequential argu1. Convexity and Underestimation: The ReLU function is convex as the pointwise maximum of affine functions (Rule 3). The composition h(x) = 1 max(x, 0) preserves convexity through affine transformation (Rule 2). For all D: When < 0: h(x) = 1 = L0-1(x), since = 1 RePO: ReLU-based Preference Optimization When 0: h(x) = 0 = L0-1(x) Thus h(x) L0-1(x) over D. 2. Maximality Among Convex Underestimators: Let g(x) be any convex function satisfying g(x) L0-1(x). For [a, 0), convexity implies: g(x) g(a) + 1 + g(0) (cid:16) (cid:17) x since g(a) 1 and g(0) 0. For 0, g(x) 0. Hence g(x) h(x) for all D. 3. Epigraph Characterization: The epigraph epi(h) coincides with the convex hull of epi(L0-1) (D R). The affine segment h(x) = on [a, 0) connects the points (a, 1) and (0, 0), forming the tightest convex fit to the 0-1 losss discontinuity. By Theorem 1 in (Princeton University, Accessed 2025), this construction achieves the convex envelope. B.3. Proof of Corollary 4. Corollary 4.3 (Optimality Preservation). Let be convex. Then: 1. For binary classification with loss L0-1: 2. When = [a, b]: arg min ˆℓ L0-1(ˆℓ) = arg min ˆℓ convDL0-1(ˆℓ). arg min xD L0-1(x) = arg min xD 1 ReLU(x). Proof. Part 1: By Theorem 1 in the lecture notes (Page 5), for any function and convex set S: Let = and = L0-1. The equality of minima implies: min xS (x) = min xS convSf (x). {x L0-1(x) = min L0-1} {x convDL0-1(x) = min convDL0-1}. To show reverse inclusion, suppose arg min convDL0-1. Since convDL0-1(x) L0-1(x) and convDL0-1 attains its minimum at the same points as L0-1, must also minimize L0-1. Part 2: For = [a, b], both L0-1(x) and 1 1 ReLU(x) is strictly decreasing, achieving its minimum at = 0. Thus: ReLU(x) attain their minimum value 0 on [0, b]. For [a, 0), arg min xD L0-1(x) = arg min xD 1 ReLU(x) = [0, b]. B.4. Proof of Corollary 4. Corollary 4.5 (Logistic Loss is Not the Convex Envelope of 0-1 Loss). The logistic loss flogsigmoid(x) = log σ(x) is not the convex envelope of L0-1. Proof. We demonstrate violation of the convex envelopes defining property. Consider = [1, 1]: 1. Underestimation Failure: For > 0: log σ(x) = log (cid:19) (cid:18) 1 1 + ex = log(1 + ex) > 0 = L0-1(x) 15 RePO: ReLU-based Preference Optimization Thus flogsigmoid L0-1 over D, violating the envelope requirement. 2. Non-Maximality: Even if scaled, the logistic losss curvature differs from the ReLU envelope. For (1, 0), d2 dx2 ( log σ(x)) = σ(x)(1 σ(x)) > 0, making it strictly convex incompatible with the affine structure of convDL0-1. Hence flogsigmoid cannot be the convex envelope. C. Experiments C.1. Implementation Details Empirical observations indicate significant performance sensitivity to model parameter initialization and learning rate selection across compared methods. To establish rigorous comparison benchmarks, we conducted systematic hyperparameter searches adhering to the specifications in each methods original publication. The complete search space configuration is documented in Table 3. Notably, substantial architecture updates to both Llama3-8B and Instruct-7B necessitated re-implementation of the SimPO method, as the original implementation became incompatible with the revised model interfaces. Training Protocol All experiments employed standardized training configurations to ensure comparability: Batch size: 128 (consistent across methods) Learning rate: Searched in {3e-7, 5e-7, 8e-7, 1e-6} Training duration: Single epoch with cosine annealing schedule Warmup: 10% of total training steps Optimizer: Adam (Kingma & Ba, 2014) (β1 = 0.9, β2 = 0.999) Sequence length: 2048 tokens (fixed for all inputs) The learning rate schedule follows triangular policy with amplitude decay, selected through cross-validation on held-out development sets. All implementations utilize full-precision floating-point arithmetic to prevent gradient quantization artifacts. Table 3. Various preference optimization objectives and hyperparameter search range. Method DPO (Rafailov et al., 2023) IPO (Azar et al., 2023) CPO (Xu et al., 2024) KTO (Ethayarajh et al., 2024) ORPO (Hong et al., 2024) Objective (cid:16) log σ (cid:16) β log πθ (yw x) πref(yw x) β log πθ (ylx) πref(ylx) (cid:17) log πθ (yw x) πref(yw x) log πθ (ylx) πref(ylx) 1 2τ (cid:17) Hyperparameter β [0.01, 0.05, 0.1] log σ (β log πθ(ywx) β log πθ(ylx)) λ log πθ(ywx) (cid:17) (cid:17) β log πθ (yw x) λwσ where zref = E(x,y)D [βKL (πθ(yx)πref(yx))] πref(yw x) zref + λlσ zref β log πθ (ylx) πref(ylx) (cid:16) (cid:16) τ [0.01, 0.1, 0.5, 1.0] α = 1.0, β [0.01, 0.05, 0.1] , λl = λw = 1.0 β [0.01, 0.05, 0.1] log pθ(ywx) λ log σ (cid:16) 1 where pθ(yx) = exp log πθ(yx) (cid:16) 1pθ (yw x) log pθ (ylx) 1pθ (ylx) log pθ (yw x) (cid:17) (cid:17) , λ [0.1, 0.5, 1.0, 2.0] (cid:16) β log πθ (yw x) πref(yw x) β log πθ (ylx) πref(ylx) (αyw αyl) R-DPO (Park et al., 2024) log σ SimPO (Meng et al., 2024) log σ RePO (cid:16) β yw log πθ(ywx) β yw log πθ(ywx) 1 yl log πθ(ylx) γ (cid:17) yl log πθ(ylx) γ)] ReLU[( 1 (cid:17) α [0.05, 0.1, 0.5, 1.0] β [0.01, 0.05, 0.1] β [2.0, 4.0, 6.0, 8.0] γ [0.3, 0.5, 1.0, 1.2, 1.4, 1.6] γ [0.2, 0.4, 0.6, 0.8] Hyperparameters in RePO. Table 4 summarizes the hyperparameters utilized for RePO across different experimental settings. Our methodology only involves one hyperparameter: γ. Based on empirical evidence, we recommend setting γ to default value of 0.5, as this configuration has consistently demonstrated reliability. 16 RePO: ReLU-based Preference Optimization Table 4. The hyperparameter values in RePO used for each training setting. Setting Mistral-Instruct Llama3-Instruct Llama3-Instruct-v0.2 Gemma2-Instruct γ 0.4 0.6 0.6 0.4 Learning rate 6e-7 1e-6 1e-6 8e-7 Decoding Hyperparameters. The decoding hyperparameters employed in this study align with those used in SimPO3. We express our gratitude to the SimPO team for their generosity in sharing their insights and configurations, which have been instrumental in our work. Computation Environment. All training experiments described in this paper were conducted using 8A100 GPUs. The experimental setup follows the guidelines provided in the alignment-handbook repository4, ensuring reproducibility and consistency with established practices. C.2. Downstream Task Evaluation Table 5. Downstream task evaluation results of tasks on the huggingface open leaderboard. MMLU (5) ARC (25) HellaSwag (10) TruthfulQA (0) Winograd (5) GSM8K (5) Average Llama3-Instruct SFT RRHF SLiC-HF DPO IPO CPO KTO ORPO R-DPO SimPO RePO SFT RRHF SLiC-HF DPO IPO CPO KTO ORPO R-DPO SimPO RePO 67.06 67.20 66.41 66.88 66.52 67.05 66.38 66.41 66.74 65.63 64. 67.06 66.60 66.91 65.57 66.06 65.67 65.99 65.75 66.17 65.18 65.00 61.01 61.52 61.26 63.99 61.95 62.29 63.57 61.01 64.33 62.80 62.03 61.01 63.74 61.77 65.87 64.85 62.12 62.88 63.99 65.36 67.15 68.09 78.57 79.54 78.80 80.78 77.90 78.73 79.51 79.38 80.97 78.33 77.58 51.66 53.76 53.23 59.01 54.64 54.01 58.15 54.37 60.32 60.70 60.96 Llama3-Instruct v0. 78.57 80.98 79.17 79.66 81.02 79.63 79.02 79.91 79.98 78.04 80.50 51.66 59.40 56.36 63.08 57.29 56.34 54.66 57.02 57.94 64.92 64.38 74.35 74.19 76.16 74.66 73.09 73.72 73.40 75.77 74.82 73.32 72.93 74.35 76.32 76.40 74.51 76.72 77.98 74.66 78.06 75.06 73.88 76.16 68.69 66.11 66.57 49.81 58.23 67.40 57.01 64.59 43.90 50.72 66.49 68.69 58.68 68.23 73.01 76.12 75.28 76.42 75.13 75.36 71.34 69. 66.89 67.05 67.07 65.86 65.39 67.20 66.34 66.92 65.18 65.25 67.49 66.89 67.62 68.14 70.28 70.34 69.50 68.94 69.98 69.98 70.08 70.58 To assess the impact of RePO on downstream task performance, we evaluate models trained with different preference optimization methods on diverse set of tasks from the Huggingface Open Leaderboard (Beeching et al., 2023). The tasks include MMLU (Hendrycks et al., 2020), ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), TruthfulQA (Lin et al., 2022), Winograd (Levesque et al., 2012), and GSM8K (Cobbe et al., 2021). We adhere to standard evaluation protocols and present the results for all models in Table 5. 3https://github.com/princeton-nlp/SimPO/tree/main/eval 4https://github.com/huggingface/alignment-handbook 17 RePO: ReLU-based Preference Optimization Overall Performance. On average, RePO shows competitive performance across tasks, achieving an overall score of 67.49 on the Llama3-Instruct model and 70.58 on the Llama3-Instruct v0.2 model. The performance is generally close to that of other preference optimization methods, but it is worth noting that in some cases, it slightly lags behind models like SimPO or DPO, particularly on tasks such as ARC, HellaSwag, and TruthfulQA. However, the results suggest that RePO maintains balanced performance profile across the evaluated tasks. General Knowledge and Reasoning. On MMLU, which tests general knowledge and reasoning, RePO shows slight reduction in performance (64.95 for Llama3-Instruct and 65.00 for Llama3-Instruct v0.2) compared to models such as RRHF and SimPO. This minor decline is consistent with the trend observed for other preference optimization methods and indicates that RePO may preserve general knowledge to similar extent while possibly focusing more on improving performance in other areas such as reading comprehension and reasoning. Reading Comprehension and Commonsense Reasoning. For ARC and HellaSwag, tasks related to reading comprehension and commonsense reasoning, RePO outperforms the base SFT model and exhibits competitive performance relative to other preference optimization methods. The Llama3-Instruct v0.2 model with RePO achieves score of 80.50 on HellaSwag, which is comparable to the best-performing methods. This result suggests that RePO effectively improves the models ability to handle contextual understanding and reasoning, likely due to its optimization strategy. Truthfulness. On the TruthfulQA task, RePO consistently shows improvements over the base SFT model, with score of 60.96 for Llama3-Instruct and 64.38 for Llama3-Instruct v0.2. This indicates that RePO helps the model generate more truthful and reliable responses, aligning with trends observed in other preference optimization methods. The improvement in this area is especially notable given the inherent difficulty of this task, which tests the models ability to avoid generating false information. Math Performance. The GSM8K benchmark, which tests mathematical reasoning, shows drop in performance for RePO relative to the base SFT model. Specifically, the Llama3-Instruct model with RePO achieves score of 66.49, which is lower than other methods such as SimPO or R-DPO, which focus more on improving mathematical reasoning. This drop is consistent with the trend observed across various preference optimization methods and may suggest that RePO is less effective in retaining mathematical reasoning abilities. Further investigation into this issue could provide insights into potential strategies for addressing this gap. Task-Specific Variability. Overall, RePO exhibits varied performance across tasks. While it performs well in certain areas, such as commonsense reasoning and truthfulness, it lags behind in others, particularly in general knowledge (MMLU) and mathematical reasoning (GSM8K). This variability is in line with the performance trends observed for other preference optimization methods, which often show task-dependent improvements and declines. This suggests that RePO has strengths in some domains, but it may benefit from further refinement to improve performance across all tasks. Conclusion and Future Work. In summary, RePO shows promising results across range of downstream tasks, with notable improvements in tasks related to commonsense reasoning and truthfulness. However, it faces challenges in preserving general knowledge and mathematical reasoning. Future work could focus on incorporating additional strategies to mitigate performance drops in these areas, possibly through the integration of task-specific objectives or enhanced fine-tuning techniques. Additionally, further empirical studies are needed to explore the impact of different preference optimization datasets and objectives on downstream performance. C.3. RePO with varying γ Figure 8 illustrates the effect of the hyperparameter γ on model performance across two evaluation metrics: LC Win Rate and Raw Win Rate. The analysis is conducted on two models, Llama3-8B-Instruct (left) and Gemma2-9B-Instruct (right). The LC Win Rate, shown in red (left y-axis), represents the models alignment with learned preferences, whereas the Raw Win Rate, shown in blue (right y-axis), evaluates overall ranking performance based on human preference comparisons. Moderate γ values lead to optimal performance. Moderate values of γ (0.40.6) yield the best balance between preference alignment and generalization. Both models achieve their highest LC Win Rate in this range, indicating that preference optimization is most effective when applied at an intermediate level. As γ increases beyond 0.6, LC Win Rate starts to decline, likely due to overfitting, where the model overly aligns with preference data at the expense of generalization. Conversely, at γ = 0.0, where no preference optimization is applied, the LC Win Rate remains low, emphasizing the necessity of preference tuning. 18 RePO: ReLU-based Preference Optimization Figure 8. Impact of the hyperparameter γ on LC Win Rate and Raw Win Rate for Llama3-8B-Instruct (left) and Gemma2-9B-Instruct (right). Figure 9. Gradient weighting functions of RePO++ (sθ I(Mθ < γ)). Raw Win Rate trends reveal model robustness differences. The Raw Win Rate follows similar trend but highlights differences in robustness across models. For Llama3-8B-Instruct, the Raw Win Rate peaks at γ = 0.4 before declining, suggesting that excessive preference optimization (γ > 0.6) negatively impacts the models ability to generalize. In contrast, Gemma2-9B-Instruct exhibits more stable Raw Win Rate across wider range of γ, reaching its highest performance at γ = 0.6 before experiencing sharp decline at γ = 0.8. This suggests that Gemma2-9B-Instruct maintains better robustness to preference optimization compared to Llama3-8B-Instruct. Gemma2-9B-Instruct outperforms Llama3-8B-Instruct. Gemma2-9B-Instruct consistently outperforms Llama3-8BInstruct in both LC Win Rate and Raw Win Rate. This observation indicates that Gemma2-9B-Instruct not only aligns more effectively with learned preferences but also retains superior generalization capability. The results highlight the importance of carefully selecting γ to avoid performance degradation at extreme values. Future work could explore adaptive strategies for dynamically tuning γ, ensuring that preference optimization enhances alignment without compromising generalization. C.4. Analsis on RePO++ Figure 9 illustrates the relationship between the gradient and the implicit reward margin. As shown in the figure, when the implicit reward margin is greater than γ, the gradient becomes zero. In this case, the model can stop updating for well-separated pairs, thus preventing overfitting. On the other hand, when the implicit reward margin is less than γ, the model continues to increase the weight for less-separated pairs. Furthermore, the harder the pair is to distinguish, the larger the gradient becomes, eventually converging to 1.0. This behavior is reminiscent of curriculum learning, where more difficult samples are assigned higher weights."
        }
    ],
    "affiliations": [
        "Alibaba Group, Hangzhou, China",
        "University of Science and Technology of China, Hefei, China"
    ]
}