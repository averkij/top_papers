{
    "paper_title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs",
    "authors": [
        "Jeongseok Hyun",
        "Sukjun Hwang",
        "Su Ho Han",
        "Taeoh Kim",
        "Inwoong Lee",
        "Dongyoon Wee",
        "Joon-Young Lee",
        "Seon Joo Kim",
        "Minho Shim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video large language models (LLMs) achieve strong video understanding by leveraging a large number of spatio-temporal tokens, but suffer from quadratic computational scaling with token count. To address this, we propose a training-free spatio-temporal token merging method, named STTM. Our key insight is to exploit local spatial and temporal redundancy in video data which has been overlooked in prior work. STTM first transforms each frame into multi-granular spatial tokens using a coarse-to-fine search over a quadtree structure, then performs directed pairwise merging across the temporal dimension. This decomposed merging approach outperforms existing token reduction methods across six video QA benchmarks. Notably, STTM achieves a 2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and a 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is query-agnostic, allowing KV cache reuse across different questions for the same video. The project page is available at https://www.jshyun.me/projects/sttm."
        },
        {
            "title": "Start",
            "content": "Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs Jeongseok Hyun1* Sukjun Hwang2 Su Ho Han1 Taeoh Kim3 Inwoong Lee Dongyoon Wee3 Joon-Young Lee4 2Carnegie Mellon University Seon Joo Kim1 Minho Shim3 3NAVER Cloud 4Adobe Research 1Yonsei University 5 2 0 2 0 1 ] . [ 1 0 9 9 7 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Video large language models (LLMs) achieve strong video understanding by leveraging large number of spatiotemporal tokens, but suffer from quadratic computational scaling with token count. To address this, we propose training-free spatio-temporal token merging method, named STTM. Our key insight is to exploit local spatial and temporal redundancy in video data which has been overlooked in prior work. STTM first transforms each frame into multigranular spatial tokens using coarse-to-fine search over quadtree structure, then performs directed pairwise merging across the temporal dimension. This decomposed merging approach outperforms existing token reduction methods across six video QA benchmarks. Notably, STTM achieves 2 speed-up with only 0.5% accuracy drop under 50% token budget, and 3 speed-up with just 2% drop under 30% budget. Moreover, STTM is queryagnostic, allowing KV cache reuse across different questions for the same video. The project page is available at https://www.jshyun.me/projects/sttm. 1. Introduction Integrating visual understanding capabilities into Large Language Models (LLMs) has led to significant advances in multimodal systems [1, 11, 17, 45, 47, 50, 56]. However, Video LLMs, which extend these capabilities to video understanding, face unique computational challenges due to the inherently large number of visual tokens required to represent spatio-temporal information [7, 26, 32, 35, 39, 58]. Early video LLMs [26, 27, 39, 58] reduce token count by training abstractors (e.g., Q-Former [25]) to compress visual information. FlashAttention (FA) [9, 10] reduces attentions quadratic memory cost to linear, enabling the training of long-context LLMs. Building on this, LLaVAstyle [29] video LLMs use linear projectors that preserve *This work was done during an internship at NAVER Cloud. Co-corresponding authors. Figure 1. Comparison of training-free token reduction methods using LLaVA-Video-7B under 50% and 30% pre-filling token budgets. Query-aware (Q. Aware) methods require re-computation for each new query, whereas query-agnostic methods support KVcache reuse. The evaluated video QA datasets cover short (<3 min), long (<1 hr), and needle-in-a-haystack (NIAH) videos. (a, b): Per-dataset accuracy. (c, d): Average results across all. high-resolution spatial features and deliver strong performance [59, 60]. More recently, Ring Attention [31] extends FA [9] across GPUs, enabling very long-context models, such as LWM [30] and LongVILA [7], to process hour-long videos with high spatial and temporal resolution features. Long latency remains key bottleneck for deploying video LLMs. Attention FLOPs still scale quadratically with token count, and long video contexts must be processed in full to pre-fill Key-Value (KV) states before answering any In practice (e.g., Gemini [16]), KV states question [34]. for the video are cached to avoid recomputation for subse1 quent queries, supporting efficient multi-turn querying over shared video context. However, existing training-free token reduction methods [6, 15, 20, 43], which avoid the overhead of retraining, overlook this scenario and instead focus on query-aware strategies that discard video tokens based on attention scores between the video and the query, without preserving KV cache reusability. Designing query-agnostic token reduction method is essential for enabling KV cache reuse, but it is challenging due to the lack of signals to guide token selection. VideoMAE [12, 46] demonstrates that videos can be reconstructed from lower token ratio than images, highlighting the inherently redundant nature of video content due to its spatio-temporal continuity. However, existing trainingfree token reduction methods do not explicitly leverage this spatio-temporal structure [6, 15, 38, 43, 49]. To address this, we propose novel spatio-temporal token merging method, named STTM, which is applied at an early layer of the LLM. STTM performs decomposed merging: it first merges tokens along the spatial dimension, followed by merging along the temporal dimension. To perform spatial merging, we build multi-level quadtree for each video frame, linking each parent node to its four child nodes. parent nodes token is retained at the coarsest level when its similarity to all four child nodes exceeds threshold. Regions containing fine-grained details indicated by low similarity are subdivided to the next finer level to preserve high-frequency information. Consequently, each frame is represented by multi-granular spatial tokens, capturing both coarse and fine detail. Extending spatial merging to the spatio-temporal domain is not trivial, due to varying spatial granularities across frames. To handle this challenge, we exploit spatiotemporal locality by restricting merging candidates to spatially overlapping tokens between consecutive frames. Unlike prior temporal-only methods that compare singlegranularity tokens across adjacent frames [15], our approach fully leverages the spatial structure and temporal continuity in video data to guide token merging. After comparing spatially overlapping tokens across time, similar token pairs are chained into spatio-temporal graphs. Within each graph, nodes are merged into single token representing token tracklet, with merging directed toward the earlier frame. This strategy accumulates temporal changes into the token where the content first appears. The same region may be represented at different spatial levels across frames, leading to two cases: many-toone (fine-to-coarse) and one-to-many (coarse-to-fine). The many-to-one case is straightforward: merging fine tokens into single coarse token in an early frame. The one-toIdeally, we would select the many case is complicated. most similar fine-grained token as the destination, but this requires per-region comparisons that are difficult to vectorize and inefficient on GPUs. To address this, we approximate the merge direction by selecting the top-left token among candidates. This approximation allows us to implement temporal merging using vectorized union-find algorithm [44], leading to efficient parallel processing. Comprehensive experimental results across six video QA benchmarks demonstrate that our proposed method effectively reduces video tokens while maintaining high performance. As shown in Fig. 1, STTM outperforms both query-aware and query-agnostic methods overall. In the average results Fig. 1 (c, d), it achieves the highest accuracy with competitive latency under both 50% and 30% token budgets. closer look at the per-dataset accuracy in Fig. 1 (a, b) further highlights its robustness, particularly on the challenging NIAH and long video datasets. Moreover, we validate STTMs generalization by applying it to other LLMs and large-scale 72B Video LLM. 2. Related Work 2.1. Spatio-Temporal Redundancy in Videos Videos exhibit strong spatio-temporal locality information within nearby regions in space and time is often redundant. This property has long been exploited across various domains. In deep learning, CNNs [18, 23] leverage spatial locality and are extended to 3D convolutions [4, 48] to model spatio-temporal correlations in video. In transformers, MAE [19] and VideoMAE [12, 46] show that reconstructing masked regions using local context is an effective self-supervised learning objective, implicitly benefiting from redundancy. Similarity, video compression methods explicitly eliminate redundancy. Quadtree-based partitioning [13, 37] adapts block resolution based on regional complexity [41, 42, 52]. Temporal redundancy is also reduced, for example through the Skip mode in H.264/AVC [52], which omits re-encoding regions that remain unchanged across frames. Inspired by these principles, we propose multi-granular spatio-temporal token merging method that explicitly compresses redundant video tokens by exploiting local similarity in both space and time. 2.2. Long Context Modeling in Video LLMs In the early stages, LLMs had limited maximum context length; for instance, LLaMA [47] supported only 2048 tokens. To handle extensive video context, multimodal LLMs integrate abstractor architectures [5, 24, 25] that reduce visual tokens before passing them into the language model. For video LLMs, Q-Former [25] based architecture has been widely adopted to reduce the number of visual tokens from video [2628, 32, 35, 36, 39, 51, 58], instead of using linear projectors, which preserve the original token count. However, with the introduction of FlashAttention [9, 10], 2 Figure 2. (Left) Our spatio-temporal token merging method is training-free, plug-and-play module that produces spatio-temporally multigranular tokens. (Middle) In step 1, tokens are merged based on spatial locality, where similar tokens within 2D grid are combined into single token. (Right) In step 2, spatially multi-granular tokens are further merged along the temporal dimension, where similar tokens across frames are consolidated into their earliest occurrence. The arrows indicate the direction of token merging. The green lines indicate merging over one timestep, and magenta lines are merging over two timesteps. The scale and the number of tokens are set for illustration. which reduces the memory complexity of attention from linear projectorquadratic to linear in sequence length, based approaches have recently gained more attention and showed better performance than abstractor-based approaches. Furthermore, recent methods such as RingAttention [31] have enabled video LLMs [7, 30] to process up to one million video tokens. Following this trend, handling thousands of frames without downsampling is expected to become common in the future. However, such large number of tokens significantly increases latency, highlighting the need for effective video token compression. 2.3. Training-Free Token Reduction in Video LLMs Token reduction aims to identify and reduce highly redundant or less informative tokens, and has been researched in the architecture of Vision Transformer [2, 8]. Due to high computational costs in long video LLMs [7], studying token reduction in the regime of multimodal LLMs is being spotlighted [6, 15, 20, 21, 38, 43, 49, 55]. In the initial works [6, 38], visual tokens are reduced without considering the visual structure such as the 2D locality. One-dimensional token reduction methods treating visual tokens as sequential stream, similar to text tokens, without considering spatial or temporal relationships. FastV [6] reduces low-rank visual tokens based on attention patterns in the early layers. Additionally, it employs text queries to search attention weights, requiring token reduction to be performed again for each new query. In contrast, our method is query-agnostic, allowing KV cache reuse in LLMs, making it more efficient for conversational and multi-question scenarios. Temporal token reduction methods exploit the redundancy between video frames. FrameFusion [15] demonstrates that the token similarity distribution condenses in deeper layers while preserving ranking consistency. It merges similar tokens between frames using similarity metrics and prunes unnecessary tokens based on attention scores. We argue that leveraging spatio-temporal characteristics, rather than operating in single dimension, enables more effective and efficient token reduction. 3. Methodology We propose Spatio-Temporal Token Merging (STTM) that merges video tokens along spatial and temporal dimensions, producing multi-granular video tokens. This module is training-free and can be easily plugged into an intermediate layer of an LLM. As the STTM operates along the spatiotemporal dimension and outputs multi-granular tokens, it is designed as single-pass operation, inserted into single early layer of the transformer. Furthermore, our token reduction method is question-agnostic, enabling the reuse of KV caches across different questions for the same video. 3.1. Overview of STTM Module As illustrated in Fig. 2, we use the video tokens from the previous layers output to perform token merging. After the merging process, the video tokens, ZV RT HW C, are reduced into spatio-temporally merged tokens, ZST RNST C, where NST . To exploit spatial locality, we first merge tokens that are similar within the 2D grid of each frame. As labeled by Lv.1, Lv.2, and Lv.3 in Fig. 2, we define the different scales of the 2D grid for merging unit. Based on these multi-scale merging grids, large redundant region can be represented by single coarse-grained token, achieving high token reduction ratio. On the other hand, fine-grained tokens are used for representing the region with large variation. This spatial merging process results in spatially multi-granular tokens, Ti C, where NTi . RN Ti In videos, locality further extends to the spatio-temporal dimension. Thus, we merge tokens by comparing their similarities at the same 2D region across consecutive frames. When tokens remain similar over time, we chain them along connected path and merge them into the earliest occurrence. Tokens with different spatial granularities can also be For each level, we compute the cosine similarity between the current levels nodes and their child nodes. When the similarities of four child nodes are higher than the spatial threshold value, τS, we can regard this region has low spatial detail and terminate the further searching process for the corresponding node. As result, all child nodes for this node are pruned in the quadtree. In contrast, when any of the child nodes show low similarity, we prune the current node and instead use the child nodes to represent the corresponding region. Thus, this quadtree search process can be regarded as the granularity decision process. The computational complexity of our quadtree-based spatial merging algorithm follows O(HW ) per frame. The quadtree search is an iterative process that performs four comparisons for each node at given level. In the worst case scenario, where all nodes are subdivided down to the deepest level, the computational cost of spatial token merging, CST , is the geometric series following: Lv (cid:88) CST = HW 4i 4 (1) i=1 Asymptotically, CST is bounded by the sum of infinite geometric series: lim Lv CST = HW 1 1 4 = 4HW (2) Notably, we implement the quadtree search algorithm in parallel at each level, ensuring that the actual running time remains linear in complexity. 3.3. Temporal Token Merging To capture the spatio-temporal redundancy, we compute the similarity of tokens representing the same region in neighboring two frames and make the connection between pairs of tokens exhibiting similarity higher than τT . As we merge into the earlier tokens, this process results in directed graphs, where the source node is from + 1, and the destination node is at . Based on these directed graphs built from consecutive frames, we obtain connected graphs and identify the common root node for all nodes in the connected graph. The common root node and other nodes are set as destination and source, respectively, for merging. So, the nodes can be merged not only in consecutive twoframe sequences but also in the far distance. As shown in Fig. 2 (right), token representing the top-left token at T2 is merged into the token at T0, while the top-right token at T2 is merged into the token at T1. This merging process results in spatio-temporally multi-scale tokens. Since spatial merging results in varying scales of tokens, we cannot directly merge the tokens at the same exact spatial location across frames. Instead, we merge the tokens under their spatio-temporally overlapping regions. For example, in Fig. 2 step 2, top-left region is expressed in varying scales across frames. We compare the similarity beFigure 3. coarse-to-fine spatial search is performed using quadtree structure. If all four fine child nodes exhibit high similarity with the coarse parent node, the search process terminates, and the parent node is used to represent the corresponding region. Otherwise, the search continues until the finest level is reached. Here, the scale for each level is an example for illustration. connected across frames when they overlap. As depicted in step 2 of Fig. 2, some fine-grained tokens at T1 are merged into coarser token at T0, while others remain separate due to low similarity. Our spatial and temporal hybrid merging allows multi-granular tokens in both dimensions. 3.2. Spatial Token Merging As illustrated in Fig. 3, our approach employs coarseto-fine hierarchical search based on quadtree data structure [13, 37]. At every level, each region requires only four comparisons for the granularity decision process; this process determines whether features at the current level sufficiently represent the region. If not, finer-grained features from the next level are adopted to represent the region without losing details. This hierarchical method effectively balances computational efficiency and representation flexibility. The quadtree structure organizes spatial tokens into hierarchical representation, where each coarse-level parent node corresponds to four (22) finer-level child nodes within its respective 2D grid region. This hierarchical organization enables structured approach to spatial token merging, preserving 2D locality, which is essential for subsequent temporal merging. Building the quadtree follows three steps. First, we initialize leaf nodes of the quadtree based on the initial token feature map, Ti lv RHW C. Second, we construct multi-scale representation by recursively downsampling the feature map until the feature map size reaches 22, which we denote as Lv.1, representing the coarsest spatial resolution. At each level, the feature map is downsampled by averaging spatially neighboring 22 feature patches, yielding coarser-level representation, Ti 2 C. During this downsampling operation, we connect four leaf nodes from Ti lv1. Third, using this precomputed densely connected quadtree, we perform quadtree search as described in Fig. 3 and keep only the necessary nodes to represent each frame. lv to single parent node from Ti lv1 2 50% MLVU Method VNBench Q. Agn. VideoMME EgoSchema LongVideoBench Token Budget 100% LLaVA-Video 7B 77.6 0.962 11149 63.1 2.039 22086 59.6 1.805 19624 70.9 2.343 25088 58.7 2.312 25069 82.9 0.659 8116 n.a 50.0 96.8 49.3 97.5 49.6 98.6 50.0 98.1 49.3 98.0 43.7 99.5 30.0 92.5 33.2 92.8 28.4 96.2 30.0 93.6 33.2 94.6 27.3 97.8 Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV n.a 50.0 48.4 49.0 50.0 48.4 45.5 30.0 31.9 27.7 30.0 32.0 27. 93.7 + FastV 92.9 + DyCoke 98.2 + FrameFusion 95.5 + ToMe + DyCoke-stage1 94.4 99.8 + STTM (Ours) 79.3 + FastV 72.0 + DyCoke 93.3 + FrameFusion 82.7 + ToMe + DyCoke-stage1 82.3 98.0 + STTM (Ours) 50.4 50.0 98.2 45.4 47.7 99.9 48.4 98.4 47.0 50.0 100.1 51.9 47.7 100.1 48.4 45.2 48.6 99.9 33.4 30.0 97.1 30.0 31.1 99.4 28.4 27.0 97.8 30.0 97.9 36.1 31.1 100.0 34.6 33.4 29.2 98.9 50.0 96.2 50.8 47.9 97.5 45.4 48.7 99.4 47.2 50.0 97.4 52.6 48.6 47.9 97.6 47.1 100.0 49.6 33.8 30.0 91.8 30.2 31.2 95.9 28.8 27.4 95.7 36.5 30.0 94.5 35.0 31.2 94.9 34.1 25.8 95.6 50.0 96.7 49.4 97.4 49.4 98.3 50.0 97.4 49.4 98.2 45.5 99.2 30.0 93.8 33.0 96.2 28.1 96.2 30.0 93.8 33.3 95.0 25.8 98.8 50.0 96.4 48.5 98.1 49.1 97.9 50.0 98.4 48.5 98.4 45.6 98.6 30.0 94.5 32.1 95.0 27.9 95.2 30.0 94.5 32.1 96.8 28.3 96.7 50.0 99.3 47.7 99.0 49.1 99.6 50.0 99.6 47.7 99.3 42.8 99.5 30.0 98.4 31.1 98.3 27.4 98.8 30.0 98.4 31.1 98.5 29.1 98. n.a 51.2 46.3 47.9 52.7 49.6 48.4 34.1 30.9 29.5 36.8 35.7 33.1 52.3 47.6 49.0 53.8 51.4 47.3 34.9 32.0 30.3 37.8 37.2 31.1 50.7 44.7 47.1 51.1 48.1 50.1 33.5 29.3 28.5 35.3 34.3 31.4 53.5 50.0 50.9 56.1 53.0 48.9 36.4 34.7 33.0 40.2 38.9 35.7 49.7 44.7 46.3 50.9 47.9 49.2 32.7 29.2 27.8 35.0 34.0 32.8 NExT-QA 30% Avg. Table 1. Comparison of training-free token reduction methods using LLaVA-Video-7B under 50% and 30% pre-filling token budgets. Token-reduced results are reported relative to the result with 100% . tween single Lv.1 token at T0 and 2 2 Lv.2 tokens at T1. Two Lv.2 tokens at T1 with high similarity are merged into the token at T0, while the other two remain to represent change in T1s top-left region. During temporal merging, we set the direction of merging as tokens at the earlier frame. In the case of top-left at T1 and T2, the token at T2 is coarser than T1 and has four possible destination paths for merging. When there are multiple connections with similarity higher than τT , we provide the priority to the token located towards top-left. We simplify the choice of choosing the top-left merging token for such multiple destination cases primarily for parallel indexing implementation, although merging with the most similar token is the naive method. The computational complexity of our temporal merging algorithm follows O(T HW ) per video. Suppose the worst case is that spatial tokens of all frames are in the finest level, incurring (T 1) comparisons. However, we can expect more efficiency gain since spatial merging results in fewer tokens at every frame, reducing the comparisons. Computing the common root node can also be achieved in O(T HW ) by adapting the union-find algorithm [44] into vectorized form. 3.4. Token Reordering after Merging After spatio-temporal token merging, the resulting video tokens form graph structure. To serve as input for an LLM, this graph must be linearized into one-dimensional sequence. In our reordering strategy, we prioritize tokens based on two criteria. First, spatial ordering follows Zshaped scan based on each tokens top-left coordinate. Second, temporal ordering gives precedence to tokens from earlier frames over those from later ones. Once reordered into 1D sequence, handling positional embeddings becomes crucial. Recent LLMs typically use rotary positional embeddings (RoPE) [40], which apply rotation matrix computed from each tokens position index. We evaluate three strategies for assigning RoPE after merging: (1) Merged RoPE, which averages the RoPEs of merged tokens; (2) Survived RoPE, which retains the original RoPEs of the surviving tokens; (3) Reassigned RoPE, which reassigns position IDs based on the new token order for alignment with the original positional encoding. 4. Experiments 4.1. Evaluation Setting Datasets. To focus on visual understanding, we adopt an evaluation setting without subtitles. We evaluate methods on six diverse video QA benchmarks. These include short-form datasets, EgoSchema [33] and NExT-QA [54], as well as long-form datasets with hour-long videos: VideoMME [14], LongVideoBench [53], and MLVU [62]. To rigorously evaluate fine-grained spatio-temporal understanding, we use VNBench [61], synthetic dataset simulating the needle in haystack task [22]. It introduces subtle visual or textual needles into short segments of video irrelevant to the original content but relevant to the question. Evaluation Metrics. We report standard accuracy for multiple-choice question answering. For practicality, we include the average running time (in seconds) and the number of visual tokens (NV ). Since our focus is on the pre-filling stage, we measure the time-to-first-token (TTFT). In addition to absolute values, we provide relative values (R.) with respect to performance without token reduction. Implementation Details. We sample video frames at 1 FPS, but uniformly sample frames if video exceeds the maximum frame limit. To ensure coverage of injected needles in VNBench, we set the maximum number of frames to 180; for other datasets, we use limit of 128 frames. single and four A100 80G GPUs are used for 7B and 72B models, respectively. Our merging threshold values (τS and τT ) are empirically adjusted to approximately meet specific token budget for fair comparisons with other methods. 4.2. Comparison with Existing Methods We compare other methods [2, 6, 15, 43] under the same experimental setup. ToMe [2] is applied within an LLM rather than its original ViT-based setting. DyCoke-stage1 refers to 5 50% Method VNBench Q. Agn. VideoMME LongVideoBench MLVU EgoSchema NExT-QA Avg. Token Budget 100% LLaVA-OV 7B 50.7 95.3 + FastV 45.3 94.3 + DyCoke 47.2 98.6 + FrameFusion 97.5 52.4 + ToMe + DyCoke-stage1 94.6 49.7 103.6 43.8 + STTM (Ours) 32.8 86.1 + FastV 30.3 78.4 + DyCoke 28.3 96.0 + FrameFusion 86.0 36.0 + ToMe + DyCoke-stage1 84.9 35.1 102.3 30.1 + STTM (Ours) Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV n/a 50.0 48.4 49.7 50.5 48.4 43.8 30.0 32.0 28.6 30.0 32.0 28.7 68.8 0.922 11149 59.0 1.904 22086 56.3 1.712 19624 67.5 2.224 25088 61.3 2.216 25069 80.6 0.630 8116 n/a n/a 50.0 98.8 50.0 99.9 49.7 48.1 49.3 100.2 44.2 48.5 101.8 42.3 50.0 100.1 46.2 49.6 101.0 44.7 50.0 100.7 51.2 50.0 102.0 49.5 49.3 100.3 48.6 48.5 102.5 46.7 46.4 102.1 45.1 44.6 103.3 45.9 32.1 30.0 96.4 30.0 97.2 30.6 29.1 33.2 96.7 32.1 101.2 26.9 27.3 29.5 98.8 28.5 98.8 25.6 30.0 98.3 30.0 102.4 33.3 34.9 34.2 33.2 98.0 32.1 100.7 32.5 33.1 101.1 32.0 30.0 101.4 33.0 52.3 50.0 99.5 47.7 99.7 47.8 49.9 100.2 49.5 50.0 99.4 54.4 47.7 100.0 51.8 49.5 43.6 99.8 34.6 30.0 98.8 33.0 31.1 98.9 31.0 28.9 99.1 38.1 30.0 98.6 37.4 31.1 98.8 38.1 22.4 98. 50.0 99.4 48.8 47.7 100.8 42.8 49.3 99.1 45.0 50.0 101.6 50.0 47.7 100.8 47.3 46.6 100.6 43.9 30.0 99.3 31.3 31.1 101.2 27.4 27.6 98.3 26.3 30.0 101.3 33.8 31.1 101.5 33.0 31.8 100.7 25.7 50.0 99.7 49.2 47.9 102.8 43.2 49.4 100.5 45.5 50.0 102.5 50.5 47.9 103.3 48.0 37.5 102.4 47.0 31.7 30.0 98.1 31.2 99.3 28.5 28.0 100.4 26.6 30.0 100.3 34.3 33.7 31.2 99.7 27.3 100.5 33.3 50.0 99.0 49.1 49.4 101.8 43.7 49.8 101.4 45.4 50.0 101.4 50.6 49.4 100.6 47.9 44.2 102.8 40.6 30.0 98.6 31.4 33.3 101.4 28.2 29.0 100.1 26.2 30.0 101.4 34.1 33.3 102.3 33.2 27.4 102.6 31.6 30% Table 2. Comparison of training-free token reduction methods using LLaVA-OneVision-7B. Relative to 100% result ."
        },
        {
            "title": "LongVideoBench",
            "content": "Token Budget Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV 100% Qwen2VL 7B n.a n.a 66.4 2.438 22025 61.8 10.745 74982 56.8 10.597 72109 n.a 50.0 50.0 99.0 46.3 + ToMe 43.4 95.5 48.8 48.8 100.2 40.3 + DyCoke-stage1 98.1 43.0 51.5 102.7 39.2 50.9 + STTM (Ours) 105.2 29.8 30.0 27.7 30.0 94.8 30.3 + ToMe 85.6 32.4 32.3 94.2 + DyCoke-stage1 81.6 26.5 29.0 27.7 100.5 22.1 30.4 + STTM (Ours) 100.4 17.2 50.0 100.1 42.0 49.6 101.2 38.8 48.9 101.8 44.3 26.4 30.0 99.3 33.4 99.9 25.2 30.6 101.0 25.7 50.0 101.4 41.9 48.1 101.2 39.1 52.3 101.1 43.5 30.0 99.5 26.3 31.4 101.2 25.4 32.9 100.1 23.3 30% 50% Avg. Table 3. Comparison using Qwen2VL-7B. Relative to 100% result ."
        },
        {
            "title": "Method",
            "content": "VideoMME Acc TTFT NV Token Budget 100% LLaVA-Video 72B 70.5 17.698 22086 50.0 47.9 44.2 30.0 31.2 30.5 100.1 47.6 + ToMe 45.5 + DyCoke-stage1 99.8 101.3 44.2 + STTM (Ours) 29.3 97.1 + ToMe 30.2 + DyCoke-stage1 98.3 99.1 30.5 + STTM (Ours) 50% 30% Table 4. Comparison using LLaVAVideo-72B. Relative to 100% result . technique for pre-filling stage [43]. We report relative values here, with absolute values provided in Appendix. Performance of LLaVA-Video-7B. Our method outperforms both query-aware and query-agnostic methods on average across benchmarks (Tab. 1). On average , it incurs only 0.5% and 2.2% relative accuracy drops under the 50% and 30% budgets, respectively. On VNBench (30% budget), other Q.Agn methods exhibit large drops (about 18%), but it shows only 2.0% drop. These results indicate that our method better preserves fine-grained spatio-temporal details. All methods apply token reduction with O(N ) complexity; thus, TTFT is dominated by attention in the LLM layers and scales with the number of retained tokens. Since tokens are reduced at the ipnut or in the early LLM layers, all methods exhibit similar TTFT at each token budget. Generalization to Other MLLMs. We also evaluate our method with LLaVA-OneVision (Tab. 2) and Qwen2VL (Tab. 3), and it continues to outperform other methods on both MLLMs. Notably, it even improves accuracy while using fewer tokens (under both 50% and 30% budgets). We observe accuracy improvements of 1.1% and 0.5%, along with 3.1 and 4.5 speed-ups, for OneVision and Qwen2VL, respectively. Qwen2VL consumes more visual tokens than LLaVA-based models, resulting in greater latency reduction due to the quadratic complexity of attention. Scaling to 72B. Our method continues to outperform other methods on the 72B LLM under both 50% and 30% token budgets (Tab. 4). Notably, it even improves accuracy by 1.3% while using only 44.2% of the tokens. The 72B model requires substantial processing time per request (e.g., an average of 17.7 seconds). As query-agnostic method, it performs token reduction without relying on the question, enabling reuse of the KV cache for the same video across different questions. This improves deployment efficiency in multi-turn or multi-query scenarios. 4.3. Ablation Study We conduct ablation studies to validate effectiveness of the proposed components using LLaVA-Video-7B. Multi-Granularity Spatial Tokens. To evaluate the effectiveness of our hierarchical spatial token merging method, we compare it against single-granularity method using bilinear interpolation (Tab. 5). On VideoMME, the singlegranularity method performs competitively, even surpassing FastV [6] while using only 41.3% of the tokens. However, its accuracy drops significantly on VNBench which requires fine-grained spatial information. In contrast, our quadtreebased multi-granularity merging maintains accuracy across both datasets. At τS of 0.80, it incurs only 1.3% and 0.8% drop in relative accuracy with token budget of approximately 50%. This highlights the advantage of incorporating multi-granularity spatial tokens for balancing compression efficiency and accuracy retention. Root Node Spatial Resolution. As shown in Tab. 5, using coarser spatial scale (22) for root node initialization leads to lower relative NV at the same spatial threshold. This indicates that more tokens are merged and each frame is represented by coarser spatial tokens. However, this higher compression ratio comes at the cost of significant accuracy drop compared to using root nodes with 44 spatial scale. The effect is particularly pronounced at lower τS, where the accuracy degradation is more severe,"
        },
        {
            "title": "Token\nGranularity",
            "content": "Lv.1 Scale τS"
        },
        {
            "title": "VideoMME",
            "content": "R.Acc R.NV R.Acc R.NV"
        },
        {
            "title": "Single\nMulti\nMulti\nSingle\nMulti\nMulti\nSingle\nMulti\nMulti",
            "content": "1313 22 0.85 44 0.85 1111 22 0.80 44 0.80 99 22 0.75 44 0."
        },
        {
            "title": "FastV",
            "content": "82.1 88.1 99.9 80.0 73.1 98.7 78.9 62.6 93.0 93.7 86.2 54.2 82.0 61.7 27.0 57.2 41.3 13.3 35.8 50.0 99.2 98.1 99.5 98.4 96.2 99.2 97.4 92.5 97.5 96.7 86.2 58.2 83.0 61.7 31.5 59.9 41.3 15.8 38.9 50.0 Table 5. Ablation study on the proposed multi-granular spatial token merging. Lv.1: spatial scale of root nodes."
        },
        {
            "title": "VideoMME",
            "content": "R.Acc. R.NV R.Acc. R.NV 98.7 100.0 98.0 73.3 57.2 53.7 25.8 27. 99.2 99.9 98.8 95.5 59.9 56.7 25.8 32.0 Table 6. Effectiveness of spatial and temporal merging modules. Sequentially combining these two modules results in synergistic effect. Last row: joint merging."
        },
        {
            "title": "LLM\nLayer",
            "content": "1 3"
        },
        {
            "title": "VideoMME",
            "content": "R.Acc R.TTFT R.NV R.Acc R.TTFT R.NV 95.8 98.0 98.2 99.5 28.3 25.8 24.8 31.7 Table 9. Ablation study of token merging position. 96.0 98.8 97.9 99.2 26.5 31.3 41.6 71.6 27.9 30.9 40.6 75. 26.4 25.8 25.6 22.6 Method R.Acc R.TTFT R.NV Optimal Top-left Optimal Top-left 99.6 99.2 97.7 98.8 47.7 47.1 27.7 25.8 58.1 50.1 39.6 31.4 Table 7. Temporal merging ablation"
        },
        {
            "title": "Merging\nSurvival\nReassignment",
            "content": "VNB R.Acc 96.0 96.5 98.0 VidMME R.Acc 95.7 96.0 98.8 Table 8. Ablation study on positional embedding after merging. G-Token VNB VidMME LVB MLVU EgoS NExT 78.2 77. 62.6 63.1 58.9 59.6 70.6 70.9 58.5 58.7 82.2 82.9 Avg. () 68.5 68.8 (+0.3) Table 10. Effect of grid token removal on accuracy. especially on VNBench. This is due to the loss of spatial details, as quadtree subdivision from 22 root may terminate prematurely, producing tokens that are too coarse to retain essential visual information. To address this, we adopt 44 spatial scale for root nodes, which enables lower τS for higher compression while preserving accuracy. Decomposed Spatio-Temporal Merging. In our algorithm, we first perform spatial merging, followed by temporal merging in sequence. Tab. 6 presents the impact of spatial and temporal merging, individually. While both methods effectively reduce the number of tokens, applying them sequentially yields synergistic effect, achieving high token reduction ratio with only marginal accuracy drop. Additionally, we experiment with joint spatio-temporal merging method based on an octree data structure, which partitions long video into predefined cubic segments and recursively subdivides them at 222 finer scale. While this method shows promising results on VideoMME, it leads to significant accuracy drop on VNBench, where rapid spatial changes occur within short time intervals. This suggests that rigid hierarchical partitioning across spatiotemporal dimensions is not effective in dynamic scenarios. Therefore, we adopt decomposed merging strategy. Approximated Temporal Matching. As shown in Tab. 7, selecting the most similar destination node does not consistently improve performance. In contrast, using top-left approximation enables vectorized union-find algorithm [44], avoiding costly similarity checks, and leads to significant runtime improvement without drop in accuracy. Positional Embedding. Tab. 8 shows the results of difFigure 4. Trade-off of accuracy and visual token retention ratio. ferent strategies for handling positional embeddings during token merging within LLM layers. We observe that preserving the original positional embeddings yields better performance than merging them. Since Qwen2VL [50] uses MRoPE, which computes positions based on (t, y, x), we cannot apply the reassignment strategy and instead adopt the survival strategy. Merging Layer Position. We evaluate the impact of different LLM layer positions for video token merging (Tab. 9). As observed in recent works [3, 6, 55, 57], performing token merging at later layers yields higher accuracy but lower speed-up, due to increased full-token attention computations. Merging tokens before the third LLM layer provides good trade-off between accuracy and latency; we adopt this configuration for 7B models, while for the 72B model, we merge tokens before the first LLM layer. Grid Token. As shown in Tab. 10, removing the special grid token used for video data in LLaVA-Video does not degrade accuracy across benchmarks. To simplify the handling of video tokens during spatio-temporal merging, we omit such token in LLaVA-Video and OneVision. Accuracy vs. Compression. Fig. 4 illustrates the trade-off between accuracy and visual token retention ratio. On both datasets, our method consistently outperforms other queryagnostic token reduction methods [2, 43] across token retention levels, ranging from 50% to 15%. This demonstrates the robustness of our method in achieving favorable balance between compression efficiency and accuracy. 7 Figure 5. Visualization of spatial token merging results. Each image patch within green box represents single token. Figure 6. Visualization of spatio-temporal token merging results on VideoMME. (a) The first eight consecutive frames are sampled. (b) Intermediate frames are sampled for illustration purpose. Empty regions indicate areas that have been merged with early tokens. 4.4. Visualization of Token Merging Unlike uniform reduction methods, STTM adaptively reduces tokens based on the redundancy present in each video. It allocates more tokens to regions that require fine-grained detail, while aggressively reducing tokens in redundant areas. On VideoMME (30% budget), its token retention ranges from 3.3% to 51.2% across videos, demonstrating its adaptability to varying content complexity. Fig. 5 presents the results of spatial merging before applying temporal merging. Fig. 5 (a) illustrates case where semantically similar tokens are effectively merged, reducing 196 visual tokens to 63 in frame. In contrast, Fig. 6 (b) shows worst-case scenario for spatial compression. In this frame, fine-grained tokens are preserved to retain small text details. Although the compression rate is low, OCR accuracy on VideoMME is maintained after token merging. While using all tokens yields an OCR accuracy of 67.6, ToMe, DyCoke-stage1, and our method achieve 58.3, 64.7, and 65.5, respectively, under token reduction. Fig. 6 shows the results of spatio-temporal merging. In Fig. 6 (a), regions with newly emerging content remain intact, while redundant regions across consecutive frames are merged. For instance, letter regions that appear in the second and sixth frames are preserved, while background tokens, such as the black areas, are merged into the earlier frames. Duplicated frames (e.g., frames 3, 4, 7, and 8) are almost entirely merged. Fig. 6 (b) illustrates an example with dynamic scene changes. Similarly, within each scene, later frames tend to be merged into the first frame of the segment (e.g., frames 4 and 5). Notably, subtle but semantically meaningful changes such as facial expressions or hand gestures are preserved in frames 6 and 7. 5. Conclusion This is the first work to explore multi-granular video token merging in training-free manner for video LLMs. We propose decomposed spatio-temporal token merging method, called STTM. Its effectiveness is validated across six video QA benchmarks. Notably, it largely outperforms others on VNBench which requires fine-grained understanding. Since our method operates independently of user instructions, the KV cache can be reused across different questions for the same video, improving computational efficiency. While our method successfully minimizes spatio-temporal redundancy, its performance currently depends on manually adjusted threshold values. Exploring adaptive threshold selection is promising direction, enabling automatic adjustment of token merging based on the given token budget. Acknowledgements. This work was partly supported by the NAVER Cloud Corporation."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1 [2] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. In ICLR, 2023. 3, 5, 7 [3] Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, et al. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling. arXiv preprint arXiv:2406.02069, 2024. 7 [4] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In CVPR, pages 62996308, 2017. 2 [5] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. Honeybee: Locality-enhanced projector for multimodal llm. In CVPR, pages 1381713827, 2024. 2 [6] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration In ECCV, pages 1935. for large vision-language models. Springer, 2024. 2, 3, 5, 6, 7 [7] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Yihui He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling long-context visual language models for long videos. In ICLR, 2025. 1, [8] Joonmyung Choi, Sanghyeok Lee, Jaewon Chu, Minhyuk Choi, and Hyunwoo Kim. vid-tldr: Training free token merging for light-weight video transformer. In CVPR, pages 1877118781, 2024. 3 [9] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2024. 1, 2 [10] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In NeurIPS, 2022. 1, 2 [11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1 [12] Christoph Feichtenhofer, Yanghao Li, Kaiming He, et al. Masked autoencoders as spatiotemporal learners. NeurIPS, 35:3594635958, 2022. [13] Raphael Finkel and Jon Louis Bentley. Quad trees data structure for retrieval on composite keys. Acta informatica, 4:19, 1974. 2, 4 [14] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In CVPR, pages 2410824118, 2025. 5 [15] Tianyu Fu, Tengxuan Liu, Qinghao Han, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei Ning, and Yu Wang. Framefusion: Combining similarity and importance for video token reduction on large visual language models. arXiv preprint arXiv:2501.01986, 2024. 2, 3, 5 [16] Google. Caching google ai, 2024. Accessed: 2025-07-09. 1 [17] Gemini Team Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. pages 770778, 2016. 2 [19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 1600016009, 2022. 2 [20] Xiaohu Huang, Hao Zhou, and Kai Han. Prunevid: Visual token pruning for efficient video large language models. arXiv preprint arXiv:2412.16117, 2024. 2, 3 [21] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In CVPR, pages 1370013710, 2024. 3 [22] Kamradt. Needle in haystack pressure testing llms. https://github.com/gkamradt/LLMTest_ NeedleInAHaystack, 2023. [23] Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, and Lawrence Jackel. recognition with backHandwritten digit propagation network. NeurIPS, 2, 1989. 2 [24] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified In ICML, vision-language understanding and generation. pages 1288812900. PMLR, 2022. 2 [25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, pages 1973019742. PMLR, 2023. 1, 2 [26] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 1, 2 [27] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multiIn CVPR, pages modal video understanding benchmark. 2219522206, 2024. 1 [28] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In EMNLP, 2024. [29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2023. 1 [30] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention. arXiv preprint arXiv:2402.08268, 2024. 1, 3 [31] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. In ICLR, 2024. 1, 3 9 [32] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In ACL, 2024. 1, [33] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. NeurIPS, 36:46212 46244, 2023. 5 [34] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. MLSys, 5:606624, 2023. 1 [35] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In CVPR, pages 14313 14323, 2024. 1, 2 [36] Michael Ryoo, Honglu Zhou, Shrikant Kendre, Can Qin, Le Xue, Manli Shu, Silvio Savarese, Ran Xu, Caiming xgen-mm-vid (blip-3Xiong, and Juan Carlos Niebles. video): You only need 32 tokens to represent video even in vlms. arXiv preprint arXiv:2410.16267, 2024. 2 [37] Hanan Samet. The quadtree and related hierarchical data structures. ACM CSUR, 16(2):187260, 1984. 2, [38] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. 2, 3 [39] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In CVPR, pages 1822118232, 2024. 1, 2 [40] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 [41] Gary Sullivan and Richard Baker. Efficient quadtree coding of images and video. TIP, 3(3):327331, 1994. 2 [42] Gary Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. Overview of the high efficiency video coding (hevc) standard. TCSVT, 22(12):16491668, 2012. 2 [43] Keda Tao, Can Qin, Haoxuan You, Yang Sui, and Huan Wang. Dycoke: Dynamic compression of tokens for fast video large language models. In CVPR, 2025. 2, 3, 5, 6, 7 [44] Robert Endre Tarjan. Efficiency of good but not linear set union algorithm. JACM, 22(2):215225, 1975. 2, 5, [45] OpenAI Team. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 1 [46] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. NeurIPS, 35:10078 10093, 2022. 2 [47] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, [48] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In ICCV, pages 44894497, 2015. 2 [49] Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, and Li Yuan. Look-m: Lookonce optimization in kv cache for efficient multimodal longcontext inference. arXiv preprint arXiv:2406.18139, 2024. 2, 3 [50] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 7 [51] Yuxuan Wang, Cihang Xie, Yang Liu, and Zilong Zheng. Videollamb: Long-context video understanding with recurarXiv preprint arXiv:2409.01071, rent memory bridges. 2024. 2 [52] Thomas Wiegand, Gary J. Sullivan, Gisle Bjontegaard, and Ajay Luthra. Overview of the h.264/avc video coding standard. TCSVT, 13(7):560576, 2003. 2 [53] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. NeurIPS, 37:2882828857, 2025. [54] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In CVPR, pages 97779786, 2021. 5 [55] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. In CVPR, 2025. 3, 7 [56] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 1 [57] Dongjie Yang, XiaoDong Han, Yan Gao, Yao Hu, Shilin Zhang, and Hai Zhao. Pyramidinfer: Pyramid kv cache compression for high-throughput llm inference. arXiv preprint arXiv:2405.12532, 2024. 7 [58] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. EMNLP Demo Track, 2023. 1, 2 [59] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 1 [60] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. [61] Zijia Zhao, Haoyu Lu, Yuqi Huo, Yifan Du, Tongtian Yue, Longteng Guo, Bingning Wang, Weipeng Chen, and Jing Liu. Needle in video haystack: scalable synthetic evaluator for video mllms. arXiv preprint arXiv:2406.09367, 2024. 5 10 [62] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, et al. Mlvu: Benchmarking multi-task In CVPR, pages 1369113701, long video understanding. 2025."
        },
        {
            "title": "Appendix",
            "content": "Tabs. 11 to 14 show the absolute values for the main comparison results. 50% Method VNBench Q. Agn. VideoMME EgoSchema LongVideoBench MLVU Token Budget Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV 100% LLaVA-Video 7B 77.6 0.962 11149 63.1 2.039 22086 59.6 1.805 19624 70.9 2.343 25088 58.7 2.312 25069 82.9 0.659 8116 68.8 1.687 18522 72.7 0.503 5575 61.0 1.034 11043 57.4 0.918 9812 68.3 1.164 12544 57.6 1.166 12535 82.4 0.353 4058 66.6 0.856 9261 + FastV 72.1 0.458 5386 61.5 0.912 10555 58.1 0.820 9393 69.5 1.046 11978 58.6 1.049 11969 82.1 0.330 3957 67.0 0.769 8873 + DyCoke 76.2 0.471 5529 62.0 0.961 10739 59.2 0.853 9616 69.4 1.083 12138 57.7 1.088 12298 82.6 0.336 4032 67.9 0.799 9059 + FrameFusion 74.1 0.518 5575 61.4 1.043 11043 58.0 0.949 9812 69.7 1.192 12544 58.7 1.199 12535 82.6 0.370 4058 67.4 0.878 9261 + ToMe + DyCoke-stage1 73.2 0.495 5386 62.0 0.981 10555 58.2 0.877 9393 69.7 1.123 11978 58.7 1.118 11969 82.4 0.350 3957 67.4 0.824 8873 77.4 0.455 4804 62.6 1.021 10771 59.6 0.895 9183 69.9 1.152 12187 58.6 1.045 10737 82.5 0.322 3452 68.4 0.815 8522 + STTM (Ours) 61.6 0.336 3345 59.2 0.683 6626 54.7 0.610 5887 69.3 1.620 17562 56.9 0.771 7520 81.6 0.240 2435 63.9 0.710 7229 + FastV 55.9 0.308 3548 60.7 0.598 6878 57.1 0.544 6131 67.3 0.684 7798 58.3 0.693 7792 81.5 0.229 2631 63.5 0.509 5797 + DyCoke 72.4 0.292 3157 60.7 0.581 6018 57.1 0.520 5462 67.5 0.650 6768 57.4 0.657 6870 81.9 0.218 2313 66.2 0.486 5098 + FrameFusion 64.1 0.364 3345 59.2 0.720 6626 56.3 0.658 5888 67.0 0.821 7527 57.4 0.834 7521 81.6 0.265 2436 64.3 0.610 5557 + ToMe + DyCoke-stage1 63.9 0.358 3548 60.0 0.700 6878 56.5 0.631 6131 68.6 0.796 7798 58.7 0.800 7792 81.7 0.256 2631 64.9 0.590 5797 76.0 0.299 2649 62.3 0.640 5929 57.0 0.616 5702 68.5 0.769 7337 58.0 0.773 7285 82.0 0.235 2168 67.3 0.555 5179 + STTM (Ours) NExT-QA 30% Avg. Table 11. Comparison of training-free token reduction methods using LLaVA-Video-7B under 50% and 30% pre-filling token budgets. 50% Method VNBench Q. Agn. VideoMME EgoSchema LongVideoBench MLVU Token Budget Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV 68.8 0.922 11149 59.0 1.904 22086 56.3 1.712 19624 67.5 2.224 25088 61.3 2.216 25069 80.6 0.630 8116 65.6 1.601 18522 100% LLaVA-OV 7B 65.6 0.467 5575 58.4 0.934 11043 56.2 0.842 9812 67.4 1.071 12544 61.0 1.080 12535 80.2 0.330 4058 64.8 0.787 9261 + FastV 64.9 0.418 5386 60.1 0.831 10555 57.9 0.740 9393 68.7 0.941 11978 61.8 0.948 11969 80.4 0.301 3957 65.6 0.697 8873 + DyCoke 67.8 0.436 5568 59.8 0.865 10888 56.6 0.778 9719 68.2 0.994 12379 60.8 0.997 12511 80.8 0.312 4056 65.7 0.730 9187 + FrameFusion 67.1 0.483 5575 59.8 0.963 11043 57.7 0.865 9812 68.8 1.101 12544 62.3 1.108 12535 80.1 0.343 4058 66.0 0.811 9261 + ToMe + DyCoke-stage1 65.1 0.459 5386 59.3 0.911 10555 58.2 0.821 9393 69.1 1.039 11978 61.8 1.049 11969 80.6 0.327 3957 65.7 0.767 8873 71.2 0.404 4573 60.7 0.773 8579 57.7 0.804 9011 69.7 1.020 11692 61.7 0.972 10944 80.5 0.312 3628 66.9 0.714 8071 + STTM (Ours) 59.2 0.303 3345 58.1 0.597 6626 55.3 0.543 5887 65.6 0.681 7526 60.9 0.694 7520 79.6 0.218 2435 63.1 0.506 5556 + FastV 53.9 0.280 3548 59.8 0.537 6878 55.9 0.488 6131 68.3 0.598 7798 62.1 0.607 7792 79.7 0.208 2631 63.3 0.453 5797 + DyCoke 66.1 0.261 3263 59.0 0.499 6154 56.5 0.455 5566 66.7 0.570 6914 60.3 0.582 7251 79.9 0.195 2400 64.7 0.427 5258 + FrameFusion 59.1 0.332 3345 59.8 0.649 6626 56.5 0.587 5888 69.1 0.740 7527 62.1 0.750 7521 79.5 0.240 2436 64.4 0.550 5557 + ToMe + DyCoke-stage1 58.4 0.324 3548 60.3 0.633 6878 56.2 0.577 6131 68.0 0.723 7798 62.3 0.731 7792 79.7 0.236 2631 64.1 0.537 5797 70.4 0.277 2773 60.6 0.601 6264 56.6 0.570 6022 68.4 0.735 7989 61.8 0.570 5621 79.7 0.240 2577 66.2 0.499 5208 + STTM (Ours) NExT-QA 30% Avg. Table 12. Comparison of training-free token reduction methods using LLaVA-OneVision-7B."
        },
        {
            "title": "Method",
            "content": "VideoMME Acc TTFT NV Token Budget 100% LLaVA-Video 72B 70.5 17.698 22086 + ToMe 70.6 8.424 11043 + DyCoke-stage1 70.4 8.052 10555 71.4 7.821 10082 + STTM (Ours) + ToMe 68.5 5.186 6626 + DyCoke-stage1 69.3 5.353 6878 69.9 5.405 6897 + STTM (Ours) 30% 50% Table 14. Comparison using LLaVAVideo-72B. Relative to 100% result ."
        },
        {
            "title": "LongVideoBench",
            "content": "Token Budget Acc TTFT NV Acc TTFT NV Acc TTFT NV Acc TTFT NV 100% Qwen2VL 7B 66.4 2.438 22025 61.8 10.745 74982 56.8 10.597 72109 61.7 7.927 56372 + ToMe 63.4 1.130 11013 61.9 4.509 37491 56.7 4.348 36054 60.7 3.329 28186 + DyCoke-stage1 65.1 1.049 10645 62.6 4.166 36057 57.5 4.148 34735 61.7 3.121 27146 69.8 0.726 7228 62.9 4.761 39217 57.4 4.610 37315 63.4 3.366 27920 + STTM (Ours) + ToMe 56.9 0.740 6608 61.4 2.835 22496 54.7 2.600 21633 57.6 2.058 16912 + DyCoke-stage1 54.2 0.706 6980 61.8 2.710 23479 57.5 2.694 22649 57.8 2.037 17703 66.7 0.420 6748 62.4 2.766 23143 56.9 2.472 20022 62.0 1.886 16638 + STTM (Ours) 30% 50% Avg. Table 13. Comparison using Qwen2VL-7B. Relative to 100% result ."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Carnegie Mellon University",
        "NAVER Cloud",
        "Yonsei University"
    ]
}