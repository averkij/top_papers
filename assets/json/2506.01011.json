{
    "paper_title": "Autoregressive Images Watermarking through Lexical Biasing: An Approach Resistant to Regeneration Attack",
    "authors": [
        "Siqi Hui",
        "Yiren Song",
        "Sanping Zhou",
        "Ye Deng",
        "Wenli Huang",
        "Jinjun Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive (AR) image generation models have gained increasing attention for their breakthroughs in synthesis quality, highlighting the need for robust watermarking to prevent misuse. However, existing in-generation watermarking techniques are primarily designed for diffusion models, where watermarks are embedded within diffusion latent states. This design poses significant challenges for direct adaptation to AR models, which generate images sequentially through token prediction. Moreover, diffusion-based regeneration attacks can effectively erase such watermarks by perturbing diffusion latent states. To address these challenges, we propose Lexical Bias Watermarking (LBW), a novel framework designed for AR models that resists regeneration attacks. LBW embeds watermarks directly into token maps by biasing token selection toward a predefined green list during generation. This approach ensures seamless integration with existing AR models and extends naturally to post-hoc watermarking. To increase the security against white-box attacks, instead of using a single green list, the green list for each image is randomly sampled from a pool of green lists. Watermark detection is performed via quantization and statistical analysis of the token distribution. Extensive experiments demonstrate that LBW achieves superior watermark robustness, particularly in resisting regeneration attacks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 1 1 0 1 0 . 6 0 5 2 : r AUTOREGRESSIVE IMAGES WATERMARKING THROUGH LEXICAL BIASING: AN APPROACH RESISTANT TO REGENERATION ATTACK Siqi Hui Xian Jiaotong University huisiqi@stu.xjtu.edu.cn Yiren Song National University of Singapore songyiren725@gmail.com Sanping Zhou Xian Jiaotong University spzhou@xjtu.edu.cn Ye Deng Southwestern University of Finance and Economics dengye@stu.xjtu.edu.cn Wenli Huang Ningbo University of Technology huangwenwenlili@126.com Jinjun Wang Xian Jiaotong University jinjun@mail.xjtu.edu.cn June 3,"
        },
        {
            "title": "ABSTRACT",
            "content": "Autoregressive (AR) image generation models have gained increasing attention for their breakthroughs in synthesis quality, highlighting the need for robust watermarking to prevent misuse. However, existing in-generation watermarking techniques are primarily designed for diffusion models, where watermarks are embedded within diffusion latent states. This design poses significant challenges for direct adaptation to AR models, which generate images sequentially through token prediction. Moreover, diffusion-based regeneration attacks can effectively erase such watermarks by perturbing diffusion latent states. To address these challenges, we propose Lexical Bias Watermarking (LBW), novel framework designed for AR models that resists regeneration attacks. LBW embeds watermarks directly into token maps by biasing token selection toward predefined green list during generation. This approach ensures seamless integration with existing AR models and extends naturally to post-hoc watermarking. To increase the security against white-box attacks, instead of using single green list, the green list for each image is randomly sampled from pool of green lists. Watermark detection is performed via quantization and statistical analysis of the token distribution. Extensive experiments demonstrate that LBW achieves superior watermark robustness, particularly in resisting regeneration attacks."
        },
        {
            "title": "Introduction",
            "content": "Recent diffusion models have demonstrated remarkable success across wide range of generative tasks, including textto-image synthesis[1, 2, 3], controllable generation[4, 5, 6], image editing[7, 8, 9, 10, 11, 12], and video generation[13, 14, 15, 16]. While diffusion models [17, 18] have dominated the landscape, autoregressive (AR)-based frameworks have emerged as compelling alternative, achieving state-of-the-art image quality [19, 20, 21]. Moreover, AR-based image modeling can be seamlessly integrated with AR-based language modeling frameworks, enabling powerful multimodal applications [22, 23]. However, their ability to generate highly realistic images concerns potential misuse, including deep-fakes and misinformation. To ensure traceability and prevent abuse, it is crucial to develop effective watermarking techniques for images generated by AR models. PREPRINT - JUNE 3, 2025 Existing watermarking techniques can be categorized into post-hoc and in-generation approaches. Post-hoc watermarking embeds watermarks into pre-generated images via imperceptible perturbations [24, 25, 26], whereas in-generation watermarking integrates watermarks directly into the diffusion-based image synthesis process by modifying intermediate states [27, 28, 29]. While effective in diffusion models, in-generation methods are incompatible with AR models, which generate images sequentially via token prediction rather than refining continuous latent representations. Consequently, watermarking within AR-based image generation remains an open challenge. Moreover, regeneration attacks pose significant threat to diffusion-based in-generation watermarking by disrupting their latent representations where watermarks are embedded [30, 31]. In contrast, AR models, which generate images through discrete token prediction, offer fundamentally different mechanism that may enhance robustness against such attacks. This motivates the development of watermarking method specifically tailored for AR models. The primary challenge in embedding watermarks in AR-generated images is determining where to introduce the watermark so that it remains detectable. key observation is that AR models quantize images into token maps, and when an AR-generated image is re-encoded, significant portion of the original tokens can be recovered (see Fig 1). This suggests that watermark information can be embedded in the token map, and subsequently detected by re-quantizing the watermarked image and analyzing the token distribution. Additionally, we observe that minor perturbations within controlled range on the token map do not significantly degrade image quality (see Fig 2, 3). These observations motivate us to embed watermarks in token maps of AR models. In this paper, drawing inspiration from text watermark techniques, we propose novel framework called Lexical Bias Watermarking (LBW), which embeds watermarks in the token map by introducing controlled bias in token selection during the autoregressive prediction process. Specifically, we partition the token vocabulary into red and green lists and encourage the model to favor green tokens during prediction by applying soft token biasing strategy, which increases the logits of green tokens with constant to enhance their likelihood of being sampled. Instead of utilizing dynamic green list [32, 33], we adopt global token partition strategy, which maintains predetermined green list throughout the entire token generation process. This design ensures compatibility with random token prediction processes [34, 21] and enhances robustness against global image watermark removal attacks. Furthermore, our method naturally extends to post-hoc watermarking by leveraging the VQ-VAE-based image reconstruction process. After an image is quantized into token map, we embed the watermark by replacing red tokens with their nearest green counterparts. The modified token map is then used to reconstruct the image, effectively embedding the watermark in post-hoc manner. To further enhance resistance against white-box attacks, we introduce multi-green-list strategy rather than relying on single green list. During watermarking, one green list is randomly selected from multiple green lists, and these green lists are predefined such that each token has an equal probability of being green token across the entire green list pool. Empirical results in Figure 5 confirm that our multi-list strategy produces token distributions nearly indistinguishable from those of clean images, whereas the single-list strategy exhibits detectable biases that can be easily inferred by an adversary. For watermark detection, we apply z-score hypothesis test to evaluate the proportion of green tokens in the token map quantized from watermarked images. Specifically, we evaluate each green list in the pool by computing the proportion of its tokens present in the token map. Given the high token consistency in AR-generated images (see Fig 1), any statistically significant deviation from the expected green token ratio provides strong evidence of watermark presence. This detection method is lightweight, requiring only VQ-VAE without access to transformer-based generative models, making it suitable for both in-generation and post-hoc watermarking. Experimental results demonstrate that our approach achieves state-of-the-art robustness against both conventional and regeneration watermark removal attacks, particularly CtrlRegen [31], strong attack designed to erase watermarks embedded in diffusion models. This highlights the effectiveness of our method in providing resilient watermarking for AR-generated images. In this work, we make the following key contributions: To the best of our knowledge, this is the first study to explore watermarking for the AR image generation process, ensuring seamless integration without disrupting the iterative token prediction mechanism. We propose LBX, unified framework that introduces lexical bias in AR-based image generation and reconstruction processes. We also introduce multi-green-list strategy to increase the security against whitebox attacks. Extensive experiments demonstrate that our method achieves comparable robustness to baseline watermarking techniques against conventional attacks while exhibiting superior resilience against regeneration attacks."
        },
        {
            "title": "2 Related Works",
            "content": "Image watermarking methods. 2 PREPRINT - JUNE 3, 2025 Image watermarking ensures digital content authenticity and security, typically categorized as post-hoc or in-generation watermarking. Post-hoc methods embed watermarks into pre-generated images via pixel-based (e.g., LSB [35]) or frequency-based techniques (e.g., DwtDct and DwtDctSvd [36]), with recent approaches leveraging deep learning (e.g., RivaGAN [25], SSL [37], StegaStamp [38]). In-generation methods integrate watermarks into the image synthesis process by modifying intermediate states, particularly in diffusion models[27, 28, 29] or VAE decoders[39, 40]. However, these methods are incompatible with AR models, which generate images via sequential token prediction. Besides, they are vulnerable to diffusion-based regeneration attacks. We firstly explores in-generation watermarking for AR image generation, demonstrating superior robustness against regeneration attacks. Text watermark methods for LLMs. Watermarking LLMs typically involves modifying logits or token sampling to embed watermarks within the generated text. KGW[41] partitions the vocabulary into \"green\" and \"red\" lists using hash-based selection strategy, biasing token selection toward green-listed tokens. EWD[42] enhances detection by assigning higher weights to low-entropy tokens. To minimize text distortion, SWEET[43] and Adaptive Watermark[44] avoid embedding watermarks in low-entropy positions, while BOW [45] selectively skips red tokens with high probabilities. WinMax[32] applies sliding window approach to defend against text mixing attacks, while semantic grouping techniques[46, 47] cluster similar tokens to resist semantic-invariant modifications. Unlike these adaptive methods, our approach utilizes globally fixed green list, ensuring compatibility with AR models, which generate tokens in any order. This design enhances robustness against global image watermark removal attacks, including blurring and DiffPure[30]. Autoregressive visual models. Early research on autoregressive (AR) image generation[48, 49, 50] modeled 2D images as 1D pixel sequences, generating pixels in row-wise raster scan order. Recent advancements leverage VQ-VAE-based tokenization[51], where models like VQGAN[52] employ decoder-only transformers to predict sequences of discrete latent tokens. Similar paradigms include VQVAE-2[53] and RQ-Transformer [54]. To overcome the limitations of unidirectional raster-scan generation, VAR[19] introduced multi-scale residual token map, improving spatial coherence. Further refinements, such as RAR[21] and RandAR [34], shuffle token generation orders during training, facilitating bidirectional token dependencies and enhancing contextual coherence. Our proposed LBW could be seamlessly integrated with AR models that employ both single-scale and multi-scale token maps as well as predefined and randomized token generation orders."
        },
        {
            "title": "3 Method",
            "content": "In this section, we provide detailed explanation of LBW, which embeds watermarks into token maps through both in-generation and post-hoc approaches, along with the corresponding watermark detection process. Section 3.1 provides an overview of the image quantization and token prediction processes fundamental to AR-based image synthesis. Section 3.2 presents two key properties of AR models that enable robust watermark embedding while preserving image quality. Section 3.3 details our in-generation and post-hoc watermarking methods, along with their detection process. 3.1 Preliminary Tokenization. Current AR image models [52, 19, 21] leverage VQ-VAE [55] to represent continuous images RHW 3 as discrete tokens (x1, x2, ..., xT ) in latent space, where each xi [V ] is an integer from vocabulary of size . Given an input image x, it is first encoded in feature map RhwC = E(x), where E() is the encoder. The quantizer Q() with learnable codebook RV then maps the feature map to discrete token map [V ]hw or stack of token maps {qk}K k=1 qk [V ]hkwk , based on the single-scale or multi-scale quantization process they introduce. The single-scale quantization process = Q(f ) maps each feature vector (i,j) to code index q(i,j) by finding the nearest code in the codebook using Euclidean distance: q(i,j) = arg min lookup(Z, v) (i,j)2, v[V ] (1) where lookup(Z, v) fetches the v-th vector from the codebook Z. This process produces the approximated feature map ˆf , which is decoded by D() to generate the reconstructed image ˆx: ˆf = lookup(Z, q), ˆx = D( ˆf ). (2) The multi-scale quantization process {qk}K k=1 = QK(f ) progressively derives token maps at each scale. set of scale parameters (hk, wk)K k=1 defines the map resolutions in ascending order, with hK = and wK = representing the largest scale. The token map qk for scale is computed by quantizing the residual feature map rk, which is derived 3 PREPRINT - JUNE 3, 2025 Figure 1: Observation 1: Token consistency for VQ-GAN, VAR, and RAR across various codebook ratios ranging from 0.1 to 1.0. by subtracting the aggregated sum of the reconstructed residual feature maps from preceding scales (each upscaled to the maximum resolution) from the original feature map: rk =f k1 (cid:88) i=1 interpolate( ˆri, hK, wK), ˆrk =lookup(Z, qk), (3) where ˆrk denotes the approximated residual feature map at scale k. Finally, the image is reconstructed by decoding ˆf : ˆx = D( ˆf ). Token prediction. AR models synthesize images through sequential token prediction after modeling images as sequences of discrete latent tokens. They utilize transformers to model the conditional probability distribution of token generation, formulated as: pθ(xtXt) = softmax(lθ(Xt)), (4) where lθ(Xt) RV is the generated logit at step t, and Xt {x1, ..., xt1} is the subset of previous generated tokens. The token sequence is generated through iterative sampling from the conditional probability. For simplicity, we use lt to denote the logits lθ(Xt) in the rest of the paper. For single-scale token prediction, tokens in Xt are generated either in predefined order (e.g., raster-scan) or in randomly permuted sequence. Multi-scale token prediction conditions token generation on all tokens from preceding scales, while allowing tokens within the same scale to be generated in parallel. For more comprehensive discussion on multi-scale quantization and token prediction, please refer to [19] and [54]. 3.2 Observations To investigate the behavior of VQ-VAE and evaluate its potential for embedding robust watermarks, we conducted two primary analyses. Observation 1 examines the consistency between token maps obtained by quantizing the original images and those derived from their reconstructed counterparts, providing insights into the feasibility of watermark embedding and detection in AR visual models. Observation 2 investigates the VQ-VAE codebook by analyzing the impact of vocabulary size reduction on image reconstruction quality. This analysis determines whether compact codebook can facilitate watermarking while preserving image fidelity. Experiments were performed on three AR models featuring different vocabulary sizes: VQ-GAN [52] (13,678 tokens), VAR [19] (4,096 tokens), and RAR [21] (1,024 tokens). The evaluation dataset comprises 5,000 images randomly sampled from 100 ImageNet classes, with 50 images per class. Observation 1: Token consistency. To evaluate the potential for watermark embedding and detection, we reconstructed images from the evaluation dataset and compared the token maps produced by quantizing both the input image and its reconstruction ˆx, yielding token maps and ˆq, respectively. The token consistency is defined as the proportion of matching tokens between these token maps. Notably, this consistency was assessed while progressively reducing the vocabulary size during the reconstruction process. For VAR, which employs multi-scale quantization process, token consistency was evaluated at its largest scale. As illustrated in Fig 1, token consistency remains robust across various codebook sizes. However, RAR exhibits more pronounced decline in consistency as the vocabulary decreases, likely due to its smaller codebook (1024) being more susceptible to quantization errors. These findings indicate that AR models employing VQ-VAEs maintain sufficient token consistency to preserve substantial portion of the original token sequence even under significant vocabulary reduction. Consequently, watermark information can be reliably embedded into token maps and subsequently detected by quantizing images into these maps again. 4 PREPRINT - JUNE 3, Figure 2: Observation 2: Image quality metrics (PSNR, SSIM, and FID) for AR reconstructed images across various codebook sizes, ranging from 0.1 to 1.0. Figure 3: Reconstructed images using only 10% of the original codebook size. Observation 2: Codebook redundancy. To further assess the influence of codebook reduction on image quality, we decreased the codebook ratio during image reconstruction and evaluated PSNR, SSIM, and FID metrics of reconstructed images. As shown in Fig 2, moderate degradation in image quality is observed as the codebook size decreases, and the overall performance remains stable. Notably, even when the vocabulary of VAR was reduced to just 10%, the reconstructed images exhibited minimal quality loss relative to those produced using the full codebook. This finding indicates that compact codebook is viable for watermarking applications without substantially compromising image quality. Additionally, Fig 3 presents the reconstructed images obtained using only 10% of the vocabulary, further demonstrating the feasibility of this approach. 3.3 Watermarking through lexical biasing Building on our previous analysis, we observe that tokens used for image generation can be recovered by encoding and quantizing AR-generated images into token maps again (see Fig 1). This finding suggests that if watermark information is embedded in the token map, it should be preserved and detected through this re-quantization process. In this paper, drawing inspiration from text watermarking techniques, we aim to embed the watermark information through biasing AR models to use specific tokens during the autoregressive token prediction process. direct approach. Formally, the codebook of AR models can be partitioned into green list and red list R. We aim to bias the model toward selecting tokens from during AR image synthesis. simple yet effective approach is to mask the logits of red list tokens while retaining those of the green list. Specifically, at each timestep t, the model predicts logits lt based on previously generated tokens Xt, which are then converted into discrete probability distribution for sampling the next token xt. To enforce token selection from G, the logits of red list tokens are set to negative infinity, modifying the logit vector ˆlt as follows: ˆlt(i) = (cid:26), lt(i), G. (5) This modification ensures that the softmax function assigns zero probability to tokens in R, thereby restricting the model to sample xt exclusively from the green list G. Unlike text watermarking, where the green and red lists dynamically adjust based on hash of previously generated tokens, we employ global token partitioning strategy, in which the same green and red lists are uniformly applied to all tokens. This approach is driven by three considerations. 1) Re-quantization Loss: The re-quantization process introduces token variations. When previously generated token changes, the corresponding green list for subsequent tokens would also shift, which hampers the robustness. 2) Vulnerability to image watermarking attacks: Unlike local text removal attacks, image watermarking attackssuch as blurring or CtrlRegen [31]impact the entire image, leading to substantial token variations across PREPRINT - JUNE 3, 2025 the token map, which also weakens the robustness of token-dependent hashing schemes. 3) Compatibility with AR models: Certain AR models generate tokens in non-sequential, randomized order [34, 21] rather than following strictly predefined sequence. In such cases, if the token generation order is unknown, dynamically determining the green lists becomes impractical. Watermark detection. Watermark presence is verified by statistically assessing the occurrence of specific tokens in the quantized token map, requiring only VQ-VAE and the predefined green list without access to transformer models. We formulate the verification process as hypothesis-testing problem. We define the null hypothesis: H0 : he image was generated without any bias toward the green list. To evaluate this hypothesis, we perform one-proportion z-test on the number of green tokens in the quantized token map. Let γ represent the proportion of green tokens used for watermarking. Under H0, the expected number of green tokens in token map Rhw follows binomial distribution with mean γ and variance γ(1 γ) w. Denoting the observed number of green tokens in the token map as sG, the z-score for watermark detection is computed as: = sG γ (cid:112)γ (1 γ) . (6) By setting threshold zth, we reject H0 and confirm watermark presence if > zth. Enhancing watermark via soft biasing. The direct approach enforces the exclusive use of green-listed tokens during token prediction. However, when the green list is overly constrained, the limited token vocabulary reduces the expressive capacity of AR models, leading to declined image quality or even generation failures (Fig 9). Moreover, restricting token selection disrupts the natural token distribution expected by VQ-VAE, thereby reducing token consistency, which in turn compromises watermark detectability and robustness (Fig 8). To address these limitations, we employ soft token biasing strategy that encourages the selection of green tokens without completely excluding red tokens. After predicting logits, instead of forcing red token logits to negative infinity, we introduce bias constant σ to increase the logits of green tokens as: (cid:26)lt(i), ˆlt(i) = G. lt(i) + σ, This ensures that when the transformer model exhibits high logits on the red list (high urge of using red token), the added bias minimally influences token selection, preserving the natural AR generation process. As result, this method maintains image quality and results in better token consistency and detectability. We also compute the z-score of the total number of green tokens in the re-encoded token map and compare it against predefined threshold to determine the presence of watermark. (7) Post-hoc watermarking via token substitution. Our approach could be extended to support post-hoc watermarking for existing images by simply substituting quantized red tokens with green tokens. Formally, given an input image x, we first quantize it into token map q. The watermark is then embedded by replacing each red token q(i) with its nearest green token, determined by the Euclidean distance in the token embedding space, ensuring minimal distortion: (8) lookup(Z, g) lookup(Z, q(i))2. (i) = arg min gG Multiple green lists. To prevent our method against white-box attacks, we propose multiple green list strategy. Concretely, set of green lists {Gi}N i=1 is established, from which one green list is randomly selected for watermark encoding. This set of green lists can be represented as binary matrix {0, 1}N as Mij = (cid:26)1, 0, if Gi, otherwise, i, j, s.t. (cid:88) Mij = γV, i, and (cid:88) Mij = γN, j. (9) j=1 Each row of corresponds to green list pool, with the first constraint ensuring that each green list maintains consistent green token ratio γ, while the second guarantees that each token is selected as green token with equal probability across the green lists. This design aligns the token distribution of watermarked images with that of clean images, thereby reducing the risk of reverse-engineering the watermark. However, finding such matrix exactly satisfying the above constraints requires solving the 0-1 integer programming problem, which is generally NP-hard and may have no feasible solution. To efficiently generate matrix that approximately meets these constraints, we employ the following algorithm: i=1 In our experiments, we use = 32 green lists and find it suffices to defend green list estimation attacks 5. During detection, the green token ratio is computed with respect to each green list in the pool, and the maximum observed ratio is used to calculate z-score to justify the presence of the watermark. Notably, the detection requires only convolutional image encoder to extract token maps, enabling efficient detection even when multiple green lists are employed. 6 PREPRINT - JUNE 3, 2025 Algorithm 1: Generate Green List Matrix Input: Number of green lists , green token ratio γ, vocabulary size Output: Binary matrix {0, 1}N 1 Randomly initialize matrix such that each row satisfies (cid:80) 2 Set threshold θ γN ; 3 repeat 4 for 1 to do j=1 Mij = γV ; Compute token frequency vector : [j] (cid:80) // Identify indices for tokens with excessively high frequency one_to_zero {j token_frequency[j] > θ and Mij = 1}; // Identify indices for tokens with too low frequency zero_to_one {j token_frequency[j] < θ and Mij = 0}; min(one_to_zero, zero_to_one); for 1 to do i=1 Mij ; Set Mi,zero_to_one[k] 1; Set Mi,one_to_zero[k] 0; 7 8 9 10 11 end 12 13 14 until convergence or maximum iterations reached; 15 return end"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setting We evaluate our watermarking methods using VQ-GAN, RAR, and VAR. For VAR, watermarks are embedded in the largest-scale token map. We apply three watermarking variants: LBW-Hard (strict green token enforcement), LBW-Soft (soft bias toward green tokens), and LBW-Post (post-hoc token substitution). Green token ratios γ are set to 0.1 for LBW-Post and LBW-Soft on VAR and RAR, and 0.2 for LBW-Hard on VQ-GAN. LBW-Soft uses bias constants σ of 7, 4, and 8 for VAR, VQ-GAN, and RAR, respectively. To further improve the generation quality, LBX-Soft adopts bias constant σ of 7, 4, and 8 for VAR, VQ-GAN, and RAR. We use = 32 green lists for embedding and detection, with consistent green list configurations across in-generation and post-hoc methods. We compare against state-of-the-art watermarking methods: DwtDct [56], DwtDctSvd [36], RivaGAN [25], SSL [37], Tree-Ring [27], and WatermarkDM [57]. Conventional attacks include Gaussian noise and blur, ColorJitter, geometric transformations (crop, resize, rotation), and JPEG compression. Regeneration attacks comprise VAE reconstruction (Stable Diffusion 1.5), DiffPure [23], and CtrlRegen [31]. Detailed attack settings can be found in the appendix D. We evaluate performance on the ImageNet dataset. For post-hoc watermarking, 1,000 images (10 per 100 classes) are watermarked. For in-generation methods, 1,000 watermarked and 1,000 clean images are generated conditionally on class labels aligned with post-hoc experiments. Detection is evaluated by ROC-AUC and TPR@1%FPR, averaged over five runs with different seeds to ensure robustness and reproducibility. 4.2 Main Results Table 1 presents comprehensive evaluation of watermarking methods under conventional and regeneration attacks, where our approach demonstrates strong robustness across all attacks. Notably, LBW achieves state-of-the-art performance against regeneration attacks; for instance, LBW-Post on RAR attains an AUC of 0.995 and TPR@1FPR of 0.937, significantly outperforming WatermarkDM. LBW-Soft further surpasses LBW-Hard in robustness. While our method is effective across different AR architectures, VAR shows slightly reduced robustness. This can be attributed to the multiscale quantization process of VAR, where high-frequency information is primarily captured by the largest-scale token map[54, 19]. CtrlRegen preserves semantic structures while suppressing fine-grained details, which makes large-scale token maps more vulnerable to such attacks. 4.3 Ablation Studies Figures 4 (top rows) show that the robustness of LBW-Post and LBW-Hard generally improves as γ decreases. An exception occurs for LBW-Hard on VQ-GAN, where γ = 0.2 outperforms γ = 0.1. This is because at very low γ values, the model frequently fails to generate images, leading to reduced token consistency and sub-optimal robustness (see appendix for detailed analyses). Based on these findings, we set the default values of γ to 0.2, 0.1, and 0.1 for VQ-GAN, VAR, and RAR, respectively. Note that when evaluating the effect of σ for LBW-Soft, the γ is set to the default values. As shown in the bottom row of Figure 4, the robustness of LBW-Soft initially improves with increasing σ and eventually saturated. The results indicate that robustness initially improves with increasing values of σ and eventually becomes saturated. While larger σ values enhance watermark detectability, excessively high values can 7 PREPRINT - JUNE 3, 2025 Table 1: Comparative evaluation of watermarking methods under conventional and regeneration attacks. The table presents AUC and TPR@1FPR metrics, where TPR@1FPR is abbreviated as T@1F. Best results are bolded. Our proposed method exhibits superior robustness, particularly against regeneration attacks. Method dwtDct dwtDctSvd rivaGan watermarkDM SSL TreeRing VAR (Ours) VQ-GAN (Ours) RAR (Ours) LBW-Post LBW-Hard LBX-Soft LBW-Post LBW-Hard LBX-Soft LBW-Post LBW-Hard LBX-Soft Metric Clean AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F 0.978 0.920 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.997 0.980 0.999 0.995 1.000 0.997 0.978 0.760 0.998 0.993 0.999 0.998 1.000 1.000 1.000 1.000 1.000 1.000 Gaus 0.906 0.714 0.949 0.850 1.000 1.000 1.000 1.000 1.000 1.000 0.945 0.902 0.988 0.943 0.995 0.967 0.995 0.977 0.972 0.728 0.969 0.909 0.990 0.977 1.000 0.999 1.000 0.998 1.000 1.000 Conventional Attack Geo 0.631 0.055 0.600 0.025 0.776 0.600 0.515 0.112 0.991 0.970 0.938 0.708 0.659 0.287 0.660 0.281 0.665 0.275 0.773 0.274 0.939 0.641 0.966 0.776 0.991 0.918 0.964 0.846 0.961 0.844 JPEG 0.520 0.005 0.649 0.020 0.939 0.780 0.999 0.991 0.621 0.312 0.991 0.952 0.981 0.912 0.988 0.932 0.989 0.934 0.948 0.699 0.921 0.745 0.995 0.977 0.999 0.995 0.999 0.993 1.000 0.999 Color 0.333 0.091 0.286 0.017 0.671 0.657 0.724 0.656 0.992 0.971 0.937 0.747 0.989 0.948 0.994 0.969 0.994 0.966 0.856 0.329 0.858 0.700 0.934 0.770 0.995 0.956 0.997 0.973 0.999 0. AVG 0.598 0.216 0.621 0.228 0.847 0.759 0.810 0.690 0.901 0.813 0.962 0.862 0.923 0.814 0.927 0.829 0.929 0.830 0.905 0.558 0.922 0.749 0.977 0.900 0.996 0.967 0.990 0.953 0.990 0.956 Regeneration Attack Diff 0.485 0.010 0.597 0.010 0.747 0.150 0.915 0.340 0.695 0.160 0.599 0.000 0.933 0.780 0.896 0.526 0.892 0.480 0.922 0.660 0.973 0.870 0.994 0.990 0.998 0.960 1.000 1.000 1.000 0.990 Ctrl 0.496 0.020 0.487 0.010 0.527 0.050 0.671 0.000 0.750 0.090 0.838 0.150 0.650 0.080 0.623 0.000 0.626 0.010 0.665 0.140 0.857 0.370 0.915 0.610 0.988 0.850 0.978 0.800 0.978 0.760 AVG 0.501 0.017 0.627 0.113 0.735 0.237 0.862 0.420 0.801 0.363 0.812 0.383 0.860 0.611 0.837 0.476 0.838 0.473 0.852 0.501 0.941 0.739 0.969 0.866 0.995 0.937 0.993 0.933 0.993 0.917 VAE 0.521 0.020 0.797 0.320 0.931 0.510 0.999 0.920 0.959 0.840 1.000 1.000 0.997 0.972 0.991 0.903 0.995 0.930 0.969 0.704 0.993 0.978 0.998 0.998 1.000 1.000 1.000 1.000 1.000 1. Figure 4: Impact of γ and σ in robustness against Conventional (Con) and Regeneration (Reg) attacks. 8 PREPRINT - JUNE 3, 2025 Figure 5: Comparison of token frequency distributions with varying green list number . compromise image quality by overly restricting token generation. To achieve balance between robustness and image fidelity, we adopt σ = 7, 4, and 8 for VAR, VQ-GAN, and RAR. To prevent green tokens from being inferred, we introduce multiple green lists for watermark embedding. Using RAR with γ = 0.1, we vary {1, 2, 8, 32, 128} and generate 10,000 post-hoc watermarked images per setting to study the impact of on the token frequency distribution of watermarked images. Figure 5 shows that when is small (e.g., 1 or 2), the token frequency distribution deviates significantly from that of clean images, enabling white-box attacks. However, as increases to 32 and beyond, the token frequency distribution converges closely to that of clean images, effectively eliminating distinguishable statistical cues and rendering frequency-based attacks. Consequently, we choose = 32 in this work. More experiments, including visual quality analysis, token consistency analysis or other visual results can be found in the appendix."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present LBW, novel watermarking framework for AR-based image generation that introduces controlled bias in token selection during the generation process to embed robust and detectable watermark. Our method can be seamlessly integrated into current AR image generation pipelines, achieving state-of-the-art robustness against regeneration attacks. Additionally, we extend LBW to post-hoc watermarking scheme, showcasing its adaptability in both in-generation and post-hoc scenarios. The multiple green list strategy is further introduced to enhance robustness against white-box attacks."
        },
        {
            "title": "References",
            "content": "[1] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [2] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [3] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [4] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80698078, 2024. [5] Yuxuan Zhang, Yiren Song, Jinpeng Yu, Han Pan, and Zhongliang Jing. Fast personalized text to image synthesis with attention injection. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 61956199. IEEE, 2024. PREPRINT - JUNE 3, 2025 [6] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025. [7] Ling Yang, Bohan Zeng, Jiaming Liu, Hong Li, Minghao Xu, Wentao Zhang, and Shuicheng Yan. Editworld: Simulating world dynamics for instruction-following image editing. arXiv preprint arXiv:2405.14785, 2024. [8] Shijie Huang, Yiren Song, Yuxuan Zhang, Hailong Guo, Xueyin Wang, Mike Zheng Shou, and Jiaming Liu. Photodoodle: Learning artistic image editing from few-shot pairwise data. arXiv preprint arXiv:2502.14397, 2025. [9] Yichun Shi, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. arXiv preprint arXiv:2411.06686, 2024. [10] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 60076017, 2023. [11] Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, and Haibo Zhao. Stable-makeup: When real-world makeup transfer meets diffusion model. arXiv preprint arXiv:2403.07764, 2024. [12] Yuxuan Zhang, Qing Zhang, Yiren Song, Jichao Zhang, Hao Tang, and Jiaming Liu. Stable-hair: Real-world hair transfer via diffusion model. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 1034810356, 2025. [13] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. International Journal of Computer Vision, pages 115, 2024. [14] Yiren Song, Cheng Liu, and Mike Zheng Shou. Makeanything: Harnessing diffusion transformers for multi-domain procedural sequence generation. arXiv preprint arXiv:2502.01572, 2025. [15] Yiren Song, Shijie Huang, Chen Yao, Xiaojun Ye, Hai Ci, Jiaming Liu, Yuxuan Zhang, and Mike Zheng Shou. Processpainter: Learn painting process from sequence data. arXiv preprint arXiv:2406.06062, 2024. [16] Cong Wan, Xiangyang Luo, Zijian Cai, Yiren Song, Yunlong Zhao, Yifan Bai, Yuhang He, and Yihong Gong. Grid: Visual layout generation. arXiv preprint arXiv:2412.10718, 2024. [17] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [18] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [19] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2025. [20] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431, 2024. [21] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. [22] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation, 2024. [23] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model, 2024. [24] Md. Maklachur Rahman. dwt, dct and svd based watermarking technique to protect the image piracy. International Journal of Managing Public Sector Information and Communication Technologies, 4(2):2132, June 2013. [25] Kevin Alex Zhang, Lei Xu, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Robust invisible video watermarking with attention, 2019. [26] Pierre Fernandez, Alexandre Sablayrolles, Teddy Furon, Hervé Jégou, and Matthijs Douze. Watermarking images in self-supervised latent spaces. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 30543058. IEEE, 2022. 10 PREPRINT - JUNE 3, 2025 [27] Yuxin Wen, John Kirchenbauer, Jonas Geiping, and Tom Goldstein. Tree-ring watermarks: Fingerprints for diffusion images that are invisible and robust. arXiv preprint arXiv:2305.20030, 2023. [28] Hai Ci, Pei Yang, Yiren Song, and Mike Zheng Shou. Ringid: Rethinking tree-ring watermarking for enhanced multi-key identification. In European Conference on Computer Vision, pages 338354. Springer, 2024. [29] Huayang Huang, Yu Wu, and Qian Wang. Robin: Robust and invisible watermarks for diffusion models with adversarial optimization, 2024. [30] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Anima Anandkumar. Diffusion models for adversarial purification, 2022. [31] Yepeng Liu, Yiren Song, Hai Ci, Yu Zhang, Haofan Wang, Mike Zheng Shou, and Yuheng Bu. Image watermarks are removable using controllable regeneration from clean noise, 2024. [32] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, and Tom Goldstein. On the reliability of watermarks for large language models. arXiv preprint arXiv:2306.04634, 2023. [33] Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, and Heng Huang. Unbiased watermark for large language models. arXiv preprint arXiv:2310.10669, 2023. [34] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William T. Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders, 2024. [35] R.B. Wolfgang and E.J. Delp. watermark for digital images. In Proceedings of 3rd IEEE International Conference on Image Processing, volume 3, pages 219222 vol.3, 1996. [36] K. A. Navas, Mathews Cheriyan Ajay, M. Lekshmi, Tampy S. Archana, and M. Sasikumar. Dwt-dct-svd based watermarking. In 2008 3rd International Conference on Communication Systems Software and Middleware and Workshops (COMSWARE 08), pages 271274, 2008. [37] Pierre Fernandez, Alexandre Sablayrolles, Teddy Furon, Hervé Jégou, and Matthijs Douze. Watermarking images in self-supervised latent spaces, 2022. [38] Matthew Tancik, Ben Mildenhall, and Ren Ng. Stegastamp: Invisible hyperlinks in physical photographs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [39] Pierre Fernandez, Guillaume Couairon, Hervé Jégou, Matthijs Douze, and Teddy Furon. The stable signature: Rooting watermarks in latent diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2246622477, October 2023. [40] Hai Ci, Yiren Song, Pei Yang, Jinheng Xie, and Mike Zheng Shou. Wmadapter: Adding watermark control to latent diffusion models, 2024. [41] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. watermark for large language models. In International Conference on Machine Learning, pages 1706117084. PMLR, 2023. [42] Yijian Lu, Aiwei Liu, Dianzhi Yu, Jingjing Li, and Irwin King. An entropy-based text watermarking detection method. arXiv preprint arXiv:2403.13485, 2024. [43] Yepeng Liu and Yuheng Bu. Adaptive text watermark for large language models. arXiv preprint arXiv:2401.13927, 2024. [44] Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin, and Gunhee Kim. Who wrote this code? watermarking for code generation, 2024. [45] Bram Wouters. Optimizing watermarks for large language models. arXiv preprint arXiv:2312.17295, 2023. [46] Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, and Lijie Wen. semantic invariant robust watermark for large language models, 2024. [47] Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, Zhaopeng Tu, Zhuosheng Zhang, and Rui Wang. Can watermarks survive translation? on the cross-lingual consistency of text watermark for large language models. arXiv preprint arXiv:2402.14007, 2024. [48] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pages 16911703. PMLR, 2020. [49] Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive networks. In International Conference on Machine Learning, pages 12421250. PMLR, 2014. [50] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International conference on machine learning, pages 40554064. PMLR, 2018. 11 PREPRINT - JUNE 3, 2025 Figure 6: Quantitative evaluation of visual quality for our LBW with varying γ and σ. Subfigures (a), (b), and (c) present PSNR, SSIM, and FID metrics for the LBW-Post watermark applied on VQ-GAN, VAR, and RAR models, while (d) and (e) show FID for our LBW-Hard and LBW-Soft, respectively. [51] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning, 2018. [52] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [53] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. [54] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. [55] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [56] Ali Al-Haj. Combined dwt-dct digital image watermarking. Journal of computer science, 3(9):740746, 2007. [57] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-Man Cheung, and Min Lin. recipe for watermarking diffusion models. arXiv preprint arXiv:2303.10137, 2023."
        },
        {
            "title": "A Visual Quality Analysis",
            "content": "This section presents comprehensive analysis of the visual quality of the proposed watermarking methodsLBW-Post, LBW-Hard, and LBW-Softthrough both quantitative and qualitative evaluations. Figure 6 reports FID scores for our watermark methods with varying γ and σ. PSNR and SSIM are reported exclusively for LBW-Post, as it is the only method with access to ground-truth images. The results indicate that increasing γ consistently enhances image quality across all models, evidenced by improved PSNR and SSIM scores for LBW-Post, and by reduced FID values of both LBW-Post and LBW-Hard. Conversely, for LBW-Soft, FID scores increase with larger σ, indicating degradation in perceptual quality. Compared to LBW-Hard at γ = 0.1, LBW-Soft achieves lower FID scores across large range of σ, reflecting its superior capability to balance watermark robustness with image fidelity. Notably, compared to VQ-GAN and RAR, VAR exhibits superior robustness to variations in both γ and σ. This robustness stems from watermark embedding exclusively within its largest-scale token map, which encodes rich high-frequency information, thereby mitigating perceptual degradation. Quantitative results Qualitative results Figure 7 offers visual comparison of watermarked images generated by VQ-GAN under the three watermarking schemes. The first two rows illustrate the results of LBW-Post and LBW-Hard, respectively, while the last 12 PREPRINT - JUNE 3, Figure 7: Qualitative evaluation of visual quality for our LBW with varying γ and σ presents the results of LBW-Soft. The image quality improves with increasing γ for both LBW-Post and LBW-Hard. For LBW-Soft, reducing the noise parameter σ enhances image quality. When γ is low (e.g., 0.1), the LBW-Hard often suffers from degraded image quality and occasionally fails to produce class-relevant images. This limitation arises because LBW-Hard enforces strict token selection constrained solely to the green list, which significantly restricts the models expressive capacity. In contrast, LBW-Soft permits sampling outside the green list with moderated bias, thereby demonstrating greater robustness and superior visual quality under low green token ratios. In summary, these quantitative and qualitative analyses corroborate that higher green token ratios γ and lower logit bias constant σ correlate with improved image quality, and our LBW-Soft effectively achieves trade-off between watermark robustness and visual fidelity."
        },
        {
            "title": "B Token Consistency",
            "content": "Figure 8 presents comparative analysis of the token consistency of our method for VQ-GAN, VAR, and RAR under different green list ratios γ and logits biasing constant σ. Overall, the token consistency of the LBW-Post method increases with γ, whereas in-generation methods (LBW-Hard & LBW-Soft) are insensitive to γ and σ. This suggests that in-generation watermarking could improve detectability by increasing γ and σ without worrying about the token exchanges during detection. Visual Comparison between LBW-Hard and LBW-Soft Figure 9 presents comparative analysis of watermarked images synthesized using our LBW-Hard and LBW-Soft approaches, integrated with RAR and VQ-GAN. The results demonstrate that LBW-Hard results in reduced image quality when applied to RAR and even fails to generate class-relevant content in the case of VQ-GAN. In contrast, LBW-Soft produces images with finer details and realistic content. These findings highlight the efficacy of LBW-Soft in striking balance between watermark robustness and image fidelity, making it more suitable approach for AR image models."
        },
        {
            "title": "D Numerical Robustness Results",
            "content": "To assess watermark robustness, we evaluate conventional and regeneration attacks. Conventional attacks include (1) Gaussian attacks randomly apply Gaussian Noise with variation 0.1 and Gaussian Blur using an 8 8 filter) (2) 13 PREPRINT - JUNE 3, 2025 Figure 8: Token consistency with varying γ. ColorJitter perturbations involving randomly applying hue adjustments (0.3), saturation scaling (3.0), and contrast scaling (3.0), (3) Geometric transformations (Crop&Resize: 0.7, Random Rotation: 0-180), and (4) JPEG compression (25% ). Regeneration attacks include (1) VAE reconstruction via the VAE of Stable Diffusion 1.5, (2) DiffPure [23] with timestep = 0.15, and (3) CtrlRegen [31]. In this section, we illustrate the numerical robustness of our proposed Lexical Bias Watermarking (LBW) methodsLBW-Post, LBW-Hard, and LBW-Softagainst the above watermark removal attacks in Table 2, Table 3, and Table 4, respectively. The evaluation is conducted on three different AR image models: VAR, VQ-GAN, and RAR, across varying watermark embedding strengths, denoted by the parameter γ, ranging from 0.1 to 0.9. For LBW-Soft, we evaluate the effect of bias constant σ under γ = 0.1, 0.2, and 0.1 for VAR, VQ-GAN, and RAR, respectively. We assess performance using AUC (Area Under Curve) and T@1F (True Positive Rate at 1% False Positive Rate), which indicate the detectability of the watermark under different attack conditions."
        },
        {
            "title": "E More Visual Results",
            "content": "Comparison between LBW-Post and other post-hoc methods. Figure 10 compares the watermarking performance of LBW-Post and traditional Post-hoc methods across different generative models (VQ-GAN, VAR, and RAR). In this comparison, LBW-Post employs green word ratio of γ = 0.1. Experimental results indicate that in multi-scale token map models (e.g., VAR), LBW-Post achieves image quality comparable to traditional Post-hoc methods. In single-scale generative models (e.g., VQ-GAN and RAR), LBW-Post also has minimal impact on image quality, effectively maintaining visual consistency. 14 PREPRINT - JUNE 3, 2025 Figure 9: Comparison between watermark images produced by LBW-Hard and LBW-Soft. Figure 10: Visual comparison between our LBW-Post and other Post-hoc methods. 15 PREPRINT - JUNE 3, 2025 γ 0.100 0.200 0. 0.400 VAR 0.500 0.600 0.700 0. 0.900 0.100 0.200 0.300 0.400 VQ-GAN 0.500 0.600 0.700 0.800 0.900 0. 0.200 0.300 0.400 RAR 0.500 0. 0.700 0.800 0.900 Metric AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F Clean Gaus 0.988 0.997 0.943 0.980 0.989 0.998 0.940 0.985 0.983 0.995 0.920 0.973 0.977 0.994 0.876 0.962 0.974 0.995 0.848 0.961 0.969 0.991 0.802 0.946 0.951 0.985 0.721 0.911 0.932 0.979 0.606 0.847 0.883 0.956 0.367 0.659 0.972 0.978 0.728 0.760 0.894 0.898 0.433 0.493 0.864 0.871 0.321 0.369 0.813 0.829 0.264 0.320 0.774 0.786 0.120 0.169 0.693 0.721 0.068 0.101 0.648 0.648 0.037 0.049 0.627 0.629 0.031 0.047 0.554 0.547 0.012 0.016 1.000 1.000 0.999 1.000 1.000 1.000 0.999 1.000 1.000 1.000 0.998 1.000 0.999 0.999 0.994 0.999 0.998 0.999 0.986 0.999 0.997 0.999 0.973 0.999 0.994 1.000 0.951 0.998 0.983 0.999 0.809 0.987 0.945 0.996 0.485 0.911 Color Geom JPEG 0.981 0.659 0.989 0.912 0.287 0.948 0.977 0.655 0.985 0.888 0.280 0.935 0.972 0.639 0.983 0.852 0.274 0.906 0.967 0.641 0.977 0.829 0.275 0.884 0.957 0.645 0.974 0.772 0.273 0.867 0.950 0.629 0.967 0.729 0.267 0.832 0.926 0.627 0.952 0.646 0.255 0.745 0.906 0.636 0.934 0.546 0.249 0.631 0.856 0.617 0.889 0.378 0.193 0.451 0.948 0.773 0.856 0.699 0.274 0.329 0.856 0.681 0.749 0.366 0.171 0.155 0.821 0.661 0.715 0.226 0.135 0.112 0.784 0.627 0.676 0.170 0.102 0.095 0.741 0.617 0.648 0.097 0.059 0.053 0.678 0.578 0.606 0.059 0.023 0.020 0.623 0.556 0.582 0.015 0.026 0.015 0.610 0.553 0.566 0.019 0.017 0.012 0.546 0.514 0.535 0.015 0.010 0.009 0.999 0.991 0.995 0.995 0.918 0.956 0.999 0.984 0.987 0.980 0.853 0.897 0.998 0.973 0.977 0.960 0.764 0.834 0.994 0.959 0.966 0.947 0.640 0.753 0.988 0.940 0.959 0.903 0.553 0.652 0.984 0.918 0.940 0.849 0.498 0.583 0.980 0.882 0.912 0.817 0.384 0.488 0.960 0.834 0.876 0.571 0.314 0.370 0.902 0.781 0.817 0.343 0.281 0. VAE 0.997 0.972 0.995 0.968 0.996 0.948 0.986 0.932 0.984 0.890 0.981 0.892 0.975 0.796 0.945 0.638 0.881 0.230 0.969 0.704 0.880 0.434 0.866 0.386 0.815 0.262 0.771 0.146 0.651 0.056 0.605 0.064 0.599 0.040 0.510 0.016 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.998 1.000 0.990 0.998 0.944 0.988 0.812 Diff 0.933 0.780 0.923 0.700 0.919 0.580 0.868 0.510 0.865 0.410 0.886 0.390 0.823 0.350 0.765 0.240 0.687 0.050 0.922 0.660 0.850 0.280 0.800 0.240 0.749 0.210 0.690 0.100 0.631 0.110 0.642 0.110 0.596 0.070 0.561 0.000 0.993 0.920 0.988 0.940 0.981 0.830 0.977 0.940 0.975 0.550 0.966 0.710 0.947 0.650 0.928 0.380 0.878 0.320 Ctrl 0.650 0.080 0.645 0.100 0.614 0.030 0.605 0.000 0.588 0.020 0.586 0.010 0.540 0.000 0.525 0.020 0.516 0.020 0.665 0.140 0.684 0.010 0.687 0.040 0.567 0.040 0.510 0.020 0.522 0.020 0.508 0.050 0.514 0.070 0.483 0.030 0.870 0.240 0.815 0.280 0.767 0.220 0.715 0.150 0.731 0.110 0.734 0.100 0.693 0.070 0.660 0.020 0.584 0.060 Table 2: Robustness for LBW-Post for VAR, VQ-GAN and RAR across different γ, ranging from 0.1 to 0.9. 16 PREPRINT - JUNE 3, γ 0.100 0.200 0.300 0.400 VAR 0.500 0.600 0.700 0.800 0.900 0. 0.200 0.300 0.400 VQ-GAN 0.500 0. 0.700 0.800 0.900 0.100 0.200 0. 0.400 RAR 0.500 0.600 0.700 0. 0.900 Metric AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F Clean Gaus 0.995 0.999 0.967 0.995 0.992 0.998 0.930 0.988 0.983 0.997 0.903 0.978 0.975 0.994 0.839 0.958 0.966 0.992 0.775 0.947 0.953 0.988 0.697 0.912 0.931 0.978 0.557 0.830 0.898 0.966 0.430 0.752 0.828 0.925 0.243 0.535 0.969 0.998 0.909 0.993 0.987 0.999 0.970 0.998 0.994 1.000 0.979 1.000 0.990 1.000 0.963 0.999 0.991 1.000 0.953 1.000 0.986 1.000 0.921 0.997 0.970 0.998 0.794 0.985 0.959 0.997 0.666 0.974 0.917 0.989 0.376 0.784 1.000 1.000 0.998 1.000 1.000 1.000 0.998 1.000 1.000 1.000 0.995 1.000 0.998 0.999 0.989 1.000 0.998 0.999 0.976 1.000 0.994 0.999 0.958 0.998 0.992 0.999 0.935 0.998 0.981 0.997 0.833 0.984 0.927 0.986 0.481 0.880 Color Geom JPEG 0.988 0.660 0.994 0.932 0.281 0.969 0.982 0.645 0.992 0.909 0.277 0.958 0.972 0.650 0.985 0.826 0.269 0.908 0.955 0.629 0.977 0.764 0.267 0.888 0.951 0.639 0.973 0.708 0.260 0.824 0.929 0.637 0.960 0.564 0.246 0.769 0.912 0.626 0.941 0.497 0.229 0.654 0.867 0.619 0.917 0.437 0.197 0.531 0.808 0.596 0.853 0.267 0.144 0.351 0.921 0.903 0.858 0.745 0.482 0.700 0.966 0.939 0.912 0.849 0.641 0.714 0.964 0.871 0.902 0.835 0.411 0.654 0.956 0.831 0.892 0.827 0.371 0.547 0.950 0.793 0.881 0.659 0.339 0.449 0.932 0.779 0.854 0.591 0.299 0.367 0.908 0.751 0.820 0.421 0.292 0.167 0.876 0.736 0.785 0.327 0.277 0.168 0.809 0.706 0.727 0.094 0.227 0.082 0.999 0.964 0.997 0.993 0.846 0.973 0.999 0.951 0.996 0.992 0.802 0.967 0.996 0.939 0.988 0.964 0.725 0.890 0.982 0.913 0.976 0.878 0.634 0.805 0.985 0.901 0.973 0.879 0.534 0.759 0.979 0.878 0.952 0.867 0.474 0.628 0.967 0.851 0.932 0.741 0.395 0.556 0.946 0.806 0.891 0.491 0.338 0.355 0.891 0.762 0.806 0.348 0.261 0.203 VAE 0.991 0.903 0.983 0.890 0.974 0.838 0.954 0.656 0.933 0.519 0.910 0.415 0.867 0.273 0.828 0.164 0.749 0.076 0.993 0.978 0.998 0.996 1.000 0.999 1.000 1.000 0.999 0.997 0.998 0.987 0.993 0.859 0.987 0.867 0.966 0.544 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.997 1.000 0.997 0.999 0.993 0.998 0.978 0.994 0.885 0.964 0.559 Diff 0.896 0.526 0.819 0.500 0.844 0.420 0.775 0.330 0.792 0.340 0.742 0.100 0.725 0.270 0.696 0.110 0.684 0.070 0.973 0.870 0.992 0.990 0.984 0.940 0.975 0.890 0.967 0.820 0.947 0.660 0.913 0.480 0.870 0.251 0.792 0.110 1.000 1.000 1.000 0.980 0.997 0.960 0.999 0.980 0.992 0.840 0.982 0.820 0.980 0.810 0.966 0.810 0.879 0. Ctrl 0.623 0.000 0.542 0.040 0.635 0.020 0.638 0.120 0.604 0.090 0.572 0.020 0.512 0.070 0.609 0.070 0.533 0.000 0.857 0.370 0.869 0.440 0.842 0.410 0.796 0.330 0.764 0.170 0.644 0.020 0.695 0.030 0.698 0.080 0.641 0.010 0.978 0.800 0.951 0.430 0.917 0.420 0.844 0.150 0.832 0.060 0.785 0.090 0.771 0.130 0.669 0.120 0.540 0.020 Table 3: Robustness for LBW-Hard for VAR, VQ-GAN and RAR across different γ, ranging from 0.1 to 0.9. 17 PREPRINT - JUNE 3, 2025 σ 1. 2.000 3.000 4.000 VAR 5.000 6. 7.000 8.000 9.000 1.000 2.000 3. 4.000 VQ-GAN 5.000 6.000 7.000 8. 9.000 1.000 2.000 3.000 4.000 RAR 5.000 6.000 7.000 8.000 9.000 Metric AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F AUC T@1F Clean Gaus 0.852 0.927 0.341 0.615 0.963 0.990 0.764 0.940 0.984 0.997 0.904 0.984 0.993 0.998 0.945 0.993 0.995 0.999 0.961 0.994 0.995 0.999 0.961 0.995 0.995 1.000 0.977 0.997 0.995 0.999 0.966 0.995 0.994 0.999 0.967 0.995 0.959 0.986 0.733 0.952 0.993 1.000 0.969 0.998 0.990 0.999 0.976 0.999 0.990 0.999 0.977 0.998 0.988 0.999 0.976 0.998 0.986 0.999 0.970 0.997 0.986 0.999 0.968 0.997 0.986 0.999 0.970 0.997 0.986 0.999 0.970 0.997 0.912 0.961 0.314 0.633 0.993 0.998 0.930 0.986 0.999 1.000 0.988 0.999 1.000 1.000 1.000 0.999 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 Color Geom JPEG 0.844 0.505 0.873 0.339 0.021 0.394 0.947 0.502 0.969 0.751 0.022 0.833 0.977 0.507 0.986 0.878 0.020 0.929 0.984 0.519 0.992 0.915 0.027 0.953 0.988 0.650 0.993 0.931 0.274 0.966 0.986 0.647 0.994 0.933 0.274 0.969 0.989 0.665 0.994 0.934 0.275 0.966 0.987 0.662 0.994 0.933 0.283 0.968 0.986 0.665 0.993 0.934 0.275 0.968 0.901 0.786 0.786 0.539 0.360 0.303 0.988 0.911 0.899 0.921 0.576 0.620 0.991 0.956 0.925 0.960 0.709 0.735 0.995 0.966 0.934 0.977 0.776 0.770 0.993 0.962 0.924 0.969 0.773 0.749 0.991 0.966 0.917 0.958 0.768 0.734 0.990 0.967 0.910 0.947 0.773 0.719 0.990 0.965 0.907 0.948 0.761 0.720 0.989 0.963 0.907 0.947 0.768 0.720 0.865 0.758 0.802 0.292 0.254 0.169 0.982 0.879 0.946 0.800 0.522 0.639 0.998 0.929 0.984 0.973 0.722 0.860 0.998 0.947 0.994 0.980 0.794 0.940 1.000 0.953 0.997 0.995 0.823 0.968 1.000 0.958 0.997 0.998 0.842 0.971 1.000 0.958 0.998 0.997 0.851 0.978 1.000 0.961 0.999 0.999 0.844 0.981 1.000 0.957 0.998 0.997 0.847 0.980 VAE 0.745 0.030 0.949 0.450 0.979 0.840 0.989 0.870 0.992 0.920 0.992 0.920 0.995 0.930 0.994 0.920 0.990 0.910 0.990 0.918 1.000 1.000 0.998 0.996 0.998 0.998 0.998 0.998 0.998 0.996 0.998 0.996 0.998 0.996 0.998 0.996 0.925 0.522 0.996 0.981 1.000 0.996 1.000 0.999 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 Diff 0.632 0.000 0.739 0.120 0.846 0.300 0.882 0.400 0.844 0.390 0.874 0.470 0.892 0.480 0.882 0.460 0.879 0.470 0.993 0.980 0.988 0.990 0.985 0.940 0.994 0.990 0.997 0.980 0.990 0.970 0.995 0.960 0.986 0.940 0.987 0.930 0.799 0.170 0.968 0.720 0.994 0.970 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.990 1.000 1.000 Ctrl 0.543 0.000 0.523 0.000 0.575 0.010 0.622 0.000 0.620 0.010 0.589 0.010 0.626 0.010 0.614 0.000 0.617 0.000 0.669 0.060 0.758 0.180 0.829 0.380 0.915 0.610 0.887 0.470 0.889 0.460 0.889 0.460 0.879 0.360 0.876 0.360 0.632 0.010 0.790 0.130 0.915 0.430 0.958 0.620 0.958 0.620 0.978 0.770 0.961 0.740 0.978 0.760 0.976 0.770 Table 4: Robustness for LBW-Soft for VAR, VQ-GAN and RAR across different σ, ranging from 1 to 9. 18 PREPRINT - JUNE 3, 2025 Furthermore, LBW-Post introduces an adjustable hyperparameter γ (green word ratio) to regulate the trade-off between watermark embedding strength and image quality. This allows users to fine-tune the embedding strategy according to specific application requirements, balancing image quality and watermark robustness, thereby adapting to various practical scenarios. More Comparison between LBW-Hard and LBW-Soft. Figure 11: Comparison between LBW-Hard and LBW-Soft across VAR, VQ-GAN, and RAR. Figure 11 compares the performance of LBW-Hard and LBW-Soft across different generative models. The results indicate that for AR models utilizing multi-scale token maps (e.g., VAR), both methods yield visually comparable results. However, for single-scale token map models (e.g., VQ-GAN and RAR), LBW-Soft outperforms LBW-Hard, producing images with richer details and stronger class relevance. Moreover, LBW-Soft demonstrates superior robustness when the green word ratio is low, scenario in which LBW-Hard may fail to generate meaningful images. This highlights LBW-Soft as more adaptable solution that ensures stable image synthesis even under challenging conditions."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Ningbo University of Technology",
        "Southwestern University of Finance and Economics",
        "Xian Jiaotong University"
    ]
}