{
    "paper_title": "OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot",
    "authors": [
        "Junhan Zhu",
        "Hesong Wang",
        "Mingluo Su",
        "Zefang Wang",
        "Huan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel one-shot pruning framework that enables accurate and training-free compression of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose a novel timestep-aware Hessian construction that incorporates a logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, a computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 1 5 7 6 0 . 0 1 5 2 : r Preprint OBS-DIFF: ACCURATE PRUNING FOR DIFFUSION MODELS IN ONE-SHOT Junhan Zhu1 , Hesong Wang1,2 , Mingluo Su1, Zefang Wang1,2, Huan Wang1 1Westlake University https://github.com/Alrightlone/OBS-Diff 2Zhejiang University Figure 1: Qualitative comparison of unstructured pruning methods on the SD3-Medium model (Esser et al., 2024). We evaluate Magnitude, DSnoT (Zhang et al., 2024c), Wanda (Sun et al., 2024), and our method (OBS-Diff) at various sparsity levels (20%, 30%, 40%, and 50%) using the same prompt and negative prompt. All images are generated at resolution of 512 512."
        },
        {
            "title": "ABSTRACT",
            "content": "Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents OBS-Diff, novel one-shot pruning framework that enables accurate and training-free compression of largescale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose novel timestep-aware Hessian construction that incorporates logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality. Corresponding author: wanghuan@westlake.edu.cn 1 Preprint"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in text-to-image generation have been largely driven by large-scale diffusion models (Rombach et al., 2022; Ramesh et al., 2022; Xie et al., 2024). These models, such as the Stable Diffusion 3 and 3.5 series (Esser et al., 2024), are capable of producing stunning images from textual prompts, revolutionizing fields from digital art to content creation. However, their massive parameter countsoften in the billions (e.g., 8B in Stable Diffusion 3.5-Large)create prohibitive computational and memory demands, severely limiting their broader accessibility. To improve the efficiency of the diffusion models, multiple research avenues have been proposed. significant area of research is dedicated to expediting the sampling process by reducing the required denoising steps (Song et al., 2021; Lu et al., 2022) or knowledge distillation (Salimans & Ho, 2022; Sauer et al., 2024). Orthogonal to these efforts, model compression aims to reduce the intrinsic computational and memory footprint of the model itself. This category includes methods like quantization (He et al., 2023; Li et al., 2023b; Shang et al., 2023; Li et al., 2023a) and pruning, which is the primary focus of our work. Pruning (Han et al., 2015; 2016; Wang et al., 2021; Fang et al., 2023a; Feng et al., 2024) is an effective way to reduce the computational and memory costs of deep neural networks. However, the rapid evolution of diffusion models underscores the severe limitations of existing pruning techniques. Current methods often lack generality (Fang et al., 2023b; Li et al., 2023c; Kim et al., 2024), as they are typically tailored to specific architectures like the U-Net and are not easily adapted to large-scale, text-to-image diffusion models with diverse structures (e.g., Multimodal Diffusion Transformer). Moreover, the efficiency gains from pruning are frequently undermined by computationally expensive requirements, such as the need for gradient information during pruning (Fang et al., 2023b; Zhang et al., 2024b) or costly post-pruning fine-tuning stage. Furthermore, unstructured and semistructured pruning remains largely unexplored for large-scale text-to-image diffusion models. All of these motivate our central research question: Can we develop general and training-free pruning framework capable of pruning diffusion models with diverse architectures and supporting multiple pruning granularities in one-shot manner? In the domain of Large Language Models (LLMs), one-shot and training-free pruning methods like SparseGPT (Frantar & Alistarh, 2023) and Wanda (Sun et al., 2024) have achieved remarkable success. Subsequent SlimGPT (Ling et al., 2024) and SoBP (Wei et al., 2024) further explored trainingfree structured pruning. These layer-wise post-training pruning method approaches efficiently compress massive models without requiring costly retraining. However, the existing training-free pruning methods from the LLM field, such as SparseGPT, cannot be directly applied to the diffusion model. This is due to the unique challenges posed by diffusion models: their iterative nature, where parameters are shared across multiple denoising steps. Furthermore, the complex architectures introduce additional difficulties for pruning. To bridge this gap, we introduce OBS-Diff, novel one-shot, training-free pruning framework designed specifically for large-scale text-to-image diffusion models. Our approach revitalizes the classic Optimal Brain Surgeon (OBS) (Hassibi et al., 1992) and tailors it to the unique, iterative nature of the diffusion denoising process. By reformulating the pruning objective to account for the temporal dynamics of generation and introducing computationally efficient calibration strategy, OBS-Diff efficiently removes redundant weights with minimal impact on performance, all without requiring any training during the pruning process or fine-tuning. Our contributions are summarized as below: We adapt the OBS framework to handle the complex architectures of modern diffusion models, such as the Multimodal Diffusion Transformer (MMDiT), and demonstrate OBS-Diff versatility across unstructured, semi-structured (e.g., 2:4 sparsity patterns), and structured pruning (e.g., removing entire attention heads or FFN neurons). Recognizing that errors introduced in the early stages of the iterative denoising process have compounding effect, we propose Timestep-Aware Hessian Construction. This novel construction weights the importance of parameters according to their influence across the entire denoising trajectory, prioritizing the more sensitive early steps through logarithmic weighting scheme. To overcome the prohibitive cost of sequential calibration in iterative models, we devise groupwise sequential pruning strategy built upon Module Packages. This approach amortizes the 2 Preprint expensive data collection process by processing layers in batches, striking an effective balance between computational time and memory requirements for the pruning process. Extensive experiments demonstrate that OBS-Diff sets new state-of-the-art for training-free diffusion model pruning. It achieves inference acceleration while maintaining high visual quality, outperforming other layer-wise pruning methods across various sparsity levels and patterns."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Pruning for Diffusion Models. Several methods have explored pruning for diffusion models. An early approach, Diff-pruning (Fang et al., 2023b;a), introduced gradient-based method for structured pruning. However, its demonstration on small-scale, non-text-to-image models (e.g., DDPMs) and its dependency on expensive retraining limit its applicability to modern, large-scale systems with diverse architectures. significant line of research has since focused on compressing the UNet-based text-to-image diffusion models, with works like SnapFusion (Li et al., 2023c), MobileDiffusion (Zhao et al., 2024), BK-SDM (Kim et al., 2024), LAPTOP-Diff (Zhang et al., 2024a), and LD-Pruner (Castells et al., 2024) all targeting less salient components of the UNet architecture. Other works have explored different architectures or techniques; for instance, Tinyfusion (Fang et al., 2025) introduced depth pruning for the DiT architecture. More recently, EcoDiff (Zhang et al., 2024b) introduced general pruning framework for text-to-image models applicable to diverse architectures; however, it remains dependent on costly training phase to learn pruning mask and requires extensive hyperparameter tuning. common theme among these methods is dependency on training or fine-tuning and primary focus on architecture-specific, structured pruning. Furthermore, unstructured and semi-structured pruning for large-scale, text-to-image diffusion models remains largely unexplored area. Layer-Wise Pruning Methods. Early post-training compression methods, notably Optimal Brain Damage (OBD) (LeCun et al., 1989) and Optimal Brain Surgeon (OBS) (Hassibi et al., 1992), utilized Hessian-based saliency scores to prune individual weights. However, the prohibitive cost of computing and storing the full Hessian matrix limited their scalability. This challenge spurred the development of layer-wise approaches such as L-OBS (Dong et al., 2017) and Optimal Brain Compression (OBC) (Frantar & Alistarh, 2022), which approximate the Hessian locally to make pruning tractable. As models scaled to billions of parameters, particularly in Large Language Models (LLMs), new methods emerged. SparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2024), and DSnoT (Zhang et al., 2024c) focused on efficient unstructured and semi-structured (N:M pattern) pruning. Subsequently, SlimGPT (Ling et al., 2024) and SoBP (Wei et al., 2024) extended the OBS methodology to structured granularity. The applicability of the OBS framework has also been demonstrated in other architectures, such as the Mamba model via SparseSSM (Tuo & Wang, 2025). Nevertheless, this family of compression methods remains unexplored in the field of diffusion models."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "3.1 LAYER-WISE POST-TRAINING PRUNING Post-training pruning often decomposes the global network compression problem into series of independent, layer-wise subproblems (Hubara et al., 2021; Nagel et al., 2020; Aghasi et al., 2017). For each layer l, the objective is to find pruned weight matrix ˆWl that minimizes the output reconstruction error, given input activations Xl and target sparsity Sl. This is formulated as: argmin ˆWl (cid:13) (cid:13)WlXl ˆWlXl (cid:13) (cid:13) 2 (cid:13) (cid:13) 2 s.t. sparsity( ˆWl) = Sl, (1) where 2 optimization problem for each layer. 2 is the squared Euclidean norm. The network is pruned by sequentially solving this 3 Preprint Figure 2: Illustration of the proposed OBS-Diff framework applied to the MMDiT architecture. Target modules are first partitioned into predefined number of Module Packages and processed sequentially. For each package, hooks capture layer activations during forward pass with calibration dataset. This data, combined with weights from dedicated timestep weighting scheme, is used to construct Hessian matrices. These matrices guide the Optimal Brain Surgeon (OBS) algorithm to simultaneously prune all layers within the current package before proceeding to the next. 3.2 OPTIMAL BRAIN SURGEON FOR LAYER-WISE PRUNING The Optimal Brain Surgeon (OBS) framework (Hassibi et al., 1992) offers an efficient solution to the layer-wise problem in Eq. (1). key insight of OBS is that the ℓ2-norm objective allows the problem to be decoupled into independent subproblems for each row of the weight matrix Wl. For each row, OBS approximates the objective with second-order Taylor expansion centered around the current weights. This relies on the Hessian of the reconstruction error, = 2XlXT . This approximation yields closed-form solution to identify the least salient weight wqthe one whose removal minimally increases the errorand to compute the optimal update δw for the remaining weights in its row. The saliency score Lq and the update are defined as: Lq = w2 2[H1]qq , δw = wq [H1]qq H1 :,q , (2) where [H1]qq is the q-th diagonal element of the inverse Hessian and :,q is its q-th column. This process is repeated iteratively until the target sparsity Sl is reached. After each weight is removed, the inverse Hessian must be updated. To circumvent the prohibitive cost of full re-inversion and the error accumulation of approximate rank-one updates, SparseGPT (Frantar & Alistarh, 2023) imposes fixed pruning order. This structural constraint enables efficient and stable updates to the inverse Hessian information using methods like Cholesky decomposition (Frantar et al., 2024) as weights are progressively removed."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "4.1 TIMESTEP-AWARE HESSIAN CONSTRUCTION The layer-wise pruning objective defined in Eq. (1) is effective for models with single forward pass, but insufficient for diffusion models, which are iterative and operate over denoising trajectory 4 Preprint parameterized by discrete timesteps {1, , }.1 The impact of pruning-induced errors is not uniform across this trajectory. Errors introduced in early inference steps (small t) are inherently more damaging, as they propagate and compound through all subsequent steps (t + 1, , ), leading to larger deviations in the final output. Therefore, robust pruning strategy must prioritize preserving network function during these critical early stages. We reformulate the layer-wise optimization problem to minimize weighted reconstruction error that places greater importance on earlier, higher-impact steps: arg min ˆWl Et[1,T ] (cid:20) αt (cid:13) (cid:13)WlXl,t ˆWlXl,t (cid:13) (cid:13) 2 (cid:13) (cid:13) 2 (cid:21) , (3) Here, Xl,t is the input to layer at step t, and αt is step-dependent weight. We define αt using simple and effective logarithmically decreasing schedule: αt = αmin + αmax αmin ln(T ) ln(T + 1), {1, 2, , }. (4) This schedule ensures the weight is highest at the beginning of inference and decays smoothly, such that α1 > α2 > > αT > 0. By incorporating this weighting, we adapt the Optimal Brain Surgeon framework (Hassibi et al., 1992). The Hessian, which captures the second-order information of this weighted loss, is now computed as weighted sum over all inference steps: Hl = 2 (cid:88) t=1 αtE[Xl,tX l,t], (5) which is termed as Timestep-Aware Hessian. It encapsulates the varying importance of parameters over the generation process. Saliency scores derived from its inverse are thus more sensitive to weights that are critical during the early, formative stages of the denoising process, resulting in more faithfully pruned model. 4.2 MODULE PACKAGES: GROUP-WISE SEQUENTIAL PRUNING STRATEGY Conventional post-training pruning methods, such as SparseGPT (Frantar & Alistarh, 2023), employ sequential layer-wise calibration. This paradigm is computationally prohibitive for diffusion models, as calibrating each layer necessitates executing full, multi-step denoising trajectory. To address this bottleneck, we introduce Module Packages, group-wise strategy that amortizes calibration costs by processing layers in batches. Our approach is built upon two concepts. Basic Unit is set of layers with mutually independent inputs in forward pass (e.g., query, key, and value projections), allowing for parallel processing. Module Package comprises one or more Basic Units, which are pruned and calibrated collectively. Our framework processes these packages sequentially. For each package, we first execute Groupwise Data Collection phase: we run the complete denoising trajectory once across the calibration dataset, using forward hooks to concurrently gather input statistics for all modules within the package. Subsequently, all modules are pruned simultaneously using their respective Timestep-Aware Hessian matrices. Crucially, the network state is updated sequentially between packages but remains static within package during data collection. This preserves the principle of sequential calibration at coarser, group-wise granularity, rendering the process computationally feasible. This strategy drastically reduces the number of calibration runs, with the primary trade-off being an increased memory footprint to store multiple Hessian matrices concurrently. Notably, our empirical results demonstrate that pruning accuracy has low sensitivity to package granularity, granting practitioners the flexibility to balance computational cost against memory constraints without significant performance sacrifice. 4.3 EXTENSION TO SEMI-STRUCTURED AND STRUCTURED PRUNING key advantage of our OBS-Diff framework is its adaptability. While focusing on unstructured pruning, it readily extends to both semi-structured and structured sparsity. 1Here, denotes the sequential index of the denoising iteration during inference, where is the total number of inference steps (e.g., for = 28, ranges from 1, 2, , 28). 5 Preprint Semi-Structured Pruning. For semi-structured patterns like 2:4 sparsity, the extension is direct. Within each block of four weights, we simply prune the two with the lowest per-weight OBS-Diff saliency scores, efficiently creating hardware-friendly models. Structured Pruning. For structured pruning of Feed-Forward Network (FFN) layers, we assess neurons importance by aggregating the saliency of its associated weights. The saliency Lq for an entire neuron (column q) and the corresponding weight update are: Lq = (cid:80) 2 :,q 2[H1]qq , δW = W:,q [H1]qq H1 :,q , (6) where the lowest-scoring neurons are removed. Similarly, for Multi-Head Attention (MHA), we prune entire heads. Our approach, inspired by SlimGPT (Ling et al., 2024), quantifies the saliency of each head. The calculation begins with the full Hessian matrix, H, for the output projection layer. For the j-th head, we consider its weight matrix Wj and the corresponding Hessian block Hj. The total saliency for this head, Lj, is found by aggregating the importance of its individual weights. The saliency is calculated as: Lj = (cid:88) k= (cid:80)(Wj)2 :,k 1)k,k (Hj , (7) where (Wj):,k is the k-th column of the weight matrix Wj, (Hj of the inverse Hessian block, and is the dimension of each head. 1)k,k is the k-th diagonal element However, MMDiTs joint attention mechanism presents unique challenge. Shared attention heads process concatenated multi-modal inputs, but are fed into separate, modality-specific output paths. This structure yields two distinct importance rankings for the same set of heads (one for each modality), while OBS-Diff processes the two output projection matrices after separation. To resolve this, we fuse these rankings into single, decisive list using Reciprocal Rank Fusion (RRF): SRRF = 1 + rankA(j) + 1 + rankB(j) , (8) where rankA(j) is the rank of head for modality A, and is stabilizing hyperparameter (e.g., 60). This fused score provides unified ranking to guide the pruning of shared attention heads. Subsequently, the weights of the entire output projection layer are updated using the full Hessian matrix, H, following the formulation presented in Eq. (6)."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 SETTINGS Models. To demonstrate the generalizability of OBS-Diff, we evaluate it across diverse range of text-to-image models: Stable Diffusion v2.1-base (866M) (Rombach et al., 2022), Stable Diffusion 3-Medium (2B) (Esser et al., 2024), Stable Diffusion 3.5-Large (8B), and Flux.1-dev (12B) (Black Forest Labs, 2024). For comparison with prior work, we also evaluate our method on DDPM (35.7M) (Ho et al., 2020) trained on the CIFAR-10 (32 32) dataset (Krizhevsky et al., 2009). Baselines. For text-to-image models, we compare against methods adapted from the Large Language Model (LLM) domain for unstructured/semi-structured sparsity, namely Wanda (Sun et al., 2024) and DSnoT (Zhang et al., 2024c), as well as standard magnitude pruning. For structured pruning, we employ an L1-norm based baseline (Li et al., 2017). On the CIFAR-10 DDPM, our method is directly compared with Diff-Pruning (Fang et al., 2023b). The sparsity refers to the pruning ratio of all the linear layers within MHA and FFN for each MMDiT block. For calibration, we utilize text prompts from the GCC3M dataset (Sharma et al., 2018). To ensure fair comparison, all methods and baselines utilize identical configurations (computational resources provided in Appendix B). The Wanda (Sun et al., 2024) and DSnoT (Zhang et al., 2024c) baselines are originally designed for unstructured and semi-structured pruning of LLMs. Their direct application to diffusion models is 6 Preprint Table 1: Quantitative comparison of unstructured pruning methods on text-to-image diffusion models. The best result per metric is highlighted in bold. (a) SD v2.1-base and SD 3-medium (b) SD 3.5-large and Flux 1.dev Base Model Sparsity (%) Method FID CLIP ImageReward Base Model Sparsity (%) Method FID CLIP ImageReward Dense Model SD v2.1-base 40 50 Dense Model SD 3-medium 60 Magnitude DSnoT Wanda OBS-Diff Magnitude DSnoT Wanda OBS-Diff Magnitude DSnoT Wanda OBS-Diff Magnitude DSnoT Wanda OBS-Diff 31. 27.86 31.63 27.96 28.19 49.38 69.05 41.84 27.41 36.14 221.24 63.37 43.98 27.20 349.53 211.58 170.33 28.49 0. 0.3111 0.3099 0.3122 0.3131 0.2959 0.2829 0.2988 0.3102 0.3162 0.1864 0.2908 0.3000 0.3167 0.1864 0.2222 0.2352 0.3099 0. 0.1864 -0.0422 0.1367 0.2061 -0.5580 -1.1395 -0.4704 -0.0356 0.9029 -2.2719 -0.5941 -0.1076 0.6468 -2.2807 -2.2271 -2.0641 0.1213 Dense Model SD 3.5-large 50 60 Dense Model Flux 1.dev 70 Magnitude DSnoT Wanda OBS-Diff Magnitude DSnoT Wanda OBS-Diff Magnitude DSnoT Wanda OBS-Diff Magnitude DSnoT Wanda OBS-Diff 31. 35.21 32.82 27.49 29.61 156.21 81.99 48.80 29.15 39.16 42.06 41.55 37.65 39.40 251.58 44.35 49.68 39.79 0. 0.3052 0.3113 0.3123 0.3142 0.2302 0.2706 0.2859 0.3119 0.3110 0.2974 0.3095 0.3086 0.3075 0.2104 0.2970 0.2957 0.2986 0. 0.1465 0.2323 0.4215 0.6146 -2.0296 -1.3198 -0.6402 0.3984 0.9661 -0.1945 0.7111 0.7576 0.7777 -2.2271 -0.3459 -0.1046 0.3697 non-trivial due to the iterative nature of the diffusion model. Specifically, we extended their pruning logic by incorporating the concept of module packages, enabling them to perform unstructured and semi-structured pruning targeted at the key components of the diffusion architecture. Critically, to ensure an equitable comparison with our Hessian-based method, the adapted DSnoT baseline is configured to use its Hessian-based importance score calculation mode. Evaluation Metrics. We evaluate the performance of the text-to-image models on subset of 5K prompts from the MS-COCO 2014 validation set (Lin et al., 2014). The evaluation is based on three metrics: Frechet Inception Distance (FID) (Heusel et al., 2017), CLIP Score (ViT-B/16) (Hessel et al., 2021), and ImageReward (Xu et al., 2023). For the DDPM on CIFAR-10, we report the FID score. We measure efficiency gains in terms of wall-clock time reduction and the decrease in FLOPs. 5.2 RESULTS OF UNSTRUCTURED PRUNING The results in Table 1 show the superiority of our OBS-Diff in terms of CLIP score and ImageReward. An interesting phenomenon is observed with the FID metric the pruned model can occasionally outperform the original dense model. E.g., at 40% sparsity on SD v2.1-base, the Magnitude method beats the dense model in FID, while our results suggest Magnitude does not produce visually better results. It is thereby conceived that FID may not be very reliable metric here to evaluate different pruning methods. Table 2: Performance of semi-structured (2:4 sparsity pattern) pruning on the Stable Diffusion 3.5-Large model. Pruning is applied to the 3rd through 25th MMDiT blocks. The best result is shown in bold. Base Model Method FID CLIP ImageReward SD 3.5-Large Dense Model 31.59 0.3156 Magnitude DSnoT Wanda OBS-Diff 45.39 32.40 32.08 32.13 0.2945 0.3069 0.3036 0.3129 0.7549 -0.4705 0.0307 -0.1363 0.4493 Regarding the CLIP score, OBS-Diff is the best-performing method in the vast majority of test cases, exhibiting only slight decrease compared to the dense models. Most notably, OBS-Diff consistently leads in the ImageReward metric across all benchmarks, indicating superior alignment with human aesthetic preferences. The superiority of our approach becomes most pronounced at high sparsity levels. For example, at 60% sparsity on SD 3.5-Large or 70% on Flux 1.dev, the performance of all baseline methods collapses, resulting in metrics that are significantly worse than ours. This quantitative degradation corresponds to qualitative failure; as illustrated in Figure 1, the images generated by baseline methods at high sparsity are often totally destroyed and suffer from severe artifacts, whereas OBS-Diff continues to produce high-quality and coherent results. Beyond its performance in generation quality, OBS-Diff is also highly efficient. For instance, the entire pruning process for the 2B-parameter SD 3-medium model completes in under 15 minutes on single NVIDIA RTX 4090, highlighting 7 Preprint Table 3: Performance of structured pruning on the Stable Diffusion 3.5-Large model across various sparsity levels. The first and last transformer blocks were excluded from the pruning process. The TFLOPs metric represents the theoretical computational cost for single forward pass of the entire transformer. For each sparsity group, the best result per metric is highlighted in bold. Base Model Sparsity (%) Method #Params TFLOPs FID CLIP ImageReward Dense Model 8.06 11. 31.59 0.3156 15% SD 3.5-Large 20% 25% 30% L1-norm OBS-Diff L1-norm OBS-Diff L1-norm OBS-Diff L1-norm OBS-Diff 7.28 9.63 (14.5%) 7.02 9.09 (19.3%) 6.76 8.55 (24.1%) 6.54 8.10 (28.1%) 158.89 32.64 189.50 32.46 228.82 33.73 327.48 34.51 0.2376 0. 0.2124 0.3149 0.2040 0.3128 0.2093 0.3107 0.7549 -2.0502 0.6446 -2.2385 0. -2.2651 0.3741 -2.2663 0.2221 its excellent cost-effectiveness. Detailed analyses of pruning time and the impact of sparsity on ImageReward are provided in Appendix C.1. 5.3 RESULTS ON SEMI-STRUCTURED PRUNING The results for 2:4 semi-structured pruning are presented in Table 2. Although Wanda obtains slightly better FID of 32.08 compared to our 32.13, OBS-Diff shows substantial advantages in semantic-level metrics. Notably, it surpasses the strongest baseline by large margin in both CLIP score (0.3129) and ImageReward (0.4493). This highlights our methods effectiveness in maintaining high-level semantic consistency and visual fidelity under hardware-friendly sparsity constraints. 5.4 RESULTS ON STRUCTURED PRUNING The results are presented in Table 3. The baseline L1-norm pruning suffers from catastrophic performance degradation even at modest 15% sparsity, with its FID score deteriorating from 31.59 to 158.89. In stark contrast, our method, OBS-Diff, demonstrates remarkable resilience. At the same 15% sparsity, OBS-Diff maintains an FID of 32.64, nearly identical to the dense models performance. This robustness persists up to 30% sparsity, where OBS-Diff sustains strong FID of 34.51 while the baseline model fails completely (FID of 327.48). These findings highlight OBS-Diffs superior ability to preserve critical model structures under aggressive structured pruning. To benchmark our method against established techniques, we compare OBS-Diff with DiffPruning (Fang et al., 2023b), well-recognized method that leverages gradient information for structured pruning on small class-conditional DDPMs. For brevity, the detailed results and analyses for the comparison are deferred to the C.3, where our method outperforms consistently. 5.5 WALL-CLOCK TIME COMPARISON To quantify the practical efficiency gains, we measure the wall-clock time for single forward pass through an MMDiT block of the SD3.5-Large model, on single NVIDIA 4090 GPU with batch size 4, resolution 10241024. Table 4 shows that both methods effectively reduce inference latency. The 2:4 semi-structured approach achieves 1.23 speedup, while our structured pruning method attains 1.31 speedup at 30% sparsity. These results validate the tangible practical acceleration benefits of applying these pruning techniques. Table 4: Wall-clock inference time (ms) and speedup for single MMDiT block under various sparsity schemes. Sparsity Type Time (ms) Speedup Dense Semi-structured (2:4) Structured (15%) Structured (20%) Structured (25%) Structured (30%) 14.36 11.71 13.96 11.95 11.17 10.99 / 1.23 1.03 1.20 1.29 1.31 8 Preprint Table 5: Ablation study of timestep weighting strategies, conducted on the SD3-Medium model at 50% unstructured sparsity. (For reference, the ImageReward of uniform strategy is 0.6355.) Table 6: Ablation study on the impact of the number of module packages on resource usage and performance, conducted on SD3-Medium model at 30% unstructured sparsity. Weight strategy ImageReward Pkgs. Mem. (GB) Time (s) ImageReward Linear increase Linear decrease Log increase Log decrease 0.6174 0.6384 0.6244 0.6438 1 4 10 20 30.67 24.05 22.75 22.08 572.20 896.52 1539.37 2594. 0.8569 0.8442 0.8429 0.8564 5.6 ABLATION STUDY We perform an ablation study to analyze the impact of three key components: (1) the timestepaware Hessian construction, (2) the number of module packages, and (3) the number of prompts in the calibration dataset. For this study, all variants are evaluated using the ImageReward metric on 1,000 prompts from the MS-COCO 2014 validation set. Timestep-Aware Hessian Matrix Establishment. To incorporate temporal information from the diffusion process, we introduce timestep-aware weighting during the Hessian matrix construction. This method assigns distinct weight to the hooked activations at each timestep. Empirical results demonstrate that assigning greater importance to earlier inference steps yields superior performance. As shown in Table 5, logarithmic decrease strategy significantly outperforms other weight distribution methods. Module-Package. The concept of module packages partitions the models layers for layer-wise compression. This approach introduces critical trade-off between computational resources and time. Processing the model in more packages reduces peak GPU memory, as the Hessian matrix for each pruning step is smaller. However, it proportionally increases the total runtime because the entire calibration dataset must be forwarded for each package. As shown in our ablation study  (Table 6)  , while the resource trade-off is evident, the number of packages does not show clear, predictable relationship with the final pruned models performance. Consequently, practitioners can select configuration that best fits their hardware constraints without sacrificing final model quality. Figure 3: Effect of the number of prompts in calibration dataset on the ImageReward. The Number of the Prompts in the Calibration Dataset. The size of the calibration dataset is critical hyperparameter that directly influences the quality of the approximated Hessian matrix. To find an optimal size, we evaluated post-pruning performance against the number of text prompts in the calibration dataset, as shown in Figure 3. The pruned models ImageReward score improves sharply up to 100 prompts and then plateaus, indicating point of diminishing returns where additional data offers no significant benefit to the Hessian approximation. Therefore, to balance performance gains with computational efficiency, we selected 100 prompts for our calibration dataset in all main experiments."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This work introduces OBS-Diff, novel one-shot, training-free pruning framework tailored for large-scale text-to-image diffusion models. By revitalizing the classic Optimal Brain Surgeon method, we address the unique challenges of iterative denoising through our proposed timestepaware Hessian construction, which prioritizes critical early-stage generation steps. To overcome prohibitive calibration costs, we devise group-wise sequential pruning strategy that effectively balances memory overhead and computational efficiency. The versatility of our framework extends across unstructured, semi-structured, and structured pruning, demonstrating its broad applicability. 9 Preprint Extensive empirical results show that OBS-Diff establishes new state-of-the-art in training-free diffusion model pruning, consistently outperforming existing methods by maintaining high generative quality, especially at high sparsity regimes."
        },
        {
            "title": "REFERENCES",
            "content": "Alireza Aghasi, Afshin Abdi, Nam Nguyen, and Justin Romberg. Net-trim: Convex pruning of deep neural networks with performance guarantee. In NeurIPS, 2017. Black Forest Labs. Flux. https://blackforestlabs.ai/, 2024. Accessed: 2025-09-25. Thibault Castells, Hyoung-Kyu Song, Bo-Kyeong Kim, and Shinkook Choi. Ld-pruner: Efficient pruning of latent diffusion models using task-agnostic insights. In CVPR, 2024. Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. In NeurIPS, 2017. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Depgraph: Towards any structural pruning. In CVPR, pp. 1609116101, 2023a. Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning for diffusion models. In NeurIPS, 2023b. Gongfan Fang, Kunjun Li, Xinyin Ma, and Xinchao Wang. Tinyfusion: Diffusion transformers learned shallow. In CVPR, 2025. Sicheng Feng, Keda Tao, and Huan Wang. Is oracle pruning the true oracle? arXiv preprint arXiv:2412.00143, 2024. Elias Frantar and Dan Alistarh. Optimal brain compression: framework for accurate post-training quantization and pruning. In NeurIPS, 2022. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In ICML, 2023. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Optq: Accurate quantization for generative pre-trained transformers. In ICLR, 2024. Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. 28, 2015. Song Han, Huizi Mao, and William Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. In ICLR, 2016. Babak Hassibi, David Stork, and Gregory Wolff. Optimal brain surgeon and general network pruning. In NeurIPS, 1992. Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Ptqd: Accurate posttraining quantization for diffusion models. In NeurIPS, 2023. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 10 Preprint Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training quantization with small calibration sets. In ICML, 2021. Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. Bk-sdm: lightweight, fast, and cheap version of stable diffusion. In ECCV, 2024. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In NeurIPS, 1989. Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. In ICLR, 2017. Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdquant: Absorbing outliers by low-rank component for 4-bit diffusion models. In ICLR, 2023a. Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. Q-diffusion: Quantizing diffusion models. In ICCV, 2023b. Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. In NeurIPS, 2023c. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. Gui Ling, Ziyang Wang, and Qingwen Liu. Slimgpt: Layer-wise structured pruning for large language models. In NeurIPS, 2024. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. In NeurIPS, 2022. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In ICML, 2020. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In ECCV, 2024. Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In CVPR, 2023. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. simple and effective pruning approach for large language models. In ICLR, 2024. Kaiwen Tuo and Huan Wang. Sparsessm: Efficient selective structured state space models can be pruned in one-shot. arXiv preprint arXiv:2506.09613, 2025. 11 Preprint Huan Wang, Can Qin, Yulun Zhang, and Yun Fu. Neural pruning via growing regularization. In ICLR, 2021. Jiateng Wei, Quan Lu, Ning Jiang, Siqi Li, Jingyang Xiang, Jun Chen, and Yong Liu. Structured optimal brain pruning for large language models. In EMNLP, 2024. Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In NeurIPS, 2023. Dingkun Zhang, Sijia Li, Chen Chen, Qingsong Xie, and Haonan Lu. Laptop-diff: Layer pruning and normalized distillation for compressing diffusion models. arXiv preprint arXiv:2404.11098, 2024a. Yang Zhang, Er Jin, Yanfei Dong, Ashkan Khakzar, Philip Torr, Johannes Stegmaier, and Kenji arXiv preprint Kawaguchi. Effortless efficiency: Low-cost pruning of diffusion models. arXiv:2412.02852, 2024b. Yuxin Zhang, Lirui Zhao, Mingbao Lin, Sun Yunyun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, and Rongrong Ji. Dynamic sparse no training: Training-free fine-tuning for sparse llms. In ICLR, 2024c. Yang Zhao, Yanwu Xu, Zhisheng Xiao, Haolin Jia, and Tingbo Hou. Mobilediffusion: Instant text-to-image generation on mobile devices. In ECCV, 2024."
        },
        {
            "title": "A DECLARATION OF LLM USAGE",
            "content": "The use of Large Language Models (LLMs) in this work served two purposes: (1) to aid and polish the paper writing, and (2) to generate some of the text prompts used by the diffusion model to create figures that are shown in the paper."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "This section provides further details on the experimental setup, including common configurations, baseline adaptations, and the computational hardware used for our evaluations. Common Configurations. To ensure controlled and fair comparison, all experiments, unless otherwise specified, adhere to common set of configurations. For all text-to-image generation tasks, we set the output resolution to 512 512 pixels to facilitate rapid experimentation across the diverse and large-scale models. For our method and baselines such as Wanda (Sun et al., 2024) and DSnoT (Zhang et al., 2024c), we consistently group model parameters into 4 module packages. Furthermore, logarithmic decreasing timestep weighting scheme (log decrease) was uniformly applied across all diffusion models and pruning methods to schedule the pruning process over the diffusion timesteps. Computational Resources. The training of the DDPM on the CIFAR-10 dataset was conducted on NVIDIA A100 GPUs. For the large text-to-image models, all pruning methods are training-free. The pruning and evaluation for Stable Diffusion v2.1-base, Stable Diffusion 3-Medium, and Stable Diffusion 3.5-Large were performed on single NVIDIA RTX 4090 GPU, each equipped with 48GB of VRAM. Due to its substantial memory footprint, all experiments involving the FLUX.1dev model, including its pruning and evaluation, was conducted on single NVIDIA A100 GPU with 80GB of VRAM. 12 Preprint"
        },
        {
            "title": "C MORE EXPERIMENTAL RESULTS",
            "content": "C.1 MORE ANALYSIS FOR UNSTRUCTUREDLY PRUNED SD3-MEDIUM As illustrated in Figure 4, our proposed OBS-Diff method consistently outperforms all baseline approaches in terms of the ImageReward metric across all evaluated sparsity levels. The superiority of our method is particularly pronounced at higher sparsity ratios. For instance, at 60% sparsity, the performance of competing methods collapses, yielding negative ImageReward scores. In stark contrast, OBS-Diff maintains positive score, demonstrating its exceptional robustness in highcompression scenarios. In terms of computational efficiency, Table 7 indicates that OBS-Diff has the longest pruning time on single NVIDIA RTX 4090. However, the additional overhead is marginal, requiring only slightly more time than DSnoT (14.95 vs. 14.25 minutes). Considering the substantial gains in generation quality and model robustness, we conclude that OBS-Diff offers superior trade-off between performance and computational cost, establishing it as highly cost-effective pruning solution. Table 7: Pruning time of different unstructured pruning methods on SD3-Medium (2B) at 50% sparsity. Method Time (min) Magnitude 0 7.32 Wanda 14.25 DSnoT 14.95 OBS-Diff Figure 4: ImageReward vs. sparsity for various unstructured pruning methods on SD3Medium. C.2 STRUCTURED PRUNING FOR SD3-MEDIUM We evaluate our structured pruning method, OBS-diff, on the Stable Diffusion 3-medium model and compare it against the widely-used L1-norm magnitude pruning baseline. As summarized in Table 8, the baseline method suffers from severe performance degradation as sparsity increases. In contrast, our approach maintains performance remarkably close to the original dense model across all tested sparsity levels, demonstrating its effectiveness and robustness. Table 8: Performance comparison of structured pruning methods at 10%, 15%, 20%, and 25% sparsity on the Stable Diffusion 3-medium model (2B). The first and last transformer blocks were excluded from the pruning process. The TFLOPs metric represents the theoretical computational cost for single forward pass of the entire transformer. For each sparsity group, the best result per metric is highlighted in bold. Base Model Sparsity (%) Method #Params TFLOPs FID CLIP ImageReward Dense Model 2.03 2.84 31.59 0.3156 10% SD 3-medium 15% 20% 25% L1 norm Ours (OBS-diff) L1 norm Ours (OBS-diff) L1 norm Ours (OBS-diff) L1 norm Ours (OBS-diff) 1.91 2.59 (8.8%) 1.83 2.43 (14.4%) 1.78 2.31 (18.7%) 1.72 2.19 (22.9%) 267.32 35.65 326.92 34.33 348.77 33. 365.24 32.96 0.2035 0.3166 0.1942 0.3168 0.1926 0.3163 0.1906 0.3143 0. -2.2611 0.8118 -2.2768 0.6717 -2.2768 0.4997 -2.2786 0.2782 13 Preprint C.3 COMPARISON WITH DIFF-PRUNING ON DDPM To evaluate the generalizability of our method beyond large-scale text-to-image models, we adapt it to the task of structured pruning for Denoising Diffusion Probabilistic Model (DDPM) on the CIFAR-10 dataset. Our adaptation leverages the column masks identified by Diff-Pruning, which are then integrated with our OBS weight update mechanism as detailed in Eq. (6). As presented in Table 9, our method surpasses the current state-of-the-art baseline, Diff-Pruning, by achieving superior FID score under an identical fine-tuning budget (100K steps). This result demonstrates not only the versatility of our approach but also suggests that the model pruned by OBS-Diff serves as more effective checkpoint for subsequent fine-tuning. Table 9: Performance of pruned DDPMs on CIFAR-10 (32 32). All pruned models are finetuned for 100K steps. Evaluations are conducted on samples generated via 100 DDIM steps. The best FID score is highlighted in bold. Method Pretrained Random Pruning Magnitude Pruning Diff-Pruning OBS-Diff #Params MACs FID Train Steps 35.7M 13.95M 13.95M 13.95M 13.95M 6.1G 2.1G 2.1G 2.1G 2.1G 4.50 7.85 7.91 7. 7.55 800K 100K 100K 100K 100K"
        },
        {
            "title": "D ADDITIONAL QUALITATIVE RESULTS",
            "content": "Figure 5: More qualitative comparison of unstructured pruning methods on the SD3-Medium model. We evaluate Magnitude, DSnoT, Wanda, and our method (OBS-Diff) at various sparsity levels (20%, 30%, 40%, and 50%) using the same prompt and negative prompt. All images are generated at resolution of 512 512. 14 Preprint Figure 6: More qualitative comparison of unstructured pruning methods on the SD3-Medium model. We evaluate Magnitude, DSnoT, Wanda, and our method (OBS-Diff) at various sparsity levels (20%, 30%, 40%, and 50%) using the same prompt and negative prompt. All images are generated at resolution of 512 512. Figure 7: More qualitative comparison of unstructured pruning methods on the SD3-Medium model. We evaluate Magnitude, DSnoT, Wanda, and our method (OBS-Diff) at various sparsity levels (20%, 30%, 40%, and 50%) using the same prompt and negative prompt. All images are generated at resolution of 512 512. Preprint Figure 8: Qualitative comparison of unstructured pruning methods on Flux 1.dev at 70% sparsity. Results from Magnitude, DSnoT, Wanda, and our proposed OBS-Diff are shown. Figure 9: Qualitative comparison of structured pruning methods on the SD3.5-Large model at various sparsity levels (15%, 20%, 25%, and 30%). Results from the L1-norm baseline and our proposed OBS-Diff are shown. 16 Preprint Figure 10: Qualitative comparison of structured pruning methods on the SD3.5-Large model at various sparsity levels (15%, 20%, 25%, and 30%). Results from the L1-norm baseline and our proposed OBS-Diff are shown. Figure 11: Qualitative comparison of structured pruning methods on the SD3.5-Large model at various sparsity levels (15%, 20%, 25%, and 30%). Results from the L1-norm baseline and our proposed OBS-Diff are shown. Figure 12: Qualitative comparison of structured pruning methods on the SD3.5-Large model at various sparsity levels (15%, 20%, 25%, and 30%). Results from the L1-norm baseline and our proposed OBS-Diff are shown."
        }
    ],
    "affiliations": [
        "Westlake University",
        "Zhejiang University"
    ]
}