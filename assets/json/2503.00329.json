{
    "paper_title": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs",
    "authors": [
        "Benjamin Schneider",
        "Florian Kerschbaum",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual embedding models excel at zero-shot tasks like visual retrieval and classification. However, these models cannot be used for tasks that contain ambiguity or require user instruction. These tasks necessitate a multimodal embedding model, which outputs embeddings that combine visual and natural language input. Existing CLIP-based approaches embed images and text independently, and fuse the result. We find that this results in weak interactions between modalities, and poor user control over the representation. We introduce ABC, an open-source multimodal embedding model that uses a vision-language model backbone to deeply integrate image features with natural language instructions. ABC achieves bestfor-size performance on MSCOCO image-to-text retrieval and is the top performing model on classification and VQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly unified vision-language representation, ABC can use natural language to solve subtle and potentially ambiguous visual retrieval problems. To evaluate this capability, we design CtrlBench, a benchmark that requires interleaving textual instructions with image content for correct retrieval. ABC advances the state of multimodal embeddings by offering high-quality representations and flexible natural language control. Our model and datasets are available at our project page."
        },
        {
            "title": "Start",
            "content": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs Benjamin Schneider 1 2 Florian Kerschbaum 1 Wenhu Chen 1 2 https://tiger-ai-lab.github.io/ABC/"
        },
        {
            "title": "Abstract",
            "content": "Visual embedding models excel at zero-shot tasks like visual retrieval and classification. However, these models cannot be used for tasks that contain ambiguity or require user instruction. These tasks necessitate multimodal embedding model, which outputs embeddings that combine visual and natural language input. Existing CLIP-based approaches embed images and text independently, and fuse the result. We find that this results in weak interactions between modalities, and poor user control over the representation. We introduce ABC, an open-source multimodal embedding model that uses vision-language model backbone to deeply integrate image features with natural language instructions. ABC achieves bestfor-size performance on MSCOCO image-to-text retrieval and is the top performing model on classification and VQA tasks in the Massive Multimodal Embedding Benchmark. With strongly unified vision-language representation, ABC can use natural language to solve subtle and potentially ambiguous visual retrieval problems. To evaluate this capability, we design CtrlBench, benchmark that requires interleaving textual instructions with image content for correct retrieval. ABC advances the state of multimodal embeddings by offering high-quality representations and flexible natural language control. Our model and datasets are available at our project page. 5 2 0 2 1 ] . [ 1 9 2 3 0 0 . 3 0 5 2 : r 1. Introduction Visual embeddings have become foundational representation in computer vision. Image embedding models have become the state of the art for many zero-shot tasks, including visual retrieval (Chen et al., 2024) and image classification (Yu et al., 2022). Since the release of CLIP (Radford 1Cheriton School of Computer Science, University of Waterloo 2Vector Institute, Toronto. Correspondence to: Benjamin Schneider <Benjamin.Schneider@uwaterloo.ca>. Figure 1: An example use case of our model; information retrieval on specific aspects of scene. et al., 2021), its dual encoder architecture has remained the state of the art for producing high-quality visual embeddings (Yu et al., 2022; Sun et al., 2023; Chen et al., 2024). However, CLIP only supports separately embedding images or text (Radford et al., 2021). Therefore, complex visual embedding tasks which require additional specification are impossible. For example, CLIP model cannot distinguish which is the correct answer in Figure 1, as both captions are plausible unless the user provides additional instruction. For such tasks, multimodal embedding, representation that interleaves vision and natural language, is essential. We find that existing approaches suffer from two problems: (1) Vague and repetitive instructions. During training the same instructions are repeated and reused, which results in instruction overfitting (Gudibande et al., 2023). (2) Weak interaction between modalities. Previous works fuse embeddings outputted by CLIP models (Zhang et al., 2024; Wei et al., 2023). This approach prevents deeper interaction between modalities, resulting in superficial use of the instructions (Jiang et al., 2024b). To this end, we introduce ABC, model that uses user instruction to control multimodal embeddings. ABCs visionlanguage model (VLM) backbone allows it to integrate natural language instructions when crafting visual embeddings. We find that training our model has two fundamental challenges: (1) Extracting useful contrastive embeddings from 1 ABC: Achieving Better Control of Multimodal Embeddings using VLMs Figure 2: An overview of our training regime. We use negative mining to augment our pretraining dataset with almost plausible text negatives for each image query. In our instruction finetuning stage, we craft multiple instructions from each image. We use multiple for captions for same image as negatives, the model must use the natural language instruction to choose the best (positive) text candidate for the query. pretrained generative VLM. (2) Designing an instruction fine-tuning method that lets users modify multimodal embeddings using natural language instructions. To train ABC, we adopt multi-stage training process. In the initial pretraining stage, we use contrastive training with carefully selected negatives to develop model that generates embeddings, similar to CLIP. In the second stage, we train lightweight adapter using synthetic natural language instructions that correspond to different aspects of the same image. This training process results in model that produces powerful and flexible multimodal embeddings. ABC achieves impressive zero-shot performance in retrieval, classification, and visual question answering (VQA) tasks. In MSCOCO (Lin et al., 2015) image-to-text retrieval, our model outperforms all CLIP models containing at most 8 billion parameters. Furthermore, our model outperforms all other models on the zero-shot classification and VQA splits of MMEB (Jiang et al., 2024b), multimodal embedding benchmark spanning 19 tasks. Lastly, we design CtrlBench to measure our models ability to use natural language instructions to control retrieval. Using CtrlBench, we show that ABC can accomplish visual retrieval tasks that are fundamentally ambiguous without utilizing natural language instructions. Our contribution is threefold. (1) ABC: an open-source multimodal embedding model that uses natural language instructions to control visual embeddings. We demonstrate that ABC produces powerful embeddings by benchmarking on zero-shot tasks and robustly utilizes natural language instructions to control embeddings. (2) We provide decoupled methodology for adapting VLMs into SOTA multimodal embedding models. Previous work integrates instructions during the large-scale contrastive training run. We show that these stages can be decoupled, resulting in lightweight and adaptable instruction fine-tuning stage that requires only 100 training steps. (3) Lastly, we introduce CtrlBench, novel benchmark for measuring instructioncontrolled retrieval. CtrlBench requires the model to interleave modalities to retrieve the correct response. 2. Related Works Visual embeddings. Since Radford et al. (2021) introduced CLIP, various aspects of the original model have been improved. DataComp (Gadre et al., 2023) demonstrated the importance of data filtering during contrastive pretraining to create CLIP models. Training CLIP models requires largescale pretraining and device parallelization to achieve the throughput and batch sizes needed to produce the best performing models (Cherti et al., 2023b; Sun et al., 2023; Yu et al., 2022). InternVL-G (Chen et al., 2024) is the largest and best performing open-weight CLIP model to date, 14 billion parameter model trained to create vision encoder for the InternVL family of VLMs. Multimodal embeddings. MagicLens (Zhang et al., 2024) and UniIR (Wei et al., 2023) are multimodal embedding models that fuse CLIP embeddings. By combining multiple CLIP models, they take image-text pairs and align them with image-text pairs using contrastive loss. VLM2Vec (Jiang et al., 2024b) and E5-V (Jiang et al., 2024a) also use VLM backbones to produce multimodal embeddings. VLM2Vec uses contrastive training to align text-image queries to textimage candidates; alternatively, E5-V is trained only by aligning text pairs. Jiang et al. (2024b) introduced Massive 2 ABC: Achieving Better Control of Multimodal Embeddings using VLMs Multimodal Embedding Benchmark (MMEB), benchmark for variety of multimodal embedding tasks. Extracting Embeddings from LLMs. Several architectural modifications have been proposed for adapting LLMs to produce dense embeddings (Wang et al., 2024a; BehnamGhader et al., 2024; Lee et al., 2024). LLM2VEC (BehnamGhader et al., 2024) changed the attention mask to be bidirectional, which allows information to flow between all tokens, improving embedding quality. Several methods for extracting dense embedding from LLMs have been proposed, these include picking the last token (Jiang et al., 2024b), mean token pooling (BehnamGhader et al., 2024) or an additional attention layer (Lee et al., 2024). In the text-only domain, the Mistral-7b (Jiang et al., 2023) backboned NV-Embed (Lee et al., 2024) has achieved the top score on MTEB (Muennighoff et al., 2023), popular text embedding benchmark. Mined negatives and synthetic instructions. Negative mining is technique in contrastive training where somewhat relevant candidates are included in each batch (de Souza P. Moreira et al., 2024). This improves the quality of embeddings by ensuring they can differentiate between subtly different candidates. This is often used for training text embedding models, and features prominently in NV-Embeds training (Lee et al., 2024). MegaPairs (Zhou et al., 2024) uses VLM to create open-ended instructions to train multimodal retrievers. They use this data synthesis pipeline to train two versions of their MMRet model, one uses CLIP (Radford et al., 2021) as its backbone and the other uses LLava-NeXT (Liu et al., 2024a). 3. ABC Figure 2 is an overview of our training regime and model architecture. Our training regime consists of 2 distinct stages; pretraining and instruction fine-tuning. The pretraining stage uses self-supervised training on image-caption pairs to adapt features used for generative modeling into features for dense embeddings. As our pretraining does not require instructions, it can be easily scaled using any large image-captioning data source (Changpinyo et al., 2021; Schuhmann et al., 2022). In the second stage, we train using queries consisting of images and synthetic text instructions. We also revise the positive caption to correspond to the specific aspect of the image that is relevant to the instruction. 3.1. Model Design We require that natural language instructions modify image representations, and vice versa. Therefore, an architecture that supports many attention interactions between modalities is ideal. For this reason, we utilize VLM as our model backbone. To adapt the VLM to output dense embeddings, we make several architectural changes. Following Figure 3: sample from our pretraining dataset. The positive caption (green) is the best caption for the image. The mined negatives (red) are relevant but not the best choice. BehnamGhader et al. (2024), we enable bidirectional attention, allowing all tokens to attend to all other tokens. To create our dense embeddings, we mean pool over tokens in the last hidden layer and project the result using simple residually connected LP layer given by equation 1. LP (x) = + Ag(Bx) (1) Where and are parameter matrices and is the elementwise SELU function (Klambauer et al., 2017). To train our backbone we apply LoRA (Hu et al., 2021) adapters on both the vision encoder and LLM modules. We optimize the contrastive loss temperature hyperparameter τ during pretraining, but freeze it during instruction fine-tuning. At the beginning of the instruction fine-tuning stage, we fuse the pretrained LoRA weights into the base model and freeze the LP adapter layer. We then initialize new lower rank LoRA adapter on our LLM backbone. 3.2. Data and Training To create our pretraining dataset we employ negative mining on Conceptual Captions (Sharma et al., 2018). We derive the mined negatives for our dataset as follows: (1) We do small pretraining run using only in-batch negatives. (2) We use the resulting model to calculate similarity scores between all images and captions in our pretraining dataset. This approach avoids circular dependence on third-party embedding model to train our embedding model. Therefore, it is easily extensible to modalities where an existing embedding model is not publicly available. (3) To prevent our negatives from being too similar to our positive samples, we set similarity threshold ϵ [0, 1]. We only sample negatives that have similarity score of at most ϵ times the similarity score of the correct candidate. We randomly choose our mined hard negatives from the 100 candidate captions below the threshold. Our approach is similar to negative mining technique from NV-Retreiver (de Souza ABC: Achieving Better Control of Multimodal Embeddings using VLMs MSCOCO (5K test set) Flickr30K (1K test set) Image Text Text Image Image Text Text Image Model R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 CLIP (Radford et al., 2021) ALIGN (Jia et al., 2021) FLAVA (Singh et al., 2022) FILIP * (Yao et al., 2021) CoCa * (Yu et al., 2022) OpenCLIP-G (Cherti et al., 2023a) EVA-02-CLIP-E+ (Sun et al., 2023) ABC (ours) 58.4 58.6 42.7 61.3 66.3 67.3 68. 69.2 81.5 83.0 76.8 84.3 86.2 86.9 87.8 87.9 88.1 89.7 - 90.4 91.8 92.6 92.8 93.2 37.8 45.6 38.4 45.9 51.2 51.4 51. 47.6 62.4 69.8 67.5 70.6 74.2 74.9 75.0 72.1 72.2 78.6 - 79.3 82.0 83.0 82.7 80.6 88.0 88.6 67.7 89.8 92.5 92.9 93. 90.7 98.7 98.7 94.0 99.2 99.5 99.3 99.4 99.0 99.4 99.7 - 99.8 99.9 99.8 99.8 99.5 68.7 75.7 65.2 75.0 80.4 79.5 78. 74.6 90.6 93.8 89.4 93.4 95.7 95.0 94.2 92.6 95.2 96.8 - 96.3 97.7 97.1 96.8 95.45 Table 1: Comparison of retrieval performance on MSCOCO (Lin et al., 2015) and Flickr30K (Plummer et al., 2016) datasets (Karpathy split). Best performance is bold, second best is underlined. * indicates closed-weight model. P. Moreira et al., 2024), text embedding model. This results in the text candidates shown in Figure 3. The mined text negatives are clearly relevant, but the correct caption is still the best answer. Stage 1: Pretraining with mined negatives. In our pretraining run, each batch consists of image queries (without instructions) and text candidates. We include mined text negatives in each batch. Therefore, each image query has 1 corresponding mined negatives. Our pretraining loss function is given by Equation (2). (cid:88) log (cid:80)N j=1(exp( exp(x yj τ yi/τ ) 1 k=1 exp( ) + (cid:80) (2) nk τ )) For given query xi, nk is its kth corresponding mined hard negative and and yi is its positive caption. τ is the temperature hyperparameter used to scale the loss. nk is mined negative for xi when = j, otherwise it acts as regular in-batch negative. The embeddings for xi, yi and nk are unit normalized before loss is computed. We scale the loss by 1 , the number of image queries in the batch. Instruction design. To create our instruction fine-tuning dataset we use Visual Genome (Krishna et al., 2016), dataset comprised of images with captioned bounding boxes. When choosing which bounding boxes to use for each image, we filter by the size of the bounding box (width height). We exclude the 5 largest bounding boxes, as we find they often do not represent specific objects or aspects of the scene. For each image, we randomly choose 4 bounding boxes and their respective captions. We then prompt GPT4o (OpenAI, 2024) to create instructions corresponding to each bounding box caption. We sample multiple captions from each image so that they can be used as negatives for each other during instruction fine-tuning. This ensures that each image query has multiple realistic captions in its batch. Therefore, utilizing the instruction is required to choose the correct text candidate unambiguously. We provide the prompt and generation settings used to create our instruction fine-tuning dataset in Appendix A. Stage 2: Instruction Fine-tuning. Following the standard VLM instruction format, we insert the instruction tokens after the image tokens in our prompt template: <Image> Instruction : <Instruction> (3) We instruction fine-tune using exclusively in-batch negatives. However, we group all queries that contain the same image into the same batch. This ensures that queries always have multiple relevant text candidates. We do not back-propagate through text candidate embeddings, only the queries containing an image and instruction. All text candidates are embedded using the frozen model from stage 1. This ensures that the embeddings of our image-text queries share the same features as the pretrained model. We can easily alternate between embedding with or without instructions by disabling the instruction fine-tuned LoRA adapter. 4. Experiments Benchmarks. We evaluate two aspects of the model. First, we assess the overall quality of the embeddings output by the model. We evaluate our model on variety of zero-shot retrieval, classification and VQA tasks (Section 4.1). We further demonstrate that scaling VLM encoder resolution during test time correlates with significant improvement on certain benchmarks (Section 4.2). Secondly, we measure our models ability accomplish tasks that are ambiguous without natural language instructions. To measure this, we construct CtrlBench, benchmark where the model must use both the image and instruction to retrieve the most appropriate caption (Section 4.4). Ablations. We ablate over several model design and training decisions. We find that our mined negative pretraining ABC: Achieving Better Control of Multimodal Embeddings using VLMs CLIP OpenCLIP SigLIP UniIR MagicLens BLIP2 MMRet ABC (ours) Classification (9 tasks) ImageNet-1K (Russakovsky et al., 2015) HatefulMemes (Kiela et al., 2021) VOC2007 (Everingham et al.) SUN397 (Xiao et al., 2010) Place365 (Lopez-Cifuentes et al., 2020) ImageNet-A (Djolonga et al., 2020) ImageNet-R (Hendrycks et al., 2021) ObjectNet (Barbu et al., 2019) Country-211 (Radford et al., 2021) All Classification VQA (10 tasks) OK-VQA (Marino et al., 2019) A-OKVQA (Schwenk et al., 2022) DocVQA (Mathew et al., 2021b) InfographicsVQA (Mathew et al., 2021a) ChartQA (Masry et al., 2022) Visual7W (Zhu et al., 2016) ScienceQA (Lu et al., 2022) VizWiz (Gurari et al., 2018) GQA (Hudson & Manning, 2019) TextVQA(Singh et al., 2019) All VQA 55.8 51.1 50.7 43.4 28.5 25.5 75.6 43.4 19.2 43.7 7.5 3.8 4.0 4.6 1.4 4.0 9.4 8.2 41.3 7.0 9.1 63.5 51.7 52.4 68.8 37.8 14.2 83.0 51.4 16. 48.8 11.5 3.3 5.3 4.6 1.5 2.6 10.2 6.6 52.5 10.9 10.9 45.4 47.2 64.3 39.6 20.0 42.6 75.0 40.3 14.2 43.2 2.4 1.5 4.2 2.7 3.0 1.2 7.9 2.3 57.5 1. 8.4 58.3 56.4 66.2 63.2 36.5 9.8 66.2 32.2 11.3 44.5 25.4 8.8 6.2 4.6 1.6 14.5 12.8 24.3 48.8 15.1 16.2 48.0 49.0 51.6 57.0 31.5 8.0 70.9 31.6 6. 39.3 12.7 2.9 3.0 5.9 0.9 2.5 5.2 1.7 43.5 4.6 8.3 10.3 49.6 52.1 34.5 21.5 3.2 39.7 20.6 2.5 26.0 8.7 3.2 2.6 2.0 0.5 1.3 6.8 4.0 9.7 3. 4.2 49.1 51.0 74.6 60.1 35.3 31.6 66.2 49.2 9.3 47.4 28.0 11.6 12.6 10.6 2.4 9.0 23.3 25.9 41.3 18.9 18.4 71.2 52.1 81.4 71.8 40.7 49.4 86.8 67.7 18. 60.0 48.1 37.3 28.5 7.9 11.7 25.6 26.3 29.4 60.1 35.4 31.0 Table 2: Zero-shot classification and VQA results on MMEB (Jiang et al., 2024b). We compare with pretrained CLIP models (Radford et al., 2021; Cherti et al., 2023a), instruction finetuned models derived from CLIP (Wei et al., 2023; Zhang et al., 2024) and other LLM backbone approaches (Li et al., 2023; Zhou et al., 2024). regime increases performance and stabilizes loss dynamics during pretraining (Section 4.3). We evaluate our choice of VLM backbone and its effect on representation quality (Section 4.5). Lastly, we provide ablations over adapter and attention configuration in Appendix B. Training settings. We use Qwen2-VL-7B (Wang et al., 2024b) as our VLM backbone for all experiments. We train our negative mining model using only in-batch negatives with batch size of 256 for 1000 steps. We set ϵ = 0.95 and sample 7 mined negatives for each image. We pretrain using batches of 512 image queries and 4096 text candidates sharded across 8 NVIDIA A100-SXM4-80GB GPUs (Qu et al., 2021) for 4000 steps. We use LoRA adapter with rank of 64 and an alpha of 128. We limit the number of tokens output by the vision encoder to 512 during training. For our optimizer, we use AdamW (Loshchilov & Hutter, 2019) with learning rate of 4 105, betas of 0.9 and 0.999 and weight decay of 103. We warm-up for 3% of training steps and initialize the temperature τ as 7 102. In our instruction fine-tuning stage, we use lower rank LoRA adapter with rank and alpha of 16 and 32, respectively. Our instruction fine-tuning stage can be short, as our VLM backbone is already instruction fine-tuned. Therefore, we only instruction fine-tune for 100 steps. Each batch contains 128 unique images, with each image appearing four times, paired with different instruction and corresponding positive text candidate. Modifications to save VRAM. Due to our use of LoRA (Hu et al., 2021), the VRAM requirements for gradients and optimizer state is relatively low. Consequentially, the model activations used by autograd for the backward pass account for most of VRAM used during training. To address this, we make aggressive use of activation checkpointing, recomputing the activations of each decoder block during the backward pass. Furthermore, we modify the VLM backbone to skip the logits and cross-entropy loss computation. With the large vocabulary size of modern LLMs, the output layer has become the most memory-intensive layer (Wijmans et al., 2024). As we only use the hidden state for calculating embeddings, the logits tensor is unnecessary. We find that this simple change saves up to 11 GB of memory per device, allowing us to use to significantly larger batch size. 4.1. Zero-shot Evaluations Table 1 demonstrates the retrieval capabilities of our model on MSCOCO (Lin et al., 2015) and Flickr30K (Plummer et al., 2016). Our model demonstrates impressive imageto-text retrieval capabilities, achieving competitive performance to models that have been contrastively trained with hundreds of GPUs and massive batch sizes (Sun et al., 2023; 5 ABC: Achieving Better Control of Multimodal Embeddings using VLMs Figure 4: (Left) Pixel distributions of benchmarks. (Right) Scaling the number of pixels (tokens) in the vision encoder. Cherti et al., 2023a). We achieve the best performance in MSCOCO image-to-text retrieval. Comparatively, our textto-image retrieval is weaker. This follows from us training use image-to-text negatives, not text-to-image negatives. To benchmark our models zero-shot image classification and VQA abilities, we use MMEB (Jiang et al., 2024b), collection of multimodal embedding tasks. We note that the classification labels in MMEB are short, often only one or two words. This is problematic for image-captioning models that have been trained on full sentences (Yu et al., 2022). To alleviate this issue, we use Radford et al. (2021)s technique of embedding classification labels in sentence template. We use photo of {label}. as our template for all classification evaluations. We use instructions for VQA but not for image-level retrieval or classification, as we find that instructions are unnecessary for these tasks. Our zero-shot classification and VQA results  (Table 2)  are very strong compared to other multimodal embedding models. We average 11.2% better in classification and 12.6% better in VQA than the next-best model. We find that plausible negatives are crucial during instruction fine-tuning, not just pretraining. In just 100 training steps using instructions that include negatives, our model surpasses MMRet (Zhou et al., 2024) on VQA, which was trained with millions of instructions. Interestingly, CLIP derivatives fine-tuned with instructions such as MagicLens (Zhang et al., 2024) and UniIR (Wei et al., 2023) do not perform significantly better than their respective baselines, despite MMEB providing instructions for each task. This evidences that encoder-only embedding models struggle to effectively utilize natural language instructions (Jiang et al., 2024b). 4.2. Image Resolution during Test Time Qwen2-VL-7B (Wang et al., 2024b) supports image inputs with variable resolution. This allows the user to effectively trade-off image resolution with inference speed by adjusting the number of tokens output by the vision encoder. We examine how this trade-off influences embedding quality for image classification. We find that performance on certain tasks, like ObjectNet (Barbu et al., 2019) and ImageNetA (Djolonga et al., 2020), strongly correlate with the resolution used in the VLM vision encoder (Figure 4). On average, ObjectNet images have 13 times more pixels than those from ImageNet-1K. When the number of tokens produced by the vision encoder is scaled up, we see large improvement on ObjectNet, with accuracy increasing by 23.4%. However, lower resolution benchmarks like ImageNet-1K do not benefit nearly as much from scaling resolution. Notably, smartphone camera takes photos with significantly higher resolution than images in ImageNet-1K or ObjectNet, by default (Apple Inc.). This motivates the need for benchmarks containing larger resolution images. Existing low-resolution benchmarks may under predict the capabilities of models that are able to natively utilize high resolution images. Furthermore, we find that correctly classifying the natural adversarial examples of ImageNet-A (Djolonga et al., 2020) is largely function of resolution. Our accuracy on ImageNet-A increases from only 12% to 47.5% simply by scaling the resolution used by the vision encoder during evaluation. 4.3. Temperature and Loss Dynamics The setting of the temperature hyperparameter (τ ) is known to be crucial for contrastive training (Jia et al., 2021). We find that poor treatment of the τ parameter is the easiest way to lose model performance. In particular, optimizing τ throughout the pretraining process is crucial. In shorter runs like training our negative mining model, we find that shows that both the initialization and optimization of τ is crucial. large initialization of τ requires many additional training steps to optimize to its correct value, whereas low initialization results in training instability (Figure 5). For our negative mining model, we find τ = 0.07 to be temperature initialization that is both stable and performant. Temperature in pretraining. We find that the mined nega6 ABC: Achieving Better Control of Multimodal Embeddings using VLMs Figure 5: Visualization of temperature (τ ) and its effects on training dynamics. (Left) The effect of temperature initializations on gradient norm while training our negative mining model. (Middle) Temperature behavior during pretraining, without and without mined negatives. (Right) Validation accuracy during pretraining, with and without mined negatives. tives are essential for training stability. Figure 5 shows our pretraining run with batch size of 128 images queries and 1024 text candidates. If randomized negatives are used instead of our chosen mined negatives, the optimizer tends to push τ very close to 0. This results in numerical instability, loss spikes, and eventually catastrophic failure. When our mined negatives are used in our pretraining, the temperature stabilizes at very reasonable value of around 0.03. 4.4. CtrlBench To measure our models instruction following capabilities, we construct CtrlBench, an instruction-controlled retrieval benchmark. CtrlBench has similar format to the Flicker30K (Plummer et al., 2016) test split. However, instead of each image having multiple valid captions, we instead provide an instruction from which the model can infer the most relevant text candidate. As each image has 5 associated captions, model that cannot utilize instructions can (in expectation) achieve at most 20% R@1 on the benchmark. Therefore, CtrlBench tests both retrieval and instruction following capabilities. To construct CtrlBench, we sample 1000 test images from VisualGenome (Krishna et al., 2016). To create 5000 instructions and text candidate pairs, we generate 5 instructions for each image. We follow the same process for generating instructions as in Section 3.2. We remove duplicated captions from the dataset to prevent ambiguity in our text candidates. We filter CtrlBench to ensure that no images, instructions or retrieval candidates used during training are contained in the benchmark. We provide discussion on CtrlBenchs motivation and design choices in Section 5.3 and samples in Appendix E. Table 3: Performance on CtrlBench. Model R@1 R@5 R@10 UniIR (Wei et al., 2023) MagicLens (Zhang et al., 2024) VLM2Vec (Jiang et al., 2024b) ABC (ours) 0.0 9.7 24.0 39.7 0.0 23.9 49.7 69.1 0.1 32.94 61.1 78.9 Table 4: Accuracy vs. generative task performance. Model LLaVA-NeXT InternVL-2-8B Qwen2-VL-7B Accuracyval MMLUval MMBenchEN 35.3 51.8 54.1 68.7 81.7 83.0 62.6 65.9 70.2 Table 3 shows the performance of multimodal embedding models on CtrlBench. We compare with 2 instruction fine-tuned CLIP derivatives: UniIR (Wei et al., 2023) and MagicLens (Zhang et al., 2024) as well as VLM2Vec (Jiang et al., 2024b), concurrent work on adapting VLMs into multimodal embedding models. We find that neither of the CLIP-based architectures have above 20% R@1 on CtrlBench, the performance that indicates that the model is non-trivially utilizing instructions. Conversely, the VLM architectures are better at instruction-controlled retrieval. Both ABC and VLM2Vec have R@1 above 20%. 4.5. VLM Backbone In this section, we explore whether the choice of VLM backbone has an effect on the quality of our multimodal embedding. We ablate over 3 popular choices of VLM backbone, all at approximately the 8 billion parameter scale: Qwen2-VL-7B (Wang et al., 2024b), our chosen backbone. InternVL-2-8B from the internVL family of VLMs (Chen et al., 2024). Lastly, LLaVA-NeXT (Liu et al., 2024a) with Mistral-7B (Jiang et al., 2023) as its LLM component. We pretrain each model for 1000 steps with query batch size of 128 and candidate batch size of 1024. Table 4 shows the validation accuracy of ABC with different backbones. We find that our backbone choice, Qwen2-VL7B, produces the best results. We also note each backbones performance on two standard generative VLM benchmarks: MMMU (Yue et al., 2024) and MMBench (Liu et al., 2024b). We find that performance after contrastive training strongly correlates with the performance of the backbone on generative tasks. This indicates that training better VLMs naturally results in better backbones for our embedding model. 7 ABC: Achieving Better Control of Multimodal Embeddings using VLMs 5. Discussion and Future Work 5.1. Decoupling Pretraining and Instruction Fine-tuning The separation of these two stages is an important step for increasing the accessibility of building instruction fine-tuned multimodal embedding models. Our pretraining run barely fits into the 640 GB of VRAM provided by single A100 node, and takes several days to complete. Furthermore, our work indicates that these models could benefit substantially from further scaling (Appendix C). In contrast, the instruction fine-tuning stage completes in less than an hour. This allowed us to quickly iterate on our instruction fine-tuning stage, without pretraining from scratch. We hope that this decoupling encourages more practitioners to experiment with instruction fine-tuning methods for embeddings. 5.2. Important Factors during Pretraining Prior work has largely focused on what architectural adaptations to the make the VLM to convert it into an embedding model. These include the attention mask, how the embedding is pooled and adapter architecture (BehnamGhader et al., 2024; Lee et al., 2024; Jiang et al., 2024b). Throughout our experiments, we find that most of these choices are often interchangable or only produce marginal improvements (Appendix B). However, We find that many of the crucial factors when training CLIP models are also important when adapting VLMs. In particular, well-chosen data, batch size and number of samples seen during training are all important factors (Appendix C), just like with CLIP models (Gadre et al., 2023; Cherti et al., 2023a). 5.3. Better Multimodal Benchmarks Developing benchmarks that measure the capabilities of models to natural language instructions and image representations is an important direction for future work. We identify the following properties as crucial for good quality benchmark. Firstly, the benchmark should require the use of both modalities together, and using only the image or the text should be insufficient to accomplish the task. We note that ensuring this property requires inspecting not only the queries but the candidate pool as well. For example, consider Figure 6, upon initial inspection the query seems well constructed. However, when the retrieval candidates are examined, it is clear that there is only one answer that is plausible given the instruction. Therefore, inspecting the image isnt required to successfully complete the task. We find this common pitfall when adapting open-ended generative benchmarks into multimodal embedding benchmarks. Secondly, the instructions should be diverse. Users phrase natural language tasks in diverse and unpredictable ways, and benchmarks should reflect that (Trippas et al., 2024). It is easy to overfit on instruction phrasing, leading to mis8 Figure 6: An example of poorly constructed task. The candidate pool has no relevant negatives, and therefore the image isnt needed to solve the task. leading evaluations of the models ability to utilize instructions (Gudibande et al., 2023; Wei et al., 2022). Therefore, models should not be tested using instructions that they are trained on. Ensuring these two properties was our key motivation for constructing CtrlBench. However, CtrlBench only represents one multimodal embedding task, ideally more benchmarks satisfying the above two properties would be available to evaluate models. 6. Conclusion We introduce ABC, multimodal embedding model that leverages VLM backbone to control image representations via natural language instructions. It achieves the best zero-shot results on variety of multimodal tasks, spanning retrieval, classification, and VQA. Our multi-stage training process isolates the computationally expensive contrastive pretraining from lightweight instruction finetuning phase, which allows for easy iteration of our model. We explore what factors are the most crucial when adapting VLMs to output multimodal embeddings. In particular, we find that training with well-chosen negatives, vision encoder resolution, and VLM backbone are all important factors for achieving the best performance. Lastly, we design CtrlBench to measure our models ability to use instructions to accomplish subtle natural language guided retrieval tasks. ABC: Achieving Better Control of Multimodal Embeddings using VLMs"
        },
        {
            "title": "References",
            "content": "Apple Inc. iphone 16 tech specs. https://support. apple.com/en-ca/121029. Accessed: 2025-0122. Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., and Katz, B. Objectnet: large-scale bias-controlled dataset for pushing the limits of object recognition models. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper_files/paper/2019/file/ 97af07a14cacba681feacf3012730892-Paper. pdf. BehnamGhader, P., Adlakha, V., Mosbach, M., Bahdanau, D., Chapados, N., and Reddy, S. Llm2vec: Large language models are secretly powerful text encoders, 2024. URL https://arxiv.org/abs/2404.05961. Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts. CoRR, abs/2102.08981, 2021. URL https://arxiv.org/ abs/2102.08981. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., Li, B., Luo, Internvl: Scaling up P., Lu, T., Qiao, Y., and Dai, J. vision foundation models and aligning for generic visuallinguistic tasks, 2024. URL https://arxiv.org/ abs/2312.14238. Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., and Jitsev, J. Reproducible scaling laws for conIn 2023 IEEE/CVF trastive language-image learning. Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, June 2023a. doi: 10.1109/ cvpr52729.2023.00276. URL http://dx.doi.org/ 10.1109/CVPR52729.2023.00276. Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., and Jitsev, J. Reproducible scaling laws for conIn 2023 IEEE/CVF trastive language-image learning. Conference on Computer Vision and Pattern Recognition (CVPR), pp. 28182829. IEEE, June 2023b. doi: 10.1109/cvpr52729.2023.00276. URL http://dx. doi.org/10.1109/CVPR52729.2023.00276. text embedding models with effective hard-negative mining, 2024. URL https://arxiv.org/abs/2407. 15831. Djolonga, J., Yung, J., Tschannen, M., Romijnders, R., Beyer, L., Kolesnikov, A., Puigcerver, J., Minderer, M., DAmour, A., Moldovan, D., Gelly, S., Houlsby, N., Zhai, X., and Lucic, M. On robustness and transferability of convolutional neural networks. CoRR, abs/2007.08558, 2020. URL https://arxiv.org/ abs/2007.08558. Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., and Zisserman, A. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html. Gadre, S. Y., Ilharco, G., Fang, A., Hayase, J., Smyrnis, G., Nguyen, T., Marten, R., Wortsman, M., Ghosh, D., Zhang, J., Orgad, E., Entezari, R., Daras, G., Pratt, S., Ramanujan, V., Bitton, Y., Marathe, K., Mussmann, S., Vencu, R., Cherti, M., Krishna, R., Koh, P. W., Saukh, O., Ratner, A., Song, S., Hajishirzi, H., Farhadi, A., Beaumont, R., Oh, S., Dimakis, A., Jitsev, J., Carmon, Y., Shankar, V., and Schmidt, L. Datacomp: In search of the next generation of multimodal datasets, 2023. URL https://arxiv.org/abs/2304.14108. Gao, L., Zhang, Y., Han, J., and Callan, J. Scaling deep contrastive learning batch size under memory limited setup, 2021. URL https://arxiv.org/abs/ 2101.06983. Gudibande, A., Wallace, E., Snell, C., Geng, X., Liu, H., Abbeel, P., Levine, S., and Song, D. The false promise of imitating proprietary llms, 2023. URL https:// arxiv.org/abs/2305.15717. Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. Vizwiz grand challenge: Answering visual questions from blind people, 2018. URL https://arxiv.org/abs/1802.08218. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: critical analysis of out-of-distribution generalization, 2021. URL https://arxiv.org/ abs/2006.16241. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv. org/abs/2106.09685. de Souza P. Moreira, G., Osmulski, R., Xu, M., Ak, R., Schifferer, B., and Oldridge, E. Nv-retriever: Improving Hudson, D. A. and Manning, C. D. Gqa: new dataset for real-world visual reasoning and compositional question 9 ABC: Achieving Better Control of Multimodal Embeddings using VLMs answering, 2019. URL https://arxiv.org/abs/ 1902.09506. Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up visual and vision-language representation learning with noisy text supervision. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 49044916. PMLR, 18 24 Jul 2021. URL https://proceedings.mlr. press/v139/jia21b.html. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.- A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b, 2023. URL https: //arxiv.org/abs/2310.06825. Jiang, T., Song, M., Zhang, Z., Huang, H., Deng, W., Sun, F., Zhang, Q., Wang, D., and Zhuang, F. E5-v: Universal embeddings with multimodal large language models, 2024a. URL https://arxiv.org/abs/2407.12580. Jiang, Z., Meng, R., Yang, X., Yavuz, S., Zhou, Y., and Chen, W. Vlm2vec: Training vision-language models for massive multimodal embedding tasks, 2024b. URL https://arxiv.org/abs/2410.05160. Kiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A., Fitzpatrick, C. A., Bull, P., Lipstein, G., Nelli, T., Zhu, R., Muennighoff, N., Velioglu, R., Rose, J., Lippe, P., Holla, N., Chandra, S., Rajamanickam, S., Antoniou, G., Shutova, E., Yannakoudakis, H., Sandulescu, V., Ozertem, U., Pantel, P., Specia, L., and Parikh, D. The hateful memes challenge: Competition report. In Escalante, H. J. and Hofmann, K. (eds.), Proceedings of the NeurIPS 2020 Competition and Demonstration Track, volume 133 of Proceedings of Machine Learning Research, pp. 344360. PMLR, 0612 Dec 2021. URL https://proceedings.mlr.press/ v133/kiela21a.html. Klambauer, G., Unterthiner, T., Mayr, A., and Hochreiter, S. Self-normalizing neural networks. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 5d44ee6f2c3f71b73125876103c8f6c4-Paper. pdf. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma, D. A., Bernstein, M. S., and Li, F.-F. Visual genome: Connecting language and vision using crowdsourced dense image annotations, 2016. URL https://arxiv. org/abs/1602.07332. Lee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catanzaro, B., and Ping, W. Nv-embed: Improved techniques for training llms as generalist embedding models, 2024. URL https://arxiv.org/abs/2405.17428. Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023. URL https://arxiv.org/abs/2301.12597. Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., and Dollar, P. Microsoft coco: Common objects in context, 2015. URL https://arxiv.org/abs/1405. 0312. Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning, 2024a. URL https:// arxiv.org/abs/2310.03744. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., Chen, K., and Lin, D. Mmbench: Is your multi-modal model an all-around player?, 2024b. URL https://arxiv.org/abs/ 2307.06281. Loshchilov, I. and Hutter, F. Decoupled weight decay regIn International Conference on Learning ularization. Representations, 2019. URL https://openreview. net/forum?id=Bkg6RiCqY7. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. Lopez-Cifuentes, A., Escudero-Vinolo, M., Bescos, J., and Alvaro Garcıa-Martın. Semantic-aware scene recognition. Pattern Recognition, 102:107256, 2020. ISSN 00313203. doi: https://doi.org/10.1016/j.patcog.2020.107256. https://www.sciencedirect.com/ URL science/article/pii/S0031320320300613. Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Ok-vqa: visual question answering benchmark requiring external knowledge, 2019. URL https://arxiv. org/abs/1906.00067. Masry, A., Long, D. X., Tan, J. Q., Joty, S., and Hoque, E. Chartqa: benchmark for question answering about charts with visual and logical reasoning, 2022. URL https://arxiv.org/abs/2203.10244. 10 ABC: Achieving Better Control of Multimodal Embeddings using VLMs Mathew, M., Bagal, V., Tito, R. P., Karatzas, D., Valveny, Infographicvqa, 2021a. URL E., and Jawahar, C. V. https://arxiv.org/abs/2104.12756. Mathew, M., Karatzas, D., and Jawahar, C. V. Docvqa: dataset for vqa on document images, 2021b. URL https://arxiv.org/abs/2007.00398. Muennighoff, N., Tazi, N., Magne, L., and Reimers, N. Mteb: Massive text embedding benchmark, 2023. URL https://arxiv.org/abs/2210.07316. OpenAI. Gpt-4o system card, 2024. URL https:// arxiv.org/abs/2410.21276. Plummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C., Hockenmaier, J., and Lazebnik, S. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models, 2016. URL https: //arxiv.org/abs/1505.04870. Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, W. X., Dong, D., Wu, H., and Wang, H. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering. In Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and Zhou, Y. (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 58355847, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.466. URL https://aclanthology. org/2021.naacl-main.466/. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. Imagenet large scale visual recognition challenge, 2015. URL https:// arxiv.org/abs/1409.0575. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., and Jitsev, J. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022. URL https://arxiv.org/abs/2210.08402. Schwenk, D., Khandelwal, A., Clark, C., Marino, K., and Mottaghi, R. A-okvqa: benchmark for visual question answering using world knowledge, 2022. URL https: //arxiv.org/abs/2206.01718. Sharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Annual Meeting of the Association for Computational Linguistics, 2018. URL https://api.semanticscholar. org/CorpusID:51876975. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read, 2019. URL https://arxiv. org/abs/1904.08920. Singh, A., Hu, R., Goswami, V., Couairon, G., Galuba, W., Rohrbach, M., and Kiela, D. Flava: foundational language and vision alignment model, 2022. URL https://arxiv.org/abs/2112.04482. Sun, Q., Fang, Y., Wu, L., Wang, X., and Cao, Y. Eva-clip: Improved training techniques for clip at scale, 2023. URL https://arxiv.org/abs/2303.15389. Trippas, J. R., Al Lawati, S. F. D., Mackenzie, J., and Gallagher, L. What do users really ask large language models? an initial log analysis of google bard interIn Proceedings of the 47th Inactions in the wild. ternational ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 24, pp. 27032707, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400704314. doi: 10. 1145/3626772.3657914. URL https://doi.org/ 10.1145/3626772.3657914. Wang, L., Yang, N., Huang, X., Yang, L., Majumder, R., and Wei, F. Improving text embeddings with large language models, 2024a. URL https://arxiv.org/abs/ 2401.00368. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., and Lin, J. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024b. URL https://arxiv.org/abs/2409.12191. Wei, C., Chen, Y., Chen, H., Hu, H., Zhang, G., Fu, J., Ritter, A., and Chen, W. Uniir: Training and benchmarking universal multimodal information retrievers, 2023. URL https://arxiv.org/abs/2311.17136. Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners, 2022. URL https:// arxiv.org/abs/2109.01652. 11 ABC: Achieving Better Control of Multimodal Embeddings using VLMs Wijmans, E., Huval, B., Hertzberg, A., Koltun, V., and Krahenbuhl, P. Cut your losses in large-vocabulary language models, 2024. URL https://arxiv.org/ abs/2411.09009. Xiao, J., Hays, J., Ehinger, K. A., Oliva, A., and Torralba, A. Sun database: Large-scale scene recognition from abbey In 2010 IEEE Computer Society Conference to zoo. on Computer Vision and Pattern Recognition, pp. 3485 3492, 2010. doi: 10.1109/CVPR.2010.5539970. Yao, L., Huang, R., Hou, L., Lu, G., Niu, M., Xu, H., Liang, X., Li, Z., Jiang, X., and Xu, C. Filip: Finegrained interactive language-image pre-training, 2021. URL https://arxiv.org/abs/2111.07783. Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y. Coca: Contrastive captioners are imagetext foundation models, 2022. URL https://arxiv. org/abs/2205.01917. Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., Wei, C., Yu, B., Yuan, R., Sun, R., Yin, M., Zheng, B., Yang, Z., Liu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2024. URL https://arxiv.org/abs/2311.16502. Zhang, K., Luan, Y., Hu, H., Lee, K., Qiao, S., Chen, W., Su, Y., and Chang, M.-W. Magiclens: Self-supervised image retrieval with open-ended instructions. In The Forty-first International Conference on Machine Learning (ICML), pp. to appear, 2024. Zhou, J., Liu, Z., Liu, Z., Xiao, S., Wang, Y., Zhao, B., Zhang, C. J., Lian, D., and Xiong, Y. Megapairs: Massive data synthesis for universal multimodal retrieval, 2024. URL https://arxiv.org/abs/2412.14475. Zhu, Y., Groth, O., Bernstein, M., and Fei-Fei, L. Visual7w: Grounded question answering in images, 2016. URL https://arxiv.org/abs/1511.03416. 12 ABC: Achieving Better Control of Multimodal Embeddings using VLMs A. Instruction Prompt Design We use the following prompt template when using GPT-4o to generate instructions: \"<Image> Given this image, provide user prompt where the following caption would be reasonable answer: Only return the prompt.\" {Caption}. Where {Caption} is the caption for the bounding box from Visual Genome. We use temperature of 0 (deterministic) when generating instructions. We choose this template to collect diverse set of instructions that user would plausibly ask. B. Architecture Ablation LLM2Vec (BehnamGhader et al., 2024) introduced several architecture modifications for adapting LLMs into text embedding models. In the multimodal setting, we find that using casual vs. bidirectional attention is marginal for improving model accuracy (Figure 7). Figure 7: (Left) Causal vs Bidirectional attention. (Right) LoRA adapter rank ablation. On average, bidirectional attention slightly outperforms casual attention. Ablating over adapter rank, we find that different rank adapters tend to converge to the same accuracy, with higher rank adapters performing fraction of percentage better. Overall, better data and scaling (Appendix C) present much more promising direction for improving embedding quality. C. Scaling Training We find that batch size and number of samples seen during training are both important factors. By increasing our batch size by 4x, from 128 queries and 1024 candidates to our full pretraining run size (512 queries and 4096 candidates), our validation accuracy on an 800 sample validation batch using in-batch negatives increases from 92.5% to 95.0%. We control for total samples seen by training the smaller batch size model for 4x more steps. Interestingly, this effect is even more pronounced on OOD validation data. For example, on similar batch of MSCOCO data, accuracy increases from 73.8% to 84.0%. Therefore, techniques for scaling batch size under limited VRAM, such as GradCache (Gao et al., 2021), are promising direction to improve our pretrained model. Furthermore, we find that more steps (more samples seen) is also straightforward way to increase the pretrained models performance. As shown by Appendix C, doubling step count steadily decreases loss during our pretraining run. 13 ABC: Achieving Better Control of Multimodal Embeddings using VLMs D. Comparison to VLM2Vec With the MMEB benchmark, Jiang et al. (2024b) also include mutltimodal embedding model: VLM2Vec. We exclude VLM2Vec from Tables 1 and 2 as it has been trained on MSCOCO and several tasks from the MMEB benchmark. We provide the comparison of the two models here while specifying which tasks are out of distribution (OOD) for VLM2Vec. All tasks are OOD for our model. VLM2Vec ABC (ours) ImageNet-1K (Russakovsky et al., 2015) HatefulMemes (Kiela et al., 2021) VOC2007 (Everingham et al.) SUN397 (Xiao et al., 2010) Place365 (Lopez-Cifuentes et al., 2020) ImageNet-A (Djolonga et al., 2020) ImageNet-R (Hendrycks et al., 2021) ObjectNet (Barbu et al., 2019) Country-211 (Radford et al., 2021) Average Average OOD 65.6 67.1 88.6 72.7 42.6 19.3 70.2 29.5 13.0 52.1 34. 71.2 52.1 81.4 71.8 40.7 49.4 86.8 67.7 18.5 60.0 52.6 Table 5: Classification results for VLM2Vec and ABC (ours). Gray background indicates that VLM2Vec has been trained on this task. Table 5 compares the performance of our model to VLM2Vec (Jiang et al., 2024b). Gray rows indicate tasks that VLM2Vec is trained on. On average, VLM2Vec performs better on tasks that are in its distribution, while our model outperforms on tasks where both models are OOD. Interestingly, our model performs better on ImageNet-1K even though it is not trained on it. 14 ABC: Achieving Better Control of Multimodal Embeddings using VLMs E. CtrlBench Q: What is depicted in the image? A: This is signage Q: What can be seen growing near the base of the signpost? A: long limp green blades of plant Q: What does the bottom sign say? A: sign that says plums Q: What is written on the top sign in this image? A: The word Apples written in white on black background Q: What does the middle sign in the image say? A: dark grey sign that says Pears. Q: What is featured prominently in the foreground of the image, facing the ocean? A: lone wood bench Q: What do you notice about the water in this coastal scene? A: pretty blue ocean water Q: What is the structure visible on the horizon in this coastal landscape? A: distant, stone lighthouse Q: What landmark is visible in the background by the ocean? A: old European lighthouse Q: What do you notice about the weather in this seaside photo? A: blue sky in the distance ABC: Achieving Better Control of Multimodal Embeddings using VLMs Q: What is happening in the top left corner of the image? A: Tennis ball in the air Q: What is the player about to hit with his racket? A: round tennis ball Q: What color are the shorts the tennis player is wearing? A: black and yellow shorts Q: What action is taking place on the tennis court in the image? A: man serving tennis ball. Q: Whats prominently dividing the ground area in the image? A: White line on court Q: What part of the animal is prominently displayed in the image? A: the neck of polar bear Q: What colors can you see on the bear in the image? A: black, brown and white bear neck Q: What stands out to you about the bears appearance in this image? A: the bear has such tiny ears for such huge animal Q: What is the bear doing with its eyes in this picture? A: the bear has his eyes closed Q: Whats the polar bear swimming in? A: body of water"
        }
    ],
    "affiliations": [
        "Cheriton School of Computer Science, University of Waterloo",
        "Vector Institute, Toronto"
    ]
}