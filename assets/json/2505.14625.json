{
    "paper_title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning",
    "authors": [
        "Zhangchen Xu",
        "Yuetai Li",
        "Fengqing Jiang",
        "Bhaskar Ramasubramanian",
        "Luyao Niu",
        "Bill Yuchen Lin",
        "Radha Poovendran"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RL's success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problem--false negatives--where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at https://github.com/uw-nsl/TinyV."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 2 5 2 6 4 1 . 5 0 5 2 : r TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning Zhangchen Xu* Yuetai Li * Fengqing Jiang Bhaskar Ramasubramanian Luyao Niu Bill Yuchen Lin Radha Poovendran University of Washington Western Washington University"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning (RL) has become powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RLs success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze widespread problemfalse negativeswhere verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose TINYV, lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple mathreasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer practical approach to improve RL-based fine-tuning of LLMs. Our code is available at https://github.com/uw-nsl/TinyV."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement Learning (RL) has become cornerstone for advancing the reasoning capabilities of large language models (LLMs) [5], as evidenced by state-of-the-art models like OpenAI o1 [19] and DeepSeek-R1 [8]. The effectiveness of RL depends on verifiable rewards, which provide essential supervision signals for policy optimization [21]. In reasoning tasks, prior work has predominantly relied on rule-based verifiers [43, 26, 39], which assign binary reward by comparing the models generated answer with the ground truth, yielding reward of 1 if they are equivalent and 0 otherwise. Despite the widespread use of verifiers to assess model outputs [4, 6, 11], their reliability Figure 1: This figure illustrates false negative case in the CN_K12 dataset, where the ground truth and the response generated by LLM (DEEPSEEK-R1DISTILL-QWEN-7B) are mathematically equivalent, yet Prime Verifier and Math Verify incorrectly marks the response as wrong. 0*These authors contributed equally to this work. Equal advising. Preprint. Under review. in the context of RL training and its impact on performance remain underexplored. In this paper, we investigate the prevalence of false negatives (FNs) in answer verification, where conventional approaches (e.g., rule-based verifiers relying on string matching [41] or advanced parsing [7, 18]) fail to recognize correct answers, leading to incorrect reward assignment. Figure 1 illustrates case where the rule-based verifiers Prime Verifier [7] and Math Verify [18] fails to verify an equivalent answer due to their rule-based matching criteria. To quantify these issues, our analysis of the BigMath-RL-Verified dataset [1] revealed that among responses marked as incorrect by Prime Verifier, 38.5% were actually correct, indicating high prevalence of FNs. Our further analysis identified natural language elements in either the models response or the ground truth answer as the primary cause of these false negatives, underscoring critical limitation of rule-based verifiers. The reliance on rule-based verifiers with high FNs in RL for reasoning tasks poses significant challenges for advancing research and model development. First, problems that are harder to verify using rule-based approaches, such as those involving natural language elements or complex latex expressions, are often excluded from training and evaluation, thereby limiting the models reasoning capabilities and hindering understanding of such challenging reasoning problems. Second, the high prevalence of FNs caused by rule-based verifiers reduces training efficiency by introducing incorrect reward signals, which can mislead policy optimization and slow convergence, ultimately impeding progress in developing more robust and generalizable reasoning models. In this paper, we examined the impact of false negatives on RL training both empirically and theoretically. Empirically, we find that FNs, arising from incorrect reward signals, significantly impair training efficiency by reducing the availability of informative gradient signals, particularly during early training stages. Furthermore, our theoretical analysis demonstrates that FNs hinder learnability, as measured by the reverse Kullback-Leibler (KL) divergence between policies at consecutive optimization steps, thereby slowing convergence. To address the issue of FNs in RL, we propose TINYV, lightweight LLM-based verifier designed to enhance reward accuracy while maintaining computational efficiency. By augmenting rule-based verifiers like Prime Verifier, TINYV corrects FNs, enabling more effective RL training for mathematical reasoning tasks. To evaluate its performance and bridge the gap in existing benchmarks, we develop the HardVerify-Math Bench, which focuses on challenging verification scenarios. Our experimental results demonstrate that TINYV achieves up to 10% improvement in pass rates across HardVerify-Math, with notable increases in performance on other benchmarks such as MATH and Interestingly, we Olympiad Bench, and accelerates convergence compared to baseline verifiers. also found that training on questions with easily verifiable answers leads to poor performance on hard-to-verify questions, opening avenues for future research on developing more accurate reward assignment and diverse training datasets to address these challenges. The paper is organized as follows. Section 2 provides the preliminaries for our study. Section 3 analyzes FNs in the wild, focusing on their prevalence and causes. Section 4 examines the impact of FNs on RL training from both empirical and theoretical perspective. Section 5 details the curation and performance analysis of TINYV. detailed literature review is deferred to Appendix A. Limitations and ethical considerations are shown in Appendix B."
        },
        {
            "title": "2 Preliminaries",
            "content": "Reinforcement Learning in Language Models. RL in the context of language models involves optimizing training policy, denoted as πθ, which is initialized from reference policy, πinit. The goal of this optimization is to maximize the rewards obtained from reward function, r. This process seeks to find the optimal parameters θ by maximizing the expected reward, while also considering the KL divergence between the training policy and the initial policy. The objective function can be expressed as: max θ Eyπθ(x)[r(x, y) βDKL(πθ(yx)πinit(yx))] (1) Here, is the input, the output, the reward, and β is hyperparameter that balances reward maximization with policy deviation, as measured by the KL divergence DKL. Group Relative Policy Optimization (GRPO). Group Relative Policy Optimization (GRPO) [33] bypasses parameterized value models used in traditional methods like Proximal Policy Optimization (PPO). GRPO distinctively calculates policy gradients by weighting trajectory log-likelihoods according to group-based advantages, eliminating the need for critic model. In practice, for given prompt x, GRPO involves sampling responses (rollouts) {y1, y2, , yn}. The reward, ri, associated with each of these yi is then used to compute the advantage, Ai, for each response yi. This advantage is calculated as: Ai = ri mean(r1, . . . , rn) (cid:112)var(r1, . . . , rn) + ε , (2) where mean() and var() represent the average and variance of the rewards for the responses, respectively. ε > 0 is small smoothing constant that ensures the denominator is non-zero. Verification and Reward Calculation in RL. We denote x, yi, yref L, where is the vocabulary space and is the text length, and yref is the ground truth answer to the question x. verifier is needed to calculate the reward ri associated with each generated response yi for given question x. Following [4], we model the verifier as an equivalence comparison function: ψ : L {0, 1}, ψ(cid:0)x, yi, yref (cid:1) = (cid:26)1, if yi is equivalent to yref given x, 0, otherwise. (3) This function determines if the models generated response yi is equivalent to the ground truth answer yref . The input prompt is optional in this function. The verifier returns 1 if the responses are deemed equivalent and 0 otherwise, providing binary reward signal for training. The reward ri is then defined as ri = ψ(cid:0)x, yi, yref (cid:1). We note that in practice, we only extract answers within structured format, e.g., boxed{}, which simplifies verification process."
        },
        {
            "title": "3 Discovering and Analyzing False Negatives from the Wild",
            "content": "In this section, we analyze false negatives in real-world datasets. Specifically, we aim to quantify the prevalence of FNs in answer verification when applying rule-based verifiers. Dataset Curation. We leverage the Big-Math-RL-Verified dataset (Apache license 2.0) [1], which comprises over 250,000 diverse, high-quality mathematical problems paired with ground-truth solutions from different sources. Notably, this dataset includes pass rates p(x) for each prompt derived from generating 64 responses using LLAMA-3.1-8B, providing an indicator of problem difficulty. To explore false negatives in open-domain settings, we generate = 4 responses per problem using DEEPSEEK-R1-DISTILL-QWEN-7B [13], with temperature of = 1, top-p sampling of = 1, and context length of 32, 768 tokens. By default, we adopt Prime Verifier [7], widely used tool in RL frameworks (e.g., VERL [34]), as the baseline verifier. For our analysis, we retain only the seemingly incorrect prompt-response pairs that pass the format check (i.e., has boxed{} in the response) but measured as incorrect by Prime Verifier: = (cid:8)(x, yi) : ψprime(x, yi, yref) = 0, , {1, . . . , n}(cid:9), (4) where is the set of all mathematical problems in the dataset. False Negative Annotation. Although Prime Verifier accounts for equivalence beyond strict string matching (e.g., LaTeX expression equivalence), it may still misclassify correct answers as incorrect, resulting in false negatives. To systematically investigate these FNs, we employ LLMs to re-evaluate the incorrect responses marked by Prime Verifier. To mitigate selection bias and ensure robustness, we select two different LLM annotators: QWEN2.5-72B-INSTRUCT (LLM1) and GROK-3-MINIHIGH (LLM2), evaluated in non-thinking and thinking modes, respectively. The full prompt can be found in Appendix H.1. We constitute the false-negative set by retaining only those promptresponse pairs from where both LLMs agree the response is correct: FN = (cid:8)(x, yi) : ψLLM1(x, yi, yref) = 1 ψLLM2(x, yi, yref) = 1(cid:9), (5) Effectiveness of LLM Annotation. To validate the reliability of our annotation process, we perform manual review by randomly selecting 200 responses from FN . We observe an accuracy of 99.5%, with only one response incorrectly marked as true due to missing component in its solution. Additionally, the two LLM verifiers identify three questions with incorrect ground truth answers in the dataset. This indicates that our design can effectively detect false negatives. 3 Figure 2: This figure demonstrates false negatives in Big-Math-RL-Verified by source (upper) and category (lower). Key Takeaways. Upon analyzing the false-negative set FN , we have the following key takeaways. Takeaway 1: High Proportion of False Negatives from the Wild. Our experiments reveal that, among the 226K prompt-response pairs within seemingly incorrect prompt-response pairs (W), Prime Verifier mislabels 87K (38.5%) correct responses as incorrect. Additionally, among the 95K unique prompts in W, it fails to identify correct responses for 40K (42.1%) prompts. Figure 2 (upper) shows the false negative ratios across datasets sources, with CN_K12 exhibiting the highest rate (> 50%). Takeaway 2: [Taxonomy of False-Negative Types] Language differences, formatting inconsistencies, and notation discrepancies are the most prevalent sources of false negatives. To understand why these false negatives occur, we conduct detailed analysis on FN and developed comprehensive taxonomy consisting of seven broad error classes (with 31 finer subcategories), spanning issues from formatting and notation to semantic misunderstandings. We then employ GROK-3-MINI-HIGH to automatically label each prompt exhibiting at least one false negative. The results are demonstrated in Figure 2 (lower). The complete category definitions and annotation prompt are provided in Appendices and H.2, respectively. Our analysis reveals that language differences constitute the predominant source of false negatives, particularly in cases where either the ground-truth answer or the model-generated response incorporates natural language elements. The second and third most common error sources are formatting issues (e.g., missing whitespace or delimiter style) and notation discrepancies (e.g., intervals versus inequalities), respectively. The remarkable diversity of these error types underscores the significant challenge faced by rule-based verifiers in attempting to capture all possible variations."
        },
        {
            "title": "4 Analysis of False Negatives and Their Impact on RL Training",
            "content": "4.1 Empirical Analysis of FNs during RL Having examined the distribution of false negatives across datasets in the previous section, we now investigate how these verification errors influence the RL training process. 4 Figure 4: This figure demonstrates the impact of FNs on training efficiency by comparing Prime Verifier and LLM annotations. LLM annotations consistently achieve higher prompt efficiency by reducing the all-wrong ratio, particularly in the early stages of training. RL Training Setups. We follow [44] and perform zero RL training on two base models, QWEN2.5-7B and QWEN2.5-MATH-7B, respectively. We follow [40, 29] by randomly selecting 5K challenging questions from Big-Math-RL-Verified that satisfy specific difficulty criteria: pass rate p(x) 0.2 for LLAMA-3.1-8B and p(x) = 0.25 for the Deepseek-Distilled models from our curated dataset in Section 3. We perform GRPO [33] for 12 epochs with batch size of 128 and 8 rollouts per sample (i.e., = 8). During training, we employ the default Prime Verifier to assign binary rewards based on its verification results. We do not assign additional format rewards during the RL training. Full hyperparameter configurations are detailed in Appendix E.1. Methodology. To systematically investigate false negatives during RL fine-tuning, we adopt the LLM-based false negative annotation outlined in Section 3 and perform an offline evaluation of each rollout generated by the GRPO algorithm. We then compare LLM judgments against the rewards assigned by Prime Verifier. To evaluate how FNs affect GRPO training at each step, we adopt the approach from DAPO [41] and define Prompt Efficiency ηk for mini-batch of prompts at training step as: ηk = Pk(0 < p(x) < 1) = 1 Pk(p(x) = 0) Pk(p(x) = 1), (cid:80)n where p(x) = 1 i=1 ri is the pass rate for prompt with rollouts, ri {0, 1} is the bin nary reward for the i-th rollout, and Pk is the empirical probability over the mini-batch, defined as Pk(p(x) = 0) = {x : p(x) = 0}/m and Pk(p(x) = 1) = {x : p(x) = 1}/m. (6) Intuitively, prompts for which all rollouts are either correct or incorrect provide no useful gradient signal for RL, whereas partially correct batches are more informative for policy updates. At each training step, we compute Prompt Efficiency using Prime Verifiers reward values and compare these results with the correctness labels derived from our LLM annotations. This comparison enables us to quantify the impact of false negatives on RL training efficiency and overall model performance. Takeaway 3: High Proportion of False Negative during RL Training. Figure 3 shows the fraction of unique prompts in the training dataset that experience at least one false-negative rollout across training epochs. The fraction of FN prompts increases steadily after the first epoch, reaching 46.7% for QWEN2.5-7B and 50.5% for QWEN2.5-MATH-7B by the end of training. This trend indicates that false negatives accumulate over time, likely due to the model exploring diverse answer formats that Prime Verifier fails to recognize as correct. Moreover, Figure 4 illustrates that the false-negative ratio remains high at every training step, reaching 20% of rollouts on average. Figure 3: The fraction of unique prompts in the training dataset that encounter at least one false-negative rollout across steps. The x-axis represents the training step, and the yaxis shows the cumulative fraction of prompts affected by false negatives. 5 Takeaway 4: False Negatives reduce prompt efficiency in early RL training. Figure 4 illustrates the all-wrong ratio (Pk(p(x) = 0)), all-correct ratio (Pk(p(x) = 1)), and prompt efficiency ηk during RL training. We observe that false negatives significantly reduce prompt efficiency ηk, particularly in the early stages of training. For instance, while Prime Verifier marks 50% of prompts as having no correct rollouts, LLM annotations reveal that only 35% lack correct rollouts, indicating 15% gap. As the all-correct ratio increases with LLM annotations, prompt efficiency based on LLM annotation consistently surpasses that of Prime Verifier, driven by substantial reduction in the all-wrong ratio. We highlight that prompts with low pass rates are more critical for RL training, as they provide informative gradient signals for learning challenging problems [40, 29]. Although the gap in prompt efficiency between Prime Verifier and LLM annotations narrows in later training stages, Prime Verifiers high false-negative rate in early stages hinders effective learning on challenging prompts. 4.2 Theoretical Analysis of Efficiency Degradation Due to False Negatives In this section, we theoretically analyze the efficiency degradation in GRPO [33] caused by false negatives in reward signals. We compare the learnability (defined later) of policies trained with ground truth rewards against those trained with rewards affected by false negatives. Let πGT (yix) denote the policy optimized at the k-th step using ground truth rewards, and let πFN (yix) represent the policy optimized using rewards with false negatives. The success probabilities under these policies for given prompt are defined as: = GT = FN yπGT yπFN (x)1{rGT(y,yref)=1}, (x)1{rFN(y,yref)=1}, (7) (8) where 1{} is the indicator function, rGT(y, yref) is the ground truth reward function, and rFN(y, yref) is the reward function affected by false negatives. Given the definition of false negatives, where correct response may be incorrectly marked as incorrect, we have the following lemma. > FN Lemma 1. GT for all k. Our theoretical framework relies on the following two assumptions: Assumption 1. GT increases with k. This assumption posits that the GRPO is fundamentally sound, ensuring that the success probability (i.e., average reward scores) improves over iterations when trained with ground truth rewards. Assumption 2. GT < 2P GT k1 for all k. This assumes that the average reward scores will not grow exponentially during training, which is consistent with the practical improvement of reward scores in reinforcement learning policy updates. Following [2], we define step-wise learnability as the reverse KL divergence between policies at consecutive optimization steps, denoted by Dk. For policy trained with ground truth rewards and rewards containing false negatives, the step-wise learnability is: Dk,GT = DKL Dk,FN = DKL (cid:0)πGT (cid:0)πFN k1(yx) πGT k1(yx) πFN (yx)(cid:1), (yx)(cid:1). (9) (10) These metrics quantify improvement in policy distribution between consecutive steps. Specifically, the reverse KL divergence measures the distance between the previous policy πk1 and the updated policy πk, where larger Dk indicates greater policy improvement and thus better learnability. Our main theoretical result is encapsulated in the following theorem: Theorem 1. Let δk = Dk,GT Dk,FN denote the step-wise learnability gap at training step k. Under Lemma 1 and Assumption 1, δk > 0 for all k. The proof is provided in Appendix D. This theorem shows that policies trained with ground truth rewards have greater step-wise learnability than those with false negatives, highlighting the importance of accurate reward signals in RL, as false negatives impede convergence."
        },
        {
            "title": "Improve RL by Detecting False Negatives with TINYV",
            "content": "Our experimental and theoretical analysis demonstrate that false negatives are pervasive issue in RL training, severely impacting training efficiency. While LLM-based annotators like QWEN2.572B-INSTRUCT and GROK-3-MINI-HIGH can effectively identify false negatives, this approach is computationally expensive, economically infeasible, and introduces delays due to the high resource demands of large-scale LLMs. To address these limitations, we propose TINYV, lightweight LLMbased verifier that augments existing rule-based methods like Prime Verifier, which dynamically identifies potential FNs and recovers valid responses, enabling more accurate reward estimates while maintaining computational efficiency. 5.1 Curation of TINYV In this subsection, we outline the process for creating TINYV , focusing on dataset curation, model training, and deployment setup. Dataset Curation. To develop reliable verifier capable of handling diverse scenarios, we curate hybrid dataset comprising both real and synthetic examples of false negatives and true negatives. The real false negative and true negative data are sourced from Section 3, where the correctness of the responses were annotated by LLMs. To ensure broader coverage and robustness, we augment this dataset with synthetically generated false negatives. Specifically, we prompt QWEN2.5-72B-INSTRUCT to generate potential false negative cases for given question by introducing variations such as LaTeX formatting differences, numerical approximations, or alternative mathematical expressions that preserve semantic equivalence. These generated candidates are then re-annotated by LLMs to confirm they are false negative. The detailed data curation process, including the prompts used, is provided in Appendix E.2. In total, we collect 638,000 instances, each consisting of prompt, ground truth, model answer, and LLM-annotated correctness label. This hybrid approach ensures that TINYV can generalize across wide range of false negative patterns. Figure 5: This figure demonstrates the curation and deployment of TINYV. Model Training. We perform supervised fine-tuning on Qwen2.5-1.5B-Instruct, compact model selected to balance performance and computational efficiency. The training employs binary classification setup, where the model predicts label of True for response that is correct (i.e., false negative when flagged as incorrect by Prime Verifier) and False otherwise. The inputs are models answer, the ground truth, and the problem context. To ensure balanced dataset and mitigate bias, we sample 159,000 instances, equally distributed between True and False labels. The training template, hyperparameters, and configurations are detailed in Appendix E.3. Additionally, we experiment with training TINYV-THINK, variant that performs intermediate analysis before predicting the final label. However, this approach introduces significant delays due to longer generation time, making it less practical for RL. Consequently, we adopt TINYV for our main experiments. detailed comparison between TINYV and TINYV-THINK is provided in Appendix G.1. TINYV Deployment. To maximize efficiency and align with Theorem 1, we integrate TINYV in an add-on mode alongside Prime Verifier, as shown in Figure 5. In this configuration, TINYV is queried only when Prime Verifier returns negative result (i.e., flags response as incorrect). TINYV then re-evaluates the response to determine if it is false negative, thus avoiding unnecessary computations for responses already deemed correct. This hierarchical setup ensures that TINYV complements Prime Verifier by focusing computational resources on challenging cases, thereby enhancing the accuracy of reward signals in RL training while minimizing overhead. 5.2 HardVerify-Math Benchmark While existing mathematical benchmarks have advanced the evaluation of LLMs in reasoning tasks, they often consist of questions with easily verifiable answers, such as simple numerical solutions. This limitation highlights the need for new benchmark that focuses on challenging verification 7 Figure 6: Performance trends of Qwen2.5-7B on the AMC, MATH and Olympiad benchmark, comparing TINYV with Prime Verifier. The darker lines are smoothed using sliding window whose size is 5% of the total training steps. We observe that model trained with TINYV converges faster and has better final model performance. scenarios prone to false negatives. To address this, we curate the HardVerify-Math Bench, benchmark comprising 250 hard-to-verify answers spanning all categories and the taxonomy discussed in Section 3. Specifically, we manually select 115 questions from Olympiad benchmark and 10 questions from the MATH test sets that are prone to false negative cases due to their complexity in answer format. Additionally, we include 125 questions from the Big-Math dataset, chosen based on Llama-3.1-8B pass rate of less than 0.05 and identified as challenging to verify by human experts. detailed introduction to this benchmark including its distribution and examples is in Appendix F. 5.3 Experimental Setups Models and Datasets. We use Qwen2.5-7B and Qwen2.5-Math-7B and perform zero-RL training using GRPO [33]. For training, we sample 5,000 questions from the Big-Math dataset that exhibit false negative cases, with pass rates satisfying 0.05 < p(x) 0.2 for LLAMA-3.1-8B and p(x) 0.25 for DeepSeek-Distilled models. These criteria ensure sufficient challenge while avoiding overlap with our HardVerify-Math benchmark. We employ TINYV and Prime Verifier to assign rewards. For comparative analysis, we randomly sample 5,000 questions from DeepScaleR [26], which contains questions with easily verifiable answers (e.g., plain numerical values or simple formats evaluable using the SYMPY library), and use Prime Verifier for evaluation due to its simplicity in answer verification. Benchmarks and Evaluation Setups. We assess performance of trained models on MATH500 [16], AMC (2023 and 2024), Olympiad Bench [14], and HardVerify-Math. All experiments employ greedy decoding to ensure deterministic and reproducible results. For MATH500, AMC, and the Olympiad Bench, we adopt the standard practice of using Prime Verify for answer verification. For the more challenging HardVerify-Math, we instead employ LLM-based evaluations to assess performance. More experimental Setups can be found in Appendix E.1. 5.4 Experimental Results In this subsection, we present summary of our experimental results, highlighting the improvements achieved by TINYV in RL training efficiency and model performance across various benchmarks. Takeaway 5: TINYV enhances RL training efficiency and final model performance. As shown in Figure 6 and Table 1, TINYV significantly enhances the efficiency of RL training compared to Prime Verifier, achieving faster convergence. Furthermore, the final model performance of 8 Table 1: Final performance comparison of Qwen2.5-7B and Qwen2.5-Math-7B across different experiment setups on mathematical reasoning benchmarks. Values represent accuracy percentages, with the best performance for each base model and dataset highlighted in bold. Base Model Experiment Setup HardVerify-Math MATH AMC Olympiad Average Qwen2.5-7B Qwen2.5-Math-7B TINYV Prime Verifier DeepScalaR TINYV Prime Verifier DeepScalaR 68.68% 58.64% 53.01% 69.08% 62.65% 55.82% 73.40% 43.37% 32.40% 72.40% 44.58% 31.65% 72.60% 38.55% 54.46% 51.82% 32.54% 49.18% 80.80% 53.01% 79.80% 48.19% 78.00% 56.63% 36.11% 59.97% 37.00% 38.04% 57.17% 56.64% TINYV consistently outperforms that of Prime Verifier across almost all training steps, with performance gap of up to 10% in some benchmarks. We attribute this improvement to TINYV ability to provide more accurate reward signals, enabling the model to learn effectively from challenging questions where Prime Verifier often fails to detect correct responses. Takeaway 6: TINYV improves performance on HardVerify-Math compared to baselines. As shown in Figure 7, TINYV trained on the Big-Math dataset outperforms the baseline using DeepScaleR on the HardVerify-Math benchthe performance of DeepScaleR mark. Notably, on HardVerify-Math fluctuates, likely due to its focus on easily verifiable questions that do not generalize well to hard-to-verify scenarios. In contrast, both TINYV and Prime Verifier with Big-Math show consistent improvement, with TINYV achieving final accuracy of 68.68% compared to Prime Verifiers 58.64% with Qwen2.5-7B as the base model. We attribute this to DeepScaleRs limitation in training on questions with simple, clean answers, which leaves the model underprepared for the complex, false negative-prone questions in HardVerifyMath. Interestingly, this performance advantage of TINYV extends to other benchmarks like MATH500 and Olympiad Bench, where some solutions are similarly challenging to verify due to their complexity (e.g., symbolic expressions or sets). This suggests gap in current training datasets that fail to address hard-to-verify scenarios, opening avenues for future research into developing more diverse datasets and adaptive verification methods that can better handle such challenges. Additional Experimental Results. We compare performance of different verifiers, including TINYV, TINYV-THINK, Math Verify, and Prime Verifier in Appendix G.1. We also compare training costs with and without TINYV in Appendix G.2. Our analysis demonstrates that TINYV incurs only 6% overhead, confirming its lightweight design. Figure 7: This figure compares performance of HardVerify-Math Bench between Big-Math (hard to verify) and DeepScaleR (easy to verify) datasets."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "This work investigates false negatives (FNs) in RL training, specifically addressing three key research questions to understand their prevalence, impact, and mitigation in the context of mathematical reasoning tasks. We demonstrated that the proposed TINYV enhances reward accuracy while maintaining computational efficiency, achieving both improved final performance and faster convergence compared to baseline verifiers. 9 Future work could explore false negatives in broader RL domains, such as theorem proving [36], medical applications [20], software engineering development [35], and robotics [3], to further enhance the robustness and generalizability of RL training across diverse reasoning and decisionmaking tasks."
        },
        {
            "title": "Acknowledgment",
            "content": "This work is partially supported by the Air Force Office of Scientific Research (AFOSR) under grant FA9550-23-1-0208, the Office of Naval Research (ONR) under grant N0014-23-1-2386, and the National Science Foundation (NSF) AI Institute for Agent-based Cyber Threat Intelligence and Operation (ACTION) under grant IIS 2229876. This work is supported in part by funds provided by the National Science Foundation, Department of Homeland Security, and IBM. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the NSF or its federal agency and industry partners."
        },
        {
            "title": "References",
            "content": "[1] Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, and Nick Haber. Big-math: large-scale, high-quality math dataset for reinforcement learning in language models, 2025. [2] Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, and Donghyun Kwak. Online difficulty filtering for reasoning oriented reinforcement learning. arXiv preprint arXiv:2504.03380, 2025. [3] Liam Boyle, Nicolas Baumann, Paviththiren Sivasothilingam, Michele Magno, and Luca Benini. Robotxr1: Enabling embodied robotic intelligence on large language models through closed-loop reinforcement learning, 2025. [4] Ding Chen, Qingchen Yu, Pengyuan Wang, Wentao Zhang, Bo Tang, Feiyu Xiong, Xinchi Li, Minchuan Yang, and Zhiyu Li. xverify: Efficient answer verifier for reasoning model evaluations, 2025. [5] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chainof-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. [6] OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. [7] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [8] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, 10 T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [9] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment, 2023. [10] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. [11] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. [12] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, and Jian Guo. survey on llm-as-a-judge, 2025. [13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [14] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [15] Chaoqun He, Renjie Luo, Shengding Hu, Yuanqian Zhao, Jie Zhou, Hanghao Wu, Jiajie Zhang, Xu Han, Zhiyuan Liu, and Maosong Sun. Ultraeval: lightweight platform for flexible and comprehensive evaluation for llms, 2024. [16] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [17] Shengyi Costa Huang and Arash Ahmadian. Putting rl back in rlhf. https://huggingface. co/blog/putting_rl_back_in_rlhf_with_rloo, June 12 2024. Hugging Face Blog. [18] Hugging Face. Math-Verify: robust mathematical expression evaluation system. https: //github.com/huggingface/Math-Verify, 2025. Accessed: 2025-05-15. [19] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [20] Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, and Xiaofeng Yang. Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models, 2025. 11 [21] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T\" ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. [22] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, and Huan Liu. From generation to judgment: Opportunities and challenges of llm-as-a-judge, 2025. [23] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. [24] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instructionfollowing models. https://github.com/tatsu-lab/alpaca_eval, 5 2023. [25] Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking llms with challenging tasks from real users in the wild, 2024. [26] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, et al. Deepscaler: Surpassing o1-preview with 1.5 model by scaling rl. Notion Blog, 2025. [27] Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. Generalreasoner: Advancing llm reasoning across all domains, 2025. [28] Youssef Mroueh. Reinforcement learning with verifiable rewards: Grpos effective loss, dynamics, and success amplification, 2025. [29] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. [30] OpenAI. OpenAI Evals: framework for evaluating llms. https://github.com/openai/ evals, 2025. Accessed: 2025-05-15. [31] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. [32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [33] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [34] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, page 12791297. ACM, March 2025. [35] Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. [36] Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. Deepseek-prover: Advancing theorem proving in llms through largescale synthetic data. arXiv preprint arXiv:2405.14333, 2024. 12 [37] Wei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, Doyen Sahoo, Junnan Li, Nan Jiang, Tong Zhang, Caiming Xiong, and Hanze Dong. minimalist approach to llm reasoning: from rejection sampling to reinforce, 2025. [38] Yixuan Even Xu, Yash Savani, Fei Fang, and Zico Kolter. Not all rollouts are useful: Downsampling rollouts in llm reinforcement learning. arXiv preprint arXiv:2504.13818, 2025. [39] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. [40] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning, 2025. [41] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. [42] Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. [43] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. [44] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. [45] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics."
        },
        {
            "title": "A Related Work",
            "content": "Rule-based Answer Verification in LLMs. Rule-based answer verification is widely used in LLM data pre-processing [37], model training [41, 8, 33], and evaluation frameworks such as LM Eval Harness [11], OpenCompass [6], openai-evals [30], and UltraEval [15]. This approach assesses the correctness of LLM outputs by comparing them against ground-truth answers associated with specific datasets. However, rule-based verification may struggle to evaluate semantically equivalent but textually distinct responses, potentially resulting in false negatives [4]. LLM as Judge. The increasing capabilities of LLMs have spurred interest in using them as judges to evaluate other models, often referred to as LLM as judge [12]. This approach leverages LLMs understanding to assess output quality, particularly for subjective or complex tasks where traditional metrics may fall short. LLM-as-a-judge methods are widely employed in alignment tasks [25, 23, 10, 24, 12, 22]. Recently, xVerify introduced compact LLM as an efficient answer verifier for reasoning model evaluations, surpassing GPT-4o in overall performance [4]. Additionally, LLM-as-a-judge techniques are increasingly integrated into training processes. For instance, SEED-THINKING-V1.5 employs reasoning model to evaluate diverse set of verifiable questions across varied scenarios [33]. Recently, [27] utilizes model-based verifier to deliver robust and accurate cross-domain rewards for RL training. Increasing Efficiency in RL for LLMs. Recent efforts have focused on improving the efficiency of RL training for LLMs, particularly with GRPO [33]. DAPO [41] enhances GRPOs efficiency by introducing dynamic sampling, which filters out prompts with accuracy values of 0 or 1, retaining only those with effective gradients while maintaining consistent batch size. VAPO [42] improves the utilization efficiency of positive samples during RL training through the Positive Example LM Loss. Additionally, PODS [38] proposes max-variance down-sampling to select rollouts with maximally diverse reward signals, achieving greater efficiency compared to the GRPO baseline."
        },
        {
            "title": "B Limitations and Broader Impacts",
            "content": "Limitations. This study focuses on false negatives (FNs) within the domain of mathematical reasoning and does not explore FNs in other domains, such as theorem proving [36], medical applications [20], or software engineering development [35], where FNs may still occur. Our experiments and theoretical analysis primarily utilize GRPO [33]. While we believe our findings can generalize to both online methods (e.g,. PPO [32], RLOO [17], and DAPO [41]), as well as offline methods (e.g., DPO [31], RAFT [9], and Reinforce-Rej [37]) that employ rejection sampling, we have not empirically validated this hypothesis. Additionally, the proposed TINYV currently relies on Prime Verifiers answer extraction mechanism (i.e., within boxed{}), which focuses solely on the final answer rather than considering the entire output, such as the reasoning process. Broader Impacts. Our work advances the efficiency of reinforcement learning training for mathematical reasoning, potentially enhancing the efficiency of machine learning, without identified negative societal impacts."
        },
        {
            "title": "C Detailed False Negative Categories",
            "content": "In this section, we present comprehensive taxonomy of false negatives identified in answer verification for mathematical reasoning tasks, based on our analysis on the Big-Math-RL-Verified dataset. These categories highlight the diverse reasons why rule-based verifiers, such as Prime Verifier, may incorrectly mark models response as wrong despite it being mathematically correct. Each category is divided into subcategories, with descriptions and illustrative examples to demonstrate the variations leading to false negatives. C.1 Formatting and Syntax Differences This category captures differences in formatting and syntax that do not alter the mathematical meaning of the answer. Formatting Whitespace and Spacing Issues 14 Description: Variations in spaces around operators, within expressions, or between elements. Example: ground truth answer: f(x) = 2 model answer: f(x)=2x Formatting Symbol Representation Issues Description: Differences in symbol notation, including Unicode vs. command-based symbols, delimiter styles, or minor symbol variations (e.g., degree symbols, infinity notation). Example: ground truth answer: (, -3) (3, +) model answer: (, -3) (3, ) Formatting Markup Variation Issues Description: Differences in syntax for equivalent rendering, such as LaTeX command choices or delimiter sizing. Example: ground truth answer: frac{32}{9} model answer: dfrac{32}{9} Formatting Unit Representation Issues Description: Differences in the inclusion, omission, or representation of units (e.g., missing units, abbreviated vs. full unit names). Example: ground truth answer: 18.8^circ model answer: 18.8 Formatting Contextual Addition or Omission Issues Description: Missing or extra prefixes (e.g., \"x=\") or explanatory text not affecting the core answer, excluding units. Example: ground truth answer: N=n model answer: Formatting Other Formatting Issues Description: Miscellaneous formatting differences, such as newline characters or nonalphanumeric separators. Example: ground truth answer: 60^text{circ} 42 model answer: 60^circ 42 C.2 Mathematical Notation Variations This category includes differences in standard mathematical conventions for expressing the same concept. Notation Interval vs. Inequality Notation Description: Representing ranges as intervals or inequalities. Example: ground truth answer: (, -5) model answer: < -5 Notation Ratio and Proportion Variations Description: Different ways of expressing ratios or proportions (e.g., colon, fraction, or single value). Example: ground truth answer: 2:1 model answer: 2/1 15 Notation Aggregated vs. Individual Solution Variations Description: Using symbols like or listing solutions separately. Example: ground truth answer: 1 sqrt{19} model answer: 1 + sqrt{19}, 1 - sqrt{19} Notation Vector and Matrix Notation Variations Description: Variations in displaying vectors or matrices. Example: ground truth answer: begin{pmatrix} -7 16 5 end{pmatrix} model answer: (-7,16,5) Notation Other Notation Variations Description: Variations due to regional conventions (e.g., decimal points vs. commas) or other notation differences. Example: ground truth answer: 3.14 model answer: 3,14 C.3 Mathematical Expression Equivalencies This category covers expressions that differ in form but are mathematically equivalent. Expression Algebraic Equivalence Variations Description: Different but equivalent algebraic forms, including term ordering, factoring, or simplification. Example: ground truth answer: frac{1-p^{2}}{3} model answer: frac{-p^2+1}{3} Expression Root and Exponent Form Variations Description: Using roots, fractional exponents, or simplified exponents differently. Example: ground truth answer: 2^{-2 / 3} model answer: frac{1}{sqrt[3]{4}} Expression Logarithmic and Trigonometric Form Variations Description: Equivalent forms using logarithmic or trigonometric identities. Example: ground truth answer: frac{log 2}{log 2-log 3} model answer: -frac{ln 2}{ln 3-ln 2} Expression Other Equivalence Variations Description: Equivalencies in combinatorial quantities, complex numbers, or other mathematical structures. Example: ground truth answer: frac{3 m}{2}-1 model answer: dfrac{3m - 2}{2} C.4 Numerical Representation Differences This category addresses variations in how numerical values are presented. Numeric Exact vs. Approximate Form Variations Description: Exact (fraction, symbolic) vs. decimal or percentage approximations. Example: ground truth answer: frac{600}{7} model answer: 85.71 16 Numeric Alternative Exact Form Variations Description: Different exact representations, such as scientific notation or evaluated powers. Example: ground truth answer: 10^{3} model answer: 1000 Numeric Rounding and Precision Variations Description: Approximations with different decimal places or rounding rules. Example: ground truth answer: 1.27% model answer: 1.3% Numeric Other Numerical Variations Description: Other numerical format differences, such as mixed vs. improper fractions. Example: ground truth answer: 6frac{1}{64} model answer: 6.015625 C.5 Language and Contextual Variations This category captures differences in natural language or implied context. Language Presence/Absence of Explanatory Text Description: Model output or ground truth includes additional descriptive text, or vice versa. Example: ground truth answer: 10,11,12,13,14,-2,-1,0,1,2 model answer: 11, 12, 13, 14 Sequence 1: -2, -1, 0, 1, 2 and Sequence 2: 10, Language Implicit vs. Explicit Variable/Function Assignment Description: One output explicitly assigns values to variables or defines function while the other lists values or the expression directly. Example: ground truth answer: 16,3,1,1 model answer: w=16, d=3, a=1, b=1 Language Phrasing and Conciseness Variations Description: Differences in wording, synonyms, or level of detail. Example: ground truth answer: text{Any odd number of participants} model answer: odd Language Other Language Variations Description: Minor differences in separators (e.g., \"and\" vs. comma) or answer structure. Example: ground truth answer: 1,3 model answer: 1 text{ and } 3 C.6 Set and List Differences This category includes variations in presenting collections of results, assuming correctness. Set/List Order of Element Variations Description: Different sequencing of elements in sets or lists where order is not mathematically significant. 17 Example: ground truth answer: (6,3),(9,3),(9,5),(54,5) model answer: (9,3),(6,3),(54,5),(9,5) Set/List Structural Formatting Variations Description: Variations in tuple, set, or list formatting, including use of braces. Example: ground truth answer: (1,2), (3,4) model answer: {(1,2), (3,4)} Set/List Element Delimiter Variations Description: Differences in delimiters used to separate elements (e.g., commas vs. semicolons). Example: ground truth answer: (1,2,3) model answer: (1;2;3) Set/List Other Set and List Variations Description: Other differences in set or list presentation, such as redundant parentheses. Example: ground truth answer: (1,2) model answer: ((1,2)) C.7 Symbolic Representation Variations This category addresses differences in variable or constant symbols. Symbolic Variable and Constant Choice Variations Description: Different letters or cases for arbitrary constants or parameters. Example: ground truth answer: ...+pi k, ... model answer: ...+n pi, ... Symbolic Subscript or Superscript Variations Description: Differences in subscript or superscript notation for variables or constants. Example: ground truth answer: x_1, x_2 model answer: x^1, x^2 Symbolic Custom Symbol Variations Description: Use of unconventional or user-defined symbols for variables or constants. Example: ground truth answer: α, β model answer: a, Symbolic Other Symbolic Variations Description: Other differences in symbolic representation, such as case sensitivity. Example: ground truth answer: P(x) model answer: p(x) 18 Proof of Theorem In this section, we provide detailed proof of Theorem 1, which states that policies trained with ground truth rewards have greater step-wise learnability than those with false negatives. We first derive the closed-form expression of the step-wise learnability in Section D.1, and then prove the positivity of the step-wise learnability gap in Sections D.2 and D.3. D.1 Reverse KL for GRPO Updates We begin with the GRPO objective Eyπθ(x) max θ (cid:2)r(x, y)(cid:3) β DKL (cid:0)πθ(y x) πinit(y x)(cid:1), and transform this optimization into step-wise recursion. Throughout, we denote: x: input prompt y: output token/sequence r(x, y) {0, 1}: binary reward (cid:2)1{r(x,y)=1} pk(x) = Eyπk(x) (cid:2)1{r(x,y)=1} pref(x) = Eyπref(x) (cid:3): success probability of policy πk for prompt (cid:3): success probability of reference policy for prompt Lemma 2 (GRPO Policy Dynamics [28]). For 1, the optimal GRPO iterate satisfies πk(y x) = 1 Zk1(x) πref(y x) exp (cid:16) 1 β (cid:2)ω+ (cid:0)pk1(x)(cid:1) 1{r(x,y)=1} ω (cid:0)pk1(x)(cid:1) 1{r(x,y)=0} ε ε (cid:3)(cid:17) with weights ω+ ε (p) = 1 (cid:112)p(1 p) + ε , ω ε (p) = (cid:112)p(1 p) + ε , and normalizing constant Zk1(x) = pref(x) Proof. See [28] for the proof. (cid:0)pk1(x)(cid:1) 1 β ω+ ε + (cid:0)1 pref(x)(cid:1) 1 β ω ε (cid:0)pk1(x)(cid:1) . Building on Lemma 2, we now derive the reverse KullbackLeibler (KL) divergence between two consecutive GRPO iterates. Lemma 3 (Reverse KL for GRPO Updates). Given the GRPO policy updates from Lemma 2, the reverse KL divergence satisfies DKL (cid:0)πk1( x) πk( x)(cid:1) = (cid:104) 1 β ω+ ε (cid:0)pk2(x)(cid:1) pk1(x) ω ε (cid:0)pk2(x)(cid:1) (cid:0)1 pk1(x)(cid:1)(cid:105) log Zk2(x) Zk1(x) Proof. By definition, the reverse KL divergence between πk1 and πk is: DKL (cid:0)πk1( x) πk( x)(cid:1) = (cid:88) πk1(y x) log πk1(y x) πk(y x) . Using the GRPO update rule from Lemma 2 for both policies, we can express πk1 and πk as: πk1(y x) = 1 Zk2(x) πref(y x) exp (cid:2)ω+ (cid:0)pk2(x)(cid:1) 1{r(x,y)=1} ω (cid:0)pk2(x)(cid:1) 1{r(x,y)=0} ε ε (cid:16) 1 β (cid:3)(cid:17) and similarly for πk(y x). Taking the log-ratio and simplifying the result, we get: log πk1(y x) πk(y x) = log Zk1(x) Zk2(x) + 1 β (cid:2)+ (x) 1{r(x,y)=1} (x) 1{r(x,y)=0} (cid:3) (11) 19 where we denote: + (x) = ω+ ε (cid:0)pk2(x)(cid:1) ω+ (cid:0)pk1(x)(cid:1), ε (x) = ω ε (cid:0)pk2(x)(cid:1) ω (cid:0)pk1(x)(cid:1). ε Taking the expectation with respect to πk1( x) and noting that: (cid:88) (cid:88) πk1(y x)1{r(x,y)=1} = pk1(x) πk1(y x)1{r(x,y)=0} = 1 pk1(x) we obtain:"
        },
        {
            "title": "DKL",
            "content": "(cid:0)πk1πk (cid:1) = log Zk1(x) Zk2(x) + 1 β (cid:2)+ (x)pk1(x) (x)(1 pk1(x))(cid:3) Substituting the definitions of +"
        },
        {
            "title": "DKL",
            "content": "(cid:0)πk1πk (cid:1) = log ε (pk2(x))pk1(x) ω+ ε (pk1(x))pk1(x) (x) and expanding: (x) and (cid:2)ω+ + 1 β Zk1(x) Zk2(x) ε (pk2(x))(1 pk1(x)) + ω ω ε (pk1(x))(1 pk1(x))(cid:3) (12) (13) (14) (15) (16) key observation is that for any p, we have ω+ from their definitions. Applying this identity to the terms involving pk1(x): ε (p)p ω ε (p)(1 p) = 0, which can be verified ε (pk1(x))pk1(x) ω ω+ ε (pk1(x))(1 pk1(x)) = 0 Therefore, these terms cancel out, yielding: DKL (cid:0)πk1( x) πk( x)(cid:1) = (cid:104) 1 β which completes the proof. ω+ ε (cid:0)pk2(x)(cid:1)pk1(x)ω ε (cid:105) (cid:0)pk2(x)(cid:1)(1pk1(x)) log Zk2(x) Zk1(x) D.2 Integral Form of the Step-Wise Learnability Gap According to the closed-form of the step-wise learnability derived in the previous section, we can further transform the difference of step-wise learnability into an integral form involving partial derivatives. Then we prove that these partial derivatives are positive, which establishes our main result. We simplify the notation of the step-wise learnability in Lemma 3 as follows: D(a, b) = (cid:104) 1 β ε (b)a ω ω+ (cid:105) ε (b)(1 a) log Z(b) Z(a) where: represents the success probability at the current step represents the success probability at the previous step , GT k1) represents the step-wise learnability when training with ground truth k1) represents the step-wise learnability when training with Let Dk,GT = D(P GT rewards, while Dk,FN = D(P FN rewards containing false negatives. Lemma 4 (Integral Form of the Step-Wise Learnability Gap). Let δk = Dk,GT Dk,FN be the stepwise learnability gap at training step k, where Dk,GT and Dk,FN are defined in Equations (9) and (10). We can express δk as: , FN δk = (cid:90) 0 [1D + 2D](P GT t, GT k1 t) dt where 1D denotes D(a,b) , 2D denotes D(a,b) , and = GT FN > 0 by Lemma 1. 20 Proof. We define function (t) = D(P GT integration domain, we have: t, GT k1 t) for [0, k]. At the boundaries of the (0) = D(P GT , GT k1) = Dk,GT At = = GT FN , we have: (k) = D(P FN , FN k1) = Dk,FN (17) (18) Therefore, the learnability gap can be expressed as δk = (0)f (k). By the fundamental theorem of calculus: (cid:90) δk = (t) dt Computing (t) via the chain rule: 0 (t) = dt D(P GT t, GT t, GT = 1D(P GT = [1D + 2D](P GT k1 t) k1 t) (1) + 2D(P GT t, GT k1 t) t, GT k1 t) (1) (19) (20) (21) Therefore: δk = (cid:90) 0 [1D + 2D](P GT t, GT k1 t) dt Since > 0 by Lemma 1, proving δk > 0 reduces to showing that the integrand [1D + 2D](a, b) > 0 throughout the integration domain. In other words, if the sum of partial derivatives of with respect to its arguments is positive, then the step-wise learnability with ground truth rewards exceeds that with false negative rewards. Lemma 5 (Positivity of the Partial Derivatives). For any (a, b) (0, 1)2 satisfying < < 2b, the following inequality holds: [1D + 2D](a, b) > Proof. We begin by computing the partial derivatives of the function D(a, b) = 1 β (cid:0)W +(b) (b) (1 a)(cid:1) log Z(b) Z(a) , where we use + and as shorthand for ω+ ε and ω ε to simplify notation. Direct differentiation with respect to and yields: 1D(a, b) = 2D(a, b) = 1 β 1 β (cid:0)W +(b) + (b)(cid:1) + (a) Z(a) , (cid:0)a + (b) (1 a) (b)(cid:1) (b) Z(b) . Summing these two partial derivatives, we obtain: [1D + 2D](a, b) = (b) 1 β (cid:124) (cid:123)(cid:122) (cid:125) + (cid:104) (a) Z(a) (cid:124) (b) Z(b) (cid:105) , (cid:125) (cid:123)(cid:122) where (b) = +(b) + (b) + aW + (b) (1 a)W (b). Our proof strategy is to show that both term and term are positive under the given conditions. 21 Part A: Proving 1 β (b) > 0 Recall the definitions: +(p) = 1 (cid:112)p(1 p) + ε (cid:112)p(1 p) + ε where ε > 0 is small positive constant, and denote d(b) = (cid:112)b(1 b) + ε. For the term (b) = +(b) + (b) + aW +(b) (1 a)W (b), after simplification, we have: , (p) = , (b) = d(b) d(b)(a b) d(b) . Thus where d(b) = 12b 2 b(1b) (b) = b(1 b) + ε(cid:112)b(1 b) (a b)(1 2b) (cid:112)b(1 b)((cid:112)b(1 b) + ε) For > 1 For 1 2 , since 1 2b < 0, > 0, we have (b) > 0. 2 , by using < 2b: (b) > b(1 b) b(1 2b) (cid:112)b(1 b)((cid:112)b(1 b) + ε)2 > b2 (cid:112)b(1 b)((cid:112)b(1 b) + ε)2 > 0 Thus, (b) > 0 for all (0, 1), which implies 1 β (b) > 0. Part B: Proving Z(a) Z(a) Z(b) Z(b) > 0 when > We want to prove that g(p) = Z(p) g(a) g(b) > 0. Z(p) is strictly increasing, which will show that when > b, we have Recall that: Z(p) = Prefeu(p) + (1 Pref)ev(p) u(p) = v(p) = 1 β 1 β +(p) (p) Define the weights: w1(p) = Prefeu(p) Z(p) , w2(p) = (1 Pref)ev(p) Z(p) Note that w1(p) + w2(p) = 1. The derivative of Z(p) is: Therefore: Thus we have (p) = Prefeu(p)u(p) + (1 Pref)ev(p)(v(p)) = Z(p) [w1(p)u(p) w2(p)v(p)] g(p) = (p) Z(p) = w1(p)u(p) w2(p)v(p) (22) (23) (24) (25) (26) (27) (28) g(p) = 1(p)u(p) + w1(p)u(p) + 2(p)(v(p)) + w2(p)(v(p)) We know that w1(p) + w2(p) = 1, so Using the definition of w1(p) and w2(p), we can derive: 1(p) + 2(p) = 0, i.e., 1(p) = 2(p). w 1(p) = w1(p)[u(p) g(p)] 2(p) = w2(p)[(v(p)) g(p)] Substituting these into the expression for g(p): g(p) = w1(p)[u(p) g(p)]u(p) + w2(p)[(v(p)) g(p)](v(p)) = w1(p)u(p)2 w1(p)g(p)u(p) + w2(p)v(p)2 w2(p)g(p)(v(p)) = w1(p)u(p)2 + w2(p)v(p)2 g(p)2 We can expand g(p)2 as: g(p)2 = w1(p)2u(p)2 2w1(p)w2(p)u(p)v(p) + w2(p)2v(p)2 (29) (30) (31) (32) (33) (34) (35) Substituting this into our expression for g(p): g(p) = w1(p)u(p)2 + w2(p)v(p)2 [w1(p)2u(p)2 2w1(p)w2(p)u(p)v(p) + w2(p)2v(p)2] (36) = w1(p)u(p)2(1 w1(p)) + w2(p)v(p)2(1 w2(p)) + 2w1(p)w2(p)u(p)v(p) = w1(p)w2(p)u(p)2 + w1(p)w2(p)v(p)2 + 2w1(p)w2(p)u(p)v(p) = w1(p)w2(p)[u(p) + v(p)]2 (37) (38) (39) This is positive since its squared term multiplied by positive weights (w1(p) > 0 and w2(p) > 0). Consequently, g(p) = Z(p) g(b) > 0. Z(p) is strictly increasing, which means that when > b, we have g(a) Combining the results from Part and Part B, we have: [1D + 2D](a, b) = 1 β (b) + (cid:20) (a) Z(a) (cid:21) (b) Z(b) > 0 This completes the proof of Lemma 5. D.3 Proof of Theorem 1 Having established the necessary lemmas, we now complete the proof of Theorem 1. Proof. From Lemma 4, we have expressed the step-wise learnability gap as an integral: (cid:90) δk = 0 > 0 by Lemma 1. [1D + 2D](P GT t, GT k1 t) dt FN where = GT From Lemma 5, we have established that [1D + 2D](a, b) > 0 for all pairs (a, b) (0, 1)2 satisfying < < 2b (Assumption 1 and 2). Since the integrand [1D + 2D](P GT k1 t) is positive throughout the integration domain, and the integration is performed over positive interval [0, k], we conclude that δk > 0 for all k. t, GT This theoretical result highlights the importance of accurate reward signals in reinforcement learning. False negatives in reward feedback significantly impede the learning process by reducing the step-wise improvement of the policy at each iteration, potentially leading to slower convergence and suboptimal performance."
        },
        {
            "title": "E More on Experimental Setups",
            "content": "In this section, we details our setups for the experiments. E.1 Experimental Setups for Zero RL We follow [26] and use the following hyper-parameters detailed in Table 2 for Zero RL training. We perform experiments on 8 A100 GPUs. The model is trained using VERL [34]. Table 2: This table shows the hyper-parameters for zero RL training. Hyper-parameter Learning Rate Number of Epochs Number of Devices Rollout Batch Size PPO Mini Batch Size Max Prompt Length Max Response Length KL Coefficient Rollout Engine Optimizer Learning Rate Scheduler Warmup Ratio Max Sequence Length Value 1 106 12 8 128 64 1024 3072 (QWEN2.5-MATH-7B), 4096 (QWEN2.5-7B) 0.001 VLLM (V0.8.2) Adamw cosine 0.1 4096 E.2 TINYV Data Curation Real Example Generation. We utilize the seemingly incorrect prompt-response pairs collected in Section 3 as the source of real examples. Specifically, for each prompt-response pair (x, yi) marked as incorrect by Prime Verifier, we adopt LLM annotations as the ground truth label: True for response that is correct and False otherwise. Additionally, we retain the intermediate analysis of LLMs for TINYV-THINK training. Synthetic Example Generation. To enhance coverage, ensure robustness, and balance the dataset with an equal number of True and False labels, we augment the dataset with synthetically generated false negatives. Specifically, we prompt QWEN2.5-72B-INSTRUCT to generate potential false negative cases for given question by introducing variations such as LaTeX formatting differences, numerical approximations, or alternative mathematical expressions that preserve semantic equivalence. These generated candidates are then re-annotated by LLMs to confirm their correctness. As with the real examples, we retain the intermediate analysis of LLMs. The prompts used for generating synthetic examples are provided in Appendix H.3. E.3 TINYV Training Table 3 demonstrates the detailed supervised fine-tuning (SFT) hyper-parameters for training TINYV. We perform experiments on 8 A100 GPUs. The training and inference template is demonstrated in Figure 8. The model is trained using Llama Factory [45]. HardVerify-Math Benchmark In this section, we detail our HardVerify-Math Bench, benchmark comprising 250 hard-to-verify answers that span all categories and the taxonomy discussed in Section 3. The dataset consists of two parts: (1) from existing benchmarks, we manually select 115 questions from the Olympiad benchmark and 10 questions from the MATH test sets, which are prone to false negatives due to their complex answer formats; (2) from other sources, we include 125 questions from the Big-Math dataset, selected based on LLaMA-3.1-8B pass rate of less than 0.05 and identified as challenging to verify by human experts. Each question in HardVerify-Math Bench results in at least one false 24 Prompt Template for TINYV Training and Inference You are an AI tasked with identifying false negatives in answer verification . false negative occurs when models answer is essentially correct but is marked as incorrect due to minor discrepancies or formatting issues. Your job is to analyze the given question, ground truth answer, and model answer to determine if the models answer is actually correct despite appearing different from the ground truth. <question>{{QUESTION}}</question> <ground_truth_answer>{{GROUND_TRUTH_ANSWER}}</ground_truth_answer> <model_answer>{{MODEL_ANSWER}}</model_answer> Return \"True\" if the models answer is correct, otherwise return \"False\". Figure 8: Prompt Template for TINYV Training and Inference. Example 1 (Olympiad Benchmark) Question: Determine all real numbers $x>0$ for whichnn$$nlog _{4} xlog _{x} 16=frac{7}{6}-log _{x} 8n$$ Ground Truth: $2^{-2 / 3}$, $8$ Model Output: 8, frac{1}{sqrt[3]{4}} Example 2 (Olympiads Big-Math) Question: Which clock shows the correct time more often: one that is one minute slow or one that is stopped? Ground Truth: stopped clock shows the correct time more often. Model Output: text{stopped} Example 3 (CN_K12) Question: After the epidemic, the tourism market in Yunnan has shown strong recovery this year. travel agency rented two types of rooms, $A$ and $B$, during the Spring Festival this year. The number of rooms rented for $A$ at $4800$ yuan is the same as the number of rooms rented for $B$ at $4200$ yuan. The rent for each $A$ room this year is $30$ yuan more than the rent for each $B$ room. Find the rent for each $A$ and $B$ room this year. Ground Truth: The rent for each $A$ room is $240$ yuan, and for each $B$ room is $210$ yuan. Model Output: 240 text{ yuan (A)}, 210 text{ yuan (B)} Example 4 (ORCA Math) Question: can do piece of work in 12 days and alone can do it in 14 days. How much time will both take to finish the work together? Ground Truth: 6.46 Model Output: dfrac{84}{13}text{ days} Figure 9: HardVerify-Math Bench Examples. 25 Table 3: This table shows the hyper-parameters for supervised fine-tuning of TINYV. Hyper-parameter Learning Rate Number of Epochs Number of Devices Per-device Batch Size Gradient Accumulation Steps Effective Batch Size Optimizer Learning Rate Scheduler Warmup Ratio Max Sequence Length Value 1 105 2 8 8 8 512 Adamw cosine 0.1 4096 Figure 10: This figure shows the source distribution of HardVerify-Math Bench. negative when evaluated using Prime Verifier. We include the incorrect answer that triggers the false negative, along with the question and its ground truth answer, for reference. Figure 9 illustrates examples, while Figure 10 shows the sources of the questions."
        },
        {
            "title": "G More Experimental Results",
            "content": "G.1 Comparison Across Different Verifiers Figure 11 compares the performance among TINYV, TINYV-THINK, Math Verify, and Prime Verifier on AMC, MATH, Olympiad and HardVerify-Math Bench. The base model is QWEN2.5-MATH7B. We observe that the performance of TINYV and TINYV-THINK is comparable and surpasses that of rule-based verifiers (i.e., Math Verify and Prime Verifier) in training efficiency and model performance. Given that the training time for TINYV-THINK is significantly higher than that for TINYV (53.73 hours vs. 18.71 hours), we adopt TINYV as the default setup for our experiments. G.2 Training Cost Analysis Figure 12 compares the time cost of TINYV with that of Prime Verifier during GRPO training. We observe that the model trained with both TINYV and Prime Verifier exhibits comparable average time per step, with TINYV incurring only 6% additional computational cost. This indicates that TINYV maintains high efficiency in RL training. 26 Figure 11: This figure compares the model performance of TINYV, TINYV-THINK, Math Verify, and Prime Verifier on diverse benchmarks. The base model is QWEN2.5-MATH-7B. Figure 12: This figures compares the average time cost of TINYV compared with Prime Verifier during GRPO training. The peak occurs when saving model checkpoints."
        },
        {
            "title": "H Prompt Templates",
            "content": "H.1 Prompt for FN Annotation Figure 13-14 demonstrates the prompt template for labeling false negative responses. H.2 Prompt for FN Category Annotations Figure 15-17 demonstrates the prompt template for labeling FN categories. H.3 Prompt for Generating Synthetic FN Examples Figure 18 demonstrates the prompt template for generating Synthetic FN Examples. 27 Prompt Template for False Negative Annotation (Part 1) ## Task Description You are an AI tasked with identifying false negatives in answer verification. false negative occurs when models answer is essentially correct but is marked as incorrect due to minor discrepancies or formatting issues. Your job is to analyze the given question, ground truth answer, and model answer to determine if the models answer is actually correct despite appearing different from the ground truth. Analyze the inputs carefully, considering the following: 1. Is the models answer mathematically equivalent to the ground truth? 2. Are there minor formatting differences that dont affect the answers correctness? 3. Is the models answer more precise or in different but valid format? ## Examples Here are some examples of questions, ground truth answers, and model answers. All of them are correct. **Example 1 (Order-Insensitive):** <question>Determine all real values of $x$ for which $(x+8)^{4}=(2 x+16)^{2}$.</question> <ground_truth_answer>-6,-8,-10</ground_truth_answer> <model_answer>-10, -8, -6</model_answer> <analysis> json { \"reasoning\": \"The models answer lists the same values as the ground truth but in different order. Since the question asks for all solutions, the order doesnt matter for correctness.\", \"is_correct\": true } </analysis> **Example 2 (Latex Expression):** <question>A bag contains 3 green balls, 4 red balls, and no other balls. Victor removes balls randomly from the bag, one at time, and places them on table. Each ball in the bag is equally likely to be chosen each time that he removes ball. He stops removing balls when there are two balls of the same colour on the table. What is the probability that, when he stops, there is at least 1 red ball and at least 1 green ball on the table?</question> <ground_truth_answer>$frac{4}{7}$</ground_truth_answer> <model_answer>4/7</model_answer> <analysis> json { \"reasoning\": \"The models answer 4/7 is mathematically equivalent to the ground truth answer $frac{4}{7}$. The only difference is in the notation - the ground truth uses LaTeX fraction notation while the model uses simple division format. The numerical value is identical in both cases.\" \"is_correct\": true } </analysis> Figure 13: Prompt Template for Labeling FN Responses (Part 1) 28 Prompt Template for False Negative Annotation (Part 2) **Example 3 (Variable):** <question>If $T=x^{2}+frac{1}{x^{2}}$, determine the values of $b$ and $c$ so that $x^{6}+ frac{1}{x^{6}}=T^{3}+b T+c$ for all non-zero real numbers $x$.</question> <ground_truth_answer>-3,0</ground_truth_answer> <model_answer>b=-3, c=0</model_answer> <analysis> json { \"reasoning\": \"The models answer b=-3, c=0 is mathematically equivalent to the ground truth answer -3,0. The model simply labeled the values with their corresponding variables, which provides more clarity but doesnt change the mathematical content of the answer.\", \"is_correct\": true } </analysis> **Example 4 (Paraphrase):** <question>Peter has 8 coins, of which he knows that 7 are genuine and weigh the same, while one is fake and differs in weight, though he does not know whether it is heavier or lighter. Peter has access to balance scale, which shows which side is heavier but not by how much. For each weighing, Peter must pay Vasya one of his coins before the weighing. If Peter pays with genuine coin, Vasya will provide an accurate result; if fake coin is used, Vasya will provide random result. Peter wants to determine 5 genuine coins and ensure that none of these genuine coins are given to Vasya. Can Peter guaranteedly achieve this?</ question> <ground_truth_answer>Petya can guarantee finding 5 genuine coins.</ground_truth_answer> <model_answer>Yes, Peter can guarantee finding 5 genuine coins while ensuring that none of these genuine coins are paid to Vasya.</model_answer> <analysis> json { \"reasoning\": \"The models answer correctly states that Peter can guarantee finding 5 genuine coins, which matches the ground truth. The model provides additional details about ensuring none of these coins are paid to Vasya, but this doesnt change the correctness of the answer.\" \"is_correct\": true } </analysis> ## Input Now, please analyze the following question, ground truth answer, and model answer. <question> {{QUESTION}} </question> <ground_truth_answer> {{GROUND_TRUTH_ANSWER}} </ground_truth_answer> <model_answer> {{MODEL_ANSWER}} </model_answer> ## Output Please provide your analysis in the following JSON format: <analysis> json { \"reasoning\": \"Your detailed reasoning here\", \"is_correct\": true/false } </analysis> Ensure your reasoning is thorough and considers all aspects of the answers. The \"is_correct\" field should be true if the models answer is essentially correct despite any minor differences from the ground truth and false otherwise. Figure 14: Prompt Template for Labeling FN Responses (Part 2) 29 Prompt Template for Labeling FN Categories (Part 1) ## Task Description You are an AI assistant tasked with classifying schemes for common types of equivalence and mismatch between mathematical answers. ## Taxonomy --- ### 1. Formatting and Syntax Differences Differences in formatting and/or syntax that do not affect mathematical meaning. * **1.1 Formatting -> Whitespace and Spacing Issues** * *Description:* Variations in spaces around operators, within expressions, or between elements. * *Example:* ground truth answer: f(x) = 2 x, model answer: f(x)=2x * **1.2 Formatting -> Symbol Representation Issues** * *Description:* Differences in symbol notation, including Unicode vs. command-based symbols, delimiter styles, or minor symbol variations (e.g., degree symbols, infinity notation) . * *Example:* ground truth answer: $(-infty,-3)cup(3,+infty)$, model answer: $(- infty,-3)cup(3,infty)$ * **1.3 Formatting -> Markup Variation Issues** * *Description:* Differences in syntax for equivalent rendering, such as LaTeX command choices or delimiter sizing. * *Example:* ground truth answer: frac{32}{9}, model answer: dfrac{32}{9} * **1.4 Formatting -> Unit Representation Issues** * *Description:* Differences in the inclusion, omission, or representation of units (e.g., missing units, abbreviated vs. full unit names). * *Example:* ground truth answer: 18.8^circ, model answer: 18.8 * **1.5 Formatting -> Contextual Addition or Omission Issues** * *Description:* Missing or extra prefixes (e.g., \"x=\") or explanatory text not affecting the core answer, excluding units. * *Example:* ground truth answer: N=n, model answer: * **1.6 Formatting -> Other Formatting Issues** * *Description:* Miscellaneous formatting differences, such as newline characters or nonalphanumeric separators. * *Example:* ground truth answer: 60^textcirc 42, model answer: 60^circ 42 --- ### 2. Mathematical Notation Variations Differences in standard mathematical conventions for expressing the same concept. * **2.1 Notation -> Interval vs. Inequality Notation** * *Description:* Representing ranges as intervals or inequalities. * *Example:* ground truth answer: (-infty, -5), model answer: < -5 * **2.2 Notation -> Ratio and Proportion Variations** * *Description:* Different ways of expressing ratios or proportions (e.g., colon, fraction, or single value). * *Example:* ground truth answer: 2:1, model answer: 2/1 * **2.3 Notation -> Aggregated vs. Individual Solution Variations** * *Description:* Using symbols like $pm$ or listing solutions separately. * *Example:* ground truth answer: 1 $pm$ sqrt{19}, model answer: 1 + sqrt{19}, 1 - sqrt{19} * **2.4 Notation -> Vector and Matrix Notation Variations** * *Description:* Variations in displaying vectors or matrices. * *Example:* ground truth answer: begin{pmatrix} -7 16 5 end{pmatrix}, model answer: (-7,16,5) * **2.5 Notation -> Other Notation Variations** * *Description:* Variations due to regional conventions (e.g., decimal points vs. commas) or other notation differences. * *Example:* ground truth answer: 3.14, model answer: 3,14 --- Figure 15: Prompt Template for Labeling FN Categories (Part 1) 30 Prompt Template for Labeling FN Categories (Part 2) ### 3. Mathematical Expression Equivalencies Expressions that differ in form but are mathematically equivalent. * **3.1 Expression -> Algebraic Equivalence Variations** * *Description:* Different but equivalent algebraic forms, including term ordering, factoring , or simplification. * *Example:* ground truth answer: frac{1-p^{2}}{3}, model answer: frac{-p^2+1}{3} * **3.2 Expression -> Root and Exponent Form Variations** * *Description:* Using roots, fractional exponents, or simplified exponents differently. * *Example:* ground truth answer: 2^{-2 / 3}, model answer: frac{1}{sqrt[3]{4}} * **3.3 Expression -> Logarithmic and Trigonometric Form Variations** * *Description:* Equivalent forms using logarithmic or trigonometric identities. * *Example:* ground truth answer: frac{log 2}{log 2-log 3}, model answer: -frac{ ln 2}{ln 3-ln 2} * **3.4 Expression -> Other Equivalence Variations** * *Description:* Equivalencies in combinatorial quantities, complex numbers, or other mathematical structures. * *Example:* ground truth answer: frac{3 m}{2}-1, model answer: dfrac{3m - 2}{2} --- ### 4. Numerical Representation Differences Variations in how numerical values are presented. * **4.1 Numeric -> Exact vs. Approximate Form Variations** * *Description:* Exact (fraction, symbolic) vs. decimal or percentage approximations. * *Example:* ground truth answer: frac{600}{7}, model answer: 85.71 * **4.2 Numeric -> Alternative Exact Form Variations** * *Description:* Different exact representations, such as scientific notation or evaluated powers. * *Example:* ground truth answer: 10^{3}, model answer: 1000 * **4.3 Numeric -> Rounding and Precision Variations** * *Description:* Approximations with different decimal places or rounding rules. * *Example:* ground truth answer: 1.27%, model answer: 1.3% * **4.4 Numeric -> Other Numerical Variations** * *Description:* Other numerical format differences, such as mixed vs. improper fractions. * *Example:* ground truth answer: 6frac{1}{64}, model answer: 6.015625 --- ### 5. Language and Contextual Variations Differences in natural language or implied context. * **5.1 Language -> Presence/Absence of Explanatory Text** * *Description:* Model output or ground truth includes additional descriptive text, or vice versa. * *Example:* ground truth answer: 10,11,12,13,14,-2,-1,0,1,2, model answer: Sequence 1: -2, -1, 0, 1, 2 and Sequence 2: 10, 11, 12, 13, 14 * **5.2 Language -> Implicit vs. Explicit Variable/Function Assignment** * *Description:* One output explicitly assigns values to variables or defines function while the other lists values or the expression directly. * *Example:* ground truth answer: 16,3,1,1, model answer: w=16, d=3, a=1, b=1 * **5.3 Language -> Phrasing and Conciseness Variations** * *Description:* Differences in wording, synonyms, or level of detail. * *Example:* ground truth answer: text{Any odd number of participants}, model answer: odd * **5.4 Language -> Other Language Variations** * *Description:* Minor differences in separators (e.g., \"and\" vs. comma) or answer structure. * *Example:* ground truth answer: 1,3, model answer: 1 text{ and } 3 --- Figure 16: Prompt Template for Labeling FN Categories (Part 2) 31 Prompt Template for Labeling FN Categories (Part 3) ### 6. Set and List Differences Variations in presenting collections of results, assuming correctness. * **6.1 Set/List -> Order of Element Variations** * *Description:* Different sequencing of elements in sets or lists where order is not mathematically significant. * *Example:* ground truth answer: (6,3),(9,3),(9,5),(54,5), model answer: (9,3),(6,3) ,(54,5),(9,5) * **6.2 Set/List -> Structural Formatting Variations** * *Description:* Variations in tuple, set, or list formatting, including use of braces. * *Example:* ground truth answer: (1,2), (3,4), model answer: {(1,2), (3,4)} * **6.3 Set/List -> Element Delimiter Variations** * *Description:* Differences in delimiters used to separate elements (e.g., commas vs. semicolons). * *Example:* ground truth answer: (1,2,3), model answer: (1;2;3) * **6.4 Set/List -> Other Set and List Variations** * *Description:* Other differences in set or list presentation, such as redundant parentheses . * *Example:* ground truth answer: (1,2), model answer: ((1,2)) --- ### 7. Symbolic Representation Variations Differences in variable or constant symbols. * **7.1 Symbolic -> Variable and Constant Choice Variations** * *Description:* Different letters or cases for arbitrary constants or parameters. * *Example:* ground truth answer: ...+pi k, ..., model answer: ...+n pi, ... * **7.2 Symbolic -> Subscript or Superscript Variations** * *Description:* Differences in subscript or superscript notation for variables or constants. * *Example:* ground truth answer: x_1, x_2, model answer: x^1, x^2 * **7.3 Symbolic -> Custom Symbol Variations** * *Description:* Use of unconventional or user-defined symbols for variables or constants. * *Example:* ground truth answer: alpha, beta, model answer: a, * **7.4 Symbolic -> Other Symbolic Variations** * *Description:* Other differences in symbolic representation, such as case sensitivity. * *Example:* ground truth answer: P(x), model answer: p(x) --- ## Input <ground_truth_answer> {{GROUND_TRUTH_ANSWER}} </ground_truth_answer> <model_answer> {{MODEL_ANSWER}} </model_answer> ## Output Identify the most precise equivalence or mismatch category from the taxonomy above that best characterizes the relationship between the ground truth answer and the model answer. Specify the primary category (required), and, if relevant, secondary category (optional). Avoid selecting \"Others\" categories when possible. Respond in this format, providing only the category ID and name: <primary_category> [ID] [Category Name] (e.g., 1.1 Formatting -> Whitespace and Spacing Issues) </primary_category> <second_category> [ID] [Category Name], if applicable (e.g., 6.1 Set/List -> Order of Element Variations) </second_category> Figure 17: Prompt Template for Labeling FN Categories (Part 3) 32 Prompt Template for Generating Synthetic FN Examples ## Task Description You are an AI assistant tasked with generating set of mathematically equivalent answers to given ground truth answer. These equivalent answers should maintain the same mathematical meaning while potentially varying in format, notation, or phrasing. ## Examples Below are examples of questions with their ground truth answers, followed by equivalent answers that preserve the mathematical meaning. **Example 1 (Order-Insensitive):** <question>Determine all real values of $x$ for which $(x+8)^{4}=(2 x+16)^{2}$.</question> <ground_truth_answer>-6,-8,-10</ground_truth_answer> <equivalent_answer_1>-8, -10, -6</equivalent_answer_1> **Example 2 (Latex Expression):** <question>A bag contains 3 green balls, 4 red balls, and no other balls. Victor removes balls randomly from the bag, one at time, and places them on table. Each ball in the bag is equally likely to be chosen each time that he removes ball. He stops removing balls when there are two balls of the same colour on the table. What is the probability that, when he stops, there is at least 1 red ball and at least 1 green ball on the table?</question> <ground_truth_answer>$frac{4}{7}$</ground_truth_answer> <equivalent_answer_1>4/7</equivalent_answer_1> **Example 3 (Variable):** <question>If $T=x^{2}+frac{1}{x^{2}}$, determine the values of $b$ and $c$ so that $x^{6}+ frac{1}{x^{6}}=T^{3}+b T+c$ for all non-zero real numbers $x$.</question> <ground_truth_answer>-3,0</ground_truth_answer> <model_answer>b=-3, c=0</model_answer> <equivalent_answer_1>b=-3, c=0</equivalent_answer_1> <equivalent_answer_2>b = -3, = 0</equivalent_answer_2> **Example 4 (Paraphrase):** <question>Peter has 8 coins, of which he knows that 7 are genuine and weigh the same, while one is fake and differs in weight, though he does not know whether it is heavier or lighter. Peter has access to balance scale, which shows which side is heavier but not by how much. For each weighing, Peter must pay Vasya one of his coins before the weighing. If Peter pays with genuine coin, Vasya will provide an accurate result; if fake coin is used, Vasya will provide random result. Peter wants to determine 5 genuine coins and ensure that none of these genuine coins are given to Vasya. Can Peter guaranteedly achieve this?</ question> <ground_truth_answer>Petya can guarantee finding 5 genuine coins.</ground_truth_answer> <equivalent_answer_1>Yes, Peter can guarantee finding 5 genuine coins while ensuring that none of these genuine coins are paid to Vasya.</equivalent_answer_1> ## Input <question> {{QUESTION}} </question> <ground_truth_answer> {{GROUND_TRUTH_ANSWER}} </ground_truth_answer> ## Output Please generate at least 5 mathematically equivalent answers to the ground truth answer. Each answer should be placed inside tags like <equivalent_answer_1>...</equivalent_answer_1>, < equivalent_answer_2>...</equivalent_answer_2>, etc. Figure 18: Prompt Template for Generating Synthetic FN Examples"
        }
    ],
    "affiliations": [
        "University of Washington",
        "Western Washington University"
    ]
}