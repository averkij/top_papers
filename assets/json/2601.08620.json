{
    "paper_title": "ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios",
    "authors": [
        "António Loison",
        "Quentin Macé",
        "Antoine Edy",
        "Victor Xing",
        "Tom Balough",
        "Gabriel Moreira",
        "Bo Liu",
        "Manuel Faysse",
        "Céline Hudelot",
        "Gautier Viaud"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore."
        },
        {
            "title": "Start",
            "content": "ViDoRe V3: Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios António Loison* Quentin Macé1,3 Antoine Edy1 Victor Xing1 Tom Balough2 Gabriel Moreira2 Bo Liu2 Manuel Faysse3 Céline Hudelot3 Gautier Viaud1 1Illuin Technology 2NVIDIA 3CentraleSupélec, Paris-Saclay {antonio.loison, quentin.mace, antoine.edy}@illuin.tech 6 2 0 J 3 1 ] . [ 1 0 2 6 8 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe V3, comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising 26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking and substantially improve performance, hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under commercially permissive license1."
        },
        {
            "title": "Introduction",
            "content": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2021) has become the dominant paradigm for knowledge-intensive NLP tasks (Gao et al., 2024; Fan et al., 2024). Yet practical deployments introduce complexities that academic benchmarks often overlook when focusing on single- *Equal contribution Work done while at Illuin Technology 1https://hf.co/vidore 1 Figure 1: ViDoRe V3 sample. Each query is annotated with the relevant pages, document-grounded answer, bounding boxes localizing supporting evidence and modality labels for each bounding box. Documents are provided in image, text and PDF formats. document textual retrieval. First, documents encode critical information in visual elements such as tables, charts, and images designed for human interpretation, which text-only pipelines often ignore (Abootorabi et al., 2025; Cho et al., 2024a). Second, user queries often require open-ended synthesis, comparison, and reasoning over scattered information, not simple factoid lookup (Tang and Yang, 2024; Thakur et al., 2025; Conti et al., 2025). Third, trustworthy systems must ground responses to specific source locations (e.g., bounding boxes), to mitigate hallucinations (Gao et al., 2023; Ma et al., 2024b). Existing benchmarks leave these requirements only partially addressed. Early Visual Document Understanding (VDU) benchmarks focus on singlepage comprehension, ignoring the complexity of large document corpora (Mathew et al., 2021b). Recent retrieval-centric benchmarks do not evaluate generation quality and grounding (Faysse et al., 2025; Günther et al., 2025). Some multimodal datasets attempt to bridge this gap but rely on extractive, short-answer tasks that fail to exercise complex reasoning (Cho et al., 2024b), or lack multilingual diversity and fine-grained visual grounding (Peng et al., 2025). To address these limitations, we introduce ViDoRe V3, benchmark designed for complex and realistic end-to-end RAG evaluation on visually rich document corpora. Our contributions are: 1. Human Annotation Methodology for Realistic Queries We propose an annotation protocol for generating diverse queries and fine-grained query-page annotations. By restricting annotator access to document content during query formulation, we capture authentic search behaviors and mitigate bias toward simple extractive queries. VisionLanguage Model (VLM) filtering combined with human expert verification enables efficient, highquality annotation at scale. 2. The ViDoRe V3 Benchmark Applying this methodology to 10 industry-relevant document corpora, we build ViDoRe V3, multilingual RAG benchmark comprising 26,000 pages and 3,099 queries, each available in 6 languages. Two datasets are held out as private test set to mitigate overfitting. The benchmark is fully integrated into the MTEB ecosystem and leaderboard2 (Muennighoff et al., 2023), and the public datasets are released under commercially permissive license. 3. Comprehensive Evaluation and Insights Leveraging our granular annotations, we benchmark state-of-the-art models on (i) retrieval accuracy by modality and language, (ii) answer quality across diverse retrieval pipeline configurations, and (iii) visual grounding fidelity. Our analysis surfaces actionable findings for RAG practitioners."
        },
        {
            "title": "2 Related Work",
            "content": "Component-Level Benchmarks (VDU and Retrieval) VDU has traditionally relied on singlepage datasets like DocVQA (Mathew et al., 2021b), alongside domain-specialized variants (Mathew et al., 2021a; Zhu et al., 2022; Wang et al., 2024). These ignore the multi-page context inherent to RAG. Recent work evaluating bounding-box source grounding (Yu et al., 2025b) proposes singlepage and multi-page tasks but does not address the 2https://mteb-leaderboard.hf.space retrieval component. Conversely, the emergence of late-interaction visual retrievers (Ma et al., 2024a; Faysse et al., 2025; Yu et al., 2025a; Xu et al., 2025) spurred the creation of retrieval-centric visual benchmarks like Jina-VDR (Günther et al., 2025) and ViDoRe V1&V2 (Faysse et al., 2025; Macé et al., 2025), but none of these benchmarks jointly evaluate retrieval and answer generation. End-to-End Multimodal RAG While recent textual RAG benchmarks now capture complex user needs like reasoning or summarizing (Thakur et al., 2025; Tang and Yang, 2024; Su et al., 2024), multimodal evaluation often remains limited to single page queries (Faysse et al., 2025). Multi-page datasets like DUDE (Van Landeghem et al., 2023), M3DocRAG (Cho et al., 2024a), ViDoSeek (Wang et al., 2025) or Real-MM-RAG (Wasserman et al., 2025) prioritize extractive retrieval, lacking the diversity of queries encountered in realistic settings. UniDocBench (Peng et al., 2025) represents concurrent effort that similarly addresses diverse query types and provides comparative evaluation across multiple RAG paradigms. While this benchmark offers valuable contributions, it relies on synthetically generated queries via knowledge-graph traversal, is restricted to English documents, and constrains grounding annotations to parsed document elements. In contrast, our benchmark offers several complementary strengths: fully human-verified annotations, multilingual coverage, free-form bounding box annotations, and more systematic evaluation of individual visual RAG pipeline components."
        },
        {
            "title": "3 Benchmark Creation",
            "content": "We design the benchmark to mirror the diversity of information retrieval situations in large-scale realistic environments. To enable pipeline-agnostic evaluation of the 3 core RAG components (retrieval, generation and grounding), while avoiding limitations of synthetic benchmarks, we employ rigorous three-stage human-in-the-loop annotation process involving document collection, query generation and grounded query answering (Figure 2)."
        },
        {
            "title": "3.1 Document Collection",
            "content": "We curate 10 diverse corpora by manually selecting openly-licensed documents from governmental, educational, and industry sources, focusing on English and French documents (7 and 3 corpora respectively). The corpora span Finance, Computer Science, Energy, Pharmaceuticals, Human 2 Figure 2: Overview of the benchmark creation process. Queries are sourced from 3 streams: human extractive (using raw pages), human blind contextual (using summaries to mitigate extractive bias), and synthetic blind contextual. For each query, VLM pre-filtered subset of candidate pages is labeled by 13 human annotators that perform relevance scoring, bounding box localization and answer generation. final response aggregation combines annotator answers into single answer. Resources, Industrial Maintenance, Telecom, and Physics. Each features domain-specific terminology and document structures representative of realistic retrieval tasks (details in Table 6)."
        },
        {
            "title": "3.2 Query Generation",
            "content": "Query Taxonomy To evaluate document visual retrieval systems across diverse realistic scenarios, we develop query taxonomy with two orthogonal dimensions: Query Type, defining the users information need, and Query Format, describing the querys syntactic structure. This dual-axis classification enables more nuanced performance analysis than benchmarks focusing solely on interrogative extractive queries. We define 7 Query Types: openended, extractive, numerical, multi-hop, comparecontrast, boolean, and enumerative, and 3 Query Formats: question, keyword, and instruction. Context Preparation We further ensure query diversity by pulling summaries from heterogeneous set of contexts during the generation process. Two types of input contexts are used: specific document sections that target local information retrieval and cross-section summaries that target multi-document context retrieval. These summaries are produced through refined process inspired by ViDoRe V2 (Macé et al., 2025). First, the text is extracted from PDFs using Docling (Auer et al., 2024) along with image descriptions. Then, summaries are generated with Qwen3235B-Instruct (Qwen Team, 2025) from each document section. They are clustered to group similar summaries together using Qwen3-Embedding0.6B (Zhang et al., 2025) as embedder, UMAP (McInnes et al., 2020) for dimension reduction and HDBSCAN (Campello et al., 2013) for clustering. Additionally, cross-section summaries are produced by synthesizing the summaries of 2 to 3 randomly selected sections per cluster. From this pool of summaries, final subset is curated to maintain strict balance between single-section and cross-section summaries. The selection also 3 ensures an even distribution across section modalities (text, images, and tables) as defined by the Docling element classification. Synthetic Query Generation Queries are generated from the summaries using first synthetic generation pipeline based on Qwen3-235B. For each summary, prompt is constructed by sampling query type and format at random, together with variable attributes such as length and difficulty, in order to promote diversity. The generated queries are subsequently evaluated by the same LLM acting as an automatic judge, which filters outputs according to 4 criteria: information richness, domain relevance, clarity and adherence to query type/format. Finally, 50% of the retained queries are rephrased to further enhance linguistic variance. This pipeline is implemented using NeMo Data Designer (NeMo Data Designer Team, 2025) to facilitate generation scaling. Human Query Writing Human annotators are provided 2 kinds of contexts: synthetic summaries or specific PDF pages. They are tasked with generating one query following specific query type and format and one query of their choice that is most adapted to the context provided."
        },
        {
            "title": "3.3 Answer Detection and Generation",
            "content": "Queries are filtered and linked to relevant pages using hybrid pipeline of VLM pre-filtering and human annotation. It is followed by human answer annotation and visual grounding. Query-Page Linking Given the scale of our corpora, manual verification of each page relevance for each query is intractable. We therefore adopt two-stage annotation pipeline. First, Qwen2.5-VL32B-Instruct (Bai et al., 2025) pre-filters candidate pages by assessing whether each page image is relevant to the query. Queries whose answers span more than 30 flagged pages are discarded. Human annotators then review the remaining query-page pairs, evaluating query quality and rating page relevance on three-point scale (Not Relevant, Critically Relevant, Fully Relevant). Relevant Page Selection To ensure annotation quality, each task is completed by multiple annotators and reviewed by annotation supervisors. Since VLM pre-filtering biases the distribution toward relevant pages, we report Gwets AC2, as it remains stable under prevalence skew, at 0.760 (see Section for dataset-level breakdowns). Given this strong but imperfect agreement, we implement tiered review process: extractive queries require at least one annotator and one reviewer, while more complex non-extractive queries require at least two annotators and one reviewer. page is retained as relevant if marked by either (i) one annotator and one reviewer, or (ii) at least two annotators. Answer Generation For each selected query, annotators were tasked with writing an answer based on the pages they marked as relevant. Given that different annotators might have different answer interpretations and tend not to be exhaustive in their answers, we use Qwen2.5-VL-32B-Instruct to generate final answer based on the relevant page images marked by the annotators and their answers. Bounding Boxes and Modality Types For each relevant page, annotators delineate bounding boxes around content supporting the query and attribute modality type to each bounding box: Text, Table, Chart, Infographic, Image, Mixed or Other. Because multiple valid interpretations of bounding boxes can exist, we perform consistency study to evaluate inter-annotator agreement and establish human performance upper bound for the task. We compute inter-annotator agreement on the subset of query-page pairs labeled by two or three annotators. For each annotator, we merge all their bounding boxes into single zone. We then compare zones across annotators by measuring pixellevel overlap, reporting Intersection over Union (IoU) and F1 score (Dice coefficient). When three annotators label the same sample, we average over all pairwise comparisons. Across all 10 datasets, we observe an average IoU of 0.50 and F1 of 0.60. These moderate agreement scores reflect the inherent subjectivity of the task: annotators typically agreed on the relevant content but differed in granularity (Appendix I), with some marking tight bounds around specific content while others included surrounding context. Quality Control The annotation was conducted by curated pool of 76 domain-qualified experts with native-level language proficiency. Quality control was performed by 13 senior annotators with enhanced domain knowledge and extensive annotation experience. Detailed protocols regarding the annotator pool and training are provided in Appendix C."
        },
        {
            "title": "3.4 Final Query Distribution",
            "content": "We conducted final human review to remove lowquality queries and resolve labeling ambiguities. Figure 3 shows the resulting distribution. Extractive queries predominate due to human annotator preference, followed by open-ended queries from targeted sampling. Multi-hop queries were the hardest to scale, suggesting need for dedicated pipelines. Figure 4 details page modalities; while text is most prevalent, visual elements like tables, charts, and infographics are well-represented. mitigate data contamination (which was shown to be major preoccupation for Information Retrieval (Liu et al., 2025)), we adopt split-release strategy. 8 datasets are made public to facilitate research, while 2 are retained as private hold-out sets. This enables blind evaluation, ensuring that performance metrics reflect true generalization rather than overfitting to public samples."
        },
        {
            "title": "4 Experiments and Results",
            "content": "Using our benchmark, we conduct extensive evaluations across all 3 components of RAG pipelines. We assess textual and visual retrievers and rerankers on retrieval performance, evaluate leading VLMs and LLMs on their ability to generate accurate answers from various retrieved contexts, and test VLMs on bounding box generation for visual grounding. From these results, we compile practical insights for RAG practitioners."
        },
        {
            "title": "4.1 Retrieval",
            "content": "Figure 3: Query Type Distribution per Domain We evaluate large panel of visual and textual retrievers on page-level retrieval ability. Visual retrievers are given page images, while textual retrievers process the Markdown text of each page processed by the NeMo Retriever extraction service3 (NVIDIA Ingest Development Team, 2024). The results reported in Table 1 corroborate findings from existing document retrieval benchmarks (Faysse et al., 2025; Günther et al., 2025): for given parameter count, visual retrievers outperform textual retrievers, and late interaction methods score higher than dense methods. analyze ColEmbed-3B-v2, the bestperforming retriever we evaluated across query type, content modality, and query language. We Figure 4: Content Type Distribution per Domain"
        },
        {
            "title": "3.5 Dataset Release and Distribution",
            "content": "We extend the benchmark to rigorously assess cross-lingual retrieval. While source documents are maintained in English and French, we use Qwen3-235B-Instruct to provide translations in 6 languages: English, French, Spanish, German, Italian, and Portuguese. This configuration challenges models to bridge the semantic gap between the query language and the document language, critical requirement for modern RAG systems. Finally, to ensure the integrity of evaluation and Performance is aligned with query complexity Figure 5 shows that performance is inversely correlated with query complexity: simple query types such as Boolean and Numerical score significantly higher than Open-ended and Multi-hop queries. Question formulations consistently outperform Instruction and Keyword formats across nearly all categories, underscoring the need for improved handling of these query structures. Visual Content and multi-page queries are hardest for retrievers Figure 6 highlights that queries 3Chunking within pages or providing image descriptions did not improve our results. Thus, we report the results of the simplest pipeline."
        },
        {
            "title": "Model",
            "content": "Size (B) C.S. Nucl. Fin. Phar. H.R. Ind. Tel. Phys. Ener. Fin. Avg."
        },
        {
            "title": "French Datasets",
            "content": "Textual Retrievers Qwen3-8B Jina-v4 LFM2-350M Qwen3-0.6B BGE-M3 BM25S Visual Retrievers ColEmbed-3B-v2 Jina-v4 ColNomic-7B ColEmbed-3B ColNomic-3B ColEmbed-1B ColQwen2.5 Nomic-7B ColQwen2 Nomic-3B ColPali 8 3 0.35 0.6 0.57 - 3 3 7 3 3 1 3 7 2 3 7 71.7 64.3 63.5 66.4 58.0 28.7 77.1 71.8 76.2 75.2 72.7 71.3 72.3 66.6 68.6 58.5 65. 39.0 44.3 37.8 32.8 30.2 17.4 50.7 50.0 45.0 49.1 42.1 47.3 38.1 36.7 35.7 32.2 32.9 49.4 48.4 39.0 42.7 39.8 17.6 64.2 59.3 56.6 60.9 56.3 58.9 52.3 48.8 39.0 44.2 34.4 59.2 54.9 56.4 50.6 52.0 27.3 66.0 63.1 62.3 63.7 61.1 62.6 57.9 58.9 52.2 55.3 53. 47.6 52.8 43.5 37.7 42.4 12.8 62.3 59.5 58.7 58.7 57.3 57.0 51.2 46.2 45.1 43.3 44.8 40.4 38.4 34.4 31.6 28.5 15.6 51.7 50.4 50.1 47.1 47.4 46.6 41.3 37.9 38.3 33.2 35.6 62.8 56.3 56.9 55.7 51.6 33.3 69.7 64.8 67.2 67.0 64.5 64.7 61.3 57.8 57.4 53.7 54. 45.6 43.6 41.8 43.3 35.9 14.8 47.0 46.6 48.3 45.1 47.5 44.1 45.9 44.2 41.6 42.0 41.7 58.9 60.1 47.0 51.3 49.8 21.9 64.9 64.0 64.0 62.1 65.0 60.9 59.7 57.5 48.8 51.4 47.1 35.8 41.3 28.2 25.8 25.2 14.0 44.4 46.1 45.5 43.8 44.3 42.4 39.1 36.0 20.0 28.9 21. 51.0 50.4 44.9 43.8 41.3 20.3 59.8 57.6 57.4 57.3 55.8 55.6 51.9 49.0 44.7 44.3 43.1 Table 1: Retrieval performance (NDCG@10) across the benchmark. Best results per category in bold. : single-vector models. Following MTEB conventions, the average score is macro-average over all datasets. Full model names and references are found in Table 8. Figure 5: ColEmbed-3B-v2 NDCG@10 by query type and format. Figure 6: ColEmbed-3B-v2 NDCG@10 by modality. involving visual content like tables or images tend to be more difficult. The Mixed content type scores the lowest, which suggests that integrating information across different modalities within single page remains challenge. Additionally, we observe consistent decline in performance as the number of annotated pages increases (Figure 7), suggesting that retriever effectiveness decreases when aggregating information from multiple sources is required. Figure 7: ColEmbed-3B-v2 NDCG@10 by number of annotated pages. Cross-language queries degrade performance Retrieval performance is 23 points higher in monolingual settings (Table 9 and Table 10) than crosslingual settings  (Table 1)  , showing that models need to better adapt to these settings. Textual rerankers outperform visual ones We evaluate the impact of adding reranker to the textual and visual pipelines of the Jina-v4 retriever. We select zerank-2 (Zero Entropy, 2025) and jina-reranker-m0 (Jina AI, 2025) as two of the leading textual and visual rerankers to date."
        },
        {
            "title": "Model",
            "content": "C.S. Nucl. Fin. Phar. H.R. Ind. Tel. Phys. Ener. Fin. Avg."
        },
        {
            "title": "French Datasets",
            "content": "Textual pipeline Jina-v4textual + zerank-2 Visual pipeline Jina-v4visual + jina-reranker-m0 64.3 82.1 17.8 44.3 53.5 9.2 48.4 69. 20.8 54.9 66.2 11.3 52.8 66.5 13.7 38.4 53. 14.8 56.3 71.5 15.2 43.6 48.2 4.6 60.1 71.5 11. 41.3 53.7 12.4 50.4 63.6 13.2 71.8 76.7 4.9 50.0 50.8 0. 59.3 59.2 0.1 63.1 65.4 2.3 59.5 56.0 3.5 50.4 50.9 0.5 64.8 70.8 6.0 46.6 46.9 0. 64.0 61.7 2.3 46.1 39.8 6.3 57.6 57.8 0.2 Table 2: Retrieval performance (NDCG@10) of retriever + reranker pipelines. Results in Table 2 reveal significant disparity in reranking efficacy between modalities. While the visual retriever initially outperforms the textual base, the textual reranker yields substantial gains (+13.2 NDCG@10), enabling the textual pipeline to achieve the highest overall retrieval performance. In contrast, the visual reranker provides only marginal average improvement of +0.2 and degrades performance in 4 datasets, underscoring the need for better multilingual visual rerankers."
        },
        {
            "title": "4.2 Final Answer Generation",
            "content": "We evaluate end-to-end answer quality by providing LLMs and VLMs with queries and their corresponding retrieved pages, examining the effects of retrieval pipeline selection, context modality, and generation model choice  (Table 3)  . For this evaluation, we use the best-performing textual and visual retrieval pipelines. We additionally establish an upper bound using an oracle pipeline that supplies the model with ground-truth annotated pages. In the hybrid configuration, we concatenate the top-5 results from the visual retriever (images) with the top-5 results from the textual retriever (text), without removing duplicates; the retrieval performance is detailed in Table 11. We also consider hybrid oracle setup, which provides the model with all the ground-truth pages in both modalities. The correctness of generated answers is assessed against the ground truth final answer by an LLM judge (details in Appendix H). Private datasets are omitted to maintain their integrity. Some benchmark queries involve general knowledge manageable by LLMs without retrieval. To prevent memorization from confounding our assessment of the RAG pipeline, we stratify queries by difficulty based on parametric knowledge. query is categorized as easy if any model in 6LLM panel answers it correctly without context; otherwise, it is labeled hard. Overall, 48.6 % of queries are easy (see Table 16 for details). Visual context helps generation With fixed Gemini 3 Pro generator, image-based context outperforms text-based context on the hard subset by 2.4 and 2.8 percentage points for the oracle and ColEmbed-3B-v2 pipelines, respectively  (Table 3)  . This confirms that preserving the visual content of document pages provides better grounding for complex answer generation. Hybrid retrieval yields the best performance on challenging queries The hybrid pipeline achieves 54.7 % accuracy on hard queries, surpassing both the strongest textual (52.1 %) and visual (54.5 %) baselines. This complementary effect suggests that text and image representations capture different aspects of document content, and their combination can provide more robust evidence for downstream generation. Hard queries expose the limits of parametric knowledge in current models Even with oracle context, performance on hard queries lags behind easy queries by more than 10 percentage points. This gap suggests that the multi-step reasoning and long-context synthesis required for difficult queries remain challenging for current models. While the models we evaluate achieve comparable overall scores, their relative ranking may shift when parametric knowledge is less of an advantage, as shown by GPT 5.2 outperforming Gemini 3 Pro on easy queries but trailing on hard ones. ViDoRe V3 leaves significant room for future retriever improvements The 10-point gap between the best non-oracle result (54.7 %) and the image oracle (64.7 %) on hard queries underscores substantial opportunities for improving the retrieval pipeline. Moreover, even with oracle contexts, Gemini 3 Pro performance remains modest, indi7 Retrieval pipeline Oracle Context modality Generation model Text Image Hybrid Gemini 3 Pro Jina-v4text. + zerank-2 Text Gemini 3 Pro 80.9 66.0 English Datasets French Datasets C.S. Fin. Phar. H.R. Ind. Phys. Ener. Fin. 80.9 86.5 86. 70.2 70.6 68.9 71.4 76.1 73.4 59.9 72.3 71.1 70.4 63.2 66.4 68.2 65. 60.4 71.2 74.5 69.2 69.2 69.2 69.8 69.5 64.9 62.8 64.1 62. 54.7 Avg. Hard 62.3 64.7 63.4 52.1 Avg. Easy 79.3 79.7 77. 75.5 Avg. Global 70.6 72.6 70.7 64.9 Jina-v4text. + zerank-2 & ColEmbed-3B-v2 ColEmbed-3B-v Hybrid Gemini 3 Pro 85.1 65.0 65.9 64. 59.4 69.9 62.7 52.8 54.7 76. 65.7 Text Image Gemini 3 Pro Kimi K2 82.3 81.4 83.3 Gemini 3 Pro 80.9 Gemini 3 Flash 86.5 GPT-5.2 Qwen3-VL-235B 86. 62.5 56.6 67.3 64.1 59.5 59.9 61.0 59.1 62.9 63.5 68.1 64.0 62.9 55.7 65.4 63.8 66.0 60. 56.2 55.8 57.2 55.1 61.5 57.2 64.9 73.8 67.9 68.2 76.5 71.9 62.3 60.4 64.3 63.3 66.2 59. 49.4 43.1 47.8 47.8 49.1 44.4 51.7 44.6 54.5 50.3 54.1 51.0 73.0 74.3 74.1 74.4 78.1 74. 62.7 60.7 64.5 63.3 66.7 63.0 Table 3: End-to-end evaluation of final answer generation. We report the percentage of correct final answers as determined by an LLM judge across the 8 public datasets. \"Oracle\" rows represent the upper-bound performance using gold-standard contexts. Average Easy and Average Hard denote performance stratified by query difficulty. For each column, the best result is bolded and the best non-oracle result is underlined. cating that generation models still struggle to fully exploit the provided information."
        },
        {
            "title": "4.3 Visual Grounding",
            "content": "Beyond generating correct answers, it is highly desirable for RAG pipelines to identify where in the source documents the answer originates, enabling users to verify the grounding of the query answer. We therefore evaluate the ability of LLMs to generate accurate bounding boxes within their final answer. Among the few LLM families with visual grounding capabilities, we select Qwen3VL-30B-A3B-Instruct and Gemini 3 Pro for evaluation. For each query, we provide the model with the candidate pages shown to the human annotators and prompt it to answer the query while inserting inline bounding boxes in XML format <bboxes image=\"N\"> ... </bboxes> to delimit relevant content (full instructions in Appendix G). We use the bounding boxes produced by the human annotators as our ground truth. Since each query may have 13 human annotators, we evaluate VLM predictions independently against each annotator using the same zone-based methodology as the inter-annotator consistency analysis (Section 3.3), and report the highest F1 score. This best-match strategy reflects the inherent subjectivity of evidence selection: annotators may legitimately highlight different regions to support the same answer, and model should not be penalized for matching any valid interpretation. Visual grounding lags human performance Inter-annotator agreement on evidence localization reaches an F1 of 0.602, whereas the bestperforming models achieve markedly lower scores: 0.089 for Qwen3-VL-30B-A3B-Instruct and 0.065 for Gemini 3 Pro. page-level analysis  (Table 4)  reveals that on pages where humans provided bounding boxes, both models annotated the same page only 1617 % of the time, while 26 27 % of human-annotated pages received no model annotation at allhighlighting recall as the primary bottleneck. Detailed per-domain results and qualitative analysis appear in Appendix and I. Category Outcome Qwen3-VL-30B-A3B Gemini 3 Pro Agreement Both annotated Neither annotated Discrepancy Model only Human only 17 % 46 % 10 % 26 % 16 % 49 % 7 % 27 % Table 4: Page-level bounding box agreement between models and human annotators. Each page is classified by whether the model and human both annotated it, both left it unannotated, or only one provided annotations."
        },
        {
            "title": "5 Conclusion",
            "content": "This work introduces ViDoRe V3, multilingual, human-annotated RAG benchmark that evaluates retrieval, final answer generation, and visual grounding on large industry-relevant document corpora. We design human-in-the-loop annotation methodology, deployed in 12,000-hour annotation campaign, that produces diverse realistic queries paired with relevant pages, bounding boxes, and reference answers. Evaluating state-of-the-art RAG pipelines, we find that visual retrievers outperform textual ones, late interaction and textual reranking yield substantial gains, and visual con8 text improves answer generation quality. Looking ahead, ViDoRe V3 highlights several concrete research directions for practical multimodal RAG. Retriever models still struggle on cross-lingual and open-ended queries requiring visual interpretation, while VLMs need improvement in answer generation from multi-page contexts as well as accurate visual grounding. To drive progress in multimodal RAG, ViDoRe V3 has been integrated into the MTEB leaderboard, offering rigorous framework that fosters the creation of more robust document understanding systems."
        },
        {
            "title": "Limitations",
            "content": "Language coverage While our benchmark is multilingual, it is restricted to English and French source documents and queries in 6 high-resource Western European languages. Future iterations of the benchmark should include more diverse set of language families and non-Latin scripts to mitigate this bias. Document distribution bias Our benchmark focuses on publicly available long-form document corpora, representing one specific mode of existing document distribution. For example, enterprise RAG may need to handle wider variety of document types, often in private repositories, that include noisy, short-form types such as emails, support tickets, or scanned handwritten notes that are not represented in our source documents. Human annotation Annotations for open-ended reasoning and visual grounding inherently contain degree of subjectivity. We acknowledge that for complex exploratory queries, multiple valid retrieval paths and answer formulations may exist outside of our annotated ground truths."
        },
        {
            "title": "Ethical considerations",
            "content": "Annotator Welfare and Compensation Human annotation was conducted by the creators of the benchmark and single external annotation vendor. Multiple established vendors were evaluated with respect to the annotation protocol and relevant ethical considerations, and one vendor was selected based on demonstrated compliance with these criteria. Annotators were recruited from the vendors existing workforce in accordance with the demographic requirements described in the Annotator Pool and Selection section (Section C) and were compensated at rates designed to provide fair pay based on geographic location and required skill sets. The data were curated such that annotators were not exposed to harmful or offensive content during the annotation process. The use of human annotators was limited to standard annotation and verification tasks for benchmark construction and did not constitute human-subjects research; accordingly, the data collection protocol was determined to be exempt from formal ethics review. Data Licensing and Privacy All documents included in the benchmark were manually selected from governmental, educational, and enterprise websites that met open license criteria. The annotations were collected in order not to contain any private or personally identifiable information and are GDPR-compliant. The benchmark is released under commercially permissive license to facilitate broad research adoption while respecting the intellectual property rights of original document creators. Linguistic and Geographic Bias We acknowledge that our benchmark is restricted to English and French source documents and queries in 6 highresource Western European languages. This limitation may inadvertently favor RAG systems optimized for these languages and does not reflect the full diversity of practical document retrieval scenarios globally. We encourage future work to extend evaluation to underrepresented language families and non-Latin scripts. Environmental Impact The creation of this benchmark required substantial computational resources for VLM pre-filtering, synthetic query generation, and model evaluation. We report these costs to promote transparency: approximately 12,000 hours of human annotation effort and extensive GPU compute for model inference across our evaluation suite. Specifically, the compute totaled 3,000 hours on NVIDIA H100 GPUs on low emission energy grid, with an estimated environmental impact of 200 kg CO2e."
        },
        {
            "title": "Detailed Contributions",
            "content": "Benchmark Design Loison, Macé, Edy, Moreira and Liu designed the benchmark. Data and Annotation Loison and Macé developed the synthetic data generation pipeline. Loison generated the queries, while Macé predicted links between queries and pages. Loison, Macé, and 9 Balough defined annotation guidelines; Balough coordinated the annotation campaign. Macé and Edy managed final answer merging. Loison, Macé, Edy, Xing, and Balough reviewed the final annotations. Evaluation Macé, Edy and Loison conceptualized the evaluations. Macé and Loison worked on retrieval evaluation, with Moreira focusing on the evaluation of ColEmbed models. Edy led the end-to-end evaluation, reranking analysis, and visualization. Macé and Edy integrated the results into the MTEB leaderboard. Xing led bounding box evaluations and result analysis. Writing and Supervision The manuscript was written by Loison, Macé, Xing, and Edy. Senior supervision and strategic guidance were provided by Xing, Faysse, Liu, Hudelot, and Viaud, with Faysse closely advising on project direction and planning."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was conducted with contributions from NVIDIA. We thank all the people that allowed this work to happen, in particular Eric Tramel, Benedikt Schifferer, Mengyao Xu and Radek Osmulski, Erin Potter and Hannah Brandon. Crucially, we thank the dedicated team of annotators for their essential efforts. It was carried out within the framework of the LIAGORA \"LabCom\", joint laboratory supported by the French National Research Agency (ANR) and established between ILLUIN Technology and the MICS laboratory of CentraleSupelec. The benchmark was partially created using HPC resources from IDRIS with grant AD011016393."
        },
        {
            "title": "References",
            "content": "Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin Asgari. 2025. Ask in any modality: comprehensive survey on multimodal retrieval-augmented generation. Preprint, arXiv:2502.08826. Christoph Auer, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Nikolaos Livathinos, Panos Vagenas, Cesar Berrospi Ramis, Matteo Omenetti, Fabian Lindlbauer, Kasper Dinkla, and 1 others. 2024. Docling technical report. arXiv preprint arXiv:2408.09869. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others. 2025. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923. Ricardo JGB Campello, Davoud Moulavi, and Jörg Sander. 2013. Density-based clustering based on hierarchical density estimates. In Pacific-Asia conference on knowledge discovery and data mining, pages 160172. Springer. Antoine Chaffin. 2025. Gte-moderncolbert. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. BGE M3Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through SelfKnowledge Distillation. arXiv preprint. Version Number: 3. Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, and Mohit Bansal. 2024a. M3docrag: Multi-modal retrieval is what you need for multiPreprint, page multi-document understanding. arXiv:2411.04952. Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, and Mohit Bansal. 2024b. M3docrag: Multimodal retrieval is what you need for multi-page arXiv preprint multi-document understanding. arXiv:2411.04952. Max Conti, Manuel Faysse, Gautier Viaud, Antoine Bosselut, Céline Hudelot, and Pierre Colombo. 2025. Context is gold to find the gold passage: Evaluating and training contextual document embeddings. Preprint, arXiv:2505.24782. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. survey on rag meeting llms: Towards retrieval-augmented large language models. Preprint, arXiv:2405.06211. Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. 2025. Colpali: Efficient document retrieval with vision language models. Preprint, arXiv:2407.01449. Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations. Preprint, arXiv:2305.14627. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-augmented generation for large language models: survey. Preprint, arXiv:2312.10997. Michael Günther, Saba Sturua, Mohammad Kalim Akram, Isabelle Mohr, Andrei Ungureanu, Sedigheh Eslami, Scott Martens, Bo Wang, Nan Wang, and jina-embeddings-v4: Universal Han Xiao. 2025. 10 embeddings for multimodal multilingual retrieval. Preprint, arXiv:2506.18902. Jina AI. 2025. jina-reranker-m0: Multilingual multimodal document reranker. Accessed: 2025-12-22. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-augmented generation for knowledgeintensive nlp tasks. Preprint, arXiv:2005.11401. Liquid AI. 2025. Lfm2 technical report. arXiv preprint arXiv:2511.23404. Frank Liu, Kenneth Enevoldsen, Roman Solomatin, Isaac Chung, Tom Aarsen, and Zoltán Fodi. 2025. Introducing rteb: new standard for retrieval evaluation. Xing Han Lù. 2024. Bm25s: Orders of magnitude faster lexical search via eager sparse scoring. Preprint, arXiv:2407.03618. Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. 2024a. Unifying multimodal retrieval via document screenshot embedding. Preprint, arXiv:2406.11251. Xueguang Ma, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Wenhu Chen, and Jimmy Lin. 2024b. Visa: Retrieval augmented generation with visual source attribution. Preprint, arXiv:2412.14457. Quentin Macé, António Loison, and Manuel Faysse. 2025. Vidore benchmark v2: Raising the bar for visual retrieval. Preprint, arXiv:2505.17166. Andrés Marafioti, Orr Zohar, Miquel Farré, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, Vaibhav Srivastav, Joshua Lochner, Hugo Larcher, Mathieu Morlon, Lewis Tunstall, Leandro von Werra, and Thomas Wolf. 2025. Smolvlm: Redefining small and efficient multimodal models. Preprint, arXiv:2504.05299. Minesh Mathew, Viraj Bagal, Rubèn Pérez Tito, Dimosthenis Karatzas, Ernest Valveny, and C. Jawahar. 2021a. Infographicvqa. Preprint, arXiv:2104.12756. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. 2021b. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209. Leland McInnes, John Healy, and James Melville. 2020. Umap: Uniform manifold approximation and projection for dimension reduction. Preprint, arXiv:1802.03426. Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2023. Mteb: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 20142037. NeMo Data Designer Team. 2025. Nemo data designer: framework for generating synthetic data from scratch or based on your own https://github.com/NVIDIA-NeMo/ seed data. DataDesigner. GitHub Repository. Nomic Team. 2025. Nomic embed multimodal: Interleaved text, image, and screenshots for visual document retrieval. NVIDIA Ingest Development Team. 2024. NVIDIA Ingest: An accelerated pipeline for document ingestion. Xiangyu Peng, Can Qin, Zeyuan Chen, Ran Xu, Caiming Xiong, and Chien-Sheng Wu. 2025. Unidocbench: unified benchmark for document-centric multimodal rag. Preprint, arXiv:2510.03663. Qwen Team. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary Siegel, Michael Tang, and 1 others. 2024. Bright: realistic and challenging benchmark for reasoning-intensive retrieval. arXiv preprint arXiv:2407.12883. Rikiya Takehi, Benjamin Clavié, Sean Lee, and Aamir Shakir. 2025. Fantastic (small) retrievers and how to train them: mxbai-edge-colbert-v0 tech report. Preprint, arXiv:2510.14880. Yixuan Tang and Yi Yang. 2024. Multihop-rag: Benchmarking retrieval-augmented generation for multihop queries. Preprint, arXiv:2401.15391. Paul Teiletche, Quentin Macé, Max Conti, Antonio Loison, Gautier Viaud, Pierre Colombo, and Manuel Faysse. 2025. Modernvbert: Towards smaller visual document retrievers. arXiv preprint arXiv:2510.01149. Nandan Thakur, Jimmy Lin, Sam Havens, Michael Carbin, Omar Khattab, and Andrew Drozdov. 2025. Freshstack: Building realistic benchmarks for evaluating retrieval on technical documents. Preprint, arXiv:2504.13128. Jordy Van Landeghem, Rubèn Tito, Łukasz Borchmann, Michał Pietruszka, Pawel Joziak, Rafal Powalski, Dawid Jurkiewicz, Mickaël Coustaty, Bertrand Anckaert, Ernest Valveny, and 1 others. 2023. Document understanding dataset and evaluation (dude). In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1952819540. Qiuchen Wang, Ruixue Ding, Zehui Chen, Weiqi Wu, Shihang Wang, Pengjun Xie, and Feng Zhao. 2025. Vidorag: Visual document retrieval-augmented generation via dynamic iterative reasoning agents. arXiv preprint arXiv:2502.18017. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, and 1 others. 2024. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697. Navve Wasserman, Roi Pony, Oshri Naparstek, Adi Raz Goldfarb, Eli Schwartz, Udi Barzelay, and Leonid Karlinsky. 2025. Real-mm-rag: real-world multi-modal retrieval benchmark. arXiv preprint arXiv:2502.12342. Mengyao Xu, Gabriel Moreira, Ronay Ak, Radek Osmulski, Yauhen Babakhin, Zhiding Yu, Benedikt Schifferer, and Even Oldridge. 2025. Llama nemoretriever colembed: Top-performing text-image retrieval model. Preprint, arXiv:2507.05513. Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, and Maosong Sun. 2025a. Visrag: Vision-based retrieval-augmented generPreprint, ation on multi-modality documents. arXiv:2410.10594. Wenhan Yu, Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Lei Sha, Deguo Xia, and Jizhou Huang. 2025b. Bbox docvqa: large scale bounding box grounded dataset for enhancing reasoning in document visual question answer. Preprint, arXiv:2511.15090. Zero Entropy. 2025. Introducing zerank-2. Accessed: 2025-12-22. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. 2025. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176. Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua. 2022. Towards complex document understanding by discrete reasoning. In Proceedings of the 30th ACM International Conference on Multimedia, pages 48574866."
        },
        {
            "title": "A Dataset examples",
            "content": "Figure 8: Examples from the ViDoRe V3 datasets. Featuring varied query types and visually rich document formats across multiple domains, the benchmark captures the complexity of real-world retrieval scenarios."
        },
        {
            "title": "B Supplementary benchmark details",
            "content": "Domains Table 6 details the type of documents used in each corpus as well as several statistics. Query type and format descriptions Table 5 describes the types and formats of the queries, while Figure 9 gives details about query type intersection frequency. Query type by generation method Query type distributions by generation method (Figure 10) confirm that open-ended queries dominate synthetic queries as the synthetic pipeline attributed more weight to this type, while extractive queries dominate human-image queries since they are more naturally chosen by annotators."
        },
        {
            "title": "C Annotator pool and training details",
            "content": "Annotator Pool and Selection. Annotation was conducted by curated pool of 76 annotators who were selected based on having: (1) bachelors degree or higher in the relevant domain, (2) professional experience in the domain, (3) native-level language proficiency as required by task, and (4) prior experience with RAG, retrieval, or VQA annotation projects. Quality control was performed by 13 senior annotators with enhanced domain knowledge and extensive annotation experience, with project oversight provided by data leads with multiple years of experience in human data generation. Training and Pilot Phase. The annotation process began with comprehensive onboarding phase where annotators received task-specific training using gold-standard examples. For each domain, pilot of several hundred tasks was conducted with 100% quality control coverage and multiple annotators per task. During this phase, data leads and the research team continuously evaluated annotations, provided clarifications, and refined guidelines. Inter-annotator agreement and time-per-task baselines were calculated to establish ongoing evaluation benchmarks. The pilot concluded upon validation of both data quality and guideline effectiveness."
        },
        {
            "title": "D Supplementary agreement metrics",
            "content": "Pages were pre-filtered by VLM before human annotation; as most pages shown to annotators were likely relevant, this created skewed class distribution. This prevalence imbalance causes traditional chance-corrected metrics like Krippendorffs Alpha to appear paradoxically low even when annotators genuinely agree, as inflated expected chance agreement penalizes the score. To address this, we report 2 complementary metrics: Krippendorffs Alpha (ordinal) as the standard measure and Gwets AC2 which remains stable under prevalence skew. Overall, annotators achieved α = 0.469, AC2 = 0.760. The divergence between Alpha and AC2/Weighted Agreement is expected given the pre-filtered data and confirms substantial agreement despite the skewed distribution."
        },
        {
            "title": "E Supplementary retrieval details",
            "content": "Retriever model reference Table 8 lists the retriever models evaluated in this work, along with their HuggingFace model names and citations. Monolingual performance Tables 9 and 10 present the monolingual performance of our models, where retrieval is conducted using languagematched queries and documents for English and French, respectively."
        },
        {
            "title": "Dataset",
            "content": "α (ord) Gwets AC2 Computer science Energy Finance (EN) Finance (FR) H.R. Industrial Maintenance Telecom Nuclear Pharma Physics"
        },
        {
            "title": "Overall",
            "content": "0.467 0.463 0.514 0.320 0.413 0.496 0.464 0.389 0.478 0.213 0.469 0.809 0.714 0.798 0.736 0.793 0.740 0.772 0.794 0.755 0.334 0.760 Table 7: Inter-annotator agreement for relevance ratings by dataset. Additional Retrieval Modality Performances To evaluate the hybrid retrieval setup, we use the multimodal Jina-v4 model to generate separate visual and textual rankings. We then construct hybrid retrieval set by merging the top-5 results from each modality and removing duplicates. Because this set-union operation does not preserve strict ranking order, we report the unranked F1 score. As shown in Table 11, the hybrid approach consistently outperforms single-modality baselines. 14 Category Definition Example Query Types Open-ended Extractive Seeks explanatory or descriptive information that requires synthesis. Requires the retrieval of specific piece of information. Compare Contrast Mandates comparison between multiple entities Boolean Numerical Multi-hop Enumerative or data points. Poses yes/no question necessitating multi-step reasoning. Asks for specific quantitative value that must be derived or calculated. Requires integrating information from multiple sections or sources. Requests list of all instances sharing common property. Query Formats What drives the rise in womens workforce involvement in EU nations? Bank of America preferred stock MM dividend rate Explain the factors contributing to the reduction in R2R rates for ANDAs. Did JPMorganChase execute more than half of its planned repurchase program? percentage increase in Morgan Stanley revenue from 2023 to 2024 Summarize the steps involved in error reporting in ISMPs MERP. Specify the ISCO codes used to define domestic workers in the EU. Question An interrogative sentence. Keyword Instruction non-verbal phrase or set of terms. directive specifying task. What was Citigroups net interest margin in 2024? female employment rate European Union 2023 Identify the use case of drill point gauge. Table 5: Taxonomy of Query Types and Formats. Figure 9: UpSet plot illustrating the distribution and intersection of query types in ViDoRe V3. The horizontal bars on the left display the total count of queries for each individual type. The vertical bars at the top represent the frequency of specific combinations (intersections), as indicated by the connected dots in the matrix below. While Extractive queries are the most prevalent overall, Open Ended queries form the dominant unique category. Complex dependencies are evident in the frequent intersection of Enumerative and Extractive types, indicating substantial subset of queries requiring list-based fact retrieval. ColEmbed-3B-v2 performance breakdown Table 12 details the retrieval scores of ColEmbed3B-v2 by query language, highlighting small performance variations by language. Performance by number of annotated pages As seen in Figure 7, performance drops with the number of annotated pages. However, potential confounding factor is the correlation between query type and the number of annotated pages, since more complex query types also have higher number of annotated pages (Figure 11). We perform stratified regression analysis to isolate these two effects. We model NDCG@10 as linear function of the number of annotated pages (P ) stratified by query type. For each of the 7 query types, we fit an ordinary least squares regression: DCG@10 = + + ϵ. Results in Figure 12 and Table 13 reveal that all query types suffer significant performance 15 Corpus Domain(s) Description Lang. # Docs # Pages # Queries Main modaliFinance-EN U.S. Public Company Annual Reports Computer Science Textbooks Computer Science / Education Consists of 6 10-K annual reports from major U.S. financial institutions for the fiscal year ended December 31, 2024. Consists of two open-source, peer-reviewed textbooks from OpenStax covering foundational topics in computer science, Python, and data science. en en 6 2 ties 2942 Text, Table 1360 215 Books FDA Reports Pharmaceuticals Consists of FDA presentations en 52 2313 364 Slides, Books and Springer books (20162023) covering regulatory policies, drug development, and public health initiatives. Includes recent European Commission reports and papers on EU labour markets, social development, and employment policies. Comprises U.S. military technical orders and manuals for aircraft maintenance, safety procedures, and material handling, revised through 2025. collection of educational materials offering an interdisciplinary exploration of modern physics and complexity science. Contains the 20232024 annual reports of major French luxury companies (Dior, Hermès, Kering, LOréal, LVMH). Gathers official documents from French public agencies on energy, economic, and environmental issues in France. HR Reports from EU HR USAF Technical Orders Industrial Maintenance Physics French Physics Lectures Finance-FR Energy French Public Company Annual Reports French Governmental Energy Reports en 14 1110 Reports en 27 5244 283 Manuals fr 42 1674 302 Slides fr 5 2384 320 Reports fr 2229 308 Reports, Slides Table 6: Description of ViDoRe V3 public corpora. Number of queries is without translations Figure 10: Query type distribution by generation method."
        },
        {
            "title": "Reference",
            "content": "Zhang et al. (2025) Günther et al. (2025) Zhang et al. (2025) Liquid AI (2025) Chen et al. (2024) Lù (2024) Qwen3-Embedding-8B jina-embeddings-v4 Qwen3-Embedding-0.6B LFM2-ColBERT-350M BGE-M3 BM25S llama-nemoretriever-colembed-3b-v2 Xu et al. (2025) colnomic-embed-multimodal-7b llama-nemoretriever-colembed-3b-v1 Xu et al. (2025) colnomic-embed-multimodal-3b llama-nemoretriever-colembed-1b-v1 Xu et al. (2025) colqwen2.5-v0.2 nomic-embed-multimodal-7b colqwen2-v1.0 nomic-embed-multimodal-7b colpali-v1.3 mxbai-edge-colbert-v0-32m Qwen3-8B Jina-v4 Qwen3-0.6B LFM2-350M BGE-M3 BM25S ColEmbed-3B-v2 ColNomic-7B ColEmbed-3B ColNomic-3B ColEmbed-1B ColQwen2.5 Nomic-7B ColQwen2 Nomic-3B ColPali Mxbai Edge 32M GTE-ModernColBERT GTE-ModernColBERT-v1 ColModernVBERT ColSmol-256M Faysse et al. (2025) Nomic Team (2025) Faysse et al. (2025) Nomic Team (2025) Faysse et al. (2025) Takehi et al. (2025) Chaffin (2025) Teiletche et al. (2025) Marafioti et al. (2025) colmodernvbert colSmol-256M Nomic Team (2025) Nomic Team (2025) Table 8: Retriever reference table. Model aliases used in Tables 1, 9, and 10 are mapped to their HuggingFace model name and citation. Figure 11: Average number of annotated pages by query type. penalty as the number of annotated pages increases. Slope values are nearly uniform (a 0.024), suggesting similar drop in retrieval accuracy across most query types. The open-ended and enumerative types are the two exceptions: despite having the lowest NDCG@10 for low page counts, they also have the shallowest slope, which suggests that retrieval success on these queries is constrained by the models fundamental difficulty in synthesizing multiple relevant sources rather than the volume of relevant context. 17 Figure 12: ColEmbed-3B-v2 NDCG@10 by number of annotated pages and query type. Performance by content type NDCG@10 by content type in Table 14 show that retrieval is more challenging for visual content, with Image performing 10pp below Text. However, content type and query type are correlated in our benchmark: for instance, tables appear in numerical queries 2.2 more often than the baseline, while images are over-represented in open-ended queries (Figure 13). Since numerical queries are easier than open-ended"
        },
        {
            "title": "Model",
            "content": "Size (B) C.S. Nucl. Fin. Phar. H.R. Ind. Tele. Average Textual Retrievers Jina-v4 Qwen3-8B LFM2-350M Mxbai Edge 32M BM25S Qwen3-0.6B GTE-ModernColBERT BGE-M3 Visual Retrievers ColEmbed-3B-v2 ColEmbed-3B ColEmbed-1B Jina-v4 ColNomic-7B ColNomic-3B ColQwen2.5 Nomic-7B ColQwen2 ColPali Nomic-3B ColModernVBERT ColSmol-256M 3 8 0.35 0.03 - 0.6 0.15 0.57 3 3 1 3 7 3 3 7 2 7 3 0.25 0.25 67.3 73.5 70.6 68.0 64.7 70.5 63.6 63.6 78.6 77.8 75.5 74.2 78.2 75.5 75.2 70.9 73.5 72.5 62.1 59.7 57.4 48.2 42.2 45.4 44.4 45.9 39.7 41.7 34.3 52.9 53.4 52.2 52.4 48.2 45.5 42.9 42.3 44.1 38.1 37.2 42.0 36. 56.5 54.8 48.3 48.2 49.9 51.5 39.8 43.9 69.1 69.5 67.0 66.1 63.1 63.0 61.2 57.6 50.9 43.3 53.3 50.4 47.7 59.0 62.4 62.1 62.5 56.9 57.4 62.0 54.7 67.6 66.9 66.2 65.2 64.6 63.7 60.9 63.8 58.1 57.7 59.2 56.6 51.4 58.8 52.3 53.2 52.7 49.6 46.2 46.2 45.3 65.4 64.9 64.5 64.6 62.9 62.6 59.2 55.9 54.7 53.3 51.9 47.0 46. 45.8 45.3 47.9 47.1 45.6 42.4 44.6 39.0 56.8 57.0 56.1 55.9 54.2 52.8 49.4 48.5 49.8 47.0 41.1 43.9 38.5 61.0 66.0 63.8 61.9 58.3 59.7 59.7 54.3 71.7 69.4 68.7 68.7 69.6 68.6 65.3 62.0 63.2 59.2 57.2 55.2 47.5 56.7 56.6 55.9 55.0 53.0 52.3 51.1 47.9 66.0 65.6 64.3 63.9 63.0 61.7 59.2 57.3 56.3 53.0 51.7 50.7 46. Table 9: English-only retrieval performance (NDCG@10). : single-vector models. Results are computed on the English queries of the English datasets. Model Size (B) Phys. Ener. Fin. Average Textual Retrievers Jina-v4 Qwen3-8B Qwen3-0.6B BGE-M3 BM25S Visual Retrievers ColEmbed-3B-v2 ColNomic-7b ColNomic-3b Jina-v4 ColEmbed-3B ColEmbed-1B ColQwen2.5 Nomic-7B ColQwen2 Nomic-3B ColPali 3 8 0.6 0.57 - 3 7 3 3 3 1 3 7 3 3 7 44.0 45.8 43.8 38.3 39.8 48.2 48.5 48.5 46.8 46.6 44.7 47.8 45.6 43.9 43.6 43. 63.4 60.2 54.9 53.1 57.4 67.5 67.0 67.9 66.7 66.3 64.6 62.3 61.6 55.6 56.4 50.5 44.8 37.6 28.5 28.4 35.9 48.2 47.9 46.8 48.6 48.9 47.8 43.6 41.3 26.5 34.4 23.6 50.7 47.9 42.4 39.9 44.4 54.6 54.5 54.4 54.0 53.9 52.4 51.2 49.5 42.0 44.8 39. Table 10: French-only retrieval performance (NDCG@10). : single-vector models. Results are computed on the French queries of the French datasets. ones, we test whether the effect of content type is byproduct of query type confounding. We fit an additive model that predicts performance as the sum of independent query-type and content-type effects. Figure 14 shows the residuals which measure deviation from this baseline. We see that most residuals 18 English Datasets French Datasets Modality Visual Textual Hybrid Avg. # Pages for hybrid C.S. Nucl. Fin. Phar. H.R. Ind. Tel. Phys. Ener. Fin. Avg. 39.4 35.4 43. 6.96 25.5 23.1 27.7 7.38 28.4 24.5 30.9 7.77 27.5 24.2 29. 7.40 30.0 27.4 32.6 7.29 21.4 16.5 22.2 7.77 31.4 29.0 35. 7.09 26.6 25.8 26.5 7.26 25.2 23.9 29.8 6.97 22.9 20.4 24. 7.61 27.8 25.0 30.2 7.35 Table 11: Performance comparison of retrieval modalities (F1@10) on Jina-v4. Evaluation is performed using the multimodal retriever Jina-v4. The Hybrid method combines the top-5 visual and top-5 textual matches, subsequently removing duplicates. The final row reports the average number of unique pages remaining in the hybrid set. The Hybrid setup constantly outperforms both textual and visual retrieval. Query Language NDCG@10 English French Portuguese Spanish Italian German 60.8 59.8 59.6 59.6 59.1 57.9 Table 12: ColEmbed-3B-v2 NDCG@10 by query language. Query Type Slope Intercept Boolean Numerical Extractive Compare-contrast Enumerative Multi-hop Open-ended -0.0239 -0.0255 -0.0230 -0.0247 -0.0172 -0.0237 -0.0129 0.797 0.742 0.745 0.710 0.669 0.680 0.577 R2 0.101 0.059 0.084 0.117 0.080 0.114 0.057 Table 13: Linear regression analysis of NDCG@10 decay with number of annotated pages, by query type. The slope represents performance sensitivity to retrieval context size, while the intercept represents intrinsic difficulty at minimum context size. are below 5pp, indicating that the two factors combine additively without significant interaction. Content type NDCG@10 Content type count Text Chart Infographic Table Other Image Mixed 59.3 56.3 55.2 53.9 50.8 49.3 45.1 17244 2364 2814 6480 492 1140 1164 Table 14: ColEmbed-3B-v2 NDCG@10 by content type. Content type is labeled on each annotated page based on the nature of the query-relevant content delimited by the bounding boxes. One page may be tagged with several content types if it contains multiple relevant sections of distinct nature. The Mixed type corresponds to annotations encompassing several content types. Figure 13: Lift of query types by content type. Each cell shows the ratio of observed query type frequency to baseline frequency for given content type. Values >1 indicate over-representation (e.g., tables appear 2.15 more in numerical queries than expected), while values <1 indicate under-representation."
        },
        {
            "title": "G Bounding box annotations",
            "content": "Inter-annotator agreement Table 15 shows IoU and F1 scores between human annotations, to detail results of Section 3.3. Bounding box predictions Figure 27 shows the prompt used to generate final answers with inline bounding boxes for visual grounding, and Figure 15 reports bounding box localization F1 scores by dataset."
        },
        {
            "title": "H Final answer evaluation",
            "content": "Evaluation setup Generated final answers are evaluated in pass@1 setting using GPT 5.2 with medium reasoning effort as the LLM judge. The judge compares each generated answer against the ground-truth annotation and returns binary correctness label. The answer generation and judge"
        },
        {
            "title": "Metric",
            "content": "C.S. Nucl. Fin. Phar. H.R. Ind. Tele. Phys. Ener. Fin. Average IoU F1 0.500 0.608 0.476 0.594 0.462 0.569 0.615 0. 0.474 0.594 0.502 0.611 0.526 0.637 0.443 0.540 0.470 0.569 0.503 0. 0.497 0.602 Table 15: Inter-annotator agreement metrics on bounding box annotations. test to the most challenging corpus in each language: Industrial Maintenance (English) and Finance (French). We measured an average score of 65.74 % with standard deviation of 0.94 %. Crucially, the evaluation signal remains robust against generative noise, achieving Krippendorffs α of 0.80. This agreement confirms that the end-to-end results are statistically reliable even when subjected to the most difficult evaluation scenarios. Easy/hard query filtering To classify queries by difficulty, we prompt panel of 6 LLMs to answer each query without access to any corpus context. We select GPT-5-nano, GPT-5-mini, GPT5, Qwen3-VL-30B-A3B, Gemini 2.5 Flash, and Gemini 2.5 Pro to span different model families and capability levels. Each model receives only the query text and is asked to provide direct answer with the prompt in Figure 23. Answers are evaluated for correctness using the same GPT-5.2 judge described above. query is labeled easy if at least one model answers correctly, and hard otherwise. Table 16 reports per-model accuracy and the resulting proportion of easy queries for each dataset. The distribution varies substantially across domains: knowledge-intensive datasets such as Computer Science and Physics have over 85% easy queries, while domain-specific datasets such as Finance and Energy contain fewer than 35% easy queries, reflecting the specialized nature of their content. Figure 14: Residuals from additive performance model. Each cell shows the difference between observed NDCG@10 and the value predicted by an additive model of query type and content type main effects. Values near zero (white) indicate no interaction; positive values (red) indicate better-than-expected performance for that combination; negative values (blue) indicate worse-than-expected. prompts are shown in Figure 25 and Figure 24 respectively. We evaluated Gemini 3 Pro with low thinking effort, GPT-5 with medium reasoning effort, as well as the thinking version of Qwen3-VL235B-A22B. To assess the reliability of our judge, we conducted 5 independent evaluation runs on fixed set of Gemini 3 Pro outputs. Individual run scores showed minimal fluctuation (mean 72.09 %, σ = 0.22 %) and high internal consistency (Krippendorffs α = 0.91), confirming that the judge is consistent given fixed context. End-to-End Pipeline Stability While the judge demonstrates high consistency on fixed inputs, the full evaluation pipeline introduces second layer of variability: the models generation process. To quantify the end-to-end variance under rigorous conditions, we performed 5 independent runs. For computational efficiency, we restricted this stress 20 Figure 15: Model bounding box localization performance. Each F1 score measures the zone-based overlap between model-generated bounding boxes and human annotations, using the annotator yielding the highest F1."
        },
        {
            "title": "Model",
            "content": "C.S. Fin. Phar. H.R. Ind. Phys. Ener. Fin. Total"
        },
        {
            "title": "74.4\nGPT-5-nano\n79.1\nGPT-5-mini\nGPT-5\n76.3\nQwen3-VL-30B-A3B 60.9\n66.1\nGemini 2.5 Flash\n70.2\nGemini 2.5 Pro",
            "content": "7.4 13.3 25.2 3.9 8.7 16.8 30.5 37.4 50.8 19.2 30.8 29.4 12.9 17.6 29.9 6.3 13.5 15.4 15.6 20.5 32.2 9.9 15.9 23.7 74.2 80.1 80.5 60.9 63.6 63.9 14.0 13.6 26.0 6.8 14.9 20. 9.1 13.1 22.2 4.4 13.1 20.0 29.8 34.3 42.9 21.5 28.3 32.9 Easy queries (%) 86.5 31.7 57. 36.5 38.9 86.4 32.5 30.0 48. Table 16: Percentage of queries correctly answered by LLMs without corpus context. panel of 6 LLMs is asked to answer the queries of the 8 public datasets without access to any corpus context. Queries correctly answered by at least one of the 6 models are classified as easy queries, while the rest are labeled as hard. Easy queries account for 48.6 % of all the queries."
        },
        {
            "title": "K Prompts",
            "content": "All the prompts used for both dataset generation and evaluations are detailed from Figure 20 to Figure 27. Qualitative analysis reveals distinct failure modes. Gemini frequently produces off-by-one page inthe predicted coordinates would dexing errors: correctly localize the target content if applied to an adjacent page. The two models also differ in box granularity: Gemini tends to draw tight boxes around individual elements (e.g., single table cell or text line), whereas Qwen3-VL generates larger boxes encompassing entire sections or paragraphs, more closely matching human annotation patterns. Figures 16 and 17 illustrate these tendencies across four dataset pages: Qwen3-VLs bounding boxes are comparatively wide and encompass entire page elements (pages (a), (c), and (d)), while Gemini 3 Pros visual grounding is more precise (pages (b) and (c)). This difference in granularity partially explains Qwen3-VLs higher F1 scores, as broader boxes are more likely to overlap with the groundtruth zones used in our evaluation. Both models exhibit errors and omissions: in page (b), the chart is not labeled by Qwen3-VL, and in page (d), Gemini 3 Pro predicts incorrect bounding boxes for the bottom table while Qwen3-VL provides grounding for the wrong table. J"
        },
        {
            "title": "Instructions given to Annotators",
            "content": "Query Generation Figure 18 details step-bystep instructions to annotators to generate queries from summaries and images. Query-Page Relevancy linking Figure 19 details the step-by-step instructions provided to annotators for assessing page relevance, identifying content modalities, and localizing evidence via bounding boxes. Table 17 gives the definitions of relevancy scores used by the human annotators. Score Label Definition 2 Fully Relevant Critically Relevant 0 Not Relevant The page contains the complete answer. The page contains facts or information required to answer the query, though additional information is required. Provides no information relevant to the query. Table 17: Relevance definitions used for page-level annotations. 22 Figure 16: Visual grounding comparative examples for Qwen3-VL-30B-A3B. Each panel shows document page with Qwen3-VLs predicted bounding boxes (solid magenta) and human bounding boxes (dashed blue and green, one color per annotator). Corresponding datasets and queries: (a) finance_en: What was the average daily Value at Risk (VaR) for Goldman Sachs during 2024?, (b) finance_en: List the 3 components of regulatory capital under Basel III, and determine the role of each component., (c) hr_en: Analyze how full-time employment among returning health workers evolved in the Netherlands and Italy from 2018 to 2023, and describe the differences in their employment trends., (d) finance_fr: Croissance Mode Maroquinerie vs Vins Spiritueux 2023 performance 23 Figure 17: Visual grounding comparative examples for Gemini 3 Pro. Each panel shows document page with Geminis predicted bounding boxes (solid magenta) and human bounding boxes (dashed blue and green, one color per annotator). Corresponding datasets and queries: (a) finance_en: What was the average daily Value at Risk (VaR) for Goldman Sachs during 2024?, (b) finance_en: List the 3 components of regulatory capital under Basel III, and determine the role of each component., (c) hr_en: Analyze how full-time employment among returning health workers evolved in the Netherlands and Italy from 2018 to 2023, and describe the differences in their employment trends., (d) finance_fr: Croissance Mode Maroquinerie vs Vins Spiritueux 2023 performance 24 # Step 1 The annotator will be provided with content(text summary or image(s)) and list of instructions on the queries that are expected. # Step 2 Read and analyse the content. # Step 3 - The annotator writes series of queries that can, supposedly, be answered by the summarized content. It is okay if the information needed to answer the question is in other parts of the document or not explicitly written in the summary. - If the summary is not adapted to specific type or format of questions, the annotator may skip. - They should follow the number of queries asked for each category. - They should follow the expected type of queries provided and the format. Figure 18: Instructions given to human annotators to create queries # Task overview In this task, the annotator will be provided with query and pages that are supposed to be relevant to answer the query. The annotators goal is to rate the relevance in answerability of each page with respect to the query. # Step 1: - Review the query and pages to get an understanding of the content and domain # Step 2: Rate the query quality. - If adheres to guidelines > Good (1) - If doesnt adhere to guidelines> Poor(0) - If the query is Poor quality, skip the task. # Step 3: - For each page, rate the relevance with respect to the query - If page completely answer query > Fully Relevant(2) - If page contains information required to answer the query > Critically Relevant(1) - If page contains no relevant information > Not Relevant(0) # Step 4: - For each page, annotate the modalities in which the relevant information is located concerning the query-page link, relevant information can be located in multiple modalities at the same time: Modality : [text, table, chart, infographic, image, other] - If relevance score = 0, modality may be N/A # Step 5: - For each page, draw bounding boxes around the relevant text/chart/image/infographic (if any) - If relevance score = 0, do not draw bounding box # Step 6: - Repeat steps 3, 4 and 5 for all pages # Step 7: - Propose an answer to the query, given the relevant pages. - If the query is not answerable, rate it as unanswerable Figure 19: Instructions given to human annotators to annotate query-page relevancy <mission> You are an assistant specialized in visual document understanding tasks. You will be given context, summarizing the content of section or multiple document sections. Your goal is to carefully analyze the context and to solve series of tasks related to its content. You are tasked with generating query-answer pairs. Your queries will be used to simulate user unfamiliar with the specific content of the page, and who is looking for information in large knowledge base through search engine. The user does not have access to the document and is looking for information that can be present in any document in the knowledge base. </mission> <definitions> - query is said to be fully answerable if the page contains precise and complete answer to the query. - query is said to be partially answerable if the page contains relevant information that is directly related to the query but some key information is missing and must be retrieved in other pages or documents in order to give precise and complete answer. - An open-ended query is an explanatory or descriptive query that synthesizes information; may be broad in scope and focused on qualitative aspects of the summary - compare-contrast query is query that requires comparing and/or contrasting multiple entities or topics that are closely related to each other - An enumerative query is query that asks to list all examples that possess common property, optionally requesting details about the specifics of each example. - numerical query is query that asks for specific number or calculated number given summary. The query should require more than simply reading numbers directly from the page. - boolean query is yes/no query that may involve multiple steps of reasoning. - An extractive query is clear and specific query that can be answered using only specific piece of information. - multi-hop query is complex query that requires retrieving and integrating information from multiple sources or steps to produce complete answer. - question query is complete sentence that ends with question mark, typically used to seek specific information or clarification. - keyword query is brief, often fragmented phrase or set of terms used to search or filter information, without forming full grammatical sentence. - An instruction query is directive that describes task to be performed on the documents, often in the form of command or request. </definitions> <rules> <queries> - Generate queries only in {{ language }}. - Make queries diverse, natural, and plausible for someone unfamiliar with the document. - Each query must be standalone; do not reference the page, the table, the figure, the document, the text, the table of contents, etc. - Rephrase; avoid copying wording from the source so semantic matching, not surface matching, is tested. - You may include queries about relationships or trends often shown in tables/figures/graphs, but never refer to specific table/figure. - Avoid overly generic queries that apply to any document. - Keep each query concise ({{ length }} words). - When appropriate, write multi-hop queries that integrate information across the provided pages. </queries> </rules> <instructions> Used Documents: {{ document_names }} <summary> {{summary}} </summary> Using the provided context, generate {{ difficulty }}, {{ reasoning_type }}, {{ answerability }} query. The query should be {{ query_type }} using the provided context and have the format of {{ query_format }} query. The query should be self-sufficient and related to the context. </instructions> Figure 20: Prompt used generate synthetic queries with the NeMo Data Designer tool 26 <mission> You are an assistant specialized in visual document understanding tasks. You will be given document page by page and question. Your goal is to carefully analyze the page and say if it is related to the question's answer. You are tasked with generating question-page affiliation as well as the question answer if it exists in the page. </mission> <definitions> - question is said to be fully answerable if the corresponding page contains precise and complete answer to the question. - question is said to be partially answerable if the corresponding page content is necessary to answer the question but some key information is missing. - question is said to be unanswerable if the corresponding page contains information related to the question's topic or domain but upon closer inspection does not contain information that is useful to answer the question. Or if the page has no link whatsoever with the query. </definitions> <rules> <page_affiliation> - Be sure to put the relevance (and only that) between the tags <relevance>...</relevance>. The possible values are: <relevance>fully answerable</relevance>, <relevance>partially answerable</relevance> <relevance>unanswerable</relevance>. - Be very careful when doing your page affiliation. Only say page is relevant when it really is. </page_affiliation> <answers> - You must generate the answer between the tags <answer>...</answer>. Between these tags, you should only put the answer to the question. - You must generate answers in the following language: {language} - Your answers should be complete sentences. - When the question is ambiguous, your answer should state that there is an ambiguity in the question. - You should always generate the answer based on all the information available on the page, even if the question was generated only on part of the page. </answers> </rules> <instructions> Return if the following question is \"fully answerable\" \"partially answerable\" or \"unanswerable\" based on the content of the page between the tags <relevance> ... </relevance>. If the question is answerable, provide the answer. Here is the question : {{ query }} And there is the page content : </instructions> Figure 21: Prompt used to pre-filter the irrelevant pages for given query 27 You are given set of document pages (images), query, and list of one or more proposed answers. Query : {{ query }} Proposed Answers: {{ answers }} Your task is to carefully analyze the provided pages, the query, and the proposed answers. You must return single, syntactically correct JSON object with the following structure: ```json { \"reasoning\": \"<string>\", \"information_in_pages\": <true or false>, \"answer_correctness\": [<true or false>, ...], \"reformulated_answer\": \"<string>\" } ``` Instructions for each field: - reasoning: Explain the logic for each boolean in the `answer_correctness` list. For each proposed answer, state why it is correct or incorrect, citing specific evidence from the document pages. - information_in_pages: Set to `true` if the information needed to definitively answer the query is present in the pages. Otherwise, set to `false`. - answer_correctness: list of booleans, corresponding to each proposed answer in the original order. Use `true` if the answer is verifiably correct based on the pages and `false` otherwise. - reformulated_answer: single string containing the most precise and correct answer to the query, derived only from information in the pages. If any of the proposed answers are correct, use them as basis for synthesizing this improved answer. The reformulation must be concise and factual. Important rules: - Base your entire analysis strictly on the content of the provided document pages. Do not use outside knowledge. - Do not invent, infer, or assume information that is not explicitly stated in the pages. - Always provide string for the `reformulated_answer`, even if no correct answer can be formed from the text. - Your final output must be only the JSON object. Figure 22: Prompt used to merge human annotators answers Give very precise and concise answer to the following query. If you are unable to answer, output 'I don't know'. Query: {{ query }} Figure 23: Easy/hard query filtering prompt 28 You are an expert judge evaluating the accuracy of test answer against gold-standard true answer. Your goal is to determine if the test answer captures the essential \"core information.\" ### Evaluation Criteria: - Correct: The test answer contains all core information of the true answer. Minor omissions of non-essential details or the addition of minor, non-contradictory information should still be marked as \"Correct.\" - Partially Correct: The test answer captures some of the core information, but suffers from significant omissions or includes substantial extra information that was not requested or present in the true answer. - Incorrect: The test answer is fundamentally wrong, contradicts the true answer, or misses the core information entirely. ### Input Data: Query: {{ query }} True Answer: {{ true_answer} } Test Answer: {{ test_answer }} ### Output Format: Provide very brief explanation for your judgment. You must output your final response in JSON format with two fields: \"explanation\" and \"judgment\" (which must be \"Correct\", \"Partially Correct\", or \"Incorrect\"). Figure 24: Judge prompt used for end to end evaluation You are an expert at answering query based on documents. Here is list of relevant documents: {{ documents }} Based on the above documents, answer the following query: {{ query }} Keep the response short when appropriate. Output the answer only. Figure 25: Answer generation prompt used for end to end evaluation English text to translate: {{ query }} Translate the English text above to French. Make sure you follow the format of the English text. Don't change acronyms. Follow the following json schema. { \"french_translation\": ... } Figure 26: Query translation from English to French prompt 29 # Role and Objective - Serve as an expert in document analysis and visual grounding. - Given query and multiple document page images, provide natural language answer with inline grounding references. # Instructions - Analyze all provided pages to answer the query comprehensively. - For each piece of information used in your answer, provide visual grounding by including bounding box coordinates of all the sections of the document that help answer the query. - Use this format to include the list of all bounding boxes of image N: <bboxes image=\"N\">[[x_{min}, y_{min}, x_{max}, y_{max}], ...]</bboxes> - image=\"N\" specifies the 0-indexed page number (0=first page, 1=second page, etc.) - Include bounding boxes inline in your answer, immediately after mentioning the relevant information - given page may contain multiple non-contiguous sections that help answer the query. In this case, you must output the list of the bounding boxes of all these sections. - You must group all the bounding boxes of given page into single <bboxes image=\"N\">...</bboxes> tag. # Grounding Principles - DO NOT output more than 5 bounding boxes per page. - Adjacent logical units must be enclosed in single, continuous bounding box. - Return multiple bounding boxes only if information is clearly independent and separated by significant non-relevant content. # Output Format - Provide natural language answer to the query. - Embed grounding tags directly inline where relevant information is discussed. - Example: \"The valuation technique described on page 1 <bboxes image=\"0\">[[120, 450, 890, 670], [100, 800, 330, 960]]</bboxes> uses discounted cash flow analysis.\" Figure 27: Bounding box prediction prompt"
        }
    ],
    "affiliations": [
        "CentraleSupélec, Paris-Saclay",
        "Illuin Technology",
        "NVIDIA"
    ]
}