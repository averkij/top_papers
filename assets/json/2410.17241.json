{
    "paper_title": "Frontiers in Intelligent Colonoscopy",
    "authors": [
        "Ge-Peng Ji",
        "Jingyi Liu",
        "Peng Xu",
        "Nick Barnes",
        "Fahad Shahbaz Khan",
        "Salman Khan",
        "Deng-Ping Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Colonoscopy is currently one of the most sensitive screening methods for colorectal cancer. This study investigates the frontiers of intelligent colonoscopy techniques and their prospective implications for multimodal medical applications. With this goal, we begin by assessing the current data-centric and model-centric landscapes through four tasks for colonoscopic scene perception, including classification, detection, segmentation, and vision-language understanding. This assessment enables us to identify domain-specific challenges and reveals that multimodal research in colonoscopy remains open for further exploration. To embrace the coming multimodal era, we establish three foundational initiatives: a large-scale multimodal instruction tuning dataset ColonINST, a colonoscopy-designed multimodal language model ColonGPT, and a multimodal benchmark. To facilitate ongoing monitoring of this rapidly evolving field, we provide a public website for the latest updates: https://github.com/ai4colonoscopy/IntelliScope."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 2 ] . e [ 1 1 4 2 7 1 . 0 1 4 2 : r"
        },
        {
            "title": "Frontiers in Intelligent Colonoscopy",
            "content": "Ge-Peng Ji, Jingyi Liu, Peng Xu, Nick Barnes, Fahad Shahbaz Khan, Salman Khan, Deng-Ping Fan AbstractColonoscopy is currently one of the most sensitive screening methods for colorectal cancer. This study investigates the frontiers of intelligent colonoscopy techniques and their prospective implications for multimodal medical applications. With this goal, we begin by assessing the current data-centric and model-centric landscapes through four tasks for colonoscopic scene perception, including classification, detection, segmentation, and vision-language understanding. This assessment enables us to identify domainspecific challenges and reveals that multimodal research in colonoscopy remains open for further exploration. To embrace the coming multimodal era, we establish three foundational initiatives: large-scale multimodal instruction tuning dataset ColonINST, colonoscopydesigned multimodal language model ColonGPT, and multimodal benchmark. To facilitate ongoing monitoring of this rapidly evolving field, we provide public website for the latest updates: https://github.com/ai4colonoscopy/IntelliScope. Index TermsColonoscopy, Survey, Vision-language, Multimodal Language Model, Medical Image, Abdomen, Healthcare AI."
        },
        {
            "title": "1 INTRODUCTION",
            "content": "D ESPITE declining colorectal cancer (CRC) rates in highincome countries, it remains the third most diagnosed cancer worldwide and is increasing in developing countries [1]. Colonoscopy, as an efficient method for CRC screening, utilises flexible camera-equipped tube to visually examine the colons interior. As illustrated in Fig. (1-a), this clinical procedure also facilitates intervention with specialised instruments such as snares, forceps, and cautery devices to remove precancerous growths, such as serrated and adenomatous polyps. recent study [2] indicates that incorporating artificial intelligence (AI) into colonoscopy reduces the miss rate of colorectal neoplasia by approximately 50% compared to traditional methods. This success motivates us to investigate the frontiers in intelligent colonoscopy. Colonoscopy, an endoscopic optical imaging technique, usually presents visual patterns (e.g., non-uniform illumination, homogeneity) that differ from those of general-purpose imaging data, e.g., ImageNet [3], due to the complex and folded anatomy of the colon. This suggests that special methods are needed to interpret the colonoscopic data. In response, we begin with an investigation of the latest intelligent techniques for colonoscopy, assessing the current landscape to sort out domain-unique challenges and underexplored areas. Our analysis reveals that multimodal research in colonoscopy remains largely untapped. To bridge this gap, we contribute three efforts to the community, as illustrated in Fig. (1-b). Contribution. (a) We investigate the latest research progress in four colonoscopic scene perception tasks (refer to Fig. 2) Corresponding author: Deng-Ping Fan (dengpfan@gmail.com). Deng-Ping Fan is with the Nankai Institute of Advanced Research (SHENZHEN-FUTIAN), Guangdong, China, and also with the College of Computer Science & VCIP, Nankai University, Tianjin, China. Ge-Peng Ji and Nick Barnes are with the School of Computing, Australian National University, Canberra, Australia. Jingyi Liu is with the Graduate School of Science and Technology, Keio University, Yokohama, Japan. Peng Xu is with the Department of Electronic Engineering, Tsinghua University, Beijing, China. Fahad Shahbaz Khan and Salman Khan are with Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE. (a) Illustration of colonoscope inside large intestine (colon) (b) Highlights of this study 63 colonoscopy datasets 137 related models Fig. 1. Introductory diagram. We depict (a) the anatomy of the large intestine (colon) within the digestive tract, the polypectomy procedure during colonoscopy examination, and the components of colonoscope. The bottom figure (b) summarises three highlights of this study. from both data-centric and model-centric perspectives. Our investigation summarises key features of 63datasets and 137 representative deep techniques published since 2015. Additionally, we highlight emerging trends and opportunities for future study. (b) We introduce ColonINST, pioneering instruction tuning dataset tailored for multimodal research, aimed at instructing models to execute user-driven tasks interactively. Assembled from 19 publicly available sources, the ColonINST dataset contains 303,001 colonoscopy images across 62 sub-categories, reflecting diverse scenarios encountered in colonoscopy procedures. We expand these visual samples in two aspects. First, we leverage the multimodal AI chatbot, GPT-4V [4], to generate 128,620 medical captions. Second, we restructure 450,724 human-machine conversations for multimodal adaptation. (c) Leveraging the instruction tuning data, we build multimodal language model, ColonGPT to assist endoscopists through interactive 2 Fig. 2. Colonscopic scene perception from visual to multimodal perspectives. In clinical practice, purely visual tasks, including (a) classification, (b) detection, and (c) segmentation, are applied to identify targets of interest such as polyps and instruments. (d) Multimodal applications improve colonoscopy procedures by performing interactive, user-driven tasks aligned with clinical needs. The chatbot provides personalised advice, automated reporting, and streamline procedural workflows. dialogues. To ensure reproducibility for average community users, we implement ColonGPT in resource-friendly way, using 0.4B-parameter visual encoder SigLIP-SO [5] and 1.3B-parameter lightweight language model Phi1.5 [6]. Unlike previous vision-language (VL) linking methods [7] [9] that employ multilayer perceptrons to handle equally all tokens from the visual encoder, we propose multigranularity adapter to selectively sample visual tokens according to their significance. This strategy reduces the visual tokens 34% of the original number without compromising to performance, securing the top spot in our newly-created multimodal benchmark across three tasks. Importantly, our ColonGPT can be trained within five hours on four A10040GB GPUs, facilitating rapid proof-of-concept development for subsequent research. Scope. This study differs from previous works in several aspects. Earlier surveys on traditional [10] and deep learning [11][14] methods conducted before 2020 are now out of date. Although recent study [15] explores various applications of colonoscopy, such as quality analysis and abnormality detection, it lacks numerical validation. Other benchmarks [16][18] are limited to specific narrow research subfields. By contrast, we delve into four tasks for colonoscopic scene perception and evaluate their current state to sort out key challenges and under-researched areas. Importantly, our vision goes beyond by laying the foundations for the coming era, multimodal world. To embrace this era, we further undertake three initiatives: multimodal instruction tuning dataset, multimodal language model, and multimodal benchmark for the community. Organisation. The remaining sections are structured as such: 2 provides historical background and discusses the domain-unique challenges. 3 investigates 63 colonoscopy related datasets, followed by survey of 137 deep models in 4. In 5, we introduce three initiatives towards the multimodal era: the creation of ColonINST, the technical details of ColonGPT, and comparative multimodal benchmark along with ablative analyses. Finally, this paper is concluded in 6."
        },
        {
            "title": "2.1 Origin and evolution",
            "content": "The history of colonoscopy has two key milestones. In 1968, gastrointestinal surgeons Hiromi Shinya and William Wolff found link between colonic polyps and intestinal tumours, but they lacked equipment to examine them. In 1969, they discovered Corning Incorporateds optical fibres and collaborated with Olympus to create the fiberoptic colonoscope, groundbreaking device to examine the colon and remove polyps using wire loops. The second milestone came in 1983 with the introduction of the electronic colonoscope [79], which allows visualisation of the colon on screen and polyp removal using polypectomy snare, enhancing detection rates and reducing bleeding. The 21st century brings the AI era, where computer-aided diagnosis systems provide greater precision and reliability in procedures [80]. This study explores the transformative impact of intelligent techniques for colonoscopy, which is type of endoscopy [81], while other related techniques such as laparoscopy [82] are summarised in our appendix."
        },
        {
            "title": "2.2 Intrinsic traits and domain-unique challenges",
            "content": "We summarise five unique challenges associated with colonoscopic vision tasks, primarily caused by procedural aspects and imaging conditions during colonoscopy. (a) Non-linear camera ego-motion. Procedural constraints force the camera (i.e., colonoscope) to actively move in nonlinear and unpredictable manner, challenging ego-motion compensation [83] and causing motion blur [62]. (b) Presence of medical instruments. The colonoscopy procedure often includes instruments such as scopes, guidewires, and snares, which should be distinguished properly from anatomical structures [84] for efficient analysis. (c) Limited observable field. The intricate folds and blind spots within the colon restrict the visible area in colonoscopy data. This requires algorithms capable of extracting relevant information from limited visual landscapes [85]. (d) Non-uniform illumination. The mucosal surface of the colon, prone to wetness and sheen, results in highly variable and diffuse illumination with complex reflections such as non-Lambertian reflections and interreflections. Traditional lighting-based algorithms struggle under these conditions [86]. (e) Variability in tissue appearance. Mucosal textures and colours vary considerably due to constant movement, disease states, anatomical differences, and instrument effects. Furthermore, benign polyps or lesions usually have weak or homogeneous boundaries [87], making them blend into surrounding tissues and difficult to detect. These issues require robust response from AI models to inherent morphological and colour fluctuations. TABLE 1 Data statistics for colonoscopy datasets. The columns include: number of images (#IMG) and videos (#VID), classification tag (Cls), bounding box (Bbx), segmentation mask (Seg), text (Tx). The categories not related to colonoscopy, such as stomach and esophagitis, are marked in grey. 3 CVC-ColonDB [19] ETIS-Larib [20] CVC-ClinicDB [21] ASU-Mayo [22] Dataset Publication PR12 CARS14 CMIG15 TMI15 Ye et al. [23] MedIA16 IJCNN16 JBHI16 TMI16 CVC-ClinicVideoDB [27] MICCAIw17 Kvasir [28] MMSys17 Nerthus [29] MMSys17 JHE17 EIO17 Deeba et al. [24] CU-ColonDB [25] ColonoscopicDS [26] EndoSceneStill [30] KID1 [31] KID2 [31] NBIPolyp-UCdb [32] WLPolyp-UCdb [33] ASEI [34] Cho et al. [35] EAD2019 [36] Liu et al. [37] Kvasir-SEG [38] EIO17 BSPC19 EIO19 MM19 PeerJ19 arXiv 19 ISBI20 MMM20 PICCOLO [39] ApplSci20 EDD2020 [40] CAD-CAP [41] arXiv20 EIO20 ACP-ColonDB530 [42] NPJDM20 HyperKvasir [43] SData20 WCE-Polyp [44] TMI20 EAD2020 [45] MedIA21 ISVC21 BKAI-Small [46] BKAI-Large [46] ISVC21 CPC-Paired [47] MICCAI21 LDPolyVideo [48] MICCAI21 Celik et al. [49] MICCAI21 MMM21 CP-CHILD [51] BMCMI21 EIO21 FMOLB21 GIE21 CROHN-IPI [52] C-E Crohns Disease [53] SUN-database [54] Kvasir-Instrument [50] 300 196 612 36,458 7,894 100 1,930 - 10,924 8,000 5,525 912 2,371 86 3,040 4,470 2,342 14,317 1,000 3,433 47 11 42 - 18 - 39 #IMG #VID ClsBbxSegTx Number of categories (#C) Category names #C1 polyp #C2 polyp, non-polyp #C1 polyp #C2 polyp, non-polyp 15 - 31 38 10 - - 76 18 #C2 polyp, non-polyp #C2 polyp, non-instance #C2 bleeding, non-bleeding #C3 hyperplasia polyps, adenomatous polyps, non-polyp #C3 serrated adenomas, hyperplastic lesions, adenoma URL Link Link Link Link Link - - Link Link #C8 cecum, polyps, ulcerative colitis, dyed and lifted polyp, dyed resection margins, Z-line, pylorus, esophagitis Link #C4 BBPS (Boston-Bowel-Preparation-Scale) 0/1/2/3 Link Link Link #C10 angiectasias, ulcers, stenoses, villous edema, nodular lymphangiectasias, chylous cysts, polyps, aphthae, normal/no pathology, intraluminal hemorrhage #C1 polyp - 21 - #C4 vascular anomalies, polypoid anomalies, inflammatory anomalies, normal images #C2 adenomas, hyperplastic #C2 polyp, normal mucosa - 328,927 112 #C4 dyed-lifted-polyps, dyed-resection-margins, instruments, polyp #C1 cecum #C7 imaging artefacts, contrast, specularity, instrument, bubbles, motion blur, saturation #C2 polyp, non-polyp #C1 polyp #C17 Paris classification (protruded lesions: 0-Ip/0-Ips/0-Is, elevated lesions: 0-IIa/0-IIa+c, flat lesions: 0-IIb), NICE classification (type 1/2/3), Diagnosis (adenocarcinoma/adenoma/hyperplasia), Histological stratification (high grade dysplasia/hyperplasia/invasive adenocarcinoma/low grade dysplasia/no dysplasia) - #C5 suspicious area, high-grade dysplasia, adenocarcinoma, polyp, normal dysplastic Barretts oesophagus #C4 vascular lesions, fresh blood, ulcero-inflammatory lesions, normal images #C13 adenomatous polyp, hyperplastic polyp, other polyp, bleeding, IC valve, instrument, artefact, normal colon structure, bubble, inside colon background, stool, lumen, outside colon background 25,124 1,686 221,976 - 110,079 374 #C23 cecum, retroflex rectum, BBPS 0-1/2-3, ulcerative colitis grade 1/2/3/0-1/1-2/2-3, polyps, dyed lifted polyps, dyed resection margins, hemorrhoids, Barretts, terminal ileum, Z-line, esophagitis grade A, esophagitis grade B-D, pylorus, retroflex stomach, Barretts (short-segment), impacted stool #C1 polyp #C8 specularity, bubbles, saturation, contrast, blood, instrument, blur, imaging artefacts #C3 non-neoplastic polyp, neoplastic polyp, background #C4 non-neoplastic polyp, neoplastic polyp, undefined polyp, background 541 2,531 1,200 7,466 681 - - - - - 901,666 263 2,224 590 9,500 3,498 - - - - 467 164 159,232 113 Kvasir-Sessile [55] Kvasir-Capsule [56] JBHI21 SData21 4,741,504 117 196 - KUMC [57] ERS* [58] PONE21 arXiv22 1,354,667 1,520 37,899 155 Tian et al. [59] MICCAI22 BSPC22 ISBIw22 MIR WCE-CCDD [60] PolypGen2.0 [61] SUN-SEG [62] 807,069 253 - 6,000 3,446 SinGAN-Seg [63] ENDOTEST [64] MEDVQA-GI [65] GastroVision [66] PONE22 SJG22 CLEF23 ICMLw 10,000 253,754 3,949 8,000 - 58 - - #C2 hyperplastic polyp, adenoma #C2 polyp, non-polyp #C2 polyps, Barretts esophagus #C1 GI procedure tools (e.g., snares, balloons, and biopsy forceps) #C2 colonic polyp, normal or other pathological images #C7 erythema, edema, aphthoid ulceration, ulceration (310mm, >10mm), stenosis, non-pathological #C1 Crohns lesions #C7 hyperplastic polyp, low grade adenoma, high-grade adenoma, traditional serrated adenoma, invasive carcinoma, sessile serrated lesion, negative #C1 polyp (<10mm) #C14 polyp, Ileocecal valve, lymphangiectasia, erythema, angiectasia, foreign body, erosion, ulcer, blood (fresh), blood (hematin), normal clean mucosa, reduced mucosal view, pylorus, ampulla of Vater #C2 hyperplastic polyps, adenomatous polyps #C27 ulcerative colitis (active/quiescent), stricture (postoperative/inflammatory/malignant), polyp, melanosis, diverticulosis, fistula, crohnsdisease (active/quiescent), lipoma, proctitis, hemorrhoids, submucosal tumor, solitary ulcer, bleeding of unknown origin, ileitis, diverticulitis, colitis: ischemic, colorectal cancer, angiodysplasia, rectal ulcer, foreign body, polyposis syndrome, postoperative appearance, parasites #C2 polyp, non-polyp #C4 ulcer, polyps, normal, esophagitis 46 #C2 serrated, adenomas 159,232 1,013 #C7 hyperplastic polyp, low grade adenoma, high-grade adenoma, traditional serrated adenoma, invasive carcinoma, sessile serrated lesion, negative #C1 polyp #C2 polyp, non-polyp #C2 polyp, surgical equipment #C27 accessory tools, angiectasia, blood in lumen, cecum, colon diverticula, resection margins, colorectal cancer, dyed-lifted-polyps, erythema, ulcer, dyed-resection-margins, retroflex rectum, mucosal inflammation large bowel, resected polyps, colon polyps, lleocecal valve, normal mucosa and vascular pattern in the large bowel, esophagitis, Barretts esophagus, duodenal bulb, esophageal varices, gastric polyps, gastroesophageal junction normal z-line, normal esophagus, normal stomach, pylorus, small bowel terminal ileum W-Polyp [67] LIMUC [68] PS-NBI2K [16] PolypGen [69] MedFMC** [70] GB-WCE Dataset [71] REAL-Colon [72] Xu et al. [73] Kvasir-VQA [74] CapsuleVision2024 [75] COLON [76] WCEBleedGen [77] PolypDB [78] CVPR23 IBD23 JBHI23 SData23 SData23 MD23 1,450 11,276 2,000 8,037 22,349 226 SData24 2,757,723 251 TMI24 6,500 MMw24 58,124 CVIP24 arXiv24 430,000 2,618 arXiv24 3,934 arXiv24 - - - - - - - - 60 #C1 polyp #C4 Mayo endoscopic score (MES) 0/1/2/3 #C1 polyp 23 #C2 polyp, negative #C5 ulcer, erosion, polyp, tumor, and non-instance #C2 bleeding or lesions, normal #C2 polyp, negative #C4 Mayo endoscopic score (MES) 0/1/2/3 #C5 polyps, ulcerative colitis, instrument, normal, esophagitis #C10 angioectasia, bleeding, erosion, erythema, foreign body, lymphangiectasia, polyp, ulcer, worms, normal 30 #C3 adenoma, hyperplastic, non-pathological case - #C2 bleeding, non-bleeding - #C1 polyp (multiple imaging modalities and multiple medical centers) *NOTE The ERS dataset [58] includes 99 annotated categories in total. For the sake of brevity, we list only 27 colon-related categories within ERS. **NOTE The MedFMC dataset [70] comprises 23,349 medical images across five modalities. This table only enumerates the categories specific to the endoscopic modality."
        },
        {
            "title": "3.1 Medical data for colonoscopy",
            "content": "images of other organs, such as the pylorus in [28] or the stomach in [66]. Next, we review these selected datasets according to their different task objectives. Tab. 1 presents our investigation of 63 datasets with their essential statistics for four tasks related to colonoscopic scene perception. We search for them using queries such as colonoscopy dataset/benchmark and gastrointestinal disease dataset. They consist of images or videos related to the human colon. In particular, some datasets also include Classification datasets have been widely used for varied purposes, such as colon disease classification in images [28], [31], [40], [43], [52], [58], [60], [66], [68], [73], [75]/videos [54], [56], [62], polyp identification [27], [33], [37], [39], [41], [48], [51], fine-grained polyp classification [25], [26], [47], bleeding condition [24], [71], [77], anomaly recognition [59], Link Link Link Link Link Link - Link Link Link - - Link Link Link Link Link Link Link Link Link Link Link - Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link - Link Link cecum recognition [35], and pre-operative assessment [29]. Detection datasets provide both categorical and localisation labels for targets of interest, such as colonic diseases [54], [57], [62], accessory instruments [34], [42], [42], polyps [23], [48], [59], [64], [72], endoscopic artefacts [36], [45], and gastrointestinal diseases [43], [56]. In addition, the organisation of competitions has accelerated growth within the colonoscopy community by establishing shared platforms for data collection and model evaluation, significantly advancing research areas such as the detection of intestinal disease [40] and polyp [38], [61], [69]. Segmentation datasets for colonoscopy research originate from two sources. The first source comprises real data, mainly utilised for single-target segmentation of entities such as polyps [16], [30], [32], [33], [44], [49], Crohns disease [53], and accessory tools [50]. Some datasets, like BKAISmall/Large [46], provide instance-level masks for neoplastic and non-neoplastic polyps. Other datasets come from organised competitions, such as polyp segmentation datasets [19][22], [61] and gastrointestinal diseases segmentation dataset [40], or extensions of existing databases, offering pixel-wise masks (e.g., for polyps [38], [55], colorectal disease [62]) or scribble labels (e.g., for polyps [59], [67]). The other source, such as SinGAN-Seg [63], generates synthetic images for polyp segmentation. VL datasets remain relatively scarce so far, with two known datasets for this specific purpose. MEDVQA-GI [65] contributes the first dataset with three VL tasks, including visual question answering, visual question generation, and visual location question answering. Kvasir-VQA [74] collects 6,500 question-answer pairs from existing datasets [43], [50], for gastrointestinal diagnostics, such as image captioning, visual question answering."
        },
        {
            "title": "3.2 Discussion",
            "content": "Based on the 63 revisited datasets, we have several datacentric observations that could inspire more future ideas. Data granularity requires improvement to better understand patient conditions and treatment efficacy. (a) More than quarter of polyp-containing datasets provide finegrained categorisation, often with inadequate detail. For example, BKAI-Small/-Large [46] provide two instance annotations of neoplastic and non-neoplastic polyps. ColonoscopicDS [26] categorises at the video level into hyperplasic, serrated, and adenoma lesions. SUN-database [54] provides fine-grained labels, documenting measurements of polyp size (height, width) and morphology (pedunculated, sessile, flat), along with their anatomical locations (e.g., rectum, sigmoid colon). Several data-centric areas remain underexplored, such as temporal lesion evolution recording, granularity improvement, graded severity tagging, and instancelevel target annotation. (b) Furthermore, label orthogonality, an often overlooked issue, treats labels as isolated entities. Current works seldom discuss potential inter-class correlations, such as the co-occurrence of inflammatory bowel disease with erosion symptoms, Crohns disease with fistula complications, or colorectal cancer accompanied by bleeding. Future studies should consider causality [88] and comorbidity [89] to address these correlations effectively. Data diversity is crucial to developing fair and reliable models. Three aspects deserve consideration. (a) Datasets 4 for rare colorectal diseases appear to be limited, due to case scarcity and expertise requirements. For example, Crohns disease, which affects an estimated 58 to 241 per 100,000 adults in the United States [90], has so far been discussed in three datasets [52], [53], [58]. Such an unbalanced situation leads to data-hungry models performing better in common cases than in rarer or novel ones. Thus, increasing attention to rare gastrointestinal diseases could potentially improve the ability to treat long-tailed [91] or open-vocabulary [92] problems. (b) Multimodal research in colonoscopy appears to be in its early stages, with limited data [65], [74] for analysis. Therefore, collecting varied patient information (e.g., age, gender, eating habits) combined with expert interpretations (e.g., clinical report, medication advice) could be advantageous and ultimately facilitate personalised and side-effect-minimised colonoscopy practices [93]. Data inconsistency in colonoscopy research is due to two main factors. (a) Expert interpretations vary due to varying experience, expertise, and observed nuances, leading to subjective judgements and labelling uncertainties. For example, ColonoscopicDS [26] provides raw diagnostic labels for each sample from multiple experts and beginners, reflecting their underlying (dis)agreement. In addition, SUN-SEG [62] releases the rejected segmentation masks from their annotation workflow, highlighting the challenges in reaching consistent polyp boundaries. (b) Existing datasets often have study-specific targets, leaving others unlabelled or classified as background. Nerthus [28], for example, focuses on assessing the quality of bowel preparation, but ignores diagnostic findings such as polyps. Furthermore, multiple categories in GastroVision [66] are not mutually exclusive; for example, case labelled as accessory tool could also fall into the category blood in lumen. Segmentation data like Kvasir-Instrument [50] annotates only medical instruments, ignoring other targets like polyps, whereas Kvasir-SEG [38] provides polyp label, leaving out others like instruments. These observations prompt future research into learning from partial [94], noisy [95], or missing [96] labels."
        },
        {
            "title": "4.1 Classification models",
            "content": "Input phase. Tab. 2 lists the training and testing data used for each deep model. We note that many classification models [106], [109], [112][114], [116], [118] in colonoscopy use in-house data for model development, partly resulting in the absence of domain-recognised benchmarks. This issue stems from the different categorical goals pursued by individual studies, such as identifying polyps from white-light and narrow-band imaging pairs [47], [111], images [106], or videos [59], evaluating polyp size [116], and recognising colonic diseases [112] or landmarks [119]. TABLE 2 Summary of classification models in colonoscopy. Dataset: CU=CU-ColonDB [25], CDS=ColonoscopicDS [26], Private=private data, HK= HyperKvasir [43], KC=Kvasir-Capsule [56]. Backbone: CaffeNet [97], D-121=DenseNet121 [98], R-12/-18/-50/-101=ResNet12/18/50/101 [99], ViT-S16 or ViT-B16 [100], MobV2=MobileNetV2 [101], R50-Att=ResNet50 with attention module [102], C3D [103], Inc-v3=Inceptionv3 [104], I3D [105]. Customised means base network modified for the current task or model independent of the base network choice. Head: classifier implemented by the fully connected (FC) and support vector machine (SVM) layers, or using the ℓ2 norm to measure the disparity between the input and output. Arch: the architectures shown in Fig. 3. Sup: learning strategies such as fully supervised (FS), semi-supervised (SS), unsupervised (US), and weakly supervised (WS). For simplicity, the following tables use consistent abbreviations unless specified otherwise. 5 Model Zhang et al. [25] RIIS-DenseNet [106] FSAD-Net [107] Gammulle et al. [108] ADGAN [37] Carneiro et al. [109] SSL-WCE [110] PolypsAlign [47] CPC-Trans [111] FFCNet [112] DLGNet [113] Yue et al. [114] DAFON [115] SSL-CPCD [73] BseNet [116] Byrne et al. [118] d e - m o i Publication JBHI16 MICCAI18 MICCAI20 MICCAI20 ISBI20 MedIA20 MedIA20 MICCAI21 MICCAI22 MICCAI22 MedIA23 TIM23 ESWA24 TMI24 Core design domain transfer learning rotation-invariant, similarity constrained mutual information maximisation relational mapping dual adversarial learning model uncertainty & calibration adaptive aggregated attention teacher-student alignment cross-modal representation consistency frequency domain learning Gaussian mixture model class imbalance loss few-shot open-set learning composite pretext-class discrimination MICCAI18 unsupervised depth estimation, LSTM [117] Training dataset CU, CDS Private Private Testing dataset CU, CDS Private Private Kvasir [28], Nerthus [29] Kvasir [28], Nerthus [29] Liu et al. [37] Private CAD-CAP [41] CPC-Paired [47] CPC-Paired [47] Private Private Private, HK Kvasir-Capsule [56] LIMUC [68] Private Private Private WVAD [59] Liu et al. [37] Private CAD-CAP [41] CPC-Paired [47] CPC-Paired [47] Private Private Private, HK Kvasir-Capsule [56] Private, LIMUC [68] Private Private Private WVAD [59] Backbone CaffeNet D-121 D-121 R-50 Customised D-121 D-121 R-50 ViT-S16 R-18 R-18 MobV2 R-12 R50-Att C3D Inc-v3 ViT-B16 I3D Arch Head Sup URL SVM FS BF#1 FS FC SF US FC BF#2 FS FC MF#1 ℓ2 US BF#2 FS FC SF SS FC BF#2 FS FC BF#2 FS FC BF#2 FS FC SF FS FC BF#2 FS FC SF FS FC BF#2 FS FC BF#2 FS FC SF FS FC SF FC SF FS FC WS SF - - Link - - - Link Link Link Link Link Link - Link - - - Link Gut19 Tamhane et al. [119] MICCAIw22 MICCAI22 Tian et al. [59] real-time assessment system vision transformer based multiple instance learning Fig. 3. Gallery of deep-based architectures. The single-stream framework (SF) features single input and output with sequential data flow. Multi-stream frameworks predict single output but involve parallel processing streams, either at the decoding stage (MF#1) or the encoding stage (MF#2). Branched frameworks extend multi-stream framework to produce multiple outputs from either single input (BF#1) or multiple inputs (BF#2). These side outputs typically receive supervision from additional supervisory signals, such as boundary cues. Processing phase. We discuss data flow management strategies based on two attributes. (a) Backbone: Early models [106], [108] typically employ well-trained convolutional backbones (e.g., ResNet [99], DenseNet [98]) from ImageNet [3], while recent studies explore alternatives such as using vision transformer in [111] and lightweight network in [114]. Another strategy is SSL-CPCD [73], which involves pre-training model to generate domain-specific representations, followed by its generalisation on various downstream perception tasks. (b) Architecture: Classification models involve various designs as in Fig. 3. At first, basic idea is of using single-stream framework (SF) that sequentially processes visual features based on confidence calibrated [109] or 3D convolutional [59], [116] networks. Second, Gammulle et al. [108] proposed dual decoding flow approach for hierarchical feature encoding, typified as MF#1. Third, to ensure reliable predictions, branched frameworks are adopted for multi-objective learning, such as the integration of parallel feature flows [25], interclass Gaussian loss [113], and global-to-local consistency [111]. Output phase. (a) Prediction head: An early model [25] applies two SVM layers to classify polyps into three categories. Modern methods usually adopt fully connected layer as the final classifier due to its simplicity and flexibility. special case is ADGAN [37], generative adversarial network that computes the ℓ2 norm differential between input and output to identify anomalous images. (b) Supervision strategy: Most methods use fully-supervised learning with pre-annotated categories, but some explore dataefficient strategies, including semi-supervised [110], weaklysupervised [59], and unsupervised [37], [107] learning. Remarks. We observe three aspects of the above classification models. (a) Novel visual backbones, like the state space model [149], remain underexplored. Furthermore, reformulating the classification paradigm within VL models, e.g., CLIP [150], can yield unexpected results. (b) Benchmarks for multicategory classification remain underexplored. The Kvasir series [28], [43], [56] offers valuable sources for further study. We will explore these public data on their potential synergy in 5.1. (c) Several new task settings have emerged in colonoscopy. For example, Tian et al. [59] recognise abnormal frames from colonoscopy videos from an out-of-distribution view. DAFON [115] solve an open-set classification problem within few-shot framework."
        },
        {
            "title": "4.2 Detection models",
            "content": "Input phase. Detection models classify targets and locate them using boxes, assisting surgical intervention and planning. The goals of interest are diverse, such as identifying the polyp(s) in images [126][128], [130][134], [136]/videos [22], [137][139], [141][148], or locating multiple findings [42] like bleeding, polyps, and accessory tools. Processing phase. This has three key configurations for the analysis. (a) Backbone: There are two general strategies for network initialisation. The first group [42], [126][128], [131], [132], [134], [136], [143], [146][148] leverages the ResNet series [99] pre-trained on the ImageNet [3] dataset. The second group relies on well-trained object detectors, such as [130], [142] using DarkNet series [120], [121] and [133] employing EfficientDet-D0 [122]. (b) Workflow: Detection models are often built on general-purpose architectures. In the WF column of Tab. 3, we categorise the models according to TABLE 3 Summary of detection models in colonoscopy. Dataset: C6=CVC-ClinicDB [21], ETIS=ETIS-Larib [20], ASEI [34], C3=CVC-ColonDB [19], KUMC [57], LDPV=LDPolyVideo [48], SUN=SUN-database [54], PL=PICCOLO [39], KID=KID1&2 [31], CDS [26], KSe=Kvasir-Sessile [55], ASU =ASU-Mayo [22], CDB=CVC-ClinicVideoDB [27], ES=EndoSceneStill [30], CU [25], ACP=ACP-ColonDB530 [42]. Backbone: R-34/-50/-101 [99], CDN-53=CSPDarkNet53 [120], DN-53=DarkNet-53 [121], EffDet-D0=EfficientDet-D0 [122], AlexNet [123], V-16=VGG16 [124], R-50v2= ResNet50V2 [125]. WF: one-stage (OS) or two-stage (TS) workflows. NMS: non-maximum suppression. EC: edge-sensitive cues. 6 d e - m s o s - V Model Publication Yang et al. [126] TIM20 ConsolidatedPolypDA [127] MedIA21 MDeNetplus [128] MedIA21 FedInI [129] MICCAI22 CIBM22 Pacal et al. [130] Core design parallel detection & segmentation Gaussian Fourier domain adaptation 2D Gaussian shapes prediction federated learning, structural causal model improved YOLOv3 [121]/v4 [120] SMPT++ [131] TMI22 source-free domain adaptation CIBM23 attention module & context information fusion FRCNN-AA-CIF [132] Haugland et al. [133] SCAN++ [134] TFCNet [135] MI23 TMM23 CIBM24 DUT [136] TNNLS24 IPMI15 TMI15 JBHI16 ICPR18 JBHI19 AIPDT [142] MICCAI20 Tajbakhsh et al. [137] Tajbakhsh et al. [22] Yu et al. [138] Mo et al. [139] Qadir et al. [141] modality translation enhanced semantic conditioned adaptation fine-grained feature compensation decoupled unbiased teacher patch descriptor & edge classification extension on [137] online and offline integration building upon Faster R-CNN [140] temporal dependency parallel detection and tracking AI-doscopist [42] NPJDM20 spatial-temporal fusion STFT [143] MICCAI21 Yu et al. [144] AIM22 EMSEN [145] TII22 YONA [146] MICCAI23 Intrator et al. [147] MICCAI23 arXiv24 V2I-DETR [148] spatial-temporal feature transformation instance tracking head (plug-and-play) explainable multitask Shapley explanation feature alignment & contrastive learning self-supervised polyp re-identification video-to-image knowledge distillation Training dataset Private, C6, ETIS C6 C6 KUMC SUN, PL Private, C6, ETIS, ASEI, KID Private Private, PL, CDS C6, ASEI Testing dataset Private, C6, ETIS ETIS, ASEI C3, ETIS KUMC SUN, PL, ETIS Private, C6, ETIS, ASEI, KID Private PL, KUMC C6, ASEI C6, KUMC, LDPV C6, KUMC, LDPV, KSe C6, ASEI, Private Private C3 ASU CDB ASU, C6 Private, CDB C6, ETIS, C3, ASU, CU, ACP ASU, CDB Private, C6, CDB CDS SUN, CDB, LDPV Private SUN ASEI, Private Private C3, ASU ASU C6, C3, CDB, ES ASU, CDB CDB C6, ETIS, C3, ASU, CU, ACP ASU, CDB Private, CDB, ETIS CDS SUN, CDB, LDPV Private SUN Backbone WF Arch NMS EC Sup URL RR-50 R-101 R-34 R-101 TS BF#1 TS BF#2 OS MF#1 TS BF#2 CDN-53/DN-53 OS BF#1 OS BF#1 TS BF#1 OS BF#2 OS BF#2 OS BF#1 OS BF#2 TS BF#1 TS BF#1 Customised OS MF#2 R-101 EffDet-D0 R-101 CDN-53 R-101 AlexNet AlexNet V-16 V-16 TS BF#1 TS BF#1 DN-53, AlexNet OS BF# R-50 R-50 V-16 OS BF#2 OS BF#2 OS BF#2 Customised OS BF#2 R-50 R-50v2 R-50 TS BF#2 OS MF#2 OS BF# FS US FS FS FS US FS FS - Link - Link Link Link - - FS, US Link FS US FS FS FS FS FS FS FS FS FS FS FS US FS - Link - - - - - - - Link - - Link - - their processing stages. The two-stage workflow decouples detection into the region proposal and classification phases, like models [127], [132], [139], [141] based on Faster RCNN [140]. One-stage models prioritise speed and simplicity, operating in single forward. For example, some studies [42], [130], [142] are based on the YOLO framework [120], [121], and Yu et al. [144] uses the SSD framework [151]. (c) Architecture: Detection models predict target categories and spatial coordinates, typically implemented in branched frameworks (BF#1/BF#2) as shown in Fig. 3. Two special cases [128], [138] adapt the design of multistream framework to first pop out pixel-wise attention regions, then convert them to bounding boxes. Output phase. (a) Post-processing techniques are employed to eliminate duplicate predictions and select the most relevant targets, with non-maximum suppression (NMS) being widely-adopted method [42], [130][133], [139], [141], [144], [146]. (b) Auxiliary information can improve prediction reliability, such as edge cues suggested to provide geometric patterns for object detection in [22], [128], [137]. (c) Supervision strategy is currently dominated by fully supervised learning, such as using region-level labels [126], [130], [132], [133], [139], [141][145], [148] and pixel-wise [22], [128], [137], [138] labels, and introducing the box-assisted contrastive learning [146]. Other models [127], [131], [134], [136] explore unsupervised domain adaptation techniques to detect polyps across colonoscopy devices. Remarks. We emphasise few observations for the above review. (a) Most models focus on detecting polyp(s), while other colonoscopic findings receive less attention. We encourage exploring public multitarget [34] or multicentre [78] data. (b) Learning strategies are underexplored. General-purpose detection models like using weak supervision [255] provide valuable references, being potentially more feasible and cost-effective since they require less detailed annotations from medical experts. (c) Beyond wellestablished convolution-based detection frameworks, recent methods like transformer-based DETR [256] and diffusionbased DiffusionDet [257], open exciting opportunities for this field. Moreover, exploring cross-task synergy is promising, as three video-based models [142], [144], [147] demonstrated effectiveness in unifying polyp detection and tracking frameworks. (d) Although some datasets such as the SUN-database [54] (>158K samples) and LDPolyVideo [48] (>900K samples) are relatively large, this field still lacks standardised evaluation benchmark."
        },
        {
            "title": "4.3 Segmentation models",
            "content": "Compared to the above two topics, the segmentation research appears to be well established. This is evidenced by the extensive amount of research documented in Tab. 4. Input phase. Most segmentation models focus on single target (i.e., polyp), typically adopting as binary segmentation paradigm. These models usually follow the wellestablished testbed of PraNet [175] for their development and comparison. An exception case, AFP-Mask [196], provides an anchor-free framework for segmenting polyp instances. Recent works [208], [217], [230] have also focused on segmenting surgical tools during procedures. Processing phase. (a) Backbone: The visual encoders for the segmentation task have been extensively explored. common option is to use general backbone pre-trained on ImageNet [3], such as using CNN [175], [214], vision Transformer [205], [215], hybrid CNN-Transformer network [186], [193], [223], multilayer perceptron [199], state space model [220], [238], [239], [253], receptance weighted key value (RWKV) [224], and Kolmogorov-Arnold network [224]. An alternative is to use well-trained perception models such as Point DETR [162] used in [209], DeepLab series [157], [167] applied in [189], [233], [234]. Recently, there has been shift towards promptable architectures. The first way is to exploit the foundation models, for example, by fine-tuning the segment anything modal (SAM) [165] with location prompts [232], adapting SAM during the test time [217], exploiting the hybrid CNN-Transformer network [219], or incorporating trainable adapter layers into the SAM2s encoder [240]. Another way aims to adapt the model to unseen TABLE 4 Summary of segmentation models in colonoscopy. Dataset: C6 [21], ES [30], KS=Kvasir-SEG [38], C3 [19], ETIS [20], HK [43], ASU [22], CDB [27], BKAI=BKAI-Small [46], KSe [55], GI=GIANA [152], SUN-S [62], PG=PolypGen [69], K-I=Kvasir-Instrument [50]. Backbone: ResUNet [153], R-18/-34/-50/-101 [99], R-50v2 [125], R2-50=Res2Net-50 [154], V-16 [124], DeiT [155], Eff-B4= EfficientNet-B4 [156], DLV3+=DeepLab V3+ [157], PB2/3/5=PVTv2-B2/-B3/-B5 [158], CvT [159], MiT-B2 [160], CMLP=CycleMLP [161], P-DETR=Point DETR [162], D-121=DenseNet121 [98], MN= MSCAN [163], Swin-T [164], SAM [165], SAM2 [166], ViT-B16 [100], DLV2=DeepLabV2 [167], HR-W48=HRNet-W48 [168], CN-T=ConvNeXt-Tiny [169], SFB3=SegFormer-B3 [160], M2Former=Mask2Former [170]. Edge-sensitive analysis by explicitly (EX) using edge map as supervision or input and implicitly exploring edge-aware representation (IM#1) or edge-aware uncertainty (IM#2). 7 ThresholdNet [44] ResUNet++ [173] Yuan et al. [171] Model Publication JBHI17 SFA [172] MICCAI19 ISM19 ACSNet [174] MICCAI20 PraNet [175] MICCAI20 UI-CNN [176] MedIA20 TMI20 SCR-Net [177] AAAI21 BI-GCN [178] BMVC21 ICCV21 FDSemi [179] CCBANet [180] MICCAI21 CCD [181] MICCAI21 HRENet [182] MICCAI21 LOD-Net [183] MICCAI21 MSNet [184] MICCAI21 SANet [185] MICCAI21 Transfuse [186] MICCAI21 EndoUDA [49] MICCAI21 domain adaptation, variational autoencoder training UACANet [187] Core design weak bottom-up & strong top-down saliency area & boundary constraints enhanced deep residual UNet [153] adaptive context selection reverse attention, parallel partial decoder Monte Carlo guided back-propagation confidence-guided manifold mixup semantic calibration & refinement boundary-aware graph convolution collaborative & adversarial learning cascading context & balancing attention constrained contrastive distribution learning hard region enhancement learnable oriented derivatives multiscale subtraction network colour exchange & probability correction fusing transformers and CNNs MM21 Jha et al. [55] JBHI21 MPA-DA [188] JBHI21 DW-HieraSeg [189] MedIA21 IJCAI22 BoxPolyp [191] MICCAI22 LDNet [192] MICCAI22 ICGNet [190] uncertainty augmented context attention ResUNet++ [173] with conditional random field & test-time augmentation mutual-prototype adaptation network hierarchical segmentation, dynamic-weighting context-based reverse-contour guidance segmentation with extra box labels dynamic kernel generation & update PPFormer [193] MICCAI22 polyp-guided self-attention, local-to-global method SSFormer [194] MICCAI22 TRFR-Net [195] MICCAI22 JBHI22 AFP-Mask [196] BCNet [197] JBHI22 cross-layer integration, bilateral boundary extraction BSCA-Net [198] stepwise local & global feature aggregation task-relevant feature replenishment anchor-free instance segmentation PR22 Polyp-Mixer [199] TCSVT22 ACL-Net [200] AAAI23 WS-DefSegNet [67] CVPRw23 WeakPolyp [201] MICCAI23 PETNet [202] MICCAI23 bit slice context attention context-aware MLP-based network affinity contrastive learning deformable transformer, sparse foreground loss mask-to-box transformation, scale consistency Gaussian-probabilistic guided semantic fusion S2ME [203] MICCAI23 spatial-spectral mutual teaching, ensemble learning Polyp-PVT [205] RPANet [206] FEGNet [207] BS-Loss [208] Su et al. [204] MICCAI23 AIR23 IPMI23 JBHI23 JBHI23 Point SEGTR [209] MedIA23 MIR23 PR DGNet [210] CFA-Net [211] ColnNet [212] feature propagation & aggregation Improved pyramid vision transformer coarse-to-fine self-supervision feedback enhancement gate network boundary-sensitive loss with location constraint multi-point and symmetric consistency deep gradient learning cross-level feature fusion, boundary aggregation Training dataset Private ES C6, KS ES, KS C6, KS ES ES KS C6, KS C6, KS C6, ES, KS HK, Liu et al. [37] C6, KS C6, KS C6, KS C6, KS C6, KS Celik et al. [49] C6, KS C6, C3, ETIS, KS, ASU, CDB ES, KS ES ES, KS C6, KS Private, C6, KS C6, KS C6, KS C3, ETIS, KS Private, GI KS C6, KS C6, KS C6, KS W-Polyp [67] SUN-S, Private C6, KS SUN-S C6, KS C6, KS Private C6, KS K-I C6, ETIS C6, KS C6, KS C6, KS C6, KS C6, KS C6, KS Testing dataset C6 ES C6, KS ES, KS C6, ES, KS, C3, ETIS ES ES, WCE-Polyp [44] KS C6, ES, KS, C3, ETIS C6, KS C6, ES, KS HK, Liu et al. [37] C6, KS, C3 C6, ES, KS, C3, ETIS C6, ES, KS, C3, ETIS C6, ES, KS, C3, ETIS C6, ES, KS, C3, ETIS Celik et al. [49] C6, ES, KS, C3, ETIS C6, C3, ETIS, KS, ASU, CDB ETIS ES ES, C3, KS C6, ES, KS, C3, ETIS Private, C6, KS, C3, ETIS C6, ES, KS, C3, ETIS C6, C3, ETIS, KS C3, ETIS, KS Private, C6, ETIS C6, ES, KS C6, ES, KS, C3, ETIS C6, KS, C3, ETIS C6, ES, KS, C3, ETIS C6, KS, C3, ETIS, ES SUN-S, Private C6, ES, KS, C3, ETIS C6, KS, SUN-S, PG C6, ES, KS, C3, ETIS C6, ES, KS, C3, ETIS C6, ETIS, KS C6, ES, KS, C3, ETIS K-I C6, ETIS C3, ETIS C6, ES, KS, C3, ETIS C6, ES, KS, C3, ETIS C6, KS C6, ES, KS, C3, ETIS C6, KS, C3, ETIS, ES TMI23 statistical attention, anomaly boundary approximation MCANet [214] FANet [213] TNNLS23 arXiv23 Polyper [215] AAAI24 EMCAD [216] CVPR24 efficient multi-scale convolutional attention decoding C6, C3, ETIS, KS, BKAI C6, C3, ETIS, KS, BKAI feedback attention, run-length encoding multi-scale cross-axis attention boundary sensitive attention Sch on et al. [217] CVPR24 ICML24 MH-pFLID [218] ASPS [219] MICCAI24 SAM [165], test-time adaptation federated learning, injection-distillation paradigm SAM [165], uncertainty-guided regularisation Polyp-Mamba [220] MICCAI24 vision state space model, semantic relationship mining QueryNet [222] MICCAI24 LSSNet [223] MICCAI24 MM BSBP-RWKV [224] unified framework of segmentation & detection local & shallow feature supplementation Perona-Malik diffusion, RWKV [225] CFATransUnet [226] CIBM24 channel-wise cross fusion attention and transformer PolypMixNet [227] CIBM24 JBHI24 JBHI24 JBHI24 MIR24 out-of-distribution modelling, latent standardisation consistency regularisation, soft pseudo labeling randomised global illumination augmentation multi-task synergetic network multi-scale dual-encoding network RGIAug [228] EMTS-Net [229] MSDE-Net [230] Polyp-OOD [231] MedSAM [232] NComs24 FoBS [233] TCSVT SAM [165], cross-organ/modality tuning multi-level boundary-enhanced framework DCL-PS [234] Gao et al. [235] U-KAN [236] SliceMamba [238] ProMamba [239] SAM2-UNet [240] in-context learning, dual similarity checkup U-shaped Kolmogorov-Arnold network [237] vision state space model, bidirectional slice scan TMI24 domain-interactive contrastive learning, self-training TMI24 arXiv24 arXiv24 arXiv24 vision state space model, promptable segmentation arXiv24 Puyal et al. [241] MICCAI20 PNS-Net [242] MICCAI21 SSTAN [243] MICCAI22 IJCAI22 TCCNet [244] Puyal et al. [245] MedIA22 MIR22 SAM2 [166], adapter-based tuning temporal correlation via hybrid 2D/3D CNNs normalised self-attention, progressive learning spatial-temporal attention temporal consistency, context-free loss extend [241] with optimal setups extend [242] with global-to-local learning PNS+ [62] EUVPS [246] AAAI24 cross-scale region linking, cross-wise scale alignment LGRNet [247] MICCAI24 cyclic neighbourhood propagate, Hilbert selective scan SALI [248] MICCAI24 short-term alignment, long-term interaction module Diff-VPS [249] MICCAI24 FlowICBNet [250] CIBM24 MIR24 Drag&Drop [251] arXiv24 SSTFB [252] arXiv24 video state space model, spatio-temporal selective scan Vivim [253] arXiv24 MAST [254] diffusion model, adversarial temporal reasoning iterative feedback units, frame filtering & selecting weakly-supervised temporal annotator self-supervised encoder, sub-branching mechanism Siamese transformer, mixture attention module K-I, CDB, KS Private C6, KS C6, KS C6, KS C6, KS KS C6, KS C3, C6, KS, ETIS C3, C6, ETIS, KS C6, KS K-I SUN-S Hybrid datasets KS, ES ETIS, HK, ES, KS C3 C6 C6, KS C6, KS C6, KS Private, KS C6, C3, ASU, KS LDPolyVideo [48] C6, C3 Private, KS SUN-S SUN-S, C6 C6, C3, SUN-S SUN-S SUN-S SUN-S SUN-S SUN-S KS, ASU, C3, C6 SUN-S K-I, CDB, KS Private C6, ES, KS, C3, ETIS C6, ES, KS, C3, ETIS C6, ES, KS, C3, ETIS C6, ES, KS, C3, ETIS KS C6, KS C3, C6, KS, ETIS C3, C6, ETIS, KS C6, ES, KS, C3, ETIS K-I SUN-S, C6, C3, ETIS, KS Hybrid datasets KS, ES, ETIS, C3 ES, KS C3 C6 C6, KS C6, KS, C3, ETIS, ES, BKAI C6, ES, KS, C3, ETIS Private C6, C3 LDPolyVideo [48] C6, C3, ETIS Private, SUN SUN-S SUN-S, C6 C6, C3, SUN-S SUN-S SUN-S SUN-S SUN-S SUN-S, CDB C3, C6 SUN-S Backbone Arch Edge Customised BF#1 Customised BF#1 ResUNet MF#1 BF#1 BF#1 EX, IM#1 MF#1 BF#2 EX, IM#1 R-34 R2-50 V-16 R-101 - EX - - IM# R2-50 - EX IM#1 IM#1 - EX EX Customised MF#1 BF#1 Customised BF#2 BF#1 BF#2 BF#1 MF#2 MF#1 EX, IM#1 MF#2 R-50v2, ViT-B16 BF#2 BF#2 BF#1 EX, IM#2 R-34 R-18 R-34 R-50 R2-50 R2-50 Eff-B4 R2-50 - EX EX Sup US FS FS FS FS FS FS FS FS SS FS US FS FS FS FS FS URL - Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link FS, US Link Link FS ResUNet MF#1 - FS Link PB2 PB2 R-101 R2-50 MN Swin-T PB2 SAM P-DETR Eff-B4 R2-50 D-121 R2-50 R2-50 CMLP R-50 R2-50 PB2 PB2 R-101 DLV3+ R-34 R2-50, PB2 R2-50 V-16, CvT MiT-B2 RIM#2 - EX - - EX - - - EX EX - - - - - - EX EX - EX EX - EX EX EX - - IM#1 - EX - IM#2 EX - EX EX - - - - - - - BF#2 BF#1 BF#1 BF#1 BF#1 BF#1 MF#1 BF#2 Customised BF#1 BF#1 BF#1 BF#1 BF#2 BF#1 BF#2 BF#1 Customised BF#1 BF#1 BF#1 BF#2 BF#1 Customised MF#1 BF#2 BF#1 BF#1 BF#1 Customised MF#2 MF#1 MF#1 BF#1 BF#2 Customised BF#2 SAM, MN BF#2 VMamba [221] MF#1 M2Former MF#2 BF#1 Customised BF#1 BF#1 BF#2 Customised BF#2 BF#1 R-34, DeiT MF#2 MF#2 MF#2 BF#2 EX, IM#1 BF#2 BF#2 Customised MF#1 Customised MF#1 Vim [149] MF#2 BF#1 MF#2 BF#2 BF#2 BF#2 EX, IM#1 BF#2 BF#2 BF#2 BF#2 BF#2 BF#2 BF#2 BF#2 BF#2 Customised BF#2 BF#2 SAM2 R-101 R2-50 ResUNet R2-50 R-101 R2-50 HR-W48 R2-50 PB5 SFB3 R2-50 - R2-50 ViT-B16 SAM DLV3+ DLV2 SAM - - - - EX - - - - EX - - - - - - EX - - - PB3 R-34 R2-50 PB2 PB2 FS, US Link Link - Link Link - Link FS, US Link FS FS WS FS FS FS FS FS FS FS SS - - Link Link Link WS, SS Link Link Link Link - Link - - Link WS FS WS FS FS FS, US FS FS FS, WS, SS - FS FS FS FS FS FS FS WS FS FS FS FS FS FS FS SS FS FS FS US WS FS Link Link - Link Link Link Link - Link Link - Link Link - Link Link Link - - Link Link Link FS, US Link WS FS FS WS FS FS FS SS SS FS FS FS FS FS Link - - Link - Link Link Link - Link Link Link Link FS, US Link Link Link - Link Link FS WS US, FS FS FS d e - m e d b - V scenarios through in-context learning [235]. (b) Architecture: The community favours the encoder-decoder design for its superior ability to perceive hierarchical features. Current models usually opt for multistream or branched frameworks, as in Fig. 3. Various modifications in this area have been explored, such as incorporating residual connected flows [55], [173], probing cross-task synergy [222], providing additional edge cues [210], using the model-ensembling strategy [186], [193], [219], calculating latent statistics [231], exploring spatio-temporal relationships through 3D conTABLE 5 Comparison of image polyp segmentation models. They are evaluated using the mean scores (%) of structure measure (S [258]) and Dice coefficient (D) on two test sets, with boxplots illustrating the distribution of their consistency and variability across test cases. Kvasir-SEG (100 test images) [38] CVC-ClinicDB (62 test images) [21] 8 Model [#Rank] Polyp-PVT [205] 92.51 [#1] CFA-Net [211] 92.40 [#2] MSNet [184] 92.31 [#3] BoxPolyp [191] 92.30 [#4] SSFormer [194] 92.21 [#5] UACANet [187] 91.67 [#6] PraNet [175] 91.50 [#7] SANet [185] 91.45 [#8] DGNet [210] 90.98 [#9] MCANet [214] 90.25 [#10] Polyper [215] 90.08 [#11] UNet++ [259] 86.21 [#12] UNet [260] 85.76 [#13] SFA [172] 78.14 [#14] 0.0 0.5 1.0 [#Rank] 0.0 0.5 1.0 91.74 [#2] 91.47 [#4] 90.23 [#7] 91.84 [#1] 91.71 [#3] 91.21 [#5] 89.82 [#8] 90.41 [#6] 89.72 [#9] 89.55 [#10] 89.12 [#11] 82.08 [#12] 81.83 [#13] 72.31 [#14] [#Rank] 95.00 [#2] 95.07 [#1] 94.68 [#3] 93.70 [#6] 92.87 [#9] 94.30 [#4] 93.68 [#7] 93.98 [#5] 93.39 [#8] 91.79 [#10] 91.29 [#11] 87.33 [#13] 89.00 [#12] 79.33 [#14] 0.0 0.5 1.0 [#Rank] 0.0 0.5 1.0 93.68 [#1] 93.25 [#2] 91.48 [#6] 91.81 [#4] 90.60 [#7] 92.63 [#3] 89.90 [#9] 91.57 [#5] 90.44 [#8] 89.70 [#10] 88.63 [#11] 79.42 [#13] 82.25 [#12] 70.06 [#14] volutions [241], [245] or self-attention modules [62], [242], [248], and approaching with the teacher-student paradigm [67], [209]. (c) Edge-sensitive analysis: Geometric patterns are beneficial in enhancing the models capability to differentiate foreground objects from the background. The current techniques are in two main ways. The first involves the explicit use of edge maps derived from image gradients, either for direct supervision [172], [182], [210] or as an auxiliary input [49]. Moreover, some methods emphasise edge-aware calculation within their loss functions, such as boundary weighted [193], [240] and customised [183], [208]). Second, edge information can be implicitly integrated by embedding edge-aware representations (e.g., reverse attention [175], morphology operator [215], subtraction operator [44]), or by quantifying edge-aware uncertainties [176], [187]. Moreover, some methods adopt hybrid strategy, e.g., both the subtraction operator and the edge-aware loss are used in MSNet [184]. Output phase. Most models are trained in fully supervised way, always incorporating deep supervision at various decoding stages, as seen in [175], [205]. Recent models have extended beyond with data-efficient approaches, for example, weakly supervised mask-to-box transformation [201] and unsupervised techniques such as contrastive learning [181], out-of-distribution modelling [231], and pseudo-label supervision [191], [243]. Hybrid supervised strategies are also present in which models [49], [188], [195], [206] undergo fully supervised training in the source domain, and are then adapted to the target domain in an unsupervised way. Point SEGTR [209] exploits three types of supervision to enhance the model. Additionally, some teacher-student networks receive hybrid supervision signals; for example, Ren et al. [67] employs weakly supervised approach for the teacher while the student undergoes semi-supervised training. Remarks. To reflect current field progress, we analyse 14 open-source image segmentation models on two popular test datasets, as shown in Tab. 5. All models are trained on the same dataset from Fan et al.s benchmark [175]. First, we observe that current learning strategies are underexplored, as evidenced by BoxPolyp, weakly supervised model obtaining the best score (91.84%) on Kvasir-SEG. In addition, some models achieve better performance, yet exhibit wider interquartile ranges in boxplots, indicating their variability in predictions. For example, Polyp-PVT, which ranks highest in the score on Kvasir-SEG, shows wider interquartile range than other models like SSFormer. From these results, we suggest several promising opportunities for future study. (a) The current gold benchmark [175] comprises less than 1.5K samples and is focused on one category (polyp). In general, scaling up both the data size and diversity could be natural way to improve robustness and generalisability. This demand is driving innovations, such as developing semi-auto image annotator [165], [166] to reduce expert labour and synthesising high-fidelity content via diffusion [261] and autoregressive [262] techniques. (b) Moreover, infinite data scaling is not sustainable. Developing data-efficient strategies [94][96] that require fewer or weaker labels is more cost-effective for average users in the community. (c) Finally, providing procedural support to physicians is essential, including anomaly detection, navigation planning, risk assessment, and intervention advice. We can adopt innovations from similar topics [263]."
        },
        {
            "title": "4.4 VL models",
            "content": "Compared to the above three topics, multimodal research has relatively fewer references. Most existing methods are discriminative models that aim to learn decision boundaries between multimodal inputs. Some studies demonstrate the models effectiveness in referring segmentation tasks, for example, by incorporating textual attention of lesion attributes (e.g., size and number of polyps) into U-shaped model [264], diffusion model [265], or hybrid network [266]. Other studies [267], [268] have developd prompt engineering pipelines based on well-trained GLIP [269] for polyp detection. Moreover, the SAM is capable of operating in VL setting, obtaining location prompts from either the image-text activation map [270] or zero-shot grounding detector [271], [272]. In the MEDVQA-GI competition [65], most solutions are discriminative-based, approaching VL tasks as classification mapping problem, where predefined labels are assigned to image-text pairs. An alternative is generative-based solution [273] that adapts pretrained BLIP-2 model [274] to generate predictions. Remarks. Two possible reasons might explain the lag in VL research for colonoscopy. (a) Data-centric issue. The lack of well-structured and high-quality image-text pairs hinders progress. Future insights can be learnt from existing (a) Data overview (c) Data taxonomy (e) Pipeline of caption generation (b) Data statistic (colonoscopy images) Train Val Test Total Positive 74,407 8,929 45,284 128,620 Negative 106,570 17,328 50,483 174,381 Total 180,977 26,257 95,767 303,001 (d) Word cloud distribution CLS REG REC (f) Data statistic (human-machine dialogues) Total Train 74,407 54,237 54,237 74,407 257,288 8,929 4,874 4,874 27,606 Val Test 45,284 37,631 37,631 45,284 165,830 Total 128,620 96,742 96,742 128,620 450,724 8,929 CAP Fig. 4. Details of the established ColonINST. (a) Three sequential steps to create the instruction tuning dataset for multimodal research. (b) Numbers of colonoscopy images designated for training, validation, and testing purposes. (c) Data taxonomy of three-level categories. (d) word cloud of the category distribution by name size. (e) Caption generation pipeline using the VL prompting mode of GPT-4V [4]. (f) Numbers of humanmachine dialogues created for four downstream tasks. ideas. First, crawling unlabelled image-text data from social media [275] and scientific literature [276] can be used to build domain-specific foundation models. Second, language models such as GPT-4V [4] can generate diverse professional descriptions, offering an economical and scalable way to expand the knowledge space of the data. (b) Modelcentric issue. Current VL techniques in colonoscopy have not kept pace, even with recent developments in multimodal language models (MLMs) [8], [277] for general domains. These techniques opt for decoder-only strategy that unifies multiple tasks (e.g., detection, captioning) into unified autoregressive framework (i.e., next-token prediction). These models are flexibly capable of processing input and output texts of varying lengths, without requiring additional taskspecific prediction heads for different tasks."
        },
        {
            "title": "5.1 Established instruction tuning dataset: ColonINST",
            "content": "Overview. Fig. (4-a) depicts our semi-automated workflow to create instruction tuning data in three steps. We begin by assembling colonoscopy images from public datasets. Following this, we use category-specific prompt to interact with multimodal AI chatbot, GPT-4V [4], yielding descriptive medical captions for these positive cases within the assembled data. Lastly, we reorganise the instruction tuning pairs derived from the data afore-prepared, enabling the model to perform four different colonoscopy tasks in an interactive, dialogue-based manner. Data collection. As shown in Fig. (4-b), ColonINST contains 303,001 colonoscopy images, including 128,620 positive and 174,381 negative cases collected from 19 different data sources. To ensure data integrity and avoid label leakage, we establish series of management rules to divide each dataset. For datasets with predefined divisions, such as KUMC [57], PICCOLO [39], WCE-CCDD [60], BKAI-Small [46], CP-CHILD [51], Kvasir-Instrument [50], and PS-NBI2K [16], we follow their original division rules. When such predefined rules are not available, we conform to widely recognised benchmarks, such as CVC-ClinicDB [21], CVCColonDB [19], ETIS-Larib [20], and the polyp category in Kvasir [28] according to the benchmark by Fan et al. [175], as well as positive samples in SUN-database [54] following the benchmark by Ji et al. [62]. For the remaining datasets (HyperKvasir [43], Kvasir-Capsule [56], CPC-Paired [47], Nerthus [29], GastroVision [66], EDD2020 [40], PolypGen [69], negative samples in SUN-database [54], remaining categories in Kvasir [28]), we allocate them proportionally as 60%/10%/30% for training/validation/testing purposes. image diWith the above management rules, our final vision for is roughly 59.73%/8.77%/31.61%. As depicted in Fig. (4-c), all images are classified into three-level structure, including 4 root/13 parent/62 child categories. In detail, the root level contains three positive categories: the pathological findings of various colonic diseases (110,970 cases); the anatomical structure related to the colon (5,511 cases); and therapeutic interventions related to colonoscopy (12,139 cases), covering the pre-operative, intra-operative, and post-operative stages. Targets outside our interest (e.g., stomach, oesophagus, normal Z-line, and gastric polyp not TABLE 6 Details of instruction tuning dataset ColonINST. For each task, we provide five templates for human instructions, the data sources used to organise human-machine dialogues, and an example of human-machine conversation. 10 Human-machine dialogue sample Task CLS REG REC CAP Instruction templates 1. Categorize the object. 2. Determine the objects category. 3. Identify the category of the object. 4. Classify the objects category. 5. Assign the object to its corresponding category. 1. What category does {object coordinates} belong to? 2. Can you tell me the category of {object coordinates}? 3. Could you provide the category for {object coordinates}? 4. Please specify the category of {object coordinates}. 5. What is the category for {coordinates}? 1. Where is the location of {object category}? 2. Could you give the position of {object category}? 3. Where is {category} located? 4. Could you specify the location of {object category}? 5. Please specify the coordinates of {object category}. 1. Describe what you see in the image. 2. Interpret what the image shows. 3. Detail the visual elements in the image. 4. Explain the images visuals thoroughly. 5. Offer thorough explanation of the image. Data source 19 sources SUN-database [54], PolypGen [69], CVC-ClinicDB [21], ETIS [20], KUMC [57], Kvasir [28], PSNBI2K [16], CVC-ColonDB [19], EDD2020 [40], Kvasir-Capsule [56], CP-CHILD [51], BKAI-Small [46], PICCOLO [39], WCE-CCDD [60], CPC-Paired [47], HyperKvasir [43], Nerthus [29], GastroVision [66], Kvasi-Instrument [50] 11 sources SUN-database [54], PolypGen [69], CVC-ClinicDB [21], ETIS [20], KUMC [57], Kvasir [28], PSNBI2K [16], CVC-ColonDB [19], EDD2020 [40], Kvasir-Capsule [56], Kvasi-Instrument [50] 11 sources SUN-database [54], PolypGen [69], CVC-ClinicDB [21], ETIS [20], KUMC [57], Kvasir [28], PSNBI2K [16], CVC-ColonDB [19], EDD2020 [40], Kvasir-Capsule [56], Kvasi-Instrument [50] 19 sources SUN-database [54], PolypGen [69], CVC-ClinicDB [21], ETIS [20], KUMC [57], Kvasir [28], PSNBI2K [16], CVC-ColonDB [19], EDD2020 [40], Kvasir-Capsule [56], CP-CHILD [51], BKAI-Small [46], PICCOLO [39], WCE-CCDD [60], CPC-Paired [47], HyperKvasir [43], Nerthus [29], GastroVision [66], Kvasi-Instrument [50] occurred during colonoscopy) or lack of objects (e.g., normal mucosa, colon background) are classified under the negative root category (174,381 cases). We intentionally keep these negative samples, as they may be valuable for future gastrointestinal research. Fig. (4-d) presents word cloud distribution for all categories names within ColonINST. Caption generation. The behavioural study [279] suggests that language affects both higher-level (recognition) and lower-level (discrimination) processes of visual perception. This encourages us to extend positive cases (i.e., 128,620 images featuring various colonoscopic findings) with descriptive captions. For this purpose, straightforward way to create captions is to wrap the category name in basic template, like photo of [category] as used by Radford et al. [150]. However, these simple sentences tend to yield suboptimal multimodal alignment, as they are less informative. As shown in Fig. (4-e), we introduce pipeline to generate more descriptive captions. We interact with GPT-4V [4] using custom prompt for each colonoscopy image. These prompts act as prior, conditioned by the images category. Compared to simple sentences, the generated captions explain three features. First, our generated captions describe the unique patterns of the target, such as smooth, round, and nodular appearance, providing details of the surface of the object. Second, conditioned by category priors, our captions can better differentiate between benign hyperplastic polyps and malignant lesions, describing lesion as surface appears intact without evidence of erosion or ulceration. Third, our captions cover not only the lesion itself but also the surroundings, e.g., mucosa appears slightly erythematous, but without significant inflammatory changes, offering more holistic and accurate interpretation. Organising tuning data. In the final step, we convert all positive cases into one-round conversational format, i.e., image+human instruction machine response. As depicted in Fig. (4-f), we reorganise 450,724 human-machine dialogue pairs from various image/label/caption sources, covering four different tasks. Specifically, the classification task (CLS) requires the model to assign category tag to colonoscopy image. Using the localisation labels, we introduce two related tasks: referring expression generation (REG), which involves classifying specified image region, and referring expression comprehension (REC), which inFig. 5. Response comparison for colonoscopy image classification. We evaluate the zero-shot language responses from three AI chatbots against the response from our multimodal model, ColonGPT. volves locating an object with the given category. We also introduce the image captioning (CAP) task that uses GPT4V-generated captions as machine responses. To enhance dialogue diversity, we set up five question templates per task, in which we randomly select one to form humanmachine instruction pair, explained in Tab. 6."
        },
        {
            "title": "5.2 Proposed multimodal language model: ColonGPT",
            "content": "Motivation. As illustrated in Fig. 5, three AI chatbots LLaVA-v1.5 [8], LLaVA-Med-v1.5 [280], and GPT-4V [4] are evaluated for their zero-shot language response capabilities. They often produce inaccurate or vague responses and cannot be readily adaptable to specific colonoscopy tasks. This motivates us to develop colonoscopy-specific MLM for the community. As result, ColonGPT classifies the image as polyp according to the user instructions, allowing for more precise and personalised applications. Overview. We strive to verify the efficacy of language models (LMs) in interpreting both visual and textual signals within the field of medical optical imaging. We present baseline model, called ColonGPT, to execute colonoscopy tasks following human instructions. As shown in the left of Fig. 6, we follow the framework of MLM [8], which typically involves four basic components. (a) language tokenizer translates human instruction Xq into sequence of tokens D, where Nq signifies the length of textual tokens Tq and represents the embedding dimension. (b) visual RNq 11 1 + + s2 + 1) denotes D, where Nv = (s2 RNv tokens Tv the length of the visual tokens. Implementation. Our model can be integrated with modern off-the-shelf visual encoders and LMs. To improve reproducibility for average users, we implement ColonGPT in an resource-friendly way. First, we employ SigLIP-SO (0.4B parameters) [5] as the visual encoder, with an input resolution of = = 384, patch size of = 14, and visual embedding dimension of = 1152. This configuration yields 1152, where the visual embedding Ev with shape R729 2. In addition, Phinumber of visual tokens is 729 = 1.5 (1.3B parameters) [6] serves as the language tokenizer and language model, with embedding size of = 2048. To reduce computational cost, the size of pooling kernels is set to , significantly reducing the visual tokens Nv from 729 to 246, reduction of 66.26%. This design allows us to complete training in five hours, facilitating rapid proof-of-concept development. s1, s2} 384 14 14, 7 = { { } Training recipe. We implement our model using PyTorch library, accelerated by four NVIDIA A100-40GB GPUs. The AdamW optimiser is used with an initial learning rate of 2e-3 and cosine learning rate scheduler. Our ColonGPT is trained on the combination of training and validation dialogues from ColonINST. The complete training runs for three epochs, with batch size of 16 per GPU and gradient accumulation every 2 steps. During training, the visual encoder is frozen, focusing on the trainable multimodal adapter and the LM. For efficiency, we utilise the low-rank adaptation (LoRA [282]) strategy for LM, with rank of = 128 and scaling factor of α = 256."
        },
        {
            "title": "5.3.1 Multimodal benchmark",
            "content": "Model competitors. To establish widely accepted multimodal benchmark for the community, we select eight popular MLMs as competitors, including six general-purpose and two medically tailored models. As shown in Tab. 7, each competitor has two training setups depending on whether it uses LoRA [282] or initialises knowledge from additional pre-training data. We retrain all competitors using the combined training and validation dialogues from ColonINST, as used in our ColonGPT. Evaluation protocols. We quantitatively evaluate three conversational tasks for the multimodal benchmark. For the two classification-based tasks, namely CLS and REG, ) to calculate the ratio we adopt the accuracy metric ( of correctly predicted categories to the total number of predictions. For the REC task, we use the intersection over union (IoU) metric to measure the localisation precision. Furthermore, due to the subjective nature of language in the CAP task, we qualitatively analyse the medical accuracy of the responses by verifying the correct identification of the anatomical structures and category names visible in the images, or relevant clinical information descriptions. Learning ability. We begin by conducting an open-book test for each model to quantitatively measure how effectively each model has internalised the visual and linguistic patterns from the training phase. Specifically, we evaluate each model on the samples they have seen during training, i.e., 27,606 validation dialogues. The seen columns in Tab. Fig. 6. Details of our multimodal language model, ColonGPT. RH encoder, typically based on transformer, condenses 3, with height and width colonoscopy image Xv HW d. Here, , into flattend visual embedding Ev 2 denotes the patch size and refers to the token dimension. (c) multimodal adapter transforms the visual embedding D, matching the Ev into Nv numeric tokens Tv language dimension with Tq. (d) Finally, language model receives the concatenated visual tokens Tv and text tokens Tq as input. Using the chain rule of probability, sequence of length is generated in an autoregressive way, formulated as p(Y) = (cid:81)L Tv, Tq, Y<i), where Y<i = [y1, y2, 1] is the sequence of predicted language tokens indexed before i. i=1 p(yi RNv , yi P HW 2 Multigranularity multimodal adapter. Previous works [7][9] generally employ multilayer perceptron architecture as multimodal adapter, typically consisting of triple linear layers with intervening GELUs. However, handling all visual tokens introduces redundancy because not every token is equally significant, and it also incurs higher computational costs given the quadratic complexity in relation to the number of input tokens. To embrace these challenges, we propose multimodal adapter that incorporates multigranularity pooling layers between two linear layers. As illustrated in the right of Fig. 6, we transform from d-dim to D-dim using the embedding Ev linear layer followed by GELU, then reshape it into D. To reduce the number the spatial format Fv of visual tokens while avoiding performance drops, we roll out three modifications for the pooling phase, each validated in 5.3.2. (a) Multigranularity views. We add set of adaptive average pooling operations with kernel sizes s1, . . . , sN } to obtain multigranularity features. In particu- { lar, this adaptive operator accommodates input sequences of varying lengths, i.e., the pooled feature for the kernel size D. (b) Positional encoding. Inspired sn is shaped as Rsn by [281], we enhance the spatial information within each pooled feature by applying 2D convolutional layer with the appropriate zero-padding setting. By default, zeropixel boundary is added around the input feature. (c) Global view. We also use global average pooling layer with kernel size 1 on the feature Fv to obtain global view with the D. Lastly, we reshape each pooled feature shape of R1 Rs2 D. We into flattened vectors: concatenate these vectors and process the resulting vector through the second linear layer to produce the final visual n=1 and Fg R1 sn Fn { } 1 12 TABLE 7 Multimodal benchmark for three conversational tasks. LoRA refers to fine-tuning using low-rank adaptation [282]. EXT indicates the use of pre-trained weights on extra data. We compare the results on the seen samples from the validation set and the unseen samples from the testing set of ColonINST. The symbol signifies that higher score reflects better performance. Visual encoder (input shape/URL) Language model (model size/URL) REC task (IoU LoRA EXT Model MiniGPT-v2 [7] EVA-G/14 (448px/link) LLaMA2 (7B/link) LLaVA-v1 [277] CLIP-L/14 (224px/link) Vicuna-v1.3 (7B/link) LLaVA-v1.5 [8] CLIP-L/14 (336px/link) Vicuna-v1.5 (7B/link) Bunny-v1.0-3B [9] SigLIP-SO (384px/link) Phi2 (2.7B/link) MGM-2B [283] CLIP-L/14 (336px/link) & ConvNeXt-L (768px/link) Gemma (2B/link) MobileVLM-1.7B [284] CLIP-L/14 (336px/link) MobileLLaMA (1.4B/link) LLaVA-Med-v1.0 [280] CLIP-L/14 (224px/link) LLaMA1 (7B/link) LLaVA-Med-v1.5 [280] CLIP-L/14 (224px/link) Mistral-v0.2 (7B/link) ColonGPT (Ours) SigLIP-SO (384px/link) Phi1.5 (1.3B/link) No. #A1 #A2 #B1 #B2 #C1 #C2 #D1 #D2 #E1 #E2 #F1 #F2 #G1 #G2 #H1 #H2 - ) CLS task ( unseen seen 15.36% 91.49% 31.13% 90.00% 12.72% 87.86% 3.24% 89.61% 34.32% 92.97% 42.31% 93.33% 31.24% 91.16% 41.48% 92.47% 16.00% 92.97% 25.23% 93.24% 31.46% 93.02% 34.80% 93.64% 24.89% 93.52% 20.85% 93.84% 41.97% 93.62% 87.22% 12.95% 94.02% 85.81% 99.02% 83.42% 65.89% 45.77% REG task ( seen 94.69% 87.65% 84.55% 86.87% 98.58% 99.32% 96.61% 96.02% 98.17% 98.75% 97.78% 97.87% 97.74% 97.35% 99.30% 90.40% ) unseen 72.05% 70.23% 68.11% 46.85% 70.38% 72.88% 69.45% 75.08% 69.81% 74.30% 73.14% 78.03% 75.07% 75.25% 73.05% 70.00% ) unseen 77.93% 76.82% 72.08% 42.17% 79.10% 80.89% 75.50% 79.50% 78.99% 78.69% 78.75% 80.44% 78.04% 77.38% 79.24% 66.51% seen 23.45% 27.97% 20.05% 21.81% 55.72% 61.97% 46.24% 54.00% 39.78% 57.25% 47.30% 51.36% 41.60% 39.43% 64.69% 13.39% Fig. 7. Illustration of ColonGPTs multimodal capabilities. Our model can execute various multimodal colonoscopy tasks through conversational interactions, including comprehension (CLS, REG), localisation (REC), and captioning (CAP) based. 7 show that we achieve the highest scores in the CLS = 94.02%) and REC (IoU = 65.89%) tasks. This implies ( that ColonGPT has better learning ability, which allows it to classify images and understand reference expressions related to specific visual regions. Furthermore, we achieve an accuracy of 99.02%, below 99.32% achieved by LLaVAv1.5 (#C2). This gap ( = 0.3%) is due to 7B-level LM used in LLaVA-v1.5, which has also been pretrained on additional data and thus gained more knowledge. Generalisation ability. We further conduct closed-book test to examine each models ability to generalise knowledge to unseen conditions, i.e., 165,830 testing samples of ColonINST. The unseen columns in Tab. 7 consistently reveal our superior performance in unseen samples in all three tasks. Recall that our model performs worse than LLaVA-v1.5 in the REG task for seen scenarios. However, this gap is overtaken when exposed to unseen scenarios, where we achieve an accuracy of 83.42%, even exceeding the 7B-level LLaVA-v1.5 (#C2) by 10.54%. Compared to the runner-up model, LLaVA-Med-v1.5 (#H1), our model surpasses it by 1.2% in the seen scenarios for the REC task, and this gap is further widened by 3.8% in the unseen scenarios. These results show the potential of ColonGPT as conversational assistant for multimodal colonoscopy tasks, especially in generalising to unseen data. Qualitative analysis. Fig. 7 illustrates our models three multimodal abilities across four conversational tasks. (a) Comprehension ability: In the CLS task, we identify subtle visual features, distinguishing high grade dysplasia from adenoma in visually similar images. In the REG task, we correctly translate complex visual features from the given coordinates into precise medical terminology. (b) Localisation ability: This entails ColonGPT understanding language query and localising visual target within complex colon environment. The outputs of the REC task showcase ColonGPTs precision in localising specified expressions using bounding boxes. (c) Captioning ability: This requires the synthesis of visual information into coherent, clinically relevant text. Our model provides descriptions of pedunculated polyp, detailing its morphology, contextual characteristics, TABLE 8 Diagnostic studies of three core components in ColonGPT. : interpolate the position embeddings for higher resolution, specifically from 224px to 384px. Our default configurations are shaded with gray background. 13 (a) Different presentations from visual encoder CLS REG REC Visual encoder input/URL ConvNeXtV2-L 384px/link 82.95% 78.63% 33.74% ViT-L 384px/link 82.16% 77.04% 40.78% MAE-L* 384px/link 80.85% 75.87% 38.53% MAE-L 224px/link 81.95% 77.62% 43.25% DINOv2-L* 384px/link 35.03% 22.91% 6.79% DINOv2-L 224px/link 21.22% 7.96% 2.69% CLIP-L 336px/link 83.99% 78.67% 41.54% SigLIP-SO 384px/link 85.81% 83.42% 45.77% (b) Multigranuarity multimodal adapter { { CLS REG token (ratio) REC MLP baseline 729 (100.00%) 83.53% 81.80% 43.70% 321 (44.03%) 84.39% 80.90% 46.37% 246 (33.74%) 85.81% 83.42% 45.77% 245 (33.61%) 85.01% 82.49% 43.62% 181 (24.83%) 83.74% 81.60% 45.94% 126 (17.28%) 84.28% 82.01% 46.46% 81 (11.11%) 84.70% 81.36% 45.30% 246 (33.74%) 84.50% 82.91% 40.09% } } } } } } w/o Pos. Enc. 16, 8, 1 14, 7, 1 14, 7 12, 6, 1 10, 5, 1 8, 4, 1 { { { { (c) Fine-tuning strategy CLS REG α REC Strategy - 78.06% 73.79% 50.20% - full-tuning 8 85.43% 82.75% 45.02% 4 LoRA 16 84.45% 80.78% 44.98% LoRA 8 32 84.39% 80.81% 45.90% LoRA 16 LoRA 32 64 84.91% 82.73% 45.56% LoRA 64 128 83.84% 81.19% 43.57% LoRA 128 256 85.81% 83.42% 45.77% LoRA 256 512 82.93% 79.96% 48.27% and potential clinical relevance. Additionally, ColonGPT can describe the treatment procedure when an instrument is present, e.g., The polyp is being examined using snare."
        },
        {
            "title": "5.3.2 Diagnostic studies",
            "content": "Visual encoder. Our diagnostic experiment begins with an inquiry What types of visual representations are appropriate for multimodal colonoscopy data? We prepare four sets of representations from various large-scale pre-training strategies: supervised learning (ConvNeXtV2 [285], ViT [100]), reconstructive learning (MAE [286]), and contrastive learning using vision-only data (DINOv2 [287]) or VL data (CLIP [288], SigLIP [5]). As shown in Tab. (8-a), all encoders use pretrained weights from Huggingface. To ensure consistency, we manually interpolate the smaller position embedding for MAE and DINOv2 from 224px to 384px (marked with ), while leaving the default input of the remaining models unchanged. Our observation reveals that contrastive learning encoders using VL data outperform other strategies. This suggests that visual representations pre-aligned with weak texts during their pre-training facilitate us to transform visual embeddings into the language space. Regarding the other unimodal encoders, both supervised learning methods (ConvNeXtV2, ViT) and the reconstructive approach (MAE) give satisfactory feedback. However, the vision-only contrastive learning model (DINOv2) struggles, especially in the REC and REG tasks. Multiple efforts to optimise DINOv2 with various hyperparameters, including input size, model size, and learning rate, did not produce acceptable results for multimodal colonoscopy tasks. Multigranularity multimodal adapter. It serves as key component in linking vision and language modalities, reducing visual tokens to mitigate computational overhead. As detailed in Tab. (8-b), we analyse its effectiveness from three perspectives. (a) How to configure the pooling kernels? As reference, we initialise baseline variant of ColonGPT with multimodal adapter used in [7], [8], which employs pure MLP architecture to process all input tokens equally. We then gradually decrease the size of the pooling kernels , , across five variants: } . Concerning the performance-cost trade-offs, and { our setup is optimal. It decreases the visual tokens from 100% (729 tokens) to 33.74% (246 tokens) while maintaining impressive results across three conversational tasks. To illustrate, we observe performance gains of 2.28%, 1.62%, and 2.07% in the CLS, REG, and REC tasks, respectively. (b) Is global context necessary? We remove the global view 14, 7, 1 for the multigranularity from our default setup { . adapter, producing controlled variant with setup } The performance then declines, indicating the necessity of } 14, 7, 1 16, 8, 1 12, 6, 14, 7, 1 10, 5, 1 8, 4, 1 14, 7 { { } { { } } { } { } , , capturing the global context within visual embeddings for improved outcomes. (c) Is positional encoding important? As shown in the last row of Tab. (8-b), our setup without positional encoding shows significant performance drop in REC task, from 45.77% to 40.09%. This suggests that the relative position information for the visual sequence is crucial for the localisation task. Fine-tuning strategy. Lastly, we investigate how to effectively tune our model on multimodal colonoscopy data. As shown in Tab. (8-c), we initiate set of variants to tune the language model, Phi1.5. It includes seven variants with different setups for LoRA and full-tuning variant as reference point. The best performance was observed in the LoRA variant with configuration r/α = 128/256, achieving 85.81% and 83.42% in the CLS and REG tasks, respectively. In addition, the other two variants achieve outstanding scores in the REC task, with the full-tuning variant achieving 50.20% and the LoRA with configuration r/α = 256/512 reaching 48.27%. This implies that more tunable parameters for the language model could allow ColonGPT to capture more complex patterns in the spatial planes. Therefore, we believe that there remains room to improve the localisation ability of ColonGPT in the REC task, such as increasing the rank factor and finding matched scaling factor α. For training efficiency, we finally choose r/α = 128/256 as our default configuration, as it allows us to reduce the training time from eight to roughly five hours."
        },
        {
            "title": "5.3.3 Empirical takeaways",
            "content": "This study represents preliminary exploration of multimodal instruction tuning techniques in colonoscopy. We unify the multimodal and multitask paradigms in causal language model, which features two insights: interpreting visual content within the linguistic space and tackling various visual tasks under next-token prediction framework. We finally derive lessons from experiments that may guide future advances in multimodal research. Embracing data scarcity. In general, MLMs [8], [277] opt for two-stage strategy trained on massive data, e.g., 558K samples for multimodal alignment, followed by 665K instruction tuning samples to ensure human compliance. Alternatively, we adopt single-stage strategy to directly fine-tune ColonGPT on comparatively smaller training data with 285K instructions. This strategy appears to be effective in colonoscopy, data-limited scenario. We suggest two feasible ways to compensate for this datacentric issue. (a) Scaling up data size is straightforward way to improve the domain-specific representation ability. cost-efficient way is to consider synthesised data once the public data sources are used up [289]. (b) Diversifying the human-machine dialogue can efficiently train an AI specialist for colonoscopy applications. This involves expanding question-answer pairs with advanced AI chatbots and organising more executable tasks, such as converting masks into polygons for segmentation [290] or modelling multiframe correlations for video analysis [291]. Efficiency drives progress. As discussed above, we take less data to obtain greater performance than other model rivals. This success benefits from the way we build ColonGPT. (a) Colonoscopy data inherently contains redundant information, such as the fact that most mucosal surfaces are similar, as well as camouflaged patterns between benign lesions and their surroundings, as discussed in 2.2. To reduce redundancy, we propose multigranularity multimodal adapter that selectively samples tokens without compromising performance. For future improvement, we can draw on the wisdom of previous token reduction techniques [292]. (b) As shown in Tab. 7, the Phi1.5 model [6], although lightweight, shows surprising efficiency, even outperforming other 7B-level competitors. This indicates that larger models appear to require more colonoscopy data. Thus, future efforts should prioritise enhancing training and inference efficiency, especially for the medical field, rather than racing with massive computational resources. promising idea to streamline the MLM framework using an encoder-free solution [293] to interpret visual pixels. Improving spatial perception. We observe that the ability to accurately locate targets given descriptions remains limited. This is evident in the REC results shown in Tab. 7, where IoU scores fall below 50% in most models when tested on unseen samples. To break through this performance bottleneck, we suggest two potential routes. (a) In constructing ColonGPT, we leverage pre-trained visual encoder and language model from the general domain. This approach presents challenges of the gaps between general and medical optical data, as well as the gap between vision and language modalities. As recommended in [294], pre-training and pre-aligning the multimodal space before instruction tuning would be promising approach to alleviate these issues. (b) The next-token prediction framework of causal language models may struggle with arithmetic tasks due to the snowballing error resulting from the chain rule [295]. For example, LMs are not responsible for accurately predicting coordinates in the REC task. We encourage that the vision and language parts of the next-generation framework can handle their respective roles, such as parallel framework [296] that predicts segmentation masks and generates language captions, simultaneously."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We investigate the frontiers in intelligent colonoscopy techniques and their broader implications in the multimodal field. Our structure follows two primary threads. First, we survey the landscape of four colonoscopic scene perception tasks and sort out the key challenges and understudied areas. Second, our survey reveals that multimodal research in colonoscopy is underexplored. To embrace this, we contribute three initiatives to the community: largeinstruction tuning dataset ColonINST, scale multimodal 14 colonoscopy-specific multimodal language model ColonGPT, and multimodal benchmark. Importantly, multimodal colonoscopy research is rapidly advancing. Future developments can build on recent breakthroughs, such as executing role-playing tasks with agents [297]."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This research was supported by NSFC (NO.62476143). We express our sincere gratitude to Yu-Cheng Chou (JHU) and Stephen Gould (ANU) for interesting discussions."
        },
        {
            "title": "REFERENCES",
            "content": "[4] [3] [1] C. Eng, T. Yoshino, E. Ruız-Garcıa, N. Mostafa, C. G. Cann, B. OBrian, A. Benny, R. O. Perez, and C. Cremolini, Colorectal cancer, The Lancet, vol. 394, no. 10207, pp. 14671480, 2024. [2] M. B. Wallace, P. Sharma, P. Bhandari, J. East, G. Antonelli, R. Lorenzetti, M. Vieth, I. Speranza, M. Spadaccini, M. Desai et al., Impact of artificial intelligence on miss rate of colorectal neoplasia, Gastro, vol. 163, no. 1, pp. 295304, 2022. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: large-scale hierarchical image database, in IEEE CVPR, 2009. J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, Sigmoid loss for language image pre-training, in IEEE ICCV, 2023. Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T. Lee, Textbooks are all you need ii: phi-1.5 technical report, arXiv preprint arXiv:2309.05463, 2023. J. Chen, D. Zhu, X. Shen, X. Li, Z. Liu, P. Zhang, R. Krishnamoorthi, V. Chandra, Y. Xiong, and M. Elhoseiny, Minigpt-v2: large language model as unified interface for vision-language multitask learning, arXiv preprint arXiv:2310.09478, 2023. [5] [6] [7] [8] H. Liu, C. Li, Y. Li, and Y. J. Lee, Improved baselines with visual instruction tuning, in IEEE CVPR, 2024. [9] M. He, Y. Liu, B. Wu, J. Yuan, Y. Wang, T. Huang, and B. Zhao, Efficient multimodal learning from data-centric perspective, arXiv preprint arXiv:2402.11530, 2024. [10] V. S. Prasath, Polyp detection and segmentation from video capsule endoscopy: review, J. Imaging, vol. 3, no. 1, p. 1, 2016. [11] B. Taha, N. Werghi, and J. Dias, Automatic polyp detection in endoscopy videos: survey, in IEEE IASTED, 2017. [12] L. F. Sanchez-Peralta, L. Bote-Curiel, A. Picon, F. M. SanchezMargallo, and J. B. Pagador, Deep learning to find colorectal polyps in colonoscopy: systematic literature review, AIIM, vol. 108, p. 101923, 2020. I. Pacal, D. Karaboga, A. Basturk, B. Akay, and U. Nalbantoglu, comprehensive review of deep learning in colon cancer, CIBM, vol. 126, p. 104003, 2020. [13] [14] B. unzer, K. Schoeffmann, and L. osz ormenyi, Content-based processing and analysis of endoscopic images and videos: survey, MTAP, vol. 77, pp. 13231362, 2018. [15] M. Taghiakbari, Y. Mori, and D. von Renteln, Artificial intelligence-assisted colonoscopy: review of current state of practice and research, WJG, vol. 27, no. 47, p. 8103, 2021. [16] G. Yue, G. Zhuo, S. Li, T. Zhou, J. Du, W. Yan, J. Hou, W. Liu, and T. Wang, Benchmarking polyp segmentation methods in narrow-band imaging colonoscopy images, IEEE JBHI, vol. 27, no. 7, pp. 33603371, 2023. [18] [17] Z. Wu, F. Lv, C. Chen, A. Hao, and S. Li, Colorectal polyp segmentation in the deep learning era: comprehensive survey, arXiv preprint arXiv:2401.11734, 2024. J. Mei, T. Zhou, K. Huang, Y. Zhang, Y. Zhou, Y. Wu, and H. Fu, survey on deep learning for polyp segmentation: Techniques, challenges and future trends, arXiv preprint arXiv:2311.18373, 2023. J. Bernal, J. Sanchez, and F. Vilarino, Towards automatic polyp detection with polyp appearance model, PR, vol. 45, no. 9, pp. 31663182, 2012. [19] [20] [21] J. Silva, A. Histace, O. Romain, X. Dray, and B. Granado, Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer, CARS, vol. 9, no. 2, pp. 283293, 2014. J. Sanchez, G. Fernandez-Esparrach, D. Gil, J. Bernal, F. C. Rodrıguez, and F. Vilari no, Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians, CMIG, vol. 43, pp. 99111, 2015. [22] N. Tajbakhsh, S. R. Gurudu, and J. Liang, Automated polyp detection in colonoscopy videos using shape and context information, IEEE TMI, vol. 35, no. 2, pp. 630644, 2015. [23] M. Ye, S. Giannarou, A. Meining, and G.-Z. Yang, Online tracking and retargeting with applications to optical biopsy in gastrointestinal endoscopic examinations, MedIA, vol. 30, pp. 144157, 2016. [24] F. Deeba, F. M. Bui, and K. A. Wahid, Automated growcut for segmentation of endoscopic images, in IJCNN, 2016. [25] R. Zhang, Y. Zheng, T. W. C. Mak, R. Yu, S. H. Wong, J. Y. Lau, and C. C. Poon, Automatic detection and classification of colorectal polyps by transferring low-level cnn features from nonmedical domain, IEEE JBHI, vol. 21, no. 1, pp. 4147, 2016. [26] P. Mesejo, D. Pizarro, A. Abergel, O. Rouquette, S. Beorchia, L. Poincloux, and A. Bartoli, Computer-aided classification of gastrointestinal lesions in regular colonoscopy, IEEE TMI, vol. 35, no. 9, pp. 20512063, 2016. [27] Q. Angermann, J. Bernal, C. Sanchez-Montes, M. Hammami, G. Fernandez-Esparrach, X. Dray, O. Romain, F. J. Sanchez, and A. Histace, Towards real-time polyp detection in colonoscopy videos: Adapting still frame-based methodologies for video sequences analysis, in MICCAI-W, 2017. [28] K. Pogorelov, K. R. Randel, C. Griwodz, S. L. Eskeland, T. de Lange, D. Johansen, C. Spampinato, D.-T. Dang-Nguyen, M. Lux, P. T. Schmidt et al., Kvasir: multi-class image dataset for computer aided gastrointestinal disease detection, in ACM MMSys, 2017. [29] K. Pogorelov, K. R. Randel, T. de Lange, S. L. Eskeland, C. Griwodz, D. Johansen, C. Spampinato, M. Taschwer, M. Lux, P. T. Schmidt et al., Nerthus: bowel preparation quality video dataset, in ACM MMSys, 2017. [30] D. Vazquez, J. Bernal, F. J. Sanchez, G. Fernandez-Esparrach, A. M. opez, A. Romero, M. Drozdzal, and A. Courville, benchmark for endoluminal scene segmentation of colonoscopy images, JHE, vol. 2017, no. 1, p. 4037190, 2017. [31] A. Koulaouzidis, D. K. Iakovidis, D. E. Yung, E. Rondonotti, U. Kopylov, J. N. Plevris, E. Toth, A. Eliakim, G. W. Johansson, W. Marlicz et al., Kid project: an internet-based digital video atlas of capsule endoscopy for research purposes, EIO, vol. 5, no. 06, pp. E477E483, 2017. I. N. Figueiredo, L. Pinto, P. N. Figueiredo, and R. Tsai, Unsupervised segmentation of colonic polyps in narrow-band imaging data based on manifold representation of images and wasserstein distance, BSPC, vol. 53, p. 101577, 2019. [32] [33] P. N. Figueiredo, I. N. Figueiredo, L. Pinto, S. Kumar, Y.-H. R. Tsai, and A. V. Mamonov, Polyp detection with computeraided diagnosis in white light colonoscopy: comparison of three different methods, EIO, vol. 7, no. 02, pp. E209E215, 2019. [34] T.-H. Hoang, H.-D. Nguyen, V.-A. Nguyen, T.-A. Nguyen, V.-T. Nguyen, and M.-T. Tran, Enhancing endoscopic image classification with symptom localization and data augmentation, in ACM MM, 2019. [35] M. Cho, J. H. Kim, K. S. Hong, J. S. Kim, H.-J. Kong, and S. Kim, Identification of cecum time-location in colonoscopy video by deep learning analysis of colonoscope movement, PeerJ, vol. 7, p. e7256, 2019. S. Ali, F. Zhou, C. Daul, B. Braden, A. Bailey, S. Realdon, J. East, G. Wagnieres, V. Loschenov, E. Grisan et al., Endoscopy artifact detection (ead 2019) challenge dataset, arXiv preprint arXiv:1905.03209, 2019. [36] [37] Y. Liu, Y. Tian, G. Maicas, L. Z. C. T. Pu, R. Singh, J. W. Verjans, and G. Carneiro, Photoshopping colonoscopy video frames, in IEEE ISBI, 2020. [38] D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. d. Lange, D. Johansen, and H. D. Johansen, Kvasir-seg: segmented polyp dataset, in MMM, 2020. [39] L. F. Sanchez-Peralta, J. B. Pagador, A. Pic on, A. J. Calder on, F. Polo, N. Andraka, R. Bilbao, B. Glover, C. L. Saratxaga, and F. M. Sanchez-Margallo, Piccolo white-light and narrow-band [40] imaging colonoscopic dataset: performance comparative of models and datasets, ApplSci, vol. 10, no. 23, p. 8501, 2020. S. Ali, N. Ghatwary, B. Braden, D. Lamarque, A. Bailey, S. Realdon, R. Cannizzaro, J. Rittscher, C. Daul, and J. East, Endoscopy disease detection challenge 2020, arXiv preprint arXiv:2003.03376, 2020. [41] R. Leenhardt, C. Li, J.-P. Le Mouel, G. Rahmi, J. C. Saurin, F. Cholet, A. Boureille, X. Amiot, M. Delvaux, C. Duburque et al., Cad-cap: 25,000-image database serving the development of artificial intelligence for capsule endoscopy, EIO, vol. 8, no. 03, pp. E415E420, 2020. [42] C. C. Poon, Y. Jiang, R. Zhang, W. W. Lo, M. S. Cheung, R. Yu, Y. Zheng, J. C. Wong, Q. Liu, S. H. Wong et al., Ai-doscopist: real-time deep-learning-based algorithm for localising polyps in colonoscopy videos with edge computing devices, NPJDM, vol. 3, no. 1, p. 73, 2020. [43] H. Borgli, V. Thambawita, P. H. Smedsrud, S. Hicks, D. Jha, S. L. Eskeland, K. R. Randel, K. Pogorelov, M. Lux, D. T. D. Nguyen et al., Hyperkvasir, comprehensive multi-class image and video dataset for gastrointestinal endoscopy, SData, vol. 7, no. 1, p. 283, 2020. [45] [44] X. Guo, C. Yang, Y. Liu, and Y. Yuan, Learn to threshold: Thresholdnet with confidence-guided manifold mixup for polyp segmentation, IEEE TMI, vol. 40, no. 4, pp. 11341146, 2020. S. Ali, M. Dmitrieva, N. Ghatwary, S. Bano, G. Polat, A. Temizel, A. Krenzer, A. Hekalo, Y. B. Guo, B. Matuszewski et al., Deep learning for detection and segmentation of artefact and disease instances in gastrointestinal endoscopy, MedIA, vol. 70, p. 102002, 2021. [46] P. Ngoc Lan, N. S. An, D. V. Hang, D. V. Long, T. Q. Trung, N. T. Thuy, and D. V. Sang, Neounet: Towards accurate colon polyp segmentation and neoplasm detection, in ISVC, 2021. [47] Q. Wang, H. Che, W. Ding, L. Xiang, G. Li, Z. Li, and S. Cui, Colorectal polyp classification from white-light colonoscopy images via domain alignment, in MICCAI, 2021. [48] Y. Ma, X. Chen, K. Cheng, Y. Li, and B. Sun, Ldpolypvideo benchmark: large-scale colonoscopy video dataset of diverse polyps, in MICCAI, 2021. [49] N. Celik, S. Ali, S. Gupta, B. Braden, and J. Rittscher, Endouda: modality independent segmentation approach for endoscopy imaging, in MICCAI, 2021. [50] D. Jha, S. Ali, K. Emanuelsen, S. A. Hicks, V. Thambawita, E. Garcia-Ceja, M. A. Riegler, T. de Lange, P. T. Schmidt, H. D. Johansen et al., Kvasir-instrument: Diagnostic and therapeutic tool segmentation dataset in gastrointestinal endoscopy, in MMM, 2021. [51] W. Wang, J. Tian, C. Zhang, Y. Luo, X. Wang, and J. Li, An improved deep learning approach and its applications on colonic polyp images detection, BMCMI, vol. 20, pp. 114, 2020. [52] A. de Maissin, R. Vallee, M. Flamant, M. Fondain-Bossiere, C. Le Berre, A. Coutrot, N. Normand, H. Mouch`ere, S. Coudol, C. Trang et al., Multi-expert annotation of crohns disease images of the small bowel for automatic detection using convolutional recurrent attention neural network, EIO, vol. 9, no. 07, pp. E1136E1144, 2021. [53] Z. Kong, M. He, Q. Luo, X. Huang, P. Wei, Y. Cheng, L. Chen, Y. Liang, Y. Lu, X. Li et al., Multi-task classification and segmentation for explicable capsule endoscopy diagnostics, FMOLB, vol. 8, p. 614277, 2021. [54] M. Misawa, S.-e. Kudo, Y. Mori, K. Hotta, K. Ohtsuka, T. Matsuda, S. Saito, T. Kudo, T. Baba, F. Ishida et al., Development of computer-aided detection system for colonoscopy and publicly accessible large colonoscopy video database (with video), GIE, vol. 93, no. 4, pp. 960967, 2021. [55] D. Jha, P. H. Smedsrud, D. Johansen, T. de Lange, H. D. Johansen, P. Halvorsen, and M. A. Riegler, comprehensive study on colorectal polyp segmentation with resunet++, conditional random field and test-time augmentation, IEEE JBHI, vol. 25, no. 6, pp. 20292040, 2021. [56] P. H. Smedsrud, V. Thambawita, S. A. Hicks, H. Gjestang, O. O. Nedrejord, E. Næss, H. Borgli, D. Jha, T. J. D. Berstad, S. L. Eskeland et al., Kvasir-capsule, video capsule endoscopy dataset, SData, vol. 8, no. 1, p. 142, 2021. [57] K. Li, M. I. Fathan, K. Patel, T. Zhang, C. Zhong, A. Bansal, A. Rastogi, J. S. Wang, and G. Wang, Colonoscopy polyp detection and classification: Dataset creation and comparative evaluations, PONE, vol. 16, no. 8, p. e0255809, 2021. [58] J. Cychnerski, T. Dziubich, and A. Brzeski, Ers: novel comprehensive endoscopy image dataset for machine learning, compliant with the mst 3.0 specification, arXiv preprint arXiv:2201.08746, 2022. [59] Y. Tian, G. Pang, F. Liu, Y. Liu, C. Wang, Y. Chen, J. Verjans, and G. Carneiro, Contrastive transformer-based multiple instance learning for weakly supervised polyp frame detection, in MICCAI, 2022. [60] F. J. P. Montalbo, Diagnosing gastrointestinal diseases from endoscopy images through multi-fused cnn with auxiliary layers, alpha dropouts, and fusion residual block, BSPC, vol. 76, p. 103683, 2022. S. Ali and N. Ghatwary, Endoscopic computer vision challenges 2.0, 2022. [Online]. Available: https://endocv2022. grand-challenge.org/ [61] [62] G.-P. Ji, G. Xiao, Y.-C. Chou, D.-P. Fan, K. Zhao, G. Chen, and L. Van Gool, Video polyp segmentation: deep learning perspective, MIR, vol. 19, no. 6, pp. 531549, 2022. [63] V. Thambawita, P. Salehi, S. A. Sheshkal, S. A. Hicks, H. L. Hammer, S. Parasa, T. d. Lange, P. Halvorsen, and M. A. Riegler, Singan-seg: Synthetic training data generation for medical image segmentation, PONE, vol. 17, no. 5, p. e0267976, 2022. [64] D. Fitting, A. Krenzer, J. Troya, M. Banck, B. Sudarevic, M. Brand, W. ock, W. G. Zoller, T. osch, F. Puppe et al., video based benchmark data set (endotest) to evaluate computer-aided polyp detection systems, SJG, vol. 57, no. 11, pp. 13971403, 2022. S. Hicks, A. Storas, P. Halvorsen, T. de Lange, M. Riegler, and V. Thambawita, Overview of imageclef medical 2023medical visual question answering for gastrointestinal tract, in CLEF (Working notes), 2023. [65] [66] D. Jha, V. Sharma, N. Dasu, N. K. Tomar, S. Hicks, M. Bhuyan, P. K. Das, M. A. Riegler, P. Halvorsen, T. de Lange et al., Gastrovision: multi-class endoscopy image dataset for computer aided gastrointestinal disease detection, in ICML-W, 2023. [67] G. Ren, M. Lazarou, J. Yuan, and T. Stathaki, Towards automated polyp segmentation using weakly-and semi-supervised learning and deformable transformers, in IEEE CVPR-W, 2023. [68] G. Polat, H. T. Kani, I. Ergenc, Y. Ozen Alahdab, A. Temizel, and O. Atug, Improving the computer-aided estimation of ulcerative colitis severity according to mayo endoscopic score by using regression-based deep learning, IBD, vol. 29, no. 9, pp. 1431 1439, 2023. S. Ali, D. Jha, N. Ghatwary, S. Realdon, R. Cannizzaro, O. E. Salem, D. Lamarque, C. Daul, M. A. Riegler, K. V. Anonsen et al., multi-centre polyp detection and segmentation dataset for generalisability assessment, SData, vol. 10, no. 1, p. 75, 2023. [69] [70] D. Wang, X. Wang, L. Wang, M. Li, Q. Da, X. Liu, X. Gao, J. Shen, J. He, T. Shen et al., real-world dataset and benchmark for foundation model adaptation in medical image classification, SData, vol. 10, no. 1, p. 574, 2023. [71] H. Khan, Ali; Malik, Gastrointestinal bleeding wce images dataset, 2023, doi: 10.17632/8pbbjf274w.1. [72] C. Biffi, G. Antonelli, S. Bernhofer, C. Hassan, D. Hirata, M. Iwatate, A. Maieron, P. Salvagnini, and A. Cherubini, Realcolon: dataset for developing real-world ai applications in colonoscopy, SData, vol. 11, no. 1, p. 539, 2024. [73] Z. Xu, J. Rittscher, and S. Ali, Ssl-cpcd: Self-supervised learning with composite pretext-class discrimination for improved generalisability in endoscopic image analysis, IEEE TMI, 2024, doi: 10.1109/TMI.2024.3411933. S. Gautam, A. Storas, C. Midoglu, S. A. Hicks, V. Thambawita, P. Halvorsen, and M. A. Riegler, Kvasir-vqa: text-image pair gi tract dataset, in ACM MM-W, 2024. [74] [75] P. Handa, A. Mahbod, F. Schwarzhans, R. Woitek, N. Goel, D. Chhabra, S. Jha, M. Dhir, D. Gunjan, J. Kakarla et al., Capsule vision 2024 challenge: Multi-class abnormality classification for video capsule endoscopy, in CVIP, 2024. [76] L. Ruiz, F. Sierra-Jerez, J. Ruiz, and F. Martinez, Colon: The largest colonoscopy long sequence public database, arXiv preprint arXiv:2403.00663, 2024. [77] P. Handa, M. Dhir, A. Mahbod, F. Schwarzhans, R. Woitek, N. Goel, and D. Gunjan, Wcebleedgen: wireless capsule endoscopy dataset and its benchmarking for automatic bleeding classification, detection, and segmentation, arXiv preprint arXiv:2408.12466, 2024. [78] D. Jha, N. K. Tomar, V. Sharma, Q.-H. Trinh, K. Biswas, H. Pan, R. K. Jha, G. Durak, A. Hann, J. Varkey et al., Polypdb: 16 curated multi-center dataset for development of ai algorithms in colonoscopy, arXiv preprint arXiv:2409.00045, 2024. [79] M. Sivak, Gastrointestinal endoscopy: past and future, Gut, vol. 55, no. 8, pp. 10611064, 2006. [80] T. M. Berzin and E. J. Topol, Adding artificial intelligence to gastrointestinal endoscopy, The Lancet, vol. 395, no. 10223, p. 485, 2020. [81] G. Iddan, G. Meron, A. Glukhovsky, and P. Swain, Wireless capsule endoscopy, Nature, vol. 405, no. 6785, pp. 417417, 2000. [82] R. Anteby, N. Horesh, S. Soffer, Y. Zager, Y. Barash, I. Amiel, D. Rosin, M. Gutman, and E. Klang, Deep learning visual analysis in laparoscopic surgery: systematic review and diagnostic test accuracy meta-analysis, SEND, vol. 35, pp. 15211533, 2021. S. Shao, Z. Pei, W. Chen, W. Zhu, X. Wu, D. Sun, and B. Zhang, Self-supervised monocular depth and ego-motion estimation in endoscopy: Appearance flow to the rescue, MedIA, vol. 77, p. 102338, 2022. A. Cer on, G. O. Ruiz, L. Chang, and S. Ali, Real-time J. C. instance segmentation of surgical instruments using attention and multi-scale feature fusion, MedIA, vol. 81, p. 102569, 2022. [84] [83] [85] Y. Blau, D. Freedman, V. Dashinsky, R. Goldenberg, and E. Rivlin, Unsupervised 3d shape coverage estimation with applications to colonoscopy, in IEEE ICCV-W, 2021. [86] Y. Zhang, S. Wang, R. Ma, S. K. McGill, J. G. Rosenman, and S. M. Pizer, Lighting enhancement aids reconstruction of colonoscopic surfaces, in IPMI, 2021. [87] D.-P. Fan, G.-P. Ji, P. Xu, M.-M. Cheng, C. Sakaridis, and L. Van Gool, Advances in deep concealed scene understanding, VI, vol. 1, no. 1, p. 16, 2023. [88] W. M. de Vos and E. A. de Vos, Role of the intestinal microbiome in health and disease: from correlation to causation, Nutr. Rev., vol. 70, no. suppl 1, pp. S45S56, 2012. [89] Y. Li and P. Agarwal, pathway-based view of human diseases and disease relationships, PONE, vol. 4, no. 2, p. e4346, 2009. [90] B. Veauthier and J. R. Hornecker, Crohns disease: diagnosis and management, AFP, vol. 98, no. 11, pp. 661669, 2018. [91] L. Yang, H. Jiang, Q. Song, and J. Guo, survey on long-tailed [92] visual recognition, IJCV, vol. 130, no. 7, pp. 18371872, 2022. J. Wu, X. Li, S. X. H. Yuan, H. Ding, Y. Yang, X. Li, J. Zhang, Y. Tong, X. Jiang, B. Ghanem et al., Towards open vocabulary learning: survey, IEEE TPAMI, vol. 46, no. 7, pp. 50925113, 2024. [93] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., Large language models encode clinical knowledge, Nature, vol. 620, no. 7972, pp. 172180, 2023. J. Zhang, Y. Xie, Y. Xia, and C. Shen, Dodnet: Learning to segment multi-organ and tumors from multiple partially labeled datasets, in IEEE CVPR, 2021. [94] [95] D. Karimi, H. Dou, S. K. Warfield, and A. Gholipour, Deep learning with noisy labels: Exploring techniques and remedies in medical image analysis, MedIA, vol. 65, p. 101759, 2020. [96] H.-F. Yu, P. Jain, P. Kar, and I. Dhillon, Large-scale multi-label learning with missing labels, in ICML, 2014. [97] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, Caffe: Convolutional architecture for fast feature embedding, in ACM MM, 2014. [98] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, Densely connected convolutional networks, in IEEE CVPR, 2017. [99] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in IEEE CVPR, 2016. [100] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, in ICLR, 2021. [101] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, Mobilenetv2: Inverted residuals and linear bottlenecks, in IEEE CVPR, 2018. [102] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, Cbam: Convolutional block attention module, in ECCV, 2018. [103] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, Learning spatiotemporal features with 3d convolutional networks, in IEEE ICCV, 2015. [104] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, Rethinking the inception architecture for computer vision, in IEEE CVPR, 2016. [105] J. Carreira and A. Zisserman, Quo vadis, action recognition? new model and the kinetics dataset, in IEEE CVPR, 2017. [106] Y. Yuan, W. Qin, B. Ibragimov, B. Han, and L. Xing, Riisdensenet: rotation-invariant and image similarity constrained densely connected convolutional network for polyp detection, in MICCAI, 2018. [107] Y. Tian, G. Maicas, L. Z. C. T. Pu, R. Singh, J. W. Verjans, and G. Carneiro, Few-shot anomaly detection for polyp frames from colonoscopy, in MICCAI, 2020. [108] H. Gammulle, S. Denman, S. Sridharan, and C. Fookes, Twostream deep feature modelling for automated video endoscopy data analysis, in MICCAI, 2020. [109] G. Carneiro, L. Z. C. T. Pu, R. Singh, and A. Burt, Deep learning uncertainty and confidence calibration for the five-class polyp classification from colonoscopy, MedIA, vol. 62, p. 101653, 2020. [110] X. Guo and Y. Yuan, Semi-supervised wce image classification with adaptive aggregated attention, MedIA, vol. 64, p. 101733, 2020. [111] W. Ma, Y. Zhu, R. Zhang, J. Yang, Y. Hu, Z. Li, and L. Xiang, Toward clinically assisted colorectal polyp recognition via structured cross-modal representation consistency, in MICCAI, 2022. [112] K.-N. Wang, Y. He, S. Zhuang, J. Miao, X. He, P. Zhou, G. Yang, G.-Q. Zhou, and S. Li, Ffcnet: Fourier transform-based frequency learning and complex convolutional network for colon disease classification, in MICCAI, 2022. [113] K.-N. Wang, S. Zhuang, Q.-Y. Ran, P. Zhou, J. Hua, G.-Q. Zhou, and X. He, Dlgnet: dual-branch lesion-aware network with the supervised gaussian mixture model for colon lesions classification in colonoscopy images, MedIA, vol. 87, p. 102832, 2023. [114] G. Yue, P. Wei, Y. Liu, Y. Luo, J. Du, and T. Wang, Automated endoscopic image classification via deep neural network with class imbalance loss, IEEE TIM, vol. 72, pp. 111, 2023. [115] Y. Luo, X. Guo, L. Liu, and Y. Yuan, Dynamic attribute-guided few-shot open-set network for medical image diagnosis, ESWA, vol. 251, p. 124098, 2024. [116] H. Itoh, H. R. Roth, L. Lu, M. Oda, M. Misawa, Y. Mori, S.-e. Kudo, and K. Mori, Towards automated colonoscopy diagnosis: binary polyp size estimation via unsupervised depth learning, in MICCAI, 2018. [117] J. Schmidhuber, S. Hochreiter et al., Long short-term memory, Neural Comput., vol. 9, no. 8, pp. 17351780, 1997. [118] M. F. Byrne, N. Chapados, F. Soudan, C. Oertel, M. L. Perez, R. Kelly, N. Iqbal, F. Chandelier, and D. K. Rex, Real-time differentiation of adenomatous and hyperplastic diminutive colorectal polyps during analysis of unaltered videos of standard colonoscopy using deep learning model, Gut, vol. 68, no. 1, pp. 94100, 2019. [119] A. Tamhane, T. Mida, E. Posner, and M. Bouhnik, Colonoscopy landmark detection using vision transformers, in MICCAI-W, 2022. [120] A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, Yolov4: Optimal speed and accuracy of object detection, arXiv preprint arXiv:2004.10934, 2020. [121] J. Redmon, Yolov3: An incremental improvement, arXiv preprint arXiv:1804.02767, 2018. [122] M. Tan, R. Pang, and Q. V. Le, Efficientdet: Scalable and efficient object detection, in IEEE CVPR, 2020. [123] B. Li, W. Wu, Q. Wang, F. Zhang, J. Xing, and J. Yan, Siamrpn++: Evolution of siamese visual tracking with very deep networks, in IEEE CVPR, 2019. [124] K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, in ICLR, 2015. [125] K. He, X. Zhang, S. Ren, and J. Sun, Identity mappings in deep residual networks, in ECCV, 2016. [126] X. Yang, Q. Wei, C. Zhang, K. Zhou, L. Kong, and W. Jiang, Colon polyp detection and segmentation based on improved mrcnn, IEEE TIM, vol. 70, pp. 110, 2020. [127] X. Liu, X. Guo, Y. Liu, and Y. Yuan, Consolidated domain adaptive detection and localization framework for cross-device colonoscopic images, MedIA, vol. 71, p. 102052, 2021. [128] H. A. Qadir, Y. Shin, J. Solhusvik, J. Bergsland, L. Aabakken, and I. Balasingham, Toward real-time polyp detection using fully cnns for 2d gaussian shapes prediction, MedIA, vol. 68, p. 101897, 2021. [129] X. Liu, W. Li, and Y. Yuan, Intervention & interaction federated abnormality detection with noisy clients, in MICCAI, 2022. 17 [130] I. Pacal, A. Karaman, D. Karaboga, B. Akay, A. Basturk, U. Nalbantoglu, and S. Coskun, An efficient real-time colonic polyp detection with yolo algorithms trained by using negative samples and large datasets, CIBM, vol. 141, p. 105031, 2022. [131] X. Liu and Y. Yuan, source-free domain adaptive polyp detection framework with style diversification flow, IEEE TMI, vol. 41, no. 7, pp. 18971908, 2022. [132] R. Gong, S. He, T. Tian, J. Chen, Y. Hao, and C. Qiao, Frcnnaa-cif: An automatic detection model of colon polyps based on attention awareness and context information fusion, CIBM, vol. 158, p. 106787, 2023. [133] M. R. Haugland, H. A. Qadir, and I. Balasingham, Deep learning for improved polyp detection from synthetic narrow-band imaging, in SPIE Med. Imaging, 2023. [134] W. Li, X. Liu, and Y. Yuan, Scan++: Enhanced semantic conditioned adaptation for domain adaptive object detection, IEEE TMM, vol. 25, pp. 70517061, 2023. [135] X. Pan, Y. Mu, C. Ma, and Q. He, Tfcnet: texture-aware and fine-grained feature compensated polyp detection network, CIBM, vol. 171, p. 108144, 2024. [136] X. Liu, W. Li, and Y. Yuan, Decoupled unbiased teacher for source-free domain adaptive medical object detection, IEEE TNNLS, vol. 35, no. 6, pp. 72877298, 2024. [137] Tajbakhsh, Nima and Gurudu, Suryakanth and Liang, Jianming, comprehensive computer-aided polyp detection system for colonoscopy videos, in IPMI, 2015. [138] L. Yu, H. Chen, Q. Dou, J. Qin, and P. A. Heng, Integrating online and offline three-dimensional deep learning for automated polyp detection in colonoscopy videos, IEEE JBHI, vol. 21, no. 1, pp. 6575, 2016. [139] X. Mo, K. Tao, Q. Wang, and G. Wang, An efficient approach for polyps detection in endoscopic videos based on faster r-cnn, in IEEE ICPR, 2018. [140] S. Ren, K. He, R. Girshick, and J. Sun, Faster r-cnn: Towards realtime object detection with region proposal networks, in NeurIPS, 2015. [141] H. A. Qadir, I. Balasingham, J. Bergsland, L. Aabakken, and Y. Shin, Improving automatic polyp detection using cnn by exploiting temporal dependency in colonoscopy video, IEEE JBHI, vol. 24, no. 1, pp. 180193, 2019. J. Solhusvik, [142] Z. Zhang, H. Shang, H. Zheng, X. Wang, J. Wang, Z. Sun, J. Huang, and J. Yao, Asynchronous in parallel detection and tracking (aipdt): Real-time robust polyp detection, in MICCAI, 2020. [143] L. Wu, Z. Hu, Y. Ji, P. Luo, and S. Zhang, Multi-frame collaboration for effective endoscopic video polyp detection via spatialtemporal feature transformation, in MICCAI, 2021. [144] T. Yu, N. Lin, X. Zhang, Y. Pan, H. Hu, W. Zheng, J. Liu, W. Hu, H. Duan, and J. Si, An end-to-end tracking method for polyp detectors in colonoscopy videos, AIIM, vol. 131, p. 102363, 2022. [145] D. Wang, X. Wang, S. Wang, and Y. Yin, Explainable multitask shapley explanation networks for real-time polyp diagnosis in videos, IEEE TII, vol. 19, no. 6, pp. 77807789, 2022. [146] Y. Jiang, Z. Zhang, R. Zhang, G. Li, S. Cui, and Z. Li, Yona: You only need one adjacent reference-frame for accurate and fast video polyp detection, in MICCAI, 2023. [147] Y. Intrator, N. Aizenberg, A. Livne, E. Rivlin, and R. Goldenberg, Self-supervised polyp re-identification in colonoscopy, in MICCAI, 2023. [148] Y. Jiang, Z. Zhang, J. Wei, C.-M. Feng, G. Li, X. Wan, S. Cui, and Z. Li, Let video teaches you more: Video-to-image knowledge distillation using detection transformer for medical video lesion detection, arXiv preprint arXiv:2408.14051, 2024. [149] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang, Vision mamba: Efficient visual representation learning with bidirectional state space model, in ICML, 2024. [150] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in ICML, 2021. [151] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg, Ssd: Single shot multibox detector, in ECCV, 2016. [152] J. Bernal, N. Tajkbaksh, F. J. Sanchez, B. J. Matuszewski, H. Chen, L. Yu, Q. Angermann, O. Romain, B. Rustad, I. Balasingham et al., Comparative validation of polyp detection methods in video colonoscopy: results from the miccai 2015 endoscopic vision challenge, IEEE TMI, vol. 36, no. 6, pp. 12311249, 2017. 18 [153] Z. Zhang, Q. Liu, and Y. Wang, Road extraction by deep residual u-net, IEEE GRSL, vol. 15, no. 5, pp. 749753, 2018. convolution network for biomedical image segmentation, in BMVC, 2021. [154] S.-H. Gao, M.-M. Cheng, K. Zhao, X.-Y. Zhang, M.-H. Yang, and P. Torr, Res2net: new multi-scale backbone architecture, IEEE TPAMI, vol. 43, no. 2, pp. 652662, 2019. [179] H. Wu, G. Chen, Z. Wen, and J. Qin, Collaborative and adversarial learning of focused and dispersive representations for semisupervised polyp segmentation, in IEEE ICCV, 2021. [155] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jegou, Training data-efficient image transformers & distillation through attention, in ICML, 2021. [156] M. Tan and Q. Le, Efficientnet: Rethinking model scaling for convolutional neural networks, in ICML, 2019. [157] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, Encoder-decoder with atrous separable convolution for semantic image segmentation, in ECCV, 2018. [158] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao, Pvt v2: Improved baselines with pyramid vision transformer, CVMJ, vol. 8, no. 3, pp. 415424, 2022. [159] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang, Cvt: Introducing convolutions to vision transformers, in IEEE ICCV, 2021. [160] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, Segformer: Simple and efficient design for semantic segmentation with transformers, in NeurIPS, 2021. [161] S. Chen, E. Xie, C. Ge, R. Chen, D. Liang, and P. Luo, Cyclemlp: mlp-like architecture for dense prediction, in ICLR, 2022. [162] L. Chen, T. Yang, X. Zhang, W. Zhang, and J. Sun, Points as queries: Weakly semi-supervised object detection by points, in IEEE CVPR, 2021. [163] M.-H. Guo, C.-Z. Lu, Q. Hou, Z. Liu, M.-M. Cheng, and S.-M. Hu, Segnext: Rethinking convolutional attention design for semantic segmentation, in NeurIPS, 2022. [164] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows, in IEEE ICCV, 2021. [165] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Dollar, and R. Girshick, Segment anything, in IEEE ICCV, 2023. [166] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. Radle, C. Rolland, L. Gustafson et al., Sam 2: Segment anything in images and videos, arXiv preprint arXiv:2408.00714, 2024. [167] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs, IEEE TPAMI, vol. 40, no. 4, pp. 834848, 2017. [168] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu, M. Tan, X. Wang et al., Deep high-resolution representation learning for visual recognition, IEEE TPAMI, vol. 43, no. 10, pp. 33493364, 2020. [169] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, convnet for the 2020s, in IEEE CVPR, 2022. [170] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar, Masked-attention mask transformer for universal image segmentation, in IEEE CVPR, 2022, pp. 12901299. [171] Y. Yuan, D. Li, and M. Q.-H. Meng, Automatic polyp detection via novel unified bottom-up and top-down saliency approach, IEEE JBHI, vol. 22, no. 4, pp. 12501260, 2017. [172] Y. Fang, C. Chen, Y. Yuan, and K.-y. Tong, Selective feature aggregation network with area-boundary constraints for polyp segmentation, in MICCAI, 2019. [173] D. Jha, P. H. Smedsrud, M. A. Riegler, D. Johansen, T. De Lange, P. Halvorsen, and H. D. Johansen, Resunet++: An advanced architecture for medical image segmentation, in IEEE ISM, 2019. [174] R. Zhang, G. Li, Z. Li, S. Cui, D. Qian, and Y. Yu, Adaptive context selection for polyp segmentation, in MICCAI, 2020. [175] D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, and L. Shao, Pranet: Parallel reverse attention network for polyp segmentation, in MICCAI, 2020. [176] K. Wickstrøm, M. Kampffmeyer, and R. Jenssen, Uncertainty and interpretability in convolutional neural networks for semantic segmentation of colorectal polyps, MedIA, vol. 60, p. 101619, 2020. [177] H. Wu, J. Zhong, W. Wang, Z. Wen, and J. Qin, Precise yet efficient semantic calibration and refinement in convnets for realtime polyp segmentation from colonoscopy videos, in AAAI, 2021. [178] Y. Meng, H. Zhang, D. Gao, Y. Zhao, X. Yang, X. Qian, X. Huang, and Y. Zheng, Bi-gcn: Boundary-aware input-dependent graph [180] T.-C. Nguyen, T.-P. Nguyen, G.-H. Diep, A.-H. Tran-Dinh, T. V. Nguyen, and M.-T. Tran, Ccbanet: cascading context and balancing attention for polyp segmentation, in MICCAI, 2021. [181] Y. Tian, G. Pang, F. Liu, Y. Chen, S. H. Shin, J. W. Verjans, R. Singh, and G. Carneiro, Constrained contrastive distribution learning for unsupervised anomaly detection and localisation in medical images, in MICCAI, 2021. [182] Y. Shen, X. Jia, and M. Q.-H. Meng, Hrenet: hard region enhancement network for polyp segmentation, in MICCAI, 2021. [183] M. Cheng, Z. Kong, G. Song, Y. Tian, Y. Liang, and J. Chen, Learnable oriented-derivative network for polyp segmentation, in MICCAI, 2021. [184] X. Zhao, L. Zhang, and H. Lu, Automatic polyp segmentation via multi-scale subtraction network, in MICCAI, 2021. [185] J. Wei, Y. Hu, R. Zhang, Z. Li, S. K. Zhou, and S. Cui, Shallow attention network for polyp segmentation, in MICCAI, 2021. [186] Y. Zhang, H. Liu, and Q. Hu, Transfuse: Fusing transformers and cnns for medical image segmentation, in MICCAI, 2021. [187] T. Kim, H. Lee, and D. Kim, Uacanet: Uncertainty augmented context attention for polyp segmentation, in ACM MM, 2021. [188] C. Yang, X. Guo, M. Zhu, B. Ibragimov, and Y. Yuan, Mutualprototype adaptation for cross-domain polyp segmentation, IEEE JBHI, vol. 25, no. 10, pp. 38863897, 2021. [189] X. Guo, C. Yang, and Y. Yuan, Dynamic-weighting hierarchical segmentation network for medical images, MedIA, vol. 73, p. 102196, 2021. [190] X. Du, X. Xu, and K. Ma, Icgnet: Integration context-based reverse-contour guidance network for polyp segmentation, in IJCAI, 2022. [191] J. Wei, Y. Hu, G. Li, S. Cui, S. Kevin Zhou, and Z. Li, Boxpolyp: Boost generalized polyp segmentation using extra coarse bounding box annotations, in MICCAI, 2022. [192] R. Zhang, P. Lai, X. Wan, D.-J. Fan, F. Gao, X.-J. Wu, and G. Li, Lesion-aware dynamic kernel for polyp segmentation, in MICCAI, 2022. [193] L. Cai, M. Wu, L. Chen, W. Bai, M. Yang, S. Lyu, and Q. Zhao, Using guided self-attention with local information for polyp segmentation, in MICCAI, 2022. [194] J. Wang, Q. Huang, F. Tang, J. Meng, J. Su, and S. Song, Stepwise feature fusion: Local guides global, in MICCAI, 2022. [195] Y. Shen, Y. Lu, X. Jia, F. Bai, and M. Q.-H. Meng, Task-relevant feature replenishment for cross-centre polyp segmentation, in MICCAI, 2022. [196] D. Wang, S. Chen, Q. Chen, Y. Cao, B. Liu, X. Liu, and X. Sun, Afp-mask: Anchor-free polyp instance segmentation in colonoscopy, IEEE JBHI, vol. 26, no. 7, pp. 29953006, 2022. [197] G. Yue, W. Han, B. Jiang, T. Zhou, R. Cong, and T. Wang, Boundary constraint network with cross layer feature integration for polyp segmentation, IEEE JBHI, vol. 26, no. 8, pp. 40904099, 2022. [198] Y. Lin, J. Wu, G. Xiao, J. Guo, G. Chen, and J. Ma, Bsca-net: Bit slicing context attention network for polyp segmentation, PR, vol. 132, p. 108917, 2022. [199] J.-H. Shi, Q. Zhang, Y.-H. Tang, and Z.-Q. Zhang, Polyp-mixer: An efficient context-aware mlp-based paradigm for polyp segmentation, IEEE TCSVT, vol. 33, no. 1, pp. 3042, 2022. [200] H. Wu, W. Xie, J. Lin, and X. Guo, Acl-net: semi-supervised polyp segmentation via affinity contrastive learning, in AAAI, 2023. [201] J. Wei, Y. Hu, S. Cui, S. K. Zhou, and Z. Li, Weakpolyp: You only look bounding box for polyp segmentation, in MICCAI, 2023. [202] T. Ling, C. Wu, H. Yu, T. Cai, D. Wang, Y. Zhou, M. Chen, and K. Ding, Probabilistic modeling ensemble vision transformer improves complex polyp segmentation, in MICCAI, 2023. [203] A. Wang, M. Xu, Y. Zhang, M. Islam, and H. Ren, S2me: Spatialspectral mutual teaching and ensemble learning for scribblesupervised polyp segmentation, in MICCAI, 2023. [204] Y. Su, Y. Shen, J. Ye, J. He, and J. Cheng, Revisiting feature propagation and aggregation in polyp segmentation, in MICCAI, 2023. [205] B. Dong, W. Wang, D.-P. Fan, J. Li, H. Fu, and L. Shao, Polyp-pvt: Polyp segmentation with pyramid vision transformers, CAAI AIR, vol. 2, p. 9150015, 2023. [206] J. Wang and C. Chen, Unsupervised adaptation of polyp segmentation models via coarse-to-fine self-supervision, in IPMI, 2023. [207] Q. Jin, H. Hou, G. Zhang, and Z. Li, Fegnet: feedback enhancement gate network for automatic polyp segmentation, IEEE JBHI, vol. 27, no. 7, pp. 34203430, 2023. [208] J. Du, K. Guan, P. Liu, Y. Li, and T. Wang, Boundary-sensitive loss function with location constraint for hard region segmentation, IEEE JBHI, vol. 27, no. 2, pp. 9921003, 2023. [209] Y. Shi, H. Wang, H. Ji, H. Liu, Y. Li, N. He, D. Wei, Y. Huang, Q. Dai, J. Wu et al., deep weakly semi-supervised framework for endoscopic lesion segmentation, MedIA, vol. 90, p. 102973, 2023. [210] G.-P. Ji, D.-P. Fan, Y.-C. Chou, D. Dai, A. Liniger, and L. Van Gool, Deep gradient learning for efficient camouflaged object detection, MIR, vol. 20, no. 1, pp. 92108, 2023. [211] T. Zhou, Y. Zhou, K. He, C. Gong, J. Yang, H. Fu, and D. Shen, Cross-level feature aggregation network for polyp segmentation, PR, vol. 140, p. 109555, 2023. [212] S. Jain, R. Atale, A. Gupta, U. Mishra, A. Seal, A. Ojha, J. Kuncewicz, and O. Krejcar, Coinnet: convolution-involution network with novel statistical attention for automatic polyp segmentation, IEEE TMI, vol. 42, no. 12, pp. 39874000, 2023. [213] N. K. Tomar, D. Jha, M. A. Riegler, H. D. Johansen, D. Johansen, J. Rittscher, P. Halvorsen, and S. Ali, Fanet: feedback attention network for improved biomedical image segmentation, IEEE TNNLS, vol. 34, no. 11, pp. 93759388, 2023. [214] H. Shao, Q. Zeng, Q. Hou, and J. Yang, Mcanet: Medical image segmentation with multi-scale cross-axis attention, arXiv preprint arXiv:2312.08866, 2023. [215] H. Shao, Y. Zhang, and Q. Hou, Polyper: Boundary sensitive polyp segmentation, in AAAI, 2024. [216] M. M. Rahman, M. Munir, and R. Marculescu, Emcad: Efficient multi-scale convolutional attention decoding for medical image segmentation, in IEEE CVPR, 2024. [217] R. Sch on, J. Lorenz, K. Ludwig, and R. Lienhart, Adapting the segment anything model during usage in novel situations, in IEEE CVPR, 2024, pp. 36163626. [218] L. Xie, M. Lin, T. Luan, C. Li, Y. Fang, Q. Shen, and Z. Wu, Mhpflid: Model heterogeneous personalized federated learning via injection and distillation for medical data analysis, in ICML, 2024. [219] H. Li, D. Zhang, J. Yao, L. Han, Z. Li, and J. Han, Asps: Augmented segment anything model for polyp segmentation, in MICCAI, 2024. [220] Z. Xu, F. Tang, Z. Chen, Z. Zhou, W. Wu, Y. Yang, Y. Liang, J. Jiang, X. Cai, and J. Su, Polyp-Mamba: Polyp Segmentation with Visual Mamba , in MICCAI, 2024. [221] Y. Liu, Y. Tian, Y. Zhao, H. Yu, L. Xie, Y. Wang, Q. Ye, and Y. Liu, Vmamba: Visual state space model, NeurIPS, 2024. [222] J. Chai, Z. Luo, J. Gao, L. Dai, Y. Lai, and S. Li, QueryNet: Unified Framework for Accurate Polyp Segmentation and Detection , in MICCAI, 2024. [223] W. Wang, H. Sun, and X. Wang, LSSNet: Method for Colon Polyp Segmentation Based on Local Feature Supplementation and Shallow Feature Supplementation , in MICCAI, 2024. [224] X. Zhou and T. Chen, Bsbp-rwkv: Background suppression with boundary preservation for efficient medical image segmentation, in ACM MM, 2024. [225] B. Peng, E. Alcaide, Q. G. Anthony, A. Albalak, S. Arcadinho, S. Biderman, H. Cao, X. Cheng, M. N. Chung, L. Derczynski, X. Du, M. Grella, K. K. GV, X. He, H. Hou, P. Kazienko, J. Kocon, J. Kong, B. Koptyra, H. Lau, J. Lin, K. S. I. Mantri, F. Mom, A. Saito, G. Song, X. Tang, J. S. Wind, S. Wozniak, Z. Zhang, Q. Zhou, J. Zhu, and R.-J. Zhu, RWKV: Reinventing RNNs for the transformer era, in EMNLP, 2023. [226] C. Wang, L. Wang, N. Wang, X. Wei, T. Feng, M. Wu, Q. Yao, and R. Zhang, Cfatransunet: Channel-wise cross fusion attention and transformer for 2d medical image segmentation, CIBM, vol. 168, p. 107803, 2024. [227] X. Jia, Y. Shen, J. Yang, R. Song, W. Zhang, M. Q.-H. Meng, J. C. Liao, and L. Xing, Polypmixnet: Enhancing semi-supervised polyp segmentation with polyp-aware augmentation, CIBM, vol. 170, p. 108006, 2024. 19 [229] M. Wang, X. An, Z. Pei, N. Li, L. Zhang, G. Liu, and D. Ming, An efficient multi-task synergetic network for polyp segmentation and classification, IEEE JBHI, vol. 28, no. 3, pp. 12281239, 2024. [230] L. Yang, Y. Gu, G. Bian, and Y. Liu, Msde-net: multi-scale dual-encoding network for surgical instrument segmentation, IEEE JBHI, vol. 28, no. 7, pp. 40724083, 2024. [231] G.-P. Ji, J. Zhang, D. Campbell, H. Xiong, and N. Barnes, Rethinking polyp segmentation from an out-of-distribution perspective, MIR, vol. 21, no. 4, pp. 631639, 2024. [232] J. Ma, Y. He, F. Li, L. Han, C. You, and B. Wang, Segment anything in medical images, NComms, vol. 15, no. 1, p. 654, 2024. [233] Z. Liu, S. Zheng, X. Sun, Z. Zhu, Y. Zhao, X. Yang, and Y. Zhao, The devil is in the boundary: Boundary-enhanced polyp segmentation, IEEE TCSVT, vol. 34, no. 7, pp. 54145423, 2024. [234] Z. Lu, Y. Zhang, Y. Zhou, Y. Wu, and T. Zhou, Domaininteractive contrastive learning and prototype-guided selftraining for cross-domain polyp segmentation, IEEE TMI, 2024. [235] J. Gao, Q. Lao, Q. Kang, P. Liu, C. Du, K. Li, and L. Zhang, Boosting your context by dual similarity checkup for in-context learning medical image segmentation, IEEE TMI, 2024. [236] C. Li, X. Liu, W. Li, C. Wang, H. Liu, and Y. Yuan, U-kan makes strong backbone for medical image segmentation and generation, arXiv preprint arXiv:2406.02918, 2024. [237] Z. Liu, Y. Wang, S. Vaidya, F. Ruehle, J. Halverson, M. Soljaˇcic, T. Y. Hou, and M. Tegmark, Kan: Kolmogorov-arnold networks, arXiv preprint arXiv:2404.19756, 2024. [238] C. Fan, H. Yu, L. Wang, Y. Huang, L. Wang, and X. Jia, Slicemamba with neural architecture search for medical image segmentation, arXiv preprint arXiv:2407.08481, 2024. [239] J. Xie, R. Liao, Z. Zhang, S. Yi, Y. Zhu, and G. Luo, Promamba: Prompt-mamba for polyp segmentation, arXiv preprint arXiv:2403.13660, 2024. [240] X. Xiong, Z. Wu, S. Tan, W. Li, F. Tang, Y. Chen, S. Li, J. Ma, and G. Li, Sam2-unet: Segment anything 2 makes strong encoder for natural and medical image segmentation, arXiv preprint arXiv:2408.08870, 2024. [241] J. G.-B. Puyal, K. K. Bhatia, P. Brandao, O. F. Ahmad, D. Toth, R. Kader, L. Lovat, P. Mountney, and D. Stoyanov, Endoscopic polyp segmentation using hybrid 2d/3d cnn, in MICCAI, 2020. [242] G.-P. Ji, Y.-C. Chou, D.-P. Fan, G. Chen, H. Fu, D. Jha, and L. Shao, Progressively normalized self-attention network for video polyp segmentation, in MICCAI, 2021. [243] X. Zhao, Z. Wu, S. Tan, D.-J. Fan, Z. Li, X. Wan, and G. Li, Semisupervised spatial temporal attention network for video polyp segmentation, in MICCAI, 2022. [244] X. Li, J. Xu, Y. Zhang, R. Feng, R.-W. Zhao, T. Zhang, X. Lu, and S. Gao, Tccnet: Temporally consistent context-free network for semi-supervised video polyp segmentation. in IJCAI, 2022. [245] J. G.-B. Puyal, P. Brandao, O. F. Ahmad, K. K. Bhatia, D. Toth, R. Kader, L. Lovat, P. Mountney, and D. Stoyanov, Polyp detection on video colonoscopy using hybrid 2d/3d cnn, MedIA, vol. 82, p. 102625, 2022. [246] Z. Fang, X. Guo, J. Lin, H. Wu, and J. Qin, An embeddingunleashing video polyp segmentation framework via region linking and scale alignment, in AAAI, 2024. [247] H. Xu, Y. Yang, A. I. Aviles-Rivero, G. Yang, J. Qin, and L. Zhu, Lgrnet: Local-global reciprocal network for uterine fibroid segmentation in ultrasound videos, in MICCAI, 2024. [248] Q. Hu, Z. Yi, Y. Zhou, F. Peng, M. Liu, Q. Li, and Z. Wang, Sali: Short-term alignment and long-term interaction network for colonoscopy video polyp segmentation, in MICCAI, 2024. [249] Y. Lu, Y. Yang, Z. Xing, Q. Wang, and L. Zhu, Diff-vps: Video polyp segmentation via multi-task diffusion network with adversarial temporal reasoning, in MICCAI, 2024. [250] L. Wan, Z. Chen, Y. Xiao, J. Zhao, W. Feng, and H. Fu, Iterative feedback-based models for image and video polyp segmentation, CIBM, vol. 177, p. 108569, 2024. [251] Y.-C. Chou, B. Li, D.-P. Fan, A. Yuille, and Z. Zhou, Acquiring weak annotations for tumor localization in temporal and volumetric data, MIR, vol. 21, no. 2, pp. 318330, 2024. [252] Z. Xu, J. Rittscher, and S. Ali, Sstfb: Leveraging self-supervised pretext learning and temporal self-attention with feature branching for real-time video polyp segmentation, arXiv preprint arXiv:2406.10200, 2024. [228] Z. Zhang, Y. Li, and B.-S. Shin, Generalizable polyp segmentation via randomized global illumination augmentation, IEEE JBHI, vol. 28, no. 4, pp. 21382151, 2024. [253] Y. Yang, Z. Xing, and L. Zhu, Vivim: video vision mamba for medical video object segmentation, arXiv preprint arXiv:2401.14168, 2024. 20 [279] G. Lupyan, R. A. Rahman, L. Boroditsky, and A. Clark, Effects of language on visual perception, TICS, vol. 24, no. 11, pp. 930944, 2020. [280] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao, Llava-med: Training large language-and-vision assistant for biomedicine in one day, in NeurIPS, 2024. [281] M. A. Islam, S. Jia, and N. D. Bruce, How much position information do convolutional neural networks encode? in ICLR, 2020. [282] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., Lora: Low-rank adaptation of large language models, in ICLR, 2022. [283] Y. Li, Y. Zhang, C. Wang, Z. Zhong, Y. Chen, R. Chu, S. Liu, and J. Jia, Mini-gemini: Mining the potential of multi-modality vision language models, arXiv preprint arXiv:2403.18814, 2024. [284] X. Chu, L. Qiao, X. Lin, S. Xu, Y. Yang, Y. Hu, F. Wei, X. Zhang, B. Zhang, X. Wei et al., Mobilevlm: fast, reproducible and for mobile devices, arXiv strong vision language assistant preprint arXiv:2312.16886, 2023. [285] S. Woo, S. Debnath, R. Hu, X. Chen, Z. Liu, I. S. Kweon, and S. Xie, Convnext v2: Co-designing and scaling convnets with masked autoencoders, in IEEE CVPR, 2023. [286] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, Masked autoencoders are scalable vision learners, in IEEE CVPR, 2023. [287] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., Dinov2: Learning robust visual features without supervision, TMLR, 2024. [288] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in ICML, 2021. [289] P. Villalobos, A. Ho, J. Sevilla, T. Besiroglu, L. Heim, and M. Hobbhahn, Position: Will we run out of data? limits of llm scaling based on human-generated data, in ICML, 2024. [290] B. Xiao, H. Wu, W. Xu, X. Dai, H. Hu, Y. Lu, M. Zeng, C. Liu, and L. Yuan, Florence-2: Advancing unified representation for variety of vision tasks, in IEEE CVPR, 2024. [291] D. Jiang, X. He, H. Zeng, C. Wei, M. Ku, Q. Liu, and W. Chen, Interleaved multi-image instruction tuning, arXiv Mantis: preprint arXiv:2405.01483, 2024. [292] J. B. Haurum, S. Escalera, G. W. Taylor, and T. B. Moeslund, Which tokens to use? investigating token reduction in vision transformers, in IEEE ICCV-W, 2023. [293] H. Diao, Y. Cui, X. Li, Y. Wang, H. Lu, and X. Wang, Unveiling encoder-free vision-language models, in NeurIPS, 2024. [294] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu et al., Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, in IEEE CVPR, 2024. [295] G. Bachmann and V. Nagarajan, The pitfalls of next-token prediction, in ICML, 2024. [296] X. Huang, J. Wang, Y. Tang, Z. Zhang, H. Hu, J. Lu, L. Wang, and Z. Liu, Segment and caption anything, in IEEE CVPR, 2024. [297] Y. Kim, C. Park, H. Jeong, Y. S. Chan, X. Xu, D. McDuff, C. Breazeal, and H. W. Park, Adaptive collaboration strategy for llms in medical decision making, in NeurIPS, 2024. [254] G. Chen, J. Yang, X. Pu, G.-P. Ji, H. Xiong, Y. Pan, H. Cui, and Y. Xia, Mast: Video polyp segmentation with mixture-attention siamese transformer, arXiv preprint arXiv:2401.12439, 2024. [255] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, Learning deep features for discriminative localization, in IEEE CVPR, 2016. [256] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, End-to-end object detection with transformers, in ECCV, 2020. [257] S. Chen, P. Sun, Y. Song, and P. Luo, Diffusiondet: Diffusion model for object detection, in IEEE ICCV, 2023. [258] M.-M. Cheng and D.-P. Fan, Structure-measure: new way to evaluate foreground maps, IJCV, vol. 129, pp. 26222638, 2021. [259] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, Unet++: Redesigning skip connections to exploit multiscale features in image segmentation, IEEE TMI, vol. 39, no. 6, pp. 18561867, 2019. [260] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional networks for biomedical image segmentation, in MICCAI, 2015. [261] Q. Chen, X. Chen, H. Song, Z. Xiong, A. Yuille, C. Wei, and Z. Zhou, Towards generalizable tumor synthesis, in IEEE CVPR, 2024. [262] K. Tian, Y. Jiang, Z. Yuan, B. Peng, and L. Wang, Visual autoregressive modeling: Scalable image generation via next-scale prediction, arXiv preprint arXiv:2404.02905, 2024. [263] M. Hu, P. Xia, L. Wang, S. Yan, F. Tang, Z. Xu, Y. Luo, K. Song, J. Leitner, X. Cheng et al., Ophnet: large-scale video benchmark for ophthalmic surgical workflow understanding, in ECCV, 2024. [264] N. K. Tomar, D. Jha, U. Bagci, and S. Ali, Tganet: Text-guided attention for improved polyp segmentation, in MICCAI, 2022. [265] Y. Zhao, J. Li, L. Ren, and Z. Chen, Dtan: Diffusion-based text attention network for medical image segmentation, CIBM, vol. 168, p. 107728, 2024. [266] Y. Zhao, J. Li, and Z. Hua, Tact: Text attention based cnntransformer network for polyp segmentation, IJIST, vol. 34, 2023. [267] Z. Qin, H. Yi, Q. Lao, and K. Li, Medical image understanding with pretrained vision language models: comprehensive study, in ICLR, 2023. [268] M. Guo, H. Yi, Z. Qin, H. Wang, A. Men, and Q. Lao, Multiple prompt fusion for zero-shot lesion detection using visionlanguage models, in MICCAI, 2023. [269] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J.-N. Hwang et al., Grounded language-image pre-training, in IEEE CVPR, 2022. [270] S. Wang, Y. Zhu, X. Luo, Z. Yang, Y. Zhang, P. Fu, M. Wang, Z. Song, Q. Li, P. Zhou et al., Knowledge extraction and distillation from large-scale image-text colonoscopy records leveraging large language and vision models, arXiv preprint arXiv:2310.11173, 2023. [271] R. Biswas, Polyp-sam++: Can text guided sam perform better for polyp segmentation? arXiv preprint arXiv:2308.06623, 2023. [272] Y. Zhao, Y. Zhou, Y. Zhang, Y. Wu, and T. Zhou, TextPolyp: Point-supervised Polyp Segmentation with Text Cues , in MICCAI, 2024. [273] S. Wang, W. Zhou, Y. Yang, H. Huang, Z. Ye, T. Zhang, and D. Yang, Adapting pre-trained visual and language models for medical image question answering, in CLEF (Working notes), 2023. [274] J. Li, D. Li, S. Savarese, and S. Hoi, Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, in ICML, 2023. [275] Z. Huang, F. Bianchi, M. Yuksekgonul, T. J. Montine, and J. Zou, visuallanguage foundation model for pathology image analysis using medical twitter, NM, vol. 29, no. 9, pp. 23072316, 2023. [276] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia, A. Poulton, V. Kerkez, and R. Stojnic, Galacfor science, arXiv preprint tica: large language model arXiv:2211.09085, 2022. [277] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, in NeurIPS, 2024. [278] C. Li, Y. Ge, D. Li, and Y. Shan, Vision-language instruction tuning: review and analysis, TMLR, 2024."
        }
    ],
    "affiliations": []
}