{
    "paper_title": "MARS-M: When Variance Reduction Meets Matrices",
    "authors": [
        "Yifeng Liu",
        "Angela Yuan",
        "Quanquan Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Matrix-based preconditioned optimizers, such as Muon, have recently been shown to be more efficient than scalar-based optimizers for training large-scale neural networks, including large language models (LLMs). On the other hand, recent benchmarks on optimizers for LLM pre-training have demonstrated that variance-reduction techniques such as MARS can achieve substantial speedups over standard optimizers that do not employ variance reduction. In this paper, to achieve the best of both worlds, we introduce MARS-M, a new optimizer that integrates the variance reduction technique in MARS with Muon. Under standard regularity conditions, we prove that Muon-M converges to a first-order stationary point at a rate of $\\tilde{\\mathcal{O}}(T^{-1/3})$, which improves upon $\\tilde{\\mathcal{O}}(T^{-1/4})$ rate attained by Muon. Our empirical results on language modeling and computer vision tasks demonstrate that MARS-M consistently yields lower losses and improved performance across various downstream benchmarks. The implementation of MARS-M is available at https://github.com/AGI-Arena/MARS/MARS_M."
        },
        {
            "title": "Start",
            "content": "MARS-M: When Variance Reduction Meets Matrices Yifeng Liu Angela Yuan Quanquan Gu 5 2 0 2 0 2 ] . [ 1 0 0 8 1 2 . 0 1 5 2 : r Abstract Matrix-based preconditioned optimizers, such as Muon, have recently been shown to be more efficient than scalar-based optimizers for training large-scale neural networks, including large language models (LLMs). On the other hand, recent benchmarks on optimizers for LLM pre-training have demonstrated that variance-reduction techniques such as MARS can achieve substantial speedups over standard optimizers that do not employ variance reduction. In this paper, to achieve the best of both worlds, we introduce MARS-M, new optimizer that integrates the variance reduction technique in MARS with Muon. Under standard regularity conditions, we prove that Muon-M converges to first-order stationary point at rate of (cid:101)O(T 1/3), which improves upon (cid:101)O(T 1/4) rate attained by Muon. Our empirical results on language modeling and computer vision tasks demonstrate that MARS-M consistently yields lower losses and improved performance across various downstream benchmarks. The implementation of MARS-M is available at https://github.com/AGI-Arena/MARS/MARS_M."
        },
        {
            "title": "Introduction",
            "content": "The development of preconditioned gradient methods, such as AdamW (Loshchilov and Hutter, 2019), AdaFactor (Shazeer and Stern, 2018) and Lion (Chen et al., 2023), plays an important role in the advance of large-scale deep learning. lot of renowned large language models (LLMs), including ChatGPT (OpenAI, 2023), LLaMA-3 (Dubey et al., 2024) and DeepSeek-R1 (Guo et al., 2025) are trained with adaptive gradient methods such as Adam (Kingma and Ba, 2015) and AdamW. Recently, matrix-based preconditioned optimization methods, such as Shampoo (Gupta et al., 2018), SOAP (Vyas et al., 2024) and Muon (Jordan et al., 2024; Liu et al., 2025), have been introduced to speed up the training of large models like Kimi K2 (Team et al., 2025) and GLM-4.5 (Z.ai, 2025). Unlike vector-based methods, matrix-based approaches operate directly on the model parameter matrices without flattening them, thereby preserving their inherent 2D structure and the underlying optimization geometry. On the other hand, stochastic optimization methods are often hindered by high variance in the stochastic gradients during training, which can slow convergence and degrade stability. To address this issue, numerous variance reduction techniques have been proposed, including SAG (Roux et al., 2012), SVRG (Johnson and Zhang, 2013), SARAH (Nguyen et al., 2017a,b), SPIDER (Fang et al., 2018), SNVRG (Zhou et al., 2020) and STORM (Cutkosky and Orabona, 2019), to mention few. Equal contribution Department of Computer Science, University of California, Los Angeles, CA, USA; email: liuyifeng@cs.ucla.edu Department of Computer Science, University of California, Los Angeles, CA, USA; email: hzyuan@cs.ucla.edu Corresponding Author, Department of Computer Science, University of California, Los Angeles, CA, USA; email: qgu@cs.ucla.edu 1 However, these methods have seen limited success in training large-scale deep neural networks, largely due to incompatibilities with the practice of deep learning and neural network architectures (Defazio and Bottou, 2019). To make variance reduction work for training large-scale deep neural networks and LLMs, MARS (Yuan et al., 2025) was recently proposed. It introduces scaling parameter into the STORM optimizer (Cutkosky and Orabona, 2019), effectively resolving the incompatibility of variance-reduction techniques with preconditioned optimization methods, which have demonstrated superior performance in both language modeling and computer vision tasks. It is also worth noting that similar scaling idea has also been proposed for SVRG in Yin et al. (2024). In fact, the recent benchmarks of optimizers for LLM pretraining (Wen et al., 2025; Semenov et al., 2025) have empirically demonstrated that matrix-based optimizers (e.g., Shampoo, SOAP and Muon) outperform scalar-based optimizers (e.g., AdamW and Lion), and variance-reduction approaches (e.g., MARS, NadamW (Dozat, 2016)) also yield additional, discernible speedup. Therefore, natural question arises: Can we achieve the best of both worlds by combining matrix-based optimizers with variance reduction such as MARS? It is worth mentioning that in the original MARS work (Yuan et al., 2025), the authors have already proposed MARS-shampoo, which attempts to combine matrix-based optimizer with variance reduction. However, MARS-shampoo displays performances inferior to vector-based variants like MARS-AdamW, leaving open the question of whether variance reduction can be effectively integrated with matrix-based optimizers. In this paper, we give an affirmative answer to this question by introducing MARS-M, matrix-based MARS optimizer that integrates variance reduction with the Moonlight version (Liu et al., 2025) of Muon. Additionally, we also propose the approximated version of MARS-M as practical variant, which is also the bridge between variance reduction techniques and traditional preconditioned optimizers. In summary, our contributions are highlighted as follows: We propose MARS-M, an instantiation of MARS tailored to the Moonlight optimizer. In addition, we develop an approximate version of MARS-M to further accelerate training. We show the connections and differences between MARS-M and MARS-Shampoo. We also show that the approximate version of MARS-M can be seen as variant of Moonlight with adjusted momentum parameters. We further provide convergence analysis of MARS-M, and show that it attains convergence rate of (cid:101)O(T 1/3), which surpasses the previously established O(T 1/4) rate achieved by Muon (Li and Hong, 2025; Shen et al., 2025; Pethick et al., 2025). Empirically, we evaluate the performances of MARS-M as well as Muon (Moonlight) optimizer for training GPT-2 (Radford et al., 2019) series models on OpenWebText and FineWeb-Edu 100B datasets. The experiment results show that MARS-M can manifest stable improvement on training and validation losses. Moreover, on downstream tasks, MARS-M can also achieve better performances on benchmarks such as Hellaswag (Zellers et al., 2019) and SciQ (Johannes Welbl, 2017) than baseline optimizers. Additionally, MARS-M can also achieve better test accuracy than AdamW and Muon on computer vision tasks. Notation We use bold capital letters X, Y, to denote matrices. For matrix Rmn, we j=1 Wij to denote its Frobenius norm. In this paper, we use Xt Rmn to use WF = (cid:113)(cid:80)m i=1 (cid:80)n 2 represent the parameters of the language model at training step t, where the training data for each step is sequence of independent random variables, ξ1, . . . , ξT Ξ. For differentiable objective function , we assume that the expected value of (X, ξt) given is (X) for all and t. In this paper, we may explicitly denote the input data, since variance reduction algorithms may utilize both the training data from the current step (ξt) and the previous step (ξt1) to calculate different gradients for the same parameter. MARS-M is directly inspired by the success of matrix-based optimizers Muon and moonlight."
        },
        {
            "title": "2 Related Work\nPreconditioned gradient methods Preconditioned gradient methods, inspired by Newton’s\nmethod, is a powerful class of optimization algorithms widely used in training deep neural networks.\nTo reduce the computational and memory costs of computing and storing the full Hessian matrix, a lot\nof approximate preconditioners have been proposed, including diagonal approximations (Duchi et al.,\n2011; Loshchilov and Hutter, 2019; Liu et al.; Yao et al., 2021) and sketched approximations (Erdogdu\nand Montanari, 2015; Gonen and Shalev-Shwartz, 2015). Despite these advances, there are only a few\npractical matrix-based optimization methods for large-scale models. A notable breakthrough was the\nShampoo optimizer (Gupta et al., 2018), which demonstrated the practical feasibility of incorporating\nsecond-order information in large-scale training. To reduce the number of hyperparameters and\ncomputational overhead of Shampoo, Vyas et al. (2024) proposed SOAP, which implements AdamW\nin the eigenbasis of Shampoo’s preconditioners, significantly improving the training efficiency in\nterms of both number of iterations and wall clock time. Recently, Muon (Jordan et al., 2024)\nis proposed by leveraging a Newton–Schulz approximation of the singular value decomposition\n(SVD) of the gradient momentum, while Moonlight (Liu et al., 2025) extends Muon by introducing\nweight decay and coupling step sizes across components optimized by AdamW and Muon, thereby\nimproving hyperparameter search efficiency. Building on this line of research, PolarGrad (Lau et al.,\n2025) introduces a preconditioning framework based on the polar decomposition of the gradient\nmatrix, generalizing the principles of Muon. Scion (Pethick et al., 2025) further unifies Muon and\nrelated methods under a linear minimization oracle (LMO) framework, and Gluon (Riabinin et al.,\n2025) extends both Muon and Scion by incorporating momentum-based generalizations. Nowadays,\nmatrix-based optimizers are becoming increasingly more popular in training industrial large language\nmodels, including Kimi K2 (Team et al., 2025) and GLM-4.5 (Zeng et al., 2025).",
            "content": "In order to understand the convergence of Muon, Li and Hong (2025), Shen et al. (2025) and Pethick et al. (2025) proved that Muon has convergence rate of O(T 1/4). An et al. (2025) and Shen et al. (2025) also analyzed the convergence of Muon without momentum. Kovalev (2025) investigated the convergence rate of Muon with additional assumptions of spectral norm-based Lipschitz smoothness and second-order smoothness, while Li and Hong (2025), Sfyraki and Wang (2025) as well as Sato et al. (2025) analyzed the convergence rate under big batch sizes. Our analysis of MARS-M pushes the theoretical frontier of Muon-based optimization algorithms. Variance Reduction Methods. The earliest efforts to accelerate stochastic gradient descent (SGD) through variance reduction include SAG (Roux et al., 2012) and SDCA (Shalev-Shwartz and Zhang, 2013). These were soon followed by simpler yet equally effective algorithms such as SVRG (Johnson and Zhang, 2013) and SAGA (Defazio et al., 2014), which achieved the same improved convergence guarantees. Building on this line of work, SARAH (Nguyen et al., 2017a) introduced biased recursive gradient estimator that reduces memory requirements while retaining optimal complexity bounds for convex optimization. Additionally, variance reduction has also been 3 studied in conjunction with preconditioning in the convex setting (Frangella et al., 2024; Derezinski, 2023). In the non-convex regime, algorithms like SVRG (Allen-Zhu and Yuan, 2016; Reddi et al., 2016) and SARAH (Nguyen et al., 2017b) paved the way for methods such as SPIDER (Fang et al., 2018), which integrates normalized gradient descent (Nesterov, 2013; Hazan et al., 2015) with stochastic path-integral differential estimator, and SNVRG (Zhou et al., 2020), which leverages multiple reference points to enhance variance reduction. SpiderBoost (Wang et al., 2019) further improved SPIDER by permitting large constant step sizes without compromising near-optimal oracle complexity. Subsequently, STORM (Cutkosky and Orabona, 2019) streamlined SPIDER and SNVRG via stochastic recursive momentum, and was later extended into parameter-free variant, STORM+ (Levy et al., 2021). Recent work has also explored combining variance reduction with adaptive gradient methods. For instance, Adam+ (Liu et al., 2020) reduces variance in Adam by estimating gradients only at extrapolated points, while SuperAdam (Huang et al., 2021) and VRAdam (Li, 2024) integrate variance reduction into AdamW to accelerate convergence. AdaSPIDER (Kavis et al., 2022) extends SPIDER with adaptive step sizes. However, these variancereduced adaptive optimization algorithms have been evaluated only on relatively simple computer vision and natural language modeling benchmarks with modest model sizes. To our knowledge, MARS (Yuan et al., 2025) is the first method to achieve stellar performance on large language models. This work pointed out that previous algorithms introduced excessive gradient correction and it incorporates scaled gradient correction into adaptive gradient methods, achieving better performances in language modeling and computer vision tasks. MARS-M is built upon MARS, while utilizing matrix-based optimizer muon/moonlight."
        },
        {
            "title": "3 Preliminaries\nIn this section, we present the problem setting and relevant preliminaries, including a review of the\nMuon and MARS optimizers.",
            "content": "In this paper, we consider minimizing an objective function () : Rmn as follows: (X) = EξD[f (X, ξ)], min (3.1) where (X, ξ) is possibly nonconvex loss function, Rmn is matrix-variate optimization variable, ξ is random vector (e.g., training data point) drawn from an unknown data distribution D. We assume the access to the first-order oracle, which returns an unbiased estimator of the gradient E[f (X, ξ)] = (X). Throughout the paper, without loss of generality, we assume n."
        },
        {
            "title": "3.1 Muon",
            "content": "Muon (Jordan et al., 2024) was proposed to exploit the 2D geometric information of model parameter matrices during optimization. Given the momentum β (0, 1) and learning rate ηt > 0, the update rule for Muon is as follows: Mt = βMt + (Xt, ξt), Ot = NewtonSchulz (Mt) , Xt+1 = Xt ηtOt. (3.2) (3.3) (3.4) Here Newton-Schulz iteration (Bernstein and Newhouse, 2024) is utilized to approximate UtVt, where UtΣtVt = Mt is the Singular Value Decomposition (SVD) of the momentum matrix. In 4 practice, (cid:102)Mt = βMt + (Xt, ξt) instead of Mt is usually applied in NewtonSchulz iteration1. It is worth noting that Muon is targeted only for optimizing matrix-like parameters, while AdamW is often applied to optimize vector-like parameters, including embeddings, language model head and RMSNorm. However, lot of empirical evidences (Yuan et al., 2025; Semenov et al., 2025) found that the original version of Muon does not perform so satisfactorily. Liu et al. (2025) discovered that the inferior performance is due to mismatched updates on the two kinds of parameters (i.e., matrix-like parameters and vector-like parameters). In detail, the RMS norm of AdamW update typically ranges from approximately 0.2 to 0.4 during LLM training. Based on this observation, they proposed variant of Muon (referred to as Moonlight in this paper): Ut = βUt1 + (Xt, ξt), Mt = βUt + (Xt, ξt), Ot = NewtonSchulz (Mt) , (3.5) (3.6) (3.7) Xt+1 = Xt ηt(0.2 Ot (cid:112)max(m, n) + λXt), where Xt Rmn is the matrix-like parameter to be optimized, and λ > 0 is the decoupled weight decay parameter. Such corrected version demonstrates great performance in lot of benchmarks (Semenov et al., 2025; Wen et al., 2025) and successfully applied in training industrial large language models (Team et al., 2025; Zeng et al., 2025). (3.8)"
        },
        {
            "title": "3.2 MARS",
            "content": "To accelerate the convergence of SGD, lot of variance reduction techniques have been proposed trying to reduce the variance in update and achieve stabler and faster training. typical form for variance reduction is exampled by STORM (Cutkosky and Orabona, 2019), which introduces gradient correction to the momentum update rule: mt = βmt1 + (1 β) (cid:104) (xt, ξt) + β 1 β (cid:124) (cid:0)f (xt, ξt) (xt1, ξt)(cid:1) (cid:125) (cid:123)(cid:122) gradient correction (cid:105) . where β > 0 is momentum parameter. Here the noise brought by the randomness of data can be canceled out by the stochastic gradient difference term β 1β (f (xt, ξt) (xt1, ξt)). Based on this term, the estimation of the true gradient for the parameter in the last step (xt1) can be transferred to the true gradient for the parameter in the current step (xt). Based on STORM, MARS (Yuan et al., 2025) (See Algorithm 1) introduces scaling parameter to the gradient correction part such that the corrected gradient is calculated as: ct = (xt, ξt) + γt β 1 β (cid:124) (cid:0)f (xt, ξt) (xt1, ξt)(cid:1) (cid:125) (cid:123)(cid:122) scaled gradient correction , (3.9) where γt > 0 is the coefficient for scaling. Moreover, it also introduces gradient clipping on the corrected gradient for stabler training: (cid:101)ct = Clip(ct, 1) = (cid:40) ct ct2 ct if ct2 > 1, otherwise. (3.10) 1In Jordan et al. (2024)s newest implementation, both (Xt, ξt) in (cid:103)Mt and in (3.2) are substituted with (1 β)f (xt, ξt). However, in this paper, we just focus on the original algorithm. 5 Algorithm 1 MARS 1: input: x0, β, {γt}, {ηt} 2: Set m0 0 and x1 x0 3: for = 1, to do 4: Sample ξt and let ct = (xt, ξt) + γt if ct2 > 1, then (cid:101)ct = ct (cid:101)ct = ct 5: ct2 6: mt = βmt1 + (1 β)(cid:101)ct 7: 8: end for (cid:8)ηt mt, + 1 xt+1 = arg minx else 2 xt2 Ht β 1β (cid:9) (cid:0)f (xt, ξt) (xt1, ξt)(cid:1) For the ease of exposure, the MARS algorithm is summarized in Algorithm 1, where line 7 is the general formula for adaptive gradient methods such as AdamW (Loshchilov and Hutter, 2019), Lion (Chen et al., 2023) and Shampoo (Gupta et al., 2018), with approximated Hessian matrix Ht. As will be seen later, our work can be seen as tailored version of MARS for Muon/moonlight."
        },
        {
            "title": "4.1 MARS-M",
            "content": "We present MARS-M, which is an instantiation of MARS by substituting the update rule (3.4) with (3.8), and the resulted algorithm is shown in Algorithm 2. MARS-M is an instantiation of MARS tailored to the Moonlight optimizer. It is worth noting that MARS-M is similar to MARS-Shampoo in (Yuan et al., 2025), which is another instantiation of MARS to simplified version of Shampoo. In detail, the update rule in MARS-Shampoo is instantiated as: Ut, Σt, Vt = SVD(Mt), Xt+1 = Xt ηtUtV . (4.1) In practice, SVD computation can be substituted by approximate numerical methods such as Newton iteration (Lakić, 1998; Higham, 2008; Anil et al., 2020; Bernstein and Newhouse, 2024) and Newton-Schulz iteration (Schulz, 1933; Higham, 2008). The main differences between MARS-M by 0.2 (cid:112)max(m, n) and incorporates and MARS-Shampoo are that MARS-M scales Ot UtV weight decay λXt, both of which are essential for achieving superior performance according to Liu et al. (2025). Algorithm 2 MARS-M 1: input: X0 Rmn, λ, β, {γt}, {ηt} 2: Set M0 0 and X1 X0 3: for = 1, to do 4: 5: Mt = βMt1 + (1 β)Clip(Ct, 1) 6: Ot = NewtonSchulz(Mt) 7: Xt+1 = Xt ηt(0.2 Ot (cid:112)max(m, n) + λXt) 8: end for sample ξt and let Ct = (Xt, ξt) + γt( β 1β )(cid:0)f (Xt, ξt) (Xt1, ξt)(cid:1)"
        },
        {
            "title": "4.2 Approximated MARS-M",
            "content": "Since calculating the stochastic gradients twice requires much more computation, in practice, approximate version of MARS is usually utilized where (Xt1, ξt) is replaced by (Xt1, ξt1). If clipping on Ct is ignored, lines 4 to 5 in Algorithm 2 can be approximated by: Ct = (Xt, ξt) + γt (cid:18) β 1 β (cid:19) (cid:0)f (Xt, ξt) (Xt1, ξt1)(cid:1), (4.2) Mt = βMt1 + (1 β)Ct. With some calculation, it can be shown that the approximate version of MARS-M without clipping is equivalent to the following Muon/Moonlight-style update rule: Ut = βUt1 + (1 γt)(1 β) β (Xt, ξt), Mt = βUt + γtf (Xt, ξt), Ot = NewtonSchulz (Mt) , Xt+1 = Xt ηt(0.2 Ot (cid:112)max(m, n) + λXt). (4.3) (4.4) By comparing (4.3), (4.4) with (3.5), (3.6), we can see that approximate MARS-M can be seen as variant of Moonlight with adjusted momentum parameters (1 γt)(1 β)/β and γt (in contrast to 1s in moonlight). From another perspective, we note that (3.5) and (3.6) in the original Moonlight algorithm can be equivalently expressed as: Ct = 1 1 β (Xt, ξt) + 1 (cid:18) β 1 β (cid:19) (cid:0)f (Xt, ξt) (Xt1, ξt1)(cid:1), Mt = βMt1 + (1 β)Ct. (4.5) (4.6) Comparing (4.2) with (4.5), it can be seen that approximate version of MARS-M is Moonlight with stochastic gradient (Xt, ξt) being scaled down by factor of (1 β) and setting γt = 1. The experiments in Section 5 empirically demonstrate that this difference in momentum parameters leads to noticeable impact on the loss."
        },
        {
            "title": "4.3 Convergence Analysis",
            "content": "To analyze the convergence of MARS-M, similar to Yuan et al. (2025), we first make the following assumptions: Assumption 4.1 (Bounded Variance). We assume that the variance of gradient estimator is bounded by σ2. i.e., for any noise ξ, parameter X, and (X) = E[f (X, ξ)], there exists positive σ such that: Assumption 4.2 (L-Smoothness). We assume that for arbitrary ξ, (X, ξ) is L-smooth: E(cid:2)f (X, ξ) (X)2 (cid:3) σ2. (X, ξ) (Y, ξ)F YF , X, Y. 7 (4.7) (4.8) Both assumptions are standard in the literature (Cutkosky and Orabona, 2019; Yuan et al., 2025). We have the following theorem, which guarantees the convergence of MARS-M in Algorithm 2. Theorem 4.3. In Algorithm 2, under Assumptions 4.1 and 4.2, when choosing λ = 0, ηt = (s + t)2/3, 2, suppose βt+1 = 1 2ηt, then for s, it holds that 1 (cid:88) t=1 EF (Xt)F 2LG 1/3 2(cid:112)2LB log(s + ) 1/3 + + 2G 1/ + 2B 1/3 log(s + ) 2 4LT 1/3 (cid:88) t=1 (cid:102)Mt+1 ηt . where = (X1) minX (X) + value defined in (C.1). 2s1/3σ2 4L + 3Ln 2s1/ , = (cid:16) 2 2σ2 + 3 2Ln (cid:17) and (cid:102)Mt+1 is non-negative We defer the proof of the above theorem to Appendix C. It is worth noting that time-varying momentum parameter βt is required for theoretical analysis, while it can be chosen as constant in practice. Theorem 4.3 suggests that MARS-M can achieve non-asymptotic convergence rate of O(T 1/3 log(T )), the same rate as general MARS as proved in Yuan et al. (2025). As comparison, Li and Hong (2025), Shen et al. (2025) and Pethick et al. (2025) proved that Muon can achieve only O(T 1/4) convergence rate. To sum up, Muon-M constitutes new addition to the MARS family, broadening the framework to encompass more matrix-based optimizers with strong theoretical guarantees."
        },
        {
            "title": "5.1 LLM Experiments",
            "content": "We evaluate the performance of our algorithm with baseline algorithms2, including Moonlight and AdamW, on language modeling tasks based on the nanoGPT (Karpathy, 2022) architecture and GPT-2 (Radford et al., 2019) model. We implement experiments with OpenWebText (Gokaslan et al., 2019) and FineWeb-Edu 100B (Lozhkov et al., 2024) dataset. For OpenWebText, the training and validation datasets contain around 9 billion and 4.4 million tokens, respectively; while the training and validation sets of FineWeb-Edu 100B are with 100 billion and 0.1 billion tokens. The experiments are conducted on scales of small (125M parameters), medium (355M parameters), large (770M parameters) as well as XL (1.5B parameters). Moreover, we disabled biases, and set the Dropout rate (Srivastava et al., 2014) to 0.0. For experiments with small models, we utilize 16 NVIDIA H800 GPUs; and for models with larger sizes, we implement with 32 NVIDIA H800. Other hyper-parameters of training are listed in Appendix A."
        },
        {
            "title": "5.2 Experiment Results",
            "content": "We show the zoomed-in curves for training and validation losses for large-size models trained on OpenWebText and XL-size models trained on FineWeb-Edu 100B datasets in Figures 3 and 1. Moreover, we also show the curves for XL-size models for all the training process in Figure 2. Other curves can be found in Appendix B. We also add the curves for the experiments trained with AdamW in Yuan et al. (2025), where the maximum learning rate is 6e 4 for XL-size model and 2e 4 for large-size model. It can be observed that the experiments trained with MARS-M display steady 2For training efficiency, we use approximated MARS-M for LLM experiments, since the performances of exact version and approximate version are similar as discussed in Yuan et al. (2025). And we compare their differences in computer vision experiments. 8 improvement on both training and validation losses over Moonlight. Additionally, although the loss for AdamW is lower in the middle of training due to smaller maximum learning rate, model trained with MARS-M achieve lower losses than that of AdamW in the final phase. Since the learning rates for all the experiments trained with baseline optimizers results from grid search, it can be concluded that MARS-M can indeed improve the performance of large language model training. Figure 1: The training and validation loss of XL-size models (1.5B) trained with different optimizers on the FineWeb-Edu 100B dataset. Figure 2: The training and validation loss of XL-size models (1.5B) trained with different optimizers on the FineWeb-Edu 100B dataset. Furthermore, We also evaluate 0-shot and 2-shot performances of these optimizers on benchmarks including ARC (Yadav et al., 2019), HellaSwag (Zellers et al., 2019), OBQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), SciQ (Johannes Welbl, 2017), WinoGrande (Sakaguchi et al., 2020) and MMLU (Hendrycks et al., 2021), based on lm-evaluation-harness codebase (Gao et al., 2024). We only display the 2-shot performances for XL models in Table 1, and postpone other results in the Appendix B. It can be observed that MARS-M outperforms Moonlight on most of the benchmarks, showing that our algorithm can enhance the performance of pre-trained large language models on wide range of downstream tasks."
        },
        {
            "title": "5.3 Computer Vision Experiments",
            "content": "Besides language model task, we also conduct experiments on computer vision tasks with these optimizers on CIFAR-10 dataset (Krizhevsky et al., 2009) and ResNet-18 model (He et al., 2016). 9 Figure 3: The training and validation loss of large-size models (770M) trained with different optimizers on the OpenWebText dataset. Table 1: The evaluation results of XL models pre-trained using the FineWeb-Edu 100B dataset (2-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: WG = WinoGrande. Method Muon (Moonlight) MARS-M (γ = 0.01) MARS-M (γ = 0.025) Avg. ARC-C ARC-E Hellaswag MMLU OpenBookQA PIQA SciQ WG 40.87 57.33 57.54 91.90 41.64 92.30 58.96 57.73 57.70 58.09 42. 74.05 73.99 74.21 92.30 55.80 57.01 57.09 24.89 24.75 25.57 71.21 73.57 72.10 42.40 39.60 39.60 We set γ = 0.025 for MARS-M and do grid search over {104, , 102} for the learning rate. We set β = 0.95 for all the optimizers. All the experiments are done with training batch size 128 on 1 NVIDIA A6000 GPU, with total of 200 training epochs. Additionally, we use MultiStepLR scheduler so that the learning rate will decrease to 10% at 100th step and 1% at 150th step. The test loss and accuracy for MARS-M and MARS-M approximate, as well as Moonlight optimizer, are shown in Figure 4. It can be seen that MARS-M achieve better test accuracy and lower test loss over Moonlight and MARS-M-approx, validating the effect of variance reduction. Figure 4: The test loss and test accuracy for different optimizers on CIFAR-10 dataset. 10 Figure 5: The training and validation loss of small-size models (125M) trained with MARS-M with different γs on the OpenWebText 100B dataset."
        },
        {
            "title": "5.4 Ablation Study",
            "content": "We also implement ablation study on γ in MARS-M to unearth the effect of scale of gradient correction on the performance. The experiments are done on small models trained with OpenWebText datasets, and the results are shown in Figure 5. It can be observed that the scale on gradient correction should be much smaller than 1, as usually taken in most of the variance reduction approaches (Cutkosky and Orabona, 2019; Nguyen et al., 2017a; Fang et al., 2018). However, the training and validation losses for experiments with γ [0.005, 0.025] are very close, showing that small enough γ is relatively robust to variation."
        },
        {
            "title": "References",
            "content": "Allen-Zhu, Z. and Yuan, Y. (2016). Improved svrg for non-strongly-convex or sum-of-non-convex objectives. In International conference on machine learning. PMLR. An, K., Liu, Y., Pan, R., Ren, Y., Ma, S., Goldfarb, D. and Zhang, T. (2025). Asgo: Adaptive structured gradient optimization. arXiv preprint arXiv:2503.20762 . Anil, R., Gupta, V., Koren, T., Regan, K. and Singer, Y. (2020). Scalable second order optimization for deep learning. arXiv preprint arXiv:2002.09018 . Bernstein, J. and Newhouse, L. (2024). Old optimizer, new norm: An anthology. arXiv preprint arXiv:2409.20325 . 11 Bisk, Y., Zellers, R., Bras, R. L., Gao, J. and Choi, Y. (2020). PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press. Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Pham, H., Dong, X., Luong, T., Hsieh, C.-J., Lu, Y. et al. (2023). Symbolic discovery of optimization algorithms. Advances in neural information processing systems 36. Cutkosky, A. and Orabona, F. (2019). Momentum-based variance reduction in non-convex sgd. Advances in neural information processing systems 32. Defazio, A., Bach, F. and Lacoste-Julien, S. (2014). Saga: fast incremental gradient method with support for non-strongly convex composite objectives. Advances in neural information processing systems 27. Defazio, A. and Bottou, L. (2019). On the ineffectiveness of variance reduced optimization for deep learning. Advances in Neural Information Processing Systems 32. Derezinski, M. (2023). Stochastic variance-reduced newton: Accelerating finite-sum minimization with large batches. In OPT 2023: Optimization for Machine Learning. Dozat, T. (2016). Incorporating nesterov momentum into adam . Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A. et al. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783 . Duchi, J., Hazan, E. and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research 12. Erdogdu, M. A. and Montanari, A. (2015). Convergence rates of sub-sampled newton methods. Advances in Neural Information Processing Systems 28. Fang, C., Li, C. J., Lin, Z. and Zhang, T. (2018). Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator. Advances in neural information processing systems 31. Frangella, Z., Rathore, P., Zhao, S. and Udell, M. (2024). Promise: Preconditioned stochastic optimization methods by incorporating scalable curvature estimates. Journal of Machine Learning Research 25 157. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K. and Zou, A. (2024). framework for few-shot language model evaluation. Gokaslan, A., Cohen, V., Pavlick, E. and Tellex, S. (2019). Openwebtext corpus. http: //Skylion007.github.io/OpenWebTextCorpus. Gonen, A. and Shalev-Shwartz, S. (2015). Faster sgd using sketched conditioning. arXiv preprint arXiv:1506.02649 . Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X. et al. (2025). Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 . Gupta, V., Koren, T. and Singer, Y. (2018). Shampoo: Preconditioned stochastic tensor optimization. In International Conference on Machine Learning. PMLR. Hazan, E., Levy, K. and Shalev-Shwartz, S. (2015). Beyond convexity: Stochastic quasi-convex optimization. Advances in neural information processing systems 28. He, K., Zhang, X., Ren, S. and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J. (2021). Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. Higham, N. J. (2008). Functions of Matrices. Society for Industrial and Applied Mathematics. Huang, F., Li, J. and Huang, H. (2021). Super-adam: faster and universal framework of adaptive gradients. Advances in Neural Information Processing Systems 34 90749085. Johannes Welbl, M. G., Nelson F. Liu (2017). Crowdsourcing multiple choice science questions. Johnson, R. and Zhang, T. (2013). Accelerating stochastic gradient descent using predictive variance reduction. Advances in neural information processing systems 26. Jordan, K., Jin, Y., Boza, V., Jiacheng, Y., Cecista, F., Newhouse, L. and Bernstein, J. (2024). Muon: An optimizer for hidden layers in neural networks. Karpathy, A. (2022). NanoGPT. https://github.com/karpathy/nanoGPT. Kavis, A., Skoulakis, S., Antonakopoulos, K., Dadi, L. T. and Cevher, V. (2022). Adaptive stochastic variance reduction for non-convex finite-sum minimization. Advances in Neural Information Processing Systems 35 2352423538. Kingma, D. P. and Ba, J. (2015). Adam: method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings (Y. Bengio and Y. LeCun, eds.). Kovalev, D. (2025). Understanding gradient orthogonalization for deep learning via non-euclidean trust-region optimization. arXiv preprint arXiv:2503.12645 . Krizhevsky, A., Hinton, G. et al. (2009). Learning multiple layers of features from tiny images . Lakić, S. (1998). On the computation of the matrix k-th root. ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik: Applied Mathematics and Mechanics 78 167172. 13 Lau, T. T.-K., Long, Q. and Su, W. (2025). Polargrad: class of matrix-gradient optimizers from unifying preconditioning perspective. arXiv preprint arXiv:2505.21799 . Levy, K., Kavis, A. and Cevher, V. (2021). Storm+: Fully adaptive sgd with recursive momentum for nonconvex optimization. Advances in Neural Information Processing Systems 34 2057120582. Li, H. (2024). Smoothness and Adaptivity in Nonlinear Optimization for Machine Learning Applications. Ph.D. thesis, Massachusetts Institute of Technology. Li, J. and Hong, M. (2025). note on the convergence of muon. arXiv preprint arXiv:2502.02900 . Liu, H., Li, Z., Hall, D. L. W., Liang, P. and Ma, T. (????). Sophia: scalable stochastic second-order optimizer for language model pre-training. In The Twelfth International Conference on Learning Representations. Liu, J., Su, J., Yao, X., Jiang, Z., Lai, G., Du, Y., Qin, Y., Xu, W., Lu, E., Yan, J. et al. (2025). Muon is scalable for llm training. arXiv preprint arXiv:2502.16982 . Liu, M., Zhang, W., Orabona, F. and Yang, T. (2020). Adam +: stochastic method with adaptive variance reduction. arXiv preprint arXiv:2011.11985 . Loshchilov, I. and Hutter, F. (2019). Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. Lozhkov, A., Ben Allal, L., von Werra, L. and Wolf, T. (2024). Fineweb-edu: the finest collection of educational content. Mihaylov, T., Clark, P., Khot, T. and Sabharwal, A. (2018). Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018 (E. Riloff, D. Chiang, J. Hockenmaier and J. Tsujii, eds.). Association for Computational Linguistics. Nesterov, Y. (2013). Introductory lectures on convex optimization: basic course, vol. 87. Springer Science & Business Media. Nguyen, L. M., Liu, J., Scheinberg, K. and Takáč, M. (2017a). Sarah: novel method for machine learning problems using stochastic recursive gradient. In International conference on machine learning. PMLR. Nguyen, L. M., Liu, J., Scheinberg, K. and Takáč, M. (2017b). Stochastic recursive gradient algorithm for nonconvex optimization. arXiv preprint arXiv:1705.07261 . OpenAI (2023). Chatgpt. https://chat.openai.com/. Accessed: [Date of Access]. Pethick, T., Xie, W., Antonakopoulos, K., Zhu, Z., Silveti-Falls, A. and Cevher, arXiv preprint V. (2025). Training deep learning models with norm-constrained lmos. arXiv:2502.07529 . 14 Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I. et al. (2019). Language models are unsupervised multitask learners. OpenAI blog 1 9. Reddi, S. J., Hefny, A., Sra, S., Poczos, B. and Smola, A. (2016). Stochastic variance reduction for nonconvex optimization. In International conference on machine learning. PMLR. Riabinin, A., Shulgin, E., Gruntkowska, K. and Richtárik, P. (2025). Gluon: Making muon & scion great again!(bridging theory and practice of lmo-based optimizers for llms). arXiv preprint arXiv:2505.13416 . Roux, N., Schmidt, M. and Bach, F. (2012). stochastic gradient method with an exponential convergence _rate for finite training sets. Advances in neural information processing systems 25. Sakaguchi, K., Bras, R. L., Bhagavatula, C. and Choi, Y. (2020). Winogrande: An adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press. Sato, N., Naganuma, H. and Iiduka, H. (2025). Convergence bound and critical batch size of muon optimizer. Schulz, G. (1933). Iterative berechnung der reziproken matrix. Z. Angew. Math. Mech. 13 5759. Semenov, A., Pagliardini, M. and Jaggi, M. (2025). Benchmarking optimizers for large language model pretraining. arXiv preprint arXiv:2509.01440 . Sfyraki, M.-E. and Wang, J.-K. (2025). Lions and muons: Optimization via stochastic frank-wolfe. arXiv preprint arXiv:2506.04192 . Shalev-Shwartz, S. and Zhang, T. (2013). Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research 14. Shazeer, N. and Stern, M. (2018). Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning. PMLR. Shen, W., Huang, R., Huang, M., Shen, C. and Zhang, J. (2025). On the convergence analysis of muon. arXiv preprint arXiv:2505.23737 . Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. and Salakhutdinov, R. (2014). Dropout: simple way to prevent neural networks from overfitting. The journal of machine learning research 15 19291958. Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y. et al. (2025). Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534 . Vyas, N., Morwani, D., Zhao, R., Shapira, I., Brandfonbrener, D., Janson, L. and Kakade, S. M. (2024). Soap: Improving and stabilizing shampoo using adam for language modeling. In The Thirteenth International Conference on Learning Representations. 15 Wang, Z., Ji, K., Zhou, Y., Liang, Y. and Tarokh, V. (2019). Spiderboost and momentum: Faster variance reduction algorithms. Advances in Neural Information Processing Systems 32. Wen, K., Hall, D., Ma, T. and Liang, P. (2025). Fantastic pretraining optimizers and where to find them. arXiv preprint arXiv:2509.02046 . Yadav, V., Bethard, S. and Surdeanu, M. (2019). Quick and (not so) dirty: Unsupervised selection of justification sentences for multi-hop question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 (K. Inui, J. Jiang, V. Ng and X. Wan, eds.). Association for Computational Linguistics. Yao, Z., Gholami, A., Shen, S., Mustafa, M., Keutzer, K. and Mahoney, M. (2021). Adahessian: An adaptive second order optimizer for machine learning. In proceedings of the AAAI conference on artificial intelligence, vol. 35. Yin, Y., Xu, Z., Li, Z., Darrell, T. and Liu, Z. (2024). coefficient makes svrg effective. In The Thirteenth International Conference on Learning Representations. Yuan, H., Liu, Y., Wu, S., Gu, Q. et al. (2025). Mars: Unleashing the power of variance reduction for training large models. In Forty-second International Conference on Machine Learning. Z.ai (2025). Glm-4.5: Reasoning, coding, and agentic abililties. https://z.ai/blog/glm-4.5. Accessed: 2025-07-31. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A. and Choi, Y. (2019). Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers (A. Korhonen, D. R. Traum and L. Màrquez, eds.). Association for Computational Linguistics. Zeng, A., Lv, X., Zheng, Q., Hou, Z., Chen, B., Xie, C., Wang, C., Yin, D., Zeng, H., Zhang, J. et al. (2025). Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471 . Zhou, D., Xu, P. and Gu, Q. (2020). Stochastic nested variance reduction for nonconvex optimization. Journal of machine learning research 21 163. 16 Hyper-parameter Settings For training parameters, for experiments with either OpenWebText or FineWeb-Edu 100B datasets, we do grid search over learning rates between {5e 4, 1e 3, 3e 3, 5e 3, 6e 3, 1e 2} for Moonlight optimizer. We just apply the same learning rate for MARS-M optimizer. Since Moonlight and MARS-M optimizers are only designed for matrix parameters, we optimize the vector-like parameters and embeddings with AdamW with the same learning rate. We list the architectural hyperparameters for GPT-2 models with 125M (small), 355M (medium), 770M (large) and 1.5B (XL) parameters in Table 2. Moreover, the general hyperparameters used across all experiments are summarized in Table 3, while Table 4 display the training hyperparameters for models with different sizes. Table 2: Architecture hyperparameters for GPT-2 series models (Radford et al., 2019). Model #Param #Layer nhead demb GPT-2 small GPT-2 medium GPT-2 large GPT-2 XL 125M 355M 770M 1.5B 12 24 36 48 12 16 20 25 768 1024 1280 1600 Table 3: General hyper-parameters for the experiments. Hyper-parameter Steps Batch size in total Context length Gradient clipping threshold Dropout Learning rate schedule Warm-up steps Base seed Value 100,000 480 1024 1.0 0.0 Cosine 2000 5000 Table 4: Hyper-parameters for GPT-2 experiments for different datasets. We use β = 0.95 for the optimizers. Max learning rate Hyper-parameter GPT-2 Size small (125M) medium (355M) large (770M) XL (1.5B) small (125M) medium (355M) large (770M) XL (1.5B) Min learning rate OpenWebText FineWebEdu 100B 6e-3 5e-3 5e-3 - 3e-5 6e-5 1e-5 - 1e-2 5e-3 5e-3 3e-3 3e-5 6e-5 1e-5 1e-5 Additional Experiments We just display the curves for training and validation losses for small, medium and large models on the FineWeb-Edu 100B and OpenWebText datasets in Figures 6-8 and Figures 9-11, respectively. 17 And the 0-shot and 2-shot evaluation results for these models in Tables 5-11 and Tables 12-17, respectively. The results of AdamW are taken from Yuan et al. (2025). According to these figures and tables, it can be observed that MARS-M can achieve better performances than AdamW and Moonlight optimizers in most of these cases. Figure 6: The training and validation loss of small-size models (125M) trained with different optimizers on the FineWeb-Edu 100B dataset. Figure 7: The training and validation loss of medium-size models (355M) trained with different optimizers on the FineWeb-Edu 100B dataset. Table 5: The evaluation results of small models pre-trained using the FineWeb-Edu 100B dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: WG = WinoGrande. Method AdamW Muon (Moonlight) MARS-M (γ = 0.01) MARS-M (γ = 0.025) ARC-C ARC-E Hellaswag MMLU OpenBookQA PIQA SciQ WG 50.36 26.54 50.67 26.88 26.28 52.01 51.14 27. 24.49 23.12 23.26 26.01 51.43 52.48 51.47 52.57 71.50 71.20 70.90 71.90 64.53 64.85 65.94 65.23 30.60 32.80 31.80 32.00 36.26 37.58 37.77 37. Avg. 44.46 44.95 44.93 45.55 18 Figure 8: The training and validation loss of large-size models (770M) trained with different optimizers on the FineWeb-Edu 100B dataset. Figure 9: The training and validation loss of small-size models (125M) trained with different optimizers on the OpenWebText dataset. Figure 10: The training and validation loss of medium-size models (355M) trained with different optimizers on the OpenWebText dataset. Figure 11: The training and validation loss of large-size models (770M) trained with different optimizers on the OpenWebText dataset. Table 6: The evaluation results of small models pre-trained using the FineWeb-Edu 100B dataset (2-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: WG = WinoGrande. Avg. ARC-C ARC-E Hellaswag MMLU OpenBookQA PIQA SciQ WG 47.17 51.38 82.90 28.41 27.82 86.40 53.51 48.21 47.47 50.75 82.10 28.41 Method Muon (Moonlight) MARS-M (γ = 0.01) MARS-M (γ = 0.025) Table 7: The evaluation results of medium models pre-trained using the FineWeb-Edu 100B dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: WG = WinoGrande. 64.20 65.02 65.34 25.83 25.66 26. 57.62 58.16 58.42 37.00 37.50 37.06 30.00 31.60 31.20 Method Avg. ARC-C ARC-E Hellaswag MMLU OpenBookQA PIQA SciQ WG MARS 49.66 54.85 76.10 32.68 Muon (Moonlight) 49.40 54.14 32.17 76.80 MARS-M (γ = 0.01) 77.50 30.55 49.33 54.06 MARS-M (γ = 0.025) 32.25 78.30 56.20 49.94 Table 8: The evaluation results of medium models pre-trained using the FineWeb-Edu 100B dataset (2-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: WG = WinoGrande. 59.34 58.92 59.60 60.65 24.98 23.28 25.13 24. 68.88 69.64 69.91 67.90 45.05 45.46 46.11 45.92 35.40 34.80 31.80 33.80 ARC-C ARC-E Hellaswag MMLU OpenBookQA PIQA SciQ WG 33.53 55.80 54.06 33.45 53.83 34.73 54.93 35.24 Method Avg. AdamW 52.17 Muon (Moonlight) 52.21 MARS-M (γ = 0.01) 52.32 MARS-M (γ = 0.025) 52.38 Table 9: The evaluation results of large models pre-trained using the FineWeb-Edu 100B dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: WG = WinoGrande. 86.70 89.00 88.70 89. 66.46 66.33 66.12 67.30 69.10 69.21 70.40 68.28 45.02 45.48 46.23 45.82 35.40 35.40 34.00 33.40 25.32 24.73 24.56 24.98 Method AdamW Muon (Moonlight) MARS-M (γ = 0.01) MARS-M (γ = 0.025) ARC-C ARC-E Hellaswag MMLU OpenBookQA PIQA SciQ WG 57.54 34.64 34.30 57.77 56.59 36.18 54.85 31.74 70.89 72.31 71.65 70.35 63.22 63.34 64.52 62.84 25.45 24.18 25.89 24.35 80.60 81.50 82.30 79.70 50.10 51.93 52.29 51. 38.60 38.80 37.60 35.80 Avg. 52.63 53.07 53.45 51.45 20 Table 10: The evaluation results of large models pre-trained using the FineWeb-Edu 100B dataset (2-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: WG = WinoGrande. ARC-C ARC-E Hellaswag MMLU OpenBookQA PIQA SciQ WG 55.80 38.05 37.03 56.43 39.08 37.03 Method AdamW Muon (Moonlight) MARS-M (γ = 0.01) MARS-M (γ = 0.025) Table 11: The evaluation results of XL models pre-trained using the FineWeb-Edu 100B dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: WG = WinoGrande. Avg. 55.26 70.46 72.03 55.21 72.25 93.30 56.91 55.78 55.41 71.22 38.20 38.20 37.60 39.40 50.30 51.57 52.07 52.22 70.29 70.08 70.16 72.01 26.87 25.32 24.90 25.10 92.10 91. 55.64 90.90 25.47 25.32 24.68 23.57 39.00 43.20 39.80 41.40 85.30 83.50 84.70 85.30 72.69 73.78 73.23 73. 68.22 67.30 68.14 68.56 53.93 56.37 56.83 57.12 ARC-C ARC-E Hellaswag MMLU OpenBookQA PIQA SciQ WG Avg. 54.72 54.78 38.40 58.41 55.87 39.08 55.42 57.22 38.74 55.59 56.75 38.31 Method AdamW Muon (Moonlight) MARS-M (γ = 0.01) MARS-M (γ = 0.025) Table 12: The evaluation results of small models pre-trained using the OpenWebText dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: WG = WinoGrande. Method AdamW Muon (Moonlight) MARS-M (γ = 0.01) MARS-M (γ = 0.025) Table 13: The evaluation results of small models pre-trained using the OpenWebText dataset (2-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: WG = WinoGrande. Method Muon (Moonlight) MARS-M (γ = 0.01) MARS-M (γ = 0.025) Table 14: The evaluation results of medium models pre-trained using the OpenWebText dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: WG = WinoGrande. ARC-C ARC-E Hellaswag MMLU OpenBookQA PIQA SciQ WG 22.27 67.50 52.01 51.38 66.50 23.81 51.38 65.10 23.72 50.83 66.40 23.04 ARC-C ARC-E Hellaswag MMLU OpenBookQA PIQA SciQ WG 24.23 24.49 23. Avg. 53.91 43.04 42.50 51.46 42.60 49.41 Avg. 41.08 41.38 42.47 41.32 27.80 28.60 27.80 30.20 63.00 63.06 74.80 63.22 31.73 33.19 33.36 33.44 41.37 41.67 40.66 40. 22.97 22.83 22.97 22.93 26.80 26.20 27.40 75.20 73.50 77.40 43.18 43.52 43.43 25.00 25.35 23.99 33.20 33.47 33. 62.79 62.02 62.40 Avg. ARC-C ARC-E Hellaswag MMLU OpenBookQA PIQA SciQ WG 42.60 52.49 23.98 52.25 44.40 25.60 24.49 54.46 44.44 44.42 52.64 24.57 Method AdamW Muon (Moonlight) MARS-M (γ = 0.01) MARS-M (γ = 0.025) Table 15: The evaluation results of medium models pre-trained using the OpenWebText dataset (2-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: WG = WinoGrande. 43.43 44.99 44.49 46.42 67.60 72.00 68.01 70.00 65.56 65.94 67.08 66. 27.20 31.20 33.40 32.00 22.80 23.20 23.34 23.28 37.76 40.00 40.22 40.21 Method Muon (Moonlight) MARS-M (γ = 0.01) MARS-M (γ = 0.025) Avg. ARC-C ARC-E Hellaswag MMLU OpenBookQA PIQA SciQ WG 46.65 52.80 81.30 27.13 81.60 26.79 46.68 52.64 26.37 82.40 53.67 46.73 31.40 28.80 31. 48.82 50.17 48.82 67.03 67.14 65.78 24.93 25.42 24.68 39.77 40.87 40.53 21 Table 16: The evaluation results of large models pre-trained using the OpenWebText dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: WG = WinoGrande. Method Avg. ARC-C ARC-E Hellaswag MMLU OpenBookQA PIQA SciQ WG AdamW 45.13 51.46 26.19 Muon (Moonlight) 46.55 55.64 27.30 MARS-M (γ = 0.01) 53.43 26.88 46.94 MARS-M (γ = 0.025) 27.05 57.30 47.02 Table 17: The evaluation results of large models pre-trained using the OpenWebText dataset (2-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: WG = WinoGrande. Method Muon (Moonlight) MARS-M (γ = 0.01) MARS-M (γ = 0.025) ARC-C ARC-E Hellaswag MMLU OpenBookQA PIQA SciQ WG Avg. 49.38 56.12 84.20 27.90 86.40 56.27 49.44 27.99 49.12 55.49 85.50 27.47 72.80 68.12 68.93 72.50 69.37 74.30 72.50 68.82 31.40 30.80 32.00 33.20 46.30 47.85 49.37 48.48 23.10 23.85 24.41 23.59 41.70 45.56 45.74 45. 53.45 53.96 53.07 45.27 45.94 45.33 69.53 68.82 69.21 25.77 24.76 25.10 32.80 31.40 31.80 Proof of Theorem 4. We have the following lemma from Yuan et al. (2025): Lemma C.1 (Yuan et al. 2025). In Algorithm 2. Under Assumption 4.1 and 4.2, if 1 βt+1 0, t, denote := (Xt+1, ξt+1) (Xt, ξt+1), under approximate choice of γt+1: (cid:0)Et2 βt+1Et2 βt+1Et2 Et2 βt+1Et2 Gt+1 + βt+ γt+1 = 1 Gt+1 (cid:1)(cid:17) = (cid:16) , we have EF (Xt+1) Mt+12 β2 t+1 EF (Xt) Mt2 + 2β t+1L2EXt+1 Xt2 + 2(1 βt+1)2σ2 (cid:102)Mt+1, where and (cid:32) (cid:102)Mt+1 := Et2 A2 t+1 (cid:16) βt+1(1 γt+1) At+1 (cid:33) (cid:17) , (C.1) At+1 := Gt+1 + βt+1tr(cid:0) Var (cid:0)t Et2 (cid:1)(cid:1) , Gt+1 := (1 βt+1)E (cid:28) t, (Xt+1, ξt+1) (Xt+1) (cid:29) (cid:28) + βt+1E (Xt+) (Xt), (Xt) Mt . (cid:29) We also need the following lemmas, which can be seen as an extension of Lemma C.3 in Yuan et al. (2025). Lemma C.2. For Algorithm 2, under Assumption 4.2, we have the following inequality hold: (Xt+1) (Xt) ηtMtF + ηt ρ (Xt) Mt2 + ηtρ 4 Ot + Lη2 2 Ot2 , where ρ > 0 is an arbitrarily chosen parameter. 22 Proof. According to Assumption 4.2, we have the upper bound for the function value: (Xt+1) (Xt) + (Xt), Xt+1 Xt + = (Xt) + (Xt), Xt+1 Xt + 2 2 Xt+1 Xt2 Xt+1 Xt2 Lη2 2 = (Xt) ηtMt, Ot ηtF (Xt) Mt, Ot + Ot2 Here Ot = UtV is defined in (4.1). According to the definition of Ot, we have: ηtMt, Ot = ηtMt, UrV = ηtMt ηtMtF , where the last inequality follows from Mt MtF . Moreover, by AM-GM inequality, it holds that ηtF (Xt) Mt, Ot ηt (cid:18) 1 ρ (Xt) Mt2 + ρ 4 Ot2 (cid:19) for some constant ρ > 0 . Putting all pieces together, we can obtain (Xt+1) (Xt) ηtMtF + ηt ρ (Xt) Mt2 + ηtρ 4 Ot2 + Lη2 2 Ot2 This completes the proof. Lemma C.3. Let ηt = (s + t)2/3, 1, 1. Then η1 η1 t1 η1/2 , 1. Proof. By the definition of ηt, it holds that 1 ηt 1 ηt1 = (s + t)2/3 (s + 1)2/3 2 3(s + 1)1/3 η1/2 , where the first inequality follows by the concavity of h(x) = x2/3. This finishes the proof. Now were ready to prove Theorem 4.3. Proof of Theorem 4.3. 3 First, we define the Lyapunov function as (cid:104) Φt = (Xt) + ρt 16L2ηt (Xt) Mt2 (cid:105) , 1. ηt. Then we calculate the difference between two consecutive Lyapunov functions where ρt = 4 as: 2L Φt+1 Φt = E[F (Xt+1) (Xt)] (cid:125) (cid:124) (cid:123)(cid:122) I1 (cid:34) + (cid:124) ρt+1 16L2ηt (Xt+1) Mt+12 ρt 16L2ηt (cid:123)(cid:122) I2 (Xt) Mt2 . (C.2) (cid:35) (cid:125) 3Here we just ignore the factor 0.2 (cid:112)max(m, n) since such factor can be integrated into ηt. 23 I2 = (cid:104) ρt+1 16L2ηt ρt 16L2 ρt 16L2ηt (cid:16) β2 t+1 ηt ρt 16L2 (cid:102)Mt+1 For I1, we use Lemma C.2 to obtain (cid:104) I1 ηtMtF + ηt ρt (Xt) Mt2 + ηtρt 4 Ot2 + Lη2 2 Ot2 (cid:105) . (C.3) For I2, we use Lemma C.1 to obtain (Xt+1) Mt+12 ρt 16L2ηt1 (Xt) Mt2 (cid:105) (cid:16) β2 t+1 ηt (cid:17) 1 ηt1 EF (Xt) Mt2 + t+ ρtβ2 8ηt EXt+1 Xt2 + ρt(1 βt+1)2σ2 8L2ηt (cid:17) 1 ηt1 EF (Xt) Mt2 + ρt 8ηt EXt+1 Xt2 + ρtc2ηtσ2 8L2 ρt 16L2ηt (cid:102)Mt+1, (C.4) where the first inequality is due to ρt+1 ρt, and the last inequality follows from the definition that βt+1 = 1 2ηt. Further, for the first term on the right hand side, we have ρt 16L2 (cid:16) β2 t+1 ηt (cid:17) 1 ηt1 = ρt 16L2 ρt 16L2 (cid:16) βt+1 ηt (cid:16) 1 ηt 1 ηt1 1 ηt (cid:17) ρt 16L2 (cid:16) 1 2ηt ηt (cid:17) 1 ηt = (cid:17) . 2 From Lemma C.3, we know that 1 ηt 1 ηt η1/2 . Since ρ2 = 32L2ηt, we obtain4 ρt 16L2 (cid:16) β2 t+1 ηt (cid:17) 1 ηt1 (cid:16) ρt 16L2 η1/2 (cid:17) (cid:16) ρt 16L2 (cid:17) 1 = 2ηtρ1 . (C.5) Bringing (C.5) into (C.4), we arrive at the upper bound for I2: I2 2ηt ρt EF (Xt) Mt2 + ρt 8ηt EXt+1 Xt2 + ρtc2ηtσ2 8L2 ρt 16L2ηt (cid:102)Mt+1. (C.6) Now combining (C.2), (C.3) and (C.6), we derive Φt+1 Φt ηt ρt EF (Xt) Mt2 ηtEMtF + ρt 8ηt EXt+1 Xt2 + ρtc2ηtσ2 8L2 ρt 16L2ηt (cid:102)Mt+1 + ηtρt Ot2 + Lη2 2 Ot2 . (C.7) Moreover, according to the definition of Ot, we have Ot2 (cid:88) i=1 1 = n. 4As long as ηt 1/2, 1 βt+1 = 1 2ηt >= 0 is satisfied. 24 Therefore, ρt 8ηt EXt+1 Xt2 ηtρt 4 Lη2 2 Ot2 Ot = ρtηt 8 EOt2 ρtηtn 8 , EηtOt2 = ρt 8ηt ρtηtn 4 Lη2 2 , . Taking telescoping sum for = 1, , in (C.7), and applying ρt = 4 2L ηt give (cid:88) (cid:16) t=1 4 ηt 2L EF (Xt) Mt + ηtEMtF (cid:17) Φ1 ΦT +1 + Φ1 ΦT +1 + σ2c2 8L2 (cid:16) (cid:88) ρtηt + t=1 2σ2 3 + (cid:88) t=1 (cid:0) 3ρtηtn 8 + Lη2 2 (cid:1) (cid:88) t=1 2 ηt 4L (cid:102)Mt+ 2Ln 2 (cid:17) (cid:88) t=1 1 + + Ln 2 ηt 4L (cid:102)Mt+1 (cid:88) t= 1 (s + t)4/3 (cid:88) t=1 (cid:102)Mt+1, Φ1 ΦT +1 + log(s + ) + 3Ln 2s1/3 (cid:88) t=1 2 ηt 4L (cid:16) 2 where = And for Φ1, 2σ2 + 2Ln 2 (cid:17) . By the definition of Φt, we have ΦT +1 (XT +1) minX (X). (cid:104) Φ1 = (X1) + (X1) + (X1) + (cid:105) ρ1s2/3 16L2 (X1) M12 2s1/3 4L 2s1/3σ2 4L . E[F (X1) (X1, ξ1)2 ] Consequently, defining = (X1) minX (X) + 2s1/3σ2 4L + 3Ln 2s1/3 , the following inequality holds:"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 EF (Xt) Mt"
        },
        {
            "title": "4\nT",
            "content": "2LG ηT +"
        },
        {
            "title": "4\nT",
            "content": "2LB ηT log(s + ) 2 ηT (cid:88) t= (cid:102)Mt+1 ηt 8LG 2/3 + 8LB 2/3 log(s + )"
        },
        {
            "title": "2\nT 2/3",
            "content": "T (cid:88) t=1 (cid:102)Mt+1 ηt , where the last inequality holds when s. And it also holds that"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 ηtEMtF"
        },
        {
            "title": "G\nT",
            "content": "+"
        },
        {
            "title": "B\nT",
            "content": "log(s + ) 2 4LT (cid:88) t=1 (cid:102)Mt+1 ηt . 25 Therefore, 1 (cid:88) t= EMtF ηT + ηT log(s + ) 2 4LT ηT (cid:88) t=1 (cid:102)Mt+1 ηt G(s + )2/3 + B(s + )2/3 log(s + ) 2G 1/ + 2B 1/3 log(s + ) 2 4LT 1/3 (cid:88) t=1 (cid:102)Mt+1 ηt . 2(s + )2/3 4LT (cid:88) t= (cid:102)Mt+1 ηt Finally, by triangle inequality, we have 1 (cid:88) t=1 EF (Xt)F 1 EF (Xt) MtF + 1 (cid:88) t=1 EMtF (cid:88) t=1 (cid:88) t="
        },
        {
            "title": "1\nT\n(cid:118)\n(cid:117)\n(cid:117)\n(cid:116)",
            "content": ""
        },
        {
            "title": "1\nT\n√",
            "content": "(cid:113) EF (Xt) Mt2 + (cid:88) t=1 EF (Xt) Mt +"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:88) t=1 EMtF EMtF 2 2LG 1/3 2(cid:112)2LB log(s + ) 1/3 + + 2G 1/3 + 2B 1/3 log(s + ) 2 4LT 1/3 (cid:88) t=1 (cid:102)Mt+1 ηt . This completes the proof."
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of California, Los Angeles, CA, USA"
    ]
}