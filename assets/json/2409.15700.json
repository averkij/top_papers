{
    "paper_title": "Making Text Embedders Few-Shot Learners",
    "authors": [
        "Chaofan Li",
        "MingHao Qin",
        "Shitao Xiao",
        "Jianlyu Chen",
        "Kun Luo",
        "Yingxia Shao",
        "Defu Lian",
        "Zheng Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) with decoder-only architectures demonstrate remarkable in-context learning (ICL) capabilities. This feature enables them to effectively handle both familiar and novel tasks by utilizing examples provided within their input context. Recognizing the potential of this capability, we propose leveraging the ICL feature in LLMs to enhance the process of text embedding generation. To this end, we introduce a novel model bge-en-icl, which employs few-shot examples to produce high-quality text embeddings. Our approach integrates task-related examples directly into the query side, resulting in significant improvements across various tasks. Additionally, we have investigated how to effectively utilize LLMs as embedding models, including various attention mechanisms, pooling methods, etc. Our findings suggest that retaining the original framework often yields the best results, underscoring that simplicity is best. Experimental results on the MTEB and AIR-Bench benchmarks demonstrate that our approach sets new state-of-the-art (SOTA) performance. Our model, code and dataset are freely available at https://github.com/FlagOpen/FlagEmbedding ."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 2 ] . [ 1 0 0 7 5 1 . 9 0 4 2 : r MAKING TEXT EMBEDDERS FEW-SHOT LEARNERS Chaofan Li1,2, MingHao Qin1,3,, Shitao Xiao1, Jianlyu Chen1,4, Kun Luo1,3, Yingxia Shao2, Defu Lian4, Zheng Liu1 1: Beijing Academy of Artificial Intelligence 2: Beijing University of Posts and Telecommunications 3: Chinese Academy of Sciences 4: University of Science and Technology of China {cfli, shaoyx}@bupt.edu.cn qinminghao24@ia.ac.cn stxiao@baai.ac.cn chenjianlv@mail.ustc.edu.cn liandefu@ustc.edu.cn {luokun695, zhengliu1026}@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) with decoder-only architectures demonstrate remarkable in-context learning (ICL) capabilities. This feature enables them to effectively handle both familiar and novel tasks by utilizing examples provided within their input context. Recognizing the potential of this capability, we propose leveraging the ICL feature in LLMs to enhance the process of text embedding generation. To this end, we introduce novel model bge-en-icl, which employs few-shot examples to produce high-quality text embeddings. Our approach integrates task-related examples directly into the query side, resulting in significant improvements across various tasks. Additionally, we have investigated how to effectively utilize LLMs as embedding models, including various attention mechanisms, pooling methods, etc. Our findings suggest that retaining the original framework often yields the best results, underscoring that simplicity is best. Experimental results on the MTEB and AIR-Bench benchmarks demonstrate that our approach sets new state-of-the-art (SOTA) performance. Our model, code and dataset are freely available at https://github.com/FlagOpen/FlagEmbedding."
        },
        {
            "title": "INTRODUCTION",
            "content": "Text embeddings are vector representations that capture the semantic and contextual meaning of natural language text. They play pivotal role in natural language processing (NLP) tasks, facilitating wide range of applications such as information retrieval, text classification, item recommendation, and question answering (Karpukhin et al., 2020; Xiong et al., 2020; Lu et al., 2020). Pre-trained bidirectional encoder and encoder-decoder architectures have been widely adopted as backbone models for embedding model, owing to their effectiveness in producing high-quality vector embeddings for text thanks to their extensive pre-training (Xiao et al., 2022; Gao et al., 2021). Recent advancements in LLMs have significantly shifted the focus towards embedding models that rely primarily on decoder-only architectures (Ma et al., 2023; Li et al., 2024; Wang et al., 2023). These LLM-based embedding models have demonstrated remarkable improvements in in-domain accuracy and generalization, particularly when trained using supervised learning approaches (Wang et al., 2023). However, despite these advances, embedding models still struggle to follow unseen task instructions and execute complex retrieval tasks Su et al. (2024); Weller et al. (2024). This limitation stems from mismatch between the relatively narrow range of instructions encountered during training and the broader variety of real-world text embedding tasks. In-context learning (ICL) is core capability of LLMs, enabling them to incorporate task-specific examples directly into input prompts to generate desired outputs (Radford et al., 2019; Brown, 2020; Co-first authors Corresponding author 1 Gao et al., 2020). The scope of ICL extends beyond tasks seen during training; it enables LLMs to generalize to new and complex tasks by learning patterns from the provided examples. This allows LLMs to adapt dynamically to novel tasks without additional training, making them highly applicable to large-scale, real-world scenarios (Wei et al., 2022; Yao et al., 2022; Dong et al., 2022). Recognizing the robust ICL abilities of LLMs, in this study, we propose to generate more adaptable text embeddings with ICL strategy. Specifically, we guide the model by including task-specific examples directly within the query prompt. By doing so, we leverage the ICL capabilities of LLMs to produce embeddings that are not only more relevant to the specific domain but also more generalizable across various contexts. Moreover, LLMs are predominantly utilized for text generation tasks, and adapting them for embedding representation tasks requires specific fine-tuning strategies. Recent studies have introduced various approaches, including the generation of high-quality training data through LLMs (Wang et al., 2023), modifications to attention mechanisms, and changes in pooling methods (Ma et al., 2023; Li et al., 2024). Following previous works (Muennighoff et al., 2024; BehnamGhader et al., 2024), we investigate how to effectively utilize LLMs as embedding models by modifying various architectures, e.g., bidirectional attention, meaning pooling, etc. Our experimental findings indicate that in the ICL scenario, making complex modifications to the models does not lead to significant improvements. Surprisingly, the best results are obtained using the original, unmodified architecture. By employing only the ICL strategy, our model bge-en-icl achieves state-of-the-art (SOTA) results on both the MTEB and AIR-Bench benchmarks. We have also released multilanguage embedding model bge-multilingual-gemma2 and lightweight reranker bge-rerankerv2.5-gemma2-lightweight. The lightweight reranker also serves as the teacher model for training embedding models through distillation. Further details are provided in Appendices and D. In summary, the key contributions of our work are as follows: We propose bge-en-icl, which incorporate few-shot examples into the query side to enhance the query embeddings. This integration leverages the in-context learning (ICL) capabilities of large language models (LLMs) in text embedding tasks. We rethink and explore how to effectively utilize LLMs as embedding models by evaluating various attention mechanisms, pooling methods, and the incorporation of passage prompts. Our findings highlight that simplicity is best; simply combining ICL capabilities with embedding models can achieve excellent performance. In contrast to other leading models on the MTEB benchmark, we provide open access to our model checkpoint, dataset, and training scripts."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Text embedding is critical research direction in the field of information retrieval, with wide-ranging applications including web search, question answering, and dialogue systems. The fundamental principle involves encoding both queries and documents into embedding vectors within the same latent space. By calculating similarity scores between these vectors, effective retrieval is achieved. In recent years, numerous studies have leveraged pre-trained language models such as BERT (Devlin, 2018), T5 (Raffel et al., 2020), and RoBERTa (Liu, 2019) as the backbone for embedding models. These models have consistently demonstrated superior performance compared to traditional sparse retrieval methods. The capability of the backbone is crucial determinant in the effectiveness of retrieval systems. (Luo et al., 2024) have demonstrated that performance improves with increased scale and extensive pretraining. Currently, numerous studies have explored the effectiveness of utilizing LLMs as backbone encoders for text embedding tasks. Repllama (Ma et al., 2023) fine-tuned Llama-2 to serve as both dense retriever and reranker, demonstrating the effectiveness of applying large language models (LLMs) in text embedding tasks. To further align LLMs with text embedding tasks, Llama2Vec (Li et al., 2024) introduced two pretraining tasks specifically designed to enhance the models performance in such tasks, which led to significant improvements on the BEIR benchmark. E5-mistral and Gecko (Wang et al., 2023; Lee et al., 2024b) advanced the training of LLM-based embedding models through the use of syn2 Figure 1: The architecture of the ICL-based model. thetic data, markedly boosting their performance across diverse range of retrieval and non-retrieval tasks. NV-Embed (Lee et al., 2024a) innovatively proposed latent attention layer to replace conventional pooling methods and implemented two-stage training strategy to address the challenge of false negatives in non-retrieval tasks. This model has shown strong performance in both retrieval and non-retrieval domains. Additionally, GRIT (Muennighoff et al., 2024) successfully integrated text embedding and generation within single LLM, achieving performance levels on par with specialized models focused solely on either embedding or generation. In the exploration of LLMs as embedding models from an unsupervised perspective, LLM2Vec (BehnamGhader et al., 2024) presented novel unsupervised method to transform decoder-only LLMs into embedding models. This approach demonstrated significant potential for modifying LLM backbone encoders to perform retrieval without any supervision. Similarly, PromptReps (Zhuang et al., 2024) leveraged chat-based LLMs aligned with human preferences to generate high-quality dense representations in an unsupervised manner. The LLM-based embedding models mentioned above exhibit commendable performance across both retrieval and non-retrieval tasks. However, much of the existing work has disproportionately focused on altering model architectures, thereby neglecting the intrinsic capabilities of LLMs. Even models like GritLM, which integrate generation and embedding functionalities, fail to fully exploit the potential ICL capabilities of LLMs within the embedding process. By leveraging the innate ICL capabilities of LLMs, embedding models can be more versatile and adapt to diverse scenarios without necessitating additional fine-tuning. Our model not only achieves SOTA results on the MTEB and AIR-Bench benchmarks but also effectively utilizes the inherent strengths of LLMs across tasks."
        },
        {
            "title": "3 METHOLOGY",
            "content": "3.1 IN-CONTEXT LEARNING FOR EMBEDDING MODELS Previous embedding models often involve directly inputting the query into the model to generate target embeddings. However, this method struggles to handle tasks with different intents, limiting the models adaptability and generalization capabilities. To address this, researchers have introduced task instructions (Su et al., 2022) appended to queries, enabling single embedding model to generalize across tasks in various domains by altering the instructions. 3 Despite these advances, studies such as Su et al. (2024); Weller et al. (2024) reveal that embedding models have limited ability to follow unseen embedding task instructions and conduct complex retrieval tasks. This limitation arises from gap between the limited diversity of instructions seen during training and the vast range of real-world scenarios. Inspired by the ability of LLMs to generalize to unseen tasks through in-context learning (ICL), we explore whether embedding models can be enhanced by leveraging ICL, thereby significantly improving their generalization and versatility across diverse embedding tasks with various user intents. In this work, we demonstrate the potential of embedding models to benefit from ICL through fewshot contrastive training. Consider query-passage pair (qi, pi) in an embedding task. We first construct an example template as follows: Instruct {task definition} query {qi} response {pi} (1) Here, task definition represents the description of the specific embedding task. This example template is applied to new input queries for each embedding task (Figure 1). For relevant querypassage pair (q+, p+), the modified query q+ exp is constructed as follows: {example 1} ... {example n} Instruct {task definition} query {q+} response (2) All modified queries and passages in the corpus are encoded using the same LLM to obtain their embedding representations. Specifically, we append an [EOS] token to the end of the input modified , hp+) by extracting queries and passages, feeding them into the LLM to obtain embeddings (hq+ the final layers [EOS] vector. We employ the standard InfoNCE (Izacard et al., 2021) loss function L, utilizing both in-batch negatives and hard negatives for training: exp = log exp(s(q+ exp, p+ exp(s(q+ )) + (cid:80) exp, p+ )) exp(s(q+ exp, )) (3) denotes the set of negative passages, and s(q, p) is the scoring function between the query and passage. In this work, we adopt temperature-scaled cosine similarity function defined as: s(q, p) = 1 τ cos(hq, hp) (4) where τ is temperature hyperparameter, which is fixed at 0.02 during training."
        },
        {
            "title": "3.2 REPRESENTATION METHOD",
            "content": "The attention mechanism in LLM-based embedding models is typically unidirectional, aligned with the next-token prediction task fundamental to their pre-training (Touvron et al., 2023). However, recent studies indicate that unidirectional attention may limit the models capacity for representation learning. Evidence suggests that bidirectional attention is more effective at integrating contextual information, resulting in improved performance on certain tasks. For example, LLM2Vec (BehnamGhader et al., 2024) introduces an additional training phase with masked token prediction task, preconditioning the model for bidirectional attention. Approaches such as NV-Embed (Lee et al., 2024a) and GritLM (Muennighoff et al., 2024) replace unidirectional attention with bidirectional attention during the embedding training phase, often employing mean pooling or more sophisticated latent attention layers to obtain representations for queries and passages. Despite these advances, we argue that incorporating bidirectional attention during embedding finetuning creates mismatch with the models pre-training design, potentially undermining its incontext learning and generative properties. To address the trade-off between enhancing embedding representations for specific tasks and preserving the models inherent generative properties for deep semantic pattern understanding, our approach retains the unidirectional attention mechanism, consistent with the majority of existing embedding methods. We use the [EOS] tokens output embedding as the vector representation for queries and passages, positioning it at the end of inputs to capture both semantic and ICL patterns through causal attention 4 mechanisms, thereby aligning with the foundational pretraining methodology of LLMs. Specifically, given the tokenized input sequence T: [BOS], t1, ..., tN is sent into the LLM (Figure 1): ht = LLM(T)[EOS] (5) The text embedding is taken from the output embedding of the special token [EOS]. 3.3 ICL-BASED INSTRUCTION-TUNING While previous works (Wang et al., 2023; Lee et al., 2024a) have proposed the training method of instruction-tuning, which incorporates large number of task-specific instructions during the training process, enabling the model to adapt to various downstream retrieval tasks based on different instructions, it is not applicable to the ICL strategy. As demonstrated by GRIT (Muennighoff et al., 2024), directly supplying few-shot examples when generating embeddings can actually degrade model performance. To incorporate ICL capabilities into models, we need to modify the conventional instruction tuning strategy. Our approach involves integrating ICL abilities during the training phase. Specifically, we provide task-relevant examples to the query throughout the training process, allowing the model to develop ICL capabilities as it learns. Recognizing the risk of compromising zero-shot capabilities if examples are consistently provided during training, we propose dynamic training process. In each training step, queries are supplied with variable number of few-shot examples, ranging from zero to n, determined by sampling function. This approach maintains balance between developing ICL abilities and preserving zeroshot performance. To further enhance the models ICL capabilities, we introduce an innovative technique for examples selection. By incorporating in-batch pairs as few-shot examples, we train the model to better differentiate between examples and inputs, aims to improve the models ability to generate reliable embeddings based on the provided examples."
        },
        {
            "title": "4 EXPERIMENTENS",
            "content": "In this section, we examine the effectiveness of the ICL training pipeline and reconsider the training methodologies for LLM-based embedding models. RQ 1: What is the effectiveness of the ICL training strategy for both zero-shot and few-shot learning scenarios? RQ 2: How does the ICL training strategy impact performance compared to traditional training methods? RQ 3: How does the integration of in-batch examples affect the performance of the ICL training strategy. RQ 4: What are the implications of replacing causal attention mask with bidirectional attention mask within the framework of LLMs? RQ 5: What is the impact of various representation strategies, including last token pooling and mean pooling, on model performance? RQ 6: Do passage-based prompts enhance performance in the ICL training strategy? 4.1 SETUP LLM. Following E5-Mistral (Wang et al., 2023), SFR, and NV-Embedder (Lee et al., 2024a), we have adopted Mistral-7B (Jiang et al., 2023) as the backbone for our framework. Evaluation. We evaluate the performance of our model on MTEB (Muennighoff et al., 2022) and AIR-Bench. MTEB is comprehensive benchmark designed to evaluate the performance of text embedding models. AIR-Bench is dedicated to the evaluation of retrieval performance, its testing data is automatically generated by large language models without human intervention. 5 Training Data. To ensure fair comparison, we use the same public datasets from E5-Mistral (Wang et al., 2023), which includes ELI5 (Fan et al., 2019), HotpotQA (Yang et al., 2018), FEVER (Thorne et al., 2018), MIRACL (Zhang et al., 2023), MSMARCO passage and document ranking (Nguyen et al., 2016), NQ (Karpukhin et al., 2020), NLI (Gao et al., 2021), SQuAD (Karpukhin et al., 2020), TriviaQA (Karpukhin et al., 2020), Quora Duplicate Questions (DataCanary et al., 2017), MrTyDi (Zhang et al., 2021), DuReader (Qiu et al., 2022), and T2Ranking (Xie et al., 2023), all of which are also used for LLM2Vec (BehnamGhader et al., 2024). However, methods that typically perform exceptionally well, such as NV-Embedder (Lee et al., 2024a) and SFR, often require more training data. Additionally, some of these methods, such as GTE-Qwen2 (Li et al., 2023), do not disclose their sources of training data. In response, we have developed an enhanced version of our model that leverages more comprehensive dataset, which includes the following training sets: Retrieval: ELI5, HotpotQA, FEVER, MSMARCO passage and document ranking, NQ, NLI, SQuAD, TriviaQA, Quora Duplicate Questions, Arguana (Wachsmuth et al., 2018), and FiQA (Maia et al., 2018). Reranking: SciDocsRR (Cohan et al., 2020) and StackOverFlowDupQuestions (Liu et al., 2018). Classification: 2013), AmazonReviews-Classification AmazonCounterfactual-Classification Banking77- (ONeill Classification (Casanueva et al., 2020), Emotion-Classification (Saravia et al., 2018), TweetSentimentExtraction-Classification (Maggie, 2020), MTOPIntent-Classification (Li et al., 2020), IMDB-Classification (Maas et al., 2011), ToxicConversations-Classification (Adams et al., 2019). (McAuley & Leskovec, et 2021), al., Clustering: {Arxiv/Biorxiv/Medrxiv/Reddit/StackExchange}-Clustering-{S2S/P2P}, TwentyNewsgroups-Clustering (Lang, 1995). STS: STS12 (Agirre et al., 2012), STS22 (Chen et al., 2022), STS-Benchmark (Cer et al., 2017). Training Detail. We fine-tune the Mistral-7B model using contrastive loss and conduct the process over single epoch. For efficient fine-tuning, we employ Low-Rank Adaptation (LoRA) (Hu et al., 2021), setting the LoRA rank to 64 and the LoRA alpha to 32, with learning rate of 1e-4. For retrieval tasks, we use in-batch negatives, strategy not adopted for other tasks. Each dataset incorporates 7 hard negatives. The batch size is set to 512 for retrieval tasks and 256 for other types of tasks. We maintain consistency by using the same dataset throughout one training step, and the maximum sequence length is set at 512 tokens. To distill the score from reranker in retrieval tasks, we use the bge-reranker model as the teacher. For in-context learning training, we implement randomized sampling method. For each query, we select between 0 to 5 examples from the in-batch training data. The maximum allowable lengths for example queries and documents are set to 256 tokens each, and the combined length for query with examples is set at 2048 tokens. Evaluation. We evaluate the performance of our model under both zero-shot and few-shot conditions. In the few-shot scenario, consistent set of in-context examples is applied to each query. The examples utilized for evaluation are sourced from training datasets. In cases where training datasets are unavailable, examples are generated using ChatGPT."
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "MTEB. Table 1 presents the performance of our model, bge-en-icl, evaluated on the MTEB benchmark. This evaluation contrasts the results obtained from using the full dataset with those obtained from using only the public dataset. When leveraging the full dataset, our model demonstrates strong capabilities in both zero-shot and few-shot settings, achieving SOTA results in few-shot scenarios. However, it is important to note that the use of full datasets may introduce inconsistencies, as different models often rely on varying datasets. Notably, many of these models do not disclose the specific datasets they use, leading to potential unfair comparisons. For fairer comparison and to better understand the impact of in-context learning, we conducts an evaluation using only the public dataset. Under these constraints, our models performance in 6 Task # of datasets E5-mistral-7b-instruct GritLM-7B SFR-Embedding Linq-Embed-Mistral voyage-large-2-instruct NV-Embed-v1 bge-multilingual-gemma2 stella en 400M v5 gte-Qwen2-7B-instruct SFR-Embedding-2 stella en 1.5B v5 bge-en-icl (zero-shot) bge-en-icl (few-shot) E5-mistral-7b-instruct GritLM-7B LLM2Vec-Mistral-supervised bge-en-icl (zero-shot) bge-en-icl (few-shot) Retr. Rerank. Clust. PairClass. Class. 15 12 11 4 3 STS Summ. Avg. 56 1 56.90 57.41 59.00 60.19 58.28 59.36 59.24 58.97 60.25 60.18 61.01 61.67 62.16 52.78 53.10 55.99 59.59 60.08 w/ full data 50.26 50.61 51.67 51.42 53.35 52.80 54.65 56.70 56.92 56.17 57.69 57.51 57.89 60.21 60.49 60.64 60.29 60.09 60.59 59.72 60.16 61.42 60.14 61.21 59.66 59.82 w/ public data only 60.38 61.30 58.42 56.85 56. 47.78 48.90 45.54 42.61 46.55 88.34 87.16 88.54 88.35 89.24 86.91 85.84 87.74 85.79 88.07 88.07 86.93 88.14 88.47 86.90 87.99 87.87 88.51 78.47 79.46 78.33 80.20 81.49 87.35 88.08 86.67 86.58 89.05 87.63 88.62 88.95 76.80 77.00 76.63 75.47 77.31 84.66 83.35 85.05 84.97 84.31 82.84 83.88 84.22 83.04 81.26 84.51 83.74 84. 83.77 82.80 84.09 83.30 83.69 31.40 30.37 31.16 30.98 30.84 31.20 31.20 31.66 31.35 30.71 31.49 30.75 30.77 31.90 29.40 29.96 29.52 30.68 66.63 66.76 67.56 68.17 68.23 69.32 69.88 70.11 70.24 70.31 71.19 71.24 71.67 64.56 64.70 64.80 64.67 66.08 Table 1: Top MTEB leaderboard models as of August 27, 2024. Domain # of datasets wiki 1 web 1 news 1 healthcare 1 law finance 1 arxiv msmarco Avg. 1 1 8 E5-mistral-7b-instruct SFR-Embedding NV-Embed-v1 Linq-Embed-Mistral gte-Qwen2-7B-instruct stella en 1.5B v5 bge-en-icl (zero-shot) bge-en-icl (few-shot) 61.67 44.41 63.46 51.27 62.84 50.42 61.04 48.41 63.46 51.20 61.99 50.88 64.61 54.40 64.94 55. w/ full data 56.32 58.76 58.53 60.18 54.20 58.81 57.25 58.85 48.18 52.21 51.46 49.44 54.07 53.87 55.11 56.02 w/ public data only 19.32 23.27 20.65 20.34 22.31 23.22 25.10 28.29 54.79 56.94 49.89 50.04 58.20 57.26 54.81 57.16 44.78 47.75 46.10 47.56 40.27 44.81 48.46 50. bge-en-icl (zero-shot) bge-en-icl (few-shot) 64.82 54.96 66.98 56.38 55.82 57.17 57.06 59.54 28.87 32.03 54.46 58. 49.60 51.36 59.03 58.99 60.27 60.50 58.39 61.38 63.71 64.50 63.25 65.05 48.56 51.58 50.02 49.69 50.26 51.53 52.93 54.36 53.60 55.92 Table 2: QA (en, nDCG@10) performance on AIR-Bench 24.04. the zero-shot scenario is on par with, or slightly below, that of other models such as LLM2Vec and GritLM. However, in the few-shot settings, our model show significant enhancements (1.41), particularly in the classification and clustering tasks that were not part of the training data. These improvements underscore the potential advantages of in-context learning, emphasizing its efficacy in adapting to tasks beyond the direct scope of initial training parameters. Furthermore, in contrast to training exclusively with public datasets, the utilization of full training data effectively familiarizes the model with these datasets. As result, the models ability to generalize effectively is compromised, leading to only modest improvement in few-shot settings (0.43). AIR-Bench. The performance of our model is also evaluated using the AIR-Bench dataset. As illustrated in Tables 2 and 3, the model demonstrates superior performance compared to prior models in both zero-shot and few-shot scenarios, excelling across qa and long-doc tasks. Notably, there is no overlap between the training dataset and the evaluation data for these tasks, highlighting the robustness of the model in scenarios with limited prior exposure. In the few-shot setting, the model exhibits significant improvements over the zero-shot scenario, achieving gains of 1.43 points in the qa task and 1.08 points in the long-doc task. This improvement underscores the efficacy of in-context learning in enhancing the models generalization capabilities. 7 Domain # of datasets text-embedding-3-large E5-mistral-7b-instruct SFR-Embedding NV-Embed-v1 Linq-Embed-Mistral gte-Qwen2-7B-instruct stella en 1.5B v5 bge-multilingual-gemma2 bge-en-icl (zero-shot) bge-en-icl (few-shot) bge-en-icl (zero-shot) bge-en-icl (few-shot) book 2 healthcare 5 arxiv 4 74.53 72.14 72.79 77.65 75.46 63.93 73.17 71.77 78.30 79.63 w/ full data 73.16 72.44 72.41 75.49 73.81 68.51 74.38 76.46 78.21 79.36 w/ public data only 79.73 79.82 78.66 80.37 65.83 68.44 67.94 72.38 71.58 65.59 70.02 73.96 73.65 74.80 72.88 74.60 law 4 64.47 62.92 64.83 69.55 68.58 65.26 69.32 70.86 67.09 67. 70.59 71.66 Avg. 15 68.77 68.49 69.00 73.45 72.11 65.45 71.25 72.88 73.75 74.83 74.86 75.98 Table 3: Long-Doc (en, Recall@10) performance on AIR-Bench 24.04. However, when the model is trained exclusively using public data, it achieves better results compared to training with the full dataset. This could be attributed to the presence of an excessive amount of MTEB-related data, such as clustering and classification, within the full dataset. Such data might introduce the risk of overfitting, thereby potentially hampering the models generalization performance on the AIR-Bench dataset. 4.3 IN-CONTEXT LEARNING Task # of datasets w/o in-context learning w/ fix examples (zero-shot) w/ fix examples (few-shot) w/ in-batch examples (zero-shot) w/ in-batch examples (few-shot) 59.11 48.98 59.00 59.59 60.08 4 11 w/ full data 42.60 41.84 45.75 42.61 46. 57.02 56.48 56.90 56.85 56.67 Retr. Rerank. Clust. PairClass. Class. 15 12 3 STS 10 Summ. Avg. 1 87.99 85.94 88.54 87.87 88.51 76.27 74.38 75.56 75.47 77.31 83.93 84.31 84.67 83.30 83.69 30.50 29.68 30.66 29.52 30.68 64.83 61.50 65.46 64.67 66. Table 4: Evaluation of various ICL strategies on the MTEB Benchmark. To evaluate the impact of the ICL strategy, we conduct series of ablation studies using the MTEB benchmark. In these studies, we compare the performance of models fine-tuned with the ICL strategy against those fine-tuned without it. Specifically, for ICL training, we employ two distinct training approaches: fixed examples and in-batch examples. In the fixed examples approach, each task was trained using three predetermined examples. In Table 4, we present various results from our experiment. When the model is trained without ICL strategy, its average performance is 64.83. This performance is comparable to GritLM (Muennighoff et al., 2024), LLM2Vec (BehnamGhader et al., 2024), etc. When fixed examples are used during ICL training, there is significant decline in zero-shot evaluation performance, with decrease of 3.33 points. This decline is attributed to the models consistent exposure to specific training examples, which can impair its zero-shot capabilities. On the other hand, in few-shot scenarios, the model demonstrates improved performance, exceeding zero-shot results by 3.96 points and surpassing models trained without ICL by 0.63 points. This also confirms the effectiveness of the ICL strategy. Meanwhile, the use of in-batch examples, where training may involve zero examples, has preserved the zero-shot capability of the model. There is modest decrease of 0.16 points compared to the model trained without ICL. Notably, in few-shot scenarios, the performance of the model employing in-batch examples rises to 66.08 (1.25), indicating robust improvement. Furthermore, when compared with the model utilizing fixed examples, the model trained with in-batch examples displays 8 superior performance in tasks that diverge significantly from the training data, such as classification and clustering tasks. Retr. Rerank. Clust. PairClass. Class. 15 12 11 3 4 STS Summ. Avg."
        },
        {
            "title": "4.4 ATTENTION",
            "content": "Task # of datasets w/o in-context learning w/ in-context learning (zero-shot) w/ in-context learning (few-shot) w/o in-context learning causal attention & last token pooling 59.11 59.59 60.08 87.99 87.87 88.51 causal attention & mean pooling 82.14 57.02 56.85 56. 42.60 42.61 46.55 53.74 36.82 58.50 bidirectional attention & last token pooling w/o in-context learning w/ in-context learning (zero-shot) w/ in-context learning (few-shot) 59.59 59.77 60.23 56.96 58.09 57.81 44.34 44.04 44.45 87.61 87.87 88.64 w/o in-context learning w/ in-context learning (zero-shot) w/ in-context learning (few-shot) 59.13 59.53 59. 57.03 57.48 57.29 43.44 43.88 44.93 87.25 88.12 88.36 bidirectional attention & mean pooling 76.27 75.47 77.31 83.93 83.30 83. 30.50 29.52 30.68 64.83 64.67 66.08 72.37 77.62 29.10 61. 74.77 75.35 77.00 75.03 74.86 75.26 83.81 83.97 83.77 84.08 83.64 83.75 30.12 29.75 29.99 29.17 29.58 29. 64.96 65.19 65.74 64.73 64.90 65.18 Table 5: Results of different attention and pooling mechanisms on the MTEB Benchmark. Recent studies have explored modifying causal attention in LLMs to adopt bidirectional attention and employ mean pooling for embedding generation. Notably, models such as GritLM (Muennighoff et al., 2024), NV-Embed (Lee et al., 2024a), and LLM2Vec (BehnamGhader et al., 2024) have utilized these techniques with considerable experimental success. Motivated by these advancements, we explore the potential benefits of implementing bidirectional attention in the ICL scenario. Specifically, we investigate the impacts of various attention and pooling mechanisms, including causal and bidirectional attention, coupled with last token pooling and mean pooling. In causal attention framework, each token is limited to accessing only preceding tokens information and not the subsequent ones. Consequently, employing mean pooling tends to yield suboptimal results because of this restriction. We find that the model could not be trained effectively under the ICL setting. Therefore, only results from experiments without ICL are presented in this specific configuration. Table 5 presents our experimental setup and results in both non-ICL and ICL scenarios. It demonstrates that in non-ICL scenarios, most methods yield consistent performance, aside from the combination of causal attention with mean pooling. In contrast, within ICL scenarios, the integration of causal attention and last token pooling emerges as the superior approach. This configuration appears to resonate with the competencies fostered during the initial training phase of the model, suggesting strong alignment with the foundational strategies employed during pre-training. Moreover, shifting from causal attention to bidirectional attention does not result in significant improvements, and mean pooling is not necessary for the implementation of bidirectional attention. Additionally, configurations utilizing bidirectional attention paired with last token pooling are notably effective, excelling in both non-ICL and zero-shot scenarios. This configurations performance is also pronounced in few-shot reranking tasks, highlighting its adaptability and potential applicability across various demands."
        },
        {
            "title": "4.5 PROMPT",
            "content": "Recently, most LLM-based embedding models have incorporated instruction-based prompts on the query side. However, there has been limited investigation into the efficacy of utilizing prompts on the passage side. To address this gap, our study introduces and explores the use of prompts on the passage side. The specific prompt employed in our study is as follows: {passage} Summarize the above passage: (6) Table 6 presents the results obtained using passage prompts. The results demonstrate that the integration of passage prompts leads to significant decline in performance across all tasks except 9 Task # of datasets w/o passage prompt (zero-shot) w/o passage prompt (few-shot) w/ passage prompt (zero-shot) w/ passage prompt (few-shot) Retr. Rerank. Clust. PairClass. Class. 15 59.59 60.08 59.50 59. 12 75.47 77.31 71.41 72.00 11 42.61 46.55 39.57 39.40 4 56.85 56.67 46.84 46.39 3 87.87 88.51 81.25 82.25 STS 10 83.30 83.69 80.38 79.81 Summ. Avg. 56 64.67 66.08 61.61 61. 1 29.52 30.68 30.26 30.97 Table 6: Comparative results of different prompts on the MTEB benchmark. retrieval. This indicates that further exploration and experimentation are needed when employing prompts at the passage level."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we explore the utilization of in-context learning (ICL) derived from large language models (LLMs) for generating text embeddings and investigate various methods of LLMs as embedding models. Specifically, we examine the integration of attention mechanisms, different pooling methods, and passage prompts. We advocate for maintaining the models original architecture while embedding in-context learning capabilities into the dense retrieval process. Our approach necessitates no modifications to the models architecture; instead, it involves altering the prompt on the query side to include in-context learning features in the embedding generation task. Despite its simplicity, our method proves highly effective on the MTEB and AIR-Bench benchmarks."
        },
        {
            "title": "REFERENCES",
            "content": "C.J. Adams, Daniel Borkan, Jeffrey Sorensen, Lucas Dixon, Lucy Vasserman, and Nithum Thain. Jigsaw unintended bias in toxicity classification, 2019. URL https://kaggle.com/ competitions/jigsaw-unintended-bias-in-toxicity-classification. Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. Semeval-2012 task 6: pilot on semantic textual similarity. in* sem 2012: The first joint conference on lexical and computational semanticsvolume 1: Proceedings of the main conference and the shared task, and volume 2: Proceedings of the sixth international workshop on semantic evaluation (semeval 2012). Association for Computational Linguistics. URL http://www. aclweb. org/anthology/S12-1051, 2012. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961, 2024. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Inigo Casanueva, Tadas Temˇcinas, Daniela Gerz, Matthew Henderson, and Ivan Vulic. Efficient intent detection with dual sentence encoders. arXiv preprint arXiv:2003.04807, 2020. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216, 2024. Xi Chen, Ali Zeynali, Chico Camargo, Fabian Flock, Devin Gaffney, Przemyslaw Grabowicz, Scott Hale, David Jurgens, and Mattia Samory. Semeval-2022 task 8: Multilingual news article similarity. 2022. Mathieu Ciancone, Imene Kerboua, Marion Schaeffer, and Wissam Siblini. Mteb-french: Resources for french sentence embedding evaluation and analysis. arXiv preprint arXiv:2405.20468, 2024. 10 Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel Weld. Specter: Document-level representation learning using citation-informed transformers. arXiv preprint arXiv:2004.07180, 2020. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 84408451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https://aclanthology.org/ 2020.acl-main.747. DataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, and tomtung. Quora question pairs, 2017. URL https://kaggle.com/competitions/quora-question-pairs. Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay 11 Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vıtor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5: Long form question answering. arXiv preprint arXiv:1907.09190, 2019. Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020. Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821, 2021. 12 Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, arXiv preprint and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. Ken Lang. Newsweeder: Learning to filter netnews. In Machine learning proceedings 1995, pp. 331339. Elsevier, 1995. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024a. Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, et al. Gecko: Versatile text embeddings distilled from large language models. arXiv preprint arXiv:2403.20327, 2024b. Chaofan Li, Zheng Liu, Shitao Xiao, Yingxia Shao, and Defu Lian. Llama2vec: Unsupervised In Proceedings of the 62nd Annual adaptation of large language models for dense retrieval. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3490 3500, 2024. Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. Mtop: comprehensive multilingual task-oriented semantic parsing benchmark. arXiv preprint arXiv:2008.09335, 2020. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023. Xueqing Liu, Chi Wang, Yue Leng, and ChengXiang Zhai. Linkso: dataset for learning to retrieve similar question answer pairs on software development forums. In Proceedings of the 4th ACM SIGSOFT International Workshop on NLP for Software Engineering, pp. 25, 2018. Yinhan Liu. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Dingkun Long, Qiong Gao, Kuan Zou, Guangwei Xu, Pengjun Xie, Ruijie Guo, Jian Xu, Guanjun Jiang, Luxi Xing, and Ping Yang. Multi-cpr: multi domain chinese dataset for passage retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 22, pp. 30463056, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450387323. doi: 10.1145/3477495.3531736. URL https: //doi.org/10.1145/3477495.3531736. Wenhao Lu, Jian Jiao, and Ruofei Zhang. Twinbert: Distilling knowledge to twin-structured compressed bert models for large-scale retrieval. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pp. 26452652, 2020. Kun Luo, Minghao Qin, Zheng Liu, Shitao Xiao, Jun Zhao, and Kang Liu. Large language models as foundations for next-gen dense retrieval: comprehensive empirical assessment. arXiv preprint arXiv:2408.12194, 2024. Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. arXiv preprint arXiv:2310.08319, 2023. 13 Andrew Maas, Raymond Daly, Peter Pham, Dan Huang, Andrew Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pp. 142150, 2011. Wei Chen Maggie, Phil Culliton. Tweet sentiment extraction, 2020. URL https://kaggle. com/competitions/tweet-sentiment-extraction. Macedo Maia, Siegfried Handschuh, Andre Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www18 open challenge: financial opinion mining and question answering. In Companion proceedings of the the web conference 2018, pp. 19411942, 2018. Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. In Proceedings of the 7th ACM conference on Recommender systems, pp. 165172, 2013. Sepideh Mollanorozy, Marc Tanti, and Malvina Nissim. Cross-lingual transfer learning with {P}ersian. In Lisa Beinborn, Koustava Goswami, Saliha Murado uglu, Alexey Sorokin, Ritesh Kumar, Andreas Shcherbakov, Edoardo M. Ponti, Ryan Cotterell, and Ekaterina Vylomova (eds.), Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP, pp. 8995, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.sigtyp-1.9. URL https://aclanthology.org/2023.sigtyp-1.9. Niklas Muennighoff, Nouamane Tazi, Loıc Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022. Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning. arXiv preprint arXiv:2402.09906, 2024. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: human-generated machine reading comprehension dataset. 2016. James ONeill, Polina Rozenshtein, Ryuichi Kiryo, Motoko Kubota, and Danushka Bollegala. wish would have loved this one, but didnta multilingual dataset for counterfactual detection in product reviews. arXiv preprint arXiv:2104.06893, 2021. Rafał Poswiata, Sławomir Dadas, and Michał Perełkiewicz. Pl-mteb: Polish massive text embedding benchmark. arXiv preprint arXiv:2405.10138, 2024. Yifu Qiu, Hongyu Li, Yingqi Qu, Ying Chen, Qiaoqiao She, Jing Liu, Hua Wu, and Haifeng Wang. Dureader retrieval: large-scale chinese benchmark for passage retrieval from web search engine. arXiv preprint arXiv:2203.10232, 2022. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. Carer: Contextualized affect representations for emotion recognition. In Proceedings of the 2018 conference on empirical methods in natural language processing, pp. 36873697, 2018. Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text embeddings. arXiv preprint arXiv:2212.09741, 2022. Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary Siegel, Michael Tang, et al. Bright: realistic and challenging benchmark for reasoning-intensive retrieval. arXiv preprint arXiv:2407.12883, 2024. 14 Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Nandan Thakur, Nils Reimers, Andreas Ruckle, Abhishek Srivastava, and Iryna Gurevych. Beir: heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Henning Wachsmuth, Shahbaz Syed, and Benno Stein. Retrieval of the best counterargument withIn Proceedings of the 56th Annual Meeting of the Association for out prior topic knowledge. Computational Linguistics (Volume 1: Long Papers), pp. 241251, 2018. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, and Luca Soldaini. Followir: Evaluating and teaching information retrieval models to follow instructions. arXiv preprint arXiv:2403.15246, 2024. Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. Retromae: Pre-training retrieval-oriented language models via masked auto-encoder. arXiv preprint arXiv:2205.12035, 2022. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C-pack: Packed resources for general chinese embeddings. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 641649, 2024. Xiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Yiqun Liu, et al. T2ranking: large-scale chinese benchmark for passage ranking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 26812690, 2023. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. Li Yudong, Zhang Yuqing, Zhao Zhe, Shen Linlin, Liu Weijie, Mao Weiquan, and Zhang Hui. Csl: large-scale chinese scientific literature dataset. arXiv preprint arXiv:2209.05034, 2022. Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. mgte: Generalized long-context text representation and reranking models for multilingual text retrieval, 2024. Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. Mr. tydi: multi-lingual benchmark for dense retrieval. arXiv preprint arXiv:2108.08787, 2021. Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. Miracl: multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11:11141131, 2023. Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, and Guido Zuccon. Promptreps: Prompting large language models to generate dense and sparse representations for zero-shot document retrieval. arXiv preprint arXiv:2404.18424, 2024."
        },
        {
            "title": "A INSTRUCTION",
            "content": "Task Name ArguAna ClimateFEVER CQADupStack DBPedia FEVER FiQA2018 HotpotQA MSMARCO NFCorpus Natural Question QuoraRetrieval SCIDOCS SciFact Touche2020 TREC-COVID STS* SummEval AmazonCounterfactualClassification AmazonPolarityClassification AmazonReviewsClassification Banking77Classification EmotionClassification ImdbClassification Instruction Template Given claim, find documents that refute the claim. Given claim about climate change, retrieve documents that support or refute the claim. Given question, retrieve detailed question descriptions from Stackexchange that are duplicates to the given question. Given query, retrieve relevant entity descriptions from DBPedia. Given claim, retrieve documents that support or refute the claim. Given financial question, retrieve user replies that best answer the question. Given multi-hop question, retrieve documents that can help answer the question. Given web search query, retrieve relevant passages that answer the query. Given question, retrieve relevant documents that best answer the question. Given question, retrieve Wikipedia passages that answer the question. Given question, retrieve questions that are semantically equivalent to the given question. Given scientific paper title, retrieve paper abstracts that are cited by the given paper. Given scientific claim, retrieve documents that support or refute the claim. Given question, retrieve detailed and persuasive arguments that answer the question. Given query, retrieve documents that answer the query. Retrieve semantically similar text. Given news summary, retrieve other semantically similar summaries. Classify given Amazon customer review text as either counterfactual or not-counterfactual. Classify Amazon reviews into positive or negative sentiment. Classify the given Amazon review into its appropriate rating category. Given online banking query, find the corresponding intents. Classify the emotion expressed in the given Twitter message into one of the six emotions: anger, fear, joy, love, sadness, and surprise. Classify the sentiment expressed in the given movie review text from the IMDB dataset. Given user utterance as query, find the user intents. Given user utterance as query, find the user scenarios. Classify the intent domain of the given utterance in task-oriented conversation. Classify the intent of the given utterance in task-oriented conversation. Classify the given comments as either toxic or not toxic. ArxivClusteringP2P MassiveIntentClassification MassiveScenarioClassification MTOPDomainClassification MTOPIntentClassification ToxicConversationsClassification TweetSentimentExtractionClassification Classify the sentiment of given tweet as either positive, negative, or neutral. Identify the main and secondary category of Arxiv papers based on the titles and abstracts. Identify the main and secondary category of Arxiv papers based on the titles. Identify the main category of Biorxiv papers based on the titles and abstracts. Identify the main category of Biorxiv papers based on the titles. Identify the main category of Medrxiv papers based on the titles and abstracts. Identify the main category of Medrxiv papers based on the titles. Identify the topic or theme of Reddit posts based on the titles. Identify the topic or theme of Reddit posts based on the titles and posts. Identify the topic or theme of StackExchange posts based on the titles. Identify the topic or theme of StackExchange posts based on the given paragraphs. Identify the topic or theme of the given news articles. Retrieve duplicate questions from AskUbuntu forum. Retrieve relevant news articles based on user browsing history. Given title of scientific paper, retrieve the titles of other relevant papers. Retrieve duplicate questions from StackOverflow forum. Retrieve duplicate questions from Sprint forum. Retrieve tweets that are semantically similar to the given tweet. Retrieve tweets that are semantically similar to the given tweet. ArxivClusteringS2S BiorxivClusteringP2P BiorxivClusteringS2S MedrxivClusteringP2P MedrxivClusteringS2S RedditClustering RedditClusteringP2P StackExchangeClustering StackExchangeClusteringP2P TwentyNewsgroupsClustering AskUbuntuDupQuestions MindSmallReranking SciDocsRR StackOverflowDupQuestions SprintDuplicateQuestions TwitterSemEval2015 TwitterURLCorpus AIR-Bench Given question, retrieve passages that answer the question. Table 7: The instruction we used on the MTEB and AIR-Bench benchmarks."
        },
        {
            "title": "B DETAILED MTEB RESULTS",
            "content": "Dataset ArguAna ClimateFEVER CQADupStack DBPEDIA FEVER FiQA2018 HotpotQA MSMARCO NFCorpus Natural Question QuoraRetrieval SCIDOCS SciFact Touche2020 TREC-COVID BIOSSES SICK-R STS12 STS13 STS14 STS15 STS16 STS17 STS22 STSBenchmark SummEval SprintDuplicateQuestions TwitterSemEval2015 TwitterURLCorpus AmazonCounterfactual AmazonPolarity AmazonReviews Banking77 Emotion Imdb MassiveIntent MassiveScenario MTOPDomain MTOPIntent ToxicConversations TweetSentimentExtraction Arxiv-P2P Arxiv-S2S Biorxiv-P2P Biorxiv-S2S Medrxiv-P2P Medrxiv-S2S Reddit Reddit-P2P StackExchange StackExchange-P2P TwentyNewsgroups AskUbuntuDupQuestions MindSmallRerank SciDocsRR StackOverflowDupQuestions MTEB Average (56) NV-Em bed-v1 68.21 34.72 50.51 48.29 87.77 63.10 79.92 46.49 38.04 71.22 89.21 20.19 78.43 28.38 85.88 85.59 82.80 76.22 86.30 82.09 87.24 84.77 87.42 69.85 86.14 31.20 95.94 78.73 86.05 95.12 97.14 55.47 90.34 91.71 97.06 80.07 81.74 96.51 89.77 92.60 80.60 53.76 49.59 48.15 44.74 39.24 36.98 63.20 68.01 74.99 42.04 60.13 67.50 30.82 87.26 56.58 69.32 bge-multilin gual-gemma2 77.37 39.37 47.94 51.37 90.38 60.04 83.26 45.71 38.11 71.45 90.04 26.93 72.05 30.26 64.27 85.74 82.66 77.71 87.45 83.48 87.63 86.70 91.18 69.02 87.25 31.20 90.94 79.64 86.95 89.48 96.90 61.60 92.53 92.97 96.66 82.05 84.40 98.61 95.51 87.34 78.86 54.91 50.28 52.64 49.20 45.81 44.11 56.03 65.83 66.21 45.74 70.44 64.59 31.79 87.60 54.90 69.88 gte-Qwen27B-instruct 64.27 45.88 46.43 52.42 95.11 62.03 73.08 45.98 40.60 67.00 90.09 28.91 79.06 30.57 82.26 81.37 79.28 79.55 88.83 83.87 88.54 86.49 88.73 66.88 86.85 31.35 92.82 77.96 86.59 91.31 97.50 62.56 87.57 79.45 96.75 85.41 89.77 99.04 91.88 85.12 72.58 54.46 51.74 50.09 46.65 46.23 44.13 73.55 74.13 79.86 49.41 53.91 67.58 33.36 89.09 55.66 70.24 SFR-Embe dding-2 62.34 34.43 46.11 51.21 92.16 61.77 81.36 42.18 41.34 73.96 89.58 24.87 85.91 28.18 87.28 87.60 77.01 75.67 82.40 79.93 85.82 84.50 88.93 67.10 83.60 30.71 97.62 78.57 88.03 92.72 97.31 61.04 90.02 93.37 96.80 85.97 90.61 98.58 91.30 91.14 79.70 54.02 48.82 50.76 46.57 46.66 44.18 62.92 72.74 76.48 48.29 66.42 66.71 31.26 87.29 55.32 70. stella en 1.5B v5 65.27 46.11 47.75 52.28 94.83 60.48 76.67 45.22 42.00 71.80 90.03 26.64 80.09 29.94 85.98 83.11 82.89 80.09 89.68 85.07 89.39 87.15 91.35 68.10 88.23 31.49 96.04 80.58 87.58 92.87 97.16 59.36 89.79 84.29 96.66 85.83 90.20 99.01 92.78 88.76 74.84 55.44 50.66 50.68 46.87 46.87 44.65 72.86 75.27 80.29 49.57 61.43 67.33 33.05 89.20 55.25 71.19 Table 8: MTEB results with full data. 18 bge-en-icl (zero-shot) 82.76 45.35 47.23 50.42 91.96 58.77 84.98 46.72 40.69 73.85 91.02 25.25 78.33 29.67 78.11 86.35 83.87 77.73 85.98 82.34 87.35 86.54 91.25 68.08 87.92 30.75 95.06 78.54 87.19 92.88 96.86 61.28 91.42 93.31 96.91 82.26 83.92 97.99 93.56 93.16 79.90 54.42 49.17 52.32 48.38 46.13 44.20 71.20 72.17 81.29 45.53 68.51 64.80 30.60 86.90 56.32 71.24 bge-en-icl (few-shot) 83.08 45.43 47.31 51.63 92.83 59.67 85.14 46.79 41.85 73.88 90.95 25.26 79.09 30.48 79.08 86.47 83.87 78.14 86.59 82.83 87.77 87.04 91.25 70.07 88.42 30.77 97.23 79.34 87.84 93.15 96.98 61.46 91.49 93.36 96.91 82.93 85.60 98.42 94.00 93.17 79.93 54.44 49.33 53.05 48.38 45.86 44.33 72.33 72.72 81.32 46.05 68.98 65.15 30.60 86.96 56.71 71.67 Dataset ArguAna ClimateFEVER CQADupStack DBPEDIA FEVER FiQA2018 HotpotQA MSMARCO NFCorpus Natural Question QuoraRetrieval SCIDOCS SciFact Touche2020 TREC-COVID BIOSSES SICK-R STS12 STS13 STS14 STS15 STS16 STS17 STS22 STSBenchmark SummEval SprintDuplicateQuestions TwitterSemEval2015 TwitterURLCorpus AmazonCounterfactual AmazonPolarity AmazonReviews Banking77 Emotion Imdb MassiveIntent MassiveScenario MTOPDomain MTOPIntent ToxicConversations TweetSentimentExtraction Arxiv-P2P Arxiv-S2S Biorxiv-P2P Biorxiv-S2S Medrxiv-P2P Medrxiv-S2S Reddit Reddit-P2P StackExchange StackExchange-P2P TwentyNewsgroups AskUbuntuDupQuestions MindSmallRerank SciDocsRR StackOverflowDupQuestions MTEB Average (56) bge-en-icl (zero-shot) 55.81 45.17 46.03 50.79 91.96 58.49 84.34 46.52 40.16 73.56 90.79 20.56 78.10 33.64 77.89 86.80 83.83 77.80 84.90 82.53 88.33 86.14 91.65 63.79 87.27 29.52 94.79 81.53 87.30 80.78 88.57 47.55 87.57 54.29 81.14 78.54 79.27 95.57 85.32 63.58 63.47 47.22 42.87 33.17 35.00 28.74 28.10 53.83 64.40 57.50 34.21 43.65 63.71 27.90 84.31 51.48 64.67 bge-en-icl (few-shot) 55.41 45.14 46.46 51.14 92.42 58.15 84.68 46.56 40.96 74.01 90.89 20.87 79.65 34.93 79.95 87.49 83.69 78.39 85.62 82.62 88.52 86.44 91.79 64.83 87.52 30.68 96.09 82.04 87.39 83.36 92.69 49.85 88.70 54.24 84.96 79.24 82.00 96.61 88.19 64.68 63.16 48.97 45.35 38.37 37.05 30.24 31.45 59.14 65.51 68.61 36.01 51.40 62.96 27.90 84.24 51.56 66.08 Table 9: MTEB results with public data only."
        },
        {
            "title": "C MULTILINGUAL EMBEDDING MODEL",
            "content": "Considering that the LLM-based multilingual embedding models are still relatively scare, we further train LLM-based multilingual embedding model, bge-multilingual-gemma2, on diverse range of languages and tasks. It is noted that bge-multilingual-gemma2 is just our initial attempt, and we have not explored the in-context learning (ICL) capabilities of bge-multilingual-gemma2. The exploration of ICL capabilities in the multilingual embedding models is probably future research topic. However, in our experiment, the new multilingual embedding model has already achieved excellent performance on multiple embedding benchmarks, and especially led to new state-of-theart results on several multilingual benchmarks. C.1 SETUP LLM. XLM-RoBERTa (Conneau et al., 2020) demonstrated that the larger vocabulary size were beneficial for improving the multilingual capability of language models. Therefore, we employ Gemma-2-9b (Team et al., 2024) as the backbone for the new multilingual embedding model, considering that the vocabulary size of Gemma-2-9b is 256K, which is larger than the vocabulary size of other LLMs, such as Qwen2 (Yang et al., 2024) or Llama 3 (Dubey et al., 2024). In addition to MTEB (Muennighoff et al., 2022) and AIR-Bench 1, we also evaluate Dataset. the multilingual capability of bge-multilingual-gemma2 on MIRACL (Zhang et al., 2023), FRMTEB (Ciancone et al., 2024), PL-MTEB (Poswiata et al., 2024) and C-MTEB (Xiao et al., 2024). Training Data. For the Engilsh training data, we use most of the datasets used by bge-en-icl. For the Chinese training data, in addition to the datasets used by BGE-M3 (Chen et al., 2024), more datasets in retrieval, classification, and clustering tasks are included. For the multilingual training data, we still use the two multilingual datasets used by BGE-M3. The full training data used by bge-multilingual-gemma2 includes: English: The English datasets (refer to Section 4.1) used by bge-en-icl are employed, except for the MSMARCO document ranking dataset. Chinese: The Chinese datasets used by BGE-M3 (Chen et al., 2024) are employed. The retrieval datasets including the three domain-specific datasets in Multi-CPR (Long et al., 2022), the classification datasets including AmazonReviews-Classification (McAuley & Leskovec, 2013) and MultilingualSentiment-Classification (Mollanorozy et al., 2023), and the clustering datasets including CSL-Clustering-{S2S/P2P2} (Yudong et al., 2022) are addtionally employed. Multilingual: Two multilingual retrieval datasets including MIRACL (Zhang et al., 2023) and Mr.TyDi (Zhang et al., 2021) are employed. Training Detail. We fine-tune the Gemma-2-9b model using contrastive loss and conduct the process over single epoch. For efficient fine-tuning, we employ Low-Rank Adaptation (LoRA) (Hu et al., 2021), setting the LoRA rank to 64 and the LoRA alpha to 32, with learning rate of 1e-4. We use in-batch negatives only for retrieval tasks, where each dataset incorporates 7 hard negatives. For the retrieval tasks and the other tasks, we set the batch size to 512 and 256, respectively. We consistently use the same dataset throughout one step, and the maximum sequence length remains capped at 512 tokens. Meanwhile, we use bge-reranker as the teacher to distill our model in retrieval tasks. Evaluation. On the MTEB benchmark, the instructions used by bge-multilingual-gemma2 are consistent with the instructions used by bge-en-icl, which are shown in Table 7. The instructions used by bge-multilingual-gemma2 on the C-MTEB, PL-MTEB and FR-MTEB benchmarks are available in Table 16. On the MIRACL benchmark, we use the same instruction for all 18 languages: Given question, retrieve Wikipedia passages that answer the question.. On the AIR-Bench benchmark, for the sake of simplicity, we also use the same instruction for all datasets: Given question, retrieve passages that answer the question.. 1https://github.com/AIR-Bench/AIR-Bench 20 fi fr ja fa hi es ru en ko bn Avg. ar yo id Model 31.9 39.5 48.2 26.7 7.7 28.7 45.8 11.5 35.0 29.7 31.2 37.1 25.6 35.1 38.3 49.1 17.5 12.0 56.1 BM25 41.8 49.9 44.3 39.4 47.8 48.0 47.2 43.5 38.3 27.2 43.9 41.9 40.7 29.9 35.6 35.8 51.2 49.0 39.6 mDPR 43.1 52.5 50.1 36.4 41.8 21.5 60.2 31.4 28.6 39.2 42.4 48.3 39.1 56.0 52.8 51.7 41.0 40.8 41.5 mContriever 66.6 76.0 75.9 52.9 52.9 59.0 77.8 54.5 62.0 52.9 70.6 66.5 67.4 74.9 84.6 80.2 56.0 56.4 78.3 mE5large 63.4 73.3 70.3 57.3 52.2 52.1 74.7 55.2 52.1 52.7 66.8 61.8 67.7 68.4 73.9 74.0 54.0 54.1 79.7 E5mistral-7b 54.9 - OpenAI-3 69.2 78.4 80.0 56.9 56.1 60.9 78.6 58.3 59.5 56.1 72.8 69.9 70.1 78.7 86.2 82.6 62.7 56.7 81.8 BGE-M3 (Dense) mGTE-TRM (Dense) 62.1 71.4 72.7 54.1 51.4 51.2 73.5 53.9 51.6 50.3 65.8 62.7 63.2 69.9 83.0 74.0 60.8 49.7 58.3 bge-multilingual-gemma2 74.1 81.0 82.3 64.5 64.2 64.0 81.2 64.2 68.2 61.5 79.1 69.7 77.0 81.9 88.1 84.6 68.0 63.5 90.3 sw te zh de th - - - - - - - - - - - - - - - - - Table 10: Multi-lingual retrieval performance on the MIRACL (Zhang et al., 2023) dev set (measured by nDCG@10). fi fr fa hi es en bn Avg. ar yo id Model 67.3 78.7 90.0 63.6 25.4 68.1 81.2 50.2 73.8 71.8 73.6 70.1 56.4 69.9 73.3 87.5 55.1 42.8 80.1 BM25 79.0 84.1 81.9 76.8 86.4 89.8 78.8 91.5 77.6 57.3 82.5 73.7 79.7 61.6 76.2 67.8 94.4 89.8 71.5 mDPR 84.9 92.5 92.1 79.7 84.1 65.4 95.3 82.4 64.6 80.2 87.8 87.5 85.0 91.1 96.1 93.6 90.3 84.1 77.0 mContriever 94.1 97.3 98.2 87.6 89.1 92.9 98.1 90.6 93.9 87.9 97.1 93.4 95.5 96.7 99.2 98.9 93.3 90.7 93.1 mE5large 92.7 96.0 96.0 90.2 87.5 88.0 96.7 92.8 89.9 88.4 95.1 89.4 95.0 95.5 95.1 96.5 90.1 88.7 97.9 E5mistral-7b 95.5 97.6 98.7 90.7 91.1 94.0 97.9 93.8 94.4 90.5 97.5 95.5 95.9 97.2 99.4 99.1 96.9 90.9 98.7 BGE-M3 (Dense) bge-multilingual-gemma2 97.2 99.0 98.9 95.4 94.5 95.0 98.5 96.2 96.5 95.3 98.9 95.4 98.0 98.0 99.7 99.6 97.2 94.1 100.0 sw te ko zh de ru th ja Table 11: Multi-lingual retrieval performance on the MIRACL (Zhang et al., 2023) dev set (measured by Recall@100). C.2 MAIN RESULTS MIRACL. Following BGE-M3 (Chen et al., 2024), we evaluate the multilingual retrieval performance with MIRACL (Zhang et al., 2023). We cite most of the results reported in the Table 1 of BGE-M3s paper. It should be noted that the results of BM25 are lower than the results reported in MIRACLs paper, as the BM25 tested in BGE-M3s paper used the same tokenizer with BGE-M3. We also include another recent work mGTE (Zhang et al., 2024) as one of the baseline models. As shown in Table 10, our model bge-multilingual-gemma2 achieves the state-of-the-art (SOTA) performance in all 18 languages. The overall performance of bge-multilingual-gemma2 is 74.1, which is far ahead of the performance of the previous best model BGE-M3 (Dense), indicating the excellent multilingual retrieval capability of bge-multilingual-gemma2. The results of Recall@100 are available in Table 11. FR-MTEB & PL-MTEB & C-MTEB. We further evaluate our model on FR-MTEB (Ciancone et al., 2024), PL-MTEB (Poswiata et al., 2024) and C-MTEB (Xiao et al., 2024) benchmarks. FRMTEB consists of 26 datasets in 6 different tasks, PL-MTEB consists of 26 datasets in 5 different tasks, and C-MTEB consists of 35 datasets in 6 different tasks. We use the API provided by MTEB 2 to perform evaluation. The results shown in Table 12, Table 13 and Table 14 are all available in the MTEB leaderboard 3. We can observe that bge-multilingual-gemma2 leads to new SOTA performances on both FR-MTEB and PL-MTEB benchmarks, and especially achieves very excellent results in the retrieval tasks (Retr.). On the C-MTEB benchmark, bge-multilingual-gemma2 surpasses most of the baseline models, such as e5-mistral-7b-instruct, bge-large-zh-v1.5, etc. However, its overall performance on C-MTEB benchmark is still slightly worse than gte-Qwen2-7B-instruct, which can be attributed to Gemma-2s Chinese proficiency being worse than that of Qwen2. MTEB. The evaluation results of bge-multilingual-gemma2 on the MTEB benchmark are available in Table 1. The detailed results for each dataset are available in Table 8. We can also observe that bge-multilingual-gemma2 achieves good performance on the MTEB benchmark. 2https://github.com/embeddings-benchmark/mteb 3https://huggingface.co/spaces/mteb/leaderboard 21 Task # of datasets mistral-embed gte-multilingual-base voyage-multilingual-2 gte-Qwen2-1.5B-instruct gte-Qwen2-7B-instruct bge-multilingual-gemma Retr. 5 46.81 52.97 54.56 52.56 55.65 63.47 Rerank. Clust. PairClass. Class. 2 80.46 76.47 82.59 83.76 78.7 85.22 7 44.74 41.66 46.57 55.01 55.56 56.48 2 77.32 79.46 78.66 86.88 90.43 85. 6 68.61 68.72 68.56 78.02 81.76 81.62 STS 3 79.56 81.36 80.13 81.26 82.31 82.59 Summ. Avg. 26 59.41 59.79 61.65 66.6 68.25 70.08 1 31.47 29.74 29.96 30.5 31.45 31.26 Table 12: Results on the FR-MTEB (Ciancone et al., 2024) benchmark (26 datasets in the French subset). Please refer to Table 17 for the scores of bge-multilingual-gemma2 per dataset. Task # of datasets gte-multilingual-base multilingual-e5-large mmlw-roberta-large gte-Qwen2-1.5B-instruct gte-Qwen2-7B-instruct bge-multilingual-gemma Retr. 11 46.40 48.98 52.71 51.88 54.69 59.41 Clust. 1 33.67 33.88 31.16 44.59 51.36 50.29 PairClass. Class. 4 85.45 85.50 89.13 84.87 88.48 89.62 7 60.15 63.82 66.39 72.29 77.84 77.99 STS 3 68.92 66.91 70.59 68.12 70.86 70. Avg. 26 58.22 60.08 63.23 64.04 67.86 70.00 Table 13: Results on the PL-MTEB (Poswiata et al., 2024) benchmark (26 datasets in the Polish subset). Please refer to Table 17 for the scores of bge-multilingual-gemma2 per dataset. Task # of datasets multilingual-e5-large e5-mistral-7b-instruct gte-multilingual-base bge-large-zh-v1.5 gte-Qwen2-1.5B-instruct gte-Qwen2-7B-instruct bge-multilingual-gemma2 Retr. 8 63.66 61.75 71.95 70.46 71.86 76.03 73.73 Rerank. Clust. PairClass. Class. 4 56.00 61.86 68.17 65.84 68.21 68.92 68.28 4 48.23 52.30 47.48 48.99 54.61 66.06 59.3 2 69.89 72.19 78.34 81.6 86.91 87.48 86.67 9 67.34 70.17 64.27 69.13 71.12 75.09 74.11 STS 8 48.29 50.22 52.73 56.25 60.96 65.33 56.87 Avg. 35 58.81 60.81 62.72 64.53 67.65 72.05 68. Table 14: Results on the C-MTEB (Xiao et al., 2024) benchmark (35 datasets in the Chinese subset). Please refer to Table 17 for the scores of bge-multilingual-gemma2 per dataset. news healthcare 2 wiki web Domain # of datasets 2 61.42 48.86 44.40 bge-m3 (Dense) 57.16 42.91 41.61 multilingual-e5-large 58.82 45.18 42.08 e5-mistral-7b-instruct 55.04 42.95 37.30 gte-Qwen2-1.5B-instruct 64.95 51.59 48.55 gte-Qwen2-7B-instruct bge-multilingual-gemma2 65.50 51.81 47.46 2 45.74 42.18 46.06 44.50 46.51 44.68 law finance 1 26.68 19.66 19.32 11.95 22.31 22.58 2 41.85 37.38 40.45 40.24 42.42 40.45 arxiv msmarco Avg. 13 1 46.65 54.40 42.58 54.44 45.26 59.03 41.06 49.74 48.38 58.39 63.14 46.83 1 40.76 36.93 44.78 32.06 40.27 23.28 Table 15: QA (en & zh, nDCG@10) performance on AIR-Bench 24.04. AIR-Bench. For the QA task in AIR-Bench, we perform evaluation on all of the 13 datasets in 24.04 version, which consists of 8 English datasets and 5 Chinese datasets. For the Long-Doc task in AIR-Bench, we perform evaluation on all of the 15 datasets in 24.04 version, which are all English datasets. As shown in Table 15 and Table 3, bge-multilingual-gemma2 also achieves excellent performance in the out-of-distribution (OOD) evaluation on AIR-Bench, which indicates that our model has excellent generalization ability. 22 Task Name C-MTEB CLSClusteringS2S CLSClusteringP2P ThuNewsClusteringS2S ThuNewsClusteringP2P T2Reranking MMarcoReranking CMedQAv1 CMedQAv2 Ocnli Cmnli T2Retrieval MMarcoRetrieval DuRetrieval CovidRetrieval CmedqaRetrieval EcomRetrieval MedicalRetrieval VideoRetrieval PL-MTEB CBD PolEmo2.0-IN PolEmo2.0-OUT AllegroReviews PAC 8TagsClustering SICK-E-PL PPC CDSC-E PSC ArguAna-PL DBPedia-PL FiQA-PL HotpotQA-PL MSMARCO-PL NFCorpus-PL NQ-PL Quora-PL SCIDOCS-PL SciFact-PL Touche2020 TRECCOVID-PL FR-MTEB Instruction Template Identify the main category of scholar papers based on the titles. Identify the main category of scholar papers based on the titles and abstracts. Identify the topic or theme of the given news articles based on the titles. Identify the topic or theme of the given news articles based on the titles and contents. Given Chinese search query, retrieve web passages that answer the question. Given Chinese search query, retrieve web passages that answer the question. Given Chinese community medical question, retrieve replies that best answer the question. Given Chinese community medical question, retrieve replies that best answer the question. Retrieve semantically similar text. Retrieve semantically similar text. Given Chinese search query, retrieve web passages that answer the question. Given web search query, retrieve relevant passages that answer the query. Given Chinese search query, retrieve web passages that answer the question. Given question on COVID-19, retrieve news articles that answer the question. Given Chinese community medical question, retrieve replies that best answer the question. Given user query from an e-commerce website, retrieve description sentences of relevant products. Given medical question, retrieve user replies that best answer the question. Given video search query, retrieve the titles of relevant videos. Classify the sentiment of polish tweet reviews. Classify the sentiment of in-domain (medicine and hotels) online reviews. Classify the sentiment of out-of-domain (products and school) online reviews. Classify the sentiment of reviews from e-commerce marketplace Allegro. Classify the sentence into one of the two types: BEZPIECZNE POSTANOWIENIE UMOWNE and KLAUZULA ABUZYWNA. Identify of headlines from social media posts in Polish into 8 categories: film, history, food, medicine, motorization, work, sport and technology. Retrieve semantically similar text. Retrieve semantically similar text. Retrieve semantically similar text. Retrieve semantically similar text. Given claim, find documents that refute the claim. Given query, retrieve relevant entity descriptions from DBPedia. Given financial question, retrieve user replies that best answer the question. Given multi-hop question, retrieve documents that can help answer the question. Given web search query, retrieve relevant passages that answer the query. Given question, retrieve relevant documents that best answer the question. Given question, retrieve Wikipedia passages that answer the question. Given question, retrieve questions that are semantically equivalent to the given question. Given scientific paper title, retrieve paper abstracts that are cited by the given paper. Given scientific claim, retrieve documents that support or refute the claim. Given question, retrieve detailed and persuasive arguments that answer the question. Given query, retrieve documents that answer the query. MasakhaNEWSClassification AlloProfClusteringP2P AlloProfClusteringS2S HALClusteringS2S MasakhaNEWSClusteringP2P MasakhaNEWSClusteringS2S MLSUMClusteringP2P MLSUMClusteringS2S AlloprofReranking OpusparcusPC PawsXPairClassification SyntecReranking AlloprofRetrieval BSARDRetrieval SyntecRetrieval XPQARetrieval MintakaRetrieval Classify the given news article into one of the seven topic categories: politics, sports, health, business, entertainment, technology, and religion. Identify the main category of Allo Prof document based on the titles and descriptions. Identify the main category of Allo Prof document based on the titles. Identify the main category of academic passage based on the titles and contents. Identify the topic or theme of the given news articles based on the titles and contents. Identify the topic or theme of the given news articles based on the titles. Identify the topic or theme of the given articles based on the titles and contents. Identify the topic or theme of the given articles based on the titles. Given question, retrieve passages that answer the question. Retrieve semantically similar text. Retrieve semantically similar text. Given question, retrieve passages that answer the question. Given question, retrieve passages that answer the question. Given question, retrieve passages that answer the question. Given question, retrieve passages that answer the question. Given question, retrieve passages that answer the question. Given question, retrieve passages that answer the question. Table 16: The additional instruction we used on the C-MTEB, PL-MTEB and FR-MTEB benchmarks. These instructions are adopted from gte-Qwen2-7B-instruct (Li et al., 2023). To ensure sentence completeness, we add period at the end. 23 Dataset Result Dataset Result FR-MTEB AlloprofRetrieval BSARDRetrieval MintakaRetrieval (fr) SyntecRetrieval XPQARetrieval (fr) AlloprofReranking SyntecReranking AlloProfClusteringP2P AlloProfClusteringS2S HALClusteringS2S MLSUMClusteringP2P (fr) MLSUMClusteringS2S (fr) MasakhaNEWSClusteringP2P (fra) MasakhaNEWSClusteringS2S (fra) OpusparcusPC (fr) PawsXPairClassification (fr) AmazonReviewsClassification (fr) MasakhaNEWSClassification (fra) MassiveIntentClassification (fr) MassiveScenarioClassification (fr) MTOPDomainClassification (fr) MTOPIntentClassification (fr) STS22 (fr) STSBenchmarkMultilingualSTS (fr) SICKFr SummEvalFr FR-MTEB Average (23) C-MTEB CmedqaRetrieval CovidRetrieval DuRetrieval EcomRetrieval MedicalRetrieval MMarcoRetrieval T2Retrieval VideoRetrieval CMedQAv1 CMedQAv2 MMarcoReranking T2Reranking CLSClusteringP2P CLSClusteringS2S ThuNewsClusteringP2P ThuNewsClusteringS2S Cmnli Ocnli 58.50 28.52 62.53 90.37 77.42 78.62 91.83 71.20 59.64 28.19 47.75 47.46 73.86 67.24 100.00 70.14 55.19 82.49 79.60 82.18 97.20 93.07 83.28 85.09 79.39 31.26 70.08 42.21 77.46 90.46 69.3 62.02 84.7 86.26 77.4 84.62 85.60 35.43 67.48 54.65 63.68 64.32 54.57 90.13 83.21 PL-MTEB ArguAna-PL DBPedia-PL FiQA-PL HotpotQA-PL MSMARCO-PL NFCorpus-PL NQ-PL Quora-PL SCIDOCS-PL SciFact-PL TRECCOVID-PL 8TagsClustering CDSC-E PPC PSC SICK-E-PL AllegroReviews CBD MassiveIntentClassification (pl) MassiveScenarioClassification (pl) PAC PolEmo2.0-IN PolEmo2.0-OUT CDSC-R SICK-R-PL STS22 (pl) PL-MTEB Average (23) AmazonReviewsClassification (zh) IFlyTek JDReview MassiveIntentClassification (zh-CN) MassiveScenarioClassification (zh-CN) MultilingualSentiment OnlineShopping TNews Waimai AFQMC ATEC BQ LCQMC PAWSX QBQTC STS22 (zh) STSB C-MTEB Average (35) 59.71 43.19 46.12 77.03 72.69 36.72 56.85 84.47 19.53 74.43 82.75 50.29 78.23 95.43 99.24 85.58 65.00 84.13 79.41 81.93 67.24 90.42 77.77 90.97 78.16 42.79 70. 54.34 49.94 88.91 78.19 82.58 78.91 94.59 50.26 89.26 47.17 50.75 62.02 75.95 30.57 38.98 68.68 80.87 68.44 Table 17: Results of bge-multilingual-gemma2 for each dataset in the FR-MTEB, PL-MTEB and C-MTEB benchmarks. 24 LIGHTWEIGHT RE-RANKER We have also introduced lightweight version of the reranker, which incorporates both depth and width compression techniques. Specifically, depth compression is implemented on layerwise method, allowing for the selective adjustment of the number of layers according to the desired output. Regarding width compression, it is configured to execute token compression at predetermined layers, whereby tokens are merged into single token. For the input template, we use the following format: A: {query} B: {passage} {prompt} (7) where the prompt inquires about the relationship between and B, e.g., Predict whether passage contains an answer to query A. And we use the logits of Yes as our reranking score. Considering the depth compression generates output scores at each layer, we extract the linear layer connected to the logits for the Yes prediction in the language model head. This extracted linear layer is then appended to each layer, allowing every layer to compute reranking score. D.1 SETUP LLM. Our objective is to develop multilingual version of the lightweight reranker. Considering the extensive vocabulary necessitated by multilingual support, we employ Gemma-2-9b (Team et al., 2024) as the backbone for our reranker. Dataset. We evaluate the performance of our reranker bge-reranker-v2.5-gemma-lightweight on BEIR (Thakur et al., 2021) and MIRACL (Zhang et al., 2023). The BEIR benchmark encompasses variety of text retrieval tasks across multiple domains, while MIRACL serves as significant dataset for multilingual evaluation, featuring 18 distinct languages. Training Data. To enhance the multilingual capabilities and retrieval performance of the Reranker, we utilize the BGE-M3 dataset (Chen et al., 2024), along with Arguana, HotpotQA, and FEVER, for the training process. Training Detail. The reranker is trained using contrastive loss. Furthermore, LoRA is employed for fine-tuning, where the LoRA rank is set to 64 and the LoRA alpha is set to 32, accompanied by learning rate of 1e-4. During the training process, batch size of 128 is utilized, and 15 hard negatives are assigned to each query. At the same time, the training of the reranker employs self-distillation, wherein the final layer serves as the teacher for preceding layers. Throughout this training process, KL divergence loss is utilized. During training, we randomly select width compression strategy and train all depth compression strategies. Regarding depth compression, we support outputs from 8 to 42 layers. Regarding width compression, we support compression ratios of 1, 2, 4, and 8, and support width compression at 8, 16, 24, 32, and 40 layers. During the training process, we utilized four types of prompts: query to passage, query to query, passage to passage, and argument to counter-argument. The specific application of these prompts was dependent on the type of dataset used, as shown in Table 18. Evaluation. On the BEIR benchmark, we rerank the top-100 retrieval results of bge-large-en-v1.5 and E5-mistral-7b-instruct. On the MIRACL dataset, we rerank the top-100 retrieval results of bgem3 (dense). The instructions for evaluation are shown in Table 19. Task Type Instruction Template query to passage query to query passage to passage argument to counter-argument Predict whether argument and counterargument express contradictory opinions. Predict whether passage contains an answer to query A. Predict whether queries and are asking the same thing. Predict whether passages and have the same meaning. Table 18: The training instructions we used for reranker."
        },
        {
            "title": "Instruction Template",
            "content": "Predict whether argument and counterargument express contradictory opinions. ArguAna Predict whether passage contains an answer to query A. ClimateFEVER Predict whether queries and are asking the same thing. CQADupstack Predict whether passage contains an answer to query A. DBPedia Predict whether passage contains an answer to query A. FEVER Predict whether passage contains an answer to query A. FiQA2018 Predict whether passage contains an answer to query A. HotpotQA Predict whether passage contains an answer to query A. MSMARCO NFCorpus Predict whether passage contains an answer to query A. Natural Question Predict whether passage contains an answer to query A. Predict whether queries and are asking the same thing. QuoraRetrieval Predict whether passage contains an answer to query A. SCIDOCS Predict whether passage contains an answer to query A. SciFact Predict whether passage contains an answer to query A. Touche2020 Predict whether passage contains an answer to query A. TREC-COVID"
        },
        {
            "title": "MIRACL",
            "content": "Predict whether passage contains an answer to query A. Table 19: The instructions we used for the BEIR benchmark and MIRACL dataset for reranker."
        },
        {
            "title": "BEIR",
            "content": "Save Flops ArguAna ClimateFEVER CQA DBPedia FEVER FiQA2018 HotpotQA MSMARCO NFCorpus NQ QuoraRetrieval SCIDOCS SciFact Touche2020 TRECCOVID Mean bge-largeen-v1.5 - 63.54 36.49 42.23 44.16 87.17 44.97 74.11 42.48 38.12 55.04 89.06 22.62 74.64 25.08 74.89 54.31 bge-rerank er-v2-m3 - 37.70 37.99 38.24 48.15 90.15 49.32 84.51 47.79 34.85 69.37 89.13 18.25 73.08 35.68 83.39 55.36 jina-reranker-v2base-multilingual - 52.23 34.65 40.21 49.31 92.44 45.88 81.81 47.83 37.73 67.35 87.81 20.21 76.93 32.45 80.89 56.52 bge-rerankerv2-gemma - 78.68 39.07 45.85 49.92 90.15 49.32 86.15 48.07 39.73 72.60 90.37 21.65 77.22 35.68 85.51 60.71 bge-reranker-v2.5gemma-lightweight 60% 86.04 48.41 49.18 51.98 94.71 60.48 87.84 47.23 41.40 75.37 91.25 23.71 80.5 30.64 84.26 63.10 bge-reranker-v2.5gemma-lightweight 0 86.16 48.48 48.9 52.11 94.69 60.95 87.89 47.26 41.64 75.58 91.18 23.87 80.38 31.09 84.85 63.67 Table 20: The performance of various rerankers on BEIR benchmark (based on bge-large-en-v1.5). D.2 MAIN RESULTS BEIR. We rerank the retrieval results from the BEIR dataset using two models, bge-large-en-v1.5 and E5-Mistral-7b-Instruct, and we rerank top-100 retrieval results from these models. We conduct both comprehensive evaluation and lightweight evaluation. In the lightweight evaluation, we select compression ratio of 2, width compression factor of 8, and depth of the output layer set to 25. This configuration results in 60% FLOPs. It indicates that bgeTables 20 and 21 present the evaluation results for the BEIR benchmark. reranker-v2.5-gemma2-lightweight records exceptional performance in enhancing both bge-largeen-v1.5 and E5-Mistral-7b-Instruct retrieval outcomes. Furthermore, there exists positive correlation between the initial retrieval quality and the subsequent reranking performance, when reranking the retrieval results from E5-Mistral-Instruct, our reranker achieves improved performance. Addi26 BEIR Save Flops ArguAna ClimateFEVER CQA DBPedia FEVER FiQA2018 HotpotQA MSMARCO NFCorpus NQ QuoraRetrieval SCIDOCS SciFact Touche2020 TRECCOVID Mean E5-mistral7b-instruct - 61.80 38.37 42.97 48.84 87.82 56.58 75.72 43.06 38.58 63.56 89.59 16.30 76.26 26.24 87.07 56.85 bge-rerankerv2-gemma - 79.05 37.66 46.16 50.77 91.36 50.96 86.99 48.35 39.25 73.44 90.44 20.77 77.78 35.79 88.13 61.13 bge-reranker-v2.5gemma-lightweight 60% 86.02 47.27 49.06 52.45 94.85 58.81 88.49 47.65 42.28 75.00 91.09 22.20 79.94 28.69 86.61 63.36 bge-reranker-v2.5gemma-lightweight 0 86.58 47.13 49.53 52.87 95.19 61.19 88.82 47.40 42.17 76.28 91.18 22.69 80.98 31.17 87.36 64.04 Table 21: The performance of various rerankers on BEIR benchmark (based on E5-mistral-7binsturct). tionally, the implementation of the lightweight model variant results in only marginal decline in performance while achieving significant reduction in FLOPs."
        },
        {
            "title": "Language",
            "content": "bge-m3 (Dense) bge-rerankerv2-gemma FLOPS ar bn en es fa fi fr hi id ja ko ru sw te th zh de yo Mean (18) - 78.4 80.0 56.9 56.1 60.9 78.6 58.3 59.5 56.1 72.8 69.9 70.1 78.7 86.2 82.6 62.7 56.7 81.8 69.2 - 73.4 81.9 58.9 58.6 60.5 77.2 56.1 62.7 59.6 72.7 74.0 67.1 78.1 85.8 81.2 63.0 58.2 84.2 69.6 bge-rerank er-v2-m3 - 81.7 84.6 63.5 64.4 65.7 82.4 63.7 68.5 62.7 80.0 73.8 76.9 82.3 89.4 85.3 65.2 62.7 87.4 74. bge-rerankerv2-gemma - 82.3 85.0 66.6 65.3 65.5 82.6 65.4 69.4 61.2 79.7 75.1 78.3 81.8 89.6 86.1 66.8 64.0 85.9 75.0 bge-reranker-v2.5gemma-lightweight 60% 82.5 87.8 68.6 67.6 67.5 82.8 68.5 71.4 63.8 82.8 75.9 79.8 84.8 90.8 88.1 69.9 65.8 89.6 77.1 bge-reranker-v2.5gemma-lightweight 0 82.8 87.6 69.3 67.8 67.4 83.3 68.5 71.3 63.8 83.6 75.7 80.1 85.1 90.8 88.7 69.9 65.6 89.8 77.3 Table 22: Comparison of MIRACL dev nDCG@10 scores across various rerankers (based on bgem3 (Dense)). MIRACL. We further evaluate the multilingual capabilities of the reranker using the MIRACL dataset, with the results presented in Table 22. The reranking is conducted based on the top 100 retrieval results obtained from the bge-m3 (dense) model. The reranker demonstrates significant improvement in retrieval accuracy across each dataset and outperforms other multilingual rerankers. Notably, compared to monolingual (English) retrieval, the multilingual retrieval experienced minimal negative effects from model lightweighting, essentially maintaining the original performance of the model."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Beijing University of Posts and Telecommunications",
        "Chinese Academy of Sciences",
        "University of Science and Technology of China"
    ]
}