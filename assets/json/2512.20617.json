{
    "paper_title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
    "authors": [
        "Yuxi Xiao",
        "Longfei Li",
        "Shen Yan",
        "Xinhang Liu",
        "Sida Peng",
        "Yunchao Wei",
        "Xiaowei Zhou",
        "Bingyi Kang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 7 1 6 0 2 . 2 1 5 2 : r SpatialTree: How Spatial Abilities Branch Out in MLLMs Yuxi Xiao,,, Longfei Li,,, Shen Yan, Xinhang Liu, Sida Peng, Yunchao Wei, Xiaowei Zhou, Bingyi Kang, Zhejiang University, ByteDance Seed, Beijing Jiaotong University Project Lead, Equal Contribution"
        },
        {
            "title": "Abstract",
            "content": "Cognitive science suggests that spatial ability develops progressivelyfrom perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on narrow set of tasks. We introduce SpatialTree, cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover surprising transfer dynamicnegative transfer within L1, but strong cross-level transfer from lowto high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naïve RL that encourages extensive thinking is unreliable: it helps complex reasoning but hurts intuitive perception. We propose simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs. Date: December 24, 2025 Correspondence: Bingyi Kang Project Page: https://spatialtree.github.io/"
        },
        {
            "title": "Introduction",
            "content": "Spatial abilities refer to the capacity to perceive, understand, reason about, and interact with 2D and 3D space, long-standing topic in cognitive science [13, 45, 48]. In multimodal large language models (MLLMs), these abilities form the cornerstone of Spatial Intelligence (SI), yet remain challenging to study systematically due to their inherent complexity and broad scope [31, 63]. Recent research followed task-centric trajectory. Early efforts concentrated on spatial tasks within single images [9, 34, 50], such as relative object positioning and size estimation. As capabilities matured, studies expanded into the 3D domain, tackling grounding, detection, and captioning from point clouds [15, 57, 67]. More recently, the advent of multi-view and video-capable models has further diversified the landscape [10, 20, 51, 52, 60, 64], covering broad spectrum from spatial relevant reasoning to egocentric and dynamic object 1Work done during Yuxi Xiao and Longfei Lis internship at Bytedance Seed. 1 Figure 1 SpatialTree. Inspired by cognitive science, our proposed SpatialTree organizes spatial intelligence into four-layer hierarchy (L1-L4). Rooted in foundational multi-modal capabilities (L0), the tree progressively branches from Basic perception (L1) to agentic competence (L4). understanding. However, these task-centric benchmarks remain fragmented and often treat spatial capabilities as isolated or overlapping skills. This lack of unification makes it difficult to unravel the inherent structure and cross-level dependencies within the proliferation of spatial tasks. Can we move beyond disjointed, task-centric evaluations to uncover compact set of atomic capabilities that reveal how spatial abilities emerge, interact, and transfer? Drawing inspiration from cognitive science insight that \"intelligence is dynamic structure built through successive stages\" [40], we advocate paradigm shift: moving from fragmented tasks to capability-centric framework. Specifically, we structure Spatial Abilities as four level capability tree  (Fig. 1)  : L1 Perception: This level focuses on native perception of space, capturing raw geometric and physical attributes such as size, distance, and motion, without relying on language or symbolic reasoning. L2 Mental Mapping: This level maps spatial perception to language, grounding spatial concepts in linguistic semantics and forming language-structured spatial memory. L3 Mental Simulation: This level supports internal reasoning about space, enabling mental simulation, including causal reasoning about dynamics, relational and geometric problem solving, and sequential planning for actions and navigation. L4 Spatial Agent: This level executes actions in space, integrating perception, language, and reasoning to interact with the environment, interpret feedback, and complete long-horizon spatial tasks. 2 To populate this taxonomy, we first reorganize and unify data from prior works [20, 28, 30, 50, 51, 58, 60, 64, 65, 67], which primarily cover specific sub-domains within L1 to L3. To address missing abilities and enrich annotations beyond existing datasets, we construct Spatial Engine that integrates multiple expert models [27, 32, 36, 49, 5456, 61]. For L1 to L3, we enhance data diversity by introducing multiple QA formats for the same underlying problem and by incorporating additional sub-capabilities, including orientation estimation, localization, and affordance understanding. In contrast, L4 requires agentic interactions that are largely absent from prior benchmarks. We therefore meticulously curate L4 data across three representative embodiments: character navigation, robot grippers, and human hands. To facilitate MLLM adaptation to agentic tasks, we design an action mapping strategy that discretizes low-level actions into high-level motion primitives, forming an executable action space. In addition, humanobject interaction sequences are reformatted into multi-step multiple-choice tasks through manual annotation. Based on the resulting datasets and task formulations, we establish SpatialTree-bench to evaluate broad range of mainstream open-source and commercial MLLMs. Our evaluation on SpatialTree-Bench uncovers clear hierarchical dependency: while foundational abilities (L1) function independently, higher-level capabilities exhibit strong reliance on these basics. To systematically validate this structure, we move beyond static evaluation to targeted training interventions using both Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). First, our SFT experiments reveal that spatial skills do not scale uniformly. We observe that while basic abilities struggle with intra-level interference, they serve as critical stepping stones for higher-level tasks, unlocking powerful multi-ability synergy when jointly trained. Second, we explore the role of reasoning via RL, leading to surprising dichotomy between thinking and perceiving. We find that while extensive reasoning is indispensable for complex tasks, it can be detrimental to intuitive perception (e.g., numerical estimation), where over-thinking degrades precision. This necessitates auto-think strategy: suppressing reasoning for immediate perception while encouraging it for complex planning. In summary, our key contributions are: We construct SpatialTree, the first capability-centric benchmark that organizes spatial intelligence into rigorous hierarchy, enabling granular diagnosis of model capabilities. We reveal the structural transfer dynamics of spatial skills, demonstrating how foundational perception acts as prerequisite for agentic competence through systematic SFT experiments. We identify the trade-off between reasoning and perception in RL training, proposing differentiated inference strategy that balances intuitive sensing with complex reasoning."
        },
        {
            "title": "2 Related Work",
            "content": "Spatial Cognitive Modeling. Understanding spatial cognition has long been central goal in cognitive science and AI. common insight from classical theories is that spatial abilities are hierarchical, ranging from basic perception and sensorimotor interactions to higher-level reasoning and planning. Piaget [40] highlighted the developmental progression of such abilities, Tolman [48] introduced the idea of cognitive maps to represent environments for flexible navigation, and Kuipers [23, 24] formalized hierarchical spatial representation linking local perception to global knowledge. More recent symbolic and neural approaches [37, 45] extend these insights to computational models of spatial representation, memory, and reasoning. These studies collectively motivate our SpatialTree, which organizes spatial intelligence into multi-level capabilities, bridging classical theory with systematic computational evaluation. Multi-modal Large Language Models. The success of GPT-3 [6] and GPT-3.5 [38] demonstrated the potential of large language models for complex linguistic understanding and reasoning. GPT-4V [39] extends GPT-4 [1] with visual inputs, enabling single-image understanding and basic spatial reasoning. Open-sourced models such as LLaVA [29] and QwenVL [2] gradually added multi-image and video capabilities, supporting spatiotemporal reasoning. Reasoning-augmented LLMs, pioneered by OpenAI O1 [19] and DeepSeek-R1 [11], integrate chain-of-thought and reinforcement learning to enhance high-level inference. Building on these advances, GPT4O [17] and Gemini 2.5 [8] combine perception and reasoning to support complex, agentic decision-making. Collectively, these milestones progressively enable hierarchical spatial intelligence in MLLMs, motivating 3 (a) (b) (c) Figure 2 Different Emphasis across Hierarchy Levels. Taking Relation in L1, L2, L3 as an example: (a) Relation in Perception, involving basic spatial relations (e.g., inside, outside) (b) Relation in Understanding, describing the attributes and mutual relationships among different objects. (c) Relation in Causal Reasoning, leveraging visual cues and logical inference to solve more complex relational tasks. structured benchmarks and evaluation frameworks across low-level perception, intermediate reasoning, and high-level agentic competence. Benchmarks for Spatial Intelligence in MLLMs. Benchmarks for spatial abilities in MLLMs have evolved alongside the models themselves. Early efforts, such as BLINK [9], SpatialEval [50], and 3DSR-Bench [34], focused on evaluating spatial understanding tasks in single images, including distance estimation, relational question answering, and spatial captions. As MLLMs increasingly support multi-frame and video inputs, benchmarks such as VSI-Bench [60] and MMSI-Bench [64] have emerged to evaluate spatial reasoning across multiple views and dynamic scenes. To further enrich task diversity and coverage, Omnispatial [20], SITE [52], and IR3D-Bench [30] extend benchmarks to geometry puzzles, dynamic reasoning, and inverse rendering tasks. Built upon prior efforts, our SpatialTree benchmark systematically organizes spatial abilities into hierarchical framework, providing the first thorough evaluation across different capabilities."
        },
        {
            "title": "3 The SpatialTree Taxonomy",
            "content": "In this section, we introduce SpatialTree taxonomy of spatial capabilities. Different levels of the taxonomy emphasize different aspects of spatial ability, as illustrated in Fig. 2. Lower levels focus on intuitive and basic perceptual skills, while higher levels progressively require richer knowledge and more complex logical reasoning. Concrete instantiations are discussed in Sec. 4."
        },
        {
            "title": "3.1 Perception\nWe begin with Perception (L1), which characterizes the most primitive capacity to sense visual signals from\nthe environment. Rooted in the sensorimotor stage of human development, this level represents the instinctive\nability to process spatial information prior to linguistic abstraction. Based on these fundamental biological\nneeds, we categorize perception into five core abilities:",
            "content": "Geometry interprets the physical form and metric properties of the world. Evolutionarily, this allows agents to intuitively decipher three core dimensions: Distance (gauging metric and relative depth), Size (estimating physical magnitude, area, and volume), and Shape (discerning contours, boundaries, and geometric primitives). 4 These faculties enable immediate judgments, such as deciding if fruit is small enough to hold or path is wide enough to traverse. Motion reflects the innate capacity to process dynamic visual signals over time. This perception is categorized into Egocentric (sensing self-motion and heading direction) and Allocentric (perceiving the movement and speed of external objects). Interpreting these signals allows humans to instantly determine if an entity is static or mobile, supporting tasks like avoiding obstacles or tracking targets. Orientation is rooted in the necessity to maintain balance and align with the environment. Humans instinctively sense up and down via vestibular cues to avoid falling, and visually recognize object poses to interact with them effectively. This category includes Gravity (sensing the vertical axis, e.g., knowing which way is up) and Object (perceiving poses, e.g., noticing cup is tilted). Relation captures the structural continuity and spatial arrangement of the visual world. Beyond isolating individual objects, intuitive perception registers how entities are spatially arranged and whether they correspond across views. This category includes Topology (perceiving basic spatial configurations such as inside, outside, or overlap) and Correspondence (recognizing the same object or landmark across different viewpoints or visual conditions). Localization anchors visual stimuli within 2D/3D space. It addresses the fundamental question of where, allowing humans to spot friend in crowd or locate keys on cluttered table. This category includes Detection (identifying object presence and spatial extent) and Grounding (associating visual observations with spatial positions or coordinates)."
        },
        {
            "title": "3.2 Mental Mapping\nMental Mapping (L2) marks a shift toward alignment with language. It encompasses two key aspects: mapping\nspatial primitives to semantic concepts, and constructing a language-aligned memory system. Accordingly, we\nclassify Mental Mapping into two main sub-abilities:",
            "content": "Understanding interprets the semantic meaning underlying geometric perception, shifting the focus from mere existence to function and identity. This capability begins by translating visual scenes into linguistic descriptions through Spatial Captioning, and by identifying semantic Relations (e.g., distinguishing riding from sitting on). It further elevates Motion from raw kinematic cues to semantic interpretation, recognizing purposeful actions rather than simple movement. Crucially, this level includes Perspective Takingthe ability to mentally align with alternative viewpointsand Affordance understanding, which identifies the functional possibilities of objects (e.g., recognizing that handle is graspable), thereby bridging perception with potential future interactions. Memory extends spatial awareness beyond the instantaneous field of view, enabling the system to retain and update spatial information over time. This capability relies on constructing Cognitive Map, which synthesizes fragmentary observations (e.g., video frames or multi-view images) into compact and unified global representation. Based on this mental model, the system performs Memory Retrieval to ground specific semantic events or moments, allowing it to recall where an object appeared or when specific action occurred, even if the target is currently occluded or out of sight."
        },
        {
            "title": "3.3 Mental Simulation",
            "content": "Reasoning and planning prior to action execution are essential components of MLLMs, aligning naturally with the Chain-of-Thought paradigm in language model reasoning. In spatial cognitive science, this process is commonly referred to as mental simulation. We taxonomize mental simulation into two orthogonal directions: causal reasoning and sequential planning. Causal Reasoning allows MLLMs to model spatial interactions, physical dynamics, and entity relationships within simulated mental space. It includes reasoning about object geometry (e.g., how shapes interlock in spatial puzzles), motion prediction (e.g., how an object traverses path), and analyzing semanticspatial relations (e.g., object is left of object B). By mentally simulating causeeffect chains in spatial scenarios, MLLMs establish the logical substrate for subsequent planning. 5 Sequential Planning converts causal insights into coherent, goal-directed action plans expressed in language. It entails designing high-level, step-by-step strategies (e.g., \"first move toward the door, then turn right, and finally interact with the handle\") and generating abstract routes that respect spatial logic (e.g., \"go around the table to reach the sofa\"). By chaining linguistic action primitives, MLLMs produce strategic plans that ensure the conceptual sequence aligns with the overarching goal before any low-level execution."
        },
        {
            "title": "3.4 Agentic Competence",
            "content": "Agentic Competence represents the culmination of spatial intelligence, bridging the gap between cognitive planning and practical execution. It focuses on the agents ability to translate internal plans into tangible interactions within dynamic environments. This capability is grounded in diverse embodied scenarios, where the model must interpret visual streams to generate precise action sequences. Key tasks include predicting control commands from video game frames, deriving manipulation sequences from robotic arm videos, and identifying potential movement directions or navigational affordances in real-world scenes. We begin from the ultimate objective of Spatial AI Agent an MLLM-driven system that integrates multi-modal observations, updates its memory, and selects actions to interact with the 3D world in an intuitive manner. Formally, the agent performs sequential decision-making by modeling: (St, At, Mt) Pθ (cid:16) (cid:12) (cid:12) Ot, Ht1 (cid:17) , where Ht1 = {(O0, A0, M0), . . . , (Ot1, At1, Mt1)} where Ot is the current multi-modal observation, St the internal latent state (e.g., goal, plan, or belief), At the chosen action, and Mt the updated memory representation. MLLMs are expected to output interactive actions executable across 3D environments and embodiments, such as games, simulators, and the physical world. Unlike Vision-Language Action Models (VLAs) decording the low-level control signals in robotics [18], MLLMs take the language as the only interface to link with environments like GUI Agents [41]."
        },
        {
            "title": "4 Instantiating the SpatialTree Benchmark",
            "content": "This section details how the SpatialTree taxonomy is instantiated as benchmark, covering data curation pipeline (seen in in Fig. 3), and more specific examples of the QAs are demonstrated in Sec. F."
        },
        {
            "title": "4.1 Data Curation\nData Annotation Framework. As shown in Fig. 3, the data engines are organized hierarchically. At the percep-\ntion level, we employ a set of expert perception models, including DepthAnything3 [61], SpatialTracker [55, 56],\nGeoCalib [49], and OrientAnything [54], to extract intermediate perceptual representations, such as depth,\ncorrespondences, tracking results, and gravity direction. These representations are then further processed\nusing our designed QA templates and rephrased by LLMs to construct the target QAs. For L2, we leverage\n3D reconstruction pipelines (Sec. C.2) to generate BEV maps from videos, which are subsequently captioned\nand transformed into QAs for cognitive mapping and memory retrieval with multimodal LLMs. For other\nunderstanding tasks, such as affordance, we build on partially annotated datasets [33] and augment them\nby randomly constructing multiple-choice candidates through various visual prompts and by reformulating\nquestions with more abstract descriptions. For L3, we build upon annotated reasoning-related QAs and\nintroduce structured thinking templates, enabling an LLM-based rephraser to augment the original QAs with\nexplicit Chain-of-Thought (CoT) reasoning. For agentic tasks, we curate Internet data covering manipulation\nand navigation across diverse embodiments (e.g., human hands, robotic grippers, and game characters). We\nfurther process the data with our Action-Extraction Pipeline (Sec. C.2), converting embodiment-specific\nactions into unified key–mouse action sequences suitable for MLLMs. In parallel, we introduce a human\nannotation pipeline to annotate videos with executable multi-option action sequences.",
            "content": "Data Resources. Our SpatialTree-Bench is constructed by systematically reorganizing numerous recent datasets (detailed in the Appendix) [20, 28, 34, 50, 51, 58, 60, 62, 64, 65, 67] to address their scattered capability coverage and over-reliance on simple questions. We first map each question to our SpatialTree framework and then enhance the evaluation protocol; for instance, complex reasoning tasks from CameraBench and MMSI-Bench are converted to hybrid multi-option + LLM-as-a-Judge format for finer-grained assessment. 6 Figure 3 Benchmark Data Engines. Level-specific engines process data and construct QAs. To fill the remaining capability gaps, we introduce SpatialPlus, new dataset targeting underrepresented abilities (e.g., L1 Orientation, L1 Shape, L2 Spatial Caption) with primary emphasis on L4 Agentic Competence. We leverage our proprietary SpatialEngine to automatically create annotations from diverse array of video sources, including 3D reconstruction datasets, in-game footage [21], egocentric manipulation videos [16], and robotics data [22]. More implementation details are discussed in Appendix."
        },
        {
            "title": "4.2 Evaluation Metrics Designs",
            "content": "To facilitate robust evaluation within our hierarchical benchmark, we design diverse metrics tailored to specific downstream tasks. As illustrated in Fig. 4, these metrics are primarily categorized into multiple-choice questions (70.7%), numeric accuracy (e.g., mean relative accuracy), LLM-based evaluation (LLM-as-a-Judge) and specific numeric metrics."
        },
        {
            "title": "5 A Hierarchical Analysis of Spatial Capabilities",
            "content": "7 Figure 4 Distribution of Benchmark data and Evaluation Metrics. We analyze the metric usage across 41 tasks in our benchmark. The evaluation relies primarily on multiple-choice questions (70.7%), complemented by task-specific numeric metrics (e.g., cognitive map accuracy) and LLM-as-a-Judge protocols."
        },
        {
            "title": "5.1 Models and Metrics\nBenchmarked Models. We categorize the evaluated MLLMs into three groups: (1) Thinking Models, i.e.,\nmodels augmented with explicit reasoning or chain-of-thought generation mechanisms (reasoning-augmented),\nincluding Gemini 2.5 Flash, Gemini 2.5 Pro [8], GLM-4.5V [14], and SeedVL1.6-Vision [12]; (2) Non-Thinking\nModels, which do not explicitly optimize for reasoning-style generation, such as Gemini 2.5-Pro-Nonthink,\nGemini 2.5-Flash-Nonthinking, and GPT-4o [17]; and (3) Open-Source Models, including Qwen2.5-VL [3],\nQwen3-VL [59], and Kimi-VL [47], representing recent community-driven multimodal advances. This diverse\nselection enables comprehensive comparisons across reasoning and non-reasoning paradigms, proprietary and\nopen-source ecosystems, and model scales ranging from 32B to 72B parameters—providing a holistic overview\nof the current MLLM landscape. A full list of evaluated models is shown in Tab. 1.",
            "content": "Evaluation Metrics. Our evaluation employs multi-faceted set of metrics tailored to the specific abilities at each level of the SpatialTree. For perception and understanding tasks (L1-L2), we primarily use accuracybased metrics, such as classification accuracy for object recognition, Mean Squared Error (MSE) for distance estimation, and angular difference for orientation tasks. For higher-level reasoning and planning tasks (L3-L4), we measure task success rates. In the case of agentic tasks (L4), we further analyze the quality of generated actions using metrics like positional error (L2 distance) and orientation error (angular difference) against ground-truth trajectories."
        },
        {
            "title": "5.2 Overall Performance",
            "content": "L1 Perception L2 Mental Mapping L3 Mental Simulation L4 Agentic Competence Methods Rank Avg. Geom. Motion [0.15] [0.40] Rel. [0.15] Local. Orient. Underst. Memory [0.20] [0.70] [0.30] [0.10] Non-Thinking Models GPT-4o Gemini2.5 Flash NT Gemini2.5 Pro NT Thinking Models Seed1.6-Vision GLM4.5V Gemini2.5-Pro Gemini2.5-Flash Open-source Models Qwen2.5VL-7B Qwen2.5VL-32B Qwen2.5VL-72B Qwen3VL-30B Qwen3VL-235B Kimi-VL-A3B 10 6 2 7 5 1 4 12 11 9 8 3 13 31.9 35.8 41.4 35.7 36.0 50.1 39. 27.5 27.9 33.0 35.3 40.0 24.4 23.9 31.6 36.2 34.0 35.3 47.8 33.5 17.8 21.4 24.4 31.9 33.9 13.8 38.6 29.3 30.0 32.0 24.0 32.6 23. 22.6 30.0 22.0 26.0 27.4 23.3 29.8 30.8 33.2 34.9 32.5 44.4 35.8 23.9 25.8 28.6 32.3 35.1 31.6 24.2 35.2 47.0 40.0 34.5 61.6 54. 20.6 22.0 37.5 20.7 35.4 27.0 36.2 45.4 48.5 44.4 43.7 47.9 42.0 31.6 35.1 37.3 39.2 38.9 21.4 31.2 36.4 43.3 34.5 34.5 50.5 42. 34.7 29.0 38.9 37.8 43.7 24.9 43.6 53.7 55.2 41.5 34.1 61.5 55.3 15.2 21.7 35.1 48.1 53.4 28.3 Caus. Reas. [0.65] Seq. Plan. Goal Exec. [0.35] [0.50] Open Expl. [0.50] 29.3 28.4 39.6 33.3 33.8 47.6 33.8 28.4 32.8 32.6 32.7 37.3 26. 40.5 36.9 47.5 39.2 41.0 58.2 45.3 39.8 37.0 38.4 44.2 44.7 27.8 25.8 27.6 29.2 30.1 26.8 28.3 27.1 24.5 14.1 23.9 25.8 28.8 15. 39.2 45.7 46.0 39.0 49.7 63.3 39.8 31.1 38.6 38.6 40.9 48.9 32.6 Table 1 Our-Bench. Dark gray indicates the best result among all models and light gray indicates the best result among open-source models. NT denotes the non-thinking model. Avg is computed using the weights in brackets []. 8 We first present the overall performance of all benchmarked models on our proposed SpatialTree-Bench, with detailed results summarized in Tab. 1. In our benchmark, Gemini2.5-Pro achieves the best results (50.1) and Qwen3VL-235B get 40.0 leads the benchmark among open-source models."
        },
        {
            "title": "6 Exploring Ability Dependencies and Hierarchical Transfer",
            "content": "To investigate the structure of spatial ability in MLLMs, we analyze dependencies among fine-grained sub-abilities using Pearson correlation coefficients computed from our benchmark scores. high positive correlation indicates that strong performance on one ability tends to accompany strong performance on another. Fig. 5 presents heatmap of these correlations across all models. Notably, higher-level capabilities (L3 and L4) exhibit stronger correlations (region A), suggesting that complex tasks such as route planning and causal reasoning rely on overlapping foundational sub-skills. In contrast, lower-level abilities (L1) show weak correlations, indicating they are largely independent. Based on this coarse correlation analysis, we select several underutilized low-level abilities for further investigation. These abilities are explored through Supervised Fine-Tuning (SFT) and explicit prompting to examine their influence and transfer, both within the same level and across higher levels."
        },
        {
            "title": "6.1 Probing Cross-Ability Transfer\nvia SFT",
            "content": "Figure 5 Inter-Capability Dependencies via Pearson Correlation. (A) Correlation matrix among higher-level capabilities (L3 and L4); (B) Correlation matrix among foundational L1 capabilities; (C) Salient low-level abilities influencing higher-level tasks. L1 Perception L2 Mental Mapping L3 Mental Simulation L4 Agentic Competence Methods Baseline B+Dist. B+Corr. B+Size B+Dist.+Size+Corr. B+Dist.+Size+Corr.+Mot. Baseline+75@(all spat.) Avg. 25.0 24.5 25.2 23.5 26.1 27.3 23.6 Geom. 20.9 24.1 (+3.2) 17.6 (-3.2) 24.3 (+3.4) 25.5 (+4.6) 28.6 (+7.7) 24.9 (+4.0) Motion 28.6 26.6 (2.0) 23.9 (4.7) 22.6 (6.0) 29.3 (0.7) 24.6 (4.0) 22.6 (6.0) Rel. 28.9 23.2 (5.8) 30.2 (+1.3) 21.4 (7.5) 29.4 (0.5) 20.6 (8.3) 25.9 (3.0) Local. 24.2 19.6 (4.6) 18.9 (5.3) 21.7 (2.5) 16.4 (7.8) 26.3 (+2.1) 17.4 (6.8) Orient. 34.2 34.3 (0.1) 35.6 (+1.4) 34.5 (0.3) 33.7 (0.5) 36.0 (+1.8) 31.2 (3.0) Underst. 22.6 24.6 (+2.0) 21.9 (-0.7) 21.9 (-0.8) 23.0 (+0.4) 22.2 (-0.4) 22.2 (-0.4) Memory 21.7 21.8 (0.1) 24.6 (+2.9) 19.2 (2.5) 24.2 (+2.5) 22.6 (0.9) 20.6 (1.1) Caus. Reas. 27.2 26.1 (-1.1) 21.8 (5.4) 23.4 (3.8) 25.2 (2.0) 28.2 (+1.0) 25.7 (1.5) Seq. Plan. 31.7 30.8 (-0.9) 33.9 (+2.2) 30.3 (1.5) 34.2 (+2.5) 32.8 (+1.1) 30.2 (1.5) Goal Exec. 22.1 25.5 (+3.4) 24.7 (+2.6) 21.5 (0.6) 26.0 (+3.9) 23.3 (+1.1) 19.7 (2.4) Open Expl. 26.5 26.1 (-0.4) 35.9 (+9.4) 24.3 (-2.2) 28.5 (+2.0) 35.9 (+9.4) 22.8 (3.7) Table 2 SFT Comparisons. \"B+Dist.\", \"B+Corr.\", and \"B+Size\" denote the baseline augmented with distance, correspondence, and size tuning data, respectively. Changes are color-coded as notable gains, neutral influence, and drops. Finding 1 Cross-Ability Transfer: Single-ability L1 SFT induces cross-level transfer, while yielding limited or slightly negative effects on same-level abilities. Based on naive Pearson correlation analysis, we manually select three L1 abilities that exhibit the strongest correlations with higher-level performance: Geometry Distance (L1-Geo.Dist), Geometry Size (L1-Geo.Size), and Relative Correlation (L1-Relat.Corr ). General Data Mixture. To construct the general visual-instruction data, we follow the VST [63] data mixing recipe and combine multimodal datasets from LLaVA-Video [66], LLaVA-NeXT-Interleave [26], and LLaVAOneVision [25], covering single-image, multi-image, video, and 3D tasks. We then use SpatialEngine to Figure 6 Demonstration of Capability Transfer after Distance SFT. (Top) The model is trained on distance QAs, such as object depth sorting and comparison, just using data from synthetic and indoor scenes. (Middle) This learned capability transfers in zero-shot manner to complex reasoning tasks in unseen, in-the-wild scenes, achieving 36.0% performance gain over the baseline. (Bottom) Furthermore, the skill exhibits cross-level transfer, enabling the model to perform robotic arm manipulation task with 27.1% performance gain. generate ability-specific instruction data, mixed with the general data in 1:3 ratio specifically for each. To isolate the gains from general data, we use baseline fine-tuned only on the general data with the same token consumption. Targeted SFT Data. For L1-Geo.Dist, we generate approximately 0.25M distance-relevant QA samples from SUNRGBD [46], Hypersim [43], and Matterport3D [7]. The training data is further augmented with visual prompts and multi-scale transformations to enhance distance reasoning. For L1-Relat.Corr, we generate matching data following VST [63], sampling 0.25M examples. Similarly, for L1-Geo.Size, we generate 0.25M samples from 3D bounding-box annotated datasets, including SUNRGBD, Hypersim and ArkitScenes [5]. Results and Analysis. As shown in Tab. 6, single-ability SFT on distance, correspondence, or size generally yields negligible gains or even substantial drops in other abilities at the same level. Specifically, B+Dist. increases Geom. abilities by +3.2, while decreasing Motion, Rel., and Local. by -2.0, -5.8, and -4.6, respectively. However, it provides non-trivial gains in higher-level abilities, notably Underst. (+2.0) and Goal Exec. (+3.4). To give further exploration on how this cability transfer happens and why, we provide qualitative examples in Fig. 6. After being fine-tuned on distance-QA data, B+Dist. can generalize to much more complex distance-related questions in in-the-wild scenarios, including those with novel coordinate prompts and multiple points queried simultaneously. This indicates that the model has learned an awareness of distance rather than overfitting to specific QA templates. Besides, and more intriguingly, the improved distance ability also shows clear cross-level transfer. It benefits higher-level tasks such as robot-arm manipulation, where MLLMs are 10 required to guide the gripper to move, rotate, and open/close in 3D space. better sense of metric space helps the model generate more reasonable control decisions in the real world. Finding 2 Multi-ability Synergy: The holistic integration across multiple fundamental abilities achieves synergistic gains far exceeding their individual effects. Tab. 2 reveals an interesting phenomenon: individual SFT on any single abilityDistance, Size, or Correspondencehas limited impact on overall spatial performance, and can even slightly reduce it (e.g., B+Dist. -0.5, B+Corr. +0.2, B+Size. -1.5 relative to the baseline). In contrast, combining all three abilities in blended SFT (B+Dist.+Size+Corr.) yields an overall gain of +1.1, surpassing the performance of any individual ability and even exceeding the sum of their separate contributions. Remarkably, for abilities that suffered substantial drops under single-ability SFTsuch as L1.Motion (best individual change -2.0)the compositional training produces positive improvement of +0.7."
        },
        {
            "title": "6.2 Reinforcement Learning",
            "content": "Methods Qwen2.5-VL-7B Full RL@think After SFT SFT Baseline L1 RL@think L2 RL@think L3 RL@think L4 RL@think Full RL@think Full RL@auto-think L1 Perception L2 Mental Mapping L3 Mental Simulation L4 Agentic Competence Avg. 27.5 28. Geom. 17.8 19.0 (+1.2) Motion 22.6 25.2 (+2.7) Rel. 23.9 23.6 (-0.4) Local. 20.6 20.3 (-0.3) Orient. 21.6 31.5 (-0.1) Underst. 34.7 30.6 (4.1) Memory 15.2 13.1 (2.1) Caus. Reas. 28.4 33.6 (+5.2) Seq. Plan. 39.8 26.8 (13.0) Goal Exec. 24.5 29.7 (+5.2) Open Expl. 31.1 40.0 (+9.0) 27.3 26.6 (-0.7) 26.7 (-0.5) 27.7 (0.4) 28.5 (+1.2) 30.1 (+2.9) 30.8 (+3.6) 28.6 28.4 (0.9) 24.4 (4.1) 16.2 (12.3) 23.8 (4.8) 29.7 (+1.1) 31.9 (+3.3) 24.6 30.0 (+5.4) 22.7 (1.9) 24.0 (-0.6) 25.3 (0.7) 24.7 (0.1) 28.6 (+4.0) 20.6 30.5 (+9.9) 22.3 (+1.6) 24.1 (+3.5) 22.1 (+1.4) 27.2 (+6.5) 22.0 (+1.3) 26.3 19.5 (6.8) 17.0 (9.4) 21.4 (4.9) 23.5 (2.8) 21.0 (5.3) 23.1 (3.2) 36.0 34.5 (1.4) 35.0 (-0.9) 38.5 (+2.5) 33.9 (2.1) 34.8 (1.2) 36.8 (0.8) 22.2 24.9 (+2.8) 26.6 (+4.5) 26.0 (+3.9) 25.1 (+3.0) 27.4 (+5.2) 28.0 (+5.8) 22.6 18.5 (4.1) 16.1 (6.5) 21.8 (-0.8) 20.5 (2.2) 16.7 (5.9) 22.6 (-0.1) 28.2 25.7 (2.4) 29.0 (0.8) 31.3 (+3.1) 32.0 (+3.8) 33.6 (+5.5) 33.5 (+5.4) 32.8 34.0 (+1.1) 31.4 (1.4) 34.7 (+1.9) 34.2 (+1.4) 37.6 (+4.8) 35.6 (+2.8) 23.3 24.1 (0.8) 25.6 (+2.3) 26.5 (+3.2) 27.1 (+3.9) 25.4 (+2.1) 23.4 (0.1) 35.9 29.6 (6.3) 34.5 (1.4) 38.4 (+2.5) 38.8 (+2.9) 41.7 (+5.8) 44.1 (+8.3) Table 3 RLVR Comparisons. The table compares the baseline Qwen2.5-VL-7B with the version enhanced by RL on Goal-Exec. tasks. Changes are color-coded as notable gains, neutral influence, and drops. Building on the findings from Supervised Fine-Tuning (SFT), we further investigate the potential of Reinforcement Learning with Verifiable Rewards (RLVR) to scale spatial abilities. Specifically, we employ Group Relative Policy Optimization (GRPO) [44] to align the models policy with the hierarchical nature of the SpatialTree. Our experiments reveal that uniform RL strategy is insufficient, conversely hierarchy-aware reward mechanism unlocks significant performance gains. Limitations of Naive RL on Low-Level Skills. We initially conducted experiments using standard GRPO on individual capabilities selected from each level of the SpatialTree. The results exposed critical limitations of applying naive RL to spatial tasks, particularly regarding generalization in low-level skills. When optimizing exclusively for single low-level abilities, the model tends to overfit to the specific reward signal. This results in siloed improvements that fail to generalize to other foundational skills and, more critically, provide negligible or even detrimental transfer to high-level capabilities. Ineffectiveness of Uniform Data Mixing. Attempts to mitigate this by combining datasets from multiple levels or mixing all available data for unified RL stage yielded only marginal gains. The model struggled to balance the diverse requirements of the benchmark, suggesting that \"one-size-fits-all\" reinforcement strategy cannot effectively span the spectrum from atomic perception to complex agentic planning. Hierarchy-Aware Reward Mechanism. These limitations led us to hypothesize that different levels of spatial intelligence require distinct cognitive modes during training. While high-level reasoning benefits from extensive test-time computation, low-level perception is inherently intuitive and should function as \"fast\" system. To test this, we introduced Hierarchy-Aware Reward mechanism that adjusts the training objective based on the capability level: For Intuitive Perception: For tasks such as depth estimation, object counting, and orientation, we removed rewards for thinking processes and introduced length penalty. This implicitly discourages the model from over-reasoning on direct visual signals, forcing it to rely on direct visual-text alignment. For Complex Reasoning: For nested tasks like navigation planning and causal reasoning, we retained and amplified rewards for explicit reasoning steps, encouraging the model to utilize more tokens for intermediate computation. This hierarchy-aware strategy proved highly effective. As shown in Table 3, the model trained with adaptive rewards significantly outperforms both the baseline and the naive GRPO variants across the entire SpatialTreeBench. This finding strongly validates the structure of our taxonomy: Spatial Intelligence is not flat collection of tasks, but structured hierarchy where foundational perception requires direct alignment, while higher-order competence demands deliberate reasoning. Strict Evaluation Setup. It is important to note two critical factors in our experimental design that ensure the robustness of our findings: Data Decontamination: The robotic arm data samples used for GRPO training are strictly separated from the SpatialTree-Bench testing data. There is no overlap in specific scenes or object configurations between the training and evaluation sets. Task and Metric Discrepancy: The training objective is purely maximizing the reward on discrete MCQ selection. In contrast, the SpatialTree-Bench evaluation employs diverse set of continuous and semantic metrics (e.g., Mean Squared Error for distance, angular error for orientation, and execution success rates for agentic tasks). Results and Observations. Despite the significant domain gap and the difference in task formulation, the GRPOtuned Qwen2.5-VL-7B demonstrates notable improvements across multiple levels of the SpatialTree hierarchy compared to its base counterpart. This suggests that the model is not merely memorizing dataset-specific patterns, but is effectively internalizing generalized spatial reasoning policies through the reinforcement learning process. These preliminary findings highlight the potential of RLVR as scalable pathway for advancing spatial intelligence in MLLMs."
        },
        {
            "title": "7 Conclusion and Future works",
            "content": "We present SpatialTree, the first capability-centric framework for Spatial Intelligence, organizing abilities into four hierarchical layers. This structure enables analysis of how spatial abilities emerge, compose, and transfer across levels. It also opens opportunities to efficiently scale up spatial intelligence in MLLMs, by strategically leveraging different types of data: identifying which abilities are most effective for pre-training, which can be directly applied in reinforcement learning with minimal additional reasoning data during post-training, and which are acquired through real-world interactions. We believe this could provide promising path toward advancing spatial intelligence in MLLMs."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Philip J. Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleksander Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, Frank Perbet, Guy Scully, Jeremy Shar, Stephen Spencer, Omer Tov, Ruben Villegas, Emma Wang, Jessica Yung, Cip Baetu, Jordi Berbel, David Bridson, Jake Bruce, Gavin Buttimore, Sarah Chakera, Bilva Chandra, Paul Collins, Alex Cullum, Bogdan Damoc, Vibha Dasagi, Maxime Gazeau, Charles Gbadamosi, Woohyun Han, Ed Hirst, Ashyana Kachra, Lucie Kerley, Kristian Kjems, Eva Knoepfel, Vika Koriakin, Jessica Lo, Cong Lu, Zeb Mehring, Alex Moufarek, Henna Nandwani, Valeria Oliveira, Fabio Pardo, Jane Park, Andrew Pierson, Ben Poole, Helen Ran, Tim Salimans, Manuel Sanchez, Igor Saprykin, Amy Shen, Sailesh Sidhwani, Duncan Smith, Joe Stanton, Hamish Tomlinson, Dimple Vijaykumar, Luyu Wang, Piers Wingfield, Nat Wong, Keyang Xu, Christopher Yew, Nick Young, Vadim Zubov, Douglas Eck, Dumitru Erhan, Koray Kavukcuoglu, Demis Hassabis, Zoubin Gharamani, Raia Hadsell, Aäron van den Oord, Inbar Mosseri, Adrian Bolton, Satinder Singh, and Tim Rocktäschel. Genie 3: new frontier for world models. 2025. [5] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 2020. [7] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. International Conference on 3D Vision (3DV), 2017. [8] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [9] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. [10] Mohsen Gholami, Ahmad Rezaei, Zhou Weimin, Yong Zhang, and Mohammad Akbari. Spatial reasoning with vision-language models in ego-centric multi-view scenes. arXiv preprint arXiv:2509.06266, 2025. [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [12] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [13] Mary Hegarty. Components of spatial intelligence. In Psychology of learning and motivation, volume 52, pages 265297. Elsevier, 2010. [14] Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pages arXiv2507, 2025. [15] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36: 2048220494, 2023. 13 [16] Ryan Hoque, Peide Huang, David J. Yoon, Mouli Sivapurapu, and Jian Zhang. Egodex: Learning dexterous manipulation from large-scale egocentric video, 2025. URL https://arxiv.org/abs/2505.11709. [17] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [18] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. π_{0.5}: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. [19] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [20] Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and Li Yi. Omnispatial: Towards comprehensive spatial reasoning benchmark for vision language models. arXiv preprint arXiv:2506.03135, 2025. [21] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. Advances in Neural Information Processing Systems, 37:4895548970, 2024. [22] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. [23] Benjamin Kuipers. Modeling spatial knowledge. Cognitive science, 2(2):129153, 1978. [24] Benjamin Kuipers. The spatial semantic hierarchy. Artificial intelligence, 119(1-2):191233, 2000. [25] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [26] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-nextinterleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. [27] Xiaodi Li, Pan Xie, Yi Ren, Qijun Gan, Chen Zhang, Fangyuan Kong, Xiang Yin, Bingyue Peng, and Zehuan Yuan. Infinityhuman: Towards long-term audio-driven human. arXiv preprint arXiv:2508.20210, 2025. [28] Zhiqiu Lin, Siyuan Cen, Daniel Jiang, Jay Karhade, Hewei Wang, Chancharik Mitra, Tiffany Ling, Yuhan Huang, Sifan Liu, Mingyu Chen, et al. Towards understanding camera motions in any video. arXiv preprint arXiv:2504.15376, 2025. [29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [30] Parker Liu, Chenxin Li, Zhengxin Li, Yipeng Wu, Wuyang Li, Zhiqin Yang, Zhenyuan Zhang, Yunlong Lin, Sirui Han, and Brandon Feng. Ir3d-bench: Evaluating vision-language model scene understanding as agentic inverse rendering. arXiv preprint arXiv:2506.23329, 2025. [31] Weichen Liu, Qiyao Xue, Haoming Wang, Xiangyu Yin, Boyuan Yang, and Wei Gao. Spatial reasoning in multimodal large language models: survey of tasks, benchmarks and methods. arXiv preprint arXiv:2511.15722, 2025. [32] Xinhang Liu, Yuxi Xiao, Donny Chen, Jiashi Feng, Yu-Wing Tai, Chi-Keung Tang, and Bingyi Kang. Trace anything: Representing any video in 4d via trajectory fields. arXiv preprint arXiv:2510.13802, 2025. [33] Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, and Dacheng Tao. Learning affordance grounding from exocentric images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22522261, 2022. [34] Wufei Ma, Haoyu Chen, Guofeng Zhang, Yu-Cheng Chou, Celso de Melo, and Alan Yuille. 3dsrbench: comprehensive 3d spatial reasoning benchmark. arXiv preprint arXiv:2412.07825, 2024. [35] Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, and Kaipeng Zhang. Yume: An interactive world generation model. CoRR, abs/2507.17744, 2025. 14 [36] Yongsen Mao, Junhao Zhong, Chuan Fang, Jia Zheng, Rui Tang, Hao Zhu, Ping Tan, and Zihan Zhou. Spatiallm: Training large language models for structured indoor modeling. arXiv preprint arXiv:2506.07491, 2025. [37] Nora Newcombe and Janellen Huttenlocher. Making space: The development of spatial representation and reasoning. MIT press, 2000. [38] OpenAI. Openai api documentation. https://platform.openai.com/docs/models/gpt-3-5, 2023. Accessed on September 19, 2025. [39] OpenAI. Gpt-4v(ision) system card, 2023. URL https://cdn.openai.com/papers/GPTV_System_Card.pdf. Accessed: 2025-09-19. [40] Jean Piaget. The construction of reality in the child. Routledge, 2013. [41] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. UI-TARS: pioneering automated GUI interaction with native agents. CoRR, abs/2501.12326, 2025. [42] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In International Conference on Computer Vision, 2021. [43] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Ángel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021. IEEE, 2021. [44] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [45] Shenna Shepard and Douglas Metzler. Mental rotation: effects of dimensionality of objects and type of task. Journal of experimental psychology: Human perception and performance, 14(1):3, 1988. [46] Shuran Song, Samuel Lichtenberg, and Jianxiong Xiao. Sun rgb-d: rgb-d scene understanding benchmark suite. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 567576, 2015. [47] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. [48] Edward Tolman. Cognitive maps in rats and men. Psychological review, 55(4):189, 1948. [49] Alexander Veicht, Paul-Edouard Sarlin, Philipp Lindenberger, and Marc Pollefeys. Geocalib: Learning single-image calibration with geometric optimization. In European Conference on Computer Vision, pages 120. Springer, 2024. [50] Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Yixuan Li, and Neel Joshi. Is picture worth thousand words? delving into spatial reasoning for vision language models. In The Thirty-Eighth Annual Conference on Neural Information Processing Systems, 2024. [51] Siting Wang, Luoyang Sun, Cheng Deng, Kun Shao, Minnan Pei, Zheng Tian, Haifeng Zhang, and Jun Wang. Spatialviz-bench: Automatically generated spatial visualization reasoning tasks for mllms. arXiv preprint arXiv:2507.07610, 2025. [52] Wenqi Wang, Reuben Tan, Pengyue Zhu, Jianwei Yang, Zhengyuan Yang, Lijuan Wang, Andrey Kolobov, Jianfeng Gao, and Boqing Gong. Site: towards spatial intelligence thorough evaluation. arXiv preprint arXiv:2505.05456, 2025. [53] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 49094916. IEEE, 2020. [54] Zehan Wang, Ziang Zhang, Tianyu Pang, Chao Du, Hengshuang Zhao, and Zhou Zhao. Orient anything: Learning robust object orientation estimation from rendering 3d models. arXiv:2412.18605, 2024. 15 [55] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: Tracking any 2d pixels in 3d space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2040620417, 2024. [56] Yuxi Xiao, Jianyuan Wang, Nan Xue, Nikita Karaev, Yuri Makarov, Bingyi Kang, Xing Zhu, Hujun Bao, Yujun Shen, and Xiaowei Zhou. Spatialtrackerv2: 3d point tracking made easy. arXiv preprint arXiv:2507.12462, 2025. [57] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: Empowering large language models to understand point clouds. In European Conference on Computer Vision, pages 131147. Springer, 2024. [58] Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, and Kevin Liang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models. arXiv preprint arXiv:2505.17015, 2025. [59] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [60] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. [61] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1037110381, 2024. [62] Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, et al. Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. arXiv preprint arXiv:2502.09560, 2025. [63] Rui Yang, Ziyu Zhu, Yanwei Li, Jingjia Huang, Shen Yan, Siyuan Zhou, Zhe Liu, Xiangtai Li, Shuangye Li, Wenqian Wang, Yi Lin, and Hengshuang Zhao. Visual spatial tuning. arXiv preprint arXiv:2511.05491, 2025. [64] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, et al. Mmsi-bench: benchmark for multi-image spatial intelligence. arXiv preprint arXiv:2505.23764, 2025. [65] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, et al. Spatial mental modeling from limited views. arXiv preprint arXiv:2506.21458, 2025. [66] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024. URL https://llava-vl.github.io/ blog/2024-04-30-llava-next-video/. [67] Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. Llava-3d: simple yet effective pathway to empowering lmms with 3d-awareness. arXiv preprint arXiv:2409.18125, 2024."
        },
        {
            "title": "Contents",
            "content": "1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4 Instantiating the SpatialTree Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.1 Data Curation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2 Evaluation Metrics Designs",
            "content": "5 Hierarchical Analysis of Spatial Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Models and Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Overall Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Exploring Ability Dependencies and Hierarchical Transfer . . . . . . . . . . . . . . . . . . . 6.1 Probing Cross-Ability Transfer via SFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Conclusion and Future works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Visualization of Data Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Evaluation Metrics Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . SpatialPlus: Complementary Data Annotations for SpatialTree . . . . . . . . . . . . . . . C.1 Orientations (L1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Agentic Competence (L4) Ability Transfer via Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Benchmark Metric Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . More Visualizations for QAs in SpatialTree Bench . . . . . . . . . . . . . . . . . . . . . . . . 1 3 4 4 5 5 6 6 7 7 8 8 9 9 11 12 18 19 19 20 24 24 25 17 Figure Construction of SpatialTree-Bench. We build our benchmark by reorganizing various existing datasets and mapping them to our capability tree, where SpatialPlus, complementary dataset are introduced to ensure the capability coverage."
        },
        {
            "title": "A Visualization of Data Sources",
            "content": "How different datasets contribute to our SpatialTree evaluation is shown in Fig. A."
        },
        {
            "title": "B Evaluation Metrics Details",
            "content": "Multi-Option QAs. For multi-option question answering, each model is evaluated on its ability to select the correct option from predefined set. We measure accuracy by comparing the predicted choice against the ground-truth answer. This paradigm captures models understanding of spatial relations, object properties, and causal dynamics within scene, corresponding to the lowand mid-level capabilities in the SpatialTree (L1L3). Numeric QAs. Numeric QAs require models to predict continuous quantities such as distances, angles, or 3D coordinates. We evaluate performance using relative error metrics, for example: Relative Error = ˆy y , where ˆy is the predicted value and is the ground truth. This metric ensures that predictions are scaled appropriately across different magnitudes and emphasizes precision in spatial reasoning. 18 Figure Orientation Annotations. The left side is the gravity field estimated from GeoCalib [49], while the right side is from OrientAnything. GPT Judge. For tasks that are open-ended or involve complex reasoning (e.g., trajectory description, action sequence explanation), we leverage GPT-based judge to assess correctness. The judge evaluates whether the generated response satisfies the task requirements, optionally scoring partial correctness. This approach allows flexible evaluation beyond rigid numeric or multiple-choice formats, especially for midand high-level capabilities in L3L4. Agentic Evaluation. To assess agentic competence, models are deployed in interactive simulated environments, such as those provided by EmbodiedBench [62]. We evaluate navigation and manipulation tasks along multiple dimensions: success rate in completing the target goal, relative translation accuracy, and directional alignment. For each action step, combined metric is computed using relative distance and cosine similarity of movement vectors, producing normalized score in [0, 1]. Aggregating scores over all steps yields comprehensive measure of an agents ability to plan and execute actions in long-horizon, embodied tasks. SpatialPlus: Complementary Data Annotations for SpatialTree C.1 Orientations (L1) The Orientation capability, fundamental yet under-explored area, involves estimating both gravity direction and 3D object orientation. To generate annotations, we leveraged Geocalib [49] for gravity vector estimation for and OrientAnything [54] for object poses. We applied these tools to datasets suited for each task: gravity, we annotated 500 images sampled from the diverse, drone-captured TartanAir [53] dataset; for object orientation, we utilized the object-centric Co3dv2 [42] dataset (Seen in Fig. B). For gravity, the goal is to estimate the cameras orientation relative to the gravity vector, typically represented by the pitch and roll angles. Formally, let the gravity vector in the world frame be: gw = , 0 0 1 (1) and let Rcw SO(3) denote the rotation from the world frame to the camera frame. The gravity direction in the camera frame is then: From gc = [gx, gy, gz], the pitch and roll angles can be computed as: gc = Rcw gw. pitch = arctan 2(gx, (cid:113) + g2 g2 ), roll = arctan 2(gy, gz). (2) (3) (4) Here, pitch measures the forwardbackward tilt of the camera, while roll measures the sideways tilt. To evaluate an MLLMs proficiency in this task, we require the model to analyze the input image and return 19 these same three parameters in structured JSON format. An example of our prompt template is shown in Fig. C. Figure Prompt template for Orientation Estimation. For evaluation, we move beyond simple absolute error metric and adopt probabilistic approach that accounts for the inherent uncertainty of the ground-truth annotations provided by Geocalib. For each predicted parameter (pitch, roll, and vFOV), Geocalib also outputs an uncertainty value, which we interpret as the standard deviation (σgt). We then calculate normalized similarity score (S) for each parameter using Gaussian kernel, defined as: S(ypred, ygt, σgt) = exp (cid:18) (cid:19) (ypred ygt)2 2σ2 gt (5) where ypred is the MLLMs prediction, ygt is the ground-truth value from Geocalib, and σgt is its associated uncertainty. This scoring function gracefully penalizes deviations from the ground truth: the score is 1 for perfect match and decays towards 0 as the error increases. Crucially, larger uncertainty σgt in the ground truth leads to slower decay, making the scoring more lenient when the ground truth itself is less certain. The final score for the task is the average of the individual scores for pitch, roll, and vFOV. For object orientation estimation, most of metrics are similar to gravity, and the evaluation are conducted on Azimuth, Polar and Rotation these three angles. C.2 Agentic Competence (L4) (a) (b) Figure Navigation Data Curation. (a) shows paired images used for evaluation, where MLLMs are expected to move from left to right. (b) illustrates our curation process: reconstructing metric 3D models and camera trajectories, then converting them into actions. Spatial Action Mapping. In the context of spatial agents, navigation and manipulation represent the most common forms of interaction within 3D environments. We address each with distinct action space design. For navigation, we conceptualize agent movement as series of camera motion controls (referring to recent video world models [4, 35]). To enable precise and intuitive control, we decompose complex camera movements 20 Table Spatial Action Mapping. This table defines standardized interface that maps continuous 6-DoF motions and discrete control signals into action primitives with unified parameterization, enabling MLLMs to plan and execute embodied behaviors for agentic competence evaluation. Primitive Primitive Term Category Description Action Mapping Param. Threshold Ptruck Truck Translation Pdolly Dolly Translation Ppedestal Pedestal Translation Ppan Ptilt Proll Pan Tilt Roll Rotation Rotation Rotation Ogripper Gripper Gripper Control Opush/pull Push/Pull Gesture Ograb Grab Gesture Move camera left/right (X-axis) Move camera forward/backward (Z-axis) Move camera up/down (Y-axis) Turn camera left/right (yaw) Tilt camera up/down (pitch) Roll camera CW/CCW (roll) Open or close the gripper Push or pull object along forward axis Grab or release object A/D W/S Q/E / / Z/X G/H P/L None vx vz vy ωy ωx ωz 0.01 m/s 0.01 m/s 0.01 m/s 0.5/s 0.5/s 0.5/s State {0, 1} N/A Dir. {1, +1} N/A State {0, 1} G/H into set of fundamental motion primitives inspired by established cinematography techniques. This approach allows us to translate high-level language instructions (e.g., \"move to the left,\" \"look up\") into structured, low-level action space. The six fundamental primitives, their corresponding cinematic terms, degrees of freedom (DoF), and parameterization are detailed in Tab. A. Formally, the camera trajectories are defined with series of Camera-to-World (C2W) transformation matrices Tmotion = {Ti , = 0, 1, . . . , 1. Then the continuous camera transformation can be decomposed into different components corresponding to each motion primitive, and discretized into the navigation action Anav using speed threshold: 0, = 0, 1, . . . , t}, while the camera transformation at each moment is Tii+1 = Ti+1T Anav = Tii+1 = {R, t} (6) (cid:26) ti vi, tk ωk i, {x, y, z}, ti, tk Z0 (cid:27) where = (Rx, Ry, Rz) represents the rotation components obtained via Euler decomposition, = (tx, ty, tz) denotes the translation components along the x, y, and axes, and ti, tk are discrete integers ranging from 0 up to the video frame rate (FPS). For manipulation, we focus on two representative scenarios to simplify the problem and enable controlled evaluation: human-hand manipulation and robotic gripper manipulation. For the gripper setting, we include gripper open/close actions along with wrist-level 6-DoF motion. For the human-hand setting, we define small set of intuitive gesture primitives (i.e., push, pull, grab) seen in Tab. that capture essential interaction patterns. These manually defined mappings create unified yet tractable action space for analyzing MLLMs planning and manipulation competence. Building on the proposed spatial action mapping, we curate annotated data from diverse sources, including human-hand manipulation videos, navigation video games, robotic arm manipulation datasets, and simulation environments. This unified dataset enables us to evaluate whether MLLMs can accurately plan and execute actions in the defined metric action space. Further implementation details are provided in Sec. 4 and in the experimental section. Goal-driven Navigation. We leverage our SpatialEngine to get the action annotations as shown in Fig. D. We first extract the metric pose trajectories from the games videos, and convert them into discrete actions with our spatial action mapping, and then we randomly sample several image pairs from the video with the correspondence checking. For evaluation, the goal is image, and the MLLMs are supposed to control the character to move to the target positions. We use the prompt template as below: Figure Prompt of navigation. In this prompt, translation and rotation steps are computed from the actual movement, while capping the number of steps at 10 to prevent overly long action sequences. To evaluate MLLMs, we compute normalized metric in the range [0, 1] by combining relative distance and directional accuracy. Specifically, for each step, let ppred and pgt denote the predicted and ground-truth translation vectors, respectively. The relative distance score is defined as: sd = max (cid:16) 0, 1 ppred pgt pgt (cid:17) , and the directional score is computed by the cosine similarity: sθ = ppred pgt ppred pgt . The final step-wise accuracy is then: sstep = sd max(0, sθ) which ensures value in [0, 1], where 1 indicates perfect alignment in both distance and direction. Aggregating sstep across all steps provides comprehensive measure of the models precision in executing end-effector motions. 22 Goal-driven Manipulation For the Goal-Driven Manipulation capability, we utilize action annotations from the Droid [22] and EgoDex [16] datasets. This task requires the MLLM to generate sequence of precise actions to move robot end-effector or human hand from starting state to target state, both specified by images. The action space for Droid encompasses 7-DoF control: 6-DoF for the end-effectors pose (translation and rotation) and binary state for the gripper (open/close). similar action space is adapted for EgoDex, controlling wrist pose and finger grip. The MLLM is prompted to generate sequence of continuous action vectors, as shown in the template below: Figure Prompt for Goal-Driven Manipulation with 7D Action Representation. To evaluate the MLLMs performance, we assess the accuracy of the predicted action sequence against the ground truth. For the translational component of the motion, we reuse the step-wise accuracy metric sstep from the navigation task, which combines relative distance and directional scores. For the rotational component, we compute normalized score based on the angular difference between the predicted orientation and the ground truth. Let Rpred and Rgt be the predicted and ground-truth rotation matrices for step. The rotational error 23 angle θerr is calculated from the error rotation matrix Rerr = RpredRT gt : θerr = arccos (cid:18) Tr(Rerr) 1 2 (cid:19) . The rotation score srot is then defined as: srot = max (cid:16) 0, 1 (cid:17) , θerr π which normalizes the error to [0, 1] range, where 1 indicates perfect rotational match. Finally, the gripper score sgripper is binary accuracy (1 if the predicted state matches the ground truth, 0 otherwise). The final score for each step is weighted combination of these three metrics, providing holistic evaluation of the models ability to perform precise, multi-faceted manipulation tasks."
        },
        {
            "title": "D Ability Transfer via Prompting",
            "content": "Figure Correspondence Prompting for Navigation. The correspondence prompt guides Gemini2.5-pro to navigate and move more accurately within 3D environments. In addition to SFT, we investigate cross-level ability influence through explicit prompting. Specifically, we consider representative task pair: low-level abilities (L1.Corr, L1.Dist, L1.Size) and high-level task (L4.Imaged Goal Navigation). Intuitively, correspondence is necessary component for navigation. Using Gemini2.5-pro, we provide models with explicit prompts derived from matching visualizations, depth, and size context. As shown in Fig. G, correspondence guidance improves target direction recognition, increasing accuracy by 7.1%, while distance and size prompting yield gains of 5.5% and 2.1%, respectively. These results suggest that grounding MLLM reasoning with explicit low-level visual information can substantially enhance performance on complex spatial navigation tasks."
        },
        {
            "title": "E Benchmark Metric Aggregation",
            "content": "To derive single, comprehensive score for models spatial intelligence, we employ hierarchical aggregation methodology. This approach is designed to reflect the complex, multi-layered nature of spatial cognition, rather than treating all abilities as equally important. The design is principally guided by established theories 24 in cognitive psychology, which posit that spatial intelligence is constructed hierarchically, with fundamental perceptual skills forming the bedrock for more abstract reasoning and planning. Our aggregation framework is built upon the SpatialTree structure. The assignment of weights within this tree is determined by synthesis of theoretical principles and empirical, data-driven insights: Figure An illustration of the hierarchical weighting scheme for metric aggregation with in the SpatialTree. Each node represents capability layer, with the assigned weight used for the bottom-up calculation of the final score. The weighting prioritizes foundational perceptual abilities (L1) as they are prerequisites for higher-level cognitive tasks. Cognitive Hierarchy. In line with cognitive science literature, our weighting scheme prioritizes foundational capabilities, as shown in Fig. H. The L1 layer, which represents low-level spatial perception, is assigned the largest weight, as these skills are prerequisites for almost all higher-level spatial tasks found in L2 (Mental Mapping) and L3 (Mental Simulation). Empirical Dependency from Correlation Analysis. The theoretical hierarchy is further refined and validated by our empirical findings from the Pearson correlation heatmap  (Fig. 5)  . The heatmap allows us to identify atomic abilities that exhibit strong, widespread correlations with multitude of other skills. These influential abilities are considered more fundamental to the overall spatial intelligence network and are consequently assigned higher weights within their respective sub-trees. This ensures our metric is not just theoretically sound, but also reflects the actual dependencies observed in model performance. The final score is calculated via bottom-up, weighted summation. The performance score for any parent node in the tree is the weighted sum of the scores of its immediate children. This process is recursively applied until the root node is reached, yielding single, principled score that holistically quantifies the spatial intelligence of given MLLM."
        },
        {
            "title": "F More Visualizations for QAs in SpatialTree Bench",
            "content": "We declare that Large Language Models (LLMs) were used in limited capacity during the preparation of this manuscript. Specifically, LLMs were employed for grammar checking, word choice refinement, and typo correction. All core technical contributions, experimental design, analysis, and conclusions are entirely our own. The use of LLMs did not influence the scientific methodology, result interpretation, or theoretical contributions of this research. 25 Geometry Prompt Answer Which is wider, the width of the painting on the wall or the width of the wooden table next to the sofa? The wooden table Prompt The coordinates [ , ] are normalized to 0-1 and scaled by 1000, with [ 0 , 0 ] at the top-left. The x-axis represents the width, and the y-axis represents the height. What is the depth at the coordinates [ 389 , 180 ] in the image (in mm)? Answer 1915 Prompt Answer Which line is closer to the edge? Table Examples of L1.Geometry. 26 Relation Prompt Answer Prompt Answer When you were taking the photo in Image 1, where was the exit of the room relative to you? Rear left Track [ 150, 470 ] from Image 1 to its correspondence in Image 2. Table Examples of L1.Relation. 27 Orientation Prompt Answer Prompt Answer Motion Prompt Answer Analyze the image to determine the vertical field of view (vfov) and calculate the cameraagainst the vertical axis (gras roll and pitch angles vity). [ \"roll_unc\": 0.61629718542099, \"pitch_unc\": 1.862020492553711, \"vfov_unc\": 20.077800750732422 ] If stand at the cats position facing where it is facing, is the knife in front of me or behind me? In front of Table Examples of L1.Orientation. As shown in the video, which direction is the view moving towards ? Prompt Answer From view 1 to view2, in which direction is the race car moving? First to the front left, then to the front right. Table Examples of L1.Motion. 28 Localization Prompt Answer Please ground mirror in this image. The 3D bounding box is defined in the format: \"bbox_3d\": [u_center, v_center, z_center, x_size, y_size, z_size, roll, pitch, yaw] [ ] \"bbox_3d\": [901, 558, 4.87, 0.541, 1.698, 0.243, 179.279, -29.539, 176.489] Table Examples of L1.Localization. Understanding Prompt From the man wearing the red hat, how many people are on his left? [ A. zero, B. one, C. three, D. two ] Answer Prompt Answer To ride this bicycle along the seawall, where would you place your hands to steer, where would you sit, and where would you put your feet to pedal? [744, 439, 783, 489] Table Examples of L2.Understanding. 29 Memory Prompt From the man wearing the red hat, how many people are on his left? [ A. Leather loveseat with three seat cushions, B. TV, C. Two single sofas, D. Leather loveseat Answer ] Prompt Answer How many bed(s) are in this room? 2 Table Examples of L2.Memory. 30 Causal Reasoning Prompt If the dog on the right reaches the camera in 5 s, what is its speed? [ A. 14.7m/s B. 1.9m/s C. 21.7m/s D. 11.9m/s Answer ] Prompt 3x3 grid paper undergoes two folds as shown. hole is punched in the folded state. Which option (A, B, C, or D) shows the unfolded paper? Answer Prompt What object is located immediately to the right of point [710 991] in the second image, just outside of the frame ? Answer Wooden Table Table Examples of L3.Causal Reasoning. 31 Sequential Planning Prompt Answer The camera is moving forward. Please arrange these three images in chronological order? B-C-A Prompt The top row of images shows different views of the initial state of cube stack, while the bottom row shows different views of the final state after transformation. During the transformation process, blocks can move one unit in any direction (forward, backward, left, right, up, down). If the target position is empty, the block can move there directly; if the target position already has block, they swap places. Blocks cannot float in the air. If block is moved away from position, any block above it will fall down until reaching supporting surface. The xyz axes are shown in the diagram, and each blocks position can be precisely identified using coordinates (x1,y1,z1). Which of the following transformation sequences can change the cube stack from the initial state to the final state shown in the diagram? Please answer from options A, B, C, or D. A. (1, 0, 0) y+ -- (0, 0, 1) zB. (1, 0, 0) x+ -- (1, 0, 0) y+ C. (2, 0, 0) x- -- (1, 0, 0) y+ -- (2, 0, 0) xD. (0, 0, 0) x+ -- (0, 1, 0) y- -- (0, 0, 1) y+ Answer Table Examples of L3.Sequential Planning. 32 L4 Agentic Competence Prompt You are an intelligent agent observing video sequence that depicts task being performed which is \"please get the blue pan out of the bottle\". For each image in the sequence that provides candidate action options, select the single most appropriate action to perform in the current state. In the final frame, do not select any action. Instead, determine whether the overall task shown in the sequence has been successfully completed (1) or not completed (0). Please output the selected action for each intermediate frame and the completion flag for the final frame. A. EB1 B. CC1 C. EA0 D. EE1 E. AD0 F. EE0 G. CD0 H. EC0 Answer Prompt Which image shows the robot making the most progress towards the task placing the fork to the right side of the orange kitchen wipe? A. First. B. Second. C. Third. D. Fourth Answer Table Examples of L4 Agentic Competence. 33 L4 Agentic Competence (Continued) Prompt Task: Visual Navigation Action Sequence Generation You are an expert visual navigation agent. Your task is to generate sequence of actions to navigate robot from starting visual state (Image 2) to target visual state (Image 3) based on the provided visual information. Context and Example We provide three sequential images: Image 1, Image 2, and Image 3. To help you understand the task, we are providing the complete action sequence that navigates from Image 1 to Image 2 as an example. Example Action Sequence from Image 1 to Image 2: { \"actions\": [\"Dolly In\", \"Truck Left\", \"Pedestal Up\", \"Pan Left\", \"Roll CW\"], \"step_nums\": [0, 3, 4, 5, 0] } Your Core Task Now, carefully observe Image 2 (the starting state) and Image 3 (the target state). Your mission is to generate the action sequence required to navigate from Image 2 to Image 3. 1. Action Space You must choose from the following 12 elementary actions. In your output, you must use the symbol specified to represent each action. Category Action Sym Description Trans. Rot. Dolly In Dolly Out Truck Left Truck Right Pedestal Up Pedestal Down Move forward Move backward Move left Move right space Move up shift Move down Pan Left Pan Right Tilt Up Tilt Down Roll CW Roll CCW Neg. rot around +Y Pos. rot around +Y Pos. rot around +X Neg. rot around +X Neg. rot around +Z Pos. rot around +Z Special Stay STOP No movement 2. Step Size Parameters The magnitude of each action is determined by step_num combined with unit step size. Translation: 0.0626 meters per step. Rotation: 0.0725 radians per step. E.g., action_symbol: \"W\" and step_num: 10 means moving forward by 10 0.0626 meters. 3. Required Output Format Your final output MUST be JSON object containing two keys: \"actions\" and \"step_nums\". The lengths of both arrays must be identical. Answer { \"actions\": [ \"Truck Left\", \"Pedestal Up\", \"Pan Left\" ], \"step_nums\": [ 1, 4, ] } Table Examples of L4 Agentic Competence (Continued)."
        }
    ],
    "affiliations": [
        "Beijing Jiaotong University",
        "ByteDance",
        "Zhejiang University"
    ]
}