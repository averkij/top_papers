{
    "paper_title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling",
    "authors": [
        "Yizhi Li",
        "Qingshui Gu",
        "Zhoufutu Wen",
        "Ziniu Li",
        "Tianshun Xing",
        "Shuyue Guo",
        "Tianyu Zheng",
        "Xin Zhou",
        "Xingwei Qu",
        "Wangchunshu Zhou",
        "Zheng Zhang",
        "Wei Shen",
        "Qian Liu",
        "Chenghua Lin",
        "Jian Yang",
        "Ge Zhang",
        "Wenhao Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in aligning large language models via reinforcement learning have achieved remarkable gains in solving complex reasoning problems, but at the cost of expensive on-policy rollouts and limited exploration of diverse reasoning paths. In this work, we introduce TreePO, involving a self-guided rollout algorithm that views sequence generation as a tree-structured searching process. Composed of dynamic tree sampling policy and fixed-length segment decoding, TreePO leverages local uncertainty to warrant additional branches. By amortizing computation across common prefixes and pruning low-value paths early, TreePO essentially reduces the per-update compute burden while preserving or enhancing exploration diversity. Key contributions include: (1) a segment-wise sampling algorithm that alleviates the KV cache burden through contiguous segments and spawns new branches along with an early-stop mechanism; (2) a tree-based segment-level advantage estimation that considers both global and local proximal policy optimization. and (3) analysis on the effectiveness of probability and quality-driven dynamic divergence and fallback strategy. We empirically validate the performance gain of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours from 22\\% up to 43\\% of the sampling design for the trained models, meanwhile showing up to 40\\% reduction at trajectory-level and 35\\% at token-level sampling compute for the existing models. While offering a free lunch of inference efficiency, TreePO reveals a practical path toward scaling RL-based post-training with fewer samples and less compute. Home page locates at https://m-a-p.ai/TreePO."
        },
        {
            "title": "Start",
            "content": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling ByteDance Seed, M-A-P, UoM Full author list in Contributions Abstract Recent advancements in aligning large language models via reinforcement learning have achieved remarkable gains in solving complex reasoning problems, but at the cost of expensive on-policy rollouts and limited exploration of diverse reasoning paths. In this work, we introduce TreePO, involving self-guided rollout algorithm that views sequence generation as tree-structured searching process. Composed of dynamic tree sampling policy and fixed-length segment decoding, TreePO leverages local uncertainty to warrant additional branches. By amortizing computation across common prefixes and pruning low-value paths early, TreePO essentially reduces the per-update compute burden while preserving or enhancing exploration diversity. Key contributions include: (1) segment-wise sampling algorithm that alleviates the KV cache burden through contiguous segments and spawns new branches along with an early-stop mechanism; (2) tree-based segment-level advantage estimation that considers both global and local proximal policy optimization. and (3) analysis on the effectiveness of probability and quality-driven dynamic divergence and fallback strategy. We empirically validate the performance gain of TreePO on set reasoning benchmarks and the efficiency saving of GPU hours from 22% up to 43% of the sampling design for the trained models, meanwhile showing up to 40% reduction at trajectory-level and 35% at token-level sampling compute for the existing models. While offering free lunch of inference efficiency, TreePO reveals practical path toward scaling RL-based post-training with fewer samples and less compute. Home page locates at https://m-a-p.ai/TreePO. 5 2 0 2 4 ] . [ 1 5 4 4 7 1 . 8 0 5 2 : r Figure 1 Demonstration of the Validation Performance Curves along Training based on Qwen2.5-7B (Left, Mid ) and Demonstration of TreePO Sampling (Right). Left, Mid : Compared to the GRPO setting, although replaced additional treed-based sampling causes slower convergence, it could stabilize the training. When cooperate the health."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement Learning (RL) has emerged as powerful paradigm for enhancing the complex reasoning abilities of Large Language Models (LLMs) [13]. However, the efficacy and scalability of RL face fundamental constraints from two long-standing challenges: exploration (generating diverse responses) and exploitation (obtaining guidance from external feedback). In the context of LLMs, these challenges become even more pronounced, as models must generate sequences spanning thousands of tokens before receiving single reward signalwhich is typically sparse and delayed [4, 5]. This constraint creates two critical research challenges: (1) How can we enable LLMs to explore potentially correct reasoning paths while maintaining or reducing computational costs? and (2) How can we accurately attribute sparse outcome rewards to the specific tokens that contributed to correct answers? We present key observations that inspire our approach to addressing these challenges: standard RL approaches typically generate multiple independent trajectories for single querya strategy that is both computationally inefficient and conceptually sub-optimal. From computational perspective, this approach creates paths with separate Key-Value (KV) caches, failing to utilize shared KV caching mechanisms that could significantly accelerate inference. Conceptually, continuing to explore paths already known to be impossible or incorrect, without early termination, represents critical limitation in adaptability. That is, while this sampling strategy may appear simple to implement, its lack of structural design ultimately limits its effectiveness. promising sampling strategy is Monte Carlo Tree Search (MCTS) [6] or its variants [7, 8], which enables agents to leverage tree structures to achieve functions like early termination and roll back. Despite its promise, MCTS is often inefficient for LLM inference, requiring numerous sequential rollouts that are poorly suited for parallelized engines. Recent efforts have moved toward better utilization of LLM inference engines, recognizing that optimizing the data generation process itself is critical frontier [9, 10]. We believe this is the correct direction and accordingly propose heuristic, self-guided, tree-based sampling mechanism designed to fully leverage the Key-Value (KV) cache mechanism. By structuring the rollout process as tree, we maximize the reuse of shared prefixes as demonstrated in the Figure 1 (Right). Our findings show this approach can averagely reduce 40% of trajectory-level inference time for the baselines (see 4.1), thereby improving computational efficiency without sacrificing performance. To address the second question of credit assignment, our tree-based sampling structure naturally facilitates more granular advantage estimation. This allows us to propose new advantage function that is distinct from recent related works like TreeRL [11] and SPO [12]. While these methods also leverage tree or segment-based structures, their advantage calculations are primarily MCTS-like, focusing on the value difference between parent and its child node to assign credit. Our approach, in contrast, models entire sub-trees as coherent sub-groups, enabling more robust relative advantage calculation based on the collective outcomes of all descendants. More critically, our design is proven to be feasible for training directly from base model, aligning with the \"RL-zero\" paradigm where reasoning capabilities are elicited without prior supervised fine-tuning (SFT). This stands in contrast to the mentioned peers, which are demonstrated on models that have already undergone SFT. In this paper, we introduce Tree-based Policy Optimization (TreePO), framework that integrates these solutions into unified RL pipeline. TreePO replaces inefficient independent rollouts with computationally efficient and algorithmically flexible tree search. This structure not only improves sampling efficiency but also enables principled credit assignment and controllable exploration. We introduce novel heuristic sampling strategies, including dynamic divergence and probability-based fallback, which strategically allocate the generation budget to explore more diverse and promising reasoning paths. This transforms the rollout phase into transparent and controllable search process, providing powerful tool for analyzing the training dynamics of RL models. In summary, our contributions are: We introduce TreePO, novel RL training scheme that replaces standard i.i.d. sequential sampling with heuristic tree-based rollout mechanism. By implementing heuristic-driven exploration strategies, including dynamic divergence and probability-based fallback, this mechanism enhances the models ability to explore the reasoning space effectively while significantly improving computational efficiency by leveraging KV-caching. 2 We propose new tree-based advantage estimation function that enables more precise credit assignment and is uniquely suited for training LLMs from base model, without requiring an initial instruction tuning stage. We demonstrate through extensive experiments that TreePO provides superior trade-off between computational cost and model performance, establishing more efficient and scalable frontier for training large reasoning models."
        },
        {
            "title": "2 TreePO: A Tree-based Training Scheme for Policy Optimization",
            "content": "Figure 2 Multiple sampled trajectories from the same prompt, with shared reasoning segments highlighted in matching colors. Despite stochastic generation, key problem-solving steps are consistently reproduced."
        },
        {
            "title": "2.1 Case Study: The Aligned Model Produces Shared Prefix",
            "content": "We begin with an empirical observation on the structure of reasoning trajectories. Given fixed prompt, we perform 16 independent stochastic rollouts using temperature of 0.8 to encourage diverse generation while preserving coherence. Upon close inspection, we find that despite the variation in final solutions, the generated trajectories share extensive overlapping segments, particularly in the early and intermediate stages of reasoning. As illustrated in Figure 2, components such as problem interpretation, variable assignment, and initial logical deductions appear nearly identical across multiple rollouts. These recurring segments are highlighted with consistent colors, visually demonstrating the emergence of stable reasoning prefixes. This phenomenon indicates that, even under stochastic sampling, the model consistently follows common path for the initial stages of reasoning before diverging at later decision points. Such redundancy across trajectories suggests fundamental inefficiency in standard on-policy reinforcement learning: each rollout independently recomputes the same prefix tokens, leading to duplicated computation and KV cache storage. Since reasoning paths naturally form tree-like structure where common prefixes branch into diverse continuations, it is both feasible and highly beneficial to model sequence generation as tree-structured search process. By explicitly representing shared prefixes only once and amortizing computation over them, using TreePO avoids redundant forward passes. Furthermore, the natural branching points provide ideal locations for uncertainty-driven exploration, enabling efficient and targeted expansion of reasoning paths."
        },
        {
            "title": "2.2 Tree-based Rollout Algorithm\nPreliminaries. For a given query qi ∈ Q, we formalize the problem of complex reasoning with chain of\nthought (CoT) [13] as the search algorithm to acquire a group of corresponding answers, oi,j ∈ O, under a\ncertain constraint of the computing budget. Specifically, we define the exact input of model as a prompt p, to\ndistinguish the query itself as the input might contain additional context. In the TreePO sampling setting, we\nalign terminology of RLVR and tree search to define:",
            "content": "1. the query as the root node at depth 0; 2. the number of complete trajectories as the tree width, w; 3 Algorithm 1 Tree-based Sampling Require: An array of queries = {q1, q2, . . . , qn} Ensure: Rollout responses that satisfy the budget requirement for all Q. 1: 2: Branching(P ) 3: while = do 4: 5: 6: 7: 8: Inference(P ) last for sk in do if Finish(sk) or FailedNode(sk) then 9: 10: 11: 12: 13: {{plast sk} else {plast sk} end if end for Branching(P ) Fallback(P, O) 14: 15: 16: end while 17: return Init inference prompts with queries Fork the prompts with designed policy Inference one step Clean up the inference queue Iterate throuhg the generated segments Build the full response for final output Concatenate the segment as new prompt Fork the prompts with designed policy Do fallback for unsufficient outputs Return the final outputs 3. the maximum decoding steps of trajectory as the depth, d; 4. the maximum decoding token of each time as the length of the segment, l; and 5. branching budget for each segment node. Under the context of RL training of large language models, the computing budget for sampling the trajectories of given set of queries could be defined by the trajectory group size of each query (also noted as the tree width w), if the maximum trajectory length is fixed. Segment-level Tree Sampling. As shown in Figure 1 (Upper Right), the vanilla sampling design requires the model to conduct token-level decoding and stem multiple complete trajectories from the same query independently. We re-organize such sampling progress into hybrid of segment-level tree searching and token-level decoding as in Figure 1 (Lower Right): for each trajectory, the model generates segment in max length step by step, until it hits the maximum response length or meets the any self-designed criteria of early stopping. We maintain queue of prompts to manage the sampling progress, and assign the queries as the initial prompt set. For an input query set q, the token-level decoding stops when the model generates [EOS] token or reaches the preset maximum segment token l; and the overall segment-level tree sampling progress ends when the prompt queue becomes empty (P = ). Specifically, given in each step of decoding, the inference engine would produce set of output segments in the exact number of . And each generated segment will be either appended to existing contexts to form new input prompt in the queue, or stop generation as leaf node if it contains flawed sub-string patterns or answer boxed. We introduce the branching of each search paths by forking the corresponding prompts times before segment inference, where the value of is dynamically calculated and assigned by design (see the details in the following literature). To fulfill the requirement of acquiring trajectories for each when the searching paths stop early before the tree reaches w, we introduce the feedback mechanism and stem new branches from the stopped paths to achieve better efficiency. Branching and Fallback. After reformulating the sampling progress into tree-based search, subtle balance between the rollout efficiency and model exploration space could be achieved by well defined branching and fallback protocol. In TreePO, we define vanilla -ary tree as baseline searching strategy, i.e., the branching budget for the root node (query) at depth is until it reaches the maximum width w. To 4 avoid the inference loading skew caused by the scarce long responses and the over-bias on the short paths, we coordinate two balancing tree searching strategies with the inference engine: 1. Branching Budget Transfer: As early stopped short search paths could derive small request batch to the inference engine and thus cause low utilization, we assign the maximum branching budget at depth to all existing active paths evenly (or determined by heuristic information). 2. Depth-First Search Fallback: To avoid sampling progress overly conducts fallback on the early stopped short paths and lose the capability of long complex reasoning, TreePO launches the fallback mechanism only when there is no active path for and the tree does not have enough trajectories wq < w. Heuristic Sampling. With the designed segment-level tree sampling protocol, we can now accordingly introduce more fine-grained and flexible control over the sampling progress with heuristic information. Without waiting for external signals, the TreePO sampling could exploit more in the desired search space by leveraging heuristic control on early stopping, branching, and fallback strategies. We first introduce simple early stopping trick for the flawed searching path by detecting the pattern with repetitive substrings within the new generated segment, which could reduce redundant computing, and forcedly prune the branches within the mumbling distribution that are usually generated by the less aligned base models. While conducting fallback, only those stopped paths containing formatted answer or ending with [EOS] can be selected as the candidate to randomly fallback in segment level. Other than the average branching budget assignment and random fallback strategy, there are more possible customized heuristic metrics could be applied when maintaining efficiency of TreePO, as long as no additional bubble of the pipeline is introduced. In the later 4.4, we take advantage of the log probabilities to steer the sampling progress without additional cost, as they are calculated during token-level decoding and returned from the inference engines by default."
        },
        {
            "title": "2.3 Tree-based Advantage Estimation for Policy Optimization",
            "content": "We take the GRPO [2] optimizing objective and adopt the improved modifications proposed in DAPO [3] as our starting point, which further highlights clip-higher gradient, dynamic sampling, and token-level loss: JTreePO(θ) = (cid:34) (q,a)D,{oi}G i=1πθold (q) (cid:88) oi (cid:88) (cid:16) (cid:80)G min 1 i=1 oi (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) < G. (cid:12){oi is_equivalent(a, oi)} ri,t(θ) ˆAi,t, clip t=1 i= s.t. 0 < (cid:16) ri,t(θ), 1 εlow, 1 + εhigh (cid:17) (cid:17) ˆAi,t (cid:35) , where ri,t(θ) = πθ(oi,t q, oi,<t) πθold(oi,t q, oi,<t) , ˆAi,t = Ri mean({Ri}G std({Ri}G i=1) i=1) . (1) (2) Although the delicate modifications in DAPO [3] largely improve the stability of the vanilla GRPO, the parallel-generated responses could still look homogeneous in the sequence-level to the policy model under certain circumstances (e.g., inference with low temperature or train with an over-confident model). Benefiting from the tree structure in the proposed rollout algorithm, the searching paths could be sourced during advantage calculation. Given arbitrary trajectory oi, it can be divided into multiple segments Sj by its inference step j: oi = s1 s2 sj1 sj, {j max depth} (3) Such prior allow us to reveal the nuanced segment-level difference among the trajectories, and introduce more accurate intra-response variations for the advantages to alleviate the obscurity brought by similar responses. Leveraging the shared prefixes, the advantage estimation function for the trajectory could be further calibrated by the subgroups derived from the shared predecessor nodes for the leaf nodes. Let the root In the context of math reasoning, we set this condition as including legal answer surrounded by boxed{}. 5 Figure 3 Demonstration of the TreePO Advantage Estimation. Assuming that the tree-based sampling has derived 8 trajectories (leaf nodes) given query q, we take node c2,2 as an example to calculate the sub-group advantages. The tree-based sub-groups could be further defined by its predecessors c2, c, and q. Thus the final advantages can be calculated as the averagely aggregated sub-group advantages. node be the sharing parent as the largest group G, we could denote sub-group Gj as the set of trajectories sharing the same predecessor node at inference depth j, satisfying: GJ GJ G2 G1 G, {j < max depth} (4) Given the formulated sub-groups, we keep using the average reward within sub-groups as the advantage baselines and conduct mean pooling on the relative advantages as the aggregated estimation. Furthermore, we incorporate the global variance normalization strategy as in REINFORCE++ [14] to improve the robustness of the estimation function, as the probability-based branching could bring potential turbulent rollout rewards across queries, and conduct dynamic rejection sampling to remove the queries with all correct or all wrong responses as in DAPO [3]. Hence the final TreePO advantage estimation function could be depicted as: ˆAi,t = (cid:80)J j= ˆAi,t,j std({ ˆAi,t,j}J1) , ˆAi,t,j = Ri mean({Ri,j}Gj ), s.t. std({Ri}G) = 0 (5)"
        },
        {
            "title": "3.1 Hyper Parameters\nModel. The main part of the reinforcement training experiments are trained from the Qwen2.5-7B base\nmodel [15]. Moreover, to further probe on the efficiency performance of the tree-based sampling on well\naligned LLMs, we use the Qwen2.5-7B-Instruct and Qwen2.5-Math-7B-Instruct to compare the vanilla\nsequential and the tree-based sampling.",
            "content": "Data and Evaluation. One source of the training samples is the the MATH dataset [16], deriving about 8 thousands queries of difficulty level 3 to 5 from, same as the setting in SimpleRL [17]. Another part of the training set consists 40 thousands samples from the DeepScaler [18] collection. For evaluation, we use the AIME 2024 [19], AMC 2023 [20], MATH500[16], MINERVA [21], and Olympiad Bench [22]. During validation and testing, we set the rollout as 16 and use the majority voting accuracy via 1000 times of sampling as the main metric. For the overall metric, we use the weighted average among the individual benchmarks bases on the sizes of the test sets. 6 Tree Setting. With the constraint of response length, we search three sets of the depth and segment token budget of tree sampling in online training: {28 256, 14 512, 7 1024}. The fixed branching budget at each depth is set as 2d, i.e., it will form binary tree search paths if no early stop happens. And the maximum tree width is set as = 16, where the sequential sampling baselines share the same group size parameter. During training, we explore whether additional initial branching budget by randomly assign 2 to 8 divergences, which is expected to improve the distribution diversity and thus break through the upper bound. We use More Init Divergence and Fixed Init Divergence to distinguish whether additional initial branching budget is allowed. Training. We filter out the prompts longer than 1024 tokens, and set the response length as 7 1024 in training. The trainings run on 64 GPUs with the VeRL framework [23] on FSDP mode, and use V0 inference engine of vLLM [24] as the inference backend. The learning rate is set as 1e 6 with 10 warm up steps. And the training batch size are set as 512 with the limit of maximum 20 epoch. The checkpoint saving interval as 50 steps. As the dynamic sampling strategy from DAPO is adopted, queries of 3 bsz would be sent to sampling out group of 16 trajectories, where 512 queries with std({Ri}G) = 0 are randomly selected. When there is not sufficient queries to form training batch, maximum two other additional samplings will be conducted, which could cause less training steps due to the enumeration logic of the data loader."
        },
        {
            "title": "3.2 Main Results",
            "content": "The results of the main experiment set are provided in Table 1, where we use sequential sampling to validate the full potential of the model performances. Based on the provided results and the training curves in Figure 1 (Left, Mid ), the introduction of tree-based methods TreePO sampling and advantage estimatorserves to significantly enhance training stability and computational efficiency, albeit with trade-off against raw convergence speed and peak accuracy in some configurations. The effect of Tree Sampling is twofold. First, as shown in Table 1, adding TreePO sampling to the baseline GRPO model provides substantial performance boost across all datasets, increasing the overall accuracy from 46.63% to 54.61%. This improvement is corroborated by the validation metric curves, where GRPO w/ TreePO Sampling (orange line) demonstrates far greater training stability compared to the volatile performance of the GRPO (blue line). Second, Table 2 reveals that while tree-based sampling does not always outperform strong sequential baseline in final accuracy (e.g., 58.21% for Sequential vs. 58.06% for TreePO b=8 in the \"More Init Divergence\" model), it consistently and significantly reduces computation time, cutting GPU hours by 12% to 43%. Beyond that, the TreePO advantage estimator, when used in conjunction with tree sampling, further enhances the training process, either with More Init Divergence (3.6% ) or Fixed Init Divergence setting (2.27% ). The green line in the validation curves shows the most stable and consistently high-performing trajectory during training. This indicates that the estimator component provides more precise reward signal bases on the tree hierarchy, guiding the model, and leading to more reliable convergence. Table 1 Performance Comparison with Sequential Sampling with Major@16 Accuracy. Model AIME AMC MATH MINERVA GRPO GRPO w/ TreePO Sampling TreePO w/ Fixed Init Divergence TreePO w/ More Init Divergence 17.13% 44.42% 72.89% 19.66% 51.63% 81.85% 28.89% 56.63% 82.41% 27.83% 55.53% 85.34% 30.94% 33.74% 35.76% 34.98% Olympiad Bench 35.09% 44.76% 47.75% 49.15% Overall 46.63% 54.61% 56.88% 58.21%"
        },
        {
            "title": "4 Discussion",
            "content": "This chapter is organized around set of research questions (RQs) that guide our investigation, with targeted ablation studies presented in the subsections that follow. RQ1. Does tree-based sampling improve sampling efficiency relative to nontree baselines, and under which segment and branching configurations? 7 Table 2 Performance Comparison Between Sequential and Tree-based Sampling with Major@16 Accuracy. Model Sampling AIME AMC MATH MINERVA TreePO w/ Fixed Init Divergence TreePO w/ More Init Divergence Sequential 8x2048, = 2 8x2048, = 4 8x2048, = 8 Sequential 8x2048, = 2 8x2048, = 4 8x2048, = 8 28.89% 56.63% 82.41% 23.33% 57.83% 81.80% 23.33% 57.83% 84.00% 26.67% 55.42% 83.60% 27.83% 55.53% 85.34% 21.52% 53.99% 81.89% 22.90% 57.24% 84.66% 26.21% 56.72% 85.23% 35.76% 36.76% 36.03% 36.40% 34.98% 33.93% 35.66% 35.02% Olympiad Bench 47.75% 45.93% 48.00% 46.22% 49.15% 44.41% 47.19% 48.81% Overall GPU Hour 56.88% 5.78 56.03% 4.29 (26%) 57.50% 4.82 (17%) 56.60% 5.09 (12%) 6.40 58.21% 54.67% 3.65 (43%) 57.26% 4.56 (29%) 58.06% 5.05 (22%) RQ2. How does the design of the TreePO advantage (e.g., subgroup aggregation choices) shape the optimization dynamics during training? RQ3. How do tree-sampling hyperparameters (segment length, branching factor, depth, and prefix alignment) affect stability and convergence? RQ4. What are the trade-offs between offline efficiency and efficacy across tasks and compute budgets under different tree-sampling settings? RQ5. How can we leverage branching budget assignment at segment-level modeling and provide more control signal in the heuristic sampling?"
        },
        {
            "title": "4.1 Sampling Efficiency Analysis\nSetup. To isolate efficiency, we conduct offline efficiency analyses using three variants of the Qwen2.5:\nQwen2.5-Math-7B, Qwen2.5-Math-7B-instruct, and Qwen2.5-7B-instruct. We benchmark throughput on\nrandomly sampled prompts from a held-out pool independent of model training (used solely for efficiency\nmeasurement). Experiments were run on NVIDIA H100 80GB GPUs without any parallel, such as tensor\nparallel and data parallel, maintaining a GPU utilization of 60%. Single inference maximum output length\nis determined by the maximum segment length. Unless noted, each run processes a batch of 64 prompts\nand, for tree-based sampling, 64 rollouts per prompt. We fix a per-trajectory token budget B = 7,000\nand vary tree depth d and max segment length Lseg subject to d × Lseg = B (segments are equal-length\nchunks). The non–tree baseline generates the same number of completions per prompt with identical sampling\nhyperparameters and the same budget B. We report Tokens per second (TokenPS; total model-processed\ntokens, including prefill and decode) and Trajectories per second (TrajPS; completed continuations per second),\nmeasured as wall-clock throughput.",
            "content": "(a) Qwen2.5-7B-Instruct (b) Qwen2.5-Math-7B-Instruct (c) Qwen2.5-Math-7B Figure 4 Performance comparison between Tree-based Sampling and Conventional Sampling across different tree depths. Tree-based sampling improve efficiency Under the same batch size, rollout count, and budget B, tree-based sampling yields on average +40% TrajPS and +30% TokenPS across the three models (geometric mean across configurations). Efficiency peaks at an intermediate depthsegment trade-off. Figure 4 shows that both TokenPS and TrajPS peak at intermediate depthsegment combinations rather than grow monotonically with depth. Prefill prefers longer segments and shallower trees, which reduces repeated KV cache and attention computation; decoding prefers deeper trees with more branches and parallel rollouts, better exploiting speculative execution and batched sampling. If segments are too short, the extra recomputation offsets the gains from depth, and the peak appears where these opposing effects balance. The optimal depthsegment configuration is model-specific. Qwen2.5-7B-Instruct peaks at depth 28, likely because instruction-following finds mid-depth balance: segments are not too short (better batched prefilling and context retention) while depth still yields sufficient decoding parallelism. Qwen2.5-Math-7B peaks at depth 14; for compute-intensive math reasoning, longer segments at shallower depth reduce repeated KV-cache and attention recomputation, improving throughput under the fixed budget. Qwen2.5-Math-7B-Instruct splitsTokenPS peaks at 14, whereas TrajPS peaks at 28 and 56consistent with deeper trees (which shorten segments under the 7k-token budget) lowering token-level throughput via recomputation and decoder overhead, but raising trajectory-level throughput by enabling more branching and parallel rollouts. (a) Qwen2.5-7B-Instruct (b) Qwen2.5-Math-7B-Instruct (c) Qwen2.5-Math-7B Figure 5 Performance comparison between Tree-based Sampling and Conventional Sampling across different numbers of rollouts. Rollout scaling is modeland workload-dependent. Qwen2.5-7B-Instruct shows nearly linear TokenPS/TrajPS growth as rollouts increase under tree-based sampling (with query count fixed at 64 and tree depth 28), reaching roughly 2 the baseline thanks to shared-prefix prefilling and more parallel decoding; by contrast, standard autoregressive decoding yields only modest gains. Qwen2.5-Math-7B-Instruct maintains stable 2 speedup across rollout counts, as structured, semantically aligned math trajectories sustain high cache-hit rates and efficient KV reuse, keeping batched decoding effective. Qwen2.5-Math-7B is non-monotonic: throughput peaks around 16 rollouts, then TokenPS/TrajPS decline as trajectory divergence reduces shared prefixes, KV-cache fragmentation and management overhead grow, memory pressure rises, and batching efficiency degrades; the lack of instruction tuning further loosens output structure. Overall, more rollouts can boost parallelism and cache reuse but also amplify memory and synchronization costs when trajectories diverge, implying modeland workload-dependent optimum."
        },
        {
            "title": "4.2 Analysis on the TreePO Advantage Estimation",
            "content": "(1)(3) use depthsegment 14 512 with 512-token fallback; (4) uses 7 1024 rollout but still Setup. 512-token fallback, inducing prefix misalignment. Figure 7 reports MATH/AIME accuracy, entropy loss, and response length, and the Subgroup-size Weighted curve serves as reference baseline for comparison across variants. ˆAi,t = (cid:80)J j=1 Gj ˆAi,t,j std({ ˆAi,t,j}J1)(cid:80)J j=1 Gj , 9 (6) Figure 6 Study on the Terms in TreePO Advantage. These group of experiments sets the depthsegment as 7 1024 and uses the subgroup size weighted aggregation advantage as the baseline. Simple averaging across subgroups is better than subgroup-size weighting. We further propose modified estimation function Equation 6 from Equation 5 to validate whether simple modification on the aggregation, based on subgroup size, is more appropriate for modeling the advantages. Averaging tracks higher accuracy on both MATH and AIME, with lower and more stable entropy and no unnecessary growth in response length. Size-weighting over-emphasizes large/easy subgroups and down-weights informative small/hard ones, whereas simple averaging preserves balanced signal; we therefore adopt averaging in the method and keep size-weighting only for discussion. ˆAi,t = (cid:80)J j=1 Gj ˆAi,t,j std({ ˆAi,t,j}J1) (cid:80)J j=1 Gj , ˆAi,t,j = Ri mean({Ri,j}Gj ), s.t. std({Ri,j}Gj ) = (7) Naïve rejection hurts performance Demonstrated in Equation 7, we also test the effectiveness of dynamic rejection sampling at the subgroup level as additional the subgroup hierarchy information is provided. Such DAPO-style subgroup rejection that discards all-positive or all-negative subgroups biases the feedback signal and weakens learning: accuracy lags, entropy is less favorable, and generations become longer. These extreme subgroups actually calibrate margins; removing them strips away high-signal cases, so we avoid subgroup-level rejection. Removing the root-group advantage does not degrade performance Using only the aggregated subgroup advantages while dropping the root-group term yields comparable curves, indicating that integrated subgroup signals can approximate the full-group optimization signal. This redundancy suggests the root term is not strictly necessary and is promising direction for further analysis of credit assignment. 10 Misaligned fallback degrades accuracy and inflates response length. With 7 1024 segments but 512-token random fallback, trajectories can share an abstract tree prefix while being token-misaligned. Figure 6 shows drop in AIME accuracy and sharp rise in response length for the misaligned variant, highlighting that token-aligned segments are important for stable optimization and precise stopping behavior. Figure 7 Study on the Online Depth-Segment under Setting the Group Size Weighted TreePO Advantage."
        },
        {
            "title": "4.3 Analysis of Segment Budget\nSetup We adopt the same subgroup-size weighted setting as in §4.2 to explore a the combination parameter\nof d × Lseg. Here we set the Lseg ∈ 128, 256, 512, 1024 and adjust the maximum depth to fit the response\nlength limit 7 × 1024 accordingly. The training curves are shown in Figure 7",
            "content": "Depthsegment trade-off, 14 512 is the sweet spot while 7 1024 underperforms. Under the group-size weighted advantage, 14 512 attains the highest final MATH/AIME accuracy; 56 128 and 28 256 are close, whereas 7 1024 lagsespecially on AIMEindicating that deeper trees with moderate segments provide stronger credit assignment than shallow rollouts with very long segments. Accuracylength coupling, Better accuracy comes with longer generations. The best-performing 14 512 also drives the largest growth in response length (and higher entropy), while 7 1024 keeps outputs shorter but sacrifices accuracy. This suggests online TreePO benefits from more exploratory, longer reasoning traces; shorter traces trade accuracy for brevity."
        },
        {
            "title": "4.4 Analysis of Probability-based Branching Assignment\nSetup With segment-level control, TreePO sampling provide a more feasible environment to study the\ntraining dynamics of the LLM bounding to the decoding progress. Stemming from the TreePO “w/ More Init\nDivergence” setting, we conduct a set of experiments to control modify the branching assignment at a given\ndepth d. Under this setting, the total branching budget 2d is assign among the active paths conditioned on\nthe log probabilities of their last segment, return from the inference engine. To prevent a sudden truncation\nof the search, the probabilities as passed through a softmax function with temperature set as 2.0, and all the\nactive search paths are guaranteed with at least one branching budget. The comparison between different\nbranching budget controls are shown in Figure 8, where the \"Low Prob Encourage\" suggests the paths with\nlower probability get more branching budget, and vice versa for \"High Prob Encourage\". We also try with a",
            "content": "11 Figure 8 Study on Probability-based Heuristic Tree Branching Budget Assignment. more sophisticated \"Low Prob Encourage\" setting that the temperature of the softmax function is schedule from 5.0 to 1.0 across the training. Monotonous Pattern Could Be Harmful. As shown in Figure 8, both static heuristic controls\"Low Prob Encourage\" and \"High Prob Encourage\"underperform the baseline and the scheduled variant. The \"Low Prob Encourage\" strategy, in particular, consistently yields the lowest accuracy on both benchmarks. This performance degradation is strongly correlated with significant increase in response length and entropy loss, suggesting that forcing the model to explore low-probability states leads to less efficient and coherent search trajectories across the whole training. Conversely, the \"High Prob Encourage\" setting, while performing better, results in the lowest entropy and shortest responses, indicating potentially overly greedy search that may prune promising, less obvious paths too early. Even when the scheduled \"Low Prob Encourage\" setting ensure similar branching assignment scheme at the beginning of the training, it still does not provide any advantages. Such Branching Control Does Not Show Significant Benefit Even with Higher Entropy. The most striking observation from our study is the disconnect between search diversity and task performance. The \"Low Prob Encourage\" setting was explicitly designed to increase exploration by allocating more resources to less likely search paths. This is reflected in its entropy loss, which is substantially higher than all other methods throughout training. However, this artificially inflated entropy does not translate into better results. Instead, it correlates with the worst performance on both benchmarks. This suggests that merely forcing the model to explore more diverse paths is not beneficial; the exploration must be meaningful. In this case, allocating budget to low-probability segments appears to push the model into irrelevant or erroneous reasoning paths, leading to longer, less effective solutions, as evidenced by the Response Length plot. The baseline maintains more moderate entropy level, which proves more effective for complex reasoning tasks, indicating it strikes better intrinsic balance between exploration and exploitation."
        },
        {
            "title": "4.5 Compute Scaling for Tree Sampling\nSetup As the sampling mechanism has been modified, the scaling curves along compute do not necessarily\nfollow the same trend as sequential sampling. To investigate this, we analyze the test-time compute scaling of\nTreePO by evaluating model performance under various computational budgets, as shown in Figure 9. Note",
            "content": "12 Figure 9 Test-time Compute Scaling of TreePO Sampling on the Aggregated Benchmark. The x-axis represents the compute budget on log scale, while the y-axis shows average performance. Each curve corresponds to different tree divergence factor = 2, 4, 8. The results illustrate that larger divergence factor can achieve higher peak performance at the cost of larger compute budget, revealing trade-off between the exploration strategy and computational cost that distinguishes it from the scaling behavior of conventional sequential sampling. that the number rollout starts from when calculating the compute curves. Distinct Rules The experiment varies the tree divergence factor (tree_div in Figure 9), which controls the number of branches generated at each divergence point, to observe its effect on the performance-compute trade-off. The results show that all configurations follow predictable scaling pattern: performance improves with increased compute before eventually reaching point of diminishing returns, which is consistent with established inference scaling observations. However, the key distinction from conventional sequential sampling becomes apparent when comparing the different divergence strategies. In sequential sampling, scaling compute is typically achieved by increasing the number of independent samples (N), which generally traces single performance-compute curve. In contrast, TreePO generates family of scaling curves, where each curve corresponds to different internal search strategy controlled by d. At lower compute budgets, smaller divergence factor (d = 2) is more efficient, achieving better performance for less cost. As the compute budget increases, wider search strategies (d = 4 and = 8) become superior, with = 8 ultimately reaching the highest peak performance. This demonstrates that the optimal TreePO sampling strategy is dependent on the available compute budget, allowing for more flexible \"compute-optimal inference\". Rather than simply scaling the number of samples, one can select the optimal tree structure to maximize performance for given computational constraint."
        },
        {
            "title": "5 Related Work",
            "content": "Efficient Sampling. Recent work on efficient sampling for RL and inference concentrates on making the rollout loop lighter by batching many completions together, re-using the prompt KV-cache and hiding latency behind parallel decoding; typical examples are [25] [26], [27], and [28], which all treat prompt as mini-batch and schedule tokens in groups so that GPUs stay busy. [10] keeps this idea but breaks large group into small micro groups, runs them with continuous inter-leaving, and adds length-aware scheduler; this saves memory and keeps the buffer fixed, yet it does not look at the partial trajectories while they are generated, introduces extra scheduling logic, and leaves the advantage estimator untouched [10]. [9] improves wall-time by cutting every sampled chain after short window and back-propagating early; the price is that long-range information is lost and credit assignment becomes harder. Segment-level Modeling. second line of research studies reinforcement learning with tree search. Recent systems such as [29], [26], [11] and [12] build explicit trees and use them to explore many reasoning branches in one rollout, giving denser feedback than plain chain sampling [11, 12, 26, 29]. TreeRL couples on-policy tree expansion with process-level rewards, but its trees stay shallow and the algorithm rolls one full answer 13 to compute log-probabilities before it can branch again, which doubles the running time [11]. [30] adopts an unconstrained tree and progress advantage similar to Monte-Carlo returns; while this brings simple tree-based update, it lacks depth control and is not validated against frozen base policy. Similarly, ARPO [31] apply segment-level entropy-guided divergence strategy based on the finished tool call trajectories, analogical to the FR3E algorithm [32] in math domain."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduced TreePO, reinforcement learning framework designed to address the computational inefficiency and exploration instability in training large language models for complex reasoning. By reformulating on-policy rollouts as segment-based tree search and using hierarchical advantage estimator, TreePO significantly reduces reasoning computational costs while improving training stability and maintaining strong performance. The efficiency and structural modeling of TreePO open promising avenues for scaling reinforcement learning to more complex, long-horizon tasks such as multi-turn dialogue, tool use, and multi-agent systems."
        },
        {
            "title": "Authorship",
            "content": "Yizhi Li 1,3*, Qingshui Gu 1*, Zhoufutu Wen 2*, Ziniu Li 2, Tianshun Xing 1, Shuyue Guo 1, Tianyu Zheng 1, Xin Zhou 2, Xingwei Qu 1,2,3, Wangchunshu Zhou 1, Zheng Zhang 2, Wei Shen 2, Qian Liu 1, Chenghua Lin3, Jian Yang1, Ge Zhang1, 2, Wenhao Huang2 1M-A-P 2Bytedance Seed 3The University of Manchester * Equal contribution Corresponding Author"
        },
        {
            "title": "References",
            "content": "[1] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [2] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [3] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [4] Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: simple, In International effective, and efficient reinforcement learning method for aligning large language models. Conference on Machine Learning, pages 2912829163. PMLR, 2024. [5] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [6] Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In European conference on machine learning, pages 282293. Springer, 2006. [7] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. [8] Maciej Świechowski, Konrad Godlewski, Bartosz Sawicki, and Jacek Mańdziuk. Monte carlo tree search: review of recent modifications and applications. Artificial Intelligence Review, 56(3):24972562, 2023. [9] Tiantian Fan, Lingjun Liu, Yu Yue, Jiaze Chen, Chengyi Wang, Qiying Yu, Chi Zhang, Zhiqi Lin, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Bole Ma, Mofan Zhang, Gaohong Liu, Ru Zhang, Haotian Zhou, Cong Xie, Ruidong Zhu, Zhi Zhang, Xin Liu, Mingxuan Wang, Lin Yan, and Yonghui Wu. Truncated proximal policy optimization, 2025. [10] Liangyu Wang, Huanyi Xie, Xinhai Wang, Tianjin Huang, Mengdi Li, and Di Wang. Infinite sampling: Efficient and stable grouped rl training for large language models, 2025. [11] Zhenyu Hou, Ziniu Hu, Yujiang Li, Rui Lu, Jie Tang, and Yuxiao Dong. Treerl: Llm reinforcement learning with on-policy tree search. arXiv preprint arXiv:2506.11902, 2025. [12] Yiran Guo, Lijie Xu, Jie Liu, Dan Ye, and Shuang Qiu. Segment policy optimization: Effective segment-level credit assignment in rl for large language models, 2025. [13] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [14] Jian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models. arXiv preprint arXiv:2501.03262, 2025. [15] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. [16] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. [17] Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp. notion.site/simplerl-reason, 2025. Notion Blog. 15 [18] Michael Luo, Sijun Tan, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, passing o1-preview with 1.5b model by scaling rl. DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Surhttps://pretty-radio-b75.notion.site/ and Ion Stoica. Deepscaler: [19] MAA. Aime 2024 problems, 2024. Accessed: 2025-05-11. [20] MAA. Amc 2023 problems, 2023. Accessed: 2025-05-11. [21] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. [22] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [23] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [24] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [25] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models, 2025. [26] Coleman Hooper, Sehoon Kim, Suhong Moon, Kerem Dilmen, Monishwaran Maheswaran, Nicholas Lee, Michael W. Mahoney, Sophia Shao, Kurt Keutzer, and Amir Gholami. Ets: Efficient tree search for inference-time scaling, 2025. [27] Zhen Zheng, Xin Ji, Taosong Fang, Fanghao Zhou, Chuanjie Liu, and Gang Peng. Batchllm: Optimizing large batched llm inference with global prefix sharing and throughput-oriented token batching, 2025. [28] Ozgur Guldogan, Jackson Kunde, Kangwook Lee, and Ramtin Pedarsani. Multi-bin batching for increasing llm inference throughput, 2024. [29] Kaiwen Wang, Jin Peng Zhou, Jonathan Chang, Zhaolin Gao, Nathan Kallus, Kianté Brantley, and Wen Sun. Value-guided search for efficient chain-of-thought reasoning, 2025. [30] Xingzhou Lou, Junge Zhang, Jian Xie, Lifeng Liu, Dong Yan, and Kaiqi Huang. Spo: Multi-dimensional preference sequential alignment with implicit reward modeling, 2024. [31] Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, and Zhicheng Dou. Agentic reinforced policy optimization, 2025. [32] Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu Wen, Chenghua Lin, Wenhao Huang, Qian Liu, Ge Zhang, and Zejun Ma. First return, entropy-eliciting explore, 2025."
        }
    ],
    "affiliations": [
        "ByteDance",
        "M-A-P",
        "UoM"
    ]
}