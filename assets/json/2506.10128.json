{
    "paper_title": "ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs",
    "authors": [
        "Xiyao Wang",
        "Zhengyuan Yang",
        "Chao Feng",
        "Yongyuan Liang",
        "Yuhang Zhou",
        "Xiaoyu Liu",
        "Ziyi Zang",
        "Ming Li",
        "Chung-Ching Lin",
        "Kevin Lin",
        "Linjie Li",
        "Furong Huang",
        "Lijuan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has shown great effectiveness for fine-tuning large language models (LLMs) using tasks that are challenging yet easily verifiable, such as math reasoning or code generation. However, extending this success to visual perception in vision-language models (VLMs) has been impeded by the scarcity of vision-centric tasks that are simultaneously challenging and unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption Hallucination Critic), an RL proxy task that trains VLMs to localize a subtle, synthetic visual hallucination injected into paragraphs of human-written image captions. Starting from a 200-word captions, we inject a single, subtle visual description error-altering a few words on objects, attributes, counts, or spatial relations-and task the model to pinpoint the corrupted span given the image and the modified caption. This formulation preserves the full perceptual difficulty while providing a binary, exact-match reward that is easy to compute and unambiguous. Models trained with the ViCrit Task exhibit substantial gains across a variety of VL benchmarks. Crucially, the improvements transfer beyond natural-image training data to abstract image reasoning and visual math, showing promises of learning to perceive rather than barely memorizing seen objects. To facilitate evaluation, we further introduce ViCrit-Bench, a category-balanced diagnostic benchmark that systematically probes perception errors across diverse image domains and error types. Together, our results demonstrate that fine-grained hallucination criticism is an effective and generalizable objective for enhancing visual perception in VLMs."
        },
        {
            "title": "Start",
            "content": "ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Xiyao Wang1,2 , Zhengyuan Yang2(cid:95) , Chao Feng3 Yongyuan Liang1, Yuhang Zhou1, Xiaoyu Liu1, Ziyi Zang4, Ming Li1 Chung-Ching Lin2, Kevin Lin2, Linjie Li2, Furong Huang1, Lijuan Wang2 1University of Maryland, College Park 2Microsoft 3University of Michigan 4Cardiff University"
        },
        {
            "title": "First Authors",
            "content": ""
        },
        {
            "title": "Equal Advising",
            "content": "(cid:95)"
        },
        {
            "title": "Project Lead",
            "content": "Reinforcement learning (RL) has shown great effectiveness for fine-tuning large language models (LLMs) using tasks that are challenging yet easily verifiable, such as math reasoning or code generation. However, extending this success to visual perception in visionlanguage models (VLMs) has been impeded by the scarcity of vision-centric tasks that are simultaneously challenging and unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption Hallucination Critic), an RL proxy task that trains VLMs to localize subtle, synthetic visual hallucination injected into paragraphs of human-written image captions. Starting from 200-word captions, we inject single, subtle visual description erroraltering few words on objects, attributes, counts, or spatial relationsand task the model to pinpoint the corrupted span given the image and the modified caption. This formulation preserves the full perceptual difficulty while providing binary, exact-match reward that is easy to compute and unambiguous. Models trained with the ViCrit Task exhibit substantial gains across variety of VL benchmarks. Crucially, the improvements transfer beyond naturalimage training data to abstract image reasoning and visual math, showing promises of learning to perceive rather than barely memorizing seen objects. To facilitate evaluation, we further introduce ViCrit-Bench, category-balanced diagnostic benchmark that systematically probes perception errors across diverse image domains and error types. Together, our results demonstrate that fine-grained hallucination criticism is an effective and generalizable objective for enhancing visual perception in VLMs. Date: Jun 11, 2025 Code Repository: https://github.com/si0wang/ViCrit Model Weights: https://huggingface.co/collections/russwang/ViCrit ViCrit Training Dataset: https://huggingface.co/datasets/zyang39/ViCrit-Train ViCrit-Bench: https://huggingface.co/datasets/russwang/ViCrit-Bench 5 2 0 2 1 1 ] . [ 1 8 2 1 0 1 . 6 0 5 2 : r 1. Introduction Reinforcement learning (RL) has recently emerged as dominate paradigm (Guo et al., 2025, Jaech et al., 2024) for fine-tuning large language models (LLMs) when training tasks are both challenging and automatically verifiable. Successful examples include mathematical reasoning tasks with concise numerical answers (Hendrycks et al., 2021, of America, 2024), and software engineering problems (Zheng et al., 2023, Miserendino et al., 2025) whose correctness can be checked in sandboxed environment. By focusing on tasks that strike this balancesufficiently challenging to have room for improvements yet straightforward to Corresponding author(s): Xiyao Wang xywang@umd.edu; Zhengyuan Yang zhengyang@microsoft.com ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Figure 1: Overview of the ViCrit framework. Starting from high-quality imagecaption pairs, we synthetically inject visual hallucinations by minimally altering noun phrases. The model is trained to localize incorrect spans in the caption given the image, receiving verifiable reward through exact string matching. This finegrained perceptual objective improves visual perception in vision-language models (VLMs) and generalizes to downstream reasoning tasks across diverse visual domains. grade deterministicallyRL can explore the solution space effectively, extract genuinely useful strategies, and transfer those gains to broader domains. Despite its success in textual reasoning, RL training with verifiable rewards has yet to demonstrate comparable significance in improving the visual perception abilities of visionlanguage models (VLMs). This is largely due to the lack of vision-centric tasks that are both perceptually challenging and automatically gradable. Whereas multi-hop math problems naturally compress numerous premises into single verifiable answer, the semantic elements within an image rarely collapses into such tidy question-answer pair. Even advanced visual-question-answering benchmarks (Antol et al., 2015, Hudson and Manning, 2019, Singh et al., 2019, Marino et al., 2019) often probe only fragments of scene, allowing shallow perception to suffice. Attempts to increase the perceptual difficulty, such as exhaustive image captioning that enumerates every visual element (Pont-Tuset et al., 2020, Deitke et al., 2024, Lian et al., 2025, Chen et al., 2024a, Awadalla et al., 2024, Wu et al., 2024a), yield paragraph-length outputs (200+ words) that are nearly impossible to grade unambiguously. The central challenge, therefore, is to craft task that forces the model to perceive the full scene yet produces concise, deterministically verifiable response. To bridge this gap, we propose ViCrit (Visual Caption Hallucination Critic), reinforcement learning proxy task that offers both perceptual difficulty and evaluation simplicity. ViCrit trains VLMs to localize synthetic visual hallucinations injected into paragraph-length image captions. It is designed to be both challenging, requiring fine-grained visual perception, and verifiable, enabling rule-based deterministic reward signals for scalable RL training. As shown in Figure 1, the task begins with human-annotated detailed image captions with more than 200 words (Deitke et al., 2024), and synthetically injects visual hallucinations. Such subtle errors misdescribe object, attribute, count, scene text, or spatial relation as its visually similar alternative. The model is trained to act as critic: given an image and its corrupted caption, it must identify the specific 2 ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs tokens that are incorrect. This token-level span detection can be easily graded via string matching yet requires fine-grained visual perception across the entire image, encouraging the model to internalize robust visual perception strategies extracted during the RL exploration trajectories. Training Qwen2.5-VL-7B-Instruct and 72B-Instruct with the proposed ViCrit RL task yields consistent gains across ten vision-language benchmarks. In addition to better hallucination benchmark results, these improvements extend well beyond the natural-image domain seen during ViCrit RL training, onto abstract image reasoning and visual math: Qwen2.5-VL-72B-Instruct improves from 35.2% to 40.1% on MathVision (Wang et al., 2024a), from 61.3% to 65.8% on VLMsAreBlind (Rahmanzadehgervi et al., 2024), and from 45.5% to 49.4% on Charxiv (Wang et al., 2024d). These cross-domain improvements indicate that the learned perceptual strategies transfer effectively to general VL domains. By training models to pinpoint fine-grained errors, ViCrit encourages the development of internal perception strategies that cross-check textual claims against visual evidence. Unlike supervised fine-tuning on captioning data (Sariyildiz et al., 2020, Desai and Johnson, 2021, Tschannen et al., 2023), which can lead to surface-level memorization, our RL task rewards perceptual correctness and penalizes hallucinations directly. As result, the model moves beyond merely memorizing the seen object lists, towards learning to decide how to perceive an image. Comprehensive analyses on how ViCrit-induced chain-of-thoughts generalize to broad spectrum of downstream VLM tasks further reveals the effectiveness and working mechanism of the ViCrit RL training. In addition to ViCrit training, we present benchmark named ViCrit-Bench for evaluating VLMs on hallucination detection. We group images into four categories and hallucination types into eight fine-grained hallucination classes, enabling detailed diagnostic analysis. We then manually curate set of images selected from PixMo-Cap (Deitke et al., 2024) and inject eight types of hallucinations into their corresponding captions. This process results in high-quality, fine-grained, and highly challenging hallucination detection benchmark, containing 607 samples. The benchmark supports zero-shot evaluation and exposes clear correlations with downstream perception tasks, making it powerful probe of VLMs perception limitations. We benchmark range of state-of-the-art open-source and closed-source vision-language models on ViCrit-Bench. Even proprietary systems such as OpenAI-o3 and Gemini-2.5-Pro achieve only 47.7% and 45.2% accuracy. After in-domain reinforcement learning with the ViCrit task, Qwen2.5-VL-72B attains an improved accuracy of 43.0%. The gains are uniform across all four image categories and are especially pronounced on document and abstract images, highlighting the efficacy of ViCrit-based RL for strengthening generalizable visual perception. Our contributions are summarized as follows: We introduce ViCrit, an RL task for visual perception that requires VLMs to identify token-level visual hallucinations in paragraph-length image captions. The task is both perceptually challenging and automatically verifiable, enabling scalable RL training with precise, unambiguous supervision. Training VLMs with the ViCrit Task significantly enhances their performance on wide range of VL benchmarks. The improvements also generalize to other image domains such as abstract image reasoning and visual math, which shows the advantage of ViCrit incentivizing models to verify visual detail against text, rather than merely memorize seen objects. We present ViCrit-Bench that systematically probes eight hallucination types across four image domains. The benchmark supports zero-shot evaluation and serves as diagnostic tool for assessing fine-grained visual perception capabilities in VLMs. Furthermore, its scores track averaged VLM accuracy monotonically, making it strong indicator of the overall VLM performance. 3 ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs 2. Related Work Large language model reasoning. Prompting-based Chain-of-thought methods (Wei et al., 2022, Kojima et al., 2022) first explored the reasoning abilities of large language models (LLMs) (Brown et al., 2020, Chowdhery et al., 2023) by eliciting chains of intermediate thoughts, markedly improving arithmetic and commonsense benchmarks (Cobbe et al., 2021, Patel et al., 2021, Koncel-Kedziorski et al., 2016). Subsequent decoding strategies aim to further improve test-time performance with extra test-time computation. For example, Self-Consistency sampling (Wang et al., 2022b) that votes over multiple thought paths to boost reliability. Expanding beyond linear traces, structured search frameworks like Tree-of-Thoughts (Yao et al., 2023) and Graph-of-Thoughts (Jin et al., 2024) let the model explore branching space of candidate thought states before committing to an answer. Studies (Muennighoff et al., 2025) also explore hacking the thought process to generate long CoT that is beyond the CoT length distribution. Moving from test-time scaling to training, process reward models (Lightman et al., 2023, Uesato et al., 2022, Wang et al., 2023) grade each reasoning step rather than only final answers, which can be coupled with Monte Carlo Tree Search (Xie et al., 2024) for fine-grained value estimates. Most recently, large-scale reinforcement learning (Guo et al., 2025, Jaech et al., 2024) with outcome-based rewards alone can induce emergent multi-step reasoning skills. The key to its success is challenging tasks that can be automatically verified, such that the RL can be effectively scaled up with minimal noise in its reward signals. The goal for this study is to find such tasks for VLMs visual perception. VLM reasoning. Based on the modern vision language models (OpenAI, 2023, Wang et al., 2022a, Liu et al., 2023, Hurst et al., 2024, Liu et al., 2024, Bai et al., 2025, Chen et al., 2024c, Tong et al., 2024, Li et al., 2024b, Yang et al., 2023a), recent studies explore the use of multimodal CoT to further improve vision-language reasoning tasks (Hao et al., 2025, Wang et al., 2024a, Lu et al., 2024, Yu et al., 2024a) with both grounded textual thoughts (Lu et al., 2022, Zhang et al., 2024c) and multimodal thoughts (Rose et al., 2023, Wu et al., 2024b, Fu et al., 2025). Techniques like rationale distillation and self-reflection further boost these models reasoning capabilities (Zhang et al., 2024b, Yang et al., 2023b, Zhou et al., 2024b, Wang et al., 2024b, Xiong et al., 2024, Wang et al., 2024c, Deng et al., 2024). Inspired by the success on outcome-based reward based RL in LLMs, recent studies (Deng et al., 2025, Huang et al., 2025, Meng et al., 2025b,a, Wang et al., 2025, Peng et al., 2025, Chen et al., 2025, Zheng et al., 2025, Ni et al., 2025) applies similar techniques to visual math and other visual-question-answering benchmarks. Despite the improvements in visual math and STEM questions, they still fall short of significantly advancing fine-grained visual perception. Visual-centric VLMs and reasoning. One threads of works aim to improve visual perception in VLMs via text-supervised visual representation learning (Sariyildiz et al., 2020, Desai and Johnson, 2021, Tschannen et al., 2023), which trains the model to generate good description of the image. This line of work show great promises with the recent success in obtaining ultra-descriptive image captions (Pont-Tuset et al., 2020, Deitke et al., 2024, Lian et al., 2025, Chen et al., 2024a, Awadalla et al., 2024, Betker et al., 2023, Wu et al., 2024a). However, supervised fine-tuning on captioning data may lead to superficial object memorization, while paragraph captioning task does not have reliable rewards for RL training. In this work, we present an RL proxy task to close this gap. 3. ViCrit RL Training Recent progress in outcome-based reinforcement learning shows that LLMs learn richer reasoning procedures when trained with hard questions whose answers can be unambiguously verified. The same recipe, however, is not immediately available to visual perception in VLMs. The traditional caption-supervision objective 4 ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Figure 2: Instead of asking the model to write paragraph-long caption that is hard to grade (e.g., the 200-word example above), ViCrit feeds the model an almost-correct caption containing single, deliberately inserted visual hallucination and trains it to locate that error. The short, token-level response is just as demanding in terms of visual perception, yet it is far easier to verify automatically. optimizes model for recalling fixed list of objects, but never for deciding where to look next. Our goal is therefore to turn visual perception into an RL problem whose reward (i) compels the model to interrogate every visual details and (ii) remains as cheap and deterministic to evaluate as in code or math. Examples of our proposed ViCrit task is shown in Figure 2. Instead of asking model to generate perfect, paragraph-length caption (200+ words), which is difficult to grade, we present it with an almost-correct caption containing single, synthetically injected visual error and reward the model for pinpointing the mistaken span (2 words). Solving this task is as hard as perfect captioning: critic that can reliably spot any hallucination must perceive and understand the entire scene; yet the answer collapses into few words that can be matched exactly. This simple reshape of the objective delivers the two missing ingredients for perception-centric RL: genuinely challenging perception task and an evaluation rule that reduces to simple string equality. 3.1. ViCrit Task Task description. For every training instance we start with an image and its exhaustive, human-annotated caption drawn from the PixMo-Cap dataset (Deitke et al., 2024), with an average caption length of 196 words. We then prompt GPT-4 (OpenAI, 2023) to select one object description within that 200-word-length paragraph and perturb it into visual hallucination o, such that the perturbed object is visually similar, semantically plausible, and without ambiguity. We also sample two examples from small set of manually crafted in-context examples when prompting the LLM. The complete prompt is in Appendix. The desired 5 ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs output is minimally modified caption that differs from by exactly one visual span (e.g., two words in Figure 2). We instruct for diverse types of selected objects and resulted hallucination o, such as object substitution, attribute flip, scene-text error, relation swap, etc.. After data generation we task the model to identity the visual hallucination given the image and caption C. positive reward is given if predicted words matches ground-truth o. Because the reward depends purely on exact string match, it is deterministic and easy to scale in RL training. Discussion on task difficulty. Perfectly performing the ViCrit task requires the model to perceive the entire visual scene, which is the same level of visual perception demanded of an oracle visual captioner that can exhaustively describe every image elements. Indeed, flawless ViCrit critic could be repurposed into such an oracle by iteratively proposing refinements to an image caption. Thus, ViCrit imposes the same visual perceptual requirements as paragraph captioning, yet its single-span output is easily verifiable, enabling scalable outcome reward based reinforcement learning. Data. We build the image caption starting from all samples in the PixMo-Cap dataset (Deitke et al., 2024). Filtering out the invalid image URLs yields 384K image caption pairs. We then prompt LLM to create the visual hallucination and use it to replace the original object description to create the minimally modified caption C. In the end, we collect 875K pairs of images and modified captions. 3.2. Model Training We use Qwen2.5-VL as the base VLM for experiments and finetune all model parameters via the ViCrit RL proxy task. We train the model with Group Relative Policy Optimization (GRPO) (Shao et al., 2024): ℒGRPO = i[min(Ai ρi, Ai clip(ρi, 1 ϵ, 1 + ϵ))], Ai = (ri r), ρi = πθ(yix) πθold (yix)"
        },
        {
            "title": "The sample reward ri",
            "content": "is computed based on deterministic string matching between the injected visual hallucination string and the model prediction ˆo: ranswer = { +1, 0, if == ˆo, otherwise."
        },
        {
            "title": "We relax the string matching",
            "content": "such that the model is not penalized for copying additional words before or after the selected span o, as long as they are an exact copy from the original caption C. In addition to answer correctness reward, we also follow the standard practice to instruct the model to follow specific prompt format, which group thoughts with special tokens <think>...</think> and final answers with special tokens boxed{}. The format reward rformat is 1 if it correctly uses the special format tokens and 0 otherwise. The final reward for sample is ri = 0.9 ranswer + 0.1 ormat . 4. ViCrit Benchmark Motivated by the substantial gains yielded by reinforcement learning with the ViCrit task, we hypothesize that zero-shot ViCrit accuracy also correlates with VLMs perception capability and can therefore anchor diagnostic benchmark. We thus present ViCrit-Bench, high-quality, fine-grained, and highly challenging benchmark for hallucination detection. In this section, we first present the image domains and hallucination task categories defined in ViCrit-Bench. We then describe the human annotation procedure and dataset construction pipeline. Finally, we provide comprehensive statistics and distributional insights of ViCrit-Bench. ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Figure 3: Data examples from ViCrit-Bench, which involve four image categories and eight visual hallucination types. We manually verify each images long caption, and carefully inject different kinds of proper visual hallucinations. 4.1. Image Domains and Hallucination Categories ViCrit-Bench partitions its images into four broad domains, each chosen to probe complementary slice of visual perception: (1) Natural images: everyday photos of landscapes, animals, people, and objects captured in the wild; (2) Documents: images dominated by structured content such as tables, charts, plots, diagrams, or dense textual screenshots; (3) Scene-textheavy images: images where scene text is appeared in the scene, such as street signs, memes, comic panels, and illustrative layouts; (4) Abstract images: images that do not directly depict real-world objects or scenes, but instead convey meaning through geometric shapes, symbols, color patterns, synthetic compositions, or artistic illustrations; these images emphasize structure, style, or conceptual abstraction rather than natural realism or textual content. We note that the training data distribution from the PixMo-Cap dataset contains dominated natural images. The annotation on subset of 4k randomly sampled PixMo-Cap images shows category percentage of 59%, 10%, 24%, and 7% for image domains 1-4, respectively. Human annotators thus go through additional candidate images to find the proper sources for categories 2 and 4. Based on the image domains above, we then systematically categorized all visual hallucinations into eight distinct hallucination task types, defined as follows: (1) Count: evaluates whether the quantity of objects or elements is incorrectly described; (2) Material: assesses the models ability to accurately recognize the material composition of objects; (3) Spatial: determines whether the spatial configuration and relative positioning of entities are misrepresented; (4) Color: examines the consistency between the described and actual color attributes of visual elements; (5) Object: identifies cases where objects are incorrectly classified into wrong semantic ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs categories; (6) Condition: checks whether the physical state, dynamic action, or emotional expression of entities is appropriately conveyed; (7) Shape: measures the accuracy of describing the geometric structure or contour of objects; (8) Text: verifies whether embedded textual content within the image is correctly detected and interpreted. 4.2. Annotation Pipeline All samples in ViCrit-Bench originate from PixMo-Cap, whose 200-word, detailed captions provide fertile substrate for hallucination synthesis. The construction pipeline proceeds in three stages. Stage 1: Image selection and caption sanitization. For the four image categories, we first employ OpenAIo3 model to perform an initial classification over the entire PixMo-Cap dataset, identifying candidate images that align with the definitions of natural images, document images, scene-textheavy images, and abstract images. Subsequently, human annotators manually filter the candidates and select 20 images for each hallucination task with each image category, ensuring images strictly adhere to the domain-specific criteria. Besides, given that some image captions in PixMo-Cap contain annotation errors, we first used o3 to review and automatically correct the captions, addressing the majority of semantic and factual issues. During the final annotation phase, each caption is individually reviewed and validated by human annotators to guarantee its accuracy and consistency. total of four human annotators are involved in this process. Stage 2: Hallucination injection. Each image category is assigned to dedicated annotator, who injects the selected hallucination types by surgically replacing single noun phrase. This is done by replacing noun phrases with semantically plausible but misleading alternatives that are visually similar, as shown in Figure 3. Each image is allowed to be modified with only one hallucination. For each image category, the number of images per hallucination task is capped at 20. However, in certain categories, some types of hallucinations may be inherently rare or difficult to instantiatefor example, material hallucinations in abstract imageswhich may result in fewer than 20 finalized examples for those specific tasks. Stage 3: Cross-validation. final round of cross-validation by two independent annotator ensures the correctness and clarity of the injected hallucinations across all task types. 4.3. Statistics Following the aforementioned image selection and hallucination injection procedures, the final ViCrit-Bench contains 607 images, each paired with manually verified and edited captions containing total of 607 fine-grained hallucination instances. The distribution of hallucination tasks across the dataset is illustrated in Figure 4. All hallucination task types, except for Material, are relatively balanced, each comprising around 13% the total instances. This reflects the comprehensive and well-balanced design of ViCrit-Bench. Due to its unique nature, the Material task appears mostly in first three categories, resulting in lower overall proportion of 7.9%. Figure 4: Hallucination task distribution of ViCrit-Bench. 4.4. Metric and Evaluation For each sample, we combine the image and corrupted caption with predefined evaluation prompt template (see Appendix A.2) to form the final evaluation model input. Given this prompt, VLM must locate 8 ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Table 1: Comparison between ViCrit-RL-7B and ViCrit-RL-72B with other open-source VLMs. After training on the ViCrit task using Qwen2.5-VL-7B-Instruct and Qwen2.5-VL-72B-Instruct as base models, hallucination rates are significantly reduced, achieving the best performance across all three hallucination benchmarks. Moreover, training on the ViCrit task substantially improves general vision-language performance. On eight general VL benchmarks, ViCrit-RL-72B achieves SOTA results on seven tasks, with the average accuracy increasing from 59.78 to 63.16. Hallucination Benchmark General benchamrk Model Size Model 7B 72B GPT-4o o1 Molmo-7B-D-0924 LLaVA-OneVision-7B InterVL2.5-8B Qwen2.5-VL-7B-Instruct ViCrit-RL-7B (Ours - Qwen2.5-7B) Molmo-72B-0924 LLaVA-OneVision-72B InterVL2.5-78B Qwen2.5-VL-72B-Instruct A R 36.7 35.0 29.2 28.0 25.2 -2.8 28.8 27.4 25.9 26.4 6.0 5.5 5.4 5. 4.5 -0.6 5.7 4.9 5.2 4.8 M 3.03 3.12 3.65 3. 3.77 +0.03 3.54 3.71 3.89 3.82 ViCrit-RL-72B (Ours - Qwen2.5-72B) 21.0 -5.4 3.9 -0.9 3.91 +0. s t i s o V M n r t i U r M V - i i o r r Avg. 63.8 73. 54.1 63.2 64.4 67.8 36.8 58.2 19.5 17.4 22.0 23.6 50.2 57.0 23.2 26.2 39.5 44.5 69.1 78. 40.2 48.8 54.9 50.6 64.7 52.6 61.7 62.8 61.7 69.1 59.2 57.5 68.8 66.0 50.4 57. 43.3 40.1 47.6 49.3 52.7 55.1 30.8 31.3 32.9 41.4 57.10 40.38 43.28 49.11 50.61 25. 70.7 53.01 61.9 +2.9 +2.1 +1.8 +1.4 +0.2 +1.1 +3.3 +6.4 +2.40 46.3 52.0 47.8 67.1 52. 61.1 67.5 72.3 74.8 24.7 29.3 34.9 35.2 30.9 39.1 51.7 53.3 48.3 56.8 68.7 63.4 58.4 66.1 68.9 68.4 65.5 63.7 72.3 76. 46.9 49.6 59.8 61.3 35.2 38.2 42.4 45.5 46.38 51.29 58.75 59.78 63.16 69.8 77.3 +2.5 +4.9 +6.5 +2.6 +1.4 +0.8 +4.5 +3.9 +3.38 49.4 65. 77.1 40.1 59.8 66.0 the hallucinated span inside as an open-ended QA task. prediction is considered correct if the models prediction ˆo exactly matches o. We take this string exact match accuracy as the metric for ViCrit-bench. 5. Experiments 5.1. Effectiveness of ViCrit RL Training We evaluate the effectiveness of ViCrit-based RL on various VL benchmarks. Through extensive comparisons with SOTA VLMs, we demonstrate the effectiveness of ViCrit as an RL training task and show that reinforcement fine-tuning on this task leads to general VL performance improvements. Baseline VLMs. We start from the Qwen2.5-VL-7B-Instruct and Qwen2.5-VL-72B-Instruct checkpoints. Applying RL training with the ViCrit task produces our ViCrit-RL-7B and ViCrit-RL-72B, respectively. Qwen2.5 models thus constitute our primary models of interest as well as fairly comparable baselines for ablation. For external comparison, we report benchmark results for three widely used open-source VLMs: Molmo (Deitke et al., 2024), LLaVA-OneVision (Li et al., 2024a), and InternVL2.5 (Chen et al., 2024c), including both their 7B-level and 72B-level variants. We also reference proprietary models include GPT-4o and o1. All training and evaluation is conducted with 880G A100 GPUs. Evaluation benchmarks. (i) Hallucination mitigation: we first quantify ViCrit trainings impact on dedicated visual hallucination benchmarks. (ii) Broad VLM generalization: we then examine if the perceptual skills instilled by ViCrit-based RL transfer to general visionlanguage benchmarks. (i) Hallucination mitigation. We adopt two widely used benchmarks: CHAIR (Rohrbach et al., 2018) ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs and MMHal (Sun et al., 2023). Specifically, CHAIR quantifies the proportion of hallucinated content in image captions. Following the setting in previous works (Zhou et al., 2024a,b), we randomly sample 500 images from the COCO Val2014 dataset and use prompts from the LLaVA-150k detailed description dataset, and . MMHal calculate CHAIR as follows: CHAIR serves as complementary benchmark for evaluating hallucination in VLMs on VQA tasks. We employ GPT-4 as the scoring model to assess the hallucination severity in model responses. = {captions with hallucinated objects} = {hallucinated objects} {all mentioned objects} {all captions} , CHAIR (ii) Broad generalization. We use 8 widely adopted VLM benchmarks covering mathematical reasoning (MathVista (Lu et al., 2024), MathVision (Wang et al., 2024a), MathVerse (Zhang et al., 2024a)), general knowledge (MMMU (Yue et al., 2024), MMStar (Chen et al., 2024b), MMVet (Yu et al., 2024a,b)), visual understanding (Blind (Rahmanzadehgervi et al., 2024)), and chart reasoning (ChartXiv (Wang et al., 2024d)). Finding 1: RL with the ViCrit task significantly reduce visual hallucination. The middle three columns of Table 1 compares our ViCrit-RL with other VLMs on visual hallucination benchmarks (Rohrbach et al., 2018, Sun et al., 2023). At the 7B scale, compared with the baseline Qwen2.5-VL7B, ViCrit-based RL training reduces CHAIRs and CHAIRi from 28.0 and 5.1 to 25.2 and 4.5, respectively. The MMHal increases to 3.77, which surpassing multiple 72B-level models. At the 72B scale, the improvement is even more pronounced: CHAIRs and CHAIRi reaches 21.0 and 3.9, and MMHal improves to 3.91, outperforming all SOTA VLMs across all three hallucination metrics. The consistency of the improvements across scales and benchmarks validates ViCrits effectiveness in reducing visual hallucination and improving perception across various description types. Finding 2: RL with the ViCrit task improves VL performance in general. Beyond curbing hallucinations, the right side of Table 1 shows that ViCrit-RL consistently lifts accuracy on the eight heterogeneous VLM benchmarks that constitute our general visionlanguage suite. Because the ViCrit proxy forces the model to verify every noun phrase against the image, it refines low-level perception and yields more faithful intermediate representations. These improvements appear to propagate to downstream reasoning tasks, with an averaged improvement of 2.4% on 7B scale and 3.4% on 72B scale. More importantly, the improvements generalize well onto the low-source training image domains. For example, the 72B model improves +4.9% on MathVision, +4.5% on VLMsareBlind and +3.9% on ChartXiv, despite math and abstract images only account for 7% of the PixMo-Cap training data, and 10% for chart and document images. This indicates that the model is not merely memorizing object lists but has learned transferable strategy for how to look at an image before generating text. We provide qualitative chain-of-thought analysis in Section 5.3 to probe this generalization pattern further. 5.2. ViCrit-Bench Results We benchmark broad range of SOTA VLMs on our ViCrit-Bench, which probes eight fine-grained visual hallucination types across four image domains. For closed-source models, we evaluate OpenAI-GPT-series which includes 4o, o1 and o3, and Gemini-series which includes 2.0-Flash, 2.5-Flash, and 2.5-Pro. For opensource models, we follow the same experimental setup as Section 5.1 and evaluate Molmo, LLaVA-OneVision, InternVL2.5, and Qwen2.5-VL series. Table 2 shows that ViCrit-Bench is markedly challenging: the best model o3 reaches only 47.7% correctness, while the best open-source model Qwen2.5-VL-72B-Instruct achieves 42.4%. Spatial hallucination emerges as the dominant failure mode, with the top-performing model achieving 10 ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Table 2: Evaluation results of range of closed-source and open-source VLMs on ViCrit-Bench. The results indicate that ViCrit-Bench poses substantial challenge to current modelseven the best-performing model, OpenAI-o3, achieves only 47.7 accuracy. After reinforcement fine-tuning on the ViCrit task, ViCrit-RL-72B achieves the highest accuracy of 43.0 over all opensouce models on the benchmark. Moreover, we observe strong correlation between performance on ViCrit-Bench and the average accuracy on general visionlanguage tasks for open-source models. Models that score higher on ViCrit-Bench tend to perform better on general benchmarks, suggesting that ViCrit-Bench serves as reliable indicator of overall reasoning and understanding capabilities. Hallucination Type Image Type T e . Models OpenAI-GPT-4o OpenAI-o1 OpenAI-o3 Gemini-2.0-Flash Gemini-2.5-Flash Gemini-2.5-Pro Molmo-7B-D-0924 LLaVA-OneVision-7B InternVL-2.5-8B Qwen-2.5-VL-7B ViCrit-RL-7B 40.48 43.28 49.11 50.61 53.01 Molmo-72B 46.38 LLaVA-OneVision-72B 51.29 58.75 InternVL-2.5-78B 59.78 Qwen-2.5-VL-72B ViCrit-RL-72B 63.16 j Overall l a t a p t C h e i n a a a I m D m a x n S a b a 23.3 45.8 47.7 19.3 44.4 45.2 9.6 12.4 20.0 21.9 35. 18.2 24.5 32.7 42.4 43.0 16.25 43.75 17.50 47.50 48.10 60.00 46.25 67.50 22.50 30.00 60.00 41.25 68.75 50.00 66.67 13.58 27.08 25.93 64.58 22.22 50.00 62.50 54.43 60.42 18.99 39.58 16.25 6.17 50.63 60.42 28.40 47.50 39.24 43.75 20.99 18.75 20.25 26.58 40.51 57.69 32.91 22.78 15.19 32.91 30. 13.75 40.00 50.00 15.09 26.17 27.04 25.00 42.14 47.26 53.46 39.29 46.31 51.57 49.29 43.40 16.11 16.35 19.29 25.16 42.86 48.43 40.94 44.65 46.43 52.83 34.23 46.54 25.00 20.00 26.25 30.00 8.75 11.25 11.25 23.75 10.42 10.42 25.00 39.58 9.88 12.35 22.22 9. 5.00 6.25 12.50 12.50 3.75 15.00 15.00 8.75 6.33 12.65 30.38 45.57 7.59 10.13 18.99 12.66 5.66 17.61 27.04 35.22 13.57 7.86 15.00 12. 8.81 9.43 13.21 20.13 10.74 14.09 24.16 18.12 47.50 46.25 68.75 6. 38.75 37.50 21.52 31.65 41.51 30. 38.99 30.87 11.25 20.00 21.25 36.25 36.25 42.50 46.25 57.50 60.00 6.25 11.11 10.42 17.50 22.22 25.00 45.83 20.00 22.22 58.33 28.40 26.25 40. 13.75 21.25 26.25 46.25 56.25 25.32 22.78 46.84 54.43 27.85 24.05 37.97 37.97 23.90 30.82 38.99 47.17 18.57 19.29 29.29 40. 13.21 24.53 31.45 44.03 16.78 22.82 30.20 36.91 25.32 39.24 47.80 41.43 44. 37.58 48.75 70.83 17.28 only 28.40%, whereas object hallucination and material hallucination looks easier on paper. However, the higher number is because of an easier question subset on foreground objects, and it remains nontrivial to perform perfectly on any one of these eight classes (cf. the corgi object example in Figure 2.) With respect to image types, Document Image and Abstract Image are the most challenging ones, as nearly all models exhibited significantly lower accuracy on these two types compared to image types. Furthermore, RL training with ViCrit task leads to substantial gains on ViCrit-Bench. ViCrit-RL-7B and ViCrit-RL-72B achieves an improved accuracy of 35.6% and 43.0%, respectively. Among four image categories, the largest gains occur on the Document and Abstract image domains, precisely the areas where baseline models struggle. This improvement also foreshadows the models enhanced performance on downstream benchmarks involving multimodal mathematical reasoning and chart understanding after ViCrit-based training, as shown in Table 1. However, we observe significant drop in accuracy for the Spatial and Text tasks after RL training. We attribute this to data imbalance in the construction of the training set. Furthermore, RL training led to substantial performance gains for the 7B model on ViCrit-Bench, whereas the improvements for the 72B model are relatively marginal. We hypothesize that this is due to the constructed training set being insufficiently challenging for Qwen-2.5-VL-72B, which already possesses strong visual perception capabilities. To further enhance the performance of the 72B model, more complex and demanding data may be required. ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Figure 5: Correlation between average VLM task performance and ViCrit-Bench performance (Task Avg. and Overall columns in Table 2). Each point represents different model, and the fitted linear regression line highlights strong positive relationship, indicating that better ViCrit-Bench results are associated with higher stronger VLM capabilities. Finding 3: ViCrit-Bench results well foreshadow the VLM performance. Beyond the raw results, Table 2 exposes monotonic link between the ViCrit-Bench results and the general VLM performance. Models rank in precisely the same order in the ViCrit-Bench Overall column, as in the averaged VLM performance column quoted from Table 1. Figure 5 demonstrates strong positive linear correlation between average VLM task performance and ViCrit-Bench scores. ViCrit-Bench scores rise almost linearly with models average performance across eight vision-language tasks (r = 0.96), showing that the benchmark effectively evaluates the visual perception, and is strong proxy for overall VLM capability. In the 7B class, performance rises step-wise from Molmo-7B through LLaVA-OneVision-7B, InternVL 2.5-8B, and Qwen2.5-VL-7B, to ViCrit-RL-7B that tops every metric. The pattern repeats at the 72B scale where ViCrit-RL-72B achieves the best performance on both ViCrit-bench and general VLM evaluation. This finding echoes our original motivation in building ViCrit-bench, with the hypothesis that ViCrit accuracy could foreshadow VLMs perception capability as well as the overall multimodal performance. Furthermore, the consistency of the ordering across scales suggests that ViCrit-RLs better hallucination performance in Finding 1 is not by-product of merely training on hallucination detection. Rather, models that learn to cross-check textual claims against visual evidence via ViCrit also perceive and reason better on general VL problems, such as chart, math, and abstract images. 5.3. Qualitative Results We showcase two representative cases (Figure 6) that reveal how training on the ViCrit task sharpens visual perception and, consequently, improving VLM performance. Example 1. ViCrit-RL-72B is able to accurately identify all objects in the image in clockwise order, capturing detailed attributes such as color and shape, and successfully making the correct calculation. Example 2. ViCrit-RL-72B correctly identifies all relevant 12 ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Figure 6: Two examples demonstrate the behavioral differences between models before and after training with the ViCrit task. It can be seen that ViCrit-RL-72B pays closer attention to image details and arrives at the correct final reasoning through its enhanced perception capabilities. visual detailsincluding colors and the number of edges of each objectand uses this information to derive the correct answer. In contrast, Qwen2.5-VL-Instruct fails to capture the complete visual content due to its limited perception ability, leading to incorrect reasoning. These examples demonstrate that training on ViCrit task significantly improves visual perception, which is crucial foundation for enhancing the VLM performance. 6. Conclusion We have presented ViCrit, an RL proxy task that trains VLMs to pinpoint fine-grained, synthetically injected visual hallucinations in paragraph-length captions. Because each targeted span is unambiguously verifiable, ViCrit provides challenging yet noise-free reward signal that compels models to internalize stronger perceptual strategies, yielding consistent gains across broad suite of benchmarks. Furthermore, we release ViCrit-Bench, carefully curated dataset that enables rigorous evaluation of VLM perception. We hope this new task will spark further breakthroughs in multimodal RL, from standalone VLMs to end-to-end-trained tool-augmented multimodal agents."
        },
        {
            "title": "Acknowledgment",
            "content": "Wang, Liang, Zhou, Liu, and Huang are supported by DARPA Transfer from Imprecise and Abstract Models to Autonomous Technologies (TIAMAT) 80321, DARPA HR001124S0029-AIQ-FP-019, DOD-AFOSR-Air Force Office of Scientific Research under award number FA9550-23-1-0048, National Science Foundation NSF-IIS-2147276 FAI, National Science Foundation NAIRR240045, National Science Foundation TRAILS Institute (2229885). Private support was provided by Peraton. 13 ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs"
        },
        {
            "title": "References",
            "content": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433, 2015. Anas Awadalla, Le Xue, Manli Shu, An Yan, Jun Wang, Senthil Purushwalkam, Sheng Shen, Hannah Lee, Oscar Lo, Jae Sung Park, et al. Blip3-kale: Knowledge augmented large-scale dense captions. arXiv preprint arXiv:2411.07461, 2024. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision language models with less than $3. https://github.com/Deep-Agent/R1-V, 2025. Version v0.2 released 3 Feb 2025. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024a. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024b. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024c. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, Quanquan Gu, James Zou, Kai-Wei Chang, and Wei Wang. Enhancing large vision language models with self-training on image comprehension. Advances in Neural Information Processing Systems, 37:131369131397, 2024. 14 ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement, 2025. URL https: //arxiv.org/abs/2503.17352. Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1116211173, 2021. Xingyu Fu, Minqian Liu, Zhengyuan Yang, John Corring, Yijuan Lu, Jianwei Yang, Dan Roth, Dinei Florencio, and Cha Zhang. Refocus: Visual editing as chain of thought for structured image understanding. arXiv preprint arXiv:2501.05452, 2025. URL https://arxiv.org/abs/2501.05452. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, and many others. Deepseekr1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. URL https://arxiv.org/abs/2501.12948. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. Includes AIME and other competition-level problems. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models, 2025. URL https://arxiv.org/abs/2503.06749. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang, Suhang Wang, Yu Meng, and Jiawei Han. Graph chain-of-thought: Augmenting large language models by reasoning on graphs. arXiv preprint arXiv:2404.07103, 2024. URL https://arxiv.org/abs/2404. 07103. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. Mawps: math word problem repository. In Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, pages 11521157, 2016. ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al. Multimodal foundation models: From specialists to general-purpose assistants. Foundations and Trends in Computer Graphics and Vision, 16(1-2):1214, 2024b. Long Lian, Yifan Ding, Yunhao Ge, Sifei Liu, Hanzi Mao, Boyi Li, Marco Pavone, Ming-Yu Liu, Trevor Darrell, Adam Yala, et al. Describe anything: Detailed localized image and video captioning. arXiv preprint arXiv:2504.16072, 2025. Hunter Lightman, Karl Cobbe, Vineet Kosaraju, Yura Burda, Harri Edwards, Jan Leike, and Ilya Sutskever. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. URL https://arxiv.org/abs/2305. 20050. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge, 2019. URL https://arxiv.org/abs/1906. 00067. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025a. URL https://arxiv.org/abs/2503.07365. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025b. Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can frontier llms earn $1 million from real-world freelance software engineering? arXiv preprint arXiv:2502.12115, 2025. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. 16 ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Minheng Ni, Zhengyuan Yang, Linjie Li, Chung-Ching Lin, Kevin Lin, Wangmeng Zuo, and Lijuan Wang. Point-rft: Improving multimodal reasoning with visually grounded reinforcement finetuning. arXiv preprint arXiv:2505.19702, 2025. Mathematical Association of America. American invitational mathematics examination (aime) 2024: Competition problems. https://artofproblemsolving.com/wiki/index.php/2024_AIME_I_Problems, 2024. Accessed 13 May 2025. OpenAI. Gpt-4v(ision) system card. 2023. URL https://api.semanticscholar.org/CorpusID: 263218031. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191, 2021. Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. URL https://arxiv.org/abs/2503.07536. Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pages 647664. Springer, 2020. Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind. In Proceedings of the Asian Conference on Computer Vision, pages 1834, 2024. Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156, 2018. Daniel Rose, Vaishnavi Himakunthala, Andy Ouyang, Ryan He, Alex Mei, Yujie Lu, Michael Saxon, Chinmay Sonar, Diba Mirza, and William Yang Wang. Visual chain of thought: bridging logical gaps with multimodal infillings. arXiv preprint arXiv:2305.02317, 2023. Mert Bulent Sariyildiz, Julien Perez, and Diane Larlus. Learning visual representations with caption annotations. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part VIII 16, pages 153170. Springer, 2020. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, and Lucas Beyer. Image captioners are scalable vision learners too. Advances in Neural Information Processing Systems, 36:46830 46855, 2023. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022a. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024a. URL https://openreview. net/forum?id=QWTCcxMpPA. Peiyi Wang, Lei Li, Zhihong Shao, Rui Xia, Damai Dai, Yifei Li, De Li Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. URL https://arxiv.org/abs/2312.08935. Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Furong Huang, et al. Enhancing visual-language modality alignment in large vision language models via self-improvement. arXiv preprint arXiv:2405.15973, 2024b. Xiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Scaling inference-time search with vision value model for improved visual comprehension. arXiv preprint arXiv:2412.03704, 2024c. Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b. URL https://arxiv.org/abs/2203.11171. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024d. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: generative region-to-text transformer for object understanding. In European Conference on Computer Vision, pages 207224. Springer, 2024a. Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei. Minds eye of llms: Visualization-of-thought elicits spatial reasoning in large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. 18 ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy Lillicrap, Kenji Kawaguchi, and Michael Q. Shieh. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv preprint arXiv:2405.00451, 2024. URL https://arxiv.org/abs/2405.00451. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712, 2024. Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1):1, 2023a. Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. Idea2img: Iterative self-refinement with gpt-4v (ision) for automatic image design and generation. arXiv preprint arXiv:2310.08541, 2023b. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2024a. URL https: //arxiv.org/abs/2308.02490. Weihao Yu, Zhengyuan Yang, Lingfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, and Xinchao Wang. Mm-vet v2: challenging benchmark to evaluate large multimodal models for integrated capabilities. arXiv preprint arXiv:2408.00765, 2024b. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multidiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024a. Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Improve vision language model chain-of-thought reasoning. arXiv preprint Pang, and Yiming Yang. arXiv:2410.16198, 2024b. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-ofthought reasoning in language models. Trans. Machine Learning Research, 2024c. Sage Zheng, Darwin Hou, Yujie Pan, Xinyun Li, Amanpreet Singh, Dawn Song, Percy Liang, Jason Wei, et al. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/EasyR1, 2025. 19 ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning. arXiv preprint arXiv:2402.11411, 2024a. Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. arXiv preprint arXiv:2405.14622, 2024b. 20 ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs"
        },
        {
            "title": "Appendix",
            "content": "A. Prompts used in experiments A.1. Prompt for Training Data Generation We provide the prompt used for generating ViCrit task training data in Table 5. A.2. Prompt for ViCrit-Bench Evaluation We provide the prompt used for ViCrit-Bench evaluation in Table 3. Table 3: Prompt template used for ViCrit-Bench evaluation. Prompt Template: You are provided with an image and the description corresponding to this image. There is one hallucination in this description. Find out the hallucination phase and answer with the hallucination phase directly in list. Your output should only be list that contains the hallucination phase you find. Description: B. Comparison with SFT In this section, we perform SFT on Qwen-2.5-VL-7B and 72B using 900k captioning samples from PixMo-Cap, and compare the results with ViCrit-RL models trained using the same amount of data through ViCrit task RFT. As shown in Table 4, we find that although SFT significantly reduced hallucination in VLMs, it do not lead to notable performance improvements on general benchmarksin fact, the 7B model even shows performance drop. This highlights the effectiveness of ViCrit task RFT, which not only reduces hallucinations but also generalizes well to enhance VLM performance on general reasoning tasks. 21 ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Table 4: Comparison between ViCrit-RL and ViCrit-RL with using same captioning data for SFT. We find that although hallucinations in the VLM are significantly reduced after SFT, the performance improvement is difficult to generalize to general tasks. Hallucination Benchmark General benchamrk Model A R C Qwen2.5-VL-7B-Instruct Qwen2.5-VL-7B-CapSFT ViCrit-RL-7B (Ours - Qwen2.5-7B) 28.0 25.5 25.2 -2.8 Qwen2.5-VL-72B-Instruct 26.4 Qwen2.5-VL-72B-CapSFT 21.6 ViCrit-RL-72B 21.0 -5.4 (Ours - Qwen2.5-72B) 5.1 4.4 4.5 -0.6 4.8 3.6 3.9 -0. H 3.74 3.78 3.77 +0.03 3.82 3.89 3.91 +0.09 i a i s o V i n r t n M r M V - i n a v a Avg. 23.6 20.1 25.7 50.61 61.7 67.8 48.41 53.4 67.4 70.7 53.01 61.9 +2.9 +2.1 +1.8 +1.4 +0.2 +1.1 +3.3 +6.4 +2.40 41.4 38.0 47. 49.3 47.3 52.6 66.0 64.7 67.1 50.6 52.1 52.0 44.5 44.3 46.3 59.78 68.4 74.8 60.78 68.9 76.1 77.3 63.16 69.8 +2.5 +4.9 +6.5 +2.6 +1.4 +0.8 +4.5 +3.9 +3.38 76.3 76.5 77. 45.5 44.7 49.4 35.2 34.8 40.1 63.4 65.3 66.0 61.3 63.0 65.8 53.3 57.9 59.8 ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs Table 5: Prompt used for training data generation. You are helpful assistant designed to manipulate text with precision. Your task is as follows: 1. Identify all noun phrases in given paragraph. noun phrase consists of noun and its modifiers (e.g., \"the wooden bridge,\" \"a flock of birds\"). Noun phrase is two to five words long. Do not output list of multiple noun phrases. 2. Randomly select one noun phrase from the list, it can be small background objects, scene text, foreground objects. Try to select scene text and small background objects more often when possible. 3. Replace the chosen noun phrase with another phrase that is visually similar, such as changing the object attributes, replacing the object with visually similar noun, or adding and removing characters within the scene text. The replacement should be visually similar but not identical to the original phrase. Be creative and dont always focus on the most obvious or common replacements such as color. 4. However, the replacement should introduce clear change, such that it is impossible to be ambiguous. The change should be directly related to image and be visual description. Do not only change words to its synonyms or make ambigious changes. Do not merely change words to its synonyms. Do not merely change words to its synonyms. Do not merely change words to its synonyms. 5. Ensure the edited paragraph is still be plausible image description, and the change is not too obvious. 6. Group the original phrase in <Before>original</Before>, and changed phrase in <After>changed</After>. <Caption> is used to give input caption and should not be generated. Perform this transformation accurately and naturally. Here are some examples: 1. <Caption>This image appears to be screenshot taken from an iPhone displaying the interface of food delivery app, likely DoorDash, around the Chicago and Gary, Indiana area. The top of the screen indicates the time as 2:30 PM, with the phone connected to an LTE network. The battery icon suggests low battery level of 15-20 Central to the image is map highlighting various regions with color codes: red areas represent high traffic or demand, likely meaning those areas are \"busy\" for delivery drivers, as indicated by red text banner. Lighter red and green sections represent varying levels of demand. At the top of the image, black banner labeled \"Promos\" is displayed, accompanied by blue notification bell icon with the number two beside it, indicating two notifications. The bottom of the screen shows black navigation bar. It contains options for \"Dash,\" \"Schedule,\" \"Account,\" \"Ratings,\" and \"Earnings.\" The \"Dash\" option is highlighted in red, suggesting it is currently selected. Centrally located in this bar is red \"Dash Now\" button, implying that the user can begin delivering immediately. An additional black banner, located just above the navigation bar, reads \"In... Hammond.\" Overall, this detailed caption gives comprehensive idea of the apps functionality, likely indicating areas of high demand where food delivery services are needed the most.</Caption> <Before>a low battery level of 15-20%</Before> <After>a high battery level of 75-80%</After> 2. <Caption>The image depicts screenshot from strategy video game with third-person, aerial view. The central character, named Anselm, navigates through complex, industrial-style building that evokes the aesthetic of games like Metal Gear. The environment is dark and futuristic, with certain areas illuminated, revealing various paths and stairs. The top left corner displays the yellow text Instructor Eastwood, alongside graph-like design. The upper center features game-related instructions in white text stating Defensive Measures Use Range to Your Advantage, with the notations 5 and 5.1 accompanying the instructions. Additionally, map of the area is situated in the lower left corner, while the bottom right corner features interactive elements or key potentially indicating available weapons. The overall scene suggests mission-focused gameplay scenario requiring strategic maneuvering and tactical decision-making.</Caption> <Before>text Instructor Eastwood,</Before> <After>text Instructor Westwood,</After> 3. <Caption>The image depicts three-dimensional panoramic view of conference room where business meeting is taking place. The setting is typical meeting room with white walls, fluorescent lighting, and windows equipped with blinds on some, including wooden slats on one. At the center of the room is round, yellow table that appears slightly distorted due to the panoramic effect. On this table, there are various items, including box of tissues, white mug, teacup, and pamphlets. Surrounding the table, seated in circle, are eight individuals. They appear to be mixed group of men and women, predominantly of Asian descent, and are dressed in variety of attire ranging from business to casual. All attendees are wearing name tags on the left side of their chests, indicating their participation in the meeting. Their seating arrangement includes black chairs with green backs, and each person either has their hands folded in their laps or is holding something, possibly drink. From left to right, the attendees include woman in peach-colored t-shirt with writing, man in blue shirt, woman in gray sweater, woman in green shirt, another woman in blazer, an empty chair, man in yellowish shirt, man wearing ball cap, and man in blue and green jacket. One notable aspect is that the meeting environment, though professional, is quite understated with minimalistic decor and standard conference room furnishings.</Caption> <Before>eight individuals</Before> <After>seven individuals</After> Here is the input caption: <Caption>{CAPTION}</Caption>"
        }
    ],
    "affiliations": [
        "Cardiff University",
        "Microsoft",
        "University of Maryland, College Park",
        "University of Michigan"
    ]
}