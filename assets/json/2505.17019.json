{
    "paper_title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework",
    "authors": [
        "Chenhao Zhang",
        "Yazhe Niu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Metaphorical comprehension in images remains a critical challenge for AI systems, as existing models struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. While multimodal large language models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they struggle with a fundamental limitation on image implication tasks: contextual gaps that obscure the relationships between different visual elements and their abstract meanings. Inspired by the human cognitive process, we propose Let Androids Dream (LAD), a novel framework for image implication understanding and reasoning. LAD addresses contextual missing through the three-stage framework: (1) Perception: converting visual information into rich and multi-level textual representations, (2) Search: iteratively searching and integrating cross-domain knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment image implication via explicit reasoning. Our framework with the lightweight GPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English image implication benchmark and a huge improvement on Chinese benchmark, performing comparable with the GPT-4o model on Multiple-Choice Question (MCQ) and outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work provides new insights into how AI can more effectively interpret image implications, advancing the field of vision-language reasoning and human-AI interaction. Our project is publicly available at https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 9 1 0 7 1 . 5 0 5 2 : r Let Androids Dream of Electric Sheep: Human-like Image Implication Understanding and Reasoning Framework Chenhao Zhang1,2 Yazhe Niu1,3 1Shanghai AI Laboratory 2Huazhong University of Science and Technology 3The Chinese University of Hong Kong zhangchenhao@pjlab.org.cn niuyazhe@pjlab.org.cn"
        },
        {
            "title": "Abstract",
            "content": "Metaphorical comprehension in images remains critical challenge for AI systems, as existing models struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. While multimodal large language models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they struggle with fundamental limitation on image implication tasks: contextual gaps that obscure the relationships between different visual elements and their abstract meanings. Inspired by the human cognitive process, we propose Let Androids Dream (LAD), novel framework for image implication understanding and reasoning. LAD addresses contextual missing through the threestage framework: (1) Perception: converting visual information into rich and multi-level textual representations, (2) Search: iteratively searching and integrating cross-domain knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment image implication via explicit reasoning. Our framework with the lightweight GPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English image implication benchmark and huge improvement on Chinese benchmark, performing comparable with the GPT-4o model on Multiple-Choice Question (MCQ) and outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work provides new insights into how AI can more effectively interpret image implications, advancing the field of vision-language reasoning and human-AI interaction. Our project is publicly available at https: //github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep."
        },
        {
            "title": "Introduction",
            "content": "Do androids dream of electronic sheep? The question actually has two levels: The first level is to ask if androids dream, and the second level is to ask if they dream of electronic sheep. Philip K. Dick (1968) Metaphors are not just abstract concepts found in literature; they are also prevalent in our daily lives. For instance, when we say \"time is money\" or \"life is journey,\" we are using metaphors to convey complex ideas in more contextual and understandable way. These metaphors highlight the integral role that metaphoric thinking plays in human communication and cognition. Just as we use metaphors to make sense of the world around us, we aim to enable AI to understand metaphors in human-like manner. In linguistic terms, as George Lakoff and Mark Johnson elaborated in \"Metaphors We Live By\" [11], metaphors are not merely ornamental language devices but fundamental cognitive tools that allow us to conceptualize our surroundings. Metaphors possess characteristics such as systematicity, Technical Report the creation of similarity, and imaginative rationality. Through cross-domain mapping, one concept can be used to comprehend another, allowing for more insightful interpretation. With the rapid advancement of large language models (LLMs), models such as OpenAI o1 [20], DeepSeek-R1 [4], and QwQ [25] have demonstrated remarkable text-reasoning capabilities. However, significant amount of knowledge in the real world cannot be fully represented by text alone. Visual information, for instance, contains wealth of knowledge that is not easily captured through text. As result, there has been growing interest in integrating visual information into text-reasoning tasks. Compared to language, vision is inherently complex, with its diverse representation of information, subjective understanding, and the difficulty in quantifying its information. In recent years, multimodal reasoning models such as QVQ [24] and K1.5 [23] have achieved outstanding performance. For example, K1.5 model has reached high score on math, code and multimodal reasoning benchmarks [9, 14, 16, 26, 36]. However, these models still perform poorly on image metaphor questions [15, 37]. They tend to focus on the superficial elements of the image, neglecting the deeper connections and emotional expressions among these elements, as shown in Figure 1. It is important to note that these models excel at logical reasoning tasks, which are based on different set of cognitive principles compared to image metaphor tasks. In contrast to the VQA task, which primarily centers on concrete image comprehension, the image metaphor entails stronger emphasis on abstract meaning and higher-order reasoning capabilities. It is not simple logical reasoning task and requires different method to understand and generate implications. It requires the model to understand complex and abstract information, such as metaphors, symbols, and emotions in the image, rather than just the concrete contents. Figure 1: An image is worth thousand words: For the image implication understanding task, different elements combination lead to different thinking paths, but the correct path needs all elements with multiple reasoning thoughts. Image implication tasks consist of two main aspects: understanding and generation. Understanding image implication is more complex and challenging task than understanding conventional images. It requires advanced cognitive abilities such as multi-hop reasoning and sophisticated theory of mind (ToM), which are inherent to human cognition [15, 37]. Compared to understanding, generating implication is even more difficult. The fundamental challenge stems from the lack of contextual understanding of the key elements and internal relationships of the image. This lack of context hinders our ability to decipher the intended message or to create images that effectively convey specific meanings. Without the background of cultural, historical, or environmental context, the significance of key visual components remains elusive, impeding both interpretation and creative expression. Existing methods for solving the image metaphor understanding can be mainly divided into two categories: explicit metaphor mapping and model implicit reasoning. The former achieves image metaphor understanding by establishing correspondence between metaphor ontology and visual representation. For example, the CLOT method [40] realizes image metaphor understanding through the mapping between metaphor ontology and visual representation. Model implicit reasoning relies on the models reasoning ability and does not require the explicit mapping construction. For example, C4MMD method [32] adopts an untrained chain-of-reasoning approach. However, explicit metaphor mapping, although it can provide clear mapping, has limitations when dealing with complex manyto-many mappings and dynamically changing cultural backgrounds. On the other hand, model implicit reasoning, despite its potential, still faces challenges in handling complex metaphor understanding tasks, especially in situations involving multimodal information and cultural backgrounds. To address these problems, we analyze how humans understand metaphors and find that the essence of the difficulty in metaphor understanding and generation is contextual missing. Therefore, we propose novel framework that more closely aligns with human cognitive processes for metaphor 2 interpretation. Our framework first transforms visual information into textual representations and then iteratively searches to enrich these representations with out-of-domain knowledge, enabling deeper inferential reasoning. Experiments from both Multiple-Choice Question and Open-Style Question consistently verify the superiority of the proposed framework. Our key contributions are listed as follows: We systematically analyze image implication tasks and find the difficulty of the metaphor understanding and reasoning task lies in contextual missing. From the perspective of human cognition, we proposed new direction for solving these tasks Contextual Alignment. We propose novel human-like three-stage framework Let Androids Dream (LAD), which implements the lightweight GPT-4o-mini model to achieve SOTA on English image implication benchmark and huge improvement on Chinese image implication benchmark, comparable with the GPT-4o model and other top closed-source models on Multiple-Choice Question. We design the challenging Open-Style Question (OSQ) with comprehensive metric to automatic evaluate the image implication tasks. This metric aligns 95.7% with human annotations, making it more suitable for diverse evaluation. Our LAD outperforms the GPT-4o model 36.7% on OSQ."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Image Implication Image implication encompasses various cognitive aspects, including humor, sarcasm, and broader metaphorical understanding. Early research in this domain focused on specialized aspects, such as humor recognition [6, 7] and sarcasm detection [5]. As the rapid development of large language models (LLMs) brings new opportunities for analyzing image implication, we need more comprehensive evaluation frameworks. DeepEval [34] provided systematic taxonomy of image implications. Subsequently, II-Bench [15] emerged as the first English image implication benchmark, followed by CII-Bench [37], which extended this evaluation framework to Chinese images. Image implication understanding requires sophisticated multi-hop reasoning and theory of mind (ToM) capabilities [15, 37]. Existing approaches fall into two categories: explicit metaphor mapping and model implicit reasoning. The first approach, represented by CLOT [40], constructs mappings between metaphor ontologies and visual representations. However, this approach faces key challenges: metaphorical relationships have complex many-to-many mappings that are difficult to formalize, and cultural references are too dynamic for static mappings. The second approach, exemplified by C4MMD [32], employs training-free CoT reasoning. Despite its promise, this approach struggles with the complex nature of metaphorical understanding, which surpasses traditional reasoning. The large search space for out-of-domain reasoning and changing cultural contexts limits its effectiveness. To address this, we propose novel methodology that transforms visual information into texts and iteratively enriches them with out-of-domain knowledge, better aligning with human cognitive processes. 2.2 Vision-language Reasoning The rapid advancement of LLMs has demonstrated remarkable text reasoning capabilities, as evidenced by models such as o1 [20], DeepSeek-R1 [4], and QwQ [25, 33]. However, real-world knowledge often transcends textual representation, with visual information encapsulating substantial world knowledge that pure language models cannot access. For example, images inherently contain rich, multi-layered information that often resists straightforward textual description, including spatial relationships, contextual nuances, and implicit knowledge that humans process intuitively. This limitation has driven research toward integrating visual information into text-based reasoning frameworks. Current research has developed three primary approaches to incorporate visual information into model reasoning: 1) Comprehensive MLLM Description: This approach treats visual content as text grounding problem, as demonstrated by LLAVA-COT [31] and Mulberry [35]. 2) Multi-turn MLLM Interaction: Models like VoCoT [13] and V* [28] employ iterative question-answering to extract fine-grained visual information at various levels of detail. 3) Tool-augmented Reasoning: Frameworks such as Visual Sketchpad [8] and Whiteboard-of-Thought [17] leverage tool-based approaches to modify images and augment reasoning with prior knowledge embedded in these tools. Figure 2: The general framework of Let Androids Dream (LAD), which includes three stages: (1) Perception: converting raw visual information into rich and multi-level textual representations, (2) Search: iteratively searching and integrating cross-domain knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment image implication interpretations via explicit reasoning."
        },
        {
            "title": "3 Method",
            "content": "Inspired by the human cognitive process, we introduce new paradigm for solving image implication tasks Contextual Alignment. We have detailed discussion for this point in Section 1 and Section 5. Therefore, we propose Let Androids Dream (LAD), novel framework for image implication understanding and reasoning. This framework operates through the three-stage framework, as shown in Figure 2: (1) Perception: converting visual information into rich and multi-level texts, (2) Search: iteratively searching and integrating cross-domain knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment analysis via explicit reasoning. 3.1 Stage I: Perception The initial stage, Perception, aims to transform raw visual inputs into structured, hierarchical textual representations, mirroring the human cognitive process of initial intuition-driven observation and subsequent identification of key elements. This stage operates in manner analogous to human System 1 (intuitive, holistic processing) and System 2 (analytical, focused processing). First, we utilize MLLM to process the input image and produce detailed textual narrative. This description captures coarse-grained visual information, including discernible text within the image, prominent colors, overall layout, and salient objects or entities. This step provides holistic foundational understanding of the content of the image. Following this, we derive fine-grained keyword set. The MLLM condenses the above image description into concise set of approximately 7 keywords. These keywords are specifically chosen to encapsulate critical aspects relevant to implication understanding, such as the perceived emotion, the domain or context (e.g., political, social, cultural) and any rhetorical devices that might be visually suggested. Keywords also re-emphasize crucial textual elements or entities identified in the description. This two-tiered representation, comprising rich description and focused keywords, provides robust foundation for the subsequent Search and Reasoning stages by converting unstructured visual data into actionable textual information. The keywords, in particular, serve as vital cues for guiding the knowledge retrieval in stage II. 3.2 Stage II: Search The Search stage addresses semantic ambiguities and enhances contextual comprehension by iteratively retrieving and integrating cross-domain knowledge critical for interpreting image implications. 4 This stage employs adaptive search, which dynamically selects the most appropriate search method. The process is systematically organized into three main phases: Plan, Search, and Summary. 1. Plan: The process begins by formulating targeted search queries. Using the keywords generated in Stage I, the MLLM, guided by prompt specifically designed for image implication tasks, generates five different levels of search questions. These questions aim to uncover latent meanings, cultural references, or background information pertinent to the image implications. 2. Search: This phase executes the search based on the generated questions, employing the Self-Judge mechanism to determine the optimal search strategy for each question. (a) Self-Judge: The MLLM acts as judge, assigning confidence score to each search question. This score reflects criteria such as the perceived popularity or commonness of the knowledge required, relevance to real-time or recent events, and whether the question involves contemporary internet slang or meme culture. Questions scoring high, indicating need for up-to-date or niche information, are routed to WebSearch. Questions scoring low, suggesting the answer might reside within general world knowledge, are directed to ModelSearch. This adaptive routing optimizes for both knowledge coverage and inference efficiency. (b) ModelSearch: For questions deemed suitable for internal knowledge retrieval, ModelSearch leverages the MLLMs own parametric memory. Using specialized prompt, the model directly generates an answer based on its pre-trained knowledge base. This approach is efficient for recalling established facts or common concepts. (c) WebSearch: For questions requiring external, dynamic, or highly specific information, WebSearch is invoked. Inspired by LLM search methods like MindSearch [3], but focusing on image implication tasks, our WebSearch component first employs the planner. The planner, acting as high-level strategist, decomposes the initial search question into series of more granular sub-questions. These sub-questions are structured into directed acyclic graph (DAG), simulating multi-step, exploratory information-seeking process. Subsequently, the searcher executes this plan. It performs hierarchical information retrieval for each sub-question from the internet, gathering relevant snippets and facts. This multi-agent method, with distinct planner and searcher modules, allows for parallel processing and dynamic refinement of the search strategy. The retrieved information for sub-questions is then synthesized to answer the original search question. This ensures access to recent developments and broad spectrum of public knowledge, crucial for understanding contemporary image implications. 3. Summary: The raw outputs from the Search phase are refined into concise search summary. (a) RankSummary: The set of five question-answer pairs is evaluated. The MLLM ranks these pairs based on their relevance to understanding the core implication of the original image. The top three most relevant question-answer pairs are selected. (b) RefineSummary: The selected pairs are further processed. The MLLM, guided by the ranking reason from the ranking step, rewrites and consolidates these pairs. This involves removing irrelevant or redundant information, reconciling diverse pieces of information, and potentially supplementing details to create single, optimized, and concise search summary. This final summary serves as the enriched contextual input for Stage III. 3.3 Stage III: Reasoning The final stage, Reasoning, performs explicit reasoning to derive contextually grounded interpretations of image implications. This stage synthesizes all previously gathered information the hierarchical textual representations from Stage (descriptions and keywords) and the domain-enriched knowledge from Stage II into coherent implication framework. For image implication tasks, we employ specific reasoning format. The MLLM is prompted to articulate its reasoning trajectory using designated markers, such as <think> . . . </think> special tokens. Within these markers, the model explicitly lays out its step-by-step reasoning process, connecting the visual cues, keywords, and external knowledge to arrive at the final image implication analysis and explanation. This domain-specific CoT method not only guides the model towards more robust and grounded output, but also makes the inferential pathway transparent. The framework ultimately generates contextually-aligned implication understanding that emerges from 5 Model Multiple-Choice Question Open-Style Question Qwen2.5-VL-7B [2] DeepSeek-VL2 [29] Gemini-2.0-flash [22] QwenVL-2.0-72B [27] QwenVL-2.5-72B [2] GLM-4V-plus [39] Gemini-2.0-pro [22] Grok-3 [30] Claude-3.5-Sonnet [1] GPT-4o [19] GPT-4.1 [19] en General Models 46% 46% 70% 68% 72% 64% 68% 66% 68% 74% 74% zh 40% 36% 68% 54% 56% 64% 62% 64% 62% 58% 62% Vision-language Reasoning Models Gemini-2.0-flash-thinking [22] QVQ-72B [24] Doubao-1.5-thinking-vision-pro [21] Grok-3-reasoning [30] 64% 62% 66% 74% 68% 56% 66% 64% Our Method en 2.34 2.82 1.60 2.84 1.56 3.01 1.66 3.24 3.22 2.94 3. 1.66 3.10 3.16 3.06 zh 2.58 2.86 3.12 3.04 3.12 3.12 3.18 2.96 3.78 3.76 3.92 2.84 3.42 3.90 2.92 GPT-4o-mini [19] + LAD (Stage + III) + LAD (Stage + II + III) Improv. 44% 68% 74% +30 (68.2%) 42% 44% 52% +10 (23.8%) 2.98 3.84 4.02 +1.04 (34.9%) 3.36 3.58 3.66 +0.3 (8.9%) Table 1: Overall results of different models on Multiple-Choice Question and Open-Style Question. The best-performing model in each category is in-bold, and the second best is underlined. the integration of visual-semantic inputs and cross-domain knowledge, formalizing the LAD systems capacity for evidence-based visual reasoning. 3.4 LAD Pipeline The Let Androids Dream (LAD) framework operates as sequential pipeline, integrating the three distinct stages described in Figure 2 and Algorithm 1. Stage (Perception) initiates the process. It takes an input image and employs the MLLM to generate comprehensive image description. This description is then further processed to extract seven salient keywords. The outputs of this stage are the image description and the set of keywords. These keywords serve as the primary input for Stage II (Search). Here, the MLLM transforms the keywords into five targeted search questions. self-judge mechanism then directs these questions to either ModelSearch (for internal knowledge retrieval) or WebSearch (for external, dynamic information). The resulting question-answer pairs are ranked for relevance, with the top three being selected and subsequently refined into concise search summary. This search summary is the key output of Stage II. Finally, Stage III (Reasoning) receives the original image, the image description and keywords from Stage I, and the search summary from Stage II. The MLLM integrates these multi-modal inputs and, through an explicit reasoning process (guided by structured CoT), generates the final image implication. This implication represents the culmination of the LAD pipelines understanding and reasoning about the input image."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Baselines Models. To comprehensively compare with LAD, we carefully select diverse range of MLLMs, encompassing both open-source and closed-source models, with the aim of covering wide spectrum of model characteristics and scales. These models span parameter sizes from 7B to 300B, ensuring that models of varying complexity and capability are thoroughly assessed. In selecting the models, we focus on the following key aspects: 1) General and Reasoning models, 2) Open-Source and Closed-Source models, and 3) model parameter scaling law. The experiment setup is in Appendix B. 6 Figure 3: case study of different methods on Multiple-Choice Question. The End2End method shows superficial reasoning and the CoT method shows over-inference, while our LAD framework shows the correct path via more contextual alignment analysis. The full prompt is listed in Appendix D. Evaluation. Our evaluation utilizes two comprehensive image implication benchmarks, II-Bench [15] and CII-Bench [37], both featuring Multiple-Choice Question (MCQ). Furthermore, we manually construct high-quality benchmark by randomly selecting 50 images from varied image types like illustrations and comics. And we measure accuracy by comparing the models selected option to the ground truth. Aware of potential MCQ biases [12, 18, 38] and the greater difficulty of generation over judgment tasks, we introduce novel evaluation method Open-Style Question (OSQ). It uses the same images with the fixed question: What is the implication in this image?. And we use GPT-4o with specialized evaluation metric as evaluators, validated by multiple human consistency checks. 4.2 Multiple-Choice Question 4.2.1 Implementation Details Our high-level benchmark includes diverse images such as comics, posters, illustrations, English and Chinese Internet memes, and Chinese traditional artworks, all rich in visual information and cultural significance. Each image is paired with one question, each offering six options with only one correct answer. The question is What is the implication in this image? (mostly) or different levels of image understanding, such as overarching interpretation and nuanced details. case study of different methods on MCQ is in Figure 3. 4.2.2 Results and Analysis Table 1 presents comprehensive results of MCQ across different MLLMs on our high-level benchmark. The LAD framework demonstrates remarkable effectiveness, achieving SOTA performance with the lightweight GPT-4o-mini model. In English MCQ, our framework matches the performance of closed-sourced models like GPT-4o, GPT-4.1, and Grok-3-reasoning (74%), while significantly outperforming Claude-3.5-Sonnet and Gemini-2.0-pro by 9%. For Chinese MCQ, our framework achieves comparable results to GPT-4o, while substantially surpassing DeepSeek-VL2 by 44.4%. The improvement over the base GPT-4o-mini model is particularly noteworthy, with relative improvements of 68.2% for English and 23.8% for Chinese, far exceeding the capabilities of other open-source and reasoning models. Interestingly, we observe that reasoning models show minimal advantage Figure 4: Evaluation metric and evaluation standard of Open-Style Question. over general models on image implication task, with comparable accuracy rates across categories. This finding suggests that current RL-based reasoning approaches exhibit limited generalization capability for image implication understanding, underscoring the distinct complexity of this task compared to basic VQA tasks and classic logical reasoning domains like math and code. 4.3 Open-Style Question 4.3.1 Implementation Details Evaluation Metric. To comprehensively assess MLLMs understanding of image implication, we develop multifaceted evaluation metric. This metric is designed to probe both the surface-level information readily apparent in the image and the deeper emotion, domain and rhetorical skills that inform its creation and interpretation. Our evaluation metric encompasses five key perspectives: Surface-level Information, Emotional Expression, Domain and Context, Rhetorical Skills, and Deep Implications. For each perspective, we give its detailed description in Figure 4. MLLM-based Automatic Evaluation. To evaluate image implication comprehension in MLLMs, we develop an MLLM-based evaluation standard based on evaluation metrics, as illustrated in Figure 4. Our experiment utilize the same dataset from MCQ experiment, comprising 50 English images and 50 Chinese images. We employ human-written descriptions and implication interpretations as ground truth. We choose the same MLLMs with MCQ experiment to generate image implications for these images, which are subsequently scored using GPT-4o and our evaluation standard. The evaluation prompt is in Appendix D. To validate the models scoring efficacy, we enlist 16 PhD students and researchers well-versed in English and Chinese metaphorical imagery to independently score the dataset. The human-model scoring consistency reached 95.7%, affirming the methods validity. The detailed human-model consistency study is in Appendix C. 4.3.2 Results and Analysis Table 1 presents comprehensive results of OSQ across different MLLMs on our high-level benchmark. The LAD framework demonstrates exceptional effectiveness, achieving SOTA performance with the lightweight GPT-4o-mini model. In English OSQ, our framework substantially outperforms closed-sourced models like GPT-4o by 36.7% and Claude-3.5-Sonnet by 24.8%. For Chinese OSQ, 8 Model Multiple-Choice Question Open-Style Question en zh en zh w/o CoT Standard CoT LAD-CoT 44% 50% 68% GPT-4o-mini 42% 42% 44% 2.98 3.10 3.84 3.36 3.28 3.58 Table 2: Results of different CoT methods. Our LAD-CoT method achieves the best improvement. The best-improvement method in each category is in-bold. while slightly below top closed-sourced models like GPT-4.1 and Doubao-1.5-thinking-vision-pro, our method still significantly surpasses Gemini-2.0-pro by 15.1% and DeepSeek-VL2 by 30%. The enhancement over the GPT-4o-mini is particularly noteworthy, with improvements of 34.9% for English and 8.9% for Chinese, far exceeding other open-source and reasoning models. Unlike MCQ results, we observe significant performance disparities between reasoning and general models on OSQ, highlighting the distinct challenges of image implication generation. Interestingly, several models (e.g., QwenVL-2.5-72B, Gemini-2.0-pro) exhibit substantial performance gaps between MCQ and OSQ. Upon manual examination of model outputs, we attribute this to potential overfitting to multiple-choice formats and insufficient exposure to open-style generation tasks. In addition, LLMs or even MLLMs may not genuinely understand the questions but rather predict options as answers, introducing evaluation bias and demonstrating sensitivity to option positioning [38]. 4.4 Ablation Study 4.4.1 Stage (Perception) and Stage III (Reasoning) We incorporate LADs Stage (Perception) and Stage III (Reasoning), collectively LAD-CoT. This method shows significant improvements in Table 1, with GPT-4o-mini scores increasing from 44% to 68% (English) in the MCQ, and from 2.98 to 3.84 (English) and 3.36 to 3.58 (Chinese) in the OSQ. Compared to standard CoT, the results are shown in Table 2. While standard CoT offers minor gains in English (MCQ: 44% to 50%; OSQ: 2.98 to 3.10), it shows no improvement or even slight decline in Chinese (MCQ: 42% unchanged; OSQ: 3.36 to 3.28). In contrast, LAD-CoT substantially outperforms both the baseline and standard CoT across all types. For instance, LAD-CoT achieves 68% on English MCQ while standard CoT only 50%, and score of 3.84 on English OSQ compared to 3.10 for standard CoT. These findings highlight the superior efficacy of our LAD-CoT for image implication over standard CoT methods. case study of various CoT on MCQ is in Figure 3. The standard CoT prompt and other details is in Appendix D. 4.4.2 Stage II (Search) We conduct detailed analysis of LADs Stage II (Search), named LAD-Search. It shows significant improvements in Table 1, with GPT-4o-mini scores increasing from 68% to 74% (English) and 44% to 52% (Chinese) in the MCQ, and from 3.84 to 4.02 (English) and 3.58 to 3.66 (Chinese) in the OSQ. Compared with Grok-3-search [30], GPT-4o-mini-search-preview, and GPT-4o with Perplexity.ai (Pro version), the results are shown in Table 3. GPT-Search, when applied to GPT-4o-mini, improves MCQ scores but degrades OSQ performance (English OSQ: 3.84 to 3.62, Chinese OSQ: 3.58 to 3.34). Grok-Search, on the Grok-3 model, provides limited gains, mainly in English MCQ (66% to 72%), exhibits inconsistent Chinese performance, and shows minimal OSQ improvement. Perplexity.ai search with GPT-4o significantly boosts MCQ accuracy, but it markedly lowers OSQ scores (English OSQ: 2.94 to 2.88, Chinese OSQ: 3.76 to 3.28). In contrast, LAD-Search consistently enhances performance across both MCQ and the more challenging OSQ. This underscores its superior ability to effectively integrate external knowledge for implication understanding, outperforming other search methods particularly in open-style reasoning scenarios where they often falter. 9 Model Multiple-Choice Question Open-Style Question en 66% 72% w/o search Grok-Search w/o search Perplexity (pro) 74% 80% w/o search GPT-Search LAD-Search 68% 72% 74% zh Grok-3 64% 64% GPT-4o 58% 66% GPT-4o-mini 44% 48% 52% en zh 3.24 3.25 2.94 2.88 3.84 3.62 4.02 2.96 2.92 3.76 3.28 3.58 3.34 3.66 Table 3: Results of different search methods. Our LAD-Search method achieves the best improvement. The best-improvement method in each category is in-bold."
        },
        {
            "title": "5 Discussion",
            "content": "5.1 How to Let Androids Dream? Perception and Reasoning The question How to Let Androids Dream? metaphorically addresses the foundational challenge of enabling AI systems to interpret the nuanced implications embedded in images. Our framework tackles this by first emulating human-like perception (Stage I), converting raw visual input into rich, multi-level textual representations, including comprehensive descriptions and salient keywords. These keywords are designed to capture not only objects and scenes but also potential emotional tones, relevant domains (e.g., cultural, social, political), and discernible rhetorical devices. Subsequently, LADs Stage III employs an explicit, structured CoT process. This structured reasoning guides the model to systematically connect the perceived visual elements with retrieved contextual knowledge, thereby constructing coherent understanding of implications. This method is vital because, as our experiments (Section 4) and recent work on social reasoning [10] show, comprehending implications extends beyond basic VQA tasks and classic logical reasoning; it inherently involves sophisticated social reasoning and the interpretation of contextual cues often missed by MLLMs. 5.2 How to Dream of Electric Sheep? Search Building upon the capacity to analyze, How to Dream of Electric Sheep? delves into how AI can generate accurate and specific image implicationsthe metaphorical electric sheep. LADs Stage II (Search) is the key to achieving this goal. This stage acknowledges that the meaning of visual elements, particularly in metaphorical contexts, often relies on external information, such as cultural norms, historical events, or contemporary affairs, which may not be adequately represented in MLLMs static pre-trained knowledge. LADs adaptive search mechanism, which includes formulating targeted queries from keywords and dynamically selecting between internal ModelSearch and external WebSearch via Self-Judge, systematically enriches the initial perception with relevant cross-domain knowledge. This iterative retrieval and integration of contextual information, especially for popular metaphors or ambiguous visual cues, significantly broadens the models interpretive horizon. By providing this essential external context, the Search stage empowers LAD to move beyond superficial interpretations and accurately capture the intended, often subtle, implications of an image, as demonstrated by its robust performance on Open-Style Question (OSQ)."
        },
        {
            "title": "6 Conclusion",
            "content": "Understanding image implications remains challenging for MLLMs, mainly due to missing contextual information. Our work introduces LAD, novel three-stage frameworkPerception, Search, and Reasoning. Inspired by human cognitive processes, this framework is designed to achieve contextual 10 alignment by explicitly integrating visual interpretation with external knowledge retrieval. We conduct comprehensive experiments to demonstrate its effectiveness. Utilizing the lightweight GPT-4o-mini, LAD achieves SOTA results on English and Chinese implication benchmarks, performing comparable or even surpassing GPT-4o and other top closed-source models, particularly on challenging OpenStyle Question. In summary, LAD bridges the gap between superficial perception and deep reasoning in multimodal AI systems, offering promising direction for context-aware reasoning."
        },
        {
            "title": "Limitation and Future Work",
            "content": "While our work represents huge step towards image implication tasks, the LAD framework still suffers from the following limitations: 1) The search stage, particularly the websearch and multiple model calls, will make latency in generating image implications. 2) Furthermore, although our Open-Style Question (OSQ) evaluation incorporates average multiple model calls and human consistency checks (the human-model scoring consistency reached 95.7% with 16 PhD students and researchers) to mitigate subjectivity, its foundation on the GPT-4o model judgments may still retain degree of inherent bias. In future work, we aim to prioritize optimizing the search strategy to enhance efficiency and reduce model calls without compromising performance, alongside further refining our evaluation method."
        },
        {
            "title": "Ethics Statement",
            "content": "The LAD framework aims to enhance AIs nuanced understanding of image implications, crucial aspect of human-like cognition. We acknowledge that advanced interpretative capabilities carry ethical considerations, including potential biases inherited from underlying MLLMs or training data, and the risk of misuse in generating or interpreting content. Our use of public benchmarks promotes transparency in evaluation. We are committed to fostering responsible development and encourage continued research into robust safeguards and ethical AI practices within multimodal reasoning to ensure beneficial applications."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Model card addendum: Claude 3.5 haiku and upgraded claude 3.5 sonnet, 2024. [2] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Z. Chen, K. Liu, Q. Wang, J. Liu, W. Zhang, K. Chen, and F. Zhao. Mindsearch: Mimicking human minds elicits deep ai searcher. arXiv preprint arXiv:2407.20183, 2024. [4] DeepSeek-AI et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [5] P. Desai, T. Chakraborty, and M. S. Akhtar. Nice perfume. how long did you marinate in it? multimodal sarcasm explanation. In AAAI, 2022. [6] J. Hessel, A. Marasovic, J. D. Hwang, L. Lee, J. Da, R. Zellers, R. Mankoff, and Y. Choi. Do androids laugh at electric sheep? humor understanding benchmarks from the new yorker caption contest. In ACL, 2023. [7] Z. Horvitz, J. Chen, R. Aditya, H. Srivastava, R. West, Z. Yu, and K. McKeown. Getting serious about humor: Crafting humor datasets with unfunny large language models. In ACL, 2024. [8] Y. Hu, W. Shi, X. Fu, D. Roth, M. Ostendorf, L. Zettlemoyer, N. A. Smith, and R. Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. [9] N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. 11 [10] H. Kim, M. Sclar, T. Zhi-Xuan, L. Ying, S. Levine, Y. Liu, J. B. Tenenbaum, and Y. Choi. Hypothesis-driven theory-of-mind reasoning for large language models. arXiv preprint arXiv:2502.11881, 2025. [11] G. Lakoff and M. Johnson. Metaphors we live by. University of Chicago press, 2008. [12] W. Li, L. Li, T. Xiang, X. Liu, W. Deng, and N. Garcia. Can multiple-choice questions really be useful in detecting the abilities of llms? arXiv preprint arXiv:2403.17752, 2024. [13] Z. Li, R. Luo, J. Zhang, M. Qiu, and Z. Wei. Vocot: Unleashing visually grounded multi-step reasoning in large multi-modal models. arXiv preprint arXiv:2405.16919, 2024. [14] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [15] Z. Liu, F. Fang, X. Feng, X. Du, C. Zhang, et al. Ii-bench: An image implication understanding benchmark for multimodal large language models. In NeurIPS, 2024. [16] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2024. [17] S. Menon, R. Zemel, and C. Vondrick. Whiteboard-of-thought: Thinking step-by-step across modalities. arXiv, 2024. [18] A. Myrzakhan, S. M. Bsharat, and Z. Shen. Open-llm-leaderboard: From multi-choice to open-style questions for llms evaluation, benchmark, and arena. arXiv preprint arXiv:2406.07545, 2024. [19] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [20] OpenAI. Learning to reason with llms, 2024. [21] B. Seed. Doubao-1.5-thinking-vision-pro, 2025. [22] G. Team. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [23] K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, et al. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [24] Q. Team. Qvq: To see the world with wisdom, 2024. [25] Q. Team. Qwq: Reflect deeply on the boundaries of the unknown, 2024. [26] K. Wang, J. Pan, W. Shi, Z. Lu, M. Zhan, and H. Li. Measuring multimodal mathematical reasoning with math-vision dataset. In NeurIPS, 2024. [27] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, Y. Fan, K. Dang, M. Du, X. Ren, R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [28] P. Wu and S. Xie. V*: Guided visual search as core mechanism in multimodal llms. arXiv preprint arXiv:2312.14135, 2023. [29] Z. Wu, X. Chen, Z. Pan, X. Liu, W. Liu, D. Dai, H. Gao, Y. Ma, C. Wu, B. Wang, et al. Deepseekvl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. [30] xAI. Grok 3 beta the age of reasoning agents, 2025. [31] G. Xu, P. Jin, H. Li, Y. Song, L. Sun, and L. Yuan. Llava-cot: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [32] Y. Xu, Y. Hua, S. Li, and Z. Wang. Exploring chain-of-thought for multi-modal metaphor detection. In ACL, 2024. [33] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, G. Dong, H. Wei, H. Lin, J. Tang, J. Wang, J. Yang, J. Tu, J. Zhang, J. Ma, J. Xu, J. Zhou, J. Bai, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [34] Y. Yang, Z. Li, Q. Dong, H. Xia, and Z. Sui. Can large multimodal models uncover deep semantics behind images? In ACL, 2024. 12 [35] H. Yao, J. Huang, W. Wu, J. Zhang, Y. Wang, S. Liu, Y. Wang, Y. Song, H. Feng, L. Shen, and D. Tao. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. [36] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. [37] C. Zhang, X. Feng, Y. Bai, X. Du, et al. Can mllms understand the deep implication behind chinese images? arXiv preprint arXiv:2410.13854, 2024. [38] C. Zheng, H. Zhou, F. Meng, J. Zhou, and M. Huang. Large language models are not robust multiple choice selectors. In ICLR, 2024. [39] Zhipu.ai. Glm-4v, 2024. [40] S. Zhong, Z. Huang, S. Gao, W. Wen, L. Lin, M. Zitnik, and P. Zhou. Lets think outside the box: Exploring leap-of-thought in large language models with creative humor generation. arXiv preprint arXiv:2312.02439, 2024."
        },
        {
            "title": "A Algorithm",
            "content": "Algorithm 1: Let Androids Dream (LAD) Input: Image IM G, Task TM CQ, Task TOSQ Output: Answer AM CQ, Answer AOSQ // Stage I: Perception 1 img_dep MLLM.Perception(IM G) 2 keywords MLLM.Perception(img_dep) // Stage II: Search 3 search_qs MLLM.Plan(keywords) 4 all_qa 5 for each in search_qs do 6 strategy MLLM.Self-Judge(q) if strategy = WebSearch then answer WebSearch(q) end else if strategy = ModelSearch then answer ModelSearch(q) 7 8 9 10 12 end all_qa.add((q, answer)) 13 14 end 15 search_sum MLLM.Summary(img_dep, all_qa) /* Gen. description. */ /* Gen. 7 keywords */ /* 5 questions for image implication */ /* External knowledge */ /* Parametric knowledge */ /* Rank top-3, refine */ // Stage III: Reasoning 16 AM CQ MLLM.Reasoning(IM G, img_dep, keywords, search_sum, TM CQ) /* Explicit CoT */ 17 AOSQ MLLM.Reasoning(IM G, img_dep, keywords, search_sum, TOSQ) /* Explicit CoT */ 18 return AM CQ, AOSQ 19 Function WebSearch(q) // Planner: Decompose query sub_qs MLLM.RewriteQuery(q) // Searcher: Hierarchical retrieval snippets SearchAPI.BatchQuery(sub_qs) sel_urls MLLM.SelectPages(snippets, q) content PythonCrawler.FetchContent(sel_urls) // Summarizer: Generate answer summary MLLM.Summary(content, q) return summary 20 21 23"
        },
        {
            "title": "B Experiment Setup",
            "content": "/* Titles, summaries, URLs */ We use the lightweight GPT-4o-mini-0718 [19] with LAD framework in experiments. We set the model temperature as 0.5 and top_p as 0.9 in MCQ experiments, and temperature as 0.7 and top_p as 0.9 in OSQ experiments. Additionally, we set the evaluation model GPT-4o temperature as 0 and evaluate more than three times to get the average score in OSQ experiments. All experiments are conducted on NVIDIA A800 GPUs. Human-Model Consistency Study To validate our automated OSQ evaluation based on the GPT-4o model, we conduct human-model consistency study. We construct dedicated dataset by randomly selecting 25 images with questions each from our English and Chinese OSQ. We recruit 16 PhD students and researchers, all proficient in both English and Chinese and experienced with metaphorical imagery, to independently score the model responses. Their evaluations are based on ground truth answers and detailed scoring standard. We calculate human inter-annotator agreement by averaging the scores for each response after discarding the highest and lowest individual scores. This process yields the consistency of 94.8% for Chinese and 96.5% for English. The average human-model scoring consistency reached 95.7%, affirming the methods validity for assessing image implication comprehension."
        },
        {
            "title": "D Prompts",
            "content": "In experiments, the prompts of different settings are as follows: D.1 Evaluation Figure 5: The evaluation prompt of Open-Style Question (OSQ). D.2 End2End Figure 6: The end2end prompt of Multiple-Choice Question (MCQ). Figure 7: The end2end prompt of Open-Style Question (OSQ). 15 D.3 CoT Figure 8: The CoT prompt of Multiple-Choice Question (MCQ). Figure 9: The CoT prompt of Open-Style Question (OSQ)."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "Shanghai AI Laboratory",
        "The Chinese University of Hong Kong"
    ]
}