{
    "paper_title": "IGOR: Image-GOal Representations are the Atomic Control Units for Foundation Models in Embodied AI",
    "authors": [
        "Xiaoyu Chen",
        "Junliang Guo",
        "Tianyu He",
        "Chuheng Zhang",
        "Pushi Zhang",
        "Derek Cathera Yang",
        "Li Zhao",
        "Jiang Bian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Image-GOal Representations (IGOR), aiming to learn a unified, semantically consistent action space across human and various robots. Through this unified latent action space, IGOR enables knowledge transfer among large-scale robot and human activity data. We achieve this by compressing visual changes between an initial image and its goal state into latent actions. IGOR allows us to generate latent action labels for internet-scale video data. This unified latent action space enables the training of foundation policy and world models across a wide variety of tasks performed by both robots and humans. We demonstrate that: (1) IGOR learns a semantically consistent action space for both human and robots, characterizing various possible motions of objects representing the physical interaction knowledge; (2) IGOR can \"migrate\" the movements of the object in the one video to other videos, even across human and robots, by jointly using the latent action model and world model; (3) IGOR can learn to align latent actions with natural language through the foundation policy model, and integrate latent actions with a low-level policy model to achieve effective robot control. We believe IGOR opens new possibilities for human-to-robot knowledge transfer and control."
        },
        {
            "title": "Start",
            "content": "IGOR: Image-GOal Representations IGOR: Image-GOal Representations Atomic Control Units for Foundation Models in Embodied AI Xiaoyu Chen*, , Zhao*,, and Jiang Bian *Equal contributions, Microsoft Research, , Junliang Guo*, , Tianyu He*, , Chuheng Zhang*, , Pushi Zhang, Derek Yang , Li Tsinghua University We introduce Image-GOal Representations (IGOR), aiming to learn unified, semantically consistent action space across human and various robots. Through this unified latent action space, IGOR enables knowledge transfer among large-scale robot and human activity data. We achieve this by compressing visual changes between an initial image and its goal state into latent actions. IGOR allows us to generate latent action labels for internet-scale video data. This unified latent action space enables the training of foundation policy and world models across wide variety of tasks performed by both robots and humans. We demonstrate that: (1) IGOR learns semantically consistent action space for both human and robots, characterizing various possible motions of objects representing the physical interaction knowledge; (2) IGOR can migrate the movements of the object in the one video to other videos, even across human and robots, by jointly using the latent action model and world model; (3) IGOR can learn to align latent actions with natural language through the foundation policy model, and integrate latent actions with low-level policy model to achieve effective robot control. We believe IGOR opens new possibilities for human-to-robot knowledge transfer and control. Keywords: World Models, Foundation Agents Website: https://aka.ms/project-igor 4 2 0 2 7 1 ] . [ 1 5 8 7 0 0 . 1 1 4 2 : r Figure 1: Image-GOal Representations (IGOR) based training framework for embodied AI. IGOR learns unified latent action space for humans and robots by compressing visual changes between an image and its goal state on data from both robot and human activities. By labeling latent actions, IGOR facilitates the learning of foundation policy and world models from internet-scale human video data, covering diverse range of embodied AI tasks. With semantically consistent latent action space, IGOR enables human-to-robot generalization. The foundation policy model acts as high-level controller at the latent action level, which is then integrated with low-level policy to achieve effective robot control. Project Lead: Li Zhao (lizo@microsoft.com) 1 IGOR: Image-GOal Representations"
        },
        {
            "title": "1 Introduction",
            "content": "Learning foundation models for embodied AI has been notably constrained by lack of interaction data. Unlike text or video data, which are abundantly available, interaction data is much scarcer. Research efforts have been devoted to creating large-scale interaction datasets, such as Open-X-Embodiment (Collaboration et al., 2023) and DROID (Khazatsky et al., 2024). Based on multi-task interaction data, series of generalist agents (or foundation policy models) have been proposed, such as RT-1 (Brohan et al., 2022), Robocat (Bousmalis et al., 2023), RT-2 (Brohan et al., 2023), Octo (Team et al., 2024), and OpenVLA (Kim et al., 2024). However, the volume of interaction data remains several orders of magnitude smaller than that of internet text or video data. Given that the success of foundation models relies on scaling up datasets and extracting knowledge from such large-scale datasets, it is essential to design methods for building embodied AI foundation models that can effectively utilize internet-scale video data. Internet-scale video data contains abundant sequential records of human activities and perfect demonstrations of how human perform various tasks by interacting with the real world. When the human brain extracts information from videos, instead of doing it frame by frame, it modularizes the differences between frames into single word such as move, open, close. We refer to these highly compressed, modularized actions as latent actions that are shared across different tasks. The question to ask here is, is it possible to recover latent actions from video datasets with humans and robots performing various real embodied AI tasks? While recent works such as Genie (Bruce et al., 2024) and LAPO (Schmidt & Jiang, 2023) made attempts in recovering such latent actions from videos, they primarily focus on 2D platformer games where each latent action at corresponds to specific control button. The action space is highly designed to fit specific scenario and incomparable to the complex human and robot action space in various embodied AI tasks. To take step further, the question would be, can we learn unified, semantically consistent latent action space, allowing the transfer of knowledge across different tasks, and embodiments including human and various robots? In this paper, we propose Image-GOal Representations (IGOR), which learns unified and semantically consistent latent action space shared across different tasks and embodiments, enabling the knowledge transfer among internet-scale video data. We propose latent action model designed to capture robot and human actions across various embodied AI tasks. IGOR compresses the visual changes between an image and its goal state into latent actions, which are also embeddings of sub-tasks defined by reaching the goal from the initial image. IGOR is trained by minimizing the reconstruction loss of the goal state, which is predicted based on the image and the latent action. The core insight behind IGOR is that if compressed sufficiently, the image-goal pairs with similar visual changes will have similar embeddings. We argue that, besides text embeddings for human instruction understanding and image/video embeddings for state understanding, image-goal representations for latent action learning and sub-task understanding are yet another crucial building blocks, which may hold great potential for next-level generalization in embodied AI. With the latent action model, we can transform internet-scale human video data into interaction data labeled with latent actions, which largely expands the data available to building embodied AI foundation models. This unified latent action space allows us to train foundation policy and world models on nearly arbitrary tasks performed by robots and humans. Specifically, we train foundation policy model on large-scale video data with text labels. This model uses text to describe tasks and makes decisions, generating the next latent action to perform. Additionally, we train foundation world model on the same dataset, learning to simulate the outcome of executing the foundation policy model. Image-Goal Representations can be viewed as atomic control units in visual space. They function both as latent actions for foundational policy model to predict in visual trajectory planning and as sub-tasks for robot-specific low-level policy to execute. We train our models on human video data and robot data with actions removed, with RT-1 dataset held out for OOD evaluation. First, we evaluate the latent action model qualitatively, and find that image-goal pairs with similar latent actions have similar visual changes, corresponding to semantically consistent movements, even on OOD scenarios. Then we evaluate the world model by extracting latent actions from video and applying such latent action (or action sequence) to the initial frames of other videos, generating the rest of frames. We find that, jointly with the latent action model and world model, 2 IGOR: Image-GOal Representations Figure 2: We extract latent actions from Image-Goal pairs in the solid line boxes, and apply the latent actions to different initial frames, generating subsequent videos via world model as shown in the corresponding dashed boxes. The first half illustrates examples from real-world videos with diverse object categories, while the second half demonstrates generalization from human to robot arms. Full videos are available on our website. IGOR successfully migrates the movements of the object in the one video to other videos, as shown in Figure 2. We also apply different latent actions to the same initial image, and find that the world model has learned various possible movements of the object in the image, suggesting that it has absorbed the physical interaction knowledge. For the foundation policy model, we show its ability in following diverse language instruction via iteratively rolling out the foundation policy and world model using latent actions. We further integrate it with low-level policy, and show that IGOR-based policy training can improve performance on Google Robot tasks in low-data regime with the SIMPLER (Li et al., 2024) simulator. 3 IGOR: Image-GOal Representations"
        },
        {
            "title": "2.1 Latent Action Model",
            "content": "The primary objective of the latent action model is to label latent actions from unlabeled open-domain videos in an unsupervised manner. Given sequence of video frames o1:t+1, the goal is to derive the latent action at, which captures the key information describing only the changes that occur at time step t, removing other redundant information. In contrast to prior works (Schmidt & Jiang, 2023; Bruce et al., 2024), which primarily focus on 2D platformer games where each latent action at corresponds to specific control button, we aim to develop more generalizable model. Our model is designed to handle the significantly greater complexity of open-world scenarios, where latent actions may not correspond to any specific underlying actions. This presents several additional challenges. First, rather than focusing solely on absolute position of pixel changes, the latent action model must learn to capture semantic movements that remain consistent across varying scenarios. Moreover, due to the temporal redundancy, actions are often sparse given long contexts, which can lead the model to infer ot+1 directly from the history, bypassing the need for more informative latent action at. To address these issues, we propose novel model architecture. Our latent action model consists of pair of Inverse Dynamics Model (IDM) and Forward Dynamics Model (FDM). IDM is trained to predict the latent action at based on the full sequence of observations o1:t+1. Instead of using the raw observations, we first apply random cropping c1 to the inputs: at = (c1[o1:t+1]). For the architecture of I, we first extract features for each frame through Vision Transformer (ViT) (Dosovitskiy et al., 2021) and then adopt Spatio-Temporal transformer (ST-transformer) (Bruce et al., 2024; Xu et al., 2021) with temporal causal mask as the backbone. Learnable readout tokens are then used to extract and compress the visual changes into tokens. To further compress the information stored in latent action, we apply vector quantization to each token, restricting them to discrete codebook of size C. Finally, we derive the latent action at RND where is the dimension of each code. We refer to at as the latent action embedding, or sub-task embedding, as they describe the information that takes the observation ot to the next observation ot+1. For the FDM F, we propose using single-frame Vision Transformer to reconstruct ot+1, in contrast to previous works (Schmidt & Jiang, 2023; Bruce et al., 2024), which reconstruct the next frame given the entire context o1:t. This approach mitigates the case where the model might predict the next frame directly from the context, bypassing the latent action. By conditioning on single frame, it encourages more information to flow into the latent action at. For reconstruction, we apply another random cropping c2, and the next frame is predicted as ˆot+1 = (c2[ot], at). By using different croppings c1 and c2, the model is encouraged to learn more semantically invariant latent action across different trajectories. The models are trained jointly with the reconstruction loss c2[ot+1] ˆot+12 and the commitment loss in vector quantization."
        },
        {
            "title": "2.2 Foundation World Model",
            "content": "Our foundation world model is continuous-time Rectified Flow (Liu et al., 2023b; Esser et al., 2024) that learns to predict the future frames ot+1:T conditioned on the history observation frame o1:t, and future latent actions at:T1. To achieve this goal, there are two key challenges: 1) Generating the photo-realistic frame that describes the states precisely; 2) Controlling the generated frames by the latent actions. Accordingly, we start our foundation world model with the pre-trained Open-Sora (Zheng et al., 2024). It consists of two components: 3D Variational AutoEncoder (VAE) that encodes the raw observation into latent space with 8 8 times downsampling in spatial dimension and 4 times downsampling in temporal dimension; Spatial-Temporal Rectified Flow Transformer (ST-RFT) that generates the latent from the text conditions. To enable the control from the observation and action, we make two modifications to the original Open-Sora: 1) We replace the original text input of the pre-trained model with our latent actions a1:T obtained from LAM. Zero-padding is applied for the last action. For each frame, we map the latent actions into single token and feed it to the ST-RFT via the cross-attention mechanism; 2) We also make the generation conditioned on the output of FDM ˆot+1:T, which provides coarse-grained prediction according to the input latent action. For the conditioning of ˆot+1:T, we encode it to the latent space with the same 3D VAE and directly add it to the noisy input element-wise. Formally, Rectified Flow (Liu et al., 2023b; Albergo & Vanden-Eijnden, 2023; Esser et al., 2024) aims at 4 IGOR: Image-GOal Representations directly regressing vector field that generates probability path between noise distribution and data distribution. For [0, 1], we define the interpolation between the two distributions as: xn = (1 n)x0 + nx1, where x0 is the clean data, x1 is the sampled noise, and xn is the noisy data. During training, we train vector-valued neural network xθ with L2 loss: (1) En,x0,x1 x0 xθ(xn, n, at:T1, ˆot+1:T)2. Instead of predicting the conditional expectation directly, we follow Liu et al. (2023b) to parameterize the velocity with neural network vθ and train it on: (2) Lworld(θ) = En,x0,x1 (x1 x0) vθ(xn, n, at:T1, ˆot+1:T)2. (3) It should be noted that, our foundation world model can be fine-tuned to accommodate the different action spaces of robots with various embodiments. The fine-tuning of the foundational world model is left as future work."
        },
        {
            "title": "2.3 Foundation Policy Model and Low-level Policy Model",
            "content": "The training of the policy model consists of two stages. In the first pretraining stage, taken as input the raw observation frames o1:t and textual description for the task, the foundation policy model predicts latent actions at = I([o1:t+1]) labeled by the IDM in the latent action model at each step. The training dataset of this stage is the same as that used for the latent action model, i.e., with large-scale and diverse sources of videos. In the second finetuning stage, we add an extra prediction component on the foundation policy model to predict real continuous robot actions, with taking the raw observations as well as the latent actions predicted by the first stage model as input. In this stage, only the prediction component (i.e., the low-level policy model) is optimized on small-scale and task-specific downstream datasets, while other components are frozen. Specifically, similar to the latent action model, the backbone of foundation policy model is also STtransformer equipped with ViT image encoder, with feed-forward layer as the final prediction layer. The textual description is encoded to latent representation by pre-trained text encoder, which is concatenated with the observation representation encoded by the ViT encoder as the joint input to the model. We use the L2 distance between the predicted hidden output and the latent action as the loss function. Given trajectory consists of observations o1:t, the training objective can be written as: Lpolicy = P([s; o1:t]) at2, (4) where P() denotes the policy model. In the second stage, we train the low-level policy model to predict the real continuous actions within each latent action, where the image-goal latent actions can be seen as representations for sub-tasks defined by reaching goal from an initial image. The low-level policy model is also an ST-transformer with prediction layer. The input consists of the textual representation s, the observation o1:t and latent actions predicted by the foundation policy model P([s; o1:t]), which are concatenated together at the patch level as one part. The latent action P([s; o1:t]) predicted by the foundation policy model also serves as sub-task embedding for the low-level policy model. We denote that each latent action corresponds to τ real robot actions, and the latent action at corresponds to real robot action u1:τ . Denote the low-level policy model as Pf (), we train the second stage model also by L2 distance: t = Pf ([s; P([s; o1:t]); o1:t]) u1:τ 2, (5) where only the parameters of the low-level policy are optimized."
        },
        {
            "title": "3.1 Dataset",
            "content": "In the pretraining stage, we construct large-scale dataset comprising diverse domains, including robotic data from various embodiments and substantial amount of human activity videos. 5 IGOR: Image-GOal Representations Figure 3: Image-goal pairs with similar latent actions in OOD RT-1 dataset. In each row, we choose the leftmost image-goal pair, and retrieve 3 nearest pairs on latent action embedding. The original task instructions of the pairs are shown under the images. We find that each row shares the similar visual changes semantically, and the latent actions generalize across different raw language tasks. Data Mixture. For the robotic data, we select subset of Open-X Embodiment dataset (Collaboration et al., 2023) with single arm end-effector control, excluding RT-1 dataset for out-of-distribution (OOD) evaluation. We follow the preprocessing and data mixture weights from Team et al. (2024); Kim et al. (2024). In total, we utilize approximately 0.8M robot trajectories. While our dataset includes data from real robots, we discard the associated actions and proprio-states, using only image frames and text instructions during pretraining. Additionally, we incorporate large-scale open-world videos with language instructions, including human daily activities from Something-Something v2 (Goyal et al., 2017), and egocentric videos such as EGTEA (Li et al., 2018), Epic Kitchen (Damen et al., 2020), and Ego4D (Grauman et al., 2022; Pramanick et al., 2023). In total, we derive approximately 2.0M human activity video clips with high quality. Overall, our dataset for pretraining comprises around 2.8M trajectories and video clips, where each trajectory contains language instruction and sequence of observations. Data Preprocessing. In practice, we found that the video quality has big impact on the model performance. We exclude low-quality videos characterized by excessive shakiness or rapid camera movement, and apply stabilization techniques to the remaining videos. To ensure proper amount of changes between frames in the latent action model, we choose the optimal frame rates for robotics dataset and human activity videos. In the finetuning stage, we use RT-1 dataset, large-scale dataset for real-world robotic experiences. We uniformly sample 1% number of episodes from RT-1 dataset for finetuning, where each episode comprises language instruction, sequence of image observations, and sequence of low-level actions. The action space is 7-dimensional, including 3 dimensions of robot arm movement Pos, 3 dimensions of robot arm rotation Rot, and 1 dimension of robot gripper action Grp. We provide more details in Appendix A."
        },
        {
            "title": "3.2 Training Details",
            "content": "We first pretrain our latent action model on our pretraining dataset. Then, we use the pretrained latent action model to label latent actions on our pretraining dataset, and pretrain foundation policy model and foundation world model on the labeled dataset. Finally, we finetune our low-level policy model on top of our pretrained models on RT-1. For latent action model, we use codebook with = 4 tokens, and codebook size of = 32, each with 6 IGOR: Image-GOal Representations Figure 4: Controllability of latent action among multiple objects. The last two rows show the generated image by applying 6 different latent actions to the initial frame. Effects of applying different latent actions are highlighted in dashed squares: (a,b) move the apple, (c,d) move the tennis, (e,f) move the orange. Full generated videos from the world model are available on our webpage. an embedding size of = 128. We use sub-task length of τ = 3 for finetuning the low-level policy model on RT-1 dataset. Please refer to Appendix for more training details."
        },
        {
            "title": "3.3 Qualitative Results on Latent Actions",
            "content": "We present qualitative results on latent actions learned from robotics and human activity dataset. Specifically, we answer the following questions on learned latent actions: Do similar latent actions reflect similar visual changes? Can latent actions encode semantically consistent changes across different tasks, and embodiments including human and robots? If so, are we able to migrate movements in videos across embodiments and tasks via latent action? Does the policy foundation model properly follow language instructions for task solving? 3.3.1 Visualization of Image-Goal Pairs with Similar Latent Actions We investigate whether similar learned latent actions reflect similar visual changes on robotics manipulation dataset. We use RT-1 dataset, which was excluded from the latent action model training and serves as out-of-domain samples for evaluation. We randomly select image-goal pairs from RT-1 dataset, and present the image-goal pairs with smallest euclidean distance in latent action embedding in RT-1 dataset in Figure 3. We observe that pairs with similar embeddings indeed have similar visual changes, and also similar sub-tasks in semantic, for example, open the gripper, move left, and close the gripper. Furthermore, each sub-task appears in different raw language tasks, suggesting the latent actions are reused, thereby facilitating generalization in model learning. 3.3.2 Controllability of Latent Actions We demonstrate that latent actions are able to control the changes in objects on different real world scenes, and the effects of latent actions generalize across tasks and embodiments. Specially, the generalizability of latent actions enable IGOR to successfully migrate human movement videos into robot movements provided the initial image, despite they largely differ in embodiments. Object Controllability Among Multiple Objects. We evaluate the controllability of the latent actions on object movements among multiple objects on the same image. In Figure 4, we generate subsequent 7 IGOR: Image-GOal Representations Figure 5: Generated image sequence jointly by the foundation policy and world model via only latent actions, following 3 different instructions from the same initial image. Full generated videos from the world model are available on our webpage. images by applying 6 different actions to the same original image on the foundation world model. We observe that the latent action model and foundation world model learn to control specific objects movement among multiple objects. Object Controllability Across Embodiments and Tasks. We evaluate the semantic consistency of the latent actions across different setups, including embodiments and tasks. We use pairs of image-goal in the real world manipulation videos to generate latent actions, and apply the same set of actions to other images in different scene setups with foundation world model to generate subsequent videos. The results are shown in Figure 2. Impressively, we observe that (1) latent actions are semantically consistent across different tasks with different object categories; (2) latent actions are semantically consistent across human and robots. By applying latent actions extracted from human demonstrations, we generate videos of robot arm movements. With only one demonstration, the robot arm can successfully migrate human behaviors, which opens up new possibilities for few-shot human-to-robot transfer and control. 3.3.3 Counterfactual Video Generation with Diverse Instructions We analyze whether the foundation policy model has the ability to follow human instructions. To this end, we interpret the effect of latent actions visually with the foundation world model. Starting from single initial image, the foundation policy and world model can jointly generate diverse behaviors in videos that follow diverse instructions using only latent actions. We experiment with initial images from RT-1 and Bridge dataset and manually written instructions, and show the image clips of generated videos in Figure 5. The results show that the foundation policy model can properly follow different language instructions for task solving."
        },
        {
            "title": "3.4 Quantitative Results",
            "content": "3.4.1 Evaluation on the Google Robot Tasks in SIMPLER We evaluate our IGOR-based training framework on the Google robot tasks in the SIMPLER simulator under low-data regime, utilizing only 1% of the data from the large RT-1 dataset for the low-level policy learning stage. Evaluation Setups. We test different models ability to control the Google Robot following language tasks with RGB images as observations, where all robots are controlled with low-level end-effector control actions, after finetuning on the same amount of data from RT-1 dataset. We evaluate the success rate on three tasks: Pick Coke Can, Move Near, and Open / Close Drawer. 8 IGOR: Image-GOal Representations Figure 6: (a). Success rate of IGOR and the low-level policy trained from scratch methods on Google Robot tasks under SIMPLER simulator, finetuned on 1% data of RT-1. (b). Predictiveness of latent action on robot action. X-axis: log(N), where is the number of nearest neighbours in latent action embedding. Y-axis: normalized standard deviation in action embedding with respect to movement actions (orange), rotation actions (blue), and gripper actions (green). Baseline Method. We compare our method with the same low-level policy model architecture with ST-Transformer, without latent action embedding concatenated on the observation feature embedding. We present the success rate of different methods in Figure 6(a). From the figure, we observe that IGOR achieves higher or equal success rate than the model trained from scratch, showing the generalizability of the learned latent action to real robotics actions. 3.4.2 Predictiveness of Latent Actions on Robot Actions We analyze whether our learned latent actions are predictive of real robot actions. On RT-1 dataset, we randomly sample number of = 15, 000 pairs of images, and compute their latent action embeddings. For each pair of image, we find nearest neighbours of image pairs in RT-1 dataset with the closest latent action embedding, and compute the standard deviation of real robot actions among neighbours on each action dimension, normalized by the standard deviation of robot actions over each dimension over the whole RT-1 dataset. By varying N, we assess whether closer latent actions correspond to more similar downstream actions. The results are shown in Figure 6(b). The fact that smaller leading to lower normalized standard deviation, and all normalized standard deviation being below 1.0, show that the latent actions are predictive of real robot actions including robot movements, rotations and gripper actions. It is also shown that the latent actions are more predictive of the robot movement than rotations and gripper actions, suggesting that the IGOR learned action space reflects more information in robot movements than robot arm rotations and gripping."
        },
        {
            "title": "3.5 Ablation Studies",
            "content": "We provide additional ablation studies on the pretraining dataset of latent action model, showing that using mixture of robotics and human activity dataset benefits the generalization of latent action model. Detailed ablation studies results are provided in Appendix C."
        },
        {
            "title": "4 Related Work",
            "content": "Foundation Agents for Robots Open-ended task-agnostic training and high-capacity neural network architectures have been recognized as key to the success of foundation models. In this context, series of generalist agents have been proposed as the foundation policy models for robots (Brohan et al., 2022; Bousmalis et al., 2023; Brohan et al., 2023; Team et al., 2024; Kim et al., 2024). RT-1 (Brohan et al., 2022) contributes large-scale multi-task dataset and robotic transformer architecture,facilitating and assessing generalization across multiple tasks. RoboCat builds on Gato (Reed et al., 2022), further 9 IGOR: Image-GOal Representations enabling multi-embodiment generalization. RT-2 highlights the importance of leveraging vision-language models trained on internet-scale data (Brohan et al., 2023). Octo (Team et al., 2024) and OpenVLA (Kim et al., 2024) can be seen as open versions of RoboCat and RT-2 respectively, with some additional technical contributions. IGOR is similar to RT-2 and OpenVLA in the sense that we both leverage Internet-scale data. The difference lies in that we use video data (with text labels) of human/robot performing embodied AI tasks, while they use text data and visual question answering data for the training of vision language models. To the best of our knowledge, we present the first foundation policy model that performs decision making at the sub-task (i.e. latent action) level. Image-Goal Visual Changes Tracking visual changes and establishing correspondence between an image and its goal state is crucial for dynamic visual understanding in embodied AI. SiamMAE (Gupta et al., 2023) proposes to use siamese encoder on the image and goal to learn visual correspondence. Voltron (Karamcheti et al., 2023) introduces language-guided visual representation learning on imagegoal pairs. FLOWRETRIEVAL (Lin et al., 2024) and AVDC (Ko et al., 2023) leverage optical flow between image and goal to capture visual changes and correspondence, while Video-LaVIT (Jin et al., 2024) utilizes motion vectors. iVideoGPT (Wu et al., 2024) proposes using image-conditioned goal representations as state representations to predict within world model. VPT (Baker et al., 2022) proposes to recover latent actions in videos using an inverse dynamics model trained on interaction data to predict real actions. Perhaps the most similar approaches to our methods are LAPO (Schmidt & Jiang, 2023) and Genie (Bruce et al., 2024). Both works primarily focus on 2D platformer games where each latent action corresponds to specific control button. By contrast, we aim to develop more generalizable model to handle the significantly greater complexity of open-world scenarios, where latent actions may not correspond to any specific underlying actions. Video Generation for Embodied AI Video generation is another research topic closely related to embodied AI. It has been proposed that video can be seen as the new language for real-world decision making (Yang et al., 2024b). Many works on world models build on video generation techniques (Bruce et al., 2024; Wu et al., 2024; Hu et al., 2023; Yang et al., 2024a; Xiang et al., 2024). Some text-to-video works claim to be real-world simulators, such as Sora (Brooks et al., 2024) and WorldDreamer (Wang et al., 2024). Unipi (Du et al., 2023) proposes to first predict the next goal state, then infer real robot actions with an inverse dynamics model. By contrast, our foundation policy model first predicts the latent action, which can specify the goal state, and then uses the latent action to enable sub-task level generalization. We argue that forward prediction in latent action space, rather than the original image space, offers several advantages. For example, we can perform sub-task understanding for image-goal representations, and the compressed latent action could be easier to predict than the entire image. Pre-trained Visual Representations Pre-trained Visual Representations target on training representations for images/videos in self-supervised learning manner (He et al., 2021; Xiao et al., 2022; Radosavovic et al., 2022; Majumdar et al., 2023; Radford et al., 2021; Nair et al., 2022; Ma et al., 2023; Oquab et al., 2023; Darcet et al., 2023; Kirillov et al., 2023; Assran et al., 2023; Bardes et al., 2024), and has been demonstrated to be very effective for state understanding in embodied AI. By contrast, IGOR learns image-goal representations for sub-task understanding, which we believe are another crucial building blocks, that may significantly enhance generalization in embodied AI."
        },
        {
            "title": "5 Conclusions, Limitations, and Future Work",
            "content": "In this paper, we propose IGOR, novel training framework, taking the first step towards learning unified action space for humans and robots in various embodied AI tasks. Qualitatively, we demonstrate that: IGOR learns similar representations for image pairs with similar visual changes. The learned latent action has control over the next state given the current image. The foundation world model acquires knowledge about objects and their potential movements. The foundation policy model learns to follow instructions across different states. Quantitatively, we show that: 10 IGOR: Image-GOal Representations On RT-1 dataset, image-goal pairs with similar latent actions are associated with similar low-level robot actions. The IGOR framework improves policy learning, potentially due to its capability to predict the next sub-task by leveraging internet-scale data, thereby enabling sub-task level generalization. The IGOR framework is limited in the following perspective: we cannot separate visual changes caused by the agents, other agents (such as dogs), or the shakiness of camera. To address this, we mitigated camera shakiness and used only ego-centric videos without other agents in view. Just like any other representation learning methods, scaling up the dataset and model size is always most straightforward and effective. To facilitate the usage of more data, incorporating image processing methods such as object segmentation with IGOR will be part of future works. For better applications in embodied AI, the foundation world model can also be tuned to match real world scenarios, along with other improvements such as adapting the latent action model for multi-agent scenarios. References Albergo, M. S. and Vanden-Eijnden, E. Building normalizing flows with stochastic interpolants. In International Conference on Learning Representations, 2023. Assran, M., Duval, Q., Misra, I., Bojanowski, P., Vincent, P., Rabbat, M., LeCun, Y., and Ballas, N. Self-supervised learning from images with joint-embedding predictive architecture, 2023. URL https://arxiv.org/abs/2301.08243. Baker, B., Akkaya, I., Zhokhov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton, B., Sampedro, R., and Clune, J. Video pretraining (vpt): Learning to act by watching unlabeled online videos, 2022. URL https://arxiv.org/abs/2206.11795. Bardes, A., Garrido, Q., Ponce, J., Chen, X., Rabbat, M., LeCun, Y., Assran, M., and Ballas, N. Revisiting feature prediction for learning visual representations from video, 2024. URL https://arxiv.org/ abs/2404.08471. Belkhale, S., Cui, Y., and Sadigh, D. Hydra: Hybrid robot actions for imitation learning. arxiv, 2023. Bousmalis, K., Vezzani, G., Rao, D., Devin, C., Lee, A. X., Bauza, M., Davchev, T., Zhou, Y., Gupta, A., Raju, A., Laurens, A., Fantacci, C., Dalibard, V., Zambelli, M., Martins, M., Pevceviciute, R., Blokzijl, M., Denil, M., Batchelor, N., Lampe, T., Parisotto, E., Zołna, K., Reed, S., Colmenarejo, S. G., Scholz, J., Abdolmaleki, A., Groth, O., Regli, J.-B., Sushkov, O., Roth orl, T., Chen, J. E., Aytar, Y., Barker, D., Ortiz, J., Riedmiller, M., Springenberg, J. T., Hadsell, R., Nori, F., and Heess, N. Robocat: self-improving generalist agent for robotic manipulation, 2023. URL https://arxiv.org/abs/2306.11706. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., Florence, P., Fu, C., Arenas, M. G., Gopalakrishnan, K., Han, K., Hausman, K., Herzog, A., Hsu, J., Ichter, B., Irpan, A., Joshi, N., Julian, R., Kalashnikov, D., Kuang, Y., Leal, I., Lee, L., Lee, T.-W. E., Levine, S., Lu, Y., Michalewski, H., Mordatch, I., Pertsch, K., Rao, K., Reymann, K., Ryoo, M., Salazar, G., Sanketi, P., Sermanet, P., Singh, J., Singh, A., Soricut, R., Tran, H., Vanhoucke, V., Vuong, Q., Wahid, A., Welker, S., Wohlhart, P., Wu, J., Xia, F., Xiao, T., Xu, P., Xu, S., Yu, T., and Zitkovich, B. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In arXiv preprint arXiv:2307.15818, 2023. Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., and Ramesh, A. Video generation models as world simulators. 2024. URL https://openai.com/research/video-generation-models-as-world-simulators. Bruce, J., Dennis, M. D., Edwards, A., Parker-Holder, J., Shi, Y., Hughes, E., Lai, M., Mavalankar, A., Steigerwald, R., Apps, C., et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 11 IGOR: Image-GOal Representations Chen, L. Y., Adebola, S., and Goldberg, K. Berkeley UR5 demonstration dataset. https://sites.google. com/view/berkeley-ur5/home. Collaboration, O. X.-E., ONeill, A., Rehman, A., Maddukuri, A., Gupta, A., Padalkar, A., Lee, A., Pooley, A., Gupta, A., Mandlekar, A., Jain, A., Tung, A., Bewley, A., Herzog, A., Irpan, A., Khazatsky, A., Rai, A., Gupta, A., Wang, A., Kolobov, A., Singh, A., Garg, A., Kembhavi, A., Xie, A., Brohan, A., Raffin, A., Sharma, A., Yavary, A., Jain, A., Balakrishna, A., Wahid, A., Burgess-Limerick, B., Kim, B., Sch olkopf, B., Wulfe, B., Ichter, B., Lu, C., Xu, C., Le, C., Finn, C., Wang, C., Xu, C., Chi, C., Huang, C., Chan, C., Agia, C., Pan, C., Fu, C., Devin, C., Xu, D., Morton, D., Driess, D., Chen, D., Pathak, D., Shah, D., uchler, D., Jayaraman, D., Kalashnikov, D., Sadigh, D., Johns, E., Foster, E., Liu, F., Ceola, F., Xia, F., Zhao, F., Frujeri, F. V., Stulp, F., Zhou, G., Sukhatme, G. S., Salhotra, G., Yan, G., Feng, G., Schiavi, G., Berseth, G., Kahn, G., Yang, G., Wang, G., Su, H., Fang, H.-S., Shi, H., Bao, H., Amor, H. B., Christensen, H. I., Furuta, H., Walke, H., Fang, H., Ha, H., Mordatch, I., Radosavovic, I., Leal, I., Liang, J., Abou-Chakra, J., Kim, J., Drake, J., Peters, J., Schneider, J., Hsu, J., Bohg, J., Bingham, J., Wu, J., Gao, J., Hu, J., Wu, J., Wu, J., Sun, J., Luo, J., Gu, J., Tan, J., Oh, J., Wu, J., Lu, J., Yang, J., Malik, J., Silverio, J., Hejna, J., Booher, J., Tompson, J., Yang, J., Salvador, J., Lim, J. J., Han, J., Wang, K., Rao, K., Pertsch, K., Hausman, K., Go, K., Gopalakrishnan, K., Goldberg, K., Byrne, K., Oslund, K., Kawaharazuka, K., Black, K., Lin, K., Zhang, K., Ehsani, K., Lekkala, K., Ellis, K., Rana, K., Srinivasan, K., Fang, K., Singh, K. P., Zeng, K.-H., Hatch, K., Hsu, K., Itti, L., Chen, L. Y., Pinto, L., Fei-Fei, L., Tan, L., Fan, L. J., Ott, L., Lee, L., Weihs, L., Chen, M., Lepert, M., Memmel, M., Tomizuka, M., Itkina, M., Castro, M. G., Spero, M., Du, M., Ahn, M., Yip, M. C., Zhang, M., Ding, M., Heo, M., Srirama, M. K., Sharma, M., Kim, M. J., Kanazawa, N., Hansen, N., Heess, N., Joshi, N. J., Suenderhauf, N., Liu, N., Palo, N. D., Shafiullah, N. M. M., Mees, O., Kroemer, O., Bastani, O., Sanketi, P. R., Miller, P. T., Yin, P., Wohlhart, P., Xu, P., Fagan, P. D., Mitrano, P., Sermanet, P., Abbeel, P., Sundaresan, P., Chen, Q., Vuong, Q., Rafailov, R., Tian, R., Doshi, R., Martin-Martin, R., Baijal, R., Scalise, R., Hendrix, R., Lin, R., Qian, R., Zhang, R., Mendonca, R., Shah, R., Hoque, R., Julian, R., Bustamante, S., Kirmani, S., Levine, S., Lin, S., Moore, S., Bahl, S., Dass, S., Sonawani, S., Song, S., Xu, S., Haldar, S., Karamcheti, S., Adebola, S., Guist, S., Nasiriany, S., Schaal, S., Welker, S., Tian, S., Ramamoorthy, S., Dasari, S., Belkhale, S., Park, S., Nair, S., Mirchandani, S., Osa, T., Gupta, T., Harada, T., Matsushima, T., Xiao, T., Kollar, T., Yu, T., Ding, T., Davchev, T., Zhao, T. Z., Armstrong, T., Darrell, T., Chung, T., Jain, V., Vanhoucke, V., Zhan, W., Zhou, W., Burgard, W., Chen, X., Chen, X., Wang, X., Zhu, X., Geng, X., Liu, X., Liangwei, X., Li, X., Pang, Y., Lu, Y., Ma, Y. J., Kim, Y., Chebotar, Y., Zhou, Y., Zhu, Y., Wu, Y., Xu, Y., Wang, Y., Bisk, Y., Dou, Y., Cho, Y., Lee, Y., Cui, Y., Cao, Y., Wu, Y.-H., Tang, Y., Zhu, Y., Zhang, Y., Jiang, Y., Li, Y., Li, Y., Iwasawa, Y., Matsuo, Y., Ma, Z., Xu, Z., Cui, Z. J., Zhang, Z., Fu, Z., and Lin, Z. Open X-Embodiment: Robotic learning datasets and RT-X models. https://arxiv.org/abs/2310.08864, 2023. Cui, Z. J., Wang, Y., Shafiullah, N. M. M., and Pinto, L. From play to policy: Conditional behavior generation from uncurated robot data. arXiv preprint arXiv:2210.10047, 2022. Damen, D., Doughty, H., Farinella, G. M., Fidler, S., Furnari, A., Kazakos, E., Moltisanti, D., Munro, J., Perrett, T., Price, W., et al. The epic-kitchens dataset: Collection, challenges and baselines. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):41254141, 2020. Darcet, T., Oquab, M., Mairal, J., and Bojanowski, P. Vision transformers need registers, 2023. Dass, S., Yapeter, J., Zhang, J., Zhang, J., Pertsch, K., Nikolaidis, S., and Lim, J. J. CLVR jaco play dataset, 2023. URL https://github.com/clvrai/clvr_jaco_play_dataset. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. URL https://arxiv.org/abs/2010.11929. Du, Y., Yang, M., Dai, B., Dai, H., Nachum, O., Tenenbaum, J. B., Schuurmans, D., and Abbeel, P. Learning universal policies via text-guided video generation, 2023. URL https://arxiv.org/abs/2302.00111. Ebert, F., Yang, Y., Schmeckpeper, K., Bucher, B., Georgakis, G., Daniilidis, K., Finn, C., and Levine, S. Bridge data: Boosting generalization of robotic skills with cross-domain datasets. arXiv preprint arXiv:2109.13396, 2021. Esser, P., Kulal, S., Blattmann, A., Entezari, R., uller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 12 IGOR: Image-GOal Representations Goyal, R., Ebrahimi Kahou, S., Michalski, V., Materzynska, J., Westphal, S., Kim, H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M., et al. The something something video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pp. 58425850, 2017. Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1899519012, 2022. Gupta, A., Wu, J., Deng, J., and Fei-Fei, L. Siamese masked autoencoders, 2023. URL https://arxiv. org/abs/2305.14344. He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. Masked autoencoders are scalable vision learners, 2021. URL https://arxiv.org/abs/2111.06377. Heo, M., Lee, Y., Lee, D., and Lim, J. J. Furniturebench: Reproducible real-world benchmark for long-horizon complex manipulation. In Robotics: Science and Systems, 2023. Hu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A., Shotton, J., and Corrado, G. Gaia-1: generative world model for autonomous driving, 2023. URL https://arxiv.org/abs/2309.17080. Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., Levine, S., and Finn, C. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, pp. 9911002. PMLR, 2022. Jin, Y., Sun, Z., Xu, K., Xu, K., Chen, L., Jiang, H., Huang, Q., Song, C., Liu, Y., Zhang, D., Song, Y., Gai, K., and Mu, Y. Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization, 2024. URL https://arxiv.org/abs/2402.03161. Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. In CoRL, pp. 651673, 2018. Karamcheti, S., Nair, S., Chen, A. S., Kollar, T., Finn, C., Sadigh, D., and Liang, P. Language-driven representation learning for robotics. In Robotics: Science and Systems (RSS), 2023. Khazatsky, A., Pertsch, K., Nair, S., Balakrishna, A., Dasari, S., Karamcheti, S., Nasiriany, S., Srirama, M. K., Chen, L. Y., Ellis, K., Fagan, P. D., Hejna, J., Itkina, M., Lepert, M., Ma, Y. J., Miller, P. T., Wu, J., Belkhale, S., Dass, S., Ha, H., Jain, A., Lee, A., Lee, Y., Memmel, M., Park, S., Radosavovic, I., Wang, K., Zhan, A., Black, K., Chi, C., Hatch, K. B., Lin, S., Lu, J., Mercat, J., Rehman, A., Sanketi, P. R., Sharma, A., Simpson, C., Vuong, Q., Walke, H. R., Wulfe, B., Xiao, T., Yang, J. H., Yavary, A., Zhao, T. Z., Agia, C., Baijal, R., Castro, M. G., Chen, D., Chen, Q., Chung, T., Drake, J., Foster, E. P., Gao, J., Herrera, D. A., Heo, M., Hsu, K., Hu, J., Jackson, D., Le, C., Li, Y., Lin, K., Lin, R., Ma, Z., Maddukuri, A., Mirchandani, S., Morton, D., Nguyen, T., ONeill, A., Scalise, R., Seale, D., Son, V., Tian, S., Tran, E., Wang, A. E., Wu, Y., Xie, A., Yang, J., Yin, P., Zhang, Y., Bastani, O., Berseth, G., Bohg, J., Goldberg, K., Gupta, A., Gupta, A., Jayaraman, D., Lim, J. J., Malik, J., Martın-Martın, R., Ramamoorthy, S., Sadigh, D., Song, S., Wu, J., Yip, M. C., Zhu, Y., Kollar, T., Levine, S., and Finn, C. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv: 2403.12945, 2024. Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E., Lam, G., Sanketi, P., et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., Dollar, P., and Girshick, R. Segment anything, 2023. URL https://arxiv.org/abs/2304. 02643. Ko, P.-C., Mao, J., Du, Y., Sun, S.-H., and Tenenbaum, J. B. Learning to act from actionless videos through dense correspondences, 2023. URL https://arxiv.org/abs/2310.08576. Li, X., Hsu, K., Gu, J., Pertsch, K., Mees, O., Walke, H. R., Fu, C., Lunawat, I., Sieh, I., Kirmani, S., Levine, S., Wu, J., Finn, C., Su, H., Vuong, Q., and Xiao, T. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024. 13 IGOR: Image-GOal Representations Li, Y., Liu, M., and Rehg, J. M. In the eye of beholder: Joint learning of gaze and actions in first person video. In Proceedings of the European conference on computer vision (ECCV), pp. 619635, 2018. Lin, L.-H., Cui, Y., Xie, A., Hua, T., and Sadigh, D. Flowretrieval: Flow-guided data retrieval for few-shot imitation learning, 2024. URL https://arxiv.org/abs/2408.16944. Liu, H., Nasiriany, S., Zhang, L., Bao, Z., and Zhu, Y. Robot learning on the job: Human-in-the-loop autonomy and learning during deployment. In Robotics: Science and Systems (RSS), 2023a. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations, 2023b. Luo, J., Xu, C., Geng, X., Feng, G., Fang, K., Tan, L., Schaal, S., and Levine, S. Multi-stage cable routing through hierarchical imitation learning. arXiv preprint arXiv:2307.08927, 2023. Luo, J., Xu, C., Liu, F., Tan, L., Lin, Z., Wu, J., Abbeel, P., and Levine, S. Fmb: functional manipulation benchmark for generalizable robotic learning. arXiv preprint arXiv:2401.08553, 2024. Lynch, C., Wahid, A., Tompson, J., Ding, T., Betker, J., Baruch, R., Armstrong, T., and Florence, P. Interactive language: Talking to robots in real time. IEEE Robotics and Automation Letters, 2023. Ma, Y. J., Sodhani, S., Jayaraman, D., Bastani, O., Kumar, V., and Zhang, A. Vip: Towards universal visual reward and representation via value-implicit pre-training, 2023. URL https://arxiv.org/abs/2210. 00030. Majumdar, A., Yadav, K., Arnaud, S., Ma, Y. J., Chen, C., Silwal, S., Jain, A., Berges, V.-P., Abbeel, P., Malik, J., Batra, D., Lin, Y., Maksymets, O., Rajeswaran, A., and Meier, F. Where are we in the search for an artificial visual cortex for embodied intelligence? 2023. Mandlekar, A., Zhu, Y., Garg, A., Booher, J., Spero, M., Tung, A., Gao, J., Emmons, J., Gupta, A., Orbay, E., et al. Roboturk: crowdsourcing platform for robotic skill learning through imitation. In Conference on Robot Learning, pp. 879893. PMLR, 2018. Mees, O., Borja-Diaz, J., and Burgard, W. Grounding language with visual affordances over unstructured data. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), London, UK, 2023. Mendonca, R., Bahl, S., and Pathak, D. Structured world models from human videos. CoRL, 2023. Nair, S., Rajeswaran, A., Kumar, V., Finn, C., and Gupta, A. R3m: universal visual representation for robot manipulation, 2022. URL https://arxiv.org/abs/2203.12601. Nasiriany, S., Gao, T., Mandlekar, A., and Zhu, Y. Learning and retrieval from prior data for skill-based imitation learning. In Conference on Robot Learning (CoRL), 2022. Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Howes, R., Huang, P.-Y., Xu, H., Sharma, V., Li, S.-W., Galuba, W., Rabbat, M., Assran, M., Ballas, N., Synnaeve, G., Misra, I., Jegou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. Dinov2: Learning robust visual features without supervision, 2023. Pramanick, S., Song, Y., Nag, S., Lin, K. Q., Shah, H., Shou, M. Z., Chellappa, R., and Zhang, P. Egovlpv2: Egocentric video-language pre-training with fusion in the backbone. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 52855297, 2023. Quere, G., Hagengruber, A., Iskandar, M., Bustamante, S., Leidner, D., Stulp, F., and Vogel, J. Shared Control Templates for Assistive Robotics. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 7, Paris, France, 2020. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. Radosavovic, I., Xiao, T., James, S., Abbeel, P., Malik, J., and Darrell, T. Real-world robot learning with masked visual pre-training. CoRL, 2022. 14 IGOR: Image-GOal Representations Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., Eccles, T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell, R., Vinyals, O., Bordbar, M., and de Freitas, N. generalist agent, 2022. URL https: //arxiv.org/abs/2205.06175. Rosete-Beas, E., Mees, O., Kalweit, G., Boedecker, J., and Burgard, W. Latent plans for task agnostic offline reinforcement learning. In Proceedings of the 6th Conference on Robot Learning (CoRL), 2022. Saxena, S., Sharma, M., and Kroemer, O. Multi-resolution sensing for real-time control with visionlanguage models. In 7th Annual Conference on Robot Learning, 2023. URL https://openreview.net/ forum?id=WuBv9-IGDUA. Schmidt, D. and Jiang, M. Learning to act without actions. arXiv preprint arXiv:2312.10812, 2023. Shafiullah, N. M. M., Rai, A., Etukuru, H., Liu, Y., Misra, I., Chintala, S., and Pinto, L. On bringing robots home, 2023. Shah, R., Martın-Martın, R., and Zhu, Y. MUTEX: Learning unified policies from multimodal task specifications. In 7th Annual Conference on Robot Learning, 2023. URL https://openreview.net/ forum?id=PwqiqaaEzJ. Team, O. M., Ghosh, D., Walke, H., Pertsch, K., Black, K., Mees, O., Dasari, S., Hejna, J., Kreiman, T., Xu, C., et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. Walke, H., Black, K., Lee, A., Kim, M. J., Du, M., Zheng, C., Zhao, T., Hansen-Estruch, P., Vuong, Q., He, A., Myers, V., Fang, K., Finn, C., and Levine, S. Bridgedata v2: dataset for robot learning at scale, 2023. Wang, X., Zhu, Z., Huang, G., Wang, B., Chen, X., and Lu, J. Worlddreamer: Towards general world models for video generation via predicting masked tokens, 2024. URL https://arxiv.org/abs/2401. 09985. Wu, J., Yin, S., Feng, N., He, X., Li, D., Hao, J., and Long, M. ivideogpt: Interactive videogpts are scalable world models. arXiv preprint arXiv:2405.15223, 2024. Xiang, J., Liu, G., Gu, Y., Gao, Q., Ning, Y., Zha, Y., Feng, Z., Tao, T., Hao, S., Shi, Y., Liu, Z., Xing, E. P., and Hu, Z. Pandora: Towards general world model with natural language actions and video states, 2024. URL https://arxiv.org/abs/2406.09455. Xiao, T., Radosavovic, I., Darrell, T., and Malik, J. Masked visual pre-training for motor control. arXiv:2203.06173, 2022. Xu, M., Dai, W., Liu, C., Gao, X., Lin, W., Qi, G.-J., and Xiong, H. Spatial-temporal transformer networks for traffic flow forecasting, 2021. URL https://arxiv.org/abs/2001.02908. Yan, G., Wu, K., and Wang, X. ucsd kitchens Dataset. August 2023. Yang, M., Du, Y., Ghasemipour, K., Tompson, J., Kaelbling, L., Schuurmans, D., and Abbeel, P. Learning interactive real-world simulators, 2024a. URL https://arxiv.org/abs/2310.06114. Yang, S., Walker, J., Parker-Holder, J., Du, Y., Bruce, J., Barreto, A., Abbeel, P., and Schuurmans, D. Video as the new language for real-world decision making, 2024b. Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., and You, Y. Open-sora: Democratizing efficient video production for all, March 2024. URL https://github.com/hpcaitech/Open-Sora. Zhou, G., Dean, V., Srirama, M. K., Rajeswaran, A., Pari, J., Hatch, K., Jain, A., Yu, T., Abbeel, P., Pinto, L., Finn, C., and Gupta, A. Train offline, test online: real robot learning benchmark, 2023. Zhu, X., Tian, R., Xu, C., Ding, M., Zhan, W., and Tomizuka, M. Fanuc manipulation: dataset for learning-based manipulation with fanuc mate 200id robot. 2023a. Zhu, Y., Stone, P., and Zhu, Y. Bottom-up skill discovery from unsegmented demonstrations for longhorizon robot manipulation. IEEE Robotics and Automation Letters, 7(2):41264133, 2022. Zhu, Y., Joshi, A., Stone, P., and Zhu, Y. Viola: Imitation learning for vision-based manipulation with object proposal priors, 2023b. 15 IGOR: Image-GOal Representations"
        },
        {
            "title": "A Dataset",
            "content": "We present the datasets used for pre-training in Table 1. In total, these datasets comprise approximately 0.8 million robot trajectories and 2.0 million filtered human activity video clips. The robot data ratios are from (Team et al., 2024). Robot Dataset Mix Ratio (%) Kuka (Kalashnikov et al., 2018) Bridge (Walke et al., 2023; Ebert et al., 2021) Taco Play (Rosete-Beas et al., 2022; Mees et al., 2023) Jaco Play (Dass et al., 2023) Berkeley Cable Routing (Luo et al., 2023) Roboturk (Mandlekar et al., 2018) Viola (Zhu et al., 2023b) Berkely Autolab UR5 (Chen et al.) Toto (Zhou et al., 2023) Language Table (Lynch et al., 2023) Stanford Hydra Dataset (Belkhale et al., 2023) Austin Buds Dataset (Zhu et al., 2022) NYU Franka Play Dataset (Cui et al., 2022) Furniture Bench Dataset (Heo et al., 2023) UCSD Kitchen Dataset (Yan et al., 2023) Austin Sailor Dataset (Nasiriany et al., 2022) Austin Sirius Dataset (Liu et al., 2023a) DLR EDAN Shared Control (Quere et al., 2020) IAMLab CMU Pickup Insert (Saxena et al., 2023) UTAustin Mutex (Shah et al., 2023) Berkeley Fanuc Manipulation (Zhu et al., 2023a) CMU Stretch (Mendonca et al., 2023) BC-Z (Jang et al., 2022) FMB Dataset (Luo et al., 2024) DobbE (Shafiullah et al., 2023) DROID (Khazatsky et al., 2024) Ego4D (Grauman et al., 2022) Something-Something V2 (Goyal et al., 2017) EPIC-KITCHENS (Damen et al., 2020) EGTEA Gaze+ (Li et al., 2018) 7.72 8.08 1.82 0.24 0.12 1.40 0.55 0.73 1.21 2.67 2.67 0.12 0.49 1.46 0.06 1.34 1.03 0.06 0.55 1.34 0.43 0.12 4.56 4.31 0.85 6.07 32.1 9.5 8.0 0.4 Table 1: Dataset, mixture weights, and number of training examples after filtering in the pre-training stage in IGOR. Data Filtering We observed that video quality significantly affects the action model, particularly for human activities video. Excessive shakiness in videos can introduce visual changes between consecutive frames that are unrelated to the agents actions. We calculate the camera motion over the videos, and filter approximately 40% percent of open-world video data. For the remaining data, we further stabilize the videos. Although we retain only 60% percent of open-world video data, we find that the action model improves dramatically. Frame Interval noticeable amount of visual changes is crucial for our latent action model. If we select two frames that are too close in time, the agent may barely move, resulting in visual changes that are not significant enough for inferring meaningful actions. Conversely, if the frames are too far apart, the changes might be too large to model accurately. To address this issue, we tune the sampling interval. For robot data, we choose frames that are three intervals apart, using st and st+3 as the image-goal pair. For real world videos, we control the sampling. For real world data, we control the sample interval to be within [0.1s, 0.5s]. For the action and policy model, the context frames follow the same interval, ensuring that each pair of consecutive frames maintains this consistent spacing. 16 IGOR: Image-GOal Representations"
        },
        {
            "title": "B Training Details",
            "content": "B.1 Latent Action Model Training The latent action model uses an ST-transformer equipped with frozen DINO-v2 pretrained ViT image encoder. The latent action model uses patch size of 14, and codebook with = 4 tokens and size = 32, each with an embedding size of = 128. We train the latent action model with batch size = 512, training iterations of 140K steps, and learning rate of 1.5e 4 with Adam optimizer. B.2 Foundation World Model Training We start on the top of the OpenSora (Zheng et al., 2024) model with newly initialized projection layers. The foundation world model with batch size = 12, training iterations of 48K, and learning rate of 1e 4 with Adam optimizer. B.3 Foundation Policy Model and Low-Level Policy Model The latent action model uses an ST-transformer equipped with frozen DINO-v2 pretrained ViT image encoder, following the latent action models image encoder. The foundation policy model consists of 12 layers of spatial and temporal attentions, each with 12 attention heads and hidden dimension as 768 and patch size of 14. We use frozen CLIP features for text instructions. We pretrain the foundation policy model with batch size = 128, training iterations of 124K, and learning rate of 1e 4 with Adam optimizer. For the low-level policy model, we add extra layers on top of the foundation policy model. We use sub-task length of τ = 3 for finetuning the low-level policy model on RT-1 dataset. We finetune the low-level policy model with batch size = 128, training iterations of 32K, and learning rate of 1e 4 with Adam optimizer."
        },
        {
            "title": "C Additional Ablation Results",
            "content": "C.1 Dataset ablation for Latent Action Model We compare two different settings for the pre-training dataset: only use the robotic dataset (robot data), and use both robotic and human activity dataset (mixed data). We evaluate the validation loss on the latent action model on RT-1 dataset, which is held out from the pretraining dataset and serves for OOD evaluation. Validation loss of the latent action model assesses the extent to which the IDM and FDM can jointly generate latent actions and recover goal states from these latent actions conditioned on states on the unseen dataset. The results are shown in Table 2. We find that the OOD validation loss is greatly reduced by adding human activity dataset. This may be due to the diversity of human videos, which comprise real daily life environments with lots of diverse backgrounds and objects. These results demonstrate that it is promising to leverage human data for improving robot tasks under the IGOR framework. Validation Loss Robot Data Mixed Data 0.145 0. Table 2: Validation loss on held-out dataset (RT-1) with different training data."
        },
        {
            "title": "D Additional Highlight of Contributions",
            "content": "We would like to especially acknowledge Pushi Zhangs contributions, including his involvement from the very beginning in shaping the initial ideas for the IGOR project, his valuable support in experiment analysis and debugging, his dedicated efforts during the intense final stages of paper writing and refinement, and his crucial work in building the project webpage. We also would like to thank Kaixin Wangs valuable suggestions on paper title and great effort for preparing the well-designed paper template."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "Tsinghua University"
    ]
}