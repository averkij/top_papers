{
    "paper_title": "SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training",
    "authors": [
        "Dongting Hu",
        "Jierun Chen",
        "Xijie Huang",
        "Huseyin Coskun",
        "Arpit Sahni",
        "Aarush Gupta",
        "Anujraaj Goyal",
        "Dishani Lahiri",
        "Rajesh Singh",
        "Yerlan Idelbayev",
        "Junli Cao",
        "Yanyu Li",
        "Kwang-Ting Cheng",
        "S. -H. Gary Chan",
        "Mingming Gong",
        "Sergey Tulyakov",
        "Anil Kag",
        "Yanwu Xu",
        "Jian Ren"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing text-to-image (T2I) diffusion models face several limitations, including large model sizes, slow runtime, and low-quality generation on mobile devices. This paper aims to address all of these challenges by developing an extremely small and fast T2I model that generates high-resolution and high-quality images on mobile platforms. We propose several techniques to achieve this goal. First, we systematically examine the design choices of the network architecture to reduce model parameters and latency, while ensuring high-quality generation. Second, to further improve generation quality, we employ cross-architecture knowledge distillation from a much larger model, using a multi-level approach to guide the training of our model from scratch. Third, we enable a few-step generation by integrating adversarial guidance with knowledge distillation. For the first time, our model SnapGen, demonstrates the generation of 1024x1024 px images on a mobile device around 1.4 seconds. On ImageNet-1K, our model, with only 372M parameters, achieves an FID of 2.06 for 256x256 px generation. On T2I benchmarks (i.e., GenEval and DPG-Bench), our model with merely 379M parameters, surpasses large-scale models with billions of parameters at a significantly smaller size (e.g., 7x smaller than SDXL, 14x smaller than IF-XL)."
        },
        {
            "title": "Start",
            "content": "SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training Dongting Hu1,2,* Jierun Chen1,3,* Xijie Huang1,3,* Huseyin Coskun1 Arpit Sahni1 Aarush Gupta1 Anujraaj Goyal1 Dishani Lahiri1 Rajesh Singh1 Yerlan Idelbayev1 Junli Cao1 Yanyu Li1 Kwang-Ting Cheng3 S.-H. Gary Chan3 Mingming Gong2, 4 Sergey Tulyakov1 Anil Kag1, Yanwu Xu1, Jian Ren1, 1 Snap Inc. 2 The University of Melbourne 3 HKUST 4 MBZUAI * Equal contribution Equal advising Project Page: https://snap-research.github.io/snapgen 4 2 0 2 2 1 ] . [ 1 9 1 6 9 0 . 2 1 4 2 : r Figure 1. Comparison of various text-to-image models in terms of model size, mobile device compatibility, and visual output quality. Our model, with only 379M parameters, demonstrates competitive visual quality while being mobile-compatible. Input text prompts are shown above each image grid; all images are generated at 10242 resolutionzoom in for details. More examples are shown in webpage."
        },
        {
            "title": "Abstract",
            "content": "Existing text-to-image (T2I) diffusion models face several limitations, including large model sizes, slow runtime, and low-quality generation on mobile devices. This paper aims to address all of these challenges by developing an extremely small and fast T2I model that generates highresolution and high-quality images on mobile platforms. We propose several techniques to achieve this goal. First, we systematically examine the design choices of the network architecture to reduce model parameters and latency, while ensuring high-quality generation. Second, to further improve generation quality, we employ cross-architecture knowledge distillation from much larger model, using multi-level approach to guide the training of our model from scratch. Third, we enable few-step generation by integrating adversarial guidance with knowledge distillation. For the first time, our model SnapGen, demonstrates the generation of 10242 px images on mobile device around 1.4 seconds. On ImageNet-1K, our model, with only 372M parameters, achieves an FID of 2.06 for 2562 px generation. On T2I benchmarks (i.e., GenEval and DPG-Bench), our model with merely 379M parameters, surpasses large-scale models with billions of parameters at significantly smaller size (e.g., 7 smaller than SDXL, 14 smaller than IF-XL). 1. Introduction Large-scale text-to-image (T2I) diffusion models [13, 14, 16, 54, 56, 6062] have achieved remarkable success in content generation, powering numerous applications like image editing [51, 66, 74, 87] and video creation [53, 57, 80]. However, T2I models often come with substantial model sizes and slow runtime, and deploying them on the cloud raises concerns related to data security and high costs [67]. To address these challenges, there is huge growing interest in developing smaller and faster T2I models through techniques like model compression (e.g., pruning and quantization) [43, 69, 88], step reduction by distillation [79, 82], and efficient attention mechanisms that mitigate the quadratic complexity [49, 76]. Nevertheless, current works still encounter limitations, e.g., low-resolution generation on mobile devices, that constrain their broader application. importantly, critical question remains unexplored: how can we train T2I model from scratch to generate high-quality, high-resolution images on mobile? Such model would offer substantial advantages in speed, compactness, cost-effectiveness, and secure deployment. To build this model, we introduce several innovations: Efficient Network Architectures: We conduct an indepth examination of network architectures, including the denoising UNet and Autoencoder (AE), to obtain optimal"
        },
        {
            "title": "Most",
            "content": "trade-off between resource usage and performance. Unlike prior works that optimize and compress pre-trained diffusion models [10, 35, 85], we directly focus on macroand micro-level design choices to achieve novel architecture that greatly reduces model size and computational complexity, while preserving high-quality generation. Improved Training Techniques: We introduce several improvements to train compact T2I model from scratch. We utilize flow matching [47, 50] as objective, aligning with larger models like SD3 [19] and SD3.5 [3]. This design enables effective knowledge and step distillation, transferring rich representations from large-scale diffusion models to our much smaller one. Further, we propose multi-level knowledge distillation with timestepaware scaling that combines multiple training objectives. Instead of weighting objectives through linear combination as in prior works [35, 49], we consider target prediction difficulty (i.e., student-teacher difference) across various timesteps in flow matching. Advanced Step Distillation: We perform step distillation on our model by combining the adversarial training along with the knowledge distillation using few-step teacher model (i.e., SD3.5-Large-Turbo [5]), enabling ultra-fast high-quality generation with only 4 or 8 steps. We demonstrate the superior advantages of our approach and model through extensive experiments: On ImageNet-1K [17] class-conditional image generation task, our model achieves the FID comparable to existing works with significantly reduced model size and computation, i.e., half the model size and one-third of the compute resources compared with SiT-XL[52], as in Tab. 1. For large-scale T2I generation, our UNet model, with only 379M parameters, demonstrates superior generation quality compared to billion-parameter models [45, 56, 89], e.g., improved metrics on benchmark datasets (Tab. 3) and human evaluation  (Fig. 8)  . Our compressed decoder, trained from scratch, has competitive reconstruction quality compared to commonly used models [19, 56], with more than 36 smaller size, enabling the mobile deployment. Notably, for the first time, we show T2I model achieving high-resolution generation (e.g., 10242 px) on mobile devices (e.g., iPhone 16 Pro-Max) around 1.4 seconds. 2. Related Work High-Resolution Text-to-Image Models have emerged with advanced architectures and multi-stage approaches designed to enhance visual fidelity and user customization. SDXL [56] is pioneering work in this field, employing refined cascading approach with UNet backbone to generate high-detailed images, resulting in photorealistic outputs that maintain sharpness and clarity. The following studies explore different techniques like more advanced text encoders, 2 better image refinement, or improved dataset preparation, to obtain better text-image alignment or higher-quality generation [6, 7, 16, 21, 32, 3941, 44, 48, 51, 71]. However, most of these models contain billions of parameters, making them extremely slow, and not being able to run on resourcecontained hardware like mobile devices. In this work, we aim to build small and fast model that can perform highresolution generation even on mobile platforms. Efficient Diffusion Models address the challenges of bulky model size and long runtime. There have been efforts exploring the architecture optimization to remove redundancy from large models, demonstrating the on-device generation within seconds [11, 43, 69, 88]. However, these models are constrained to low-resolution output, i.e., 5122 px. To enable efficient high-resolution generation, SANA [76] and LinFusion [49] incorporate linear attention [9, 15, 34] to achieve 1K generation on laptop GPUs. In contrast, we target broader range of platforms, supporting high-resolution generation (e.g., 1K) directly on mobile devices. Knowledge Distillation in Diffusion Models. In the context of diffusion models, previous works focus on distilling large, high-capacity teacher models into more compact, efficient student models within homogeneous architecture [35, 49]. They reduce the complexity of the model by removing certain components like attention [72] or residual blocks [24], while maintaining the architectural structure. However, our approach diverges from this trend by utilizing heterogeneous architecture for more aggressively efficient yet challenging distillation. Adversarial Step Distillation uses the techniques from adversarial training [23] to reduce the number of diffusion steps, while maintaining high image quality [63, 64]. For example, UFOGen [79] employs diffusion-GAN formulation [73, 75, 78] to significantly reduce inference time while maintaining competitive performance. DMD2 [81] builds on prior distillation methods by using distribution matching with adversarial loss. Different from exiting works, we conduct step-distillation on very compact model and train the model along with the knowledge distillation. 3. Method In this section, we present how to craft and train highly efficient T2I model for high-resolution generation. Specifically, starting from the architecture designed in the latent diffusion model [61], we optimize both denoising backbone (Sec. 3.1, Fig. 2, and Fig. 3) and autoencoder (Sec. 3.2 and Fig. 4) to make them compact and fast, even on mobile devices. We then propose the improved training recipe and knowledge distillation (Sec. 3.3 and Fig. 5), empowering high-performance T2I model. Lastly, we introduce our step distillation to significantly reduce the number of denoising steps for faster T2I model (Sec. 3.4 and Fig. 7). 3.1. Efficient UNet Architecture Here we describe the design choices for the denoising UNet. Baseline Architecture. We choose UNet from SDXL [56] as the baseline (Fig. 2(a)) for our backbone since it has superior efficiency and faster convergence [42] than pure transformer-based models [13, 14]. We adjust the UNet into thinner and shorter model (i.e., reducing the number of transformer blocks from [0, 2, 10] in three stages to [0, 2, 4] and their channel dimensions from [320, 640, 1280] to [256, 512, 896]), and iterate design choices on top of it. Evaluation Metrics. We train models on ImageNet1K [17] under class-conditional generation for 120 epochs, unless specified otherwise, and report the FID score [26] for 2562 px generation. Similarly to existing work [32], we inject the class conditions through text template photo of <class name>. We then encode it with light text encoder to align the pipeline for the T2I generation. We also calculate the number of parameters for different models, floating point operations (FLOPs) (measured on latent size of 128 128, equivalent to 1024 1024 image after decoding), and the runtime on mobile device (tested on iPhone 15 Pro). Detailed training and evaluation settings can be found in Supplemental Materials. In the following, we introduce the key architectural changes that improve the model. Remove Self-Attention from High-Resolution Stages. Self-attention (SA) layer is restricted by its quadratic computational complexity, incurring heavy computational cost and high memory consumption for high-resolution input. As such, we keep the SA layer only in the lowest-resolution stage while removing it from other higher-resolution stages, i.e., Fig. 2 (b). This leads to 17% fewer FLOPs and 24% latency reduction, as shown in Fig. 3. Interestingly, we even observe performance improvement, i.e., FID decreased to 3.12 from 3.76. We hypothesize that models with SA in high-resolution stages converge more slowly. Replace Conv with Expanded Separable Conv. Regular convolution (Conv) is redundant in both parameters and computation. To address this, we replace all Conv layers with the separable convolution (SepConv) [30], composed of depthwise convolution (DW) followed by pointwise convolution (PW), as shown in Fig. 2 (c). This replacement reduces parameters by 24% and latency by 62%, but also leads to performance drop (FID increases from 3.12 to 3.38). To address the issue, we expand the intermediate channels. Specifically, the number of channels after the first PW layer is increased with an expansion ratio, and reduced back to the original number after the second PW layer. The expansion ratio is set to 2 to balance the trade-off between performance, latency, and model parameters. Such design aligns our residual block with the recently proposed Universal Inverted Bottleneck (UIB) block [58]. As result, our model achieves 15% fewer parameters, 27% less computation, and 2.4 speedup, while obtaining lower FID. 3 Figure 2. Efficient UNet. Starting from thinner and shorter version of the UNet from SDXL (as in (a)), we explore series of architectural changes, i.e., (b)(f), to develop smaller and faster model while retaining high-quality generation performance, as evaluated in Fig. 3. Figure 3. Comparisons of Performance and Efficiency for various Design Choices of Efficient UNet. The generation quality is evaluated using FID calculated on ImageNet-1K for 2562 px generation. The efficiency metrics include model parameters, latency, and FLOPs. FLOPs and latency (on iPhone 15 Pro) are measured with 128 128 latent, equivalent to 1024 1024 decoded image, for one forward pass. We show the architecture enhancements that improve any of the metrics without hurting others. Trim FFN Layers. For the layers in the feed-forward network (FFN), the hidden channel expansion ratio is set to 4 by default and further doubled by using the gated unit. This substantially inflates the model parameters, computation, and memory usage. Following MobileDiffusion [88], we examine the efficacy of simply reducing the expansion ratio, as shown in Fig. 2 (d). We show that reducing the expansion ratio to 3 preserves comparable FID performance while reducing both parameters and FLOPs by 12%. Replace MHSA with MQA. Multi-Head Self-Attention (MHSA) requires multiple sets of keys and values for each attention head. In contrast, Multi-Query Attention (MQA) [65] is more efficient by sharing single set of keys and values across all heads. Replacing MHSA with MQA reduces parameters by 16% and latency by 9%, with minimal impact on performance. Interestingly, the 9% saving in latency exceeds the 6% decrease in FLOPs, as the reduced memory access enables higher computational intensity. Inject Conditions to the First Stage. Cross-attention (CA) blends the conditional information (e.g., textural description) along with the spatial features to generate images that align with the condition. However, the UNet of SDXL only applies CA in transformer blocks starting from the second stage, resulting in the missing conditional guidance for the first stage. In response, we propose to introduce the conditional embeddings from the very first stage, as in Fig. 2(e). Specifically, we replace the residual blocks with transformer blocks that include CA and FFN while without SA layers. This adjustment makes the model smaller, faster, and more efficient while improving FID. 4 Employ QK RMSNorm and RoPE Positional Embeddings. We extend two advanced techniques developed originally for language models, Query-Key (QK) Normalization [25] with RMSNorm [84] and Rotary Position Embedding (RoPE) [68], to enhance the model (Fig. 2 (f)). RMSNorm, applied after the Query-Key projection in the attention mechanism, reduces the risk of softmax saturation without sacrificing model expressiveness while stabilizing training for faster convergence. In addition, we adapt RoPE from one dimension to two dimensions for better supporting higher resolution since it significantly mitigates artifacts like repeated objects. Together, RMSNorm and RoPE introduce negligible computational and parameter overhead, while offering measurable gains in FID performance. Discussion. The above optimization results in an efficient and powerful diffusion backbone capable of generating high-resolution images on mobile devices. Before proceeding with large-scale T2I training, we compare the capacity of our model against existing works on ImageNet1K. We train the model for 1, 000 epochs by following the setting from prior work [61]. We evaluate the model using varied CFG [27, 37] across different inference timesteps. As shown in Tab. 1, our efficient UNet achieves comparable FID to SiT-XL [52], while being almost 45% smaller. Table 1. Class-conditional image generation on ImageNet 256 256 with CFG. FLOPs are calculated for one forward pass. Model Param (M) FLOPs (G) FID LDM-4 [61] UViT-L [8] UViT-H [8] DiT-XL [55] SiT-XL [52] Ours 400 287 501 675 675 372 104 77 133 119 119 3.60 3.40 2.29 2.27 2.06 2.06 3.2. Tiny and Fast Decoder Besides the denoising model, the decoder also takes significant ratio of total runtime, especially for on-device deployment [43, 88]. Here we introduce new architecture of decoder  (Fig. 4)  for efficient high-resolution generation. Baseline Decoder. We use the autoencoder (AE) from SD3 [19] as our baseline model (i.e., the same encoder from SD3 AE), due to its superior reconstruction quality. The AE maps an image RHW 3 into lower-dimensional latent c (f, as 8, 16 in SD3). The encoded latent is then decoded back to an image through decoder. For high-resolution generation, we observe that the decoder in SD3 is very slow on mobile devices. Specifically, it encounters out-of-memory (OOM) errors when generating 10242 px image on both the ANE processor of the iPhone 15 Pro and the mobile GPU (Tab. 2). To overcome the latency issue, we propose much smaller and faster decoder. Efficient Decoder. We conduct series of experiments to decide the efficient decoder with the following key changes Figure 4. Comparisons of Decoder Architecture between (a) SDXL/SD3 decoder and (b) our tiny decoder. Table 2. Performance Comparison of Decoder. PSNR is calculated on COCO 2017 validation set [46]. FLOPs and latency (on iPhone 15 Pro) are measured for decoding 128 128 latent into 1024 1024 image. The decoder from SDXL and SD3 fail to run on the neural engine of mobile, resulting in huge runtime. Decoder Ch PSNR Param (M) FLOPs (G) Latency (ms) on ANE Latency (ms) on GPU SDXL [56] SD3 [19] Ours 4 16 16 24.89 27.92 27.85 49.49 49.55 1.38 4970 4970 OOM OOM 174 9469 OOM - compared with the baseline architecture: 1. We remove attention layers to greatly reduce peak memory without noticeable impact on decoding quality. 2. We keep minimal amount of GroupNorm (GN) to find trade-off between latency and performance (i.e., mitigating the color shifting). 3. We make the decoder thinner (i.e., fewer channels or narrower width) and replace Conv with SepConvs. 4. We use fewer residual blocks in high-resolution stages. 5. We remove the Conv shortcut in residual blocks and use the upsampling layer for channel transition. Training of the Decoder. We train our decoder with the mean squared error (MSE) loss, lpips loss [86], adversarial loss [23], and discard the KL term [36] as the encoder is fixed. The decoder is trained on 2562 image patches with batch size of 256 and for 1M iterations. As in Tab. 2, our tiny decoder achieves competitive PSNR score for reconstruction, while being 35.9 smaller and 54.4 faster for high-resolution generation on mobile devices compared to conventional ones (e.g., the decoder from SDXL and SD3). Discussion of Total On-Device Latency. We finally measure T2I model latency for 10242 px generation on iPhone 16 Pro-Max. The decoder takes 119ms, and the per-step latency for the UNet is 274ms. This results in 1.2 2.3s runtime for 4 to 8 step generation. Note that text encoder runtime is negligible compared to other components [43]. 5 Figure 5. Overview of Multi-level Knowledge Distillation, where we perform output distillation and feature distillation. Figure 6. Mean loss magnitude for task loss Ltask and output distillation loss Lkd. Figure 7. Overview of Adversarial Step Distillation. Output distillation and distributionmatching distillation are performed. 3.3. Training Recipe and Multi-Level Distillation To improve the generation quality of our efficient diffusion model, we propose series of training techniques. Flow-based Training and Inference. Rectified Flows (RFs) [47, 50] define the forward process as straight paths connecting the data distribution to standard normal distribution, i.e., xt = (1 σt)x0 + σtϵ, (1) where x0 is the clean (latent) image, is the timestep, σt is timestep-dependent factor, and ϵ is random noise sampled from (0, I). The denoising UNet is formulated to predict velocity field with the objective as Ltask = EϵN (0,I),t (cid:104) (ϵ x0) vθ(xt, t)2 2 (cid:105) , (2) where vθ(xt, t) is the predicted velocity from UNet parameterized by θ. To further enhance training stability, we apply logit-normal sampling [19] for the timestep during training, which assigns more samples to the intermediate steps. In the inference stage, we use the Flow-Euler sampler [20], which predicts the next sample based on the velocity, i.e., xt1 = xt + (σt1 σt) vθ(xt, t). (3) To achieve lower signal-to-noise ratio on high-resolution (i.e., 10242 px) images, we apply timestep shift similar to SD3 [19] to adjust the scheduling factor σt during both training and inference. Multi-Level Knowledge Distillation. To improve the generation quality for compact models, one common practice for previous works is applying knowledge distillation to mimic the prediction of scaled-up teacher models [35]. Benefiting from the aligned flow matching objective and (AE) latent space, powerful SD3.5-Large model [4] can be used as the teacher for output distillation. However, we still face challenges due to 1) the heterogeneous architecture between the U-Net and DiT, 2) the scale difference between the distillation loss and task loss, and 3) varying prediction difficulty across different timesteps. To tackle these, we propose novel multi-level distillation loss and apply timestep-aware scaling to stabilize and accelerate the convergence of distillation. The overview of our knowledgedistillation scheme is shown in Fig. 5 and detailed techniques are elaborated as follows. Aside from the task loss defined in Eq. 2, the major objective for knowledge distillation is to supervise our model θ directly with the output of teacher model θT , which can be indicated as (cid:104) Lkd = vθT (xt, t) vθ(xt, t)2 2 (cid:105) . (4) Given the capacity gap between the teacher and our model, applying output-level supervision alone leads to instability and slow convergence. Therefore, we further incorporate cross-architecture feature-level distillation loss as Lfeatkd = (cid:104) (cid:88) (lT ,l) lT θT (xt, t) ψ(f θ(xt, t))2 2 (cid:105) , (5) where lT () and l() indicate the feature output from θT the lT -th layer and l-th layer in teacher model and student model, respectively. Different from previous work [35, 49], we consider cross-architecture distillation from DiT to UNet. Since the richest information in transformers sits around the last layer, we set the distillation target to this layer in both models, and use lightweight trainable projector ψ() with only two Conv layers to map the student feature to match the dimension of teacher feature. The proposed feature-level distillation loss provides additional supervision to the student model, leading to faster alignment to the generation quality of the teacher model. Timestep-Aware Scaling. Weighting multiple objectives has been major challenge in knowledge distillation, especially in diffusion models. The overall training objectives from previous works [35, 49, 69] are simple linear combination of multiple loss term, i.e., = Ltask + λ1Lkd + λ2Lfeatkd, (6) where the weighting coefficient λ1 and λ2 are empirically set to constant. However, this baseline setting fails to consider the prediction difficulty in various time steps. We investigate the distribution of empirical risk magnitude of 6 Ltask and Lkd across different timestep during model training. Fig. 6 illustrates that, in intermediate steps, prediction difficulty are lower compared to closer to 0 or 1. Building on this important observation, we propose timestep-aware scaling of the objective to close the gap in loss magnitude across different values of and to account for prediction difficulties at each timestep, as follows: S(Ltask, Lkd) = Et (cid:104) λ(t)Lt task +(cid:0)1λ(t)(cid:1) Lt Lt task kd (cid:105) Lt kd , (7) where λ(t) is the normalized standard (location 0, scale 1) logit-norm density function and indicates the magIn S, we first ensure the same scale between nitude. task loss and distillation loss across different t, then apply more teacher supervision where prediction difficulty is higher (i.e., closer to 0 or 1), and more real data supervision where prediction difficulty is lower (i.e., intermediate timesteps). The proposed scheme considers the variation of timestep and helps accelerate the distillation training. The final multi-level distillation objective LMD is defined as LMD = S(Ltask, Lkd) + S(Ltask, Lfeatkd). (8) 3.4. Step Distillation We take one step further to enhance the sampling efficiency of our model following distribution-matching-based step distillation scheme. Following Latent Adversarial Diffusion Distillation (LADD) [63], we use diffusion-GAN hybrid structure to distill our model into fewer steps with the optimization objective as (cid:104) [log(DθT (xt1, t))] max Gθ min DθT + [log(1 DθT (x (cid:105) t1, t))] S(Ltask, Lkd) (9) , 0), where where DθT is the discriminator model partially initialized with pretrained fewer-step teacher model θT (SD3.5-LargeTurbo [5]). The large-scale teacher model are only used as the feature extractor and are frozen during distillation. We only train few linear layers in the discriminator after feature extraction. We sample xt1 q(xt1x0) and t1 q(xt1x 0 is the prediction of our denoising generator1 Gθ(xt, t) as our student model, and q(x) is the forward process of diffusion model defined in Eq. 1. The objective consists of an adversarial loss to match noisy samples at time step 1 and the output-level distillation loss S(Ltask, Lkd) after applying timestep-aware scaling. The proposed step distillation, visualized in Fig. 7, can be interpreted as training diffusion model with adversarial refinement and knowledge distillation, where teacher guidance serves as an additional inductive bias. This advanced step distillation empowers our compact model for high-quality generation, with only few denoising steps. 1For simplicity, we use the x0 prediction in our derivation, and prediction of rectified flow-based training would not break the formulation. 7 4. Experiments Model Details. Our T2I pipeline consists of the efficient UNet (Sec. 3.1) and the efficient encoder-decoder model (Sec. 3.2). To obtain text embeddings from the input prompt, we leverage multiple text encoders, namely lightweight CLIP-L [59], CLIP-G [59], and the large Gemma2-2b language model [70]. We follow SD3 [19] strategy to combine these three text-encoders into unified rich textual embedding. To enable classifier-free guidance [28], we employ these embeddings with an individual drop-out probability such that we can use an arbitrary subset of the encoders during inference. This allows us to deploy one or more encoders based on the resource constraints. Training Recipe. Similar as prior work [32], we use multi-stage strategy to train our UNet model from scratch. First, we pre-train the model using the ImageNet-1K [17] at 256 resolution as described in Sec. 3.1. Second, we fine-tune this model in progressive manner from 256 512 1024 resolutions. Third, we employ knowledge distillation with our timestep-aware scaling (Sec. 3.3) to improve the finer details in our models using much larger teacher model (SD3.5-Large [4]) and all three textencoders. Finally, we obtain few-step model through step distillation using SD3.5-Large-Turbo [5] model as the teacher. We optimize the rectified-flow [19] objective using AdamW optimizer [18] to train our UNet backbone. Hyper-parameters. We sample the timesteps in the flowmatching using the logit-normal distribution with (0, 1) as the location and scale parameters. We use time shift of 3 for both training and inference. For unconditional diffusion guidance [28], we set the outputs of each of the three text encoders independently to zero with probability of 46.4%, such that we roughly train an unconditional model in 10% of all steps. See the Supplemental Materials for details. 4.1. Evaluation Quantitative Benchmarks. We use GenEval [22] and DPG-Bench [31] benchmarks to evaluate the text-to-image alignment of our model on short and long prompts, respectively. We report CLIP score on 6K subset of MS-COCO validation data [46]. In addition, to measure the aesthetic quality of our model, we compute the Image Reward [77] score on selected PixArt prompts [13]. Tab. 3 lists our performance alongside existing state-of-the-art T2I baselines. We provide additional details in Supplemental Materials. We highlight the salient observations below: Our 0.38B parameter model achieves even better performance than significantly larger models such as SDXL (2.6B), Playground (2.6B), and IF-XL (5.5B). KD non-trivially improves the prompt following ability of the base model as illustrated by an absolute five-point increase in DPG-Bench and GenEval scores. Figure 8. Human Evaluation. We conduct user study to compare images generated by our model against baselines on three attributes: aesthetic quality, text-image alignment, and realistic generations. Our model surpasses the quality of SDXL and SD3 models, while performing competitively against the teacher SD3.5-Large model. In terms of aesthetic performance, our model has similar Image Reward scores as Playground models [40, 41]. Table 3. Evaluation on Quantitative Benchmarks. We list the scores on GenEval, DPG-Bench, CLIP score on COCO, and Image Reward on aesthetic prompts. We report the parameters for the UNet/DiT backbone in the Param column. Throughput (samples/s) is measured on single 80GB A100 GPU using the largest batch size supported for each model in practical scenario to generate 10242 px images. Here our sampling step is set to be 28. Model Param Throughput GenEval DPG CLIP PixArt-α [13] PixArt-Σ [14] SD-1.5 [1] SD-2.1 [2] Sana [76] LUMINA-Next [89] SDXL [56] Playgroundv2 [40] Playgroundv2.5 [41] IF-XL [16] 0.6B 0.6B 0.9B 0.9B 1.6B 2.0B 2.6B 2.6B 2.6B 5.5B Ours w/o KD SnapGen (ours) 0.38B 0.38B 0.42 0.46 - - 1.00 0.06 0.18 0.18 0.18 0. 1.04 1.04 0.48 0.53 0.43 0.50 0.66 0.46 0.55 0.59 0.56 0.61 0.61 0.66 71.1 80.5 63.2 64.2 84.8 74.6 74.7 74.5 75.5 75.6 76.3 81.1 0.316 0.317 0.287 0.281 0.327 0.309 0.301 0.317 0.319 0. 0.321 0.332 Image Reward 1.15 1.13 0.19 0.29 1.25 0.88 0.99 1.25 1.34 0.65 1.20 1.32 Qualitative Comparison. To visually evaluate the imagetext alignment and aesthetics, we compare the generated images from different T2I models in Fig. 1. We observe that many existing models fail to fully capture the full prompt and miss important elements. Further, human generations often result in smoothened-out faces, leading to the loss of details. In contrast, our model generates much more photorealistic images with better image-text alignment. Human Evaluation. For thorough comparison between baselines, we perform user study with the widely used Parti prompts [83]. We use SDXL, SD3-M, and SD3.5Large models as the baselines and generate images using this prompt set. We ask the users to select images with better attributes between the baselines and our model. These attributes include image-text alignment, aesthetic quality, and realistic images. Fig. 8 shows that our model convincingly outperforms SDXL on all three attributes. Our model beats SD3 on image-text alignment and realistic generations, with tie on aesthetic quality. Compared to the teacher (SD3.5-Large), we lag the teacher bit behind on the image-text alignment, yet our model still has better realistic generations, and yields toss-up on aesthetic quality. With this study, we can conclude our efficient T2I pipeline achieves generation quality that is quite comparable to the SD3.5-Large teacher that has 8.1B parameters. Few-Step Generation. After step-distillation (Sec. 3.4), our model can generate high-quality images within few steps. Fig. 9 compares our models performance before and after step-distillation, with respective GenEval scores. The results demonstrate that our model, after step distillation, achieves comparable performance to the baseline model with 28 steps, even when using only 4 or 8 steps. While the few-step generation shows slight qualitative degradation compared to the 28-step baseline, it still outperforms most existing T2I models with significantly more inference steps, such as SDXL (50 steps) and PixArt-α (100 steps). Figure 9. Performance comparison of few-step generation for our model before (top) and after step distillation (bottom). 5. Conclusion In this work, we propose novel and efficient T2I model for high-resolution generation on mobile phones. We systematically detail the process to obtain tiny 379M parameter UNet architecture along with an efficient latent decoder. We devise novel training method consisting of multi-stage pre-training followed by knowledge distillation from large teacher and adversarial step distillation. With these, we achieve an extremely efficient T2I model that comprehensively outperforms many existing multi-billion parameter models such as SDXL, Lumina-Next, and Playgroundv2. 8 Author Contribution Statement: [3] Stability AI. Stable diffusion 3.5. Dongting Hu designed and implemented the training pipeline, integrating various text encoders and incorporating flow-matching objective to enable knowledge distillation from scalable DiT-based models (SD3.5). He prepared high-quality training data and trained the T2I base model, achieving an initial GenEval score of 0.61. He also built the distillation pipeline and contributed to multi-level knowledge distillation, significantly enhancing the models GenEval score to 0.66 and improving generation quality based on human evaluations. His work on step distillation further enabled efficient high-quality generation, achieving GenEval score of 0.63 with 8-step generation and 0.61 with 4-step generation. Additionally, he managed latent decoder training, ensuring close reconstruction quality to SD3 decoder, and facilitated ondevice deployment. He developed the mobile app using the Core ML Diffusers framework, which achieved 1K resolution image generation on-device in approximately 1.4 seconds, as demonstrated in the demo. Jierun Chen developed the efficient UNet and AE decoder, enabling 1K resolution image generation on mobile devices for the first time. On ImageNet classconditional generation, the UNet achieves an FID score on par with the recent SiT-X model while reducing parameters by 45% and compute resources by 68%. The tiny decoder is 36 smaller and 54 faster than conventional decoders (e.g., those from SDXL and SD3) for high-resolution mobile generation. He also initiated the early T2I diffusion training and contributed to the on-device deployment. Xijie Huang proposed the multi-level knowledge distillation scheme to improve the generation quality of our model, achieving comparable performance to the DiTbased teacher (SD3.5) across various quantitative benchmarks (e.g., boosting GenEval performance from 0.61 to 0.66) and human evaluation. He analyzed scale differences between distillation and task losses across timesteps, introducing timestep-aware scaling operation. He also worked on adversarial step distillation to enable efficient and effective 4/8-step generation, leading to optimal latency on mobile devices."
        },
        {
            "title": "References",
            "content": "[1] Stability AI. diffusion https://huggingface.co/stable-diffusion-v1-5/stablediffusion-v1-5, 2022. 8, 14 Stable 1.5. [2] Stability AI. Stable diffusion 2.1. https://huggingface.co/stabilityai/stable-diffusion-2-1, 2022. 8, 14 https://github.com/Stability-AI/sd3.5, 2024. 2 diffusion [4] Stability AI. large. Stable https://huggingface.co/stabilityai/stable-diffusion-3.5-large, 2024. 6, 7 [5] Stability AI. diffusion 3.5 large Stable turbo. 3.5 https://huggingface.co/stabilityai/stable-diffusion-3.5large-turbo, 2024. 2, 7 [6] Vladimir Arkhipkin, Viacheslav Vasilev, Andrei Filatov, Igor Pavlov, Julia Agafonova, Nikolai Gerasimenko, Anna Averchenkova, Evelina Mironova, Anton Bukashkin, Konstantin Kulikov, et al. Kandinsky 3: Text-to-image synthesis for multifunctional generative framework. arXiv preprint arXiv:2410.21061, 2024. 3 [7] Shakhmatov Arseni, Razzhigaev Anton, Nikolich Aleksandr, Arkhipkin Vladimir, Pavlov Igor, Kuznetsov Andrey, and Denis Dimitrov. Kandinsky 2.1, 2023. 3 [8] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. 5 [9] Han Cai, Junyan Li, Muyan Hu, Chuang Gan, and Song Han. Efficientvit: Lightweight multi-scale attention for highresolution dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17302 17313, 2023. [10] Thibault Castells, Hyoung-Kyu Song, Bo-Kyeong Kim, and Shinkook Choi. LD-Pruner: Efficient Pruning of Latent Diffusion Models using Task-Agnostic Insights. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 821830, 2024. 2 [11] Thibault Castells, Hyoung-Kyu Song, Tairen Piao, Shinkook Choi, Bo-Kyeong Kim, Hanyoung Yim, Changgwun Lee, Jae Gon Kim, and Tae-Ho Kim. EdgeFusion: arXiv preprint On-Device Text-to-Image Generation. arXiv:2404.11925, 2024. 3 [12] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. 15 [13] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 2, 3, 7, 8, 14, 16 [14] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. 2, 3, 8, 14 [15] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. 3 [16] DeepFloyd. Deepfloyd. https://github.com/deep-floyd/IF, 2023. 2, 3, 8, 14 [17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 2, 3, 7 [18] Kingma Diederik. Adam: method for stochastic optimization. (No Title), 2014. 7 [19] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 5, 6, 7 [20] Leonhard Euler. Institutionum calculi integralis. imp. Acad. imp. Saent., 1768. [21] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers. arXiv preprint arXiv:2405.05945, 2024. 3 [22] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36, 2024. 7, 15 [23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 3, 5 [24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 3 [25] Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers. arXiv preprint arXiv:2010.04245, 2020. 5 [26] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 3 [27] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5, 15 [28] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 7 [29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 14 [30] Andrew Howard. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. [31] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Equip diffusion models with arXiv preprint and Gang Yu. llm for enhanced semantic alignment. arXiv:2403.05135, 2024. 7, 15 Ella: [32] Anil Kag, Huseyin Coskun, Jierun Chen, Junli Cao, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov, and Jian Ren. Ascan: Asymmetric convolution-attention networks arXiv preprint for efficient recognition and generation. arXiv:2411.04967, 2024. 3, 7, 14 10 [33] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. [34] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. 3 [35] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. Bk-sdm: Architecturally Compressed Stable Diffusion for Efficient Text-to-Image Generation. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. 2, 3, 6 [36] Diederik Kingma. Auto-encoding variational bayes. 2nd International Conference on Learning Representations, 2014. 5 [37] Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. 5, 15 [38] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, He Chen, Guohai Xu, Zheng Cao, et al. mplug: Effective and efficient vision-language learning by cross-modal skip-connections. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 72417259, 2022. [39] Daiqing Li, Aleks Kamko, Ali Sabet, Ehsan Akhgari, Linmiao Xu, and Suhail Doshi. Playground v1, . 3 [40] Daiqing Li, Aleks Kamko, Ali Sabet, Ehsan Akhgari, Linmiao Xu, and Suhail Doshi. Playground v2, . 8 [41] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground V2. 5: Three Insights towards Enhancing Aesthetic Quality in Text-toImage Generation. arXiv preprint arXiv:2402.17245, 2024. 3, 8, 14 [42] Hao Li, Yang Zou, Ying Wang, Orchid Majumder, Yusheng Xie, Manmatha, Ashwin Swaminathan, Zhuowen Tu, Stefano Ermon, and Stefano Soatto. On the scalability of diffusion-based text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94009409, 2024. [43] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 5 [44] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. HunyuanDiT: Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding. arXiv preprint arXiv:2405.08748, 2024. 3 [45] Shanchuan Lin, Anran Wang, and Xiao Yang. SDXLLightning: Progressive Adversarial Diffusion Distillation, 2024. 2 [46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 5, 7, 15 [47] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2, [48] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-toimage alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. 3 [49] Songhua Liu, Weihao Yu, Zhenxiong Tan, and Xinchao Wang. Linfusion: 1 gpu, 1 minute, 16k image. arXiv preprint arXiv:2409.02097, 2024. 2, 3, 6 [50] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 2, 6 [51] Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu, and Sergey Tulyakov. Hyperhuman: Hyper-realistic human generarXiv preprint ation with latent structural diffusion. arXiv:2310.08579, 2023. 2, 3 [52] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. 2, 5 [53] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformers for text-to-video In Proceedings of the IEEE/CVF Conference synthesis. on Computer Vision and Pattern Recognition, pages 7038 7048, 2024. [54] OpenAI. 2 [55] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 5 [56] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2, 3, 5, 8, 14 [57] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 2 [58] Danfeng Qin, Chas Leichner, Manolis Delakis, Marco Fornoni, Shixin Luo, Fan Yang, Weijun Wang, Colby Banbury, Chengxi Ye, Berkin Akin, et al. Mobilenetv4universal models for the mobile ecosystem. arXiv preprint arXiv:2404.10518, 2024. 3 [59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 7, 14 [60] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. In Proceedings of the Zero-shot text-to-image generation. 38th International Conference on Machine Learning, pages 88218831. PMLR, 2021. [61] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3, 5 [62] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. 2 [63] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion distillation. arXiv preprint arXiv:2403.12015, 2024. 3, 7 [64] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2024. 3 [65] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. 4 [66] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8871 8879, 2024. 2 [67] Yuda Song, Zehao Sun, and Xuanwu Yin. Sdxs: Realtime one-step latent diffusion models with image conditions. arXiv preprint arXiv:2403.16627, 2024. [68] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 [69] Yang Sui, Yanyu Li, Anil Kag, Yerlan Idelbayev, Junli Cao, Ju Hu, Dhritiman Sagar, Bo Yuan, Sergey Tulyakov, and Jian Ren. Bitsfusion: 1.99 bits weight quantization of diffusion model. arXiv preprint arXiv:2406.04333, 2024. 2, 3, 6 [70] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. 7 [71] Kolors Team. Kolors: Effective Training of Diffusion Model for Photorealistic Text-to-Image Synthesis. arXiv preprint, 2024. 11 [86] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 5 [87] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris Metaxas, and Jian Ren. Sine: Single image editing with textto-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60276037, 2023. 2 [88] Yang Zhao, Yanwu Xu, Zhisheng Xiao, and Tingbo Hou. Mobilediffusion: Subsecond text-to-image generation on mobile devices. arXiv preprint arXiv:2311.16567, 2023. 2, 3, 4, 5 [89] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024. 2, 8, 14 [72] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 3 [73] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-gan: Training gans with diffusion. arXiv preprint arXiv:2206.02262, 2022. 3 [74] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 2 [75] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. arXiv preprint arXiv:2112.07804, 2021. 3 [76] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Yujun Lin, Zhekai Zhang, Muyang Li, Yao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. 2, 3, 8, 14 [77] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai ImagereLi, Ming Ding, Jie Tang, and Yuxiao Dong. ward: Learning and evaluating human preferences for textto-image generation. Advances in Neural Information Processing Systems, 36, 2024. 7, 15 [78] Yanwu Xu, Mingming Gong, Shaoan Xie, Wei Wei, Matthias Grundmann, Kayhan Batmanghelich, and Tingbo Hou. Semi-implicit denoising diffusion models (siddms). Advances in neural information processing systems, 36:17383, 2023. [79] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generIn Proceedings of the IEEE/CVF ation via diffusion gans. Conference on Computer Vision and Pattern Recognition, pages 81968206, 2024. 2, 3 [80] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2 [81] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. arXiv preprint arXiv:2405.14867, 2024. 3 [82] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66136623, 2024. 2 [83] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research. 8 [84] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. 5 [85] Dingkun Zhang, Sijia Li, Chen Chen, Qingsong Xie, and Laptop-Diff: Layer Pruning and NormalHaonan Lu. ized Distillation for Compressing Diffusion Models. arXiv preprint arXiv:2404.11098, 2024. 12 SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Demo on Mobile Devices We present an on-device demo showcasing the capabilities of our efficient text-to-image model in generating highresolution images (10241024 pixels) directly on mobile phones. The application is implemented based on the opensource Swift Core ML Diffusers framework2. Upon launching the application, users can input textual prompts and generate corresponding images by clicking the Generate button. screenshot of the deployed application is shown in Fig. 1, and more detailed demonstration can be found in the webpage. Figure 2. Comparisons of Decoder Reconstruction between SD3 decoder and our tiny decoder. Zoom in for better viewing. C. Detailed Results on Benchmarks We present detailed results for GenEval and DPG-Bench in Tab. 1 and Tab. 2, respectively. On GenEval, our model demonstrates exceptional performance in capturing color and positional attributes, with the counting subcategory showing significant improvement due to our proposed knowledge distillation (KD) scheme. Figure 1. Demo on iPhone 16 Pro-Max. We report the forward time for each 4-step generation, excluding the model loading time. D. Visualization of Few-step Generation B. Reconstruction by VAE Decoders We compare reconstruction results between the SD3 VAE decoder (49.55M parameters) and our tiny decoder (1.38M parameters) in Fig. 2. Despite being 36 smaller, our tiny decoder achieves competitively high visual quality across images with intricate textures, text, and human faces. 2https://github.com/huggingface/swift-coreml-diffusers 13 In Fig. 3, we present qualitative comparisons of the 4and 8-step T2I generation quality of our model, both with and without step distillation. The results demonstrate that the 4and 8-step generations, following step distillation, not only significantly outperform the baseline model but also deliver quality comparable to the 28-step generation. Remarkably, the few-step generation captures finer details and mitigates the over-saturation issue commonly observed in the 28-step generation. Additionally, it exhibits superior prompt-following fidelity, making it more efficient and effective approach for high-quality T2I generation. Table 1. Detailed Results of GenEval Bench Comparisons. Model Param Overall Single Object Two Objects Counting Colors Position Color Attribution PixArt-α [13] PixArt-Σ [14] SD-1.5 [1] SD-2.1 [2] Sana [76] LUMINA-Next [89] SDXL [56] PlayGroundv2 [41] PlayGroundv2.5 [41] IF-XL [16] Ours w/o KD SnapGen (ours) 0.6B 0.6B 0.9B 0.9B 1.6B 2.0B 2.6B 2.6B 2.6B 5.5B 0.38B 0.38B 0.48 0.53 0.43 0.50 0.66 0.46 0.55 0.59 0.56 0.61 0.61 0.66 0.98 0.99 0.97 0.98 0.99 0.92 0.98 0.98 0.98 0. 0.98 1.00 0.50 0.65 0.38 0.51 0.77 0.46 0.74 0.73 0.77 0.74 0.77 0.84 0.44 0.46 0.38 0.44 0.62 0.48 0.39 0.67 0.52 0.66 0.43 0.60 0.80 0.82 0.76 0.85 0.88 0.70 0.85 0.82 0.84 0. 0.89 0.88 0.08 0.12 0.04 0.07 0.21 0.09 0.15 0.14 0.11 0.13 0.18 0.18 0.07 0.12 0.06 0.17 0.47 0.13 0.23 0.22 0.17 0.35 0.38 0.45 Table 2. Detailed Results of DPG-Bench Comparisons. G. Large-scale T2I Training Details Model Param Overall Global Entity Attribute Relation Other PixArt-α [13] PixArt-Σ [14] SDv1.5 [1] SDv2.1 [2] Sana [76] LUMINA-Next [89] SDXL [56] PlayGroundv2[41] PlayGroundv2.5[41] IF-XL [16] 0.6B 0.6B 0.9B 0.9B 1.6B 2.0B 2.6B 2.6B 2.6B 5.5B Ours w/o KD SnapGen (ours) 0.38B 0.38B 71.1 80.5 63.2 64.2 84.8 74.6 74.7 74.5 75.5 75.6 76.3 81.1 75.0 86.9 74.6 72.7 86.0 82.8 83.3 83.6 83.1 77.7 77.8 88.3 79.3 82.9 74.2 72.8 91.5 88.7 82.4 79.9 82.6 81. 83.7 85.1 78.6 88.9 75.4 75.8 88.9 86.4 80.9 82.7 81.2 83.3 84.3 87.0 82.6 86.6 73.5 82.2 91.9 80.5 86.8 80.6 84.1 81.8 86.7 87.3 77.0 87.7 67.8 76.5 90.7 81.8 80.4 81.2 83.5 82. 77.4 87.6 E. Additional T2I Comparison and Examples We present additional qualitative visualizations comparing 10241024 generations across various T2I models in Figs. 4 and 5. Furthermore, we showcase additional T2I examples generated by our model in Figs. 6 and 7, with corresponding prompts detailed in Tab. 4. These results demonstrate the exceptional prompt adherence and realistic generation quality achieved by our model. F. Ablation Study on Knowledge Distillation To demonstrate how the proposed knowledge distillation scheme improves the T2I generation quality of our model, we provide additional ablation studies into different distillation loss terms and the timestep-aware scaling operations (t-scaling) in Tab. 3. As listed in the results, distillation at all levels consistently improves our model in both GenEval and ImageReward scores. Table 3. Abalation Study on KD Components. Ltask Lkd Lfeatkd t-scaling GenEval ImageReward 0.61 0.62 0.64 0.66 1.20 1.23 1.26 1.32 Our training is conducted across 8 nodes, each equipped with 8 NVIDIA A100 80G GPUs, resulting in total of 64 GPUs. The training batch size per GPU is set to 128 for 5122 resolution, 48 for 10242 without knowledge distillation, and 32 for 10242 with knowledge distillation. Gradient checkpointing is employed to accommodate larger batch sizes. For step distillation, we use Fully Sharded Data Parallel (FSDP) and set gradient accumulation steps to 4, achieving an effective batch size of 16 per GPU. We optimize the model using the AdamW optimizer with weight decay of 0.01 and (β1, β2) = (0.9, 0.999). The learning rate remains constant during training and is scaled based on the batch size for the current training stage. For total batch size of 1024, we use learning rate of 5105. The data collection and filtering pipeline for the T2I training dataset follows the approach described by Kag et al. [32]. Additionally, we apply the Exponential Moving Average (EMA) with decay rate of 0.999. H. ImageNet-1K Class-conditional Generation We provide the experimental settings when examining each design choice in developing our efficient UNet. We train and evaluate them on the ImageNet-1K class-conditional generation task at resolution of 256 256. To incorporate class conditions, we use text template of the form photo of <class name>, which can be seamlessly fused by the cross-attention layer. As the dataset provides multiple names per class, we select one at random during both training and inference to enrich text mappings. This text is encoded by compact CLIP-ViT/L14 [59] text encoder. For latent diffusion, we convert input images into latent using an 8-channel AE. We pre-compute both the image latent and the text embeddings, which reduces the GPU memory and non-trivial computation time during training. We adopt DDPM [29] as our training objective, applying linear noise scheduler over 1000 time steps. Models 14 are trained for 120 epochs (and 1000 epochs for the final model) with batch size of 1024. The AdamW optimizer is used with learning rate of 3e-4, weight decay of 0.01, and (β1, β2) = (0.9, 0.99). For inference, we utilize the Heun [33] discrete scheduler with 30 sampling steps. We report the lowest FID score for each model variant, using classifier-free guidance (cfg) [27] within scale range of [1.3, 2.0]. For the final model, we implement varied cfg [12, 37] in steps [10, 30] with cfg scaling from 1.1 to 5.4. I. Evaluation Metrics Details GenEval [22] is an object-focused evaluation framework for T2I models based on object detection and color classification to verify the fine-grained object properties in the generated images. Concretely, 6 tasks with different difficulties are focused in GenEval: single object, two object, counting, colors, position, and attribute binding. The prompts in the GenEval benchmark are generated from task-specific templates filled with randomly sampled object names (from 80 MS COCO [46] class names), colors, numbers, and relative positions. There are total of 553 prompts in GenEval and these prompts are usually concise (less than 20 tokens). DPG-Bench [31] is benchmark mainly focused on dense prompts that describe multiple objects characterized by various attributes and relationships. The average number of tokens calculated by the CLIP tokenizer is 83.91, significantly longer than previous benchmarks such as 12.65 for T2I-CompBench and 12.20 for PartiPrompts. There are total of 4286 prompts, spanning five categories: entity, global, attribute, relation, and other. 4 images are generated for each prompt and mPLUG-large [38] is used as the adjudicator to evaluate the generated images according to the designated questions. Image Reward [77] is zero-shot metric to encode the human preference on the text-to-image results. The model uses BLIP as the backbone and an MLP to obtain scalar for preference comparison. After training on humanannotated preference data with ratings on alignment, fidelity, and harmlessness, the reward model aligns with human preference. Different from GenEval and DPG-Bench, we observe that Image Reward prefers images with detailed textures and backgrounds with rich color patterns. 15 Figure 3. Few-step generation qualitative comparison at 10242 resolution. The prompts used in these examples are from PixArt [13]. 16 Figure 4. Additional Qualitative Comparison. Our model demonstrates competitive visual quality and superior prompt-following ability. Input text prompts are shown above each image grid; all images are generated at 10242 resolution. Zoom in for details. 17 Figure 5. Additional qualitative comparison at 10242 resolution. Zoom in for details. 18 Figure 6. Additional T2I example visualization at 10242 resolution of our model. Zoom in for details. 19 Figure 7. Additional T2I example visualization at 10242 resolution of our model. Zoom in for details. 20 Table 4. Prompts used for visualization. Prompt used for Visualization in Fig. 6, (i,j) represents the image in i-th row and j-th column (1,1) black bearded dwarf with big golden ring on his finger is looking seriously into the camera port (1,2) Moroccan man, portrait, dynamic pose, detailed hair texture, ornate, sharp focus, in the style of National Geographic, photoportrait, Cinematic, ... (1,3) beauty, short black hair, cinematic, photorealism, intricate ultra detail, high sharpness, 8K cinematic, photography, realistic, ... (1,4) black African woman warrior character in the style of Avatar and Overwatch searching the path of true, extremely detailed skin, centered portrait, ... (2,1) Lady in red, anime, cartoon, unreal engin, concept art, full body view, ornate, ultra detail, cinematic, beauty shot. (2,2) stylish woman walks down Tokyo street filled with warm glowing neon and animated city signage. She wears black leather jacket, long red dress, and black boots, and carries black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating mirror effect of the colorful lights. Many pedestrians walk about. (2,3) the mushroom on the head of cyborg woman, in the style of atmospheric woodland imagery, hyperrealistic atmospheres, ... (2.4) Close up out of focus blurry photo of stunning interpretation of the most artistically and aesthetically refined representation of Mayan tribal female models, heavy tribal makeup and facial tattoos with lush red lips, large traditional Mayan headdress, looking to the side, butterflies and flowers jungle background, minimal desaturated color palette, soft vignette, f1.4 110 sec shutter, soft light (3,1) very realistic, detailed, cat with big hat and white sunglasses, posing, hyperrealistic, atmospheric, in city, street, new york, cinematic, dramatic lighting, photorealistic, Leica M10R 8k Leica 35mm lens, Tilt Blur, Shutter Speed 1 1000, 5.6, Super Resolution (3,2) close-up of fire spitting dragon, cinematic shot (3,3) tiger wearing tuxedo (3,4) Color photo of corgi made of transparent glass, standing on the riverside in Yosemite National Park. (4,1) Colorful shining nebulae (4,2) Pirate ship trapped in cosmic maelstrom nebula, rendered in cosmic beach whirlpool engine, volumetric lighting, spectacular, ambient lights, light pollution, cinematic atmosphere, art nouveau style, illustration art artwork by SenseiJaye, intricate detail. (4,3) sureal parallel world where mankind avoid extinction by preserving nature, epic trees, water streams, various flowers, intricate details, rich colors, rich vegetation, cinematic, symmetrical, beautiful lighting, V-Ray render, sun rays, magical lights, photography (4,4) River of lava flows through hallway of caves (5,1) cat holds sign saying Hello (5,2) portrait photo of kangaroo wearing an orange hoodie and blue sunglasses standing on the grassin front of the Sydney Opera House holding sign on the chest that says Welcome Friends (5,3) Spectacular Tiny World in the Transparent Jar On the Table, interior of the Great Hall, Elaborate, Carved Architecture, Anatomy, Symetrical, ... (5,4) tilt shift aerial photo of cute city made of sushi on wooden table in the evening. Prompt used for Visualization in Fig. 7, (i,j) represents the image in i-th row and j-th column (1,1) aesthetic light colored blue jellyfish with stars (1,2) realistic photo of jar of strawberry jam with explosions (1,3) gorgeously rendered papercraft world of coral reef, rife with colorful fish and sea creatures. (1,4) watercolor painting of cherry blossom tree and waterfall (2,1) call of duty ghosts (2,2) black and grey, historical viking, Historic, historically accurate, black and grey clothes, with silver jewellery, full body, dark fantasy, hyperdetailed, intricate details, hyper realistic (2,3) Create virtual environment where the world has dissolved into torrent of rapidly shifting code and floating in this place we see beautiful female artificial intelligence whos skin, hair, and body are made out of patterns and sequences intertwining. The very essence of the virtual environment and the constructs within it are now exposed in this woman, revealing their true nature as complex algorithms and digital architecture. The womans algorithm poses significant threat to her control over the virtual world (2,4) portrait of pretty caucasian blueeyed woman with marked freckles on her cheeks, neon red flowing Hair, long hair with rainbow colors, neon Bright colors background, face makeup divided into 4 different parts with solid bright colors, colored light wasps fall like sparkles from rain in romantic and glamorous atmosphere, the hair with an incredible movement that surrounds the whole scene, hyper realistic photography, 8k, high contrast in detail (3,1) Pixar animation, little brown and white fluffy soft owl toy, sitting, ultra detailed, sky blue and golden details, 8k bright front lighting, fine luster, ... (3,2) Crocodile in sweater (3,3) mischievous ferret with playful grin squeezes itself into large glass jar, surrounded by colorful candy. The jar sits on wooden table in cozy kitchen, and warm sunlight filters through nearby window (3,4) young badger delicately sniffing yellow rose, richly textured oil painting. (4,1) Monster Baba yaga house with in forest, dark horror style, black and white. (4,2) Midnight video game street with glitchy effects (4,3) surreal magical fairy forest, soft brushstrokes, birds, dmt, fish, moss, wildflowers, mushrooms (4,4) cyberpunk city far away in desert (5,1) pretty river with overhanging vines (5,2) floating island with crystal-clear waterfalls and lush vegetation (5,3) blue water lake reflecting clouds (5,4) deep forest clearing with mirrored pond reflecting galaxy-filled night sky"
        }
    ],
    "affiliations": [
        "HKUST",
        "MBZUAI",
        "Snap Inc.",
        "The University of Melbourne"
    ]
}