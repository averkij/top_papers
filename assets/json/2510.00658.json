{
    "paper_title": "Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents",
    "authors": [
        "Beomsu Kim",
        "Byunghee Cha",
        "Jong Chul Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With diffusion and flow matching models achieving state-of-the-art generating performance, the interest of the community now turned to reducing the inference time without sacrificing sample quality. Consistency Models (CMs), which are trained to be consistent on diffusion or probability flow ordinary differential equation (PF-ODE) trajectories, enable one or two-step flow or diffusion sampling. However, CMs typically require prolonged training with large batch sizes to obtain competitive sample quality. In this paper, we examine the training dynamics of CMs near convergence and discover that CM tangents -- CM output update directions -- are quite oscillatory, in the sense that they move parallel to the data manifold, not towards the manifold. To mitigate oscillatory tangents, we propose a new loss function, called the manifold feature distance (MFD), which provides manifold-aligned tangents that point toward the data manifold. Consequently, our method -- dubbed Align Your Tangent (AYT) -- can accelerate CM training by orders of magnitude and even out-perform the learned perceptual image patch similarity metric (LPIPS). Furthermore, we find that our loss enables training with extremely small batch sizes without compromising sample quality. Code: https://github.com/1202kbs/AYT"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 8 5 6 0 0 . 0 1 5 2 : r Preprint. ALIGN YOUR TANGENT: TRAINING BETTER CONSISTENCY MODELS VIA MANIFOLD-ALIGNED TANGENTS Beomsu Kim*, Byunghee Cha*, Jong Chul Ye Graduate School of AI, KAIST *Equal Contribution"
        },
        {
            "title": "ABSTRACT",
            "content": "With diffusion and flow matching models achieving state-of-the-art generating performance, the interest of the community now turned to reducing the inference time without sacrificing sample quality. Consistency Models (CMs), which are trained to be consistent on diffusion or probability flow ordinary differential equation (PF-ODE) trajectories, enable one or two-step flow or diffusion sampling. However, CMs typically require prolonged training with large batch sizes to obtain competitive sample quality. In this paper, we examine the training dynamics of CMs near convergence and discover that CM tangents CM output update directions are quite oscillatory, in the sense that they move parallel to the data manifold, not towards the manifold. To mitigate oscillatory tangents, we propose new loss function, called the manifold feature distance (MFD), which provides manifold-aligned tangents that point toward the data manifold. Consequently, our method dubbed Align Your Tangent (AYT) can accelerate CM training by orders of magnitude and even out-perform the learned perceptual image patch similarity metric (LPIPS). Furthermore, we find that our loss enables training with extremely small batch sizes without compromising sample quality. Code: https://github.com/1202kbs/AYT"
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models (DM) (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021a) and flow models (FM) (Liu et al., 2022; Liu, 2022; Lipman et al., 2023) have achieved remarkable progress in generative modeling over the past few years. Their strength lies in their ability to trade-off sample quality with sampling cost. Concretely, by increasing the number of score model or velocity evaluations during sample synthesis, one can reduce error in solving diffusion SDEs or flow ODEs, and thus enhance the quality of synthesized samples (Song et al., 2021b; Lipman et al., 2023). With DMs and FMs achieving state-of-the-art generative performance (Dhariwal & Nichol, 2021; Karras et al., 2022; 2023; Esser et al., 2023), the interest of the community turned to reducing the inference cost without compromising sample quality (Lu et al., 2022; Dockhorn et al., 2022; Salimans & Ho, 2022; Zhang & Chen, 2023; Kim & Ye, 2023). One promising learning-based approach to accelerating DMs and FMs is consistency models (CM) (Song et al., 2023), which are trained to transport noise to data along PF-ODE trajectories with only minimal number of, e.g., one or two, neural net evaluations. However, CM learning is often unstable, and is prone to divergence during training (Song et al., 2023). Subsequent works have found that better hyper-parameter choices (Song & Dhariwal, 2024; Lu & Song, 2025), techniques such as truncation (Lee et al., 2025), or joint learning of diffusion score or flow velocity (Kim et al., 2024a; Boffi et al., 2025; Geng et al., 2025a; Sabour et al., 2025) can accelerate and stabilize training. In this paper, we take an orthogonal, training dynamics-based approach to improving CMs by examining and enhancing CM loss functions. While there are works which propose better loss functions for DM or FM training (Hoogeboom et al., 2023; Kim et al., 2025a; Lin & Yang, 2024; Berrada et al., 2025), losses for CMs have been left relatively under-explored after the pseudo-Huber loss gained popularity due to its ability to reduce variance during training (Song & Dhariwal, 2024). The learned perceptual image patch similarity (LPIPS) metric (Zhang et al., 2018) has shown to be powerful loss for training CMs (Song et al., 2023; Kim et al., 2024a), but construction of LPIPS involves extensive engineering such as supervised pre-training on ImageNet (Deng et al., 2009) and fine-tuning on human-curated dataset of patch similarity. 1 Preprint. Figure 1: Left: CM tangents, i.e., CM output update directions, exhibit large oscillations throughout training. To mitigate this, we learn feature maps ϕ whose level sets ϕ1(α) model increasingly perturbed data manifolds, so feature map gradients point towards the manifold. CM tangents in the feature space are expressed as linear combinations of feature map gradients, so we obtain manifoldaligned tangents. Right: Manifold-aligned tangents (AYT) enable up to 10 faster convergence and competitive FIDs with 1/8 batch size (bs). We use Easy Consistency Training (ECT) (Geng et al., 2025b). Shaded regions indicate min/max FIDs over three sample generation trials. Given such situation, in our work, we propose loss function which is as powerful as LPIPS for training CMs, but is simpler to construct and does not require human supervision. Unlike pseudoHuber or LPIPS, our loss is grounded on rigorous analysis of CM training dynamics, such that it consists of interpretable design choices. We test our loss function on number of variety of image generation tasks, and show that it accelerates CM training by orders of magnitude when compared to training with the pseudo-Huber loss. Furthermore, our loss simultaneously improves generative performance, and even outperforms LPIPS. Concretely, our contributions can be summarized as: We discover potential cause of slow convergence in CM training (Section 4). We examine the training dynamics of CMs near convergence and identify that tangents, i.e., update directions for CM outputs, contain non-trivial amount of components which oscillate parallel to the data manifold. We hypothesize that such oscillatory components can hinder the convergence of CMs, and that one must amplify manifold-orthogonal components, i.e., components which point towards the data manifold, to enhance performance. We propose the manifold feature distance to accelerate convergence (Section 5). We discover that when CM loss is computed in feature space, tangents are linear combinations of the rows of the feature map Jacobian. Inspired by this observation, we design manifold feature maps whose Jacobian consists of directions that point toward the data manifold. Consequently, computing consistency losses with our manifold feature distance provides manifold-aligned tangents with minimal oscillatory components. We verify our method on number of benchmark tasks (Section 6). We train CMs on standard benchmark datasets CIFAR10 and ImageNet 64 64 with our manifold feature distance. We observe that our loss accelerates convergence by orders of magnitude compared to training with the pseudo-huber loss, and beats LPIPS. Furthermore, we discover that training with manifold feature distance is robust to batch size, yielding competitive FID scores with batch size as small as 16. Overall, experiments corroborate our hypothesis that oscillatory components in tangents hinder CM convergence."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Regularization for DMs and CMs. Similar to GANs, regularization for efficient training has also been actively explored in diffusion and consistency models. In particular, early stopping has been introduced to mitigate overfitting, which often occurs at small timesteps (Nichol & Dhariwal, 2021; Lee et al., 2025). Although simple, this approach is effective in preventing the model from overadapting to the data distribution and has therefore received significant attention. Subsequently, various data augmentation strategies have been proposed. For example, non-leaking augmentation (Karras et al., 2022) and noise perturbation techniques (Daras et al., 2025; Ning et al., 2023) have been 2 Preprint. employed to improve generalization and enhance robustness under diverse conditions. More recently, research has shifted toward incorporating more sophisticated auxiliary learning signals. For instance, contrastive learning objectives have been adopted to encourage the model to acquire more discriminative image samples (Stoica et al., 2025), while the outputs of pre-trained representation models have been aligned with intermediate diffusion features to accelerate training and improve convergence stability (Yu et al., 2025; Jeong et al., 2025; Chefer et al., 2025). Perceptual objectives. Various studies have explored the use of perceptual metrics to facilitate the training of diffusion and consistency models. However, due to the nature of score matching, directly minimizing perceptual metrics can adversely affect the training of diffusion models. To address this, some methods use perceptual losses only after the diffusion model has been pretrained (Lin & Yang, 2024), while others incorporate them as auxiliary losses during training (Berrada et al., 2025). In the case of consistency models, perceptual metrics such as LPIPS (Zhang et al., 2018) can be directly employed as consistency losses without compromising theoretical guarantees (Song et al., 2023). Fast sampling of diffusion and flow models. While diffusion models have demonstrated remarkable performance in image and video generation, their sampling process often requires hundreds to thousands of steps, resulting in significant computational cost. To address this limitation, wide range of approaches have been proposed to enable fast sampling, where high-quality samples can be generated with only few steps. Early studies (Lu et al., 2022; Zhang & Chen, 2023; Dockhorn et al., 2022; Zhou et al., 2024b) primarily focused on improving ODE solvers. By mitigating error accumulation across timesteps, these methods reduced the required number of sampling steps to about 10. Beyond solver improvements, several approaches have aimed to directly train models capable of efficient sampling. representative example is Rectified Flow (Liu et al., 2022; Liu, 2022; Zhu et al., 2024; Liu et al., 2024; Lee et al., 2024; Kim et al., 2025a), which straightens the ODE trajectory from noise to image, thereby minimizing error accumulation under small number of steps. Another line of research is diffusion model distillation (Salimans & Ho, 2022; Meng et al., 2023; Kim et al., 2024b). In this paradigm, pretrained diffusion model is distilled into new single-step generative model by leveraging objectives such as diffusion losses. More recently, flow mapbased approaches (Song et al., 2023; Kim et al., 2024a; 2025b; Sabour et al., 2025; Geng et al., 2025a) have been introduced, which learn the trajectory of an ODE directly, predicting the destination at target timestep from an input at source timestep. These methods are particularly notable as they can be applied both in the context of distillation and from-scratch training. Consistency models can be interpreted as special case of this family, where the target timestep is set to zero, allowing the model to predict the final image directly from noise."
        },
        {
            "title": "3 BACKGROUND\nOur goal is to learn a generative model of a data distribution p(x) supported on Rd. Given a forward\nor corruption process from data x",
            "content": "p(x) to noise ϵ (0, I) xt = αtx + σtϵ (1) [0, ) with boundary conditions x0 = and limt αt/σt = 0, let us parametrized by time denote the distribution of xt at time as pt. The corresponding probability flow ordinary differential equation (PF-ODE), i.e., an ODE whose marginal equals pt for all time t, is given by x,ϵxt[ xt] dt = x,ϵxt[ αtx + σtϵ] dt dxt = (2) and velocity can be learned by solving flow matching (Lipman et al., 2023; Albergo et al., 2023) (0, σ2 2 2]. v(xt, t) ( αtx + σtϵ) minv Ex,ϵ,t[ In particular, we note that p0 = and pT I) for sufficiently large time , so we can (0, I) and solving the PF-ODE down from time = to 0 with sample from by sampling ϵ terminal condition xT = σT ϵ. However, numerical integration of the PF-ODE involves multiple evaluations of the velocity, so the generation process is often slow and costly. consistency model (CM) fθ : Rd , 0) = idRd is trained to be consistent, i.e., to have identical outputs, on PF-ODE trajectories (Song et al., 2023; Song & Dhariwal, 2024; Lu & Song, 2025). Hence, an optimal CM fθ will map all points on the PF-ODE trajectory back to its initial point at = 0 with single function evaluation. In particular, (0, I) will be distributed according to p, so CM can bypass the the output fθ (σT ϵ, ) for ϵ computational burden of solving the PF-ODE. Rd with boundary condition fθ( (3) [0, ) Preprint. Figure 2: CM tangent visualization on CIFAR10 after training to near-convergence (400k iterations). First row: inputs xt = x0 + tϵ. Second row: outputs fθ(xt, t). Third row: vanilla CM tangents computed with Eq. (6). Tangents are averaged along the channel dimension for visualization, and red and blue pixels indicate positive and negative values, resp. Fourth row: manifold-aligned tangents (AYT) computed with Eq. (14). The discrete CM objective (Song et al., 2023) forces fθ to be consistent on consecutive timesteps1: (4) Here, is loss function such as LPIPS, mean squared error, and pseudo-Huber. With the choice of d(x, y) = 1 2 2, one can derive an alternative objective with equivalent gradients: minθ Ex,ϵ,t,t[(t)1d(fθ(xt, t), fsg[θ](xtt, t))] 2 minθ Ex,ϵ,t,t[fθ(xt, t)(fsg[θ](xt, t)/t)], (5) fsg[θ](xt, t)/t := (fsg[θ](xt, t) (6) Depending on how we approximate xtt given xt = αtx + σtϵ in Eq. (6), we obtain consistency distillation (CD) and consistency training (CT): the former uses xtt to distill the velocity, whereas the latter uses xtt αttx + σttϵ. fsg[θ](xtt, t))/t. v(xt, t) xt xt xt Letting 0 in Eq. (5) yields the continuous CM objective (Lu & Song, 2025) minθ Ex,ϵ,t[fθ(xt, t)(dfsg[θ](xt, t)/dt)], (7) dfsg[θ](xt, t)/dt = (8) The derivative in Eq. (8) is called the tangent, since it is tangential to the trajectory traced out by the CM output fθ(xt, t) as xt follows the PF-ODE in Eq. (2). Analogous to discrete CMs, we obtain continuous CD or CT depending on how we estimate dxt/dt in the tangent. CD uses the flow velocity dxt/dt = v(xt, t), whereas CT uses dxt/dt = xt = αtx + σtϵ. xt fsg[θ](xt, t)(dxt/dt) + fsg[θ](xt, t)/t. From Eq. (5) and Eq. (7), we can interpret discrete CM and continuous CM learning from an unified along the negative perspective of contracting each path tangent towards fθ(x0, 0) = x0 = x. The only difference between discrete CM and continuous CM lies in whether we calculate the tangent using finite differences or the exact derivative. Hence, we may use the tangent, both discrete and continuous, to analyze training dynamics of CMs. fθ(xs, s) : dxs = v(xs, s) ds, x0 = { }"
        },
        {
            "title": "4 THE OSCILLATORY TANGENT HYPOTHESIS",
            "content": "in Rd.2 Since tangents We now further assume data is supported on low-dimensional manifold , small perturrepresent instantaneous changes in path bations of the path can induce large variations in the tangent. Given the stochasticity within CM fθ(xs, s) : dxs = v(xs, s) ds, x0 = } { 1While the original CM objective also contains time-dependent weight function w(t), we omit it without loss of generality since it can be absorbed into the density function for t. 2An m-dimensional manifold in Rd is space in Rd which resembles an m-dimensional Euclidean space at neighborhood of each point, and low-dimensional manifold means manifold with d. The space which exists in, Rd in this case, is called the ambient space. The tangent space of manifold at point can intuitively be interpreted as linear approximation of the manifold at the point (Lee, 2012). Our work assumes that the manifold hypothesis, which asserts that high-dimensional data lie in vicinity of low-dimensional manifold (Narayanan & Mitter, 2010), holds in practice. 4 Preprint. (a) Vanilla CM Tangent Analysis (b) AYT (Ours) Tangent Analysis Figure 3: Tangent analysis on 2D discs after training to near-convergence (200k iterations) for vanilla CM and align your tangent (AYT). In each figure, we visualize CM inputs, CM outputs, CM tangents, manifold-parallel component of tangents, and manifold-orthogonal component of tangents. training, we hypothesized that tangents are oscillatory and unlikely to guide the CM output exactly . We also hypothesized that this phenomenon actutowards the low-dimensional data manifold ally occurs in practice, and adversely affects CM convergence. From here on, these claims will be referred to as the oscillatory tangent hypothesis. To validate this hypothesis, we began by examining CM tangents on CIFAR10 (Krizhevsky, 2009). Observations on CIFAR10. On CIFAR10, we optimized CM via consistency training (CT) for 400k iterations until near-convergence, so there were no longer large changes in the FID score.3 We then computed tangents at various noise levels ranging from = 0.01 to 80. Upon visual inspection of CM tangents in the third row of Fig. 2, we noticed that tangents contained structured patterns that could imply large movements along the manifold, not toward the manifold, in accordance with our hypothesis. To provide further evidence for the oscillatory tangent hypothesis, we performed an additional experiment on synthetic dataset with known manifold structure. Analysis on synthetic data. To further analyze training dynamics of CMs, we considered the dataset of images of two-dimensional discs which move vertically or horizontally. As previously noted by Kadkhodaie et al. (2024), this dataset is two-dimensional curved manifold with tangent space at point spanned by deformations corresponding to vertical or horizontal movement. Analogous to the previous experiment on CIFAR10, we trained CM for 200k iterations until convergence, and computed tanwith models gents at 0.01, 0.1, 1, 10, 80 200k. at iterations } { Figure 4: Amount of manifold-orthogonal components in tangents for vanilla CM and our manifold aligned tangents (AYT) throughout training. and the tangent space of Motivated by our observations on CIFAR10, we decomposed each tangent into manifold-parallel and orthogonal components. Concretely, given CM output fθ(xt, t), we computed its projection onto the manifold at the projected point. Let us denote the CM output, its projection, and the tangent space at the projected point as z, ˆz, and ˆzM . Since vectors , we defined the manifold-parallel component of the tangent dfθ(xt, t)/dt in ˆzM as ProjT ˆzM(dfθ(xt, t)/dt), and the manifold-orthogonal component as the remainder obtained by subtracting the manifold-parallel component from the tangent. We note that by definition, manifoldparallel and orthogonal components are mutually orthogonal vectors. lie along M Left panel of Fig. 3 displays CM tangents and their decomposition into manifold-parallel and orthogonal components. Indeed, we see that tangents are quite oscillatory despite the CM FID having converged tangents contain non-trivial manifold-parallel components, especially at 1. This may be concerning, because oscillatory tangents at large are at odds with the objective of CM, which is to map pure noise at large to data. In fact, as shown in the left panel of Fig. 4, we found overwhelmingly large amount of manifold-parallel components in the tangent. Altogether, there 3ECT attains 2-step FID scores of 2.20 at iteration 100k, and 2.11 at iteration 400k (Geng et al., 2025b). Preprint. were strong evidences which corroborated the oscillatory tangent hypothesis. This motivated us to design loss function which amplifies manifold-relevant components for CM training."
        },
        {
            "title": "5.1 CONSISTENCY MODEL TRAINING WITH FEATURE DISTANCE",
            "content": "2 Kim et al. (2025a) demonstrated that using loss functions of the form ℓ(x, y) = 2 with an invertible linear map ϕ for flow matching can accelerate flow model convergence by amplifying certain directions in the model gradient. For instance, the loss function with ϕ = + λ HPF, where HPF is high-pass filter, magnifies gradient components in the high-frequency regime by factor of λ + 1. Taking inspiration from this observation, we adopt similar approach for designing new loss function for training consistency models (CMs). Let us consider using (not necessarily linear) feature map ϕ : Rd into feature space Rn to define feature distance dϕ(x, y) := feature distance as the CM loss function, the continuous CM objective gradient becomes Rn which maps points in Rd 2. With the squared ϕ(x) ϕ(x) ϕ(y) ϕ(y) limt0 θ 1 2t ϕ(fθ(xt, t)) ϕ(fsg[θ](xtt, 1 (ϕ(fθ(xt, t)) = limt0 1 = limt0 (ϕ(fθ(xt, t)) =(dϕ(fθ(xt, t))/dt)Jϕ(fθ(xt, t)) θfθ(xt, t) such that CM objective with same gradient is given as ϕ(fθ(xtt, ϕ(fθ(xtt, 2 2 t)) t))) θϕ(fθ(xt, t)) t)))Jϕ(fθ(xt, t)) θfθ(xt, t) minθ Ex,ϵ,t[fθ(xt, t) sg[(dϕ(fθ(xt, t))/dt)Jϕ(fθ(xt, t))]] (dϕ(fθ(xt, t))/dt)Jϕ(fθ(xt, t)) = (cid:80)n where Jϕ(fθ(xt, t)) is the Jacobian of ϕ w.r.t. fθ(xt, t). i=1(dϕi(fθ(xt, t))/dt) fθ ϕi(fθ(xt, t)) (9) (10) (11) (12) (13) (14) It follows that when we use the squared feature distance as the loss, the d-dimensional vector Eq. (14) plays the role of CM tangent during optimization. We observe that Eq. (14) is linear combination of the rows of the Jacobian of ϕ, so ϕ completely determines which direction the tangent points to. Thus, with judiciously chosen ϕ, one can potentially suppress oscillatory components in the tangent. However, when ϕ = idRd , which is the case of the original CM, the Jacobian becomes the full-rank identity matrix Id, so the tangent is computed as linear combination of the standard basis, and is free to point in any direction. It turns out that, to align tangents toward the data manifold, one should use manifold features, which we present in the next section. 5.2 ALIGN YOUR TANGENT (AYT) WITH MANIFOLD FEATURES Eq. (14) along with our observations in Section 4 implies that an ideal feature map ϕ for optimizing CMs should possess Jacobians whose rows, i.e., gradients zϕi(z) for = 1, . . . , point toward . To this end, we consider ϕ such that for each coordinate i, its level set at the data manifold zero ϕ1 correspond to increasingly perturbed α .4 Since the gradient of scalar-valued function is orthogonal to its level set, we can versions of expect , depending on how the manifold is perturbed. Hence, we shall call each ϕi manifold feature, and dϕ as manifold feature distance. zϕi(z) would also point towards (α) for increasing values of , and ϕ1 (0) = M i α : Rd In our work, we consider pointwise manifold perturbations of the form where collection of such transformations := , αx : M} {T Rd is transformation smoothly parametrized by α T0 = idRd . Given i=1, we can parametrize ϕ with neural net and optimize } (15) α with α ϕi( α(x)) 2 2] such that ϕi(x) = α for due to via, e.g., Gaussian the condition noise addition may be sufficient to generate manifold-orthogonal feature gradients, it can also be beneficial to use anisotropic transformations to further emphasize certain off-manifold directions. T0 = idRd . We also remark that while isotropic perturbation of . In particular, with optimal ϕ, ϕi(x) = 0 for all {T minϕ x,i[n],αR[ αM 4By ϕ1 (α), we mean the level set of ϕi at α, i.e., ϕ1 (α) := {x Rd : ϕi(x) = α}. 6 Preprint. Figure 5: Ablation studies on CIFAR10. For transformation ablation and AYT vs. LPIPS, we use batch size 64. Shaded regions indicate min/max FIDs over three generation trials. Transformation and implementation details. We further limit our scope to the image domain, and consider image transformations for . We consider three image degradations given by Gaussian noise perturbation, Gaussian blur, and Mixup (Zhang et al., 2017), four geometric transformations given by isotropic scaling, anisotropic scaling, fractional rotation, and fractional translation, and four color transformations given by perturbations in brightness, contrast, hue, and saturation. This yields feature space of dimension = 15. Readers are referred to Appendix for comprehensive description of how the transformations are defined. Manifold feature ϕ is parametrized with VGG16 classification network (Simonyan & Zisserman, 2015), and in the spirit of LPIPS (Zhang et al., 2018), we also use intermediate max-pooling features as manifold features. Example for Gaussian Blur. The transformation is given as kernel with standard deviation α. Manifold feature can be learned by optimizing α(x) := κα x, where κα is blurring where unif(0, αmax) is uniform distribution on [0, αmax], and discrete CM optimization by minϕ x,αunif(0,αmax)[(ϕ(κα x) α)2] minθ Ex,ϵ,t,t[(t)1(ϕ(fθ(xt, t)) ϕ(fsg[θ](xtt, t))2]. In other words, in contrast to standard CMswhere ϕ is fixed to idRd AYT learns ϕ dynamically. (16) (17) Conceptual comparison with LPIPS. Previous works such as Song et al. (2023) and Kim et al. (2024a) have used LPIPS for training CMs. Given that LPIPS also uses classifier features to define distance between images, one may question the novelty of the manifold feature distance. Our distance distinguishes itself from LPIPS in two levels. First, our distance is tailor suited to improving CM training by aligning the tangent towards the data manifold. Second, the construction of manifold feature distance requires no human supervision and is completely self-supervised, whereas LPIPS requires ImageNet class labels and human curated dataset of patch similarities. Furthermore, as we shall show in Section 6, CMs trained with manifold feature distance beats CMs trained with LPIPS, and LPIPS suffers from FID degradation possibly due to mismatch between dataset representations."
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "6.1 ABLATION STUDIES Sanity check in controlled settings. To verify whether the manifold feature distance suppresses oscillatory components in tangents, we repeated the experiments in Section 4 with our loss in place of the mean squared error (MSE). Concretely, the tangents were now computed with Eq. (14) instead of Eq. (8). In the bottom row of Fig. 2, we observed that the tangents were scattered and sparse, possibly implying the removal of off-manifold noise. To confirm our intuition, on the two-dimensional discs dataset, we again decomposed tangents into manifold-parallel and orthogonal components, and computed the amount of manifold-orthogonal component in the tangent. Manifold feature distance successfully removed oscillatory components from the tangent, as corroborated by the dominance of manifold-orthogonal components (right panels of Fig. 3 and Fig. 4). Transformation ablations. As mentioned in Section 5.2, we considered three groups of transformations to train manifold features: three degradation-based transformations (DEG), four geometric transformations (GEO), and four color transformations (CLR). The left panel of Fig. 5 shows changes in CM learning curves as ϕ was trained with increasing number of transformations. We observed that compounding transformations was always beneficial for CM training. Especially, the 7 Preprint. Table 1: Sample quality on unconditional CIFAR10 and class-conditional ImageNet 64 64. Unconditional CIFAR10 Method Class-Conditional ImageNet 64 NFE FID Method NFE FID Diffusion models & Fast Samplers DDPM EDM (Karras et al., 2022) DPM-Solver++ (Lu et al., 2025) DPM-Solver-v3 (Zheng et al., 2023b) 1000 35 10 10 Diffusion Distillation DFNO (LPIPS) (Zheng et al., 2023a) PD (Salimans & Ho, 2022) TRACT (Berthelot et al., 2023) DMD (Yin et al., 2024b) SiD (Zhou et al., 2024a) CTM (Kim et al., 2024a) Consistency Training iCT (Song & Dhariwal, 2024) iCT-deep (Song & Dhariwal, 2024) ECT (Geng et al., 2025b) ECT+AYT (Ours) 1 1 2 1 2 1 1 1 1 2 1 2 1 2 1 2 Diffusion models & Fast Samplers EDM (Karras et al., 2022) EDM2 (Karras et al., 2023) DPM-Solver (Lu et al., 2022) 79 63 20 Diffusion Distillation DFNO (LPIPS) (Zheng et al., 2023a) PD (Salimans & Ho, 2022) TRACT (Berthelot et al., 2023) DMD (Yin et al., 2024b) DMD2 (Yin et al., 2024a) SiD (Zhou et al., 2024a) CTM (Kim et al., 2024a) Consistency Training iCT (Song & Dhariwal, 2024) iCT-deep (Song & Dhariwal, 2024) ECT-S (Geng et al., 2025b) ECT-S+AYT (Ours) 1 1 2 1 2 1 1 1 1 1 2 1 2 1 2 1 2 2.44 1.33 3.42 7.83 10.70 4.70 7.43 4.97 2.62 1.28 1.52 1.92 1.73 4.02 3.20 3.25 2.77 5.51 3.18 4.42 3.27 3.17 2.01 2.91 2.51 3.78 8.34 5.58 3.78 3.32 3.77 1.92 1. 2.83 2.46 2.51 2.24 3.60 2.11 2.61 2.13 addition of geometric transformations led to the largest improvement in FID scores, implying vanilla tangents fail to provide strong training signal towards the data manifold in geometric directions. 16, 32, 64, 128 Robustness to batch size. The middle panel of Fig. 5 displays learning curves when training with batch sizes in . Surprisingly, AYT exhibited strong FID scores even when trained with batch sizes as small as 16, and beat ECT trained with batch size 128. This result further affirms the oscillatory tangent hypothesis, and shows that removing oscillatory components from tangents is crucial for reducing variance during training. { } AYT vs. LPIPS. In the right panel of Fig. 5, we observe that AYT beats LPIPS in terms of both one and two-step generation. In particular, CM trained with LPIPS showed degradation in two-step FIDs after 200k iterations. Further analysis revealed that this pathology with LPIPS was caused by inaccurate CM outputs at small corrupting outputs at larger t. As shown in Fig. 6, denoising FIDs at = 0.8 (FID between data x0 and denoised samples fθ(x0 + tϵ, t)) for CMs trained with LPIPS diverged rapidly after 50k iterations. We speculate that this behavior arises from the distributional mismatch between ImageNet and CIFAR10 (note that LPIPS is trained on ImageNet). However, it is unclear how one can generalize LPIPS to other datasets without human supervision. This demonstrates yet another advantage of AYT over LPIPS AYT presents simple and interpretable self-supervised pipeline for constructing manifold feature distances on arbitrary datasets, enabling CM training with unbiased representations. Figure 6: Comparison of denoising FIDs at = 0.8 for CMs trained by LPIPS, pseudo-huber (PH), and manifold feature distance (AYT). 6.2 COMPARISON WITH OTHER METHODS. To evaluate the effectiveness of our method, we compare against three major families of few-step generative approaches: advanced diffusion samplers, consistency models, and diffusion distillation methods. We report FID scores across methods and numbers of function evaluations (NFE) in Tab. 1. Comparison within consistency models. On CIFAR10, AYT improves the 1-step FID from 3.60 to 2.61 over Easy Consistency Training (ECT), while maintaining comparable 2-step performance 8 Preprint. (2.11 vs. 2.13). Notably, AYT also outperforms Improved Consistency Training (iCT) (2.83 FID), despite the latter relying on multi-stage training schedules that progressively reduce timestep gaps. On ImageNet 64 64, AYT outperforms ECT by non-trivial margin in both 1and 2-step settings, reducing 1-step FID from 5.51 to 4.41, and maintains competitive 2-step performance (3.27 vs. 3.18). It also achieves competitive performance relative to iCT, while using significantly fewer resourcesmost notably, batch size of 128, which is 8 smaller than the 1024 used by iCT. These results highlight the effectiveness of our tangent alignment strategy in stabilizing consistency model training, without the need for schedule tuning, multi-stage optimization, or large-scale training infrastructure. Comparison with distillation. On CIFAR10, our method achieves competitive performance compared to state-of-the art diffusion distillation models such as Consistency Trajectory Model (CTM, FID 1.87) and Score Identity Distillation (SiD, FID 1.92), despite not relying on any pretrained 64, AYT outperforms several distillationteacher model or adversarial training. On Imagenet 64 based approaches while reducing the gap between state-of-the-art distillation approaches. This result is particularly notable given that these baselines often inherit strong priors and score functions from large pretrained diffusion models. In contrast, we train our model from scratch, yet reach comparable or superior sample quality. Comparison with fast samplers. Finally, we compare with high-order diffusion ODE solvers. Our 2-step performance surpasses that of methods such as DPM-Solver++ and DPM-Solver-v3 that operate with NFE 10, despite our significantly smaller sampling cost. These results affirm the practicality of our method for high-quality generation under extreme computational budgets."
        },
        {
            "title": "7 DISCUSSION",
            "content": "Further implications. Beyond images, it will be interesting to explore other domains such as audio, text, or multimodal data with diverse augmentation strategies. For example, in audio data, common augmentations include time-stretching, pitch-shifting, masking, or adding background noise, all of which can be utilized for learning data manifold features. Applying our approach in such settings could provide valuable insights into how well the proposed distance metric generalizes across modalities. Such extensions could further demonstrate the generality of the framework. Limitations. Our study has so far focused on relatively small-scale settings. While the method requires additional training and increases memory usage, the auxiliary classifier is lightweight: it trains much faster than the main model and adds little memory overhead. As result, we expect these constraints to be less critical in practice, even when scaling to larger datasets. We have also restricted 64. While higher-resolution experiments remain open, the our evaluation up to resolution 64 consistent improvements on CIFAR10 and ImageNet suggest that the approach may transfer well to more demanding settings. Moreover, recent high-resolution training often relies on latent diffusion models (LDMs) (Rombach et al., 2022), which downsample images by factor of 8, making our method potentially well-suited for such pipelines. In this sense, the present work should be viewed as first step: AYT establishes strong evidence on standard benchmarks while opening several promising directions for scaling and broader applications in self-supervised and generative learning."
        },
        {
            "title": "8 CONCLUSION",
            "content": "In this paper, we analyzed the training dynamics of consistency models (CMs) and showed that their update directions (tangents) often contain manifold-parallel oscillatory components, which slow convergence. Motivated by this, we introduced the MFD simple, self-supervised objective computed in the feature space of an auxiliary network trained to be sensitive to off-manifold perturbations. By aligning tangents toward the data manifold (i.e., amplifying manifold-orthogonal components), MFD contracts CM trajectories more efficiently without relying on human supervision or curated perceptual datasets. Empirically, MFD stabilizes training by orders of magnitude over the pseudo-Huber loss while improving sample quality. On CIFAR10 and class-conditional 64, our method outperforms consistency-training baselines, attains FIDs competitive ImageNet 64 with distillation approaches despite training from scratch, and remains robust even with very small batch sizes (e.g., 16). These results indicate that matching the optimization geometry of CMs to the structure of the data manifold is practical and powerful route to faster, more reliable few-step generation. 9 Preprint."
        },
        {
            "title": "REFERENCES",
            "content": "Michael S. Albergo, Nicholas M. Boffi, and Eric Vanden-Eijnden. Stochastic Interpolants: Unifying Framework for Flows and Diffusions. arXiv preprint arXiv:2303.08797, 2023. Tariq Berrada, Pietro Astolfi, Melissa Hall, Marton Havasi, Yohann Benchetrit, Adriana Romero-Soriano, Karteek Alahari, Michal Drozdzal, and Jakob Verbeek. Boosting Latent Diffusion with Perceptual Objectives. In ICLR, 2025. David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, and Eric Gu. Tract: Denoising Diffusion Models with Transitive Closure Time-Distillation. arXiv preprint arXiv:2303.04248, 2023. Nicholas Matthew Boffi, Michael Samuel Albergo, and Eric Vanden-Eijnden. Flow map matching with stochastic interpolants: mathematical framework for consistency models. In TMLR, 2025. Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models. In ICML, 2025. Giannis Daras, Adrian Rodriguez-Munoz, Adam Klivans, Antonio Torralba, and Constantinos Daskalakis. Ambient Diffusion Omni: Training Good Models with Bad Data. arXiv preprint arXiv:2506.10038, 2025. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In CVPR, 2009. Prafulla Dhariwal and Alex Nichol. Diffusion Models Beat GANs on Image Synthesis. In NeurIPS, 2021. Tim Dockhorn, Arash Vahdat, and Karsten Kreis. GENIE: Higher-Order Denoising Diffusion Solvers. In NeurIPS, 2022. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. In ICML, 2023. Zhengyang Geng, Mingyang Deng, Xingjian Bai, J. Zico Kolter, and Kaiming He. Mean Flows for One-step Generative Modeling. arXiv preprint arXiv:2505.13447, 2025a. Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and J. Zico Kolter. Consistency Models Made Easy. In ICLR, 2025b. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In NeurIPS, 2020. Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for high resolution images. In ICML, 2023. Hyeonho Jeong, Chun-Hao Paul Huang, Jong Chul Ye, Niloy Mitra, and Duygu Ceylan. Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation. In CVPR, 2025. Zahra Kadkhodaie, Florentin Guth, Eero P. Simoncelli, and Stephane Mallat. Generalization in Diffusion Models Arises from Geometry-Adaptive Harmonic Representations. In ICLR, 2024. Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training Generative Adversarial Networks with Limited Data. In NeurIPS, 2020. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the Design Space of Diffusion-Based Generative Models. In NeurIPS, 2022. Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and Improving the Training Dynamics of Diffusion Models. In CVPR, 2023. Beomsu Kim and Jong Chul Ye. Denoising MCMC for Accelerating Diffusion-Based Generative Models. In ICML, 2023. Beomsu Kim, Yu-Guan Hsieh, Michal Klein, Marco Cuturi, Jong Chul Ye, Bahjat Kawar, and James Thornton. Simple ReFlow: Improved Techniques for Fast Flow Models. In ICLR, 2025a. Beomsu Kim, Jaemin Kim, Jeongsol Kim, and Jong Chul Ye. Generalized Consistency Trajectory Models for Image Manipulation. In ICLR, 2025b. 10 Preprint. Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. In ICLR, 2024a. Sanghwan Kim, Hao Tang, and Fisher Yu. Distilling ODE Solvers of Diffusion Models into Smaller Steps. In CVPR, 2024b. Diederik P. Kingma and Jimmy Lei Ba. Adam: Method for Stochastic Optimization. In ICLR, 2015. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. John M. Lee. Introduction to Smooth Manifolds. Springer, 2012. Sangyun Lee, Zinan Lin, and Giulia Fanti. Improving the Training of Rectified Flows. NeurIPS, 2024. Sangyun Lee, Yilun Xu, Tomas Geffner, Giulia Fanti, Karsten Kreis, Arash Vahdat, and Weili Nie. Truncated Consistency Models. In ICLR, 2025. Shanchuan Lin and Xiao Yang. Diffusion Model with Perceptual Loss. arXiv preprint arXiv:2401.00110, 2024. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow Matching for Generative Modeling. In ICLR, 2023. Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the Variance of the Adaptive Learning Rate and Beyond. In ICLR, 2020. Qiang Liu. Rectified Flow: Marginal Preserving Approach to Optimal Transport. arXiv preprint arXiv:2209.14577, 2022. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. arXiv preprint arXiv:2209.03003, 2022. Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation. In ICLR, 2024. Cheng Lu and Yang Song. Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models. In ICML, 2025. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-Solver: Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps. In NeurIPS, 2022. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2206.00927, 2025. Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On Distillation of Guided Diffusion Models. In CVPR, 2023. Hariharan Narayanan and Sanjoy Mitter. Sample Complexity of Testing the Manifold Hypothesis. In NeurIPS, 2010. Alex Nichol and Prafulla Dhariwal. Improved Denoising Diffusion Probabilistic Models. In ICML, 2021. Mang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, and Rita Cucchiara. Input Perturbation Reduces Exposure Bias in Diffusion Models. In ICML, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. In CVPR, 2022. Amirmojtaba Sabour, Sanja Fidler, and Karsten Kreis. Align Your Flow: Scaling Continuous-Time Flow Map Distillation. arXiv preprint arXiv:2506.14603, 2025. Tim Salimans and Jonathan Ho. Progressive Distillation for Fast Sampling of Diffusion Models. In ICLR, 2022. Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. In ICLR, 2015. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. In ICML, 2015. 11 Preprint. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. In ICLR, 2021a. Yang Song and Prafulla Dhariwal. Improved Techniques for Training Consistency Models. In ICLR, 2024. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. In ICLR, 2021b. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency Models. In ICML, 2023. George Stoica, Vivek Ramanujan, Xiang Fan, Ali Farhadi, Ranjay Krishna, and Judy Hoffman. Contrastive Flow Matching. arXiv preprint arXiv:2506.05350, 2025. Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024a. Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T. Freeman, and Taesung Park. One-step Diffusion with Distribution Matching Distillation. In CVPR, pp. 66136623, 2024b. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think. In ICLR, 2025. Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. Mixup: Beyond Empirical Risk Minimization. In ICLR, 2017. Qinsheng Zhang and Yongxin Chen. Fast Sampling of Diffusion Models with Exponential Integrator. In ICLR, 2023. Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The Unreasonable Effectiveness of Deep Features as Perceptual Metric. In CVPR, 2018. Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. In ICML, 2023a. Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode solver with empirical model statistics. In NeurIPS, 2023b. Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation. In ICML, 2024a. Zhenyu Zhou, Defang Chen, Can Wang, and Chun Chen. Fast ODE-based Sampling for Diffusion Models in Around 5 Steps. In CVPR, 2024b. Yuanzhi Zhu, Xingchao Liu, and Qiang Liu. SlimFlow: Training Smaller One-Step Diffusion Models with Rectified Flow. In ECCV, 2024. 12 Preprint."
        },
        {
            "title": "A EXPERIMENT SETTINGS",
            "content": "We build our method on top of the Easy Consistency Training (ECT) (Geng et al., 2025b) framework with minor modifications. Unless otherwise noted, we follow ECT defaults for data preprocessing, forward process xt = + tϵ, timestep sampling, and evaluation protocol. A.1 DATA PREPROCESSING CIFAR10 and ImageNet 64 in https://github.com/NVlabs/edm. 64 datasets are preprocessed with code provided by Karras et al. (2022) A.2 MODEL ARCHITECTURES AND INITIALIZATION Classifier architectures. We use VGG16 classification networks (Simonyan & Zisserman, 2015) to parametrize manifold features. All VGG16 networks are trained from scratch without any special initialization schemes. Consistency model architectures. We adopt the same backbone choices as ECT. Specifically, we use DDPM++ (Song et al., 2021b) for CIFAR10 and EDM2-S (Karras et al., 2023) for ImageNet 64. On both datasets, we initialize the consistency model with pretrained diffusion model of 64 the corresponding architecture. A.3 CLASSIFIER TRAINING CONFIGURATIONS We use identical training configurations on CIFAR10 and ImageNet 64 64. Specifically, we use the Adam optimizer (Kingma & Ba, 2015) with learning rate 0.0001 and batch size 512. Each manifold feature is trained for 400k iterations to minimize Eq. (15). Our color and geometric transformation pipeline largely follows that described in Appendix of Karras et al. (2020). We describe the augmentation pipeline for degradations below. Specifically, given p(x), Gaussian noise. Sample α Gaussian blur. Sample α Mixup. Sample α unif(0, αmax), ϵ unif(0, αmax) , return κα x. unif(0, 0.5) and another datapoint y, return (1 (0, I), return + αϵ. α)x + αy. Here, κα is Gaussian blur kernel with sigma α. A.4 CONSISTENCY MODEL TRAINING CONFIGURATIONS We use global batch size of 128 for all runs, except in the batch-size ablation. Exponential moving average (EMA) is enabled throughout training, with dataset-specific settings detailed below. CIFAR10. We train consistency models for 400K iterations without any multi-stage schedule unlike iCT/ECT. We use the RAdam optimizer (Liu et al., 2020) with learning rate 0.0001 and exponential moving average (EMA) decay rate of 0.9999. 64. We train for 200k iterations with the same multi-stage schedule as ECT. To ImageNet 64 mitigate early-stage overfitting, we enable our loss (AYT) after 75K iterations. We use the Adam optimizer (Kingma & Ba, 2015) with learning rate 0.001 and an inverse-square-root decay schedule of decay parameter 2000. EMA follows the Power-EMA formulation introduced in EDM2, but we do not apply post-hoc EMA after training. A.5 SAMPLING AND EVALUATION We evaluate 1-step and 2-step sampling with Frechet Inception Distance (FID), computed between the training set and 50K generated samples. For 2-step sampling, the intermediate timestep is fixed to = 0.821 on CIFAR10 and = 1.526 on ImageNet 64 64, following ECT. Unless otherwise stated, we follow the ECT evaluation setup and report FID computed with 50k samples. 13 Preprint."
        },
        {
            "title": "B CONSISTENCY MODEL SAMPLES",
            "content": "(a) Samples from CM trained via ECT. (b) Samples from CM trained via AYT (Ours). Figure 7: Uncurated one-step CM samples on CIFAR10. 14 Preprint. (a) Samples from CM trained via ECT. (b) Samples from CM trained via AYT (Ours). Figure 8: Uncurated one-step CM samples on ImageNet 64 64."
        }
    ],
    "affiliations": [
        "Graduate School of AI, KAIST"
    ]
}