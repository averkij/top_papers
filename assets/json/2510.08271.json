{
    "paper_title": "SViM3D: Stable Video Material Diffusion for Single Image 3D Generation",
    "authors": [
        "Andreas Engelhardt",
        "Mark Boss",
        "Vikram Voletti",
        "Chun-Han Yao",
        "Hendrik P. A. Lensch",
        "Varun Jampani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically based rendering (PBR) materials, given a single image. Recently, video diffusion models have been successfully used to reconstruct 3D objects from a single image efficiently. However, reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits. We extend a latent video diffusion model to output spatially varying PBR parameters and surface normals jointly with each generated view based on explicit camera control. This unique setup allows for relighting and generating a 3D asset using our model as neural prior. We introduce various mechanisms to this pipeline that improve quality in this ill-posed setting. We show state-of-the-art relighting and novel view synthesis performance on multiple object-centric datasets. Our method generalizes to diverse inputs, enabling the generation of relightable 3D assets useful in AR/VR, movies, games and other visual media."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 1 7 2 8 0 . 0 1 5 2 : r SViM3D: Stable Video Material Diffusion for Single Image 3D Generation Andreas Engelhardt1,2 Mark Boss1 Vikram Voletti1 Chun-Han Yao1 Hendrik P. A. Lensch2 Varun Jampani1 1Stability AI 2University of Tubingen Figure 1. SViM3D. SViM3D predicts multi-view-consistent spatially-variant material parameters and normals in addition to RGB, conditioned on single image and camera path. In addition to relighting and material editing subsequent optimization stage enables high-quality 3D asset generation for physically based rendering (PBR). Visit the project page at https://svim3d.aengelhardt.com."
        },
        {
            "title": "Abstract",
            "content": "We present Stable Video Materials 3D (SViM3D), framework to predict multi-view consistent physically based rendering (PBR) materials, given single image. Recently, video diffusion models have been successfully used to reconstruct 3D objects from single image efficiently. However, reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits. We extend latent video diffusion model to output spatially varying PBR parameters and surface normals jointly with each generated view based on explicit camera control. This unique setup allows for relighting and generating 3D asset using our model as neural prior. We introduce various mechanisms to this pipeline that improve quality in this ill-posed setting. We show state-ofthe-art relighting and novel view synthesis performance on multiple object-centric datasets. Our method generalizes to diverse inputs, enabling the generation of relightable 3D assets useful in AR/VR, movies, games and other visual media. Work done during internship at Stability AI. 1. Introduction 3D asset generation and relighting are important tasks for various use cases in movies, gaming, e-commerce, and AR/VR. In nearly all cases, objects are placed in new environments and lighting conditions. This means illumination information needs to be disentangled from an objects shape and material robustly for it to integrate seamlessly into new scene. Think of the subtle, yet essential differences between glossy metallic and matte finish. For generative 3D models without precise material prediction, relighting becomes nearly impossible, resulting in assets that feel out of place. Estimating these properties from single image under natural illumination, also known as inverse rendering [11, 55, 76, 84, 112], is highly ill-posed and still unsolved problem. Multi-view Material Generation. In this work, we present Stable Video Materials 3D (SViM3D), probabilistic generative diffusion model that tackles object-centric inverse rendering from single image. Conditioned on camera pose sequence it generates both high-quality appearances 1 GT GT GT GT w/o w/o w/o w/o SViM3D (Ours) SViM3D (Ours) SViM3D (Ours) SViM3D (Ours) n s a s e e V e V a r o m o m o t o i o c c h n m e r v o g g t h o i h i E o c e r n i n i g k e e M GT V k y n r s a g m o e c i r n c o p o g h t n v n g t n r E n g L m i g h GT w/o SViM3D (Ours) to ensure that material details remain coherent. w/o w/o SViM3D (Ours) SViM3D (Ours) GT i i M a m o e c m i E i i Figure 2. SViM3D Improvements on Common Issues. Our method introduces several new contributions which improve the reconstruction quality of our method drastically. as well as the corresponding multi-view consistent material properties. Unlike prior approaches that decouple material estimation from 3D reconstruction, SViM3D is the first camera-controllable multi-view model that can produce fully spatially-varying PBR parameters, RGB color and surface normals simultaneously. The additional output can be leveraged in various applications, hence we consider SViM3D foundational model that provides unified neural prior for both 3D reconstruction and material understanding. SViM3Ds output can be used to relight the views directly, perform material edits or to generate full 3D assets by lifting the multi-view material parameters to 3D. As 3D training data paired with material parameters is scarce, we leverage the accumulated world knowledge of latent video diffusion model [8]. Specifically, we adapt SV3D [99], video diffusion model [8] fine-tuned for camera-control by incorporating several crucial modifications: Multi-illumination multi-view training dataset: We render high-quality photorealistic synthetic dataset, capturing the complexity of real-world lighting and material variations. Material latent representation: We treat the material parameters and surface normals as images reusing the image-based autoencoder to encode all inputs into unified latents. Adapted UNet architecture: We make crucial changes in the core architecture and training scheme to smoothly adapt from image to image+material+normal generation. We use the multi-view PBR video output of SViM3D as pseudo-ground truth for 3D reconstruction. To achieve highquality 3D reconstructions, we introduce several innovations in our 3D optimization: View-dependent masking: Loss contributions of the generated views are weighted based on perspective distortion Homography correction: learnable homography correction mitigates residual multi-view inconsistencies, enhancing reconstruction fidelity. Fast differentiable environment-based lighting: Our novel differentiable rendering module leverages precomputed multi-level illumination pyramids to achieve both faster and more accurate lighting optimization. Fig. 1 highlights examples of relighting and 3D assets in novel environments. We extensively evaluate SViM3D on novel view synthesis (NVS), material prediction, relighting and 3D generation. Our method not only achieves stateof-the-art multi-view consistency but also significantly improves material reproduction in real-world settings as the approach inherently understands and exploits multi-view appearance consistency. Please find more examples of our results and more at https://svim3d.aengelhardt.com. 2. Related works Inverse rendering is challenging and ambiguous problem, traditionally performed in controlled laboratory settings [4, 10, 6264, 104]. Building on insights from constrained estimation, various methods propose casual acquisition setups for planar surfaces, using single shot [1, 9, 26, 40, 65, 84], few-shot [1, 106] or multi-shot [2, 10, 27, 28, 35] captures. Casual capture has also been extended to joint lighting model and shape reconstruction [57, 11, 55, 76, 84, 112], even on scenes [66, 86]. Recovering lighting under unknown passive illumination is significantly more challenging as it requires disentangling shape and materials from the illumination. Implicit representations. Methods based on neural fields achieved decomposition of scenes under varying illumination [12, 13] or fixed illumination [67, 113, 114, 120, 121], even with uncertain or unknown camera parameters [14, 32]. Also, 3D Gaussians have been explored as scene representation in this context [36, 83]. However, all these methods rely on multi-view input and need to be optimized per object. 3D reconstruction with material prediction. BRDF parameter autoencoders [13, 105] or lighting constraints [13, 37, 38] have shown to help with inverse rendering. Recently, diffusion models have gained traction for their probabilistic handling of ambiguity in casual capture scenarios. Du et al. [31] explore intrinsic imaging with diffusion models, leveraging LoRA [46] and small datasets, showing Stable Diffusions [82] potential, despite quality limits. Material Palette [71] and ControlMat [96] generate tileable textures, while Xu et al. [105] incorporate SDS loss [79] and Deep Marching Tetrahedra (DMTet) [75, 87] for reconstruction. Intrinsic Image Diffusion (IID) [59] is one of the first works to explore diffusion models for PBR material estimation, it fine-tunes Stable Diffusion on an interior dataset [122] for PBR parameter prediction and relighting. RGBX [111] estimates PBR data as part of their materialand lightingaware neural rendering pipeline, their model can predict either albedo, roughness, metallic or diffuse irradiance maps conditioned on single image and text prompt. MaterialFusion [68] proposes 2D material denoising diffusion prior called StableMaterial based on StableDiffusion 2.1 [82] but trained on object centric data, and employs an SDS-based optimization to achieve 3D asset generation. Gaussian-ID [30] proposes 3D reconstruction with diffusion-based material priors, using multi-view data and 3D Gaussian Splatting [57] with parametric lighting, building on Kocsis et al. [59]. In contrast to all these methods, SViM3D jointly estimates all material parameters in multi-view consistent manner, making it robust foundation model for 3D reconstruction. 3D generation with materials. In contrast to texture generation given 3D geometry [25, 93, 108, 110] we focus on joint 3D and material generation. Recent techniques in 3D generation often guide the optimization of DMTet [75] with reflectance field. special case of these 3D generations is to create an asset from single image. These approaches [15, 45, 48, 89, 99, 116] benefit from largescale pretraining on image data, often followed by supervised fine-tuning on synthetic data. First steps in diffusion based pair-wise view generation with camera control have been achieved by zero-123 [69] and its follow-ups. Recently, video data has also been utilized in the context of video diffusion models [20, 99]. Guidance from pretrained image/video diffusion model is leveraged using either (1) Score Distillation Sampling (SDS) [79] loss, or (2) photometric reconstruction loss. Fantasia3D [19] uses Stable Diffusion [82], UniDream [70] predicts normals and albedo with multi-view diffusion model. RichDreamer [80] employs two models for albedo and normal-depth generation. However, SDS optimization has several drawbacks, including multi-view inconsistency, long runtimes, and issues with oversaturated colors and blurry details. AssetGen [89] replaces SDS-based optimization with photometric loss, using multi-view diffusion model for albedo and radiance prediction. transformer converts views into triplane representation [45], enabling 3D reconstruction with UV texture refinement. CLAY [116] generates materials conditioned on geometry previously generated by different module using an adapted MVDream [88] model without explicit camera control. However, PBR parameters that enable relighting still require multi-view data input with potentially changing illumination [13, 36], additional workflow steps or text input [19, 72, 89, 116], or are simply not available in current methods [20, 47, 99]. Recent work has shown that direct image-to-image relighting without explicit 3D reconstruction is feasible using diffusion-based models conditioned on illumination [50, 109, 117]. In contrast, SViM3Dpredicts RGB, PBR parameters, and normals simultaneously, enabling explicit 3D shape and illumination reconstruction using single model and achieving high 3D consistency. 3 3. Preliminaries Video diffusion based 3D generation. Recently, video diffusion models have been exploited for novel view synthesis and 3D reconstruction tasks [20, 99], due to improved view consistency and generalization from being trained on huge image and video datasets [85, 103]. 3D video diffusion model is conditioned on reference image R3 of an object, and camera orbit of sequence of poses around it. It is then trained to generate K-frame orbital video RK around the object using the diffusion formulation [8, 43, 91, 97, 98]. During inference, the user provides an image and camera trajectory, and the 3D video diffusion model is used to iteratively generate an orbital video in multiple diffusion steps. These views can then be used in multi-view 3D reconstruction pipeline [57, 73, 75, 98]. Physically-based rendering (PBR). From rendering perspective, an object is rendered as an image by computing its radiance at each location, which is then denoted by its RGB pixel value R3. Radiance is computed by appropriately factoring in the contributions of the objects PBR material properties := [bc; br; bm] consisting of albedo basecolor bc R3, roughness br R, and metallic-ness bm R; as well as its surface normal R3. Specifically, the outgoing radiance = L(ωo) in direction ωo is defined by simplified rendering equation [52] as: (cid:90) L(ωo) = Li(ωi)f (ωi, ωo)(ωi n)dωi, (1) Ω i.e. the integral over the hemisphere Ω of the incoming light Li(ωi) from direction ωi multiplied with the Bidirectional Reflectance Distribution Function (BRDF) (ωi, ωo) and the cosine shading term (ωi n). We model the specular portion of the BRDF with the analytical Cook-Torrance microfacet model [22], yielding: + (2) bc π (1 bm) (ωi, ωo) = DF 4(ωo n)(ωi n) where D, , represent the normal distribution function (NDF), Fresnel term, and the geometric attenuation function, respectively. For we rely on the GGX distribution [100]. We adopt the parametrization of the Disney BRDF [16], where bc and bm characterize , and br characterizes and [78, 95]. In SViM3D, we adapt 3D video diffusion model framework [98], and jointly generate images c, PBR materials b, and normal for each target view. 4. SViM3D: Multi-view PBR Generation Overview. The aim of SViM3D is to convert single 2D image and camera orbit into RGB frames, corresponding material parameters, and normal maps. Fig. 3 lays out the key components of our method. Problem setup. The inputs to the model are: color image R3 of an object, and Figure 3. The SViM3D pipeline. We train video diffusion model on multi-view and multi-illumination data to generate multi-view images with material parameters. During inference, given single image, SViM3D can generate 21 views with consistent RGB radiance, albedo, roughness, metallic, and camera space normals. We then use the synthesized novel views for 3D reconstruction that yields textured meshes with PBR materials. Starting from illumination pre-optimization, we further propose several techniques to aid the 3D reconstruction pipeline in this sparse view setting, such as visibility masking, homography correction, fast differentiable rendering. camera pose trajectory defined by tuples of elevation and i=1 centered at 2 = {(ei, ai)}K azimuthal angles π RK the object, with = 21 views. The goal is to estimate an augmented orbital video RK , i.e. K-frame video of 11-channel frames: 3 channels for the image RGB color R3, 5 channels for PBR parameters from the Cook-Torrance model [22] R5, namely basecolor bc R3, roughness br R, metallic bm R, and 3 channels for the unit-length surface normal in camera space R3. SViM3D is trained to iteratively generate through denoising diffusion process, similar to 3D video diffusion models but with the above augmented channels. 4.1. SViM3D Architecture and Training The architecture of SViM3D is built on that of SV3D [99], which in turn is built on that of SVD [8]. However, we introduce important elements to the architecture in order to adapt to generating material parameters and normals. Material latent representation. While it is prudent to re-use pretrained 3D video diffusion model to leverage the rich image and video priors it has learned, such models operate in the latent space, using variational autoencoder (VAE) to first encode the images into latents, perform denoising step, then finally decode latents into images. Material properties or normals do not have standard latent representation, though. Inspired by other diffusion model works [56, 115] that take additional conditioning, our main insight is that the VAE of an image generative model can encode material properties and surface normals, by treating them as images. The VAE of SV3D takes 3-channel image input, and outputs 4-channel latent at 1/8th the original image sidelength. For training, we use this VAE to encode the RGB 4 image c; albedo basecolor bc; concatenation of roughness and metallic-ness padded with zeros to make 3 channels [0; br; bm] to align with the Occlusion-Roughness-Metallic (ORM) channel layout often used in real-time graphics [58]; and the surface normal n, each into 4-channel latents. Therefore, the network predicts 16-channel stack. We preprocess all latents, and feed the concatenated tensors to the UNet after adding the time step-specific noise. UNet adaptation. Each denoising step is performed by UNet with multiple layers at different scales. Each layer consists of one residual block with Conv3D layers, and two transformer blocks (spatial and temporal) with attention layers as illustrated in Fig 3. While SV3D captures only the latents of the RGB frames of the orbital video, we augment the input and output to include PBR material and surface normal by extending the channel dimension of the input and output layers from 4 to 16. The newly extended weights are initialized by repeatedly copying the existing parameters from the weights for the RGB latent channels. The rest of the architecture follows that of SV3D [98]. Multi-illumination multi-view training dataset. We combine multiple data sources and render synthetic photorealistic dataset using Blenders Cycles [21] render engine. We exclude data that includes subsets of the Poly Haven data which we use for testing. Per object, we randomly select four environment maps. For each illumination setting, we sample random camera trajectory with fixed distance between camera and object that also ensures that the convex hull of the scene content is inside the camera frustum for all views. Training details. We use the popular EDM framework [54] for training with the simplified diffusion loss from [8]. While the pre-trained VAE is reused, we train the denoising UNet in two phases. First, we freeze all temporal attention blocks (see Fig. 3) while training on all data for roughly 100k steps. Examples featuring low quality material parameters like missing texture maps or uniform parameters we only use for RGB and normal supervision. Afterwards, we finetune the whole UNet on the highest quality PBR data for another 60k steps. This staged training helps avoid forgetting of the temporal knowledge in the initial model, and stabilizes the training for task adaptation compared to full fine-tuning from the start. For inference, we follow triangular Classifier-free Guidance (CFG) [42] scaling similar to that of SV3D [99]. 4.2. Relighting Adjusting an objects appearance to fit into new environment is critical for seamless integration into AR or believable compositions in media production. SViM3D generates material parameters that can be directly used for (re-)lighting in image space without an explicit surface reconstruction (2.5D). Given an illumination representation like an HDRI and virtual camera we can use the generated normal direction to define the ray geometry to evaluate the predicted BRDF. Using split sum illumination model (Eq. 3) we can compute the lighting at interactive rates (see Fig. 5). Fast environment-based lighting. We propose fast environment map-based rendering, which can encode significantly more lighting details than the low-frequency illumination models used in SV3D [99], for example. Our imagebased lighting [53] which leverages pre-filtered importance sampling [61] for fast integration of the incoming light supports both 2.5D relighting in image space as well as 3D reconstruction and rendering. The third row of Fig. 2 shows that our scheme delivers better image-based lighting. Specifically, as Monte Carlo integration is costly and can lead to high noise levels, we adopt the split sum approximation [53] from real-time rendering. This technique has proven effective in prior work [13, 75], and approximates the integral of Eq. 1 as: (cid:90) L(ωo) = (ωi, ωo)(ωin)dωi (cid:90) Li(ωi)D(ωi, ωo)(ωin)dωi Ω Ω (3) The first integral depends only on BRDF roughness and the cosine term, and is pre-computed into 2D lookup texture. The second term, involving incoming radiance and the NDF D, is pre-integrated into filtered environment map at multiple fixed roughness levels. Since rougher materials need lower resolution, we store the result in an image pyramid, or environment pyramid. Rendering then becomes multiplication of two lookups based on (r, (ωi n)) and pyramid level selected by and direction ωi. To account for multiple scattering, we use attenuated cosine-weighted radiance from lower mip level, following [34]. As precomputing the filtered environment map is expensive, we introduce several optimizations. Unlike [75], we use Monte Carlo integration. For increasing roughness val5 ues from [0 1] we use 0, 4, 16, 24, and 24 samples over 5 mip levels during optimization, and 8 levels with up to 256 samples for relighting. The first level (mirror direction) is excluded. To reduce noise, we apply filtered importance sampling [61], where environment resolution is adapted to sample likelihood. For diffuse lighting, we filter low-res environment image (no NDF) using 16 samples. We reduce the perceptible noise by drawing random samples from the Halton sequence. While our method supports arbitrary environment map formats, we find octahedral maps [33] to work well as they yield fewer pole artifacts than spherical ones. Rendering is performed in linear HDR color space and tonemapped using AgX [90]. 4.3. 3D Reconstruction using SViM3D Outputs We use the outputs of SViM3D as pseudo-ground-truth (pGT) for 3D reconstruction. Our pipeline is agnostic to the 3D representation, we use NeRF-based implicit function. For illumination we use our environment lighting representation introduced above. It is implemented in pure PyTorch and, therefore, fully differentiable while still fast enough to execute the pre-filtering in each training step. As visualized in Fig. 3, our reconstruction pipeline comprises four phases: 0. An illumination representation is pre-optimized using the orbital video to initialize Phase 1. 1. modified Instant-NGP [74] is optimized using photometric rendering loss relying on the jointly optimized illumination and supervision from the reference views. 2. We optimize DMTet [75] representation initialized from the results of Phase 1 via marching cubes. 3. mesh is finally extracted, UV unwrapped using xatlas [107] and all textures baked. The generated asset can be used in any computer graphics pipeline, e.g. integrated into new scenes and lighting. Optimization. For optimal results, we run Phase 0 for 200 steps, followed by 800 steps of Phase 1, and 1500 steps of Phase 2. 3D reconstruction quality is dependent on the quality of the 3D representation and the illumination. However, lighting effects are highly view-dependent, and cannot be modeled with low-frequency illumination models, leading to degraded reconstruction performance as shown in the last row of Fig. 2. We weigh the normal supervision losses high as our pGT are of high quality. For quick previews, Phase 0 and potentially also Phase 2 and Phase 3 can be omitted, giving 3D representation suitable for novel view synthesis and basic relighting. In addition to the reconstruction loss, we employ two LPIPS instances on randomly sampled triplets of the rendered output ˆI and the pGT inspired by [18]. Further details about the losses and optimization process are available in the appendix. We observe that even slight multi-view inconsistency in the SViM3D pGT outputs may lead to blurry results in the 3D asset, as shown in Fig. 2. Therefore, we introduce viewTable 1. Single frame material prediction. Given single RGB image the corresponding material parameters basecolor, roughness and metallic are generated and compared to GT from our Poly Haven test set. For SViM3D we only evaluate the output for the condition frame. Results are averaged over 3 samples. Basecolor / Albedo Roughness-Metallic Method PSNR SSIM LPIPS FID PSNR RMSE 8.62 IID [59] 20.59 SM [68] RGBX [111] 16.01 SViM3D (ours) 28.68 120.17 11.80 19.1 31.21 17.4 57.9 25.36 18. 0.49 0.072 0.12 0.037 0.66 0.86 0.83 0.92 0.33 0.16 0.16 0.09 Normal PSNR RMSE 12.48 - 6.96 27.57 0.28 - 0.45 0.05 dependent masking and homography correction to compensate for minute inconsistencies in the input pGT, and improve the detail drastically. To further add consistency between the rendered result and our PBR prediction, we perform fast differentiable environment-based lighting to enable more high-frequency lighting details compared to parametric illumination models [12, 99], which leads to textures with even lesser light baked in. View-dependent masking. We observe that artifacts are prominent in parts of the generated views where the perspective distortion is heaviest. Therefore, regions in an image with the least distortion should contribute the most. The first row of Fig. 2 shows that our scheme produces more detailed texture information. good proxy to identify the trusted areas that are parallel to the image plane is the dot product ˆn vi of the bilaterally filtered surface normal ˆn and view direction vi from surface points R3 to camera position πi. Higher values, i.e. higher correlation between vi and ˆn indicate better alignment and consequently higher trust. Ai for view is then used to mask all geometry related losses. Homography correction. To address remaining inconsistencies between the pseudo-ground-truth (pGT) views, we introduce learnable per-view homography correction. The second row of Fig. 2 shows that our scheme corrects view inconsistencies. Specifically, for each pGT view, we jointly optimize homography matrix Hi, initialized as the identity transform, for the latter part of Phase 1. During optimization, Hi is applied to the rendered view ˆIi during loss computation as ˆIi = HiˆIi, enabling the model to match the view to the potentially imperfect pGT. 5. Experiments and Results Datasets. To evaluate our new task set, we introduce the Poly Haven object dataset [39] consisting of 315 high-quality assets with PBR materials. We render synthetic scenes similar to our training dataset, with 21 frames per orbit and multiple illumination settings per object. We also evaluate the relighting and 3D reconstruction performance on the Stanford Orb [60] benchmark, which contains real-world multi-view scenes with scanned meshes and ground truth environment maps. The supplements contain results from other datasets. 6 Figure 4. Multi-view consistency. We compare the generated materials from different neural diffusion priors in multi-view setting. SV3D [99] shows multi-view consistent RGB output similar to SViM3D that also generates multi-view consistent Basecolor. Generating albedo maps on top of the SV3D views using RGBX [111], StableMaterial (SM) of MaterialFusion [68] or Intrinsic Image Diffusion (IID) [59] yields inconsistent results compared to the GT. Metrics. The generated RGB radiance and, as an indicator of decomposition quality, the albedo basecolor are evaluated using PSNR, SSIM, and the distribution matching metrics LPIPS [119], CLIP-Score (CLIP-S) [41], the CLIP Maximum-Mean Discrepancy (CMMD) [49], and FID [17]. CMMD compares the distribution of the CLIP [81] embeddings of generated and reference images, and has been shown to be better indicator of low-level image quality than FID [49, 94]. As these image metrics have limited meaning for the material maps, we evaluate them via PSNR and root mean squared error (RMSE). We average over three samples and match the scale and shift for albedo and roughness predictions to the ground truth (GT) to compensate for the inherent ambiguity. See the appendix for visual comparison across multiple samples. 3D optimization is evaluated in the appendix, too. Baselines. We compare SViM3D with IID [59], StableMaterial (SM) of MaterialFusion [68], and RGBX [111]. Although not capable of PBR material estimation, we also compare against SV3D [99] which generates multi-view RGB images from single view, and also allows for 3D reconstruction similarly to our method. Since none of the available methods exactly matches our task, i.e. joint multi-view and material prediction from single image with camera control, we compare the models on multiple tasks to evaluate efficacy. Single image material prediction. Given the lack of closely related baselines, we also compare existing techniques on single-image material estimation. Tab. 1 shows the perforTable 3. 3D reconstruction ablation. We ablate different key aspects of our pipeline using subset of the Poly Haven test set. Table 4. Stanford Orb. Novel view synthesis (NVS) and relighting evaluated on Stanford Orb [60]. Configuration PSNR SSIM Method w/o homography correction w/o environment lighting w/o view masking SViM3D full 13.7 15.0 17.8 22.4 0.76 0.81 0.84 0.90 NVS PSNR SSIM Relighting PSNR SSIM SF3D [15] SViM3D 16.81 19.34 0.74 0. 20.1 21.86 0.88 0.90 NVS is evaluated in the top part of Tab. 2 using the RGB radiance output. The lower part of the table compares multi-view albedo generation, which we obtain for the other methods by first running multi-view NVS for RGB images using SV3D [98], and then the respective material generation conditioned on each multi-view image. We also replicate the GT RGB input frames using the GT illumination and our generated PBR materials with 2.5D relighting (also see Fig. 5). Fig. 7 shows examples for multiple view and light directions, comparing our results to the ground truth and multiple diffusion based relighting baselines. Please find more information on the baseline models and visual examples in the supplements. We use the default inference configuration for all other methods, and we use 50 steps of the deterministic DDIM sampler [92] with the guidance scheme described in Sec. 4 for ours. From Tab. 2, we see that SViM3D achieves state-of-the-art performance on almost all metrics. Interestingly, the multi-view RGB prediction improved compared to the baseline, indicating that, although potentially more challenging task, PBR generation also helps RGB generation. 3D Relighting and Novel View Synthesis We compare rendered 3D reconstruction results using the provided test views with original and novel illuminations from the Stanford Orb benchmark [60] in Tab. 4. The metrics show that the reconstructed material parameters are well suited for physically based rendering in diverse illumination settings and allow better reproduction of the original illumination compared to SF3D [15]. Please find more info on SF3D in the appendix. Visual results. Fig. 4 visualizes the superior 3D consistency of our multi-view generation compared to all other variants. This underlines the benefit of combined neural prior for video and 3D based applications. The results in Tab. 1 and Tab. 2 support these observations. We show further results in the appendix. Fig. 5 shows more examples of SViM3Ds output. The 21 frames are represented by 5 novel views sampled from an orbit around the object. The model is capable of generating 3D consistent surface representation as evident in the normal maps and preservation of fine details, e.g. of the wheelchair. Illumination is successfully disentangled from the basecolor and roughness and metallic maps contain the spatial variance expected from the RGB views or ground truth given in Fig. 6. The relighting results indicate that physically plausible material prediction is achieved which, Figure 5. Multi-view PBR materials. Given the input image SViM3D generates multi-view consistent novel views with corresponding basecolor, roughness, metallic and normal maps. These can directly be used to generate views under novel illumination. We show 5 samples from generated orbit and two new illumination settings as examples. The objects are sourced from our Poly Haven [39] test dataset. Please find additional results in the supplementary material. Table 2. Multi-view NVS with material parameters. Given single RGB image multi-view orbit around the scene is generated with corresponding PBR materials and normals. We compare RGB NVS and albedo generation as stand-in for PBR materials against rendered GT on our Poly Haven test set. Additionally, GT illumination is used to reproduce the RGB radiance from the predicted materials and normals for SViM3D . Results are averaged over 3 samples for all 21 frames. Method RGB radiance 21 frames SV3D [99] SViM3D (ours) SViM3D (ours) 2.5D relit Basecolor / Albedo 21 frames SV3D + IID [59] SV3D + RGBX [111] SV3D + SM [68] SViM3D (ours) PSNR SSIM LPIPS FID CLIPS CMMD 7.8 0.097 0.089 6.93 0.089 15.15 28.41 22.13 17.22 9.42 15.62 15.15 18.12 18.27 1.81 1.05 0.96 1.16 0.76 0.83 0.83 0.85 0.18 0.11 0.10 0. 0.81 0.80 0.81 0.82 18.41 19.57 19.99 0.84 0.85 0.83 0.83 0.85 0.87 1.06 1.12 0.08 mance for this subtask (on the reference frame) of our task as the condition view is part of the camera trajectory. Results clearly show that we comfortably outperform the singleimage material prediction baselines. Multi-view novel view image and albedo synthesis (NVS). 7 Figure 6. Single image PBR materials. We compare the generated materials from different neural diffusion priors for single image from the Poly Haven [39] test set. Besides the GT rendering and SViM3D (ours) results from RGBX [111], StableMaterial (SM) of MaterialFusion [68] and Intrinsic Image Diffusion (IID) [59] are presented. Note that IID uses monocular normals that are separately generated and SM does not provide any normals. tions. Furthermore, our PBR representation cannot represent more complex materials such as transparent objects. Enhancing the material and illumination complexity in the diffusion denoising process pose interesting future work. 6. Conclusion We present SViM3D, the first foundational multi-view material model. Given conditioning image and user-defined camera path, SViM3D jointly predicts multi-view consistent RGB colors, spatially varying PBR material parameters and surface normals. We demonstrate the quality and consistency of SViM3Ds outputs by employing them as pseudo ground truth in 3D reconstruction pipeline, showing that it enables high-quality 3D reconstructions in this ill-posed setting. We adapt video diffusion model, introducing key modifications to the network architecture and training data to enable simultaneous material prediction. We also introduce several innovations in 3D reconstruction to correct multi-view inconsistencies, and add fast differentiable environment-based lighting. Our extensive experiments demonstrate the stateof-the-art performance of SViM3D in several tasks related to novel view and material synthesis. We hope SViM3D can serve as foundational model for future research on multiview consistent material generation."
        },
        {
            "title": "Acknowledgements",
            "content": "This work has been partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy EXC number 2064/1 Project number 390727645 and SFB 1233, TP 02 - Project number 276693517. Figure 7. Relighting comparison. We compare image-based relighting of recent diffusion-based methods IC-Light [117], NeuralGaffer [51] and DiLightNet [109] against SViM3D and the synthetic ground truth (GT) on examples from Poly Haven [39] data. given the correct illumination, can reproduce the ground truth. Compared to the other methods, the roughness and metallic from Stable Material (SM) [68] are smoother than the ones from RGBX, but our results are overall closer to the ground truths. Most other methods use monocular prior for the normal generation or are trained with annotations from pre-trained model [59, 111]. Runtime. SViM3D generates 21 views at 576576 in 20s. While other methods may be faster per frame, they require minutes to generate full sequences. Our 3D reconstruction takes 15 mins., with the 3 min overhead vs. SV3D due to the added PBR optimization. Ablation study. We ablate different aspects of SViM3D in terms of reconstruction quality and present quantitative results computed on subset of the Poly Haven [39] dataset in Tab. 3 in addition to the visual examples in Fig. 2. Every ablated component contributes significantly to the final reconstruction quality. Limitations. Currently, our model focuses on object centric images, limiting its applicability to general video applica-"
        },
        {
            "title": "References",
            "content": "[1] Miika Aittala, Timo Aila, and Jaakko Lehtinen. Reflectance modeling by neural texture synthesis. ACM TOG, 2018. 2 [2] Rachel Albert, Dorian Yao Chan, Dan B. Goldman, and James F. OBrian. Approximate svBRDF estimation from mobile phone video. Eurographics Symposium on Rendering, 2018. 2 [3] Mohammad Asim, Christopher Wewer, Thomas Wimmer, Bernt Schiele, and Jan Eric Lenssen. Met3r: Measuring multi-view consistency in generated images, 2024. 16 [4] Louis-Philippe Asselin, Denis Laurendeau, and JeanFrancois Lalonde. Deep SVBRDF estimation on real materials. threedv, 2020. 2 [5] Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Miloˇs Haˇsan, Yannick Hold-Geoffroy, David Kriegman, and Ravi Ramamoorthi. Neural reflectance fields for appearance acquisition. arXiv, 2020. 2 [6] Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Miloˇs Haˇsan, Yannick Hold-Geoffroy, David Kriegman, and Ravi Ramamoorthi. Deep reflectance volumes: Relightable reconstructions from multi-view photometric images. ECCV, 2020. [7] Sai Bi, Zexiang Xu, Kalyan Sunkavalli, David Kriegman, and Ravi Ramamoorthi. Deep 3d capture: Geometry and reflectance from sparse multi-view images. CVPR, 2020. 2 [8] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets, 2023. 2, 3, 4, 14 [9] Mark Boss and Hendrik P.A. Lensch. Single image brdf parameter estimation with conditional adversarial network. arXiv, 2019. 2 [10] Mark Boss, Fabian Groh, Sebastian Herholz, and Hendrik P. A. Lensch. Deep Dual Loss BRDF Parameter Estimation. Workshop on Material Appearance Modeling, 2018. 2 [11] Mark Boss, Varun Jampani, Kihwan Kim, Hendrik P.A. Lensch, and Jan Kautz. Two-shot spatially-varying BRDF and shape estimation. CVPR, 2020. 1, [12] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, and Hendrik P.A. Lensch. NeRD: Neural reflectance decomposition from image collections. ICCV, 2021. 2, 6 [13] Mark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan T. Barron, and Hendrik P.A. Lensch. Neural-pil: Neural pre-integrated lighting for reflectance decomposition. NeurIPS, 2021. 2, 3, 5 [14] Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen Li, Deqing Sun, Jonathan T. Barron, Hendrik P.A. Lensch, and Varun Jampani. SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections. NeurIPS, 2022. 2 [15] Mark Boss, Zixuan Huang, Aaryaman Vasishta, and Varun Jampani. SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and Illumination Disentanglement. arXiv preprint, 2024. 3, 7, 15, 16, 17 [16] Brent Burley. Physically-based shading at disney. ACM Transactions on Graphics (SIGGRAPH), 2012. 9 [17] Naresh Babu Bynagari. Gans trained by two time-scale update rule converge to local nash equilibrium. Asian Journal of Applied Science and Engineering, 2019. 6 [18] Thomas Chambon, Eric Heitz, and Laurent Belcour. Passing Multi-Channel Material Textures to 3-Channel Loss, 2021. 5 [19] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3D: Disentangling Geometry and Appearance for Highquality Text-to-3D Content Creation, 2023. 3 [20] Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, and Huaping Liu. V3D: Video Diffusion Models are Effective 3D Generators, 2024. 3 [21] Blender Online Community. Blender - 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. 4 [22] Robert L. Cook and Kenneth E. Torrance. reflectance model for computer graphics. ACM TOG, 1982. 3, 4 [23] Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning, 2023. 14 [24] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. 14 [25] Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Ramanan, Jun-Yan Zhu, Tinghui Zhou, and Maneesh Agrawala. Flashtex: Fast relightable mesh texturing with lightcontrolnet. In European Conference on Computer Vision (ECCV), 2024. 3 [26] Valentin Deschaintre, Miika Aitalla, Fredo Durand, George Drettakis, and Adrien Bousseau. Single-image SVBRDF capture with rendering-aware deep network. ACM TOG, 2018. 2 [27] Valentin Deschaintre, Miika Aitalla, Fredo Durand, George Drettakis, and Adrien Bousseau. Flexible SVBRDF capture with multi-image deep network. Eurographics Symposium on Rendering, 2019. 2 [28] Valentin Deschaintre, George Drettakis, and Adrien Bousseau. Guided fine-tuning for large-scale material transfer. Eurographics Symposium on Rendering, 2020. 2 [29] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B. McHugh, and Vincent Vanhoucke. Google scanned objects: highquality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pages 25532560, 2022. 17, [30] Kang Du, Zhihao Liang, and Zeyu Wang. GS-ID: Illumination Decomposition on Gaussian Splatting via Diffusion Prior and Parametric Light Source Optimization, 2024. 3, 15 [31] Xiaodan Du, Nicholas Kolkin, Greg Shakhnarovich, and Anand Bhattad. Generative Models: What do they know? Do they know things? Lets find out!, 2023. 2 [32] Andreas Engelhardt, Amit Raj, Mark Boss, Yunzhi Zhang, Abhishek Kar, Yuanzhen Li, Deqing Sun, Jonathan T. Barron, Hendrik P.A. Lensch, and Varun Jampani. SHINOBI: Shape and Illumination using Neural Object decomposition via Brdf optimization In-the-wild. CVPR, 2024. 2 [33] Thomas Engelhardt and Carsten Dachsbacher. Octahedron environment maps. In International Symposium on Vision, Modeling, and Visualization, 2008. 5 [34] Carmelo Fernandez-Aguera. multiple-scattering microfacet model for real-time image based lighting. Computer Graphics Techniques, 8, 2019. 5 [35] Duan Gao, Xiao Li, Yue Dong, Pieter Peers, and Xin Tong. Deep inverse rendering for high-resolution SVBRDF estimation from an arbitrary number of images. ACM Transactions on Graphics (SIGGRAPH), 2019. [36] Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, and Yao Yao. Relightable 3d gaussian: Real-time point cloud relighting with brdf decomposition and ray tracing. arXiv preprint arXiv:2311.16043, 2023. 2, 3 [37] James Gardner, Bernhard Egger, and William Smith. Rotation-equivariant conditional spherical neural fields for learning natural illumination prior. In Advances in Neural Information Processing Systems, 2022. 2 [38] James A. D. Gardner, Bernhard Egger, and William A. P. Smith. Reni++ rotation-equivariant, scale-invariant, natural illumination prior, 2023. 2 [39] Poly Haven. Poly Haven Poly Haven polyhaven.com. https://polyhaven.com/, 2024. [Accessed 22-082024]. 6, 7, 8, 15, 16, 17, 18, 20 [40] Philipp Henzler, Valentin Deschaintre, Niloy Mitra, and Tobias Ritschel. Generative modelling of BRDF textures from flash images. ACM Transactions on Graphics (SIGGRAPH ASIA), 2021. 2 [41] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: Reference-free Evaluation Metric for Image Captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. [42] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 5, 14 [43] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [44] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large reconstruction model for single image to 3D. arXiv preprint arXiv:2311.04400, 2023. 15 [45] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large Reconstruction Model for Single Image to 3D, 2023. 3 [46] Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 2 [47] Zixuan Huang, Stefan Stojanov, Anh Thai, Varun Jampani, and James M. Rehg. ZeroShape: Regression-based Zeroshot Shape Reconstruction, 2024. [48] Zixuan Huang, Mark Boss, Aaryaman Vasishta, James Rehg, and Varun Jampani. SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images, 2025. 3 [49] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards better evaluation metric for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 93079315, 2024. 6 [50] Haian Jin, Yuan Li, Fujun Luan, Yuanbo Xiangli, Sai Bi, Kai Zhang, Zexiang Xu, Jin Sun, and Noah Snavely. Neural gaffer: Relighting any object via diffusion. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 3, 16 [51] Haian Jin, Yuan Li, Fujun Luan, Yuanbo Xiangli, Sai Bi, Kai Zhang, Zexiang Xu, Jin Sun, and Noah Snavely. Neural Gaffer: Relighting Any Object via Diffusion, 2024. arXiv:2406.07520 [cs]. 8, 18, 20 [52] James T. Kajiya. The rendering equation. ACM Transactions on Graphics (SIGGRAPH), 1986. 3 [53] Brian Karis. Real shading in unreal engine 4. Technical report, Epic Games, 2013. 5 [54] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative In Advances in Neural Information Processing models. Systems, 2022. 4 [55] Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Ferrari, and Luc Van Gool. Uncalibrated neural inverse rendering for photometric stereo of general surfaces. ICCV, 2021. 1, 2 [56] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 4 [57] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM TOG, 42(4), 2023. 3 [58] khronos. glTF-2.0 Documentation, 2024. 4 [59] Peter Kocsis, Vincent Sitzmann, and Matthias Niessner. Intrinsic image diffusion for indoor single-view material estimation. CVPR, 2024. 2, 3, 6, 7, 8, 15, 16, 21 [60] Zhengfei Kuang, Yunzhi Zhang, Hong-Xing Yu, Samir Agarwala, Shangzhe Wu, and Jiajun Wu. Stanford-orb: realworld 3d object inverse rendering benchmark, 2023. 6, [61] Jaroslav Kˇrivanek and Mark Colbert. Real-time shading with filtered importance sampling. Eurographics Symposium on Rendering, 2008. 5 [62] Jason Lawrence, Szymon Rusinkiewicz, and Ravi Ramamoorthi. Efficient BRDF importance sampling using factored representation. ACM TOG, 2004. 2 [63] Hendrik P.A. Lensch, Jochen Lang, M. Sa Asla, and HansPeter Seidel. Planned sampling of spatially varying BRDFs. Comput. Graph. Forum, 2003. [64] Hendrik P. A. Lensch, Jan Kautz, Michael Gosele, and HansPeter Seidel. Image-based reconstruction of spatially varying materials. Eurographics Conference on Rendering, 2001. 2 10 [65] Zhengqin Li, Kalyan Sunkavalli, and Manmohan Chandraker. Materials for masses: SVBRDF acquisition with single mobile phone image. ECCV, 2018. [66] Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker. Inverse rendering for complex indoor scenes: Shape, spatially-varying lighting and SVBRDF from single image. CVPR, 2020. 2 [67] Ruofan Liang, Huiting Chen, Chunlin Li, Fan Chen, Selvakumar Panneer, and Nandita Vijaykumar. ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting. arXiv, 2023. 2 [68] Yehonathan Litman, Or Patashnik, Kangle Deng, Aviral Agrawal, Rushikesh Zawar, Fernando De la Torre, and Shubham Tulsiani. MaterialFusion: Enhancing Inverse Rendering with Material Diffusion Priors, 2024. 3, 6, 7, 8, 15, 16, 17, 21 [69] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3D object. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 3 [70] Zexiang Liu, Yangguang Li, Youtian Lin, Xin Yu, Sida Peng, Yan-Pei Cao, Xiaojuan Qi, Xiaoshui Huang, Ding Liang, and Wanli Ouyang. UniDream: Unifying diffusion priors for relightable text-to-3D generation, 2023. 3 [71] Ivan Lopes, Fabio Pizzati, and Raoul de Charette. Material Palette: Extraction of Materials from Single Image, 2023. 2 [72] Antoine Mercier, Ramin Nakhli, Mahesh Reddy, Rajeev Yasarla, Hong Cai, Fatih Porikli, and Guillaume Berger. HexaGen3D: StableDiffusion is just one step away from Fast and Diverse Text-to-3D Generation, 2024. [73] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. ECCV, 2020. 3, 14, 17 [74] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM TOG, 2022. 5 [75] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Mueller, and Sanja Fidler. Extracting Triangular 3D Models, Materials, and Lighting From Images. CVPR, 2022. 2, 3, 5 [76] Giljoo Nam, Diego Gutierrez, and Min H. Kim. Practical SVBRDF acquisition of 3d objects with unstructured flash photography. ACM Transactions on Graphics (SIGGRAPH ASIA), 2018. 1, 2 [77] Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. CVPR, 2022. 14 [78] Matt Pharr, Wenzel Jakob, and Greg Humphreys. Physically Based Rendering: From Theory to Implementation. Morgan Kaufmann Publishers/Elsevier, Cambridge, MA, third edition edition, 2017. Eleventh International Conference on Learning Representations, 2023. 2, 3 [80] Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, and Xiaoguang Han. RichDreamer: Generalizable Normal-Depth Diffusion Model for Detail Richness in Textto-3D, 2023. 3 [81] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, pages 87488763. PMLR, 2021. 6, 14 [82] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1067410685, New Orleans, LA, USA, 2022. IEEE. 2, 3, 15 [83] Shunsuke Saito, Gabriel Schwartz, Tomas Simon, Junxuan Li, and Giljoo Nam. Relightable gaussian codec avatars. In CVPR, 2024. 2 [84] Shen Sang and Manmohan Chandraker. Single-shot neural relighting and SVBRDF estimation. ECCV, 2020. 1, 2 [85] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. [86] Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W. Jacobs, and Jan Kautz. Neural inverse rendering of an indoor scene from single image. ICCV, 2019. 2 [87] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 2 [88] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. MVDream: Multi-view Diffusion for 3D Generation, 2024. 3 [89] Yawar Siddiqui, Tom Monnier, Filippos Kokkinos, Mahendra Kariya, Yanir Kleiman, Emilien Garreau, Oran Gafni, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, and David Novotny. Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials, 2022. 3 [90] Troy Sobotka. Sobotka/AgX. https://github.com/ sobotka/AgX, 2025. Accessed: 2025-03-05. 5 [91] Shuran Song and Thomas Funkhouser. Neural illumination: Lighting prediction for indoor environments. CVPR, 2019. 3 [79] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The [92] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In Advances in Neu11 ral Information Processing Systems, pages 1243812448. Curran Associates, Inc., 2020. 7 [93] Shimon Vainer, Mark Boss, Mathias Parger, Konstantin Kutsy, Dante De Nigris, Ciara Rowles, Nicolas Perony, and Simon Donne. Collaborative control for geometryIn Computer Vision conditioned pbr image generation. ECCV 2024: 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part XIII, page 127145, Berlin, Heidelberg, 2024. Springer-Verlag. 3 [94] Shimon Vainer, Mark Boss, Mathias Parger, Konstantin Kutsy, Dante De Nigris, Ciara Rowles, Nicolas Perony, and Simon Donne. Collaborative control for geometryconditioned PBR image generation. arxiv preprint, 2024. 6 [95] Eric Veach. Robust Monte Carlo Methods for Light Transport Simulation. PhD thesis, Stanford University, 1997. [96] Giuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy Boubekeur. ControlMat: Controlled Generative Approach to Material Capture. arxiv preprint, 2023. 2 [97] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. MCVD: Masked conditional video diffusion for prediction, generation, and interpolation. In Advances in Neural Information Processing Systems, 2022. 3 [98] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitrii Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. SV3D: Novel multi-view synthesis and 3D generation from single image using latent video diffusion. In European Conference on Computer Vision, 2024. 3, 4, 7, 14, 15, 16, 17, 21 [99] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitrii Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. SV3D: Novel multi-view synthesis and 3D generation from single image using latent video diffusion. In European Conference on Computer Vision (ECCV), 2024. 2, 3, 4, 5, 6, 7, 14, 16 [100] Bruce Walter, Stephen R. Marschner, Hongsong Li, and Kenneth E. Torrance. Microfacet models for refraction through rough surfaces. Eurographics Symposium on Rendering, 2007. 3 [101] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2069720709, 2024. [102] Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. MeshLRM: Large reconstruction model for highquality mesh. arXiv preprint arXiv:2404.12385, 2024. 15 [103] Tianwei Xiong, Yuqing Wang, Daquan Zhou, Zhijie Lin, Jiashi Feng, and Xihui Liu. Lvd-2m: long-take video dataset with temporally dense captions. NeurIPS, 2024. 3 [104] Jiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu. Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction. cvmj, 2022. 2 [105] Xudong Xu, Zhaoyang Lyu, Xingang Pan, and Bo Dai. MATLABER: Material-Aware Text-to-3D via LAtent BRDF autoEncodeR, 2023. 2 [106] Zexiang Xu, Jannik Boll Nielsen, Jiyang Yu, Henrik Wann Jensen, and Ravi Ramamoorthi. Minimal BRDF sampling for two-shot near-field reflectance acquisition. ACM Transactions on Graphics (SIGGRAPH), 35(6), 2016. 2 [107] Jonathan Young. Xatlas. https://github.com/ jpcy/, 2024. [108] Kim Youwang, Tae-Hyun Oh, and Gerard Pons-Moll. Paintit: Text-to-texture synthesis via deep convolutional texture map optimization and physically-based rendering. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [109] Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, and Xin Tong. Dilightnet: Fine-grained lighting control for diffusion-based image generation. In ACM SIGGRAPH 2024 Conference Papers, 2024. 3, 8, 18, 20 [110] Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, Bin Fu, Yong Liu, and Gang Yu. Paint3d: Paint anything 3d with lighting-less texture diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42524262, 2024. 3 [111] Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, and Miloˇs Haˇsan. RGB-X: Image decomposition and synthesis using materialand lighting-aware diffusion models. ArXiv, 2024. 2, 6, 7, 8, 15, 16, 21 [112] Jianzhao Zhang, Guojun Chen, Yue Dong, Jian Shi, Bob Zhang, and Enhua Wu. Deep inverse rendering for practical object appearance scan with uncalibrated illumination. ACG, 2020. 1, 2 [113] Jingyang Zhang, Yao Yao, Shiwei Li, Jingbo Liu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. Neilf++: Inter-reflectable light fields for geometry and material estimation. ICCV, 2023. 2 [114] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. PhySG: Inverse rendering with spherical Gaussians for physics-based material editing and relighting. CVPR, 2021. [115] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 4 [116] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. CLAY: controllable large-scale generative model for creating high-quality 3D assets. ACM Transactions on Graphics (SIGGRAPH), 43(4), 2024. 3 [117] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Scaling in-the-wild training for diffusion-based illumination harmonization and editing by imposing consistent light transport. In The Thirteenth International Conference on Learning Representations, 2025. 3, 8, 16, 18, 20 [118] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 14 [119] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. CVPR, 2018. 6 [120] Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul Debevec, William T. Freeman, and Jonathan T. Barron. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. ACM Trans. Graph., 40(6), 2021. 2 [121] Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou. Modeling indirect illumination for inverse rendering. CVPR, 2022. 2 [122] Jingsen Zhu, Fujun Luan, Yuchi Huo, Zihao Lin, Zhihua Zhong, Dianbing Xi, Rui Wang, Hujun Bao, Jiaxiang Zheng, and Rui Tang. Learning-based inverse rendering of complex indoor scenes with differentiable monte carlo raytracing. In SIGGRAPH Asia 2022 Conference Papers. ACM, 2022. 2 [123] Michael Zollhofer, Patrick Stotko, Andreas Gorlitz, Christian Theobalt, Matthias Nießner, Reinhard Klein, and Andreas Kolb. State of the Art on 3D Reconstruction with RGB-D Cameras. Computer Graphics Forum, 37(2):625 652, 2018. 16 13 B. Optimization B.1. UNet training details We pre-compute latents and CLIP-embeddings [81] for all training data. The RGB color rendering is composed on solid random color or white, the basecolor AOV stays always on white. The other outputs keep their black backgrounds. We follow the EDM framework and use the diffusion loss for fine-tuning described by Blattmann et al. [8]. We employ Flash Attention v2 [23, 24] to keep the memory footprint low such that batch size of two is still possible for 21 frames on similar hardware to the SV3D [98] training. Guidance. Compared to conventional video generation with reference frame as the starting point we have circular orbits both starting and ending close to the reference view. To reduce over-sharpening caused by classifier-free-guidance (CFG) [42] we also adapt triangular CFG scaling similar to the one proposed in [99] where the guidance scale is adapted based on the distance to the reference view. B.2. Geometry regularization. We adopt several geometric priors to regularize the reconstructed shape. Firstly we supervise the normal using the predicted normal maps. Especially during the beginning of the NeRF optimization this supervision loss is strictly enforced eliminating the need for any additional monocular prior. Since our normal maps generally contain more detail than can be represented by the mesh representation, starting from the second half of phase 1, we additionally optimize bump map represented by small auxiliary field conditioned on the coordinate embeddings from DMTet. bilateral smoothness loss is also added to the normals in phase 1 and increased during phase 2. Similarly, we utilize the smooth depth loss from RegNeRF [77]. While the supervision loss with the pseudoGT (pGT) and the photometric rendering loss are high in the beginning of the NeRF reconstruction (Phase 1) we slowly increase the weight of the LPIPS [118] over the course of the reconstruction ultimately dominating the reconstruction at the end of Phase 1. Our homography correction scheme is also added in Phase 1 after an initial warmup phase of 400 steps. In Phase 2 the LPIPS loss is slowly reduced little and bilateral smoothness regularizers increased in weight to clean up remaining noise. B.3. View dependent masking We normalize the masks by the maximum value over all views and apply smoothstep function fs followed by gamma correction to smoothly clip to the range of 0 to 1 and to steer the mask contrast. B.4. Homography correction"
        },
        {
            "title": "To make the optimization more robust to outlier views where\nthe image is warped wrongly due to homogeneous image",
            "content": "Figure 8. Multiple samples. Demonstrating the stochastic sampling process by taking three samples with the same condition image. For views that are less constrained by the conditioning diverse examples can be generated depending on the initial noise. Note, that the roughness and metallic parameters (blue and green here) are consistent with the RGB predictions, though."
        },
        {
            "title": "Overview",
            "content": "In the supplement to ... A. Additional Background A.1. Video Diffusion Denoising The conditioning image is concatenated to the noisy latent state input zt at noise timestep t. The CLIP-embedding [81] matrix of the conditioning image is provided to the crossattention layers of each transformer block as its key and value. The camera poses, represented as angles ei and ai as well as the noise timestep are encoded into sinusoidal position embeddings. The camera pose embeddings are linearly transformed and added to the noise timestep embedding. The result is added to each residual blocks output features after being run through another linear layer to match the feature dimension as in SV3D [98]. A.2. Coordinate-based MLPs and NeRF [73] NeRFs [73] use dense neural network to model continuous function that takes 3D location R3 and view direction R3 and outputs view-dependent output color R3 and volume density σ R. camera ray r(t) = + td is cast into the volume, with ray origin R3 and view direction d. The final color is then approximated via numerical quadrature of the integral: ˆc(r) = (cid:82) tf σ(t) dt), using tn the near and far bounds of the ray tn and tf respectively [73]. (t)σ(t)c(t) dt with (t) = exp( (cid:82) tn 14 Figure 9. Multi-view material prediction. Additional examples from the Poly Haven [39] test dataset. SViM3D successfully converts single image to sequence of novel views with spatially-varying PBR material parameters and surface normals. These can directly be used to relight the novel views as shown in the two bottom rows. regions or complex edge features, we introduce masking scheme in Phase 2. Based on the loss difference in the albedo map, it is decided if the current view is warped or not. If view is consistently masked, then Hi is reinitialized and further refined. C. Further results In the following section we provide additional results including evaluation on additional datasets and qualitative comparisons related to the reconstruction pipeline. C.1. Overview of baseline methods Intrinsic Image Diffusion (IID) [59] is one of the first works to explore diffusion models for PBR material estimation. Their model outputs albedo, roughness and metallic parameters for single frame. Originally trained on interior scenes, it has also been applied to general 3D reconstruction [30]. MaterialFusion [68] proposes 2D material denoising diffusion prior based on StableDiffusion 2.1 [82] with the same output as above but trained on object centric data. They employ an SDS based optimization to achieve 3D asset generation. Finally, RGBX [111] released latent image diffusion model that can generate PBR data as part of their (a) GT rendering (b) SF3D (c) SViM3D Figure 10. Material parametrization. Compared to SF3D [15], recent method for single image to 3D generation, our material model is able to replicate spatially-varying roughness and metallic parameters which help to represent real-world objects realistically. materialand lighting-aware neural rendering pipeline. Their material model can generate either albedo, roughness, metallic or diffuse irradiance maps conditioned on single image and text prompt to select the task. Significantly faster is SF3D [15] which is based on transformer decoder architecture like LRM [44, 102]. Since the 3D reconstruction code for SV3D [98] is not publicly available at the time of writing we decide to compare against SF3D instead. As evident in 15 Table 5. View consistency. Multiview consistency evaluated using MEt3R [3] on the Poly Haven test data. Method MEt3R score SV3D RGBX [111] SV3D + IID [59] SV3D + SM [68] SViM3D 0.54 0.51 0.54 0.57 Table 6. Baseline Methods. Features of existing methods used in our evaluation compared to SViM3D. Method SV3D [99] SF3D [15] IID [59] RGBX [111] SM [68] SViM3D RGB NVS Multi-view Joint PBR Spatially-varying PBR Normals Textured mesh Figure 11. Multi-view error distribution. We compare the SSIM results of the Basecolor prediction across frames over the Poly Haven test set. Table 7. Baseline Methods Relighting. Features of existing methods for image based relighting compared to SViM3D. Method IC Light [117] Neural Gaffer [50] SViM3D LDR output HDR output Global Illum NVS Multi-view Material Editing Interactive speed spatially-varying PBR and normal prediction as well as 3D reconstruction of textured mesh. C.2. Additional multi-view material results In Fig 9, Fig. 12 and Fig 16 we show additional raw outputs of our diffusion model given reference images from multiple datasets. SViM3D generates plausible material maps for variety of object classes and surface materials. The high metallic value in Fig 16 is questionable in physical sense but apparently helps the model to represent the specific shine of the dinosaur figure which might correspond to the way an artist might work in this case. In Fig. 23 we compare the generated material maps to the ground truth AOVs from synthetic data. Despite the ambiguity the model is able to predict plausible solutions also reflected in the RMSE values in Tab. 1. In addition to our newly introduced Poly Haven [39] object dataset we also evaluate our model on test split of the recently introduced BlenderVault dataset [68] in Tab. 9. The results are consistent with our evaluation on Poly Haven verifying the plausabiliy of our test results. C.3. Quantitative evaluation across views Fig. 11 compares the mean error across all generated views between all evaluated models from Tav. 2. Our method consistently yields the best results over all views, although it varies depending on the camera view. The observation that the side views are the most challenging generations might be explained by the occurrence of more extreme angle configurations in the context of the surface shading. Traditionally, grazing angles and samples close to object boundaries can lead to inconsistencies in 3D reconstruction [123] and generation might suffer from similar effects. Additionally, Tab. 5 shows the results of MeT3R [3], view consistency metric based on the recently introduced DUST3R [101] for calibration free 3D point cloud reconstruction. The metric also reflects the improved multi-view consistency in SViM3D compared to the SV3D [98] baselines. Figure 12. Multi-view PBR materials. Given the input image SViM3D generates multi-view consistent novel views with corresponding basecolor, roughness, metallic and normal maps. These can directly be used to generate views under novel illumination. We show 5 samples from generated orbit and two new illumination settings as examples. The objects are sourced from our Poly Haven [39] test dataset. Please find additional results in the supplementary material. Fig 10 SF3Ds material model is limited as it does not allow for spatially-varying roughness and metallic values. This poses severe limitation for real-world objects composed from multiple materials. Our spatially-varying parametrization yields shading results closer to the GT. Tab. 6 gives high-level overview of the features available in the compared methods. SViM3D is the only one offering RGB view synthesis and material synthesis as multi-view task with joint 16 Table 8. 3D reconstruction. We evaluate the model against SF3D [15] on subset of the Google Scanned Objects (GSO) [29] featuring real-world household items. The mesh quality is reported as Chamfer distance and IoU compared to the scanned GT pointclouds."
        },
        {
            "title": "Method",
            "content": "3D Geometry Chamfer IoU SF3D [15] SViM3D 0.031 0.034 0.52 0. Figure 13. More 3D reconstruction results. Objects sourced from Poly Haven [39] and GSO [29], rendered in Blender. Table 9. Multi-view NVS with material parameters on BlenderVault dataset. Given single RGB image multi-view orbit around the scene center is generated with corresponding PBR materials and normals. We compare RGB NVS and albedo / basecolor generation as stand-in for PBR materials against rendered GT on subset (100 objects) of the BlenderVault dataset [68]. We also compare against the MaterialFusion [68] baseline on their single view prediction task. Method RGB radiance 21 images SViM3D (ours) Basecolor / Albedo 21 images SViM3D (ours) Basecolor single image (ref view) SM [68] (from paper) SViM3D (ours) PSNR SSIM LPIPS FID CLIPS CMMD 0.081 24.95 24.70 27.35 0.91 0. - 0.05 - 1.08 - 46.0 - 0.83 19.80 20. 0.86 0.08 1.08 1.14 0.86 0. 40.0 0.86 Figure 14. 2.5D Relighting. Using the output of SViM3D and an environment map we can directly relight an object. We can use the same illumination representation and deferred shading as in the differentiable rendering pipeline. C.4. 3D reconstruction Fig. 15 illustrates our single image to 3D reconstruction pipeline using an example image from our test set. Starting with the multi-view novel view synthesis with material parameters and surface normals, the output is lifted to 3D representation, first NeRF [73], then polygon mesh. It is worth noting that the material parameters are well preserved thanks to our pseudo GT supervision. Finally, the mesh can be rendered under novel illumination, again. We show additional 3D reconstruction results in Fig. 13. Fig. 22 features two generations conditioned on smartphone capture illustrating in-the-wild performance. C.5. Multiple samples Fig. 8 compares three samples of denoising process given the same condition image. It is visible that there is some diversity in the predictions while they still all represent physically plausible solutions in the context of the conditioning given the underconstrained task. The diversity of the deviations increases the further the camera moves away from the condition frame, of course. C.6. 3D Geometry We evaluate the quality of the reconstructed geometry using Chamfer distance and Intersection over Union (IoU) against ground truth point clouds provided by the Google Scanned Objects (GSO) [29] dataset and report the results in Tab. 8. We select random subset of 80 real-world objects for the comparison against SF3D [15]. Compared to the feed-forward architecture of SF3D can our reconstruction method fail in rare cases where some views do not align for some reason. This is reflected in the slightly lower scores. In cases where reconstruction succeeds the quality is visually very close, often keeping bit finer detail in the case of SViM3D at the expensive of some additional noise (see also Fig 10). RGB only view synthesis Using the SV3D [98] baseline without PBR material prediction yields lower quality results also for the RGB color generation as reported in Tab. 2. We argue that enforcing reasoning over illumination as part of the material estimation also helps the generation of consistent lighting in the RGB views. C.7. Relighting In Fig. 14 we give additional insights into our 2.5D relighting approach. We show metallic and plastic surface lit by different rotations of the spherical environment map. Using all Figure 15. 3D reconstruction example. SViM3D pipeline starts with single image at the bottom left. First novel views and the corresponding material parameters and surface normals are generated. Following, an intermediate 3D representation is optimized given the multi-view material prior. Finally, 3D mesh can be extracted and integrated into downstream applications. Here we show an example from our Poly Haven [39] test dataset. Figure 16. Multi-view material examples from GSO. Two objects from the GSO [29] dataset representing common real-world houshold items. SViM3D generalizes well to this domain as long as the scene is object centric. the generated material channels and the normal directions we can achieve dynamic direct illumination at real-time speed. We also present the intermediate illumination representation used in our deferred shading pipeline. Our pipeline also enables material editing as further analyzed in Fig. 18. Fig 20 shows examples for different illumination directions and camera views. To achieve indirect illumination, full 3D reconstruction can be completed. Relighting comparison We present additional results from our 2.5D relighting pipeline in Fig. 19. As baselines we use IC-Light [117], Neural Gaffer [51] and DiLightNet [109], three diffusion based methods for image-based relighting recently introduced. In Tab. 7 we give an overview of the feature sets of all relighting methods. Neural Gaffer supports environment map inputs as conditioning which is fed as low and high dynamic range representation. IC-Light provides image editing based on background image. And DiLightNet adds radiance hints to the conditioning via environment maps. In our comparison we preprocess the environment maps to serve the methods, respectively. We compare the results against the GT obtained from our 2.5D rendering pipeline here, using the synthetic PBR material maps. SViM3D is the only model capable of joint novel view synthesis and relighting. This is reflected in better multi-view consistency and fewer artifacts like the residual highlight in the example of Neural-Gaffer. IC-Light generally generated high-contrast output which is difficult to edit in real-world use cases. 3D relighting application As shown in Fig. 15 as well as Fig. 1 the 3D reconstructed models can be easily integrated into new environments thanks to the PBR materials. Using path tracer global illumination effects can then be achieved, 18 Figure 17. Multi-view material examples from generated images.. Multi-view generations conditioned on generated images from text-to-image models, wizard raccoon and silver teapot. SViM3D is capable of estimating plausible and view consistent results. The wizard raccoon is an out-of-distribution example due to the lack of stylized character models in the training data. Figure 18. Material editing. The explicit material parameters of SViM3Ds output can be edited in physically-plausible way and the result visualized using our rendering framework. In this example the material roughness is varied between almost zero and close to one while the original value is close to the version second to left. too. Please find additional dynamic relighting and scene integration examples in the supplemental video. Analysis of ambiguous materials We constructed small dataset of pathological test cases for the ambiguity between metallic and glossy plastic surfaces. In over 90% of the cases low roughness value with near zero metalness is predicted. The predictions of higher values often are for objects that would usually have metal in their material. See Fig. 21 for visual example. These findings can be explained by dataset bias. 19 Figure 19. Relighting comparison. We compare image-based relighting results on an example object from the Poly Haven [39] dataset between the synthetic ground truth (GT), IC-Light [117], Neural-Gaffer [51], DiLightNet [109] and SViM3D (ours). Figure 21. Glossiness vs. Metalness ambiguity. Examples from our generated test cases and the corresponding model predictions. Figure 20. Relighting. Using the output of SViM3D and an environment map (HDRI) we can directly relight any view on the camera trajectory using our 2.5D approach. Figure 22. Real-world results. Example generations from casual smartphone captures of shaker instrument and strawberry. 20 Figure 23. Comparison of multi-view material generation on Poly Haven objects. We compare generated materials of RGBX [111], StableMaterial (SM) of MaterialFusion [68] and Intrinsic Image Diffusion (IID) [59] based on SV3D [98] generations and SViM3D for three views around the object against GT renders."
        }
    ],
    "affiliations": [
        "Stability AI",
        "University of Tubingen"
    ]
}