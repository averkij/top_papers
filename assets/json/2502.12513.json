{
    "paper_title": "RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm",
    "authors": [
        "Tiancheng Gu",
        "Kaicheng Yang",
        "Chaoyi Zhang",
        "Yin Xie",
        "Xiang An",
        "Ziyong Feng",
        "Dongnan Liu",
        "Weidong Cai",
        "Jiankang Deng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language representation learning. To fully leverage these unpaired documents, we initially establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, a dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. Extensive experiments demonstrate that RealSyn effectively advances vision-language representation learning and exhibits strong scalability. Models pre-trained on RealSyn achieve state-of-the-art performance on multiple downstream tasks. To facilitate future research, the RealSyn dataset and pre-trained model weights are released at https://github.com/deepglint/RealSyn."
        },
        {
            "title": "Start",
            "content": "RealSyn: An Effective and Scalable Multimodal Interleaved Document"
        },
        {
            "title": "Transformation Paradigm",
            "content": "Tiancheng Gu*, Kaicheng Yang*, Chaoyi Zhang, Yin Xie, Xiang An, Ziyong Feng Dongnan Liu, Weidong Cai, Jiankang Deng The University of Sydney DeepGlint Imperial College London tigu8498@uni.sydney.edu.au, kaichengyang@deepglint.com https://garygutc.github.io/RealSyn"
        },
        {
            "title": "Abstract",
            "content": "5 2 0 2 8 1 ] . [ 1 3 1 5 2 1 . 2 0 5 2 : r After pre-training on extensive image-text pairs, Contrastive Language-Image Pretraining (CLIP) demonstrates promising performance on wide variety of benchmarks. However, substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language representation learning. To fully leverage these unpaired documents, we initially establish Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. Extensive experiments demonstrate that RealSyn effectively advances vision-language representation learning and exhibits strong scalability. Models pre-trained on RealSyn on achieve multiple downstream tasks. To facilitate future research, the RealSyn dataset and pretrained model weights are released at https: //github.com/deepglint/RealSyn. state-of-the-art performance"
        },
        {
            "title": "Introduction",
            "content": "The rapid proliferation of mobile networks and social platforms has led to exponential growth in large-scale data, offering robust foundation for vision-language representation learning (Guo et al., 2019; Baltrušaitis et al., 2018; Guo et al., 2024). By predicting the correspondence between images * Equal contribution. Corresponding author. 1 Figure 1: Multimodal interleaved documents are unsuitable for vision-language representation learning. We construct distinct image-text pairs from such documents via retrieval and generation. and description texts, Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021) pretrains separate uni-modal encoders and achieves excellent performance across range of downstream tasks (Tian et al., 2020; He et al., 2020; Chen et al., 2020; Chopra et al., 2005; Shao et al., 2024; Sun et al., 2024; Luo et al., 2024). Notably, the representational capacity of such models improves significantly with the scale of the training dataset (Li et al., 2024b). However, the availability of paired data in the real world is limited. In recent years, the release of large-scale imagetext pair datasets (Schuhmann et al., 2021, 2022; Thomee et al., 2016; Byeon et al., 2022; Gadre et al., 2024) has significantly advanced visionlanguage representation learning. Due to download failures and non-English captions in the YFCC (Thomee et al., 2016) dataset, DeCLIP (Li et al., 2022) reprocesses new version of the YFCC15M dataset. LAION400M (Schuhmann et al., 2021) provides 400 million image-text pairs collected from the web and filtered using CLIP similarity. COYO700M (Byeon et al., 2022) offers 747 million high-quality pairs curated through advanced filtering strategies, including CLIP similarity, watermark detection, and aesthetic scoring. Despite these advancements, substantial amount of non-paired data, particularly interleaved imagetext documents (Zhu et al., 2024; Laurençon et al., 2024; Li et al., 2025) containing multiple images and text paragraphs without explicit correspondence, remains incompatible with conventional vision-language representation learning methods. Recent studies (Fan et al., 2024; Yu et al., 2024; Zheng et al., 2024a; Lai et al., 2024) aim to enhance the quality of open-source image-text datasets using existing models. For instance, LaCLIP (Fan et al., 2024) leverages large language models (LLMs) to rewrite raw captions for better alignment with images. CapsFusion (Yu et al., 2024) fine-tunes an LLM using ChatGPT-generated instruction dataset to mitigate hallucination issues. However, the diversity and distribution of synthetic data generated by these methods are inherently constrained by the limitations of the underlying generative models. In this paper, we explore two fundamental questions: 1) How to utilize multimodal interleaved documents for vision-language representation learning. 2) How to effectively leverage both realistic and synthetic texts to enhance representation performance. To this end, as shown in Fig. 1, we first establish Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design hierarchical retrieval method to efficiently associate each image with multiple semantically relevant texts. To enhance fine-grained image understanding, we propose visual semantic augmented generation module for synthetic text production. Furthermore, we employ semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct the RealSyn dataset, which integrates both realistic and synthetic texts with three sizes: 15M, 30M, and 100M. Comprehensive experimental results show that RealSyn is effective for vision-language representation learning and exhibits excellent scalability. The main contributions of this paper are summarized as follows: We propose an effective and scalable multimodal interleaved document transformation paradigm for vision-language representation learning. We release RealSyn, large-scale semantic balanced dataset that integrates both realistic and synthetic texts and is available in three sizes: 15M, 30M, and 100M. We conduct extensive experiments and demonstrate the effectiveness and scalability of our proposed RealSyn dataset."
        },
        {
            "title": "2 Related Work",
            "content": "Large-Scale Pre-training Dataset. In recent years, several large-scale image-text datasets (Chen et al., 2015; Clark and Gardner, 2017; Goyal et al., 2017; Byeon et al., 2022; Li et al., 2024a) collected from the Internet have been released. The YFCC100M (Thomee et al., 2016) dataset provides comprehensive overview of the evolution of photo and video documentation and sharing from the inception of Flickr in 2004 until early 2014. Due to download failures and non-English captions, DeCLIP (Li et al., 2022) reprocesses new version of the YFCC15M dataset. Additionally, the LAION400M (Schuhmann et al., 2021) dataset contains 400 million image-text pairs collected from Common Crawl and widely used in vision-language pre-training. Recent advancements have also introduced several large-scale interleaved image-text document datasets (Li et al., 2025; Zhu et al., 2024; Laurençon et al., 2024). The OBELICS (Laurençon et al., 2024) dataset uses comprehensive filtering strategy and includes 141 million web pages, 353 million associated images, and 115 billion text tokens extracted from Common Crawl. However, due to data format constraints and training inefficiencies, interleaved image-text documents are currently unsuitable for vision-language representation learning. Vision Language Pre-training. As pioneering work in visual language pre-training, CLIP has attracted extensive attention due to its powerful zero-shot recognition and exceptional transfer learning performance (Wang and Kang, 2024; Tang et al., 2024; Shao et al., 2024; Martin et al., 2024). Inspired by CLIP, numerous visual-language pretraining works have been published in recent years (Mu et al., 2022; Li et al., 2022; Yang et al., 2023). SLIP (Mu et al., 2022) enhances performance by combining self-supervised learning with CLIP pre-training. DeCLIP (Li et al., 2022) improves pre-training efficiency by integrating multiview supervision across modalities and nearestneighbor supervision from similar pairs. To mitigate the influence of noisy data, ALIP (Yang et al., 2 Figure 2: The Real-World Data Extraction pipeline to extract high-quality images and texts from interleaved image-text documents. 2023) introduces gating mechanism to dynamically allocate weights to samples. However, all these methods rely on large-scale image-text pairs crawled from the Internet. Synthetic Captions. Recent works (Yang et al., 2023; Yu et al., 2024; Chen et al., 2024) indicate that image-text pairs obtained from websites contain intrinsic noise, which directly impacts the effectiveness of vision-language pre-training. To enhance data diversity, LaCLIP (Fan et al., 2024) uses the in-context learning capability of large language models to rewrite text descriptions associated with each image. CapsFusion (Yu et al., 2024) employs large language models to refine information from web-based image-text pairs and synthetic captions, improving the quality of multimodal pre-training data. However, the quality and diversity of synthetic data are inherently limited by the capabilities of the generative model."
        },
        {
            "title": "3.1 Real-World Data Extraction",
            "content": "To transform interleaved image-text documents for vision-language representation learning, we establish Real-World Data Extraction pipeline  (Fig. 2)  to extract high-quality images and texts. This pipeline consists of three steps: Data Extraction, Image Filtration, and Sentence Filtration. Data Extraction. We employ 118M interleaved image-text documents from the OBELICS (Laurençon et al., 2024) as the primary data source. All images are extracted and stored in dedicated image database, while sentences are segmented using the Natural Language Toolkit (NLTK) (Bird and Loper, 2004) and stored in separate sentence database. This process yields 336M images and 2.13B sentences from the interleaved documents. Image Filtration. After extracting 336M images, we apply two-stage filtering process to ensure data quality and reduce redundancy. First, we discard images that meet any of the following criteria: 1) the shorter dimension is fewer than 100 pixels, or 2) the aspect ratio exceeds 3 or is below 1/3. This step removes 51M low-quality images. Next, following CLIP-CID (Yang et al., 2025), we use the EVA02-CLIP E/14-plus model (Sun et al., 2023) to extract image embeddings and apply the Union-Find algorithm (Tarjan, 1975) to eliminate perceptually and semantically redundant images. This step removes an additional 87M images, resulting in refined dataset of 198M high-quality images. Sentence Filtration. After extracting 2.13B sentences from interleaved image-text documents, we conduct rigorous filtering based on quality, semantics, and redundancy. Initially, we eliminate sentences based on the following criteria: 1) presence of emojis or URLs; 2) sentences containing fewer than 3 or more than 81 words; and 3) following CAT (Radenovic et al., 2023), we retain samples with at least C1 caption complexity and incorporating an action. This phase reduces the corpus size from 2.13B to 1.82B sentences. Then we apply semantic filtering to the remaining sentences, eliminating those with minimal information assessed through information entropy: θ(x) = (cid:88) i=1 p(xi) log p(xi), (1) where denotes the number of words in sentence, xi represents the i-th words in the sentences x, and p(xi) is the probability of the word xi in the entire corpus. Based on human cognition principles and empirical experience, we filter out sentences with 3 by Unicom (An et al., 2023), we utilize the standard K-Means algorithm (Ikotun et al., 2023) offline to partition the 0.84B texts into 2M clusters with the help of efficient feature quantization (Johnson et al., 2019). Hierarchical Retrieval. Given the prohibitive computational overhead of direct semantic text retrieval from 0.84B sentences (exceeding 10,000 hours on 8 A100 GPUs), we devise hierarchical retrieval method to optimize computational efficiency. We first perform inter-cluster retrieval to find the most relevant cluster center for each image. Then, we group images sharing the same cluster center and perform intra-cluster retrieval to obtain multiple semantically relevant sentences. This approach enables the retrieval of 198M images and 0.84B sentences within 40 hours using 8 A100 GPUs. Image Semantic Augmented Generation. Although the retrieved realistic texts achieve satisfactory performance, they exhibit limitations in capturing fine-grained visual semantics. To address this issue, we introduce the Image Semantic Augmented Generation module. This module initially employs the OFA model to generate concise caption for each image. We then integrate the open-set image tagging model RAM++, which extracts object detection tags. Considering that RAM++ supports only 4,000 tags, we expand this set to 8,000 by incorporating an additional 4,000 tags derived from real-world sentences. Following CapsFusion (Yu et al., 2024), we utilize ChatGPT4 Turbo to merge the retrieved realistic texts with concise captions and image tags to construct 100K instruction dataset (the prompt we used is presented in the supplementary material). Subsequently, we fine-tune the LLaMA3-8B model (Dubey et al., 2024) using the LLaMA Factory (Zheng et al., 2024b) and deploy the vLLM (Kwon et al., 2023) for large-scale inference. Ultimately, we convert data from 118M multimodal interleaved documents into 198M image-text pairs, where each image is associated with multiple retrieved realistic texts and synthetic texts."
        },
        {
            "title": "3.3 Semantic Balance Sampling",
            "content": "To further improve the quality and diversity of our dataset, we implement semantic balancing sampling across the 198M image-text pairs. Specifically, we use EVA02-CLIP E/14-plus (Sun et al., 2023) to encode and calculate the cosine similarity between imaged and synthetic texts. To reduce the Figure 3: The architecture of our proposed framework, which constructs distinct image-text pairs from realworld data extracted from interleaved documents via retrieval and generation. score below 0.3. To further refine the corpus by removing difficult or ambiguous sentences, we use GTP2-large (Radford et al., 2019) to calculate the perplexity score PPL for each sentence: (cid:40) PPL(x) = exp 1 (cid:88) i=1 (cid:41) log pθ(xi x<i) , (2) where represents the token number of the sentence, and pθ(xi x<i) is the likelihood of the i-th token given the previous tokens. We retain sentences with perplexity scores between 30 and 200. The overall semantics filtering reduces the corpus to 1.16B sentences. In the final stage, similar to redundancy image filtering, we perform both perceptual and semantic deduplication of sentences. This process results in refined corpus of 0.84B sentences that include extensive real-world knowledge."
        },
        {
            "title": "3.2 Retrieval and Generation Framework",
            "content": "After extracting high-quality images and sentences from documents, we propose an efficient and scalable framework to retrieve multiple semantically relevant texts for each image and leverage large language models to integrate retrieved realistic text with fine-grained visual information and generate synthetic text. As shown in Fig. 3, the architecture of our framework primarily consists of three components: Text Semantic Clustering, Hierarchical Retrieval, and Image Semantic Augmented Generation. Text Semantic Clustering. To efficiently retrieve multiple semantic relevant texts for each image, we initially encode all sentences using the EVA02CLIP E/14-plus (Sun et al., 2023) model. Inspired 4 1 0 1 o"
        },
        {
            "title": "Data Scale Dataset F",
            "content": "15M 30M YFCC 67.2 LAION 71.0 RealSyn 77.1 LAION 76.1 RealSyn 81.2 100M LAION 80.2 RealSyn 84.2 0 1 I 90.4 93.3 94.5 94.5 95.4 95.7 96.3 0 0 1 I 70.8 78.1 78.7 80.0 81. 82.5 83.5 s B 47.7 41.0 43.4 47.4 48.4 51.3 54. 7 9 3 66.7 66.3 71.4 70.3 74.5 73.4 76.2 r A 29.7 43.0 42.7 45.9 45.2 46.1 47.6 C 23.8 76.9 64.7 82.3 73. 85.3 77.4 62.4 71.2 71.3 74.7 74.2 75.6 75.6 P 65.7 74.5 79.9 80.3 84.1 83.2 86.3 t 80.1 87.6 90.0 89.8 91. 91.1 92.1 w 90.0 88.2 88.2 89.5 90.6 92.0 91.7 0 1 S 94.7 93.6 96.4 95.6 97.2 96.9 97.7 o 94.9 95.3 96.2 95.5 96. 95.2 96.8 5 4 S 79.4 82.9 87.2 84.5 89.2 85.9 90.6 I 75.4 72.2 72.4 72.6 74.5 68.4 73.1 n 18.4 13.5 16.7 15.2 19. 17.4 21.1 1 0 1 70.8 75.4 79.9 76.6 82.6 80.0 83.7 e 48.6 55.7 55.7 56.2 55.0 57.3 57.3 2 56.2 57.3 57.7 60.0 56. 61.4 58.9 e I 56.7 59.3 64.0 64.3 68.5 68.3 71. a A 64.5 69.8 71.4 72.6 73.9 74.4 75.8 Table 1: Linear probe on 20 downstream datasets. Pre-training ViT-B/32 on RealSyn achieves 1.3%-6.9% average performance improvement. 1 0 1 o"
        },
        {
            "title": "Data Scale Dataset F",
            "content": "15M 30M YFCC 36.3 LAION 49.1 RealSyn 60.0 LAION 58.9 RealSyn 67.5 100M LAION 68.9 RealSyn 73.5 0 1 I 74.0 85.7 85.7 85.9 89.0 90.5 89.5 0 0 1 I 40.3 56.9 58.3 63.1 65. 68.6 68.8 s B 19.4 11.5 10.5 17.4 15.0 23.6 20. 7 9 3 41.8 45.1 56.4 54.8 60.6 60.6 65.0 r A 2.3 3.8 5.5 4.3 7.9 7.8 10.2 C 2.1 49.9 27.6 61.0 39. 68.3 48.5 12.0 25.7 33.2 36.4 37.8 41.2 46.1 P 19.8 54.6 61.7 65.5 70.5 74.7 76.7 t 59.8 78.1 80.2 82.0 84. 87.1 87.6 w 48.9 30.5 31.2 41.3 42.2 47.7 48. 0 1 87.7 89.5 92.4 91.3 93.8 94.4 94.4 o 21.2 36.7 56. 40.3 59.9 45.6 69.0 5 4 S 20.3 36.1 56.2 43.7 61.9 53.4 65. T 23.8 21.7 34.0 24.3 27.7 23.6 24.6 n 5.1 5.6 8. 7.2 10.6 10.4 12.1 1 0 1 27.8 38.2 52.6 47.4 56.7 54.5 60. m 47.4 48.8 53.3 51.5 52.5 51.9 52.4 2 50.1 49.9 51. 50.1 50.1 53.3 54.1 e I 32.3 37.1 43.3 44.9 50. 52.8 56.2 r 33.6 42.7 47.9 48.6 52.1 53.9 56.2 Table 2: Zero-shot transfer on 20 downstream datasets. Pre-training ViT-B/32 on RealSyn achieves 2.3%-14.3% average performance improvement. impact of OCR-related or mismatched pairs during pre-training, we filter out 29.7M pairs with cosine similarities above 0.61 or below 0.51. Inspired by MetaCLIP (Hu Xu, 2023), we introduce an easy but efficient cluster-based semantic balance sampling strategy. We cluster the image embeddings from the remaining 168.3M pairs into 1M centers. To enhance the semantic diversity of our dataset, we randomly select 20, 35, and 180 samples from clusters exceeding these thresholds, while retaining all samples from smaller clusters. This approach culminates in the construction of the RealSyn15M, RealSyn30M, and RealSyn100M datasets."
        },
        {
            "title": "4 Experiments and Results",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "We initially collect 118M interleaved imagetext documents from the OBELICS (Laurençon et al., 2024) as our primary data source. We use OFAbase (Wang et al., 2022) and RAM++large (Huang et al., 2023) to generate brief captions and semantic tags. To validate the dataset performance, we pre-train standard CLIP supervised by the text randomly selected from the three retrieved realistic texts and one synthetic text. During pre-training, we adopt AdamW (Loshchilov, 2019) as the optimizer, with learning rate of 1e-3 and weight decay of 0.2. The parameters β1 and β2 are set to 0.9 and 0.98, respectively. The input image size is 224224, and the input text sequence length is 77. The temperature parameter τ is initialized to 0.07. We train 32 epochs with 4096 batch sizes on 8 A100 (80G) GPUs. Please refer to the supplementary material for more details. To validate the effectiveness of the RealSyn dataset, we compare RealSyn with the previous datasets across various models and data scales. We compare RealSyn15M with the YFCC15M filtered by DeCLIP (Li et al., 2022). Following ALIP (Yang et al., 2023), we also compare with LAION15M, LAION30M, and LAION100M (subset randomly selected from LAION400M)."
        },
        {
            "title": "4.2 Main Results",
            "content": "Linear Probe. In Tab. 1, we present the linear probe performance of the ViT-B/32 model across 20 downstream datasets. When pretrained on the 15M scale, RealSyn15M surpasses YFCC15M in 16 out of 20 datasets, registering an average performance increase of 6.9%. Moreover, RealSyn15M outperforms LAION15M in 18 out of 20 datasets, with an average improvement of 1.6%. Upon scaling the dataset to 30M and 100M, RealSyn achieves average performance improvement of 1.3% and 1.4% over LAION, respectively. These results demonstrate the efficacy of the RealSyn dataset in vision-language representation learning. Zero-shot Transfer. We evaluate the zeroshot transfer performance of the ViT-B/32 model across 20 classification benchmarks using the same prompt templates as SLIP (Mu et al., 2022). As indicated in Tab. 2, RealSyn15M surpasses YFCC15M on 18 of 20 datasets, achieving an aver-"
        },
        {
            "title": "Data Scale Dataset",
            "content": "R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@"
        },
        {
            "title": "Image Retrieval",
            "content": "Flickr30k"
        },
        {
            "title": "MSCOCO",
            "content": "Flickr30k"
        },
        {
            "title": "MSCOCO",
            "content": "15M 30M 100M"
        },
        {
            "title": "LAION\nRealSyn",
            "content": "37.1 49.1 72.9 59.6 76.0 67.5 81.6 64.8 76.8 91.1 83.5 93.3 87.9 96. 75.9 84.5 95.1 89.8 96.9 93.0 97.3 21.3 28.4 43.8 35.9 48.2 43.3 52. 45.1 53.0 69.5 62.4 74.6 68.0 76.7 57.0 64.9 79.6 73.2 83.0 78.1 85. 23.5 33.3 49.5 42.4 54.0 50.4 58.8 47.3 60.5 76.3 70.1 80.0 77.2 84. 58.3 70.9 84.6 79.4 87.6 85.5 90.5 13.2 17.4 25.8 22.1 29.5 27.1 32. 32.0 38.3 50.6 45.5 55.2 52.1 58.9 43.1 49.7 62.5 57.6 66.9 63.8 70. Table 3: Zero-shot image-text retrieval performance on Flickr30k and MSCOCO. Pre-training CLIP-B/32 on RealSyn dataset achieves significant improvement on all metrics."
        },
        {
            "title": "Data Scale Dataset",
            "content": "IN-V2 IN-A IN-R ObjectNet IN-Sketch Average 15M 30M 100M"
        },
        {
            "title": "LAION\nRealSyn",
            "content": "27.3 30.7 37.1 37.5 42.9 44.6 47.6 12.3 6.0 12.5 8.9 16.1 12.2 19. 20.8 46.5 47.7 54.4 56.6 62.5 62.5 25.3 28.7 35.0 35.5 41.5 42.2 45. 6.3 24.3 25.4 31.8 31.9 37.9 37.9 18.4 27.2 31.5 33.6 37.8 39.9 42. Table 4: Zero-shot robustness comparison. Pre-training CLIP-B/32 on RealSyn demonstrates superior robustness across all datasets. age performance improvement of 14.3%. In comparison to LAION15M, RealSyn15M excels on 18 of 20 datasets with an average improvement of 5.2%. Upon expanding the dataset sizes to 30M and 100M, RealSyn achieves average performance improvements of 3.5% and 2.3% compared to LAION, highlighting its efficiency and scalability. It is important to note that RealSyn demonstrates significant decrease in performance on certain datasets, such as Cars and Flowers. This reduction is primarily attributed to the unique data distribution of RealSyn, characterized by scarcity of data for specific concepts, which hampers the models ability to effectively learn these concepts. For instance, as depicted in Fig. 4, samples related to cars constitute merely 0.9% of the dataset. Zero-shot Image-Text Retrieval. In Tab. 3, we present the zero-shot image-text retrieval performance of the ViT-B/32 model pre-trained on different scale of datasets. RealSyn achieves superior results across all evaluation metrics. Specifically, RealSyn15M improves Recall@1 by 35.8%&26% on Flickr30K (Young et al., 2014) and by 22.5%&12.6% on MSCOCO (Lin et al., RealSyn30M improves Recall@1 by 2014). 16.4%&11.6% on Flickr30K (Young et al., 2014) and by 12.3%&7.4% on MSCOCO (Lin et al., 2014). This significant enhancement in crossmodal retrieval performance indicates that the RealSyn dataset effectively improves vision-language representation learning by utilizing realistic and synthetic texts, resulting in robust representations Figure 4: T-SNE (Van der Maaten and Hinton, 2008) projection of LDA (Blei et al., 2003) topic cluster from randomly selected 1M samples from RealSyn. and enhanced cross-modal alignment. Zero-shot Robustness. In Tab. 4, we present the zero-shot robustness performance. The results show that RealSyn significantly improves the robustness of vision-language pre-training models. Specifically, compared to LAION, RealSyn increases average performance by 4.3%, 4.2%, and 2.8% for datasets of 15M, 30M, and 100M, respectively. This notable improvement in performance primarily stems from the use of retrieved realistic texts that are not bound by the limitations of generative models, coupled with superior conceptual diversity compared to YFCC and LAION, thereby substantially enhancing model robustness."
        },
        {
            "title": "5.1 Statistics Analysis",
            "content": "Topic-based Assessment. Following MMC4 (Zhu et al., 2024), we ran LDA (Blei et al., 2003) on random sampling 1M image-realistic text pairs with 30 topics. Fig. 4 presents the proportions and examples for six topics: animal, food, airplane, flower, automotive, and landmark. Notably, the dataset contains minimal samples related to flower and automotive topics, representing merely 0.4% and 0.9% of the total, respectively. This paucity of examples hinders the models ability to sufficiently learn these concepts, thereby compromising its performance in the linear probe and zero-shot transfer evaluations on the Flowers and Cars datasets. 6 (a) Richness assessment comparison Figure 6: Model scaling capability. We compare the models pre-trained on LAION30M and RealSyn30M. (b) Diversity assessment comparison Figure 5: The richness assessment and diversity assessment on different datasets. RealSyn-R1: the most relevant retrieved realistic text. RealSyn-S1: the semantic augmented synthetic text based on RealSyn-R1. Richness Assessment. Fig. 5a presents image-text similarity and text token distribution of 15M samples from YFCC15, LAION, RealSyn-R1 (the most relevant retrieved realistic text), and RealSyn-S1 (the semantic augmented synthetic text based on RealSyn-R1). Compared to the datasets collected from the Internet, RealSyn exhibits robust similarity metrics, even after removing OCR data. Moreover, both the retrieved realistic texts and synthetic texts contain larger quantity of words, which can provide richer textual context that enhances visionlanguage representation learning. Diversity Assessment. The RealSyn is constructed based on real-world interleaved image-text documents, which encompasses wide array of diverse information. Following previous work (Lai et al., 2025), we randomly select 0.2M samples to calculate the number of unique entities in the caption to assess the data diversity of different datasets. As depicted in Fig. 5b, both the retrieved realistic texts and image semantic augmented synthetic texts exhibit higher number of distinct entities. Such diversity enriches the dataset, facilitating the models acquisition of comprehensive knowledge and enhancing both performance and robustness. Model Scaling. In Sec. 4.2, RealSyn exhibits superior performance across various data scales (More data scaling analysis is presented in the supplementary material). To further explore the model scaling capability, we present the downstream task performance of three models in Fig. 6. Notably, compared to LAION, RealSyn demonstrates steeper slopes in performance curves across linear probing, 7 Figure 7: Image captioning comparisons on COCO2017 and Flickr30k. B4, MT., RL. and Cd. represent the metric of BLEU, METEOR, ROUGE-L, and Cider. zero-shot transfer, and robustness, indicative of its superior model scaling capabilities."
        },
        {
            "title": "5.2 Ablation Study",
            "content": "Image Captioning via MLLM. Following LLaVA1.5 (Liu et al., 2024), we initially align visual features to the text space using 558k data. Subsequently, we construct an image captioning dataset from LAION and RealSyn, which we then use for instruction tuning. Specifically, we split the realistic text and synthetic text for the same image into two independent samples, totaling 2M samples for training for one epoch. Concurrently, we randomly select 1M samples from LAION and train for two epochs. As illustrated in Fig. 7, RealSyn demonstrates significant enhancement in performance compared to LAION across all evaluation metrics on both the COCO2017 (Lin et al., 2014) and Flickr30k (Young et al., 2014). Extension to Pure Image. To further extend our method for pure images, we conduct experiments on ImageNet (Deng et al., 2009). Initially, we retrieve semantically relevant realistic texts for each ImageNet image from our sentence database and generate image semantic augmented synthetic texts. Then, we pre-train ResNet50 (He et al., 2016) supervised by the text randomly selected from the retrieved realistic texts and synthetic texts. Comparative analysis with SimCLR (Chen et al., 2020) under identical conditions shows linear probe average performance enhancement of 2.1% across 12 datasets using our constructed data. Detailed exper-"
        },
        {
            "title": "Robustness\nAvg",
            "content": "CLIP-B/32 YFCC LAION RealSyn-Random RealSyn-Balance 64.5 69.8 70.7 71.4 33.6 42.7 46.8 47.9 18.4 27.2 30.5 31.5 Table 5: Comparison of concept balance sampling and random sampling on the 15M dataset. 1 2 3 4 5 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)"
        },
        {
            "title": "Linear probe\nAvg",
            "content": "70.3 71.0 71.2 70.9 70.6 1 2 3 4 5 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)"
        },
        {
            "title": "Linear probe\nAvg",
            "content": "70.2 70.0 69.9 69.4 69.1 Table 6: Ablation experiment results using different types of text on the 15M dataset. : the k-th retrieved semantic relevant realistic text. : the image semantic augmented synthetic text for . imental results are provided in the supplementary material. Ablation on Semantic Balance Sampling. To demonstrate the effectiveness of our proposed semantic balance sampling method, we compare it with random sampling. As indicated in Tab. 5, concept balance sampling yields performance improvements of 0.7%, 1.1%, and 1.0% in linear probe, zero-shot transfer, and robustness, respectively. Besides, we visualize the data distribution using different sampling methods by clustering 15M samples into 1M centers. As shown in Fig. 8, the distribution from semantic balanced sampling is smoother, facilitating the learning of long-tail concepts. Figure 8: Clustering distribution of 15M data obtained from random sampling and semantic balanced sampling. Ablation on Realistic Texts and Synthetic Texts. We conduct ablation experiments on the impact of varying quantities of realistic and synthetic texts using the CLIP-B/32 model. As shown in Tab. 6, increasing the quantity of realistic text from 1 to 3 progressively improves model performance, attributable to enhanced text augmentation that provides extensive real-world knowledge. However, expanding from 3 to 5 slightly reduces performance due to information saturation and noise introduction. Meanwhile, increasing the number of synthetic text from 1 to 5 introduces more noise, gradually decreasing performance. Importantly, supervising image training solely with retrieved realistic Figure 9: Visualization of the raw interleaved document, the retrieved realistic text, and synthetic text. Image semantic-related information is highlighted in green. texts significantly enhances performance, achieving 71.2% accuracy compared to 69.8% with the LAION15M dataset. This highlights the crucial role of real-world knowledge in advancing visionlanguage representation learning. Case Study. In Fig. 9, we present the visualization of retrieved realistic text and synthetic text obtained from an interleaved document using our proposed transformation paradigm. Both realistic and synthetic texts contain extensive descriptive information consistent with the image semantics, such as lyell glacier, crags, and valley. We provide more visualizations in the supplementary material."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper explores two open-ended questions: 1) How to utilize multimodal interleaved documents for vision-language representation learning. 2) How to effectively leverage both realistic and synthetic texts to enhance representation performance. To this end, we first establish Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design hierarchical retrieval method to efficiently associate each image with multiple semantically relevant texts. To enhance fine-grained image understanding, we propose visual semantic augmented generation module for synthetic text production. Furthermore, we employ semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we present RealSyn, dataset driven by both real and synthetic texts with three sizes: 15M, 30M, and 100M. Comprehensive experimental results show that RealSyn is efficient in vision-language representation learning and exhibits excellent scalability."
        },
        {
            "title": "7 Limitations",
            "content": "To provide more fine-grained visual information, this paper utilizes vision expert models combined with Large Language Model (LLM) to generate synthetic text. Given the inference cost and efficiency, the utilization of Modified Large Language Models (MLLM) for synthetic text generation is left for the community to explore. Furthermore, the transformation paradigm proposed in this paper is directly applicable to other multimodal document datasets, including MMC4 (Zhu et al., 2024) and OmniCorpus (Li et al., 2025). We hope our work provides insights into vision-language representation learning."
        },
        {
            "title": "8 Ethics Statement",
            "content": "We abide by the ACL Code of Ethics. The data resources used in this study are publicly available."
        },
        {
            "title": "References",
            "content": "Xiang An, Jiankang Deng, Kaicheng Yang, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, and Tongliang Liu. 2023. Unicom: Universal and compact representation learning for image retrieval. In ICLR. Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. 2018. Multimodal machine learning: survey and taxonomy. TPAMI. Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle Alexander, David Jacobs, and Peter Belhumeur. 2014. Birdsnap: Large-scale fine-grained visual categorization of birds. In CVPR. Steven Bird and Edward Loper. 2004. NLTK: The natural language toolkit. In ACL. David Blei, Andrew Ng, and Michael Jordan. 2003. Latent dirichlet allocation. JMLR. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. 2014. Food-101mining discriminative components with random forests. In ECCV. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. 2022. Coyo-700m: Image-text pair dataset. https: //github.com/kakaobrain/coyo-dataset. Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2024. Sharegpt4v: Improving large multi-modal models with better captions. In ECCV. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. simple framework for contrastive learning of visual representations. In ICML. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv:1504.00325. Gong Cheng, Junwei Han, and Xiaoqiang Lu. 2017. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE. Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005. Learning similarity metric discriminatively, with application to face verification. In CVPR. Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. 2014. Describing textures in the wild. In CVPR. Christopher Clark and Matt Gardner. 2017. Simple and effective multi-paragraph reading comprehension. In ACL. Adam Coates, Andrew Ng, and Honglak Lee. 2011. An analysis of single-layer networks in unsupervised feature learning. In AISTATES. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: large-scale hierarchical image database. In CVPR. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv:2407.21783. Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. 2024. Improving clip training with language rewrites. In NeurIPS. Li Fei-Fei, Rob Fergus, and Pietro Perona. 2004. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. 2024. Datacomp: In search of the next generation of multimodal datasets. In NeurIPS. Andreas Geiger, Philip Lenz, and Raquel Urtasun. 2012. Are we ready for autonomous driving? the kitti vision benchmark suite. In CVPR. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR. Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei Liu. 2024. Regiongpt: Towards region understanding vision language model. In CVPR. 9 Wenzhong Guo, Jianwen Wang, and Shiping Wang. 2019. Deep multimodal representation learning: survey. IEEE Access. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised visual representation learning. In CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In CVPR. Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. 2019. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. Xiaoqing Ellen Tan Hu Xu, Saining Xie. 2023. Demystifying clip data. arXiv:2309.16671. Xinyu Huang, Yi-Jie Huang, Youcai Zhang, Weiwei Tian, Rui Feng, Yuejie Zhang, Yanchun Xie, Yaqian Li, and Lei Zhang. 2023. Inject semantic concepts into image tagging for open-set recognition. arXiv:2310.15200. Abiodun M. Ikotun, Absalom E. Ezugwu, Laith Abualigah, Belal Abuhaija, and Jia Heming. 2023. K-means clustering algorithms: comprehensive review, variants analysis, and advances in the era of big data. Inf. Sci. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. IEEE Billion-scale similarity search with GPUs. Transactions on Big Data, 7(3):535547. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes challenge: Detecting hate speech in multimodal memes. In NeurIPS. Jonathan Krause, Michael Stark, Jia Deng, and Li FeiFei. 2013. 3d object representations for fine-grained categorization. In ICCVW. Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images. Technical Report. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In SOSP. Zhengfeng Lai, Vasileios Saveris, Chen Chen, HongYou Chen, Haotian Zhang, Bowen Zhang, Juan Lao Tebar, Wenze Hu, Zhe Gan, Peter Grasch, et al. 2025. Revisit large-scale image-caption data in pre-training multimodal foundation models. In ICLR. Zhengfeng Lai, Haotian Zhang, Bowen Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, and Meng Cao. 2024. Veclip: Improving clip training via visual-enriched captions. In ECCV. Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. 2024. Obelics: An open web-scale filtered dataset of interleaved image-text documents. In NeurIPS. Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, et al. 2025. Omnicorpus: unified multimodal corpus of 10 billion-level images interleaved with text. In ICLR. Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, and Ling-Yu Duan. 2024a. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. In NeurIPS. Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. 2022. Supervision exists everywhere: data efficient contrastive language-image pre-training paradigm. In ICLR. Zichao Li, Cihang Xie, and Ekin Dogus Cubuk. 2024b. Scaling (down) clip: comprehensive analysis of data, architecture, and training strategies. arXiv preprint arXiv:2404.08197. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In ECCV. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved baselines with visual instruction tuning. In CVPR. Loshchilov. 2019. Decoupled weight decay regularization. In ICLR. Yan Luo, Min Shi, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, Tobias Elze, et al. 2024. Fairclip: Harnessing fairness in visionlanguage learning. In CVPR. Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. 2013. Fine-grained visual classification of aircraft. arXiv:1306.5151. Ségolène Martin, Yunshi Huang, Fereshteh Shakeri, Jean-Christophe Pesquet, and Ismail Ben Ayed. 2024. Transductive zero-shot and few-shot clip. In CVPR. Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. 2022. Slip: Self-supervision meets language-image pre-training. In ECCV. Maria-Elena Nilsback and Andrew Zisserman. 2008. Automated flower classification over large number of classes. In Sixth Indian Conference on Computer Vision, Graphics & Image Processing. 10 Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. 2012. Cats and dogs. In ICCV. Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. JMLR. Jingyun Wang and Guoliang Kang. 2024. Learn to rectify the bias of clip for unsupervised semantic segmentation. In CVPR. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. Ofa: Unifying architectures, tasks, and modalities through simple sequence-to-sequence learning framework. In ICML. Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. 2010. Sun database: Large-scale scene recognition from abbey to zoo. In ICCV. Kaicheng Yang, Jiankang Deng, Xiang An, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, and Tongliang Liu. 2023. Alip: Adaptive language-image pre-training with synthetic caption. In ICCV. Kaicheng Yang, Tiancheng Gu, Xiang An, Haiqiang Jiang, Xiangzi Dai, Ziyong Feng, Weidong Cai, and Jiankang Deng. 2025. Clip-cid: Efficient clip distillation via cluster-instance discrimination. In AAAI. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL. Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and Jingjing Liu. 2024. Capsfusion: Rethinking image-text data at scale. In CVPR. Kecheng Zheng, Yifei Zhang, Wei Wu, Fan Lu, Shuailei Ma, Xin Jin, Wei Chen, and Yujun Shen. 2024a. Dreamlip: Language-image pre-training with long captions. In ECCV. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024b. Llamafactory: Unified efficient fine-tuning of 100+ language models. In ACL. Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. 2024. Multimodal c4: An open, billion-scale corpus of images interleaved with text. In NeurIPS. Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende, Yash Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Mahajan. 2023. Filtering, distillation, and hard negatives for vision-language pre-training. In CVPR. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In ICML. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022. Laion5b: An open large-scale dataset for training next generation image-text models. In NeurIPS. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 2021. Laion-400m: Open dataset of clipIn NeurIPS filtered 400 million image-text pairs. Workshop. Shuai Shao, Yu Bai, Yan Wang, Baodi Liu, and Yicong Zhou. 2024. Deil: Direct-and-inverse clip for openworld few-shot learning. In CVPR. Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. 2012. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv:1212.0402. Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. 2023. Eva-clip: Improved training techniques for clip at scale. arXiv:2303.15389. Shuyang Sun, Runjia Li, Philip Torr, Xiuye Gu, and Siyang Li. 2024. Clip as rnn: Segment countless visual concepts without training endeavor. In CVPR. Yuwei Tang, Zhenyi Lin, Qilong Wang, Pengfei Zhu, and Qinghua Hu. 2024. Amu-tuning: Effective logit bias for clip-based few-shot learning. In CVPR. Robert Endre Tarjan. 1975. Efficiency of good but not linear set union algorithm. JACM. Bart Thomee, David Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. 2016. Yfcc100m: The new data in multimedia research. Communications of the ACM. Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020. Contrastive multiview coding. In ECCV. 11 This supplementary material introduces the experiment settings and the instruction prompt we used in Sec. A. Then, we introduce the downstream datasets, detailed experiment results, data scaling law, and additional ablation study results in Sec. B. Finally, we further analyze our proposed RealSyn dataset by comparison with current datasets and visualize examples in Sec. C."
        },
        {
            "title": "A Detail Experiment Settings",
            "content": "A.1 Experiment Settings In Tab. 7, we present the detailed settings used in training CLIP."
        },
        {
            "title": "Value",
            "content": "0.07 0.2 4096 0.001 Initial temperature Weight decay Batch size Learning rate Learning rate scheduler OneCycleLR Pct start Training epochs GPU Adam β1 Adam β2 Adam ϵ 0.1 32 8A100 0.9 0.98 106 Table 7: Hyperparameters used for CLIP pre-training. A.2 Detail Instruction Prompt The prompt we used for ChatGPT to construct the 100K instruction dataset is present in the following: \"Please merge the information from the given raw text and the synthetic caption with the help of the highly relevant detection tags. The raw caption offers detailed real-world information, yet it suffers from flaws in sentence structure and grammar. The synthetic caption exhibits impeccable sentence structure but often lacks in-depth real-world details and may contain false information. The highly relevant detection tags are provided to enrich the semantic information of the raw caption, while some are redundant and noisy. You are great information integration and summary expert, you are also good at enriching semantic information. Ensure well-structured sentence while retaining the detailed real-world information provided in the raw caption. Avoid simply concatenating the sentences and avoid adding external information to describe. Correctness and simplify sentences finally. Raw caption:<raw caption>, synthetic"
        },
        {
            "title": "Classes Train size Test size Evaluation metric",
            "content": "Food101 CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft DTD Pets Caltech101 Flowers STL10 EuroSAT RESISC45 KITTI Country211 UCF101 Memes SST2 ImageNet 102 10 100 500 397 196 100 47 37 101 102 10 10 45 4 211 101 2 2 1000 75,750 50,000 50,000 42,138 19,850 8,144 6,667 3,760 3,680 3,000 2,040 5,000 10,000 3,150 6770 42,200 9,537 8,500 7,792 1,281,167 25,250 10,000 10,000 2,149 19,850 8,041 3,333 1,880 3,669 5,677 6,149 8,000 5,000 25,200 711 21,100 1,794 500 1,821 50,000 accuracy accuracy accuracy accuracy accuracy accuracy mean per class accuracy mean per class mean per class mean per class accuracy accuracy accuracy accuracy accuracy accuracy ROC AUC accuracy accuracy Table 8: List of linear probe datasets with the data distribution and evaluation metrics. caption:<synthetic caption>, and highly relevant detection tags:<detection tags>\"."
        },
        {
            "title": "B Detail External Results",
            "content": "B.1 Downstream Datasets To comprehensively demonstrate the performance of CLIP trained on RealSyn, we compared the linear probe results of CLIP trained on RealSyn, YFCC (Thomee et al., 2016), and LAION (Schuhmann et al., 2021) across 20 datasets. These datasets include Food101 (Bossard et al., 2014), CIFAR10 (Krizhevsky et al., 2009), CIFAR100 (Krizhevsky et al., 2009), Birdsnap (Berg et al., 2014), SUN397 (Xiao et al., 2010), Stanford Cars (Krause et al., 2013), FGVC Aircraft (Maji et al., 2013), DTD (Cimpoi et al., 2014), Pets (Parkhi et al., 2012), Caltech101 (FeiFei et al., 2004), Flowers102 (Nilsback and Zisserman, 2008), SLT10 (Coates et al., 2011), EuroSAT (Helber et al., 2019), RESISC45 (Cheng et al., 2017), KITTI (Geiger et al., 2012), Country211 (Radford et al., 2021), UCF101 (Soomro et al., 2012), Hateful Memes (Kiela et al., 2020), SST2 (Radford et al., 2021), and ImageNet (Deng et al., 2009). Details on each dataset and the corresponding evaluation metrics are provided in Tab. 8. B.2 Detailed Model Scaling Results Linear Probe. In Tab. 9, we present the detailed linear probe results of different scale CLIP model trained on the 30M dataset. The ViT-L/14 trained on RealSyn30M achieves an average performance 1 0 1 Dataset LAION RealSyn LAION RealSyn LAION RealSyn 76.1 81.2 82.1 87.5 84.7 90.3 Model ViT-B/32 ViT-B/16 ViT-L/14 0 1 I 94.5 95.4 95.1 95. 96.4 97.5 0 0 1 I 80.0 81.8 81.4 82.5 83.5 86.2 s i 47.4 48.4 57.5 59.4 59.2 64.3 7 9 3 70.3 74. 73.4 77.5 75.5 79.7 r A 45.9 45.2 47.1 48. 46.6 51.4 C 82.3 73.4 87.3 81.0 88.5 83.6 D 74.7 74.2 76.1 77.9 77.8 79.6 P 80.3 84.1 84.4 88. 85.0 90.0 t 89.8 91.3 91.5 92.5 92.6 94.5 w F 89.5 90.6 92.7 94.2 94.3 94.8 0 1 95.6 97.2 96.8 98. 97.9 98.9 o 95.5 96.5 95.6 96.9 95.9 96.6 5 4 S 84.5 89.2 86.8 91.5 88.0 92.7 I 72.6 74.5 70.8 70. 71.7 73.8 n 15.2 19.0 17.6 22.1 18.7 25.0 1 0 1 U 76.6 82.6 80.3 85.1 81.1 86.4 e 56.2 55.0 59.5 60. 58.6 63.8 2 60.0 56.2 65.6 64.7 64.6 66.1 e m 64.3 68.5 68.8 73.9 71.2 76.7 r 72.6 73. 75.5 77.5 76.6 79.6 Table 9: Linear probe on 20 downstream datasets. Pre-training different scale CLIP models on RealSyn30M and LAION30M, achieves 1.3%-3.0% average performance improvement. 1 0 1 Dataset LAION RealSyn LAION RealSyn LAION RealSyn 58.9 67.5 67.6 75.8 70.8 80.7 Model ViT-B/ ViT-B/16 ViT-L/14 0 1 I 85.9 89.0 89.1 89.6 88.8 94. 0 0 1 I 63.1 65.2 63.5 64.7 69.5 73.1 s B 17.4 15.0 20.8 18.9 22.8 20.9 7 9 3 54.8 60.6 55.7 64. 61.6 66.4 r A 4.3 7.9 5.4 7.9 4.9 10. a 61.0 39.2 66.9 48.2 69.7 53.6 36.4 37. 39.0 41.2 40.8 48.1 P 65.5 70.5 70.2 76.0 68.0 72. e C 82.0 84.0 84.9 87.5 87.3 89.4 w 41.3 42. 42.9 45.2 42.2 49.8 0 1 91.3 93.8 94.3 95.1 95.3 96. S E 40.3 59.9 31.1 56.8 41.5 68.5 5 4 S 43.7 61. 45.4 64.3 53.7 70.1 I 24.3 27.7 34.0 27.1 25.9 32. t C 7.2 10.6 8.7 13.1 10.4 15.3 1 0 1 47.4 56. 52.2 59.1 54.7 63.9 e 51.5 52.5 54.5 54.5 54.1 54. 2 50.1 50.1 50.6 54.0 51.8 56.9 e I 44.9 50.9 49.4 55.9 51.5 59.5 r 48.6 52.1 51.3 54. 53.3 58.8 Table 10: Zero-shot transfer on 20 downstream datasets. Pre-training different scale CLIP models on RealSyn30M and LAION30M, achieves 3.5%-5.5% average performance improvement."
        },
        {
            "title": "Dataset",
            "content": "IN-V2 IN-A IN-R ObjectNet IN-Sketch Average ViT-B/32 ViT-B/16 ViT-L/"
        },
        {
            "title": "LAION\nRealSyn",
            "content": "37.5 42.9 42.4 48.0 45.1 52.8 8.9 16.1 12.8 24.1 17.1 34. 54.4 56.5 60.3 63.1 64.9 71.6 35.5 41.5 40.2 46.7 43.1 50. 31.8 31.9 34.8 36.8 39.0 42.4 33.6 37.8 38.1 43.8 41.8 50. PreTable 11: Zero-shot robustness comparison. training different scale CLIP models on RealSyn30M and LAION30M, achieves 4.2%-8.6% average performance improvement. improvement of 3.0% across 20 datasets compared to the model trained on the LAION30M. Zero-shot Transfer. As shown in Tab. 10, ViTL/14 trained on our proposed RealSyn30M outperforms LAION30M on 18 of 20 downstream datasets and achieves an average improvement of 5.5%. Zero-shot Robustness. We present the detailed zero-shot robustness performance in Tab. 11, compared with LAION30M, the model trained on RealSyn30M boosts average robustness performance by 8.6% on ViT-L/14. B.3 Data Scaling Law We present the data scaling law (Kaplan et al., 2020) derived from our RealSyn dataset, justifying its scalability over samples. Specifically, we conduct series of visual-language pre-trainings with proposed datasets ranging from 12M to 60M, and fit each performance metric to the inverse of logarithmic functions with respect to the number of millions of training samples x. Based on the fitting results from these preliminary experiments, we extrapolate each performance scaling law to 100M samples, and validate their predicted scaling Figure 10: Data Scaling Analysis. Pre-training ViTB/32 on RealSyn in different data scales. trends with our RealSyn100M dataset as shown in Fig. 10. Notably, as indicated by the coefficients shown in Eq. 3, these performance laws also likely suggest an upper bound of model capability that ViT-B/32 could possibly reach through our proposed visual-language pre-training paradigm with multimodal interleaved documents: Linear Probe: L(x) Transfer: L(x) Robustness: L(x) 0.21 log(x 4.23) 0.30 log(x 5.68) 0.60 log(x 3.17) + 0.80 + 0. (3) + 0.56 B.4 Ablation on Retrieved Realistic and"
        },
        {
            "title": "Synthetic Texts",
            "content": "Tab. 14 shows the ablation experiment results on the text augmentation using different text types. After introducing the image semantic augmented synthetic text, due to its ability to supplement finegrain visual semantic information, there are 0.2%,"
        },
        {
            "title": "Method",
            "content": "1 0 1 F 0 1 I 0 0 1 I n i ResNet SimCLR Real&Syn Texts 68.4 72.5 90.6 89.1 71.6 69.0 37.4 57.1 7 9 3 S 58.8 63.6 r A 50.3 48.1 C 50.3 51. 7 0 0 2 80.5 85.5 74.5 69.7 P 83.6 90. 1 0 1 t 90.3 88.1 w 91.2 88.8 r 70.6 72. Table 12: Linear probe on 12 downstream datasets. Pre-training CLIP (ResNet50) on image-text pairs achieves 2.1% performance improvement."
        },
        {
            "title": "Dataset",
            "content": "#Images #Avg Tokens / Image #Avg Texts / Image"
        },
        {
            "title": "Source Type",
            "content": "12 000 000 CC12M 15 000 000 YFCC15M 120 000 000 CapsFusion LAION400M 400 000 000 RealSyn15M 15 239 498 RealSyn30M 30 328 852 RealSyn100M 100 862 786 16 27 40 38 36 1 1 1 1 4 4 4 Realistic Realistic Synthetic Realistic Realistic & Synthetic Realistic & Synthetic Realistic & Synthetic Website Website Image-Text Pair Website Interleaved Image-Text Interleaved Image-Text Interleaved Image-Text Table 13: Current Dataset Comparison. Comparison with large-scale image-text pre-training datasets."
        },
        {
            "title": "C Further Analysis of RealSyn",
            "content": "T 1 2 3 1 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) 70.3 71.2 70.2 71.4 42.4 46.8 39.7 47.9 25.7 30.7 24.0 31.5 Table 14: Ablation experiment results using different types of text on RealSyn15M. : the k-th retrieved semantic relevant realistic text. : the image semantic augmented synthetic text for . 1.1%, 0.8% performance enhancement on the linear probe, zero-shot transfer, and zero-shot robustness compared to solely utilizing the retrieved realistic texts. C.1 Compare with Existing Datasets In Tab. 13, we compare our proposed RealSyn dataset with existing widely used large-scale imagetext pre-training datasets. Compared to previous datasets, our proposed RealSyn dataset provides four textual descriptions for each image, with an average token length of 36-40, significantly higher than LAION400M and YFCC15M. Furthermore, unlike previous datasets, the RealSyn dataset is sourced from real-world interleaved image-text documents and includes both realistic and synthetic texts, thereby expanding the scope for future research exploration. B.5 Detailed Results on Pure Image C.2 Visualization of Examples In Fig. 11 and Fig. 12, we visualize additional image-text pairs randomly selected from our proposed RealSyn dataset. is the k-th retrieved semantically relevant real-world sentence and is the semantic augmentic caption for . We also highlighted the image semantic-related information in green and marked the image size below the image. To further extend our method for pure images, we conduct experiments on ImageNet (Deng et al., 2009). For each image, we retrieve three semantically relevant real-world sentences from our preconstructed sentence database and generate single semantically augmented synthetic caption based on the top retrieved text. Following SimCLR (Chen et al., 2020), we utilize 4096 batch size and pretrain ResNet50 (He et al., 2016) for 90 epochs supervised by the text randomly selected from the three retrieved realistic texts and one synthetic text. As shown in Tab. 12, compared with SimCLR (Chen et al., 2020) under the same conditions, the model trained on our constructed image-text pairs shows an average performance improvement of 2.1% across 12 downstream datasets. The results demonstrate that our method can effectively transform pure images into high-quality image-text pairs through retrieval and generation for visionlanguage pre-training. Figure 11: Visualization of image-text pairs in our proposed RealSyn dataset. k : the image semantic augmented synthetic text for green. : the k-th retrieved realistic text. . Image semantic-related information is highlighted in 15 Figure 12: Visualization of image-text pairs in our proposed RealSyn dataset. k : the image semantic augmented synthetic text for green. : the k-th retrieved realistic text. . Image semantic-related information is highlighted in"
        }
    ],
    "affiliations": [
        "DeepGlint",
        "Imperial College London",
        "The University of Sydney"
    ]
}