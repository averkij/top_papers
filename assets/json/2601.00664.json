{
    "paper_title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
    "authors": [
        "Taekyung Ki",
        "Sangwon Jang",
        "Jaehyeong Jo",
        "Jaehong Yoon",
        "Sung Ju Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline."
        },
        {
            "title": "Start",
            "content": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation Taekyung Ki1, Sangwon Jang1, 1KAIST Jaehyeong Jo1 2NTU Singapore Jaehong Yoon2 Sung Ju Hwang1,3 3DeepAuto.ai 6 2 0 J 2 ] . [ 1 4 6 6 0 0 . 1 0 6 2 : r {taekyung.ki, sangwon.jang, sungju.hwang}kaist.ac.kr https://taekyungki.github.io/AvatarForcing Figure 1. Overview of Avatar Forcing. It can generate real-time interactive avatar video conditioned on user motion and audio, as well as avatar audio. The avatar naturally mirrors the users expression, such as smiling when the user smiles, for more engaging interactions."
        },
        {
            "title": "Abstract",
            "content": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the users audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Equal contribution. Furthermore, we introduce direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency ( 500ms), achieving 6.8 speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline. 1. Introduction Talking head generation animates static portrait images into lifelike avatars that can speak like humans. These systems are increasingly used to create virtual presenters, hosts, and educators that can substitute for real human presence in many scenarios. They also support customized avatars that users can interact with, for instance, chatting with their favorite characters [20, 42, 52], offering practical tool for content creation and visual communication. However, existing avatar generation models fail to fully replicate the feeling of real face-to-face interaction. They primarily focus on generating audio-synchronized lip or natural head motions [27, 32, 38, 53, 6063] to deliver information accurately and naturally, rather than engaging in interactive conversations. Such one-way interaction overlooks the bidirectional nature of real-life conversations, where the continuous exchange of verbal and non-verbal signals plays crucial role in communication. For instance, active listening behaviors such as nodding or empathic responses encourage the speaker to continue, while expressive speaking behaviors such as smiling or making eye contact contribute to more realistic and immersive conversation. We identify two major challenges in generating truly interactive avatars. The first challenge is the real-time processing of users multimodal inputs. An interactive avatar system must continuously receive and respond to the users multimodal cues, such as speech, head motion, and facial expression, which requires both low inference time and minimal latency. Existing methods [69] achieve fast inference within motion latent space [13] but have high latency because they need the full conversation context (e.g., over 3 seconds), including future frames. The model must wait for sufficiently long audio segment before generating motion, causing notable delay in user interaction, as illustrated in Fig. 4. This highlights the need for causal motion generation framework that reacts immediately to live inputs. The second challenge is learning expressive and vibrant interactive motion. Expressiveness in human interactions is inherently difficult to define or annotate, and the lack of well-curated data makes the natural interactive behaviors hard to model. For listening behaviors in particular, we observe that most of the training data are less expressive and low-variant, often exhibiting stiff posture (See Fig. 5). Moreover, unlike lip synchronization, which is strongly tied with the avatar audio and therefore relatively easy to learn, reacting appropriately to user cues can correspond to wide range of plausible motions. This ambiguity greatly increases learning difficulty, often resulting in less diverse, stiff motions, particularly in response to non-verbal cues. To address these challenges, we present new interactive head avatar generation framework, Avatar Forcing, which models the causal interaction between the user and the avatar in learned motion latent space [27, 58]. Inspired by recent diffusion-forcing-based interactive video generative models [2, 24, 64], we employ causal diffusion forcing [5, 29, 64] to generate interactive head motion latents while continuously processing multimodal user inputs in real-time. Unlike the previous approach [69] that requires the future conversation context (See Fig. 4), Avatar Forcing causally generates interactive avatar videos by efficiently reusing past information through key-value (KV) caching. This design enables real-time reactive avatar generation with minimal latency. Moreover, following the learning-from-losing paradigm of preference optimization methods [40, 45, 57], we propose preference optimization method to enhance the interactiveness of the avatar motion. By synthesizing underexpressive motion latents via dropping the user signals and using them as less-preferred samples, we achieve significantly improved expressiveness and reactiveness, without the need for additional human annotations for natural interaction. As result, Avatar Forcing can produce more natural and engaging interactive videos as illustrated in Fig. 1. Extensive experiments demonstrate that Avatar Forcing supports real-time interaction with latency of roughly 500ms. Moreover, the proposed preference optimization significantly improves motion reactiveness and richness, producing more natural and engaging videos. In human evaluations, our method is preferred over 80% against the strongest baseline, demonstrating clear advantages in naturalness, responsiveness, and overall interaction quality. 2. Related Works 2.1. Talking Avatar Generation Talking avatar generation aims to generate lifelike talking avatar video from given reference image and an audio. Earlier methods in this field had focused on synthesizing accurate lip movement from the driving audio [3, 17, 26, 43, 68]. These approaches are extended to generate holistic head movements, including rhythmical head motion [27, 53, 61, 68] and vivid facial expressions, such as eye blink and jaw movement [13, 14, 18, 61, 65]. SadTalker [65], for instance, leverages 3D morphable models (3DMM) [12] as an intermediate representation for the non-verbal facial expression. EMO [53] and its subsequent models [7, 9, 25] leverage the foundation image diffusion model (e.g., StableDiffusion [46]) for photorealistic portrait animation. Recent works [27, 35, 61] introduce generative modeling techniques, such as diffusion models or flow matching into learned motion space [13, 58], achieving real-time head motion generation. 2.2. Listening Avatar Generation Another line of works [23, 34, 37, 39, 48, 67] focus on generating realistic listening head motions, such as nodding or focusing. Generating such responsive motions is challenging because the relationship between the speaker and listener is inherently one-to-many [39, 59], and the cues are context-dependent and weakly supervised. Hence, most of the works generate the personalized listening motion [39, 67] or leverage explicit control signals, such as text instruction [37] and pose prior [59]. 2 2.3. Dyadic Conversational Avatar Generation Recently, several studies investigated dyadic motion generation [19, 54, 69], which involves modeling the interactive behavior of two participants engaged in conversational setting. DIM [54] quantizes dyadic motions into two discrete latent spaces [56], one for the verbal and one for the non-verbal. However, it requires manual roleswitching signal between the two spaces, resulting in discontinuous transitions between speaker and listener states. INFP [69] addresses this limitation by introducing audioconditioned verbal and non-verbal memory banks within unified motion generator. However, this is ill-suited for real-time interactive generation as its bidirectional transformer [41] requires access to the entire context of the conversation. ARIG [19] generates implicit 3D keypoints [18] as its motion representation, primarily focusing on facial expressions. Yet it struggles with temporal consistency and fails to produce holistic head motions. In this work, we focus on modeling the interactive behavior of both participants continuously influencing each other through verbal and non-verbal signals. We generate holistic interactive motions, for instance, talking, head movement, listening, and focusing, using diffusion forcing framework that promptly responds to the multimodal user signals. 3. Background 1, x2 1, , xN Diffusion Forcing Diffusion forcing [5] stands out as an efficient sequential generative model that predicts next token conditioned on the noisy past tokens. Let x1 = 1 ) RN 3HW denote sequence of (x1 tokens sampled from data distribution p1. Each token is corrupted with per-token independent noise levels := (t1, t2, , tN ) [0, 1]N , forming an independently , , xN , x2 noised sequence xt := (x1 = tN t2 t1 tnxn 0 )N 0 and x0 = (xn n=1 p0. Here, we follow the noise scheduler of flow matching [33]. The training objective of diffusion forcing is to regress the vector field vθ toward the target vector field vn tn (cid:2)vθ(xn tn LDF (θ) = En,tn,xn 1 + (1 tn)xn ), where xn tn , tn) vn tn 1 xn 0 : = xn (cid:3) . (1) tn Diffusion forcing reformulates conventional teacher forcing in terms of diffusion models, allowing causal sequence generation with diffusion guidance [21] for controllable generation and flexible sampling procedure. Direct Preference Optimization Direct Preference Optimization (DPO) [45] aligns model with human preferences without explicitly training reward model. The training objective LDP is formulated as follows: Ec,xw,xl (cid:20) (cid:18) log σ β πθ(xwc) πref(xwc) β πθ(xlc) πref(xlc) (cid:19)(cid:21) , (2) 3 where is the condition, xw and xl denote preferred and less preferred samples, respectively, and πref is frozen reference model during the optimization, typically initialized by the pre-trained weight of πθ. Here, σ() is the sigmoid function, and β is the deviation parameter. DiffusionDPO [57] extends DPO to diffusion models by reformulating the objective with diffusion likelihoods, enabling preference optimization using the evidence lower bound. 4. Avatar Forcing In this work, we present Avatar Forcing, which generates video of real-time interactive head avatar, conditioned on the avatar audio and the multimodal signals of the user. We provide an overview of our framework in Fig. 2. In Sec. 4.1, we present our framework for achieving realtime interactive head avatar generation based on causal diffusion forcing in the motion latent space. This consists of two key steps: encoding multimodal user and avatar signals, and causal inference of avatar motion. In Sec. 4.2, we introduce preference optimization method for the interactive motion generation, which enhances expressive interaction without the need for additional human labels. 4.1. Diffusion Forcing for Real-Time Interaction Motion Latent Encoding For motion encoding, we employ the motion latent auto-encoder from Ki et al. [27]. The latent auto-encoder maps an input image R3HW to latent Rd whose identity and motion are decomposable: = zS + mS Rd, (3) where zS Rd and mS Rd are the identity and motion latents, respectively. The identity latent zS encodes identity representation of the avatar image (e.g., appearance), while the motion latent mS encodes rich verbal and nonverbal features (e.g., facial expression, head motion). We use this latent representation to capture holistic head motion and fine-grained facial expression, which are crucial for realistic head avatar generation. We provide more details on the auto-encoder in Appendix B. Interactive Motion Generation In Avatar Forcing, the motion latents are generated by conditioning on multimodal user signals and avatar audio. This can be formulated as an autoregressive model as follows: pθ(m1:N ) = (cid:89) i=1 pθ(mim<i, ci), (4) where each motion latent mi is predicted from past motion latents and the condition triplet ci = (ai u, ai) consisting of user audio ai u, and avatar audio ai. u, user motion mi u, mi Figure 2. Overall architecture of Avatar Forcing. We encode the use motion and audio, as well as avatar audio into unified condition by Dual Motion Encoder. Causal Motion Generator infer the motion latent block of the avatar, which are then decoded into an avatar video. Figure 3. Architecture of vθ. The look-ahead causal attention mask enables smooth transition across the blocks. Based on this formulation, we introduce diffusion forcing-based causal motion generator operating in the motion latent space, which is modeled using vector field model vθ. As illustrated in Fig. 3, the model vθ comprises two main components: Dual Motion Encoder and Causal DFoT Motion Generation. and ai The goal of the Dual Motion Encoder is to capture the bidirectional relationship between multimodal user signals and avatar audio, and to encode them into unified condition. As illustrated in Fig. 3(a), the encoder first takes the user signals (mi u) and aligns them through cross-attention layer, which captures the holistic user motion. This representation is then integrated with the avatar audio using another cross-attention layer, which learns the causal relation between the user and the avatar, producing unified user-avatar-condition. In Sec. 5.5, we empirically validate the importance of using user motion mi for generating an interactive avatar. For the causal motion generator, we adopt the diffusion forcing transformer (DFoT) [49] with blockwise causal structure [31, 64]. The latent frames are divided into blocks to capture local bidirectional dependencies within each block while maintaining causal dependencies across different blocks. For each block, we assign shared noise timestep to all frames and apply an attention mask that prevents the current block from attending to any future blocks. In Fig. 4, we compare our motion generator architecture with the standard bidirectional DiT architecture used in INFP [69]. Unlike INFP, which requires the full temporal context, our diffusion forcing allows stepwise motion generation under causal constraints and user-avatar interaction Figure 4. Architectural comparison between bidirectional and causal structure. (a) Bidirectional DiT used in INFP [69] requires access to the entire temporal window for motion generation. (b) Our blockwise causal DFoT predicts the next block without using future context and supports KV caching. with low latency. However, we observe that the strict causal mask in the blockwise causal attention fails to ensure smooth temporal transition across blocks. To address this issue, we introduce look-ahead in the causal mask, which allows each block to attend to limited number of future frames while preserving overall causality. We define this blockwise look-ahead causal mask as follows (illustrated in Fig. 3(b), right): Mi,j = 1 if j/B i/B + else 0, (5) where and are the frame indices, denotes the block size, and is the look-ahead frame size. With look-ahead, we effectively mitigate severe per-frame motion jittering observed in the simple blockwise causal structure, which we provide video examples in the supplementary materials. Based on the blockwise causal structure, the motion generator takes noisy latent block as input, which is concatenated with the avatar motion latent mS serving as the reference motion. We use cross-attention layer to condition on the unified condition from the Dual Motion Encoder. Additionally, we employ sliding-window attention mask of size 2l along the time axis for temporal smooth conditioning. We provide the more details on vθ in Appendix B. Training and Inference To train the vector field model vθ, we formulate the diffusion forcing training objective in 4 Algorithm 1 Motion inference with KV caching Require: ODE timesteps {tn}T n=0, motion generator vθ, video length , block size B, max cahce size , user inputs (au, mu), avatar audio ai, latent-to-frame decoder Dec, and id latent zS. 1: Divide the frames into = N/B blocks 2: Initialize KV, cKV [ ], [ ] Frame & condition caches 3: for = 1 to do 4: 5: t0 (0, I), mi u, mi u) and avatar audio ai . t0 RBd Sample Noise block mi Acquire User inputs (ai Set ci (ai u, ai) u, mi for = 0 to do Condition triplet Solve ODE: mi tj+1 vθ(mi tj , tj; ci, KV, cKV) end for Decode & Return xi Update caches kvi, ckvi vθ(zi if KV = cKV = then 1 Dec(zS, mi KV.pop(0) and cKV.pop(0) 1) RB3HW 1, 1; ci, KV, cKV) end if KV.append(kvi) and cKV.append(ckvi) 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for Eq. (1) into motion latent generation objective as follows: LDF (θ) = En,tn,mn vθ(mn tn , tn, cn)(mn 1 mn 0 ), (6) tn where [1, ] denotes the frame index, tn [0, 1] is the u, ai) is the userper-frame noise timestep, cn = (ai Rd is the noisy motion laavatar condition triplet, mn tn tent. For simplicity, we omit the reference motion condition mS in Eq. (6). u, mi With the trained model, we can generate the interactive avatar motion given user inputs and avatar audio in an autoregressive rollout. We provide the pseudo code for motion sampling in Algorithm 1, which adopts rolling KV cache in blockwise manner [24, 64]. 4.2. Enhancing Motion Expressiveness While our model enables real-time motion generation, we observe that it struggles to produce expressive motions crucial for natural human conversation. In contrast to achieving an accurate lip-sync that has an almost one-to-one match with the avatar audio, appropriate reaction to the users motion and audio is highly ambiguous, as there is no single correct response [37, 39, 59]. Moreover, as shown in Fig. 5, existing listening datasets generally exhibit lower motion expressiveness compared to speaking datasets, which leads models to learn passive and non-expressive listening behaviors. While prior work attempted to address it with personalized motion [39] or external text instructions [37], these methods do not address the core challenge of learning the true interactive behavior for natural interaction. To generate vibrant and expressive reactions, we formulate this as an alignment problem and apply the Rein5 Figure 5. Variance visualization of the L2-norm of 3DMM expressions [15] for the speaker and listener on ViCo [67] dataset. Higher variance indicates higher expressiveness. forcement Learning from Human Feedback (RLHF) [40] approach. The main challenge lies in defining an explicit reward for the avatars interactive behavior, which must account for the interplay between avatar audio, user audio, user motion, and the generated video, which are difficult to evaluate even for humans. We adapted reward-free method, Direct Preference Optimization (DPO) [45, 57], for fine-tuning our motion generation based on diffusion forcing. Specifically, we construct preference pairs of motion latents (mw, ml) as follows: Preferred sample mw: the motion latent from the ground-truth video, exhibiting expressive and contextually appropriate responses. Less preferred sample ml: the motion latent generated by separately trained talking avatar model [27], conditioned solely on the avatar audio. This paired design yields clear signal for enhancing expressiveness, emphasizing active listening and reactive motion while leaving other aspects, such as lip sync or speechdriven motion, unchanged. Building on the original DPO objective LDPO (Eq. (2)) with our constructed pairs (mw, ml), we fine-tune our diffusion forcing model vθ with the following objective: Lf t(θ) = LDF (θ) + λLDP O(θ), (7) where λ is balancing coefficient. As result, our model achieves efficient preference alignment without requiring dedicated reward model, which we validate in Sec. 5.5. We provide the detailed formulation of LDPO in Appendix C. 5. Experiments 5.1. Dataset and Preprocessing We use dyadic conversation video datasets: RealTalk [16] and ViCo [67]. We first detect scene changes using PySceneDetect [44] to split the videos into individual clips. We then detect and track each face using FaceAlignment [4], crop and resize it to 512512. Next, we separate and assign the speaker and listener audio using visual-grounded speech separation model [30]. All videos are converted to 25 fps, and audio is resampled to Table 1. Quantitative comparison results on RealTalk [16]. Best results highlighted in bold. denotes the reproduced version that is publicly unavailable. We also report the results from non-interactive talking head model [27], shown in gray, for reference. Interaction Reactiveness Motion Richness Visual Quality Lip Synchronization Method User Input Latency rPCC-Exp rPCC-Pose SID FLOAT [27] INFP [69] Ours GT N/A 2.4s 3.4s 0.5s N/A 0.054 0.035 0.003 0.000 0.182 0.064 0.036 0.000 2.785 2.343 2. 3.972 Var 2.778 1.638 1.734 1.658 FID FVD CSIM LSE-D LSE-C 81.297 438.817 24.551 159.000 24.328 170.874 N/A N/A 0.845 0.867 0.833 0.796 8.135 8.027 8. 7.790 6.361 6.536 6.723 6.940 Table 2. Human preference study on interactive avatar generation models, comparing Avatar Forcing and INFP. Table 3. Comparison with talking head generation models on the HDTF [66] dataset. Second-best results are underlined. Table 4. Comparison with listening head generation models on the ViCo [67] dataset. denotes results inherited from DIM [54]. Visual Quality Lip Synchronization FD rPCC SID Var Method FID FVD CSIM LSE-D LSE-C Method Exp Pose Exp Pose Exp Pose Exp Pose SadTalker [65] 64.744 32.794 Hallo3 [9] 25.110 FLOAT [27] 27.155 INFP* [69] 342.996 184.341 167.463 187.977 0.697 0.865 0.881 0.840 Ours 20.332 149. 0.870 8.046 8.498 7.553 7.810 7.700 7.171 7.487 8.006 7.325 7.560 RLHG [67] 39.02 0.07 0.08 0.02 3.62 3.17 1.52 0.02 L2L [39] 33.93 0.06 0.06 0.08 2.77 2.66 0.83 0.02 DIM [54] 23.88 0.06 0.06 0.03 3.71 2.35 1.53 0.02 17.52 0.07 0.01 0.07 2.19 3.20 2.10 0.03 INFP* [69] Ours 16.64 0.05 0.01 0.01 3.12 3.00 2.80 0.03 16 kHz. Additionally, we randomly select 50 videos from the talking-head dataset HDTF [66] to evaluate the performance of talking-head generation. 5.2. Implementation Details We use the Adam optimizer [28] with learning rate of 104 and batch size of 8. We retrain the motion latent auto-encoder from Ki et al. [27] on our dataset. The latent dimension is set to = 512. For our model vθ, we use 8 attention heads with hidden dimension of = 1024 and 1D RoPE [51]. It is trained with = 50 frames over = 5 blocks (i.e., 10 frames per block) and = 2 look-ahead frames. For audio encoding, we extract 12 multi-scale features from Wav2Vec2.0 [1]. For motion sampling, we use 10 NFEs with the Euler solver and classifier-free guidance [21] scale of 2. All the experiments are conducted on single NVIDIA H100 GPU. We will release our code and models upon publication. 5.3. Interactive Avatar Evaluation Metrics We evaluate our model across five aspects of interactive avatar generation: Latency, Reactiveness, Motion Richness, Visual Quality, and Lip Synchronization. For Latency, we assume that the pre-extracted audio features and measure motion generation time using 10 NFEs. For Reactiveness, we measure the motion synchronization between the user and the avatar by utilizing the residual Pearson correlation coefficients (rPCC) on facial expression (rPCC-exp) and head pose (rPCC-pose) [10]. For Motion Richness, we measure the Similarity Index for diversity (SID) [39] and variance (Var), following the previous studies [54, 69]. For Visual Quality, we compute the Frechet inception distance (FID) [47] and the Frechet video distance (FVD) [55] for the generated videos. We also assess the identity preservation using the cosine similarity of identity embeddings (CSIM) [11] between the reference avatar image and the generated videos. In Lip Synchronization, we compute the lip sync error distance and confidence (LSE-C and LSE-D) using the generated video and the avatars audio. Please refer to Appendix for details on the metrics. Comparison with Interactive Head Avatar We compare our model with the reproduced state-of-the-art dyadic talking avatar model, INFP* [69], since its official implementation is not publicly available. For reference, we also include talking avatar generation model [27] which does not use user motion or audio. In Tab. 1, we provide quantitative comparison on the RealTalk [16] dataset. Avatar Forcing significantly outperforms INFP* in terms of Reactiveness and Motion Richness, indicating that our generated avatar is much more reactive and expressive compared to the baselines. Notably, 6 Figure 6. Qualitative comparison of interactive head avatar generation models on the RealTalk [16] dataset. Our model generates more reactive (red arrow) and expressive (red box) avatar motion compared to INFP*. We provide the videos in supplementary materials. Avatar Forcing achieves latency of 0.5s, enabling realtime interaction, and maintains Visual Quality and Lip Synchronization comparable to INFP*. In contrast, INFP*s 3.4s latency makes it unsuitable for real-time applications. We visualize the generated samples of ours and INFP* in Fig. 6, where ours demonstrate more reactive (red arrow) and expressive motions (red box), compared to INFP*. As shown in Tab. 2, our model is strongly preferred across all metrics, achieving over 80% preference in overall quality. In particular, our generated avatars exhibit expressive motion and strong non-verbal alignment, showing the effectiveness of the preference optimization (Sec. 4.2). 5.4. Comprehensive Analysis Human Preference Study We conduct human preference study comparing our model with the interactive head avatar generation model INFP*. We recruited 22 participants to evaluate 8 video sets using five perceptual metrics: Reactiveness measures how well the avatar reacts to user motion and audio, Motion Richness evaluates expressiveness of the avatar motion, Verbal Alignment evaluates the lip synchronization with the audio, Non-verbal Alignment assesses the non-verbal behaviors such as eye contact or nodding, and Overall Preference. We provide further details of the human study in the supplementary materials. Comparison with Talking Head Avatar We evaluate the talking capability of the avatar. We compare Avatar Forcing with four state-of-the-art talking head avatar generation models: SadTalker [65], Hallo3 [9], FLOAT [27], and INFP. We measure the visual quality with FID, FVD, and CSIM metrics, and assess lip synchronization using LSED and LSE-C used in Tab. 1. We provide the quantitative results on the HDTF dataset [66] in Tab. 3. Avatar Forcing shows competitive performance on all metrics and achieves the best image and video quality. We provide visual examples in Appendix D. Table 5. Ablation study on user motion and preference optimization. w/ mu indicates whether the user motion latent mu is provided as input to the model during both training and inference. Method Reactiveness Motion Richness w/ mu DPO rPCC-Exp rPCC-Pose SID 0.052 0. 0.003 0.175 0.146 0.036 2.165 2.236 2.442 Var 1.586 1.408 1.734 Comparison with Listening Head Avatar We further evaluate the listening capability of our model. We compare Avatar Forcing with the listening head avatar generation models, including RLHG [67], L2L [39], DIM [54], and INFP*, using the ViCo [67]. We measure Frechet distance (FD) for the expression and pose, and rPCC, SID, and Var metrics used in Tab. 1 but with respect to expression and head pose, respectively. Since the baseline models are not publicly available, we take the results from DIM [54]. In Tab. 4, our model outperforms the baselines on almost all of the metrics. In particular, Avatar Forcing achieves the best useravatar synchronized motion generation (rPCC). 5.5. Ablation Study In this section, we conduct ablation studies on (i) the necessity of using user motion in the Dual Motion Encoder and (ii) the importance of preference optimization. We provide further ablation studies, including the blockwise look-ahead attention mask, in Appendix D. User Motion To validate the necessity of using user motion for interactive avatar generation, we compare our model with variant that does not take user motion as input. As shown in Tab. 5, removing user motion leads to significantly less reactive behavior (Reactiveness) and reduced expressiveness in motion (Motion Richness). Furthermore, as visualized in Fig. 7, the model without user motion produces static behavior whenever the users audio is silent. This occurs even in the presence of strong non-verbal cues, such as smiling, as the model cannot perceive visual signals. In contrast, our model, which uses user motion, generates reactive avatar that naturally smiles right after the user smiles (Fig. 7 red arrow) and becomes more focused when the user speaks (Fig. 7 green box). Direct Preference Optimization To assess the impact of the preference optimization, we compare our model against variant without fine-tuning. As shown in Tab. 5, preference optimization significantly improves the Reactiveness metrics (rPCC-Exp and rPCC-Pose), which quantify useravatar motion synchronization. It also substantially boosts the Motion Richness metrics (SID and Var), indicating more expressive and varied motion. As visualized in Fig. 8, the model without preference optimization generates noticeably reduced diversity in facial Figure 7. Ablation study on the user motion. Without mu, the avatar remains static even when the user smiles. With mu, our model reacts by smiling after the user (red arrow) and shifting to focused expression when the user begins speaking (green box). Figure 8. Ablation study on preference optimization. Model fine-tuned with DPO produces more expressive motion (red box) and reactive (red arrow) compared to model without DPO. expressions and head movement. interaction, failing to respond to the users smile. It also exhibits weaker In contrast, our fine-tuned model generates an expressive avatar that shows natural head movement (Fig. 8 red box) and smiles more broadly along with the user (Fig. 8 red arrow). We provide further analysis of our DPO method in Appendix D. 6. Conclusion We proposed Avatar Forcing, real-time interactive head avatar model based on diffusion forcing, that generates reactive and expressive motion using both the verbal and nonverbal user signals. Avatar Forcing takes step toward truly interactive virtual avatars and opens new possibilities for 8 real-time humanAI communication. We also plan to release our code and pre-trained model weights to advance community development and accelerate future research. Discussion We leave further discussion, including ethical considerations, limitations, and future work, in the Appendix E."
        },
        {
            "title": "References",
            "content": "[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in Neural Information Processing Systems, 33:1244912460, 2020. 6 [2] Philip J. Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleksander Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, Frank Perbet, Guy Scully, Jeremy Shar, Stephen Spencer, Omer Tov, Ruben Villegas, Emma Wang, Jessica Yung, Cip Baetu, Jordi Berbel, David Bridson, Jake Bruce, Gavin Buttimore, Sarah Chakera, Bilva Chandra, Paul Collins, Alex Cullum, Bogdan Damoc, Vibha Dasagi, Maxime Gazeau, Charles Gbadamosi, Woohyun Han, Ed Hirst, Ashyana Kachra, Lucie Kerley, Kristian Kjems, Eva Knoepfel, Vika Koriakin, Jessica Lo, Cong Lu, Zeb Mehring, Alex Moufarek, Henna Nandwani, Valeria Oliveira, Fabio Pardo, Jane Park, Andrew Pierson, Ben Poole, Helen Ran, Tim Salimans, Manuel Sanchez, Igor Saprykin, Amy Shen, Sailesh Sidhwani, Duncan Smith, Joe Stanton, Hamish Tomlinson, Dimple Vijaykumar, Luyu Wang, Piers Wingfield, Nat Wong, Keyang Xu, Christopher Yew, Nick Young, Vadim Zubov, Douglas Eck, Dumitru Erhan, Koray Kavukcuoglu, Demis Hassabis, Zoubin Gharamani, Raia Hadsell, Aäron van den Oord, Inbar Mosseri, Adrian Bolton, Satinder Singh, and Tim Rocktäschel. Genie 3: new frontier for world models. 2025. 2 [3] Antoni Bigata, Rodrigo Mira, Stella Bounareli, Michał Stypułkowski, Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic. Keysync: robust approach for leakagefree lip synchronization in high resolution. arXiv preprint arXiv:2505.00497, 2025. 2 [4] Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d face alignment problem? (and In International dataset of 230,000 3d facial landmarks). Conference on Computer Vision, 2017. 5 [5] Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. In Advances in Neural Information Processing Systems, 2024. 2, 3 [6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 2 [7] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and Chenguang Ma. Echomimic: Lifelike audio-driven portrait In Proanimations through editable landmark conditions. ceedings of the AAAI Conference on Artificial Intelligence, pages 24032410, 2025. 2 [8] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Asian Conference on Computer Vision, 2016. 3 [9] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with video diffusion transformer. In Conference on Computer Vision and Pattern Recognition, 2025. 2, 6, 7, 4 [10] Radek Danˇeˇcek, Michael Black, and Timo Bolkart. Emoca: Emotion driven monocular face capture and animation. In Conference on Computer Vision and Pattern Recognition, 2022. 6, 2 [11] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Conference on Computer Vision and Pattern Recognition, 2019. 6, 3 [12] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set. In Conference on Computer Vision and Pattern Recognition Workshops, 2019. [13] Nikita Drobyshev, Jenya Chelishev, Taras Khakhulin, Aleksei Ivakhnenko, Victor Lempitsky, and Egor Zakharov. Megaportraits: One-shot megapixel neural head avatars. In Association for Computing Machinery International Conference on Multimedia, 2022. 2 [14] Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, and Maja Pantic. Emoportraits: Emotion-enhanced multimodal one-shot head avatars. In Conference on Computer Vision and Pattern Recognition, 2024. 2 [15] Panagiotis Filntisis, George Retsinas, Foivos ParaperasPapantoniou, Athanasios Katsamanis, Anastasios Roussos, and Petros Maragos. Visual speech-aware perceptual 3d facial expression reconstruction from videos. arXiv preprint arXiv:2207.11094, 2022. 5, 2 [16] Scott Geng, Revant Teotia, Purva Tendulkar, Sachit Menon, and Carl Vondrick. Affective faces for goal-driven dyadic communication. arXiv preprint arXiv:2301.10939, 2023. 5, 6, 7 [17] Jiazhi Guan, Zhanwang Zhang, Hang Zhou, Tianshu Hu, Kaisiyuan Wang, Dongliang He, Haocheng Feng, Jingtuo Liu, Errui Ding, Ziwei Liu, et al. Stylesync: High-fidelity generalized and personalized lip sync in style-based generator. In Conference on Computer Vision and Pattern Recognition, 2023. 2 [18] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. 2, 3 [19] Ying Guo, Xi Liu, Cheng Zhen, Pengfei Yan, and Xiaoming Wei. Arig: Autoregressive interactive head generation for real-time conversations. In International Conference on Computer Vision, 2025. 9 [20] Hedra. Hedra realtime avatar. https://hedra.com/ app/avatar, 2025. 1 [21] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3, 6 [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [23] Ailin Huang, Zhewei Huang, and Shuchang Zhou. Perceptual conversational head generation with regularized driver and enhanced renderer. In Association for Computing Machinery International Conference on Multimedia, 2022. 2 [24] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. In Advances in Neural Information Processing Systems, 2025. 2, [25] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng. Loopy: Taming audio-driven portrait avatar with long-term motion dependency. arXiv preprint arXiv:2409.02634, 2024. 2 [26] Taekyung Ki and Dongchan Min. Stylelipsync: Style-based In International personalized lip-sync video generation. Conference on Computer Vision, 2023. 2 [27] Taekyung Ki, Dongchan Min, and Gyeongsu Chae. Float: Generative motion latent flow matching for audio-driven In International Conference on Computer talking portrait. Vision, 2025. 2, 3, 5, 6, 7, 1, 4 [28] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [29] Akio Kodaira, Tingbo Hou, Ji Hou, Masayoshi Tomizuka, and Yue Zhao. Streamdit: Real-time streaming text-to-video generation. arXiv preprint arXiv:2507.03745, 2025. 2 [30] Kai Li, Runxuan Yang, Fuchun Sun, and Xiaolin Hu. Iianet: An intra-and inter-modality attention network for audioIn International Conference on visual speech separation. Machine Learning, 2024. 5 [31] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vecIn Advances in Neural Information Protor quantization. cessing Systems, 2024. 4 [32] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: Rethinking the scaling-up of one-stage conditioned human animation models. arXiv preprint arXiv:2502.01061, 2025. 2 [33] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In International Conference on Learning Representations, 2023. 3, 1 [34] Jin Liu, Xi Wang, Xiaomeng Fu, Yesheng Chai, Cai Yu, Jiao Dai, and Jizhong Han. Mfr-net: Multi-faceted responsive listening head generation via denoising diffusion model. In Association for Computing Machinery International Conference on Multimedia, 2023. 2 [35] Tao Liu, Feilong Chen, Shuai Fan, Chenpeng Du, Qi Chen, Xie Chen, and Kai Yu. Anitalker: animate vivid and diverse talking faces through identity-decoupled facial motion In Proceedings of the 32nd ACM International encoding. Conference on Multimedia, pages 66966705, 2024. [36] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 1 [37] Xi Liu, Ying Guo, Cheng Zhen, Tong Li, Yingying Ao, and Pengfei Yan. Customlistener: Text-guided responsive interaction for user-friendly listening head generation. In Conference on Computer Vision and Pattern Recognition, 2024. 2, 5 [38] Chetwin Low and Weimin Wang. Talkingmachines: Realtime audio-driven facetime-style video via autoregressive diffusion models. arXiv preprint arXiv:2506.03099, 2025. 2 [39] Evonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor Darrell, Angjoo Kanazawa, and Shiry Ginosar. Learning to listen: Modeling non-deterministic dyadic facial motion. In Conference on Computer Vision and Pattern Recognition, 2022. 2, 5, 6, 8, 3, 4 [40] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 2022. 2, 5 [41] William Peebles and Saining Xie. Scalable diffusion models with transformers. In International Conference on Computer Vision, 2023. [42] Pika. Pika audio-driven performance model. https:// pika.art/api, 2025. 1 [43] KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In Association for Computing Machinery International Conference on Multimedia, 2020. 2 [44] PySceneDetect. Pyscenedetect. https://github.com/ Breakthrough/PySceneDetect, 2025. 5 [45] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 2023. 2, 3, [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition, 2022. 2 [47] Maximilian Seitzer. pytorch-fid: FID Score for PyTorch, 2020. Version 0.3.0. 6, 3 [48] Maksim Siniukov, Di Chang, Minh Tran, Hongkun Gong, Ashutosh Chaubey, and Mohammad Soleymani. Ditailistener: Controllable high fidelity listener video generation with diffusion. arXiv preprint arXiv:2504.04010, 2025. 2 [49] Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion. In International Conference on Machine Learning, 2025. 4 [50] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based 10 [64] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In Conference on Computer Vision and Pattern Recognition, 2025. 2, 4, 5 [65] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audiodriven single image talking face animation. In Conference on Computer Vision and Pattern Recognition, 2023. 2, 6, 7, 4 [66] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with highresolution audio-visual dataset. In Conference on Computer Vision and Pattern Recognition, 2021. 6, 7 [67] Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun Zhao, and Tao Mei. Responsive listening head generation: benchmark dataset and baseline. In European Conference on Computer Vision, 2022. 2, 5, 6, 8, 4 [68] Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, and Dingzeyu Li. Makelttalk: speaker-aware talking-head animation. Association for Computing Machinery Transactions on Graphics, 2020. 2 [69] Yongming Zhu, Longhao Zhang, Zhengkun Rong, Tianshu Hu, Shuang Liang, and Zhipeng Ge. Infp: Audio-driven interactive head generation in dyadic conversations. In Conference on Computer Vision and Pattern Recognition, 2025. 2, 3, 4, generative modeling through stochastic differential equaIn International Conference on Learning Representions. tations, 2021. 1 [51] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. 6 [52] Synthesia. Synthesia. https://www.synthesia.io/, 2025. 1 [53] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision, pages 244260, 2024. 2 [54] Minh Tran, Di Chang, Maksim Siniukov, and Mohammad Soleymani. Dim: Dyadic interaction modeling for social beIn European Conference on Computer havior generation. Vision, 2024. 3, 6, 8, 2, 4 [55] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 6, 3 [56] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in Neural Information Processing Systems, 2017. [57] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Conference on Computer Vision and Pattern Recognition, 2024. 2, 3, 5 [58] Yaohui Wang, Di Yang, Francois Bremond, and Antitza image animator: Learning to aniarXiv preprint Dantcheva. mate images via latent space navigation. arXiv:2203.09043, 2022. 2, 1 Latent [59] Yinuo Wang, Yanbo Fan, Xuan Wang, Guo Yu, and Fei Wang. Diffusion-based realistic listening head generation In Conference on Computer via hybrid motion modeling. Vision and Pattern Recognition, 2025. 2, 5 [60] Cong Wei, Bo Sun, Haoyu Ma, Ji Hou, Felix Juefei-Xu, Zecheng He, Xiaoliang Dai, Luxin Zhang, Kunpeng Li, Tingbo Hou, et al. Mocha: Towards movie-grade talking character synthesis. arXiv preprint arXiv:2503.23307, 2025. 2 [61] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time. Advances in Neural Information Processing Systems, 2024. [62] Zunnan Xu, Zhentao Yu, Zixiang Zhou, Jun Zhou, Xiaoyu Jin, Fa-Ting Hong, Xiaozhong Ji, Junwei Zhu, Chengfei Cai, Shiyu Tang, et al. Hunyuanportrait: Implicit condition control for enhanced portrait animation. In Conference on Computer Vision and Pattern Recognition, 2025. [63] Hongwei Yi, Tian Ye, Shitong Shao, Xuancheng Yang, Jiantong Zhao, Hanzhong Guo, Terrance Wang, Qingyu Yin, Zeke Xie, Lei Zhu, et al. Magicinfinite: Generating infinite talking videos with your words and voice. arXiv preprint arXiv:2503.05978, 2025. 2 11 Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation"
        },
        {
            "title": "Supplementary Material",
            "content": "Organization The appendix is organized as follows: In Sec. A, we provide the additional backgrounds of our work. We describe the details of our model architecture in Sec. and the preference optimization method in Sec. C. Experimental details are presented in Sec. D, and further discussion is provided in Sec. E. A. Background Flow Matching Flow matching [33, 36] is generative model that transforms simple prior distribution p0, for example, Gaussian distribution, into the target data distribution p1 via an ordinary differential equation (ODE): dxt dt = vθ(xt, t), [0, 1], (8) where for fixed x0 p0 and x1 p1, the intermediate sample is linear interpolation xt = tx1 + (1 t)x0. The training objective of flow matching is to regress the vector field vθ toward the target vector field vt = x1 x0: LF (θ) = Et,xt [vθ(xt, t) (x1 x0)] . (9) It then generates target samples by solving Eq. (8). Note that flow matching can be interpreted as diffusion model [22, 50] where noise schedule follows the linear trajectory between the prior and the target data. B. Details on Model Architecture In Sec. B.1, we provide more details on the motion latent auto-encoder. In Sec. B.2, we provide more details on the vector field predictor vθ. B.1. Motion Latent Auto-encoder In Fig. 9, we show an overview of the motion latent autoencoder. It encodes an image into latent vector that can be decomposed into an identity representation (i.e., appearance) and the motion representation. This auto-encoder is trained to reconstruct driving image using source image that shares the same identity. During each training iteration, the encoder encodes two images and drawn from the same video clip, and computes the zSD := zS + mD that transforms the source image into the reconstructed ˆD. This explicit decomposition yields compact motion representation, enabling fast motion generation. We train this auto-encoder on our dataset, following the training objective described in the original paper. For more details, including the property of this latent space, training details, please refer to the original papers [27, 58]. Figure 9. Overview of Motion Latent Encoder. It encodes an image into latent vector that has explicit identity-motion decomposition. Figure 10. Detailed architecture for Motion Generator in vθ. B.2. Vector Field Model vθ Model Architecture The model vθ comprises two main components: the Dual Motion Encoder and the Causal DFoT Motion Generator. The Dual Motion Encoder unifies three multimodal inputs through cross-attention layer, (cid:17) computed as softmax , where (cid:16) QK = qWq, = kWk, = vWv, (10) and Wq, Wk, Wv Rdd are learnable projection ma1 trices for the query q, key k, and value RN d, respectively (N is the number of latents). In the first crossattention layer, the encoder captures holistic verbal and nonverbal user motion by using the user motion latent mu as the query. In the second layer, it integrates this aligned user motion with the avatars audio by taking the avatar audio as the query. We use four attention heads, each with hidden dimension of = 512, for both cross-attention layers. In Fig. 10, we provide detailed architecture for the motion generator. It consists of eight DFoT transformer blocks followed by transformer head. Specifically, in each DFoT block, noisy latents (m1 , m2 ) are modulated t2 t1 by the flow time = (t1, t2, , tN ) through shared AdaLN scale-shift coefficients (ToTimeEmb layer) [6]. , , mN tN For the attention modules, we use Blockwise Causal Look-ahead Mask (Eq. (5)) in self-attention and Slidingwindow Attention Mask for aligning the driving signal c1:N to the noisy latents. Specifically, we introduce the Blockwise Casual Look-ahead Mask to ensure the causal motion generation in our motion latent space, which significantly improves the temporal consistency of the generated video, as demonstrated in Sec. D.5. Unlike the recent video diffusion models that employ spatio-temporal compression module [24, 64] where each latent correlates to multiple video frames by the compression rate (e.g., 4 or 8), our motion latent has one-to-one correspondence with each frame in pixel space. Under this setting, simple (blockwise) causal mask alone produces the temporal inconsistencies across the frames or blocks. C. Details on Preference Optimization Training Objective Formulation Inspired by DiffusionDPO [57], we formulate the training objective LDP in the context of diffusion forcing [5]. Let (ml, mw) denote pair of less-preferred and preferred motion latents, each consisting of frames, where ml := (ml,n)N n=1 and mw := (mw,n)N n=1. Following the per-token independent noising process of diffusion forcing, we construct the noisy latent pairs as: mw,n tn ml,n tn := tnmw,n + (1 tn)mn 0 , := tnml,n + (1 tn)mn 0 , (11) where [1, ] is the frame index, tn [0, 1] is the nn=1 RN is the noise th flow time, and m0 := (mn sequence. With these notations, we formulate LDP as 0 )N LDP O(θ) = En,tn,cn,(mw,n,ml,n) (cid:16) log σ tn vθ(mw,n tn , tn, cn) β(cid:2)vw,n tn vref(mw,n tn vθ(ml,n tn vref(ml,n tn , tn, cn) tn , tn, cn) tn , tn, cn))(cid:3)(cid:17) , vw,n (vl,n vl,n (12) 2 where cn is the n-th unified condition, vref is the reference vector field model, the target vector fields for less-preferred and preferred samples are given by vl,n tn := ml,n mn 0 and vw,n tn := mw,n mn 0 . (13) D. Experimental Details D.1. Training Details We train the vector field model vθ in Eq. (6) for 2000k steps while freezing the motion latent auto-encoder. Through the training, we use L1 distance for . For fine-tuning the model using the proposed preference optimization method in Eq. (7), we set the balancing coefficient to λ = 0.1 and the deviation parameter to β = 1000. We initialize the reference model vref with the same weights as the trained vθ. We fine-tune vθ for 5k steps and observe that additional tuning does not yield further performance gains. D.2. Baseline Implementation One major challenge of evaluating an interactive head avatar model is the absence of official implementation of baseline methods. To bridge this gap, we reproduce INFP [69] on the motion latent space of [27], following its core module, Motion Guider and denote the reproduced model as INFP*. Based on the description in the original paper, we adopt bidirectional Transformer encoder for motion generation, where single window consists of = 75 frames and additional 10 frames serve as context frames. For Motion Guider, we set = 64 for both verbal and non-verbal motion memory banks. We train INFP* on our dataset for 2000k steps. D.3. Evaluation Metrics In this section, we provide more details on the evaluation metrics used in the main paper. Reactiveness and Motion Richness Reactiveness and Motion Richness are computed using the EMOCAbased [10] 3D morphable models (3DMMs) that model the facial dynamics via 50-dim expression parameters and 6dim pose parameters. We extract those parameters using an off-the-shelf 3DMM extractor, SPECTRE [15], for each video frame. Let us denote RN as the ground-truth user parameters, RN as the ground-truth avatar parameters, and ˆy RN as the generated avatar parameters. is the number of frames and is the feature dimension. As reported in Tab. 1 and Tab. 4, we can compute rPCC, SID, Var, and FD for expression and pose, respectively. rPCC (residual Pearson Correlation Coefficients) [54] is to measure the motion synchronization between the Table 6. Ablation study with additional metrics on user motion and preference optimization. w/ mu indicates whether the user motion latent mu is provided as input to the model during both training and inference. Method Reactiveness Motion Richness Visual Quality Lip Synchronization w/ mu DPO rPCC-Exp rPCC-Pose SID 0.052 0.042 0. 0.175 0.146 0.036 2.165 2.236 2.442 Var 1.586 1. 1.734 FID FVD CSIM LSE-D LSE-C 28.746 25.600 185.593 175. 24.328 170.874 0.818 0.854 0.833 8.260 8.160 8. 6.423 6.803 6.723 user parameters and avatar parameters. Specifically, L1 distance is used to measure the discrepancy between generated PCC and ground-truth PCC where we define PCC as function of RN given ground-truth user parameters x: PCC(zx) = (cid:80)(zi z)(xi x) (cid:112)(cid:80)(zi z)2 (cid:80)(xi x)2 , (14) where [0, ] is the frame index, and and denote the mean of and y, respectively. Based on this notation, we can define rPCC as PCC(yx) PCC(ˆyx). SID (Shannon Index for Diversity) [39] is to measure the motion diversity of the generated avatars using K-means clustering on 3DMM parameters. Following [39], we compute the average entropy (Shannon index) of the clusters with = 15, 9 for expression and pose, respectively. Var is the variance of the parameters from generated avatars, which is computed along the time axis and then averaged along the feature axis. FD (Frechet Distance) [47] measures the distance between the expression and pose distributions of the generated avatars and the ground truth by calculating µˆy µy + tr(Σˆy + Σy 2(ΣˆyΣy) 1 2 ), (15) where µ and Σ are the mean and the covariance matrix, respectively. Visual Quality We utilize FID [47] and FVD [55] to assess the image and video quality of the generated avatars, and CSIM [11] to measure the identity preservation performance of avatar generation models. FID (Frechet Inception Distance) measures the quality of the generated frames by comparing the distribution of image features extracted from pre-trained feature extractor [47]. The FD computation in Eq. (15) is adopted using the extracted image features. FVD (Frechet Video Distance) quantifies the spatiotemporal quality of the generated videos by comparing the feature distributions of real and generated videos in learned video feature space [55]. It reflects both framewise quality and temporal consistency. The FD computation in Eq. (15) is adopted using the extracted video features. CSIM (Cosine Similarity for Identity Embedding) evaluates identity preservation by computing cosine similarity between the facial embeddings from the generated and the source image, extracted using ArcFace [11]. Lip Synchronization We compute LSE-D and LSE-C [8] to assess the alignment between the generated lip motion and the corresponding audio. LSE-D and LSE-C (Lip Sync Error Distance and Confidence): Both metrics are derived from pre-trained SyncNet-based audiovisual synchronization model. LSE-D measures the distance between the audio and lip embeddings, where lower values indicate better synchronization. LSE-C measures the confidence score of synchronized audiovisual pairs, where higher values indicate more accurate lip-audio alignment. D.4. Human Evaluation In Fig. 12, we show the interface used for our human evaluation. To improve the evaluation consistency, we additionally provided participants with reference test and answer sheet. We asked 22 participants to compare 8 videos based on 5 evaluation metrics and indicate their preference. We also provide video test sheet. Please refer to 04_human_evaluation_XX.mp4. Figure 11. Attention Mask Comparison. (Left) framewise causal mask; (Middle) blockwise causal mask; (Right) blockwise causal look-ahead mask (Ours). D.5. Additional Ablation Studies In Tab. 6, we present model with additional metrics, the ablation studies on our including Visual Qual3 ity and Lip Synchronization. We provide video results of to the videos 02_ablation_wo_user_motion.mp4 and 02_ablation_wo_DPO.mp4. the ablation study."
        },
        {
            "title": "Please refer",
            "content": "Moreover, we provide video ablation results on the attention mask, where each masking method is illustrated in Fig. 11. The motion jittering observed when using only the blockwise causal mask is clearly visible in the video results, yet difficult to capture with quantitative metrics. We highly recommend watching the ablation video 02_ablation_attention_mask_XX.mp4. D.6. Supplementary Visual Results latent space, which enables natural and expressive interactions. However, this design limits the modeling of richer bodily cues, such as hand gestures, that contribute to more dynamic communication. Moreover, while our model captures user-driven conversational cues via the motion latents, certain scenarios may require more explicit controllability, such as directing eye gaze or emphasizing emotional shifts. We believe that incorporating additional user signals, including eyetracking or emotion-tracking inputs, can address these limitations. Since our framework has no architecture constraints on adding new conditions, such signals can be incorporated into future extensions of our system. to further Comparison with Interactive Head Avatar We provide the video results the vito 01_intersual results in Fig. 6. active_avatar_comparison_XX.mp4. We also provide video comparison results using the DEMO videos Please refer to 01_interacof Official INFP [69]. tive_avatar_comparison_demo.mp"
        },
        {
            "title": "Please refer",
            "content": "support Comparison with Talking Head Avatar In Fig. 13, we compare our model with SadTalker [65], Hallo3 [9], FLOAT [27], and INFP* [69] for talking head avatar generation bu dropping the user condition at inference. Avatar Forcing can generate competitive results compared to stateof-the-art models, while our model successfully reflects user signals. We also provide the video comparison results. Please refer to 03_talking_XX.mp4. Comparison with Listening Head Avatar In Fig. 14, we compare our model with RLHG [67], L2L [39], DIM [54], and INFP* [69] for listening head avatar generation. Avatar Forcing can generate competitive results with more exPlease refer to 03_listenpressive facial expression. ing_XX.mp4 for video results. E. Discussion Ethical Consideration Our method can generate more engaging and interactive head-avatar videos, broadening positive applications such as virtual avatar chat, virtual education, and other communication tools by providing users with more immersive experience. However, realistic interactive head avatar videos also pose risks of misuse, including identity spoofing or malicious deepfakes. Adding watermarks to generated videos and applying restricted license can help mitigate these risks. We also encourage the community to use our generated data to train deepfake detection models. Limitation and Future Work Our system focuses on modeling interactive conversations through head-motion 4 Figure 12. Human evaluation interface. (Left) Instructions for human evaluation; (Middle) reference sheet for consistent evaluation; (Right) Test and answer sheet. Figure 13. Qualitative comparison on talking head avatar generation. 5 Figure 14. Qualitative comparison on listening head avatar generation."
        }
    ],
    "affiliations": [
        "DeepAuto.ai",
        "KAIST",
        "NTU Singapore"
    ]
}