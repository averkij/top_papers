{
    "paper_title": "EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing",
    "authors": [
        "Yehonathan Litman",
        "Shikun Liu",
        "Dario Seyb",
        "Nicholas Milef",
        "Yang Zhou",
        "Carl Marshall",
        "Shubham Tulsiani",
        "Caleb Leak"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 1 ] . [ 1 1 3 0 5 1 . 2 0 6 2 : r EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing Yehonathan Litman1,2, Shubham Tulsiani2, Caleb Leak1 1Meta Reality Labs, 2Carnegie Mellon University, 3Meta AI Work done at Meta , Shikun Liu3, Dario Seyb1, Nicholas Milef1, Yang Zhou1, Carl Marshall1, High-fidelity generative video editing has seen significant quality improvements by leveraging pretrained video foundation models. However, their computational cost is major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting masks size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features novel local video context module that operates solely on masked tokens, yielding computational cost proportional to the edit size. This local-first generation is then guided by lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation. Date: February 17, 2026 Correspondence: litman@cmu.edu Website: https://yehonathanlitman.github.io/edit_ctrl"
        },
        {
            "title": "1 Introduction",
            "content": "Generative video editing is rapidly advancing field in media content creation, with potential applications ranging from professional film production to real-time augmented reality. fundamental and challenging task within this domain is video inpainting: the process of replacing arbitrary regions of video with highfidelity, contextually consistent content. Although data-driven feed-forward approaches have shown success in simple content removal (e.g., object removal, background filling, deblurring) (Zhou et al., 2023; Liu et al., 2021; Li et al., 2022; Zhang et al., 2024a, 2022), they often struggle with generative tasks such as adding entirely new objects or large-scale scene replacements. The advancement of large-scale text-to-video diffusion models (Wang et al., 2025; HaCohen et al., 2024; Kong et al., 2024; Polyak et al., 2024) has pushed the boundaries of generation and, in turn, enabled follow-up work on high-quality, semantically-aware video inpainting applications (Ma et al., 2025; Huang et al., 2025a; Zhang et al., 2024c; Liu et al., 2025b; Lee et al., 2025; Yang et al., 2025). However, video generation incurs significant computational costs, which makes current state-of-the-art inpainting methods (Bian et al., 2025; Jiang et al., 2025; Li et al., 2025) and applications inefficient. This is due to processing the entire spatiotemporal context of the video, regardless of whether an edit is needed in given region. This dense, full-attention approach is profoundly inefficient, making it prohibitive for real-world applications that need fast inference, such as real-time augmented reality, editing high-resolution videos, or applying multiple distinct edits simultaneously. In this paper, we address this critical efficiency bottleneck and propose novel framework, EditCtrl, that decouples localized sparse inpainting generation from the global video context, enabling high-fidelity generation results at fraction of the computational cost. Instead of processing the entire video, our approach strategically allocates compute only to the pixels that require editing. Our approach achieves this with two key components: i) sparse local context module that operates exclusively on tokens within the target edit masks; and ii) lightweight temporal global context embedder that captures video-wide coherence and guides 1 Figure 1 EditCtrl: Real-time Generative Video Editing Pipeline. EditCtrl supports complex, prompt-guided edits on 4K videos, simultaneously handling an arbitrary number of user-defined masks (Top). To maintain real-time performance, our inference pipeline dynamically allocates compute proportional to the edit mask size (Middle). EditCtrl also intelligently propagates object edits from initial frames into the future (after the orange line), ensuring high temporal and object consistency in the resulting edit (Bottom). the local generation process. This design yields several transformative advantages. First, the computational cost becomes proportional to the size of the target edit mask, independent of the input video resolution. Second, our adapter modules are added without updating the model weights of the base diffusion model, preserving its powerful generative priors. This design, unlike previous methods that fine-tune the base video diffusion model, makes EditCtrl natively compatible with distilled autoregressive models for content propagation, further accelerating inference latencies. We demonstrate the effectiveness of our approach qualitatively and quantitatively, showing that it matches the performance of its full-attention base model and outperforms other state-of-the-art methods. Finally, we highlight the practical utility of our model by showcasing its applications in demanding scenarios, including fast arbitrary-resolution video editing, simultaneous multi-region inpainting, and real-time augmented reality content editing and propagation."
        },
        {
            "title": "2 Related Works",
            "content": "Generative Video Inpainting Early data-driven video inpainting methods used spatiotemporal transformers or convolutional networks to attend to the global context of the video while propagating information across frames using optical flow (Zhou et al., 2023; Li et al., 2022; Zhang et al., 2024a, 2022). These methods efficiently inpaint target regions, but are limited in their ability to handle complex motion, masked regions, or generate coherent semantic content given condition like text prompt. To increase video inpainting quality and diversity, generative-based methods repurposed powerful priors in the form of video diffusion models (Bian et al., 2025; Jiang et al., 2025; Li et al., 2025; Liu et al., 2025b; Samuel et al., 2025; Ma et al., 2025; Lee et al., 2 2025). While generative-based models inpaint high quality content coherently, they take both the entire video as context and per-pixel mask indicating whether to preserve or generate each pixel. This approach is inefficient for interactive editing since the underlying diffusion model needs to repeatedly generate pixels outside the local masked edit area. Furthermore, it ties mask information to the video context, preventing generative inpainting from making multiple distinct edits at different regions or propagating inpainting information when video frames are not available in tasks like image-to-video generation. Accelerating Video Generation To accelerate video editing, straightforward approach is to reduce the number of diffusion sampling steps the base video diffusion model needs to produce coherent outputs. This includes knowledge distillation (Yin et al., 2024, 2025; Luo et al., 2023; Heek et al., 2024), sparse attention (Liu et al., 2025a; Zhang et al., 2025b; Shin et al., 2025; Zhang et al., 2025a), or linear transformers (Xie et al., 2025; Chen et al., 2025). This can be naively applied to the base video diffusion model used for generative editing, accelerating editing as result. However, this only reduces the number of underlying diffusion steps and not the data processed for content editing in terms of the local and global context sizes. As such, runtime and memory scale with the input video resolution regardless of the target edit area. Token merging based approaches (Bolya et al., 2023; Li et al., 2024; Kahatapitiya et al., 2024) address this by pruning tokens outside the local context, though this leads to significant quality degradation and may actually slow inference due to the token importance calculation step. Some image generation methods compress the global context together with the local context (Nitzan et al., 2024; Beyer et al., 2025), but this requires training specialized generative models that can take the new information, which is expensive and not competitive with the quality of pretrained full-attention diffusion models. This design also prevents integration with additional control variants without modifying the base model weights. Overall, methods that accelerate the base video diffusion model do not target the bottleneck of local-global context for editing and must process the entire video. Alternatively, token-merging and compression approaches are destructive, and runtime may not cleanly scale with the size of the edit. For interactive editing, the computational cost must be decoupled from the global context of the video and thus the total video resolution. Controlled Video Generation As video editing is task that requires spatiotemporal control, the local and global context can be considered as high-frequency and low-frequency control components, respectively. Recent work in controllable generation has shown success in guiding pretrained image and video models using external signals (Gu et al., 2025; Zhang et al., 2024b; Zhang and Agrawala, 2023). These signals include low-frequency ones, such as human pose parameters (Zhou et al., 2025; Hu et al., 2024) and camera parameters (Wang et al., 2024; He et al., 2024; Bahmani et al., 2025; Bai et al., 2025), or high-frequency ones such as segmentation and depth maps (Cai et al., 2024; NVIDIA et al., 2025; HaCohen et al., 2024; Esser et al., 2023) as well as environmental illumination and intrinsic decomposition (Litman et al., 2025a; Liang et al., 2025; He et al., 2025; Litman et al., 2025b), allowing fine-grained control over generated content. In addition, overlaying multiple low and high-frequency control signals causes guidance to become multimodal task (NVIDIA et al., 2025; Ruan et al., 2023; Corona et al., 2025), but this has not been explored for accelerating generation given multiple control inputs. EditCtrl separates the low and high frequency components of the mask, which correspond to the global and sparse local contexts, to significantly accelerate video editing while matching the quality when using the full global context, making it invariant to video resolution and enabling fast editing. Our approach naturally extends to interactive editing capabilities such as multi-prompt editing for different regions and content propagation, which are challenging for non-disentangled frameworks that rely on full-attention."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we introduce our video editing framework EditCtrl  (Fig. 2)  , which accepts an input video, corresponding masked video, and text prompt to produce the edited video with computational saving proportional to the number of unmasked pixels. We first outline our notation and review the preliminaries of video diffusion (Sec. 3.1). We then describe the frameworks foundation and how it is derived from base inpainting control module (Sec. 3.2). Finally, we show how our approach can be adapted at inference time for interactive tasks such as multi-region inpainting and also real-time video editing and propagation for 3 Figure 2 EditCtrl Video Diffusion Framework Overview. EditCtrl edits source video given target edit mask. Foreground content is masked out, giving the background video that is also down-sampled to constant resolution regardless of the original resolution. The compact global context of the down-sampled background video and the local context at the mask edit region are then encoded. These are given to trainable local and global adapters inside pretrained text-to-video diffusion model that denoises tokens zt only in the masked edit region given text prompt. After diffusion, the tokens are scattered into the masked edit region in the encoded source video latents. Our method shows proportional speedup with respect to the target mask area ratio. augmented reality (Sec. 3.3)."
        },
        {
            "title": "3.1 Preliminaries and Notation",
            "content": "Text-to-video diffusion models are primarily structured as latent diffusion models using powerful diffusion transformer (DiT) backbone ϵθ to generate visually appealing, temporally consistent content given text prompt p. The model employs pretrained variational autoencoder (VAE) consisting of an encoder to encode videos to smaller latent space and reduce computational cost. At training, and decoder W, defined as sequence of frames of size encodes source video Vsrc = timestep 1, diffusion process: W, into latent space: (Vsrc). sampled noise ϵt is added to the encoded latent using scheduler (Ho et al., 2020) with . ϵθ takes the produced noisy video latent zt to predict the noise ϵt in the forward { RF , } LDM = ϵθ (cid:0)zt, t; p(cid:1) 2 2, ϵt (1) To make use of the generative prior knowledge in pretrained text-to-video diffusion models for video editing, we train conditional model cϕ to encode additional conditional information and steer the predictions from corresponding to target edit regions. These are ϵθ. is derived from editing mask frames Vm that is encoded used to mask the foreground in the input video, giving the background video Vb (Vb), Vm), where Vm are the . is constructed as channel-wise concatenation of the information ( with editing masks that are down-sampled to the latent resolution, which gives our conditional diffusion process: RF RF LCDM = ϵθ (cid:0)zt, t; p, cϕ(C)(cid:1) 2 2, ϵt (2)"
        },
        {
            "title": "3.2 Disentangled Editing Architecture Design",
            "content": "Our goal is to design an architecture that generates video content in desired region while efficiently incorporating global temporal context from the rest of the input video. EditCtrl builds on ControlNet-like architecture (Zhang and Agrawala, 2023), with trainable context control module used to guide frozen, pre-trained video diffusion model (HaCohen et al., 2024; Jiang et al., 2025; NVIDIA et al., 2025). This context module is trained for full-frame inpainting with bi-directional full-attention, processing the entire video Vsrc and editing masks Vm, making it computationally expensive regardless of the edit size. 4 Figure 3 EditCtrl: Local and Global Control Modules. Given the source video Vsrc and target edit masks Vm, we extract the background content Vb and encode it with video VAE encoder . This is then concatenated channel-wise with the down-sampled masks to give the control context C. Tokens in outside the down-sampled edit mask region are then masked out, giving the local context tokens Clocal which go to the local encoder module cϕ, whose outputs are added to selected transformer layers. The global embedder Gψ receives the query feature tokens and global context tokens produced from the down-sampled background content Vb and modulates the noisy cross-attended features. Our key architectural insight is to disentangle this process. We modify the framework by introducing two control adapters: i) local context adapter module re-purposed from the original control by fine-tuning it to operate only on tokens within the masked region (Sec. 3.2.1); and ii) new, lightweight temporal global context embedder that efficiently encodes the full video context to maintain global coherence (Sec. 3.2.2). This design preserves the base models high-quality generative capabilities while making the computational cost of editing proportional to the target area size, not the full video resolution. 3.2.1 Local Context Encoder To repurpose the inpainting control module for local context editing, we use Vm as an attention mask for the tokens corresponding to the foreground. Background tokens are selected with Vm to produce Clocal. Masking out all the background tokens results in poor blending, so Vm is dilated to neighboring pixels before selection (Liu et al., 2021). This attention mask operation is also applied to zt, and the output from the control module is summed element-wise with the feed-forward network output after selected video transformer layers. By processing only the corresponding local tokens through the video diffusion model and the control module, the diffusion process is proportionally accelerated. However, generation quality is much worse as the pretrained control module was trained with full-attention and not sparse attention patterns corresponding to the target mask region. Thus, the module is fine-tuned into local context encoder using mask-aware diffusion loss Lϕ = ϵθ (cid:0)zt, t; p, cϕ(Clocal)(cid:1) ϵt Vm 2 2, (3) The fine-tuned local context encoder enables high-quality local edit generation without access to the full spatiotemporal context. At inference, z0 is scattered into (Vsrc) for the final edited video output. 3.2.2 Global Context Embedder The local context encoder could generate high-fidelity content from local context tokens alone, but it suffers from having no access to the surrounding context. Without the global temporal context of the video, the local encoder cannot integrate important structure into the generation, such as appearance and scene cues (e.g., 5 lighting, structure, dynamics, camera motion). To ensure that local edits are consistent with the full video context, we insert lightweight global context embedder module Gψ into the base video model to integrate global information during diffusion. Cross Attention Modulation. To incorporate global information at generation, we enrich the cross-attended features produced with the video DiT by injecting temporally-aware attention features as shown in Fig. 3. This is similar to using CLIP for image-guided generation, but in our case we use temporal global context (Vb ), denoted as zb. Vb is spatially down-sampled to representation computed using the VAE encoder 256 regardless of the original resolution to give Vb . This is to increase robustness to aspect resolution of 256 ratio and the number of frames we can encode. zb is then processed by trainable patch layer to produce the global context token embeddings, efficiently capturing video-wide temporal evolution and high level scene cues while remaining invariant to the original video resolution. Gψ takes the global context embeddings to calculate the attention weights of query tokens with the global feature keys and values Kg, Vg. These are added to the features after cross-attending to the text prompt = + W0 Attention(Q, Kg, Vg) (4) where W0 is zero initialized linear layer. This attention operation enables minimal yet sufficient control over the text prompt embedding, enriching the features without degrading the information from the text embedding. Gψ thus steers the generation of local token features to ensure they align with the text prompt and high level global video context with minimal compute overhead. We define the mask-aware loss function with zb as Lψ = ϵθ (cid:0)zt, t; p, Gψ(zb), cϕ(Clocal)(cid:1) ϵt Vm 2 2, (5) Lϕ and Lψ from the start makes training unstable, however, as cϕ learns to generate local content Using both given the text prompt and Gψ complicates that by modifying cross-attended features. Conversely, Gψ is not able to better guide the generation since cϕ gives poor predictions at the beginning of training. We use piecewise training loss function defined as combination of Lϕ and Lψ = Lϕ, Lψ, if < n, if n. (6) where is the training iteration and is the predefined number of training iterations."
        },
        {
            "title": "3.3 Interactive Editing",
            "content": "The disentangled nature of EditCtrl enables efficient, flexible video editing that scales with the masked region rather than full video resolution. This enables several interactive applications, shown in Fig. 1, which full-attention methods do not naturally lend themselves to. First, as computational cost is independent of the overall video size, EditCtrl supports efficient editing of very high-resolution videos. Moreover, editing content only within the target area allows the framework to handle multiple, distinct edits simultaneously by batch-processing individual regions and merging the results into their respective locations in the output latents. This is not possible with full-attention control module that uses the full binary video mask. Finally, our approach also facilitates real-time content propagation where future frames are unavailable. EditCtrl can be deployed together with an autoregressive video diffusion model (Huang et al., 2025b) to propagate edited content into the future and place it in frames when they are acquired. The high frame rate of videos means the global context does not change much, and simply padding the initial frames of Vb with themselves provides sufficient global context, in addition to using the last frames propagated background for local context. This enhances real-time video editing and eliminates latency for applications such as augmented reality, since we are generating and propagating into the future. Additional details and results are provided in the Appendix. 6 (a) Change the flower color to blue. (b) Remove the railing. Figure 4 Video Editing Comparison. EditCtrl generates visually appealing and structurally coherent edited content while the baselines either fail to edit the video correctly or produce content with poor appearance and blending. EditCtrls localized editing greatly increases efficiency and enables real-time generative editing. Masked Region Preservation Text Alignment Temp. Coherence Throughput Method #Par. PFLOPS ReVideo (Mou et al., 2024) 1.5B VideoPainter (Bian et al., 2025) 5B 1.3B VACE (Jiang et al., 2025) 14B VACE (Jiang et al., 2025) EditCtrl EditCtrl 1.5B 16B 193.39 817.81 76.31 589.19 17.42 124.53 PSNR 15.52 22.63 23.84 24.02 24.16 24. SSIM LPIPS 102 MSE 102 MAE 0.49 0.91 0.91 0.92 0.92 0.93 27.68 7.65 5.44 5.13 5.54 5. 3.49 1.02 0.92 0.84 0.99 0.80 11.14 2.90 2.78 2.68 3.01 2.65 CLIP 9.34 8.67 9.76 9. 9.58 9.46 CLIP (M) CLIP Sim 20.01 20.20 21.51 21. 21.70 21.73 0.42 0.18 0.13 0.13 0.15 0.13 FPS 0.11 0.12 0.66 0.10 4.67 1. Table 1 Video Editing Comparison on VPBench-Edit. EditCtrl outperforms editing baselines as well as the full attention base model in edited video quality, background preservation, and content alignment with the text prompt, all at much higher throughput and lower computational cost (PFLOPS). In each column, the best , second best , and third best results are marked."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate on diverse video datasets labeled with target edit masks and detailed prompts to showcase EditCtrls performance and efficiency in video editing tasks. We compare against previous video editing and inpainting baselines. We showcase EditCtrls editing and inpainting performance, compute efficiency, and generalization with complex prompts, masks, and video content. We further provide ablations on our method to demonstrate how the local encoder and global embedder contribute to generative editing quality while reducing compute."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Datasets. For training the local encoder and global embedder adapters, we used an internal dataset of diverse stock footage videos with segmentation annotations and descriptive text captions. Similar to VACE (Jiang et al., 2025), EditCtrl was trained on videos annotated with text captions, and the edit instructions in the figures are abstractions of translated target captions. We evaluate EditCtrl on the VPBench-Edit test set and the DAVIS and VPBench-Inp video inpainting test sets (Bian et al., 2025; Pont-Tuset et al., 2017). The masks are randomly augmented during training (Suvorov et al., 2021) to enable editing with diverse unstructured edit masks. This allows for inpainting on randomly shaped and placed masks, such as those in DAVIS, and structured segmentation maps found in VPBench. Both test datasets are publicly available. 7 Figure 5 Video Inpainting Comparison. Even with full-attention, baseline methods struggle to inpaint content that is coherent and visually appealing, while our method successfully generates high fidelity content that aligns with the scene using much less compute. Method ProPainter (Zhou et al., 2023) VideoPainter (Bian et al., 2025) 5B VACE (Jiang et al., 2025) VACE (Jiang et al., 2025) #Par. PSNR 50M 20.97 23.32 1.3B 22.62 14B 23.03 EditCtrl EditCtrl 1.3B 23.17 14B 23.60 ProPainter (Zhou et al., 2023) VideoPainter (Bian et al., 2025) 5B VACE (Jiang et al., 2025) VACE (Jiang et al., 2025) 50M 23.99 25.27 1.3B 25.75 14B 26.12 EditCtrl EditCtrl 1.5B 25.44 16B 25.89 Masked Region Preservation Text Alignment Temp. Coherence Throughput SSIM LPIPS MSE 102 MAE 102 0.87 0.89 0.85 0. 0.86 0.88 0.92 0.94 0.88 0.91 0.86 0.90 9.89 6.85 9.30 7.65 8.52 8.23 5.86 4.29 5.11 4. 5.31 5.25 1.24 0.82 1.25 0.98 1.29 1.11 0.98 0.45 0.34 0.33 0.39 0.34 3.56 2.62 4.43 3. 3.91 3.63 2.48 1.41 2.03 2.01 2.01 1.99 CLIP 7.31 8.66 9.77 9.27 9.81 9. 7.54 7.21 7.27 7.81 7.33 7.78 CLIP (M) CLIP Sim 17.18 21.49 21.55 22.18 21.86 21.96 16.69 18.46 17.83 18.75 18.02 18.50 0.44 0.15 0.17 0.14 0.17 0. 0.12 0.09 0.10 0.09 0.12 0.10 FPS 5.34 0.12 0.66 0.10 5.24 1.30 5.51 0.12 0.66 0. 5.57 1.41 Table 2 Video Inpainting Comparison. EditCtrl matches or outperforms inpainting baselines for the VPBench-Inp (top) and DAVIS (bottom) test sets while significantly increasing inpainting efficiency. Fine-tuning Details. We initialize the local encoder module with the weights from VACE (Jiang et al., 2025), which was trained for video editing using full-attention. We use Low-Rank Adaptation (LoRA) (Hu et al., 2022) to fine-tune the local encoder with rank 128 using the loss described in Eq. 6. We fine-tune small and large version of the local encoder consisting of 1.3 billion and 14 billion parameters, respectively. The global embedder module was randomly initialized, with the zero convolution layers weights and biases set to zero at the start of training. The global token patch layers weights were copied at the start of training from the video DiT token patch layer. EditCtrl was trained on 8 NVIDIA A100 GPUs for about 1 day with batch of 8 videos, gradient accumulation of 8, and learning rate 1e 5 with AdamW (Loshchilov and Hutter, 2019) and warmup. Each video in the batch was sampled to 49 frames, and both the frames and masks were down-sampled to 480 720. At training and inference, the edit mask is used to set the corresponding area in Vsrc to 0.5 to give Vb. Evaluation Metrics. We evaluate editing and inpainting performance across multiple metrics for the quality of the edit, preservation of regions not meant for editing, and throughput. We measure the masked region preservation (i.e., the background context where no edit is to be done) by comparing it to the groundtruth input and computing the PSNR, SSIM, LPIPS, MSE, and MAE. The semantic alignment between the text 8 Figure 6 Local and Global Adapter Ablation. Removing adapter components harms video editing quality, but together they let EditCtrl perform comparably to method operating with full-attention. prompt and the edited video is measured using the average CLIP score across the full and masked video frames (Radford et al., 2021; Hessel et al., 2021). We follow VideoPainters (Bian et al., 2025) procedure for computing temporal coherence and perceptual quality as the average CLIP similarity across all neighboring frames. Finally, we report the inference throughput in terms of frames per second (FPS) on an NVIDIA A6000Ada, excluding VAE encoding and decoding times. We use 25 DDPM iterations at inference for evaluating VACE and EditCtrl."
        },
        {
            "title": "4.2 Video Editing & Inpainting",
            "content": "To validate EditCtrls video editing and inpainting approach, we report the average results for the metrics across all videos in the test datasets containing 45 and 150 6-second videos for editing and inpainting, respectively. Baselines. We compare EditCtrl against several state-of-the-art generative and non-generative methods (Zhou et al., 2023; Mou et al., 2024; Bian et al., 2025) as well as VACE (Jiang et al., 2025) which uses full-attention. The generative methods operate as text-to-video methods except for ReVideo which requires the first frame to be edited. This is provided via an image inpainting backbone (Labs, 2024). Results. As shown in Tab. 1 and Tab. 2, EditCtrl generates edited and inpainted videos that are better aligned with the prompt, better preserve the background, and at greatly reduced compute load with more than 50 speedup over generative baselines. Moreover, our trained models exhibit proportional acceleration over the full-attention baseline while matching or even slightly exceeding editing and inpainting quality as well as prompt alignment. Fig. 4 and Fig. 5 show qualitative comparison examples of EditCtrls small model video editing and inpainting results against the baselines, respectively. The strong performance of our decoupled framework in both editing and inpainting demonstrates its effectiveness as general-purpose, high quality and efficient video editing solution. 9 Masked Region Pres. Text Alignment Temp. Coherence Throughput Method PSNR 23.84 VACE Ours (Naive) 23.24 Ours (No Gψ) 23.80 24.16 Ours SSIM 0.91 0.86 0.90 0.92 LPIPS 102 5.44 6.96 5.74 5.54 CLIP 9.76 8.83 9.41 9. CLIP (M) CLIP Sim 21.51 20.49 21.28 21.70 0.13 0.19 0.15 0. FPS 0.10 4.90 4.90 4.67 Table 3 Quantitative Local and Global Adapter Ablation Comparison. We observe clear improvement in quality at minimal reduction in efficiency with the local and global control modules. With both adapters, EditCtrl greatly increases throughput and even exceeds the full-attention model in edit quality."
        },
        {
            "title": "4.3 Interactive Editing",
            "content": "While EditCtrl enables fast video editing in target regions, it also lends itself to additional interactive applications such as editing multiple regions with distinct text prompts and real-time content propagation, which we show in Fig. 1. We show more results for these applications in the Appendix. Multi-Prompt Editing. Since generation is performed independently on masked regions, we can easily process multiple, non-contiguous masks simultaneously. Each regions local tokens can even be conditioned on different text prompt, allowing for complex, multi-prompt edits in single pass using batch inference. Content Propagation. Because the base video diffusion model is not fine-tuned for editing, it can be easily swapped out with an autoregressive video diffusion model (Huang et al., 2025b) for content propagation and reducing compute even further. By defining the initial edit in one or more frames and propagating the mask, our model can coherently generate the new content across subsequent frames in the local context. Due to the high frame-rate nature of video feeds, the global context does not change much in the near future. Thus, we can treat the global embedding as causal embedding by padding Vb with its own last available frames to provide adequate global context about the future. This forgoes the need for global context at inference time and allows us to edit the video into the future. The mask can also be propagated forward using motion cues such as optical flow or camera pose, enabling applications such as augmented reality editing where content needs to be generated before the headset acquires the frame and projected to the user once the frame is displayed."
        },
        {
            "title": "4.4 Ablation Studies",
            "content": "We ablate the local encoder and global embedder components from the architecture and compare video editing performance on VPBench-Edit in Tab. 3. Qualitative comparisons for inpainting on DAVIS are also shown in Fig. 6. With the naive approach, where we do not use Gψ and drop all tokens outside the mask before feeding to the non-fine-tuned local encoder, we observe drastic drop in quality. When using the fine-tuned local encoder without Gψ, editing quality increases drastically but leads to overfitting. Since the local encoder does not know about the global context of the video, it may attend to the prompt too strongly and generate content in the target region, as seen in Fig. 6."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced EditCtrl, video diffusion editing approach with disentangled control that generatively edits only where needed. Our method greatly improves over state of the art methods in terms of quality and efficiency and introduces novel interactive functionality, yet limitations remain. First, the video VAE causes significant degradation to the background context. In addition, the local encoder struggles in videos with very fast motion, which is due to both the VAE and fast shifts in spatiotemporal local context. Lastly, the VAE encode/decode overhead is not bottleneck for 480 720 videos in terms of end-to-end throughput, but is for 4K videos as we need to tile encode/decode the video due to VRAM constraints. Future work could explore extensions for encoding and integrating additional fundamental temporal information such as motion into generative editing."
        },
        {
            "title": "References",
            "content": "Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David B. Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. In Proceedings of the International Conference on Computer Vision (ICCV), 2025. Lao Beyer, Tianhong Li, Xinlei Chen, Sertac Karaman, and Kaiming He. Highly compressed tokenizer can generate without training. In Proceedings of the International Conference on Machine Learning (ICML), 2025. Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, and Qiang Xu. Videopainter: Any-length video inpainting and editing with plug-and-play context control. In Proceedings of SIGGRAPH, 2025. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your ViT but faster. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. Shengqu Cai, Duygu Ceylan, Matheus Gadelha, Chun-Hao Huang, Tuanfeng Wang, and Gordon. Wetzstein. Generative rendering: Controllable 4d-guided video generation with 2d diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, et al. Sana-video: Efficient video generation with block linear diffusion transformer. arXiv, 2025. Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos Kolotouros, Thiemo Alldieck, and Cristian Sminchisescu. Vlogger: Multimodal diffusion for embodied avatar synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the International Conference on Computer Vision (ICCV), 2023. Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, Wenping Wang, and Yuan Liu. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. In Proceedings of SIGGRAPH, 2025. Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, and Ori, et al. Gordon. Ltx-video: Realtime video latent diffusion. arXiv, 2024. Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv, 2024. Kai He, Ruofan Liang, Jacob Munkberg, Jon Hasselgren, Nandita Vijaykumar, Alexander Keller, Sanja Fidler, Igor Gilitschenski, Zan Gojcic, and Zian Wang. Unirelight: Learning joint decomposition and synthesis for video relighting. In Advances in Neural Information Processing Systems (NeurIPS), 2025. Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. arXiv, 2024. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems (NeurIPS), 2020. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Jiaxin Huang, Sheng Miao, Bangbang Yang, Yuewen Ma, and Yiyi Liao. Vivid4d: Improving 4d reconstruction from monocular video by video inpainting. In Proceedings of the International Conference on Computer Vision (ICCV), 2025a. 11 Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. In Advances in Neural Information Processing Systems (NeurIPS), 2025b. Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. In Proceedings of the International Conference on Computer Vision (ICCV), 2025. Kumara Kahatapitiya, Adil Karjauv, Davide Abati, Fatih Porikli, Yuki M. Asano, and Amirhossein Habibian. Object-centric diffusion for efficient video editing. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv, 2024. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Yao-Chih Lee, Erika Lu, Sarah Rumbley, Michal Geyer, Jia-Bin Huang, Tali Dekel, and Forrester Cole. Generative omnimatte: Learning to decompose video into layers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Xiaowen Li, Haolan Xue, Peiran Ren, and Liefeng Bo. Diffueraser: diffusion model for video inpainting. arXiv, 2025. Xirui Li, Chao Ma, Xiaokang Yang, and Ming-Hsuan Yang. Vidtome: Video token merging for zero-shot video editing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Zhen Li, Cheng-Ze Lu, Jianhua Qin, Chun-Le Guo, and Ming-Ming Cheng. Towards an end-to-end framework for flow-guided video inpainting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Ruofan Liang, Kai He, Zan Gojcic, Igor Gilitschenski, Sanja Fidler, Nandita Vijaykumar, and Zian Wang. Luxdit: Lighting estimation with video diffusion transformer. In Advances in Neural Information Processing Systems (NeurIPS), 2025. Yehonathan Litman, Fernando De la Torre, and Shubham Tulsiani. Lightswitch: Multi-view relighting with materialguided diffusion. In Proceedings of the International Conference on Computer Vision (ICCV), 2025a. Yehonathan Litman, Or Patashnik, Kangle Deng, Aviral Agrawal, Rushikesh Zawar, Fernando De la Torre, and Shubham Tulsiani. Materialfusion: Enhancing inverse rendering with material diffusion priors. In 3DV, 2025b. Haozhe Liu, Shikun Liu, Zijian Zhou, Mengmeng Xu, Yanping Xie, Xiao Han, Juan Camilo Perez, Ding Liu, Kumara Kahatapitiya, Menglin Jia, et al. Mardini: Masked auto-regressive diffusion for video generation at scale. Transactions on Machine Laerning Research (TMLR), 2025a. Rui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, Lewei Lu, Wenxiu Sun, Xiaogang Wang, Jifeng Dai, and Hongsheng Li. Fuseformer: Fusing fine-grained information in transformers for video inpainting. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, and Jiaya Jia. Generative video propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025b. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations (ICLR), 2019. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv, 2023. Yue Ma, Kunyu Feng, Xinhua Zhang, Hongyu Liu, David Junhao Zhang, Jinbo Xing, Yinhan Zhang, Ayden Yang, Zeyu Wang, and Qifeng Chen. Follow-your-creation: Empowering 4d creation through video inpainting. arXiv, 2025. Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo: Remake video with motion and content control. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Yotam Nitzan, Zongze Wu, Richard Zhang, Eli Shechtman, Daniel Cohen-Or, Taesung Park, and Michaël Gharbi. Lazy diffusion transformer for interactive image editing. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. NVIDIA, Hassan Abu Alhaija, Jose Alvarez, Maciej Bala, Tiffany Cai, Tianshi Cao, Liz Cha, Joshua Chen, Mike Chen, Francesco Ferroni, Sanja Fidler, Dieter Fox, Yunhao Ge, Jinwei Gu, Ali Hassani, Michael Isaev, Pooya Jannaty, Shiyi Lan, Tobias Lasser, Huan Ling, Ming-Yu Liu, Xian Liu, Yifan Lu, Alice Luo, Qianli Ma, Hanzi Mao, Fabio Ramos, Xuanchi Ren, Tianchang Shen, Shitao Tang, Ting-Chun Wang, Jay Wu, Jiashu Xu, Stella Xu, Kevin Xie, Yuchong Ye, Xiaodong Yang, Xiaohui Zeng, and Yu Zeng. Cosmos-transfer1: Conditional world generation with adaptive multimodal control. arXiv, 2025. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv, 2024. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv, 2017. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML), 2021. Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Dvir Samuel, Matan Levy, Nir Darshan, Gal Chechik, and Rami Ben-Ari. Omnimattezero: Fast training-free omnimatte with pre-trained video diffusion models. In SIGGRAPH Asia, 2025. Joonghyuk Shin, Zhengqi Li, Richard Zhang, Jun-Yan Zhu, Jaesik Park, Eli Schechtman, and Xun Huang. Motionstream: Real-time video generation with interactive motion controls. arXiv, 2025. Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. IEEE Winter Conference on Applications of Computer Vision (WACV), 2021. Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, and Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv, 2025. Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In Proceedings of SIGGRAPH, 2024. Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformer. In Proceedings of the International Conference on Learning Representations (ICLR), 2025. Peiqing Yang, Shangchen Zhou, Jixin Zhao, Qingyi Tao, and Chen Change Loy. Matanyone: Stable video matting with consistent memory propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Frédo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Huicong Zhang, Haozhe Xie, and Hongxun Yao. Blur-aware spatio-temporal sparse transformer for video deblurring. In Proceedings of the European Conference on Computer Vision (ECCV), 2024a. Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. In Proceedings of the International Conference on Machine Learning (ICML), 2025a. Kaidong Zhang, Jingjing Fu, and Dong Liu. Inertia-guided flow completion and style fusion for video inpainting. In Proceedings of the European Conference on Computer Vision (ECCV), pages 59825991, 2022. Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. Proceedings of the International Conference on Computer Vision (ICCV), 2023. Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Vsa: Faster video diffusion with trainable sparse attention. In Advances in Neural Information Processing Systems (NeurIPS), 2025b. 13 Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. In Proceedings of the International Conference on Learning Representations (ICLR), 2024b. Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, Luxin Zhang, Yinan Zhao, Peter Vajda, Dimitris Metaxas, and Licheng Yu. Avid: Any-length video inpainting with diffusion model. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024c. Jingkai Zhou, Yifan Wu, Shikai Li, Min Wei, Chao Fan, Weihua Chen, Wei Jiang, and Fan Wang. Realisdance-dit: Simple yet strong baseline towards controllable character animation in the wild. arXiv, 2025. Shangchen Zhou, Chongyi Li, Kelvin C.K Chan, and Chen Change Loy. ProPainter: Improving propagation and transformer for video inpainting. In Proceedings of the International Conference on Computer Vision (ICCV), 2023."
        },
        {
            "title": "A Additional Visualizations",
            "content": "We show additional results editing in distinct regions with multiple prompts as well as content propagation in Figs. 8-9. Furthermore, we provide additional visualizations of EditCtrls video editing and inpainting results on VPBench and DAVIS in Figs. 10-13."
        },
        {
            "title": "B Additional Details",
            "content": "Content Propagation. When using distilled autoregressive video diffusion model, video generation can be extended to an indefinite range, albeit with some loss in quality by denoising new incoming noisy latents. When used as the base video model for EditCtrl, this means we inherit the autoregressive models properties and can edit video of any length by using sliding window attention over local window of frames. This is in addition to accelerated inference with respect to the target edit area. Moreover, we can extend this to real-time video editing, where we edit video indefinitely, but without having access to the background context. This is achieved by simply copying the background context from the last known frames. If the video is static then the background context tokens used for local context can be copied over for generating next frames. In cases where the background context is dynamic, we use optical flow to propagate the last frames pixels to roughly approximate the future local context. In both static and dynamic cases, the last downsampled background frame VbN is padded to Vb to provide rough estimate for future global context. Once the future frame arrives, the generated content is pasted into the corresponding pixels along with optional edge blending. This process is repeated, where optical flow propagation is computed from the last known frame to the next future frame. These steps help generate more visually appealing content that matches the input videos motion ahead of time, whereas if we wait for future frames the local context will be much more accurate but the latency requirement would be too small, making the generated output appear laggy. An example of EditCtrl in an augmented reality setting is shown in Fig. 7, where initial frames are known and then EditCtrl propagates content to the next frames. Figure 7 Content Propagation for Augmented Reality. EditCtrl is particularly suitable for deployment in augmented reality applications given its low latency and ability to propagate content to match the users movement. 15 Figure 8 Additional Multi-Prompt Generation Visualizations. 16 Figure 9 Additional Content Propagation Visualizations. 17 Figure 10 Additional Video Editing Comparisons. (Top) \"Add hot air balloon\" (Bottom) \"Turn the sunglasses shiny red.\" 18 Figure 11 Additional Video Editing Comparisons. (Top) \"Turn the donkey into zebra.\" (Bottom) \"Add bandana to the dog.\" 19 Figure 12 Additional Video Inpainting Comparisons on DAVIS. 20 Figure 13 Additional Video Inpainting Comparisons on VPBench."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Meta AI",
        "Meta Reality Labs"
    ]
}