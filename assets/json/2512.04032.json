{
    "paper_title": "Jina-VLM: Small Multilingual Vision Language Model",
    "authors": [
        "Andreas Koukounas",
        "Georgios Mastrapas",
        "Florian Hönicke",
        "Sedigheh Eslami",
        "Guillaume Roncari",
        "Scott Martens",
        "Han Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. The model achieves leading results on standard VQA benchmarks and multilingual evaluations while preserving competitive text-only performance. Model weights and code are publicly released at https://huggingface.co/jinaai/jina-vlm ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 2 2 3 0 4 0 . 2 1 5 2 : r N A-V M: SMALL MULTILINGUAL VISION LANGUAGE MODEL Andreas Koukounas Georgios Mastrapas Florian Honicke Guillaume Roncari Scott Martens Han Xiao Sedigheh Eslami Jina AI by Elastic Prinzessinnenstr. 19-20, Berlin 10969, Germany research@jina.ai"
        },
        {
            "title": "ABSTRACT",
            "content": "We present jina-vlm, 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples SigLIP2 vision encoder with Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. The model achieves leading results on standard VQA benchmarks and multilingual evaluations while preserving competitive text-only performance. Model weights and code are publicly released at https://huggingface.co/jinaai/jina-vlm."
        },
        {
            "title": "INTRODUCTION",
            "content": "Vision-language models (VLMs) combine pretrained vision encoders with large language models to tackle tasks requiring joint visual and textual understanding (Alayrac et al., 2022; Liu et al., 2023). Recent VLMs have achieved strong results on visual question answering (VQA), OCR, and multimodal reasoning. However, two challenges limit their practical deployment. First, multilingual capabilities often degrade during vision adaptation: models that perform well on English benchmarks show uneven results across other languages (Manea & Libovicky, 2025). Second, highquality VLMs remain computationally expensive to train and deploy, limiting accessibility for researchers and practitioners with constrained resources. This work introduces jina-vlm, 2.4B parameter VLM that addresses both challenges. The model aligns SigLIP2-So400M/14-384 vision encoder (Tschannen et al., 2025) with Qwen3-1.7B-Base (Yang et al., 2025) through an attention-pooling connector, trained with two-stage pipeline that explicitly incorporates multilingual data. Among open 2B-scale VLMs, jina-vlm achieves state-of-the-art performance on multilingual multimodal benchmarks including MMMB and Multilingual MMBench, demonstrating that small models can excel at cross-lingual visual understanding without sacrificing general capabilities. On standard English benchmarks spanning diagrams, charts, documents, and OCR, jina-vlm achieves the highest average score (72.3) across eight VQA benchmarks among 2B-scale VLMs. These results are enabled by two technical contributions: an efficient arbitrary-resolution pipeline that combines overlapping tiling with attention-based token pooling to reduce visual token count by 4, and training recipe that incorporates text-only data to preserve the language understanding performance of the backbone LLM."
        },
        {
            "title": "2 RELATED WORK",
            "content": "VLM architecture and training. Modern VLMs follow an architecture introduced by PaLI (Chen et al., 2023): pretrained vision encoder extracts visual features, connector projects them into the language models embedding space, and decoder-only language model generates text conditioned on these visual tokens. Vision Transformers (ViTs) (Dosovitskiy et al., 2021) produce patch-level Equal contribution. Work done during internship at Jina AI. 1 representations that the language model processes alongside text embeddings. This design is adopted by LLaVA (Liu et al., 2023; 2024a; Xu et al., 2024; Li et al., 2024d;a), QwenVL (Bai et al., 2023; Wang et al., 2024c; Bai et al., 2025), InternVL (Chen et al., 2024d;c; 2025; Zhu et al., 2025; Wang et al., 2025), and Ovis (Lu et al., 2024b; 2025). Training strategies vary: Wang et al. (2024c); Chen et al. (2025) alternate between multimodal instruction tuning and general training; Liu et al. (2024a) incorporate academic VQA datasets; Deitke et al. (2025), Li et al. (2024a), and Tong et al. (2024) curate large-scale, diverse data mixtures. Efficient resolution-agnostic image processing. Standard ViTs process fixed-resolution images, requiring resizing that discards fine-grained detail. Since visual token count scales with resolution and Transformer computation scales quadratically with sequence length, naive high-resolution processing is prohibitive. Several solutions exist: Deitke et al. (2025) tile images with overlap; Wang et al. (2024c) introduce Naive Dynamic Resolution with Multimodal Rotary Position Embedding (Su et al., 2024; Heo et al., 2024); Lu et al. (2025) use native-resolution ViTs (Dehghani et al., 2023). Orthogonally, images often contain low-information regions (e.g., sky backgrounds), making visual tokens highly redundant. Token compression methods address this (Chen et al., 2024a; Shang et al., 2024; Yang et al., 2024; Xing et al., 2025). Chen et al. (2024c) develop Dynamic High-Resolution Tiling, and Liu et al. (2025) propose scale-then-compress strategies. Recent work on training-free token budgeting, such as HERO (Li et al., 2025), demonstrates that inference-time pruning can achieve significant speedups while preserving accuracy; our approach differs by learning compact representations during training rather than dropping tokens at inference. Vision-language connectors. The connector bridging vision encoders and language models significantly impacts both efficiency and performance. BLIP-2 (Li et al., 2023a) introduces Q-Former, learnable query-based transformer that extracts fixed-length representations from visual features, reducing the number of tokens fed to the LLM. Flamingo (Alayrac et al., 2022) uses Perceiver Resampler with cross-attention to compress visual tokens. Our attention-pooling connector shares the goal of token reduction but operates differently: rather than learning fixed set of queries, we apply local 22 attention pooling that preserves spatial structure while achieving 4 compression, which we found more effective for tasks requiring fine-grained spatial understanding. Small VLMs. Efficiency has become central objective. Chu et al. (2024) demonstrate competitive performance below 2B parameters. Shao et al. (2025) combine quantization with aggressive resolution reduction for mobile deployment, matching larger models performance. MiniCPM-V (Yao et al., 2024) targets edge deployment while maintaining strong OCR and multilingual capabilities. Marafioti et al. (2025) systematically explore design parameters to train VLMs as small as 256M parameters. Multilingual VLMs. Many lightweight VLMs (Beyer et al., 2024; Steiner et al., 2024; Abdin et al., 2024) achieve strong English performance but degrade on other languages. Wang et al. (2024c) and Chen et al. (2024c) address this through targeted multilingual training data. Yue et al. (2025) introduce instruction-tuning data spanning 39 languages. Retaining text-only performance. Multimodal training often degrades text-only capabilities. Mitigation strategies include balanced data mixtures, careful learning rate scheduling (Laurencon et al., 2024), and partial backbone freezing (Li et al., 2024a; Wang et al., 2025)."
        },
        {
            "title": "3 MODEL ARCHITECTURE",
            "content": "Figure 1 illustrates the architecture of jina-vlm. The model uses overlapping image tiling following Deitke et al. (2025), combined with attention-based token pooling to reduce sequence length while preserving spatial information. The vision encoder, SigLIP2-So400M/14-384, is 27-layer Vision Transformer with 400M parameters that processes 378378 pixel inputs as 2727 grids of 1414 patches. To handle arbitrary resolutions, we decompose each image into overlapping tiles of this size and process each tile independently through the encoder. global thumbnail, the full image resized to 378378, provides context alongside the tile representations. We use default of 12 tiles during training; this limit can be increased at inference or during continued training to handle higher resolutions, with memory scaling linearly with tile count. The tiling algorithm is detailed in Appendix A.1. 2 Figure 1: Architecture of jina-vlm. Images are resized to fit grid of up to 12 overlapping tiles, plus global thumbnail. Each tile is square 378378 crop; adjacent tiles overlap by 112 pixels with stride of 266 pixels between tile origins. 43 grid therefore spans 1176910 pixels, and images exceeding this effective resolution are downscaled to fit the tile budget. Each tile produces 729 patches via SigLIP2 (Tschannen et al., 2025). The VL connector concatenates features from layers 24 and 18, the thirdand ninth-to-last layers, then applies 22 attention pooling to reduce 729 tokens to 182 before projecting to the decoder dimension. Visual tokens are combined with text embeddings for the Qwen3 decoder (Yang et al., 2025). 3.1 VISION-LANGUAGE CONNECTOR Rather than using the final ViT output, jina-vlm concatenates features from two intermediate layers: the third-to-last and ninth-to-last, corresponding to layers 24 and 18 of the 27-layer encoder. This captures both fine-grained spatial details from earlier layers and high-level semantics from later layers. The connector then applies attention pooling over 22 patch neighborhoods, using meanpooled features as queries. This reduces the token count by 4 while preserving local structure. SwiGLU projection layer maps the pooled representations to the language models embedding dimension. In more formal terms, let H(ℓ) RN dv denote the hidden states from ViT layer ℓ, where is the number of patches, dv is the vision encoder hidden size, and negative indices count from the final layer (e.g., ℓ = 1 is the last layer). We concatenate features from two internal layers: Hconcat = [H(3); H(9)] RN 2dv (1) For each 22 patch neighborhood Ni, we compute query vector as the mean of the neighborhood features: qi = hj, = [q1; . . . ; qM ] RM 2dv (2) 1 4 (cid:88) jNi where Ni contains the four patches at positions (2ix, 2iy), (2ix + 1, 2iy), (2ix, 2iy + 1), and (2ix + 1, 2iy + 1) and = N/4. Attention pooling is then computed as: Hpooled = (softmax (cid:18) QWQ(HconcatWK) dk (cid:19) HconcatWV )WO RM dv (3) where dk = dv and WQ R2dvdk , WK R2dvdk , WV R2dv2dv and WO R2dvdv are learnable weight matrices. Finally, the pooled visual features are projected to the language model embedding dimension via SwiGLU (Shazeer, 2020) layer: Hproj = (Swish(HpooledW1) (HpooledW2)) W3 RM dl (4) 3 where Swish(x) = σ(x), σ is the sigmoid function, denotes element-wise multiplication, W1, W2 Rdv3dl , W3 R3dldl are learnable parameters, and dl is the language model embedding size."
        },
        {
            "title": "3.2 LANGUAGE DECODER",
            "content": "The language decoder is initialized from Qwen3-1.7B-Base1, which empirically outperformed the instruction-tuned variant in our setting. We introduce three special tokens to structure visual inputs: <im start> and <im end> delimit image and thumbnail sequences, while <im col> marks row boundaries within the patch grid, where tokens are arranged left-to-right and top-tobottom. Input and output embedding weights are not tied."
        },
        {
            "title": "3.3 EFFICIENCY ANALYSIS",
            "content": "Table 1 quantifies the computational benefits of attention pooling. With the default 12-tile configuration (plus thumbnail), the unpooled baseline would produce 9,477 visual tokens per image, while our 22 pooling reduces this to 2,366 tokens. Since the ViT processes each tile identically regardless of pooling, the savings apply exclusively to the LLM: we observe 3.9 reduction in prefill FLOPs and 4 reduction in KV-cache memory. The overall FLOPs reduction is 2.3 when including the shared ViT cost. Table 1: Efficiency comparison with and without 22 attention pooling for the default 12-tile configuration. FLOPs are computed for LLM prefill; KV-cache assumes fp16 precision. Metric No Pooling With Pooling Reduction Visual tokens LLM prefill FLOPs KV-cache memory 9,477 27.2 TFLOPs 2.12 GB 2,366 6.9 TFLOPs 0.53 GB 4.0 3.9 4."
        },
        {
            "title": "4 TRAINING",
            "content": "Training proceeds in two stages, both updating all model components (encoder, connector, and decoder) without freezing, following Deitke et al. (2025). The combined data comprises approximately 5M multimodal samples and 12B text tokens across 30+ languages, with roughly half in English and the remainder spanning highand moderate-resource languages. Table 2 summarizes hyperparameters for both stages. 4.1 STAGE 1: ALIGNMENT TRAINING The first stage focuses on cross-language semantic grounding rather than task-specific objectives. Training data consists primarily of caption datasets (PixmoCap (Deitke et al., 2025), PangeaIns (Yue et al., 2025)) spanning diverse visual domains: natural scenes, documents, infographics, and diagrams. We include 15% text-only data from PleiAS/common corpus (Langlais et al., 2025) to mitigate degradation on text-only tasks. The connector uses higher learning rate and shorter warmup than the encoder and decoder. 4.2 STAGE 2: INSTRUCTION FINE-TUNING The second stage trains instruction-following for VQA and reasoning tasks. We combine public dataset collections, including LLaVA OneVision (Li et al., 2024a), Cauldron (Laurencon et al., 2024), Cambrian (Tong et al., 2024), PangeaIns (Yue et al., 2025), and FineVision (Wiedmann et al., 2025), with text-only instruction data from Singh et al. (2024). The mixture covers academic VQA, document understanding, OCR, mathematics, and reasoning. Appendix A.2 shows representative examples. 1https://huggingface.co/Qwen/Qwen3-1.7B-Base 4 Table 2: Model training hyperparameters across pre-training and fine-tuning stages."
        },
        {
            "title": "Hyperparameter",
            "content": "Pre-Training Fine-Tuning Warmup ViT Warmup Con. Warmup LLM LR ViT LR Con. LR LLM Cosine Decay Eps. Betas Batch Size Steps Samples Tokens GPU Hours 10% 1% 10% 6e-6 2e-4 2e-5 0.1 1e-6 0.9, 0.95 128 25K 3.2M 10B 296 10% 10% 10% 5e-6 5e-6 1e-5 0.1 1e-6 0.9, 0.95 256 60K 15.3M 37B 1,000 Table 3: Comparison of general visual question answering performance. Model Name AI2D ChartQA TextVQA DocVQA InfoVQA jina-vlm Qwen2-VL-2B Qwen3-VL-2B InternVL3-2B InternVL3.5-2B 82.0 74.7 76.9 78.6 78.8 (test avg) 81.9 73.5 77.2 80.2 80. (val) 83.2 79.7 79.5 77.0 76.5 (val) 90.6 89.2* 92.3* 87.4* 88.5* (val) 71.6 64.0* 71.9* 67.1* 69.3* OCR Bench SEED-2 Plus CharXiv (RQ / DQ) Overall 778 809 858 835 836 67.2 62.4 67.3* 64.6 68. 32.3 / 63.5 23.3 / 55.0* 28.8 / 62.3 28.3 / 54.7 31.6 / 65.0 72.3 66.4 71.6 69.2 71.6 Results for models other than jina-vlm are from their respective papers, Wang et al. (2025); Zhu et al. (2025); Wang et al. (2024c), except those marked with * which were computed using VLMEvalKit. All scores represent accuracy (%) except OCRBench which uses 01000 scale; for overall average computation, OCRBench scores are divided by 10 to align with the 0100 scale of other benchmarks. Given the diversity of instruction data, we found single-source batches more effective initially, likely due to the heterogeneous data mixture. We train for 30K steps with single-source batches, then 30K steps with mixed-source batches."
        },
        {
            "title": "5 EVALUATION",
            "content": "We compare jina-vlm against lightweight VLMs across six capability areas: general VQA, multimodal comprehension, multi-image reasoning, hallucination control, mathematical reasoning, textonly performance, and multilingual understanding. All evaluations use VLMEvalKit2 (Duan et al., 2024) with English prompts matching our training format (e.g., Return only the letter of the best answer option for multiple-choice, Respond very briefly for open-ended questions). 5.1 GENERAL VQA TASKS Table 3 reports results on eight VQA benchmarks covering diagrams (AI2D (Kembhavi et al., 2016)), charts (ChartQA (Masry et al., 2022), CharXiv (Wang et al., 2024e)), scene text (TextVQA (Singh et al., 2019)), documents (DocVQA (Mathew et al., 2021), InfoVQA (Mathew et al., 2022)), OCR (OCRBench (Liu et al., 2024c)), and diverse scenes (SEED-Bench-2-Plus (Li et al., 2024b)). jina-vlm achieves the highest average (72.3), with particularly strong performance on diagram interpretation and text extraction. 2https://github.com/open-compass/VLMEvalKit 5 Table 4: Comparison of generic multimodal understanding and real-world understanding performance. Model MME MMB v1.1 MMStar Overall RealWorld MME-RW R-Bench Overall (RW) (sum) (MM) (EN) (EN) (dis) QA jina-vlm 1965.8 1872.0 Qwen2-VL-2B 2000.8* Qwen3-VL-2B 2221.2 InternVL3-2B InternVL3.5-2B 2123.3 75.8 72.2 77.8 78.6 76.6 56.2 48.0 58.3 60.7 62.7 67.4 62.4 69.2 72.9 71.7 68.2 62.9 63.9 64.3 62. 50.7 38.7* 57.9* 53.8 49.7 66.7 63.2 67.3* 67.5 62.4 61.9 55.0* 63.0 61.9 58.0 Results for models other than jina-vlm are from their respective papers, Wang et al. (2025); Zhu et al. (2025); Wang et al. (2024c), except those marked with * which are computed using VLMEvalKit. All scores represent accuracy (%) except MME which uses 02800 scale; for overall average computation, MME scores are divided by 28 to align with the 0100 scale of other benchmarks. Table 5: Comparison of multi-image and hallucination performance."
        },
        {
            "title": "BLINK Muir MMT Overall HallBench",
            "content": "jina-vlm Qwen2-VL-2B Qwen3-VL-2B InternVL3-2B InternVL3.5-2B (val) 50.1 44.4 53.8 50.3 51.3 Bench (val) 34.7 25.5* 47.4 38.8 44. 57.2 55.1 60.0* 59.5 58.5 (MI) 47.3 41.7 53.7 49.5 51.3 (avg) 39.1 41.7 44.5 42.5 48.6 POPE Overall (Hall) (avg) 90.3 87.9* 88.9* 89.6 87.2 64.7 64.8 66.7 66.1 67.9 Results for models other than jina-vlm are from their respective papers, (Wang et al., 2025; Zhu et al., 2025; Wang et al., 2024c), except those marked with * which are computed using VLMEvalKit. All scores represent accuracy (%). 5.2 DOCUMENT AND REAL-WORLD UNDERSTANDING Table 4 shows results on multimodal comprehension (MME (Fu et al., 2025), MMB v1.1 (Liu et al., 2024b), MMStar (Chen et al., 2024b)) and real-world understanding (RealWorldQA (xAI, 2024), MME-RealWorld (Zhang et al., 2025), R-Bench (Li et al., 2024c)). jina-vlm scores 67.4 on multimodal tasks and 61.9 on real-world tasks, achieving the best RealWorldQA result (68.2). 5.3 MULTI-IMAGE REASONING AND HALLUCINATION Table 5 reports multi-image reasoning (BLINK (Fu et al., 2024), MuirBench (Wang et al., 2024a), MMT (Ying et al., 2024)) and hallucination benchmarks that measure the tendency to fabricate visual details (HallBench (Guan et al., 2024), POPE (Li et al., 2023b)). jina-vlm scores 47.3 on multi-image tasks, which is expected given limited multi-image training data, but achieves the best POPE score (90.3), indicating low hallucination rates. 5.4 MATHEMATICAL REASONING Table 6 reports structured reasoning benchmarks: multidisciplinary comprehension (MMMU (Yue et al., 2024)), visual mathematics (MathVista (Lu et al., 2024a), MathVision (Wang et al., 2024b), MathVerse (Zhang et al., 2024), WeMath (Qiao et al., 2025)), and logical reasoning (LogicVista (Xiao et al., 2024)). jina-vlm performs comparably to InternVL3-2B and outperforms Qwen2VL-2B. 5.5 TEXT-ONLY PERFORMANCE Table 7 compares jina-vlm against the backbone Qwen3-1.7B on text-only benchmarks: MMLU (Hendrycks et al., 2021), MMLU-Pro (Wang et al., 2024d), GSM-8K (Cobbe et al., 2021), ARCC (Clark et al., 2018), and HellaSwag (Zellers et al., 2019). Results show mixed preservation of text-only capabilities: jina-vlm matches or exceeds the backbone on commonsense reasoning Table 6: Comparison of multimodal reasoning and mathematical problem-solving performance. Model MMMU MathVista MathVision jina-vlm Qwen2-VL-2B Qwen3-VL-2B InternVL3-2B InternVL3.5-2B 45.6 41.1 53.4 48.6 59.0 59.5 43.0 61.3 57.0 71.8 / 61.5 19.2 12.4 31.6 21.7 42.8 / 26.5 MathVerse (Vision Only) 23.9 17.3* 22.7* 25.3 53.4 / 35.3 WeMath LogicVista Overall 17.1 10.9* 28.0* 22.4 48.5 / 19.1 33.3 27.3* 35.4* 36.9 47.7 / 41.4 33.1 25.3 38.7 35.3 50.7 Results for models other than jina-vlm are from their respective papers, (Wang et al., 2025; Zhu et al., 2025; Wang et al., 2024c), except those marked with * which are computed using VLMEvalKit. indicates scores for InternVL3.5-2B without thinking mode, evaluated using VLMEvalKit. All scores represent accuracy (%). Table 7: Comparison of Text-only benchmarks."
        },
        {
            "title": "Model",
            "content": "MMLU MMLU-Pro GSM-8k ARC-C HellaSwag Overall jina-vlm Qwen3-1.7B 59.4 59.0 Results are collected using our evaluation code. All scores represent accuracy (%). 77.3 73.4 56.1 62.6 71.3 75. 30.3 46.4 58.9 63.3 (ARC-C, HellaSwag) and retains most performance on MMLU and GSM-8K. However, MMLU-Pro shows substantial degradation (46.4 30.3), likely because this benchmark emphasizes extended multi-step reasoning that conflicts with our instruction-tuning toward concise visual responses. This suggests trade-off between optimizing for multimodal tasks and preserving complex text-only reasoning, which future work could address through more balanced data mixtures or curriculum scheduling. 5.6 MULTILINGUAL UNDERSTANDING Table 8 reports multilingual multimodal benchmarks: MMMB (Sun et al., 2025), Multilingual MMBench (Sun et al., 2025), and MTVQA (Tang et al., 2025). jina-vlm achieves state-of-the-art multilingual performance among 2B-scale VLMs, with the highest averages on MMMB (78.8) and Multilingual MMBench (74.3)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We presented jina-vlm, 2.4B vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. Our results demonstrate that small VLMs can attain strong cross-lingual visual understanding through careful architectural and training choices: attention-based token pooling reduces visual tokens by 4 while preserving spatial information, and incorporating text-only data during multimodal training mitigates the catastrophic forgetting typically observed in vision-adapted language models. On standard English VQA benchmarks, jina-vlm achieves leading results, demonstrating that multilingual capabilities need not come at the cost of general performance. The current approach has limitations. Multi-tile processing introduces computational overhead that scales with image resolution, and tiling can fragment global spatial context, potentially impairing performance on tasks requiring holistic scene understanding such as object counting or precise spatial reasoning across tile boundaries. While the global thumbnail partially mitigates this, nativeresolution approaches (Dehghani et al., 2023) may be better suited for such tasks. We have not emphasized safety-critical training or alignment, and multi-image reasoning remains weak due to limited training data in this regime. Future work could explore more efficient resolution handling, targeted improvements for counting and spatial tasks, and investigate whether our multilingual training recipe transfers to larger model scales. 7 Table 8: Comparison of multilingual multimodal understanding performance. Benchmark jina-vlm Qwen2-VL-2B Qwen3-VL-2B InternVL3-2B InternVL3.5-2B ar cn en pt ru tr avg ar cn en pt ru tr avg MMMB Multi. MMBench MTVQA Overall 76.9 80.0 82.0 79.2 79.2 75.5 78.8 70.0 75.9 78.8 74.7 75.3 71.1 74.3 25.6 59.6 68.3 74.2 78.3 72.6 72.8 61.8 71.3 66.7 67.0 71.1 72.1 69.9 69.3 69. 20.6 53.8 72.7* 75.7* 80.7* 75.0* 75.9* 68.5* 75.0* 66.2* 75.7* 77.8* 71.4* 75.9* 67.0* 72.3* 27.3* 58. 68.6 78.3 81.9 75.4 74.6 62.9 73.6 66.4 77.8 81.3 75.9 70.7 59.5 71.9 26.7 57.4 68.5 77.7 80.2 75.9 76.3 69.1 74.6 63.7 75.9 78.4 73.7 71.4 62.0 70. 28.5 58.0 Results for baseline models are derived from their original publications, (Wang et al., 2025; Zhu et al., 2025; Wang et al., 2024c), except those marked with * which are computed using VLMEvalKit. All scores represent accuracy (%)."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, et al. Phi-3 Technical Report: Highly Capable Language Model Locally on Your Phone. arXiv preprint arXiv:2404.14219, 2024. Manoj Acharya, Kushal Kafle, and Christopher Kanan. TallyQA: Answering Complex Counting Questions. In Proc. 33rd AAAI Conference on Artificial Intelligence, AAAI19, 2019. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, et al. Flamingo: Visual Language Model for Few-Shot Learning. In Proc. 36th International Conference on Neural Information Processing Systems, NeurIPS 2022, 2022. Jinze Bai, Shuai Bai, Shusheng Yang, et al. Qwen-VL: Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv preprint arXiv:2308.12966, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, et al. Qwen2.5-VL Technical Report. arXiv preprint arXiv:2502.13923, 2025. Lucas Beyer, Andreas Steiner, Andre Susano Pinto, et al. PaliGemma: Versatile 3B VLM for Transfer. arXiv preprint arXiv:2407.07726, 2024. Liang Chen, Haozhe Zhao, Tianyu Liu, et al. An Image is Worth 1/2 Tokens After Layer 2: Plugand-Play Inference Acceleration for Large Vision-Language Models. In Computer Vision ECCV 2024, pp. 1935, 2024a. Lin Chen, Jinsong Li, Xiaoyi Dong, et al. Are We on the Right Way for Evaluating Large VisionLanguage Models? In Proc. 38th International Conference on Neural Information Processing Systems, NeurIPS 2024, 2024b. Xi Chen, Xiao Wang, Soravit Changpinyo, et al. PaLI: Jointly-Scaled Multilingual LanguageImage Model. In ICLR 2023, 2023. Zhe Chen, Weiyun Wang, Hao Tian, et al. How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites. Sci. China Inf. Sci., 2024c. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, et al. InternVL: Scaling up Vision Foundation In 2024 IEEE/CVF Conference on Models and Aligning for Generic Visual-Linguistic Tasks. Computer Vision and Pattern Recognition (CVPR), 2024d. 8 Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, et al. Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling. arXiv preprint arXiv:2412.05271, 2025. Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, et al. MobileVLM V2: Faster and Stronger Baseline for Vision Language Model. arXiv preprint arXiv:2402.03766, 2024. Peter Clark, Isaac Cowhey, Oren Etzioni, et al. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, et al. Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168, 2021. Mostafa Dehghani, Basil Mustafa, Josip Djolonga, et al. Patch pack: NaViT, vision transformer for any aspect ratio and resolution. In Proc. 37th International Conference on Neural Information Processing Systems, NeurIPS 2023, 2023. Matt Deitke, Christopher Clark, Sangho Lee, et al. Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models. In 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In 9th International Conference on Learning Representations, ICLR 2021, 2021. Haodong Duan, Xinyu Fang, Junming Yang, et al. VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models. In Proc. 32nd ACM International Conference on Multimedia, MM 24, 2024. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, Rongrong Ji, Caifeng Shan, and Ran He. MME: Comprehensive Evaluation Benchmark for Multimodal Large Language Models. arXiv preprint arXiv:2306.13394, 2025. Xingyu Fu, Yushi Hu, Bangzheng Li, et al. BLINK: Multimodal Large Language Models Can See but Not Perceive. In Computer Vision ECCV 2024, 2024. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. Int. J. Comput. Vision, pp. 398414, April 2019. Tianrui Guan, Fuxiao Liu, Xiyang Wu, et al. HallusionBench: An Advanced Diagnostic Suite for In Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. PathVQA: 30000+ Questions for Medical Visual Question Answering. arXiv preprint arXiv:2003.10286, 2020. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring Massive Multitask Language Understanding. arXiv:2009.03300, 2021. Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun. Rotary Position Embedding for Vision Transformer. In Computer Vision ECCV 2024, 2024. Yu-Chung Hsiao, Fedir Zubach, Gilles Baechler, et al. ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots. In Proc. 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics, 2025. Yiming Jia, Jiachen Li, Xiang Yue, et al. VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search. In Proc. 2025 Conference on Empirical Methods in Natural Language Processing, EMNLP 2025, 2025. 9 Justin Johnson, Bharath Hariharan, Laurens van der Maaten, et al. CLEVR: Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, et al. Diagram Is Worth Dozen Images. In Computer Vision ECCV 2016, 2016. Pierre-Carl Langlais, Carlos Rosas Hinostroza, et al. Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training. arXiv preprint arXiv:2506.01732, 2025. Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? In Proc. 38th International Conference on Neural Information Processing Systems, NeurIPS 2024, 2024. Bo Li, Yuanhan Zhang, Dong Guo, et al. LLaVA-OneVision: Easy Visual Task Transfer. arXiv preprint arXiv:2408.03326, 2024a. Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension. arXiv preprint arXiv:2404.16790, 2024b. Chunyi Li, Jianbo Zhang, Zicheng Zhang, et al. R-Bench: Are your Large Multimodal Model Robust to Real-world Corruptions? arXiv preprint arXiv:2410.05474, 2024c. Feng Li, Renrui Zhang, Hao Zhang, et al. LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models. arXiv preprint arXiv:2407.07895, 2024d. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In Proc. 40th International Conference on Machine Learning, ICML 2023, 2023a. Xu Li, Yuxuan Liang, Xiaolei Chen, Yi Zheng, Haotian Chen, Bin Li, and Xiangyang Xue. HERO: Rethinking Visual Token Early Dropping in High-Resolution Large Vision-Language Models. arXiv preprint arXiv:2509.13067, 2025. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating Object Hallucination in Large Vision-Language Models. In Proc. 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, 2023b. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. In Proc. 37th International Conference on Neural Information Processing Systems, NeurIPS 2023, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved Baselines with Visual Instruction Tuning. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024a. Yuan Liu, Haodong Duan, Yuanhan Zhang, et al. MMBench: Is Your Multi-modal Model an Allaround Player? In Computer Vision ECCV 2024, 2024b. Yuliang Liu, Zhang Li, Mingxin Huang, et al. OCRBench: On the Hidden Mystery of OCR in Large Multimodal Models. Science China Information Sciences, 67(12), 2024c. Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, et al. NVILA: Efficient Frontier Visual Language Models. In 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Pan Lu, Hritik Bansal, Tony Xia, et al. MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts. In 12th International Conference on Learning Representations, ICLR 2024, 2024a. Shiyin Lu, Yang Li, Qing-Guo Chen, et al. Ovis: Structural Embedding Alignment for Multimodal Large Language Model. arXiv preprint arXiv:2405.20797, 2024b. Shiyin Lu, Yang Li, Yu Xia, et al. Ovis2.5 Technical Report. arXiv preprint arXiv:2508.11737, 2025. 10 Andrei-Alexandru Manea and Jindˇrich Libovicky. Multilingual Vision-Language Models, Survey. arXiv preprint arXiv:2509.22123, 2025. Andres Marafioti, Orr Zohar, Miquel Farre, et al. SmolVLM: Redefining small and efficient multimodal models. arXiv preprint arXiv:2504.05299, 2025. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: Benchmark for Question Answering about Charts with Visual and Logical Reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, 2022. Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. DocVQA: Dataset for VQA on Document Images. arXiv preprint arXiv:2007.00398, 2021. Minesh Mathew, Viraj Bagal, Rub`en Perez Tito, Dimosthenis Karatzas, Ernest Valveny, and C. Jawahar. InfographicVQA. In 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2022. Runqi Qiao, Qiuna Tan, Guanting Dong, et al. We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning? In Proc. 63rd Annual Meeting of the Association for Computational Linguistics: ACL 2025, 2025. Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models. arXiv preprint arXiv:2403.15388, 2024. Zhenwei Shao, Zhou Yu, Jun Yu, Xuecheng Ouyang, et al. Imp: Highly Capable Large Multimodal Models for Mobile Devices. IEEE Transactions on Multimedia, 27:29612974, 2025. Noam Shazeer. GLU Variants Improve Transformer. arXiv preprint arXiv:2002.05202, 2020. Amanpreet Singh, Vivek Natarajan, Meet Shah, et al. Towards VQA Models That Can Read. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Jaspreet Singh et al. Aya Dataset: An Open-Source Multilingual Instruction Dataset. In Proc. 62nd Annual Meeting of the Association for Computational Linguistics, ACL 2024, 2024. Andreas Steiner, Andre Susano Pinto, Michael Tschannen, et al. PaliGemma 2: Family of Versatile VLMs for Transfer. arXiv preprint arXiv:2412.03555, 2024. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced Transformer with Rotary Position Embedding. Neurocomput., February 2024. Hai-Long Sun, Da-Wei Zhou, Yang Li, et al. Parrot: Multilingual Visual Instruction Tuning. arXiv preprint arXiv:2406.02539, 2025. Jingqun Tang, Qi Liu, Yongjie Ye, et al. MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering. In Findings of the Association for Computational Linguistics: ACL 2025, 2025. Shengbang Tong, Ellis Brown, Penghao Wu, et al. Cambrian-1: Fully Open, Vision-Centric Exploration of Multimodal LLMs. In Proc. 38th International Conference on Neural Information Processing Systems, NeurIPS 2024, 2024. Michael Tschannen, Alexey Gritsenko, et al. SigLIP 2: Multilingual Vision-Language Encoders arXiv preprint with Improved Semantic Understanding, Localization, and Dense Features. arXiv:2502.14786, 2025. Fei Wang, Xingyu Fu, James Y. Huang, et al. MuirBench: Comprehensive Benchmark for Robust Multi-image Understanding. arXiv preprint arXiv:2406.09411, 2024a. Ke Wang, Junting Pan, Weikang Shi, et al. Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset. In Proc. 38th International Conference on Neural Information Processing Systems, NeurIPS 2024, 2024b. Peng Wang, Shuai Bai, Sinan Tan, et al. Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. arXiv preprint arXiv:2409.12191, 2024c. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, et al. InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency. arXiv preprint arXiv:2508.18265, 2025. Yubo Wang, Xueguang Ma, Ge Zhang, et al. MMLU-Pro: More Robust and Challenging MultiIn Proc. 38th International Conference on Neural Task Language Understanding Benchmark. Information Processing Systems, NeurIPS 2024, 2024d. Zirui Wang, Mengzhou Xia, Luxi He, et al. CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs. In Proc. 38th International Conference on Neural Information Processing Systems, NeurIPS 2024, 2024e. Luis Wiedmann, Orr Zohar, Amir Mahla, et al. FineVision: Open Data Is All You Need. arXiv preprint arXiv:2510.17269, 2025. xAI. RealWorldQA: Benchmark for Real-World Spatial Understanding, 2024. URL https: //x.ai/news/grok-1.5v#real-world-understanding. Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts. arXiv preprint arXiv:2407.04973, 2024. Long Xing, Qidong Huang, Xiaoyi Dong, et al. PyramidDrop: Accelerating Your Large VisionLanguage Models via Pyramid Visual Redundancy Reduction. arXiv preprint arXiv:2410.17247, 2025. Ruyi Xu, Yuan Yao, Zonghao Guo, et al. LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images. In Computer Vision ECCV 2024, 2024. An Yang, Anfeng Li, Baosong Yang, et al. Qwen3 Technical Report. arXiv preprint arXiv:2505.09388, 2025. Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. VisionZip: Longer is Better but Not Necessary in Vision Language Models. arXiv preprint arXiv:2412.04467, 2024. Yuan Yao, Tianyu Yu, Ao Zhang, et al. MiniCPM-V: GPT-4V Level MLLM on Your Phone. arXiv preprint arXiv:2408.01800, 2024. Kaining Ying, Fanqing Meng, Jin Wang, et al. MMT-Bench: Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI. In Proc. 41st International Conference on Machine Learning, ICML 2024, 2024. Xiang Yue, Yuansheng Ni, Kai Zhang, et al. MMMU: Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Xiang Yue, Yueqi Song, Akari Asai, et al. Pangea: Fully Open Multilingual Multimodal LLM for 39 Languages. In 13th International Conference on Learning Representations, ICLR 2025, 2025. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can In Proc. 57th Annual Meeting of the Association for Machine Really Finish Your Sentence? Computational Linguistics, 2019. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, et al. MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems? In Computer Vision ECCV 2024, 2024. Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, et al. MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans? In 13th International Conference on Learning Representations, ICLR 2025, 2025. Fengbin Zhu, Wenqiang Lei, Youcheng Huang, et al. TAT-QA: Question Answering Benchmark In Proc. 59th Annual Meeting of the on Hybrid of Tabular and Textual Content in Finance. Association for Computational Linguistics, 2021. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, et al. InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 PSEUDOCODE FOR CREATING OVERLAPPING TILES Algorithm 1: GETALLTILESOVERLAPANDRESIZE Input: Image of size (h, w); Base input size = (bh, bw) ((378, 378)) Patch size (14); Maximum number of tiles (12 by default, configurable) Overlap margins (mL, mR) in patches ((4, 4)) Output: List of tiles (thumbnail + grid tiles) Tiling (th, tw) = (number of rows, number of columns) 1. Compute overlap-related sizes mtot (mL + mR) // Total overlap margin in pixels swin (cid:0)bh/p (mL + mR)(cid:1) // Tile stride in pixels 2. Select tiling on the margin-reduced image (th, tw) SELECTTILINGWITHMINIMALSCALECHANGE(cid:0)h mtot, mtot, swin, (cid:1); 3. Resize image to exactly fit the chosen tiling + margins; th swin + mtot; tw swin + mtot; Igrid RESIZE(I, [H , ]); 4. Extract overlapping tiles EXTRACTTILES(cid:0)Igrid, (th, tw), swin, bh (cid:1) // bh is the tile height, equal to bw here 5. Build thumbnail and final tile list RESIZE(I, [bh, bw]) // Global thumbnail [T ] ++ // Concatenate thumbnail and tiles return (C, (th, tw)); 13 A.2 TRAINING SET EXAMPLES Captioning & Instruction Dataset: VisualWebInstruct Jia et al. (2025) Question what is the meeting title? Answer Conflict Resolution Meeting Figure 2: Answer questions given web documents. Charts & Tables Dataset: TAT-QA Zhu et al. (2021) Question Unrecognized Tax Benefits Activity related to unrecognized tax benefits is as follows (in thousands): ... As of July 31, 2019, the Company has no income tax audits in progress in the U.S. or foreign jurisdictions. What was the increase in unrecognized tax benefits in 2019? Answer $1.3 million. Figure 3: Financial table requiring numerical reasoning over text. Document Understanding & Infographics Dataset: DocVQA Mathew et al. (2021) Question what is the response code ? Answer U19 Figure 4: Document image with question about textual fields. OCR QA (text-centric VQA) Dataset: TextVQA Singh et al. (2019) Question what number is the cab Answer 3G54 Figure 5: Photo with textual question needing OCR reading."
        },
        {
            "title": "General VQA",
            "content": "Dataset: VQAv2 Goyal et al. (2019) Question Where is he looking? Answer down Figure 6: General visual question answering on natural images. Grounding, Spatial & Counting Dataset: TallyQA Acharya et al. (2019) Question How many more people can ride on the vehicle? Answer 0 Figure 7: Scene requiring counting and spatial reasoning accuracy. Math & Geometry (vision) Dataset: CLEVR Johnson et al. (2017) Question There is large shiny object; does it have the same shape as the object right of the large metallic thing? Provide short and direct response. Answer Yes. Figure 8: Synthetic shapes testing compositional spatial reasoning. Screens, Web & GUI Dataset: ScreenQA Hsiao et al. (2025) Question What is the default period length? Answer 5 days Figure 9: User interface screenshot with structured textual elements. Medical (vision) Dataset: PathVQA He et al. (2020) Question where are liver stem cells (oval cells) located? Answer in the canals of hering Figure 10: Microscopic pathology image for medical VQA. 17 Text-only (instruction / code / math / translation) Dataset: aya dataset Singh et al. (2024) Question Quels president des Etats-Unis ne sest jamais marie? Answer James Buchanan est le seul president qui ne sest jamais marie. Figure 11: Text-only tasks covering multiple languages."
        }
    ],
    "affiliations": [
        "Elastic",
        "Jina AI"
    ]
}