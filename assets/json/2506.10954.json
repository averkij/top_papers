{
    "paper_title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks",
    "authors": [
        "Lianghong Guo",
        "Yanlin Wang",
        "Caihua Li",
        "Pengyu Yang",
        "Jiachi Chen",
        "Wei Tao",
        "Yingtian Zou",
        "Duyu Tang",
        "Zibin Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the traditional process for creating such benchmarks is notoriously challenging and labor-intensive, particularly in the stages of setting up evaluation environments, grading test outcomes, and validating task instances. In this paper, we propose SWE-Factory, an automated pipeline designed to address these challenges. To tackle these issues, our pipeline integrates three core automated components. First, we introduce SWE-Builder, a multi-agent system that automates evaluation environment construction, which employs four specialized agents that work in a collaborative, iterative loop and leverages an environment memory pool to enhance efficiency. Second, we introduce a standardized, exit-code-based grading method that eliminates the need for manually writing custom parsers. Finally, we automate the fail2pass validation process using these reliable exit code signals. Experiments on 671 issues across four programming languages show that our pipeline can effectively construct valid task instances; for example, with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per instance, while with Gemini-2.5-flash, it achieves comparable performance at the lowest cost of $0.024 per instance. We also demonstrate that our exit-code-based grading achieves 100% accuracy compared to manual inspection, and our automated fail2pass validation reaches a precision of 0.92 and a recall of 1.00. We hope our automated pipeline will accelerate the collection of large-scale, high-quality GitHub issue resolution datasets for both training and evaluation. Our code and datasets are released at https://github.com/DeepSoftwareAnalytics/swe-factory."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 4 5 9 0 1 . 6 0 5 2 : r SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks Lianghong Guo1, Yanlin Wang1,, Caihua Li1, Pengyu Yang1, Jiachi Chen1, Wei Tao2,, Yingtian Zou3, Duyu Tang3, Zibin Zheng1 1Sun Yat-sen University, 2Independent Researcher, 3Huawei guolh8@mail2.sysu.edu.cn, wangylin36@mail.sysu.edu.cn, wtao@ieee.org"
        },
        {
            "title": "Abstract",
            "content": "Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the traditional process for creating such benchmarks is notoriously challenging and labor-intensive, particularly in the stages of setting up evaluation environments, grading test outcomes, and validating task instances. In this paper, we propose SWE-Factory, an automated pipeline designed to address these challenges. To tackle these issues, our pipeline integrates three core automated components. First, we introduce SWE-Builder, multi-agent system that automates evaluation environment construction, which employs four specialized agents that work in collaborative, iterative loop and leverages an environment memory pool to enhance efficiency. Second, we introduce standardized, exitcode-based grading method that eliminates the need for manually writing custom parsers. Finally, we automate the fail2pass validation process using these reliable exit code signals. Experiments on 671 issues across four programming languages show that our pipeline can effectively construct valid task instances; for example, with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per instance, while with Gemini-2.5-flash, it achieves comparable performance at the lowest cost of $0.024 per instance. We also demonstrate that our exit-codebased grading achieves 100% accuracy compared to manual inspection, and our automated fail2pass validation reaches precision of 0.92 and recall of 1.00. Our analysis shows all precision errors are due to an error2pass phenomenon, and further analysis on these cases reveals that including them in benchmarks unfairly underestimates model capabilities, making it essential to filter them out. We hope our automated pipeline will accelerate the collection of large-scale, highquality GitHub issue resolution datasets for both training and evaluation. Our code and datasets are released at https://github.com/DeepSoftwareAnalytics/ swe-factory."
        },
        {
            "title": "Introduction",
            "content": "The GitHub issue resolution task, which involves addressing real-world software issues like bug fixes and feature enhancements [4, 18], is crucial aspect of software maintenance [10, 8]. Given its practical importance, the task has become key benchmark for evaluating the software engineering capabilities of Large Language Models (LLMs) [22, 12, 19, 23, 6, 28, 3, 21, 13]. prominent example is SWE-bench [10], large-scale benchmark for evaluation, whose success has inspired many subsequent benchmarks that extend coverage to more languages and issue types [26, 8, 2, 23, 25]. More recently, inspired by the rise of Reinforcement Learning with Verifiable Rewards (RLVR), Corresponding authors. researchers have begun to construct interactive training environments to improve model capabilities on this task [16, 24, 9]. For example, environments like SWE-Gym [16] provide the automated, test-based feedback necessary for such training and have demonstrated significant potential to enhance model performance on this task. Figure 1: Pipeline of GitHub issue resolution data collection. While many datasets for the GitHub issue resolution task have been proposed in recent studies [10, 25, 8], constructing them is still challenging and costly. This difficulty is primarily because the process is highly labor-intensive. Specifically, the overall process, as shown in Figure 1, involves four distinct stages, three of which are labor-intensive, and each presents unique difficulties: (1) The Raw Issue Collection. In this stage, the GitHub API and predefined patterns are used to automatically extract issue descriptions, test patches for verification, ground truth patches, etc. (2) The Evaluation Environment Construction. In this stage, the runtime environment and evaluation scripts are built to ensure that the tests for each issue can be executed correctly, often requiring substantial manual setup as environment configuration and dependency resolution are highly project-specific; (3) The Grading System Construction. This stage builds grading tool to complement the evaluation environment. The purpose of this tool is to extract the test status from test logs automatically. Traditionally, this requires manually writing unique parsers (e.g., using regular expressions), time-consuming process because log formats vary significantly across different projects and test frameworks; (4) The Fail2pass Validation. In this stage, each task instance is validated by checking whether it transitions from fail to pass after applying the gold patch, process that traditionally relies on manual inspection of test logs. In summary, the primary challenge of this data collection pipeline is its heavy reliance on manual effort. This challenge manifests across the latter three stages of the process in the following ways: P1 Manual Evaluation Environment Construction. The diversity of programming languages and repository configurations leads to highly varied dependencies and test commands. Additionally, each repository may have multiple versions, and dependencies or test frameworks often change between versions, making environment setup particularly complex. P2 Manual Grading Tool Construction. Different issues and repositories use variety of test frameworks, command-line options, and log formats. As result, custom parser code must be written for each case to extract pass/fail outcomes, which greatly increases implementation complexity. P3 Manual Fail2Pass Validation. This stage requires extensive manual inspection of test logs before and after applying the gold patch to verify the fail-to-pass transition, resulting in significant labor costs. In this paper, we propose SWE-Factory, an automatic issue resolution benchmark construction pipeline designed to address these challenges. First, inspired by recent advances in large language models and agent-based methods [19, 28, 22], we introduce SWE-Builder, multi-agent framework for automating evaluation environment construction (addressing P1). This framework includes repository explorer that automatically collects all necessary setup information from the repository, while an Environment Manager and Test Manager collaborate to construct the evaluation environment, which comprises the Dockerfile and corresponding test script. The entire workflow is orchestrated by Test Analyst, which validates the generated environment and, in case of failure, diagnoses the issue to provide targeted feedback for the next refinement iteration. To further enhance efficiency and accelerate the setup process, our framework utilizes an Evaluation Environment Memory Pool. This approach is based on the observation that issues from nearby versions often share similar runtime environments and test scripts. The memory pool reuses these previously validated setups as high-quality references. This reuse of existing work helps to streamline the construction 2 of new evaluation environments. Second, motivated by the observation that most mainstream test frameworks report test outcomes via exit codes [1, 11, 14, 17], we standardize test status collection by capturing the exit code from test commands (addressing P2). By adding fixed marker for exit code output, our system can robustly and uniformly extract test results without the need for complex, custom parser code. Finally, based on exit code signals, we automate the fail2pass validation process, reducing the manual effort required to verify whether applying the gold patch transitions each instance from fail to pass (fail2pass) (addressing P3). We evaluate SWE-Factory on 671 issues sampled from 12 open-source repositories across four popular programming languages: Python, Java, JavaScript, and TypeScript. Through these experiments, we identify several key findings: (1) With GPT-4.1-mini, SWE-Builder successfully constructs 269 valid task instances out of 671 issues (40.1%) at an average cost of $0.045 per instance. With Gemini-2.5-flash, SWE-Builder constructs 225 valid instances (33.5%) with the lowest cost of $0.024, while with DeepSeek-v3-0324, SWE-Builder produces 232 valid task instances (34.6%). We also observe notable differences in performance across languages: with DeepSeek-v3-0324, SWE-Builder performs best on Python and JavaScript, while with GPT-4.1-mini, it constructs the most valid task instances on Java and TypeScript. (2) We manually assess the accuracy of the exit-code-based grading method by reviewing 2,085 test logs. Exit codebased judgments are perfectly consistent with manual inspection in all cases, demonstrating that this method can reliably determine test outcomes. (3) We evaluate the effectiveness of exit-code-based fail2pass validation by comparing its results with manual annotation. Our automatic method achieves precision of 0.92 and recall of 1.00. Further analysis shows that all false positives are due to the error2pass phenomenon, where the test suite cannot be executed without applying the ground truth patch due to errors such as missing dependencies or functions. Through case study, we demonstrate that such error2pass issues are unsuitable for benchmark construction, as they can underestimate model capability if included. Our main contributions are summarized as follows: We present SWE-Factory, the first open-source automatic pipeline for constructing GitHub issue resolution benchmarks across multiple languages and repositories. Our approach automates all key stagesevaluation environment construction, grading, and fail2pass validationto minimize manual intervention. Both code and datasets are released at https: //github.com/DeepSoftwareAnalytics/swe-factory. We propose SWE-Builder, multi-agent framework that automates evaluation environment construction. SWE-Builder leverages specialized agents for repository information retrieval, Dockerfile, and evaluation script generation, and iterative test analysis. Experiments show our approach can efficiently construct high-quality environments: for example, using GPT-4.1-mini, SWE-Builder generate 269 valid task instances out of 671 issues (40.1%) at an average cost of $0.045 per instance We propose an exit-code-based automatic grading and fail2pass validation approach for benchmark construction. Manual evaluation of 2,085 test logs shows perfect accuracy in test outcome detection. Building on this, our automated fail2pass validation, applied to 1,030 raw task instances, achieves precision of 92% and recall of 100% compared to manual annotation. Through further analysis, we identify and characterize the error2pass phenomenon, where tests fail to run before patching due to errors such as missing dependencies or functions. These error2pass cases account for all remaining errors in automatic validation, and our findings provide practical guidance for filtering out unsuitable cases when constructing high-quality benchmarks."
        },
        {
            "title": "2 SWE-Factory",
            "content": "In this section, we describe the details of SWE-Factory. Our framework consists of four main stages: (1) collecting raw GitHub issue data, (2) automatically setting up the environment using SWE-Builder, (3) grading results based on exit codes, and (4) performing exit-code-based fail2pass validation. Since SWE-bench already provides reliable script for collecting GitHub issues, we directly use their code for the first step. Therefore, our work mainly focuses on automating the remaining three steps. First, we present SWE-Builder, multi-agent system for automatic environment setup, and explain its agent roles, memory mechanism, and workflow in Sections 2.1, 2.2, and 2.3. Next, we introduce our exit-code-based grading method in Section 2.4. Finally, we describe our approach for exit-code-based fail2pass validation in Section 2.5. 3 Figure 2: Framework overview of SWE-Builder. 2.1 SWE-Builder: Design of Agent Roles Inspired by the construction pipeline for GitHub issue resolution datasets, we have tailored four collaborative agent roles to simulate manual data collection process with four agents: the repository explorer, the environment manager, the test manager, and the test analyst. The overview of SWEBuilder is presented in Figure 2. We elaborate on each agents role in the following. 2.1.1 Repository Explorer The repository explorer is responsible for automatically collecting the information essential for constructing an evaluation environment for each issue. For each target repository, it autonomously extracts (1) environment dependencies from configuration files (e.g., requirements.txt for Python, pom.xml for Java); (2) the relevant test commands (such as pytest or unittest for Python, mvn test for Java); and (3) additional setup details from documentation files (e.g., README.md, CONTRIBUTING.md) or installation scripts. By automating this process, the Repository Explorer efficiently handles the environment diversity across different projects without manual intervention. Algorithm 1: The workflow of Repository Explorer Input: Issue q, repository R, LLM L, max rounds , agent prompt Output: Summary of setup information Reset the context: , None; for = 1 to do actions, status, summary L(C); if status is true then summary; return for each API call aj in actions do oj Execute aj on R; Update with oj; summary; return Algorithm 1 demonstrates the workflow of repository explorer. This agent begins by initializing its context with task-specific prompt. In each iteration, the agent queries the LLM with the current context to decide which repository APIs to invoke and whether sufficient setup information has been collected. Three core APIs are available: browse_file(file_path, custom_query), which calls the LLM to extract information relevant to the custom query from given file; browse_directory(file_path, depth), which returns the structure of the specified directory up to given depth; and search_file_by_keyword(keyword), which retrieves file paths containing the target keyword. After executing the selected API calls and updating the context with the new observations, the agent summarizes the collected information. This process repeats until either the 4 agent determines all necessary setup information has been obtained or the maximum number of rounds is reached, at which point the most recent summary is returned. 2.1.2 Environment Manager The environment manager is responsible for constructing reliable runtime environment that ensures all tests related to the target issue can be executed correctly. To achieve this, the agent outputs Dockerfile, which scripts the complex environment configuration and installation commands into reproducible and portable format. The Environment Manager operates in stepwise manner: after receiving comprehensive environment information from the context retrieval agent, it automatically generates or updates the Dockerfile to match the requirements of the repository and the specific issue. Importantly, the agent preserves its generation history across iterations, ensuring that all previous modifications are retained for robust and reproducible environment setup. In cases where generation fails, the previous Dockerfile is used as fallback to guarantee continuity. 2.1.3 Test Manager The primary responsibility of the test manager is to automatically generate shell script designed to execute tests relevant to given issue. Its workflow commences only after its prerequisite agentsthe repository explorer and environment managerhave completed their tasks. Leveraging the repository setup information from the Repository Explorer and the finalized Dockerfile from the Environment Manager, the test manager crafts the script to run within the specified containerized environment. Like the environment manager, it maintains persistent context across iterations, ensuring that all script generation history is preserved. Besides, to facilitate the automated parsing of test results, we have designed standardized output mechanism. This approach leverages the common convention in software testing where scripts exit code signals its outcome: zero for success and non-zero value for failure. Specifically, the system automatically appends set of commands to each generated evaluation script to capture and report this status. For instance, after the main test command executes, the script immediately runs rc=$? to capture the exit code, followed by echo \"OMNIGRIL_EXIT_CODE=$rc\" to print it. This creates distinct and reliable marker in the scripts output, allowing for direct and robust determination of the tests outcome simply by parsing for this unique identifier. 2.1.4 Test Analyst The test analyst is responsible for evaluating the quality of the generated evaluation environment (comprising the Dockerfile and evaluation script) and for orchestrating the subsequent multi-agent iterations. Its workflow begins after it receives these artifacts from the environment manager and test manager. The agent operates on the fundamental assumption that well-constructed environment must allow the issues ground-truth patch to pass all relevant tests. To verify this, the test Analyst first attempts to build the runtime environment using the Dockerfile applies the ground-truth patch and then executes the evaluation script. If the build is successful and all tests pass, the environment configuration is considered valid, and the process is complete. If any step fails, however, the agent analyzes the resulting error logs to diagnose the issue and generates refinement plan with targeted guidance for the relevant agents, which initiates the next iteration. 2.2 SWE-Builder: Evaluation Environment Memory Pool To enhance the efficiency and consistency of creating evaluation environments, we introduce the evaluation environment memory pool, component designed for the reuse of previously successful setups. This approach is motivated by key observation: for issues within the same code repository, their required dependency environments and testing frameworks are often highly similar, especially for those from nearby versions. Building an evaluation environment from scratch for each issue is, therefore, both inefficient and prone to inconsistency. The memory pool addresses this challenge by archiving every successfully validated evaluation configuration, including both the dockerfile and the test script. Subsequently, when the Environment Manager and Test Manager handle new issue, they first query the pool to find setups from the same repository and, from these, retrieve reference environment setup from nearby software version to use as baseline. This strategy of 5 reusing pre-existing setup aims to accelerate the generation process and improve consistency across evaluation environments. 2.3 SWE-Builder: Orchestration of Different Agents The workflow of SWE-Builder is an iterative process that orchestrates the four agents to construct and refine valid evaluation environment, as detailed in Algorithm 2. The process begins with comprehensive initial iteration. In this first step, the Repository Explorer collects setup information while the system retrieves relevant reference from the Evaluation Environment Memory Pool. This combined information serves as the initial context for the Environment Manager and Test Manager to generate the first versions of the Dockerfile and evaluation script, which are then passed to the Test Analyst for validation. In contrast, subsequent iterations are not exhaustive but are instead targeted based on feedback from the Test Analyst. If validation attempt fails, the Test Analyst identifies the cause of the error and provides specific guidance to the agent responsible for the flawed component. For example, if only the test script contains an error, the workflow invokes only the Test Manager to generate revised script, bypassing the other agents. The corrected script is then passed back for another round of validation. This refinement cycle continues until the environment is successfully validated or the maximum number of iterations is reached. 2.4 Exit-Code-Based Automatic Grading Mehthod prevalent convention in software engineering is the use of process exit codes to signal test outcomes, where an exit code of zero typically signifies success, and non-zero value indicates failure. This practice is adopted by wide range of testing frameworks, such as pytest, Maven, and npm. Our system leverages this robust convention to implement an automated grading mechanism. As established in Section 2.1.3, our Test Manager integrates this logic directly into each evaluation script. It appends commands to capture the exit code of the primary test command and report it in standardized format. As shown in Figure 3, the eval.sh, the script is modified to include commands that print unique marker to the output log (e.g., OMNIGRIL_EXIT_CODE=0). This design simplifies the grading process to parse this single, predictable line from the test output to determine the result. This exit-code-based method presents significant advantage over approaches that require custom log parsers. The output formats of test logs can vary substantially across different programming languages, frameworks, and configurations, making the development of custom parsers laborintensive and error-prone task. In contrast, our approach provides uniform interface for assessing test outcomes, thereby decreasing manual inspection or complex, issue-specific parsing logic. Figure 3: Case study exit-code-based grading. 2.5 Exit-Code-Based Fail2Pass Validation Fail2pass validation is critical step in constructing high-quality benchmark for the GitHub issue resolution task, following the methodology of SWE-bench [10]. The purpose of this validation is to ensure the validity of each evaluation environment by confirming that its test suite fails before the ground-truth patch is applied and passes after. Traditionally, however, this process presents major Algorithm 2: SWE-Builder Agent Workflow Input: Issue I; evaluation environment memory pool M; Max iteration number ; Context retrieval agent Agentrepo; Dockerfile generation agent Agentdocker; Evaluation script generation agent Agentscript Output: Dockerfile D, evaluation script Initialization: Srepo, Sdockerfile, Sscript False; // Srepo: collected information ready // Sdockerfile: Dockerfile generated // Sscript: evaluation script generated for = 1 to do if !Srepo then collected_information = Agentrepo.run(); if collected_information then Srepo True; Agentdocker.update_context(collected_information); Agentscript.update_context(collected_information); reference_val_env = M.retrieve_closet_version(I); Agentdocker.update_context(reference_val_env.dockerfile); Agentscript.update_context(reference_val_env.eval_script); if Srepo and !Sdockerfile then D, ok = Agentdocker.run(); if ok then Sdockerfile True; if Srepo and Sdockerfile and !Sscript then S, ok = Agentscript.run(D); if ok then Sscript True; if Srepo and Sdockerfile and Sscript then analysis = test_analysis_agent.run(D, S); if analysis.is_finish then M.update(I, D, S); return D, S; if analysis.guidance_collected_information then Srepo False; Agentrepo.update_context(analysis.guidance_retrieval); if analysis.guidance_docker then Sdockerfile False; Agentdocker.update_context(analysis.guidance_docker); if analysis.guidance_eval_script then Sscript False; Agentscript.update_context(analysis.guidance_eval_script); bottleneck, as it often requires manually inspecting numerous complex test reports to determine the preand post-patch outcomes, making it an extremely labor-intensive task. Our implementation automates this entire validation process by leveraging the exit-code-based grading system detailed in Section 2.4. We execute the evaluation script both before and after applying the ground-truth patch, classifying the outcome of each run based on simple rule: an exit code of 0 signifies pass, while any non-zero value indicates fail. Only instances that exhibit this clear fail-to-pass transition are retained, ensuring the quality of the final benchmark."
        },
        {
            "title": "3 Evaluation Setup",
            "content": "3.1 Evaluation Dataset Construction Using the issue collection pipeline from SWE-bench [10], we build dataset of 2,441 issues from 12 open-source repositories, referred to as SweSetupBench. All selected repositories are well-recognized 7 open-source projects, each with over 2.5k GitHub stars. These repositories span four programming languagesPython, Java, and JavaScriptwhich are among the most popular languages according to GitHub statistics. All issues included in the dataset are created before March 1st, 2025. Given the substantial computational cost of running LLM-based agent tools, we construct smaller evaluation set, SweSetupBench-lite, by performing stratified sampling based on the repository version. Specifically, we randomly sample 20% of the issues from each version. For versions with very few issues, we ensure that at least one issue is included to maintain comprehensive coverage. As result, SweSetupBench-lite contains 671 issues from 12 repositories across four languages. We use this subset for evaluation. The detailed statistics of SweSetupBench-lite are presented in Table 1. Table 1: Repository statistics for SweSetupBench-lite. Repository Name Language # Instances # Versions Time Span # Stars pallets/click python-attrs/attrs python-pillow/Pillow assertj/assertj checkstyle/checkstyle eclipse-vertx/vert.x mochajs/mocha iamkun/dayjs nodejs/undici apollographql/apollo-client tailwindlabs/tailwindcss reduxjs/redux-toolkit Python Python Python Java Java Java JavaScript JavaScript JavaScript TypeScript TypeScript TypeScript 32 34 132 39 77 70 60 28 23 78 72 26 11 24 48 32 49 14 45 7 17 29 25 13 20142025 20162025 20132025 20132025 20152025 20162024 20122025 20182023 20242025 20162025 20172025 20212025 16.5k 5.5k 12.8k 2.7k 8.6k 14.5k 22.8k 47.9k 6.9k 19.6k 88.3 11.0k 3.2 Evaluation Details Model Selection. We select three models: gpt-4.1-mini-2025-04-14, gemini-2.5-flash-preview-0417,and deepseek-chat-v3-03242, to evaluate the effectiveness of SWE-Builder. Considering the high cost of running LLM-based agents, we do not use some of the most advanced models such as GPT-4.1 and Claude-4. The statistics of the selected base models are shown in Table 2. Table 2: Statistics of selected base models. Model Input Cost Output Cost Release Date gpt-4.1-mini-2025-04-14 gemini-2.5-flash-preview-04-17 deepseek-chat-v3- $0.40 / 1M tokens $0.15 / 1M tokens $0.30 / 1M tokens April 14, 2025 $1.60 / 1M tokens $0.60 / 1M tokens April 17, 2025 $0.80 / 1M tokens March 24, 2025 Hyperparameter Settings. In our experiments, we set the maximum number of iterations for SWE-Factory to 5. The temperature of the base models is set to 0.2. We also set the maximum number of retrieval rounds for the context retrieval agent to 10. In addition, we run the experiments with 20 parallel subprocesses to improve efficiency. Evaluation Metrics. In evaluation, we use evaluation metrics as follows: Valid Rate (%): The proportion of instances for which manual inspection confirms that the generated instance passes fail-to-pass validation. An instance is considered valid if it can be validated as passing after initially failing. Success Rate (%): The proportion of tasks where our method successfully outputs result. Time (min): The average time required to process each task. Cost ($): The average inference and evaluation cost per task. Average Iteration ($): The average iteration number of SWE-Builder. 2We collect API Cost of deepseek-chat-v3-0324 from openrouter."
        },
        {
            "title": "4 Evaluation",
            "content": "4.1 RQ1: How Effective is SWE-Builder? In this section, we evaluate the effectiveness of SWE-Builder on the SweSetupBench-lite dataset. The evaluation employs three cost-effective LLMs: GPT-4.1-mini-2025-04-14, Gemini-2.5-flashpreview-04-17, and DeepSeek-v3-0324. This evaluation aims to assess the frameworks practical performance in generating evaluation environments and its capability for benchmark construction. We focus on two primary metrics: Success Rate and Valid Rate. The Success Rate measures the proportion of tasks for which SWE-Builder successfully generates complete evaluation environment (i.e., Dockerfile and an evaluation script) and also passes its own internal self-assessment. The Valid Rate, more strict metric that aligns with the Fail-to-Pass (F2P) standard from SWE-bench [10], represents the fraction of total tasks that not only complete automatically but also pass rigorous manual verification, confirming their quality for use as benchmark instances. Table 3 presents comprehensive summary of the results. Across the entire SweSetupBench-lite dataset, with GPT-4.1-mini, SWE-Builder demonstrates the highest overall effectiveness, achieving Success Rate of 57.2% and, more importantly, Valid Rate of 40.1%, which corresponds to 269 successfully validated task instances out of 671. While SWE-Builder with GPT-4.1-mini leads in effectiveness, Gemini-2.5-flash proves to be the most cost-effective solution at only $0.024 per task. breakdown by programming language reveals model-specific strengths. DeepSeek-v3-0324 excels on Python tasks (71.2% Success, 43.4% Valid), while GPT-4.1-mini shows superior performance on TypeScript (64.8% Success, 54.0% Valid) and also leads in both Java and JavaScript. These language-specific results demonstrate the frameworks capability to handle issues across diverse programming languages. In summary, our evaluation demonstrates that SWE-Builder, when paired with cost-effective LLMs, can produce significant number of valid task instances at reasonable cost. The framework also exhibits strong capability to construct evaluation environments for issues from software repositories in multiple languages. Table 3: Models performance on the full dataset and four language subsets. In this table, GPT-4.1mini-2025-04-14 is short for GPT-4.1-mini , gemini-2.5-flash-preview-04-17 is short for gemini-2.5flash, deepseek-v3-0324 is short for deepseek-v3. means higher is better, means lower is better. Model Dataset Valid Rate (%) Success Rate (%) Time (min) Cost ($) Avg Iter GPT-4.1-mini Gemini-2.5-flash DeepSeek-v Full Python Java TS JS Full Python Java TS JS Full Python Java TS JS 40.1 (269/671) 39.4 ( 78/198) 28.5 ( 53/186) 54.0 ( 95/176) 38.7 ( 43/111) 33.5 (225/671) 29.8 ( 59/198) 19.4 ( 36/186) 48.3 ( 85/176) 40.5 ( 45/111) 34.6 (232/671) 43.4 ( 86/198) 11.8 ( 22/186) 43.8 ( 77/176) 42.3 ( 47/111) 57.2 (384/671) 63.1 (125/198) 40.9 ( 76/186) 64.8 (114/176) 62.2 ( 69/111) 49.8 (334/671) 48.0 ( 95/198) 36.0 ( 67/186) 60.2 (106/176) 59.5 ( 66/111) 50.8 (341/671) 71.2 (141/198) 22.6 ( 42/186) 52.8 ( 93/176) 58.6 ( 65/111) 22.4 16.0 27.5 26.7 18.8 27.0 19.4 32.1 30.1 27.6 22.5 13.8 27.5 28.7 19. 0.045 0.034 0.072 0.033 0.039 0.024 0.014 0.042 0.022 0.013 0.043 0.029 0.078 0.032 0.026 3.21 3.05 3.74 2.92 3.04 3.57 3.63 3.88 3.20 3.51 3.45 2.62 4.32 3.56 3. 4.2 RQ2: Can the Grading Method Based on Exit Code Accurately Reflect Test Status? In Section 2.4, we propose using the exit code from the test command as an automatic indicator of test outcomes: an exit code of 0 is interpreted as pass (all tests succeed), while any non-zero exit code is interpreted as fail (at least one test fails or an error occurs). 9 To evaluate the reliability of this grading method, we manually inspect test reports generated from 1,059 unique evaluation environments (as described in Section 4.1). For each environment, we execute the test script twiceonce before and once after applying the gold patchyielding theoretical total of 2,118 test runs. During this process, we exclude 24 cases in which the evaluation environment failed to build successfully, as well as 9 cases where the test script was unable to produce valid exit code, typically due to missing explicit exit statements or unexpected script termination. After these exclusions, we obtain 2,085 evaluable test reports. Each report is labeled as either pass or fail, and we compare these manual labels with the exit-code-based results to assess accuracy. Table 4 shows that our exit-code-based approach achieves good agreement with manual inspection, yielding 100% accuracy in both pre-fix and post-fix settings. We attribute this result to the standardized behavior of major testing frameworks, which consistently return 0 for full success and nonzero values for failures or errors. By capturing the exit code after test execution, our framework can reliably determine test status without needing to parse the varied and complex formats of test log outputs. Table 4: Accuracy of exit-code-based grading compared to manual test report inspection across different evaluation environment sources. Environment Source # Total Judged Accuracy (%) GPT-4.1-mini DeepSeek-v3-0324 Gemini-2.5-flash Total 765 670 650 2, 100.0 100.0 100.0 100.0 4.3 RQ3: Can We Conduct Fail2pass Validation Based on Exit Code? In Section 4.2, we find that the exit code can accurately reflect the pass/fail status of test execution. Building on this observation, we further investigate whether exit codes can be used for automatic fail2pass validation. In the context of issue resolution benchmarks such as SWE-bench, fail2pass validation is critical step: only those environments where applying the gold patch transforms test results from failing to passing are retained as high-quality benchmark cases. Our aim is to determine whether exit code alone is sufficient to perform this filtering reliably and automatically. We begin with the set of task instances with evaluation environments generated in Section 4.1, totaling 341 for DeepSeek-v3, 384 for GPT-4.1-mini, and 334 for Gemini-2.5-flash. For each task instance, we execute the test suite both before and after applying the gold patch, recording the corresponding exit codes. If either test run (preor post-patch) fails due to environment build errors or does not produce valid exit code (e.g., missing exit statements or unexpected script termination), the task instance is excluded from further analysis. After this filtering step, we retain 329, 381, and 320 task instances for DeepSeek, GPT, and Gemini, respectively. For each of these, we automatically label task instance as fail2pass if the pre-patch exit code is nonzero and the post-patch exit code is zero. Following the protocol of SWE-bench, we manually inspect the preand post-patch test reports for each retained task instance and annotate whether it constitutes true fail2pass case. We then compare the automatic results with the manual annotations using standard metrics such as precision and recall. As shown in Table 5, the exit-code-based method achieves perfect recall (1.00) and high precision (0.93 for DeepSeek, 0.93 for GPT, and 0.90 for Gemini) in identifying fail2pass cases. This shows that using the exit code alone is an effective strategy for automatic fail2pass validation. However, since small number of false positives still exist, manual verification remains necessary to ensure the quality of the benchmark. After observing that the exit-code-based method achieves high recall but less-than-perfect precision, we further investigate the cause of the false positives (FP) by manually reviewing all FP cases. Our analysis reveals that these FP instances are actually special subclass of fail2pass, where the test suite cannot be executed before the patch is applied but becomes runnable after patching. We refer to this phenomenon as error2pass. Figure 4 from the python-attrs__attrs-830 case study illustrates this phenomenon. Before the patch is applied (left panel), an ImportError causes the test framework to crash during its collection phase, meaning no tests were actually executed. After applying the gold patch (right panel), this structural error is resolved, allowing all 21 tests to run and pass successfully. 10 Table 5: Results of fail2pass validation based on exit code. Data Sources From # Task Instances # TP # FP # TN # FN Precision (%) Recall (%) DeepSeek-V3 GPT-4.1-mini Gemini-2.5-flash Total 329 381 320 1,030 226 269 718 16 19 25 60 87 93 72 252 0 0 0 93 93 90 92 100 100 100 100 Figure 4: Case study of error2pass phenomenon: python-attrs__attrs-830. The error2pass phenomenon reveals fundamental flaw when such cases are used in benchmark for issue resolution. The issue lies in the tight coupling between the solution code and the test code, as illustrated in the python-attrs__attrs-830 example in Figure 5. Here, the gold patch introduces new function named to_bool, and the test is simultaneously updated to import and rely on that exact function name. This creates an unfair evaluation scenario. model, which is not provided with the test file, might generate logically perfect solution but name the function slightly differentlyfor instance, to_boolean. Although this patch correctly resolves the issue, it would be marked as failure by the benchmark due to an ImportError in the test. Consequently, error2pass cases do not accurately measure models issue resolution ability but rather its ability to guess specific, hidden implementation detail. To avoid underestimating model performance, it is essential to distinguish these cases from true fail2pass instances and exclude them when constructing the dataset. Figure 5: Import dependency of the test path on the gold patch in python-attrs__attrs-830."
        },
        {
            "title": "5 Related Work",
            "content": "5.1 Datasets for GitHub Issue Resolution Recently, many benchmarks have been established to evaluate the ability of large language models (LLMs) to resolve real-world GitHub issues. Among them, SWE-bench [10] is the most widely used, providing 2,294 Python issues with paired post-PR test suites for automated evaluation. SWE11 bench Verified [15] further improves reliability by manually validating 500 instances to ensure clarity and solvability. For multilingual and multimodal evaluation, OmniGIRL [8] offers 959 issues spanning Python, JavaScript, TypeScript, and Java, supporting both multimodal and multilingual assessment. SWE-bench Multimodal [23] similarly contributes tasks containing code snippets and screenshots to test models ability to integrate visual and textual information. In addition, range of other benchmarks and datasets have been proposed to cover diverse programming languages and evaluation needs [25, 26, 2]. Besides, to improve the ability of models to resolve GitHub issues, researchers have introduced number of training environments that support large-scale training and automated evaluation. For example, SWE-Gym [16] provides 2,438 Python tasks with Docker-based environments and unit tests for agent training. R2E-Gym [9] procedurally generates over 8,700 executable environments using automated test synthesis and back-translation. SWE-smith [24] automates the construction of Python execution environments and synthesizes over 50,000 training tasks from 128 repositories, supporting agent training of models. 5.2 Automatic Environment Setup Method The use of Large Language Models (LLMs) to automate repository-level environment setup has recently emerged as direct response to the tedious and complex nature of this task. This trend is exemplified by studies like ExecutionAgent [5], which presents an LLM agent capable of generating scripts to build and test projects. Further evaluations of LLM setup capabilities on hundreds of Python and JVM repositories have demonstrated that the task remains significant challenge for current models [7]. However, while these works address general environment setup, their objectives, and methods are not directly applicable to the construction of issue resolution datasets, task with more specialized requirements. Subsequently, line of research has focused specifically on automated environment setup for the issue resolution task. RepoLaunch [27], for instance, introduced an agentic system to automate environment construction for GitHub issue resolution in Python. Nevertheless, its pipeline still requires manual intervention for writing grading logic, such as parser code, and for performing the final fail-to-pass (F2P) validation. Following this, SetupAgent [20] advanced this capability by automating both environment construction and parser code generation. Still, this approach also necessitates manual F2P validation and is not open-source. In contrast, SWE-Factory makes three key contributions that fully automate the data construction pipeline. First, for environment construction, our multi-agent framework, SWE-Builder, successfully automates the setup process. Our experiments demonstrate that it can construct hundreds of valid task instances across multiple programming languages at reasonable cost. Second, for grading, we introduce an effective method that uses test command exit codes to determine outcomes, which entirely removes the need for developing custom log parsers. Third, building on this exit-code mechanism, we automate the fail-topass (F2P) validation, process proven to be highly accurate in our evaluation. Furthermore, our entire framework is released as open-source to ensure reproducibility and facilitate future research."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we propose SWE-Factory, an automatic benchmark construction pipeline for the GitHub issue resolution task. Unlike traditional workflows that require intensive manual effort in evaluation environment setup, grading system construction, and fail2pass validation, we achieve automation in these stages. For environment construction, we introduce multi-agent framework called SWEBuilder that replaces manual evaluation environment setup with collaboration among LLM-based agents. For grading system and fail2pass validation, we leverage an exit-code-based strategy to robustly extract test outcomes without the need for manual parsing or inspection. In our experiments, we show that SWE-Builder achieves strong performance in automatic environment construction; for example, using GPT-4.1-mini, SWE-Factory successfully constructs 269 valid benchmark instances out of 671 sampled issues across 12 open-source repositories. Besides, experiments show that our exit-code-based grading achieves perfect agreement with manual evaluation, and our automatic fail2pass validation exhibits high precision. We also identify the error2pass phenomenoncases where tests cannot be executed before patchingwhich should be carefully filtered when building high-quality benchmarks. We hope our automated pipeline will accelerate the collection of large-scale, high-quality GitHub issue resolution datasets for training and evaluation."
        },
        {
            "title": "References",
            "content": "[1] Exit status. https://en.wikipedia.org/wiki/Exit_status, 2024. Accessed: 2025-0611. [2] Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang. Swe-bench+: Enhanced coding benchmark for llms. arXiv preprint arXiv:2410.06992, 2024. [3] Daman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi, Aditya Kanade, and Nagarajan Natarajan. Masai: Modular architecture for softwareengineering ai agents. arXiv preprint arXiv:2406.11638, 2024. [4] Tegawendé F. Bissyandé, David Lo, Lingxiao Jiang, Laurent Réveillère, Jacques Klein, and Yves Le Traon. Got issues? who cares about it? large scale investigation of issue trackers from github. In ISSRE, pages 188197. IEEE Computer Society, 2013. [5] Islem Bouzenia and Michael Pradel. You name it, run it: An llm agent to execute tests of arbitrary projects. arXiv preprint arXiv:2412.10133, 2024. [6] Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu, Guoliang Dong, Artem Aliev, et al. Coder: Issue resolving with multi-agent and task graphs. arXiv preprint arXiv:2406.01304, 2024. [7] Aleksandra Eliseeva, Alexander Kovrigin, Ilia Kholkin, Egor Bogomolov, and Yaroslav Zharov. Envbench: benchmark for automated environment setup. arXiv preprint arXiv:2503.14443, 2025. [8] Lianghong Guo, Wei Tao, Runhan Jiang, Yanlin Wang, Jiachi Chen, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, and Zibin Zheng. Omnigirl: multilingual and multimodal benchmark for github issue resolution. arXiv preprint arXiv:2505.04606, 2025. [9] Naman Jain, Jaskirat Singh, Manish Shetty, Liang Zheng, Koushik Sen, and Ion Stoica. R2egym: Procedural environments and hybrid verifiers for scaling open-weights swe agents. arXiv preprint arXiv:2504.07164, 2025. [10] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [11] JUnit. User guide: Launcher api exit codes. https://junit.org/junit5/docs/current/ user-guide/#launcher-api-exit-codes, 2024. Accessed: 2025-06-11. [12] Yizhou Liu, Pengfei Gao, Xinchen Wang, Chao Peng, and Zhao Zhang. Marscode agent: Ai-native automated bug fixing. arXiv preprint arXiv:2409.00899, 2024. [13] Yingwei Ma, Qingping Yang, Rongyu Cao, Binhua Li, Fei Huang, and Yongbin Li. How to understand whole software repository? arXiv preprint arXiv:2406.01422, 2024. [14] Mocha. Mocha exit code. https://mochajs.org/#mocha-exit-code, 2024. Accessed: 2025-06-11. [15] OpenAI. Swe-bench verified: human-validated subset for ai model evaluation. https: //openai.com/index/introducing-swe-bench-verified, 2024. Accessed: 2024-1021. [16] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139, 2024. [17] pytest. pytest exit codes. https://docs.pytest.org/en/latest/reference/ exit-codes.html, 2024. Accessed: 2025-06-11. 13 [18] Wei Tao, Yucheng Zhou, Yanlin Wang, Hongyu Zhang, Haofen Wang, and Wenqiang Zhang. KADEL: knowledge-aware denoising learning for commit message generation. ACM Trans. Softw. Eng. Methodol., 33(5):133:1133:32, 2024. [19] Wei Tao, Yucheng Zhou, Yanlin Wang, Wenqiang Zhang, Hongyu Zhang, and Yu Cheng. MAGIS: LLM-based multi-agent framework for github issue resolution. arXiv preprint arXiv:2403.17927, 2024. [20] Konstantinos Vergopoulos, Mark Niklas Müller, and Martin Vechev. Automated benchmark generation for repository-level coding tasks. arXiv preprint arXiv:2503.07701, 2025. [21] Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Opendevin: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. [22] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024. [23] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793, 2024. [24] John Yang, Kilian Leret, Carlos Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents. arXiv preprint arXiv:2504.21798, 2025. [25] Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, et al. Multi-swe-bench: multilingual benchmark for issue resolving. arXiv preprint arXiv:2504.02605, 2025. [26] Daoguang Zan, Zhirong Huang, Ailun Yu, Shaoxin Lin, Yifan Shi, Wei Liu, Dong Chen, Zongshuai Qi, Hao Yu, Lei Yu, et al. Swe-bench-java: github issue resolving benchmark for java. arXiv preprint arXiv:2408.14354, 2024. [27] Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, Shengyu Fu, et al. Swe-bench goes live! arXiv preprint arXiv:2505.23419, 2025. [28] Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pages 15921604, 2024."
        }
    ],
    "affiliations": [
        "Huawei",
        "Independent Researcher",
        "Sun Yat-sen University"
    ]
}