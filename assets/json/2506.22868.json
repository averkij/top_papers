{
    "paper_title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing",
    "authors": [
        "Junsung Lee",
        "Junoh Kang",
        "Bohyung Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Previous text-guided video editing methods often suffer from temporal inconsistency, motion distortion, and-most notably-limited domain transformation. We attribute these limitations to insufficient modeling of spatiotemporal pixel relevance during the editing process. To address this, we propose STR-Match, a training-free video editing algorithm that produces visually appealing and spatiotemporally coherent videos through latent optimization guided by our novel STR score. The score captures spatiotemporal pixel relevance across adjacent frames by leveraging 2D spatial attention and 1D temporal modules in text-to-video (T2V) diffusion models, without the overhead of computationally expensive 3D attention mechanisms. Integrated into a latent optimization framework with a latent mask, STR-Match generates temporally consistent and visually faithful videos, maintaining strong performance even under significant domain transformations while preserving key visual attributes of the source. Extensive experiments demonstrate that STR-Match consistently outperforms existing methods in both visual quality and spatiotemporal consistency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 8 6 8 2 2 . 6 0 5 2 : r STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing Junsung Lee1 Junoh Kang1 Bohyung Han1,2 ECE1 & IPAI2, Seoul National University {leejs0525, junoh.kang, bhhan}@snu.ac.kr Figure 1: Generated videos using our proposed algorithm, STR-Match. Our proposed algorithm, STR-Match, successfully performs flexible domain transformations while preserving the visual information of the source video during the video editing process. It is also applicable to various scenarios, including large motion, multi-object, and background editing."
        },
        {
            "title": "Abstract",
            "content": "Previous text-guided video editing methods often suffer from temporal inconsistency, motion distortion, andmost notablylimited domain transformation. We attribute these limitations to insufficient modeling of spatiotemporal pixel relevance during the editing process. To address this, we propose STR-Match, training-free video editing algorithm that produces visually appealing and spatiotemporally coherent videos through latent optimization guided by our novel STR score. The score captures spatiotemporal pixel relevance across adjacent frames by leveraging 2D spatial attention and 1D temporal modules in text-to-video (T2V) diffusion models, without the overhead of computationally expensive 3D attention mechanisms. Preprint. Under review. Integrated into latent optimization framework with latent mask, STR-Match generates temporally consistent and visually faithful videos, maintaining strong performance even under significant domain transformations while preserving key visual attributes of the source. Extensive experiments demonstrate that STR-Match consistently outperforms existing methods in both visual quality and spatiotemporal consistency. Project page: https://jslee525.github.io/str-match"
        },
        {
            "title": "Introduction",
            "content": "Diffusion models [1, 2, 3] are the leading framework for high-fidelity image and video generation using text prompts. Their applications now extend to tasks such as text-guided image and video editing, where the goal is to generate outputs aligned with target text prompts while preserving regions consistent with both the source and target prompts in the original content. The overall process of text-guided image editing typically involves generating target image guided by information extracted during the forward or reconstruction process of the source imagemost often through latent optimization or attention injection, though few methods adopt alternative approaches [4, 5, 6, 7, 8, 9, 10, 11]. While text-guided image editing methods have demonstrated impressive editing capabilities, directly applying them to video editing presents several challenges, including frame inconsistency and undesired motion change. To achieve strong video editing performance while addressing these issues, many prior works [12, 13, 14, 15] leverage pretrained text-to-image (T2I) models augmented with additional components. Some other recent works [16, 17, 18, 19] adopt text-to-video (T2V) models to tackle these problems. However, these methods still suffer from the same issues and exhibit degraded performance in challenging scenarios (e.g., large domain shifts). These limitations in text-guided video editing stem from inadequate modeling of spatiotemporal pixel relevance, which is crucial for producing natural and coherent video content. To address these challenges, we introduce STR-Match, training-free algorithm that generates videos via latent optimization guided by novel STR score. The STR score, defined as the multiplicative combination of selfand temporal-attention maps, captures spatiotemporal pixel relevance across adjacent frames by combining 2D spatial and 1D temporal attention from text-to-video (T2V) diffusion model, without relying on costly 3D attention mechanisms. This joint formulation enables more effective optimization than using the attention components separately, ultimately improving video quality. Integrated into latent optimization framework with masking strategy, STR-Match produces temporally consistent, high-fidelity outputs, effectively handling challenging editing cases and maintaining the key visual attributes of the source. Our primary contributions are summarized as follows: We introduce STR-Match, novel training-free text-guided video editing approach built upon pretrained T2V diffusion models. It matches spatiotemporal information in the generation process (target latents) to that of the forward process (source latents) via latent optimization, optionally incorporating latent masking strategy for improved preservation of source content. This design addresses key limitations of existing methods stemming from insufficient modeling of spatiotemporal pixel relevances. To obtain spatiotemporal information, we propose the STR score, spatiotemporal pixel relevance score that combines selfand temporal-attention maps without requiring full 3D attention. The STR score also enables flexible optimization, resulting in enhanced overall video quality. Through extensive experiments on various video editing tasks, we demonstrate that STRMatch outperforms existing training-free video editing approaches both quantitatively and qualitatively. STR-Match generates temporally coherent, high-fidelity videos with flexible domain transformations, while preserving the visual integrity of the source video. It consistently outperforms prior methods in these aspects."
        },
        {
            "title": "2 Related works",
            "content": "2.1 Text-to-video diffusion model Recent works [20, 21] build on diffusion models by extending pretrained text-to-image (T2I) architectures. These methods commonly introduce lightweight 1D temporal modules into 2D spatial backbones, enabling efficient video generation while preserving the visual priors learned from T2I models. While previous T2V models such as VideoCrafter2 [20] and LaVie [21] extend pretrained T2I architectures by inserting lightweight temporal modules into 2D spatial backbones, more recent approaches aim to capture richer spatiotemporal pixel relevances through full 3D attention. Building on advances in efficient attention computation frameworks such as xFormers [22] and FlashAttention [23], the latest T2V models [24, 25] incorporate 3D full attention into their architectures. For example, CogVideoX [24] and Open-Sora-2.0 [25] adopt 3D autoencoding architectures with integrated 3D full attention, leveraging FlashAttention to enable efficient attention computation. However, these models typically compute attention outputs without explicitly retaining attention maps, which limits their applicability in tasks requiring controllable attentionsuch as fine-grained video editing."
        },
        {
            "title": "2.2 Training-free video editing methods",
            "content": "T2I-based video editing methods With the rapid progress of image editing works [4, 5, 6, 7, 8, 9, 10, 11], recent works [12, 13, 14, 15] leverage pretrained T2I models with addtional components to complement frame consistency. FateZero [12] manipulates attention maps using binary masks from cross-attention and improves temporal consistency by warping middle-frame features during diffusion. Ground-A-Video [13] leverages external modelssuch as GLIGEN [26], RAFT [27], ZoeDepth [28], and ControlNet [29]to guide attention modulation with attention maps. FLATTEN [14] manipulates attention maps to follow patch trajectories derived from optical flow [27], aiming to maintain frame consistency. VideoGrain [15] modulates both selfand cross-attention to address multi-grain video editing tasks, relying on external methods [6, 14] to enhance frame consistency. Although these T2I-based methods have demonstrated strong editing capabilities, they still struggle from temporal inconsistency and motion distortion. Moreover, many of these approaches rely on attention injection, which can disrupt the computational graph of the pretrained model and often lead to visual artifacts. T2V-based video editing methods In contrast to T2I-based approaches, several recent methods [16, 17, 18, 19] leverage pretrained T2V models to address temporal consistency in the video editing task. For example, DMT [17] utilizes pretrained T2V model and introduces feature descriptor extracted from intermediate layers to guide latent optimization for motion preservation. MotionFlow [16] incorporates losses from cross-, self-, and temporal-attention, along with mask-based manipulation, to preserve motion information in the source video. Zhang et al. [18] extracts motion patterns using temporal modules and applies frame-to-frame consistency loss during generation. These approaches utilize latent optimization, which preserves the pretrained models computational process, allowing for smoother outputs with fewer visual artifacts. However, these methods primarily focus only on motion guidance, which often leads to modifications in unwanted regions (e.g., backgrounds). While UniEdit [19] attempts to address these issues by applying attention injection to edit appearance or motion in source videos, it often suffers from texture misalignment in the foreground and background regions."
        },
        {
            "title": "3 Preliminary",
            "content": "Text-to-video diffusion model We summarize the basic concept of pretrained text-to-video diffusion models as we use the models to perform text-guided video editing. The key components of text-tovideo model is an encoder Enc(), decoder Dec(), and noise prediction network ϵθ(). Encoder spatially and temporally compresses video vector RF HW 3 to latent vector z0 Rf hwc, and decoder decompresses the latent vector to the video vector. The noise prediction network learns the distribution of latent vectors z0, and is trained to minimize following objective function: Ez0,c,t,ϵ[ϵθ(zt, t, c) ϵ2 2], (1) 3 Figure 2: Illustration of overall STR-Match framework. We first perform forward diffusion process, and extract the STR score Ωsrc STR,t from the source video. Then, the target latent is initialized , and during the generation process, we extract the target STR score Ωtgt as ztgt = zsrc STR,t and optimize the latent ztgt using negative cosine similarity between the source and target STR scores. To further preserve unediting regions, we optionally apply latent mask strategy using binary mask . where z0 denotes the video latent, is the corresponding text prompt, is the diffusion timestep, and zt = αtz0 + σtϵ for ϵ (0, I). αt and σt are predifined constants satisfying α0 = 1, σ0 = 0, and σT /αT 1. Attention modules While text-to-video diffusion models extend the success of text-to-image models by incorporating temporal modules, we specifically focus on two critical features: spatial self-attention map and temporal-attention map. Spatial self-attention map, whose dimension is Rf hnn, captures relevances between pixels within each frame, where denotes the number of frames, represents the number of pixels per frame, and indicates the number of attention heads. For the rest of the paper, we denote p, {1, 2, ...n} for the spatial location of pixel and i, {1, 2, ...f } for the frame number. Combining these, Ii(p) represents the pixel at location in i-th frame. Then, the self-attention map element Attn(Ii(p) Ii(q)) can be interpreted as importance of Ii(q) to Ii(p) in 2D spatial space. Similarly, temporal-attention map, whose dimension is Rnhf , encodes inter-frame relevances for each pixel, and the element Attn(Ii(p) Ij(p)) represents the importance of Ij(p) to Ii(p) in 1D temporal space."
        },
        {
            "title": "4 Methods",
            "content": "Many text-guided image editing methods [4, 5, 6, 7, 9] manipulate attention maps, demonstrating that modeling pixel relevances is crucial for effective image editing. Likewise, we expect that spatiotemporal pixel relevances in videos are essential for effective video editing. To this end, we propose the STR score, which captures spatiotemporal relevances between pixels across different frames by leveraging selfand temporal-attention maps. It is an aggregation of bidirectional pixel relevances across adjacent frames, efficiently capturing spatiotemporal information and enabling the extraction of key visual attributes from the source video. By integrating the STR score into latent optimization framework, as illustrated in Figure 2, we enable video editing that preserves source content while achieving high visual quality with flexible domain shifts. 4.1 STR score: SpatioTemporal Relevance score To quantitavely represent relevance between two pixels Ii(p) and Ij(q) in spatiotemporal space, we define two functions: bidirectional relevance g(, ), and directional relevance g( ). The directional relevance g(Ii(p) Ij(q)) quantifies the importance of Ij(q) to Ii(p) in spatiotemporal space, and intuitively, it is expected to be large if both the importance of Ij(p) to Ii(p) and Ij(q) to Ij(p) are high, or the importance of Ii(q) to Ii(p) and Ij(q) to Ii(q) are high. From this motivation, we define directional relevance between Ij(q) given Ii(p) as g(Ii(p) Ij(q)) := Attn(Ii(p) Ij(p)) Attn(Ij(p) Ij(q)) + Attn(Ii(p) Ii(q)) Attn(Ii(q) Ij(q)), (2) Figure 3: Illustration of STR score. (Left) The bidirectional pixel relevance in the spatiotemporal space g(Ii(p), Ij(q)) is computed by summing two directional relevance scores along opposite directions. (Right) Each figure illustrates the directional pixel relevance, g(Ii(p) Ij(q)) and g(Ij(q) Ii(p)), both of which are computed solely through pixel-wise multiplication of selfand temporal-attention maps. for Attn( ) defined in Section 3. The bidirectional relevance g(Ii(p), Ij(q)) extends the directional relevance by considering the connection between Ii(p) and Ij(q) in both directions, as illustrated in Figure 3. Specifically, it is defined as sum of the importance of Ij(q) to Ii(p) and the importance of Ii(p) to Ij(q): g(Ii(p), Ij(q)) := g(Ii(p) Ij(q)) + g(Ij(q) Ii(p)). (3) Notably, the bidirectional relevance is fully computed from selfand temporal-attention maps without requiring any additional training or models. To capture spatiotemporal information in the source videosuch as motion and structural layoutwe aggregate bidirectional pixel relevances across adjacent frames into unified representation, termed the STR score. The STR score ΩSTR, or spatiotemporal pixel relevance, is formally defined as follows: (cid:88) ΩSTR(i, p, q) = g(Ii(p), Ij(q)), (4) where (i) is set of neighboring frame numbers to the i-th frame. jN (i) 4.2 Overall framework: STR-Match STR,t at every timestep and noisy latent zsrc The overall procedure of our method is illustrated in Figure 2 and Algorithm 1. We first solve the forward diffusion process of the source video. During the forward process, we extract STR scores Ωsrc as initial point, we perform generation process with latent optimization. For each denoising step, we first optimize the latent variable ztgt , and then solve diffusion process with the optimized latents. The optimization is performed with the following equation: ztgt ztgt . Then, starting from ztgt = zsrc STR,t, Ωtgt Lcos(Ωsrc λztgt STR,t), (5) where Lcos is negative cosine similarity, and λ is hyperparameter for controlling the guidance strength. The equation is designed to maximize the cosine similarity between the source and target STR scores, encouraging the spatiotemporal pixel relevances in the target video to align with those of source video to promote preservation of spatiotemporal information. Since the optimization process preserves the computational graph of the pretrained model, it enables the generation of smooth, high-quality videos while maintaining key visual information from the source. Moreover, since ΩSTR is conceptually defined as the element-wise product of selfand temporal-attention maps, it enables more flexible optimization compared to using them independently, thereby further enhancing video quality. Latent mask strategy To better preserve regions that are not intended to be edited (e.g., backgrounds), we mix the optimized latent with the latent obtained during the forward process at the same timestep. For binary mask , where values are 1 for regions to be edited and 0 otherwise, and 5 Algorithm 1 STR-Match 1: Input: zsrc 0 (source video), psrc (source prompt embedding), ptgt (target prompt embedding), Φ() (ODE solver), (foreground binary mask, optional) , ϵsrc , t, psrc) , + 1) STR,t from ϵθ() 2: Hyperparameter: λ (coefficient of negative cosine similarity) 3: for = 0 to 1 do ϵsrc ϵθ(zsrc 4: Compute and save Ωsrc 5: t+1 Φ(zsrc zsrc 6: 7: end for 8: ztgt zsrc 9: for = to 1 do Obtain ϵθ(ztgt 10: Compute Ωtgt 11: ztgt ztgt ϵtgt ϵθ(ztgt t1 Φ(ztgt ztgt if use latent mask then , t, [ptgt; psrc]) STR,t from ϵθ() Lcos(Ωsrc , t, [ptgt; psrc]) , ϵtgt , 1) STR,t, Ωtgt λztgt STR,t) 12: ztgt t1 (1 dilate(M )) zsrc end if t1 + dilate(M ) ztgt t1 13: 14: 15: 16: 17: 18: end for 19: Result: ztgt 0 (target video) latents zsrc updated as obtained during the forward diffusion process of source video, the final target latents are ztgt (1 dilate(M )) zsrc + dilate(M ) ztgt . (6) This masking strategy ensures to preserve non-target regions in the source video during editing. The latent binary mask is resized and dilated version of segmentation map of editing region of the source video. The dilate function is applied to help flexible shape modification."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Implementation details Throughout the experiments, STR-Match is implemented using LaVie [21] as the pretrained T2V model, with the hyperparameter λ = 0.01, and optimized using SGD. For extreme cases of qualitative results (e.g., cat basketball), we select λ from the range [0.005, 0.015]. For efficient inference, we extract the STR score based on selfand temporal-attention maps, excluding those from the finest resolution. We compare our method against recent training-free video editing algorithms: FateZero [12], Ground-A-Video (GAV) [13], FLATTEN [14], VideoGrain [15], DMT [17], and UniEdit [19]. For T2I-based methods (FateZero, Ground-A-Video, FLATTEN, VideoGrain), we follow their official implementations. For T2V-based baselines (DMT, UniEdit), we adopt LaVie [21] as the pretrained T2V model to ensure fair comparison. We employ video segmentation model SAM-Track [30] to obtain binary mask , and OWL-ViT [31], an object detection model to obtain bounding boxes for Ground-A-Video. For more detailed description for base model and external model used in implementation, please see Table 3 in Appendix B. The number of diffusion timesteps is set to 50 and classifier-free guidance scale [32] is 7.5. We use L2 loss with λ = 0.08 for optimization using concatenation of selfand temporal-attention maps in the ablation study, which we find to be an effective weight. For all experiments, we utilize single NVIDIA L40S GPU with 48 GB of memory. Quantitative evaluation protocol For quantitative evaluation, we collect total of 54 videos, each consisting of 16 frames, comprising samples from the TGVE dataset [33] and additional videos sourced from the Internet 1. We utilize VideoLLaMA3-7B [34], pretrained video captioning model, 1https://www.pexels.com baby sleeping baby lotus daisy zebra horse cat basketball fish sweet potato u s s ) M / ( ) a / ( t i o e N A A a e Figure 4: Qualitative comparisons between STR-Match and existing methods. In each example, STR-Match demonstrates stronger foregroundbackground texture alignment, higher visual fidelity, better motion alignment, and more flexible shape transformation compared to recent existing methods. Please check our project page for edited videos. to obtain concise prompts of source videos automatically, and randomly change nouns to construct the corresponding target prompt. We measure four metrics to evaluate the fidelity and fatihfulness of the edited videos to source video and target prompt. Frame Consistency (FC) suggested in VBench [35] measures the smoothness of videos, leveraging motion priors in the frame interpolation model [36]. CLIP Similarity (CS) computes the average CLIP score [37] between the target prompt and edited video. BG-LPIPS (BL) calculates the Learned Perceptual Image Patch Similarity (LPIPS) score [38] between maksed frames of the source and generated videos, where the mask is 1 for regions to preserve. Motion Error (ME) quantifies the average motion difference between the source and generated videos. It is calculated as the pixel-wise differences of optical flows between each video pair, where the optical flows are obtained using RAFT-Large [27]. 5.2 Qualitative results Key features of STR-Match Figure 1 demonstrates STR-Matchs robust editing performance, highlighting its flexibility in challenging scenariossuch as transforming objects into entirely different categories, handling large motion, performing multi-object editing, and modifying background. For instance, transforming cat into basketball or giraffe demonstrates STR-Matchs ability to faithfully adapt object shapes to target prompts without being overly anchored to the original shape. Moreover, changing cat into dragon or robot dogobjects unlikely to appear in the original sceneillustrates STR-Matchs effective integration of edited elements with the background. These examples emphasize how STR-Match manages domain-shifted objects and significant shape changes, while ensuring the edited elements blend naturally with the background. This combination of flexibility, visual quality, and motion preservation makes STR-Match powerful tool for diverse video editing tasks. 7 Comparison to other editing methods Figure 4 compares STR-Match with recent video editing baselines, showing that our method achieves sharper visual fidelity, tighter foregroundbackground texture alignment, and more faithful shape transformations. In the baby sleeping baby case, DMT, UniEdit, and VideoGrain tint the infant while leaving the background gray, whereas STR-Match maintains consistent tonality across the entire frame by capturing spatiotemporal pixel relevance through the STR score. In the lotus daisy example, several baselines either fail to replace the lotus at all or succeed only by unintentionally changing the background. On the other hand, STR-Match successfully replaces the lotus with high fidelity while preserving the background intact. The same trend holds on more dynamic contents. In the zebra horse example, most prior methods either fail to capture the horses leg motion (e.g., lifting its leg) or degrade appearance quality, while Ground-A-Video further disrupts scene consistency. In contrast, STR-Match faithfully reproduces the motion with high visual fidelity. Furthermore, STR-Match demonstrates strong performance even in extreme video editing scenarios. In the cat basketball example, most existing methods fail to transform the cat into basketball, while DMT generates basketball at the cost of undesired background changes. Similarly, in the fish sweet potato case, DMT and FLATTEN partially modify the object but suffer from background distortion or low fidelity, and other methods fail to perform the edit. In contrast, STR-Match successfully transforms the object with high visual fidelity while preserving the background. In summary, STR-Match enables high-fidelity, and flexible shape transformation in video editing while preserving spatiotemporal information. 5.3 Quantitative comparison We quantitatively evaluate STR-Match against existing training-free video editing methods for four metrics: temporal consistency (FC), fidelity to the target prompt (CS), background preservation (BL), and motion preservation from the source video (ME). STR-Match, with and without binary masks, achieves strong performance, as evidenced by its large area in the radar graph shown in Figure 5. Notably, compared to T2I-based editing methods, STR-Match demonstrates superior frame consistency, indicating that the proposed STR score effectively captures spatiotemporal pixel relevances from the T2V model. Furthermore, when comparing STR-Match with masks to UniEdit (red solid and orange lines), both of which utilize SAM-Track, STR-Match outperforms in all evaluated metrics. In the comparison between STR-Match without masks and DMT (red dashed and green lines), the scores reveal that STR-Match more effectively captures key information from the source video, such as background and motion, while maintaining comparable fidelity. This suggests that the STR score achieves goldilocks balancepreserving essential details from the source video while maintaining the flexibility required for high-fidelity editingunlike methods that either over-preserve, reducing fidelity, or under-preserve, diminishing faithfulness. 5.4 Ablation Study Figure 5: Quantitative comparison between STR-Match and existing methods. The solid red line is STR-Match with the binary mask, and the dashed red line is STR-Match without binary mask. The solid lines are T2V-based editing methods, while dotted lines are T2Ibased methods. We provide exact metric numbers and analysis in Table 3 of Appendix B. Flexibility of STR score To evaluate the effectiveness of our proposed STR score, we compare STR-Match with baseline that optimizes the concatenation of selfand temporal-attention maps. For the baseline, we adjust the guidance strength λ to ensure that the edited video retains key attributes of the source, such as motion dynamics. Figure 6 shows that STR-Match produces significantly higher quality videos compared to the baseline. For instance, in the dog cat case, the baseline method r e e ) M / ( O ) M / ( Table 1: Quantitative comparison between STR-Match and the baseline without mask. Bold numbers indicate the better score for each metric. dog cat shark turtle Method Baseline Ours FC () CS () BL () ME () 2.293 0.979 0.981 2.402 0.117 0.216 31.24 31.61 Table 2: Ablation study on λ values. Bold black and red numbers indicate the best and second-best scores for each metric, respectively. λ 0.005 0.01 0. FC () CS () BL () ME () 0.982 3.120 2.402 0.981 2.225 0.979 31.60 31.61 31.33 0.271 0.216 0.196 Figure 6: Quantitative comparision between STRMatch and the baseline. generates oversaturated colored video and in the turtle shark case, it fails to alter the sharks shape into that of turtles. These two examples illustrate that naïvely using selfand temporal-attention maps as guidance imposes overly strict constraints, whereas the proposed STR score effectively captures key features while providing sufficient flexibility for editing, as it optimizes values that are conceptually derived from the element-wise multiplication of selfand temporal-attention maps. Moreover, Table 1 supports this conclusion, as fidelity-related metrics (FC and CS) are higher for our method. Although the baseline better preserves background and motion, it often fails to transform objects, as demonstrated in Figure 6. Ablation on the hyperparameter λ λ is the only hyperparameter in STR-Match, which controls the guidance strength during optimization. To investigate its effect, we conduct an ablation study with three values of λ: 0.005, 0.01, 0.015. As shown in Table 2, we empirically observe that smaller values of λ yield higher fidelity scores (FC, CS) but struggle to preserve background and motion dynamics, whereas larger values promote preservation at the cost of fidelity. To balance these objectives, we adopt λ = 0.01 for all experiments."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose novel spatiotemporal modeling approach that relates to key limitations in existing video editing methodssuch as frame inconsistency, motion distortion, visual artifacts, and notably, limited performance in challenging settings like large-gap domain shifts. To overcome these challenges, we propose the STR score, spatiotemporal pixel relevance score that captures essential video attributes. Notably, it is computed solely from the selfand temporal-attention maps of pretrained text-to-video (T2V) diffusion model, requiring no additional training or external models. By integrating the STR score into latent optimization framework alongside latent mask strategy, we introduce STR-Match, zero-shot, training-free video editing algorithm that is compatible with any T2V model incorporating temporal modules. Extensive experiments show that STR-Match consistently outperforms existing training-free methods across all quantitative metrics. Moreover, it generates videos with substantially improved visual quality, supporting realistic and flexible domain transformatiomn, preserved motion dynamics, and strong temporal consistency. These results demonstrate both the effectiveness and generalizability of STR-Match, establishing it as new state-of-the-art baseline for training-free text-guided video editing."
        },
        {
            "title": "References",
            "content": "[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 9 [2] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. [3] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. [4] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In ICCV, 2023. [5] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. In ICLR, 2023. [6] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In CVPR, 2023. [7] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In SIGGRAPH, 2023. [8] Hyunsoo Lee, Minsoo Kang, and Bohyung Han. Diffusion-based conditional image editing through optimized inference with guidance. In WACV, 2025. [9] Qi Si, Bo Wang, and Zhao Zhang. Contrastive learning guided latent diffusion model for image-to-image translation. arXiv preprint arXiv:2503.20484, 2025. [10] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In CVPR, 2023. [11] Junsung Lee, Minsoo Kang, and Bohyung Han. Diffusion-based image-to-image translation by noise correction via prompt interpolation. In ECCV, 2024. [12] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In ICCV, 2023. [13] Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zero-shot grounded video editing using text-to-image diffusion models. In ICLR, 2024. [14] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, JuanManuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: Optical flow-guided attention for consistent text-to-video editing. In ICLR, 2024. [15] Xiangpeng Yang, Linchao Zhu, Hehe Fan, and Yi Yang. Videograin: Modulating space-time attention for multi-grained video editing. In ICLR, 2025. [16] Tuna Han Salih Meral, Hidir Yesiltepe, Connor Dunlop, and Pinar Yanardag. Motionflow: Attention-driven motion transfer in video diffusion models. arXiv preprint arXiv:2412.05275, 2024. [17] Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, and Tali Dekel. Space-time diffusion features for zero-shot text-driven motion transfer. In CVPR, 2024. [18] Xinyu Zhang, Zicheng Duan, Dong Gong, and Lingqiao Liu. Training-free motion-guided video generation with enhanced temporal consistency using motion consistency loss. arXiv preprint arXiv:2501.07563, 2025. [19] Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, and Jiang Bian. Uniedit: unified tuning-free framework for video motion and appearance editing. arXiv preprint arXiv:2402.13185, 2024. [20] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, 2024. 10 [21] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. In IJCV, 2024. [22] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. xformers: modular and hackable transformer modelling library. https://github.com/facebookresearch/xformers, 2022. [23] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS, 2022. [24] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In ICLR, 2025. [25] Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, et al. Open-sora 2.0: Training commercial-level video generation model in $200k. arXiv preprint arXiv:2503.09642, 2025. [26] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In CVPR, 2023. [27] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV, 2020. [28] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. [29] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. [30] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and Yi Yang. Segment and track anything. arXiv preprint arXiv:2305.06558, 2023. [31] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection. In ECCV, 2022. [32] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop, 2021. [33] Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jinbin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei Huang, Yuanxi Sun, et al. Cvpr 2023 text guided video editing competition. arXiv preprint arXiv:2310.16003, 2023. [34] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. [35] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. [36] Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, Chun-Le Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation. In CVPR, 2023. [37] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In EMNLP, 2021. [38] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018."
        },
        {
            "title": "A Qualitative results",
            "content": "A.1 Additional comparisons with other methods We provide video files on our project page that showcase variety of video editing examplesranging from typical cases with minimal domain shifts to more challenging ones with significant shape transformations, as illustrated in Figures 8 and 9. For instances involving extreme shape transformations (e.g. cat dragon, goldfish snake), many competing methods distort the target objects to conform to the shape of the source, resulting in unnatural edited outputs. In the cases of extreme domain change (e.g. cat robot dog, goldfish donuts), few other methods transfer only part of the intended concept while the majority fail to perform any meaningful editing. In contrast, STR-Match consistently delivers successful video edits across these challenging scenarios, highlighting its flexibility and robust editing capabilities. We strongly encourage readers to view the HTML file included in the zip archive for more comprehensive understanding of STR-Matchs editing capabilities. A.2 STR-Match with Zeroscope cat dog goldfish clownfish red roses orange tulips Figure 7: Qualitative results of STR-Match using Zeroscope. STR-Match can be applied to Zeroscope, achieving similar performance to LaVie. Our proposed algorithm leverages the pretrained T2V model equipped with temporal modules. While we utilize LaVie [21] as pretrained T2V model for the most of the experiment, STR-Match can also be applied to other T2V models, such as Zeroscope 2. Figure 7 illustrates the results of STR-Match using Zeroscope as the base model. The results demonstrate that STR-Match can effectively edit videos with Zeroscope, achieving similar performance to LaVie."
        },
        {
            "title": "B Quantitative metrics and model dependencies",
            "content": "Table 3 presents evaluation metrics used in the radar graph shown in Figure 5 of Section 5.3 in the main paper along with the base diffusion models and external models used by each method. Overall, the proposed method, STR-Match, whether applied with and without masks, achieves balanced and superior performance across all metrics compared to other methods. Notably, while DMT generates high quality videos (evidenced by strong FC and CS), these outputs often lack fidelity to the source video (reflected in poor scores for BL and ME). Although we have provided the quantitative metrics, we encourage readers to consult the qualitatve results in Figure 4 in the main paper, Appendix A, and supplementary material, as these metrics are incomplete and often fail to reflect the true quality of videos."
        },
        {
            "title": "C Limitations",
            "content": "While STR-Match produces satisfying editing results, even in the challenging scenarios like flexible shape transformations, it still has some limitations. One limitation is its inability to edit multiple objects into different targets simultaneously. Although workaround existsediting each object individually with its corresponding maskthis approach is highly inefficient. Additionally, while the 2https://huggingface.co/cerspense/zeroscope_v2_576w 3https://huggingface.co/CompVis/stable-diffusion-v1-4 4https://huggingface.co/stabilityai/stable-diffusion-2-1 5https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5 12 Table 3: Quantitative comparison and model dependencies between STR-Match and existing methods. For quantitative metrics, bold black and red numbers indicate the best and secondbest performance for each metric, respectively. Note that FC (Frame Consistency) and CS (CLIP Similarity) are higher-is-better metrics, while BL (BG-LPIPS) and ME (Motion Error) are lower-isbetter. Method FateZero [12] FLATTEN [14] VideoGrain [15] Ground-A-Video [13] External model Base model T2I (sd1.4 3) T2I (sd2.1 4) RAFT T2I (sd1.5 5) RAFT, SAM-Track, ControlNet ControlNet [29], GLIGEN [26] RAFT [27], ZoeDepth [28], OWLViT [31] T2I (sd1.5) DMT [17] UniEdit [19] T2V (LaVie) T2V (LaVie) SAM-Track STR-Match (Ours, w/o mask) T2V (LaVie) STR-Match (Ours, w/ mask) T2V (LaVie) SAM-Track FC () CS () BL () ME () 0.979 31.56 0.139 2. 0.980 31.43 0.277 2.748 0.978 31. 0.062 1.943 0.969 30.62 0.244 3. 0.981 31.94 0.499 5.741 0.979 31. 0.134 2.632 0.981 31.61 0.216 2. 0.981 31.68 0.103 1.932 method supports flexible shape transformations, it produces suboptimal results when the objects size varies significantly. We plan to address remaining limitations in future work."
        },
        {
            "title": "D Societal Impact",
            "content": "STR-Match is training-free video editing algorithm that leverages pretrained T2V models. Since it relies heavily on these pretrained models, there is potential risk of generating videos with unintended or inappropriate contents. However, we believe this issue can be indirectly mitigated by carefully controlling the training data used for the underlying T2V models. 13 cat dragon cat robot dog goldfish snake dog cat r s s ) M / ( ) M / ( D d U Z F T V i o V Figure 8: Additional qualitative comparisons between STR-Match and existing methods. This figure illustrates the performance of STR-Match in challenging scenarios, including cat dragon, cat robot dog, goldfish snake, and dog cat. 14 bird cat cat giraffe goldfish donuts goldfish clownfish u s s ) M / ( ) a / ( t i o e N A A a e Figure 9: Qualitative comparisons between STR-Match and existing methods. This figure illustrates the performance of STR-Match in challenging scenarios, including bird cat, cat giraffe, goldfish donuts, and goldfish clownfish."
        }
    ],
    "affiliations": [
        "Seoul National University"
    ]
}