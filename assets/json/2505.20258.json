{
    "paper_title": "ARM: Adaptive Reasoning Model",
    "authors": [
        "Siye Wu",
        "Jian Xie",
        "Yikai Zhang",
        "Aili Chen",
        "Kai Zhang",
        "Yu Su",
        "Yanghua Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While large reasoning models demonstrate strong performance on complex tasks, they lack the ability to adjust reasoning token usage based on task difficulty. This often leads to the \"overthinking\" problem -- excessive and unnecessary reasoning -- which, although potentially mitigated by human intervention to control the token budget, still fundamentally contradicts the goal of achieving fully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a reasoning model capable of adaptively selecting appropriate reasoning formats based on the task at hand. These formats include three efficient ones -- Direct Answer, Short CoT, and Code -- as well as a more elaborate format, Long CoT. To train ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy Optimization (GRPO), which addresses the format collapse issue in traditional GRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by an average of 30%, and up to 70%, while maintaining performance comparable to the model that relies solely on Long CoT. Furthermore, not only does it improve inference efficiency through reduced token generation, but it also brings a 2x speedup in training. In addition to the default Adaptive Mode, ARM supports two additional reasoning modes: 1) Instruction-Guided Mode, which allows users to explicitly specify the reasoning format via special tokens -- ideal when the appropriate format is known for a batch of tasks. 2) Consensus-Guided Mode, which aggregates the outputs of the three efficient formats and resorts to Long CoT in case of disagreement, prioritizing performance with higher token usage."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 8 5 2 0 2 . 5 0 5 2 : r ARM: Adaptive Reasoning Model Siye Wu Jian Xie Yikai Zhang Aili Chen Kai Zhang Yu Su Yanghua Xiao Fudan University The Ohio State University {siyewu24, jianxie22}@m.fudan.edu.cn, shawyh@fudan.edu.cn Project Page: https://team-arm.github.io/arm"
        },
        {
            "title": "Abstract",
            "content": "While large reasoning models demonstrate strong performance on complex tasks, they lack the ability to adjust reasoning token usage based on task difficulty. This often leads to the overthinking problemexcessive and unnecessary reasoningwhich, although potentially mitigated by human intervention to control the token budget, still fundamentally contradicts the goal of achieving fully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), reasoning model capable of adaptively selecting appropriate reasoning formats based on the task at hand. These formats include three efficient onesDirect Answer, Short CoT, and Codeas well as more elaborate format, Long CoT. To train ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy Optimization (GRPO), which addresses the format collapse issue in traditional GRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by an average of 30%, and up to 70%, while maintaining performance comparable to the model that relies solely on Long CoT. Furthermore, not only does it improve inference efficiency through reduced token generation, but it also brings 2 speedup in training. In addition to the default Adaptive Mode, ARM supports two additional reasoning modes: 1) Instruction-Guided Mode, which allows users to explicitly specify the reasoning format via special tokensideal when the appropriate format is known for batch of tasks. 2) Consensus-Guided Mode, which aggregates the outputs of the three efficient formats and resorts to Long CoT in case of disagreement, prioritizing performance with higher token usage."
        },
        {
            "title": "Introduction",
            "content": "The emergence of large reasoning models (LRMs) such as OpenAI-o1 [16] and DeepSeek-R1 [9] has led to unprecedented breakthroughs in problem-solving capabilities through test-time scaling [3; 51]. These models are designed to solve tasks using Long Chain-of-Thought (Long CoT), generating more tokens to achieve better performance. However, because they are primarily trained on tasks requiring intensive reasoning, LRMs tend to apply Long CoT uniformly across all tasks, resulting in the socalled overthinking problem [4; 37]. This issue refers to the excessive use of tokens for reasoning, which yields no performance gains and may introduce noise that misleads the model [46; 7]. While some efforts aim to reduce token usage in LRMs, they often rely on clear estimations of the token budget per task [1; 41] or require specialized, length-constrained model training [13]. In reality, such estimations are not always accurate, and more desirable solution is for models to adaptively control their token usage based on task complexity without human intervention. For example, for the first easy question in Figure 1a, answering directly is the ideal choice, whereas for the second, hard question, the use of Long CoT is precisely what is needed. Thus, Long CoT is not silver bullet; selecting an appropriate reasoning format is essential to balance efficiency and effectiveness. Project lead. Preprint. (a) Comparison of model reasoning behaviors on easy and hard tasks. (b) Accuracy vs. Token Cost. Figure 1: (a) Comparison of reasoning behaviors across different models on easy and hard tasks. The General Model fails on harder tasks without elaborate reasoning. The Reasoning Model applies Long CoT across all tasks, causing the overthinking phenomenon. In contrast, our proposed ARM adapts its reasoning formats based on task difficulty, answering easy questions efficiently while adopting Long CoT for hard tasks. (b) Accuracy versus token cost for Qwen2.5 under different training strategies. SFT, +GRPO, and +Ada-GRPO refer to models trained with SFT, SFT+GRPO, and SFT+Ada-GRPO, respectively. +Ada-GRPO consistently outperforms the expected trade-off line between SFT and +GRPO, demonstrating ARMs superior effectiveness-efficiency balance. In this work, we propose Adaptive Reasoning Model (ARM), reasoning model capable of adaptively selecting reasoning formats based on task difficulty, balancing both performance and computational efficiency. ARM supports four reasoning formats: three efficient onesDirect Answer, Short CoT, and Codeand one elaborate format, Long CoT. In addition to its adaptive selection mechanism (Adaptive Mode), ARM also supports an Instruction-Guided Mode, which allows explicit control over the reasoning format via special tokens, and Consensus-Guided Mode, which aggregates the outputs of the three efficient formats and resorts to Long CoT in case of disagreement. To train ARM, we adopt two-stage training framework. In Stage 1, we apply supervised fine-tuning (SFT) to equip the language model with foundational understanding of four reasoning formats. In Stage 2, we introduce Ada-GRPO, an adaptation of Group Relative Policy Optimization (GRPO) [33], which encourages efficient format selection while preserving accuracy as the primary objective. Ada-GRPO is designed to address two key issues: 1) The uniform distribution of reasoning formats regardless of task difficulty observed during the SFT stage; 2) The format collapse problem in GRPO, where Long CoT gradually dominates as training progresses, leading to the diminished use of other, more efficient formats. Extensive evaluations show that ARM trained with Ada-GRPO achieves comparable performance while using 30% fewer tokens than GRPO (as shown in Figure 1b), across both in-domain and out-of-domain tasks in commonsense, mathematical, and symbolic reasoning. Furthermore, by leveraging the three more efficient reasoning formats in the roll-out stage, Ada-GRPO achieves approximately 2 training speedup compared to GRPO. Our additional analysis further reveals that: 1) Adaptive Mode achieves superior balance between effectiveness and token efficiency by adaptively selecting suitable reasoning formats, while Instruction-Guided Mode performs well when the specified format is suitable for the task, and Consensus-Guided Mode prioritizes performance at the cost of higher token usage. 2) The choice of backbone model has limited impact on ARMs performance when using base or instruction-tuned models, which yield similar results; however, using the DeepSeek-R1-Distill backbone improves performance on hard tasks due to the advanced reasoning capability distilled from the strong teacher DeepSeek-R1, but leads to worse performance on easy tasks despite increased token cost. 3) Lengthpenalty-based strategies for improving LRM efficiency suffer from performance degradation as the token budget decreases, whereas ARM maintains stable performance. In summary, our contributions are three-fold: 1) We propose ARM, reasoning model that balances effectiveness and efficiency by adaptively selecting task-appropriate reasoning formats. Compared to the model that relies solely on Long CoT, ARM achieves comparable performance while significantly reducing token cost, saving an average of 30% and up to 70%. 2) In addition to the default Adaptive Mode, ARM also supports Instruction-Guided Mode, which performs well when the reasoning format is appropriately specified, and Consensus-Guided Mode, which maximizes performance at the cost of higher token usage. 3) We introduce Ada-GRPO, an adaptation of GRPO that addresses the format collapse problem and achieves 2 training speedup without compromising performance."
        },
        {
            "title": "2.1 Reinforcement Learning for Improving Reasoning",
            "content": "Reinforcement Learning (RL) has demonstrated significant potential in enhancing the problemsolving abilities of large language models (LLMs) across various domains [29; 42; 17]. Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has gained substantial attention for advancing LLM capabilities [18; 9; 21], resulting in the development of large reasoning models (LRMs) [47] such as OpenAI-o1 [16] and DeepSeek-R1 [9]. Based on simple rule-based rewards, RLVR algorithms such as Group Relative Policy Optimization (GRPO) [33] enable models to use Long Chain-of-Thought (Long CoT) [50; 15]. This facilitates deep reasoning behaviors, such as searching, backtracking, and verifying through test-time scaling [3; 51]. However, these models also suffer from significant computational overhead due to extended outputs across all tasks, leading to inefficiency associated with the overthinking phenomenon [4; 31; 37]. Verbose and redundant outputs can obscure logical clarity and hinder the models ability to solve problems effectively [46; 7]."
        },
        {
            "title": "2.2 Efficiency in Large Language Models",
            "content": "Recently, many studies have focused on improving the reasoning efficiency in LLMs. Some promptguided methods [10; 48; 19] explicitly instruct LLMs to generate concise reasoning outputs by controlling input properties such as task difficulty and response length. Other approaches [11; 5; 34] explore training LLMs to reason in latent space, generating the direct answer without the need for detailed language tokens. Several techniques have also been proposed to reduce inference costs by controlling or pruning output length, either by injecting multiple reasoning formats during the pre-training stage [36] or by applying length penalties during the RL stage [40; 2; 1; 13]. Many of these methods aim to strike trade-off between token budget and reasoning performance by shortening output lengths, often relying on clear estimations of the token budget for each task or requiring specialized, length-constrained model training. However, in reality, such estimations are not always accurate, and what we ultimately expect is for models to adaptively regulate their token usage based on the complexity of the task at hand. Therefore, in this work, we propose novel training framework that enables models to adaptively select suitable reasoning formats for given tasks by themselves, optimizing both performance and computational efficiency."
        },
        {
            "title": "3 Method",
            "content": "We propose Adaptive Reasoning Model (ARM), reasoning model designed to optimize effectiveness and efficiency by adaptively selecting reasoning formats. Specifically, ARM is trained in two stages: 1) Stage 1: Supervised Fine-tuning (SFT) for Reasoning Formats Understanding: In this stage, we use 10.8K diverse questions, each annotated with solutions in four distinct reasoning formats, to fine-tune the model and build foundational understanding of different reasoning strategies. 2) Stage 2: Reinforcement Learning (RL) for Encouraging Efficient Format Selection: We adopt an adapted version of the GRPO algorithm, named Ada-GRPO, to train the model to be capable of selecting more efficient reasoning formats over solely Long CoT, while maintaining accuracy."
        },
        {
            "title": "3.1 Stage 1: SFT for Reasoning Formats Understanding",
            "content": "In this stage, we leverage SFT as cold start to introduce the model to various reasoning formats it can utilize to solve problems.2 These formats include three efficient reasoning formats Direct Answer, Short CoT, and Code, as well as the elaborate reasoning format Long CoT. We use special tokens (e.g., <Code></Code>) to embrace thinking rationale. Specifically, 1) Direct Answer: This format provides direct answer without any reasoning chain, making it the most efficient in terms of token usage. 2) Short CoT: This format begins with short reasoning and then provides an answer, which has been proved effective in mathematical problems [43]. 3) Code: This format adopts code-based reasoning, which has proven effective across variety of tasks due to its structured process [44; 45; 20]. 4) Long CoT: This format involves more detailed, iterative reasoning process, 2In preliminary experiments, models without SFT failed to distinguish between the four reasoning formats, like producing mixed outputs, wrapping Short CoT response using the special tokens intended for Long CoT. 3 thus incurs higher token usage. It is suited for tasks requiring advanced reasoning capabilities, such as self-reflection and alternative generation, where those more efficient formats fall short [27; 9; 50]."
        },
        {
            "title": "3.2 Stage 2: RL for Encouraging Efficient Format Selection",
            "content": "After SFT, the model learns to respond using various reasoning formats but lacks the ability to adaptively switch between them based on the task (see Section 4.3 for details). To address this, we propose Adaptive GRPO (Ada-GRPO), which enables the model to dynamically select appropriate reasoning formats according to the task difficulty through format diversity reward mechanism. GRPO In traditional GRPO [33], the model samples group of outputs = {o1, o2, , oG} for each question q, where denotes the group size. For each oi, binary reward ri is computed using rule-based reward function that checks whether the prediction pred matches the ground truth gt: ri = 1passed(gt,pred). (1) However, since traditional GRPO solely optimizes for accuracy, it leads, in our setting, to overuse of the highest-accuracy format while discouraging exploration of alternative reasoning formats. Specifically, if Long CoT achieves higher accuracy than other formats, models trained with GRPO tend to increasingly reinforce it, leading to an over-reliance on Long CoT and reduced exploration of more efficient alternatives. We refer to this phenomenon as Format Collapse, which ultimately hinders the models ability to develop adaptiveness. We further analyze this in Section 4.3. Ada-GRPO We propose Ada-GRPO to address the format collapse issue. Specifically, Ada-GRPO amplifies the reward ri for less frequently sampled reasoning formats, preventing their disappearance and ensuring adequate learning. Formally, we scale the reward ri to by: = αi(t) ri, αi(t) = (oi) decayi(t), decayi(t) = (cid:18) + 0.5 1 (oi) (oi) (cid:19) (cid:18) (cid:18) 1 + cos π (cid:19)(cid:19) , (2) (3) (4) where (oi) denotes the number of times the reasoning format corresponding to oi appears within its group O, and represents the training step. αi(t) is format diversity scaling factor that gradually decreases from (oi) at the beginning of training (t = 0) to 1 at the end of training (t = ). We introduce αi(t) to extend GRPO into Ada-GRPO, enabling models to adaptively select reasoning formats. Specifically, αi(t) consists of two components: 1) Format Diversity Scaling Factor (oi) : To prevent premature convergence on the highest-accuracy format (i.e., format collapse to Long CoT), we upweight rewards for less frequent formats to encourage exploration. 2) Decay Factor decayi(t): To avoid long-term misalignment caused by over-rewarding rare formats, this term gradually reduces the influence of diversity over time. For example, (oi) might make the model favor lower-accuracy format like Short CoT over Long CoT simply because it appears less frequently and thus receives higher reward. While such exploration is beneficial early in training, it can hinder convergence later. The decay mechanism mitigates this by promoting diversity initially, then shifting focus to accuracy again as training progresses. Refer to Appendix for details of the decay factor. Then the group advantage ˆAi,k for all tokens in each output is computed based on the group of reshaped rewards = {r 1, 2, , G}: mean({r 1, std({r ˆAi,k = 1, 2, , 2, , G}) G}) . (5) Finally, we optimize the model by maximizing the following objective (see Appendix for details): JAdaGRPO(θ) =E (cid:104) (Q), {oi}G i=1 πθold (Oq) (cid:105) (cid:20) 1 (cid:88) i=1 (cid:105) ˆAi,k 1 oi oi (cid:88) (cid:110) k=1 min (cid:104) πθ(oi,kq, oi,<k) πθold (oi,kq, oi,<k) ˆAi,k, β KL [πθ πref ] (cid:111)(cid:21) . (6) clip (cid:18) πθ(oi,kq, oi,<k) πθold (oi,kq, oi,<k) (cid:19) , 1 ϵ, 1 + ϵ"
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Model To assess the effectiveness of our method across models of different sizes, we select Qwen2.5-Base-3B/7B/14B [49] as backbone models. We further examine models of the same family but fine-tuned on different datasets, specifically the Instruct [49] and DeepSeek-R1-Distill variants [9], which exhibit varying levels of base reasoning capabilities. detailed analysis is given in Section 5.3. Stage 1: We use AQuA-Rat [22] as the SFT dataset, as its answers can be Training Datasets naturally transformed into four distinct reasoning formats. In addition to the Direct Answer and Short CoT rationales provided with the dataset, we utilize GPT-4o [26] and DeepSeek-R1 [9] to supplement the Code and Long CoT rationales, respectively. To ensure the quality of the generated rationales, we filter out those that lead to incorrect answers, resulting in training set containing 3.0K multiple-choice and 7.8K open-form questions, each with four reasoning formats. Appendix provides further details on the generation and filtering process. Stage 2: To prevent data leakage, we employ three additional datasets exclusively for the RL stage.3 These datasets cover range of difficulty levels, from relatively simple commonsense reasoning tasks to more complex mathematical reasoning tasks, including CommonsenseQA (CSQA) [39], GSM8K [6], and MATH [12], collectively comprising 19.8K verifiable question-answer pairs. Please refer to Appendix for details of the datasets we used and Appendix for implementation details. Baselines In addition to backbone models, we compare ARM with models trained using alternative algorithms that may enable adaptive reasoning capabilities. Specifically, Qwen2.5SFT refers to the backbone model trained on the AQuA-Rat dataset used in stage 1. In this setting, we explore whether language models can master adaptive reasoning through straightforward SFT strategy. For Qwen2.5SFT+GRPO, we examine whether SFT models, further trained with GRPO, can better understand different reasoning formats and whether this approach empowers them to select appropriate reasoning formats based on rule-based rewards."
        },
        {
            "title": "4.2 Evaluation",
            "content": "Evaluation Datasets To assess the models reasoning capabilities, we select range of evaluation datasets, including both in-domain and out-of-domain samples. These datasets span commonsense, mathematical, and symbolic reasoning tasks. For commonsense reasoning, we include CommonsenseQA (CSQA)[39] and OpenBookQA (OBQA)[25], which are easier tasks based on intuitive knowledge. For mathematical reasoning, we utilize SVAMP [30], GSM8K [6], MATH [12], and AIME25 [8] to assess models ability to solve complex mathematical problems that require advanced reasoning and strict logical thinking. For symbolic reasoning, we turn to Big-Bench-Hard (BBH) [38], benchmark for evaluating models structured reasoning ability to manipulate symbols according to formal rules. For further analysis, we group the evaluation datasets into three difficulty levels: commonsense tasks as easy; mathematical and symbolic tasks as medium; and AIME25 as hard given its competition-level difficulty. Inference During inference, we set the temperature to 0.7 and top-p to 1.0. For all evaluation datasets, we use accuracy as the metric. In addition to pass@1, to reduce bias and uncertainty associated with single generation outputs and to enhance the robustness of the results [51], we further use majority@k (maj@k), which measures the correctness of the majority vote from independently sampled outputs. For inference on the three backbone models, we use an example with short-cotbased answer within the prompt to guide the model toward specific answer formats while preserving its original reasoning capabilities as much as possible."
        },
        {
            "title": "4.3 Main Results",
            "content": "Alongside our baselines, we include several state-of-the-art general models, including GPT-4o [26] and DeepSeek-V3 [23], as well as reasoning models o1-preview [27], o4-mini-high [28], and DeepSeek-R1 [9], along with several DeepSeek-R1-Distill-Qwen (DS-R1-Distill) models ranging from 1.5B to 32B [9]. We report our results in Table 1, and we have the following findings: 3In preliminary experiments, we observed that using the same training data in both stages causes the model to recite answers rather than reasoning during the RL stage, resulting in poor generalization. 5 Table 1: Performance of various models across evaluation datasets. #Tokens refers to the token cost for each model on each dataset. For each model, = 1 corresponds to pass@1, and = 8 corresponds to maj@8. When = 8, the token cost is averaged over single output to facilitate clear comparison. denotes in-domain tasks, while denotes out-of-domain tasks. represents the difference between ARM and Qwen2.5SFT+GRPO, calculated by subtracting the accuracy of Qwen2.5SFT+GRPO from that of ARM, with the token usage expressed as the ratio of tokens saved by ARM compared to Qwen2.5SFT+GRPO, with all settings based on = 8 to ensure stable comparison. Models Easy Accuracy () Medium Hard Avg. Easy #Tokens () Medium Hard CSQA OBQA GSM8K MATH SVAMP BBH AIME25 CSQA OBQA GSM8K MATH SVAMP BBH AIME25 GPT-4o o1-preview o4-mini-high DeepSeek-V3 DeepSeek-R1 DS-R1-Distill-1.5B DS-R1-Distill-7B DS-R1-Distill-14B DS-R1-Distill-32B Qwen2.5-3B Qwen2.5-3BSFT Qwen2.5-3BSFT+GRPO ARM-3B Qwen2.5-7B Qwen2.5-7BSFT Qwen2.5-7BSFT+GRPO ARM-7B Qwen2.5-14B Qwen2.5-14BSFT Qwen2.5-14BSFT+GRPO ARM-14B 1 1 1 1 1 1 1 1 1 1 8 1 8 1 8 1 8 1 8 1 8 1 8 1 1 8 1 8 1 8 1 8 85.9 85.5 84.7 82.4 83.3 47.6 64.9 80.6 83.2 66.5 75.5 72.8 75.5 79.7 80.3 79.8 80.1 -0.2 76.7 82.0 80.8 83.9 83.1 83.7 86.1 85.7 +2.0 79.9 83.8 81.8 85.0 85.4 85.8 85.3 85.6 -0.2 94.2 95.6 96.0 96.0 94.8 48.6 77.4 93.2 94. 65.8 77.4 72.4 77.4 79.0 80.0 78.0 78.0 -2.0 78.6 86.4 81.2 84.6 82.2 84.6 84.4 85.8 +1.2 83.8 90.2 88.0 91.4 93.0 94.2 91.8 91.8 -2.4 95.9 94.2 96.9 96.5 96.4 79.4 90.0 94.0 93.5 66.9 80.9 35.7 56.0 88.7 91.4 83.8 90.8 -0.6 81.6 89.9 54.4 79.4 92.8 94.8 89.2 93.7 -1. 84.9 92.3 62.6 86.4 94.8 96.1 92.5 96.3 +0.2 75.9 92.6 97.7 91.8 97.1 84.6 93.6 95.5 93.0 37.7 50.8 20.9 27.6 66.6 74.0 62.9 72.8 -1.2 50.1 64.7 30.4 42.4 79.4 84.9 73.9 82.6 -2.3 52.7 68.4 37.4 48.8 81.7 87.1 79.1 86.4 -0.7 91.3 92.7 94.0 93.7 96.0 86.7 90.3 92.7 92. 71.3 83.7 62.3 74.7 92.0 94.7 89.7 95.0 +0.3 81.0 89.7 76.0 88.0 93.7 95.3 92.0 95.3 0 84.7 91.7 84.0 91.7 93.7 95.3 93.3 95.7 +0.4 84.7 91.8 92.2 85.8 85.0 53.5 72.1 80.4 86.3 38.4 47.1 37.4 43.5 52.6 56.2 50.0 53.8 -2.4 51.7 62.0 48.2 56.0 64.3 69.3 61.4 67.9 -1. 56.8 67.4 53.5 64.4 70.5 77.0 66.6 72.1 -4.9 10.0 40.0 96.7 36.7 70.0 20.0 40.0 50.0 56.7 0 0 0 0 6.7 6.7 6.7 6.7 0 3.3 3.3 0 0 16.7 20.0 16.7 20.0 0 3.3 3.3 0 3.3 20.0 20.0 20.0 23.3 +3.3 Avg. 389 1819 1910 732 2270 3425 2797 2585 2361 355 377 245 217 1136 1172 756 778 -33.6% 76.8 84.6 94.0 83.3 88.9 60.1 75.5 83.8 85.6 49.5 59.3 43.1 50.7 66.5 69.0 64.4 68.2 -0.8 192 573 502 231 918 987 792 816 674 97 96 99 97 425 429 118 165 492 289 213 736 1540 928 750 698 120 100 108 103 501 506 156 169 287 456 339 236 664 841 574 825 438 150 149 145 132 788 802 346 359 663 1863 1332 887 2339 3875 3093 2682 2161 419 424 229 231 1586 1590 1013 156 489 301 160 589 606 315 726 283 76 85 126 108 630 638 264 246 278 940 755 400 1030 3005 1448 1292 999 232 240 311 309 994 996 436 430 -71.3% -66.6% -55.2% -34.8% -61.4% -56.8% 984 7919 9850 2992 9609 13118 12427 11004 1393 1544 694 537 3027 3247 2958 3083 -5.1% 60.4 68.3 53.0 62.0 73.2 76.1 72.0 75.9 -0.2 73.0% 75.4% 60.1% 36.9% 62.8% 63.6% +7.9% 32.5% 247 260 347 323 1173 1164 766 786 376 370 348 361 1410 1415 889 893 182 183 245 274 1133 1135 401 413 767 881 1239 1023 3196 3145 3253 64 66 136 141 491 496 136 134 83 74 150 137 651 625 159 154 156 156 184 185 739 745 305 297 99 92 126 141 587 586 218 218 63.7 71.0 58.2 67.3 77.0 79.4 75.5 78.7 -0.7 73.7% 75.0% 57.9% 49.7% 66.5% 56.0% +7.3% 31.1% 201 217 238 232 1304 1261 853 335 325 276 288 1805 1810 903 910 611 735 527 493 4031 3723 3871 3996 132 131 161 165 693 696 294 293 77 81 152 140 565 565 212 189 139 131 254 247 945 943 420 415 60 60 140 141 531 537 128 56 55 155 149 558 552 146 145 Current reasoning models struggle with the overthinking problem, with smaller distilled models being more affected. We observe that all current reasoning models consume more than 500 tokens on easy commonsense tasks but do not always achieve corresponding improvements. For example, although DeepSeek-R1 and DS-R1-Distill-7B use nearly 4 and 10 more tokens than their backbone models, DeepSeek-V3 and Qwen2.5-7B, they do not show significant improvement and even experience performance degradation, highlighting the overthinking problem. Additionally, we find that when comparing different sizes of DS-R1-Distill, smaller models often require more tokens while delivering worse performance. SFT only teaches models about formats, yet does not teach how to choose the appropriate formats based on the task. We observe that SFT models, across three sizes, show improvement on easy commonsense tasks but experience performance drops on medium and hard tasks. To investigate the cause, we conduct deeper analysis of the reasoning formats selected during inference. Figure 2 visualizes how models allocate the four reasoning formats across three difficulty levels. Specifically, we find that for models trained with SFT, their outputs are distributed almost uniformly across the reasoning formats, with the majority in Direct Answer and the least in Long CoT, regardless of task difficulty. As shown in Figure 2, the inappropriate selection of Direct Answer, which yields extremely low accuracy (35.2%) on medium tasks and significantly hinders the models reasoning capabilities, finally leads to decline in overall performance. This suggests that while SFT teaches models various formats, it fails to help them adaptively select appropriate ones based on the task, leading to an inability to choose more advanced formats as problem complexity increases. GRPO does improve reasoning capabilities, but it tends to rely on Long CoT to solve all tasks. We observe that models trained with GRPO achieve significant improvements across all tasks, yet the token cost remains substantial, especially for the two easier tasks. Further analysis reveals that Long CoT is predominantly used in the inference stage, as shown in Figure 2. This behavior stems from the nature of GRPO (i.e., format collapse discussed in Section 3.2), where models converge to the format with the highest accuracy (i.e., Long CoT) early in training (10 steps in our experiment). As result, GRPO also fails to teach models how to select more efficient reasoning format based on the task. 6 Figure 2: Format distribution by task difficulty with Qwen2.5-7B. The hatched areas indicate the percentage of correct answers that were generated using the selected reasoning format. Table 2: Accuracy (Acc.) and token usage (Tok.) for the three reasoning modes supported by ARM-7B. In the Consensus-Guided Mode, the percentage of Long CoT usage indicates how often the model resorts to Long CoT when simpler reasoning formats fail to reach consensus. ARM-7B"
        },
        {
            "title": "Hard",
            "content": "Avg. CSQA OBQA GSM8K MATH SVAMP BBH AIME25 Acc. Tok. Acc. Tok. Acc. Tok. Acc. Tok. Acc. Tok. Acc. Tok. Acc. Tok. Acc. Tok. Adaptive 86.1 136 84.4 159 89.2 305 73.9 889 92.0 218 61.4 401 16.7 3253 72.0 766 InstDirect 84.1 InstShort CoT 81.3 44.7 13 49.7 101 10.0 2010 65.9 428 InstCode 84.4 140 81.6 147 84.2 285 65.9 559 88.3 182 57.9 344 10.0 1821 67.5 497 InstLong CoT 84.0 259 87.4 294 91.8 426 77.2 1220 94.3 340 66.9 660 20.0 4130 74.5 1047 22.9 67.0 23.1 85.0 124 70.9 633 86.7 81.8 77. 11 66 10 35 10 33 46.2 13 21 12 0 Consensus 85.8 228 87.0 260 92.9 777 78.4 2281 95.7 433 66.4 1039 20.0 7973 75.2 1856 79.2% 12.9% 21.4% 79.8% 55.1% 36.3% 56.3% 100%"
        },
        {
            "title": "Long CoT Usage",
            "content": "ARM is able to adaptively select reasoning formats based on task difficulty, while achieving comparable accuracy across all tasks compared to GRPO and using significantly fewer tokens. As shown in Table 1, across three different model sizes, all ARMs experience an average performance drop of less than 1% compared to models trained with GRPO, yet they save more than 30% of the tokens. Specifically, ARM demonstrates clear advantage on easy tasks, saving over 70% of tokens while maintaining comparable accuracy. This advantage extends to medium tasks as well. For the more challenging AIME25 task, ARM adapts to the task difficulty by increasingly selecting Long CoT, thereby avoiding performance degradation on harder tasks, with ARM-14B even surpassing its counterpart Qwen2.5-14BSFT+GRPO. Figure 2 further confirms that ARM is able to gradually adopt more advanced reasoning formats and discards simpler ones as task difficulty increases. Moreover, as shown in Figure 1b, the line connecting SFT and +GRPO illustrates the expected trade-off, while +Ada-GRPO consistently lies above it, indicating better balance between effectiveness and efficiency of ARM. Additionally, ARM-7B achieves comparable performance to DS-R1-Distill-7B while using only 27.8% of the tokens on average."
        },
        {
            "title": "4.4 Reasoning Mode Switching",
            "content": "ARM is capable of autonomously selecting appropriate reasoning formats (Adaptive Mode), while also supporting explicit guidance to reason in specified formats (Instruction-Guided Mode) or through consensus between different reasoning formats (Consensus-Guided Mode). Specifically, 1) Adaptive Mode: In this mode, ARM autonomously selects the reasoning format for each task, which is also the default reasoning mode if not specified in this paper. 2) Instruction-Guided Mode: In this mode, specific token (e.g., <Long CoT>) is provided as the first input, forcing ARM to reason in the specified format. 3) Consensus-Guided Mode: In this mode, ARM first generates answers using the three simpler reasoning formats (i.e., Direct Answer, Short CoT, and Code) and checks for consensus among them. If all formats agree, the consensus answer is adopted as the final result. Otherwise, ARM defaults to Long CoT for the final answer, treating the task as sufficiently complex. To evaluate the performance and effectiveness of the proposed reasoning modes, we conduct experiments across various evaluation datasets. Table 2 presents the results for ARM-7B. Specifically: 1) Adaptive Mode strikes superior balance between high accuracy and efficient token usage across all datasets, demonstrating its ability to adaptively select the reasoning formats. 2) Instruction-Guided Mode offers clear advantage when the assigned reasoning format is appropriate. For example, Direct Answer is sufficient for commonsense tasks, while Code, due 7 Figure 3: Accuracy comparison between ARMs Adaptive and Instruction-Guided modes. The figure shows average accuracy across evaluation datasets, with Direct Answer applied only to commonsense and symbolic tasks, as it does not appear in mathematical tasks in Adaptive mode. Figure 4: Relative accuracy and token usage of different models compared to their backbone models on CSQA. L1 denotes L1-Exact [1], τ - and TP denotes THINKPRUNE [13]. Accuracy and τ -#Tokens are reported relative to each models backbone after RL training. Figure 5: Performance on the training set across different model sizes trained with Ada-GRPO and GRPO. Except for the implementation of the algorithm, all hyperparameters are kept the same. to its structured nature, performs better on symbolic reasoning tasks compared to Direct Answer and Short CoT. Furthermore, InstLong CoT achieves better performance (74.5%) than the same-sized model trained on GRPO (73.2% in Table 1). This demonstrates that Ada-GRPO does not hinder the models Long CoT reasoning capabilities. We further validate this by analyzing the reflective words used by ARM-7B and Qwen2.5-7BSFT+GRPO in Appendix E. 3) Consensus-Guided Mode, on the other hand, is performance-oriented, requiring more tokens to achieve better performance. This mode leverages consensus across multiple formats to mitigate bias and uncertainty present in any single format, offering greater reliability, particularly for reasoning tasks that demand advanced cognitive capabilities, where simpler formats may fall short. This is evidenced by the fact that Long CoT is less likely to be used for easy tasks, but is highly likely to be selected for medium tasks and even used 100% of the time for the most difficult AIME25 task."
        },
        {
            "title": "5.1 Effectiveness of Adaptive Format Selection",
            "content": "To verify that ARMs format selection indeed adapts to the task at hand rather than relying on random selection, we compare ARMs Adaptive Mode with Instruction-Guided Mode. In InstructionGuided Mode, the reasoning format is fixed and manually specified, providing strong baseline to test whether adaptive selection offers real benefits over using uniform format across tasks. We report the accuracy of both modes in Figure 3. We observe that the accuracy of the reasoning formats selected in Adaptive Mode is higher than that in Instruction-Guided Mode. Specifically, Adaptive Mode improves accuracy by 4.7% on Direct Answer, by 2.7% on both Short CoT and Code, and even yields slight improvement on Long CoT. These results confirm that ARM is not randomly switching formats but is instead learning to select an appropriate one for each task."
        },
        {
            "title": "5.2 Comparison of Ada-GRPO and GRPO",
            "content": "We find that, compared to GRPO, ARM trained with Ada-GRPO achieves comparable performance on the evaluation dataset while achieving approximately 2 speedup in training time. To understand the source of this efficiency, we compare the training dynamics of Ada-GRPO and GRPO across different model sizes, focusing on accuracy, response length, and training time, as shown in Figure 5. The results highlight the following advantages of Ada-GRPO: 1) Comparable Accuracy. Although 8 Figure 6: ARMs performance across different backbones. Base and instruction-tuned models perform similarly, while DS-R1-Distill improves on medium and hard tasks but struggles on easy ones. Ada-GRPO initially lags behind GRPO in accuracy due to suboptimal reasoning format selection in the early training steps, both methods converge to similar final accuracy across all model sizes. This demonstrates that Ada-GRPO does not compromise final performance. 2) Half Response Length. While GRPO uses Long CoT uniformly across all tasks, Ada-GRPO adaptively selects reasoning formats based on task difficulty. Due to the length efficiency of Direct Answer, Short CoT, and Code, Ada-GRPO ultimately reduces the average response length to roughly half that of GRPO. 3) Half Training Time Cost. Since the majority of training time is spent on response generation during the roll-out stage, reducing response length directly translates into lower time cost. As result, Ada-GRPO achieves approximately 2 speedup compared to GRPO. Overall, Ada-GRPO maintains strong performance while significantly reducing computational overhead, underscoring its efficiency and reliability for training."
        },
        {
            "title": "5.3 Comparison of Backbone Models",
            "content": "Beyond the base model, we further analyze the impact of different backbone models, including instruction-tuned and DS-R1-Distill variants. Figure 6 reports accuracy and token usage across easy, medium, and hard tasks. We observe that base and instruction-tuned models have highly similar performance. This suggests that RL effectively bridges the gap left by instruction tuning, enabling base models to achieve comparable performance, consistent with findings from previous work [17]. In contrast, the DS-R1-Distill variant performs notably better on medium and hard tasks, benefiting from distilled knowledge from the stronger DeepSeek-R1 model, though at the expense of increased token cost. However, it performs significantly worse on easy tasks, even with excessive token usage, resulting from the overthinking phenomenon. For more discussion and case studies on the overthinking phenomenon, please refer to Appendix F."
        },
        {
            "title": "5.4 Comparison of ARM and Length-Penalty-Based Strategies",
            "content": "To examine whether previously proposed length-penalty-based strategiesproven effective in complex reasoningremain effective for easier tasks, we evaluate two representative methods, L1 [1] and THINKPRUNE[13], on the CSQA dataset. Since both methods are based on the DS-R1-Distill model, we ensure fair comparison by also evaluating the version of ARM trained on the same backbone. We report the relative accuracy and token usage of all three models compared to their respective backbone models in Figure 4. When using the minimum allowed lengths specified in the official settings of L1 and THINKPRUNE, both methods exhibit performance drops. In contrast, ARM maintains strong performance while using relatively fewer tokens, demonstrating its ability to balance reasoning efficiency and effectiveness. Please refer to Appendix for more details."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose Adaptive Reasoning Model (ARM), which adaptively selects reasoning formats based on task difficulty. ARM is trained with Ada-GRPO, GRPO variant that addresses format collapse via format diversity reward and achieves 2 training speedup. Experiments show that ARM maintains performance comparable to the GRPO-trained model relying solely on Long CoT, while significantly improving token efficiency. Beyond the default Adaptive Mode, ARM also supports Instruction-Guided Mode, which excels when the format is appropriately specified, and Consensus-Guided Mode, which maximizes performance at higher token usage. By adopting the adaptive reasoning format selection strategy, ARM effectively mitigates the overthinking problem and offers novel, efficient approach to reducing unnecessary reasoning overhead."
        },
        {
            "title": "References",
            "content": "[1] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. [2] Daman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025. [3] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wangxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. [4] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. [5] Jeffrey Cheng and Benjamin Van Durme. Compressed chain of thought: Efficient reasoning through dense representations. arXiv preprint arXiv:2412.13171, 2024. [6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [7] Chenrui Fan, Ming Li, Lichao Sun, and Tianyi Zhou. Missing premise exacerbates overthinking: Are reasoning models losing critical thinking skill? arXiv preprint arXiv:2504.06514, 2025. [8] Google. Aime problems and solutions, 2025. URL https://artofproblemsolving.com/ wiki/index.php/AIME_Problems_and_Solutions. [9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [10] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547, 2024. [11] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. [12] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. [13] Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296, 2025. [14] Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=nZeVKeeFYf9. [15] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. [16] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [17] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. 10 [18] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T\" ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. [19] Ayeong Lee, Ethan Che, and Tianyi Peng. How well do llms compress their own chain-ofthought? token complexity approach. arXiv preprint arXiv:2503.01141, 2025. [20] Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, and Junxian He. Codei/o: Condensing reasoning patterns via code input-output prediction. arXiv preprint arXiv:2502.07316, 2025. [21] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025. [22] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158167, 2017. [23] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [24] Yan Ma, Steffi Chern, Xuyang Shen, Yiran Zhong, and Pengfei Liu. Rethinking rl scaling for vision language models: transparent, from-scratch framework and comprehensive evaluation scheme. arXiv preprint arXiv:2504.02587, 2025. [25] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, 2018. [26] OpenAI. Hello GPT-4o, 2024. URL https://openai.com/index/hello-gpt-4o/. [27] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms. [28] OpenAI. Introducing openai o3 and o4-mini, 2025. URL https://openai.com/index/ introducing-o3-and-o4-mini/. [29] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [30] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 20802094, 2021. [31] Xiao Pu, Michael Saxon, Wenyue Hua, and William Yang Wang. Thoughtterminator: BencharXiv preprint marking, calibrating, and mitigating overthinking in reasoning models. arXiv:2504.13367, 2025. [32] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 35053506, 2020. [33] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [34] Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. Codi: Compressing chain-of-thought into continuous space via self-distillation. arXiv preprint arXiv:2502.21074, 2025. 11 [35] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. [36] DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces. In The Thirteenth International Conference on Learning Representations, 2024. [37] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Hanjie Chen, Xia Hu, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. [38] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging bigbench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. [39] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, 2019. [40] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [41] Qwen Team. Qwen3 technical report, 2025. URL https://github.com/QwenLM/Qwen3/ blob/main/Qwen3_Technical_Report.pdf. [42] Luong Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 76017614, 2024. [43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [44] Nathaniel Weir, Muhammad Khalifa, Linlu Qiu, Orion Weller, and Peter Clark. Learning to reason via program generation, emulation, and search. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=te6VagJf6G. [45] Jiaxin Wen, Jian Guan, Hongning Wang, Wei Wu, and Minlie Huang. Codeplan: Unlocking reasoning potential in large language models by scaling code-form planning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=dCPF1wlqj8. [46] Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and Yanghua Xiao. How easily do irrelevant inputs skew the responses of large language models? In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=S7NVVfuRv8. [47] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025. [48] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, 2025. [49] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 12 [50] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. [51] Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, and Chen Ma. What, how, where, and how well? survey on test-time scaling in large language models. arXiv preprint arXiv:2503.24235, 2025. [52] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 400410, 2024."
        },
        {
            "title": "Appendix",
            "content": "A Details of Ada-GRPO A.1 Training Objective Following GRPO [33], given query and set of responses = {o1, o2, . . . , oG} sampled from the old policy πold, we optimize the policy model π using the Ada-GRPO objective: JAdaGRPO(θ) =E (cid:104) (Q), {oi}G i=1 πθold (Oq) (cid:105) (cid:20) 1 (cid:88) clip (cid:18) πθ(oi,kq, oi,<k) πθold (oi,kq, oi,<k) (cid:19) , 1 ϵ, 1 + ϵ i=1 (cid:105) ˆAi,k 1 oi oi (cid:88) (cid:110) k=1 min (cid:104) πθ(oi,kq, oi,<k) πθold (oi,kq, oi,<k) ˆAi,k, β KL [πθ πref ] (cid:111)(cid:21) , (7) where πref denotes the reference model, and the KL divergence term KL serves as constraint to prevent the updated policy from deviating excessively from the reference. The advantage estimate ˆAi,k is computed based on group of rewards {r 1, G} associated with the responses in O, as defined in equation 5. 2, , A.2 Decay Factor In Ada-GRPO, the decay factor decayi(t) is introduced to regulate the influence of the format diversity scaling factor during training. Without decay, the model may continue to overly reward less frequent reasoning formats even after sufficient exploration, misaligning with our objective. To evaluate the effectiveness of the decay mechanism, we track the test set performance across three in-domain datasets (CSQA, GSM8K, and MATH) using checkpoints saved every 25 training steps for models trained with and without decay. As shown in Figure 7, models trained without decay exhibit larger performance fluctuations in test accuracy, indicating unstable exploration. In contrast, the decay mechanism stabilizes training, resulting in smoother and more consistent improvements in accuracy during the middle and later training stages."
        },
        {
            "title": "B Details of Processing SFT Dataset",
            "content": "B.1 Prompt List Figure 7: Test set accuracy with and without the decay mechanism. We use gpt-4o-2024-11-20 to generate Code reasoning rationales. Following previous work [44], we ask the model to return the output as dictionary containing all intermediate and final outputs, which is beneficial for emulating the generated programs execution. (cid:7) For the following questions and answers , generate function that solves the question . The function should return dictionary with the field answer : < answer > , as well as the values for intermediate decisions . Ensure that both the function and its call are wrapped in < CODE >... </ CODE > , and that the emulation of its execution is wrapped in (cid:4) < OUTPUT >... </ OUTPUT >. { examples } Question : { question } Answer : { rational } #### { ground_truth } (cid:6) 14 (cid:5) Table 3: Dataset in each training stage. Size Dataset"
        },
        {
            "title": "Answer Format",
            "content": "Stage 1: Supervised Finetuning AQuA-Rat Multiple-Choice Open-Form 3.0K 7.8K 10.8K Stage 2: Reinforcement Learning CSQA GSM8K MATH Multiple-Choice Open-Form Open-Form 4.9K 7.4K 7.5K 19.8K Figure 8: Token count distribution across reasoning formats in the SFT dataset AQuA-Rat, with brackets indicating average counts. We use Deepseek-R1 to generate Long CoT rationales. (cid:7) Put the answer in format \" < ANSWER >... </ ANSWER >\". { question } (cid:6) B.2 Filter Out Rationales (cid:4) (cid:5) For Code rationales, we utilize Python interpreter to execute each generated code snippet. We apply the following filters: 1) execution failure, 2) missing answer key, 3) inconsistencies between intermediate steps and execution results, and 4) mismatches between the predicted and ground-truth answers. For Long CoT rationales, we filter out those with incorrect answers. Token count distribution across reasoning formats in the SFT dataset can be seen in Figure 8."
        },
        {
            "title": "C Examples in Training",
            "content": "The question-answer format and size of the datasets in each training stage can be found in Table 3. C.1 Examples in SFT Stage We provide four reasoning formats for the SFT dataset AQuA-Rat [22], including Direct Answer, Short CoT, Code, Long CoT. Here is an example. (cid:7) Question : You collect baseball cards . Suppose you start out with 15. Maria takes (cid:4) half of one more than the number of baseball cards you have . Since you re nice , you give Peter 1 baseball card . Since his father makes baseball cards , Paul decides to triple your baseball cards . How many baseball cards do you have at the end ? Direct Answer : Code : def ca la _ eb _ rd () : # Initial number of baseball cards initial_cards = 15 # Maria takes half of one more than the number of baseball cards maria_takes = ( initial_cards + 1) / 2 cards_after_m aria = initial_cards - maria_takes # Peter takes 1 baseball card 15 cards_after_p eter = cards_ after_mar ia - # Paul triples the number of baseball cards final_cards = cards_after_p eter * 3 return { maria_takes : maria_takes , cards_after_maria : cards_after_maria , cards_after_peter : cards_after_peter , answer : final_cards } >>> c t _ se l _ ar () output = { maria_takes : 8.0 , cards_after_maria : 7.0 , cards_after_peter : 6.0 , answer : 18.0 } Short CoT : Start with 15 baseball cards . Maria takes half of one more than the number of baseball cards you have . So Maria takes half of 15 + 1 which is 8 , so you re left with 15 - 8 = 7. Peter takes 1 baseball card from you : 7 - 1 = 6 baseball cards . Paul triples the number of baseball cards you have : 6 * 3 = 18 baseball cards . So you have 18 at the end . The answer is 18. Long CoT : Okay , let me try to work through this problem step by step . So , the question is about collecting baseball cards , and there are series of steps that happen . Let me read it again to make sure understand each part . Alright , starting with 15 cards . Then Maria takes half of one more than the number have . Wait , that phrasing is bit tricky . So Maria takes half of ( one more than my current number of cards ) . Let me parse that . If have cards , Maria takes ( +1) /2? Hmm . Let me double - check . The problem says : \" Maria takes half of one more than the number of baseball cards you have .\" So , the expression would be ( ( number you have + 1 ) ) / 2. Right . So that ( current cards +1) divided by two . Then after that , give Peter 1 card . Then Paul triples my cards . So , let break it down step by step . Starting with 15 cards . Let note each step : 1. Initial cards : 15. 2. Maria takes half of one more than what have . So , first , calculate one more than my current count : 15 +1 =16. Then she takes half of that . So 16 /2 =8. So Maria takes 8 cards . Wait , but when she takes them , does that mean my total decreases by 8? Yes , think so . Because she taking them from me . So subtract 8 from my current count . So after Maria takes them : 15 -8 =7. Now have 7 cards left . Wait minute , wait . Wait , when Maria takes half of one more than the number have , which is (15+1) /2=8. So she taking 8 cards . So yes , subtract 8 from my current 15. That leaves me with 7. Then , the next step : since nice , give Peter 1 baseball card . So subtract 1 more . 7 -1 =6. 16 Then Paul decides to triple my baseball cards . So multiply my current count by 3. 6 *3 =18. So the answer should be 18. Let me go through the steps again to check . Original : 15. After Maria : (15 +1) /2 =8. So 15 -8=7. After giving Peter 1: 7 -1=6. After Paul triples : 6*3=18. Yes , that seems right . Did misinterpret any step ? Let double - check each part . \" Maria takes half of one more than the number of baseball cards you have .\" So when Maria takes , it ( my current cards +1) /2. So when have 15 , that (15+1) /2=8. She takes 8 , so have 7 left . Correct . Then giving Peter 1: 7 -1=6. Then Paul triples to 18. Yep . That seems correct . So the final answer is 18. (cid:6) C.2 Examples in RL Stage The RL dataset includes CSQA [39], GSM8K [6], and MATH [12], with examples provided below. (cid:7) < CommonsenseQA > What is the base of vase called ? ( ) transistor ground truth : ( ) seabase ( ) vessel ( ) box ( ) pie < GSM8K > Natalia sold clips to 48 of her friends in April , and then she sold half as many clips in May . How many clips did Natalia sell altogether in April and May ? ground truth : 72 < MATH > Rationalize the denominator : $ frac {1}{ sqrt {2} -1} $ . Express your answer in simplest form . ground truth : $ boxed { sqrt {2}+1} $ (cid:6)"
        },
        {
            "title": "D Implementation Details",
            "content": "Our training is performed using 8 NVIDIA A800 GPUs. The following settings are also applied to other baselines for fair comparisons. D.1 Stage 1: SFT We utilize the open-source training framework LLAMAFACTORY [52] to perform SFT. The training is conducted with batch size of 128 and learning rate of 2e-4. We adopt cosine learning rate scheduler with 10% warm-up period over 6 epochs. To enhance training efficiency, we employ parameter-efficient training via Low-rank adaptation (LoRA) [14] and DeepSpeed training with the ZeRO-3 optimization stage [32]. As validation set, we sample 10% of the training data and keep the checkpoint with the lowest perplexity on the validation set for testing and the second stage. 17 (cid:5) (cid:4) (cid:5) Table 4: Definitions and results of reflection-related ratios on AIME25."
        },
        {
            "title": "Ratio Name",
            "content": "Formula Qwen2.5-7BSFT+GRPO ARM-7B reflection_ratio correct_ratio_in_reflection_texts Nref Nref + Nref 93.8 14. 95.0 13.9 D.2 Stage 2: RL We utilize the open-source training framework VeRL [35] to perform RL. During training, we use batch size of 1024 and generate 8 rollouts per prompt (G = 8), with maximum rollout length of 4096 tokens. The model is trained with mini-batch size of 180, KL loss coefficient of 1e-3, and total of 9 training epochs. The default sampling temperature is set to 1.0."
        },
        {
            "title": "E Details of Reflective Words",
            "content": "To evaluate models Long CoT reasoning capabilities, we focus on their use of specific reflective words that signal backtracking and verifying during the reasoning process. Following prior work [24], we consider curated list of 17 reflective words: [re-check, re-evaluate, re-examine, re-think, recheck, reevaluate, reexamine, reevaluation, rethink, check again, think again, try again, verify, wait, yet, double-check, double check]. We adopt two evaluation metrics: reflection_ratio, measuring the proportion of outputs containing at least one reflective word, and correct_ratio_in_reflection_texts, assessing the correctness within reflective outputs. The formulas for these metrics are summarized in Table 4, where denotes the total number of responses, Nref the number of responses containing reflective words, and Nref + the number of correct reflective responses. Given its competition-level difficulty, we conduct our analysis on AIME25 using ARM-7B and Qwen2.5-7BSFT+GRPO. For ARM-7B, we use the Instruction-Guided Mode (InstLong CoT) to specifically assess its Long CoT reasoning. The results, averaged over 8 runs, are reported in Table 4. As shown, both models exhibit high frequency of reflective word usage, with reflection_ratio exceeding 93%, indicating that reflection behavior is well-integrated during Long CoT reasoning. The correct_ratio_in_reflection_texts remains comparable for both models, and relatively low due to the high complexity of the AIME25 tasks. These results demonstrate that Ada-GRPO does not hinder the models Long CoT reasoning capabilities."
        },
        {
            "title": "F Details of the Overthinking Phenomenon",
            "content": "Overthinking refers to the phenomenon where LLMs apply unnecessarily complex reasoning to simple tasks, leading to diminishing returns in performance [37]. As demonstrated in Table 1 and 2, using Long CoT, despite incurring higher computation costs, significantly enhances model performance on tasks requiring complex mathematical reasoning, such as MATH. However, as mentioned in Section 4.3 and 5.3, longer responses do not consistently lead to better performance for all task types. In this section, we analyze the overthinking phenomenon in depth, focusing on how overly complex reasoning formats can hurt performance when applied to certain tasks. F.1 Analysis Figure 9: Overthinking in 7B model performance across two representative datasets. We remove the extreme data points and ensure that sufficient data points fall within the intervals. We analyze the evaluation datasets and illustrate the overthinking phenomenon using two representative datasets: CSQA and TemporalSequences in BBH Benchmark. From Figure 9 (note that the 18 (cid:4) (cid:5) (cid:4) horizontal axis scales differ between SFT+GRPO and ARM.), we observe that SFT+GRPO, which relies heavily on Long CoT, shows significant drop in accuracy as the model generates lengthy responses. It indicates that the model starts generating excessive information that does not contribute to task resolution. In contrast, ARM achieves better performance with fewer tokens for simpler tasks. This analysis reinforces the idea that longer reasoning does not equate to better performance [7; 37], and the optimal reasoning format depends on task type. ARM provides more flexible, task-specific solution to avoid overthinking, ultimately improving performance. Please refer to Appendix F.3 for detailed example of how Long CoT errs in CSQA, while simpler reasoning format with lower token usage successfully solves the problem. F.2 Data Examples (cid:7) <BBH - Temporal Sequences > Today , Ashley went to the bookstore . Between what times could they have gone ? We know that : Ashley woke up at 5 am . Jason saw Ashley buying lunch at the deli from 11 am to 1 pm . Emily saw Ashley driving to the water park from 1 pm to 3 pm . Kimberly saw Ashley stretching at yoga studio from 3 pm to 6 pm . The bookstore was closed after 6 pm . Between what times could Ashley have gone to the bookstore ? ( ) 1 pm to 3 pm ( ) 11 am to 1 pm ( ) 3 pm to 6 pm ( ) 5 am to 11 am ground truth : (cid:6) F.3 An Example of Overthinking in CSQA (cid:7) Where does maid empty trash can ? ( ) corner ( ) bus stop ( ) hockey game ( ) motel ( ) alley Long CoT used by + SFT + GRPO : < LONG_COT > Okay , let see . The question is where maid would empty trash can . Hmm , pick one of the options given . Let break down the terms . First , maid is someone who works in hotel or other environment , such as cleaning . Their job might involve domestic duties for specific kind of client , perhaps upper class , but specific job would involve handling various items . So having cleaning duties like trash removal . Now , traditionally , maid service might be focused on particular areas - probably the rooms in hotel , apartment , or maybe an individual client if they work as an in - home maid . So for that_RESULTS , would probably be ... Alright . Let look at the choices : ( ) Bus stop . Seems like they shouldn dispose of anything there . bus stop would have trash cans , but personally doubt maid would dump trash in bus station . That probably not part of their responsibilities . ( ) Corner . Hmm . literal street corner , perhaps maybe public restroom ? Or other outdoor spot ? But where tomorrow trash can ? can imagine location in caring for trash outside normal public areas . Maybe mega cities and sanitation handling upgrades , but again not typical everyday task for maid . ( ) Hockey Game . Hmm . Trash removal at hockey game ... Never heard of maid working at hockey game . Probably no . The grounds might be cleaned by different personnel , like stadium employees , most likely more specialized . Hell , playerName duplication , maybe the trash cans outside the arena get cleaned ? That probably exclude maid work issue . So probably can be discounted . ( ) Motel . Hmm . Maybe the version of this ? Or hotels . Wait , motel is final answer . But wait , official work responsibility . The main point here is where professional maid service has responsibilities . For example , in hotels - motel rooms ( though maybe not modern motels , which had more community facilities ?) The specific task is they probably need to take trash cans to the disposal area - usually outside the room in compilations . So perhaps dump it into proper receptacle . That would happen in backend trash can disposal , but Solutions associated with cleaning , not directly in guest areas . ( ) Alley . Hmm . Alley avoids touristy trash ? Wait , perhaps like backstage scenario . Or alleyways can be garbage collection areas . But how typical ? Probably the communities are serviced by regular collection with professionals . For same reasons as airliner ( like trash in the alley ) . The main thing is that probably would move trash to the nearest dumpster , possibly in an alley . Though not public area of the house . Though again , problem is which alley . It not nearly as common would be this responsibility . Probably more likely that maid would take the trash to dumpster outside . So option probably can be considered . Wait , not sure , but none of the options are obvious , but Zeeland . Because the equipment is usually moved from rooms to centralized dumpster . Probably near the alley . Although may not be the primary course of action . However , this is what might be discussed . Because depending on the structure . For example , perhaps motel rooms need to dispose garbage in dumpster , making alley collection possible . But options like motel and motel would probably be the right answer . But museum ? Not typical . So perhaps neither of the answers given . Wait , maybe . </ LONG_COT > < ANSWER > </ ANSWER > Short CoT used by ARM : < SHORT_COT > maid works in motel . maid may empty trash can in motel . The answer is </ SHORT_COT > < ANSWER > </ ANSWER > (cid:6) (cid:5) Details of Length-Penalty-Based Strategies To ensure fair comparisons, we follow the official settings of L1 [1] and THINKPRUNE [13], adopting their specified minimum allowed lengths when evaluating on easy tasks. We set the temperature to 0.6 and top-p to 0.95, consistent with both papers. Specifically, we use L1-Qwen-1.5B-Exact4 at 512 tokens for L1 and DeepSeek-R1-Distill-Qwen-1.5B-thinkprune-iter2k5 for THINKPRUNE. 4https://huggingface.co/l3lab/L1-Qwen-1.5B-Exact 5https://huggingface.co/Shiyu-Lab/DeepSeek-R1-Distill-Qwen-1.5B-thinkprune-iter2k"
        }
    ],
    "affiliations": [
        "Fudan University",
        "The Ohio State University"
    ]
}