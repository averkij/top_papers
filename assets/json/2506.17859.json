{
    "paper_title": "In-Context Learning Strategies Emerge Rationally",
    "authors": [
        "Daniel Wurgaft",
        "Ekdeep Singh Lubana",
        "Core Francisco Park",
        "Hidenori Tanaka",
        "Gautam Reddy",
        "Noah D. Goodman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent work analyzing in-context learning (ICL) has identified a broad set of strategies that describe model behavior in different experimental conditions. We aim to unify these findings by asking why a model learns these disparate strategies in the first place. Specifically, we start with the observation that when trained to learn a mixture of tasks, as is popular in the literature, the strategies learned by a model for performing ICL can be captured by a family of Bayesian predictors: a memorizing predictor, which assumes a discrete prior on the set of seen tasks, and a generalizing predictor, where the prior matches the underlying task distribution. Adopting the normative lens of rational analysis, where a learner's behavior is explained as an optimal adaptation to data given computational constraints, we develop a hierarchical Bayesian framework that almost perfectly predicts Transformer next-token predictions throughout training -- without assuming access to its weights. Under this framework, pretraining is viewed as a process of updating the posterior probability of different strategies, and inference-time behavior as a posterior-weighted average over these strategies' predictions. Our framework draws on common assumptions about neural network learning dynamics, which make explicit a tradeoff between loss and complexity among candidate strategies: beyond how well it explains the data, a model's preference towards implementing a strategy is dictated by its complexity. This helps explain well-known ICL phenomena, while offering novel predictions: e.g., we show a superlinear trend in the timescale for transitioning from generalization to memorization as task diversity increases. Overall, our work advances an explanatory and predictive account of ICL grounded in tradeoffs between strategy loss and complexity."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 2 9 5 8 7 1 . 6 0 5 2 : r In-Context Learning Strategies Emerge Rationally Daniel Wurgaft1 Ekdeep Singh Lubana2 Core Francisco Park2 Hidenori Tanaka2 Gautam Reddy3 Noah D. Goodman1,4 1Department of Psychology, Stanford University 2CBS-NTT Program in Physics of Intelligence, Harvard University 3Joseph Henry Laboratories of Physics, Princeton University 4Department of Computer Science, Stanford University"
        },
        {
            "title": "Abstract",
            "content": "Recent work analyzing in-context learning (ICL) has identified broad set of strategies that describe model behavior in different experimental conditions. We aim to unify these findings by asking why model learns these disparate strategies in the first place. Specifically, we start with the observation that when trained to learn mixture of tasks, as is popular in the literature, the strategies learned by model for performing ICL can be captured by family of Bayesian predictors: memorizing predictor, which assumes discrete prior on the set of seen tasks, and generalizing predictor, where the prior matches the underlying task distribution. Adopting the normative lens of rational analysis, where learners behavior is explained as an optimal adaptation to data given computational constraints, we develop hierarchical Bayesian framework that almost perfectly predicts Transformer nexttoken predictions throughout trainingwithout assuming access to its weights. Under this framework, pretraining is viewed as process of updating the posterior probability of different strategies, and inference-time behavior as posteriorweighted average over these strategies predictions. Our framework draws on common assumptions about neural network learning dynamics, which make explicit tradeoff between loss and complexity among candidate strategies: beyond how well it explains the data, models preference towards implementing strategy is dictated by its complexity. This helps explain well-known ICL phenomena, while offering novel predictions: e.g., we show superlinear trend in the timescale for transitioning from generalization to memorization as task diversity increases. Overall, our work advances an explanatory and predictive account of ICL grounded in tradeoffs between strategy loss and complexity."
        },
        {
            "title": "Introduction",
            "content": "In-Context Learning (ICL) has significantly expanded the open-ended nature of large language models (LLMs) [15], allowing them to learn novel behaviors from merely the provided context [612]. This has motivated large body of work that analyzes controlled experimental settings to better understand ICL [1317], leading to (i) behavioral accounts of what strategies are followed by model to learn from its context [1823], e.g., the ridge estimator in linear regression tasks [14, 17]; (ii) developmental accounts identifying when, i.e., under what training [24] and data [25] conditions, particular strategy is used by the model [2430]; or (iii) mechanistic accounts characterizing how such strategies get implemented [3133], e.g., the use of induction heads [34]. While some have attempted characterizing ICL as single procedure [22, 35, 36], more recent work has argued that the broader phenomenology of ICL stems from model learning different strategies under varying experimental conditions [26, 30, 37]. Equal contribution. Email: wurgaft@stanford.edu, ekdeeplubana@fas.harvard.edu. Preprint. Under review. Figure 1: Why Does Model Learn Different Strategies for Performing ICL? To answer this question, we analyze three distinct settings where model is trained to learn mixture of tasks. (a) Model Behavior Transitions Between Memorizing and Generalizing Predictors. We first make the observation that across settings, as diversity of data distribution and amount of training are increased, model behavior transitions between two Bayesian predictors: memorizing predictor, , which assumes discrete prior over seen tasks, and generalizing predictor, G, which assumes continuous prior over the true distribution. This recapitulates prior results on task-diversity thresholds [25] and transient generalization [24] in unifying language. (b) Hierarchical Bayesian Framework Provides an Explanatory and Predictive Account of ICL. The consistency of these transitions motivates hierarchical Bayesian model of ICL, where models inference-time behavior is framed as posterior-weighted interpolation in the Bayesian predictors and G, while pretraining is seen as process of updating the preference (posterior probability) toward different predictors. We find that under reasonable assumptions regarding neural network learning dynamics, our framework is highly predictive of model behavior throughout training, without access to model weights (bottom panel). Additionally, our framework provides an explanatory account of ICL phenomena via tradeoff between the loss and the complexity of learned solutions (top panel). These results suggest that remaining hurdle for developing unified account of ICL is understanding why, in the first place, models learn different strategies with disparate generalization abilities. Indeed, given that memorizing the training distribution almost always leads to better performance, why does the model learn an underfitting, out-of-distribution generalizing solution at all [24, 25]? Moreover, if capacity limitations prevent memorization, why does the model, among all underfitting solutions, learn the one that captures the true generative process [25, 26]? Finally, why does it first learn such solution, only to eventually give way to one that does not generalize to novel tasks [24, 27]? This work. To address the questions above, we first make the observation that in popularly studied ICL settings, where model is trained to learn mixture of tasks, identified strategies can be unified in the language of Bayesian inference: across three distinct settings, we show models learn solutions that behaviorally match Bayes-optimal strategies towards generalizing to the distribution of tasks seen during training vs. the true distribution from which said tasks are sampled (see Fig. 1(a)). These solutions respectively assume discrete prior over the seen tasks (a memorizing predictor) or continuous prior over the true distribution (a generalizing predictor), capturing previously studied setting-specific solutions (e.g., those described by Raventós et al. [25] in in-context linear regression). This observation then motivates our core contribution: we propose to understand ICL by invoking the approach of rational analysis from cognitive science [3842], where learners behavior is explained as an optimal adaptation to data, given its objective and computational constraints. In our case, building on the finding that Transformers transition between memorizing and generalizing predictors, we examine how Bayes-optimal learner would trade off between these solutions across training and data conditions, given simplicity bias [4348] and power-law scaling of loss with dataset size [49, 50]. This yields hierarchical Bayesian framework that casts pretraining as process of weighing preference (posterior probability) toward different solutions based on their loss and complexity. At inference, model behavior is framed as posterior-weighted average of these solutions (which themselves are Bayesianhence the term hierarchical). Deriving closed-form expression for next-token predictions based on this framework, we are able to almost perfectly explain model behavior throughout training without assuming access to the weights. This allows us to both Figure 2: Experimental Settings: Learning Finite Mixture of Tasks. (a) General Formulation. Popularly studied experimental settings in the literature on ICL can be seen as training model to learn distribution defined using mixture of tasks (denoted Ttrain), where each task is parameterized latent function whose parameters are sampled from distribution Ttrue. (b) Considered Settings. We analyze three distinct instantiations of this general formulation: Balls & Urns, which captures the belief update interpretation of ICL and is simplification of the Markov modeling setting from prior work [26, 52], and two popularly studied settings from the literature that capture the few-shot learning interpretation of ICL, i.e., in-context linear regression [14, 25] and Classification [20, 24, 28, 29]. explain several known ICL phenomena and draw novel predictions. Overall, we make the following contributions in this work. Model Behavior Transitions Between Memorizing and Generalizing Predictors. Analyzing three distinct settings, we replicate well-known phenomena of ICL [24, 25] and show that as task diversity and training steps are varied, models primarily transition between two ICL phases, determined by two types of predictors dominating model behavior: 1) Bayesian predictor with discrete prior over seen tasks, called the memorizing predictor, and 2) Bayesian predictor with continuous prior over the true data-generating distribution, called the generalizing predictor. Hierarchical Bayesian Model of ICL Grounded in Rational Analysis. Adopting the lens of rational analysis from cognitive science [38] motivates hierarchical Bayesian framework that models ICL as posterior-weighted average over the memorizing and generalizing predictors. This is in contrast to several previous Bayesian accounts of ICL [35, 51], which frame ICL as implementing single predictor. Under reasonable assumptions about neural network learning dynamics (power-law scaling and simplicity bias), we derive closed-form expression for our model, which almost perfectly predicts Transformer next-token predictions without assuming access to weights, as well as captures dynamical ICL phenomena of task diversity effects and transience. Loss-Complexity Tradeoff Underlies ICL Phenomena and Yields Novel Predictions. Our framework makes explicit tradeoff driving model preference towards predictor as function of its loss and complexity. This tradeoff helps explain transitions in model behavior due to changes in data-diversity [25] or training time [24] as artifacts implicitly induced by changes in predictors complexity or loss, relative to other predictors. This interpretation further helps us identify several novel predictions, e.g., superlinear relationship between task diversity and time to transience. Overall, we argue our work advances unifying explanatory and predictive account of ICL. More broadly, our results suggest the value of taking normative perspective towards explaining neural networks, viewing learning behavior (both in-context and throughout training) as rational adaptation to data properties and computational constraints."
        },
        {
            "title": "2 Preliminaries: Learning a Finite Mixture of Tasks",
            "content": "To capture both few-shot learning [2, 14, 20, 25, 53] and belief update formulations [26, 34, 54, 55] of ICL, we analyze three distinct experimental settings in this paper. To this end, we first provide general formulation of the learning setup, which allows us to formalize unified language for examining model strategies for ICL in the next section. General Formulation. We cast prior settings used for studying ICL [20, 25, 26] as learning finite mixture of tasks. Specifically, settings analyzed in this work involve learning mixture distribution Ttrain defined over parametrized functions {f (w1, ), . . . , (wD, )}, or tasks. For each function (w), the parameters Rm are sampled from predefined distribution Ttrue (see Fig. 2). We call the task diversity of the mixture. Every training iteration, we randomly select function (w, ) Ttrain, and use it to generate sequence of length (details vary by setting, see below). Batches consist of independently generated sequences. We autoregressively train Transformers (GPT-NeoX architecture [56, 57]) for predefined number of iterations . Model performance is 3 Figure 3: Predictors in Different Experimental Settings. (a) Memorizing and Generalizing Predictors. We compare model behavior to two idealized Bayesian predictors: (i) Memorizing predictor (M ), which assumes discrete prior over the mixture distribution Ttrain, and (ii) Generalizing predictor (G), which assumes prior over Ttrue, the distribution from which tasks are sampled. (b) Task-Specific Instantiations. These predictors yield closed-form solutions (App. G); e.g., in Balls & Urns, the memorizing predictor computes posterior-weighted average over urns seen in training, whereas the generalizing predictor uses empirical unigram statistics with pseudo-counts. evaluated either on in-distribution (ID) sequences drawn from Ttrain, or on out-of-distribution (OOD) sequences drawn from the underlying distribution Ttrue (see App. for further details on architecture and method). We analyze three specific instantiations of this general formulation, detailed next. Balls & Urns. Related to the belief update formulation of ICL, this setting is inspired by the classic Urn Problem from the probability literature [58]. Specifically, one draws (with replacement) balls from an urn containing balls of types, and the goal is to estimate the distribution of ball types. Since solving this task only requires inferring unigram statistics of the input (a histogram), this setting simplifies the Markov modeling setup proposed by Park et al. [26]. task (w) = Categorical(w) denotes stochastic map (urn) from which states (balls) are sampled, with Ttrue = Dirichlet(1). Thus, the distribution Ttrain consists of urns {Categorical(w1), . . . , Categorical(wD)}. To generate data, we sample states from randomly selected function (w), yielding sequence := [x1 . . . xC], where Categorical(w). Linear Regression. standard problem setting in literature on understanding the few-shot learning formulation of ICL [14, 25, 53]. Here, the goal is to learn to in-context solve linear regression problems. task (w) = wx+ϵ denotes noisy linear map that transforms continuous input via linear map (0, Im) and introduces additive noise ϵ (0, σ2), where Ttrue = (0, Im) and Im denotes the identity matrix. Thus, the training distribution Ttrain consists of linear mappings {f (w1, ), . . . , (wD, )}. To generate data, we sample inputs and transform them with randomly selected function (w, x), yielding sequence := [x1, wx1 +ϵ1, . . . , xC, wxC +ϵC]. Binary Classification. Another popularly studied setting in literature on understanding the few-shot formulation of ICL [20, 28]. Our parameterization is inspired by Nguyen and Reddy [29], who define task (w, l) = to denote an item-label pair, with (0, Im/m), {0, 1} defining Ttrue. Thus, the training task distribution Ttrain consists of item-label pairs {w1 l1, . . . , wD lD}. Unlike other settings, multiple functions (w, l) are used to generate data: first, 1 item-label pairs are randomly sampled (with replacement) from Ttrain. pair is chosen from the sequence at random to be the query pairit is appended to the end of the sequence, and its label is corrupted to be 1. We noise these items via = w+σϵ 1+σ2 , with σ acting as the within-class variance, and ϵ (0, Im/m). This process yields sequence := [x1, . . . , xC1, xquery] = [ w1 l1, . . . , wC1 lC1, wquery 1]. Models are only trained to predict the label of wquery."
        },
        {
            "title": "3 What Strategies: Memorizing and Generalizing Predictors",
            "content": "Our goal in this work is to understand why model learns different strategies for performing ICL. We must thus first establish what these strategies are, allowing us to then characterize the dynamics driving changes in models preferred strategy. To this end, we build on the idea that autoregressive training with the next-token prediction objective corresponds to maximizing the likelihood of the data and learning the distribution underlying it [59]. We thus consider the two distributions forming the basis of our general formulationi.e., Ttrain and Ttrueand consider optimal strategies learner can be expected to implement if it learns these distributions. This generalizes the approach of Raventós et al. [25], and yields the following two Bayesian predictors. 4 (cid:88) wTtrain (cid:90) Memorizing Predictor (M ). The memorizing predictor assumes discrete prior over the distribution of seen tasks (Ttrain) and implements posterior predictive of the form: (sis1:i1) = p(ws1:i1)fw(sis1:i1) = EwTtrain [fw(sis1:i1)]. (1) Generalizing Predictor (G). The generalizing predictor assumes continuous prior over the distribution from which tasks are sampled (Ttrue), implementing posterior predictive of the form: G(sis1:i1) = p(ws1:i1)fw(sis1:i1) = EwTtrue [fw(sis1:i1)]. (2) wTtrue For all tasks we analyze, the predictors above can be defined in closed-form manner (see App. G), mapping onto task-specific strategies defined in prior work: e.g., what has been called dMMSE vs. ridge estimator in linear regression [25], in-weights learning vs. in-context learning in classification [20], and retrieval vs. inference in sequence modeling [26]. 3.1 Validating the Memorizing and Generalizing Predictors We next demonstrate the validity of the memorizing and generalizing predictors for the purpose of our analysis. Specifically, we show that as experimental conditions are varied, models behavior primarily transitions between these predictors. To this end, we consider two core phenomena associated with ICLan increase in models OOD performance with increasing task diversity [25, 60, 61], and the forgetting of this ability with increasing training steps phenomenon known as transient generalization [24, 30, 62]. We first replicate these results behaviorally in Fig. 1(a), finding that, across settings, Transformers transition between performing like the memorizing predictor vs. like the generalizing predictor (see App. for full results). Then, we make direct comparison by computing the distance between our trained models next-token predictions and the predictions of the memorizing and generalizing predictors. Specifically, let d(., .) denote distance measure (symmetrized KLdivergence or Euclidean distance), and denote the Transformer model trained from scratch via h(.). Then, as function of and , we can plot heatmap of the relative distance between the trained model and the memorizing and generalizing predictors, defined as drel = (r+1)/2, where := d(h,G)d(h,M ) . This metric evaluates to 0 vs. 1 if the model is closer to the generalizing vs. the memorizing predictor. Results are shown in Fig. 4; see App. H.3 for absolute distances. We clearly see that increasing for fixed , the model first behaves like memorizing predictor (in red), only to eventually transition to behaving like generalizing predictor (in blue)illustrating task-diversity effects [25]. Meanwhile, when increasing for middle values of D, the model starts closer to generalizing predictor, only to eventually give way to memorizing predictorillustrating transient generalization [24]. More broadly, across all tasks, we see there is clear delineation of the model behavior into two phases of (N, D), such that model behavior is best explained by either the memorizing or the generalizing predictor in given phase. Given the optimality of these predictors on the distribution of seen tasks (Ttrain) or the underlying distribution (Ttrue), our analysis provides an explanation for why these predictors were observed in prior work. However, several questions remain, including why generalizing strategy is learned even when it leads to worse ID performance, and why does varying experimental conditions change which strategy, among the memorizing and generalizing predictors, is implemented by model. We address these questions next. Figure 4: Relative Distance Captures Transitions in Model Behavior. We show the relative distance between model outputs and the two predictors. Marginals report the absolute distance values (e.g., symmetrized KL between model and predictor outputs for the Balls & Urns setting), holding constant (for the right plot), or constant (for the top plot), and varying the other variable (denoted with the dotted line). Across all settings, we see model behavior decomposes into two phases, explained by either the memorizing or generalizing predictor. In this figure, we use context length of 128, task dimensionality of 8, and MLP width of 256 for balls and urns. Linear regression has similar parameters except context length of 32, and classification has similar parameters other than MLP width of 512. d(G,M ) 5 Figure 5: Our Bayesian Model Captures Transitions Between Strategies Explaining Model Behavior. We plot the posterior probability of the memorizing predictor given by our theoretical model (Eq. 4). Across three broad experimental settings(a) Balls & Urns, (b) Linear Regression, and (c) Classificationwe find our model identifies the phases best explained by given predictor and the boundary between them, hence capturing the transition between solutions seen in Transformers training (as shown by the relative distance maps). Importantly, we find our model is highly predictive of the pretrained Transformers behavior (next-token predictions) across conditions used for fitting the three free parameters of our model and unseen ones. Max color bar value for Balls & Urns and Classification is determined by the performance of baseline predictor that always outputs the mean of the distribution Ttrue. In this figure, we use context length of 256, task dimensionality of 8 and MLP width of 256 for balls and urns. Linear regression and classification have similar parameters except context length of 64 and 384, respectively."
        },
        {
            "title": "4 Answering the Why: A Hierarchical Bayesian Account of ICL",
            "content": "Our analysis above shows that, except for intermediate values of and D, model behavior is primarily explained by Bayes-optimal predictors capturing the distributions Ttrain and Ttrue. Motivated by these findings, we adopt the lens of rational analysis [3842], framework in cognitive science that aims to explain learners behavior as optimal, under computational constraints. What might be considered optimal in our case? Recall the fact that ICL is an inductive problem, i.e., problem of predicting the next observation given past ones. Specifically, predictor hpred performing ICL predicts the ith token si given previous elements in the sequence s1:i1, using mechanisms it may have learned for this purpose based on sequences STtrain (N, D) seen in training (denoted STtrain from hereon for brevity). Then, given hypothesis space of possible solutions the model has learned, Bayesian inference prescribes an optimal way to solve this problem via the posterior predictive distribution: compute weighted average of predictions from each solution, with weights defined by solutions posterior probability (i.e., posterior-weighted average). Relying on the results of Sec. 3, we can assume our hypothesis space simply consists of the memorizing and generalizing predictors2. Thus, in our case, each solution itself corresponds to Bayesian predictorspecifically, predictors and Ghence resulting in the following hierarchical Bayesian model. hpred(sis1:i1, STtrain ) = p(M STtrain ) (sis1:i1) + p(GSTtrain ) G(sis1:i1). (3) The mathematical form of the predictor above frames in-context behavior as linear interpolation in and G, with posterior probabilities p(M STtrain ), p(GSTtrain ), estimated from training, determining the interpolation weights. Then, in order to use this model to explain how neural network performs ICL, we must estimate how posterior probabilities vary across training and data conditions. Modeling the Posterior Probabilities. The posterior probability for predictor Q, i.e., p(QSTtrain ) p(STtrain Q)p(Q), is comprised of likelihood term and prior termthus, these are the two terms we need to estimate. In line with the perspective of rational analysis, in modeling these terms, we consider the following two well-known computational constraints of neural networks. 2While in principle it is possible that other predictors offer reasonable hypotheses for explaining model behavior, in the settings we analyze, we find that very quickly into training, other predictors (e.g., an optimal constant solution which always predicts the mean in linear regression [63, 64]) start to perform poorly compared to and in predicting model behavior. We thus focus our analysis on the regime where and are the primary hypotheses explaining model behavior (see App. for further discussion). 6 A1: Loss scales in power-law manner with dataset-size , i.e., L(N ) L() + A/N α, where L(N ) denotes the average loss on dataset at time and is constant that depends on model loss at initialization and training hyperparameters. A2: Neural networks exhibit bias toward simpler solutions. Specifically, using K(Q) to denote the Kolmogorov complexity for predictor Q, we accommodate the Transformer-specific implementation cost by defining KT(Q) = K(Q)β. Then, taking the form of universal prior, the prior probability of learning predictor is p(Q) 2 KT(Q) = 2K(Q)β . A1 is merely paraphrased version of well-known power-law scaling behaviors seen in neural network training [49, 50] and dictates how quickly observed data updates model behavior. That is, it offers functional form for the rate at which likelihood in posterior calculation grows, i.e., the rate of evidence accumulation. Meanwhile, A2 is well-known inductive bias of neural networks [4348]. Our specific functional form for the prior is grounded in algorithmic information theory: Kolmogorov complexity K(Q) is the length of the shortest program on universal Turing machine that implements Q, and the coding theorem relates it to probability via p(Q) 2K(Q) [6567]. Following common practice [44, 45, 6871], we estimate Kolmogorov complexity (which is uncomputable) as ˆK via lossless compression: we apply several lossless compressors to the code and data for Q, and take the smallest resulting size (App. F.2). For brevity, we write ˆK as below. Returning to our goal, we now consider model trained for iterations on task-mixture Ttrain of diversity D. The log-posterior odds of the two predictors can be defined as follows. η(N, D) := log (M STtrain ) (GSTtrain ) (cid:125) (cid:123)(cid:122) (cid:124) Posterior odds = log (STtrain ) (STtrain G) (cid:123)(cid:122) (cid:125) (cid:124) Bayes factor + log . (M ) (G) (cid:124) (cid:123)(cid:122) (cid:125) Prior odds Under constraints A1, A2, this simplifies as follows (see App. D). η(N, D) = γN 1αL(D) (cid:125) (cid:124) (cid:123)(cid:122) Loss term K(D)β (cid:125) (cid:123)(cid:122) (cid:124) Complexity term , (4) where K(D)β := log 2 (KM (D)β KG(D)β) is the difference between the exponentiated Kolmogorov complexity of the two predictors; L(D) := LG(STtrain (D)) LM (STtrain (D)) is the difference between the average loss of the two predictors on dataset of sequences sampled from Ttrain; and γ is constant related to the term from constraint A1. To get the posterior probabilities for and G, we simply convert η via the sigmoid function, denoted σ(), yielding: hpred(sis1:i1) = σ (η(N, D)) (sis1:i1) + (1 σ (η(N, D))) G(sis1:i1). (5) Note that the free parameters of this Bayesian model, i.e., (α, β, γ), depend on the problem setting and the Transformers learning dynamics on it. To identify their values, we simply fit the Bayesian models predictions to the pretrained Transformer h(.)s next-token predictions on inputs retrieved from subset of values (N, D). We emphasize that we only fit three free parameters across model checkpoints in 11 different training runs to get our results. Validating the Model. We now check whether our model accurately captures the behavior of the pretrained Transformer and reproduces ICLs phenomenology. As shown in Fig. 5, our Bayesian model yields an almost perfect prediction of Transformer next-token predictions for both seen / unseen settings. Moreover, without fitting to the relative distance maps, we find an almost perfect match between the posterior probabilities of the memorizing predictor given by our model and relative distance values. These results are replicated across 72 different maps in App. H.4 with varying MLP width, context length, and task dimensionality, yielding robust support for our model. Across all maps, we find our model is highly predictive of Transformer next-token predictions, with mean R2 of 0.97 in Linear Regression, mean agreement of 0.92 in Classification, and mean Spearman rank correlation of 0.97 in Balls & Urns. Additionally, we find very strong correlations of 0.99, 0.98, 0.99 between our models posterior probabilities and the relative distance values given by the Transformer in the Linear Regression, Classification, and Balls & Urns settings, respectively. Finally, we also examine ablations of our functional form in App. I, showing the computational constraints we assume are necessary for the success of our framework. Figure 6: Novel predictions from our framework. (a) Our framework predicts that the posterior probability of the memorized predictor (and hence the relative distance, which can be thought of as an empirical estimate of this quantity) will show sub-linear scaling with respect to , and sigmoidal growth with respect to 1α (holding constant). This is clearly shown in the figure, with dark lines representing parameterized logistic fits in 1α space. (b) We predict rapid change in model behavior for intermediate values of and D, yielding crossover-like boundary between the predictor that best explains model outputs. This can be seen via the magnitude of the second derivative of the relative distance near the boundary. Marginal plots show the second derivative of the relative distance with respect to or D, with the data shown in these plots denoted on the vector map by the dotted lines. (c) Finally, our analysis predicts super-linear scaling of the time of transience (the time at which relative distance drel = 0.5) as diversity increases. 4.1 Novel Predictions We next analyze our theoretical model to make informative qualitative predictions about Transformer behavior. Unless stated otherwise, we use the Balls & Urns setting with context length of 128, task dimensionality of 8, and MLP width of 256. Sub-linear growth from generalizing to memorizing predictor. Given the empirical match between the Bayesian models posterior probabilities and the relative distance computed using the Transformer (see Fig. 6), we examine whether relative distance takes the functional form predicted by the model: sublinear growth with respect to , and sigmoidal growth with respect to 1α (holding constant). This arises from Eq. 4, 5, where evidence accumulates sub-linearly with the number of samples . As can be seen in Fig. 6(a), the relative distance of Transformer predictions between the memorizing and generalizing predictors clearly exhibits sub-linear trend with and sigmoidal growth with respect to 1α. Note also that the bottom left panel of Fig. 6(a) clearly shows that even at high task diversity values, relative distance slowly increases towards the memorizing predictor in sigmoidal manner. This stands in contrast to Raventós et al. [25]s claim that at high the Transformer only becomes closer to the generalizing predictor with increasing (see App. for further discussion). Rapid behavior change near crossover. At the point where the two hypotheses have equal log-posterior odds, our model predicts there will be crossover in which predictor dominates the posterior and explains model behavior. Given our sigmoidal functional form for η(N, D), we can expect this change to be rapidsmall variations in experimental conditions (e.g., task diversity) will yield large changes in model behavior. To confirm this, we plot the second derivative of the relative distance in Fig. 6(b), finding its magnitude is indeed greatest at the boundary η(N, D) = 0. Scaling of time to Transience with Task-Diversity. For given value of D, we can predict the critical training time at which transience, i.e., the crossover from generalizing to memorizing predictor, occurs by solving for in η(N , D) = 0 (i.e., relative distance drel = 0.5). Specifically, (cid:104) K(D)β γL(D) (cid:105) 1 1α we have: (D) = . We thus plot the predicted time to transience as function of in Fig. 6(c). As can be seen, beyond the two-hypotheses threshold (which determines the minimum value in which our model holds, see App. E), our predictions hold well with the empirically observed data. These results indicate super-linear growth of time to transience with task diversity. Importantly, we note that if this prediction derived from our model holds beyond our tested settings, it suggests that if the denominator in the expression becomes very small, e.g., when there are 8 enough tasks in the mixture such that the memorizing and generalizing predictors produce similar outputs, the time to transience can approach infinity. In such condition, generalization will persist regardless of how long model is trained. Interestingly, we find that using learning rate annealing schedule enhances the Transformers adherence to Bayes-optimal trajectories from generalization to memorization, and so we use it in this experiment. However, the effect of annealing appears to decay throughout training, yielding slower rate of advancing toward memorization, which can be seen by the last two observed transience points veering from our models predictions (see App. K). 4.2 The Loss-Complexity Tradeoff Having formalized and demonstrated the empirical validity of our hierarchical Bayesian framework, we now discuss the intuitive interpretation it offers us. Specifically, Eq. 4 suggests that the tradeoff between posterior-odds corresponding to different predictors is driven by the loss predictor achieves on the training data and its complexity (see Fig. 7): early in training, the prior dominates, therefore less complex solution the generalizing predictor in our casewill be strongly favored as per the posterior calculation. However, the memorizing predictor will almost always have lower loss than the generalizing predictor on training data. Thus, in low-to-medium task diversity settings, as training proceeds and increases, the loss term in Eq. 4 will overtake the complexity term, i.e., the likelihood eventually dominates the posterior and floods the prior. This will lead to the memorizing predictor becoming favoredexplaining the transient generalization phenomenon from prior work [24]. In contrast, in high task diversity settings, the prior strongly disfavors the memorizing predictor due to its complexity, and given the sub-linear accumulation of likelihood (i.e., the 1α term), the time for transience to occur grows superlinearlygiving rise to the phenomenon of task diversity threshold seen in prior work [25]. The importance of losscomplexity tradeoff for ICL was also arrived at by two recent papers [30, 72], which we discuss further in Sec. 5. To further exemplify the intuition above, we can consider variations on the training pipeline that can be expected to affect models ability to implement more complex predictors. Using the number of parameters as control proxy to this end, we find that scaling MLP width raises the transition boundary between memorization and generalizationi.e., memorization is preferred more  (Fig. 8)  . Our model closely captures this effect: we see decreasing complexity penalty β with larger MLP width, pointing toward the possibility that larger models penalize complexity less, yielding more memorization. Interestingly, we find that β decays exponentially with MLP width. Finally, we also vary context length and task dimensionality, and find that, once again, our model captures the effects of these interventions (see App. H.4). Figure 7: Intuition Elicited by The Bayesian Model. (a) Our framework suggests Transformers have prior preference for learning simpler solutions, which often generalize better. However, throughout training, preference is updated towards solutions that better explain the data (i.e., have greater likelihood). This happens even at the expense of higher complexity, which in our case, yields transition toward memorizing predictor. (b, c) Our framework captures the tradeoff between solutions, showing that the boundary between the two phases corresponds to equal posterior probabilities of the two predictors. Figure 8: Increased memorization with MLP width is captured by Bayesian model as reduced complexity penalty. (a) We find relative distance to memorizing predictor decreases with MLP width. (b) This transition is closely captured by our Bayesian model, in which the complexity penalty β, and hence the difference β, decays exponentially with MLP width, yielding reduced prior preference toward the generalizing predictor."
        },
        {
            "title": "5 Discussion",
            "content": "In this work, we aim to unify findings from the ICL literature by asking why Transformers learn disparate strategies for performing ICL across varying training conditions. To do so, we take normative perspective [38] which aims to explain Transformer learning as optimal under computational constraints. This lens yields hierarchical Bayesian framework, which, assuming simplicity bias and scaling laws as computational constraints, offers highly predictive account of model behavior: by fitting merely three variables to Transformer next-token predictions, we can almost perfectly predict model behavior across spectrum of experimental settings without access to its weights. Our account also implies fundamental trade-off that occurs throughout training between the loss and complexity of potential solutions learned by model, with simpler, generalizing solutions learned early in training, while more complex, better-fitting solutions eventually becoming preferred. This tradeoff helps explain prior findings in the ICL literature, and provides novel predictions regarding training dynamics. Thus, we argue for our hierarchical Bayesian framework as an explanatory and predictive account of ICL, and step towards unified understanding of its phenomenology. Relation to previous Bayesian models of ICL. Here, we discuss the relation between our work and other Bayesian or normative accounts of ICL. We provide an extended review of additional related works in App. B. Several prior works framed ICL as Bayesian at inference-time, meaning models implement single Bayesian predictor in context [25, 35, 51, 62, 73, 74]. The focus on single predictor in these works often led them to view ICL as single strategy, with some focusing solely on memorizing solution [35], while others refer mainly to generalizing solution as ICL [25]. While there are cases in which in-context behavior is well approximated by single predictor (e.g., the memorizing solution for low N, D), our results robustly show that several predictors are required to fully capture model behavior, and that the extent to which particular predictor explains model behavior varies across training. Thus, posterior-weighted average over different predictors, which considers bias towards different predictors coming from training, rather than only inference-time, is required to fully capture model behavior across conditions. In contrast with studies focusing on inference-time, two recent works, Carroll et al. [30] and Elmoznino et al. [72], offer Bayesian or normative views of pretraining to perform ICL. Importantly, while these papers take different theoretical perspectives from ours, they arrive at similar conclusion as us: the existence of tradeoff between loss and solution complexity in pretraining. Elmoznino et al. [72] offer normative theoretical analysis of training to perform ICL via next-token prediction loss, showing it yields an Occams razor objective which minimizes both loss and solution complexity. Carroll et al. [30] study task diversity effects and transient generalization in the linear regression setting of Raventós et al. [25]. Their Bayesian account of pretraining, which is rooted in theory of singular models [75] and makes different assumptions from ours, interestingly yields relatively similar functional form for the posterior odds (though their form does not take into account neural scaling laws). However, their measure of complexity is architecture-dependent [76], thus they only provide qualitative analysis as they cannot directly estimate the complexity of Bayesian predictors. In contrast, our hierarchical Bayesian framework provides quantitative, predictive account of pretraining phenomena, in addition to capturing inference-time behavior as posterior-weighted average of solutions, which is not addressed by Carroll et al. [30] or Elmoznino et al. [72]. Despite that, we view these works as valuable contributions with complementary insights to ours, in particular regarding potential explanations for the source of neural networks simplicity bias. Limitations. While we rely on specific theoretical abstraction to arrive at our results, i.e., hierarchical Bayesian model, we believe the predictive power of this abstraction corroborates its faithfulness. However, one limitation of our analysis is use of settings where model behavior is largely explained by only two predictors. It can be interesting to analyze settings where more predictors are feasible, e.g., the four predictors identified by Park et al. [26] in their task of learning finite mixture of Markov chains. We believe this extension would be straightforward: we currently only characterize the dynamics of learning memorizing vs. generalizing predictor, but additionally modeling the statistics used to produce outputs (bigram vs. unigram) should accommodate the solutions identified by Park et al. [26]. Additionally, our current modeling analysis focuses on capturing Transformer behavior for sequences seen during training (ID), and extending it to OOD evaluation is an important step. Finally, crucial limitation of our analysis comes from the simple relation we assume between algorithmic complexity and complexity of implementation by Transformerwhile the assumption of simplicity bias is well-backed by theoretical and empirical claims [43, 44, 46], we believe our 10 assumed relation could be improved by building on recent advances defining architecture-dependent measures of how many effective parameters are used by model to implement solution [76, 77]. Takeaways. We see several main takeaways from our work for understanding ICL, training dynamics, and generalization behavior in neural networks more broadly. Is ICL Bayesian? Depends on the Assumptions. There exists debate regarding whether ICL can be viewed as Bayesian [25, 35, 62, 78]. Some studies that saw the emergence of generalizing predictor have categorized it as non-Bayesian [25, 62], since it is not the Bayes-optimal solution given the training distribution (which is the memorizing solution). In contrast, taking the perspective of rational analysis, the right question is not whether ICL is Bayesian, but under what assumptions is it Bayesian. Clearly, the generalizing predictor is Bayes-optimal with respect to the true distribution Ttrue. Moreover, as we show in this work, when taking into account simplicity bias over predictors, learning generalizing predictor can be considered Bayes-optimal in certain conditions even when it is not the optimal strategy for minimizing train loss. Thus, by extending Bayesian perspective on ICL to both training and inference-time, we show that assessing the Bayes-optimality of ICL requires considering the models bias towards different predictors coming from pretraining. Cautiously, given our highly predictive results, we conclude that ICL can be considered as approximately Bayesian given constraints (assumptions) of simplicity bias and sublinear sample efficiency (though see App. for discussion of deviation from Bayes-optimality that is not accommodated by our current assumptions). Loss-Complexity Tradeoff is Fundamental to Understanding Training Dynamics. We find that tradeoff between the loss and complexity of solutions learned by models lies at the heart of ICL phenomenology. We believe the hierarchical view from which this tradeoff emerges, in which pretraining is process of updating posterior probability for different solutions based on their complexity and loss, and ICL is posterior-weighted average of solutions, can be powerful explanatory perspective for understanding Transformer learning dynamics more broadly. The Value of Normative Perspective. An important takeaway that we believe may be of independent interest to the community is that, to understand generalization behavior in Transformers and neural networks more broadly, it may be enough to observe the structure of the data, and assume the network is well-approximated by Bayes-optimal density estimator with simplicity bias and sublinear sample efficiency. We hope our work shows that such top-down normative perspective can provide highly predictive accounts, as well as potential explanations for why models behave the way they do. We encourage wider adoption of this approach for understanding neural network behavior."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank the Computation and Cognition Lab, in particular Ben Prystawski and Michael Li; Jay McClelland, Satchel Grant, Jerome Han and the PDP Lab; the Physics of Intelligence group at Harvard, especially Eric Bigelow and Sonia Murthy; the CRISP Lab at Harvard; Jesse Hoogland and Matthew Farrugia-Roberts; Surya Ganguli and the Neural Dynamics and Computation Lab; and Navin Goyal and the theory group at Microsoft Research India for useful discussions."
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [3] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model, 2023. URL https://arxiv.org/abs/2303.03378. [4] Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, et al. Foundational 11 challenges in assuring alignment and safety of large language models. arXiv:2404.09932, 2024. arXiv preprint [5] OpenAI. Openai o3 and o4-mini system card, 2025. URL https://cdn.openai.com/pdf/ 2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf. [6] Gemini Team. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [7] AnthropicAI. Claude 3.7 sonnet system card, 2025. URL https://www.anthropic.com/ news/claude-3-7-sonnet. [8] Keunwoo Peter Yu, Zheyuan Zhang, Fengyuan Hu, Shane Storks, and Joyce Chai. Eliciting in-context learning in vision-language models for videos through curated data distributional properties. arXiv preprint arXiv:2311.17041, 2023. [9] Yida Yin, Zekai Wang, Yuvan Sharma, Dantong Niu, Trevor Darrell, and Roei Herzig. Incontext learning enables robot action prediction in llms. arXiv preprint arXiv:2410.12782, 2024. [10] Core Francisco Park, Andrew Lee, Ekdeep Singh Lubana, Yongyi Yang, Maya Okawa, Kento Nishi, Martin Wattenberg, and Hidenori Tanaka. Iclr: In-context learning of representations. In The Thirteenth International Conference on Learning Representations, 2025. [11] Can Demircan, Tankred Saanum, Akshay Jagadish, Marcel Binz, and Eric Schulz. Sparse autoencoders reveal temporal difference learning in large language models. arXiv preprint arXiv:2410.01280, 2024. [12] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. [13] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads, 2022. URL https://arxiv.org/abs/2209.11895. [14] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? case study of simple function classes. Advances in Neural Information Processing Systems, 35:3058330598, 2022. [15] Suraj Anand, Michael Lepori, Jack Merullo, and Ellie Pavlick. Dual process learning: Controlling use of in-context vs. in-weights strategies with weight forgetting. arXiv preprint arXiv:2406.00053, 2024. [16] Ekin Akyürek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context language learning: Architectures and algorithms, 2024. URL https://arxiv.org/abs/2401.12973. [17] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. Advances in neural information processing systems, 36, 2024. [18] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022. [19] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. Larger language models do in-context learning differently, 2023. URL https://arxiv.org/abs/2303.03846. 12 [20] Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:1887818891, 2022. [21] Ziqian Lin and Kangwook Lee. Dual operating modes of in-context learning, 2024. URL https://arxiv.org/abs/2402.18819. [22] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 3515135174. PMLR, 2023. [23] Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International conference on machine learning, pages 1956519594. PMLR, 2023. [24] Aaditya K. Singh, Stephanie C. Y. Chan, Ted Moskovitz, Erin Grant, Andrew M. Saxe, and Felix Hill. The transient nature of emergent in-context learning in transformers, 2023. URL https://arxiv.org/abs/2311.08360. [25] Allan Raventós, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. Advances in Neural Information Processing Systems, 36, 2024. [26] Core Francisco Park, Ekdeep Singh Lubana, Itamar Pres, and Hidenori Tanaka. Competition dynamics shape algorithmic phases of in-context learning, 2024. URL https://arxiv.org/ abs/2412.01003. [27] Bryan Chan, Xinyi Chen, András György, and Dale Schuurmans. Toward understanding in-context vs. in-weight learning. arXiv preprint arXiv:2410.23042, 2024. [28] Gautam Reddy. The mechanistic basis of data dependence and abrupt learning in an in-context classification task, 2023. URL https://arxiv.org/abs/2312.03002. [29] Alex Nguyen and Gautam Reddy. Differential learning kinetics govern the transition from memorization to generalization during in-context learning, 2024. URL https://arxiv.org/ abs/2412.00104. [30] Liam Carroll, Jesse Hoogland, Matthew Farrugia-Roberts, and Daniel Murfet. Dynamics of transient structure in in-context linear regression transformers. arXiv preprint arXiv:2501.17745, 2025. [31] Kayo Yin and Jacob Steinhardt. Which attention heads matter for in-context learning? arXiv preprint arXiv:2502.14010, 2025. [32] Aaditya K. Singh, Ted Moskovitz, Felix Hill, Stephanie C. Y. Chan, and Andrew M. Saxe. What needs to go right for an induction head? mechanistic study of in-context learning circuits and their formation, 2024. URL https://arxiv.org/abs/2404.07129. [33] Aaditya Singh, Ted Moskovitz, Sara Dragutinovic, Felix Hill, Stephanie CY Chan, and Andrew Saxe. Strategy coopetition explains the emergence and transience of in-context learning. arXiv preprint arXiv:2503.05631, 2025. [34] Benjamin L. Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, and Nikolaos Tsilivis. The evolution of statistical induction heads: In-context learning markov chains, 2024. URL https: //arxiv.org/abs/2402.11004. [35] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021. [36] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers. arXiv preprint arXiv:2212.10559, 2022. [37] Andrew Kyle Lampinen, Stephanie CY Chan, Aaditya Singh, and Murray Shanahan. The broader spectrum of in-context learning. arXiv preprint arXiv:2412.03782, 2024. [38] John Anderson. The adaptive character of thought. Psychology Press, 2013. [39] Nick Chater, Mike Oaksford, Nick Chater, and Mike Oaksford. Ten years of the rational analysis of cognition. Trends in cognitive sciences, 3(2):5765, 1999. [40] Thomas Griffiths, Falk Lieder, and Noah Goodman. Rational use of cognitive resources: Levels of analysis between the computational and the algorithmic. Topics in cognitive science, 7(2):217229, 2015. [41] Noah Goodman, Joshua Tenenbaum, Jacob Feldman, and Thomas Griffiths. rational analysis of rule-based concept learning. Cognitive science, 32(1):108154, 2008. [42] Falk Lieder and Thomas Griffiths. Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources. Behavioral and brain sciences, 43:e1, 2020. [43] Preetum Nakkiran, Dimitris Kalimeris, Gal Kaplun, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Adv. in Neural Information Processing Systems (NeurIPS), 2019. [44] Guillermo Valle-Perez, Chico Camargo, and Ard Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. arXiv preprint. arXiv:1805.08522, 2018. [45] Chris Mingard, Henry Rees, Guillermo Valle-Pérez, and Ard Louis. Deep neural networks have an inbuilt occams razor. Nature Communications, 16(1):220, 2025. [46] Feng Chen, Daniel Kunin, Atsushi Yamamura, and Surya Ganguli. Stochastic collapse: How gradient noise attracts sgd dynamics towards simpler subnetworks. Advances in Neural Information Processing Systems, 36:3502735063, 2023. [47] Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington. The surprising simplicity of the early-time learning dynamics of neural networks. Adv. in Neural Information Processing Systems (NeurIPS), 2020. [48] Satwik Bhattamishra, Arkil Patel, Varun Kanade, and Phil Blunsom. Simplicity bias in transformers and their ability to learn sparse boolean functions. arXiv preprint arXiv:2211.12316, 2022. [49] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [50] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [51] Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. arXiv preprint arXiv:2305.19420, 2023. [52] Benjamin Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning, pages 57935831. PMLR, 2022. [53] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022. [54] Eric Bigelow, Ekdeep Singh Lubana, Robert Dick, Hidenori Tanaka, and Tomer Ullman. In-context learning dynamics with random binary sequences. arXiv preprint arXiv:2310.17639, 2023. 14 [55] Johannes Schubert, Akshay Jagadish, Marcel Binz, and Eric Schulz. In-context learning agents are asymmetric belief updaters. arXiv preprint arXiv:2402.03969, 2024. [56] GPT-NeoX. Gpt-neox, huggingface., 2025. URL https://huggingface.co/docs/ transformers/en/model_doc/gpt_neox. [57] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gptneox-20b: An open-source autoregressive language model, 2022. URL https://arxiv.org/ abs/2204.06745. [58] Urn problem. In Wikipedia, December 2024. URL https://en.wikipedia.org/wiki/ Urn_problem. [59] Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. Language modeling is compression. arXiv preprint arXiv:2309.10668, 2023. [60] Tianyu He, Darshil Doshi, Aritra Das, and Andrey Gromov. Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks. arXiv preprint arXiv:2406.02550, 2024. [61] Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose incontext learning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022. [62] Madhur Panwar, Kabir Ahuja, and Navin Goyal. In-context learning through the bayesian prism, 2024. URL https://arxiv.org/abs/2306.04891. [63] Jirko Rubruck, Jan P. Bauer, Andrew Saxe, and Christopher Summerfield. Early learning of the optimal constant solution in neural networks and humans, 2024. URL https://arxiv.org/ abs/2406.17467. [64] Jesse Hoogland, George Wang, Matthew Farrugia-Roberts, Liam Carroll, Susan Wei, and Daniel Murfet. Loss landscape degeneracy drives stagewise development in transformers, 2025. URL https://arxiv.org/abs/2402.02364. [65] Ray Solomonoff. Complexity-based induction systems: comparisons and convergence theorems. IEEE transactions on Information Theory, 24(4):422432, 1978. [66] Leonid Anatolevich Levin. Laws of information conservation (nongrowth) and aspects of the foundation of probability theory. Problemy Peredachi Informatsii, 10(3):3035, 1974. [67] Ming Li, Paul Vitányi, et al. An introduction to Kolmogorov complexity and its applications, volume 3. Springer, 2008. [68] Hector Zenil. review of methods for estimating algorithmic complexity: Options, challenges, and new directions. Entropy, 22(6):612, 2020. [69] Peter Grunwald and Paul Vitányi. Shannon information and kolmogorov complexity. arXiv preprint cs/0410002, 2004. [70] Stephen Fenner and Lance Fortnow. Compression complexity. arXiv preprint arXiv:1702.04779, 2017. [71] Kamaludin Dingle, Chico Camargo, and Ard Louis. Inputoutput maps are strongly biased towards simple outputs. Nature communications, 9(1):761, 2018. [72] Eric Elmoznino, Tom Marty, Tejas Kasetty, Leo Gagnon, Sarthak Mittal, Mahan Fathi, Dhanya Sridhar, and Guillaume Lajoie. In-context learning and occams razor, 2025. URL https: //arxiv.org/abs/2410.14086. [73] Aryaman Arora, Dan Jurafsky, Christopher Potts, and Noah Goodman. Bayesian scaling laws for in-context learning. arXiv preprint arXiv:2410.16531, 2024. 15 [74] Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. Explaining emergent in-context learning as kernel regression. arXiv preprint arXiv:2305.12766, 2023. [75] Sumio Watanabe. widely applicable bayesian information criterion. The Journal of Machine Learning Research, 14(1):867897, 2013. [76] Edmund Lau, Zach Furman, George Wang, Daniel Murfet, and Susan Wei. The local learning coefficient: singularity-aware complexity measure. arXiv preprint arXiv:2308.12108, 2023. [77] Jesse Hoogland, George Wang, Matthew Farrugia-Roberts, Liam Carroll, Susan Wei, and Daniel Murfet. The developmental landscape of in-context learning, 2024. URL https: //arxiv.org/abs/2402.02364. [78] Fabian Falck, Ziyu Wang, and Chris Holmes. Is in-context learning in large language models bayesian? martingale perspective. arXiv preprint arXiv:2406.00793, 2024. [79] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. [80] Samuel Gershman. The rational analysis of memory. Oxford handbook of human memory., 2021. [81] Rahul Bhui, Lucy Lai, and Samuel Gershman. Resource-rational decision making. Current Opinion in Behavioral Sciences, 41:1521, 2021. [82] Yue Lu, Mary Letey, Jacob Zavatone-Veth, Anindita Maiti, and Cengiz Pehlevan. Asymptotic theory of in-context learning by linear attention. arXiv preprint arXiv:2405.11751, 2024. [83] Charles Kemp, Noah Goodman, and Joshua Tenenbaum. Learning to learn causal models. Cognitive science, 34(7):11851243, 2010. [84] Michael Li, Fred Callaway, William Thompson, Ryan Adams, and Thomas Griffiths. Learning to learn functions. Cognitive science, 47(4):e13262, 2023. [85] Christopher Lucas and Thomas Griffiths. Learning the form of causal relationships using hierarchical bayesian models. Cognitive Science, 34(1):113147, 2010. [86] Joseph Austerweil, Sophia Sanborn, and Thomas Griffiths. Learning how to generalize. Cognitive science, 43(8):e12777, 2019. [87] Charles Kemp, Andrew Perfors, and Joshua Tenenbaum. Learning overhypotheses with hierarchical bayesian models. Developmental science, 10(3):307321, 2007. [88] Amy Perfors and Joshua Tenenbaum. Learning to learn categories. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 31, 2009. [89] Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-based meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018. [90] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks, 2017. URL https://arxiv.org/abs/1703.03400. [91] Hong Jun Jeon, Jason Lee, Qi Lei, and Benjamin Van Roy. An information-theoretic analysis of in-context learning. arXiv preprint arXiv:2401.15530, 2024. [92] Rahul Ramesh, Mikail Khona, Robert Dick, Hidenori Tanaka, and Ekdeep Singh Lubana. How capable can transformer become? study on synthetic, interpretable tasks. arXiv preprint arXiv:2311.12997, 2023. [93] Eric Todd, Millicent Li, Arnab Sen Sharma, Aaron Mueller, Byron Wallace, and David Bau. Function vectors in large language models. arXiv preprint arXiv:2310.15213, 2023. 16 [94] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. Advances in Neural Information Processing Systems, 36:4561445650, 2023. [95] Cem Anil, Esin Durmus, Nina Panickssery, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Meg Tong, Jesse Mu, Daniel Ford, et al. Many-shot jailbreaking. Advances in Neural Information Processing Systems, 37:129696129742, 2024. [96] Tian Qin, Naomi Saphra, and David Alvarez-Melis. Sometimes am tree: Data drives unstable hierarchical generalization. arXiv preprint arXiv:2412.04619, 2024. [97] Nigel Goldenfeld. Lectures on phase transitions and the renormalization group. CRC Press, 2018. [98] Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. IEEE Transactions on pattern analysis and machine intelligence, PAMI-6 (6):721741, 1984. 17 19 20 20 20 21 23 23 24 26 27 27 27 29 29 30 31 33 33 37 41 45"
        },
        {
            "title": "Table of Contents",
            "content": "A Glossary of Useful Terms Related Work B.1 Prior Work Studying Task-Diversity Effects and Transience . B.2 Hierarchical Bayesian Models of Learning to Learn . . B.3 Broader Work on Understanding ICL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Future Work Derivations D.1 Log-Posterior Odds . . . D.2 Converting from Posterior-Odds to Predictive Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Two-Hypotheses Threshold: Minimum amount of training to enable the Hierarchical Bayesian Model Experimental Details F.1 Training and Model Details . . F.2 Analysis Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Details Regarding Settings and Predictors . G.1 Balls and urns . . . G.2 Linear regression . . . G.3 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Main Results Across All Settings . . H.1 Task Diversity Effects . H.2 Transience . . . . . H.3 Absolute Distance from Predictors . H.4 Model Predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Functional Form Ablations Memorization Continues to Increase After Task Diversity Threshold Refutation of Raventós et al. [25]s Claim Learning Rate Annealing Can Improve Adherence to Bayes-Optimal Trajectories"
        },
        {
            "title": "A Glossary of Useful Terms",
            "content": "In-Context Learning (ICL). In this paper, we use broad construal of ICL advanced by Lampinen et al. [37] and Olsson et al. [79]: any way in which models use their context to adapt their predictions and reduce loss can be considered as ICL. This notion of ICL encapsulates many forms of sequence modeling and instruction following, as well as the more traditional view of ICL as few-shot learning [2]. Hence, in this work, we use tasks that capture both sequence modeling [26] and few-shot learning [25, 28] notions of ICL. Generalizing Predictor (G). predictor defined by posterior-weighted average with continuous prior over the true data-generating distribution Ttrue. Such predictor does not depend on the tasks seen during training, and can hence generalize well to novel, unseen tasks. This predictor maps onto task-learning or inference notions of ICL [26]. See App. for the form of generalizing predictors in the studied settings. Memorizing Predictor (M ). predictor defined by posterior-weighted average with discrete prior defined over the distribution of tasks seen by the model during training, Ttrain. Such predictor depends on the tasks seen during training, and hence generalizes primarily to those tasks. This predictor maps onto task-retrieval notions of ICL, though it also captures in-weights learning in the classification task introduced by Chan et al. [20]. See App. for the form of memorizing predictors in the studied settings. Task Diversity (D). For mixture distribution Ttrain defined over tasks {f (w1, ), . . . , (wD, )}, is referred to as the task diversity of the mixture. Relative Distance (drel). measure we use to characterize trade-off between predictors in settings where there are primarily two predictors (though one can easily generalize this measure to setting with more predictors). Specifically, given model h, two predictors Q1, Q2, and distance function d, we define the term := d(h,Q1)d(h,Q2) and then relative distance as drel = (r+1)/2. The latter operation rescales to scale of 0 (if = Q1) to 1 (if = Q2), essentially assessing where the model lives when line is drawn with endpoints ranging from Q1 to Q2. d(Q1,Q2) Posterior Odds, Prior Odds, Bayes Factor. Consider set of observations and two hypotheses H1 and H2 that are being assessed as candidates to explain the data. Prior odds are defined as the ratio (H1) (H2) , i.e., if no observations are seen yet, which hypothesis is apriori preferred. Bayes factor is defined as the ratio of likelihoods (XH1) (XH2) , i.e., once the observations are received, we compare how likely individual hypotheses deem these observations. Finally, posterior odds are defined as the ratio (H1X) (H2) (XH1) (XH2) = Prior odds Bayes factor, i.e., how do observations affect the prior to help assess which hypothesis is more likely. (H2X) = (H1) Rational Analysis. Rational analysis is framework in cognitive science introduced by John R. Anderson [38], in which the behavior of learner is explained as an optimal adaptation to its learning environment, given its goal and computational constraints. In the case of training neural network, the learning environment is given by the training data distribution, the goal is given by the optimization objective, and the computational constraints can be seen as constraints and inductive biases of the architecture and optimization algorithm. The framework of rational analysis is characterized as normative, since it specifies how learner should behave, in an optimal sense, given its environment, goals, and computational constraints. This approach has been successfully applied in cognitive science to explain wide range of phenomena in humans [38, 39, 41, 80, 81]. Transient Generalization (N ). For given task diversity D, when model is trained for sufficiently long, its behavior transitions from more closely resembling generalizing predictor to more closely resembling memorizing one. We define this point as the point at which the relative distance between memorizing and generalizing crosses 0.5. This phenomenon is called transience, transient generalization, or the transient nature of in-context learning (see App. for longer discussion). The critical time to reach this point is denoted in the main paper."
        },
        {
            "title": "B Related Work",
            "content": "B.1 Prior Work Studying Task-Diversity Effects and Transience We first discuss prior work studying the phenomenology of ICL that forms the core of our paper: task diversity effects and transience. These works often focus on providing mechanistic accounts that are more bottom-up in nature, i.e., suitable for studying specific settings. For example, the induction head is core mechanism employed by model to perform the in-context classification task by Singh et al. [24], Reddy [28]. When transience occurs, the role of induction head is diminished, since it is not vital for implementing memorizing predictor. However, as we show, transience occurs across spectrum of settings, including ones where induction heads are never learned during training (e.g., Balls and Urns, which we analyze using one-layer Transformer)prior mechanistic accounts would not help justify transience in such settings. Our work thus takes top-down account, i.e., we offer insights based on computational model of ICL developed by capturing its phenomenology. This helps identify relevant control variables controlling dynamic we argue lies at the heart of transience and task diversity effects: tradeoff between loss and complexity of solution, which manifests itself into transitions between generalizing and memorizing predictors. Reconciling our computational model with mechanistic approaches undertaken in prior work can be an exciting avenue for progress. Task diversity Threshold. phenomenon first popularized by Raventós et al. [25] in an incontext linear regression task, albeit originally demonstrated by Kirsch et al. [61] in prior work on in-context classification of permuted MNIST images, and recently expanded to Markov modeling setting by Park et al. [26], to another classification setting by Nguyen and Reddy [29], as well as to modular arithmetic setting by He et al. [60]. Specifically, Raventós et al. [25] empirically show that if one trains Transformers on mixture of linear regression tasks, there is critical number of tasks below which the model behavior is well-characterized by Bayesian predictor over the seen tasks (what we term the memorizing predictor), while above it the behavior is well-characterized by the standard solution of ridge regression. These results were expanded in recent theoretical work by Lu et al. [82], who study the asymptotic dynamics of linear attention Transformer and show the change in solution used by the model as function of task diversity is second-order phase transition. In contrast to their work, our empirical analysis covers broad range of settings that involve sequence modeling, regression, and classification, while the analytical parts of our paper provide predictive framework that identifies the relevant control variables and explains how they affect the behavior of standard, nonlinear Transformers across all studied settings. Transience / Transient Nature of In-Context Learning. Originally observed by Chan et al. [20] while investigating the effects of data-centric properties on ICL, the term was popularized by Singh et al. [24]. Specifically, focusing on an in-context classification task, Singh et al. [24] showed that models ability to perform the generalizing ICL solution (employing copy mechanism via the induction head) goes away when trained long enough. This phenomenon was recently generalized to Markov modeling task by Park et al. [26] and to simplified variants of the in-context classification setting by Chan et al. [27], Nguyen and Reddy [29]. We especially emphasize the work of Nguyen and Reddy [29], who empirically identify competition dynamic between memorizing and generalizing solution for their task, building on this observation to perform gradient flow analysis of their task and developing an effective theory of how transient generalization occurs. Our work differs in the sense that we provide an account of the competition dynamic and model for its origins, instead of gradient flow account that operates under the assumption of competition. B.2 Hierarchical Bayesian Models of Learning to Learn Hierarchical Bayesian models have been widely employed in cognitive science as models of learning to-learn, or meta-learning, in humans [8388]. These models allow the agent to learn the prior distribution from the data, rather than the prior being predefined. This allows learning inductive biases that constrain hypothesis spaces (called over-hypotheses), which can instruct future learning [87]. Additionally, it entails estimating evidence for several different hypothesis spaces (like the continuous vs. discrete priors in our work) and using these hypothesis spaces for generalization [86]. Note also that much of the work on hierarchical Bayesian models in cognitive science takes normative perspective. Finally, we want to highlight the work of Grant et al. [89], who explicitly 20 analyze well-known meta-learning algorithm (MAML [90]) via hierarchical Bayesian lens. We have been broadly influenced by this literature in approaching the study of ICL, which we believe has many direct parallels with work on learning to learn. B.3 Broader Work on Understanding ICL crucial benefit of our Bayesian modeling framework is that one can retrieve posterior probabilities over the predictors the model predictions are being decomposed into. In other words, we can represent the model predictions as an interpolation of the predictors outputs (since posterior probabilities sum to 1). Park et al. [26] propose similar approach: Linear Interpolation of Algorithms (LIA). Specifically, LIA represents model predictions as an interpolation of the predictors outputs, directly fitting the weights for interpolation. That is, for each condition of training time (N ), task diversity (D), LIA requires fitting set of parameters, summing up to total of parameters fit (across many of the maps in our work, this would mean over 900 parameters per combination of contextlength, task dimensionality, and MLP width). Meanwhile, our framework, by defining precise functional form that explicates the role of and D, minimizes the number of terms needed to perform fitting to 3these terms are primarily related to model family, its learning dynamics, and intrinsic randomness of the data, hence making them hard to explicate. Crucially, one can see our framework as providing grounding to LIA: under the hood, LIA is trying to identify the posterior probabilities for all experimental settings! Moreover, our framework offers an explanation for why change in predictor weights occurs during training, and importantly, can predict dynamics of N, conditions it was not trained on, which LIA cannot do since it has to be fit separately to each N, condition. Beyond the papers discussed above, there have been several complementary efforts to understand ICL from different perspectives (see the recent summary by Lampinen et al. [37] for longer review). For example, several works, some briefly discussed above, characterized the trade-off between memorizing and generalizing in more mechanistic way, as competition dynamic between circuits [24, 26, 29, 33]. We see our results as elucidating the forces driving this competition, as well as demonstrating its consistency across several settings. Additionally, several papers analogize ICL as implicitly performing optimization to learn novel tasks [22, 36, 53] or to meta-learning in general [91]; develop scaling laws for the sample efficiency of ICL [73]; identify limits of tasks that can be learned in-context [14, 17, 92]; demonstrate sudden learning curves exist in ICL [10, 54]; and characterize mechanisms employed by model to perform ICL [13, 31, 34, 93]. Broadly, all these papers offer useful insights that are complementary to ours."
        },
        {
            "title": "C Future Work",
            "content": "Our work also opens up several exciting avenues for further progress. Can we Explain In-Context Transitions? First, we note the phase transition elicited in this work primarily assumes biased optimization process, and the ability to overcome this bias by seeing data supporting another solution. Accordingly, since ICL can be viewed as an optimization process [22, 94], it may be possible to use our results to explain behavioral transitions seen in recent work on in-context learning of novel concepts or behaviors [10, 54, 95]. Does our Framework Explain other Pretraining Phenomena? we believe the competition dynamic characterized in our work is similar to the one demonstrated by Qin et al. [96] in language modeling task. There, the authors show that depending on the data diversity and complexity, model can either learn bag of heuristics or the underlying grammar to generate sentences from the language. It may be possible to explain the phenomenology elicited in that work via the hierarchical Bayes lens we take in this paper, since there likely exists tradeoff between compressibility and loss of heuristic vs. grammar-learning solution. Connecting our top-down framework to bottom-up mechanistic account. Our work intentionally takes top-down approach, and hence does not offer mechanistic account of how Transformer learning dynamics implement the loss-complexity tradeoff, or how the model weights different solutions at inference time. Such bottom-up analysis likely requires studying either the gradient flow dynamics of ICL [29] or using mechanistic interpretability tools to examine circuits [32, 33]."
        },
        {
            "title": "D Derivations",
            "content": "Below, we derive formal expressions for the functional form of log-posterior odds (Eq. 4) and show how one can convert it into predictive model (Eq. 5). For completeness, we repeat below the constraints underlying our modeling framework. A1: Loss scales in power-law manner with dataset-size , i.e., L(N ) L() + A/N α, where L(N ) denotes the average loss on dataset at time , and is constant that depends on model loss at initialization and training hyperparameters. A2: Neural networks exhibit bias toward simpler solutions. Specifically, using K(Q) to denote the Kolmogorov complexity for predictor Q, we accommodate the Transformer-specific implementation cost by defining KT(Q) = K(Q)β. Then, taking the form of universal prior, the prior probability of learning predictor is p(Q) 2 KT(Q) = 2K(Q)β . D.1 Log-Posterior Odds We consider parameterized model class H(.) learning to implement predictor {M, G} when trained using learner on dataset STtrain (N, D) of sequences sampled from the distribution Ttrain with diversity D. We approximate this models learning dynamics via hierarchical Bayes framework, i.e., we assume learning happens via posterior update by computing likelihood of the data under all considered hypotheses (which are themselves Bayesian predictors, hence the term hierarchical). Each hypothesis has an associated prior that reflects the learning pipelines proclivity towards implementing it. For brevity, we will use the notation STtrain to refer to the dataset, with sequences seen at update denoted via superscript S(n) . We also use ΘQ Ttrain to denote the set of parameters in the landscape of model-class H, such that H(θ, s) = Q(s) for any sequence if θ ΘQ. ; i.e., STtrain = nS(n) Ttrain We begin by analyzing the log-posterior of the predictor learned by the model: log (QSTtrain, T, H) = log (ΘQSTtrain , T, H) (θSTtrain , T, H) (S(1) Ttrain , . . . , S(N ) Ttrain θ) (θT, H) Neff(cid:89) n=1 Neff(cid:89) (S(n) Ttrain θ) (θT, H) (S(n) Ttrain Q) (θT, H) θΘQ n=1 = log log A1= log (cid:90) θΘQ (cid:90) θΘQ (cid:90) θΘQ (cid:90) = log = Neff(cid:88) n=1 A2 Neff log (S(n) Ttrain Q) + (cid:90) log θΘQ (θT, H) (cid:123)(cid:122) Prior of the learner and model-class towards learning (cid:124) (cid:125) Neff(cid:88) log (S(n) Ttrain Q) K(Q)β loge 2 1 Neff (cid:124) n=1 (cid:123)(cid:122) Average log likelihood of data under predictor (cid:125) = Neff LQ(STtrain ) K(Q)β loge 2. Overall, we have then, η(N, D) := log (M STtrain , T, H) (GSTtrain , T, H) (cid:0)LG(STtrain (D)) LM (STtrain (D))(cid:1) (cid:0)K(M )β K(G)β(cid:1) loge 2 = Neff = Neff L(D) K(D)β. 23 (6) (7) In the above, our constraints get operationalized as follows. A1 helps us accommodate the fact that while Bayesian learner would make optimal use of all samples shown to it, neural network training in fact makes suboptimal use of samples seen during training, which we model by defining Neff, i.e., the effective number of samples neural network learns from. We will estimate this value below in Eq. 9. A2 provides form for the prior the learning pipeline (includes the learner and model-class H) has towards implementing the predictor Q. We next model Neff. Specifically, we use the power-law scaling behavior of neural networks learning dynamics to compute the loss reduced in updates by such pipeline, identifying the number of updates an idealized Bayesian learner would have to make in order to reduce loss by this amount. Loss reduced under power-law scaling in updates Loss reduced by Bayesian learner in single update (cid:80)N n=1(L(n) L())δn LQ Neff := = ="
        },
        {
            "title": "1\nLQ",
            "content": "N (cid:88) n=1 nα δn (cid:90) 1 1α = LQ = γN 1α, 1 ˆnα δˆn 0 where LQ = LQ(STtrain (D)), ˆn = n/N and γ = A/LQ is constant that subsumes the loss of the predictor and the constant from our assumed form of power-law scaling, which depends on the random loss of network and effects of hyperparameters like batch-size and sequence lengths used to define the train data. Substituting Neff back into Eq. 7, we get our final model: η(N, D) = γN 1αL(D) K(D)β. (8) D.2 Converting from Posterior-Odds to Predictive Model At inference, the pretrained Transformer is shown sequence s, for which it makes next-token prediction. To simulate this process in our framework, we define Bayesian predictor, denoted hpred, as follows. hpred(s) := (cid:88) (QSTtrain , T, H)Q(sC) Q{M,G} (cid:88) Q{M,G} (cid:88) Q{M,G} = = = (QSTtrain , T, H) (cid:124) (cid:123)(cid:122) (cid:125) Pretraining Prior Q(s) (cid:124) (cid:123)(cid:122) (cid:125) Prediction (QSTtrain , T, H) Q{M,G} (QSTtrain , T, H) (cid:80) Q(s) exp (η(N, D)) 1 + exp (η(N, D)) (s) + 1 1 + exp (η(N, D)) G(s). Using σ(.) to denote the sigmoid function, we have the final form from Eq. 5 as follows. hpred(s) = σ(η(N, D)) (s) + (1 σ(η(N, D))) G(s). (9) (10) It is worth highlighting that η(N, D) essentially serves the role of free-energy in the Remark. analysis above. Use of free-energy to model an interpolation between two states of system is common theoretical framework used in physics to study systems that undergo transitions between disordered state to an ordered state: e.g., see in Landau theory, one considers interpolations between 24 free energy at high temperature and low temperature to model continuous (second-order) phase transitions [97]. Our overall theoretical model, and the phenomenology it elicits, are very similar to models from physics, parallel that we believe can be worth pursuing in future work, e.g., to uncover universality behavior beyond what we considered in this paper. 25 Two-Hypotheses Threshold: Minimum amount of training to enable the"
        },
        {
            "title": "Hierarchical Bayesian Model",
            "content": "One can reasonably expect our proposed Hierarchical Bayesian model to explain learning dynamics of in-context learning will not be predictive of Transformers behavior early-on in training, for otherwise we are saying even an untrained model perfectly generalizes. In actuality, the Transformer becomes amenable to approximation by our model after some minimal amount of training has occurred. To automatically calculate whether we have finished this regime of training, we calculate an optimal interpolation between the two predictors if they were capable of explaining the model behavior: specifically, we rely on the relative distance as an estimate of the optimal interpolation weighting, and use it as baseline to compare model outputs with. In particular, we compute the loss between this optimal interpolation baseline and our trained Transformer model, and if this loss is below threshold, we claim our theoretical model is applicable. We call this threshold the two-hypotheses threshold (see Fig. 6). To define the two-hypotheses threshold, we make the observation that while the loss between Transformer and interpolating predictor can be large to begin with, it very quickly reduces to small value. We can expect this latter regime is where our theoretical model is most likely to accurate at modeling the trained Transformers behavior. Motivated by this, we heuristically choose the two-hypotheses threshold as loss value 20% higher than minimum for Balls & Urns and Classification, and 10% higher than minimum for Linear Regression, on the scale defined from minimum to maximum loss (we find that stricter threshold is required for linear regression to surpass the early high-loss regime, given the larger variance in interpolation loss values). Figure 9: Two-Hypotheses Threshold. Defining an optimal interpolation between the memorizing and generalizing predictors towards minimizing the Euclidean distance to the trained Transformers predictions, we report the loss between this optimal interpolation and the Transformers predictions. We observe minimum amount of training is necessary for this loss to become sufficiently small such that our hierarchical Bayesian model, which implicitly assumes the Transformer can be functionally decomposed into the two predictors, will become applicable. The dotted lines demarcate this threshold, which we call the two-hypotheses threshold. Mean-squared error (MSE) in the figure above is normalized by dimension. KL in this figure indicates forward KL from the Transformers next token predictions to the interpolation of predictors."
        },
        {
            "title": "F Experimental Details",
            "content": "F.1 Training and Model Details Model. For all settings, we use the GPT-NeoX architecture sourced from Huggingface [56, 57]. While the number of layers / blocks in the model depend on the specific experimental setting (as reported below), we use only 1 attention head per layer and follow sequential residual stream architecture across all settings. Training. We use the Huggingface trainer with default parameters, changing only the learning rate, batch-size, total iterations, and warmup steps (reported below). Gradients are clipped to unitnorm. All models are trained on A100 GPUs, with maximum training budget reaching 2 days for all experiments encompassing the linear regression setting. We vary data-diversity from {22, 24, . . . , 212} across all settings. Settings-Specific Details. For our three core settings, we report results covering the following hyperparameters. We note that similar to Carroll et al. [30], as we vary task-diversity D, we include tasks from the lower diversity-values in the setting involving the larger onethis allows us to assess effects of increasing diversity on the learning of given task. Balls and Urns. Models of hidden dimension size 64 are trained for 100K steps, with no warmup steps, at constant learning rate of 5 104 and batch-size of 64. For our analysis, we derive experimental settings from combinations of task-dimensionality (equivalent to vocabulary-size), which varies in the set {8, 12, 16}; context length, which varies in the set {128, 256, 320}; and MLP expansion factor, which varies in the set {0.5, 4, 8}. We conduct separate experiment in which we attempt to elicit transience in higher task diversity settings (D {28, 29}). To do so on reasonable compute budget (2M, 10M steps, respectively), we, similar to prior work [24, 29], have to intervene on the training pipeline. However, unlike prior work that often relies on weight decay for this purpose, we use learning rate annealing and find it to be sufficient. Specifically, we rely on an inverse square root schedule for decaying the learning rate with number of dimensions, context length, and MLP expansion fixed to 8, 128, and 4 respectively. We train up to = 27 for 100K steps, and then train with = 28 for 2M steps, and with = 29 steps for 10M steps. Results of this experiment are shown in Fig. 6(c) and App. K. Linear Regression. Models of hidden dimension size 64 are trained for 100K steps, with 5K warmup steps, at constant learning rate of 5 104 and batch-size of 128. For our analysis, we derive experimental settings from combinations of task-dimensionality, which varies in the set {8, 12, 16}; context length, which varies in the set {16, 32, 64}; and MLP expansion factor, which varies in the set {0.5, 4, 8}. Like in Balls & Urns, we additionally train setting using learning rate annealing (setting parameters are task dimensionality of 8, Context length of 16, and MLP expansion factor of 4. We train once with warmup of 500, and once with warmup of 5000. See results in app. K). Classification. Models of hidden dimension size 64 are trained for 100K steps, with no warmup, at constant learning rate of 5 104 and batch-size of 64. For our analysis, we derive experimental settings from combinations of task-dimensionality, which varies in the set {8, 16}; context length, which varies in the set {128, 256, 384}; and MLP expansion factor, which varies in the set {0.5, 4, 8} for the 8 dimensions experiment, and is kept constant at 4 for the 16 dimensions experiment. F.2 Analysis Details Next, we specify broad details of our analysis pipeline. These notes clarify design decisions made in our evaluation and motivations underlying them. General model and predictor evaluation. For OOD evaluation of both the Transformer and our procedurally defined predictors, i.e., the memorizing predictor and generalizing predictor G, we draw 500 sequences from 500 unseen tasks (however, following still the same task distribution Ttrue). In comparison, ID evaluation involves 500 sequences from seen tasks. If task-diversity is less than 500, sequences from the same task may be seen multiple times. We note that in addition to the 27 standard ID evaluation, there is another method for ID evaluation in Classification that is slightly different. Following the popularly used pipeline of in-weights learning (IWL) first introduced by Chan et al. [20], one specifically ensures that copy of the test item does not appear in the context, thus making the use of an in-context copying solution ineffective, and testing memorization more explicitly. Fig. 1 details. In this figure, we use context length of 128, MLP expansion factor of 4, and task dimensionality of 8 for Balls & Urns. Linear Regression uses similar parameters, only with context length of 32, and Classification uses similar parameters, only with MLP expansion factor of 8. In all plots displaying task diversity effects, we hold = 100K. For classification, we show the IWL method for ID evaluation. In the transience figures, we use = 256 for Balls & Urns, = 64 for Linear Regression, and = 512 for Classification. For the comparison of relative distance maps, we show maps from the Balls & Urns setting with the parameters described above. Computing absolute distance between Transformer and predictors. Given an input, we use both the Transformer model and the procedurally defined predictors to make next-token predictions. Then, we compare distance between these predictions using either the symmetrized KL (average of forward and backward KL) for the Balls and Urns and Classification settings, or the mean-squared error (MSE) for linear regression. Computing relative distance between Transformer and predictors. Recall that relative distance, for given distance measure d(., .) between two functions or distributions (symmetrized KL-divergence or Euclidean distance), is defined as drel = (r+1)/2, where := d(h,G)d(h,M ) and h(.) denotes the Transformer model trained from scratch. This metric implicitly makes the assumption that in some function space, the model h(.) lies on line between the predictors and G. Correspondingly, for the scenarios this assumption is violated, the value of drel can go outside the range 01. This occurs relatively rarely, but nevertheless noticeably (e.g., if the model implements the optimal constant solution early on in training for linear regression). Accordingly, we clamp the metric between 01. d(G,M ) Minimum amount of training to enable the Hierarchical Bayesian Model. One can reasonably expect our proposed Hierarchical Bayesian model to explain learning dynamics of in-context learning will not be predictive of Transformers behavior early on in training, for otherwise, we are saying even an untrained model perfectly generalizes. In actuality, the Transformer becomes amenable to approximation by our model after some minimal amount of training has occurred. To automatically calculate whether we have finished this regime of training, we use the relative distance as an estimate of an optimal interpolation weight between the two predictors. We compute the loss between this optimal interpolation baseline and our trained Transformer model, and if this loss is below threshold, we claim our theoretical model is applicable now (see details on our choice of threshold in App. E) Fitting the Bayesian Model. We must perform the following three steps in order to fit our model. Approximating Kolmogorov complexity. Because true Kolmogorov complexity is not computable, we estimate an upper bound by compressing self-contained bundle for each predictor: (i) the cleaned Python source that instantiates the predictor, and (ii) any numpy arrays it needs at inference time (e.g., the full table of urn distributions for the memorizing baseline in Balls & Urns). We remove comments, docstrings, and extraneous whitespace from the source code. For arrays, we first apply simple delta-encoding (store successive differences) to expose additional structure. The pre-processed bundle is compressed with four strong, off-the-shelf algorithms: lzma (preset=9 PRESETEXTREME), bzip2 (level=9), brotli (quality=11, mode=TEXT), and zstd (level=22). We take the smallest compressed size (in bits) across the four algorithms as our estimate; this is standard practice for obtaining loose but practical upper bound. To keep estimates comparable, we exclude external libraries such as PyTorch from compression: all predictors call the same set of PyTorch primitives, so including them would add large constant offset without altering relative complexities. This choice does, however, ignore the fact that some primitives might be cognitively cheaper for Transformer to implement than othersan important caveat for future work. 28 Computation of average log likelihood per predictor. For every experimental condition we first compute the token-level log-likelihood that each predictor assigns to the in-distribution sequences (note that we use standard ID sequences for classification, not the IWL sequences). Because the irreducible error term cancels when models are compared, we use KL-based evaluations to the Balls & Urns and classification tasks, treating the linear-regression setting separately. To summarize performance, we need the mean log-likelihood per token, yet the empirical loss distribution is, at times, quite skewed: most tokens later in the context incur near-zero loss, whereas small fraction of early tokens produce large spikes. Therefore, naive arithmetic mean converges slowly and exhibits high variance. Thus, we instead use median of means, an estimator for the true mean that has better convergence under long-tailed distributions. How fitting is done. To fit the 3 free parameters of the Bayesian model, we minimize the mean KL divergence (or mean-squared error in the linear-regression setting) between the interpolated predictions and the Transformer outputs. Optimization is performed with scipy.optimize.minimize using the L-BFGS-B algorithm, capped at 1K iterations and 2K function evaluations, with gradient and function tolerances of 107. Exact gradients are supplied via PyTorchs automatic differentiation, ensuring stable convergence. For each task we fit on 80 % of the (N, D) configuration grid and reserve the remaining 20 % for held-out validation and diagnostic checks. Novel Predictions Analysis Details. To show sub-linear sample efficiency and sigmoidal curve in 1α, we fit parameterized logistic 1+exp(b(N 1αN0)) with free parameters a, b, N0 to each training run (constant value), via scipy curvefit function. We use the α value given by the Bayesian model. Curve fits are shown in Fig. 6(a). To compute the second derivative of the relative distance  (Fig. 6)  , we simply use parameters for the logistic fits described above (which provide very close fits, as can be seen in Fig. 6(a)), then compute the second derivative based on the form for the second derivative of parametrized logistic. To plot the vector field, we normalize both and directions by the larger value among the 90th percentile values for and . a"
        },
        {
            "title": "G Additional Details Regarding Settings and Predictors",
            "content": "We now give more detailed discussion of the different settings analyzed in this work: (i) Balls & Urns, (ii) Linear Regression, and (iii) Classification. We also provide details of how the memorizing and generalizing predictors are implemented for these settings. Broadly, as also visualized in Fig. 10, all settings involve learning of mixture of tasks Ttrain drawn from the true task distribution Ttrue. The number of tasks involved in the mixture is called its task diversity (denoted D). For all settings, we find models learn predictors of two types: memorizing predictor, which corresponds to the Bayesian posterior predictive distribution with discrete prior over seen tasks Ttrain, and generalizing predictor, which corresponds to the Bayesian posterior predictive distribution with prior over the true task distribution Ttrue. The precise forms of these predictors, as well as how sequences are assembled into training batches in each setting, are provided in the following sections. G.1 Balls and urns Memorizing Predictor. The memorizing predictor perform Bayesian averaging operation and requires computing weighted average of all urn distributions seen during training. The weight on each urn is derived from the likelihood of the current sequence of observations being generated by that urn. Formally, let wd denote the parameters for an urn {1, . . . , D}, with each element w(k) containing the probability for ball type {1, . . . , m} under urn D. Then, the probability of new ball being of type after seeing sequence is: p(ks) (cid:88) w(k) (cid:89) (w(k) )nk . wdTtrain k{1,...,m} With nk being the number of occurrences of ball of type in the sequence. Generalizing Predictor. Given that the true distribution is uniform Dirichlet, and that the Dirichlet distribution is conjugate prior of the categorical distribution (from which we draw our samples), the optimal way to estimate the probability of ball of particular type in sequence of length is: p(ks) = nk+1 , with nk being the number of occurrences of ball of type in the sequence. That is, 29 Figure 10: General Abstraction Capturing our Experimental Settings and their Predictors. Each setting involves mixture of parameterized functions (called task), with functions (the task diversity\"). Task consist of predicting the next element in sequence, and vary based on whether models are trained in standard auto-regressive fashion (like Balls & Urns) or whether they are only trained to predict some elements in the sequence (only function outputs in Linear Regression, and only the last label in Classification). Across settings, the solutions learned by Transformers can be characterized as memorizing predictors or generalizing predictors. memorizing predictor is defined as the Bayesian posterior predictive distribution with Ttrain, the distribution of seen tasks, as its prior. generalizing predictor is defined as the Bayesian posterior predictive with the true task distribution Ttrue as its prior. Figure 11: Visualizing the setup for Balls and Urns. Each task involves an urn that outputs ball of specific type every time it is sampled from. The task then involves seeing samples from an urn, concatenated to form sequence. memorizing predictor for this setting involves computing the sequence-level unigram statistics, i.e., the counts for each ball type, and comparing them with distributions from urns seen during training. Meanwhile, generalizing predictor simply assumes the distribution of balls follows uniform Dirichlet prior, thus predicting simply based on computing the unigram statistics from the sequence and adding 1 pseudo-count for each ball type. Thus, this predictor generalizes to novel urns not seen by the model during training. the optimal strategy for the true distribution is simply computing count for each type, adding 1 pseudo-count, and dividing by the sequence length. G.2 Linear regression Additional Details Not Provided in Main Text. To maintain constant signal-to-noise ratio across tasks with different dimensionality m, we set ϵi (0, σ2), with σ2 = 256 . Generalizing Predictor. The generalizing predictor in this case simply performs ridge regression. Given = (x C1) and = (y1, ..., yC1), the weight estimate is after seeing 1 examples 1 , ..., is: ˆw(C) = (xx + σ2Im)1xy Memorizing Predictor. The memorizing predictor in this case performs inference by Bayesian averaging: weighted average across all w(d)s seen in the training distribution, with weights 30 Figure 12: Visualizing the setup for Linear Regression. Each task involves linear regression problem, defined by parameters w, that outputs pair (x, y), where = wx + ϵ is noisy linear transformation of the vector x. The task then involves seeing sequence of such pairs, concatenated to form sequence. memorizing predictor for this setting involves computing the likelihood of the pairs seen in context under the parameters of each task seen during training, using this result to compute posterior over said tasks and posterior-weighted average with discrete prior over seen tasks. Meanwhile, the generalizing predictor is merely the ridge regression operation, which is equivalent to performing Bayesian average operation assuming continuous Gaussian prior. Correspondingly, this predictor generalizes to novel regression tasks that were not seen by the model during training. determined by the likelihood that the sequence was generated by the specific w(t). After seeing 1 examples, the weight estimate is: ˆw(C) = (cid:88) wdTtrain G.3 Classification exp( 1 2σ2 (cid:80)C1 c=1 (yc (cid:80)C1 xc)2) xc)2) c=1 (yc exp( 1 2σ2 wd wd Ttrain (cid:80) We use the classification setting with the formulation from [29] as well as inspiration from the noisy class centroids introduced by [28]. As Nguyen and Reddy [29] have shown, their simplified setting captures the phenomenology of other classification settings proposed by Chan et al. [20], Reddy [28]. For simplicity, we include only binary labels in our version. Additional Details Not Provided in Main Text. When presented in context, items are noised and presented as = w+σϵ 1+σ2 . We use within-class variance of σ2 = 0.5 in all settings, and ϵ (0, Im/m) is sampled separately for each item in the context. Memorizing Predictor. The memorizing predictor in this setting performs inference by computing posterior-weighted average over item-label pairs seen in the training distribution Ttrain, i.e., w1 l1, . . . , wD lD. The form for noisy item used for defining the input sequence is = w+σϵ 1+σ2 for some Ttrain. Thus, since ϵ has covariance Im/m, we can write noisy item 1+σ2 Im/m). The sampled from given will be distributed as: ( = wd) ( probability of the query label being 1 can then be defined as follows: 1+σ2 w, σ2 1 p(1s) (cid:88) p( wquerywd) p(1wd) wdTtrain (cid:32) (cid:88) 2σ2 (1 + σ2) where we disregard constants outside the exp term. wdTtrain exp (cid:13) (cid:13) wquery (cid:13) (cid:13) 1 1 + σ2 wd 2(cid:33) (cid:13) (cid:13) (cid:13) (cid:13) 1(ld = 1), Generalizing Predictor. The generalizing predictor in this setting performs inference by computing posterior-weighted average over item-label pairs in the context. Specifically, note that we define 31 Figure 13: Visualizing the setup for Classification. Each task involves noisy item-label pairs l, and ends with noisy query item wquery which comes from the same true item as one of the items in the sequence. Items are noised via = w+σϵ 1+σ2 , with ϵ (0, Im/m) sampled from the same distribution as the true item w. memorizing predictor knows the true items in the training distribution, computes the likelihood that the noisy query item comes from each true item, and accordingly computes posterior-weighted average using the labels for each true item. Note that this predictor completely ignores the context, and hence was described as an in-weights learning solution in previous works [24, 28]. In contrast, the generalizing predictor implements noisy copy operation. It estimates the likelihood that the query head and each item seen in context come from the same true item. Then, it predicts via posterior-weighted average according to the labels of each item seen in context. Therefore, this predictor works for OOD settings containing novel items that were not previously seen during training. sequences using noised versions of task vectors Ttrue. Importantly, we allow sampling with replacement, i.e., the same task can be used to define multiple item-label pairs. This can be thought of as biased sampling process, instead of the random sampling one, whereby task seen in-context has finite odds of being seen again in the sequence than random one. Accordingly, the prior will collapse onto just the seen item pairs (as it will be infinitesimally small for items sampled randomly). We thus merely need to compute the joint probability of the query item given the items seen in context, leading to the following form: p(1s) (cid:88) wdContext (cid:32) exp (1 + σ2)2 2σ2 (2 + σ2) (cid:13) (cid:13) wquery (cid:13) (cid:13) wd 1 + σ2 2(cid:33) (cid:13) (cid:13) (cid:13) (cid:13) 1(ld = 1)."
        },
        {
            "title": "H Main Results Across All Settings",
            "content": "In the following sections, we provide the results reported in the main paper across all settings and experiments. H.1 Task Diversity Effects We find task diversity effects [25] to be very robust across settings and experimental conditions. More specifically, we consistently find that increasing task diversity yields transition in Transformer behavior from behaving like memorizing predictor to behaving like generalizing predictor. In the following, we present evidence of this phenomenon for ID sequences. See results in following pages. 33 H.1.1 Balls & Urns Figure 14: Task Diversity Effects Across Balls & Urns Conditions. Red dashed line indicates the memorizing solution , blue dashed line indicates the generalizing solution G, and black solid line indicates Transformer behavior at the end of training (100K steps). 34 H.1.2 Linear Regression Figure 15: Task Diversity Effects Across Linear Regression Conditions. Red dashed line indicates the memorizing solution , blue dashed line indicates the generalizing solution G, and black solid line indicates Transformer behavior at the end of training (100K steps). 35 H.1.3 Classification Figure 16: Task Diversity Effects Across Classification Conditions. Red dashed line indicates the memorizing solution , blue dashed line indicates the generalizing solution G, and black solid line indicates Transformer behavior at the end of training (100K steps). IWL evaluation presented. 36 H.2 Transience Across settings and conditions, we also find the phenomenon of transience [24] to be consistent in moderate task diversity values. More specifically, in moderate task diversity values, we see the Transformer approach the generalizing solution in terms of OOD performance early in training, only to eventually begin memorizing and worsen in OOD performance. In the figures below, we show OOD performance of Transformers trained in different task diversity conditions, with low task diversity values showing immediate memorization, moderate task diversity values showing transience, and high task diversity values often continuing to generalize well throughout training. See results in following pages. 37 H.2.1 Balls & Urns Figure 17: Transience Across Balls & Urns Conditions. OOD performance presented. Blue Dashed line indicates OOD performance of generalizing solution G. 38 H.2.2 Linear Regression Figure 18: Transience Across Linear Regression Conditions. OOD performance presented. Blue Dashed line indicates OOD performance of generalizing solution G. 39 H.2.3 Classification Figure 19: Transience Across Classification Conditions. OOD performance presented. Blue Dashed line indicates OOD performance of generalizing solution G. Note that in the case of classification, it is often the case that only one or two task diversity conditions show transient generalization, as can be seen more clearly from the absolute distance maps in the next section. 40 H.3 Absolute Distance from Predictors We find that across settings and conditions, Transformers primarily learn and transition between behaving like two predictors: generalizing solution G, which consists of the Bayesian posterior predictive distribution over the true task distribution Ttrue and memorizing solution which consists of the Bayesian posterior predictive distribution over the training task distribution Ttrain. In the figures below, we display the absolute distance from each of these predictors, as well the relative distance in the background (ranging from red indicating closeness to to blue indicating closeness to G). See results in following pages. 41 H.3.1 Balls & Urns Figure 20: Absolute and Relative Distance from Predictors Across Balls & Urns Conditions. Distance from the generalizing solution shown in the dashed black line, while distance from the memorizing solution is shown in the solid line. KL indicates symmetrized KL divergence (average of forward and backward KL). 42 H.3.2 Linear Regression Figure 21: Absolute and Relative Distance from Predictors Across Linear Regression Conditions. Distance from the generalizing solution shown in the dashed black line, while distance from the memorizing solution is shown in the solid line. 43 H.3.3 Classification Figure 22: Absolute and Relative Distance from Predictors Across Classification Conditions. Distance from the generalizing solution shown in the dashed black line, while distance from the memorizing solution is shown in the solid line. KL indicates symmetrized KL divergence (average of forward and backward KL). 44 H.4 Model Predictions Across settings and training conditions, we find that our models predictions consistently perform well both in estimating the next-token prediction behavior of the Transformer, as well as capturing its change in generalization behavior across conditions, as displayed by the relative distance from predictors and . We conduct thorough stress-testing of our model across 3 settings and 72 (N, D) maps, each containing 11 different training runs, and find that our model consistently performs well across maps, thus providing robust evidence for the predictive and explanatory power of our account. See results in following pages. 45 H.4.1 Balls & Urns Figure 23: Bayesian Model Predictions Across Balls & Urns Conditions. Red indicates closeness to memorizing predictor , while blue indicates closeness to generalizing predictor G. Shown is comparison between the posterior probability of the memorizing solution given by our Bayesian model (left) and the relative distance from the Transformer (top right), as well as heatmaps indicating similarity with Transformer next-token predictions (bottom right). Max color bar value is determined by the performance of baseline predictor that always outputs the mean of the distribution Ttrue. 46 Figure 24: Bayesian Model Predictions Across Balls & Urns Conditions with Varying MLP Expansion Factors. Red indicates closeness to memorizing predictor , while blue indicates closeness to generalizing predictor G. Shown is comparison between the posterior probability of the memorizing solution given by our Bayesian model (left) and the relative distance from the Transformer (top right), as well as heatmaps indicating similarity with Transformer next-token predictions (bottom right). Context length is 128, task dimensionality is 8, and hidden size is 64 in all conditions shown. MLP width is given by hidden size times MLP expansion factor. Max color bar value is determined by the performance of baseline predictor that always outputs the mean of the distribution Ttrue. 47 H.4.2 Linear Regression Figure 25: Bayesian Model Predictions Across Linear Regression Conditions. Red indicates closeness to memorizing predictor , while blue indicates closeness to generalizing predictor G. Shown is comparison between the posterior probability of the memorizing solution given by our Bayesian model (left) and the relative distance from the Transformer (top right), as well as heatmaps indicating similarity with Transformer next-token predictions (bottom right). H.4.3 Classification Figure 26: Bayesian Model Predictions Across Classification Conditions. Red indicates closeness to memorizing predictor , while blue indicates closeness to generalizing predictor G. Shown is comparison between the posterior probability of the memorizing solution given by our Bayesian model (left) and the relative distance from the Transformer (top right), as well as heatmaps indicating similarity with Transformer next-token predictions (bottom right). Max color bar value is determined by the performance of baseline predictor that always outputs the mean of the distribution Ttrue. The conditions where task dimensionality equals 16 in this setting (bottom row) reveal limitation of our complexity measure: since the memorizing and generalizing predictors are very close in performance in low task diversities for these conditions, the loss term does not strongly bias the Transformer towards the memorizing predictor. However, the Transformer, is very close to the memorizing predictor for low task diversities, which would indicate according to our framework that memorizing few items is substantially simpler than implementing copy operation. However, this is not captured by our complexity measure, since the compressed size of the code for the memorizing and generalizing predictors is roughly the same, thus we are unable to capture the bias toward the memorizing predictor in low task diversity settings. To overcome this, in these 3 conditions only, we heuristically multiply the bit size of the code for the generalizing predictor by 5, and with that fix, we find good performance (though as can be seen, the model still under-weights the memorizing solution for some low task diversity conditions)."
        },
        {
            "title": "I Functional Form Ablations",
            "content": "Our functional form for the log posterior odds η consists of 3 free parameters, α, determining sublinear sample efficiency, β, power law on the estimated Kolmogorov complexity K, and γ, coefficient for the loss term. We find that each of these free parameters are necessary for the success of our model, since removing any of them results in worsening of the models ability to capture the phenomenology of ICL. Figure 27: Power laws over complexity measure and sample efficiency are necessarily for explaining ICL phenomenology. By ablating our functional form, we see that the free parameters α, β, γ are required for the performance of the model. In particular, the simplicity bias derived from without β term is much too sharp and over-penalizes complexity compared to the Transformer. Additionally, memorization proceeds much too rapidly without the α term, pointing toward the necessity of assuming sub-linear sample efficiency for capturing Transformer training dynamics. 50 Memorization Continues to Increase After Task Diversity Threshold Refutation of Raventós et al. [25]s Claim In Fig. 28, we show refutation of the claim made by Raventós et al. [25], who claimed that after the task diversity threshold is reached, the Transformer will only continue to get closer to the generalizing solution throughout training, regardless of how long one trains. In making this claim, Raventós et al. [25] focused only on the absolute distance between the Transformer and the generalizing solution. However, when we considering relative distance measure, we can show this claim to be false (see right side of Fig. 28): Even in conditions in which the task diversity threshold was reached and generalization is sustained throughout reasonable amount of training (100K steps), we see that by absolute distance, the Transformer not only gets closer to the generalizing solution during training, it also continues to get closer to the memorizing solution (left side of Fig. 28). It seems that the rate at which the Transformer nears the memorizing solution is greater than that at which it nears the generalizing solution in task diversity settings such as = 512 or = 256, which is why the relative distance continues to grow even despite the transformer nearing the generalizing solution. Note also, that in settings that clearly show transience, e.g., = 64, the distance from the generalizing solution shrinks until certain critical point in which it begins to grow. It is likely that this point can be reached in all task diversity settings examined given the growth trend of the relative distance. However, it may require very long training process to reach that point. Figure 28: Relative distance continues to rise throughout training, even after the task diversity threshold. The figure displays relative distance, as well as absolute distance from the memorizing and generalizing solutions, for the linear regression setting with context length of 32, MLP expansion factor of 4, and 8 dimensions. 51 Learning Rate Annealing Can Improve Adherence to Bayes-Optimal"
        },
        {
            "title": "Trajectories",
            "content": "In our main experimental settings, in which we train with constant learning rate, we find that relative distance trajectories follow sigmoidal growth pattern with respect to 1α (see Fig. 6 and Fig. 29(a) top and bottom right). However, in contrast with our theorys predicted sigmoidal curves which plateau at 1 (i.e., some amount of training would eventually yield full adherence to the memorizing solution, when it has lower loss than the generalizing solution), this does not seem to be the case with the trajectories displayed by Transformers, which appear to plateau early (see Fig. 29(a) top right = 256 for clear example). Indeed, fitting parameterized logistic curves to these trajectories yields plateau values different from 1. To explain this, we turn to foundational work from Geman and Geman [98], who showed that slow (logarithmic) temperature cooling schedule for Gibbs sampling substantially increases the likelihood of convergence to the global minimum (MAP estimate). Drawing very rough parallel to the case of deep learning, it is reasonable to assume that learning rate annealing schedule of some form is required to converge to the MAP estimate (which is the memorizing solution, in cases where it has lower loss than the generalizing solution). To test this, we repeated experiments in the Balls & Urns and Linear Regression settings and used warm-up and inverse-squared learning rate decay. Surprisingly, we indeed find that adding learning rate annealing can increase adherence to Bayes-optimal trajectories (Fig. 29(a)). However, we also find this effect is highly sensitive to training conditions: training for longer, even in conditions that yield the effect for smaller number of training steps, can lead to plateau (Fig. 29(b), top), and slight changes in training conditions, in this case number of warm-up steps, can substantially reduce the effect (Fig. 29(b), bottom). It should also be noted that in this case, adherence to the Bayes-optimal trajectory is actually negative for generalization, since it means the model will converge to the memorizing solution quicker. However, this is not necessarily the case in more realistic training regimes, where the ability to overcome simplicity bias and adopt more complex solutions is likely beneficial. Figure 29: Learning Rate Annealing Can Increase Adherence to Bayes-Optimal Trajectories. (a) Balls & Urns setting uses context length of 128, MLP width of 256, and 8 dimensions. Linear regression uses similar variables except context length of 16. (b) Effect is highly sensitive to training conditions: e.g., Training in the Linear regression setting with 5000 warmup steps failed, but succeeds with 500 warmup steps (the number used for the experiment in panel (a))."
        }
    ],
    "affiliations": [
        "CBS-NTT Program in Physics of Intelligence, Harvard University",
        "Department of Computer Science, Stanford University",
        "Department of Psychology, Stanford University",
        "Joseph Henry Laboratories of Physics, Princeton University"
    ]
}