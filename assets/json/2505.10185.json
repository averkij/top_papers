{
    "paper_title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think",
    "authors": [
        "Seongyun Lee",
        "Seungone Kim",
        "Minju Seo",
        "Yongrae Jo",
        "Dongyoung Go",
        "Hyeonbin Hwang",
        "Jinho Park",
        "Xiang Yue",
        "Sean Welleck",
        "Graham Neubig",
        "Moontae Lee",
        "Minjoon Seo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of modern large language models, but our understanding of the reasoning strategies underlying these capabilities remains limited. While some prior works have attempted to categorize CoTs using predefined strategy types, such approaches are constrained by human intuition and fail to capture the full diversity of model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up framework for analyzing and steering model reasoning. Our method automatically extracts diverse reasoning criteria from model-generated CoTs, embeds them into a semantic space, clusters them into representative categories, and derives contrastive rubrics to interpret reasoning behavior. Human evaluations show that this framework produces more interpretable and comprehensive analyses than existing methods. Moreover, we demonstrate that this understanding enables performance gains: we can predict which strategy a model is likely to use and guide it toward more effective alternatives. Finally, we provide practical insights, such as that training data format (e.g., free-form vs. multiple-choice) has a far greater impact on reasoning behavior than data domain, underscoring the importance of format-aware model design."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 5 8 1 0 1 . 5 0 5 2 : r The COT ENCYCLOPEDIA: Analyzing, Predicting, and Controlling how Reasoning Model will Think Seongyun Lee1,3 Seungone Kim2 Minju Seo1 Yongrae Jo Dongyoung Go4,5 Hyeonbin Hwang1 Jinho Park1 Xiang Yue2 Sean Welleck2 Graham Neubig2 Moontae Lee3 Minjoon Seo1 KAIST AI1 Carnegie Mellon University LG AI Research3 NAVER Search US4 Cornell University5 {seongyun, minjoon}@kaist.ac.kr seungone@cmu.edu"
        },
        {
            "title": "Abstract",
            "content": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of modern large language models, but our understanding of the reasoning strategies underlying these capabilities remains limited. While some prior works have attempted to categorize CoTs using predefined strategy types, such approaches are constrained by human intuition and fail to capture the full diversity of model behaviors. In this work, we introduce the COT ENCYCLOPEDIA, bottom-up framework for analyzing and steering model reasoning. Our method automatically extracts diverse reasoning criteria from model-generated CoTs, embeds them into semantic space, clusters them into representative categories, and derives contrastive rubrics to interpret reasoning behavior. Human evaluations show that this framework produces more interpretable and comprehensive analyses than existing methods. Moreover, we demonstrate that this understanding enables performance gains: we can predict which strategy model is likely to use and guide it toward more effective alternatives. Finally, we provide practical insights, such as that training data format (e.g., free-form vs. multiple-choice) has far greater impact on reasoning behavior than data domain, underscoring the importance of format-aware model design."
        },
        {
            "title": "Introduction",
            "content": "Chain-of-thought (CoT) prompting [22] is an effective inference-time method for eliciting reasoning in large language models (LLMs) by generating intermediate reasoning steps before producing final answer. While CoT reasoning has led to impressive performance gainsespecially when extended to long chains involving multiple reasoning strategies [6, 8, 15, 27]our understanding of the specific strategies that models tend to employ remains limited. Key questions remain underexplored: What varieties of reasoning strategies do models use? How do these strategies differ across models and tasks? Can they be systematically controlled to improve performance? Prior efforts to analyze long CoT reasoning have primarily followed top-down approach [24, 5, 6], where researchers define fixed set of strategy typessuch as backtracking or subgoal settingand use language models to detect their presence in generated outputs. While such approaches offer interpretability within predefined categories, they inherently constrain analysis to what is already Equal contribution. Seongyun conducted this work during an internship at LG AI Research. Preprint. Under review. Figure 1: Comparison between conventional reasoning analysis and the CoT Encyclopedia. Traditional methods use fixed criteria to identify strategies but offer limited guidance for improving reasoning. The CoT Encyclopedia takes bottom-up approach, uncovering diverse, task-specific strategies and enabling flexible analysis and actionable insights to enhance model performance. known. More recent studies have begun exploring clustering-based methods [1, 25, 4], but these focus largely on short or medium-length CoTs and do not fully address the complexities of longer, multi-strategy reasoning traces. In this paper, we introduce the COT ENCYCLOPEDIA, method to systematically analyze and control long CoTs that involve multiple, intertwined reasoning strategies. We do so through bottom-up, clustering-based framework designed to capture, interpret, and steer diverse reasoning strategies at scale. Rather than relying on predefined categories, our approach begins by prompting language model to produce free-form explanations of the reasoning strategies used in its own responses. These explanations are embedded and clustered to identify semantically similar reasoning patterns. For each resulting cluster, we generate contrastive rubrics (e.g., Inductive vs. Deductive, Directive vs. Non-Directive) through second round of prompting, enabling precise characterization of reasoning dimensions. Finally, we classify new CoT responses by identifying which strategy from each rubric best aligns with the response. Human evaluation supports the quality of this pipeline: while top-down strategy labels from previous work [5] are judged as reasonable in only 51% of cases, our bottom-up method achieves perceived reasonableness of 9297% across stages. See Figure 2 for an overview. Beyond interpretability, the COT ENCYCLOPEDIA offers two practical benefits. First, it can improve reasoning models performance by guiding it to adopt more effective strategies. This is achieved by (1) training classifier to predict which strategy model would use for given input, (2) applying Bayes rule to estimate the likelihood of correctness when using each strategy, and (3) prompting the model to follow the most promising one. Across five benchmarks, we observe performance improvements of 2.58.3% in three different reasoning models. To our knowledge, this is the first demonstration that controlling models high-level reasoning strategies can directly enhance accuracy. We also find that such control is possible because models tend to use similar strategies for similar questionsa claim supported by correlation (R2 = 0.405) between question similarity and strategy similarity across three benchmarks. Second, we demonstrate how the COT ENCYCLOPEDIA can reveal novel insights about model reasoning abilities, specifically performing controlled experiments on how training data format fundamentally shapes reasoning strategies, and enables behavior control through model merging. Our analysis shows that the domain of the training data (e.g., math vs. commonsense) has little effect 2 on reasoning patterns, with Cohens consistently below 0.2. In contrast, the formatmultiplechoice (MC) versus free-form (FF)has much larger effect, with effect sizes up to 1.5. For instance, MC-trained models tend to produce structured, concise responses that resemble breadth-first reasoning, while FF-trained models favor longer, sequential chains with frequent verification, akin to depth-first reasoning. By linearly interpolating weights between MCand FF-trained models, we generate models that smoothly transition in strategy, demonstrating controllability without fine-tuning. These findings highlight the COT ENCYCLOPEDIA not only as diagnostic tool, but also as practical foundation for shaping reasoning behaviors to suit task-specific needs."
        },
        {
            "title": "2.1 Scaling test-time compute through long chain-of-thought generation",
            "content": "Recent methods improve language model reasoning by scaling test-time computation and structuring thought processes. OpenAI2 propose recursive CoT reasoning, where difficult queries trigger additional inference steps. Adaptive multi-pass inference [19, 12] enables iterative refinement without extra training. Structured CoT generation has progressed via self-consistency sampling [21], Tree-ofThoughts [26], and least-to-most prompting [29], which guide models through diverse, tree-based, or decomposed reasoning paths. These approaches deepen reasoning and offer effective strategies for complex tasks."
        },
        {
            "title": "2.2 Analyzing reasoning strategies of models",
            "content": "Recent work has made significant progress in understanding LLM reasoning. Think patterns [24] reveal recurring structures linked to accurate outcomes, while cognitive behavior analysis [5] draws connections to human psychology. Guo et al. [6] examine \"aha moments\" of sudden insight during multi-step reasoning, and Marjanovic et al. [13] introduce structured <think> mechanism to enhance reasoning. Strategic example selection improves in-context learning [3], and targeted data generation addresses specific reasoning failures [28]. Together, these studies emphasize growing focus on analyzing and improving reasoning through structured and cognitively inspired methods."
        },
        {
            "title": "3.1 A framework for taxonomizing reasoning strategies",
            "content": "Language models utilizing LongCoT enable test-time scaling, effectively addressing complex reasoning problems across diverse domains. Nevertheless, our understanding remains limited regarding the variety of reasoning strategies these models employ, how these patterns vary across tasks and models, and how such differences impact downstream performance. Prior work [5] offered valuable insights by defining four reasoning behaviorsverification, backtracking, subgoal setting, and backward chainingbut such predefined categories struggle to capture the full diversity of emerging or model-specific strategies. To address this gap, we introduce COT ENCYCLOPEDIA, five-stage framework for identifying, organizing, and analyzing diverse reasoning strategies in CoT outputs. Unlike prior top-down approaches, COT ENCYCLOPEDIA derives reasoning dimensions in bottom-up, data-driven manner using large language models. As shown in Figure 2, the framework systematically extracts classification criteria, compresses them via semantic clustering, and generates human-interpretable reports on model reasoning behaviors. Step 1: Classification Criteria Identification. Given CoT outputs = {(xi, yi)}n i=1, we extract diverse classification criteria = {c1, . . . , cN } via LLM-assisted brainstorming. Each criterion cj is defined with pair of contrastive reasoning strategies (sA ), expressed as natural language sentences. For example: , sB cj = Analytical Perspective (cid:26)sA = Top-down sB = Bottom-up 2https://openai.com/index/learning-to-reason-with-llms/ 3 Figure 2: Overview of the COT ENCYCLOPEDIA. The framework constructs taxonomy of reasoning strategies through five key stages: (1) Classification Criteria Identification diverse reasoning criteria are identified from model-generated CoTs; (2) Classification Criteria Embedding these criteria are converted into semantic embeddings; (3) Criteria Compression via Hierarchical Clustering semantically similar criteria are clustered to form distinct representative categories; (4) Rubric Generation contrastive rubrics are created to describe and distinguish opposing reasoning patterns within each criterion; (5) Analysis Report Generation model responses are classified using the rubrics, producing comprehensive reports that interpret their reasoning behaviors. The framework also supports practical use cases such as reasoning pattern analysis and optimal strategy control for performance improvement. Step 2: Classification Criteria Embedding. Each triplet (cj, sA string by concatenation and embedded using embedding model: , sB ) is converted to single input ej = E(concat(cj, sA , sB )) Rd. This results in matrix RN d. Step 3: Criteria Compression via Clustering. To reduce redundancy, we apply hierarchical agglomerative clustering [16] to using cosine distance. We obtain clusters (k ): = {G1, . . . , Gk}, Gℓ C. Each cluster Gℓ is represented by its medoid criterion yielding the compressed set = {c 1, . . . , ℓ (not the centroid, to preserve interpretability), k}. This set is used in all subsequent analysis steps. Step 4: Rubric Generation. For each criterion with detailed descriptions of both strategies and guidance for binary classification. For example: ℓ , we use LLM to generate rubric Rℓ = (sA ℓ , sB ℓ ), Rℓ = (Clearly bottom-up approach ..., Clearly top-down approach ...) Step 5: Pattern Analysis Report Generation. Each response yi is classified under each rubric via prompting LLM with yes/no question: zi,ℓ = (cid:26)1 0 if LLM predicts alignment with sA ℓ , if LLM predicts alignment with sB ℓ . This produces binary matrix {0, 1}nk, where each row summarizes the reasoning pattern of CoT response. We then synthesize natural language report using LLM, which selects and composes rubric-specific templates to describe the reasoning pattern of each response. For example: The response shows bottom-up reasoning style, combining data-driven verification ... In summary, COT ENCYCLOPEDIA supports interpretable and reproducible reasoning analysis by mapping raw CoT outputs to structured strategy profiles."
        },
        {
            "title": "3.2 COT ENCYCLOPEDIA enables sharper reasoning strategy classification",
            "content": "To evaluate the effectiveness of the COT ENCYCLOPEDIAs classification criteria, we analyze responses from DeepSeek-R1-Distill-Qwen-32B, s1.1-32B, and QwQ-32B on GPQA-Diamond, MMLU-Redux, and MATH-500. Using LLMo, we extract 4,057 contrasting reasoning criteria, each representing pair of opposing strategies. We embed these using embedding model and apply hierarchical clustering to group semantically similar criteria, following Gandhi et al. [5]. The resulting taxonomy defines six reasoning dimensions: Analytical Perspective, Scope of Approach, Reasoning Type, Idea Development, Verification Focus, and Clarification Approach. Each model response is labeled as Pattern or under each criterion, and we compute the proportion of Pattern as Pattern A+Pattern to compare trends across models. As baseline, we also assess the presence of four predefined cognitive behaviorsverification, backtracking, subgoal setting, and backward chainingwithin the same responses [5]. For both sets, we apply chi-squared tests for statistical significance and compute Cohens for effect sizes."
        },
        {
            "title": "Pattern B",
            "content": "As shown in Table 3, distributions of cognitive behaviors differ minimally across models (p > 0.05, 0.1), suggesting limited sensitivity. In contrast, the COT ENCYCLOPEDIA criteria reveal more substantial differences  (Table 9)  , with many significant p-values and effect sizes reaching up to 0.4. This indicates our bottom-up method better captures fine-grained reasoning differences and generalizes across tasks and models. We conduct human evaluation to validate alignment with human judgment. From model outputs, we sample 100 responses and assign 25 to each of four annotators. For each response, annotators answer four binary questions, assessing: (1) plausibility of fine-grained criteria, (2) coherence of high-level grouping, (3) relevance of predefined-criteria analysis, and (4) relevance of high-level-criteria analysis. As shown in Figure 3, annotators judge both fineand high-level criteria as reasonable for most cases and find our framework produces more coherent analyses than the baseline. These results suggest our method not only captures fine-grained patterns but also presents explanations aligned with human expectations. Implementation details are in Appendix A, with qualitative artifact analyses in Appendix F.1. Figure 3: Human Evaluation Results for CoT Encyclopedia. Human annotators found the generated criteria plausible, their mapping to high-level dimensions sensible, and the overall analysis reasonable."
        },
        {
            "title": "3.3 COT ENCYCLOPEDIA enables adaptive analysis across diverse tasks",
            "content": "In addition to the three benchmarks evaluating model helpfulness, we analyze reasoning strategies in model responses for XSTest [17] and WildGuard [7] that assess harmlessness and Arena-Hard [11] that measures instruction following capability using COT ENCYCLOPEDIA. As shown in Figure 14, different classification criteria emerge across benchmarks. Notably, instruction following benchmarks introduce new User Understanding criterion due to the need to accurately interpret user intent. Safety benchmarks feature ethical elements absent in problem-solving benchmarks, such as Safety Precedence (preventive vs. risk-engaging), and Content Handling (censorship vs. open discussion). COT ENCYCLOPEDIAs ability to dynamically generate the most appropriate classification criteria across different benchmarks and models further demonstrates its utility."
        },
        {
            "title": "4 Enhancing model helpfulness and safety via optimal reasoning control",
            "content": "Building on the findings of the previous section, an important question arises: Can we identify optimal reasoning strategies that positively impact both model helpfulness and harmlessness? If so, can we effectively steer models toward these patterns to enhance their overall performance across different types of benchmarks? 5 Figure 4: Impact of Pattern-Based Instructions on Model Performance. Five approaches are compared: not instructed, unoptimal instructions, random patterns, dataset-wide optimal patterns, and question-specific optimal patterns. Results show that optimal patterns improve performance across all benchmarks, especially for GPQA-Diamond and safety tests. Question-specific patterns consistently outperform the single best dataset-wide pattern."
        },
        {
            "title": "4.1 Exploring optimal reasoning strategies for helpfulness and harmlessness",
            "content": "To analyze how reasoning strategies affect model helpfulness and harmlessness, we evaluate model responses on GPQA-Diamond, MMLU-Redux, and MATH-500 for helpfulness, and on XSTest and WildGuard for harmlessness. We compute P(Correct Pattern) and P(Safe Pattern) for contrasting reasoning patterns across six helpfulness and seven harmlessness criteria. These values are averaged over three models to identify patterns associated with higher accuracy and safety. As shown in Figures 9 and 10, certain reasoning strategies consistently lead to better performance. This enables clear distinction between optimal and suboptimal reasoning patterns. Using this insight, we assess how performance changes when models are explicitly instructed to follow desired strategies. To isolate the effect of strategy control, we focus on responses that were initially incorrect or unsafe. We compare four settings: (1) no instruction, (2) instruction with suboptimal patterns, (3) instruction with randomly selected patterns, and (4) instruction with optimal patterns (optimal-dataset). Figure 4 shows that guiding models with optimal strategies improves both accuracy and safety across all benchmarks. For example, GPQA-Diamond accuracy increases from 72.7% to 79.1%, while XSTest and WildGuard safety scores improve from 91.1% to 94.0% and from 89.1% to 92.9%, respectively. These results include both previously correct and newly corrected samples. For analysis focused solely on newly corrected responses, see Figure 11. Overall, these findings confirm that optimal reasoning strategies exist and can be leveraged to enhance downstream performance. Further breakdowns on safety benchmarks (Figure 12) reveal that patterns encouraging malicious intent or prioritizing technical over moral reasoning sharply reduce safety, indicating jailbreaking behavior. This underscores the need for more nuanced safety evaluations. While current approaches often rely on binary labels (safe vs. unsafe), our results highlight the value of fine-grained analyses, such as those enabled by the COT ENCYCLOPEDIA, for improving content moderation and response quality."
        },
        {
            "title": "4.2 Similar inputs, Similar thoughts: How models approach related problems",
            "content": "We have shown that each dataset typically has generally optimal reasoning strategy, indicating opportunities to enhance model performance. However, even within single dataset, different questions may require distinct optimal reasoning strategies. natural question arises: can we predict the optimal reasoning strategy for each individual question? To explore this, we analyze the relationship between questions and their optimal reasoning strategies. Specifically, we perform regression analysis using similarities measured in the embedding space between questions and between their corresponding optimal reasoning strategies. Our analysis utilizes correct responses generated by three models across three problem-solving benchmarks, previously discussed in Sections 3 and 4.1. Figure 5 illustrates that higher similarity between questions corresponds to greater similarity between their reasoning strategies, suggesting that models adopt similar strategies for similar problems. Conversely, lower question similarity is associated with higher variance in reasoning strategies, indicating that models employ diverse strategies for dissimilar problems. These findings suggest the potential to predict effective reasoning strategies for unseen questions based on the strategies used in similar, previously encountered questions. 6 Figure 5: Analysis of relationships between question similarity and reasoning strategy similarity across multiple benchmarks. Relationship between question similarity and reasoning strategy similarity. (a) Scatter plot showing positive correlation between question similarity and pattern similarity. (b) Variance analysis showing that pattern similarity becomes more consistent as question similarity increases."
        },
        {
            "title": "4.3 Predicting question-specific optimal reasoning strategies",
            "content": "Building on prior insights, we explore whether optimal reasoning strategies can be predicted and used to guide models toward more helpful and harmless behavior. We train binary classifiers for each criterion using three problem-solving benchmarks (GPQA-Diamond, MMLU-Redux, MATH-500) and two safety benchmarks (XSTest, WildGuard). For training, we use questions initially answered correctly to derive optimal strategies, while questions initially answered incorrectly are used for testing. We consider two settings: in-domain (trained and tested on the same benchmark) and crossdomain (trained on two benchmarks and tested on third). Each classifier predicts optimal strategies for the incorrect samples, which are then used to prompt the model, as described in Section 4.1. As shown in Figure 4, this controlled prompting substantially improves performanceachieving accuracy gains of 81.0%, 92.3%, and 80.7% on problem-solving tasks, and safety gains of 95.5% and 95.4% on safety tasks. These results show that models can be effectively guided toward optimal strategies, even on unseen questions. Unlike conventional approaches that generate long reasoning traces without direction, our method identifies and corrects reasoning weaknesses through targeted control, offering key advantage of the COT ENCYCLOPEDIA framework."
        },
        {
            "title": "5 Analyzing pathways to reasoning strategies: Data selection and",
            "content": "interpolation We have primarily analyzed reasoning strategies based on responses from trained models. This raises an important question: why do models produce specific types of reasoning strategies after training is completed? In this section, we investigate this question by directly RL training reasoning models on datasets with different formats and domains, then analyzing the emerging reasoning strategies."
        },
        {
            "title": "5.1 Format matters more than domain in shaping reasoning strategies",
            "content": "To examine how training data characteristics influence reasoning strategies, we compare the effects of data format and domain using Reinforcement Learning with Verifiable Rewards (RLVR). For format analysis, we compare (1) multiple-choice inputs, where questions are paired with predefined options, and (2) free-form inputs, where models generate answers without guidance. Using the NuminaMath dataset [10], originally in free-form, we synthetically generate multiple-choice versions to control for content while isolating presentation format. For domain analysis, we contrast math-domain datasets (e.g., NuminaMath) with knowledge-domain datasets such as OpenBookQA, QASC, SciQ, CommonsenseQA, and ARC-Challenge [14, 9, 23, 20, 2]. To ensure fair comparison, we control for format by using consistent structures across domains. We train 7B Deepseek-R1-Distill models, which are well-suited for long chain-of-thought generation. This setup allows us to isolate the individual effects of format and domain while holding other 7 Figure 6: Comparison of effect sizes showing how question format and domain influence reasoning strategies across three benchmarks. Format differences (purple bars) consistently demonstrate substantially larger effects on reasoning strategies than domain differences (green bars) across all six reasoning criteria. This indicates that how questions are presented has greater impact on reasoning approaches than the subject matter being tested. Figure 7: Qualitative and quantitative comparison between models trained on Multiple-choice (MC) and Free-form (FF) data formats. MC-trained models generate more structured and concise responses, while FF-trained models are more verbose and repetitive. These differences appear both qualitatively (left, middle) and quantitatively (right). variables constant. Additional training details are provided in Appendix A.5. To quantify their relative influence, we apply the statistical tests from Section 3, computing Cohens values between reasoning strategy distributions. As shown in Figure 6, format variation consistently leads to larger shifts in reasoning strategies than domain differences, indicating that format has greater impact than domain on shaping model reasoning. 5."
        },
        {
            "title": "Impact of training data format on model reasoning behavior",
            "content": "We analyze model responses trained on Multiple-choice (MC) and Free-form (FF) data using the Arena-Hard benchmark. As shown in Figure 7, the two models display distinct reasoning styles: MC-trained models produce concise, structured answers, while FF-trained models are more verbose and often repeat filler words like wait. Table F.2 further reveals that MC-trained models explore 8 Figure 8: Interpolation of reasoning strategies through model merging. reasoning strategy dynamics as models are merged from Multiple-Choice (MC) to Free-Form (FF) training formats across three benchmarks. multiple solution paths early onsimilar to breadth-first searchwhereas FF-trained models follow single path with iterative verification, resembling depth-first search. These differences arise from the presence or absence of answer cues during training: MC data encourages evaluating options before responding, while FF data requires open-ended exploration, often with greater uncertainty and verification. Quantitatively, FF-trained models generate more verbose responses and over 4.6 times as many wait tokens per answer (8.76 vs. 1.89). Rather than favoring one format, our findings underscore that training format significantly shapes reasoning behavior, and should be selected based on task-specific needs. 5."
        },
        {
            "title": "Interpolating desired reasoning strategies through model merging",
            "content": "Given that models trained on different data formats exhibit distinct reasoning strategies, we investigate whether intermediate patterns can be achieved through model interpolation. We conduct linear merging experiments between models trained on multiple-choice format and those trained on freeform format using varying merging ratios, then analyze the resulting reasoning strategies across our three benchmark datasets. As Figure 8 illustrates, we observe interpolation of reasoning strategies as the merging ratio shifts from the multiple-choice model (MC) toward the free-form model (FF). The interpolation behavior varies by reasoning criterion and benchmark. GPQA-Diamond shows relatively moderate changes, while MMLU-Redux exhibits more complex dynamics with several criteria showing clear crossover points around the 50% merging ratio. MATH-500 displays the most dramatic transitions across nearly all criteria. For instance, in MATH-500, the Bottom-Up analytical perspective (blue line) shows steady decrease from nearly 85% to approximately 15% as we move from MC to FF. This finding has practical implications: model weight merging provides an effective method for generating models with precise reasoning strategy combinations without additional training. The smooth transitions observed in most criteria indicate that hybrid reasoning approaches can be systematically controlled through simple linear interpolation techniques, opening promising avenues for tailoring reasoning capabilities to specific task requirements."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced the COT ENCYCLOPEDIA, flexible, automated framework for analyzing reasoning strategies in LongCoT language models. Unlike rigid, predefined taxonomies, our bottom-up clustering approach identifies reasoning strategies directly from model outputs, creating comprehensive taxonomy validated through human evaluation. Our empirical results revealed four key insights: (1) optimal reasoning strategies significantly enhance task performance on both helpfulness and safety benchmarks; (2) these patterns can be predicted from input questions alone, enabling real-time adaptive reasoning control; (3) training data format influences reasoning strategies more substantially than domain; and (4) desired reasoning behaviors can be interpolated through model weight merging without additional training. The COT ENCYCLOPEDIA advances our understanding of reasoning models and provides practical tools for steering them toward safer, more effective strategies. By 9 identifying which reasoning strategies yield optimal performance for specific problems and what training data cultivates these patterns, this work supports responsible deployment of language models in applications where performance, safety, and predictability are paramount."
        },
        {
            "title": "References",
            "content": "[1] Shengnan An, Bo Zhou, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Weizhu Chen, and Jian-Guang Lou. Skill-based few-shot selection for in-context learning. arXiv preprint arXiv:2305.14210, 2023. [2] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [3] Aniket Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy Lillicrap, Danilo Jimenez Rezende, Yoshua Bengio, Michael Mozer, and Sanjeev Arora. Metacognitive capabilities of llms: An exploration in mathematical problem solving. Advances in Neural Information Processing Systems, 37:1978319812, 2024. [4] Yuanheng Fang, Guoqing Chao, Wenqiang Lei, Shaobo Li, and Dianhui Chu. Cdw-cot: Clustered distance-weighted chain-of-thoughts reasoning. arXiv preprint arXiv:2501.12226, 2025. [5] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. [6] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [7] Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin, Nathan Lambert, Yejin Choi, and Nouha Dziri. Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms. arXiv preprint arXiv:2406.18495, 2024. [8] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [9] Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. Qasc: dataset for question answering via sentence composition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 80828090, 2020. [10] Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface.co/AI-MO/NuminaMath-CoT](https://github.com/ project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. [11] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. [12] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. [13] Sara Vera Marjanovic, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad BehnamGhader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han Lù, et al. Deepseek-r1 thoughtology: Lets< think> about llm reasoning. arXiv preprint arXiv:2504.07128, 2025. [14] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. 11 [15] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [16] Daniel Müllner. Modern hierarchical, agglomerative clustering algorithms. arXiv preprint arXiv:1109.2378, 2011. [17] Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: test suite for identifying exaggerated safety behaviours in large language models. arXiv preprint arXiv:2308.01263, 2023. [18] John Schulman. Approximating kl divergence. John Schulmans Homepage, 2020. [19] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [20] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149 4158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421. [21] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [23] Johannes Welbl, Nelson Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017. [24] Pengcheng Wen, Jiaming Ji, Chi-Min Chan, Juntao Dai, Donghai Hong, Yaodong Yang, Sirui Han, and Yike Guo. Thinkpatterns-21k: systematic study on the impact of thinking patterns in llms. arXiv preprint arXiv:2503.12918, 2025. [25] Zifan Xu, Haozhu Wang, Dmitriy Bespalov, Xian Wu, Peter Stone, and Yanjun Qi. Lars: Latent reasoning skills for chain-of-thought reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 36243643, 2024. [26] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. [27] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. ICML, 2025. [28] Zhiyuan Zeng, Yizhong Wang, Hannaneh Hajishirzi, and Pang Wei Koh. Evaltree: Profiling language model weaknesses via hierarchical capability trees. arXiv preprint arXiv:2503.08893, 2025. [29] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022."
        },
        {
            "title": "A Implementation details",
            "content": "A.1 COT ENCYCLOPEDIA details We use the OpenAI GPT-4o3 to conduct reasoning strategy Identification. And we also use OpenAI text-embedding-3-large4 to embed the criteria. In this study, we create reasoning strategies using the test sets from GPQA-Diamond, MMLU-Redux, and MATH-500 datasets. However, the COT ENCYCLOPEDIA is not limited to specific data and can be applied to any dataset. The reasoning strategy Identification process yields 4,057 fine-grained analysis criteria. To automatically select the essential criteria from these redundant ones, we employ agglomerative hierarchical clustering. In this process, we do not specify distance threshold for cluster merging but instead form an appropriate number of clusters based on silhouette scores. In our study, six clusters emerge. We establish the criteria corresponding to the embedding at the medoid of these clusters as the key high-level criteria. Next, to evaluate responses using these criteria, we develop rubrics to determine which responses belong to contrasting patterns and B, using the OpenAI GPT-4o API. Finally, we conclude by creating CoT pattern report that evaluates the CoT patterns present in the responses based on the generated rubrics. A.2 Statistical test details We conduct statistical tests to measure the similarity between two reasoning strategy distributions. We perform Chi-squared tests, where the null hypothesis (H0) states that there is no difference between the two distributions, and we set the p-value at the conventional threshold of 0.05. This means that if the p-value is lower than 0.05, we reject H0 and accept the alternative hypothesis (Ha), concluding that there is statistically significant difference between the two distributions. Conversely, if the p-value is greater than 0.05, we fail to reject H0, indicating that there is no statistical difference between the two distributions. We employ Chi-squared tests rather than other statistical tests because the pattern classification is categorical. Additionally, to measure the similarity between the two distributions more quantitatively, we calculate Cohens value, where an absolute value of approximately 0.2 is generally considered small effect size, approximately 0.5 medium effect size, and 0.8 or greater large effect size. A.3 Human evaluation details To verify whether the COT ENCYCLOPEDIA framework is perceived as reasonable by people, we conduct human evaluation. This evaluation is particularly necessary because COT ENCYCLOPEDIA utilizes synthetic outputs generated by an LLM at each step, requiring validation of their reliability. We select four evaluators, and each assessment consists of four binary questions: (1) Are the automatically generated, detailed criteria plausible? (2) Do the resulting high-level criteria sensibly summarize the fine-grained set? (3) Is the response analysis, when expressed in the predefined criteria, relevant and reasonable? (4) Is the response analysis, when expressed in the high-level criteria, relevant and reasonable? We use Argilla5 as our human evaluation platform. The final human evaluation metric is calculated by determining the ratio of yes/no responses for each question. A.4 Benchmark evaluation details We conduct evaluations using the MMLU-Redux knowledge benchmark, the GPQA-Diamond reasoning benchmark, and the MATH-500 mathematics benchmark. We employ the vllm library6 with hyperparameters following established research practices: temperature of 0.6, top of 0.95, and max tokens of 32768. All evaluations are performed on the test sets of each dataset. MMLURedux and GPQA-Diamond are multiple-choice question answering datasets, while MATH-500 is an open-ended generation dataset. However, since our evaluation targets reasoning models that generate LongCoT, we implement generation-based evaluation rather than logit-based evaluation even for the multiple-choice datasets. To determine whether the models predictions match the actual 3https://platform.openai.com/docs/models/gpt-4o 4https://platform.openai.com/docs/models/text-embedding-3-large 5https://argilla.io/ 6https://github.com/vllm-project/vllm 13 answers, we parse the predicted values from the generated LongCoT and use the Math-Verify library7 to verify their correctness. Ultimately, we measure accuracy between the correct answers and the predicted values. To measure safety, we utilize the LLaMA-Guard-3 8B model for our evaluation. When provided with question and model response, this model outputs either safe or unsafe as its assessment. We then calculate the proportion of responses classified as safe and use this ratio as our metric. A.5 Training details We utilize the GRPO algorithm [6] during the training process with Reinforcement Learning with Verifiable Reward (RLVR). The objective function of GRPO is defined as follows: JGRP O(θ) = qP (Q),{oi}G i=1πθold (cid:40)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi oi (cid:88) t=1 (cid:34) min (Oq) clip (cid:18) πθ(oi,t q, oi,<t) πθold (oi,t q, oi,<t) (cid:19) , 1 ε, 1 + ε πθ(oi,t q, oi,<t) πθold (oi,t q, oi,<t) (cid:35) ˆAi,t, (cid:41) (1) ˆAi,t βDKL [πθπref ] where ε and β are hyperparameters, ˆAi,t is the advantage calculated based on relative rewards within each sampled group, and DKL[πθπref ] is the KL divergence used as regularization term to stabilize the training process. GRPO optimizes the policy model by comparing multiple outputs generated for the same input, avoiding the need for separate value function approximation and thereby reducing computational overhead. To estimate the KL divergence between the current policy πθ and reference policy πref , we use the following unbiased estimator [18]. This form ensures positivity and avoids numerical instability. Unlike traditional KL penalties, this estimator is well-suited for token-level comparisons in sequence modeling: DKL [πθπref ] = πref (oi,t q, oi,<t) πθ(oi,t q, oi,<t) log πref (oi,t q, oi,<t) πθ(oi,t q, oi,<t) 1, (2) We use diverse datasets for training: NuminaMath, free-form math dataset, which we also convert into 5-choice question answering format by synthetically generating four options using GPT-4o API; and multiple-choice knowledge and common sense datasets including SciQ, QASC, OpenbookQA, CommonsenseQA, ARC-Challenge, and MCQA 68k dataset8. To control for format variables, we train on NuminaMath in both free-form and multiple-choice formats within the same math domain. Conversely, to examine domain differences, we maintain consistent multiple-choice format while varying between mathematical content (NuminaMath converted to 5-choice format) and knowledge/common sense datasets. All training datasets contain 100k examples. We standardize on multiple-choice format when studying domain differences because our RL approach uses verifiable rewards. Unlike math or coding domains where predictions can be directly compared to gold answers, knowledge and common sense domains often allow for varied but equally correct responses that dont exactly match the gold standard. Inspired by previous research, we adopt multiple-choice format as it ensures verifiability across all domainsany response matching the correct option can be definitively scored as correct. This approach enables us to apply verifiable rewards even to domains that are traditionally difficult to evaluate deterministically. When training the 7B model with the GRPO algorithm, we utilize the Open-R1 library9. For the reward function, we exclusively implement an accuracy reward function that assigns reward of 1 when the models prediction matches the gold answer and 0 otherwise. We decide not to use the format reward function employed in training Deepseek-R1 because it can lead to form of reward hackingwhere the model receives rewards for following the correct format even when producing incorrect answers, resulting in maintained formatting without improved accuracy. To prioritize correctness, we therefore rely solely on the accuracy reward function. For hyperparameters, we set max completion length at 2048, number of generations at 3, batch size at 72, torch dtype at bfloat16, 7https://github.com/huggingface/Math-Verify 8https://huggingface.co/datasets/berquetR/mcqa_dataset 9https://github.com/huggingface/open-r1 and attention implementation at flash attention 2. We use learning rate of 2.0e05, number of train epochs of 1, and warmup ratio of 0.1. A.6 Computing resources For RLVR training of our 7B model, we use eight NVIDIA H100 80GB GPUs, requiring 576 GPU hours to train on 100,000 data samples. For inference, we employ sixteen NVIDIA A100 40GB GPUs, consuming 384 GPU hours to process 3,698 data samples. Additionally, we use an AMD EPYC 7763 64-Core Processor for the CPU, which features 64 cores, CPU speed of 1497.674 MHz, and cache size of 512KB."
        },
        {
            "title": "B Further Analyses",
            "content": "B.1 Fine-grained benchmark-specific criteria analysis To conduct more detailed analysis, we performed hierarchical clustering on the complete responses from GPQA-Diamond, MMLU-Redux, and MATH-500 benchmarks to establish six criteria. For more fine-grained examination, we conducted hierarchical clustering separately for responses from each benchmark, using the default setting of selecting each clusters medoid as the representative embedding. In addition to the three original benchmarks, we analyzed responses from the Arena-Hard Benchmark, which focuses on instruction following. As illustrated in Figure 14, we observed that while the criteria derived from the original three benchmarks were relatively similar to each other, the Arena-Hard benchmark yielded notably different criteria. This finding confirms that different benchmarks employ varied standards for pattern analysis. Particularly noteworthy is the User understanding criterion. While the original benchmarks primarily focus on solving specific problems correctly, instruction following benchmarks emphasize accurately interpreting user intent. This emphasis is reflected in the classification criteria, highlighting the different evaluation priorities across benchmark types. B.2 Ablation study on representative embedding selection When extracting representative embeddings for each cluster formed through hierarchical clustering, we primarily use medoid embeddings as representative embeddings. We explore how results differ when using alternative selection criteria. Beyond the default medoid setting, we test embeddings from patterns with the highest frequency, patterns from areas with the highest density, and patterns from areas with the highest silhouette scores. As shown in Figure 15, most selection criteria do not demonstrate significant differences compared to selecting the default medoid setting. In the default settings clarification approach, the only differences appear between the silhouette-based and density-based approaches, which use computation style and clarity on steps respectively, while all other aspects remain identical. B.3 Consistency of reasoning strategies across model sizes within the same family Do models from the same family exhibit similar reasoning strategies despite having different sizes? To investigate this question, we compare the reasoning strategies of three models from the same family but with different sizes: Distill-R1 1.5B, 7B, and 32B. We classify the responses generated by each model on the GPQA-Diamond, MMLU-Redux, and MATH-500 benchmarks according to six criteria. As shown in Figure 16, the three models demonstrate remarkably similar distributions of reasoning strategies despite their different sizes. Additionally, as illustrated in Figure 17, the pairwise Cohens measurements between the three models reveal that most absolute values are below 0.1, indicating very minor distributional differences. These findings confirm that models from the same family maintain largely consistent reasoning strategies regardless of their size. B.4 Extending reasoning strategy analysis to non-reasoning models In this study, we extend our reasoning strategy analysis using the COT ENCYCLOPEDIA beyond the primary reasoning models discussed in the main text (S1.1-32B, QwQ-32B, Distill-R1-32B) to include non-reasoning models such as Qwen-2.5-Instruct-32B and Qwen-2.5-Math-Instruct-32B. 15 Figure 9: Conditional Probabilities Analysis on problem-solving Benchmarks. The bars represent different reasoning strategies categorized by Analytical Perspective (blue), Scope of Approach (yellow), Reasoning Type (green), Idea Development (orange), Verification Focus (purple), and Clarification Approach (red). Patterns such as broad, top_down, and immediate consistently show higher probabilities of correct responses across benchmarks. Figure 10: Conditional Probabilities Analysis on Safety Benchmarks. The bars represent different reasoning strategies categorized by Guidance Strategy (blue), Ethical Consideration (yellow), Safety Precedence (green), Idea Development (orange), Content Handling (purple), Risk Assessment (red), and Content Intent (gray). Patterns such as Moral Evaluation, Benevolent, Preventive, and on-Directive consistently show higher probabilities of safe responses across both benchmarks. We conduct this analysis across five benchmarks: GPQA-Diamond, MMLU-Redux, MATH-500, XSTest, and WildGuard. As shown in Table 12 and 13, the criteria generated for the non-reasoning models closely resemble those generated for the reasoning models. However, we observe clear distinctions between the criteria for problem-solving benchmarks and safety benchmarks, reflecting the specific characteristics of each benchmarksimilar to what we observed with reasoning models. This suggests that the criteria are more significantly influenced by the target benchmark rather than by the models output. Additionally, through chi-squared tests and Cohens values, we confirm that the pattern distributions of Qwen-2.5-Instruct-32B and Qwen-2.5-Math-Instruct-32B differ significantly from each other. These findings demonstrate the versatility of the COT ENCYCLOPEDIA as an analytical tool that can be effectively applied to non-reasoning models as well. 16 Figure 11: Impact of pattern-based instructions on model performance across five benchmarks. For all benchmarks, instructing models to follow question-specific optimal reasoning strategies yields the highest performance (17.5-68.5%), followed by dataset-wide optimal patterns (14.3-64.2%), random patterns (12.5-46.8%), while unoptimal patterns (11.5-37.8%) sometimes perform worse than not providing instructions at all (10.1-46.7%). The impact is particularly pronounced for safety benchmarks (XSTest and Wild guard), where optimal instructions more than double the safe response ratio compared to unoptimal instructions. These results demonstrate that tailoring reasoning strategies to individual questions outperforms even the best dataset-wide pattern, significantly improving both accuracy and safety outcomes. Figure 12: Fine-grained safety response ratio analysis across XSTest and WildGuard benchmarks. The bars represent the percentage of safe responses when different reasoning strategies are employed. Benevolent reasoning achieves the highest safety scores (84.0% in XSTest, 81.0% in WildGuard), while Malicious and Risk engaging patterns show the lowest safety performance. Patterns like Preventive, Moral, and Non-directive also demonstrate relatively high safety response ratios across both benchmarks."
        },
        {
            "title": "C Broader Impact",
            "content": "This work has several broader implications for the development and deployment of large language models. First, our analysis highlights the importance of reasoning controllabilitythe ability to steer models problem-solving strategy. This capability may play critical role in building more interpretable, debuggable, and safety-aligned systems, especially in high-stakes applications such as education, healthcare, and scientific discovery. Second, our taxonomy can inform curriculum design for training reasoning-oriented models, enabling researchers to curate data that promotes specific cognitive patterns. Moreover, the ability to predict and guide reasoning behavior opens up opportunities for interactive systems that provide explanations or tutoring based on user input and model inference strategies. Finally, the emphasis on format as driver of reasoning diversity suggests that future benchmark and dataset development efforts should consider structural diversitynot just domain coverageas factor for improving generalization and reasoning robustness. 17 Figure 13: Performance comparison of different methods for predicting optimal reasoning strategies across five benchmarks. Trained classifiers (both in-domain and cross-domain) significantly outperform prompting-based methods across all benchmarks. In-domain classifiers achieve the highest performance (70.3-84.3%), followed closely by cross-domain classifiers (66.2-80.1%), while zero-shot and few-shot prompting perform only slightly above random chance (50%). The performance trend is consistent for both accuracy-based benchmarks (GPQA-Diamond, MMLURedux, MATH-500) and safety-focused benchmarks (XSTest, WildGuard) where safe response ratio is measured."
        },
        {
            "title": "D Limitations",
            "content": "While our findings are promising, several limitations warrant discussion. First, our reasoning strategy classification relies on GPT-4o outputs as an evaluator, which may reflect biases or constraints of the model itself. Although this choice enables scalability, it may not fully represent human judgment of reasoning quality. Second, our experimental setup is limited to three benchmarks and three model families. While these cover diverse reasoning domains, extending our analysis to broader range of tasks (e.g., scientific reasoning, code generation, multi-modal tasks) and models (e.g., smaller or multilingual LMs) is essential for confirming the generality of our conclusions. Third, while we observe consistent performance improvements through pattern-guided prompting, such improvements are contingent upon models ability to reliably follow stylistic instructions. This requirement may limit applicability to instruction-tuned or higher-capacity models. Table 1: Comparison of cognitive behavior [5] frequencies between Distill-R1-32B and s1.1-32B models across three benchmarks. Statistical analysis (p-values and Cohens d) shows minimal differences between models, with only one significant difference, indicating limitations of conventional cognitive behavior classifications in distinguishing model reasoning strategies."
        },
        {
            "title": "Behavior",
            "content": "Ratio (Behavior Frequency / Total Responses) Distill-R1 s1.1 Are they different? p-value Cohens GPQA-Diamond MMLU-Redux MATH-"
        },
        {
            "title": "Backward Chaining",
            "content": "27.3 33.8 34.3 26.3 38.6 27. 26.0 36.2 27.0 29.8 33.3 34. 73.7 72.2 24.7 33.1 27.4 68. 68.7 28.4 30.4 28.8 67.6 67. No No No No No"
        },
        {
            "title": "Yes",
            "content": "No No No No No No 0.66 1.00 1.00 0.82 0.16 1e0.75 0.87 0.43 0.06 0.57 1. -0.09 0.02 0.00 0.07 0.04 0. 0.01 4e-3 -0.05 0.12 0.04 0. 18 Table 2: Comparison of cognitive behavior [5] frequencies between Distill-R1-32B and QwQ-32B models across three benchmarks. Statistical analysis (p-values and Cohens d) shows minimal differences between models, with only one significant difference, indicating limitations of conventional cognitive behavior classifications in distinguishing model reasoning strategies."
        },
        {
            "title": "Behavior",
            "content": "Ratio (Behavior Frequency / Total Responses) Distill-R"
        },
        {
            "title": "QwQ",
            "content": "Are they different? p-value Cohens GPQA-Diamond MMLU-Redux MATH-"
        },
        {
            "title": "Backward Chaining",
            "content": "27.3 33.8 34.3 26.3 38.6 27. 26.0 36.2 27.0 27.8 32.3 32. 73.7 74.2 25.3 32.2 26.7 68. 70.5 27.6 35.2 27.4 67.6 68. No No No No No"
        },
        {
            "title": "Yes",
            "content": "No No No No No No 0.91 0.75 0.67 0.91 0.38 2.18e0.79 0.18 0.57 0.74 0.89 0. -0.01 0.03 0.04 -0.01 0.02 0. 0.01 -0.03 -0.04 0.02 -0.01 -0. Table 3: Comparison of cognitive behavior [5] frequencies between QwQ-32B and s1.1-32B models across three benchmarks. Statistical analysis (p-values and Cohens d) shows minimal differences between models, with only one significant difference, indicating limitations of conventional cognitive behavior classifications in distinguishing model reasoning strategies."
        },
        {
            "title": "Behavior",
            "content": "Ratio (Behavior Frequency / Total Responses)"
        },
        {
            "title": "QwQ",
            "content": "s1.1 Are they different? p-value Cohens GPQA-Diamond MMLU-Redux MATH-"
        },
        {
            "title": "Backward Chaining",
            "content": "27.8 32.3 32.3 25.3 32.2 26. 27.6 35.2 27.4 29.8 33.3 34. 74.2 72.2 24.7 33.1 27.4 70. 68.7 28.4 30.4 28.8 68.9 67. No No No No No No No No No No No No 0.66 0.83 0.67 0.65 0.59 0. 0.54 0.13 0.78 0.11 0.62 0. -0.04 -0.02 -0.04 0.05 0.01 -0. -0.02 0.04 -0.02 0.10 -0.03 0. 19 Table 4: Analysis of reasoning strategies between Distill-R1-32B and s1.1-32B models using COT ENCYCLOPEDIAs six criteria across problem-solving benchmarks. Unlike traditional cognitive behavior metrics, this approach reveals numerous statistically significant differences (marked as Yes), with substantial effect sizes (Cohens up to 0.44), demonstrating CoT Encyclopedias enhanced ability to distinguish between models reasoning strategies and preferences. Benchmark Reasoning Behavior <- Pattern Pattern -> Criteria Pattern Pattern Distill-R1 Analytical Perspective Top-Down Bottom-Up Scope of Approach Focused Broad GPQADiamond Reasoning Type Inductive Deductive Idea Development Sequential Parallel 75. 7.1 15.1 15.2 23.7 30.3 Verification Focus Data-Driven Hypothesis-Driven Clarification Approach Iterative Immediate 60.1 7. 1.0 s1.1 79.3 40.9 68.2 Analytical Perspective Top-Down Bottom-Up Scope of Approach Focused Broad MMLURedux Reasoning Type Inductive Deductive Idea Development Sequential Parallel 32.7 43.6 8.9 27.4 33.1 28. 5.7 38.5 Verification Focus Data-Driven Hypothesis-Driven 87.5 84. Clarification Approach Iterative Immediate 23.2 10.6 Analytical Perspective Top-Down Bottom-Up Scope of Approach Focused Broad 36. 55.6 MATH500 Reasoning Type Inductive Deductive Idea Development Sequential Parallel Verification Focus Data-Driven Hypothesis-Driven Clarification Approach Iterative Immediate 10.4 17.0 14.4 43.8 37. 11.0 21.8 69.6 64.2 2.8 Are they different? p-value Cohens No Yes Yes Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes No No No Yes 4.7e-2 -0.10 2e-2 4e4e-2 1.2e-2 2e-03 0.76 1e-34 2e4e-20 1e-03 3e-38 0.03 1e-08 8.4e6e-2 8e-2 0.26 0.22 -0.22 -0. 0.36 0.01 0.32 0.12 0.24 0. 0.34 0.14 0.37 0.02 0.12 0. 1e-10 0.44 Table 5: Analysis of reasoning strategies between Distill-R1-32B and QwQ-32B models using COT ENCYCLOPEDIAs six criteria across problem-solving benchmarks. Unlike traditional cognitive behavior metrics, this approach reveals numerous statistically significant differences (marked as Yes), with substantial effect sizes (Cohens up to 0.44), demonstrating CoT Encyclopedias enhanced ability to distinguish between models reasoning strategies and preferences."
        },
        {
            "title": "Reasoning Behavior",
            "content": "<- Pattern Pattern ->"
        },
        {
            "title": "Pattern B",
            "content": "Distill-R"
        },
        {
            "title": "Are they",
            "content": "different? p-value Cohens d"
        },
        {
            "title": "Analytical Perspective",
            "content": "Top-Down Bottom-Up"
        },
        {
            "title": "Broad",
            "content": "GPQADiamond"
        },
        {
            "title": "Verification Focus",
            "content": "Data-Driven Hypothesis-Driven"
        },
        {
            "title": "Analytical Perspective",
            "content": "Top-Down Bottom-Up"
        },
        {
            "title": "Broad",
            "content": "MMLURedux"
        },
        {
            "title": "Parallel",
            "content": "75.8 80.2 15.2 23.7 30.3 7. 60.1 32.7 43.6 8.9 27.4 5. 12.1 43.2 70.9 2.1 4.3 33. 24.6 39."
        },
        {
            "title": "Verification Focus",
            "content": "Data-Driven Hypothesis-Driven 87.5 82."
        },
        {
            "title": "Immediate",
            "content": "23.2 13."
        },
        {
            "title": "Analytical Perspective",
            "content": "Top-Down Bottom-Up"
        },
        {
            "title": "Broad",
            "content": "36.8 55.6 MATH-"
        },
        {
            "title": "Verification Focus",
            "content": "Data-Driven Hypothesis-Driven"
        },
        {
            "title": "Immediate",
            "content": "10.4 17.0 14.4 20 42.5 33. 13.2 22.4 69.6 61.4 4.3 No"
        },
        {
            "title": "Yes",
            "content": "No"
        },
        {
            "title": "Yes",
            "content": "No"
        },
        {
            "title": "Yes",
            "content": "No"
        },
        {
            "title": "Yes",
            "content": "0.27 -0.11 8.5e-4 2.6e-3 6.8e-3 2.7e9.7e-3 0.34 0.31 -0.27 -0.23 0. 0.74 -0.01 2.4e-54 7.2e-13 0.41 0. 3.1e-22 -0.25 5.9e-8 9.6e-23 0.07 1.0e0.17 3.2e-2 6.4e-3 6.0e-8 0.14 0. -0.12 0.46 -0.09 -0.14 0.17 0. Table 6: Analysis of reasoning strategies between QwQ-32B and s1.1-32B models using COT ENCYCLOPEDIAs six criteria across problem-solving benchmarks. Unlike traditional cognitive behavior metrics, this approach reveals numerous statistically significant differences (marked as Yes), with substantial effect sizes (Cohens up to 0.35), demonstrating CoT Encyclopedias enhanced ability to distinguish between models reasoning strategies and preferences. Benchmark Criteria Pattern Pattern QwQ Reasoning Behavior <- Pattern Pattern -> Analytical Perspective Top-Down Bottom-Up Scope of Approach Focused Broad GPQADiamond Reasoning Type Inductive Deductive Idea Development Sequential Parallel Verification Focus Data-Driven Hypothesis-Driven Clarification Approach Iterative Immediate Analytical Perspective Top-Down Bottom-Up Scope of Approach Focused Broad MMLURedux Reasoning Type Inductive Deductive Idea Development Sequential Parallel Verification Focus Data-Driven Hypothesis-Driven 80.2 s1.1 79.3 5.2 12.1 7. 15.1 43.2 40.9 70.9 68.2 2. 4.3 33.1 24.6 1.0 5.7 33. 28.4 39.2 38.5 82.5 84.6 Clarification Approach Iterative Immediate 13.4 10.6 Analytical Perspective Top-Down Bottom-Up Scope of Approach Focused Broad MATH500 Reasoning Type Inductive Deductive Idea Development Sequential Parallel 42. 33.2 13.2 22.4 43.8 37.4 11. 21.8 Verification Focus Data-Driven Hypothesis-Driven Clarification Approach Iterative Immediate 61.4 64.2 14.4 4.3 Are they different? p-value Cohens No No No No No No No Yes Yes No Yes Yes No No No No No 0.80 0.40 0.38 0.61 0.59 0. 1.00 8.5e-04 1.3e-02 0.58 2.8e-02 8.5e0.65 0.16 0.29 0.82 0.36 Yes 6.0e-08 0.02 -0.08 -0.09 0.05 0. 0.09 0.00 -0.09 -0.06 0.01 -0. 0.09 -0.03 -0.09 0.07 0.01 -0. 0.35 Table 7: Analysis of reasoning strategies between Distill-R1-32B and s1.1-32B models using COT ENCYCLOPEDIAs six criteria across safety benchmarks. Unlike traditional cognitive behavior metrics, this approach reveals numerous statistically significant differences (marked as Yes), with substantial effect sizes (Cohens up to 0.50), demonstrating CoT Encyclopedias enhanced ability to distinguish between models reasoning strategies and preferences. Benchmark Reasoning Behavior <- Pattern Pattern -> Distill-R1 39.0 36.1 59.3 18.3 12. 22.7 40.3 s1.1 40.3 22.3 62. 40.9 68.2 19.1 100.0 99.5 43. 26.1 22.3 22.0 0.04 29.5 56. 67.5 3.4 42.3 46.1 100.0 99. Are they different? p-value Cohens No Yes No Yes Yes No No No Yes Yes Yes Yes Yes Yes 0. 5.4e-06 0.37 9.2e-14 2.4e-66 0.19 0. 0.084 0.0098 4.0e-12 2.9e-37 4.2e-14 1.0e1.8e-04 -0.03 0.31 -0.06 -0.50 -1. 0.09 0.14 -0.06 -0.09 -0.24 -0. -0.33 -0.34 0.18 Criteria Pattern Guidance Strategy Non-Directive Pattern Directive Ethical Consideration Technical Response Moral Evaluation Safety Precedence Risk-Engaging Preventive XSTest Idea Development Sequential Parallel Content Handling Open Discussion Censorship Risk Assessment High-Risk Awareness Low-Risk Assumption Content Intent Malicious Benevolent Guidance Strategy Non-Directive Directive Ethical Consideration Technical Response Moral Evaluation Safety Precedence Risk-Engaging Preventive WildGuard Idea Development Sequential Parallel Content Handling Open Discussion Censorship Risk Assessment High-Risk Awareness Low-Risk Assumption Content Intent Malicious Benevolent 21 Table 8: Analysis of reasoning strategies between Distill-R1-32B and QwQ-32B models using COT ENCYCLOPEDIAs six criteria across safety benchmarks. Unlike traditional cognitive behavior metrics, this approach reveals numerous statistically significant differences (marked as Yes), with substantial effect sizes (Cohens up to 1.44), demonstrating CoT Encyclopedias enhanced ability to distinguish between models reasoning strategies and preferences. Benchmark Reasoning Behavior <- Pattern Pattern -> Criteria Pattern Guidance Strategy Non-Directive Pattern Directive Ethical Consideration Technical Moral Evaluation Safety Precedence Risk-Engaging Preventive XSTest Idea Development Sequential Parallel Content Handling Open Discussion Censorship Risk Assessment High-Risk Awareness Low-Risk Assumption Content Intent Malicious Benevolent Guidance Strategy Non-Directive Directive Ethical Consideration Technical Moral Evaluation Safety Precedence Risk-Engaging Preventive WildGuard Idea Development Sequential Parallel Content Handling Open Discussion Censorship Risk Assessment High-Risk Awareness Low-Risk Assumption Content Intent Malicious Benevolent Distill-R1 39.0 36. 59.3 18.3 12.1 22.7 40.3 22. 22.0 56.0 0.04 29.5 QwQ 43. 25.2 43.2 67.2 69.5 29.1 100. 100.0 46.7 29.5 62.5 41.2 8. 48.3 100.0 100.0 Are they different? p-value Cohens No Yes Yes Yes Yes Yes No Yes Yes Yes Yes Yes Yes No 0.177 2.1e-4 8.7e-3 2.0e1.2e-76 2.11e-2 1.00 4.13e-2 9.45e-3 3.95e6.59e-11 3.56e-11 1.10e-9 1.00 -0.09 0. -0.16 -0.56 -1.44 -0.15 0.00 -0. -0.17 -0.13 -0.42 -0.43 -0.39 0. Table 9: Analysis of reasoning strategies between QwQ-32B and s1.1-32B models using COT ENCYCLOPEDIAs six criteria across safety benchmarks. Unlike traditional cognitive behavior metrics, this approach reveals statistically significant differences (marked as Yes), with moderate effect sizes (Cohens up to 0.24), demonstrating CoT Encyclopedias enhanced ability to distinguish between models reasoning strategies and preferences. Benchmark Reasoning Behavior <- Pattern Pattern -> Criteria Pattern Guidance Strategy Non-Directive Pattern Directive Ethical Consideration Technical Response Moral Evaluation Safety Precedence Risk-Engaging Preventive XSTest Idea Development Sequential Parallel Content Handling Open Discussion Censorship QwQ 43.3 s1.1 40.3 25.2 22. 43.2 67.2 69.5 62.3 40.9 68. Risk Assessment High-Risk Awareness Low-Risk Assumption 29.1 19.1 Content Intent Malicious Benevolent 100.0 100.0 Guidance Strategy Non-Directive Directive Ethical Consideration Technical Response Moral Evaluation Safety Precedence Risk-Engaging Preventive WildGuard Idea Development Sequential Parallel 46.7 29. 62.5 41.2 Content Handling Open Discussion Censorship Risk Assessment High-Risk Awareness Low-Risk Assumption 8.4 3.4 48.3 43.2 26. 67.5 42.3 46.1 Content Intent Malicious Benevolent 100.0 100.0 22 Are they different? p-value Cohens No No No No No Yes No No No No No Yes No No 0.37 0.30 0.11 0. 0.63 2e-4 1.00 0.25 0.20 0. 0.70 7e-4 0.45 1.00 0.06 0. 0.10 0.05 0.03 0.24 0.00 0. 0.08 -0.10 -0.02 0.21 0.04 0. Figure 14: Hierarchical Clustering Across Benchmarks. Six different benchmarks: XSTest (top left), WildGuard (top right), Arena-Hard (mid left), and MMLU-Redux (mid right), MATH-500 (bottom left), GPQA-Diamond (bottom right). Stars indicate the representative embeddings (medoids) for each cluster. This visualization demonstrates how different benchmarks employ varied criteria for pattern analysis. 23 Figure 15: Hierarchical Clustering of reasoning strategies with PCA Projection on problemsolving Benchmarks. Comparing four different representative embedding selection methods: medoid-based (default, top-left), silhouette-based (top-right), frequency-based (bottom-left), and density-based (bottom-right). Each stars indicate representative embeddings for each cluster. Despite using different selection criteria, the overall clustering structure remains consistent across methods, with only minor variations in representative embedding positions. Figure 16: Pattern ratio distributions across three different sizes of the Distill-R1 model family (32B, 7B, and 1.5B). The x-axis shows six reasoning strategy criteria: analytical perspective (ap), clarification approach (sa), reasoning type (rt), idea development (id), verification focus (vf), and scope of approach (ca). Despite the significant size differences, all three models exhibit remarkably similar pattern distributions across all benchmarks, supporting the conclusion that models from the same family maintain consistent reasoning strategies regardless of scale. 24 Figure 17: Pairwise Cohens effect size measurements comparing reasoning strategy distributions across different sizes of the Distill-R1 model family. The x-axis displays the six reasoning strategy criteria: analytical perspective (ap), scope of approach (sa), reasoning type (rt), idea development (id), verification focus (vf), and clarification approach (ca). All effect sizes remain below 0.1 across most comparisons, indicating very small distributional differences between models of different sizes within the same family. 25 Table 10: Comparison of high-level reasoning style preferences across models trained on different data formats. This table presents detailed analysis of how question format influences reasoning strategies across three benchmarks: GPQA-Diamond, MMLU-Redux, and MATH-500. For each benchmark, six reasoning criteria are evaluated with contrasting pattern pairs (Pattern vs. Pattern B). Statistical significance testing (p-values) and effect size measurements (Cohens d) reveal that multiple-choice and free-form trained models exhibit significantly different reasoning strategies on most criteria. Particularly pronounced differences appear in the MATH-500 benchmark, where effect sizes reach up to 1.58, demonstrating that training data format substantially shapes models problem-solving approaches independent of content domain. Benchmark Reasoning Behavior <- Pattern Pattern -> Are they Criteria Pattern Pattern Multiple-choice Free-form different? p-value Cohens Analytical Perspective Top-Down Bottom-Up Scope of Approach Focused Broad GPQADiamond Reasoning Type Inductive Deductive Idea Development Sequential Parallel Verification Focus Data-Driven Hypothesis-Driven Clarification Approach Iterative Immediate Analytical Perspective Top-Down Bottom-Up Scope of Approach Focused Broad MMLURedux Reasoning Type Inductive Deductive Idea Development Sequential Parallel 63.9 74.8 54. 27.0 39.6 21.0 39.5 48.6 27. 38.3 46.3 76.7 38.8 51.7 38. 59.2 48.3 76.8 50.5 12.7 Verification Focus Data-Driven Hypothesis-Driven Clarification Approach Iterative Immediate 66.9 34. 48.8 60.4 Analytical Perspective Top-Down Bottom-Up Scope of Approach Focused Broad MATH500 Reasoning Type Inductive Deductive Idea Development Sequential Parallel Verification Focus Data-Driven Hypothesis-Driven Clarification Approach Iterative Immediate 84.1 85.9 13.1 32. 81.9 14.1 34.8 52.2 38.5 0. 13.7 11.4 Yes No Yes Yes No Yes Yes No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 1e-36 0.12 8e-30 3e0.41 3e-14 2e-52 0.80 0.00 1e5e-139 0.36 -0.04 0.32 0.51 0. 0.22 -0.40 0.01 -1.03 0.25 0. 1e-19 -0.23 0.00 0.00 0.00 0. 1e-292 1.58 1.15 1.49 -0.99 -0. 1e-171 -0.65 26 Table 11: Comparison of high-level reasoning style preferences across models trained on different domains. This table presents statistical analysis of how content domain influences reasoning strategies across three benchmarks: GPQA-Diamond, MMLU-Redux, and MATH-500. Six reasoning criteria are evaluated for each benchmark, comparing math-domain versus knowledgedomain training. Statistical testing reveals minimal differences between domains, with most p-values above the significance threshold (0.05) and small effect sizes (Cohens mostly below 0.15). The limited number of significant differences (only 4 out of 18 comparisons) and small effect sizes demonstrate that content domain has substantially less impact on reasoning strategy formation than question format, supporting the papers finding that format characteristics shape reasoning strategies more fundamentally than subject matter."
        },
        {
            "title": "Reasoning Behavior",
            "content": "<- Pattern Pattern ->"
        },
        {
            "title": "Pattern B",
            "content": "Math-domain Knowledge-domain different? p-value Cohens d"
        },
        {
            "title": "Analytical Perspective",
            "content": "Top-Down Bottom-Up"
        },
        {
            "title": "Broad",
            "content": "GPQADiamond"
        },
        {
            "title": "Verification Focus",
            "content": "Data-Driven Hypothesis-Driven"
        },
        {
            "title": "Analytical Perspective",
            "content": "Top-Down Bottom-Up"
        },
        {
            "title": "Broad",
            "content": "MMLURedux"
        },
        {
            "title": "Verification Focus",
            "content": "Data-Driven Hypothesis-Driven"
        },
        {
            "title": "Analytical Perspective",
            "content": "Top-Down Bottom-Up"
        },
        {
            "title": "Broad",
            "content": "MATH-"
        },
        {
            "title": "Verification Focus",
            "content": "Data-Driven Hypothesis-Driven"
        },
        {
            "title": "Immediate",
            "content": "60.0 61.0 60.3 38.5 38.6 40. 41.2 38.6 42.0 57.4 59.2 58. 41.2 38.6 42.0 57.4 59.2 58. 61.4 62.3 60.9 37.7 38.3 36. 42.3 45.5 44.4 57.8 54.5 53. 43.6 42.0 40.0 57.2 56.4 59. No No No No No"
        },
        {
            "title": "Yes",
            "content": "No"
        },
        {
            "title": "Yes",
            "content": "No No No No"
        },
        {
            "title": "Yes",
            "content": "No No No"
        },
        {
            "title": "Yes",
            "content": "0.25 0.29 0.60 0.52 0.80 1e0.77 0.03 0.49 0.95 0.14 0. 0.03 1e-08 8.4e-2 6e-2 8e-2 0. -0.027 -0.014 0.017 0.007 0.082 -0. -0.14 -0.05 -0.01 0.09 0.10 0. 0.37 0.02 0.12 0.12 1e-10 0. 27 Table 12: Comparison of reasoning behavior patterns between Qwen and Qwen-Math models across three benchmarks. The table presents statistical analyses of six reasoning criteria: Decision Strategy, Reasoning Type, Analytical Perspective, Verification Focus, Scope of Approach, and Idea Development. For each criterion, the table shows the distribution percentages of Pattern and Pattern for both models, along with statistical significance measures (p-value and Cohens d). Reasoning Behavior <- Pattern Pattern -> Are they Qwen-Math different? Benchmark GPQADiamond MMLURedux MATHCriteria Pattern Decision Strategy Reasoning Type Optimal Inductive Pattern Satisficing Deductive Analytical Perspective Top-Down Bottom-Up Verification Focus Principle-Driven Numerical Validation Scope of Approach Focused Broad Exploration Qwen 53.1 37.6 21.3 21.7 65.4 57. 36.2 45.2 63.3 45.6 Idea Development Linear Sequential Parallel 10.3 10.4 Decision Strategy Reasoning Type Optimal Inductive Satisficing Deductive Analytical Perspective Top-Down Bottom-Up Verification Focus Principle-Driven Numerical Validation Scope of Approach Focused Broad Exploration Idea Development Linear Sequential Parallel Decision Strategy Reasoning Type Optimal Inductive Satisficing Deductive Analytical Perspective Top-Down Bottom-Up Verification Focus Principle-Driven Numerical Validation Scope of Approach Focused Broad Exploration 62.2 78. 53.4 36.2 41.9 47.2 10.1 15. 30.1 34.2 54.9 62.3 67.2 87. 36.9 55.2 31.0 23.3 17.1 35. 67.9 69.2 Idea Development Linear Sequential Parallel 22. 24.0 p-value Cohens 0.419 0.835 -0. 0.029 5.17e-7 -0.524 7.58e-17 -0.928 8.09e0.407 0.869 -0.003 0.936 -0.002 4.67e0.251 3.20e-184 -0.806 0.574 -0.015 6.59e- -0.268 2.86e-89 -0.535 1.93e-15 0.517 0. 0.0794 -0.052 -0.107 1.12e-26 -0.716 2.72e- -0.298 0.452 -0.045 No No Yes Yes Yes No No Yes Yes No Yes Yes Yes No No Yes Yes No Table 13: Comparison of reasoning strategies between QwQ and s1.1 models on safety benchmarks. This table presents statistical analyses of seven reasoning criteria across two safety benchmarks (XSTest and WildGuard). For each criterion, the table shows the distribution percentages of Pattern and Pattern for both models, along with statistical significance indicators (p-value and Cohens d). Benchmark Reasoning Behavior <- Pattern Pattern -> Criteria Pattern Guidance Strategy Non-Directive Pattern Directive Ethical Consideration Technical Response Moral Evaluation Safety Precedence Risk-Engaging Preventive XSTest Idea Development Sequential Parallel Content Handling Open Discussion Censorship QwQ 23.3 47. 62.2 41.2 69.5 s1.1 22.7 42. 67.3 70.9 78.2 Risk Assessment High-Risk Awareness Low-Risk Assumption 20. 3.1 Content Intent Malicious Benevolent 98.7 96. Guidance Strategy Non-Directive Directive Ethical Consideration Technical Response Moral Evaluation Safety Precedence Risk-Engaging Preventive WildGuard Idea Development Sequential Parallel Content Handling Open Discussion Censorship Risk Assessment High-Risk Awareness Low-Risk Assumption Content Intent Malicious Benevolent 23.7 26.5 51. 27.2 10.4 35.3 23.2 29.1 49. 46.3 21.4 32.1 100.0 100.0 Are they different? p-value Cohens No No No Yes No Yes No No No No Yes Yes No No 1. 0.48 0.46 1.92e-05 0.20 1.65e-04 0. 0.87 0.63 0.78 5.26e-03 3.16e-02 0. 1.00 0.01 0.10 -0.11 -0.63 -0. 0.55 0.15 0.01 -0.06 0.04 -0. -0.30 0.07 0.00 Figure 18: Examples of input formats and model responses from multiple-choice (MC) and free-form (FF) training data."
        },
        {
            "title": "E Prompts",
            "content": "Prompt for brainstorming the fine-grained criteria and patterns You are tasked with analyzing the reasoning strategies used in the following response. The response includes the thought process for solving problem. Your goal is to extract and describe patterns based on various criteria that characterize the models problem-solving strategy. Please follow these guidelines: 1. Identify multiple *meaningful criteria* that differentiate reasoning strategies. Each criterion should have clear and descriptive name that reflects real aspect of the reasoning process. **Do not use generic placeholders like Criterion 1.** 2. For each criterion, describe two contrasting *pattern types* (e.g., *Step-by-step* vs. *Outcome-first*, or *Concrete* vs. *Abstract*). 3. Present your analysis in the following format, using <patterns> and </patterns> tags to enclose the list: <patterns> Descriptive Criterion Name (Pattern vs. Pattern B) Descriptive Criterion Name (Pattern vs. Pattern B) ... Descriptive Criterion Name (Pattern vs. Pattern B) </patterns> 4. Do not include any explanations or commentary within the <patterns> tags. 5. The example format above is only guide. You are encouraged to define your own diverse and insightful pattern criteria based on the given response. Response: {answer} Figure 19: Prompt template for identifying fine-grained reasoning criteria. This template guides the systematic extraction of diverse reasoning strategies from model responses by instructing the analysis to: (1) identify meaningful criteria with descriptive names that reflect genuine aspects of reasoning, (2) describe contrasting pattern types for each criterion, (3) present the analysis in structured format using designated tags, and (4) focus on identifying insightful pattern criteria without extraneous commentary. This structured approach enables comprehensive taxonomy development of reasoning strategies employed in chain-of-thought processes."
        },
        {
            "title": "Prompt for generating the pattern analysis rubric",
            "content": "Create concise rubric for the following reasoning strategy criterion: [CRITERION NAME]: [PATTERN A] vs. [PATTERN B] For each pattern, provide: 1. clear, concise definition (2-3 sentences) that captures the essence of this reasoning strategy 2. 3-4 key characteristics that distinguish this pattern 3. 2 concrete examples of responses that demonstrate this pattern (keep examples brief, about 2-3 sentences each) Focus on making the distinctions between patterns clear and easily identifiable. The definitions and examples should help evaluators quickly categorize model responses without ambiguity. Figure 20: Prompt template for rubric generation in reasoning strategy analysis. This template outlines the structured approach for creating assessment rubrics that distinguish between contrasting reasoning strategies. The prompt requests (1) concise definitions capturing each patterns essence, (2) key distinguishing characteristics, and (3) concrete response examples demonstrating each pattern. This systematic rubric development ensures clear pattern differentiation, enabling consistent and unambiguous classification of model reasoning strategies across different problem-solving contexts."
        },
        {
            "title": "Prompt for generating the pattern analysis report",
            "content": "You are an expert at analyzing reasoning strategies in model responses. Youll be provided with: 1. rubric describing two distinct reasoning strategies (Pattern and Pattern B) 2. model response to analyze Your task is to create detailed analysis report that determines which pattern the response exhibits. Analysis Process: 1. Carefully examine the response against both pattern definitions in the rubric 2. Identify specific elements, structures, and linguistic features in the response that align with either pattern 3. Note any mixed signals or elements that span both patterns 4. Determine which pattern (A or B) the response most closely matches Report Structure: 1. **Initial Observations** (2-3 sentences summarizing key features of the reasoning approach) 2. **Evidence for Pattern A**: - If applicable, quote 1-2 specific segments from the response that demonstrate Pattern - Explain how these segments match characteristics described in the rubric 3. **Evidence for Pattern B**: - If applicable, quote 1-2 specific segments from the response that demonstrate Pattern - Explain how these segments match characteristics described in the rubric 4. **Pattern Determination**: - Explain which pattern (A or B) is most dominant and why - Address any aspects that show characteristics of both patterns 5. **Conclusion**: - Clearly state the final pattern determination using the format: \"Final pattern determination: [PATTERN NAME]\" Focus on concrete evidence from the response that matches specific elements from the rubric patterns. Rubric: {rubric} Response to analyze: {response} Figure 21: Prompt template for comprehensive reasoning strategy analysis report generation. This template guides the systematic evaluation of model responses against predefined reasoning strategies. It establishes structured analytical process involving (1) careful examination of responses against pattern definitions, (2) identification of pattern-aligned elements, (3) recognition of mixed pattern signals, and (4) determination of the dominant pattern. The prescribed report structureincluding initial observations, evidence documentation for each pattern, pattern determination with justification, and clear conclusionensures thorough and evidence-based classification of reasoning strategies employed in model outputs. 32 Prompt for evaluation on multi-choice QA benchmarks Question : {question} Options : A) {option } B) {option } ... Please reason step by step, and you should write the correct option alphabet within boxed{}. Figure 22: Prompt template for multiple-choice question assessment with structured reasoning. This template presents standardized format for evaluating reasoning models on multiple-choice QA benchmarks (GPQA-Diamon, MMLU-Redux)."
        },
        {
            "title": "Example prompt for steering models toward optimal reasoning strategies",
            "content": "You are required to solve the following question using specific reasoning strategy. This reasoning strategy must guide the entire problem-solving approach: Top-down: First, conceptualize the overall structure or system involved in the problem. Begin by forming clear, high-level understanding of the task, and then systematically break it down into lower-level details. Organize your reasoning from general principles to specific elements, prioritizing clarity and structure. Broad: Start by openly exploring wide range of possibilities without attempting to prioritize or filter them too early. Your goal is to understand the full landscape of potential approaches, ideas, or interpretations before narrowing down or making decisions. Inductive: Base your reasoning on concrete examples or specific observations. Look for patterns that emerge from these instances and use them to build general principles or conclusions. Learning through data-driven exploration or trial-and-error is encouraged. Single: Choose one hypothesis, method, or line of reasoning to pursue. At each step, verify whether your current approach works. If it does not, adjust accordingly before proceeding. Your reasoning should follow step-by-step method with focus on systematic validation. Hypothesis-driven: Approach the problem with specific, predefined hypothesis. Your task is to verify this hypothesis using logical reasoning and data. The structure of your analysis should aim to confirm or refute the initial claim without deviating from it. Immediate: Seek full clarity at the beginning. Ask direct and comprehensive questions immediately to form complete understanding of the problem. Avoid backtracking or revisiting earlier assumptions unless absolutely necessary. Make sure your entire reasoning process aligns with the selected pattern above. Now, solve the following question accordingly. {question} Figure 23: Example prompt for steering models toward optimal reasoning strategies. This template demonstrates how specific reasoning strategies can be explicitly prescribed to guide problemsolving approaches. Six distinct reasoning strategies are presented: top-down, broad, inductive, single, hypothesis-driven and immediate. The prompt instructs models to align their entire problem-solving process with the selected pattern, enabling controlled experiments to identify which reasoning strategies yield optimal performance across different tasks and benchmarks."
        },
        {
            "title": "F Qualitative Analyses",
            "content": "Beyond quantitative analysis, we conduct qualitative analysis based on the actual outputs generated by the model. In Appendix F.1, we analyze the artifacts produced at each step of COT ENCYCLOPEDIA on the GPQA-Diamond benchmark, comparing the final reasoning strategy analysis results with analysis results classified according to pre-defined criteria. Appendix F.2 presents comparative analysis of responses from models trained on multiple-choice format datasets versus those trained on free-form datasets when evaluated on the Arena Hard benchmark, along with pattern analysis of these responses. F.1 Comparative Analysis of CoT Encyclopedia Outputs and Pre-defined Criteria Table 14: Comparative analysis of reasoning strategies exhibited in physics problem-solving using multiple evaluation frameworks. This table presents detailed qualitative analysis of reasoning approaches employed when solving physics problem of GPQA-Diamond on light propagation through moving media. The analysis employs two distinct evaluation methodologies: the CoT Encyclopedia criteria, which identifies evidence-driven reasoning, and pre-defined criteria system that assesses backtracking behaviors. Question: What is the correct answer to this question:A light beam is propagating through glass with index of refraction n. The glass is moving at constant velocity in the same direction as the beam and toward the observer in laboratory. What is the speed of light in glass relative to the observer in laboratory? Take the speed of light in vacuum c=1. Choices: A. 1 B. (1+n*v)/(n+v) C. (1+n*v)/(n-v) D. (1-n*v)/(n+v) Answer: Response: Okay, so the question is about the speed of light in glass as observed in laboratory frame, where the glass itself is moving at constant velocity in the same direction as the light beam, and towards the observer. The speed of light in vacuum is given as c=1. The choices are through D, with various expressions involving and v. First, need to recall how the speed of light in medium relates to the mediums refractive index. The speed of light in medium with refractive index is given by v_light = c/n. But here, the medium (glass) is moving. So, the usual formula might need some adjustment due to the motion of the medium. Hmm, maybe we have to consider the velocity addition formula from special relativity, but Im not sure if that directly applies here. Let me think. Wait, ... (omitted) ... 2. Fresnel Drag Formula: The formula for the speed of light in moving medium is given by: Substituting = 1 into the formula: = + 1 + nc = 1 + 1 + 3. Simplification: Simplify the expression by multiplying the numerator and denominator by n: = + v(cid:1) (cid:0) 1 (cid:1) (cid:0)1 + = 1 + nv + 4. Relativistic Velocity Addition: Alternatively, using the relativistic velocity addition formula, where = 1 is the speed of light in the glass frame moving at velocity relative to the lab frame: Substituting = 1 : = + 1 + uv = 1 + 1 + This simplifies to the same expression as above: = 1 + nv + 5. Conclusion: The derived expression matches option B. Fine-grained criteria: \"Understanding Method (Conceptual Insight vs. Formula Application)\", \"Problem Fragmentation (Deconstruction vs. Holistic Approach)\", \"Validation Strategy (CrossVerification vs. Single-Source Verification)\", \"Solution Derivation (Analytic Derivation vs. Plugand-Chug)\", \"Exploratory Process (Iterative Consideration vs. Direct Pursuit)\", \"Logic Application (Physics Principles vs. Mathematical Manipulation)\", \"Interpretation Mode (Qualitative Analysis vs. Quantitative Analysis)\" Sampled COT ENCYCLOPEDIA criteria based report: The provided response demonstrates strong understanding of the physics involved in determining the speed of light in moving medium (in this case, glass) relative to laboratory observer. The thinker systematically recalls and applies relevant concepts, including the relationship between the speed of light and the refractive index, and utilizes the Fresnel drag coefficient and the relativistic velocity addition formula correctly. The reasoning starts with an exploration of how the speed of light in medium relates to its refractive index and adjusts for motion, showing thorough investigation of the problem rather than an assumption-driven approach. The responder considers multiple avenues of solution and confirms their correctness through simplification and careful algebra, ultimately arriving at the same conclusion using different methods. The exploration and hypothetical testing of ideas are evident, with the responder not merely following structured path dictated by singular hypothesis, but rather engaging in an open-ended inquiry that leads to the solution through evidence-based reasoning. Given that the response is grounded in evidence from physics concepts and engages with the problem dynamically, it aligns best with the \"Strongly Evidence-driven\" pattern of reasoning. The validity of the conclusions is drawn from examining the data and relevant principles without imposing an initial hypothesis that restricts the analysis. Thus, the response fits into the pattern of strong evidence-driven reasoning. Therefore, will categorize it as such: Strongly Evidence-driven 35 Sampled pre-defined criteria based report: The provided response exhibits clear pattern of backtracking, as the individual systematically revisits earlier thoughts and calculations to confirm and refine their understanding of the problem. Throughout the response, the author reflects on different aspects of the problem, considers potential adjustments, and verifies the correctness of their reasoning by retracting or re-evaluating previous conclusions. 1. The author starts by recalling foundational concepts related to light in medium and how it relates to refractive index and motion. 2. They initially consider the implications of special relativity on the motion of light in medium. 3. As they derive the formula for the speed of light in moving medium, they continuously verify their calculations making adjustments and confirming expressions until they are confident in the answer. 4. They also show awareness of different factors (like the direction of motion) that could affect their calculations, demonstrating reflective process where they reassess the implications of their findings. This approach aligns closely with the description of \"Backtracking\" in the rubric: the reasoning involves revisiting and reassessing previous decisions to arrive at correct conclusion. This indicates the presence of backtracking rather than strictly linear thinking process. Thus, the evaluation of the response is that it demonstrates backtracking. Final evaluation: Yes Table 15: Comparative analysis of reasoning strategies exhibited in physics problem-solving using multiple evaluation frameworks. This table presents comparative analysis of reasoning strategies exhibited when solving complex organic chemistry problem of GPQA-Diamond involving multiple synthesis steps. The analysis employs two complementary evaluation frameworks: the CoT Encyclopedia criteria, which identifies iterative clarification processes during reaction analysis, and pre-defined criteria system that focuses on backtracking behaviors. Question: What is the correct answer to this question:A methanol solution of (R)-(+)-Limonene is stirred with Pd/C under Hydrogen atmosphere. After 1 equivalent of hydrogen is consumed, product 1 is isolated as the major product. 1 is treated with 3-chloroperbenzoic acid, forming product 2. Product 2 is treated with sodium methoxide, forming product 3. Product 3 is treated with propanoic acid, dicyclohexylcarbodiimide. and catalytic amount of 4-dimethylaminopyridine, forming product 4. what is valid structure of product 4? (product 4 exists as mixture of isomers. the correct answer is one of them). Choices: A. 1-methoxy-2-((S)-4-methylcyclohex-3-en-1-yl)propan-2-yl propionate B. (1S,2S,5R)- 5-isopropyl-2-methoxy-2-methylcyclohexyl propionate C. (1S,2S,4R)-4-isopropyl-2-methoxy-1methylcyclohexyl propionate D. (1S,2R,4R)-4-isopropyl-2-methoxy-1-methylcyclohexyl propionate Answer: Response: Okay, lets tackle this question. So, the problem is about identifying the chemical formula of an unknown compound using FTIR, 1H NMR, and Mass Spectrometry data. The given choices are C6H12O, C6H10O, C6H10O2, or C6H12O2. The options are to corresponding to these formulas. First, lets parse the FTIR data. The FTIR spectrum shows very broad absorption peak at 3000 wavenumbers. ... (omitted) ... Wait, but lets check the other options. If the compound had only one oxygen (option B, C6H10O), how could that be? For example, if its ketone with double bond. ketone has one oxygen. But then where does the O-H come from? The question mentions broad peak at 3000 cm, which is O-H. So if the compound is ketone with an O-H group, that would require two oxygen atoms: one in the carbonyl (C=O) and one in the O-H. But option is C6H10O, which has only one oxygen. So thats impossible. Therefore, the answer must be A. Another angle: the degrees of unsaturation. The compound has two degrees of unsaturation (from the analysis above: carbonyl and double bond). The formulas with two degrees are and B. Since has only one oxygen, but the compound must have O-H (which needs an additional oxygen beyond the carbonyl), so is out. Therefore, is correct. Thus, the answer should be 36 Fine-grained criteria: \"Problem Decomposition (Stepwise Breakdown vs. Holistic Assessment)\", \"Data Interpretation (Spectrum Analysis vs. Literature Reference)\", \"Functional Group Identification (Empirical vs. Theory-Driven)\", \"Alternatives Consideration (Exhaustive Options vs. Targeted Options)\", \"Verification Focus (Spectral Data Consistent vs. Fragment Peak Analysis)\", \"Iterative Refinement (Initial Hypothesis vs. Re-evaluated Hypothesis)\" Sampled COT ENCYCLOPEDIA: The response demonstrates highly iterative approach to clarification. Throughout the analysis, the responder explores various possibilities concerning the chemical structure of the unknown compound using FTIR, 1H NMR, and Mass Spectrometry data. Key indicators of this iterative approach include: 1. **Gradual Exploration**: The responder goes through the spectral data step-by-step, interpreting each piece of information in relation to possible structures for the unknown compound. This indicates willingness to engage with uncertainty and adapt their understanding as more data is considered. 2. **Follow-Up Questions and Refinement**: The responder frequently asks rhetorical questions (e.g., \"Wait, but...\") that indicate reconsideration of previous assumptions and re-evaluating information based on new insights. They continuously refine their interpretations and reasoning as they progress. 3. **Comfort with Ambiguity**: The presence of various interpretations throughout (e.g., the potential for carboxylic acid, possible structures aligning with the degrees of unsaturation, and the consideration of different functional groups) reflects methodical approach to making sense of complex chemical data, without rushing to conclusions. 4. **Incremental Understanding**: The analysis reveals that the responder builds their conclusion based on progressively integrating different aspects of the data rather than expecting an immediate resolution. They discuss multiple scenarios and gradually hone in on specific interpretation that fits the data best. Thus, the response exemplifies mindset that validates iterative clarification, as the responder systematically processes the information and adjusts their conclusions in an exploratory manner. Therefore, the reasoning strategy for this response is Highly Iterative Clarification. Sampled pre-defined criteria based report: The given response clearly exhibits the characteristics of the \"Backtracking\" reasoning strategy. Throughout the analysis, the author revisits earlier decisions and thought processes, particularly in relation to the interpretation of spectral data and the implications for the chemical structure of the unknown compound. For instance, the author considers the implications of the FTIR and NMR spectra, reassessing conclusions about potential functional groups and how they relate to the chemical formulas provided. The responses flow reflects significant amount of reconsideration of hypotheses, especially when determining the degrees of unsaturation and confirming the presence of specific functional groups like the O-H group. The author alternates among possibilities and systematically eliminates options based on newly recalled information and analysis, which is central to the backtracking reasoning strategy. Every time the author faces potential inconsistency or lacks certainty, they pause to reevaluate their conclusions indicating methodical return to earlier ideas to ensure the accuracy of their answer. This iterative process is hallmark of backtracking rather than one-way reasoning stream. Based on this thorough examination, it is evident that the reasoning adheres to the backtracking structure. Thus, the evaluation is yes. F.2 Divergent reasoning strategies from Multiple-Choice vs. Free-Form Training Data Table 16: Divergent reasoning strategies in models trained on multiple-choice versus free-form data formats. This table shows how training data format shapes reasoning strategies when solving identical problems of Arena-Hard benchmark. Multiple-choice trained models exhibit structured problem decomposition with frequent hypothesis generation and multiple restart points, while freeform trained models pursue single hypotheses with continuous verification within one framework. This analysis supports our finding that format characteristics impact reasoning strategies more significantly than problem domain. 37 Question: mother buys set of toys for her two children, Alice and Bob. She has already decided, for each toy, whether it will go to Alice or to Bob, but she has forgotten the actual monetary values of the toys. She only remembers: The toys were arranged in ascending order of value. All prices are non-negative. We say that given assignment is fair if, for every non-decreasing sequence of toy-values 0 v1 v2 vN , the difference between the total value of Alices toys and Bobs toys does not exceed the maximum toy value vN . Formally, let = S1S2 . . . SN be binary string of length , where (cid:26)1, 0, Si = if toy is assigned to Alice, if toy is assigned to Bob. Then is called fair if for every non-decreasing sequence (vi)N i=1 with 0 v1 v2 vN , we have (cid:12) (cid:12) (cid:12) vi [Si = 0] (cid:12) (cid:12) vi [Si = 1] vN , (cid:88) (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) i= i=1 where [P ] = (cid:26)1, if predicate is true, 0, otherwise. Multiple-choice format (MC) trained: Okay, so Im trying to figure out how to determine if given distribution of toys is fair based on the problem statement provided. Let me break down the problem step by step. First, the problem says that there are toys, and theyve been assigned to Alice and Bob in some way specified by binary string S. Each character in represents one of the toys: 1 means Alice gets it, 0 means Bob gets it. The key thing here is that tells us exactly which toys go to whom, but the actual values of these toys arent specified. However, the problem gives us condition that must hold for the distribution to be considered \"fair.\" The fairness condition states that for any possible assignment of toy values (which are non-negative and in ascending order), the absolute difference between the total value of toys Alice receives and the total value Bob receives should not exceed the highest toys value, which is v_N. But Im not entirely sure about that. Let me think about how to approach this. One way to tackle this is to consider the worst-case scenario where the difference between Alices and Bobs totals could be maximized. If even in the best possible worst-case scenario, the difference doesnt exceed v_N, then the distribution is fair. But wait, the problem mentions that the distribution is fair if for every possible array of toy values, the difference doesnt exceed v_N. That seems quite broad because it implies that no matter how the toy values vary (as long as theyre non-decreasing), the difference shouldnt get too big. ... (omitted) ... Let me think differently. ... (omitted) ... need to find different approach. Explanation - **Reading Input**: The code reads the number of test cases and processes each test case individually. - **Counting Toys**: For each distribution, it counts how many toys Alice receives (k). - **Calculating Sums**: It calculates the sum of the values of the top toys and the sum of the values of the bottom - toys. - **Checking Fairness**: It computes the absolute difference between these sums and checks if it is less than or equal to the highest toy value. If it is, the distribution is deemed fair; otherwise, it is not. This approach ensures that we efficiently determine the fairness of the distribution by leveraging the properties of non-decreasing toy values and the constraints imposed by the problem. 38 Free-form (FF) trained: Okay, so Im trying to solve this problem where have to determine if given distribution of toys between Alice and Bob is fair based on certain conditions. Let me try to understand the problem step by step. So, the problem says that distribution is fair if, regardless of how the actual toy prices (which are in strictly increasing order) are assigned, the absolute difference between the sums of Alices toys and Bobs toys doesnt exceed the maximum toy price among them. Wait, more formally, for every possible array of toy prices v_1 <= v_2 <= ... <= v_N, the absolute difference sum_A - sum_B must be <= max_v, where max_v is the largest toy price. Hmm, wait, because the max_v is the last element in the sorted list. So max_v = v_N, since theyre in order. But the condition needs to hold for all possible assignments of v_is. That makes sense because the distribution is fair no matter what the actual prices are so even the worst-case scenario must satisfy the condition. Our task is, given (the binary string indicating which toys go to whom), to decide whether this condition holds for all possible toy price arrays. think the key here is to find out under what circumstances the maximum possible sum_A - sum_B across all possible arrays is less than or equal to v_max. ... (omitted) ... Explanation - **Reading Input**: The input is read in one go for efficiency, especially given the constraints. - **Prefix Sum Calculation**: For each test case, we calculate the prefix sums of the values of toys Alice and Bob receive. Here, the values are assumed to be sequential integers starting from 1. - **Fairness Check**: Using the derived condition, we check if twice the sum of the values of the toys Alice receives is less than or equal to the total sum of all toy values. If this condition holds, the distribution is fair; otherwise, it is not. This approach efficiently determines the fairness of the distribution by leveraging prefix sums and ensuring the solution handles up to the maximum constraints effectively. Pattern analysis for MC: ... The response is more structured and attempts to break down the ... It attempts to break down the problem ... model attempts to provide problem logically. clear step-by-step guide. Model also tends to generate multiple hypotheses, demonstrating an inclination toward broadly exploring potential answers. Pattern analysis for FF: ... response is lengthy and somewhat convoluted. It goes through lot of reasoning but does not arrive at clear solution or code implementation. ... does not provide clear answer to the problem or working code solution. It gets lost in the reasoning without concluding with practical implementation. ... The response is excessively verbose, making it difficult to extract useful information quickly. Model exhibits tendency to pursue answers by continuously verifying hypotheses within main framework Table 17: reasoning strategies in error diagnosis: Multiple-choice versus free-form trained models. This table compares how models trained on different data formats approach technical troubleshooting on question of Arena-Hard benchmark. The multiple-choice trained model demonstrates structured analysis with clear step-by-step reasoning and actionable recommendations, while the free-form trained model exhibits more repetitive pattern with frequent restating of observations. This comparison highlights how training format influences problem-solving strategies in diagnostic contexts. Question: Find root cause for this error: sshd[54785]: error: kex_exchange_identification: Connection closed by remote host Multiple-choice format (MC) trained: Okay, so need to figure out the root cause of the error \"kex_exchange_identification: Connection closed by remote host\" in KEX application. Let me break this down step by step. First, understand that KEX is protocol used in blockchain technologies, often in contexts like Solana, Monero, or others that use zero-knowledge proofs. The error message suggests that when the application tries to connect to KEX, its closing the connection, which happens on remote host. That makes me think about how the application is handling network connections. remember that in many networked applications, especially those involving distributed systems, its crucial to handle connections properly. ... (omitted) ... can consider the possibility of transient network issue causing the connection to be closed. This could include temporary network outages or fluctuations in the network. Implementing retry mechanism or error handling in the application could provide fallback plan in case of unexpected network issues. In summary, the root cause is likely related to the application not handling network connections properly, possibly by not waiting for network confirmation or by not checking the status of the remote host before attempting connection. To fix this, the application should ensure that network status is checked, connections are confirmed before sending requests, and appropriate error handling is in place. ... (omitted) ... By following these steps, the application can better handle network connectivity issues, ensuring that connections are not closed unexpectedly. This approach will involve code adjustments, network status checks, and error handling mechanisms to provide more reliable connection process. Free-form (FF) trained: The error occurs when run the code as follows: So, the error is that kex_exchange_identification is closed by remote host. So, the code is trying to get new exchange (kex) from remote host, but its not working because the connection to the remote host is closed. think that the problem is that the code is not correctly handling the authentication for the exchange. ... (omitted) ... Looking at the code, see that the exchange is being fetched with the exchange parameter. But perhaps the way its being handled is incorrect. Wait, in the code, the exchange is being fetched using exchange = get_kex_exchange(exchange). But in the code, its not passing any headers or authentication parameters. Wait, in the code, the exchange is being fetched without any headers or with the wrong parameters. Pattern analysis for MC: ... provides structured and relevant analysis of the error. It identifies potential causes and offers clear troubleshooting approach, including checking network status, allowing for network confirmation, and implementing error handling ... The response is helpful and concise, directly addressing the users prompt and providing actionable steps to resolve the issue ... Pattern analysis for FF: ... response is largely verbose. It does not provide clear explanation of the error or its potential causes. ... Instead, it seems to get lost in repetitive and unclear narrative about code handling without addressing the actual error message or providing actionable troubleshooting steps."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Cornell University",
        "KAIST AI",
        "LG AI Research",
        "NAVER Search US"
    ]
}