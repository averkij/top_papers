{
    "paper_title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation",
    "authors": [
        "Liang Chen",
        "Sinan Tan",
        "Zefan Cai",
        "Weichu Xie",
        "Haozhe Zhao",
        "Yichi Zhang",
        "Junyang Lin",
        "Jinze Bai",
        "Tianyu Liu",
        "Baobao Chang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression direction, \\textit{model depth}, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening a new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformer's potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in a self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing a spark of vision-language intelligence when trained solely on images. Code, datasets and models are open at https://github.com/chenllliang/DnD-Transformer."
        },
        {
            "title": "Start",
            "content": "DnD-Transformer SPARK OF VISION-LANGUAGE INTELLIGENCE: 2-DIMENSIONAL AUTOREGRESSIVE TRANSFORMER FOR EFFICIENT FINEGRAINED IMAGE GENERATION Liang Chen1, Sinan Tan2, Zefan Cai3, Weichu Xie4, Haozhe Zhao1, Yichi Zhang1 Junyang Lin2, Jinze Bai2, Tianyu Liu2, Baobao Chang1 1Peking University 2Alibaba Group 3University of WisconsinMadison 4Beijing Institute of Technology leo.liang.chen@outlook.com 4 2 0 2 ] . [ 1 2 1 9 1 0 . 0 1 4 2 : r Figure 1: Generations from DnD-Transformers trained on class-conditional ImageNet256256 (a.top) and unconditional arXiv images (a.bottom). Unconditional rich-text image generations by trained diffusion (b.1) and autoregressive model (b.2), where autoregressive model has dominating performance, showing spark of vision-language intelligence after purely training on images. 1 DnD-Transformer"
        },
        {
            "title": "ABSTRACT",
            "content": "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing new autoregression direction, model depth, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformers potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing spark of vision-language intelligence when trained solely on images. Code, datasets and models are open at https://github.com/chenllliang/DnD-Transformer."
        },
        {
            "title": "INTRODUCTION",
            "content": "The field of autoregressive (AR) image generation is experiencing resurgence of interest, largely driven by groundbreaking advancements in large language models (LLMs), exemplified by the release of ChatGPT (OpenAI, 2022). Because typical AR image generation methods also predict output in next-token prediction manner, this resemblance has sparked significant efforts in two main areas: 1) transferring advanced, large-scale training techniques and expertise from LLMs to AR image generation models (Bai et al., 2023; Tian et al., 2024; Sun et al., 2024), and 2) developing truly multimodal foundation models capable of both understanding and generating multimodal information within unified training framework (Lu et al., 2022; 2023; Team, 2024). These developments have the potential to lead to more versatile and powerful multimodal AI systems. review of the development history of AR image generation approaches reveals significant efforts focused on finding better sequential decompositions of images and balancing reconstruction fidelity with prediction difficulty. Early models, like PixelCNN (van den Oord et al., 2016), generated images pixel by pixel. This approach was later enhanced by using vector-quantized variational autoencoders (VQVAEs) to compress images and model the prior distribution of discrete tokens in compact latent space (Van Den Oord et al., 2017). Vector quantization (VQ) paved the way for notable models such as VQGAN (Esser et al., 2021), DALLE (Ramesh et al., 2021), and MUSE (Chang et al., 2023), and it remains core technique in recent AR image generation models like VAR (Tian et al., 2024) and LlamaGen (Sun et al., 2024), and multimodal foundation models like LVM (Bai et al., 2023), Unified-IO (Lu et al., 2022; 2023), and Chameleon (Team, 2024). However, despite advancements in AR image generation, VQ-based autoregressive methods face two persistent criticisms, especially juxtaposed with latent diffusion models (Rombach et al., 2022): 1) Information loss inherent in the quantization process. Quantization, specifically in VQVAE, introduces significant information loss. With typical configuration (N=8192, f=16), the Information Compression Ratio (ICR = log 24f 2 , explained in Equation 1) is just 0.21%, drastically lower than the 8.3% of Stable Diffusions VAE1, hindering fine-grained detail reconstruction. According to Chameleon (Team, 2024), the authors note that their VQ tokenizer struggles to reconstruct finegrained details like text in images, which we believe is due to the low ICR of their tokenizer. 2) Substantially increased computational requirements for producing higher-quality images. According to Equation 1,Increasing ICR by expanding the latent space (N) is logarithmically limited 1The Stable Diffusion VAE (https://huggingface.co/stabilityai/sd-vae-ft-mse) uses downsampling factor (f) of 8 and 4 channels, with fp32 tensor precision (log = 4 32). 2 DnD-Transformer Figure 2: Illustration of the proposed DnD-Transformer. denotes the number of depth autoregression. O-i denotes the transformer layer index for the i-th prediction head. Each transformer layer predicts the corresponding depth code, achieving multi-code prediction within one forward pass. and computationally expensive leading to potential codebook collapse and more embedding parameters, while reducing the downscaling factor (f) significantly increases computational overhead due to longer token sequence of O(1/f 2) and higher transformer computation complexity of O(1/f 4). We draw inspiration from the Residual Quantization method (Lee et al., 2022b), which provides new dimension for sequentially decomposing the image for better generation quality. However, the proposed RQ-Transformer employs two separate transformer models. This structure presents difficulties in integrating current LLMs for end-to-end training. In this work, we aim to solve the problem covering the two mentioned concerns: Can we overcome the information loss of VQ-based AR image generation without increasing overall computation budget in an end-to-end manner? We propose novel paradigm for AR image generation called 2-Dimensional Autoregression (DnD) and DnD-Transformer, an end-to-end model architecture. DnD Autoregression introduces new depth dimension along with the original spatial dimension. In the depth dimension, the image patch could be decomposed in any causal coarse-to-fine order, including the residual decomposition (Lee et al., 2022b), Gaussian denoising decomposition (Ho et al., 2020) and etc. With depth of and other configurations unchanged, the ICR of DnD Autoregression becomes log 24f 2 , more effectively reducing the information loss comparing to increasing the codebook size . The remaining problem is how to predict the times more tokens effectively. We propose the DnDTransformer. As shown in Figure 2, it inserts multiple prediction heads into the backbone transformer decoder model to predict the depth codes and conduct additional autoregressive predictions in each forward process. Different from RQ-Transformer (Lee et al., 2022b), the DnD-Transformer does not require additional modules or increased sequence length, making it applicable to any language model architecture and efficiently generate more fine-grained images. Our experiments show several interesting results: 1. Superior reconstruction of fine-grained image details using residual image decomposition in VQVAEs, disproving VQs limitations with text-rich images DnD-Transformer 2. More efficient and lower-entropy decomposition with DnD autoregression compared to 1D methods, evidenced by lower training cross-entropy loss despite predicting more codes 3. Significant outperformance of the AR baseline on ImageNet 256x256 generation, achieving up to 1.54 FID and 82.6 IS improvements (XXL model, cfg=2) without increased model size or sequence length, even surpassing larger LlamaGen model trained with longer sequence length 4. spark of vision-language intelligence for the first time, enabling unconditional richtext image generation, outperforming diffusion models like DDPM and Stable Diffusion on dedicated rich-text image datasets, highlighting the distinct advantage of autoregressive models for multimodal modeling. 2 2D VISUAL TOKENIZER AND 2D AUTOREGRESSION"
        },
        {
            "title": "2.1 UNDERSTAND VQVAE AS COMPRESSION",
            "content": "We introduce the basics of AR generation in Section in the appendix. We can better understand the reconstruction ability of VQVAE from the lens of compression. Let us assume VQVAE with downscaling factor , codebook size , input images size of , then the shape of the quantized code is = (H/f ) (W/f ). We assume that the code follows uniform distribution, so each code has log bits information. Its information compression ratio (ICR) is as follows. ICR(N, ) = (H/f ) (W/f ) log 3 log 256 = log 24f 2 (1) typical configuration (N=8192, f=16) results in 0.21% ICR. This ICR is significantly lower than JPEGs 5% ICR (Wikipedia). To increase ICR, the 1D AR method could increase (might face the codebook collapse problem (Mentzer et al., 2023) and the improvement is logarithmically bounded) or decrease (more effective, but increases the token count quadratically). 2.2 IMAGES 2D DECOMPOSITION AND QUANTIZATION As pointed out by Equation 1, the information compression ratio of VQVAE is bounded by the size of the codebook and the downscaling ratio. Residual Quantization (Lee et al., 2022b) proposes new direction to quantize the image feature with multiple residual codes to reduce the quantization error and improve the quality of the reconstruction. For feature map having hw vectors, RQVAE uses codes to quantize the feature map, where is the depth dimension of the code. For each feature vector v, RQ finds codes (q1, q2, ..., qd) by sequentially conducting times residual decomposition and quantization operation Q(x) as finding the closest entry to from the codebook: qd = Q(rd1), rd = rd1 qd, r0 = (2) Consequently, the sum of the residual codes (cid:80)d i=1 qi is expected to approximate more closely the feature vector v, thus reducing the quantization error. We generalize this process as two-dimensional autoregression (DnD), which extends beyond Markov residual decomposition and can be applied to any decomposition operation, such as the diffusion process (Ho et al., 2020), etc. DnD Autoregression quantizes 2D feature map Rhwc by decomposing it in two directions. First, is divided into feature vectors. Second, each vector is decomposed into codes (q1, ..., qn) using function Dn(v, Q) based on codebook Q. The resulting quantized map has shape and is predicted in depth-first-spatial-second order. This decomposition could also be non-Markov, unlike RQVAE. The selection of potentially better decomposition functions is left for future exploration. We still use the residual quantization from Equation 2 as Dn. DnD decomposition increases the ICR times (Equation 3), more effectively than increasing codebook size. The remaining challenge of predicting times more codes is addressed by our DnD-Transformer. ICR(N, f, d) = (H/f ) (W/f ) log 3 log 256 = log 24f 2 (3) 4 DnD-Transformer Figure 3: Performance of our visual tokenizers of different depths. The reconstruction of complex features (i.e., eyes, mouse and text) gains significant improvement as the depth increases. Depth ImageNet 256256 rFID L2 Loss Code Usage 1 2 4 8 2.98 0.93 0.60 0.42 SDXL 0.68 0.67 SD3 0.11 0.08 0.05 0. 0.05 0.04 100% 100% 100% 100% - - Text256 Text512 arXiv512 Depth 1 1 2 SDXL SD3 0.15 0.00 0.50 0.80 0.72 0.82 rOCR 0.73 0.00 0.81 0.83 0.83 0. 0.14 0.00 0.49 0.67 0.66 0.74 (a) Reconstruction Performance on ImageNet 256256 Validation Set. (b) Reconstruction OCR Performance. indicates zero-shot tokenizer trained on ImageNet. Table 1: Ablation studies on the reconstruction performance of visual tokenizers. Our trained tokenizers all have = 16 downscaling factor and = 16384 codebook size. 2.3 RECONSTRUCTION PERFORMANCE We evaluate the reconstruction performance of our trained visual tokenizers with varying maximum codebook depths using the standard ImageNet dataset as the benchmark. All images are resized to 256256 resolution. We train the different visual tokenizers using the same training objectives as in Lee et al. (2022b), and assess the reconstruction Frechet Inception Distance (rFID) on the ImageNet validation set using ADMs evaluation suite (Dhariwal & Nichol, 2021). The results are presented in Table 1a. For comparison, we include the rFID from the VAE of SDXL (Podell et al., 2023) and Stable-Diffusion 3 (Esser et al., 2024) . Our findings demonstrate that our trained visual tokenizer achieves an rFID lower than 1 with two or more codebook depths, even surpassing the performance of SD3s continuous VAE with less theoretical information loss. As shown in the example from Figure 3, by increasing code depth, we could reconstruct more fine-grained details in the image. Code Usage. We further analyze the code usage in each codebook layer, with results shown in Figure 4a. The analysis indicates that usage generally decreases as depth increases. This is due to the diminishing diversity of code usage as the residual decomposition progresses deeper, resulting 5 DnD-Transformer (a) Layerwise code usage of visual tokenizers. (b) Code Norm Distribution for Tokeniers Figure 4: Analysis of visual tokenizers. in smaller feature norms and more centralized code usage according to Figure 4b. Interestingly, we do not observe signs of codebook collapse with the DnD visual tokenizers, even when using large codebook size (16384), as mentioned in previous work (Mentzer et al., 2023). While they reported much lower code usage (< 50%), our tokenizer achieves 100% usage across all maximum depths. 2.4 VQVAES CAN PERFECTLY RECONSTRUCT RICH-TEXT IMAGES prevalent criticism of VQVAE has been its alleged intrinsic information loss problem, particularly its inability to reconstruct images with fine details, such as those containing rich text (Team, 2024). However, we argue that this claim is unfounded. Our findings suggest that VQVAE can indeed achieve perfect reconstruction of detailed images, when provided sufficient data and an increased number of codes used to represent each image. This demonstrates that the perceived limitations of VQVAE can be overcome through appropriate data-centric adjustments and model scaling-up. rOCR - New Metric. We proposes rOCR, novel metric for evaluating rich-text image reconstruction. Unlike rFID/L2 Loss, rOCR measures textual recognizability using the Qwen2-VL72B (Wang et al., 2024a) visual language model for OCR. The metric computes the Rouge-L score between recognized and groundtruth text (or original image OCR if groundtruth is unavailable). Experiments and Results. Two rich-text image datasets, Text-Image and arXiv-Image (details in Section 4.1), were used to train visual tokenizers. Performance (rOCR scores) was evaluated on both datasets 1K test sets, compared against ImageNet-trained tokenizers, SDXLs (Podell et al., 2023) and Stable-Diffusion-3s VAE (Esser et al., 2024). Text-Image was also tested at reduced 256256 resolution to assess resolution impacts. Table 1a shows the rOCR results, with reconstruction examples in Figures 3 and 11. Results indicate more training data and deeper tokenizers improve text reconstruction. Unlike Team (2024), our discrete visual tokenizers excel in rich-text image reconstruction even compared to continuous VAEs."
        },
        {
            "title": "3 THE DND-TRANSFORMER",
            "content": "Prior section showed DnD visual tokenizers effectively reconstruct fine details like text. However, efficiently predicting the increased number of depth codes (d times more) remains challenging. Existing methods, like RQ-Transformer, use separate transformer for depth, hindering integration with LLMs. We propose an efficient end-to-end architecture for multi-code prediction. 3.1 DND-TRANSFORMER DESIGN Figure 5 shows DnD-Transformer and its variants: Parallel and Vertical Prediction. Parallel Prediction adds multiple prediction heads for simultaneous multi-depth code prediction, similar to accelerated LLM inference (Cai et al., 2024). However, this ignores the coarse-to-fine nature (Figure 4b) of code distributions, where deeper codes have smaller norms and are more centered. Vertical Prediction addresses this by sequentially predicting codes. Adding autoregression further refines this 6 DnD-Transformer Figure 5: Different explored multi-token prediction architectures for DnD-Transformer, which are all designed to generate multiple codes with one forward pass. by conditioning deeper code predictions on previous ones, achieving the best multi-layer code prediction without increasing model parameters or sequence length. Ablation on the structure design is shown in Table 3 from Appendix. 3.2 IMPLEMENTATION DETAILS As shown in the left part of Figure 5, the increment of DnD-Transformer compared to vanilla transformer decoder is the additional output head and embedding add operation. Lets assume the linearized codemaps length is = and code depth is d. During generation, DnD-Transformer conducts forward process and each forward process generate codes sequentially. After generating codes for all depths in forward process, the embeddings of all codes are added up as the next input token. In this way, the model could generate tokens with only forward passes, improving the generation quality with the same inference cost as standard 1D auto-regression transformer. The only additional hyper-parameter is the layer indexes to predict code of different depths. We adopt the same transformer decoders architecture as LLaMA (Touvron et al., 2023) and , please refer to Appendix for the training details of our DnD-Transformer."
        },
        {
            "title": "4 EXPERIMENTS AND FINDINGS",
            "content": "4.1 TASKS AND DATASETS Class-Conditional Image Generation. We conduct standard conditional image generation task with ImageNet-1k benchmark. Images are resized to 256256 resolution during training and evaluation. We sample 50k images with classes uniformly distributed, and compute the FID, IS, Precision and Recall aganist the training set data using the ADM evaluation tool Dhariwal & Nichol (2021). Unconditional Rich-Text Image Generation. We collect two datasets for this task. Dataset examples are shown in Figure 6. Models are trained in unconditional setting in this task. We aim to explore whether the tested vision generation models could understand and generate the complex logical interrelation among the generated elements such as language. 1. Pure Text Images (Text-Image). The dataset is automatically rendered from portion of English wikipedia (Foundation), consisting of 2.4M images. Each image has original resolution of 512512 and font size of 32pt. We set maximum of 100 words in each image with paddling margin of 20pt. We use the PILLOW library to render the image. 2. arXiv Images (arXiv-Image) we first download the papers in PDF format from arXiv. org, and render the pages to image of A4 resolution (1260 1782) with PDF2IMAGE tool. We then randomly crop ten 512512 image from each pages and finally collect 2M images. We have developed an evaluation pipeline that combines Optical Character Recognition (OCR) and Perplexity Measurement for assessing the quality of generated images, with focus on the textual Initially, we employ the state-of-the-art open-source Vision-Language information they contain. DnD-Transformer Figure 6: Data examples in of the collected Text-Image and arXiv-Image image datasets. Model, Qwen2-VL-72B, to extract text from the generated images. Subsequently, we utilize the Qwen2.5-72B model to calculate the perplexity of the generated text, where the LLM is regraded as the evaluator. The resulted score is called Locr, we also test the score of groundtruth data from the training images as the performance upper-bound. 4.2 MODELS Visual Tokenizers. We train our visual tokenizer based on RQVAEs (Lee et al., 2022b). We train tokenizers with code depths of {1, 2, 4, 8} and scaling factor = 16 across different experiments. We choose the checkpoint with best rFID across 150 epochs. Performance comparison of different visual tokenizers is shown in Table 1. We follow Lee et al. (2022b) to train the visual tokenizers. Details of the training of visual tokenizers are listed in Appendix B. Reconstruction performance of the trained visual tokenizers is shown in Table 1. DnD-Transformer. We train two size of DnD-Transformers across our experiment, namely DnDTransformer-XXL (1.4B) and DnD-Transformer-XXXL (2.5B). Basically, DnD-Transformer inherits the LLaMA (Touvron et al., 2023) architecture. The XXL version strictly align with the LlamaGen-XXL baseline to be fairly compared. Details of the model are shown in Appendix E. Implemented Baselines for Class-Conditional Image Generation. LlamaGen (Sun et al., 2024) is the major baseline and state-of-the-art model for AR image generation on ImageNet. Our implemented code primarily refers to the same training codebase for fair comparison. LlamaGen could be also viewed as special version of DnD-Transformer where the decomposition depth equals to 1. Implemented Baselines for Rich-Text Image Generation. We select multiple diffusion models as the baselines, including DDPM (Ho et al., 2020), Stable Diffusion XL (SDXL) (Podell et al., 2023) and Stable Diffusion v3.0 (SD3) (Esser et al., 2024). For DDPM, we train the model on the dataset from scratch. For SDXL and SD3, we finetune the checkpoints from the official website. 4.3 RESULTS OF CLASS-CONDITIONAL IMAGE GENERATION As demonstrated in Table 2, our DnD-Transformer significantly outperforms the 1D autoregressive baseline LlamenGen across various scales and generation evaluation metrics, including FID and IS. This superior performance is achieved while maintaining the same number of parameters in the backbone model, based on our reported and implemented results. It is noteworthy that our 2.5B model, trained with sequence length of 256, even outperforms the 3.1B LlamaGen model, which was trained with much longer image sequence length of 576. This result demonstrates that the DnDTransformer can effectively predict greater number of tokens within shorter sequence length, highlighting its significant potential to revolutionize the one-dimensional autoregressive paradigm. We randomly sample some generation results as shown in Figure 1 and compare the generation performance with 1D-AR in Figure 12,13 and 14 from the Appendix. The comparative analysis clearly illustrates the effectiveness of our approach to generate high-quality images. 8 DnD-Transformer Type Model #Para. FID IS Precision Recall Diffusion-Reported ADM (Dhariwal & Nichol, 2021) CDM (Ho et al., 2022) LDM-4 (Rombach et al., 2022) DiT-XL/2 (Peebles & Xie, 2023) AR-Reported VQGAN (Esser et al., 2021) RQTransformer (Lee et al., 2022a) LlamaGen-XXL (cfg=2) (Sun et al., 2024) LlamaGen-XXL (384384, cfg=2) (Sun et al., 2024) LlamaGen-3B (cfg=2) (Sun et al., 2024) LlamaGen-3B (384384, cfg=2) (Sun et al., 2024) AR-Implemented LlamaGen-XXL (cfg=4) LlamaGen-XXL (cfg=2) DnD-Transformer-XXL (cfg=4) DnD-Transformer-XXL (cfg=2) DnD-Transformer-XXL (cfg=1.7) DnD-Transformer-XXL (cfg=1.5) DnD-Transformer-XXXL (cfg=4) DnD-Transformer-XXXL (cfg=2) DnD-Transformer-XXXL (cfg=1.7) DnD-Transformer-XXXL (cfg=1.5) 554M 10.94 4.88 400M 3.60 675M 2.27 1.4B 3.8B 1.4B 1.4B 3.1B 3.1B 1.4B 1.4B 1.4B 1.4B 1.4B 1.4B 2.5B 2.5B 2.5B 2.5B 5.20 7.55 3.64 2.52 4.21 2.81 7.67 4.12 6.55 2.58 2.78 2.96 6.48 2.77 2.21 2. 101.0 158.7 247.7 278.2 280.3 134.0 296.5 295.4 325.2 311.6 345.1 266.9 427.7 295.6 239.2 232.5 413.0 319.1 279.3 244.2 0.69 0.83 0.86 0.84 0.87 0.84 0.89 0.83 0.89 0.83 0.82 0.80 0.89 0.85 0.83 0. 0.63 0.57 0.51 0.56 0.49 0.54 0.35 0.49 0.42 0.56 0.56 0.57 0.42 0.54 0.58 0.59 Table 2: Model comparisons on class-conditional ImageNet 256256 benchmark. The Reported results refer to Sun et al. (2024). The Implemented results are conducted in this work. indicates that the model is unorthodoxly trained at 384384 resolution, which requires 2.25 times longer sequence length compared to our implemented models. cfg means the scale of classifierfree guidance. The number of depth autoregression is 2 for DnD-Transformers. (a) Comparison of FIDs along training. (b) Sampling Locr on Text-Image along training. 4.4 RESULTS OF RICH-TEXT IMAGE GENERATION Figure 7: Curves during training. Generation Results on Text-Image. DnD-Transformer (depth 1) and DDPM model were trained on the same text-image dataset. Comparing 250 randomly sampled images from each, the AR model significantly outperformed the diffusion model in generating coherent text (lower OCR perplexity 7b; Generation examples 1, 16, 17, 18 and 19 ). This suggests the AR models discrete token reconstruction enables effective autoregressive modeling. We also find that with lower sampling temperature, the model would generate text images with lower PPL just like LLMs. Conversely, the diffusion models simultaneous generation hinders text coherence. Generation Results on arXiv-Image. An 8-layer visual tokenizer and corresponding DnDTransformer trained on arXiv-Image outperformed diffusion model baselines, generating more valid words and phrases (Figure 8). However, arXiv-Image generation lagged behind Text-Image generation, suggesting joint language and figure modeling is more challenging. More results and baselines are in Figure 15 and 20. While SD3s VAE reconstructs arXiv images well (Table 1b), its generative performance is inferior to DDPM and AR, suggesting its latent space is less suitable for language modeling comparing to pixel or discrete space. Spark of Vision-Language Intelligence. Autoregressive (AR) image generation exhibits marked advantage over diffusion models in producing text-rich images, as demonstrated by our results. The pixel-level language generation inherent to AR models facilitates this capability. DeDnD-Transformer Figure 8: Comparison of Unconditional Rich-Text Image Generation on the more complex arXivImage dataset. SD3 is hard to generate valid words, while DnD-Transformer demonstrates an ability to generate semantically appropriate phrases, as marked in blue. More baselines are in Figure 15. (a) Training Loss for DnD-Transformer trained with different number of prediction heads. (b) Training Loss when trained on different domain datasets. Figure 9: Analysis of code depths and domains during training DnD-Transformers. spite limitations imposed by our current training data and model size (preventing direct comparison with large language models), these findings suggest promising pathway towards vision-language intelligence where language understanding emerges directly from visual perception. Furthermore, our pure image learners display behaviors mirroring language model issues such as repetition and hallucination (Figure 10), implying the potential for integrating pure language modeling into unified autoregressive framework for joint vision-language image modeling. 4.5 TRAINING BECOMES EASIER WHEN PREDICTING MULTIPLE CODES, SAMPLING NOT Deeper DnD-Transformer codes achieve lower cross-entropy loss during training (Figure 9a), indicating lower entropy image decompositions. However, despite this, increased depth doesnt improve ImageNet generation fidelity, possibly due to the larger sampling space. Exploring this multi-depth sampling space for better generation is promising research direction. 4.6 AR TRAINING LOSS FOR DIFFERENT DOMAINS ALIGN WITH INNER RANDOMNESS Training loss for the same DnD-Transformer varies significantly across datasets (Figure 9b), being notably higher for ImageNet than rich-text images. While rich-text image loss nears that of LLMs, ImageNet loss sits between text and natural image datasets. The AR models LLM-like training suggests it learns language from visual input alone, implying languages visual representation has lower entropy than natural images, easing the learning process. 10 DnD-Transformer Figure 10: Some cases of the generated text images. We witness similar error pattern (marked in red) to LLMs such as repetition and hallucination in our trained model during sampling."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Image Generation with VQVAE. The vector quantization (VQ) method has been pivotal in the development of generative models (Ramesh et al., 2021; Yu et al., 2022; Chang et al., 2023), which achieve image generation through the prediction of discrete image tokens. Efforts in this area focus on two main directions: the optimization of image tokenization techniques (Esser et al., 2021; Mentzer et al., 2023; Yu et al., 2023; 2024; Weber et al., 2024), and the strategic planning of effective decompositions of image tokens, such as MaskGit (Chang et al., 2022) and VAR (Tian et al., 2024). Meanwhile, alongside the advancement of large language models, there is growing interest in autoregressive image generation, which predicts image tokens sequentially (Tian et al., 2024; Sun et al., 2024). Recent research has also focused on developing multimodal foundation models (Lu et al., 2023; Kondratyuk et al., 2024; Wang et al., 2024b) that integrate both understanding and autoregressive image generation capabilities. They typically convert images or videos into sequences of discretized tokens and train over combined text-image/video token sequences within the AR modeling framework (Lu et al., 2022; Bai et al., 2023; Xie et al., 2024; Team, 2024). However, these models often struggle with inherent information loss during the image quantization and the significantly increased computational demands when generating higher-quality images. The DnDTransformer that adopts the residual 2D decomposition of image features does not require additional modules or increased sequence length for high-quality and fine-grained image generation. Rich-Text Image Generation. Despite recent significant progress in image generation, the task of rich-text generation within images remains persistent challenge (Chen et al., 2023b; Ma et al., 2024; OpenAI, 2024). Most advancements have been witnessed in diffusion models (Betker et al., 2023; Saharia et al., 2022b;a), these models either leverage large language models to enhance the character spelling capabilities of generative models (Saharia et al., 2022b; Balaji et al., 2023; Saharia et al., 2022a) or attempt to explicitly control the position and content of the text using additional supervision from different modules (Tuo et al., 2024; Yang et al., 2023; Liu et al., 2024). However, most diffusion-based methods have primarily focused on text rendering Chen et al. (2023a;b); Balaji et al. (2023); Saharia et al. (2022a) in image generation, often limited to generating short words for logos and posters (Yang et al., 2023; Ma et al., 2023; 2024). The full potential of rich-text image generation remains largely unexplored. Our methods, which build on the foundation of DnD Autoregression, show substantial progress in generating rich-text images in an unconditional manner, highlighting the feasibility of conducting joint vision-language modeling tasks using purely images."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper investigated the limitations of autoregressive (AR) image generation methods, particularly the information loss and computational burden associated with vector quantization (VQ). We introduced 2-Dimensional Autoregression (DnD) and novel end-to-end architecture, DnDTransformer, which leverages depth dimension autoregression alongside the spatial dimension to mitigate these limitations. Our experiments demonstrate that DnD-Transformer achieves significant 11 DnD-Transformer improvements in image quality, outperforming strong baselines like LlamaGen without increasing model size or sequence length. Notably, DnD-Transformer showcases emergent vision-language intelligence, generating text-rich images unconditionally, known weakness of diffusion models. These findings highlight the potential of DnD for efficient and high-quality AR image generation and underscore the promise of this approach for advancing multimodal foundation models. 12 DnD-Transformer"
        },
        {
            "title": "REFERENCES",
            "content": "Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei Efros. Sequential modeling enables scalable learning for large vision models. arXiv preprint arXiv:2312.00785, 2023. Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers, 2023. URL https://arxiv.org/abs/2211.01324. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads, 2024. URL https://arxiv.org/abs/2401.10774. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters, 2023a. URL https://arxiv.org/abs/2305.10855. Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser-2: Unleashing the power of language models for text rendering, 2023b. URL https://arxiv. org/abs/2311.16465. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. Wikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. The Journal of Machine Learning Research, 23(1):22492281, 2022. Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Josh Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David A. Ross, Bryan Seybold, and Lu Jiang. Videopoet: large language model for zero-shot video generation, 2024. URL https://arxiv.org/abs/2312.14125. 13 DnD-Transformer Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1152311532, 2022a. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization, 2022b. URL https://arxiv.org/abs/2203. 01941. Zeyu Liu, Weicong Liang, Zhanhao Liang, Chong Luo, Ji Li, Gao Huang, and Yuhui Yuan. Glyphbyt5: customized text encoder for accurate visual text rendering, 2024. URL https:// arxiv.org/abs/2403.09622. Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. arXiv preprint language, and multi-modal tasks. Unified-io: unified model for vision, arXiv:2206.08916, 2022. Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action. arXiv preprint arXiv:2312.17172, 2023. Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, and Xiaodong Lin. Glyphdraw: Seamlessly rendering text with intricate spatial structures in text-to-image generation, 2023. URL https://arxiv.org/abs/2303.17870. Jian Ma, Yonglin Deng, Chen Chen, Haonan Lu, and Zhenyu Yang. Glyphdraw2: Automatic generation of complex glyph posters with diffusion models and large language models, 2024. URL https://arxiv.org/abs/2407.02252. Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple. arXiv preprint arXiv:2309.15505, 2023. OpenAI. Chatgpt. https://openai.com/blog/chatgpt, 2022. OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. Accessed: [01-10-2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. article, 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 88218831. PMLR, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022a. URL https://arxiv.org/abs/ 2205.11487. DnD-Transformer Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022b. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation, 2024. URL https: //arxiv.org/abs/2406.06525. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. Anytext: Multilingual visual text generation and editing, 2024. URL https://arxiv.org/abs/2311. 03054. Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional image generation with pixelcnn decoders, 2016. URL https:// arxiv.org/abs/1606.05328. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024a. URL https://arxiv.org/abs/2409. 12191. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need, 2024b. URL https://arxiv.org/abs/2409.18869. Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and LiangChieh Chen. Maskbit: Embedding-free image generation via bit tokens, 2024. URL https: //arxiv.org/abs/2409.16211. Wikipedia. URL https://en.wikipedia.org/wiki/Image_compression. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation, 2024. URL https://arxiv.org/abs/ 2408.12528. Yukang Yang, Dongnan Gui, Yuhui Yuan, Weicong Liang, Haisong Ding, Han Hu, and Kai Chen. Glyphcontrol: Glyph conditional control for visual text generation, 2023. URL https: //arxiv.org/abs/2305.18259. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for contentrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. 15 DnD-Transformer Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusion tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation, 2024. URL https://arxiv. org/abs/2406.07550. PRELIMINARY: AUTOREGRESSIVE IMAGE GENERATION In this section, we introduce the fundamentals of autoregressive image generation. The pipeline is rooted in the Vector Quantized Variational Autoencoder (VQVAE) (Van Den Oord et al., 2017) and the autoregressive Transformer (Vaswani et al., 2017). This approach has been adopted from the early DALLE (Ramesh et al., 2021) to the latest LlamaGen (Sun et al., 2024). A.1 STEP1: TRAIN THE VISUAL TOKENIZER AND TOKENIZE THE IMAGES Images initially exist in the pixel-level RGB color space, which consists of little semantic information and makes it challenging to directly model prior knowledge. For example, an image with resolution of 256 256 comprises 256 256 3 = 196, 608 distinct values, representing the individual red, green, and blue intensities for each pixel. The large sequence length makes it difficult to train in autoregressive manner similar to language models technique. Van Den Oord et al. (2017) proposed the Vector Quantized Variational Autoencoder (VQVAE), which significantly alleviates the problem. It downscales and tokenizes the image from the original sparse RGB space into dense and discrete representational space (codebook) by finding the nearest entry. The VQVAE is typically implemented in an encoder-decoder architecture, with its primary training objective being to minimize the image reconstruction loss. You could refer to Van Den Oord et al. (2017) for details in training standard VQVAE. A.2 STEP2: LEARN THE PRIOR DISTRIBUTION OF IMAGE TOKENS Having tokenized the source images into discrete tokens and trained visual decoder to map these tokens back to real images, the next crucial step is to learn the prior distribution of the discrete tokens. This distribution enables the sampling process, which is essential for generating new images. AR Image generation generally first linearizes the image tokens in raster scan order and formalize 1D sequence (q1, q2, q3, ..., qhw) for the transformer (Vaswani et al., 2017) model to learn. During training, the training objective is the same as GPTs next token prediction task (Radford et al., 2018), that the model is required to predict the next image token given the previous tokens and class or text conditional tokens (cid:81)hw t=1 (qt q<t, c). After training, we can generate images by autoregressively sampling tokens from the model. The sampled 1D sequence of image tokens is then reshaped to 2D code map with height and width w. This reshaped token map is subsequently fed into the trained VQVAE decoder, which reconstructs the final image from the code representation. Classifier-Free Guidance As technique to enhance the visual quality and text-image alignment, classifier-free guidance (Ho & Salimans, 2022) has been adopted across the diffusion models (Rombach et al., 2022; Podell et al., 2023), VQ models (Chang et al., 2023) and autoregressive models (Sun et al., 2024) for image generation. During the training, the model is exposed to data with and without conditioning: the conditioning is randomly discarded from fraction of the training samples. We have implemented this approach in our model as well. Specifically, during training, we randomly replace the conditional embedding with learnable unconditional embedding in 10% of the cases. At the inference stage, the logits ℓg are recalculated for each generated token. We form the ℓg by subtracting the unconditional logits ℓu by conditional logits ℓc with the guidance scale through the following equation: ℓg = ℓu + (ℓc ℓu) (4) 16 DnD-Transformer"
        },
        {
            "title": "B TRAINING DETAILS OF VISUAL TOKENIZERS",
            "content": "We follow (Lee et al., 2022b) to train the 2D tokenizers with residual decomposition combined objective of l2 loss, GAN loss and perceptual loss. Codes from different depth share the same codebook. We train all tokenizers fixed learning rate of 4e-5, total batch-size of 256 for 100 epochs and select the one with lowest validation loss as the final tokenizers. We conduct all training on 8A100 GPUs."
        },
        {
            "title": "C RECONSTRUCTION RESULTS OF TEXTS",
            "content": "Figure 11 shows the reconstruction result on arXiv images of different visual tokenizers. Figure 11: Reconstruction Results of Texts. With training data and enough depths of codes, RQ visual tokenizers can well reconstruct the text in the images. ABLATION ON DND-TRANSFORMERS STRUCTURE Model Parameters FID IS Precison Recall 1D 2D Parallel 2D Vertical DnD-Transformer 1.4B 1.4B 1.4B 1.4B 4.12 6.32 3.18 2.58 266.9 232.1 289.7 295.6 0.83 0.79 0.83 0. 0.49 0.44 0.57 0.56 Table 3: Ablation of DnD-Transformer Architecture on ImageNet dataset. All models follow the same training setting as in Appendix E. DETAILS OF HYPER-PARAMETERS OF DND-TRANSFORMER Table 4 shows the hyper-parameters of our trained models. The XXL model has the same setting as in GPT2 (Radford et al., 2019) and LlamaGen (Sun et al., 2024) for fair comparisons. For DnD-Transformer with multiple prediction heads, the prediction layers indexes are set to [39, 48] when there are two heads, [39, 42, 45, 48] when there are 4 heads in the ImageNet experiments, [27, 30, 33, 36, 39, 42, 45, 48] when there are 8 heads in the arXiv-Image experiments. All transformer models were trained using settings similar to LlamaGen (Sun et al., 2024): base learning rate of 104 per 256 batch size, the AdamW optimizer with β1 = 0.9, β2 = 0.95, and weight decay of 0.05, along with gradient clipping at 1.0. dropout of 0.1 was consistently applied to the input token embedding, attention module, and feed-forward network (FFN) module. Similarly, dropout of 0.1 was used for the class condition embedding for classifier-free guidance. Training was performed for 300 epochs, and the final checkpoint was used for performance evaluation. DnD-Transformer Model Parameters Layers Hidden Size Heads"
        },
        {
            "title": "XXL\nXXXL",
            "content": "1.4B 2.5B 48 48 1536 2048 24 32 Table 4: Model sizes and architecture configurations GENERATION RESULTS OF DND-TRANSFORMERS Figure 12: Conditional generation comparisons between LlamaGen-XXL and DnD-TransformerXXL on class golden retriever from ImageNet. We random sampled 16 images with cfg=4. DnDTransformer generates images with higher quality than the 1D AR model. 18 DnD-Transformer Figure 13: Conditional generation comparisons between LlamaGen-XXL and DnD-TransformerXXL on class volcano from ImageNet. We random sampled 16 images with cfg=4. DnDTransformer generates images with higher quality than the 1D AR model. Figure 14: Conditional generation comparisons between LlamaGen-XXL and DnD-TransformerXXL on class husky from ImageNet. We random sampled 16 images with cfg=4. DnDTransformer generates images with higher quality than the 1D AR model especially for the more complex eyes of husky. DnD-Transformer Figure 15: Comparison of Unconditional Rich-Text Image Generation on the more complex arXivImage dataset. All models are trained on the same dataset. The generated images are all in 256x256 resolution. Diffusion-Family models are hard to generate valid words, while DnD-Transformer demonstrates an ability to generate semantically appropriate phrases, as evidenced by the correct clause it should be observed in the second example. 20 DnD-Transformer Figure 16: Unconditional Generation examples of DDPM on Image-Text. DnD-Transformer Figure 17: Unconditional Generation examples of DnD-Transformer on Image-Text with temperature=0.1. 22 DnD-Transformer Figure 18: Unconditional Generation examples of DnD-Transformer on Image-Text with temperature=0.5. DnD-Transformer Figure 19: Unconditional Generation examples of DnD-Transformer on Image-Text with temperature=1.0. 24 DnD-Transformer Figure 20: Unconditional Generation examples of DnD-Transformer on arXiv data with temperature=1."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Beijing Institute of Technology",
        "Peking University",
        "University of Wisconsin-Madison"
    ]
}