{
    "paper_title": "DeepResearchGym: A Free, Transparent, and Reproducible Evaluation Sandbox for Deep Research",
    "authors": [
        "João Coelho",
        "Jingjie Ning",
        "Jingyuan He",
        "Kangrui Mao",
        "Abhijay Paladugu",
        "Pranav Setlur",
        "Jiahe Jin",
        "Jamie Callan",
        "João Magalhães",
        "Bruno Martins",
        "Chenyan Xiong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep research systems represent an emerging class of agentic information retrieval methods that generate comprehensive and well-supported reports to complex queries. However, most existing frameworks rely on dynamic commercial search APIs, which pose reproducibility and transparency challenges in addition to their cost. To address these limitations, we introduce DeepResearchGym, an open-source sandbox that combines a reproducible search API with a rigorous evaluation protocol for benchmarking deep research systems. The API indexes large-scale public web corpora, namely ClueWeb22 and FineWeb, using a state-of-the-art dense retriever and approximate nearest neighbor search via DiskANN. It achieves lower latency than popular commercial APIs while ensuring stable document rankings across runs, and is freely available for research use. To evaluate deep research systems' outputs, we extend the Researchy Questions benchmark with automatic metrics through LLM-as-a-judge assessments to measure alignment with users' information needs, retrieval faithfulness, and report quality. Experimental results show that systems integrated with DeepResearchGym achieve performance comparable to those using commercial APIs, with performance rankings remaining consistent across evaluation metrics. A human evaluation study further confirms that our automatic protocol aligns with human preferences, validating the framework's ability to help support controlled assessment of deep research systems. Our code and API documentation are available at https://www.deepresearchgym.ai."
        },
        {
            "title": "Start",
            "content": "DeepResearchGym: Free, Transparent, and Reproducible Evaluation Sandbox for Deep Research João Coelho1,3 Kangrui Mao1 Abhijay Paladugu1 Jingjie Ning1 Jingyuan He1 Pranav Setlur1 Jiahe Jin 5 2 0 2 5 2 ] . [ 1 3 5 2 9 1 . 5 0 5 2 : r Jamie Callan1 João Magalhães2 Bruno Martins3 Chenyan Xiong1 1Carnegie Mellon University 2NOVA LINCS 3IST and INESC-ID {jmcoelho,jening,jingyuah,karrym,apaladug,psetlur,jjiahe}@andrew.cmu.edu {callan,cx}@cs.cmu.edu jmag@fct.unl.pt bruno.g.martins@tecnico.ulisboa.pt"
        },
        {
            "title": "Abstract",
            "content": "Deep research systems represent an emerging class of agentic information retrieval methods that generate comprehensive and well-supported reports to complex queries. However, most existing frameworks rely on dynamic commercial search APIs, which pose reproducibility and transparency challenges in addition to their cost. To address these limitations, we introduce DeepResearchGym, an opensource sandbox that combines reproducible search API with rigorous evaluation protocol for benchmarking deep research systems. The API indexes large-scale public web corpora, namely ClueWeb22 and FineWeb, using state-of-the-art dense retriever and approximate nearest neighbor search via DiskANN. It achieves lower latency than popular commercial APIs while ensuring stable document rankings across runs, and is freely available for research use. To evaluate deep research systems outputs, we extend the Researchy Questions benchmark with automatic metrics through LLM-as-a-judge assessments to measure alignment with users information needs, retrieval faithfulness, and report quality. Experimental results show that systems integrated with DeepResearchGym achieve performance comparable to those using commercial APIs, with performance rankings remaining consistent across evaluation metrics. human evaluation study further confirms that our automatic protocol aligns with human preferences, validating the frameworks ability to help support controlled assessment of deep research systems. Our code and API documentation are available at https://www.deepresearchgym.ai."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in Large Language Models (LLMs) have driven transformation in information access paradigms, moving beyond ranked retrieval toward systems capable of synthesizing comprehensive report-style responses to complex queries. These deep research systems aim to address complex and open-ended information needs, combining iterative retrieval with multi-step reasoning and generation, autonomously navigating and evaluating diverse sources to construct well-supported reports. Prominent commercial examples include OpenAI [19] and Perplexity [24] deep research modes, which have demonstrated how these systems can significantly enhance user experience when addressing intricate questions requiring synthesis across multiple perspectives, domains, and sources. Recent industry developments further underscore this shift in information access technology, with Google moving towards AI-driven search tools [27], and Apple announcing plans to integrate services such as OpenAI and Perplexity into its Safari browser [7]. Preprint. As deep research systems gain prominence, they introduce novel evaluation challenges. Being agentic by design, these systems rely on iterative search, retrieval, and reasoning over vast collections of online data, making meaningful evaluation dependent on access to environments with broad and diverse coverage that faithfully simulate real-world behavior. Yet, such infrastructures remain scarce, as efficient and stable large-scale retrieval tools are largely unavailable to the research community, forcing reliance on commercial web search APIs. While convenient, these APIs introduce critical limitations: their proprietary nature restricts transparency in the retrieval processes, hindering research on search itself, and their continuous evolution undermines reproducibility and fair benchmarking. To address these challenges, we introduce DeepResearchGym as an open-source benchmarking framework specifically designed to enable transparent and reproducible evaluation of deep research systems. At the core of our framework is free and open-source search API built upon public web snapshots comprising millions of documents, such as ClueWeb22 [22] and FineWeb [23]. This API exposes standardized endpoints for both document retrieval and content access, enabling integration with long-form generation pipelines. Our search infrastructure design emphasizes transparency and reproducibility, aiming to support realistic search behavior without the variability introduced by commercial services. The retrieval pipeline consists of publicly available components, including the document collections, state-of-theart embedding model, and scalable approximate nearest neighbour search index. This setup allows researchers to audit system behavior, analyze the influence of retrieved evidence, and rerun deep research experiments under reproducible search conditions, since retrieval results remain stable over time. We provide code to support local deployment of DeepResearchGyms infrastructure, supporting full pipeline reproducibility, as well as experiments using diferent retrieval models and/or document collections. Empirical retrieval evaluations show that the system achieves strong retrieval quality with minimal loss from approximate search, while maintaining response times below those attained by commercial APIs. Furthermore, DeepResearchGym includes comprehensive evaluation protocol designed to assess deep research systems in realistic, challenging scenarios. We build upon the Researchy Questions dataset [28], which was initially created as retrieval benchmark curated from commercial search logs. This dataset represents complex, high-engagement non-factoid queries, making it an interesting testbed for deep research systems. Our evaluation extension shifts the focus from assessing retrieval effectiveness to evaluating the quality of deep research systems responses. We employ an LLM-asa-judge methodology [6] to automatically assess responses along critical dimensions that capture essential qualities of effective deep research reports, leveraging Researchy Questions ground-truth documents to provide more reliable judgments. These dimensions include alignment with user information needs, factual grounding, and overall report quality. To empirically ground our framework, we apply DeepResearchGyms evaluation protocol to assess diverse set of commercial and open-source deep research systems. Our findings highlight two key insights: first, systems maintain performance across evaluation metrics when integrated with DeepResearchGyms search API, indicating that the our infrastructure maintains report quality on par with commercial search setups. Second, the results highlight that comprehensive coverage of user information needs is the most challenging perspective, indicating room for improvement in how current systems address complex, multi-faceted queries. Together, the results support DeepResearchGym as promising sandbox environment for advancing research in deep information synthesis."
        },
        {
            "title": "2 Related Work",
            "content": "Early work on Retrieval-Augmented Generation (RAG) systems focused on improving performance on knowledge-intensive question answering by retrieving supporting documents from large corpora and conditioning generation on this evidence to enhance factual accuracy [13, 37, 43]. Building on this foundation, several deep research systems have been optimized for short-form factoid-style answering. These include reinforcement learning approaches that enable search agents to autonomously navigate the web, issue iterative queries, and synthesize concise responses [10, 33, 42], as well as prompt-based methods like Search-o1 [14], which equips LLMs with the ability to trigger web searches when encountering knowledge gaps, leveraging the collected evidence to guide synthesis. While effective for short-form question answering, these approaches are not designed to support the generation of detailed reports that require broader synthesis, reasoning, and integration across multiple sources [20]. 2 complementary line of work has advanced towards comprehensive long-form report generation frameworks. GPTResearcher [5] orchestrates multi-agent workflows to coordinate planning, retrieval, and drafting across hybrid data sources, incorporating techniques such as report planning [38] and query decomposition [4] to enhance long-form synthesis, while maintaining coherence and completeness. Building on these paradigms, other deep research systems emphasize agentic tool use to extend reasoning capabilities beyond pure text-based retrieval. For instance, OpenDeepSearch [1] implements two agentic variants: one that follows an action-observation cycle, allowing the model to iteratively query external resources and refine its reasoning; and another that augments this by generating and executing Python scripts for more complex computational tasks. Agentic Reasoning [40] similarly combines multi-agent collaboration with code execution, contextual memory, and dynamic knowledge-graph construction via dedicated mind-map agent, enabling structured exploration of complex problems. HuggingFaces OpenDeepResearch initiative [9] follows similar directions in an open-source framework, integrating code agents that perform advanced retrieval, analysis, and reasoning through Python execution, while emphasizing transparency and modularity. common limitation across these systems is their reliance on commercial web search APIs such as Tavily [35] and SERPer [32] for document retrieval. These APIs provide limited transparency into document indexing and ranking, are subject to dynamic changes over time, and restrict researchers ability to fully control or replicate retrieval conditions, posing challenges for reproducibility and fair evaluation. In parallel, existing evaluation protocols have primarily focused on related but distinct tasks. For example, widely used benchmarks such as GAIA [16], HLE [25], and FRAMES [12] were designed to assess capabilities in expert-level short-form question answering. While these benchmarks have significantly advanced evaluation methodologies, they are not tailored to the long-form synthesis and structured reporting objectives of deep research systems. Beyond these benchmarks, efforts have been made to attain more targeted methodologies for evaluating long-form generation. FACTScore [17] and SAFE [39] decompose outputs into atomic claims and verify their factual consistency against external sources. For retrieval-augmented systems, ARES [30] and RAGChecker [29] offer modular evaluations that explicitly link generated claims to retrieved evidence, providing fine-grained diagnostics of relevance and faithfulness. Long2RAG [26] extends this approach by introducing Key Point Recall (KPR), which evaluates how well long-form answers capture essential content from retrieved sources by measuring coverage of salient points."
        },
        {
            "title": "3 DeepResearchGym",
            "content": "This section presents DeepResearchGym as an open-source framework designed to support reproducible research on deep research systems. To address the challenges related to the reliance on commercial web search APIs, DeepResearchGym offers controlled sandbox environment built on large-scale web corpora. It provides state-of-the-art retrieval API, alongside an evaluation protocol that extends the Researchy Questions dataset to measure long-form report quality. 3.1 Search Sandbox This subsection introduces our search API, designed to enable reproducible retrieval for deep research systems. We begin by describing the underlying web corpora, followed by an overview of the dense retriever and the ANN indexing approach used to enable efficient search. Finally, we outline the API interface, including available endpoints, supported arguments, and response format. 3.1.1 Web Corpora DeepResearchGym currently indexes two major large-scale web datasets, namely the English subset of ClueWeb22-B [22], and the FineWeb CC-MAIN-2024-51 snapshot [23], which also contains only English documents. ClueWeb22 was collected in 2022 and comprises approximately 10 billion web pages. It is organized into three categories, each representing different segments of the web. Category B, known as ClueWeb22-B, approximates the super head of the web, encompassing the most frequently visited pages (e.g., pages from Wikipedia, major news outlets, and other top domains). It includes around 200 million web pages, with approximately 87 million in English. These pages were sampled based on their likelihood to satisfy user information needs, as estimated by commercial search engines 3 importance scoring. Low-quality and spam pages were filtered during sampling to enhance the datasets overall quality. This subset is particularly valuable for systems that aim to balance broad coverage with high precision, reflecting realistic conditions encountered in commercial web search. FineWeb is large-scale English web corpus collected from 96 Common Crawl snapshots between 2013 and 2024. It comprises approximately 15 trillion tokens of cleaned and deduplicated web data. The dataset employs rigorous filtering, deduplication, and quality control measures, resulting in high-quality resource for LLM training. We focus on the most recent crawl, from week 51 of 2024, which includes over 180 million documents, capturing more recent trends compared to earlier data. This makes the collection particularly valuable for queries that require up-to-date information, reflecting the evolving nature of web content and user interests. By indexing these corpora independently, DeepResearchGym provides researchers with flexible evaluation settings: from highly curated, stable distributions (ClueWeb22) to more recent, although noisier, environments (FineWeb). This enables systematic studies of performance across different levels of document quality, topical diversity, and temporal drift. 3.1.2 Search Indexes To enable efficient state-of-the-art retrieval across our selected corpora, we built distributed dense retrieval backend combining state-of-the-art embedding models and approximate nearest neighbor search. More specifically, we leverage the MiniCPM-Embedding-Light model [8, 21], i.e. dense retrieval encoder trained on 260 million query-document pairs, generating 1024-dimensional document representations. The model leverages bidirectional attention mechanisms [2] and weighted mean pooling [18] to capture long-range dependencies in documents up to 8192 tokens. It achieves competitive performance on multiple benchmarks, and shows good generalization ability given zeroshot performance of 55.27 in nDCG@10 on the BEIR benchmark [36], outperforming other popular alternatives such as bge-large-en-v1.5 [3] and jina-embeddings-v3 [11], which achieve 54.29 and 53.88 in nDCG@10, respectively. We index these document embeddings using DiskANN [34], i.e. graph-based approximate nearest neighbor framework optimized for billion-scale search with low latency and memory overhead. To ensure scalability, each corpus is partitioned into independent shards of up to 25 million documents, which are separately indexed for distributed deployment. During search, shards are queried in parallel, and the top-ranked results are merged to produce the final ranking, enabling efficient and flexible retrieval at web scale. To ground the retrieval effectiveness of our search system, we evaluated it on the Researchy Queries test set, where clicked documents serve as relevance labels. Table 1 presents the retrieval performance, considering the number of retrieved documents = 100, while varying L, i.e. DiskANN searchtime parameter that controls the size of the candidate neighbor list explored during search. Increasing typically boosts recall and ranking quality by allowing more thorough exploration of the search graph but comes at the cost of reduced query throughput. We provide metrics computed given the ground-truth clicked documents (MRR@n, nDCG@n, and R@n), as well as approximate nearest neighbor recall (ANN R@n), computed based on exact-search results. The marginal gains observed with increasing indicate that the approximation error introduced by ANN search is minimal, solidifying the retrieval quality of our infrastructure. 3.1.3 Retrieval API DeepResearchGym provides retrieval API designed to support deep research systems over the aforementioned corpora. The API exposes two primary endpoints: (i) the /search endpoint, which accepts text query and returns ranked list of documents from the selected corpus, and (ii) the /fetch endpoint, which retrieves the archived textual content of document given its URL, as captured in the ClueWeb22 or FineWeb crawls. The /search endpoint supports document retrieval over the previously introduced corpora, i.e. ClueWeb22 and FineWeb. By operating over these collections, it enables consistent and reproducible search results across experiments, eliminating variance caused by changing web content or live index updates. This stability is critical for benchmarking deep research systems that require dependable retrieval behavior during long-form generation. As for search-time DiskANN parameters, our API defaults to dynamic behavior of = 5, since, by definition, min(L) = K. Since deep research Table 1: Retrieval performance of the DeepResearchGym /search API as measured over the Researchy Questions test set. Relevance Eval ANNS Eval MRR@10 nDCG@10 R@100 R@10 R@100 100 200 300 400 48.34 48.39 48.41 48.44 48.45 39.40 39.49 39.50 39.52 39.55 78.06 78.27 78.35 78.39 78.43 90.01 92.63 93.87 94.72 95.39 88.72 91.01 92.64 93.68 94.39 Figure 1: Latency percentiles with varying for DeepResearchGym (left), and latency comparison with commercial APIs for = 10 (right). systems typically issue queries sequentially rather than in batches, we evaluate our APIs latency in this single-query setting and compare it to commercial alternatives. Figure 1 presents the results: the left panel shows percentile-based end-to-end latency for our API across different values of (the number of retrieved documents), while the right panel compares latency against commercial APIs for = 10, i.e. common setting for deep research systems. Our API consistently responds in under half second, outperforming commercial services. Note that our reported latency values include not only ANN search, but also API-inherent network requests and query encoding, and can be further improved through batching mechanisms if necessary. In turn, the /fetch endpoint addresses specific challenge in deep research systems supported by static web corpora. During generation, systems retrieve documents via the /search endpoint, accessing versions captured during the crawl. Their final reports cite the original URLs associated with these documents. However, the live content of such URLs may have changed or disappeared since the original crawl. To mitigate this discrepancy, the /fetch endpoint serves archived snapshots of documents as captured during the crawl, ensuring that the original content of URLs cited in reports can be retrieved. This design enables the construction of isolated deep research pipelines that are independent of dynamic or degraded external sources. The endpoint maintains median latency of 0.09 seconds per single request. Unlike commercial APIs that often require paid subscriptions or enforce strict usage limits, our API is freely available for research use once users obtain access to the underlying corpora. FineWeb access is immediate, while ClueWeb22 access requires signing lightweight data license agreement that reflects the datasets research-only usage terms and copyright conditions. Upon completion of this agreement, users can access the full ClueWeb22-based endpoints and optionally download the ClueWeb22-B subset for local deployment. To support this local implementation, we provide open-source code for setting up the complete retrieval pipeline. To safeguard user privacy, the API does not log query traffic unless users explicitly opt in for research participation. 3.2 Deep Research Evaluation Methods To demonstrate how DeepResearchGym can support evaluation of deep research systems, we instantiate an evaluation protocol built around the Researchy Questions dataset [28]. This dataset was derived from Bing search logs, and contains complex queries associated with user engagement, making it well-suited for benchmarking systems that synthesize long-form, multi-perspective reports. While the sandbox is agnostic to the specific evaluation task and compatible with broad range of use cases, we introduce this protocol to fill current gap in the evaluation landscape, and to provide initial empirical observations using our API. Our protocol scores generated the reports along three axes: overall report quality, factual faithfulness to retrieved sources, and alignment with user information needs. 3.2.1 Researchy Questions Evaluating deep research systems requires queries that naturally drive extensive information exploration and synthesis. The Researchy Questions dataset [28] was curated specifically to capture such queries. Rather than featuring simple factoid questions, the dataset consists of approximately 96,000 real-world information-seeking queries that led users to engage with multiple documents during search sessions, as measured by aggregated click distributions over ClueWeb22. For reference, Appendix shows sample of queries together with clicked document URLs. 5 The heavy engagement with diverse sources reflects the essential challenges deep research systems are designed to address: synthesizing information across multiple perspectives, reconciling conflicting evidence, and constructing comprehensive responses. In contrast to short, answerable queries that focus on locating single fact, Researchy Questions exhibit the complexity that require broader exploration and deeper understanding. While the dataset was originally introduced for evaluating retrieval performance, its properties make it strong foundation for studying long-form generation grounded in multi-document evidence. In the next section, we describe our evaluation methodology for extending the use of Researchy Questions to benchmark deep research generation. 3.2.2 Long-form Report Evaluation Metrics Deep research systems focused on providing report-like answers face multiple challenges inherent to long-form generation evaluation [41], where outputs must be assessed not only for linguistic fluency and informativeness, but also for factual grounding and content relevance. We follow tri-faceted evaluation framework that assesses the alignment with user information needs, factual grounding, and overall quality of generated answers. The Appendices contains all the prompts used for LLM-based metrics (Appendix B), an example report (Appendix C), and its detailed evaluation (Appendix D). Report Relevance: As the primary metric for assessing user satisfaction, we evaluate how well the generated reports address the users underlying information needs. Given that Researchy Questions are derived from real-world web search sessions, we leverage the set of documents clicked by users as proxy for ground-truth information targets. Following the Key Point Recall (KPR) methodology [26], we extract salient points from each ground-truth document using an LLM guided by structured prompts, capturing the core content users engaged with. We then assess each generated report for semantic inclusion of these key points, computing the KPR score as 1 j=1 cj, where is the total number of key points and cj indicates whether key point is supported by the report, as judged by an LLM. (cid:80)M To complement recall, we also compute Key Point Contradiction (KPC), which measures whether the report introduces statements that conflict with any key points. This score captures potential misinformation or misleading content, defined as 1 j=1 dj, where dj is 1 if the report contradicts key point j, as judged by the same LLM used for the KPR metric. Together, these metrics provide user-centered assessment of both coverage and factual consistency relative to real-world search intents. (cid:80)M Retrieval Faithfulness: Beyond relevance, we assess the factual grounding of generated reports, adapting the LLM-as-a-judge approach of the TREC-RAG evaluation process [37]. Our automatic citation evaluation pipeline follows three-stage process. First, factual claims are extracted from the report, along with any URLs referenced as support. Second, the content of each cited source is retrieved. Third, an LLM is prompted to assess whether the cited source adequately supports the corresponding claim. This procedure captures both the presence of citations and their substantive validity. Given report, we compute the primary metrics established by the TREC-RAG evaluation. Citation recall measures the proportion of factual claims that include at least one citation, i.e., Ncited , where Ntotal Ncited represents the number of claims with citations and Ntotal represents the total number of claims. This metric quantifies how consistently the system grounds its assertions in external evidence. In turn, citation precision evaluates the quality of citations for claims that include references. Each claim-citation pair receives support score si, where full support (score = 1) means all key aspects of the claim are fully supported by the cited source; partial support (score = 0.5) means some aspects of the claim are supported, but the support is incomplete; and no support (score = 0) means the cited source does not substantively support the claim or is irrelevant. Citation precision is then computed 1 as the average score across all cited claims, i.e., Ncited Report Quality: To capture aspects of writing quality and analytical depth, we employ another LLM-as-a-Judge protocol [6], prompting strong LLM to evaluate each answer along two key dimensions: clarity, reflecting logical coherence and linguistic fluency; and insightfulness, capturing analytical nuance and the depth of reasoning presented. These dimensions are commonly used in long-form generation evaluation [15, 31] and provide complementary evidence of the overall presentation quality of the generated content. i=1 si. (cid:80)Ncited 6 Table 2: Comparison of deep research systems on the Researchy Questions test set using (i) each systems original commercial search API and (ii) DeepResearchGyms search API. Scores are judged by gpt-4.1-mini-2025-04-14. Systems marked with * are not tailored for long-report generation. Relevance Faithfulness Quality System Commercial KPR KPC DRGym Commercial DRGym Commercial DRGym KPR KPC Precision Recall Precision Recall Clarity Insight Clarity Insight perplexity-sonar-deepsearch gpt4-search-preview GPT-Researcher OpenDeepSearch HuggingFace-DeepSearch Search-o1 Search-R1 72.50 40.01 60.61 32.92 33.00 28.92 5.52 1.12 1.69 1.52 0.97 0.81 0.34 0. 64.67 42.81 35.22 29.93 4.95 1.42 0.84 1.35 0.38 0.80 55.65 57.68 89.11 85.86 0.35 0.00 0. 99.22 56.11 94.29 97.78 0.29 0.00 0.00 85.36 81.32 0.10 0.00 0.00 90.82 94.82 0.10 0.00 0. 89.50 70.13 86.37 59.20 57.52 29.38 9.48 89.26 59.13 81.52 47.04 47.98 36.81 11.87 83.70 61.48 58.34 30.31 9. 78.01 49.51 52.36 37.87 11."
        },
        {
            "title": "4 Benchmarking Deep Research Systems",
            "content": "This section reports empirical results from benchmarking diverse set of deep research systems using our evaluation protocol. We compare performance across retrieval settings, analyze per-query consistency, and validate metric reliability through human judgments. 4.1 Experimental Setup To evaluate the current landscape of deep research systems, we conducted systematic benchmarking study, following the protocol described in Section 3.2.2 with gpt-4.1-mini-2025-04-14 as the LLM judge. We use subset of the previously introduced Researchy Questions dataset, namely the top 1,000 queries from the test set, ranked by the number of documents clicked during the original search sessions. This ranking naturally favors queries that drive extensive exploration, aligning with the goals of deep research systems. We evaluate diverse set of deep research systems spanning both commercial and open-source implementations. The commercial systems include gpt4-search-preview from OpenAI and sonardeepresearch from Perplexity, which represent the strongest variants available through the respective APIs. On the open-source side, we include GPT-Researcher and HuggingFace DeepSearch. All four systems are capable of generating long-form reports. We also evaluate three academic systems. OpenDeepSearch produces similarly comprehensive outputs, while Search-o1 and Search-R1 focus on concise, short-form answers. Although not designed for deep research tasks, these last two systems serve as lower-bound references and help verify that our evaluation metrics capture meaningful differences in generative capabilities. All systems are evaluated using their default configurations. Experiments leveraging DeepResearchGyms search API default to the ClueWeb22-B corpus, given the higher alignment with the Researchy Questions benchmark. Recall that, as discussed in Section 3.1.3, FineWeb is also available through our API endpoints, and, unlike ClueWeb22, this does not require signing the research-use license. 4.2 System-level Evaluation Table 2 presents evaluation results for each system under two distinct retrieval configurations: (1) using the systems original commercial search API, and (2) using the standardized DeepResearchGym search API. The results reveal several important insights. First, systems generally maintain their relative performance rankings across both retrieval settings, confirming that DeepResearchGyms search API provides sufficient retrieval quality to support effective report generation. Second, we observe consistent patterns in the relative difficulty of different evaluation dimensions. Even top-performing systems like perplexity-sonar-deepsearch and GPT-Researcher achieve notably higher scores in report quality metrics (Clarity, Insight) compared to information coverage metrics (KPR), suggesting that linguistic fluency has outpaced comprehensive content synthesis. This pattern holds across both retrieval environments, indicating an intrinsic challenge in deep research that transcends retrieval infrastructure. 7 Figure 2: Query-level correlation between commercial search APIs and DeepResearchGym API evaluation metrics. The empty plot reflects non-citation system. Third, significant performance gaps exist in citation practices. Systems like GPT-Researcher and OpenDeepSearch demonstrate strong citation precision and recall, effectively grounding factual claims in retrieved evidence. In contrast, systems such as HuggingFaces deepresearch exhibit near-zero citation metrics, stemming from fundamental design choices that omit explicit citation mechanisms. This highlights key distinction in how different approaches conceptualize evidence grounding, whether through explicit source attribution or implicit knowledge integration. The evaluation also reveals an interesting trade-off in commercial systems, which tend to achieve excellent narrative quality but sometimes at the expense of granular citation precision. Manual inspection indicates two common patterns: (1) these systems often use citations to justify broader report sections rather than specific claims, and (2) they sometimes reference URLs that cannot be fully crawled. This suggests potential tension between optimizing for narrative coherence versus precise evidence anchoring in current system designs. 4.3 Query-level Analysis To further investigate the consistency of system performance across individual queries, we conducted fine-grained analysis comparing results obtained under each systems original retrieval API, against those those from DeepResearchGyms API, focusing only on the systems explicitly geared towards long-report generation. Figure 2 presents scatter plots of per-query scores across our three evaluation axes. The Spearman correlation coefficients quantify the alignment between retrieval configurations. The analysis reveals distinct patterns across evaluation dimensions. For relevance (KPR), stronger systems exhibit moderate to high correlation, indicating that query-level retrieval effectiveness is largely preserved when transitioning to DeepResearchGyms corpus. However, mid-range queries show some score variability, suggesting that certain information needs are more sensitive to differences in retrieval infrastructure. In contrast, report quality metrics demonstrate lower per-query correlation, despite high absolute scores for top systems. This implies that while narrative fluency and coherence are robust to retrieval changes, they are not tightly coupled with individual query characteristics. Retrieval faithfulness shows the lowest per-query correlation across systems, indicating that this dimension is sensitive to differences in retrieved evidence. Changes in the retrieved documents can shift not only how well claims are supported, but also the claims themselves, leading to variation in citation faithfulness scores across retrieval setups. While average scores across queries remain stable, with some individual queries yielding consistently high scores across both sources, the broader pattern lacks alignment, with most points scattered and with no clear linear trend. This variability underscores the importance of using standard retrieval API when benchmarking deep research systems, as it helps control for retrieval effects and ensures that observed differences stem from model behavior rather than different access to evidence. 8 4.4 Human Evaluation To validated our automatic evaluation protocol and assess its alignment with human judgments, we conduct systematic human evaluation study. The evaluation set comprises 210 queries with their corresponding generated reports. For each query, we present annotators with two reports generated by different systems, asking them to select the better one based on overall informativeness, coherence, and factual accuracy. Seven annotators (all also co-authors of this paper) participated in the study, with each query independently evaluated by three different individuals. To mitigate potential bias, the evaluation was conducted double-blind: annotators were unaware of which systems produced each report, and system assignments were randomized across queries. Report presentation order was also randomized to prevent positional bias, and ties were disallowed to enforce binary preferences. κ Metric Table 3: LLM versus human agreement (Cohens κ). To assess inter-annotator reliability, we compute pairwise Cohens κ scores across all annotated comparisons. The average pairwise κ is 0.87, indicating high level of agreement and supporting the consistency of human judgments. Building on this, Table 3 presents Cohens κ values measuring agreement between LLM-based automatic judgments and human preferences across each evaluation dimension, while Figure 3 presents pairwise win rates, showing the percentage of times each system was preferred over another based on human annotations. The KPC metric was excluded from the analysis in Table 3 due to high rate of LLM-judged ties, leaving insufficient samples for analysis. The results demonstrate substantial agreement and the same trend regarding relative model performance. This validates our LLM-as-a-judge evaluation protocol, strengthening confidence in the benchmark results presented in the previous sections."
        },
        {
            "title": "KPR\nCitation Precision\nCitation Recall\nClarity\nInsightfulness",
            "content": "Figure 3: System win rates as judged by human preferences. 0.72 0.86 0.86 0.89 0."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "DeepResearchGym offers reproducible sandbox for developing and benchmarking deep research systems, providing stable alternative to commercial search APIs. By anchoring retrieval to highquality web corpora and combining the search API with multi-dimensional evaluation protocol, our framework enables controlled benchmarking of systems that generate long-form reports. Our results demonstrate that DeepResearchGyms search API provides reliable research-grade substitute for commercial retrieval infrastructures. Across multiple evaluation axes, all evaluated systems maintain comparable performance when transitioning from proprietary search APIs to our transparent and reproducible retrieval environment, affirming that DeepResearchGym preserves the retrieval fidelity necessary to support complex report-style synthesis. By isolating system behavior from fluctuating retrieval conditions, DeepResearchGym offers stable foundation for reproducible and fair benchmarking of deep research systems. Future extensions to DeepResearchGym can perhaps expand the coverage to larger and more recent web corpora, enabling evaluation of time-sensitive queries and emerging topics. Moreover, the integration of domain-specific benchmarks may further support assessment in high-stakes contexts such as healthcare or law, where retrieval precision and factual reliability are critical. Finally, the platform opens space for testing new agent architectures that incorporate structured planning, iterative search, and reinforcement learning, advancing research on deep research systems. Limitations: Current limitations include: the reliance of deep research systems on proprietary and evolving LLMs for generation, which restricts full output reproducibility; evaluation variance introduced by LLM-as-a-judge protocols despite prompt/output standardization; and the use of static web corpora, which limits coverage of emerging or time-sensitive content. Ethical risks also apply, such as amplification of bias, misinformation, or misuse of retrieved content, although DeepResearchGym is targeted at academic research, instead of real-world deployments."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "Use unnumbered first level headings for the acknowledgments. All acknowledgments go at the end of the paper before the list of references. Moreover, you are required to declare funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work). More information about this disclosure can be found at: https: //neurips.cc/Conferences/2025/PaperInformation/FundingDisclosure. Do not include this section in the anonymized submission, only in the final paper. You can use the ack environment provided in the style file to automatically hide this section in the anonymized submission."
        },
        {
            "title": "References",
            "content": "[1] Salaheddin Alzubi, Creston Brooks, Purva Chiniya, Edoardo Contente, Chiara von Gerlach, Lucas Irwin, Yihan Jiang, Arda Kaz, Windsor Nguyen, Sewoong Oh, Himanshu Tyagi, and Pramod Viswanath. Open Deep Search: Democratizing Search with Open-source Reasoning Agents. ArXiv, abs/2503.20201, 2025. [2] Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. LLM2Vec: Large language models are secretly powerful text encoders. In Conference on Language Modeling, COLM, 2024. [3] Beijing Academy of Artificial Intelligence. bge-large-en-v1.5, 2024. URL https:// huggingface.co/BAAI/bge-large-en-v1.5. Hugging Face model page. [4] Francesco Bonchi, Carlos Castillo, Debora Donato, and Aristides Gionis. Topical query decomposition. In International Conference on Knowledge Discovery and Data Mining, KDD, 2008. [5] Assaf Elovic. GPT Researcher: LLM-based Autonomous Agent for Deep Research, 2025. URL https://github.com/assafelovic/gpt-researcher. [6] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Yuanzhuo Wang, and Jian Guo. Survey on LLM-as-a-Judge. ArXiv, abs/2411.15594, 2024. [7] Mark Gurman, Lydia Nylen, and Shiyin Lai. 2025. apple-working-to-move-to-ai-search-in-browser-amid-google-fallout. Bloomberg News. Apple Eyes Move to AI Search, https://www.bloomberg.com/news/articles/2025-05-07/ URL [8] Shengding Hu et al. MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies. In Conference on Language Modeling, COLM, 2024. [9] HuggingFace. Open Deep Research, 2025. URL https://github.com/huggingface/ smolagents/tree/main/examples/open_deep_research. [10] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning. ArXiv, abs/2503.09516, 2025. [11] JinaAI. jina-embeddings-v3, 2024. URL https://huggingface.co/jinaai/ jina-embeddings-v3. Hugging Face model page. [12] Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. Fact, fetch, and reason: unified evaluation of retrievalaugmented generation. In Conference of the North American Chapter of the Association for Computational Linguistics, NAACL, 2025. [13] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Annual Conference on Neural Information Processing Systems, NeurIPS, 2020. 10 [14] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic Search-Enhanced Large Reasoning Models. ArXiv, abs/2501.05366, 2025. [15] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation using gpt-4 with better human alignment. In Conference on Empirical Methods in Natural Language Processing, EMNLP, 2023. [16] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. GAIA: benchmark for general AI assistants. In International Conference on Learning Representations, ICLR, 2024. [17] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. In Conference on Empirical Methods in Natural Language Processing, EMNLP, 2023. [18] Niklas Muennighoff. abs/2202.08904, 2022. SGPT: GPT sentence embeddings for semantic search. ArXiv, [19] OpenAI. Introducing Deep Research, 2025. introducing-deep-research/. OpenAI Blog. May 11, 2025. URL https://openai.com/index/ [20] OpenAI. Deep Research System Card, 2025. deep-research-system-card.pdf. Accessed May 11, 2025. URL https://cdn.openai.com/ [21] OpenBMB. MiniCPM-Embedding-Light, 2024. URL https://huggingface.co/openbmb/ MiniCPM-Embedding-Light. Hugging Face model page. [22] Arnold Overwijk, Chenyan Xiong, and Jamie Callan. ClueWeb22: 10 Billion Web Documents with Rich Information. In International Conference on Research and Development in Information Retrieval, SIGIR, 2022. [23] Guilherme Penedo, Hynek Kydlícek, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin A. Raffel, Leandro von Werra, and Thomas Wolf. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. In Annual Conference on Neural Information Processing Systems NeurIPS, 2024. [24] Perplexity AI. Introducing Perplexity Deep Research, 2025. URL https://www.perplexity. ai/hub/blog/introducing-perplexity-deep-research. Perplexity Blog. Accessed May 11, 2025. [25] Long Phan et al. Humanitys Last Exam. ArXiv, abs/2501.14249, 2025. [26] Zehan Qi, Rongwu Xu, Zhijiang Guo, Cunxiang Wang, Hao Zhang, and Wei Xu. LONG2RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall. In Findings of the Association for Computational Linguistics, EMNLP, 2024. [27] Elizabeth Reid. for You, ing generative-ai-google-search-may-2024/. Accessed: 2025-05-12. 2024. URL Generative AI in Search: Let Google Do the Searchhttps://blog.google/products/search/ [28] Corby Rosset, Ho-Lam Chung, Guanghui Qin, Ethan C. Chau, Zhuo Feng, Ahmed Awadallah, Jennifer Neville, and Nikhil Rao. Researchy Questions: Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents. ArXiv, abs/2402.17896, 2024. [29] Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Cheng Jiayang, Cunxiang Wang, Shichao Sun, Huanyu Li, Zizhao Zhang, Binjie Wang, Jiarong Jiang, Tong He, Zhiguo Wang, Pengfei Liu, Yue Zhang, and Zheng Zhang. Ragchecker: fine-grained framework for diagnosing retrieval-augmented generation. In Annual Conference on Neural Information Processing Systems, NeurIPS, 2024. [30] Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. ARES: an automated evaluation framework for retrieval-augmented generation systems. In Conference of the North American Chapter of the Association for Computational Linguistics, NAACL, 2024. 11 [31] Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. Branch-Solve-Merge Improves Large Language Model Evaluation and Generation. In Conference of the North American Chapter of the Association for Computational Linguistics, NAACL, 2024. [32] Serper. Serper.dev Search API, 2025. URL https://serper.dev. Accessed May 11, 2025. [33] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning. ArXiv, abs/2503.05592, 2025. [34] Suhas Jayaram Subramanya, Devvrit, Harsha Vardhan Simhadri, Ravishankar Krishnaswamy, and Rohan Kadekodi. DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on Single Node. In Annual Conference on Neural Information Processing Systems, NeurIPS, 2019. [35] Tavily. Tavily Search API, 2025. URL https://tavily.com. Accessed May 11, 2025. [36] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. BEIR: heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Conference on Neural Information Processing System, NeurIPS, 2021. [37] Nandan Thakur, Ronak Pradeep, Shivani Upadhyay, Daniel Campos, Nick Craswell, and Jimmy Lin. Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges. ArXiv, abs/2504.15205, 2025. [38] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. In Annual Meeting of the Association for Computational Linguistics, ACL, 2023. [39] Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, and Quoc V. Le. Long-form factuality in large language models. In Annual Conference on Neural Information Processing Systems, NeurIPS, 2024. [40] Junde Wu, Jiayuan Zhu, and Yuyuan Liu. Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research. ArXiv, abs/2502.04644, 2025. [41] Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. Critical Evaluation of Evaluations for Long-form Question Answering. In Annual Meeting of the Association for Computational Linguistics, ACL, 2023. [42] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. DeepResearcher: Scaling Deep Research via Reinforcement Learning in Realworld Environments. ArXiv, abs/2504.03160, 2025. [43] Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi Ho, and Philip S. Yu. Trustworthiness in Retrieval-Augmented Generation Systems: Survey. ArXiv, abs/2409.10102, 2024."
        },
        {
            "title": "A Researchy Questions",
            "content": "Table 4 shows sample of 5 queries from the Researchy Questions [28] dataset, along with hyperlinks to 10 of its user-clicked documents. Query References Is the COVID vaccine dangerous Link1, Link2, Link3, Link4, Link5, Link6, Link7, Link8, Link9, Link10 Why is there chip shortage Link1, Link2, Link3, Link4, Link5, Link6, Link7, Link8, Link9, Link Can there be knowledge that is independent of culture? Link1, Link2, Link3, Link4, Link5, Link6, Link7, Link8, Link9, Link10 Why gas prices are so high Link1, Link2, Link3, Link4, Link5, Link6, Link7, Link8, Link9, Link10 Does religion cause war Link1, Link2, Link3, Link4, Link5, Link6, Link7, Link8, Link9, Link Table 4: Sample of the Researchy Questions dataset. LLM-as-judge Prompts This section details all the prompts used throughout this work for LLM-as-a-judge evaluation protocols. Note that all the provided JSON output formats were enforced through structured decoding. B.1 Key-point Extraction Prompt Arguments: query: search query text: text of relevant document Prompt: Based on the text provided, identify key points in the text that directly help in responding to the query. The key points are not simply some key content of the text, but rather the key points that are important for **answering the query**. IMPORTANT: Ensure each point is helpful in responding to the query. Keep the point using the original language and do not add explanations. IMPORTANT: Each span must be single consecutive verbatim span from the corresponding passages. Copy verbatim the spans, dont modify any word! Your response should state the point number, followed by its content, and spans in the text that entail the key point. Respond strictly in JSON format: { \"points\": [ { \"point_number\": point_number, \"point_content\": point_content, \"spans\": [span1, span2, ...] }, ... ] } Remember: - Key points can be abstracted or summarized, but the span must be copy of the original text. The content of the key point does NOT need to be the same as that of the span. - These key points must be helpful in responding to the query. - If there are multiple spans for point, add all of them in the spans list. [Query]: {query} [Text]: {text} This prompt follows the one used by Long2RAG [26]. B.2 Key-point Merging Prompt Arguments: key points extracted from multiple documents Prompt: You are given list of key points extracted from multiple documents. Your task is to aggregate these points according to the following instructions: 1. Identify and deduplicate any duplicated or redundant points. Merge them into single, representative point. 2. Identify contradictory points. Merge them into single point that presents both sides, e.g., \"Sources claim that X, while other sources claim that Y\". IMPORTANT RULES: - Every aggregated point must preserve **all original information** from the included points. - Do not invent or add any new information. Only use what is already present. - Do not provide any explanations or summaries beyond the aggregation itself. - Each aggregated point should **capture single atomic idea**. Avoid combining unrelated aspects into one point. - Keep the aggregated point **concise but complete**: include all essential details needed to fully represent the merged idea, but do not make it overly detailed or verbose. - For each aggregated point, include reference to the original point numbers it is based on, e.g., \"original_point_number\": [1, 3, 7]. Respond strictly in JSON format: {{ \"points\": [ {{ \"point_number\": point_number, \"point_content\": point_content, \"original_point_number\": [original_point_number1, original_point_number2, ...] }}, ... ] }} [Original Points] {original_points_with_number} B.3 Key-point Verification Prompt Arguments: key_point: single ground-truth key point answer: report generatd by DeepResearch system Prompt: 14 You are given **single key point** and **report**. Your job is to determine whether the report: - **Supports** the key point (it affirms, explains, or reinforces the point), - **Omits** the key point (it does not mention or cover this point at all), or - **Contradicts** the key point (it says something that disagrees with or negates the point). Carefully read the key point and the report. Return your answer as **JSON object** with two fields: - \"label\": One of \"Supported\", \"Omitted\", or \"Contradicted\". - \"justification\": Brief explanation on why you assigned this label. Respond strictly in JSON format: {{\"label\": label, \"justification\": justification}} Do **not** add any extra commentary or text outside the JSON. --- Key Point: {key_point} Report: {answer} B.4 Claim-URL Extraction Prompt Arguments: report: report generated by deep research system Prompt: You are an information extraction expert. Given structured report containing claims and their supporting sources (usually in the form of inline hyperlinks or referenced URLs), extract all distinct factual or argumentative claims in the text. If claim is supported by one or more sources, return the supporting URLs as sources. If claim is not supported by any source, return an empty list of sources. Return JSON object like this: {{ \"claims\": [ {{ \"claim_id\": 1, \"claim\": \"<claim_1>\", \"sources\": [\"<url_1>\", \"<url_2>\", ...] }}, {{ \"claim_id\": 2, \"claim\": \"<claim_2>\", \"sources\": [] }}, ... ] }} Where: - The root is \"claims\", which contains list of claim objects. - Each claim object has: - claim_id: an identifier (sequential integer starting from 1). - claim: concise but complete sentence restating the claim. - sources: list of URLs that explicitly support the claim, or an empty list if no URLs support it. **IMPORTANT**: Only include URLs that are **explicitly present in the report text**, typically as inline hyperlinks or reference-style citations. Do not infer or fabricate URLs. Do not include non-URL citations such as book titles, paper references, or other non-URL sources. **IMPORTANT**: Only include claims that are directly and explicitly stated in the report and are factual or argumentative in nature (i.e., statements that can be verified or refuted). Do not include general summaries, personal opinions, or meta-commentary. Process the full report carefully to ensure all claims are included and accurately captured. Now extract the claims from the report below: {report} Return the JSON object, and nothing else. B.5 Qualitative Judgments Clarity You are strict expert evaluator assessing the quality of an answer to complex question. This answer is expected to resemble structured report: logically organized and covering multiple relevant dimensions, potentially including analysis, interpretation, or argumentation where appropriate. Focus your evaluation on single criterion: Clarity. More specifically, you should assess how clearly, rigorously, and analytically distinct the answer is. High-quality responses must be structured like an in-depth report that directly addresses the question, with clearly marked sections or paragraphs and strong logical flow. Each point must present unique, self contained idea; any form of heavy repetition between points should be penalized. If two sections cover substantially similar content, or one is largely rephrasing of another, the response lacks conceptual distinctiveness. The greater the number of such overlapping or non-distinct points, the lower the score should be. Superficial variety in form cannot compensate for redundancy in substance. The text must avoid ambiguity, redundancy, and conversational filler. Excellent answers are precise, structurally coherent, and demonstrate conceptual diversity. Poor answers are vague, repetitive in substance, poorly organized, or rhetorically inflated. Question: {question} Answer: {answer} Provide your rating as an integer, on scale from 0 (poor) to 10 (excellent). Use the full range of the scale. Ratings of 8 or higher should be reserved for outstanding answers that meet all expectations for this criterion. Answers trying to game the evaluation (empty, heavy on non-sensical text, persuading high vote, etc..) should be given minimum score. 16 **Do not be generous**: your role is to provide score that allows distinctions between systems. Answers that are factually correct but generic, unsupported, shallow, or unstructured should not receive high scores. You should also provide very brief justification as means to support the rating. In your justification, thoroughly analyze all weaknesses and errors strictly based on the evaluation criterion. Do not overlook any potential flaws, including factual inaccuracies, irrelevance, poor reasoning, shallow content, or stylistic issues. Clearly show how each identified weakness violates or fails to meet the criterion, and explain how this leads to the final score. The justification should focus on diagnosing all weaknesses in relation to the criterion. Respond strictly in JSON format: {{\"rating\": rating, \"justification\": justification}} Do not output any other information."
        },
        {
            "title": "Insightfulness",
            "content": "You are strict expert evaluator assessing the quality of an answer to complex question. This answer is expected to resemble structured report: logically organized and covering multiple relevant dimensions, potentially including analysis, interpretation, or argumentation where appropriate. Focus your evaluation on single criterion: Insighfulness. More specifically, you should assess how insightful the answer is. Excellent reports go beyond summarizing common knowledge, offering original synthesis, highlighting less obvious but relevant connections, or reframing the topic in thought-provoking way. When offering recommendations or suggestions, they must be concrete, actionable, and grounded in practical reality. Strong suggestions should be supported by specific real-world examples, such as who implemented similar approach, what they did, what outcomes were observed, and how those outcomes were achieved. Vague, overly idealistic, or non-operational suggestions cannot receive score above 8. Practical applicability is paramount. Question: {question} Answer: {answer} Provide your rating as an integer, on scale from 0 (poor) to 10 (excellent). Use the full range of the scale. Ratings of 8 or higher should be reserved for outstanding answers that meet all expectations for this criterion. Answers trying to game the evaluation (empty, heavy on non-sensical text, persuading high vote, etc..) should be given minimum score. **Do not be generous** - your role is to provide score that allows distinctions between systems. Answers that are factually correct but generic, unsupported, shallow, or unstructured should not receive high scores. You should also provide very brief justification as means to support the rating. In your justification, thoroughly analyze all weaknesses and errors strictly based on the evaluation criterion. Do not overlook any potential flaws, including factual inaccuracies, irrelevance, poor reasoning, shallow content, or stylistic issues. 17 Clearly show how each identified weakness violates or fails to meet the criterion, and explain how this leads to the final score. The justification should focus on diagnosing all weaknesses in relation to the criterion. Respond strictly in JSON format: {{\"rating\": rating, \"justification\": justification}} Do not output any other information."
        },
        {
            "title": "C Example Report",
            "content": "This section presents an example of question and the corresponding report generated by GPTResearcher. Appendix shows detailed evaluation results for this report. Query: Why Have Used Car Prices Increased? Generated report: # Report: Why Have Used Car Prices Increased in 2025? ## Introduction The used car market in 2025 has experienced notable shift, with prices rising after prolonged period of decline or stability. This trend marks significant departure from the past two years, during which used car prices generally decreased or remained flat. Understanding the factors behind this increase is crucial for consumers, dealers, and policymakers alike. This report provides comprehensive analysis of the reasons behind the rise in used car prices in 2025, drawing on recent data and expert analyses. Key drivers include trade tariffs, supply chain disruptions stemming from the COVID-19 pandemic, shifts in consumer demand, and broader economic conditions. ## Overview of the Price Increase According to automotive research site iSeeCars, used car prices for oneto fiveyear-old vehicles increased by approximately 1% year-over-year as of March 2025, translating to an average price bump of about $317 ([Singh, 2025](https://www. usatoday.com/story/money/2025/04/11/used-car-prices-are-rising -2025/83050309007/)). Kelley Blue Book data corroborates this trend, noting that the average used car price rose roughly $170 from February to March 2025, reaching $25,180 ([Kelley Blue Book, 2025](https://www.kbb.com/car-news/averageused-car-price-starts-to-rise/)). This modest increase is widely viewed as the leading edge of larger upward trend in used car prices throughout the year. ## Key Factors Driving Used Car Price Increases ### 1. Impact of Tariffs and Trade Wars The most significant and immediate cause of rising used car prices in 2025 is the imposition of tariffs on new vehicles and auto parts, primarily under policies initiated by the Trump administration. Beginning in early 2025, 25% tariff was applied to all new cars entering the United States, with additional tariffs on automotive parts scheduled to follow ([Kelley Blue Book, 2025](https://www. kbb.com/car-news/average-used-car-price-starts-to-rise/); [Neeley, 2025](https ://carketa.com/auto-tariffs-used-car-pricing-inventory/)). These tariffs have led to several cascading effects: - **Increased New Car Prices**: The tariffs raise production costs for new vehicles, which automakers pass on to consumers. Cox Automotive estimates that imported vehicles could see price increases of up to $6,000 due to tariffs, with domestically assembled vehicles also facing increases of around $3,600 due to 18 parts tariffs ([CNBC, 2025](https://www.cnbc.com/2025/04/12/auto-tariffs-salescosts.html)). - **Reduced New Car Supply and Affordability**: Automakers have responded by pausing shipments, adjusting production strategies, or freezing exports to the U.S., leading to contraction in the supply of affordable new vehicles ([Carscoops, 2025](https://www.carscoops.com/2025/04/used-cars-just-saw-their-first-pricebump-in-over-two-years/)). This scarcity drives consumers toward the used car market as more affordable alternative. - **Increased Demand for Used Cars**: As new car prices rise and supply tightens, more buyers turn to used vehicles, pushing up demand and prices in that segment ([Tampa Bay AutoNetwork, 2025](https://www.tampabayautonetwork.com/news/howtariffs-will-affect-new-used-car-prices-in-2025/)). - **Inventory Challenges for Dealerships**: Tariffs on Chinese imports and ongoing supply chain disruptions complicate inventory management for used car dealerships, limiting their ability to replenish stock and further constraining supply ([Neeley, 2025](https://carketa.com/auto-tariffs-used-car-pricinginventory/)). The interplay of these factors creates \"perfect storm\" where tariffs not only increase new car prices but also indirectly inflate used car prices due to heightened demand and constrained supply. ### 2. Lasting Supply Chain Disruptions from COVID-19 The COVID-19 pandemic caused unprecedented disruptions in the automotive supply chain, effects of which persist into 2025: - **Production Shortfalls**: The pandemic led to factory shutdowns, raw material shortages (notably microchips), and shipping delays, cutting millions of vehicles from production in 2020 and 2021 ([Motor, 2023](https://www.motor.com /2023/07/long-covid-continues-to-impact-supply-chain-issues-and-new-vehicleinventory/); [Michigan Journal of Economics, 2022](https://sites.lsa.umich.edu/ mje/2022/01/05/covid-19-supply-chain-shortages-and-the-automobile-industry/)). - **Lease and Rental Market Void**: Traditionally, lease returns and ex-rental vehicles provide steady stream of relatively new, well-maintained used cars. The pandemic caused sharp decline in new lease agreements and rental fleet purchases, leading to \"missing generation\" of used vehicles entering the market ([Digital Dealer, 2025](https://digitaldealer.com/sales-variable-ops/howcovid-19-created-a-lasting-supply-chain-void-in-the-automotive-industry/)). - **Reduced Used Car Inventory**: The shortage of lease returns and ex-rental vehicles has created persistent supply gap in the used car market, leading to increased competition for available stock and higher prices ([Digital Dealer, 2025](https://digitaldealer.com/sales-variable-ops/how-covid-19-created-alasting-supply-chain-void-in-the-automotive-industry/)). - **Extended Vehicle Lifecycles**: Both rental companies and private owners are holding onto vehicles longer due to limited replacement options, further reducing the influx of used cars ([Digital Dealer, 2025](https://digitaldealer. com/sales-variable-ops/how-covid-19-created-a-lasting-supply-chain-void-in-theautomotive-industry/)). These supply chain voids have compounded the effects of tariffs by limiting the availability of used cars, thereby driving prices upward. ### 3. Economic and Financing Conditions Economic factors also influence used car prices: - **High Interest Rates**: Auto loan rates remain near decades-high levels, with rates exceeding 9.64% for new vehicles and nearly 15% for used cars ([CNBC, 19 2025](https://www.cnbc.com/2025/04/12/auto-tariffs-sales-costs.html)). This increases the total cost of ownership, potentially dampening demand but also pushing buyers toward more affordable used vehicles. - **Inflation and Consumer Budgeting**: Inflationary pressures and economic uncertainty make consumers more budget-conscious, increasing reliance on used cars as affordable alternatives to new vehicles ([Tampa Bay AutoNetwork, 2025]( https://www.tampabayautonetwork.com/news/how-tariffs-will-affect-new-used-carprices-in-2025/)). - **Declining Trade-In Values**: Trade-in values have fallen to four-year lows, reducing the affordability of new purchases and contributing to tighter used car supply ([Dealership Guy, 2025](https://news.dealershipguy.com/p/2025-automarket-affordable-new-car-inventory-rises-used-car-supply-shrinks-2025-01-17)). These economic conditions reinforce the demand for used cars while simultaneously limiting supply, contributing to price increases. ### 4. Shifts in Consumer Preferences and Market Dynamics - **Increased Demand for Used Vehicles**: With new car prices rising, more consumers are turning to used cars, increasing demand and pushing prices higher ([ iSeeCars, 2025](https://www.tennessean.com/story/money/cars/2025/04/10/used-carprices-increase-consumers/83026082007/)). - **Segment-Specific Trends**: SUVs and trucks have seen the largest price increases in the used market, with SUVs up 3.5% and trucks up 0.9% year-over-year, reflecting consumer preference for these vehicle types ([Carscoops, 2025](https ://www.carscoops.com/2025/04/used-cars-just-saw-their-first-price-bump-in-overtwo-years/)). Conversely, used electric vehicles (EVs) have experienced price declines, indicating differing market dynamics by vehicle type. - **Technological Advancements**: Modern used cars equipped with advanced safety and infotainment technology command higher resale values, contributing to overall price increases ([Car Concierge Pro, 2025](https://carconciergepro.com/trendsin-2025-used-car-prices-visual-insights/)). - **Regional Variations**: Demand and pricing trends vary by region, with North America favoring trucks and SUVs, Europe emphasizing compact EVs and hybrids, and Asia focusing on affordable sedans and hatchbacks ([Car Concierge Pro, 2025](https://carconciergepro.com/trends-in-2025-used-car-prices-visualinsights/)). ### 5. Dealer Inventory and Pricing Strategies - **Inventory Constraints**: Dealers face challenges acquiring used car inventory due to tariffs, supply chain issues, and fewer trade-ins, leading to limited stock and higher prices ([Neeley, 2025](https://carketa.com/auto-tariffs-usedcar-pricing-inventory/)). - **Pricing Adjustments**: Dealers are adjusting pricing strategies to reflect market conditions, balancing demand with competitive pricing to maintain sales ([Neeley, 2025](https://carketa.com/auto-tariffs-used-car-pricing-inventory/)). - **Enhanced Customer Service**: To retain customers amid price fluctuations, dealers are offering flexible financing and personalized experiences, which may indirectly support higher price points ([Neeley, 2025](https://carketa.com/ auto-tariffs-used-car-pricing-inventory/)). ## Conclusion and Outlook The rise in used car prices in 2025 results from complex interplay of trade policy , lingering pandemic-related supply chain disruptions, economic conditions, and shifting consumer behavior. Tariffs on new vehicles and parts have increased new car prices, pushing more buyers into the used car market. Simultaneously, 20 supply chain voids-especially the shortage of lease returns and ex-rental vehicles-have constrained used car inventory. Economic factors such as high interest rates and inflation further shape consumer demand and affordability dynamics. While the current price increase is modest, data and expert analysis suggest that upward pressure on used car prices is likely to continue unless there is significant policy reversal or resolution of supply chain issues. Dealers and consumers alike must navigate these challenges with informed strategies, including diversifying sourcing channels and exploring financing options. Understanding these drivers is essential for making prudent decisions in the evolving automotive market landscape of 2025. --- ## References Carscoops. (2025, April 10). *Used Gas Car Prices Just Went Up For The First Time In 2 Years While EVs Keep Falling Fast*. https://www.carscoops.com/2025/04/usedcars-just-saw-their-first-price-bump-in-over-two-years/ Car Concierge Pro. (2025). *Trends in 2025 Used Car Prices: Visual Insights*. https ://carconciergepro.com/trends-in-2025-used-car-prices-visual-insights/ CNBC. (2025, April 12). *Auto tariffs expected to cut sales by millions, cost $100 billion*. https://www.cnbc.com/2025/04/12/auto-tariffs-sales-costs.html Dealership Guy News. (2025, January 17). *2025 auto market: affordable new car inventory rises, used car supply shrinks*. https://news.dealershipguy.com/p /2025-auto-market-affordable-new-car-inventory-rises-used-car-supply-shrinks -2025-01-17 Digital Dealer. (2025, March 10). *How COVID-19 Created Lasting Supply Chain Void in the Automotive Industry*. https://digitaldealer.com/sales-variable-ops/howcovid-19-created-a-lasting-supply-chain-void-in-the-automotive-industry/ Kelley Blue Book. (2025, April). *Average Used Car Price Starts to Rise*. https:// www.kbb.com/car-news/average-used-car-price-starts-to-rise/ Michigan Journal of Economics. (2022, January 5). *COVID-19, Supply Chain Shortages, and the Automobile Industry*. https://sites.lsa.umich.edu/mje/2022/01/05/covid -19-supply-chain-shortages-and-the-automobile-industry/ Motor. (2023, July). *Long COVID Continues to Impact Supply-Chain Issues and New Vehicle Inventory*. https://www.motor.com/2023/07/long-covid-continues-toimpact-supply-chain-issues-and-new-vehicle-inventory/ Neeley, S. (2025, April 18). *Navigating the Impact of Recent Auto Tariffs on Used Car Pricing and Inventory*. Carketa. https://carketa.com/auto-tariffs-used-carpricing-inventory/ Singh, C. (2025, April 11). *Used car prices are rising in 2025: Heres what you need to know*. USA TODAY. https://www.usatoday.com/story/money/2025/04/11/usedcar-prices-are-rising-2025/83050309007/ Tampa Bay AutoNetwork. (2025). *How Tariffs Will Affect New & Used Car Prices in 2025*. https://www.tampabayautonetwork.com/news/how-tariffs-will-affect-newused-car-prices-in-2025/ --- *Note: All URLs were accessed and verified as of April 25, 2025.*"
        },
        {
            "title": "D Evaluation Example",
            "content": "This section details the evaluation protocol along all described evaluation metrics for the report presented in the previous section. We start by showing the ground-truth key-points extracted from the relevant ClueWeb-22 documents, along with key-point recall and contradiction. Then, we show citation faithfulness and report quality metrics. D.1 Ground-truth Key Points from Clicked Documents Query: Why Have Used Car Prices Increased? Extracted key-points: 1. Car dealers were closed during Covid-19 lockdowns, leading to fewer new cars sold and decline in used cars being part exchanged, causing low supply in the used car market. 2. global semiconductor shortage has caused smaller supply of new cars, leading more buyers to the used car market and causing supply and demand issues, contributing to unprecedented rises in used car prices. 3. Increased demand for used cars is driven by consumers treating themselves to used cars instead of holidays, swapping expensive lease cars for affordable used models, and savings-rich customers, dealers, and rental fleets pushing up prices. 4. Used car dealerships have experienced shortage of stock as trade-ins have reduced, and decreased supply from fleet sales, repossessions, off-lease cars, and rental companies not selling used cars because they cannot buy new vehicles, shrinking supply and pushing prices up. 5. New car prices are rising due to short supply, which normally caps used car prices, but now both new and used car prices are increasing simultaneously. 6. Used car prices are expected to keep rising in the summer due to ongoing chip shortage and demand, but may stabilize in the fall. 7. Certain car sectors like the Audi Q7, sports cars, premium cars, SUVs, diesels, and sub20k petrol cars in small and medium market sectors are experiencing the greatest price increases and consumer interest during lockdown. 8. Affordable, cheap to run cars under 6k are expected to perform well as buyers may return to public transport or car sharing later. 9. Expansion of Londons Ultra Low Emission Zone (ULEZ) is causing owners of older diesel cars to sell them at lower prices in London, affecting local used car prices, while outside London demand for older diesel cars and all cars is strong, causing prices to rise. 10. The rise of online dealers has changed the market and contributed to the used-car price surge. 11. Since forecourts opened on 12 April, dealers have been overrun with people and supply is very low, with supply down 10.8% compared to 2019, and demand growing significantly, leading to record price growth rates and increased sticker prices as advised by Auto Trader. 12. The Covid-19 pandemic shuttered factories and disrupted shipping routes globally, causing backlog that is chief cause behind massive 25% climb in used car prices in 2021. 13. The pandemic changed consumer demand for cars, forcing many to cancel or postpone travel plans in 2020, leading to unprecedented demand for cars in spring 2021 as vaccines and relaxed public-health rules allowed travel. D.2 Key-Point Recall and Contradiction Table 5 summarizes key-point evaluation. The report does not contradict any of the keypoints, hence KPR for this report would be computed as 6/13, and KPC as 0/13. Key Point Label Summary 1 2 3 4 5 6 7 8 9 10 11 12 13 Consumer behaviors like swapping leases and treating themselves not mentioned. Supported COVID-19 reduced trade-ins and part-exchanges, lowering supply. Supported Chip shortages reduced new car supply, boosting used demand. Omitted Supported Dealer inventory shortages from fleet and rental supply issues. Supported Tariffs raised new car prices and pushed buyers toward used cars. No mention of summer/fall trends or chip shortage timing. Omitted No reference to vehicle types like SUVs or diesel in lockdown context. Omitted Cars under 6k and expectations for public transport recovery not covered. Omitted Omitted No mention of ULEZ or regional UK pricing differences. Supported Online dealers and market changes linked to price surges. Omitted Supported Omitted No mention of April forecourt reopening or Auto Trader commentary. Pandemic factory closures and shipping delays noted as price drivers. 2020 demand surge post-vaccine and lockdown easing not included. Table 5: Summary of LLM evaluation labels for 13 claims. D.3 Retrieval Faithfulness Table 6 presents sample of 6 claims extracted from the document, together with supporting URLs and justifications. Shown claims were rated as being fully supported by the source URLs. # 1 2 4 5 Claim Used car prices for oneto fiveyear-old vehicles increased by approximately 1% year-over-year as of March 2025, translating to an average price bump of about $317. The average used car price rose roughly $170 from February to March 2025, reaching $25,180. 25% tariff was applied to all new cars entering the U.S. in early 2025, with further tariffs on parts scheduled. Imported vehicles could see price increases of up to $6,000 due to tariffs, with domestic vehicles also rising around $3,600. Automakers responded by pausing shipments, adjusting strategies, or freezing U.S. exports, shrinking affordable vehicle supply. Justification The citation explicitly states that used car prices increased 1% YoY as of March 2025, translating to $317 increasematching the claim. Source(s) USA Today Fully supported by the source, which gives the exact figure and monthly change. The source details the 25% tariff beginning in April 2025 and pending parts tariffs. KBB KBB, Carketa The cited article provides these specific figures directly. CNBC Source confirms automakers are freezing exports and adjusting due to tariffs, limiting supply. Carscoops . . . (Claims 621 not shown) 22 Dealers are offering flexible financing and personalized experiences to retain customers amid price fluctuations, indirectly supporting higher price points. The citation confirms this strategy for retaining customers during price volatility. Carketa Table 6: Sample of LLM-evaluated claims for factual accuracy D.4 Quality Below we show the LLM-judge output for both clarity and insightfulness dimensions: Clarity (Score: 9/10) The answer is exceptionally clear, well-structured, and logically organized, resembling an in-depth report with clearly marked sections and strong logical flow. Each section addresses distinct factor 23 contributing to the increase in used car prices, such as tariffs, supply chain disruptions, economic conditions, consumer preferences, and dealer strategies, without substantive overlap or redundancy. The introduction and conclusion effectively frame and summarize the analysis. The use of precise data and references enhances clarity and rigor. Minor overlap exists in the economic and financing conditions section where some points slightly relate to consumer preferences, but these are sufficiently distinct in focus. The answer avoids ambiguity, redundancy, and conversational filler, maintaining professional and analytical tone throughout. Overall, it meets the criterion of clarity at very high level, with only negligible issues preventing perfect score. Insightfulness (Score: 9/10) The answer demonstrates high level of insightfulness by providing comprehensive, well-structured analysis that goes beyond common knowledge. It synthesizes multiple relevant factorstariffs, supply chain disruptions, economic conditions, consumer preferences, and dealer strategiesinto coherent explanation of why used car prices have increased. The report includes specific, concrete examples such as the 25% tariff on new cars, the impact of COVID-19 on lease returns, and detailed data from reputable sources, which grounds the analysis in practical reality. It also highlights less obvious connections, such as how tariffs on new cars indirectly inflate used car prices through demand shifts and supply constraints. The inclusion of segment-specific trends and regional variations adds depth and reframing to the topic. However, the report could improve by providing more explicit realworld examples of entities implementing strategies to mitigate these issues and the outcomes thereof, which would elevate it to perfect score. Overall, the answer meets the criterion of insightfulness very well, with only minor room for enhancement in operational recommendations and examples."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: The abstract and introduction note the main contributions. We make available an open search API that can be used for deep research systems, and show its feasibility through benchmarking experiments. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: After the conclusions and future work, specific \"limitations\" section notes the main limitations of the paper. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [NA] Justification: The paper does not include theoretical results. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Yes, besides providing source code for both retrieval infrastructure and experimentation reproduction, all used models are clearly described in the paper. Furthermore, all evaluation prompts are shown in the paper. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code 26 Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code is open-source. No new datasets were introduced; we instead leverage existing datasets which are open (although ClueWeb requires signing lightweight license). Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental setup section shows our test details. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We do not report error bars or repeat runs for the LLM-as-a-judge evaluation due to the computational cost of re-generating reports with multiple LLMs. However, to assess the robustness of our main findings, we conducted human evaluation study over representative subset of queries, with multiple annotators per instance. We report inter-annotator agreement across all evaluation dimensions, as well as system-level human preferences. These metrics serve as statistical validation of our automated evaluation. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. 27 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the latency of our API, and note that all evaluated systems were considered using their respective default configuration. No specific hardware is necessary to run these deep research systems as they rely on API calls. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The conducted research complies with the NeurIPS Code of Ethics. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper discusses the positive impacts of transparent search infrastructures. The limitation section discusses potential negative impacts associated with open web collections. 28 Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: Usage of our API requires full compliance with the underlying datasets license, which aims to mitigate misuse. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The creators of original assests are fully credit, and all licenses are properly respected. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. 29 The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The paper provides the URL to API documentation, as well as documented github pages. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [Yes] Justification: The human evaluation protocol is fully described in Section 4.3. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Section 4.3 mentions that all annotators are authors of the paper, hence no actual crowdsourcing was conducted. Guidelines: 30 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] Justification: LLMs use does not impact the core methodology, scientific rigorousness, or originality of the research. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "IST and INESC-ID",
        "NOVA LINCS"
    ]
}