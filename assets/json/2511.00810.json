{
    "paper_title": "GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding",
    "authors": [
        "Shijie Zhou",
        "Viet Dac Lai",
        "Hao Tan",
        "Jihyung Kil",
        "Wanrong Zhu",
        "Changyou Chen",
        "Ruiyi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 59.6% on ScreenSpot-Pro, 63.8% on OSWorld-G and 91.5% on ScreenSpot-v2. Project page: https://github.com/sjz5202/GUI-AIMA"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 2 0 1 8 0 0 . 1 1 5 2 : r GUI-AIMA: ALIGNING INTRINSIC MULTIMODAL ATTENTION WITH CONTEXT ANCHOR FOR GUI GROUNDING Shijie Zhou1, Viet Dac Lai2, Hao Tan2, Jihyung Kil2, Wanrong Zhu2 Changyou Chen1, Ruiyi Zhang2 (cid:66) University at Buffalo1, Adobe Research2 ryzhang.cs@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Graphical user interface (GUI) grounding is key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 59.6% on ScreenSpot-Pro, 63.8% on OSWorld-G and 91.5% on ScreenSpot-v2. Project page: https://github.com/sjz5202/GUI-AIMA."
        },
        {
            "title": "INTRODUCTION",
            "content": "Graphical User Interface (GUI) (Hong et al., 2024; Cheng et al., 2024) agents have emerged as pivotal tools in automating interactions with digital devices, spanning from mobile applications (Rawles et al., 2023; 2024; Wang et al., 2024a; Ye et al., 2025a) to desktop software (Zhang et al., 2024; Qin et al., 2025; Deng et al., 2023; Xie et al., 2024; OpenAI, 2025; Zheng et al., 2024). GUI grounding plays an important role in mapping natural language instructions to specific elements on the screen, such as buttons, text fields, or icons (Yang et al., 2024). This process ensures that the agents actions are precise and contextually relevant, especially in environments with high-resolution displays and intricate layouts. However, the diversity of human instructions and GUI designs across platforms and applications (Li et al., 2025) makes GUI grounding challenging task. Conventional methods rely on structured representations, such as HTML for web pages or accessibility trees (Zhang et al., 2025) for mobile applications. While these provide detailed information about the interface elements, they come with limited accessibility and excessive verbosity, which can lead to inefficiencies in processing. Moreover, structured data may omit essential visual cues such as layout and icons, which are critical for accurate grounding. When humans use computers, they first determine the general area and then use visual feedback to interact with GUI elements (Ye et al., 2025b). Directly outputting coordinates from MLLMs might not be the most intuitive method. Instead, models should mimic human behavior: Core contributors, majority work done while SZ is at University at Buffalo. 1 Figure 1: An example of GUI-AIMA with optional two-step GUI grounding for high-res screenshots. first selecting the relevant visual patch and then deciding the specific click position within that patch, i.e., the coordinates in the GUI. Previous studies, such as TAG (Xu et al., 2024a) and GUIActor (Wu et al., 2025), have explored the coordinate-free GUI grounding where corordinates are determined by selecting the most relevant visual patches in screenshot instead of directly generating text tokens. However, GUI-Actor introduces extra modules into the MLLM, which requires an additional adaptation stage. Xu et al. (2024a) considers vanilla but inaccurate way to identify and aggregate attention maps from all query text tokens, referred to as vanilla attention grounding. In this work, we propose GUI-AIMA, an attention-based method for coordinate-free GUI grounding. We train the MLLMs multi-head self-attention (MHSA) (Vaswani et al., 2017) with patchwise grounding supervision using novel attention head weighting mechanism. To simplify the impractical aggregation over all query tokens in vanilla attention grounding, we append learnable <ANCHOR> token after the visual and text query tokens and supervise its attention over the visual tokens as surrogate aggregator for all query tokens text-visual attentions. For precise multi-head aggregation, we weight each attention head by the magnitude of its queryvisual correlation estimated from the MLLMs visual-sink query tokens, rather than including visual-irrelevant query tokens as in vanilla attention grounding. The MLLMs general visual-sink query tokens is identified by computing similarity between query and visual token embedding from layer-wise hidden states instead of relying on correlations within each individual attention head. These visual-sink query tokens guide the attention head weighting, prioritizing attention heads with strong interactions between visual-sink query and visual tokens, consistent with the general query-visual pattern revealed in hidden states, while down-weighting other heads. Together, compared with the vanilla attention grounding, GUI-AIMA achieves 4.5% improvements on ScreenSpot-pro. By operating directly on intrinsic attention without task-specific modules, GUI-AIMA enables data-efficient visual grounding training with only 85k screenshots. In addition, the center-encouraging and overlap-aware patchwise labeling further boosts the performance of GUI-AIMA. Furthermore, offset grounding errors are prevalent on high-resolution screenshots due to dense visual content and detail loss from image compression in MLLMs. GUI-AIMA, with strong base grounding capacity for precise candidateregion localization, can combine the initially predicted region with an extra zoomed-in step without additional training to self-correct offset errors, as shown in Fig. 1. The main contributions of GUI-AIMA are summarized as follows: We introduce GUI-AIMA, an attention-based, coordinate-free framework that aligns intrinsic MHSA with patch-wise supervision by simplifying the vanilla attention-based visual grounding with anchored-attention predictions and designing an overlapand center-aware patch-wise labeling scheme that better aligns with human click behaviors. We propose simple and effective attention head weighting mechanism using visual-sink query tokens to emphasizes heads exhibiting strong query-visual interactions, improving efficiency and generalization without extra grounding modules. With only one-stage fine-tuning on small open-sourced training dataset, GUI-AIMA-3B outperforms all models of similar sizes and rivals much larger MLLM-based GUI grounding models. Ablations demonstrate the benefits of the special anchored token, head-wise weighting mechanism using visual-sink query tokens, and weighted visual patch labels. GUI-AIMA also provides insights into how to understand and specialize the functionality of intrinsic multimodal attention for visual grounding."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Coordinate-based GUI grounding: The core challenge for GUI agents (Wang et al., 2024b) is grounding: aligning user instructions with the correct actionable elements in the screenshot, due to 2 the semantic inconsistency of layouts and UI elements across diverse GUI environments (Liu et al., 2025). In early attempts, along with the screenshots, extra structured inputs, such as HTML for UGround (Gou et al., 2024), or extractions from visual parsing modules, such as element extractions from OmniParser (Wan et al., 2024; Yu et al., 2025) and generated captions as contexts in AriaUI (Yang et al., 2024), are fed into MLLMs as the supplementary inputs for the better interface understanding and more precise localization. Later works, such as AGUVIS (Xu et al., 2024b), SeeClick (Cheng et al., 2024) and OS-Atlas (Wu et al., 2024), explore the GUI grounding leveraging MLLMs (Bai et al., 2025; Chen et al., 2024b) with screenshot-only inputs, enabling the end-to-end localization with improved scalability across diverse GUI environments. The grounding approach in these methods is to generate coordinate-based click centers or bounding boxes as text, which is indirect alignment of visual grounding requiring additional efforts, such as scaled GUI corpora (Qin et al., 2025; Chai et al., 2024) and OCR pretraining (Hong et al., 2024) for connecting coordinates with UI elements. Recent endeavors manage to address this gap from the data-intensive manner of the coordinate-based GUI grounding methods via incorporating GUI-specific grounding reward signals into RL-training (Luo et al., 2025; Lu et al., 2025; Liu et al., 2025; Tang et al., 2025) and performing autonomous GUI exploration (Fan et al., 2025; Wu et al., 2025). to have begun replace grounding: Recent works Coordinate-free GUI traditional coordinate-based grounding text with patch-level attention map predictions, where patches belonging to the correct region receive higher weight. TAG (Xu et al., 2024a) first leveraged the multi-head self-attention between GUI instructions and visual tokens in MLLMs for tuning-free GUI grounding, showcasing the intrinsic potential of MLLMs attention patterns for GUI tasks. However, the generalization of TAG for novel interfaces is restricted by the backbone and by the heuristics used for selecting text tokens and attention heads. SE-GUI (Yuan et al., 2025) retains coordinate-based prediction manner but employs self-attention to iteratively filter noisy training samples during training. Aside these attention-based improvements, GUI-Actor (Wu et al., 2025) introduces an embedding-based grounding head that derives attention between input visual patch embeddings and the final hidden state of special <ACTOR> token. Compared with GUI-AIMA, GUI-Actor adds an additional module to connect cross-layer visual and <ACTOR> embeddings, requiring an extra warm-up training phase and resulting in much lower training efficiency."
        },
        {
            "title": "3 METHODS",
            "content": "The core idea of GUI-AIMA is utilizing the intrinsic query-visual pattern within attention maps It performs supervised fine-tuning on the multi-head self-attention matrices across of MLLMs. layers. Intuitively, as shown in Section 3.1, we can extract the attention vector between text token and all visual patch tokens in each attention map of MLLM as the patch-wise attention vector, where each visual patch tokens attention weight can indicate its degree of membership in the grounding region. And the tuning-free aggregation of patch-wise attention vectors between query tokens has shown the potential for text localization (Xu et al., 2024a). Existing barriers for attentionbased GUI grounding are how to aggregate patch-wise vectors for each query text tokens and each attention head. Besides, annotation gap between patch-wise and original coordinate labels further leads to inaccurate groundings. In GUI-AIMA, we propose general attention-based GUI grounding framework that simplify the query token selection in Section 3.3 and refine the head-wise weighting mechanism for the patch-wise attention vector aggregation in Section 3.4. Furthermore, as detailed in Section 3.5, we extend GUI-AIMA with two-step zoom-in inference that mitigates offset errors in grounding. Compared with previous embedding-based method, such as GUI-Actor (Wu et al., 2025), GUI-AIMA does not add extra grounding modules to MLLMs, eliminating the extra warmup training phase. 3.1 PRELIMINARY: INHERENT GUI GROUNDING WITHIN MULTI-HEAD SELF-ATTENTION Multi-head Self-Attention (MHSA). For MLLMs with layers and attention heads per layer, given the output embeddings Hl1 of the preceding transformer layer 1 with dimension d, query and key are computed as Ql,h = Hl1Wl,h for each attention head h, 1 , where Wl,h Rddh are parameters with dimension dh = d/H. Then, the and Kl,h = Hl1Wl,h , Wl,h Figure 2: With user query Q, screenshot patches and multi-head attentions {Al,h}l[L],h[H] from the MLLM, the vanilla attention grounding needs additional aggregation between all query tokens grounding vectors. In our proposed simplified version, special <ANCHOR> token can learn to implicitly aggregate all query tokens. Then we aggregate grounding vectors of <ANCHOR> token across layers and heads with carefully designed weights to produce the patch-wise predictions. attention matrix Al,h is computed as: Al,h = softmax (cid:16) Ql,hKl,h (cid:112) / (cid:17) dh + , Mij = (cid:26), 0, < , where is the attention mask for the causal attention Al,h. , Hl1 RVd is the embedding sequence of visual patch tokens, Hl1 Inherent GUI Grounding Indications in Al,h. Given the interface image token index sequence = [v1, . . . , vV] which is patch-wise for MLLMs and the user query token index sequence = [q1, . . . , qQ], we format the intermediate token sequence as Hl1 = [Hl1 ] for layer 1, RQd is the where Hl1 embedding sequence of the user query tokens. With the self-attention matrix Al,h computed from Hl1, the extracted patch-wise attention vector Al,h and the visual sequence Hl1 can reflect the patch-wise query-visual correlation. Among all patches V, the image patch vi with large attention value Al,h qi,vi is more likely to contain regions related to qi. Intuitively, the aggregation of all patch-wise attention vectors {Al,h qi,V }l[L],h[H],i[Q] among text tokens and attention heads can produce the inherent patch-wise GUI grounding indications: qi,V between each query token Hl1 qi ˆa = 1 (cid:88) l,h,i wqi,(l,h) Al,h qi,V RV, (1) where wqi,(l,h) is the aggregation weights for each query-visual attention vector with wqi,(l,h) 0 and (cid:80) l,h,i wqi,(l,h) = 1. We denote the aggregation in Eq. (1) as the Vanilla Attention Grounding shown as vanilla in Fig. 2. In the previous training-free attempt (Xu et al., 2024a) based on this manner, every selected text tokens are treated uniformly and attention heads are roughly selected. For the better adaptation of an attention-based GUI grounding method, we need simplified and effective weighting strategy via wqi,(l,h). 3.2 COORDINATE-FREE PATCH-WISE LABELING The inherent grounding of MLLMs via the query-visual attention in Eq. (1) is patch-wise instead of the pixel manner in the original coordinate-based bounding boxes. In order to enable the patch-wise grounding supervision, we first reformat the ground truth bounding box gtbbox = [x1, y1, x2, y2] into patch-wise label vectors RV, where is the patch size of the interface I. Image patch vi is positive only if there is an overlap between gtbbox and patch vi. Considering the border patches 4 which are partially overlapped with gtbbox should be less weighted and the patches of center region should be highlighted, we weight each patch with pvi according to the overlapping ratio of itself with gtbbox and the distance from center of patch vi to the center of gtbbox with Gaussian distribution following GUI-G2 (Tang et al., 2025). In more details, we define the overlap between positive image patch and grounding box annotation gtbbox using intersection over union ratio, denoted as IoU(vi, gtbbox). We take its center point µvi = (centervi ) and the center point µgt = (centergt ) of gtbbox for the following weighting scheme: , centergt , centervi pvi = IoU(vi, gtbbox) (µvi; µgt, Σgt) , )2(cid:1) with adaptive 2D standard deviations as σgt )2, (σgt where Σgt = diag(cid:0)(σgt = α (x2 x1) and σgt = α (y2 y1). Empirically, we set α = 0.8. In this manner, coordinate-free weighted patch label = normalize(cid:0){pvi}V (cid:1) is overlapping-aware and encouraging center-clicking. With the patch-wise prediction ˆa in Eq. (1) and ground truth label derived from annotations, the attention grounding loss LAttn is defined as the KL-Divergence between and ˆa: (2) i=1 LAttn = DKL(p normalize(ˆa)) (3) 3.3 EFFICIENT GROUNDING WITH VISUAL ANCHOR TOKEN The supervision on ˆa in Eq. (1) requires carefully calibrated balance across Al,h qi,V of each query token, where the weight of each query token wqi,(l,h) is difficult to determine. Moreover, direct supervision on the attention of query text tokens will impair the MLLMs general capabilities (e.g., image captioning) and thus harm generalization (Nguyen et al., 2023). To tackle this issue, we consider adding special <ANCHOR> token into the vocabulary and place it after GUI inputs to format [V, Q, <ANCHOR>]. This design simplifies the token-wise aggregation on {Al,h qi,V }l[L],h[H],i[Q] and disentangle general understanding functionality from GUI grounding. <ANCHOR> token serves as bridge between user query tokens and correct grounding patches. Intuitively, for each attention head, the patch-wise attention vector Al,h a,V RV between <ANCHOR> token and all visual tokens learns an implicit aggregation of {Al,h qi,V }qiQ over all query tokens. With the anchored attentions, the previous aggregation via the weight wqi,(l,h) in Eq. (1) can be simplified as the multi-head aggregation: ˆa = 1 (cid:88) l,h wl,h Al,h a,V RV, (4) with the query-ignored weight vector of attention heads = [ wl,h : [L], [H] ] R1LH . This simplified attention-based grounding is shown as simplified in Fig. 2. 3.4 ATTENTION HEAD WEIGHTING USING VISUAL-SINK QUERY TOKENS In Section 3.3, we introduce special anchor token to avoid aggregating patch-wise attention vectors over all query text tokens. In this section, we continue to discuss how to determine wl,h, the weights of different attention heads across different layers for the multi-head aggregation. Previous works (Li et al., 2023; Clark et al., 2019) has shown the functional diversity between attention heads across transformer blocks in LLMs. In GUI grounding, we want to identify attention heads, which shows active query-visual interactions. The attention head with large attention value between visual tokens and text query tokens is more relevant, while others are not. As GUI grounding in the attention manner aims to capture the query-visual correspondence between the user instruction and interface, the above strategy focusing on query-visual interactions allows the attention supervision skew towards the attention heads with large query-visual correlations and affect the neutral attention heads less, thus improving the inherent attention grounding of MLLMs without impairing the pretrained capacity, as the adaptation complies with the original attention functionalities. straightforward measurement of the query-visual correlations of Al,h is the cumulative sum of all query-visual attention entries in the vanilla attention grounding: wl,h = (cid:88) Al,h qi,vi qi,viQ,V 5 (5) Figure 3: Details about how to compute the final patch-wise prediction based on the grounding vectors of the <ANCHOR> token: GUI-AIMA first specifies visual-sink query tokens via computing hidden state similarities between query tokens and visual patches; Then it computes weights of each attention head based on visual-sink query tokens in Eq. (8); Finally, GUI-AIMA aggregates the grounding vectors of <ANCHOR> token across layers and heads in Eq. (4). This strategy is under-specified as not all text tokens in the query are necessary for query-visual connections. Since <ANCHOR> is introduced in GUI-AIMA to represent the context of the whole query sequence Q, similar simplification towards Eq. (5) can be applied as: wl,h = (cid:88) viV Al,h a,vi , (6) which is the cumulative sum of the attention entry only between visual tokens and <ANCHOR> tokens. However, <ANCHOR> is not granted to function as the representative token for context summarization at the initial stage of training, as the embedding of new introduced <ANCHOR> token is random initialized. Only relying on premature <ANCHOR> token as weights in Eq. (6) will bring in noisy and biased attention aggregations. Thus, we consider adaptively selecting query token for weighting instead of using all query tokens {qi} of the given instruction or <ANCHOR> in Eq. (5) and Eq. (6). We denote the chosen token for computing wl,h as visual-sink query tokens Qs, which are global active tokens for connecting visual inputs and query tokens. Visual-sink Query Tokens Qs Selection. For user query sequence Q, each text token qi reflects different visual correlations with visual tokens in (l, h)-indexed head as (cid:80) qi,vi , the cumulative sum of qis attention weights over all visual tokens. And the text tokens with the massive visual correlation inside the model vary in different user queries. Here, in order to identify the query tokens with strong visual affinity globally for the MLLM, we directly measure the Cosine similarity with intermediate hidden states Hl shown in the step 1 of Fig. 3: viV Al,h cl qi = (cid:88) vj Sim(Hl qi , Hl vj ). (7) (cid:80) h[H] viV Al,h Tokens with larger cl query-visual affinity of qi in layer using hidden states Hl instead of attention, i.e. (cid:80) qi has strong visual correlations in layer l. The reason of measuring the = qi,vi is that the query-visual pattern discovered in hidden states Hl is not necessary statistically prevailing among each heads self-attention matrix, as only smaller subset of attention heads are semantic heads that key on semantic functionality and representation similarity reflected in Hl (Elhelo & Geva, 2024; Olsson et al., 2022; Voita et al., 2019) (an analysis is available in Section A). In comparison, the measurement in Eq. (7) is straightforward to reflect visual correlations of each text token in global and general view. For grounding, we prioritize the semantic minority heads showing the similar pattern discovered from hidden states H. Here, we denote the query tokens with topK large cl s. The general query-visual qi as the visual-sink query tokens Ql cl qi 6 correlation pattern of the MLLM captured by Ql the attention head heavily weighted for grounding for grounding, i.e., Ql visual correlation in those heads. We measure the visual correlation on Ql attention head as the cumulative sum of attention values between Ql step 2 of Fig. 3, with further normalization: derived from hidden states should be consistent in should also exhibit strong tokens in the individual and in Al,h, as shown in the wl,h = (cid:88) Al,h qs ,vj , wl,h = qs Ql s,vj exp (wl,h) h=1 exp(cid:0)wl,h (cid:80)H (cid:1) . (cid:80)L l=1 (8) That is, for semantic attention head with the strong query-visual affinity measured as Eq. (8) based on Qs, it should be emphasized using its large wl,h for aggregations in Eq. (4). In this way, the highlighted attention heads share the aligned MLLMs general patterns found in the hidden state Hl as Eq. (7). In practice, we found that unifying the same Qs for all layers wl,h, that is {1, . . . , L}: Global uniform: Ql = Qs := arg topK (cqi), (9) qiQ where cqi = (cid:80)L l=1 different in each layer: qi, leads to better training stability and performance, instead of layer-wise Ql Layer-wise: Ql := arg topK (c qi ). (10) qiQ After obtaining Qs, we compute head-wise weight wl,h via Eq. (8) and perform multi-head aggregation for final prediction ˆa via Eq. (4). Soft pattern matching considering Qs and other tokens in weighting. Above matching between the MLLMs general and head-wise query-visual patterns only considering representative visual-sink query tokens Qs. Alternatively, we can define the two token-wise query-visual disi=1) from cqi = (cid:80)L tributions: MLLMs general Dg = normalize([cqi ]Q 1 cl qi , and head-wise ]Q Dl,h i=1) from dl,h vj Al,h qi,vj . We can then measure the closeness qi between Dg and Dl,h head via negative Kullback-Leibler divergence as the weight wl,h to encourage attention head showing similar query-visual pattern with the MLLMs general query-visual pattern: head = normalize([dl,h qi = (cid:80) wl,h = DKL(DgDl,h head). (11) This strategy provides the soft matching between all query tokens. In addition to Qs, the visualirrelative query tokens are also included, but with less contributions. We denote this soft matching variant as GUI-AIMA-3B (soft). 3.5 TWO-STEP INFERENCE WITH ZOOM-IN Under GPU memory constraints, high-resolution GUI screenshots are usually down-sampled, yielding fewer visual patch tokens for the MLLM. The resulting information loss and reduced finegrained spatial granularity inevitably harm grounding accuracy. GUI-AIMA provides flexible spatial granularity since its patch-wise grounding. We can easily perform two-step inference by adding zoom-in without extra training: (1) feed the compressed high-resolution screenshot to predict the approximate location. We use the center of this predicted location to determine specific area to focus on. (2) we re-run the inference on the newly cropped region, providing much more accurate result. This two-step inference targets to mitigates failure cases on high-resolution screens where the model identifies the right region but the center prediction is slightly offset the ground-truth box. Detailed observations and analysis of the zoom-in strategy is provided in Section 4.3."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Implementation Details. We apply Qwen2.5-VL-3B-Instruct (Bai et al., 2025) as the MLLM backbone for GUI-AIMA and all ablation variants. We follow the special token setting in GUIActor (Wu et al., 2025) and retain the next-token prediction loss for GUI-AIMAs grounding format. The training of GUI-AIMA-3B is conducted on 8 NVIDIA A100-80G GPUs with effective global batch size 64 and learning rate 5e-6. We set α as 0.8 for the adaptive deviation control in Eq. (2). 7 Table 1: Performance comparison of different models across various task categories based on Text, Icon, and Average scores on ScreenSpot-Pro. - indicates unreported results in original papers. Methods with are Qwen-2.5-VL-based. Dev Average Office CAD OS Text Icon Text Icon Creative Icon Text Scientific Icon Text Model GPT-4o n Claude Computer Qwen2.5-VL-3B Qwen2.5-VL-7B OS-Atlas-7B UGround-V1-7B UI-TARS-72B JEDI-3B JEDI-7B UI-TARS-1.5-7B GUI-Actor-3B GUI-Actor-7B UI-R1-E-3B InfiGUI-R1-3B GUI-G1-3B SE-GUI-3B GUI-G2-3B S 2.0 14.5 9.1 16.8 0.0 3.7 7.3 1.6 12.2 4.7 1.2 15.8 18.8 12.5 27.4 9.4 38.0 14.1 38.6 11.0 - - - - 37.1 12.5 33.0 14.1 39.6 9.4 38.1 12.5 9.4 20.3 GUI-AIMA-3B 44.7 21.9 GUI-AIMA-3B (sof t) 53.3 15.6 GUI-AIMA-3B + zoom-in 65.0 34.4 1.3 22.0 22.1 46.8 0.0 3.9 1.4 4.1 33.1 1.4 2.8 51.9 62.9 17.2 61.0 13.8 42.9 11.0 58.4 12. - - - - 46.1 6.9 51.3 12.4 50.7 10.3 7.6 55.8 2.1 42.2 68.2 28.3 62.3 26.9 72.7 36.6 1.0 25.9 26.8 35.9 0.0 3.4 2.1 7. 28.8 2.8 9.7 47.5 57.1 15.4 53.5 8.4 50.0 11.9 58.1 15.4 - - - - 4.2 41.9 44.9 7.0 36.6 11.9 4.9 47.0 2.8 43.4 59.6 21.7 58.1 15.4 66.7 23.8 2.1 0.0 33.9 15.8 7.3 38.2 7.3 49. 37.5 7.3 57.6 14.5 64.6 20.9 54.2 18.2 72.9 25.5 66.7 21.9 - - - - 56.9 21.8 58.3 20.0 61.8 30.0 61.8 16.4 43.8 10.0 69.4 37.3 72.2 35.5 81.3 41.8 Text Icon Text Icon Text Icon Avg. 1.1 0.0 30.1 16.3 33.9 15.1 52.5 20. 33.9 5.7 60.5 13.2 63.3 26.4 64.4 32.1 75.1 47.2 74.6 35.9 - - - - 65.0 26.4 65.5 28.3 67.2 32.1 59.9 24.5 46.3 15.1 70.6 49.1 73.5 49.1 85.3 60.4 0.0 11.0 10.3 37. 0.0 4.5 1.1 6.7 27.1 4.5 7.9 38.3 42.1 15.7 38.3 9.0 33.6 16.9 49.5 13.5 - - - - 32.7 10.1 43.9 12.4 23.5 10.6 40.2 12.4 4.5 29.0 65.4 31.5 62.6 30.3 69.2 46. 1.3 23.4 23.6 38.9 0.0 7.1 3.8 7.1 0.8 17.1 16.1 26.8 28.1 4.0 18.9 31.1 8.1 45.2 50.9 17.6 38.1 49.8 13.7 36.1 52.6 18.2 39.5 57.5 16.9 42.0 42.2 - 44.6 - - - - - 33.5 49.1 14.1 35.7 49.5 16.8 37.1 50.4 11.8 35.9 25.5 6.0 37.6 62.0 30.0 49.8 63.2 27.0 49.3 73.1 37.8 59.6 Regarding the setting of visual-sink query token Qs, we select the top-1 global large tokens complying with Eq. (9). As for the training dataset, we employ the entire training set of GUIAct (Chen et al., 2024a), AndroidControl (Li et al., 2024), Wave-UI (Jeffries & Team, 2024), randomly sampled 60k samples from UGround (Gou et al., 2024) and from GTA1 training set (Yang et al., 2025), respectively, with 259k instructions and 85k images in all. For the 45k ablation training set for Section 4.2, we select the first 10k samples from GUIAct, AndroidControl, Wave-UI and first 15k samples from UGround. GUI-AIMA-3B is trained for one epoch with all parameters unfrozen, without extra modules and warmup stage. Implementation of baselines is in Section B. For two-step inference introduced in Section 3.5, we set the crop size to 448 pixels and adopt 2 zoom-in ratio for the second pass. Efficient Extraction of Self-Attention Map Fast and memory-efficient attention implementations, such as FlashAttention Dao et al. (2022), avoid materializing the full attention matrix and therefore do not store intermediate attention weights. By contrast, the eager execution in original transformer attention Vaswani et al. (2017) can return the full attention maps, but it is neither fast nor memory-efficient, which becomes bottleneck for large-scale training and for scaling up model size. In GUI-AIMA, we address this by combining FlashAttention with eager attention: we use FlashAttention for the regular layer-to-layer forward pass, and then reuse the same attention parameters to compute partial attention map, covering the rows of text tokens and the <ANCHOR> token, via eager execution for GUI-AIMAs attention grounding. Since visual tokens dominate the attention in MLLMs, the additional computation and memory cost is ignorable. Evaluation datasets and metrics. We evaluate GUI-AIMA, baselines and ablation variants on ScreenSpot-v2 (Wu et al., 2024) for general GUI visual grounding evaluation across mobile, desktop and web domains, ScreenSpot-Pro (Li et al., 2025) for testing on challenging higher-resolution screenshots from diverse and complex professional software scenarios, e.g. interfaces from different Operating Systems with large distribution gap, and OSWorld-G (Xie et al., 2025a). All benchmarks contain separated text-centered and icon-centered visual grounding tasks, where the icon set is the more abstract visual-based task less hinted by text. For grounding accuracy, we follow the standard center point-based metric (Lin et al., 2024) that the correct click-point prediction locates within the ground-truth bounding box. Baselines. Three categories of methods are compared: (1) General MLLMs including GPT4o (OpenAI, 2024), Gemini-2.5-Pro (Comanici et al., 2025), Claude Computer (Hu et al., 2024), Operator (OpenAI, 2025), Qwen2.5-VL (Bai et al., 2025); (2) GUI-specific supervised fine-tuned model: OS-Atlas (Wu et al., 2024), UGround (Gou et al., 2024), UI-TARS (Qin et al., 2025), 8 Table 2: Performance comparison of different models across various task categories based on Mobile, Desktop, Web and Average scores on ScreenSpot-v2. - indicates unreported results in original papers. Methods with are Qwen-2.5-VL-based. Model Operator n GPT-4o + OmniParser-v2 Qwen2.5-VL-3B Qwen2.5-VL-7B S OS-Atlas-7B UGround-V1-7B UI-TARS-1.5-7B UI-TARS-7B JEDI-3B JEDI-7B GUI-Actor-3B UI-R1-E-3B GUI-G2-3B GUI-AIMA-3B GUI-AIMA-3B (sof t) Mobile Desktop Web Text 47.3 95.5 93.4 97.6 95.2 95.0 95.9 96.9 96.6 96.9 97.3 83.0 96.5 99.2 99.2 Icon 41.5 74.6 73.5 87. 75.8 83.3 84.8 89.1 81.5 87.2 86.9 97.1 82.5 85.9 84.8 Text 90.2 92.3 88.1 90.2 90.7 95.0 94.9 95.4 96.9 95.9 95. 85.0 95.4 96.1 95.6 Icon 80.3 60.9 58.6 74.2 63.6 77.8 80.7 85.0 78.6 87.9 81.7 91.7 75. 88.9 84.9 Text 92.8 88.0 88.0 93.2 90.6 92.1 90.6 93.6 88.5 94.4 94.6 77.9 88.4 96.1 94. Icon 84.3 59.6 71.4 81.3 77.3 77.2 86.2 85.2 83.7 84.2 81.4 95.4 72.4 80.2 80.2 Avg. 70.5 80.7 80.9 88.8 84.1 87.6 89.7 91.6 88.6 91.7 90.4 89.2 86.3 91.5 90.5 JEDI (Xie et al., 2025b) and GUI-Actor (Wu et al., 2025) ; (3) GUI-specific Reinforcement finetuned model: UI-R1 (Lu et al., 2025), InfiGUI-R1 (Liu et al., 2025), GUI-G1 (Zhou et al., 2025), SE-GUI (Yuan et al., 2025), GUI-G2 (Tang et al., 2025). 4.1 MAIN RESULTS: GROUNDING PERFORMANCE We compare GUI-AIMA with state-of-the-art methods on ScreenSpot-Pro, ScreenSpot-v2 and OSWorld-G and report results in Table 1, Table 2 and Table 3. While only being trained with 259k samples without filtering or careful selection, GUI-AIMA-3B outperforms all same scale 3B MLLM-based models and shows comparable or even better results than larger scaled models. Among Qwen-2.5-VL based models, marked with in both tables, GUI-AIMA-3B can better handle the visual grounding in complex software scenarios across different platforms and achieves the best performance on the most challenging ScreenSpot-Pro benchmark, especially on the abstract icon task set. Specifically, GUI-AIMA-3B is better than strong large size coordinate-based UI-TARS1.5-7B, JEDI-7B, and also better than the embedding-based coordinate-free GUI-Actor-7B model, highlighting the superiority of directly supervising on the multi-head self-attention weights instead of modeling the query-visual attention map via hidden states. On ScreenSpot-v2, GUI-AIMA-3B achieve the comparable results with the strongest baselines, such as JEDI-7B and UI-TARS-7B, and better than the same size GUI-Actor-3B, while GUI-AIMA-3B is trained with much less web data. Besides the advanced performance, another advantage is the training efficiency of GUI-AIMA with only 259k training elements, as most supervised fine-tuned baselines trained on millions of GUI elements. For the comparison with reinforcement fine-tuned baselines, while both trained with smaller training sets than SFT baselines, GUI-AIMA-3B performs better and shows better generalization. Among GUI-AIMA-3B and GUI-AIMA-3B (soft), GUI-AIMA-3B is slightly better on dealing with diverse graphic environments in ScreenSpot-v2, OSWorld-G and ScreenSpot-pro. The two-step inference with zoom-in without extra training significantly improves the performance of GUI-AIMA on high-resolution benchmarks, ScreenSpot-pro and OSWorld-G, specifically 59.6% on ScreenSpotpro using GUI-AIMA-3B (soft) and 63.8% on OSWorld-G using GUI-AIMA-3B, demonstrating the flexibility of GUI-AIMAs attention-based patch-wise grounding for inference-time improvements. 4.2 ABLATIONS Table 4 shows the ablation results on ScreenSpot-v2 and ScreenSpot-Pro. All the ablation variants are trained on the 45k ablation training data with Qwen2.5-VL-3B-Instruct as the backbone. GUIActor (45k) is trained in two stages same as the original setting. 9 Table 3: Performance comparison of different models across various task categories based on Mobile, Desktop, Web and Average scores on OSWorld-G. Methods with are Qwen-2.5-VL-based. Model Text Matching Element Recognition Layout Understanding Fine-grained Manipulation Operator n T Gemini-2.5-Pro Qwen2.5-VL-3B Qwen2.5-VL-7B OS-Atlas-7B UGround-V1-7B UI-TARS-7B JEDI-7B GUI-Actor-7B UI-TARS-1.5-7B JEDI-3B GUI-Actor-3B GUI-AIMA-3B GUI-AIMA-3B (sof t) GUI-AIMA-3B+ zoom-in 51.3 59.8 41.4 45.6 44.1 51.3 60.2 65.9 65.9 52.6 67.4 64.4 64.8 63.6 71. 42.4 45.5 28.8 32.7 29.4 40.3 51.8 55.5 62.7 75.4 53.0 60.6 65.5 63.6 70.6 46.6 49.0 34.8 41.9 35.2 43.5 54.9 57.7 66.4 72.4 53.8 64.8 68.8 67.2 73. 31.5 33.6 13.4 18.1 16.8 24.8 35.6 46.9 38.2 66.7 44.3 33.6 36.8 34.9 47.4 Avg. 40.6 45.2 27.3 31.4 27.7 36.4 47.5 54.1 56.6 64. 50.9 54.6 58.3 56.9 63.8 Table 4: Ablation results on ScreenSpot-v2 and ScreenSpot-Pro of coordinate-free GUI grounding methods fine-tuned with fixed 45k randomly sampled dataset. Variants in blue represent the selected settings for GUI-AIMA. Model ScreenSpot-v2 Icon Text Avg. ScreenSpot-Pro Icon Text Avg. Existing Coordinate-free GUI Grounding GUI-Actor-3B (45k) Vanilla Attention Grounding (45k) 96.24 95.68 75.99 75. 87.42 86.87 50.67 53.02 12.25 12.42 35.99 37.51 Simplified Attention Grounding: GUI-AIMA-3B (45k) Attention Head Weighting without Qs + weighting uniformly + weighting with all Eq. (5) + weighting with <ANCHOR> in Eq. (6) Attention Head Weighting with Qs in Eq. (8) + layer-wise top-1 Ql + layer-wise top-3 Ql + global top-1 Qs + global top-3 Qs 97.08 96.24 96. 96.52 96.66 97.49 95.82 97.21 + weighted patch-wise labeling Multi-head weighting softly in Eq. (11) + weighted patch-wise labeling 78.34 78.34 76.35 81.23 79.06 78.88 70.94 79.60 88.92 88.44 87.81 89.86 88.99 89.39 84.98 89. 56.50 52.61 56.91 57.32 57.83 59.26 57.01 61.00 13.91 13.41 14.57 15.73 15.07 14.40 14.40 14.90 40.23 37.63 40.73 41.43 41.49 42.13 40.73 43. 96.94 80.14 89.62 59.37 15.89 42. Compare Coordinate-free modeling manners. In Table 4, we include 3 different coordinate-free GUI grounding manners, one embedding-based: GUI-Actor which relies on hidden embeddings for computing similarity with extra modules, and two attention-based: vanilla attention grounding as Eq. (1) and simplified attention grounding as Eq. (4). With the same importance on all the query tokens and attention heads, the vanilla attention grounding can converge faster than GUI-Actor on more complex visual grounding tasks in ScreenSpot-Pro. When simplifying the aggregation on query tokens, even with naive weighting uniformly, the simplified attention grounding can have 1.50% and 4.24% improvement over GUI-Actor for ScreenSpot-v2 and ScreenSpot-Pro. These results demonstrate the effectiveness and faster convergence of attention-based visual grounding methods than embedding-based methods, and shows the advantage of compressing query contexts into <ANCHOR> token instead of manually control the grounding importance from each query token. GUI grounding without Visual-sink Query Token Qs. For attention-based GUI grounding variants, weighting the query-visual correlation of each head via the cumulative sum of all querys predictions in Eq. (5) or only the prediction from <ANCHOR> in Eq. (6) leads to more biased attention predictions than uniform weighting, with worse ScreenSpot-v2 performance of both variants 10 Figure 4: Model convergence of the 45k ablation dataset on ScreenSpot-Pro benchmark. Table 5: Analysis experiment results on ScreenSpot-pro. Relax@k measures how many previously incorrect offset predictions are recovered when the ground-truth bounding box is expanded by visual patches along each dimension. Recovered refers to the number of offset predictions corrected by the second step, while Lost refers to the number of predictions that degrade after the second step. Model 1-step Origin 1-step 2-step Crop-only Crop + 1.5Zoom-in Crop + 2Zoom-in Crop + 3Zoom-in Crop + 4Zoom-in Relax@1 Relax@2 Relax@5 Total Recovered Lost Acc. 158 123 47 39 39 31 83 74 63 69 72 56 43 38 45 44 38 306 249 159 147 152 141 - 156 208 215 219 224 - 47.06 107 48 33 42 48 50.16 57.18 58.57 58.25 58.19 and no improvement on ScreenSpot-Pro as shown in Attention Head Weighting without Qs part of Table 4. This bias comes from the misleading weighting from visual-unrelated tokens for the variants using all and from using the premature <ANCHOR> token for grounding in the later variant. For attention head weighting softly as in Eq. (11), it achieves similar overall performance on both datasets but performs worse on text elements. The effect of Visual-sink Query Token Qs. For ablation of using Qs for wl,h, we vary the value between 1 and 3 in the topK selection of Qs tokens and explore whether differing Ql for each MLLMs layer as in Eq. (10) or using the same Qs for all layers as in Eq. (9). In Table 4, we can observe that the layer-wise manner performs slightly better on the icon-based tasks, but globallyselected uniform Qs leads to more balanced results on both text and icon sets. Among both manners, top-1 selection remains the overall best performance. These ablations verify the global top-1 Qs setting of GUI-AIMA, with 1.90% improvement over weighting uniformly on ScreenSpot-Pro. Patch-wise labeling considering overlapping and distance. Here, we also justify the overlappingand distance-aware (Tang et al., 2025) patch-wise labeling for fine-tuning GUI-AIMA in Eq. (2). Based on global top-1 Qs, down-weighting partial positive and distanced image patch instead of labeling all patch equally leads to 1.26% further enhancement on ScreenSpot-Pro. Training efficiency. In Fig. 4 , we show the convergence tendency of three methods: GUI-AIMA, vanilla attention grounding and GUI-Actor (with extra warm-up stage already) initialized from Qwen-2.5-VL-3B. GUI-AIMA converges fastest and achieves the best final results, with steady improvements. Vanilla attention grounding fluctuates as supervision on all tokens from the pretrained vocabulary impairs the general capacity. And GUI-Actor needs longer training period to customize its extra modules for GUI grounding. 4.3 ANALYSIS OF TWO-STEP INFERENCE WITH ZOOM-IN In Table 5, we present the analysis results of the two-step inference with zoom-in introduced in Section 3.5. The analysis focuses on incorrect offset predictions, where the predicted center points fall outside the ground-truth bounding box by small degree. Table 5 reports the statistics of offset errors for one-step and two-step inference with different offset distance, and the number of recovered predictions as well as the lost originally-correct predictions after applying the second crop-and-zoom-in step. We observe that performing inference on the focused region determined by the first-step prediction already provides more than 3% improvement (Crop-only), as fewer irrelevant patches act as distracting noise for our patch-wise grounding. And after cropping, zooming in by factor from 1.5 to 4 significantly reduces offset errors, especially those with slight deviation (Relax@1). It also helps resolving large-offset errors, achieving up to 31% and 32% error reduction for Relax@3 and Relax@5, respectively. From the analysis results, we find that the performance of two-stem inference is not very sensitive with the zoom-in factor, while 2-time zoom-in provides the best performance."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We presented GUI-AIMA, an attention-based, coordinate-free approach to GUI visual grounding that aligns intrinsic multi-head self-attention with patch-wise supervision. With soft, overlapand center-aware patch labels converted from coordinate-based annotations, GUI-AIMA simplifies the vanilla attention visual grounding via learnable <ANCHOR> token as the surrogate to aggregate query-to-visual attention heads. To effectively aggregate different attention heads, we first identify visual-sink query tokens based on query-visual similarity between hidden states. Experimental results of GUI-AIMA on ScreenSpot-v2 and the challenging ScreenSpot-Pro benchmarks, showing state-of-the-art performance at the 3B scale and rivals other methods with larger MLLM backbones, using only about 85k training screenshots. Ablations further verify the effectiveness of each design choice, such as anchored attention aggregation, instruction-adaptive head weighting with visual-sink query tokens, and weighted patch labels. In future work, extending GUI-AIMA to more general and complex visual grounding tasks are pending explorations."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Shuai Ren, and Hongsheng Li. Amex: Android multi-annotation expo dataset for mobile gui agents. arXiv preprint arXiv:2407.17490, 2024. Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317, 2024a. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher Manning. What does bert look at? an analysis of berts attention. arXiv preprint arXiv:1906.04341, 2019. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. Amit Elhelo and Mor Geva. Inferring functionality of attention heads from their parameters. arXiv preprint arXiv:2412.11965, 2024. Yue Fan, Handong Zhao, Ruiyi Zhang, Yu Shen, Xin Eric Wang, and Gang Wu. Gui-bee: Align gui action grounding to novel environments via autonomous exploration. arXiv preprint arXiv:2501.13896, 2025. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1428114290, 2024. Siyuan Hu, Mingyu Ouyang, Difei Gao, and Mike Zheng Shou. The dawn of gui agent: preliminary case study with claude 3.5 computer use, 2024. URL https://arxiv.org/abs/ 2411.10323. Daniel Jeffries and KentaurosAI Team. Wave ui dataset, 2024. URL https://huggingface. co/datasets/agentsea/wave-ui. Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. 13 Kenneth Li, Oam Patel, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from language model. Advances in Neural Information Processing Systems, 36:4145141530, 2023. Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on ui control agents. Advances in Neural Information Processing Systems, 37:9213092154, 2024. Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. arXiv preprint arXiv:2411.17465, 2024. Yuhang Liu, Zeyu Liu, Shuanghe Zhu, Pengxiang Li, Congkai Xie, Jiasheng Wang, Xueyu Hu, Xiaotian Han, Jianbo Yuan, Xinyao Wang, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infiguig1: Advancing gui grounding with adaptive exploration policy optimization, 2025. URL https: //arxiv.org/abs/2508.05731. Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing efficient action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. Duc Hau Nguyen, Cyrielle Mallart, Guillaume Gravier, and Pascale Sebillot. Regularization, semisupervision, and supervision for plausible attention-based explanation. In International Conference on Applications of Natural Language to Information Systems, pp. 285298. Springer, 2023. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, In-context learning and induction Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. heads. arXiv preprint arXiv:2209.11895, 2022. OpenAI. Introducing gpt-4o. Available at: https://openai.com/index/hello-gpt-4o, 2024. OpenAI. Computer-using agent: Introducing universal interface for ai to interact with the digital world, 2025. URL https://openai.com/index/computer-using-agent. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:5970859728, 2023. Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, et al. GUI-G2: Gaussian reward modeling for gui grounding. arXiv preprint arXiv:2507.15846, 2025. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Elena Voita, Talbot, Moiseev, Sennrich, and Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arxiv 2019. arXiv preprint arXiv:1905.09418, 2019. 14 Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, and Zhibo Yang. Omniparser: unified framework for text spotting key information extraction and table recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1564115653, June 2024. Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024a. Shuai Wang, Weiwen Liu, Jingxuan Chen, Yuqi Zhou, Weinan Gan, Xingshan Zeng, Yuhan Che, Shuai Yu, Xinlong Hao, Kun Shao, et al. Gui agents with foundation models: comprehensive survey. arXiv preprint arXiv:2411.04890, 2024b. Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, et al. Gui-actor: Coordinate-free visual grounding for gui agents. arXiv preprint arXiv:2506.03143, 2025. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. Xie, Deng, Li, Yang, Wu, Chen, Hu, Wang, Xu, Wang, et al. Scaling computer-use grounding via user interface decomposition and synthesis. URL https://arxiv. org/abs/2505.13227, 2025a. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, et al. Scaling computer-use grounding via user interface decomposition and synthesis. arXiv preprint arXiv:2505.13227, 2025b. Hai-Ming Xu, Qi Chen, Lei Wang, and Lingqiao Liu. Attention-driven gui grounding: Leveraging pretrained multimodal large language models without fine-tuning, 2024a. URL https: //arxiv.org/abs/2412.10840. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024b. Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, Ran Xu, Liyuan Pan, Caiming Xiong, and Junnan Li. Gta1: Gui test-time scaling agent, 2025. URL https://arxiv.org/abs/2507.05791. Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-ui: Visual grounding for gui instructions. arXiv preprint arXiv:2412.16256, 2024. Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, et al. Mobile-agent-v3: Foundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025a. Xianhang Ye, Yiqing Li, Wei Dai, Miancan Liu, Ziyuan Chen, Zhangye Han, Hongbo Min, Jinkui Ren, Xiantao Zhang, Wen Yang, et al. Gui-arp: Enhancing grounding with adaptive region perception for gui agents. arXiv preprint arXiv:2509.15532, 2025b. Wenwen Yu, Zhibo Yang, Jianqiang Wan, Sibo Song, Jun Tang, Wenqing Cheng, Yuliang Liu, and Xiang Bai. Omniparser v2: Structured-points-of-thought for unified visual text parsing and its generality to multimodal large language models, 2025. URL https://arxiv.org/abs/ 2502.16161. 15 Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, et al. Enhancing visual grounding for gui agents via selfevolutionary reinforcement learning. arXiv preprint arXiv:2505.12370, 2025. Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. Ufo: ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939, 2024. Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pp. 120, 2025. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, and Jun Xu. Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents. arXiv preprint arXiv:2505.15810, 2025. 16 ANALYSIS FOR VISUAL-SINK QUERY TOKEN Figure 5: Magnitude of normalized visual correlation of query tokens computed from hidden states (Embedding) and from multi-head self-attention (Attention). In Fig. 5, we compare normalized distributions of global visual-token correlation (cid:80)L computed from two different manners. based on hidden states. And Attention denotes visual-token correlation of qi with cl qi (cid:80) l=1 qi qi computed from Eq. (7) = qi,vi computed from multi-head attentions. From Fig. 5, we can observe different visual-correlation patterns: while the text token with largest magnitude in Attention manner sometimes falls into semantic-irrelevant tokens, such as this, embedding manner shows larger magnitudes on the <im end> token. Embedding denotes the cl viV Al,h h[H] (cid:80) The observations above verify the claim in paper that the query-visual pattern discovered in hidden states Hl is not necessarily statistically prevailing among each heads self-attention matrix, as only smaller subset of attention heads are semantic heads that key on semantic functionality and representation similarity, which is also supported by Elhelo & Geva (2024); Olsson et al. (2022); Voita et al. (2019). From ablation results in Section 4.2, the global pattern indicated from visual-sink query token computed Qs from hidden states achieved better performance, supports our selection that complies with the visual-query pattern from hidden states for weighting attention grounding."
        },
        {
            "title": "B IMPLEMENTATIONS OF BASELINES",
            "content": "Most baselines results are taken from the original papers, except GUI-G2-3B which is not reported in the source."
        },
        {
            "title": "C EXTRA DETAILS OF ANCHOR TOKEN IMPLEMENTATIONS",
            "content": "In Section 3.3, we abbreviate the implementation of [V, Q, <ANCHOR START>, <ANCHOR>, <ANCHOR END>] as single-area prediction setting in GUI [V, Q, <ANCHOR>] for brevity. For the 17 (a) Lock the memo. (b) View the language & region settings. (c) Scan QR code. (d) View settings of cocowhip light. (e) Add new page. Figure 6: Visualization examples of GUI-AIMAs grounding results on ScreenSpot-v2. grounding, we explore to expand single <ANCHOR> to multiple <ANCHOR> tokens as [V, Q, <ANCHOR 0>, <ANCHOR 1>, . . . , <ANCHOR N>] and end tokens). However, it turns out to bring no performance gains and merely adds redundancy for the singleregion grounding tasks. We will explore the multi-region grounding tasks with disentangled <ANCHOR n> token for each grounding region as future works. (abbreviate start GUI GROUNDING EXAMPLES OF GUI-AIMA We provide the visualizations of GUI-AIMAs multi-head attention grounding results on ScreenSpot-v2 and ScreenSpot-Pro as follows in Fig. 6, Figs. 7 and 8. 18 (a) Add long title. (b) Change color theme in PyCharm. Figure 7: Visualization examples of GUI-AIMAs grounding results on ScreenSpot-Pro. 19 (a) Save file. Figure 8: Visualization examples of GUI-AIMAs grounding results on ScreenSpot-Pro. (b) Select the only feature."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University at Buffalo"
    ]
}