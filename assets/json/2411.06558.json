{
    "paper_title": "Region-Aware Text-to-Image Generation via Hard Binding and Soft Refinement",
    "authors": [
        "Zhennan Chen",
        "Yajie Li",
        "Haofan Wang",
        "Zhibo Chen",
        "Zhengkai Jiang",
        "Jun Li",
        "Qian Wang",
        "Jian Yang",
        "Ying Tai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we present RAG, a Regional-Aware text-to-image Generation method conditioned on regional descriptions for precise layout composition. Regional prompting, or compositional generation, which enables fine-grained spatial control, has gained increasing attention for its practicality in real-world applications. However, previous methods either introduce additional trainable modules, thus only applicable to specific models, or manipulate on score maps within cross-attention layers using attention masks, resulting in limited control strength when the number of regions increases. To handle these limitations, we decouple the multi-region generation into two sub-tasks, the construction of individual region (Regional Hard Binding) that ensures the regional prompt is properly executed, and the overall detail refinement (Regional Soft Refinement) over regions that dismiss the visual boundaries and enhance adjacent interactions. Furthermore, RAG novelly makes repainting feasible, where users can modify specific unsatisfied regions in the last generation while keeping all other regions unchanged, without relying on additional inpainting models. Our approach is tuning-free and applicable to other frameworks as an enhancement to the prompt following property. Quantitative and qualitative experiments demonstrate that RAG achieves superior performance over attribute binding and object relationship than previous tuning-free methods."
        },
        {
            "title": "Start",
            "content": "Region-Aware Text-to-Image Generation via Hard Binding and Soft Refinement Zhennan Chen1 Yajie Li1 Haofan Wang2,3 Zhibo Chen3 Zhengkai Jiang4 Jun Li1 Qian Wang5 Jian Yang1 Ying Tai1(cid:66) 4 2 0 2 0 ] . [ 1 8 5 5 6 0 . 1 1 4 2 : r 1Nanjing University 2InstantX 3Liblib AI 4HKUST 5China Mobile https://github.com/NJU-PCALab/RAG-Diffusion Figure 1. RAG decouples raw prompts into regional prompts, processes different regions separately and strengthens the interaction between adjacent regions. It enables precise control over object relationship, action and attributes, achieving more harmonious and consistent complex compositional generation compared to competing models like FLUX and RPG. Abstract In this paper, we present RAG, Regional-Aware text-toimage Generation method conditioned on regional descriptions for precise layout composition. Regional prompting, or compositional generation, which enables fine-grained spatial control, has gained increasing attention for its practicality in real-world applications. However, previous methods either introduce additional trainable modules, thus only applicable to specific models, or manipulate on score maps within cross-attention layers using attention masks, resulting in limited control strength when the number of regions increases. To handle these limitations, we decouple the multi-region generation into two sub-tasks, the construction of individual region (Regional Hard Binding) that ensures the regional prompt is properly executed, and the overall detail refinement (Regional Soft Refinement) over regions that dismiss the visual boundaries and enhance adjacent interactions. Furthermore, RAG novelly makes repainting feasible, where users can modify specific unsatisfied regions in the last generation while keeping all other regions unchanged, without relying on additional inpainting models. Our approach is tuning-free and applicable to other frameworks as an enhancement to the prompt following property. Quantitative and qualitative experiments demonstrate that RAG achieves superior performance over attribute binding and object relationship than previous tuning-free methods. 1 1. Introduction Recent advancements in diffusion models [11, 20, 23, 27 29, 36] have substantially enhanced the aesthetic appeal and prompt adherence in text-to-image synthesis. The prevailing trend sees the denoising architecture shifting from UNet [26] to the Diffusion Transformer (DiT) [20], which excels in scaling with large datasets. Transformer-based diffusion models like PixArt-α [5], Stable Diffusion 3/3.5 [8], and FLUX.1 [3] have set new benchmark, surpassing the quality of earlier UNet-based models such as Stable Diffusion 1.5 [25] and SDXL [21]. Furthermore, employing more robust text encoders, including T5-XXL [22], has demonstrated the ability to render visual text and significantly enhance prompt adherence. Some innovative studies even leverage large language models (LLMs) for text representation, with examples like Kolors [30] utilizing GLM [6] and Playground V3 [17] employing Llama3-8B [7]. Despite this significant progress in generating high-quality images from prompts, achieving precise fine-grained spatial control remains elusive. In essence, current generative models still struggle with comprehending the quantity and spatial arrangement of objects. To address these limitations, the concept of regional prompting, also known as regional control, regional grounding, or composition generation, has emerged. Unlike providing single global description, achieving fine-grained region-controllable generation requires users to supply not only the spatial location (e.g., segmentation mask or bounding box) but also corresponding description for each region. Several approaches have been proposed under this setting, broadly falling into two categories: tuning-based and tuning-free. For tuning-based methods, they often necessitate the training of an additional module to handle explicit conditions like bounding boxes. For example, GLIGEN [16] integrates regional inputs into new trainable layers through gated mechanism, where each grounding token is combination of the semantics of the grounded entity and its spatial location. Similarly, InstanceDiffusion [31] and MS-Diffusion [32] also incorporate learnable blocks to handle per-instance conditioning. These methods generally deliver strong performance in precise regional control but are limited to specific base models due to the introduction of extra trainable components. On the other hand, tuning-free methods, such as Multidiffusion [1], RPG [34], and Omost [19], operate on the denoised latent space or attention score map with mask for each region. They frequently employ split-and-merge strategy but face challenges in maintaining precise control as the number of regions increases. In this paper, we adopt tuning-free manner and aim to improve its control strength and coherence when dealing with multiple regions. Specifically, we present RAG, novel Regional-Aware text-to-image Generation method for precise regional control, which is composed of two subtasks, Regional Hard Binding and Regional Soft Refinement. First, we implement region-aware hard binding at the beginning of the denoising process to ensure that each regional prompt is executed accurately. This step breaks down the input prompt into several regional prompts, each with its respective spatial position, and then merges the individually denoised regional latents into the original image latent. Second, to dismiss the visual boundaries and enhance interaction between adjacent regions, regional soft refinement is applied within the cross-attention layers at the subsequent steps to obtain regional latent, where and are from regional text tokens while is from original image latent, followed by weighted recombination of base image latent and regional latent. Furthermore, leveraging robust control and fusion capabilities, our framework supports users to refine specific unsatisfactory regions from the last generation while keeping all other regions intact. Both quantitative and qualitative results demonstrate our superior performance over attribute binding and object relationship than previous state-of-the-art tuning-free methods. Our contributions are summarized as follows: We propose RAG, tuning-free Regional-Aware text-toimage Generation framework on the top of DiT-based model (FLUX.1-dev), with two novel components, Regional Hard Binding and Regional Soft Refinement, for precise and harmonious regional control. RAG novelly makes image repainting feasible, allowing users to modify specific unsatisfactory regions in the previous generation while keeping all other regions intact without need for additional inpainting models. Extensive qualitative and quantitative experiments demonstrate that RAG shows superior performance over attribute binding, object relationship and complex composition on T2I-CompBench benchmark, in comparison with previous state-of-the-arts. 2. Related Work Tuning-based Regional Control. Conventional text-toimage generation only uses text as conditional input and injects control signals through cross-attention. In order to handle spatial conditions, some works introduce additional training modules, such as ControlNet [37], to handle new control conditions, including depth maps, sketches, human poses, etc. For regional control, addtional module is also introduced for training-based methods [4, 10, 16, 31 33, 35, 38] to process spatial positions, such as bounding boxes (coordinates) or segmentation masks. GLIGEN [16] adopts Fourier embedding to encode box coordinates and adds trainable gated self-attention layer at each transformer block to accept new grounding input. InstanceDiffusion [31] further allows diverse ways to specify region positions such as simple single points, scribbles, bounding boxes or segmentation masks. MS-Diffusion [32] integrates 2 Figure 2. The overall framework of RAG, which divides regional-aware generation into two stages: (1) Regional Hard Binding ensures the proper response of regional prompts by processing each region individually with its fundamental description, and bound at the first steps to ensure accurate attribute representation and entity localization. (2) Regional Soft Refinement improves the harmony of adjacent region via enabling the interaction of regional local conditions with global image latent within the cross-attention layers at the later steps. The lower left corner shows an example of spatial region set for regional hard binding and regional soft refinement. grounding tokens with its grounding resampler to correlate specific entities and spatial constraints. Other works, such as BoxDiff [33] and Attend-and-Excite [4], apply regional constraints via gradient optimization within the denoising process to ensure all regional tokens are attended. Tuning-Free Regional Control. Although tuning-based methods show strong performance, collecting training data is time-consuming and labor-intensive, and they are limited to specific models due to the introduction of additional modules on the top of base models. To address these challenges, model agnostic approaches are proposed. DenseDiffusion [15] and Omost [19] directly adjust attention scores within cross-attention layers to ensure that the activations within masked regions are promoted. Mixture of Diffusers [13] and MultiDiffusion [1] denoise different locations separately and then combine the denoised latents using regional masks. Recently, RPG [34] introduces complementary regional diffusion through resize-and-concatenate approach for region-specific compositional generation, where regional denoised latents are resized and merged as single concatenated latent at each step. However, the control strength of these methods decreases significantly when the number of regions increases. Thus, in our work, we aim to absorbs the advantages of model-agnostic tuning-free methods and improves their control capabilities when dealing with multiple regions. 3. Method 3.1. Preliminaries Diffusion Transformer (DiT). DiT [20] is novel architecture integrating transformer as the backbone network within Latent Diffusion Model (LDM), and has become dominated choice in recent text-to-image generation models like Stable Diffusion 3/3.5 [8] and FLUX [3]. By leveraging transformer, DiT efficiently captures complex dependencies in data, resulting in high-quality image generation. Consistent with the design philosophy of LDM, DiT also operates directly in the latent space, allowing the model to generate high-fidelity images that adhere to specified conditions while reducing computational overhead. Attention Mechanism. The attention mechanism is crucial component in DiT, enabling effective interactions between the diffusion network and additional control sig3 nals such as text or image. During the diffusion process, the attention mechanism incrementally captures feature representations in the latent space, facilitating efficient denoising at each step while preserving global consistency and detail accuracy in the generated results. The attention weights are calculated as follows: Attention(Q, K, ) = softmax (cid:19) (cid:18) QK V, (1) where the query matrix Q, key matrix K, and value matrix are derived from the input feature embedding vectors in self-attention layers, while and are from conditioned embedding vectors in cross-attention layers. 3.2. Overview of RAG In this subsection, we briefly illustrate the idea of RAG, which decouples the compositional generation process into the construction of individual regions and detail refinement. The implementation of RAG consists of the following steps: (1) Regional Hard Binding (Sec. 3.3): This step involves decomposing the raw and complex input prompt containing multiple objects into subset of fundamental descriptions for each individual region or object, along with their corresponding spatial positions. This process can be accomplished either through finetuned MLLM as done in [19, 34] or through manual definition. Then, each region is processed individually with its fundamental description and bound only at the early stage of denoising to ensure accurate attribute representation and entity localization. (2) Regional Soft Refinement (Sec. 3.4): At this step, highly descriptive sub-prompts are generated for each region along with global prompt. Similarly, this process can be automated using an MLLM or defined manually, further enriching the definition of each object and promote the fusion between adjacent regions. Instead of manipulating the image latent, the refinement step achieves the interaction of regional local conditions and global image latent within the cross-attention layers. It is worth noting that based on our setting, RAG can novelly support image repainting (Sec. 3.5) in free lunch manner by only re-initialize the initial noise in target areas, thereby enabling accurate modification of previously generated images while maintaining the overall generation quality without the need for an additional inpainting model. 3.3. Regional Hard Binding To ensure the proper response of regional prompts and mitigate object omission when the number of region or object increases, we apply regional hard binding in the early steps of denoising process as illustrated in Figure 2, which involves separately denoising the regions with their short fundamental descriptions and then binding local regional latents into global latent. Figure 3. Illustration of Re-painting. Different from regular image-to-image inpainting, repainting inherits from last generation with only the target area re-initialized (upper). Given the parameters in previous generation, users are allowed to specify target area with new prompt and repaint the image, without relying on addtional inpainting models (bottom). of set, mi scale} and ni = {ni Specifically, we first decompose the long input prompt containing multiple objects into set of single fundamental descriptions ˆpi with their position sets mi = {mi scale} by MLLM or manually. Subsequently, we perform text encoding on and ˆpi to obtain and ˆyi. The regional local latent ˆxi utilizes ˆyi as text condition, while global latent takes the long input prompt as condition. The formulaic process is as follows: of set, ni xtr = xtr+1 ϵθ (xtr+1, y) tr+1, ˆyi(cid:1) , tr+1 ϵθ ˆxi tr = ˆxi (cid:0)ˆxi (2) (3) where [1, k], is the number of regions. ϵθ is the noise predicted. For each denoising step, we bind ˆxi t1 to the latent space in the rectangular area given by mi,ni as follows: xtr = Replace(xtr, ˆxi tr, mi, ni), (4) where Replace() represents the process of cropping from regional local latents and pasting back into corresponding area in global latent. The binding is only executed in the early steps within denoising process. We find that few steps of binding is sufficient for regional completeness, whereas full-steps binding results in either clear visual boundaries adjacent regions or poor interactivity. 3.4. Regional Soft Refinement Images generated through direct regional hard binding allow for precise control over positioning, effectively preventing object omission. However, the rendering of attributes tends to be relatively coarse, and there is tendency for noticeable boundaries between adjacent regions. Therefore, we apply region soft refinement in the later steps of the denoising process to improve the harmony of adjacent regions. Specifically, similar to the previous binding step, we break down the original long prompt , but instead of"
        },
        {
            "title": "Model",
            "content": "Stable v1.4 [25] Composable v2 [18] Structured v2 [9] Stable v2 [25] Stable XL [2] Attn-Exct v2 [4] GORS [12] Pixart-α-ft [5] RPG* [34] Flux.1-dev* [3] Ours"
        },
        {
            "title": "Attribute Binding",
            "content": "Color 0.3765 0.4063 0.4990 0.5065 0.5879 0.6400 0.6603 0.6690 0.7476 0.7680 0.8039 Shape Texture 0.3576 0.3299 0.4218 0.4221 0.4687 0.4517 0.4785 0.4927 0.5640 0.5078 0.6016 0.4156 0.3645 0.4900 0.4922 0.5299 0.5963 0.6287 0.6477 0.6724 0.6195 0.7085 Object Relationship Spatial Non-Spatial 0.1246 0.0800 0.1386 0.1342 0.2133 0.1455 0.1815 0.2064 0.4017 0.2606 0.5193 0.3079 0.2980 0.3111 0.3127 0.3119 0.3109 0.3193 0.3197 0.3032 0.3078 0.3263 Complex 0.3080 0.2898 0.3355 0.3386 0.3237 0.3401 0.3328 0.3433 0.3702 0.3650 0.4377 Table 1. Comparison of alignment evaluation on T2ICompBench [12]. The best results are highlighted in bold, second-best in underline. The basic data is sourced from [12]. * indicates results we reproduced using the official open-source codes and configurations. breaking it down into short fundamental descriptions, we break it down into highly descriptive sub-prompts pi, with scale} and wi = set of global regions hi = {hi {wi scale}, and obtain the corresponding representation yi through the text encoder. Then, the text condition yi is projected into and i, while Qi is derived from current image latent xt(r+1), within cross-attention layers: of set, wi of set, hi Qi = ℓQ (cid:0)xt(r+1) (cid:1) , = ℓK(yi), =ℓV (yi) QiK iT i, (cid:32) (cid:33) xi t(r+1) = Softmax (5) where ℓQ, ℓk, ℓV are linear projections, xi regional-aware local latent. We crop xi corresponding to different objects to obtain xi rich attributes generated in the global region: is t(r+1) to the region t(r+1) with t(r+1) xi t(r+1) = Crop (cid:16) t(r+1), hi, wi(cid:17) xi . (6) Then, we also execute Replace() to splice these regions: xr t(r+1) = Replace (cid:16) xt(r+1), xi t(r+1), hi, wi(cid:17) . (7) To further improve the alignment between each region with the original prompt and enhance adjacent interactions, we perform linear weighted recombination of xr and xt(r+1) as follows: t(r+1) xt(r+1) = xt(r+1) δ + xr t(r+1) (1 δ), (8) where δ controls the fusion strength of image latent xt(r+1) and regional-aware local latent xr t(r+1). 3.5. Image Repainting Based on our task setting, where long complex prompt containing multiple objects is decoupled into several regional sub-prompts for individual processing, natural question is raised: Can this approach be used to repaint specific areas? By leveraging the robust control and fusion capabilities of regional hard binding and soft refinement, we can reinitialize noise in specific region requiring modification, enabling repainting of region without affecting the overall layout or attributes of other areas. Different from typical post-processing inpainting task that usually requires additional inpainting models, repainting re-generates with the same parameters from the last generation, with only the prompts in the target area modified. As shown in Figure 3, users are allowed to modify the description of specific region within , ˆpi, pi to obtain e, pi new edited prompts Pe, ˆpi e. Subsequently, we encode e, yi them as text features ye, ˆyi e. Given total denoising step , the initial noisy latent is inherited from the previous generation xT , only the target area indicated by mask is re-initialized. = init(xT , mask). To ensure the other regions intact, we simultaneously . At each timestep t, we perform denoising on xT and replace the corresponding portions of xt with the masked areas of the edited region from for repainting: (9) xt = Repaint(xt, t, mask). (10) Through the regional complementarity enabled by regional soft refinement, the repainting region can be seamlessly integrated with the surrounding areas of other regions. This approach not only makes image repainting feasible, but also ensures the coherence and consistency of the overall scene. 4. Experiments 4.1. Experiment Setting Implementation Details. We use Flux.1-dev [3] as the representative DiT-based text-to-image model for RAG. 5 Figure 4. Qualitative comparisons on compositional text-to-image generation. From top to bottom, we show 5 examples of different prompts and regions. Compared with previous methods, we demonstrate excellent regional control capabilities. The inference process is set to 20 steps, with guidance scale of 3.5. For large-scale quantitative evaluations, we employ Chain-of-Thought (CoT) template from [34] and leverage GPT-4 to automatically decouple multi-object scenes. All experiments are conducted on single A6000 GPU. Compared Methods. To comprehensively evaluate the generation quality, we compare our RAG with several stateof-the-art text-to-image approaches, including: Stable v1.4 [25], Composable v2 [18], Structured v2 [9], Stable v2 [25], Stable XL [2], Attn-Exct v2 [4], GORS [12], Pixart-α-ft [5], RPG [34], and Flux.1-dev [3]. 4.2. Main Results Quantitative Comparison. Table 1 presents RAGs results compared to leading image generation methods on the T2I-CompBench benchmark. RAG excels in attribute binding, object relationships, and complex composition, especially achieving 29% improvement over RPG for prompts containing spatial relationships. This demonstrates RAGs ability to handle complex multi-region prompts, enhancing model controllability and generation quality. 6 Figure 5. Qualitative comparisons on image repainting between our RAG and the state-of-the-art inpainting model BrushNet. Our results are more region-aware with harmonious effect with the surrounding, revealing diverse potential for applications. Qualitative Comparison. Figure 4 illustrates visual comparisons, highlighting our RAGs superiority in complex multi-region generation. While RPG [34] also excels at regional control, its lack of precise positional control may lead to object omission or fusion. In contrast, our methods hard binding and soft refinement mechanisms enable accurate multi-region control, faithfully conveying details on position, quantity, and attributes based on the input text. Figure 5 compares our image repainting results with BrushNet [14], showing RAGs strength in repainting both single and contacted regions without conflicts. Even when the repainted object differs greatly in style or shape, our approach successfully generates the target while maintaining the layout and regional attributes, enhancing the flexibility and control of the generation process. Figure 6. Qualitative analysis of Hard Binding and Soft Refinement. The former ensures the proper responses of each region, while the latter enhances the coherence among regions. Aesthetic Text-image Alignment 4.3. Ablation Study Methods RPG Stable v3 Flux.1-dev Ours 16.1% 13.2% 18.8% 51.9% 14.8% 13.4% 17.8% 54.0% Table 2. User study on aesthetics and text-image alignment. User Study. We randomly sampled 109 prompts from the T2ICompBench test set to evaluate the aesthetic quality and text-image consistency (including attributes and spatial positions) of the generated images. During the evaluation, we invited 24 users to compare methods including RAG, Flux.1-dev [3], RPG [34] and Stable v3 [8], selecting the In this way, we obtain most suitable generation results. 2616 result rankings, as shown in Table 2, RAG has the upper hand in terms of both aesthetics and alignment. 7 Effectiveness of Hard Binding & Soft Refinement. The proposed RAG comprises two key components: hard binding and soft refinement. We conducted ablation experiments on these two parts, with visual results shown in Figure 6. Results reveal that hard binding achieves precise object positioning but yields coarse attribute rendering and limited integration among objects. With soft refinement, object attributes are richly detailed and relationships are harmonious, though precise positioning may be compromised, and object omission may sometimes occur. Effectiveness of Parameter r. We introduced parameter to control the times of hard binding. As shown in Figure 7, excessive hard binding reduces the times for subsequent soft refinement, weakening its impact. This may diminish interactions between adjacent objects, potentially causing noticeable boundaries to reappear. Our tests indicate that setting between 1 and 3 typically achieves an Figure 7. Qualitative analysis of hard binding steps and base strength δ. few steps of binding is sufficient for regional completeness, and the regional coherence improves gradually when increasing base strength. partitioning. As the weight adjusted by soft refinement increases, image fusion improves gradually. In our extensive testing, δ proves to be flexible parameter, with values in the range (0,1] consistently producing high-quality images. The optimal δ can be adjusted according to specific scenes and generation requirements. Analysis of Inference Time. Since RAG employs decoupled multi-region generation operations, each region is processed independently during the generation process. This results in an increase in inference time as the number of regions grows, as shown in Figure 8. However, because RAG does not alter the original models architecture, existing acceleration techniques, such as hyper-flux [24] , can still be applied to RAG. By incorporating these acceleration methods, inference time can be significantly reduced.Moreover, after the introduction of these acceleration techniques, the region generation process in RAG remains unaffected, ensuring that the precision and detail control for each region remain stable. This demonstrates that, although inference speed presents challenge, with appropriate acceleration strategies, RAG can still efficiently and accurately generate multi-region images while maintaining both generation quality and control over the interactions between regions. 5. Conclusion Figure 8. Comparison of inference time and visual comparison between the original RAG and RAG with acceleration. ideal balance, effectively combining positional control with seamless integration between regions. δ controls the proportion of Effectiveness of Factor δ. the original prompt. As shown in Figure 7, when δ = 0, indicating no soft refinement, the results exhibit noticeable In this paper, we introduce RAG, novel training-free region-aware text-to-image generation method designed to address challenges posed by regional prompts. RAG operates through two key stages. First, regional hard binding independently constructs the content of each region. Then, regional soft refinement enhances interactions between ad8 jacent regions and improves attribute generation. Furthermore, RAG bypasses the need for external inpainting models, enabling direct image repainting to modify unsatisfactory regions from last generations. Extensive experimental results demonstrate the superiority of RAG on compositional generation compared to prior training-free methods. Limitation and Future Work. RAG offers precise regional control and flexible image repainting but has limitations. Its multi-region decoupling increases inference time as region count grows. Future work will focus on improving RAGs inference efficiency and integrating with other diffusion models for enhanced scalability and performance."
        },
        {
            "title": "References",
            "content": "[1] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. 2023. 2, 3 [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 5, 6 [3] BlackForest. Black forest labs; frontier ai lab, 2024. 2, 3, 5, 6, 7 [4] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):110, 2023. 2, 3, 5, 6 [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 2, 5, [6] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021. 2 [7] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 2 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 3, 7 [9] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032, 2022. 5, 6 [10] Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, and Jingren Zhou. Ranni: Taming text-to-image diffuIn Proceedings of sion for accurate instruction following. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47444753, 2024. 2 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [12] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and T2i-compbench: comprehensive benchXihui Liu. mark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. 5, 6 Alvaro Barbero Jimenez. Mixture of diffusers for scene composition and high resolution image generation. arXiv preprint arXiv:2302.02412, 2023. 3 [13] [14] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion. arXiv preprint arXiv:2403.06976, 2024. 7 [15] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 77017711, 2023. 3 [16] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2251122521, 2023. 2 [17] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-toimage alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. 2 [18] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. Compositional visual generation with composable diffusion models. In European Conference on Computer Vision, pages 423439. Springer, 2022. 5, 6 [19] Omost-Team. Omost github page, 2024. 2, 3, 4 [20] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 2, [21] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2 [22] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 2 [23] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 2 9 [36] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. 2 [37] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. [38] Dewei Zhou, You Li, Fan Ma, Xiaoting Zhang, and Yi Yang. Migc: Multi-instance generation controller for text-to-image In Proceedings of the IEEE/CVF Conference synthesis. on Computer Vision and Pattern Recognition, pages 6818 6828, 2024. 2 [24] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. arXiv preprint arXiv:2404.13686, 2024. 8 [25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 5, 6 [26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 2 [27] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 [28] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. [29] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [30] Kolors Team. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. arXiv preprint, 2024. 2 [31] Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, RoInstancediffusion: Instancehit Girdhar, and Ishan Misra. In Proceedings of the level control for image generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62326242, 2024. 2 [32] Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. Ms-diffusion: Multi-subject zero-shot image personalization with layout guidance. arXiv preprint arXiv:2406.07209, 2024. 2 [33] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained In Proceedings of the IEEE/CVF International diffusion. Conference on Computer Vision, pages 74527461, 2023. 2, 3 [34] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and CUI Bin. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning, 2024. 2, 3, 4, 5, 6, 7 [35] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1424614255, 2023."
        }
    ],
    "affiliations": [
        "China Mobile",
        "HKUST",
        "InstantX",
        "Liblib AI",
        "Nanjing University"
    ]
}