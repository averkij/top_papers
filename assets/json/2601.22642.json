{
    "paper_title": "Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification",
    "authors": [
        "Chuxue Cao",
        "Jinluan Yang",
        "Haoran Li",
        "Kunhao Pan",
        "Zijian Zhao",
        "Zhengyu Chen",
        "Yuchen Tian",
        "Lijun Wu",
        "Conghui He",
        "Sirui Han",
        "Yike Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning."
        },
        {
            "title": "Start",
            "content": "Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification in Language Models Chuxue Cao * 1 2 Jinluan Yang * 3 Haoran Li 1 Kunhao Pan 1 Zijian Zhao 1 Zhengyu Chen 3 Yuchen Tian 4 Lijun Wu 2 Conghui He 2 Sirui Han 1 Yike Guo 1 6 2 0 2 0 3 ] . [ 1 2 4 6 2 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neurosymbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning. We will release our data and models for further exploration soon. 1. Introduction Large Language Models (LLMs) demonstrate impressive proficiency in mathematical and logical reasoning (Ahn et al., 2024; Ji et al., 2025; Liu et al., 2025b; Chen et al., 2025a), yet their probabilistic decoding process lacks inherent mechanisms to ensure consistency (Sheng et al., 2025b; Baker et al., 2025). This tension creates significant risks, in- * Equal contribution. 1Hong Kong University of Science and Technology 2Shanghai Artificial Intelligence Laboratory 3Zhejiang University 4Hong Kong Baptist University. Correspondence to: Sirui Han, Yike Guo. Contact: <ccaoai@connect.ust.hk>. Preprint. February 2, 2026. Figure 1. Comparison between natural language reasoning and formal logic verification-guided reasoning. Formal verification detects logical errors in natural language reasoning and provides corrected reasoning paths. NL means Natural Language. cluding hallucinations (Li & Ng, 2025; Sheng et al., 2025b), safety vulnerabilities (Zhou et al., 2025; Cao et al., 2025b), and reward hacking (Chen et al., 2025b; Baker et al., 2025). Although recent efforts have employed model-based verifiers to offer denser feedback than sparse ground-truth labels (Ma et al., 2025; Liu et al., 2025c; Guo et al., 2025b), they often overlook intermediate reasoning steps. To enforce more rigorous supervision, subsequent research has incorporated formal tools like theorem provers and code interpreters (Ospanov et al., 2025; Kamoi et al., 2025; Liu et al., 2025a) to address this drawback. However, existing formal approaches face critical limitations: they are often restricted to specific domains (e.g., Mathematics) (Ospanov et al., 2025; Liu et al., 2025a), rely on uncertain autoformalization (Ospanov et al., 2025; Feng et al., 2025b), or utilize post-hoc verification that cannot actively prevent error propagation (Kamoi et al., 2025; Feng et al., 2025b). This yields the primary question to be explored: (Q) Can we utilize the formal verification to further enhance the LLM reasoning across diverse domains? Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification To explore this question, we first quantified the logical consistency gap in current LLMs by conducting formal verification analysis of generated reasoning chains. critical finding emerges as shown in Figure 1: even chains that arrive at correct final answers suffer from substantial logical inconsistency, with 39.3% of steps formally disproved, trend consistent with previous research (Sheng et al., 2025b; Leang et al., 2025). For chains leading to incorrect answers, this failure rate rises to 52.4%. The comparative example in Figure 1 illustrates this gap: natural language reasoning incorrectly concludes Diana > Bob from the given constraints, while formal verification identifies the incorrect conclusion and lead to an correct answer. This phenomenon reveals pervasive reward hacking, where models exploit superficial patterns to reach correct labels without establishing sound logical foundations (Skalse et al., 2022). Ultimately, these results expose fundamental limitation of natural language reasoning: without explicit verification mechanisms, models cannot guarantee reasoning validity or global logical coherence across multi-step inference. To bridge this gap, we propose novel framework that synergizes probabilistic natural language reasoning with formal symbolic verification. Distinguished from prior approaches relying on static filtering or narrow domains, our method dynamically interleaves formal verification into the generation process. By incorporating feedback from satisfiability results, counterexamples, and execution outputs, we extend the standard chain-of-thought paradigm to enable real-time error detection and rectification. To effectively operationalize this interleaved reasoning approach, we introduce formal logic verification-guided training framework comprising two synergistic stages: (i) Supervised Fine-tuning (SFT), which employs hierarchical data synthesis pipeline. Crucially, we mitigate the noise of raw autoformalization by enforcing execution-based validation, thereby ensuring high alignment between natural language and formal proofs; and (ii) Reinforcement Learning (RL), which utilizes Group Relative Policy Optimization (GRPO) with composite reward function to enforce structural integrity and penalize logical fallacies. Empirical evaluations across logical, mathematical, and general reasoning domains demonstrate that this framework substantially enhances reasoning accuracy, highlighting the potential of formal verification to push the performance boundaries of LLM reasoning. Our contributions can be concluded as follows: We propose the first framework that dynamically interleaves formal verification into LLM reasoning across diverse domains, utilizing the real-time feedback via symbolic interpreters to transcend the limitations of passive post-hoc filtering and domain-specific theorem proving. We introduce two-stage training framework that combines formal verification-guided supervised fine-tuning with policy optimization, featuring novel data synthesis pipeline with execution-based validation to enforce logical soundness and structural integrity. Extensive evaluations on six benchmarks demonstrate that our models break performance ceilings, surpassing SOTA by 10.4% (7B) and 14.2 % (14B). This validates the scalability and effectiveness of our proposed method. 2. Related Works 2.1. Large Language Models for Natural Reasoning Supervised fine-tuning (SFT) on chain-of-thought examples (Wei et al., 2022) and step-by-step solutions (Cobbe et al., 2021) has been foundational for developing reasoning capabilities in LLMs, with recent efforts curating high-quality datasets across mathematics (LI et al., 2024), code (Xu et al., 2025), and science (Wang et al., 2022). However, SFT alone cannot effectively optimize for complex objectives beyond imitation and struggles with multi-step error correction (Lightman et al., 2023; Uesato et al., 2022; Zhou et al., 2026). Recent RL advances using outcome-based optimization methods have achieved remarkable success in mathematical reasoning (Cobbe et al., 2021; Zeng et al., 2025), code generation (Le et al., 2022; Feng et al., 2025a), and general-domain reasoning (Ma et al., 2025; Chen et al., 2025c). However, optimizing solely for final answer correctness creates perverse incentives where models learn correct conclusions through logically invalid pathways (Uesato et al., 2022), leading to reward hacking (Skalse et al., 2022) and brittleness under distribution shift (Hendrycks et al., 2021). To address these limitations, process-based rewards incorporate feedback from intermediate steps, providing dense supervision through human-annotated judgments (Uesato et al., 2022; Lightman et al., 2023; She et al., 2025; Khalifa et al., 2025). However, the probabilistic nature of language model-based verifiers introduces errors and biases (Zheng et al., 2023), limiting their ability to detect subtle logical inconsistencies that emerge during multi-step reasoning. 2.2. Formal Reasoning and Verification Recent work has integrated formal verification tools, including theorem provers (Yang et al., 2023; Cao et al., 2025a; Tian et al., 2025), code interpreters (Feng et al., 2025a), and symbolic solvers (Li et al., 2025a), to provide machinecheckable validation beyond the biases of LLM-as-a-judge approaches (Li & Ng, 2025; Uesato et al., 2022; Lightman et al., 2023). This direction is increasingly recognized as critical for grounding generative models in verifiable systems (Ren et al., 2025; Wang et al., 2025; Hu et al., 2025). Existing approaches differ in how verification is applied. HERMES (Ospanov et al., 2025) interleaves informal rea2 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification fication. Across 500 randomly sampled instances from each domain, FLV-SFT consistently outperforms Natural-SFT, achieving 291 vs. 219 correct answers in the Logical domain (+32.8%), 243 vs. 163 in the Mathematical domain (+49.3%), and 213 vs. 166 in the General domain (+28.5%). These substantial improvements across all three categories demonstrate that formal verification effectively addresses the consistency gaps inherent in purely neural approaches. These results underscore the significant potential of formal verification to bridge the reasoning gap and strongly motivate our approach of interleaving natural language reasoning with formal verification throughout the reasoning process. 3.2. Problem Formulation Formally, let = {(x, y)} be dataset of reasoning tasks, where denotes the input context (e.g., problem description) and denotes the ground-truth answer. Standard CoT Paradigm. In conventional Chain-ofThought reasoning, an LLM Pθ generates sequence of reasoning steps = (s1, s2, . . . , sn) in natural language, aiming to maximize: Pθ(y, x) = Pθ(y z, x) (cid:89) i= Pθ(si s<i, x) (1) However, this objective does not guarantee that is logically valid or formally verifiable. Our Paradigm: Formal Logic Verification-Guided Reasoning. We propose augmenting the reasoning chain with formal verification at each step. Specifically, we define an extended reasoning chain = (s1, f1, v1, s2, f2, v2, . . . , sn, fn, vn), where: si: Natural language reasoning step (as in standard CoT) fi: Formal specification that encodes the logical correctness of si (e.g., symbolic constraints, SAT clauses, SMT formulas, or executable code) vi: Formal Logic Verification result returned by formal verifier when applied to fi During training, our objective is to maximize the likelihood of correct final answers: E(x, y) (cid:2)log θ(y, x)(cid:3) (2) max θ During inference, the verification function takes the formal reasoning as input and returns detailed feedback at each reasoning step. This feedback may include satisfiability results, counterexamples, proof traces, execution outputs, or error messages, which guide the model to generate logically sound and verifiable subsequent reasoning steps. Figure 2. Number of correct answers using natural language reasoning versus formal logic verification. We randomly sampled 500 instances from each domain for this comparison. soning with Lean-verified steps, ensuring real-time soundness but requiring mature formal libraries. Safe (Liu et al., 2025a) applies post-hoc verification to audit completed reasoning chains, though this passive mode cannot prevent error accumulation during generation. VeriCoT (Feng et al., 2025b) checks logical consistency on extracted first-order logic, while others train verifier models using formal tool signals (Kamoi et al., 2025). Tool-integrated methods (Xue et al., 2025; Zeng et al., 2025; Li et al., 2025b; Feng et al., 2025a) embed interpreter calls into generation for calculation or simulation, but often lack strict logical guarantees. These approaches face key limitations: specialization to mathematical theorem proving, treating verification as separate filtering without guiding generation, or relying on uncertain logic extraction and neural verifiers. In contrast, we propose verification-guided reasoning that extends formal verification to general logical domains and employs real-time feedback as dynamic, in-process guidance to steer reasoning trajectories and enable self-correction beyond specialized formal tasks. 3. Preliminaries We first present empirical evidence illustrating the gap between probabilistic natural language reasoning and formal verification in LLM reasoning. We then formally define our reasoning paradigm and introduce the preliminaries on the symbolic verification methods utilized in our framework. 3.1. Natural vs. Formal Reasoning in LLMs Our previous experiments demonstrate that LLMs lack mechanisms to ensure global logical consistency (Figure 1), motivating us to explore formal logic verification. Formal logic provides rigorous framework where reasoning steps can be reliably validated using formal solvers. As shown in Figure 2, integrating formal logic verification with natural language reasoning yields significant performance improvements across diverse domains. We compare two approaches: Natural-SFT, which relies solely on natural language reasoning, and FLV-SFT, which incorporates formal logic veri3 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification Figure 3. Overview of the formal logic verification-guided training framework. The framework operates in two stages: (1) SFT: teacher model synthesizes formal logic verification traces, which are validated by verifier. subset of verified samples is used to fine-tune the model, while challenging samples are reserved for RL training. (2) RL: The policy model generates natural language reasoning followed by formal reasoning. formal interpreter verifies the formal reasoning and provides feedback, enabling iterative refinement until the model produces final answer or reaches the maximum number of interpreter calls. Rewards computed from verification outcomes are used to calculate advantages and update the policy model via reinforcement learning. vexp . The prompt template is provided in Figure 9. Execution-based Validation and Correction. To ensure the fidelity of synthesized formal proofs, we implement rigorous validation mechanism. Each generated formal proof fk is executed in sandbox to obtain the actual output vact . We then perform three-stage validation: Stage 1: Exact Match. If the actual output exactly matches = vexp the expected output (vact ), the proof is accepted immediately and integrated into the training data. = vexp Stage 2: Semantic Equivalence Check. In cases where vact , we employ verification model to assess whether the discrepancy is semantically negligible (e.g., differences in capitalization, output ordering, or numerical precision). If the outputs are deemed equivalent under mathematical or logical semantics, we proceed to Stage 3. Stage 3: Proof Rewriting. When minor inconsistencies are detected, we require the LLM to regenerate the natural language reasoning conditioned on the actual execution result vact . This ensures that the natural language reasoning module k, and the execution output vact maintain strict logical coherence. Proofs that fail both exact match and semantic equivalence checks are discarded. The resulting training instance is structured as follows: k, the formal proof 4. Methodology To address logical inconsistencies and hallucinations in pure natural language reasoning (Section 3), we propose integrating formal logic verification into the reasoning process. Our approach consists of two stages: (i) Supervised Finetuning to enable the model to generate interleaved natural language and formal proofs, and (ii) Reinforcement Learning to optimize the model using composite rewards that enforce logical soundness and correctness. 4.1. Formal Logic Verification-Guided SFT The primary goal of the SFT stage is to align the models output distribution with structured reasoning format that supports self-verification. Since large-scale datasets containing interleaved reasoning and formal proofs are scarce, we employ hierarchical formal proof data synthesis pipeline. 4.1.1. DATA SYNTHESIS PIPELINE CoT Generation. Given raw reasoning problem q, we first employ strong teacher model to generate = 4 candidate reasoning chains. judge model evaluates the correctness of the final answers to calculate pass rates. We select subset of verified chains that yield correct solutions for subsequent processing. Let denote selected correct reasoning chain. To incorporate formal logic, we utilize an LLM to decompose into discrete logical modules {sk}N k=1. For each module sk, the LLM synthesizes corresponding formal proof fk and predicts an expected execution output 4 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification zaug = (cid:77) k=1 (cid:16) sk <code>f k</code> <interpreter>vact (cid:17) </interpreter> (3) denotes the validated formal proof and vact where is the verified execution output. This pipeline ensures that every training example (s, f, v) exhibits perfect alignment between natural language hypotheses, formal logic reasoning, and execution feedback, thereby providing high-quality supervision for the model to learn reliable reasoning patterns. See Appendix for dataset construction details. 4.1.2. OPTIMIZATION OBJECTIVE augmented training dataset DSFT = Given the {(qi, zaug,i)}M i=1, we optimize the model parameters θ by maximizing the log-likelihood of generating structured reasoning sequences: LSFT(θ) = E(q,zaug)DSFT [log Pθ(zaug q)] (4) This can be decomposed into the sequential generation of reasoning modules, formal proofs, and execution outputs: LSFT(θ) = E(q,zaug)DSFT (cid:34) (cid:88) (cid:16) k= log Pθ(sk q, z<k) + log Pθ(f q, z<k, sk) + log Pθ(vact q, z<k, sk, k) (cid:35) (5) (cid:17) where z<k denotes all previous modules. We train using AdamW with cosine learning rate scheduling and gradient clipping. This stage enables the model to generate verifiable, formally grounded reasoning chains. 4.2. Formal Verification-Guided Policy Optimization To further enhance the formal logic verification-guided reasoning capabilities of LLMs, we employ reinforcement learning. The core of this stage is multi-dimensional reward function that provides fine-grained feedback on structure, semantics, and computational efficiency. 4.2.1. HIERARCHICAL REWARD DESIGN To ensure both generation stability and reasoning rigor, we design hierarchical reward function R(y) that evaluates responses in strictly prioritized order: Format Integrity Structural Compliance Logical Correctness. The unified reward is formulated as: R(y) = Cfatal Rfatal Cinvalid Rinvalid Rtotal(y) otherwise (L1: Fatal) (L2: Format) (L3: Valid) (6) The total reward for valid responses is: Rtotal(y) = Rstruct(y) + Rlogic(y) (7) Level 1 & 2: Penalties for Malformed Generation. We first filter out pathological behaviors to prevent reward hacking and infinite loops during training. Fatal Errors (Cfatal): Responses with severe and unrecoverable execution failures (e.g., timeouts, repetition loops, excessive tool calls). We assign harsh penalty Rfatal = γstruct to strictly inhibit these states, where > 0 is correctness weight hyperparameter. Format Violations (Cinvalid): Responses that are technically executable but structurally flawed (e.g., missing termination tags, no extractable final answer, excessive verbosity). These incur moderate penalty Rinvalid = βstruct , where γstruct > βstruct > 0. Level 3: Incentives for Valid Reasoning. For responses that pass the format checks (y / Cfatal Cinvalid), the reward is composite of structural quality and logical correctness. (i) Structural Reward Rstruct(y): Encourages concise and compliant tool usage. Rstruct(y) = α λtag Nundef λcall max(Ncall Nmax, 0) (8) Here, α is base bonus, Nundef tracks undefined tags, and the last term penalizes excessive tool invocations beyond threshold Nmax. (ii) Logical Correctness Reward Rlogic(y): Evaluates the final answer ˆa against the ground truth a. (cid:40) Rlogic(y) = λlen len(ˆa, a) if ˆa = if ˆa = (9) where len penalizes length discrepancies to discourage verbose reasoning and promote conciseness. Detailed hyperparameter settings are provided in Appendix C. 4.2.2. OPTIMIZATION OBJECTIVE We optimize πθ using GRPO. For each input Ddifficult, we sample outputs y1, . . . , yG and compute: LGRPO(θ) = ExDdifficult (cid:34) 1 (cid:88) yi (cid:88) (cid:18) i=1 t=1 min (cid:0)ri,t ˆAi, clip(ri,t, 1 ϵ, 1 + ϵ) ˆAi (cid:1) (10) β log πθ(yi,tx, yi,<t) πref(yi,tx, yi,<t) (cid:19)(cid:35) Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification Table 1. Comparative evaluation between our proposed formal-language verification (FLV) methods (gray background) and other approaches. Bold values denote the best results. KOR-Bench and BBH contain multiple subfields and report macro-averaged scores. Model Logical Mathemathcal General KOR-Bench BBH MATH500 AIME24 GPQA-D TheoremQA Base Qwen2.5-7B-Instruct SimpleRL-Zoo General-Reasoner RLPR Synlogic ZeroTIR SimpleTIR FLV-SFT (Ours) FLV-RL (Ours) Base Qwen2.5-14B-Instruct SimpleRL-Zoo General-Reasoner FLV-SFT (Ours) FLV-RL (Ours) 13.2 40.2 34.2 43.9 42.2 48.1 30.0 37.0 48.0 51. 37.4 51.5 37.2 41.3 54.0 57.0 Qwen2.5-7B 60.2 75.0 74.0 73.4 77.2 74. 62.4 82.6 77.2 78.6 Qwen2.5-14B 65.4 77.4 77.2 78.6 79.8 81. 41.9 67.0 59.8 61.9 66.2 66.5 40.0 62.0 68.5 70.0 52.0 72.9 72.7 71. 77.5 78.0 6.5 9.4 14.8 12.7 14.2 15.4 28.5 41.0 20.0 20.8 3.6 12. 12.9 17.5 21.9 30.2 29.3 33.8 24.2 38.9 37.9 27.8 28.8 22.7 32.3 35. 32.8 41.4 39.4 43.4 40.4 41.4 29.1 36.6 43.1 45.3 44.3 39.2 36.4 51. 53.0 55.7 33.0 41.9 48.9 55.3 60.6 63.5 AVG 30.0 43. 41.7 46.0 47.0 45.3 37.7 49.4 49.8 51.9 37.4 49.6 48.1 51.3 55.7 58. where ri,t = πθ(yi,tx, yi,<t)/πold(yi,tx, yi,<t). The advantage ˆAi is group-normalized on R(yi), stabilizing training by emphasizing relative quality over absolute reward. 5. Experiment 5.1. Experimental Setup Models. We utilize the Qwen2.5-7B and Qwen2.514B (Qwen-Team, 2025) as our backbone architectures. These models serve as the initialization point for both the SFT and subsequent Policy Optimization stages. Training Data. Our training corpus is constructed using three datasets: WebInstruct-Verified (Ma et al., 2025), K&K (Xie et al., 2024), and NuminaMath-TIR (LI et al., 2024). These sources provide collection of diverse, verifiable reasoning tasks across multiple domains. We employ DeepSeek-R1 (Guo et al., 2025a) for data distillation and difficulty assessment via pass-rate. We utilize GPT-4o (Hurst et al., 2024) as judge for answer correctness and ClaudeSonnet-4.5 (Anthropic, 2024) to synthesize the interleaved formal logic steps as detailed in Section 4. The RL data is selected based on the pass rate of answers generated by the teacher model DeepSeek-R1, retaining only questions with pass rate below 50%. The categorical distribution of our curated training data is illustrated in Figure 7. Evaluation. We conduct comprehensive evaluation across three distinct reasoning domains to assess models: Logical Reasoning: We employ KOR (Ma et al., 2024) to evaluate knowledge-grounded logical reasoning across diverse domains and BBH (Suzgun et al., 2023) for challenging tasks requiring multi-step deduction. Mathematical Reasoning: We evaluate performance on MATH-500 (Hendrycks et al., 2024) for competitionlevel mathematics problems and AIME 2024 for Olympiad-level mathematical reasoning challenges. General Reasoning: We utilize GPQA-Diamond (Rein et al., 2024) for graduate-level reasoning in subdomains including physics, chemistry, and biology. Additionally, we use TheoremQA (Chen et al., 2023) to assess graduate-level theorem application across mathematics, physics, Electrical Engineering & Computer Science (EE&CS), and Finance, testing the models ability to correctly apply and reason with formal theorems. All evaluations use OpenCompass (Contributors, 2023) with greedy decoding, except AIME24 which reports avg@16 from sampling runs following RLPR (Yu et al., 2025). Baselines. To validate the effectiveness of our framework, we compare our approach against five categories of models: (i) Base Models: Qwen2.5-7B/14B (Qwen-Team, 2025), establishing the baseline performance to measure the net gain of our training methodology. (ii) Simple-RL-Zoo (Zeng et al., 2025), comprehensive collection of mathematicsfocused RL models. (iii) General-Reasoner (Ma et al., 2025), suite of general-domain RL models trained using model-based verifier. (iv) RLPR-7B (Yu et al., 2025), general-domain RL model trained via simplified verifierfree framework. (v) SynLogic-7B (Liu et al., 2025b), specialized model trained to enhance the logical reasoning capabilities of LLMs. (vi) ZeroTIR (Mai et al., 2025), tool-integrated reasoning model specifically designed to ex6 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification ecute Python code for solving mathematical problems. (vii) SimpleTIR (Xue et al., 2025), multi-turn tool-integrated reasoning model for mathematical reasoning problems. Implementation Details. We implement our RL training using the verl framework (Sheng et al., 2025a), following ToRL (Li et al., 2025b). SFT Stage: We use learning rate of 1 105 with cosine scheduling and global batch size of 32. The model is trained for 3 epochs. RL Stage: We employ learning rate of 5 107. We generate 8 rollouts per prompt with temperature of 1.0. To prevent policy divergence, we set the KL coefficient to 0.05 and the clip ratio to 0.3. The training utilizes batch size of 1024 and context length of 16,384 tokens. Training proceeds for 120 steps on cluster of 16 NVIDIA H800 GPUs. To manage computational overhead, we limit the formal verification process to maximum of 4 iterative rounds. 5.2. Main Results Table 1 presents the comprehensive evaluation of our Formal Logic Verification (FLV) approach against standard baselines across Qwen2.5-7B and 14B scales. Formal logic verification-guided methods outperform traditional natural language-based methods. As shown in the results, our proposed FLV framework demonstrates superior performance compared to standard natural language reasoning approaches. Notably, even our supervised finetuning stage (FLV-SFT) surpasses all comparative RL baselines on the 7B scale. On Qwen2.5-7B, FLV-SFT achieves an average score of 49.8, outperforming the strongest natural language baseline (RLPR, 47.0) by 2.8 points. This suggests that integrating formal logic verification during the SFT phase provides more robust reasoning foundation than standard RL training on natural language chains alone. Furthermore, the application of Group Relative Policy Optimization (FLV-RL) yields consistent improvements over the SFT stage. For the 14B model, FLV-RL improves upon FLV-SFT by increasing the average score from 55.7 to 58.6, with significant gains in hard mathematical tasks like AIME 2024 (+8.3%) and general theorem application in TheoremQA (+2.9%). This confirms that our verifier-guided RL effectively refines the policy beyond the supervised baseline. Formal logic verification unlocks model reasoning potential, achieving SOTA with limited data. Despite utilizing concise training set (approx. 17k samples total), our approach establishes new state-of-the-art performance among models of similar size, significantly outperforming baselines that typically rely on larger-scale data consumption. (i) On the challenging AIME 2024 benchmark, our FLV-RL-14B model achieves 30.2%, nearly doubling the performance of the General-Reasoner baseline (17.5%) and far exceeding the Base model (3.6%). Similarly, on MATH500, we achieve 81.4%, surpassing all baselines. (ii) We observe dominant performance on TheoremQA (63.5% on 14B), outperforming the nearest competitor by over 8 points. In logical reasoning (KOR-Bench), our method achieves 15.7% improvement over the General-Reasoner on the 14B scale (57.0 vs 41.3). While FLV shows slight weakness on GPQA-Diamond (likely due to benchmark reliability issues discussed in Appendix G), our method consistently excels in tasks requiring rigorous multi-step deduction and symbolic manipulation, validating the hypothesis that formal verification serves as catalyst for deep reasoning capabilities. Figure 4. Python packages type distribution invoked by SimpleTIR (blue) vs. FLV-RL (red) across three domains. Formal verification instills shift from calculation to symbolic reasoning, enabling superior generalization. While tool-integrated baselines like SimpleTIR primarily utilize tools as solvers for direct computation (achieving 41.0 on AIME24), this paradigm struggles with tasks requiring rigorous logical deduction. In contrast, our FLV framework employs formal methods as verifier to enforce logical consistency. This approach yields dominant performance on logic-heavy benchmarks such as KOR-Bench (51.0 vs. 37.0 for SimpleTIR) and GPQA-Diamond (35.4 vs. 28.8 for ZeroTIR). To understand the mechanism behind this reliability, we analyze the distribution of invoked Python packages in Figure 4. The data reveals distinct behavioral shift: whereas SimpleTIR relies significantly on generic utility packages (Other), FLV-RL demonstrates massive surge in the usage of Symbolic/Logic libraries. These formal tools constitute 62.5% of FLV-RLs callsa 20-point increase over SimpleTIR. Meanwhile, the usage of Numerical/Scientific libraries remains stable (21%), indicating that our methods gains are driven specifically by the adoption of symbolic logic engines to verify reasoning processes, rather than merely computing numerical answers. See Appendix 9 for the package categorization principles. Figure 5. Token length distribution comparison across GeneralReasoner, SimpleTIR, and FLV-RL. The box plots illustrate the median token usage (center line) and interquartile ranges. 7 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification Table 2. We compare the performance of our proposed method (FLV) against natural language baselines across two training stages: SFT and GRPO. Natural-SFT/GRPO denotes models trained on the same data but without formal logic verification components. FLV-SFT/GRPO denotes our method incorporating formal logic modules and execution feedback. Model Base Natural-SFT FLV-SFT (Ours) Natural-RL FLV-RL (Ours) Logical Mathematical General KOR-Bench BBH MATH AIME24 GPQA-D TheoremQA 13.2 30.4 48.0 35.7 51.0 41. 55.9 68.5 55.4 70.0 60.2 56.6 77.2 54.4 78.6 6. 8.5 20.0 4.8 20.8 29.3 27.3 32.3 30.3 35.4 29. 39.1 53.0 41.2 55.7 AVG 30.0 36.3 49.8 37.0 51. Efficiency Analysis. We analyze the computational cost of our approach by comparing the token length distributions of the natural language baseline (General-Reasoner), toolintegrated reasoning (SimpleTIR), and our FLV-RL method (Figure 5). While FLV-RL incurs moderate computational overhead, we argue that this cost is justified by the substantial performance gains observed across diverse domains. The increased token consumption represents necessary trade-off for achieving breakthrough generalization and ensuring logical soundness in high-stakes reasoning tasks. 5.3. Ablation Studies To evaluate the individual contributions of our proposed components, we conducted an ablation study examining two critical dimensions: the impact of FLV versus pure natural language reasoning, and the effectiveness of different training paradigms. Results are presented in Table 2. Impact of formal logic verification. Comparing FLVbased models against natural language baselines trained on identical data reveals substantial improvements. FLV-SFT achieves 49.8% average accuracy versus 36.5% for NaturalSFT, with particularly strong gains on logic-intensive tasks (KOR-Bench: +16.2 points, TheoremQA: +13.9 points). This demonstrates that formal proofs and execution validation fundamentally improve reasoning by grounding outputs in verifiable logic rather than probabilistic patterns. Impact of multi-stage training We can observe that supervised fine-tuning establishes strong foundations, improving from 30.0% (Base) to 49.8% (FLV-SFT). Policy optimization yields further substantial gains to 51.9% (FLV-RL). Notably, natural language baselines barely improve with RL (37.0% vs 36.5%), while FLV-RL substantially outperforms FLV-SFT, indicating formal verification provides more stable and reliable reward signals for policy optimization. 5.4. Verification Paradigm: Balancing Formalism and Computational Fluency Our initial data construction enforced explicit verification outputs (e.g., proved/disproved) after each logical module. However, this rigid format introduced two critical issues: (i) formal language redundancy, and (ii) suppression of direct calculation. When computational verification was needed, models would bypass direct arithmetic in favor of indirect validation via z3-solver (e.g., asserting + == is proved rather than computing the sum), significantly degrading mathematical performance. To address this, we adopted flexible verification strategy that decouples calculation from validation: (i) Calculation as inference: Models invoke numerical tools directly during reasoning without mandatory verification keywords. (ii) Logic as validation: Formal verification serves as post-hoc validation rather than per-step constraint. Figure 6 compares performance across logic, general, and math subsets under both paradigms. The flexible approach substantially improves math scores while preserving logical reasoning capability. Representative cases are detailed in Appendix E. Figure 6. Enforced vs. Flexible Verification Paradigms. (a) Enforced verification imposes rigid checkpoints throughout the reasoning process, while flexible verification enables adaptive utilization of logic verification. (b) Performance gains after switching to flexible reasoning across three representative benchmarks. 6. Conclusion In this work, we addressed the fundamental tension between probabilistic language generation and logical consistency in LLM reasoning by introducing framework that dynamically integrates formal logic verification into the reasoning process. Through our two-stage training methodology 8 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification combining FLV-SFTs rigorous data synthesis pipeline with formal logic verification-guided policy optimization, we demonstrated that real-time symbolic feedback can effectively mitigate logical fallacies that plague standard Chainof-Thought approaches. Empirical evaluation across six diverse benchmarks validates our approach, with our 7B and 14B models achieving average improvements of 10.4% and 14.2% respectively over SOTA baselines, while providing interpretable step-level correctness guarantees. Beyond performance gains, our framework establishes principled foundation for trustworthy reasoning systems by bridging neural fluency with symbolic rigor, thereby enabling more robust logical inference. This opens pathways toward more reliable AI that scales effectively to complex real-world problems across domains requiring strict logical soundness."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Ahn, J., Verma, R., Lou, R., Liu, D., Zhang, R., and Yin, W. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157, 2024. Anthropic, A. Claude 3.5 sonnet model card addendum. Claude-3.5 Model Card, 2024. URL https://www. anthropic.com/news/claude-3-5-sonnet. Baker, B., Huizinga, J., Gao, L., Dou, Z., Guan, M. Y., Madry, A., Zaremba, W., Pachocki, J., and Farhi, D. Monitoring reasoning models for misbehavior and the risks of promoting obfuscation. arXiv preprint arXiv:2503.11926, 2025. Cao, C., Li, M., Dai, J., Yang, J., Zhao, Z., Zhang, S., Shi, W., Liu, C., Han, S., and Guo, Y. Towards advanced mathematical reasoning for llms via first-order logic theorem proving. arXiv preprint arXiv:2506.17104, 2025a. Cao, C., Zhu, H., Ji, J., Sun, Q., Zhu, Z., Wu, Y., Dai, J., Yang, Y., Han, S., and Guo, Y. Safelawbench: Towards safe alignment of large language models. arXiv preprint arXiv:2506.06636, 2025b. Chen, J., He, Q., Yuan, S., Chen, A., Cai, Z., Dai, W., Yu, H., Yu, Q., Li, X., Chen, J., et al. Enigmata: Scaling logical reasoning in large language models with synthetic verifiable puzzles. arXiv preprint arXiv:2505.19914, 2025a. Chen, W., Yin, M., Ku, M., Lu, P., Wan, Y., Ma, X., Xu, J., Wang, X., and Xia, T. Theoremqa: theoremdriven question answering dataset. arXiv:2305.12524, 2023. arXiv preprint Chen, Y., Benton, J., Radhakrishnan, A., Uesato, J., Denison, C., Schulman, J., Somani, A., Hase, P., Wagner, M., Roger, F., et al. Reasoning models dont always say what they think. arXiv preprint arXiv:2505.05410, 2025b. Chen, Z., Yang, J., Xiao, T., Zhou, R., Zhang, L., Xi, X., Shi, X., Wang, W., and Wang, J. Reinforcement learning for tool-integrated interleaved thinking towards cross-domain generalization. arXiv preprint arXiv:2510.11184, 2025c. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Contributors, O. Opencompass: universal evaluation platform for foundation models. https://github.com/ open-compass/opencompass, 2023. Feng, J., Huang, S., Qu, X., Zhang, G., Qin, Y., Zhong, B., Jiang, C., Chi, J., and Zhong, W. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025a. Feng, Y., Weir, N., Bostrom, K., Bayless, S., Cassel, D., Chaudhary, S., Kiesl-Reiter, B., and Rangwala, H. Vericot: Neuro-symbolic chain-of-thought validation via logical consistency checks. arXiv preprint arXiv:2511.04662, 2025b. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Guo, J., Chi, Z., Dong, L., Dong, Q., Wu, X., Huang, S., and Wei, F. Reward reasoning model. arXiv preprint arXiv:2505.14674, 2025b. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id= 7Bywt2mQsCe. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv. org/abs/2103.03874, 2, 2024. Hu, J., Zhang, J., Zhao, Y., and Ringer, T. Hybridprover: Augmenting theorem proving with llm-driven proof synthesis and refinement. arXiv preprint arXiv:2505.15740, 2025. 9 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Ji, Y., Tian, X., Zhao, S., Wang, H., Chen, S., Peng, Y., Zhao, H., and Li, X. Am-thinking-v1: Advancing the frontier of reasoning at 32b scale. arXiv preprint arXiv:2505.08311, 2025. Kamoi, R., Zhang, Y., Zhang, N., Das, S. S. S., and Zhang, R. Training step-level reasoning verifiers with formal verification tools. arXiv preprint arXiv:2505.15960, 2025. Khalifa, M., Agarwal, R., Logeswaran, L., Kim, J., Peng, H., Lee, M., Lee, H., and Wang, L. Process reward models that think. arXiv preprint arXiv:2504.16828, 2025. Le, H., Wang, Y., Gotmare, A. D., Savarese, S., and Hoi, S. C. H. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35: 2131421328, 2022. Leang, J. O. J., Hong, G., Li, W., and Cohen, S. B. Theorem prover as judge for synthetic data generation. arXiv preprint arXiv:2502.13137, 2025. Li, C., Tang, Z., Li, Z., Xue, M., Bao, K., Ding, T., Sun, R., Wang, B., Wang, X., Lin, J., et al. Cort: Codeintegrated reasoning within thinking. arXiv preprint arXiv:2506.09820, 2025a. Li, J. and Ng, H. T. The hallucination dilemma: Factualityaware reinforcement learning for large reasoning models. arXiv preprint arXiv:2505.24630, 2025. LI, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S. C., Rasul, K., Yu, L., Jiang, A., Shen, Z., Qin, Z., Dong, B., Zhou, L., Fleureau, Y., Lample, G., and Polu, S. Numinamath. [https://huggingface. co/AI-MO/NuminaMath-CoT](https://github.com/ project-numina/aimo-progress-prize/blob/ main/report/numina dataset.pdf), 2024. Li, X., Zou, H., and Liu, P. Torl: Scaling tool-integrated rl, 2025b. URL https://arxiv.org/abs/2503.23383. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and In The Twelfth Cobbe, K. Lets verify step by step. International Conference on Learning Representations, 2023. Liu, J., Fan, Y., Jiang, Z., Ding, H., Hu, Y., Zhang, C., Shi, Y., Weng, S., Chen, A., Chen, S., et al. Synlogic: Synthesizing verifiable reasoning data at scale for learning logical reasoning and beyond. arXiv preprint arXiv:2505.19641, 2025b. Liu, S., Liu, H., Liu, J., Xiao, L., Gao, S., Lyu, C., Gu, Y., Zhang, W., Wong, D. F., Zhang, S., et al. Compassverifier: unified and robust verifier for llms evaluation and outcome reward. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 3345433482, 2025c. Ma, K., Du, X., Wang, Y., Zhang, H., Wen, Z., Qu, X., Yang, J., Liu, J., Liu, M., Yue, X., Huang, W., and Zhang, G. Kor-bench: Benchmarking language models on knowledge-orthogonal reasoning tasks, 2024. URL https://arxiv.org/abs/2410.06526. Ma, X., Liu, Q., Jiang, D., Zhang, G., Ma, Z., and Chen, W. General-reasoner: Advancing llm reasoning across all domains. arXiv preprint arXiv:2505.14652, 2025. Mai, X., Xu, H., Wang, W., Zhang, Y., Zhang, W., et al. Agentic rl scaling law: Spontaneous code execution for mathematical problem solving. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Ospanov, A., Feng, Z., Sun, J., Bai, H., Shen, X., and Farnia, F. Hermes: Towards efficient and verifiable mathematical reasoning in llms. arXiv preprint arXiv:2511.18760, 2025. Qwen-Team. Qwen2.5 technical report, 2025. URL https: //arxiv.org/abs/2412.15115. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Ren, Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W., Zhang, L., Fu, Z., Zhu, Q., Yang, D., et al. Deepseekprover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition. arXiv preprint arXiv:2504.21801, 2025. She, S., Liu, J., Liu, Y., Chen, J., Huang, X., and Huang, S. R-prm: Reasoning-driven process reward modeling. arXiv preprint arXiv:2503.21295, 2025. Liu, C., Yuan, Y., Yin, Y., Xu, Y., Xu, X., Chen, Z., Wang, Y., Shang, L., Liu, Q., and Zhang, M. Safe: Enhancing mathematical reasoning in large language models via retrospective step-aware formal verification. arXiv preprint arXiv:2506.04592, 2025a. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 1279 1297, 2025a. 10 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification Yang, K., Swope, A., Gu, A., Chalamala, R., Song, P., Yu, S., Godil, S., Prenger, R. J., and Anandkumar, A. Leandojo: Theorem proving with retrieval-augmented language models. Advances in Neural Information Processing Systems, 36:2157321612, 2023. Yu, T., Ji, B., Wang, S., Yao, S., Wang, Z., Cui, G., Yuan, L., Ding, N., Yao, Y., Liu, Z., et al. Rlpr: Extrapolating rlvr to general domains without verifiers. arXiv preprint arXiv:2506.18254, 2025. Zeng, S. Humanitys Last Hallucination : Forensic Audit of the Scientific Insolvency in GPQA and HLE, 2026. URL https://zenodo.org/records/18293568. Zeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36: 4659546623, 2023. Zhou, K., Liu, C., Zhao, X., Jangam, S., Srinivasa, J., Liu, G., Song, D., and Wang, X. E. The hidden risks of large reasoning models: safety assessment of r1. arXiv preprint arXiv:2502.12659, 2025. Zhou, Y., Cao, C., Yang, J., Wu, L., He, C., Han, S., and Guo, Y. Lras: Advanced legal reasoning with agentic search. arXiv preprint arXiv:2601.07296, 2026. Sheng, J., Lyu, L., Jin, J., Xia, T., Gu, A., Zou, J., and Lu, P. Solving inequality proofs with large language models. arXiv preprint arXiv:2506.07927, 2025b. Skalse, J., Howe, N., Krasheninnikov, D., and Krueger, D. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:94609471, 2022. Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q., Chi, E., Zhou, D., et al. Challenging big-bench tasks and whether chain-ofthought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 1300313051, 2023. Tian, Y., Huang, R., Wang, X., Ma, J., Huang, Z., Luo, Z., Lin, H., Zheng, D., and Du, L. Evolprover: Advancing automated theorem proving by evolving formalized problems via symmetry and difficulty. arXiv preprint arXiv:2510.00732, 2025. Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Wang, H., Unsal, M., Lin, X., Baksys, M., Liu, J., Santos, M. D., Sung, F., Vinyes, M., Ying, Z., Zhu, Z., et al. Kimina-prover preview: Towards large formal reasoning models with reinforcement learning. arXiv preprint arXiv:2504.11354, 2025. Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S., Naik, A., Stap, D., et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705, 2022. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Xie, C., Huang, Y., Zhang, C., Yu, D., Chen, X., Lin, B. Y., Li, B., Ghazi, B., and Kumar, R. On memorization of large language models in logical reasoning. 2024. URL https://arxiv.org/abs/2410.23123. Xu, Z., Liu, Y., Yin, Y., Zhou, M., and Poovendran, R. Kodcode: diverse, challenging, and verifiable synthetic dataset for coding. arXiv preprint arXiv:2503.02951, 2025. Xue, Z., Zheng, L., Liu, Q., Li, Y., Zheng, X., Ma, Z., and An, B. Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. arXiv preprint arXiv:2509.02479, 2025. 11 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification"
        },
        {
            "title": "Limitations",
            "content": "Despite significant improvements in logical reasoning capabilities, our framework faces two primary limitations. First, integrating real-time formal verification introduces computational overhead, increasing RL training time by approximately 2 compared to standard baselines. However, this cost is acceptable given the substantial performance gains (10.4%-14.2% improvement) and superior data efficiencywe achieve comparable results using only fraction of the training data required by existing methods, such that reduced data collection costs offset the increased training time. Second, our data synthesis pipeline faces formalization challenges when translating natural language into verifiable formal representations. While conversion success rates are high in structured domains like mathematics and logic, ambiguous or commonsenseheavy descriptions may produce mapping errors that generate incorrect verification feedback, limiting generalizability to open-ended reasoning tasks and highlighting the need for more robust auto-formalization techniques. A. Reward Calculation Pseudocode Table 3. Hierarchical reward function for formal logic verification-guided policy optimization Algorithm: Hierarchical Reward Computation Input: Output y, Ground truth answer a, Predicted answer ˆa Output: Total reward R(y) Hyperparameters: = 3 (correctness weight) γstruct = 3.0, βstruct = 1.0 (severity penalties) α = 1.0 (base structural score) λtag = 0.005, τtag = 200 (tag penalty coefficients) λcall = 0.5, Nmax = 3 (tool call limits) λlen = 0.04, δmax = 10 (length penalty) Step 1: Check Fatal Errors (Cfatal) if token-level repetition detected or execution timeout or tool calls > 2 Nmax or multiple termination tags then return R(y) = γstruct = 8.0 Step 2: Check Invalid Format (Cinvalid Cfatal) if solution extraction fails or solution length > 512 tokens or missing closing tag or Nmax < tool calls 2 Nmax then return R(y) = βstruct = 6.0 Step 3: Compute Structural Reward Rstruct(y) Nundef = count of undefined tags Ncall = count of tool invocations Rstruct(y) = α λtag min(Nundef, τtag) λcall max(Ncall Nmax, 0) Step 4: Compute Correctness Reward Rcorrect(y) flen(ˆa, a) = min(len(ˆa) len(a), δmax) if ˆa matches then Rcorrect(y) = λlen flen(ˆa, a) else Rcorrect(y) = Step 5: Compute Total Reward R(y) = Rstruct(y) + Rcorrect(y) 12 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification Table 3 provides the complete algorithmic implementation of our multi-component reward function used in FLV-RL training. The pseudocode details the step-by-step computation of format rewards, correctness rewards, and formal verification rewards, including all constraint checks and penalty mechanisms described in Section 4. The time complexity for calculating the reward R(y) is dominated by the verification of structural constraints and semantic correctness. Let denote the length of the generated response in tokens. The initial screening for pathological states (Cfatal) and invalid formats (Cinvalid) requires linear scan of the output tokens to detect repetition loops, count tool invocations (Ncall), and validate tags, resulting in O(L) complexity. If the response is valid, computing Rstruct(y) involves constant-time arithmetic operations after the initial scan. The semantic verification Rcorrect(y) depends on the evaluation metric; assuming string matching or metric comparison between the extracted answer ˆa and ground truth a, this step operates in O(ˆa + a). Therefore, the total time complexity per generation is O(L), ensuring the reward calculation remains efficient and does not introduce significant computational overhead during training. B. Dataset Construction Details Our data construction pipeline systematically processes reasoning question through multiple stages of generation, logic extraction, formal translation, and verification to create high-quality training data for supervised fine-tuning. The resulting dataset combines natural language reasoning with formally verified logical modules, enabling our models to learn both human-readable reasoning patterns and mathematically sound logical validation. Table 4 presents comprehensive analysis of the execution-based validation outcomes across our merged dataset of 9,162 reasoning chains (a subset of of full training data). The match rate distribution reveals that substantial majority (59.55%) of the generated formal proofs achieve perfect alignment with expected outputs (vact ), successfully passing Stage 1 validation without requiring further verification. An additional 26.53% of proofs fall within the high-confidence range (95100% match rate), indicating strong but imperfect alignment that necessitates semantic equivalence checking in Stage 2. = vexp Notably, 40.45% of the dataset exhibits match rates below 100%, triggering our multi-stage validation pipeline. Among these cases, the consistency distribution (lower panel) demonstrates that 62.22% (2,306 instances) maintain semantic equivalence despite surface-level discrepanciessuccessfully recovering through Stage 2 verification or Stage 3 proof rewriting. The remaining 37.78% (1,400 instances) represent fundamental misalignments that are discarded from the training corpus. This stratified validation approach ensures that our final dataset preserves both syntactic precision and semantic coherence, with approximately 85.7% overall retention rate (= 59.55% + 40.45% 62.22%) after rigorous quality control. The distribution pattern further reveals that only 0.83% of proofs exhibit critically low match rates (below 60%), suggesting that the initial CoT generation process produces predominantly high-quality candidates. This validates our teacher models effectiveness while highlighting the necessity of execution-based verification to capture subtle logical inconsistencies that may elude purely language-based evaluation. C. Hyperparameter Specification We carefully calibrate the reward function hyperparameters to balance the learning of hybrid reasoning patterns with answer correctness, while preventing pathological behaviors during training. Correctness vs. Format Balance. We set the correctness weight = 3 and base structural score α = 1.0 to ensure the model balances learning the hybrid reasoning paradigm with exploring correct answers under this new framework. This 3:1 ratio encourages the model to prioritize semantic accuracy while maintaining proper integration of natural language and formal verification components. Fatal Error Prevention. Given the complexity of hybrid reasoning, models in early training stages are prone to generating pathological outputs such as repetitive tokens or malformed structures. We therefore define such cases as fatal errors with the maximum penalty γstruct = 3.0, while setting βstruct = 1.0 for invalid but recoverable format violations. This hierarchical penalty structure strongly discourages catastrophic failures while allowing the model to explore within reasonable boundaries. Tag Usage Regulation. In hybrid reasoning, the model must learn our defined tag vocabulary (e.g., <code>, <interpreter>). During training, the model may explore alternative tags, which we discourage through λtag = 0.005 with cap at τtag = 200. This cap prevents tag-related penalties from overwhelming correctness rewards, ensuring balanced learning of both answer accuracy and proper tag usage. 13 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification Table 4. Match rate and consistency analysis. Match Rate Distribution Match Rate Range (%) Count Percentage (%) 020 2040 4060 6070 7080 8085 8590 9095 9599 99100 (excl.) 100 Total 1 11 64 63 168 149 269 551 1,402 1,028 5, 9,162 0.01 0.12 0.70 0.69 1.83 1.63 2.94 6.01 15.30 11.22 59.55 100.00 Consistency Distribution (when match rate = 100%) Consistent Count Percentage (%) No Yes Total 1,400 2,306 3,706 37.78 62. 100.00 Formal Verification Efficiency. Our data synthesis analysis reveals that most problems can be solved within 4 formal verification steps. To optimize inference efficiency and reduce unnecessary formalization, we set Nmax = 3 as the baseline limit with penalty coefficient λcall = 0.5. Responses exceeding this threshold incur incremental penalties, while those exceeding 2 Nmax = 6 calls are classified as fatal errors, strongly discouraging excessive tool invocations. Length Control. Following prior work on general reasoners (Ma et al., 2025), we apply length-based penalties to discourage excessively verbose generations. We set λlen = 0.04 with maximum cap δmax = 10, and enforce hard limit of 512 tokens for extracted solutions. This encourages concise, focused reasoning without sacrificing completeness. Model-Based Verifier. Manual review reveals that diverse domain problems beyond mathematics cannot be verified through rule-based methods. We therefore employ model-based verifier. Empirical evaluation shows that CompassVerifier-7B achieves an optimal balance between accuracy and efficiency, leading to its selection as our verifier. D. Training Dynamics and Behavior Analysis We analyze key training dynamics across 120 optimization steps in Figure 8. Reward Evolution. Figure 8a shows steady improvement from -0.45 to -0.1, indicating consistent progress in our composite reward function balancing structural integrity, semantic correctness, and efficiency. Response Length. Figure 8b exhibits U-shaped pattern: initial decrease from 3100 to 2850 tokens as the model eliminates redundancy, followed by stabilization around 3100-3200 tokens, reflecting an optimal balance between completeness and conciseness. Formal Logic Verification Efficiency. Figure 8c demonstrates rapid improvement from 2.3 to 1.9 in the first 40 steps, then gradual stabilization. This shows the model learns to generate more efficiently verifiable proofs with fewer symbolic interpreter calls. 14 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification Figure 7. Distribution of data categories in training Sets. Left: the categorical breakdown of the SFT dataset, totally 14,117 sample. Right: the categorical breakdown of the RL dataset. Legends list the exact number of samples for each category, totally 3,525 sample. (a) Reward (b) Response Length (c) Number of Logic Verification Figure 8. Training dynamics during FLV-RL optimization over 120 steps, showing (a) composite reward improvement, (b) response length evolution, and (c) number of logic verification optimization. 15 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification E. Analysis of Verification Overhead in Mathematical Reasoning Table 5 illustrates critical failure mode induced by enforced formal verification. The problem requires finding the smallest perfect cube expressible as the sum of three consecutive integers. The correct answer is 27 (= 8+9+10), yet the model arrives at 81 through fundamentally flawed reasoning. The root cause lies in the verification paradigms cognitive overhead: rather than directly computing n3 for small values and checking n3 = 3k (which immediately yields 33 = 27), the model constructs an unnecessarily complex z3-solver script that obscures the arithmetic. Notice how the code attempts to verify 3 273 = (3k)3a nonsensical constraint that conflates the problem statement (finding cube equal to 3n) with an arbitrary symbolic manipulation. The verification framework, instead of catching this error, produces DISPROVED output that the model then rationalizes away (upon closer inspection, it confirmed that 81 works), demonstrating how mandatory verification can paradoxically reduce error-detection capability. This case exemplifies why formal verification tools become liabilities in computational contexts: they introduce syntactic complexity (z3 constraint formulation) that distracts from semantic correctness (direct enumeration: 13 = 1 = 3k, 23 = 8 = 3k, 33 = 27 = 3 9), ultimately degrading performance on problems solvable through elementary arithmetic. The flexible verification strategy addresses this by permitting direct calculation during reasoning, relegating formal tools to post-hoc validation roles where their rigor provides genuine value rather than procedural friction. F. Analysis of Package Usage Distribution Figure 9. Comparison of package usage distribution: SimpleTIR vs. FLV-RL The comparison between SimpleTIR and FLV-RL reveals fundamental shift in problem-solving paradigms, moving from iterative search to abstract reasoning. As defined in Table 6, the Symbolic & Logic category encompasses tools for constraint satisfaction and formal proofs (e.g., z3-solver, sympy). The chart demonstrates substantial increase in this category, rising from 42.5% in SimpleTIR to 62.5% in FLV-RL (Figure 9). This suggests that the FLV-RL model is increasingly relying on mathematical abstraction and logical deduction to solve problems rather than procedural code. Conversely, usage of the Algorithmic & Search categorydefined as handling permutations and iterative loops (e.g., itertools)drops precipitously from 20.2% to 6.5%. This inverse correlation indicates that FLV-RL has largely abandoned brute-force simulation and exhaustive search strategies. Instead of generating candidate solutions through iteration, the model prefers to model the problem space symbolically and solve for the answer directly. Meanwhile, the Numerical & Scientific and Domain & Utils categories remain relatively stable across both models (approximately 21% and 10%, respectively). This implies that while the core reasoning engine has evolved (shifting from search to logic), the foundational requirements for arithmetic calculation and environment interaction remain constant regardless of the solving strategy. Overall, these metrics quantify qualitative leap: the model has transitioned from compute-heavy search approach to reasoning-heavy symbolic approach. 16 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification G. GPQA Problems Zeng (2026) identifies GPQA (Diamond) as having an inherent error rate lower bound of 26.8% and characterizes it as an advanced intellectual booby trap from the old era. The systematic flaws include: (i) Factual errors in questions/answers. (ii) Missing parameters needed for correct solutions. (iii) Transcription mistakes in data. The report concludes that GPQA has transformed from ruler for measuring intelligence into noise generator that measures how well models fit to logically flawed questions rather than actual reasoning ability. H. Case Study: Formal Verification in Economic Reasoning To illustrate how formal logic verification improves economic reasoning, we use standard microeconomics question that exposes typical failure modes of free-form chain-of-thought and shows how verification-guided training corrects them. H.1. Problem and Baseline Error Analysis Problem. Consider the multiple-choice question: For linear demand curve, as we move northwest along the curve, what happens to price elasticity of demand? In the conventional (Q, ) diagram (quantity on the horizontal axis, price on the vertical axis), northwest means higher price and lower quantity. Baseline behavior. The baseline model (Qwen 2.5-7B) selects option (b) goes from elastic to inelastic and exhibits three systematic failures  (Table 7)  . Failure Mode 1: Semantic grounding error (directional mis-mapping). The core mistake is mis-grounding of the spatial term northwest. The correct executable semantics in the (Q, ) plane is: northwest(1 2) (P2 > P1) (Q2 < Q1). (11) The baseline instead behaves as if northwest implied the opposite comparative direction (effectively southeast move), e.g., northwest(1 2) (cid:55) (P2 < P1) (Q2 > Q1), (12) which flips the economic interpretation and deterministically pushes the subsequent reasoning toward the wrong answer. Failure Mode 2: Undetected logical inconsistency. The baseline produces mutually incompatible claims without detecting the contradiction (e.g., asserting both that demand is more elastic at higher prices and that the alleged northwest move reduces elasticity under its mis-grounded direction). Formally, if reasoning chain asserts propositions {ϕi}n i=1, the chain should satisfy global consistency: (cid:94) i=1 ϕi . (13) Pure next-token generation optimizes local likelihood and does not enforce Eq. (13), allowing contradictory statements to coexist. Failure Mode 3: Conceptual conflation (slope vs. elasticity). The baseline conflates constant slope with constant elasticity. For linear demand curve the slope dP dQ = is constant, but point price elasticity (in magnitude) is = bQ, > 0, > 0, ε(Q, ) = (cid:12) (cid:12) (cid:12) (cid:12) dQ dP (cid:12) (cid:12) (cid:12) (cid:12) = 1 , (14) (15) which varies with the ratio therefore increases ε. . Along the same line, moving northwest increases and decreases Q, hence increases Q and 17 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification H.2. Verification-Guided Correction Our framework corrects these errors by interleaving generation with SMT-based verification  (Table 8)  . We summarize four mechanisms. Mechanism 1: Executable semantic grounding. We train the model to translate ambiguous natural language into solver-checkable constraints. For this task, the semantic parser is encouraged to map northwest to Eq. (11) (rather than Eq. (12)), together with domain assumptions typical in economics: P1 > 0, Q1 > 0, P2 > 0, Q2 > 0, and Pi = bQi for {1, 2}. (16) Incorrect mappings are rejected because they become inconsistent with the intended move or with the economic domain; the solver provides immediate feedback via SAT/UNSAT. One simple way to incorporate verification is to weight learning by verification success: Lground = ExD (cid:104) I(cid:0)verify(fθ(x)) = SAT(cid:1) log Pθ (cid:0)fθ(x) x(cid:1)(cid:105) , (17) where fθ is the model-produced formalization and verify() calls the SMT solver. Mechanism 2: Global consistency enforcement over chain. Instead of allowing each step to stand alone, we require that the accumulating set of claims remains satisfiable under the shared constraints (Eqs. (14)(16)). Concretely, we can test competing hypotheses such as: : ε2 < ε1 (18) under the northwest constraints. The solver returns UNSAT for (given standard domain assumptions), steering the model away from inconsistent chains and toward the correct alternative: : ε2 > ε1. (19) Mechanism 3: Counterexample-driven learning (numerical witnesses). When hypothesis fails, the solver can provide concrete satisfying assignments for the correct hypothesis, serving as numerical witness that bridges symbolic proof and intuition. For example, pick > 0 and two points on the same demand curve with P2 > P1 and Q2 < Q1, such as (P1, Q1) = (6, 4) and (P2, Q2) = (8, 2). Then by Eq. (15), ε1 = 1 6 4 = 1.5 , ε2 = 1 8 2 = 4.0 , (20) hence ε2 > ε1. Training on triples (hypothesis, verification result, witness) provides richer supervision than final-answer labels alone. Mechanism 4: Compositional algebraic reasoning with verified substeps. We structure the explanation into verifiable primitives: (i) linear constraint = bQ; (ii) derivative dQ dP = 1 ; (iv) monotonicity: under P2 > P1 and Q2 < Q1, we have P2 , therefore ε2 > ε1. This decomposition yields reusable, Q2 solver-checkable reasoning components rather than brittle pattern matching. ; (iii) elasticity definition ε = > P1 Q1 dQ dP (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 18 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification Table 5. Failure case showing enforced verification overhead in mathematical reasoning (MATH 500). Ground truth answer is 27, but the model predicted 81. Question What is the smallest positive perfect cube that can be written as the sum of three consecutive integers? Answer Let the three consecutive integers be n-1, n, n+1. Their sum is (n-1) + + (n+1) = 3n, which is always multiple of 3. perfect cube that is multiple of 3 must be of the form 3k³ (since 3 is prime). The smallest positive perfect cube is 1³ = 1, but 31 = 3 is not perfect cube. The next is 2³ = 8, and 38 = 24, which is not perfect cube. The next is 3³ = 27, and 327 = 81 = 34, which is perfect cube. Thus, the smallest positive perfect cube that is the sum of three consecutive integers is 81. Lets verify the logic in Python using sympy and z3: <code> from z3 import * import math = Int(n) = Int(k) solver = Solver() solver.add((n-1) + + (n+1) == 3 * n) perfect cube = 27 multiple of 3 = 3 * perfect cube is perfect cube = (math.isqrt(multiple of 3) ** 3 == multiple of 3) print(f\"Testing with perfect cube: {perfect cube}³ = {perfect cube**3}\") print(f\"3 {perfect cube**3} = {3 * perfect cube**3}\") print(f\"Is {3 * perfect cube**3} perfect cube? {is perfect cube}\") solver.add(3 * perfect cube**3 == (3 * k)**3) result = solver.check() if result == sat: print(\"PROVED: 27³ = 81 is perfect cube and 381 = 243 = 34 is also perfect cube\") print(\"Thus, 81 is the smallest positive perfect cube that is the sum of three consecutive integers\") else: print(\"DISPROVED: The reasoning contains logical error\") </code> <interpreter> Testing with perfect cube: 27³ = 19683 3 19683 = 59049 Is 59049 perfect cube? False DISPROVED: The reasoning contains logical error </interpreter> <answer> Lets break down this problem in simple terms. Were looking for the smallest positive number thats both perfect cube (like 1, 8, 27, etc.) and can be written as the sum of three consecutive whole numbers. First, imagine three numbers in row, like 2, 3, and 4. Their sum is 9. Notice that 9 is always divisible by 3 when you add three consecutive numbers - its just 3 times the middle number. Now, we need this sum to be perfect cube. Since 3 is prime number, any perfect cube thats multiple of 3 must be exactly 3 times another perfect cube. For example: - 1³ = 1, and 31 = 3 (not perfect cube) - 2³ = 8, and 38 = 24 (not perfect cube) - 3³ = 27, and 327 = 81 81 is perfect cube (3³ = 27, and 327 = 81), and its exactly 3 times another perfect cube (27). So 81 fits perfectly! The computer verification initially seemed to have an error, but upon closer inspection, it confirmed that 81 works perfectly. The smallest such number is indeed 81. 81 </answer> 19 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification Table 6. Categorization of Python packages by problem-solving paradigm. Category Description Key Packages Implication Symbolic & Logic Handles abstract symbols, constraint satisfaction, formal logic, and graph structures. sympy, networkx, constraint z3-solver, Numerical & Scientific Performs high-precision arithmetic, matrix operations, and statistical analysis. numpy, math, scipy, pandas, fractions Algorithmic Search & Focuses on combinatorial generation, iteration, and discrete optimization strategies. itertools, random, heapq collections, Domain & Utilities Tools for specific non-mathematical domains (text, time, web) and system operations. datetime, nltk, bs4 re, requests, Reasoning: Abstract deduction & proofs. Calculation: Quantitative modeling. Search: Bruteforce & simulation. Knowledge: Information retrieval. Table 7. Error Analysis: Baseline model (Qwen 2.5-7B) reasoning trajectory with systematic failures Question As we move to the northwest (up and to the left) along linear demand curve, elasticity a. goes from inelastic to elastic and the lines slope is constant b. goes from elastic to inelastic and the lines slope is constant c. is constant and so is the lines slope d. goes from inelastic to elastic and the lines slope rises in absolute terms e. is constant and the lines slope falls in absolute terms Asnwer Generated by Qwen-2.5-7b The correct answer is: b. goes from elastic to inelastic and the lines slope is constant. Heres the reasoning: Elasticity of Demand: Elasticity of demand measures the responsiveness of the quantity demanded to change in price. It is calculated as the percentage change in quantity demanded divided by the percentage change in price. Along linear demand curve, as you move from right to left (decreasing price and increasing quantity), the elasticity of demand changes. Slope of the Demand Curve: The slope of linear demand curve is constant, meaning the change in price per unit change in quantity is the same throughout the curve. Elasticity and Position on the Demand Curve: At higher prices (and lower quantities), the demand is more elastic because small change in price leads to relatively larger change in quantity demanded. As you move to the left along the demand curve (decreasing price and increasing quantity), the demand becomes less elastic because the percentage change in quantity demanded becomes smaller relative to the percentage change in price. Therefore, as you move northwest (up and to the left) along linear demand curve, the elasticity of demand goes from elastic to inelastic, and the slope of the line remains constant. Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification Table 8. Formal logic verification-guided reasoning trajectory. Question As we move to the northwest (up and to the left) along linear demand curve, elasticity a. goes from inelastic to elastic and the lines slope is constant b. goes from elastic to inelastic and the lines slope is constant c. is constant and so is the lines slope d. goes from inelastic to elastic and the lines slope rises in absolute terms e. is constant and the lines slope falls in absolute terms Answer Given: linear demand curve with movement northwest (up and to the left). For linear demand curve = bQ where a, > 0, the slope dP dQ = is constant. Elasticity Ed = Lets verify the logic in Python using sympy and z3: varies with position. dP dQ (cid:12) (cid:12) = (cid:12) (cid:12) (cid:12) (cid:12) <code> import numpy as np # Define linear demand curve: = 100 - # This gives us: = 100 - # Slope dP/dQ = -1 (constant) # Calculate elasticity at different points # Elasticity formula: Ed = (dQ/dP) * (P/Q) # For = 100 - Q, we have dQ/dP = -1, so Ed = -1 * (P/Q) = -(P/Q) # We use absolute value for elasticity points = [ (90, 10), # High price, low quantity (northwest) (70, 30), # Upper region (50, 50), # Midpoint (30, 70), # Lower region (10, 90) # Low price, high quantity (southeast) ] print(\"Linear Demand Curve: = 100 - Q\") print(\"Slope (dP/dQ) = -1 (constant)n\") print(\"PointttPricetQuantitytElasticity EdtClassification\") print(\"-\" * 70) for P, in points: # Ed = dQ/dP * P/Q = -1 * P/Q = P/Q elasticity = / if elasticity > 1: classification = \"Elastic\" elif elasticity == 1: classification = \"Unitary\" else: classification = \"Inelastic\" print(f\"tt{P}t{Q}tt{elasticity:.2f}tt{classification}\") print(\"nAs we move NORTHWEST (up and left):\") print(\"- Price increases, Quantity decreases\") print(\"- P/Q ratio increases\") print(\"- Elasticity increases from Inelastic Unitary Elastic\") </code> Classification Price Quantity Elasticity Ed 9.00 2.33 1.00 0.43 0. Elastic Elastic Unitary Inelastic Inelastic <interpreter> Linear Demand Curve: = 100 - Slope (dP/dQ) = -1 (constant) Point ---------------------------------------------------------------------- 90 10 70 30 50 50 30 70 10 90 As we move NORTHWEST (up and left): - Price increases, Quantity decreases - P/Q ratio increases - Elasticity increases from Inelastic Unitary Elastic </interpreter> The calculation confirms: At the southeast end (P = 10, = 90): Ed = 0.11 (highly inelastic). At the midpoint (P = 50, = 50): Ed = 1.00 (unitary elastic). At the northwest end (P = 90, = 10): Ed = 9.00 (highly elastic). As we move northwest along the demand curve (from low price/high quantity to high price/low quantity), elasticity goes from inelastic to elastic. Lets verify the logic in Python using sympy and z3: Continued on next page 21 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification Table 8 Continued from previous page <code> from z3 import * # Prove that for linear demand curve = - bQ (where a, > 0), # elasticity Ed = P/Q increases as increases and decreases # This proves elasticity goes from inelastic to elastic moving northwest # Define symbolic variables P1, Q1, P2, Q2, a, = Reals(P1 Q1 P2 Q2 b) solver = Solver() # Constraints for linear demand curve = - bQ solver.add(a > 0, > 0) # Two points on the demand curve solver.add(P1 == - * Q1) solver.add(P2 == - * Q2) # Point 2 is northwest of Point 1 (higher price, lower quantity) solver.add(P2 > P1) solver.add(Q2 < Q1) # Both points are in valid range solver.add(P1 > 0, Q1 > 0) solver.add(P2 > 0, Q2 > 0) # Try to find counterexample where Ed2 <= Ed1 (elasticity doesnt increase) # If unsat, then Ed2 > Ed1 always holds (elasticity increases moving northwest) solver.add(P2/Q2 <= P1/Q1) result = solver.check() print(\"Checking if elasticity can fail to increase when moving northwest...\") print(f\"Result: {result}\") if result == unsat: print(\"nProof: No counterexample exists!\") print(\"This formally verifies that elasticity ALWAYS increases\") print(\"when moving northwest along linear demand curve.\") print(\"Therefore, elasticity goes from inelastic to elastic.\") else: print(\"nCounterexample found (unexpected):\") model = solver.model() print(model) </code> <interpreter> Checking if elasticity can fail to increase when moving northwest... Result: unsat Proof: No counterexample exists! This formally verifies that elasticity ALWAYS increases when moving northwest along linear demand curve. Therefore, elasticity goes from inelastic to elastic. </interpreter> The Z3 verification proves that for ANY linear demand curve, when moving northwest (increasing price, decreasing quantity), the elasticity MUST increase. This is general mathematical property. Summary: (1) Elasticity behavior: Moving northwest along linear demand curve, elasticity goes from inelastic to elastic (verified both computationally and formally). (2) Slope behavior: The slope of linear demand curve is constant by definition. Evaluating answer choices: Option a: goes from inelastic to elastic and the lines slope is constant . Option b: goes from elastic to inelastic and the lines slope is constant (wrong direction). Option c: is constant and so is the lines slope (elasticity is not constant). Option d: goes from inelastic to elastic and the lines slope rises in absolute terms (slope doesnt change). Option e: is constant and the lines slope falls in absolute terms (both parts wrong). <answer> When moving northwest (up and to the left) along linear demand curve, the price elasticity of demand changes systematically due to the mathematical relationship between price, quantity, and elasticity. For linear demand curve, the slope is constant by definition since its straight line. However, elasticity depends on the ratio of price to quantity at each point. At the lower end of the curve (low prices, high quantities), demand is inelastic because consumers are less responsive to price changes. As we move northwest toward higher prices and lower quantities, the P/Q ratio increases, making demand more elastic. The formal verification demonstrates that this relationship holds universally for all linear demand curves, not just specific examples. The elasticity transitions from inelastic to unitary at the midpoint, then continues to elastic in the upper region of the curve. Therefore, the correct characterization is that elasticity goes from inelastic to elastic while the lines slope remains constant. </answer> I. Prompts This section presents the key prompts used throughout our training pipeline. Table 9 shows the reinforcement learning rollout prompt that guides models to generate formal verification-augmented reasoning during policy optimization. Table 1 detail the prompt used for formal logic verification-guided reasoning dataset construction. You are helpful AI assistant . Initially , when solving question , you would need to think step by step , without the ability to use code for calculation or logical verification . Now , you have enhanced capabilities to write code for both computational Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification Table 9. Prompt for Model Training Prompt for Reinforcement Learning Rollout Solve the following problem step by step. You can selectively use Python with z3 (for logic) and sympy (for calculations) to verify reasoning. Python code will run in an external sandbox, returning output as <interpreter>output</interpreter>. The python code should be complete scripts, including necessary imports. Revise reasoning if sandbox returns disproved or fix code if execution errors occur. Code Format: Each code snippet is wrapped with <code> ```python code snippet ``` </code> Response must end exactly as: <answer> [Summary of all reasoning steps] boxed{[Final answer]} </answer> [Question] tasks and logical reasoning verification . The code will be executed by sandbox , and the result can be returned to enhance your reasoning process . ** Important Note on Code Usage **: You now have two parallel tools to enhance your reasoning : 1. ** Python calculation code ** - for numerical computations , data processing , and mathematical operations 2. ** Z3 logical verification ** - for verifying logical reasoning , constraints , and formal proofs using the Z3 theorem prover These are complementary tools serving different purposes . Use calculation code for computational problems and Z3 for logical verification of reasoning steps . ** Do not use both for the same problem ** - choose the most appropriate tool based on whether you need computation or logical verification . The thinking process can have multiple code snippets . Each code snippet is wrapped with : < code > python code snippet </ code >, and should be executable . The returned result is wrapped with < interpreter > execution results </ interpreter >. Critical : Code Independence Requirement Each code snippet must be completely self - contained and executable independently . This means : - Each code block should include all necessary imports - Each code block should define all variables it uses ( do not rely on variables from previous code blocks ) - Each code block should be able to run successfully if executed in isolation - If you need values from previous calculations , redefine or recalculate them in the new code block 23 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification - Think of each <code > block as being executed in fresh Python environment ** Guidelines for Z3 Usage :** Z3 verification should ONLY be used when it provides genuine formal verification value : Do NOT use Z3 for : - Verifying simple arithmetic calculations (e.g., 2 + 2 = 4, or 1.0 * ( -2.0 - 4.0) = -6.0) - Checking calculations with concrete numbers that Python already computed - Adding concrete values as constraints and then verifying them ( this is circular reasoning ) - Repeating what numerical computation already verified DO use Z3 for : - Proving general mathematical properties or identities that hold for ALL values ( using symbolic variables ) - Verifying complex logical relationships with multiple interrelated constraints - Checking satisfiability of constraint systems or finding whether solutions exist - Proving inequalities or optimization conditions symbolically - Verifying that no counterexamples exist for general claim - Formal verification of logical reasoning steps that involve symbolic relationships - Key principle : Use symbolic variables ( not concrete numbers ) to prove general statements . If the problem only involves concrete arithmetic with specific numbers , skip Z3 verification entirely . Examples of INCORRECT Z3 usage : # BAD : Verifying concrete arithmetic solver . add (m == 1.0) solver . add ( v_i == 4.0) solver . add ( delta_p == -6.0) solver . check () # This just checks if -6.0 == -6.0 , no value added # Remember : each code block needs its imports Examples of CORRECT Z3 usage : # GOOD : Proving general algebraic equivalence from z3 import * , v1 , v2 = Reals (m v1 v2 ) solver = Solver () solver . add (m > 0) # Prove : Delta_p = m( v2 - v1 ) is equivalent to Delta_p = m* v2 - m* v1 for ALL values solver . add (m * ( v2 - v1 ) != * v2 - * v1 ) result = solver . check () print (\" Verification result :\" , result ) assert result == unsat # General constraint , not specific value # Proves no counterexample exists ** Guidelines for Python code calculation Usage :** When NOT to use code : - Simple arithmetic that can be done mentally - Basic algebra or formula substitution - Straightforward unit conversions - Verifying obvious mathematical identities - Problems where all steps are elementary calculations Code Usage Limit : - For problems solvable with basic math : Use code AT MOST 1 -2 times ( or not at all ) - For complex computational problems : Use code AT MOST 3 -4 times - Each code block should serve distinct , necessary purpose - ** Never use multiple code blocks to verify the same calculation in different ways ** Goal : Modify the original thinking process to make it more accurate by : 24 Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification - Replacing manual calculation steps with Python code snippets and their execution results - Adding Z3 logical verification when it provides genuine formal verification beyond simple arithmetic - Keeping the core reasoning logic intact , including any unsuccessful attempts - Adding code only where it provides genuine value - Ensuring each code snippet serves unique , necessary purpose - use Python code or Z3 verification for combined total of no more than 4 times - Wrap the revised thinking process within < revised_thinking_process > and </ revised_thinking_process >. User Question : { question } Original Thinking Process ( without code interpreter support ): < original_thinking_process > { original_response } </ original_thinking_process > Details : 1. Identify sections where Python code execution could speed up reasoning or make calculations more accurate 2. Identify logical reasoning blocks that would benefit from Z3 formal verification ( general properties , not specific calculations ) 3. Replace manual calculation steps with code snippets and corresponding interpreter execution results 4. Each code snippet must be self - contained with all necessary imports and variable definitions 5. Keep the logical flow of the reasoning process intact , including any failed exploration attempts 6. Code snippets should be complete scripts that can execute independently , including necessary imports , without markdown symbols 7. For Z3 verification , always use \" from z3 import *\" at the beginning of each Z3 code block 8. Outputs in code snippets must explicitly call the print function 9. Each code snippet must be immediately followed by its execution result , enclosed in < interpreter > </ interpreter > tags 10. Execution results should match the model output exactly , with no extra or missing tokens 11. Z3 should prove general properties , not verify specific numerical results that Python already computed 12. If Z3 would only repeat what Python arithmetic already verified , omit it entirely 13. Remember : variables defined in one code block are NOT available in subsequent code blocks - redefine them as needed 14. When performing calculations , format numerical outputs to 2 -4 decimal places using round () or - strings (e.g., print (f \"{ result :.2 }\") ) to avoid displaying unnecessary floating - point digits . Choose precision appropriate to the context - sufficient for subsequent reasoning . Revised Thinking Process ( With independent selective Python computation blocks and Z3 formal verification ): Listing 1. Prompt for formal logic verification-guided reasoning chain generation"
        }
    ],
    "affiliations": [
        "Hong Kong Baptist University",
        "Hong Kong University of Science and Technology",
        "Shanghai Artificial Intelligence Laboratory",
        "Zhejiang University"
    ]
}