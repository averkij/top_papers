{
    "paper_title": "LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven Language Representation",
    "authors": [
        "Pengzhi Li",
        "Pengfei Yu",
        "Zide Liu",
        "Wei He",
        "Xuhao Pan",
        "Xudong Rao",
        "Tao Wei",
        "Wei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we introduce LDGen, a novel method for integrating large language models (LLMs) into existing text-to-image diffusion models while minimizing computational demands. Traditional text encoders, such as CLIP and T5, exhibit limitations in multilingual processing, hindering image generation across diverse languages. We address these challenges by leveraging the advanced capabilities of LLMs. Our approach employs a language representation strategy that applies hierarchical caption optimization and human instruction techniques to derive precise semantic information,. Subsequently, we incorporate a lightweight adapter and a cross-modal refiner to facilitate efficient feature alignment and interaction between LLMs and image features. LDGen reduces training time and enables zero-shot multilingual image generation. Experimental results indicate that our method surpasses baseline models in both prompt adherence and image aesthetic quality, while seamlessly supporting multiple languages. Project page: https://zrealli.github.io/LDGen."
        },
        {
            "title": "Start",
            "content": "LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven Language Representation Pengzhi Li, Pengfei Yu, Zide Liu, Wei He,Xuhao Pan, Xudong Rao, Tao Wei, Wei Chen Li Auto Inc. 5 2 0 2 5 2 ] . [ 1 2 0 3 8 1 . 2 0 5 2 : r Figure 1: Generated image samples from LDGen. We present composed prompt with each language in different color, along with the corresponding image that exhibits high aesthetic quality and text-image alignment."
        },
        {
            "title": "Abstract",
            "content": "In this paper, we introduce LDGen, novel method for integrating large language models (LLMs) into existing text-to-image diffusion models while minimizing computational demands. Traditional text encoders, such as CLIP and T5, exhibit limitations in multilingual processing, hindering image generation across diverse languages. We address these challenges by leveraging the advanced capabilities of LLMs. Our approach employs language representation strategy that applies hierarchical caption optimization and human instruction techniques to derive precise semantic information,. Subsequently, we incorporate lightweight adapter and cross-modal refiner to facilitate efficient feature alignment Corresponding author, Project leader. and interaction between LLMs and image features. LDGen reduces training time and enables zero-shot multilingual image generation. Experimental results indicate that our method surpasses baseline models in both prompt adherence and image aesthetic quality, while seamlessly supporting multiple languages. Project page: https://zrealli.github.io/LDGen."
        },
        {
            "title": "Introduction",
            "content": "Text-to-image (T2I) models aim to generate images from text descriptions. (Rombach et al., 2022; Podell et al., 2023; Saharia et al., 2022; Bai et al., 2024; Nichol et al., 2022). Thus, natural language descriptions serve as critical bridge for conveying user intent and generating visually appealing images that accurately capture the intended semantic information. Despite the impressive performance demonstrated by advanced text-to-image models, their reliance on text encoders such as CLIP (Radford et al., 2021) and T5 (Raffel et al., 2020), which are primarily tailored for English, constrains their multilingual capabilities due to the linguistic limitations of training datasets. Recently, large language models (Bai et al., 2023; Liu et al., 2024a; Achiam et al., 2023; GLM et al., 2024; Dubey et al., 2024) have achieved notable success in the field of natural language processing. These models possess advanced language comprehension abilities, enabling them to deeply analyze prompts and provide rich, precise semantic guidance for image generation. Furthermore, many LLMs (Team et al., 2024; Bai et al., 2023; Achiam et al., 2023) are trained on multilingual corpora, granting them the ability to support multiple languages. These advantages have motivated researchers to explore the use of LLMs in textto-image generation tasks. However, some prior approaches (Xie et al., 2024; Ma et al., 2024; Xing et al., 2024; Ye et al., 2024) have attempted to directly replace text encoders with LLMs, leading to unstable training processes and significant challenges for researchers with limited computational resources. For instance, ELLA (Hu et al., 2024) and LLM4GEN (Liu et al., 2024c) seek to align LLMs with the CLIP model but require extensive training data to adapt LLMs representations within diffusion models. These methods often treat LLMs features as mere text conditions, thereby failing to fully exploit the comprehensive language understanding capabilities of LLMs. As shown in Appendix A, directly employing LLMs for image descriptions can introduce unintended content, resulting in semantic biases and adversely affecting the output quality of diffusion models. To effectively address these challenges and integrate large language models into existing textto-image tasks under resource constraints, we propose LDGen. Our approach enables the efficient incorporation of LLM into current diffusion models based on T5/CLIP text encoders with minimal computational demands. As shown in Fig. 2, we introduce robust language representation strategy (LRS). By utilizing hierarchical caption optimization and human instruction strategies, LRS fully harnesses the instructionfollowing, in-context learning, and reasoning capabilities of LLM to accurately derive textual information, thereby enhancing semantic alignment between text and image. Furthermore, inspired by recent advancements in alignment methods (Hu et al., 2024; Zhao et al., 2024; Tan et al., 2024), we employ lightweight adapter to align LLM features with T5-XXL, substantially reducing the training time required for text-image alignment. Additionally, we introduce cross-modal refiner to improve text comprehension and facilitate interaction between LLM and image features. After alignment, the LLM features processed through this refiner exhibit enhanced representational capability. Specifically, the crossmodal refiner integrates self-attention layers, crossattention layers, and feed-forward neural networks. By employing this method, LLM can be effectively integrated into existing diffusion models with minimal training. Moreover, the multilingual capabilities of LLM are preserved, enabling zero-shot multilingual image generation without the necessity for training on multilingual text-image datasets. Our experimental results demonstrate that by leveraging the intrinsic features of LLM alongside our innovative modules, LDGen surpasses the prompt comprehension performance of advanced baseline models while seamlessly supporting multiple languages. As shown in Fig. 1, we present several generated images. Our contributions can be summarized as follows: We present LDGen, which efficiently integrates LLM into existing text encoder-based diffusion models and supports zero-shot multilingual text-to-image generation. We propose language representation strategy that leverages the capabilities of LLM through hierarchical caption optimization and human instruction strategies. We introduce LLM alignment and crossmodal refiner to achieve LLM feature alignment and enhance interaction between LLM and image features, enhancing the semantic consistency of conditions."
        },
        {
            "title": "2 Related Work",
            "content": "Text-to-Image. Recently, denoising diffusion probabilistic models (DDPM) (Ho et al., 2020; Nichol and Dhariwal, 2021) have achieved breakthroughs in image synthesis and downstream applications (Zhang et al., 2023; Zhou et al., 2024; Li et al., 2024c; Wei et al., 2023; Li et al., 2023, 2024a,b; Feng et al., 2024). By mapping the image pixels to more compact latent space where Figure 2: Overview of LDGen. The dashed box shows our language representation strategy, with the bottom is our LLM alignment and cross-modal refiner training process. The detailed design of the cross-modal refiner is shown in the green box on the right. denoising network is trained to learn the reverse diffusion process, prominent text-guided generation models have achieved impressive results in terms of image quality and semantic fidelity. Earlier methods (Rombach et al., 2022; Podell et al., 2023) based on the UNet have been tremendously successful in various generative tasks. With the success of the transformer architecture in various fields, diffusion transformer-based methods (Peebles and Xie, 2023; Gao et al., 2023) are notably developing. Techniques like FLUX (Labs, 2024) and SD3 (Esser et al., 2024) introduced the MMBlock to further align text and images during training. PixArt-α (Chen et al., 2023) explored efficient text-to-image training schemes and achieved the first Transformer-based T2I model capable of generating high-quality images at 1024 resolution. Models like Lumina-T2X (Gao et al., 2024) and GenTron (Chen et al., 2024b) extended diffusion transformers from image generation to video generation. Playgroundv3 (PG3) (Liu et al., 2024b) proposed comprehensive VAE training, caption annotation, and evaluation strategy. Large Models in T2I. The text encoder plays crucial role in the text-to-image task. In the initial LDM (Rombach et al., 2022), CLIP (Radford et al., 2021) was used as the text encoder, providing the diffusion model with text comprehension capabilities. Later, Imagen (Saharia et al., 2022) discovered that using large language model with an encoderonly structure like T5 (Raffel et al., 2020) significantly enhanced the models text understanding. Following this, several works (Chen et al., 2023, 2024a; Sun et al., 2024; Betker et al., 2023; Esser et al., 2024) utilized the T5 series of models as text encoders during pre-training. Additionally, some other works (Liu et al., 2024c; Hu et al., 2024; Zhao et al., 2024; Tan et al., 2024), attempted to adapt the T5 and LLMs (Dubey et al., 2024) to the base models pre-trained based on CLIP. Considering the recent success of decoder-only large language models, some works have sought to apply them in image generation frameworks. PG3 (Liu et al., 2024b) focused on model structure, believing that knowledge in LLMs spans all layers, thus replicating all Transformer blocks from the LLM. LiDiT (Ma et al., 2024), from an application perspective, designed an LLM-infused Diffuser framework to fully exploit the capabilities of LLMs. Sana (Xie et al., 2024), focusing on efficiency, directly used the final layer of LLM features as text encoding features. Kolors (Team, 2024) adapts LLMs for use with SDXL by simply replacing the original CLIP text encoder with ChatGLM. These efforts collectively demonstrate that LLMs still hold significant research potential in the field of image generation."
        },
        {
            "title": "3.1 Motivation",
            "content": "Text encoding is pivotal component in text-toimage models, significantly influencing the quality of the generated images. As shown in Fig. 3, the CLIP (Radford et al., 2021) and T5 (Raffel et al., 2020) series models currently dominate the field of text encoders. However, the rapid advancement of large language models (Achiam et al., 2023; Team et al., 2024) is noteworthy. These models employ autoregressive language modeling techniques (Yang, 2019; Black et al., 2022) in unsupervised learning. Through processing vast amounts of text data, they are beginning to exhibit remarkable reasoning and contextual understanding capabilities. They excel across range of textual tasks. In particular, LLMs trained on multilingual corpora have demonstrated substantial promise in text-toimage generation tasks. Nonetheless, critical challenge persists: many existing models rely on CLIP/T5 series text encoders, which are predominantly trained on English corpora and perform effectively. Transitioning to LLMs by replacing the existing text encoders and retraining these models from scratch would involve considerable resource expenditures. To address this issue, we employ LDGen, which seamlessly integrates LLMs into existing diffusion models based on T5/CLIP text encoders, utilizing only small portion of the initial training resources. These new models not only outperform the originals but also enable zero-shot textto-image generation across multiple languages."
        },
        {
            "title": "3.2 Language Representation Strategy",
            "content": "Based on the above analysis, while large language models offer substantial advantages, they still encounter several significant challenges. As dialogue models, LLMs employing decoder-only architecture rely on autoregressive language modeling methods. These models learn linguistic patterns through unsupervised training on large-scale text datasets by predicting the subsequent word in sequence. However, this characteristic often makes it difficult to control the model outputs, leading to producing lot of redundant information. We observe that both LiDiT (Ma et al., 2024) and Sana (Xie et al., 2024) utilize human instructions to help LLMs produce more stable content. However, as shown in Fig. 4, these methods can conflict with the original captions. Incorrect human instructions may cause outputs to deviate from factual accuracy and generate fabricated information, thereby disrupting text-image alignment and potentially decreasing the effectiveness of training. To address these challenges, we employ hierarchical captioning strategy. This approach is complemented by extensive human instruction optimization to achieve optimal language representation and enhance semantic alignment between text and images. First, similar to PG3s (Liu et al., 2024b) multi-level image description technique, Figure 3: Distribution of text encoder and supported languages. English-based CLIP/T5 series models remain the primary text encoders. Figure 4: The red words in Sanas generated result highlight elements that do not align with the image. Providing incorrect instructions can change the original caption, potentially creating inaccurate descriptions. we utilize the Internvl2-40B model (Chen et al., 2024d,c) to re-caption all image data. We generate six captions of varying lengths, ranging from simple to detailed, to comprehensively capture the image content. For detailed captioning prompts, please refer to Appendix Fig. 8, HI-05. During training, these hierarchical captions are randomly sampled and input into the LLM. As shown in Tab. 1, compared to original single-caption methods, LRS enables the model to more effectively capture the hierarchical structure of language concepts while maintaining high CLIP score. For these complex and varied-length hierarchical captions, we further refined human instructions to ensure that the LLMs outputs maintain high CLIP score and avoid generating non-existent information. As shown in Tab. 1, the LLM surprisingly enhances the CLIP scores of the original captions, revealing that our language representation strategy effectively extracts semantic information and enhances text-to-image alignment during model training. To support multilingual text-to-image generation, we evaluated several mainstream LLMs. We selected Qwen (Yang et al., 2024) as our preferred model because it is one of the few trained on multilingual corpora and exhibits exceptional performance in text-related tasks."
        },
        {
            "title": "3.3 LLM Alignment",
            "content": "For pre-trained diffusion models (Chen et al., 2023; Podell et al., 2023), aligning the original text enTable 1: Human Instruction Comparison. Each entry has the CLIP-Score (Hessel et al., 2021) on the left and the LongCLIP-Score (Zhang et al., 2024) on the right, with the average word number is in gray brackets (.). Original refers to the initial caption. \"HI\" indicates outputs from various Human Instruction strategies. The highest scores are highlighted in bold, while the second-highest scores are underlined. Scores that surpass the original captions are marked with gray background . Caption-1 Caption-2 Caption-3 CaptionCaption-5 Caption-6 Original 27.65/29.53 (4.48) 29.65/31.49 (8.99) 30.20/33.24 (21.71) 27.53/34.64 (45.16) 25.39/34.43 (118.06) 25.42/34.65 (118.06) Ours 27.66 / 29.74 (7.63) 29.66 / 31.63 (11.67) 29.50/32.92 (25.25) 27.13/33.76 (46.96) 25.40 / 33.74 (106.18) 25.48 /33.96 (106.18) No-HI 22.21/27.44 (204.29) 22.89/28.35 (173.66) 23.75/29.52 (183.91) 24.56/30.03 (249.35) 23.87/30.33 (304.45) 23.72/31.09 (304.45) HI-01 22.79/27.95 (335.19) 23.76/30.31 (307.68) 24.40/31.05 (306.33) 24.67/31.49 (329.49) 24.86/31.78 (350.39) 25.05/32.18 (350.39) HI-02 23.33/30.04 (87.52) 24.47/31.78 (70.00) 25.29/32.77 (80.78) 25.33/33.35 (116.45) 25.37/33.37 (183.10) 25.36/33.60 (183.10) HI-03 22.61/26.59 (171.14) 23.32/28.89 (172.76) 24.31/30.31 (194.68) 24.63/30.42 (253.19) 24.38/29.65 (334.34) 24.26/30.45 (334.34) HI-04 23.06/29.61 (254.19) 24.07/31.25 (245.64) 24.72/32.37 (226.49) 25.01/33.25 (205.26) 25.22/33.52 (205.27) 25.38/33.89 (205.27) HI-05 22.40/27.97 (224.43) 23.41/29.93 (214.01) 24.33/31.03 (192.64) 24.89/32.19 (167.07) 25.30/33.20 (177.34) 25.59/33.91 (177.34) coder with LLMs features using linear layers is challenging. This is primarily due to the significant differences in the output feature spaces of T5/CLIP encoders and LLMs. As result, directly modifying and training the existing model structure can lead to instability. To address this, we employ two-step approach: first, we align the feature spaces, then fine-tune the model weights to adapt to the new feature space. This method significantly reduces training time. Specifically, we first multiply the LLM output by small coefficient to match the numerical range of T5. This effectively speeds up the feature alignment training. Next, similar to previous methods (Tan et al., 2024), we design three-layer encoder-decoder Transformer adapter to align the feature spaces of the T5 encoder and LLM output. During the adapter training, we utilize the following alignment loss functions: λ1 Lcos + λ2 LMSE. The cosine similarity loss aligns the feature space directions, and mean squared error (MSE) loss can further enhance alignment accuracy in terms of numerical range. By optimizing the alignment loss, we achieve rough alignment between LLM and T5 output feature spaces. This allows us to quickly integrate the LLM into the pre-trained diffusion model, enhancing its overall performance and adaptability."
        },
        {
            "title": "3.4 Cross-Modal Refiner",
            "content": "To improve text comprehension and facilitate interaction between LLM features and image features, we introduce lightweight module called the crossmodal refiner. This module employs sequence of components to optimize and refine LLM feature representations, enabling efficient integration of text and image features. As shown in Fig. 2, it includes elements such as self-attention mechanisms, cross-attention mechanisms, feedforward neural networks, residual connections, normalization layers, and learnable scaling factors. To enhance the interaction between image and text features, the cross-attention layer serves as pivotal component of modal interaction. This layer utilizes LLM features as queries, with latent image features acting as keys and values, to facilitate deep interaction between text and image elements. This design enables the refinement and adjustment of text features based on relevant image information, thereby enhancing the models understanding of cross-modal content. Learnable scaling factors allow the model to gradually balance between original and optimized features during training, ensuring seamless transition from pre-trained weights to new LLM input features. This mechanism effectively integrates the original LLMs robust semantic understanding into the pre-trained models, boosting overall performance. The cross-modal refiner module preserves the original LLM features and effectively integrates image-related information to produce richer, semantically aligned conditional representations. This approach allows us to efficiently integrate the LLM into existing diffusion models within relatively short training times, providing highly semantically aligned conditional information for text-toFigure 5: Comparison of our method with recent enhancement generative models ELLA (Hu et al., 2024), baseline Models SDXL (Podell et al., 2023) and PixArt-α (Chen et al., 2023). Our method achieves the best results in terms of instruction adherence and visual appeal. Table 2: Quantitative comparison results on DPG-Bench. Note that we support multiple languages. Method SD1.5 (Rombach et al., 2022) SDv2.1 (Rombach et al., 2022) LlamaGen (Sun et al., 2024) HART (Tang et al., 2024) Sana (Xie et al., 2024) ELLA (Hu et al., 2024) LLM4GEN(Liu et al., 2024c) Pixart-α (Chen et al., 2023) Ours Param Multi-Ling DPG-Bench 0.86B 0.89B 0.78B 0.73B 0.60B 0.93B 0.86B 0.61B 0.63B 61.18 68.09 65.16 80.89 83.6 80.79 67.34 71.11 80.57 SD3-Medium (Esser et al., 2024) SDXL (Podell et al., 2023) Janus (Wu et al., 2024) Janus-Pro (Chen et al., 2025) Emu-3 (Wang et al., 2024) DALL-E 3 (Betker et al., 2023) FLUX-Dev (Labs, 2024) 2.0B 2.6B 1.3B 7B 8.0B 12.0B 84.08 74.65 79.68 84.19 80.60 83.50 84.0 image generation tasks, significantly enhancing the quality and relevance of generated results."
        },
        {
            "title": "4 Experiments",
            "content": "Model Details. Our method is based on the work of PixArt-α (Chen et al., 2023), which is classic diffusion transformer text-to-image model. It uses the T5-XXL text encoder (Raffel et al., 2020) and has demonstrated excellent performance. We use Qwen2.5-7B-Instruct (Yang et al., 2024) as the LLM and adopt the output features from the last layer, which has dimension of 3584. The VAE remains consistent with PixArt-α (Chen et al., 2023). For the LLM feature alignment module, we employ 3-layer encoder-decoder transformer structure, which includes linear layers to align the LLM dimension of 3584 with the T5 dimension of 4096. The cross-modal refiner uses only one block. Training Details. To reduce computational resources, weve structured our training process into several key stages. First, we train the LLM feature alignment module using approximately 80 million text entries from internal image descriptions, with about 20% of this data being multilingual. Given that T5-XXL (Raffel et al., 2020) doesnt support multiple languages, we align the multilingual features from the LLM output with the English output features of T5-XXL (Raffel et al., 2020). This initial phase consumes around 80 A100 GPU days. Next, drawing inspiration from PixArt-αs training methodology, we adapt our model to 512 resolution and fine-tune it using 24 million text-image pairs. To minimize dataset-specific biases in training, we maintain data scale similar to PixArt-αs (Chen et al., 2023) original approach and incorporate variFigure 6: Multilingual qualitative visualization results. For each panels eight images, we generate them using eight different languages but only display the prompt in one of the languages used. Note that LDGen uses only English prompts during training but achieves zero-shot multilingual generation due to the capabilities of the LLM. Table 3: We compare our method with baseline methods and fine-tuned baseline methods on DPG-Bench and Geneval, demonstrating the effectiveness of our approach. Method Param DPG-Bench (Hu et al., 2024) Geneval(Ghosh et al., 2023) Global Entity Attri. Other Overall Single Obj. Two Obj. Counting Color Attri. Overall Pixart-α (Chen et al., 2023) 0.61B 74.97 Pixart-α(fine-tuned) 0.61B 83.18 0.63B 85.88 Ours 79.32 84.06 87. 78.60 76.69 84.07 83.61 85.21 87.85 71.11 75.05 80.57 0.98 0.95 0.88 0.50 0.37 0.55 0.44 0.37 0.35 0.07 0.43 0. 0.48 0.46 0.51 ous datasets with overlapping ranges, such as JourneyDB (Sun et al., 2023). In the final stage, we continue training at 1024 resolution, utilizing 14 million aesthetic data entries. The entire training process requires approximately 120 A100 GPU days. The count of GPU days excludes the time for T5, Qwen, and VAE feature extraction. LDGen takes only approximately 26% of the GPU days compared to PixArt-α. Evaluation Metrics. We evaluate our approach using two publicly available benchmarks: Geneval (Ghosh et al., 2023) and DPG-Bench (Hu et al., 2024). Geneval is challenging text-toimage generation benchmark designed to showcase models comprehensive generative capabilities through detailed instance-level analysis. DPGBench, comprises 1,065 semantically dense long prompts, aimed at evaluating model performance in complex semantic alignment. These two datasets provide comprehensive assessment of generative models from different perspectives."
        },
        {
            "title": "4.1 Performance Comparison and Analysis",
            "content": "We focus on evaluating the performance of our method compared to the baseline model, PixArtα (Chen et al., 2023). As shown in Tab. 2 and Tab. 3, we utilize two evaluation benchmarks, DPGBench (Hu et al., 2024) and Geneval (Ghosh et al., 2023), to thoroughly assess image-text consistency. Furthermore, we compare our results with advanced models such as the Stable Diffusion series and enhancement methods like ELLA (Hu et al., 2024) and LLM4GEN (Liu et al., 2024c). Our model not only surpasses these baseline models but also achieves approximately 13% performance improvement on DPG-Bench compared to PixArtα, approaching the metrics of some larger-scale models. For the Geneval results, we notice that while single-object scores might decrease due to the LLMs data alignment scale being significantly smaller than the hundreds of millions of samples used for text encoder training, we see significant improvements in multiple aspects, such as color attributes, with the LLMs use. Although we have made progress, there remains gap when compared to state-of-the-art models such as HART (Tang et al., 2024) and Sana (Xie et al., 2024), which are trained from scratch with extensive resources and incorporate cutting-edge techniques. Nevertheless, our method achieves significant performance gains on the base model with relatively minimal overhead. Tab. 2 presents our evaluation scores across different languages. Even without using multilingual image-text pairs during training, our model achieves score of 61.3 in some common languages, nearly matching the 61.2 of certain English-trained image generation models (like SD1.5 (Rombach et al., 2022)). As shown in Tab. 4, we conduct multilingual generation comparison with Sana and additionally support languages that are not supported by Sana. As shown in Fig. 5, we present visual comparisons with other enhancement methods like ELLA (Hu et al., 2024) and LLM4GEN (Liu et al., 2024c), as well as the baseline PixArt-α. Our method exhibits significant improvements in both aesthetics and text alignment, attributed to the integration of an LLM model with robust comprehension capabilities. Even without employing multilingual image-text data during fine-tuning, our model can generate aesthetically pleasing, instructionfollowing images in multiple languages. As shown in Fig. 6, we present generation results in eight languages, displayed from top left to bottom right: German, Spanish, Portuguese, Russian, Italian, Korean, English, and Arabic. Although the model may not generate high-fidelity details across different languages, it is still capable of creating many common scenes and objects. Table 4: Quantitative comparisons of multilingual generation results. We additionally support some languages that are not supported by Sana. Language Korean (Sana) Korean (Ours) Arabic (Sana) Arabic (Ours) Russian (Sana) Russian (Ours) Spanish (Sana) Spanish (Ours) Overall Glob. Enti. Attr. Rela. Other 20.3 21.3 20.1 20.5 23.7 68.6 68.1 73.8 10.6 50. 70.4 63.6 12.5 50.0 42.2 55.9 67.4 61.3 22.1 26.1 23.8 25.4 31.2 66.5 66.4 64. 66.3 72.9 57.5 57.2 56.6 59.7 62.2 70.2 71.4 76.1 70.8 73.5 78. 78.9 74.1 72.0 76.7 80.3 79.6 79.8 75.3 77.9 supporting up to 248 tokens, we use the LongCLIP score as an additional metric. We randomly select 5,000 samples from the training dataset for calculating their CLIPScore (Hessel et al., 2021) and LongCLIP-Score. As shown in Tab. 1, our HI strategy significantly enhances the CLIP scores of the original captions, demonstrating that our language representation strategy accurately extracts text embeddings and effectively improves text-image alignment during model training. Although our training data size is similar to PixArt-α, to eliminate the potential benefits of extra data, we fine-tune the original PixArt-α weights using the T5-XXL (Raffel et al., 2020) with the same training data. As shown in Tab. 3, our method remains superior to this fine-tuned model, validating the effectiveness of our LLM alignment module and cross-modal refiner."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper presents LDGen, which integrates LLMs with diffusion models to enhance text-toimage generation. By using the language representation strategy, LLM alignment module, and crossmodal refiner, we improve semantic alignment between text and images, reduce training demands, and enable zero-shot multilingual generation. Experiments indicate the superiority of LDGen and provide new insights into LLM-T2I tasks."
        },
        {
            "title": "6 Limitations",
            "content": "In this section, we validate our language representation strategy, LLM alignment module, and cross-modal refiner. First, we conduct detailed ablation analysis of our Human Instruction (HI) design, with specific details provided in the appendix. Some captions length exceed CLIPs evaluation capacity, but with LongCLIP (Zhang et al., 2024) Our work integrates LLM into diffusion models with text encoders, enhancing text-image alignment and enabling excellent zero-shot multilingual image generation using limited resources. However, our LLM alignment training data is smaller compared to classic text encoders, potentially affecting the understanding of complex prompts and alignment for certain concepts. Additionally, uneven multilingual corpora distribution leads to varied performance across languages. We plan to expand training data in the future to address these issues."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, QingGuo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. 2024. Meissonic: Revitalizing masked generative transformers for efficient highresolution text-to-image synthesis. arXiv preprint arXiv:2410.08261. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. 2023. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8. Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. 2024a. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. 2023. Pixartalpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426. Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. 2024b. Gentron: Diffusion transformers for image and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64416451. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. 2025. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. 2024c. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. 2024d. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. Scaling rectified flow transformers for highIn Forty-first Internaresolution image synthesis. tional Conference on Machine Learning. Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, and Zeyu Wang. 2024. Dit4edit: Diffusion transformer for image editing. arXiv preprint arXiv:2411.03286. Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. 2024. Lumina-t2x: Transforming text into any modality, resolution, and duration via flowbased large diffusion transformers. arXiv preprint arXiv:2405.05945. Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. 2023. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2316423173. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. 2023. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840 6851. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. 2024. Ella: Equip diffusion models with llm for enhanced semantic alignment. Preprint, arXiv:2403.05135. Black Forest Labs. 2024. Flux. https://github.com/ black-forest-labs/flux. Pengzhi Li, Qinxuan Huang, Yikang Ding, and Zhiheng Li. 2023. Layerdiffusion: Layered controlled image editing with diffusion models. In SIGGRAPH Asia 2023 Technical Communications, pages 14. Pengzhi Li, Qiang Nie, Ying Chen, Xi Jiang, Kai Wu, Yuhuan Lin, Yong Liu, Jinlong Peng, Chengjie Wang, and Feng Zheng. 2024a. Tuning-free image cusIn Eutomization with image and text guidance. ropean Conference on Computer Vision, pages 233 250. Springer. Yize Li, Yihua Zhang, Sijia Liu, and Xue Lin. 2024b. Pruning then reweighting: Towards dataefficient training of diffusion models. arXiv preprint arXiv:2409.19128. Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. 2024c. Photomaker: Customizing realistic human photos In Proceedings of the via stacked id embedding. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86408650. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024a. arXiv preprint Deepseek-v3 technical report. arXiv:2412.19437. Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. 2024b. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695. Mushui Liu, Yuhang Ma, Xinfeng Zhang, Yang Zhen, Zeng Zhao, Zhipeng Hu, Bai Liu, and Changjie Fan. 2024c. Llm4gen: Leveraging semantic representation of llms for text-to-image generation. Bingqi Ma, Zhuofan Zong, Guanglu Song, Hongsheng Li, and Yu Liu. 2024. Exploring the role of large language models in prompt encoding for diffusion models. arXiv preprint arXiv:2406.11831. Alexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 81628171. PMLR. Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. 2022. Glide: Towards photorealistic image generation and editIn Intering with text-guided diffusion models. national Conference on Machine Learning, pages 1678416804. PMLR. William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. Highresolution image synthesis with latent diffusion modIn Proceedings of the IEEE/CVF conference els. on computer vision and pattern recognition, pages 1068410695. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494. Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. 2023. Journeydb: benchmark for generative image understanding. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 4965949678. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. 2024. Autoregressive model beats diffusion: Llama arXiv preprint for scalable image generation. arXiv:2406.06525. Zhiyu Tan, Mengping Yang, Luozheng Qin, Hao Yang, Ye Qian, Qiang Zhou, Cheng Zhang, and Hao Li. 2024. An empirical study and analysis of textto-image generation using large language modelpowered textual representation. In European Conference on Computer Vision, pages 472489. Springer. Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, and Song Han. 2024. Hart: Efficient visual generation with hybrid autoregressive transformer. arXiv preprint arXiv:2410.10812. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836 3847. Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, and Kwan-Yee Wong. 2024. Bridging different language models and generative vision models for textIn European Conference on to-image generation. Computer Vision, pages 7086. Springer. Donghao Zhou, Jiancheng Huang, Jinbin Bai, Jiaze Wang, Hao Chen, Guangyong Chen, Xiaowei Hu, and Pheng-Ann Heng. 2024. Magictailor: Component-controllable personalization in arXiv preprint text-to-image diffusion models. arXiv:2410.13370. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. Kolors Team. 2024. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. arXiv preprint. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. 2024. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869. Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. 2023. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1594315953. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. 2024. Janus: Decoupling visual encoding for unified multimodal arXiv preprint understanding and generation. arXiv:2410.13848. Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. 2024. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629. Sen Xing, Muyan Zhong, Zeqiang Lai, Liangchen Li, Jiawen Liu, Yaohui Wang, Jifeng Dai, and Wenhai Wang. 2024. Mulan: Adapting multilingual diffusion models for hundreds of languages with negligible cost. arXiv preprint arXiv:2412.01271. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Zhilin Yang. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237. Fulong Ye, Guang Liu, Xinya Wu, and Ledell Wu. 2024. Altdiffusion: multilingual text-to-image diffusion model. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 66486656. Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. 2024. Long-clip: Unlocking the long-text capability of clip. In European Conference on Computer Vision, pages 310325. Springer."
        },
        {
            "title": "A Appendix",
            "content": "In the appendix, we provide more comprehensive analysis of the main text, enriched with additional details to enhance understanding. In Fig. 7, we provide detailed comparison between our method and the baseline method, Pixartα (Chen et al., 2023), which demonstrates weaker text comprehension capabilities. Our approach shows significant improvements in terms of aesthetic quality and prompt adherence. In Fig 8, we perform an extensive comparison utilizing the Human Instruction with the Qwen2.57B-Instruct (Yang et al., 2024) of large language model (LLM), consistent with the version applied in the main text. Our Human Instruction method ensures that the LLMs outputs not only sustain high CLIP score but also avoid generating nonexistent information. Furthermore, this method enhances the accuracy of text embeddings, leading to more reliable outcomes. Fig. 9 displays more multilingual generation results. Although some images show slight deficiencies in adhering to the prompts, they still produce outstanding results for many common scene. Fig. 10 showcases images produced from multiple perspectives, including color, theme, style, etc. These varied perspectives effectively illustrate the effectiveness and adaptability of LDGen, Figure 7: More comparisons with Pixart-α. Our method achieves better results in terms of prompt adherence and visua appeal. Figure 8: We provide detailed comparisons using human instructions ranging from simple to complex, comprehensively evaluating the effectiveness of our method. Figure 9: More multilingual qualitative visualization results. For each panels eight images, we generate them using eight different languages but only display the prompt in one of the languages used. Figure 10: More samples generated from LDGen."
        }
    ],
    "affiliations": [
        "Li Auto Inc."
    ]
}