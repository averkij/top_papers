{
    "paper_title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers",
    "authors": [
        "Wongi Jeong",
        "Kyungryeol Lee",
        "Hoigi Seo",
        "Se Young Chun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion transformers have emerged as an alternative to U-net-based diffusion models for high-fidelity image and video generation, offering superior scalability. However, their heavy computation remains a major obstacle to real-world deployment. Existing acceleration methods primarily exploit the temporal dimension such as reusing cached features across diffusion timesteps. Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free framework that accelerates inference along spatial dimension. RALU performs mixed-resolution sampling across three stages: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling on specific regions prone to artifacts at full-resolution, and 3) all latent upsampling at full-resolution for detail refinement. To stabilize generations across resolution transitions, we leverage noise-timestep rescheduling to adapt the noise level across varying resolutions. Our method significantly reduces computation while preserving image quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$ on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is complementary to existing temporal accelerations such as caching methods, thus can be seamlessly integrated to further reduce inference latency without compromising generation quality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 2 2 4 8 0 . 7 0 5 2 : r Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers Wongi Jeong1, Kyungryeol Lee1, Hoigi Seo1 Se Young Chun1,2, 1Dept. of Electrical and Computer Engineering, 2IPAI & INMC Seoul National University, Republic of Korea {wg7139,kr.lee,seohoiki3215,sychun}@snu.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Diffusion transformers have emerged as an alternative to U-net-based diffusion models for high-fidelity image and video generation, offering superior scalability. However, their heavy computation remains major obstacle to real-world deployment. Existing acceleration methods primarily exploit the temporal dimension such as reusing cached features across diffusion timesteps. Here, we propose RegionAdaptive Latent Upsampling (RALU), training-free framework that accelerates inference along spatial dimension. RALU performs mixed-resolution sampling across three stages: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling on specific regions prone to artifacts at full-resolution, and 3) all latent upsampling at full-resolution for detail refinement. To stabilize generations across resolution transitions, we leverage noise-timestep rescheduling to adapt the noise level across varying resolutions. Our method significantly reduces computation while preserving image quality by achieving up to 7.0 speed-up on FLUX and 3.0 on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is complementary to existing temporal accelerations such as caching methods, thus can be seamlessly integrated to further reduce inference latency without compromising generation quality."
        },
        {
            "title": "Introduction",
            "content": "Diffusion models [43, 44, 16] have emerged as dominant framework for generative modeling across diverse modalities such as image [37, 39, 33, 3, 52], video [18, 2, 27, 19, 5] and audio [14, 25, 40]. Convolutional U-Net architectures [38] have traditionally served as the backbone of relatively small diffusion models. However, more recently, diffusion transformers (DiTs) [37, 6, 27, 1] have been introduced, leveraging the scalability of transformers to achieve state-of-the-art results in text-toimage and text-to-video generations. Despite their superior performance, DiTs suffer from high inference latency, which hinders their deployment in real-world or resource-constrained environments. To alleviate this issue, prior works have primarily focused on model compression [22, 7, 11, 30] or temporal acceleration [29, 31, 8, 54]. In contrast, spatial acceleration [46] remains underexplored. Despite the advantages of spatial acceleration for DiTs, such as quadratically reducing the number of tokens by lowering the spatial resolution of latent representations, there are also number of critical challenges. Let us consider typical multi-resolution framework that begins denoising diffusion at low resolution and progressively restores full resolution for the final refinement. Unfortunately, we found that upsampling latents during denoising diffusion process introduces two types of artifacts: (1) aliasing artifacts that occur near edge regions, and (2) mismatching artifacts that were caused by inconsistencies in noise level and timestep. These artifacts are major issues for employing the spatial acceleration of DiTs. * Authors contributed equally. Corresponding author. Preprint. Under review. (a) 4 acceleration on FLUX-1.dev. (b) 7 acceleration on FLUX-1.dev. Figure 1: Generated 10241024 images using both temporal and spatial acceleration methods on FLUX-1.dev (FLUX) for (a) 4 and (b) 7 speedups. Both temporal acceleration (ToCa [54]) and spatial acceleration methods (Bottleneck Sampling [46]) introduce visible artifacts such as blurred edges, texture distortions and semantic inconsistencies. In contrast, our proposed RALU preserves structural fidelity and semantic details across acceleration levels. Zoom-in regions highlight the differences in visual quality, demonstrating that our RALU delivers the most visually faithful results. To efficiently and effectively alleviate these artifacts, we propose Region-Adaptive Latent Upsampling (RALU), training-free accelerating approach capable of high-fidelity image generation for DiTs. While both issues could be partially mitigated simply through early upsampling and noise-timestep rescheduling, we argue that naïvely upsampling all latents early sacrifices the computational benefits. Therefore, to tackle these challenges, our RALU introduces novel three-stage region-adaptive latent upsampling framework: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling selectively on specific regions prone to artifacts (edge regions) to suppress aliasing artifacts at full-resolution, and 3) finally all latent upsampling at full-resolution for detail refinement. To address noise-timestep mismatches and stabilize generation during resolution transitions, we incorporate noise-timestep rescheduling strategy with distribution matching (NT-DM) that can adapt the noise level across varying resolutions. Furthermore, our RALU is complementary to existing temporal acceleration methods and can be combined with caching-based techniques, achieving additional efficiency gains with minimal degradation in quality. See Fig. 1 for the results, demonstrating that our RALU preserves structural fidelity and texture details with significantly fewer artifacts even under aggressive acceleration over temporal acceleration method (e.g., ToCa [54]) and spatial acceleration method (e.g., Bottleneck Sampling [46]). The contributions of this work are summarized as follows: We propose training-free region-adaptive latent upsampling (RALU) strategy, which progressively upsamples while prioritizing edge regions to suppress upsampling artifacts. We introduce noise-timestep rescheduling with distribution matching, which stabilizes mixed-resolution sampling by aligning the noise level and timestep scheduling, and also allows RALU to be integrated with caching-based technique for additional speed-up. Our method achieves up to 7.0 speed-up on FLUX and 3.0 on Stable Diffusion 3, with negligible degradation in generation quality."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Flow matching Flow matching [24] is recent generative modeling framework that learns deterministic transport map from simple prior (e.g., standard Gaussian noise) to complex data distribution by integrating 2 an ordinary differential equation (ODE), without requiring stochastic sampling. In particular, rectified flow [26] defines linear interpolation path between the noise x0 and the data sample x1: xt = (1 t)x0 + tx1, [0, 1], (1) with constant velocity field vt = dxt dt = x1 x0. The learning objective is to train neural network vθ(xt, t) to predict this ground-truth conditional velocity field by minimizing the difference between the predicted velocity and the true velocity. 2.2 Diffusion Transformer acceleration DiTs are computationally expensive, especially when generating high-resolution images, as the cost of self-attention grows quadratically with the number of spatial tokens. To mitigate these bottlenecks, recent research has proposed various inference-time acceleration techniques, which can be broadly categorized into model compression, temporal acceleration, and spatial acceleration methods. Model compression. Model compression techniques aim to reduce model size or computational complexity without retraining from scratch. Common approaches include quantization [42, 22, 7, 9], distillation [23, 12, 51], and block pruning [11, 48, 30, 41]. In particular, block pruning methods skip transformer blocks that contribute less during inference, improving efficiency. However, they often require healing strategy, as quality degradation occurs without fine-tuning. Temporal acceleration. Temporal acceleration aims to reduce computation by skipping certain layers or reusing cached features across timesteps. Caching-based approaches have been extended to DiTs by storing internal activations such as block outputs [8, 31, 54] or attention maps [50]. Some works explore token-level pruning or selective execution [28], or introduce learnable token routers that dynamically decide which tokens to recompute and which to reuse [49, 29]. Spatial acceleration. Spatial acceleration refers to the reduction of computation by processing the latent representation at lower spatial resolutions. Recent studies [39, 17, 45, 21] have proposed cascaded diffusion frameworks that start from low-resolution and achieve high-resolution through upsampling during the denoising process. Spatial acceleration allows for quadratic reduction in computational cost. However, these frameworks require training, which requires substantial resources. To the best of our knowledge, Bottleneck Sampling [46] was the sole existing training-free method for spatial acceleration. However, they suffer from artifacts caused by latent upsampling, highlighting the need for spatial acceleration methods that can mitigate such artifacts."
        },
        {
            "title": "3 Challenges in Spatial Acceleration",
            "content": "Although spatial acceleration effectively reduces computational cost quadratically, it requires latent upsampling, which introduces two types of artifacts. Fig. 2 illustrates these artifacts: aliasing artifacts that appear near edge regions and mismatch artifacts that affect the entire image. In 3.1 and 3.2, we describe our key findings on how each type of artifact can be effectively mitigated. 3.1 Upsampling causes aliasing We observe that aliasing artifacts predominantly emerge in edge regions during latent upsampling. It lacks sufficient high-frequency detail, causing signals from neighboring regions to bleed into one another. Fig. 2(a) visualizes this phenomenon: while latent upsampling maintains fidelity within smooth interior regions, aliasing artifacts appear prominently near object boundaries. Importantly, we find that these artifacts can be avoided by performing upsampling at earlier timesteps, when the semantic structure is still coarse. detailed comparison of upsampling timing is provided in C.1. However, naïvely upsampling all latents early sacrifices the computational benefits. To address this, we propose an adaptive early-upsampling strategy that targets only edge region latents in 4.1. This approach mitigates aliasing while preserving the efficiency of fast inference. Remark 1. Region-adaptive early upsampling. Aliasing artifacts can be effectively suppressed by selectively upsampling edge region latents at earlier timesteps. (a) Aliasing artifacts. (b) Mismatch artifacts. Figure 2: (a) Aliasing artifacts and (b) noise-timestep mismatch artifacts that can arise during latent upsampling. Aliasing artifacts appear as linear patterns near semantic edges, while mismatch artifacts manifest as grid-like distortions or resemble random noise. 3.2 Upsampling causes mismatch in noise and timesteps Correlated noise should be injected after upsampling. Cascaded diffusion models trained to generate high-resolution images from low-resolution inputs [45, 21] have focused on changes in noise levels after upsampling. Specifically, Pyramidal Flow Matching [21] addressed this problem with correlated noise within flow matching models. Starting from an initial noise x0 (0, I), flow matching aims to determine the target x1 by following Eq. (1) through denoising process. Then, The conditional distribution of ˆxt at timestep is: ˆxtx1 (cid:0)tx1, (1 t)2I(cid:1) . But after upsampling, the distribution of the upsampled latent becomes: Up(ˆxt)x1 (cid:0)tUp(x1), (1 t)2Σ(cid:1) , (2) (3) where Σ is the upsampling covariance matrix, and Up() denotes the upsampling function. Since Σ is not proportional to the identity matrix regardless of the type of upsampling, the conditional distribution of Up(ˆxt) cannot be on the trajectory Eq. (1). Therefore, correlated noise should be injected to enforce an isotropic covariance and bring the latent back to the original trajectory. Noise injection in training-free manner causes noise-timestep mismatch. Since adding correlated noise shifts the timestep towards the noise, modified timestep scheduling is required. Unified training models [45, 21] become fitted to the timestep scheduling used during training. However, in training-free setting, using the same timestep schedule as the pretrained model leads to certain intervals being oversampled. If the interval is closer to noise, it will generate relatively more lowfrequency information; if closer to the image, more high-frequency information. In any case, this results in frequency imbalance, leading to noise-timestep mismatch artifacts, as shown in Fig. 2(b). Therefore, to prevent mismatch artifacts in the training-free method, it is critical to preserve the original models timestep distribution, which defines how frequently different noise levels (timesteps) are sampled during generation [53]. We address this by proposing Noise-Timestep rescheduling with Distribution Matching (NT-DM) in 4.2. Remark 2. Noise-Timestep rescheduling with Distribution Matching (NT-DM). Mismatch artifacts caused by timestep distribution misalignment after upsampling can be mitigated by matching the timestep distribution to that of the original model."
        },
        {
            "title": "4 Proposed Method",
            "content": "4.1 Region-Adaptive Latent Upsampling As illustrated in Fig. 3, our approach comprises three progressive stages of region-adaptive latent upsampling, each balancing computational efficiency and generative fidelity. Stage 1: low-resolution denoising to accelerate. The generation process begins at the lower resolution to accelerate denoising. We reduce the latent resolution by factor of 2 along both spatial dimensions (i.e., width and height), resulting in only 1/4 the number of latent tokens. Stage 2: edge region upsampling to prevent artifacts. As mentioned in 3.1, latent upsampling causes aliasing artifacts, particularly in the edge regions of the image. Therefore, we selectively 4 Figure 3: Overview of the proposed RALU framework. RALU operates in three stages: (1) lowresolution sampling for early denoising, (2) mixed-resolution sampling by upsampling edge region latents, and (3) full-resolution refinement by upsampling all remaining latents. (a) Edge region selection using Tweedies formula: we select the top-k patches with the strongest edge signals from the decoded image at timestep e1. (b) visualization of the upsampling step, where selected latents are upsampled using nearest-neighbor interpolation. (c) We add correlation noise to the upsampled latent to match the noise level, and design the noise and timestep schedule such that the divergence between the target timestep distribution (Eq. (8) and the modified distribution (Eq. (9) is minimized (see 4.2). upsample the latents corresponding to the edge regions. To identify such regions, we first estimate the clean latent x0 from the final latent of Stage 1 using Tweedies formula. This latent is then decoded into an image using the VAE decoder and apply Canny edge detection to locate structural boundaries. Next, we select the top-k latent patches corresponding to edge-dense regions (Fig. 3 (a)) and upsample them to high-resolution( Fig. 3 (b)). The FLOPs required to pass through the VAE decoder here is less than 1% of the base method, having negligible impact on inference speed (see B.6 for more information for FLOPs). Since this resolution transition alters the noise distribution, we introduce Noise-Timestep rescheduling with Distribution Matching (NT-DM) to preserve noise-related artifacts, which we discussed in 3.2. NT-DM adjusts the magnitude of the injected noise and timestep scheduling, as detailed in 4.2. Stage 3: full-resolution refinement. In the final stage, all remaining low-resolution latent tokens are upsampled to full-resolution to generate complete high-resolution image. This ensures the consistency between edge region and non-edge region in the final output. By progressively refining only the regions that are most vulnerable to upsampling artifacts and deferring full-resolution processing to the final stage, our three-stage method achieves substantial inference speedups while preserving high perceptual quality with negligible aliasing artifacts. Fig. 4 demonstrates the impact of region-adaptive early upsampling. The image generated with early upsampling shows no aliasing artifacts, whereas the image without it exhibits aliasing in edge regions. 4.2 Noise-Timestep rescheduling with Distribution Matching (NT-DM) As mentioned in 3.2, for training-free methods, appropriate correlated noise injection and timestep rescheduling is required. However, Bottleneck Sampling [46] injected isotropic Gaussian noise and 5 Figure 4: An approach to mitigating two types of artifacts caused by latent upsampling. The first, aliasing artifacts, are alleviated by early upsampling. The second, mismatch artifacts, are mitigated by Noise-Timestep rescheduling with Distribution Matching (NT-DM). used heuristic timestep rescheduling, resulting in low image quality. This section introduces method for finding the strength of correlated noise and appropriate timestep distribution. Noise injection. Starting from Eq. (3), at the ending timestep ek of stage k, the conditional distribution of the latent after 2 nearest-neighbor upsampling is: Up(ˆxek )x1 (cid:0)ekUp(x1), (1 ek)2Σ(cid:1) , (4) where Σ has blockwise structure, such that the 44 diagonal blocks are filled with ones, and all other entries are zero. Since Σ is not proportional to the identity matrix, this distribution does not lie on the trajectory Eq. (1). Therefore, appropriate rescheduling noise (0, Σ) must be added to put back onto the rectified flow trajectory: (aUp(ˆxek ) + bz) x1 Up(ˆxsk+1)x1, (5) where sk+1 is the starting timestep of Stage + 1 and a, is scalar value. By symmetry, if we choose Σ = cΣ, then through Eq. (5), we can express sk+1, and as functions of ek and c. sk+1 = ek (1 ek)/ 1 (1 ek)/ Detailed derivations are provided in A.1. Note that this noise injection is inspired by the trainingbased method of [21]. While not entirely novel, our method differs crucially in being training-free, which prevents us from arbitrarily determining noise (i.e., c) and timestep after upsampling. Therefore, we introduce timestep distribution matching algorithm that is compatible with pretrained models. + ek (1 ek)/ (1 ek)/ + ek + ek = = (6) , , . Timestep rescheduling with distribution matching. After noise injection at timestep ek, the diffusion process restarts from sk+1, meaning that directly reusing the original models timestep scheduling can cause oversampling in overlapping interval [sk+1, ek]. As mentioned in 3.2, this noise-timestep misalignment may cause mismatch artifacts. NT-DM can resolve these artifacts by matching the timestep distribution with the original model. Flow matching based models [10, 1] employ non-uniform timestep sampling. Their corresponding probability density function (PDF) and truncated PDF are denoted as: fh(t) = (1 + (h 1)t)2 (0 1), fh,s,e(t) = fh(t) Fh(e) Fh(s) (s e), (7) where is shifting parameter and Fh(t) is cumulative distribution function of fh(t). Our method injects noise at the end of each stage, which requires additional denoising over the overlapping interval [sk+1, ek]. Therefore, timestep sampling within intervals [0, 1], [s2, e1], ..., [sK, eK1] should follow fh(t). The overall sampling distribution can be written as weighted sum of truncated PDFs: Ptarget(t) = 1 1 + (cid:80)K1 k=1 (ek sk+1) (ek sk+1)fhori,sk+1,ek (t) . (8) (cid:33) (cid:32) fhori,0,1(t) + K1 (cid:88) k= 6 Table 1: Quantitative comparisons of RALU with baselines on FLUX.1-dev. We compare against ToCa [54] and Bottleneck Sampling [46]. The number in parentheses next to FLUX.1-dev indicates the total number of inference steps. Notably, while other methods fail to maintain high-quality generation at 7 speedup, RALU continues to produce strong results. ( / denotes that higher / lower metric is favorable.) Method Latency (s) TFLOPs Speed. Image quality Text alignment FID NIQE CLIP-IQA T2I-Comp. GenEval FLUX.1-dev (50) 23.39 2990.96 FLUX.1-dev (12) ToCa Bottleneck RALU (Ours) FLUX.1-dev (7) ToCa Bottleneck RALU (Ours) 5.75 5.81 5.66 5.55 3.46 4.79 3.42 3.37 729.07 737.63 729.22 723.69 431.45 601.12 431.52 426.01 1.00 4.10 4.05 4.10 4.13 6.93 4.98 6.93 7.02 30.07 28.42 51.83 34.48 28.30 27.33 159.50 38.16 28.68 6. 7.49 42.55 7.96 6.54 8.25 10.62 8.71 6.87 0.707 0.691 0.557 0.671 0.696 0.660 0.253 0.631 0.681 0. 0.565 0.482 0.558 0.549 0.532 0.249 0.545 0.566 0.665 0.634 0.509 0.669 0.639 0.585 0.178 0.649 0.646 Table 2: Quantitative comparisons of RALU with baselines on SD3. We compare against RAS [28] and Bottleneck Sampling [46]. Consistent with results on FLUX.1-dev, RALU achieves high image fidelity and text alignment on SD3 at 2 and 3 speedups. Method Latency (s) TFLOPs Speed. Image quality Text alignment FID NIQE CLIP-IQA T2I-Comp. GenEval SD3 (28) SD3 (14) RAS Bottleneck RALU (Ours) SD3 (9) RAS Bottleneck RALU (Ours) 4.01 2.13 2.07 2.13 2.04 1.46 1.40 1.44 1. 351.66 183.30 185.14 186.21 181.09 123.17 118.99 127.35 116.61 1.00 1.92 1.90 1.89 1.94 2.86 2.96 2.76 3.02 27.47 26.63 25.32 29.01 23.58 27.68 46.37 31.15 23.29 6.09 6.06 4.85 5.70 5.17 6.10 5.70 5.58 5. 0.692 0.667 0.574 0.636 0.684 0.619 0.529 0.566 0.645 0.621 0.603 0.599 0.597 0.633 0.587 0.562 0.568 0. 0.673 0.613 0.589 0.632 0.641 0.556 0.493 0.538 0.597 However, since the actual sampling intervals are [0, e1], [s2, e2], ..., [sK, 1]. Since the intervals differ, the target distribution should also be adjusted accordingly. We use stage-wise shifting parameter hk to control the PDF in each interval. Assuming we sample Nk timesteps in the k-th interval according to fhk (t, sk, ek), the resulting timestep distribution (t) is: (t) = 1 (cid:80)K j=1 Nj (cid:88) k=1 Nkfhk,sk,ek (t). (9) The objective is to minimize the Jensen-Shannon divergence (JSD) between the target distribution Ptarget(t) and the actual distribution (t). This optimization problem is solved via numerical search. The determined values are provided in A.2. To sum up, we adjust the upsampled latent along the flow by injecting correlated noise z, and adaptively determine the degree of noise (a, b) and timestep ({sk}, {hk}) rescheduling that prevents mismatch artifacts. Fig. 4 shows the effectiveness of NT-DM."
        },
        {
            "title": "5 Experiments",
            "content": "To evaluate the performance of RALU, we adopt FLUX.1-dev [1] and SD3 [34]both built on flow matchingas our baseline models. 5.1 Quantitative results Metrics. We measured image quality through FID [15], NIQE [32], and CLIP-IQA [47], and evaluated image-text alignment using T2I-CompBench [20] and GenEval [13]. Acceleration was quantified by measuring latency and FLOPs. See B.4 for details. 7 Figure 5: Qualitative comparison of images generated by baseline methods and RALU on FLUX and SD3 under various speedups. For FLUX, we compare at 4 and 7 speedups; for SD3, at 2 and 3. Zoomed-in regions on the right highlight that RALU preserves fine-grained details and avoids artifacts more effectively than the other baselines, even under high speedups. FLUX with reduced inference steps often produces unrealistic, cartoon-like outputs. Best viewed in zoom. Table 3: Quantitative results of integrating caching-based technique into RALU on FLUX under 4 and 7 speedups. The speedup increased from 4.13 to 5.00 and from 7.02 to 7.94, respectively, with only minimal degradation in image quality and text alignment. Method Latency (s) TFLOPs Speed. Image quality Text alignment FID NIQE CLIP-IQA T2I-Comp. GenEval RALU (4) RALU (4) + Caching RALU (7) RALU (7) + Caching 5.55 4.64 3.37 2.96 723.69 598.89 426.01 376. 4.13 5.00 7.02 7.94 28.30 29.31 28.68 29.35 6.54 6.55 6.87 6. 0.696 0.670 0.681 0.665 0.549 0.547 0.566 0.562 0.639 0.631 0.646 0. T2I generation performance comparison. We compare RALU with existing temporal acceleration methods such as ToCa [54] and RAS [28], and the spatial acceleration method Bottleneck Sampling [46]. Tab. 1 presents the results on FLUX. Caching-based method (ToCa) struggled to deliver strong performance in both image quality and text alignment. While Bottleneck Sampling achieves comparable text alignment, it significantly underperforms RALU in terms of image quality. Tab. 2 shows similar trend on SD3. Since SD3 uses only 28 steps by default, it is less amenable to aggressive acceleration. We therefore evaluate performance under 2 and 3 speedups. RALU consistently preserves image quality while maintaining image-text alignment, demonstrating robust generalization across different base models. 8 {hk} 1.2{hk}0 0.8{hk}0 {hk}0 {hk}0 {hk}0 NIQE CLIP-IQA GenEval c0 c0 c0 1.2c0 0.8c0 6.98 6.89 6.84 6.96 6.89 0.686 0.678 0.684 0.679 0.684 0.632 0.654 0.632 0.618 0.632 Table 4: Effect of NT-DM. {hk}0 and c0 are determined through the noisetimestep distribution matching in 4.2. We measured image quality and text alignment while varying the values. 5.2 Qualitative results We present qualitative comparison of text-to-image (T2I) synthesis results on SD3 and FLUX under various speedups. As shown in Fig. 5, base models with reduced inference stepsand temporal acceleration methods such as ToCa [54] and RAS [28]tend to produce images with noticeable blur or artifacts. While Bottleneck Samplinga representative spatial acceleration baselinegenerally preserves text alignment, it still introduces visible artifacts and suffers from substantial degradation in image quality under higher speedups. In contrast, RALU outperforms both temporal and spatial acceleration baselines, maintaining superior visual fidelity and semantic alignment even under aggressive speedup levels. 5.3 Integrating RALU with caching-based methods As discussed in 1, RALU is complementary to existing temporal acceleration and can be effectively combined. Tab. 3 presents the quantitative results of integrating caching-based technique into our RALU framework. This integration yields additional improvements in inference speed, while preserving high generation quality, as measured by both image quality and text alignment metrics. Remark 3. RALU with caching. RALU achieves additional speedup when combined with cachingbased temporal acceleration, while maintaining generation quality with minimal degradation. Further implementation details on the caching implementation are provided in B.5. 5.4 Ablation study Effect of NT-DM. We conducted experiments to evaluate whether timestep distribution (t), which minimizing JSD with the target distribution Pori(t), is effective for improving image quality. Tab. 4 records the performance metrics based on {hk} and c. Overall, selecting {hk} and to minimize JSD performed better than other cases. Figure 6: Effect of upsampling ratio on prompt-conditioned generation. We compare generated images with upsampling ratios of 0.1, 0.3, and 0.5 across different prompts. At ratio of 0.1, the model occasionally fails to follow the prompt accurately, while from 0.3 onward, the generated images consistently align with the prompt semantics. Effect of upsampling ratio. In Stage 2 of our method, increasing the amount of top-k latent upsampling results in more regions of the image having higher resolution, thus allowing for more faithful reflection of the prompt within the image. We define the upsampling ratio as the fraction of top-k latents selected for early upsampling based on region importance. Fig. 6 highlights the impact of the upsampling ratio on the generated image quality. We note trade-off where higher upsampling ratios improve text alignment but lead to an increase in FLOPs."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we proposed Region-Adaptive Latent Upsampling (RALU), training-free framework to accelerate Diffusion Transformers. RALU follows three-stage process: low-resolution denoising for global semantics, edge-selective upsampling to reduce aliasing, and full-resolution refinement. To prevent mismatch artifacts, we further introduced noise-timestep rescheduling scheme. Experiments show that RALU achieves up to 7.0 speedup with minimal quality loss and complements cachingbased methods for further gains, offering an effective solution for efficient diffusion inference."
        },
        {
            "title": "7 Acknowledgements",
            "content": "This work was supported in part by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) [NO.RS-2021-II211343, Artificial Intelligence Graduate School Program (Seoul National University)] and the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. RS-2025-02263628, RS-2022-NR067592). Also, the authors acknowledged the financial support from the BK21 FOUR program of the Education and Research Program for Future ICT Pioneers, Seoul National University."
        },
        {
            "title": "References",
            "content": "[1] Black Forest Labs. FLUX. https://github.com/black-forest-labs/flux, 2024. [2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, pages 2256322575, 2023. [3] Huiwen Chang, Han Zhang, Jarred Barber, Aaron Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Patrick Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. ICML, 2023. [4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, pages 35583568, 2021. [5] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. CVPR, 2024. [6] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In ECCV, pages 7491. Springer, 2024. [7] Lei Chen, Yuan Meng, Chen Tang, Xinzhu Ma, Jingyan Jiang, Xin Wang, Zhi Wang, and Wenwu Zhu. Q-dit: Accurate post-training quantization for diffusion transformers. arXiv preprint arXiv:2406.17343, 2024. [8] Pengtao Chen, Mingzhu Shen, Peng Ye, Jianjian Cao, Chongjun Tu, Christos-Savvas Bouganis, Yiren Zhao, and Tao Chen. delta-dit: training-free acceleration method tailored for diffusion transformers. arXiv:2406.01125, 2024. [9] Juncan Deng, Shuaiting Li, Zeyu Wang, Hong Gu, Kedong Xu, and Kejie Huang. Vq4dit: Efficient post-training vector quantization for diffusion transformers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 1622616234, 2025. [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [11] Gongfan Fang, Kunjun Li, Xinyin Ma, and Xinchao Wang. Tinyfusion: Diffusion transformers learned shallow. arXiv preprint arXiv:2412.01199, 2024. [12] Weilun Feng, Chuanguang Yang, Zhulin An, Libo Huang, Boyu Diao, Fei Wang, and Yongjun Xu. Relational diffusion distillation for efficient image generation. In ACM MM, pages 205213, 2024. [13] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. NeurIPS, 36:5213252152, 2023. [14] Zhifang Guo, Jianguo Mao, Rui Tao, Long Yan, Kazushige Ouchi, Hong Liu, and Xiangdong Wang. Audio generation with multiple conditional diffusion model. In AAAI, volume 38, pages 1815318161, 2024. [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 30, 2017. [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:6840 6851, 2020. [17] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. JMLR, 23(47):133, 2022. 10 [18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. NeurIPS, 35:86338646, 2022. [19] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. ICLR, 2023. [20] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. NeurIPS, 36:7872378747, 2023. [21] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. arXiv preprint arXiv:2410.05954, 2024. [22] Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdqunat: Absorbing outliers by low-rank components for 4-bit diffusion models. arXiv preprint arXiv:2411.05007, 2024. [23] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. NeurIPS, 36:2066220678, 2023. [24] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [25] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. [26] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [27] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. [28] Ziming Liu, Yifan Yang, Chengruidong Zhang, Yiqi Zhang, Lili Qiu, Yang You, and Yuqing Yang. Region-adaptive sampling for diffusion transformers. arXiv preprint arXiv:2502.10389, 2025. [29] Jinming Lou, Wenyang Luo, Yufan Liu, Bing Li, Xinmiao Ding, Weiming Hu, Jiajiong Cao, Yuming Li, and Chenguang Ma. Token caching for diffusion transformer acceleration. arXiv preprint arXiv:2409.18523, 2024. [30] Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-cache: Accelerating diffusion transformer via layer caching. NeurIPS, 37:133282133304, 2024. [31] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In CVPR, pages 1576215772, 2024. [32] Anish Mittal, Rajiv Soundararajan, and Alan Bovik. Making completely blind image quality analyzer. IEEE Signal processing letters, 20(3):209212, 2012. [33] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [34] William Peebles and Saining Xie. Scalable diffusion models with transformers. In CVPR, pages 41954205, 2023. [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PmLR, 2021. [36] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 21(140):167, 2020. [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 1068410695, 2022. 11 [38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [39] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. NeurIPS, 35:3647936494, 2022. [40] Flavio Schneider, Ojasv Kamal, Zhijing Jin, and Bernhard Schölkopf. Moûsai: Efficient text-to-music diffusion models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 80508068, 2024. [41] Hoigi Seo, Wongi Jeong, Jae-sun Seo, and Se Young Chun. Skrr: Skip and re-use text encoder layers for memory efficient text-to-image generation. arXiv preprint arXiv:2502.08690, 2025. [42] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19721981, 2023. [43] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, pages 22562265. pmlr, 2015. [44] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [45] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023. [46] Ye Tian, Xin Xia, Yuxi Ren, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Yunhai Tong, Ling Yang, and Bin Cui. Training-free diffusion acceleration with bottleneck sampling. arXiv preprint arXiv:2503.18940, 2025. [47] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In AAAI, volume 37, pages 25552563, 2023. [48] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. [49] Haoran You, Connelly Barnes, Yuqian Zhou, Yan Kang, Zhenbang Du, Wei Zhou, Lingzhi Zhang, Yotam Nitzan, Xiaoyang Liu, Zhe Lin, et al. Layer-and timestep-adaptive differentiable token compression ratios for efficient diffusion transformers. arXiv preprint arXiv:2412.16822, 2024. [50] Zhihang Yuan, Hanling Zhang, Lu Pu, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattn: Attention compression for diffusion transformer models. NeurIPS, 37:11961219, 2024. [51] Linfeng Zhang and Kaisheng Ma. Accelerating diffusion models with one-to-many knowledge distillation. arXiv preprint arXiv:2410.04191, 2024. [52] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. ICCV, 2023. [53] Tianyi Zheng, Peng-Tao Jiang, Ben Wan, Hao Zhang, Jinwei Chen, Jia Wang, and Bo Li. Beta-tuned timestep diffusion model. In ECCV, pages 114130. Springer, 2024. [54] Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, and Linfeng Zhang. Accelerating diffusion transformers with token-wise feature caching. arXiv preprint arXiv:2410.05317, 2024."
        },
        {
            "title": "A Derivation",
            "content": "A.1 Derivation of Eq. (6) Starting from Eq. (4), the conditional distribution of the linear combination of upsampled latent Up(ˆxek ) and the correlated noise (0, Σ) is: (aUp(ˆxek ) + bz) x1 (cid:0)aekUp(x1), a2(1 ek)2Σ + b2Σ(cid:1) , (S1) where Σ = cΣ. The conditional distribution of the latent of next stage starting timestep Up(ˆxsk+1)is: Up(ˆxsk+1)x1 (cid:0)sk+1Up(x1), (1 sk+1)2Σ(cid:1) Since we want Eq. (S1) and Eq. (S2), we can obtain the following equations for the mean and standard deviation, respectively: (S2) aek = sk+1, a2(1 ek)2Σ + b2(I cΣ) = (1 sk+1)2I From Eqs. (S3)(S4), matching the Σ coefficient to zero gives a2(1 ek)2 = b2c = a(1 ek) = c. Comparing the part, b2 = (1 sk+1)2 = = 1 sk+1. Collecting Eq. (S3), Eq. (S5) and Eq. (S6) yields: sk+1 = ek (1 ek)/ , + ek = = 1 (1 ek)/ (1 ek)/ (1 ek)/ + ek + ek , . (S3) (S4) (S5) (S6) (S7) (S8) (S9) Since Σ = cΣ 0, 0 1/4 for 2 nearest-neighbor upsampling. A.2 Values determined by NT-DM. Table S1: Determined values of Eq. (S10). Model-acceleration values we choose values determined by Eq. (S10) FLUX-4 FLUX-7 SD3-2 SD3-3 [5, 6, 7] [2, 3, 5] [5, 6, 9] [3, 3, 6] [0.3, 0.45, 1.0] [0.2, 0.3, 1.0] [5.02, 2.59, 2.23] [8.14, 2.86, 2.19] [0.2, 0.3, 1.0] [0.25, 0.3, 1.0] [6.21, 2.23, 1.97] [6.40, 2.60, 2.23] 0.0251 0.0255 0.0586 0.0255 {hk}, = arg min {hk},c JSD(Ptarget(t), (t)). (S10) We determine the values of {hk} and in 4.2 by minimizing the Jensen-Shannon divergence (JSD) between Ptarget(t) and (t) (Eq. (S10)). The resulting values are showed in Tab. S1."
        },
        {
            "title": "B Detailed Experimental Setup",
            "content": "B.1 Experiment Compute Resources We used an NVIDIA A100 GPU as our compute resource. With 80GB of VRAM, we were able to perform inference with memory-intensive models such as FLUX [1] and SD3 [34]. 13 B.2 Baseline Configurations ToCa [54] Token-wise feature Caching (ToCa) is training-free inference-time acceleration method for Diffusion Transformers that improves efficiency through token-level feature caching. Unlike naïve caching methods that reuse all token features uniformly across timesteps, ToCa selectively caches tokens based on their importance, which is determined by their influence on other tokens (via self-attention), their association with conditioning signals (via cross-attention), their recent cache frequency, and their spatial distribution within the image. ToCa operates by dividing inference into cache periods of length , where full computation is performed at the first step and cached token features are reused for the next 1 steps. Within each timestep, fraction of the tokens those deemed less important based on self-attention, cross-attention, cache frequency, and spatial distributionare selected for caching, while the remaining tokens are recomputed. We set = 15, = 90% for FLUX 4 acceleration, and = 40, = 90% for FLUX 7 acceleration. Bottleneck Sampling [46] Bottleneck Sampling is training-free, inference-time acceleration method that exploits the low-resolution priors of pre-trained diffusion models. It adopts three-stage highlowhigh resolution strategy: starting with high-resolution denoising to establish semantic structure, performing low-resolution denoising in the intermediate steps to reduce computational cost, and restoring full resolution at the final stage to refine details. To ensure stable denoising across stage transitions, Bottleneck Sampling introduces two key techniques: (1) noise reintroduction, which resets the signal-to-noise ratio (SNR) at each resolution change to avoid inconsistencies, and (2) scheduler re-shifting, which adapts the denoising schedule per stage to align with the changed resolution and noise levels. We set the number of inference steps for each stages = [4, 15, 6] for FLUX 4 acceleration, = [3, 8, 4] for FLUX 7 acceleration, = [6, 18, 7] for SD3 2 acceleration, and = [4, 12, 4] for SD3 3 acceleration. RAS [28] Region-Adaptive Sampling (RAS) is training-free inference-time acceleration method for Diffusion Transformers that dynamically adjusts the sampling ratio for different spatial regions. At each diffusion step, RAS identifies fast-update regionstypically semantically important areasbased on the models output noise and attention continuity across steps. These regions are refined using the DiT model, while slow-update regions reuse cached noise from the previous step to save computation. To prevent error accumulation in ignored regions, RAS periodically resets all regions through dense steps. Additionally, RAS employs dynamic sampling schedules (e.g., full updates in early steps and gradual reduction thereafter) and key-value caching in attention to maintain quality. This region-aware strategy yields up to 2.5 speedup on SD3 and Lumina-Next-T2I, with minimal quality degradation across standard evaluation metrics. RAS dynamically determines which spatial regions require refinement at each step by identifying fast-update areas based on noise deviation and attention continuity. The sampling ratio denotes the proportion of tokens actively updated by the DiT model in each step, while the remaining tokens reuse previously cached noise to reduce computation. We set the sampling ratio to 0.32 for SD3 2 acceleration and 0.05 for SD3 3 acceleration. B.3 Flow-Matching Based Diffusion Transformers For the quantitative comparison, we performed experiments on two flow-matching based diffusion transformers (DiTs) [34]: FLUX [1] and SD3 [34]. FLUX.1-dev FLUX.1-dev is diffusion-based text-to-image (T2I) synthesis model trained on largescale data via flow matching, achieving state-of-the-art performance. Despite its high generation quality, the model combines T5-XXL [36] and CLIP [35] text encoder, resulting in total of 12 billion parameters. This large model size leads to significant inference latency, posing serious limitations for real-world deployment. In this work, we apply various acceleration methods, including our proposed approach, to FLUX.1-dev and evaluate each method in terms of image quality and faithfulness to the input text. These evaluations demonstrate the effectiveness of our method. Stable Diffusion 3 (SD3) Stable Diffusion 3 (SD3) is text-to-image synthesis diffusion generative model trained with rectified flow objective. It conditions on three different text encodersCLIPL [35], CLIP-g, and T5-XXL [36]and has total of 8 billion parameters. Due to its large model 14 size, SD3 also suffers from non-negligible inference latency, which remains one of the key challenges. In this work, we conduct experiments on SD3 with 2 and 3 speedups to evaluate the effectiveness of our proposed method. In our experiments, we use Stable Diffusion 3 Medium. B.4 Metrics Fréchet Inception Distance (FID) [15] Score The Fréchet Inception Distance (FID) is widely adopted metric for evaluating image generative models by quantifying the discrepancy between the feature distributions of real and synthesized images. It computes the Fréchet distance between the activations of pre-trained image classification networktypically Inception-V3capturing high-level image statistics from its intermediate representations. The FID score is formally defined as: dF (N (µ, Σ), (µ, Σ)) = µ µ 2 + tr (cid:16) Σ + Σ 2(ΣΣ) (cid:17) 1 2 (S11) where µ and Σ denote the mean and covariance of real image features, while µ and Σ correspond to those of generated images. Lower FID scores indicate that the generated images are closer to real ones in terms of both fidelity and diversity. GenEval [13] GenEval is comprehensive benchmark designed to evaluate the alignment between generated images and input text prompts in text-to-image (T2I) synthesis. In this study, we use GenEval to assess how faithfully the generated outputs reflect the semantic content of the given textual descriptions. The metric comprises six sub-tasks, each capturing specific aspect of text-image alignment: 1. Single Object Generation Evaluates the models ability to generate an image from prompts containing single object (e.g., photo of giraffe). 2. Two Object Generation Assesses whether two distinct objects mentioned in the prompt are correctly rendered (e.g., photo of knife and stop sign). 3. Counting Tests whether the number of objects specified in the prompt is accurately represented (e.g., photo of three apples). 4. Color Verifies whether the color attributes mentioned in the prompt are faithfully reflected in the image (e.g., photo of pink car). 5. Position Assesses the models understanding of spatial relations (e.g., photo of sofa under cup). 6. Color Attribution Evaluates whether the model assigns the correct colors to each object when multiple objects and color attributes are mentioned (e.g., photo of black car and green parking meter). For evaluation, we generated 2,212 images using fixed random seed across 553 prompts, each producing four images. This setup ensures consistency and reproducibility across all sub-metrics. T2I-CompBench [20] T2I-CompBench is benchmark specifically designed to assess the compositional understanding of T2I generation models. It comprises structured prompts aimed at evaluating models ability to accurately associate attributes with corresponding objects, ensuring correct semantic alignment in scenarios involving multiple objects and attributes. We leveraged two subsets orthogonal to GenEvalcomplex and textureeach containing 300 prompts. By presenting diverse and challenging prompts, T2I-CompBench offers rigorous evaluation framework for diagnosing issues such as semantic neglect and attribute misassignment that are prevalent in T2I models. For quantitative evaluation, each prompt is sampled with four different random seeds, resulting in total of 300 2 4 = 2400 generated images. The BLIP-VQA score is computed over full prompts from all three attribute subsets. NIQE [32] The Natural Image Quality Evaluator (NIQE) is no-reference image quality assessment (IQA) metric that operates without any training on human opinion scores or exposure to distorted images. It is completely blind, opinion-unaware, and distortion-unaware model that measures 15 deviations from statistical regularities observed in natural images. NIQE extracts perceptually relevant natural scene statistics (NSS) features from local image patches and fits them to multivariate Gaussian (MVG) model built from corpus of pristine images. The image quality is then quantified as the distance between the MVG model of the test image and that of natural images. Unlike many existing NR-IQA models that are limited to distortion types seen during training, NIQE is generalpurpose and performs competitively with state-of-the-art methods such as BRISQUE, while requiring no supervised learning and maintaining low computational complexity. CLIP-IQA [47] The CLIP-IQA metric leverages the pre-trained vision-language model CLIP to assess both quality and abstract perception of images without task-specific training. By using novel antonym prompt pairing strategy (e.g., Good photo. vs. Bad photo.) and removing positional embeddings to accommodate variable input sizes, CLIP-IQA computes the perceptual similarity between images and descriptive prompts. This enables it to evaluate traditional quality attributes like sharpness and brightness as well as abstract attributes such as aesthetic or emotional tone. Extensive evaluations on standard IQA benchmarks and user studies suggest that CLIP-IQA achieves competitive correlation with human perception compared to established no-reference and learning-based methods, while maintaining generality and flexibility. We utilize CLIP-IQA provided by PyIQA 2 with the default prompt setting. B.5 Experimental Setup for Caching-Based Integration We integrate caching-based technique into RALU as described in 5.3. Caching is applied separately to each stage (1, 2, and 3); however, we found that caching in Stage 1 caused significant quality degradation. Therefore, we apply caching only in Stages 2 and 3. For each stage, the number of timesteps under 4 and 7 speedup settings are [5, 6, 7] and [2, 3, 5], respectively (Tab. A.2). In Stages 2 and 3, we store the noise predictions for the first two timesteps. We then compute the cosine similarity between these two noise predictions for each latent and sort the latents in descending order of similarity. The top-k latents, according to predefined ratio, are cached and reused for the remaining timesteps of that stage, skipping recomputation. Caching is not applied to the final timestep of Stage 3 to avoid quality degradation at the output boundary. When caching is applied, the selected top-k latents are excluded from DiT block input, reducing overall computation. We set the caching ratio to 0.4. When generating 10241024 images, the number of latents passed to the DiT block is 1024 in Stage 1 (no caching), 1948 in Stage 2 without caching vs. 1168 with caching, and 4096 in Stage 3 without caching vs. 2457 with caching. B.6 FLOPs of VAE decoder In RALU, we pass through the VAE decoder once more to obtain images for edge region latent selection. However, this introduces only negligible overhead in the overall computational process. The VAE decoders computational cost is 2.48 TFLOPs, which accounts for only 0.08% and 0.71% of the total FLOPs of FLUX (2990.96 TFLOPs) and the SD3 baseline (351.06 TFLOPs), respectively."
        },
        {
            "title": "C Additional Experiments",
            "content": "C.1 Temporal Sensitivity of Latent Upsampling Figure S1 illustrates simplified two-stage setting where region-adaptive sampling is removed. All latents are initially sampled at low resolution, and subsequently upsampled to high resolution at specific timestep, followed by noise-timestep (NT) rescheduling. The figure shows how the denoised image evolves depending on the upsampling timestep. When upsampling occurs at early timesteps, no visible artifacts appear in the final output. In contrast, upsampling at later timesteps leads to prominent artifacts. This phenomenon is determined at the moment of upsampling. Once substantial amount of semantic information has already been generated, applying NT-DM alone is insufficient to prevent aliasing artifacts from emerging. 2https://github.com/chaofengc/IQA-PyTorch 16 Figure S1: Variation in decoded image quality with respect to the upsampling timestep in two-stage framework, where low-resolution latents are upsampled to high resolution before stage 2. When upsampling is performed at early timesteps, denoising in stage 2 proceeds correctly. In contrast, late upsampling introduces artifacts due to accumulated semantic information that propagates to neighboring latents. C.2 Additional Qualitative Results Additional qualitative results are presented in Fig. S2S3 following the technical appendices. All experimental configurations are provided in the main paper and its appendices. We prepared additional qualitative results following Fig. 5. 17 Figure S2: Qualitative comparison of images generated by baseline methods and RALU on FLUX. Best viewed in zoom. 18 Figure S3: Qualitative comparison of images generated by baseline methods and RALU on SD3. Best viewed in zoom. 19 Figure S4: Qualitative comparison of images generated by baseline methods and RALU on FLUX for 4 speedups. Best viewed in zoom. 20 Figure S5: Qualitative comparison of images generated by baseline methods and RALU on FLUX for 7 speedups. Best viewed in zoom. 21 Figure S6: Qualitative comparison of images generated by baseline methods and RALU on SD3 for 2 speedups. Best viewed in zoom. 22 Figure S7: Qualitative comparison of images generated by baseline methods and RALU on SD3 for 3 speedups. Best viewed in zoom. 23 C.3 Uncurated Qualitative Results To demonstrate that our model consistently maintains high generation quality without cherry-picking, even under 4 and 7 speedup on FLUX [1], we present uncurated qualitative results. We randomly sampled 96 prompts from the CC12M [4] dataset and generated corresponding images. The results are shown in Fig. S8S11. Figure S8: 48 uncurated images generated by RALU on FLUX, 4 speedup. 24 Figure S9: 48 uncurated images generated by RALU on FLUX, 4 speedup. 25 Figure S10: 48 uncurated images generated by RALU on FLUX, 7 speedup. 26 Figure S11: 48 uncurated images generated by RALU on FLUX, 7 speedup."
        },
        {
            "title": "D Limitations",
            "content": "While region-adaptive early upsampling is broadly applicable to diffusion transformer models, the Noise-Timestep rescheduling with Distribution Matching (NT-DM) is tailored specifically for flowmatching-based models. Its effectiveness in other generative frameworks, such as score-based or DDIM-style diffusion, remains unverified. Moreover, generalization to other architectures or modalities (e.g., audio or 3D) remains unexplored, and further investigation is required to extend the applicability of RALU beyond current T2I generation."
        },
        {
            "title": "E Broader Impact",
            "content": "RALU enables faster and more resource-efficient generation of high-quality images using diffusion transformers, which has the potential to make such models more accessible for real-world or ondevice applications. This could democratize creative tools for broader user groups while reducing environmental costs associated with large-scale inference. However, this efficiency gain may also facilitate misuse, such as faster generation of harmful or misleading content. Additionally, the selective focus on visually salient regions (e.g., edges) may implicitly encode or reinforce dataset biases, especially in underrepresented object structures. Care must be taken to evaluate fairness and misuse risks, and we encourage future work to explore responsible deployment strategies alongside technical improvements."
        }
    ],
    "affiliations": [
        "Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea",
        "IPAI & INMC, Seoul National University, Republic of Korea"
    ]
}