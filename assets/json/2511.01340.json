{
    "paper_title": "$\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles",
    "authors": [
        "Trishanu Das",
        "Abhilash Nandy",
        "Khush Bajaj",
        "Deepiha S"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters to represent words or phrases creatively) requires a variety of skills such as image recognition, cognitive skills, commonsense reasoning, multi-step reasoning, image-based wordplay, etc., making this a challenging task for even current Vision-Language Models. In this paper, we present $\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$, a large and diverse benchmark of $1,333$ English Rebus Puzzles containing different artistic styles and levels of difficulty, spread across 18 categories such as food, idioms, sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a model-agnostic framework which uses a combination of an unstructured description and code-based, structured reasoning, along with better, reasoning-based in-context example selection, improving the performance of Vision-Language Models on $\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$ by $2.1-4.1\\%$ and $20-30\\%$ using closed-source and open-source models respectively compared to Chain-of-Thought Reasoning."
        },
        {
            "title": "Start",
            "content": "M v: Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles 5 2 0 2 3 ] . [ 1 0 4 3 1 0 . 1 1 5 2 : r Trishanu Das dastrishanu01@gmail.com Tredence Inc. India"
        },
        {
            "title": "Khush Bajaj\nIndian Institute of Technology Kharagpur\nIndia",
            "content": "Abstract Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters to represent words or phrases creatively) requires variety of skills such as image recognition, cognitive skills, commonsense reasoning, multi-step reasoning, image-based wordplay, etc., making this challenging task for even current Vision-Language Models. In this paper, we present v (Rebus Puzzle for the Word Rebus, consisting of the Re - and Bus - symbols), large and diverse benchmark of 1, 333 English Rebus Puzzles containing different artistic styles and levels of difficulty, spread across 18 categories such as food, idioms, sports, finance, entertainment, etc. We also propose RebusDescProgICE, model-agnostic framework which uses combination of an unstructured description and code-based, structured reasoning, along with better, reasoningbased in-context example selection, improving the performance of Vision-Language Models on v by 2.1 4.1% and 20 30% using closed-source and open-source models respectively compared to Chain-of-Thought Reasoning1. Keywords Rebus, puzzles, multimodal, benchmark ACM Reference Format: Trishanu Das, Abhilash Nandy, Khush Bajaj, and Deepiha S. 2018. v: Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym XX). ACM, New York, NY, USA, 7 pages. https: //doi.org/XXXXXXX.XXXXXXX Both authors contributed equally to this research. 1The dataset and code are available at https://github.com/abhi1nandy2/Re-Bus Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference acronym XX, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX Abhilash Nandy nandyabhilash@gmail.com Indian Institute of Technology Kharagpur India"
        },
        {
            "title": "1 Introduction\nRebus Puzzles are a form of wordplay that uses images, letters, and\nsymbols to represent words or syllables. They serve as a creative\ntool to spark lateral thinking, challenge conventional patterns, and\ninvite solvers to uncover hidden meanings through visual clues.\nUnderstanding such puzzles requires a plethora of capabilities such\nas image recognition, commonsense knowledge and reasoning,\nmulti-step reasoning, and understanding the creator’s intent [13].\nFig. 1 shows an example of a Rebus Puzzle containing images and\nletters. The images in the puzzle are that of a “Mill” and “Lime”,\nfollowed by letters that read “Ters”. Combining “Mill”, “Lime”, and\n“Ters” creatively (by adding/subtracting/replacing letters), we get a\nmeaningful word of “Millimeters” as the final answer of the puzzle.\nNote that the choice of the words for images are very crucial - for\ninstance, if the word “Turbine” is used instead of “Mill”, we would\nnot arrive at a meaningful final answer.",
            "content": "Figure 1: Example of Rebus Puzzle in v Puzzle-solving and reasoning abilities of Vision-Language Models have been evaluated previously. For instance, M3Exam [42] evaluates multimodal multiple choice exam questions. MATH-Vision [37] evaluates the mathematical reasoning ability of Vision-Language Models on math questions spanning several topics. There is also prior work on evaluating Rebus Puzzles in English [13] and in Italian [32] Languages. However, prior work on Rebus Puzzles neither Conference acronym XX, June 0305, 2018, Woodstock, NY Trishanu Das, Abhilash Nandy, Khush Bajaj, and Deepiha proposes diverse benchmark having different levels of difficulty, nor does any such work provide model-agnostic solution that can be applied on top of both open as well as closed-source models with minimal or no training for improved performance. The development of Vision-Language Models (VLMs) [1, 5, 15, 17, 22, 33] has witnessed substantial rise in recent years. These models have demonstrated outstanding state-of-the-art (SOTA) performance across various downstream tasks, including Image Captioning and Visual Question Answering. These models are pretrained such that images and text share common embedding space, ensuring that images and their corresponding textual descriptions have similar representations within that space. In this paper, we first inspect whether VLMs are able to understand and solve Rebus Puzzles - Given an image of Rebus Puzzle (like in Fig. 1), the VLM is expected to generate natural language answer to the puzzle as word/phrase (Millimeters in case of Fig. 1). This is challenging task that extends beyond mere image analysis and linguistic comprehension, as it involves layered process that draws on factual knowledge, contextual insight, language skills, and logical reasoning within specific boundariescore abilities essential for tackling many real-world challenges [32]. To evaluate the task, we curated large and diverse multimodal dataset v comprising of 1, 333 English Rebus Puzzles2, where each dataset sample contains an image of Rebus Puzzle which contains combination of images and/or texts, along with the answer to the puzzle, and rich suite of meticulously annotated metadata such as hint to solve the the puzzle, difficulty of the puzzle, whether the spelling of the text/objects in the image is varied in order to get the puzzles answer, is color of text in the puzzle relevant in solving the puzzle, etc., making our proposed v superior to that of the previous works on Rebus Puzzles [13, 32]. Also, to increase the diversity and difficulty of the puzzles, some samples in v are generated using ControlNet [41], which adds an ambient background as backdrop while preserving the core content of the Rebus puzzle. This added background serves as visual distraction, thereby increasing the difficulty of solving the puzzle. Additionally, we propose compute-efficient RebusDescProgICE framework, which combines structured (code-based) and unstructured reasoning in an in-context learning setup, along with lightweight example selection strategy requiring only minimal training. Unlike baselines that rely on single reasoning style, RebusDescProgICE consistently improves puzzle-solving performance. For GPT-4o, it yields steady gains (Word-Level F1 rising from 0.489 in zero-shot normal prompting to 0.512 in three-shot RebusDescProgICE). The benefits are more striking for open-source models: Qwen2-VL-7B achieves up to 2030% relative improvement over description-only and VisProg baselines (e.g., from 0.2 to 0.264 F1). These results highlight that the synergy of structured and unstructured reasoning, coupled with informed example selection, is key to unlocking better performance, particularly for weaker open-source VLMs. We make the following contributions in this paper - (1) We introduce v, large, diverse dataset of 1, 333 annotated English Rebus Puzzles (2) We enhance puzzle difficulty using ControlNet 2The answer to every puzzle is in English. to add distracting yet realistic visual backgrounds (3) We propose RebusDescProgICE, compute-efficient framework combining structured and unstructured reasoning in-context (4) We design novel in-context example selection strategy aligned with anticipated VLM reasoning patterns."
        },
        {
            "title": "2 Background\nLinguistic Puzzle Solving. Linguistic Puzzles have emerged as an\nintriguing benchmark for evaluating the reasoning and language ca-\npabilities of large language models (LLMs) [12, 20, 30]. While early\nresearch predominantly explored English-language challenges like\ncrosswords [8, 16, 31, 35], recent efforts have expanded to include\na richer variety of puzzle types, including popular games such as\nWordle [2] and the New York Times Connections [34]. Beyond\nEnglish, automated puzzle solvers have been developed for other\nlanguages as well—such as French [4], German [43], and Italian\n[3, 44]. Additionally, educational puzzle generators are available\nin languages like Italian [39] and Turkish [40], highlighting the\ngrowing global interest in computational approaches to linguistic\ngames.\nCode-based reasoning using VLMs and LLMs. Structured, code-\nbased reasoning shows improvements in performance in complex,\ncommonsense reasoning tasks. Recent approaches like VISPROG\n[14] extend this paradigm to vision-language tasks, where VLMs\nand LLMs collaborate by generating modular code that orches-\ntrates vision models and logical operations to solve complex vi-\nsual problems. Code-based reasoning methods like PoT (Program\nof Thoughts) [7] help LLMs tackle math by writing and running\ncode, separating thought from calculation. PAL (Program-Aided\nLanguage Models) [11] boost LLM performance by turning text\nproblems into code, allowing a Python Interpreter handle the com-\nputation. Madaan et al. [18] show that even without code in the\ntask, code LLMs do better when commonsense problems are framed\nas code generation.",
            "content": "3 v Dataset 3.1 Our Annotation Pipeline The entire data collection and annotation pipeline is shown in Fig. 2. We curated collection of Rebus Puzzles with meticulously annotated metadata in this section in 3 stages."
        },
        {
            "title": "3.1.1 Stage 1: Collecting Rebus Puzzles from multiple In-\nternet Sources. We collect Rebus Puzzle Images along with the\ncorresponding ground truth answers from 3 different sources -\nhttps://eslvault.com/free-printable-rebus-puzzles/ (contains a di-\nverse set of rebus puzzles that are mostly in black and white),\nhttps://kids.niehs.nih.gov/games/brainteasers/rebus-puzzles (con-\ntains mostly text-based rebus puzzles), and http://flashbynight.com/\nrebus (contains a diverse set of rebus puzzles that are mostly col-\nored). After removing duplicate Rebus Puzzle Images, we end up\nwith 722 Rebus Puzzles. We also verified the answers collected for\neach Rebus Puzzle and made manual corrections to the answer\nwherever necessary.",
            "content": "M v: Rebus Puzzles Understanding Benchmark Conference acronym XX, June 0305, 2018, Woodstock, NY Figure 2: Annotation Pipeline of v Dataset"
        },
        {
            "title": "3.2 Dataset Description and Details",
            "content": "Figure 4: Distribution of Rebus Puzzles based on their category 3API Calls to https://huggingface.co/spaces/hysts/ControlNet were performed during implementation Figure 3: Breakdown of some important Rebus Puzzle Metadata Characteristics"
        },
        {
            "title": "3.1.2 Stage 2: Annotation of Rebus Puzzle Metadata. Sev-\neral types of metadata are annotated for the rebus Puzzles using\n4 annotators, all of whom were at least in their second year of un-\ndergraduate study and enrolled in institutions where English is the\nprimary language of instruction. The annotated metadata includes\n- (1) features related to the process of solving the puzzle, such as\npuzzle difficulty (Easy/Hard), whether spelling of individual ob-\njects/text is different from that in the ground truth answer, hint for\nsolving it, number of units of reasoning to solve it (2) image feature,\nlike whether any text is present in the image (3) binary feature of\nimportance of different attributes such as color, position, orienta-\ntion, aspect ratio, size, style/texture, and number of object(s)/text\nwhen answering the puzzle. Fig. 3 shows the distribution of the\npuzzles across 3 types of binary metadata as a Venn Diagram. This\nshows that Rebus Puzzles are highly diverse, as they are spread out\nacross different combinations of the binary values of the metadata.",
            "content": "Conference acronym XX, June 0305, 2018, Woodstock, NY Trishanu Das, Abhilash Nandy, Khush Bajaj, and Deepiha The v dataset has total of 1, 333 images of Rebus Puzzles, 611 of which are generated using ControlNet [41] and are therefore of different artistic style. The Rebus puzzles included in the v Dataset encompass diverse range of linguistic and conceptual features. To better understand these underlying patterns, we employ ChatGPT [15] to systematically categorize the ground truth answers into distinct and meaningful classes by designing an appropriate prompting strategy. Fig. 4 shows the distribution of the Rebus Puzzles across the 18 finegrained categories predicted by ChatGPT. These categories belong to varied aspects of Language and Expression Usage, Knowledge and Facts, Culture and Society, Activities and Hobbies, and Nature and Living Things. To illustrate the diversity among these Rebus Puzzle Images, we project their pre-trained CLIP [27] (MIT License) image embeddings into 2D space using UMAP [21], as shown in Fig. 5. We color-code the image samples according to their artistic styles. Interestingly, despite sharing identical puzzle answers, the Rebus Puzzle images generated via ControlNet [41] are semantically far apart from their original counterparts. This results in highly diverse set of Rebus Puzzle images that span multiple categories. Figure 5: 2D UMAP Representations of CLIP Image representations of Rebus Puzzle Images"
        },
        {
            "title": "3.3 Proposed approach: RebusDescProgICE\nOur proposed approach RebusDescProgICE introduces a novel\nLLM-agnostic reasoning module in an in-context learning setting.\nThe reasoning to be generated contains 2 components - (1) an\nunstructured and detailed image description (2) a structured,\ncode-based reasoning component which elaborates the steps to be\nfollowed create the Rebus Puzzle Image. The unstructured and code-\nbased reasoning components provide explicit factual knowledge\nand procedural logic, which are necessary in order to solve a Rebus\nPuzzle correctly. Also, we use a novel in-context example selection\nmethod based on the similarity between the code-based reasoning\ncomponents (similar to Poesia et al. [26]).\nIn-Context Example Selection in RebusDescProgICE. In our\napproach, a VLM is guided using task-specific examples and in-\nstructions provided within the same session, without modifying its\nunderlying parameters. This setup enhances the VLM’s output by\nleveraging relevant contextual information. To ensure the selected",
            "content": "in-context examples are useful for the target task, we employ vector embedding-based similarity technique to retrieve training examples whose embeddings closely match that of the test input. Additionally, we propose novel technique (inspired by Poesia et al. [26]) to learn unified embedding that effectively represents Rebus Puzzle Image."
        },
        {
            "title": "4.2 Baselines\nWe use the following prompting strategies as prompts - (1) Zero-\nShot. This follows the naive zero-shot prompting strategy inspired\nby Gritsevskiy et al. [13] (2) Few-Shot CoT (Chain-of-Thought)\n[38]. In addition to mentioning the task instructions, this baseline\nuses In-Context Learning [6], where each in-context example con-\ntains the Rebus Puzzle image and a corresponding hint (annotated\nas metadata) as the input, and the corresponding ground truth an-\nswer as the output (3) Few-shot with Descriptions. This uses a\nsimilar prompt template as the previous baseline, where instead of\nusing an annotated hint, an image description generated using GPT-\n4o [15] in a zero-shot setting is used as the intermediate reasoning\nin the in-context examples. Note that for the last two baselines, the\nin-context example(s) for each test sample are randomly sampled\nfrom examples in the holdout set.",
            "content": "We benchmarked three closed-source modelsGPT-4o, GPT-4omini, and GPT-4 Turboand three open-source vision-language modelsPhi-3.5-Vision, Pixtral-12B, and Qwen2-VL-7B. GPT-4o stands out with its seamless support for text, audio, images, and video, achieving state-of-the-art multilingual, vision, and audio understanding while operating faster and more cost-efficiently than GPT-4 Turbo [23, 25]. The lighter GPT-4o-mini, obtained through model distillation, preserves much of GPT-4os multimodal capabilities at significantly lower cost and latency, excelling in reasoning and coding benchmarks [9, 24]. GPT-4 Turbo similarly offers strong performance in text and code tasks but with comparatively less advanced multimodal integration [19]. On the open-source side, Phi-3.5-Vision is lightweight, multimodal model adept at dense reasoning and efficient image processing, even enabling high-quality OCR and text extraction in resource-constrained environments [10, 29]. Pixtral-12B, 12-billion-parameter model, delivers strong instruction-following performance in both text and vision, outperforming larger open models in multimodal benchmarks thanks to its high-resolution vision encoder and long-context support [1]. Lastly, Qwen2-VL-7B introduces dynamic-resolution visual tokenization and multimodal rotary embeddings (M-ROPE), enhancing its ability to process variable-resolution images and fuse visual and textual informationreaching performance comparable to leading closedsource models in some benchmarks [36]. Each model thus presents distinct balance between capability, modality support, and computational accessibility, offering diverse testbed for evaluating rebus puzzle solving. v: Rebus Puzzles Understanding Benchmark Conference acronym XX, June 0305, 2018, Woodstock, NY"
        },
        {
            "title": "4.4 RebusDescProgICE vs. Baselines\nTables 1 and 2 show the substring accuracy and word-level F1 scores\nrespectively across closed and open-source models with varying\nnumber of in-context examples and prompting methods. We can\ninfer that - (1) The closed-source models (GPT-4o, GPT-4o-mini,\nGPT-4 turbo) consistently outperform open-source models (Phi-3.5,\nPixtral, Qwen2-VL-7B) across both metrics. For instance, in Table\n2 (Word-Level F1 Score), GPT-4o reaches 0.536 (three-shot, only\ndescription), while open-source models peak around 0.270 (Qwen2-\nVL-7B, three-shot, RebusDescProgICE). This highlights the supe-\nrior reasoning and alignment capabilities of closed-source VLMs,\nparticularly GPT-4 variants, which show more robust performance\nacross prompting strategies. (2) Our method RebusDescProgICE\nshows notable improvements, particularly when compared to sim-\npler prompting methods like \"only description.\" For example, in\nGPT-4o (Table 1), three-shot RebusDescProgICE achieves 0.422\nsubstring accuracy, comparable to or better than most other settings.\nSimilarly, in Table 2, GPT-4o reaches 0.512 F1, showing stable gains.\nEven for weaker open-source models like Qwen2-VL-7B, RebusDe-\nscProgICE boosts performance substantially (e.g., from 0.200 to\n0.241 in one-shot F1, and up to 0.264 in three-shot F1), suggesting\nthat the synergy of visual program + description generalizes across\nmodel families. (3) Increasing the number of in-context examples\ngenerally leads to modest but consistent improvements, especially\nin F1 scores. For example, GPT-4 turbo F1 improves from 0.382\n(zero-shot normal) to 0.442 (three-shot normal). Substring accuracy\nshows smaller gains, but still some improvements (e.g., GPT-4o\nfrom 0.420 zero-shot normal to 0.416–0.422 three-shot variants).\nHowever, gains plateau beyond two or three examples, indicating\ndiminishing returns. (4) Importance of combining VisProg and De-\nscription (our method). Comparing isolated prompting methods\nhighlights why both components are essential. VisProg alone (e.g.,\nGPT-4o three-shot VisProg: 0.383 substring acc, 0.506 F1) or only de-\nscription (GPT-4o three-shot: 0.434 substring acc, 0.536 F1) do well\nindividually, but RebusDescProgICE consistently balances both to\nachieve competitive performance (0.422 substring acc, 0.512 F1). In\nopen-source models, this effect is even clearer: for Qwen2-VL-7B,\nVisProg (0.214 substring acc, 0.248 F1) and only desc (0.111 sub-\nstring acc, 0.250 F1) underperform compared to RebusDescProgICE\n(0.107 substring acc, 0.264 F1). This demonstrates that combining\nstructured visual reasoning (VisProg) with descriptive context leads\nto more reliable gains than either alone.\nPerformance on Augmented Test Data.",
            "content": "Tables 3 and 4 display the Substring Accuracy and word-level F1 scores on Augmented Test Data with varying number of incontext examples and prompting methods. We can infer that - (1) The overall low scores across models arise from the complexity of our dataset, where solving Rebus puzzles requires layered semantic reasoning; this is further amplified by the ControlNet-augmented noisy backgrounds, resulting in best scores remaining modest (e.g., maximum F1 of only 0.402 for GPT-4o). (2) GPT-4o consistently achieves the highest performance across substring accuracy (0.280 with two in-context examples, only desc) and word-level F1 (0.402 with two in-context examples, VisProg), reaffirming the challenging nature of the dataset even for state-of-the-art closed-source models. (3) Open-source models such as Phi-3.5 and Pixtral struggle considerably, with word-level F1 mostly below 0.20, reflecting their limited capacity for abstract reasoning under noisy conditions, whereas Qwen2-VL-7B-Instruct shows relatively better resilience (F1 up to 0.253). (4) Our proposed RebusDescProgICE framework provides consistent gains over standard prompting, particularly for weaker models for instance, boosting Pixtrals F1 from 0.186 (cot, one example) to 0.201 (two examples, RebusDescProgICE). (5) Increasing the number of in-context examples does not uniformly improve performance, confirming that puzzle-solving is not driven by pattern-matching; instead, structured reasoning guidance through RebusDescProgICE is crucial for robustness. Overall, these results establish our dataset as strong benchmark for evaluating the reasoning capabilities of vision-language models under challenging, real-world-like conditions."
        },
        {
            "title": "5 Conclusion\nClosed-source VLMs remain far ahead of open-source ones in this\nchallenging rebus puzzle-solving task. Nevertheless, our proposed\nmethod, RebusDescProgICE, proves robust across settings and\nespecially beneficial for open-source models that otherwise struggle.\nWhile additional in-context examples help, the real performance\nboost comes from integrating both code-based reasoning (VisProg)\nand descriptive grounding, validating our design choice.",
            "content": "References [1] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. 2024. Pixtral 12B. arXiv preprint arXiv:2410.07073 (2024). [2] Benton Anderson and Jesse Meyer. 2022. Finding the optimal human strategy for wordle using maximum correct letter probabilities and reinforcement learning. arXiv preprint arXiv:2202.00557 (2022). [3] Giovanni Angelini, Marco Ernandes, and Marco Gori. 2005. Solving italian crosswords using the web. In Congress of the Italian Association for Artificial Intelligence. Springer, 393405. [4] Giovanni Angelini, Marco Ernandes, Tommaso Iaquinta, Caroline Stehlé, Fanny Simões, Kamyar Zeinalipour, Andrea Zugarini, and Marco Gori. 2023. The webcrow french crossword solver. In International Conference on Intelligent Technologies for Interactive Entertainment. Springer, 193209. [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 (2025). [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. Conference acronym XX, June 0305, 2018, Woodstock, NY Trishanu Das, Abhilash Nandy, Khush Bajaj, and Deepiha Table 1: Substring Accuracy across models with varying number of in-context examples and prompting methods. Model Zero One Two Three normal cot normal cot VisProg only desc RebusDescProgICE normal cot VisProg only desc RebusDescProgICE normal cot VisProg only desc RebusDescProgICE GPT-4o GPT-4o-mini GPT-4 turbo Phi-3.5 Pixtral Qwen2-VL-7B 0.420 0.213 0.279 0.169 0.107 0.342 0.449 0.428 0.410 0.114 0.096 0.161 0.416 0.223 0.346 0.075 0.102 0. 0.426 0.215 0.319 0.163 0.065 0.209 0.387 0.224 0.315 0.128 0.075 0.208 0.402 0.230 0.303 0.086 0.108 0.160 0.414 0.208 0.324 0.110 0.096 0.101 0.414 0.211 0.342 0.066 0.086 0.241 0.423 0.238 0.327 0.152 0.066 0. 0.380 0.221 0.307 0.099 0.075 0.185 0.428 0.227 0.321 0.071 0.093 0.139 0.411 0.230 0.328 0.098 0.078 0.068 0.416 0.223 0.342 0.062 0.095 0.343 0.415 0.202 0.319 0.154 0.083 0.146 0.383 0.229 0.328 0.087 0.093 0. 0.434 0.188 0.313 0.062 0.092 0.111 0.422 0.208 0.343 0.096 0.093 0.107 Table 2: Word-Level F1 Score across models with varying number of in-context examples and prompting methods Model Zero One Two Three normal cot normal cot VisProg onlydesc RebusDescProgICE normal cot VisProg only desc RebusDescProgICE normal cot VisProg only desc RebusDescProgICE GPT-4o GPT-4o-mini GPT-4 turbo Phi-3.5 Pixtral Qwen2-VL-7B 0.489 0.330 0.382 0.153 0.151 0.179 0.467 0.457 0.451 0.130 0.185 0.120 0.503 0.355 0.433 0.093 0.161 0. 0.516 0.356 0.421 0.161 0.189 0.206 0.507 0.355 0.424 0.130 0.216 0.219 0.514 0.346 0.413 0.191 0.209 0.200 0.513 0.352 0.398 0.198 0.209 0.241 0.511 0.358 0.440 0.112 0.170 0.211 0.549 0.370 0.432 0.178 0.207 0. 0.506 0.362 0.423 0.177 0.214 0.235 0.521 0.374 0.426 0.196 0.199 0.222 0.517 0.366 0.439 0.205 0.225 0.270 0.519 0.348 0.442 0.106 0.180 0.264 0.517 0.361 0.438 0.198 0.209 0.237 0.506 0.366 0.426 0.172 0.238 0. 0.536 0.360 0.422 0.196 0.213 0.250 0.512 0.352 0.431 0.177 0.239 0.264 Table 3: Substring Accuracy on Augmented Test Data across models with varying number of in-context examples and prompting methods. Model One Two Three normal cot VisProg only desc RebusDescProgICE normal cot VisProg only desc RebusDescProgICE normal cot VisProg only desc RebusDescProgICE GPT-4o Phi-3.5 Pixtral Qwen2-VL-7B-Instruct 0.257 0.060 0.065 0.276 0.264 0.118 0.074 0.282 0.268 0.093 0.065 0.123 0.254 0.053 0.074 0.116 0.268 0.058 0.062 0.097 0.262 0.079 0.079 0. 0.248 0.120 0.041 0.165 0.246 0.081 0.060 0.107 0.280 0.039 0.046 0.083 0.246 0.033 0.058 0.083 0.262 0.079 0.086 0.222 0.254 0.104 0.058 0. 0.245 0.079 0.072 0.090 0.259 0.025 0.067 0.067 0.259 0.039 0.048 0.070 Table 4: Word-Level F1 score on Augmented Test Data across models with varying number of in-context examples and prompting methods. Model One Two Three normal cot VisProg only desc RebusDescProgICE normal cot VisProg only desc RebusDescProgICE normal cot VisProg only desc RebusDescProgICE GPT-4o Phi-3.5 Pixtral Qwen2-VL-7B-Instruct 0.366 0.085 0.149 0.209 0.400 0.134 0.186 0.237 0.379 0.135 0.188 0.185 0.372 0.152 0.192 0.183 0.383 0.164 0.195 0. 0.384 0.125 0.150 0.212 0.395 0.161 0.190 0.234 0.397 0.169 0.190 0.207 0.395 0.163 0.174 0.207 0.374 0.164 0.201 0.235 0.381 0.125 0.161 0. 0.385 0.173 0.209 0.233 0.402 0.154 0.206 0.208 0.400 0.156 0.195 0.212 0.391 0.158 0.215 0.253 In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 18771901. https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf [7] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2023. Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. Transactions on Machine Learning Research (2023). https://openreview.net/forum?id=YfZ4ZPt8zd Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 14953 14962. [15] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). [16] Michael Littman, Greg Keim, and Noam Shazeer. 2002. probabilistic approach to solving crossword puzzles. Artificial Intelligence 134, 1-2 (2002), 2355. [8] Marco Ernandes, Giovanni Angelini, and Marco Gori. 2005. Webcrow: web- [17] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instrucbased system for crossword solving. In AAAI. 14121417. [9] EverydayAI. 2024. GPT-4o Mini: lighter, faster, and more affordable. Review distinguishing GPT-4o minis efficiency versus GPT-4o. [10] Dyland Freedman. 2024. Phi-3.5 Vision excels at OCR and text extraction. Reports strong OCR/text extraction, including handwriting. [11] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language models. In International Conference on Machine Learning. PMLR, 1076410799. [12] Panagiotis Giadikiaroglou, Maria Lymperaiou, Giorgos Filandrianos, and Giorgos Stamou. 2024. Puzzle Solving using Reasoning of Large Language Models: Survey. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 1157411591. [13] Andrew Gritsevskiy, Arjun Panickssery, Aaron Kirtland, Derik Kauffman, Hans Gundlach, Irina Gritsevskaya, Joe Cavanagh, Jonathan Chiang, Lydia La Roux, and Michelle Hung. 2024. REBUS: Robust Evaluation Benchmark of Understanding Symbols. arXiv preprint arXiv:2401.05604 (2024). [14] Tanmay Gupta and Aniruddha Kembhavi. 2023. Visual Programming: Compositional Visual Reasoning without Training. In Proceedings of the IEEE/CVF tion Tuning. arXiv:2304.08485 [cs.CV] [18] Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. 2022. Language Models of Code are Few-Shot Commonsense Learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 13841403. doi:10.18653/v1/2022.emnlp-main.90 [19] Time Magazine. 2024. OpenAI Announces More Powerful, Cheaper GPT-4 Turbo. Describes GPT-4 Turbo enhancements and cost reduction. [20] Raffaele Manna, Maria Pia Di Buono, and Johanna Monti. 2024. Riddle me this: Evaluating large language models in solving word-based games. In Proceedings of the 10th Workshop on Games and Natural Language Processing@ LREC-COLING 2024. 97106. [21] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Großberger. 2018. UMAP: Uniform Manifold Approximation and Projection. Journal of Open Source Software 3, 29 (2018). [22] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] [23] OpenAI. 2024. GPT-4o achieves state-of-the-art results in multilingual, audio, and vision benchmarks. Includes multilingual benchmarks like 88.7 MMLU. v: Rebus Puzzles Understanding Benchmark Conference acronym XX, June 0305, 2018, Woodstock, NY [24] OpenAI. 2024. GPT-4o mini: Advancing cost-efficient intelligence. Details GPT-4o minis reasoning, math, and multimodal performance. [25] OpenAI. 2024. GPT-4o System Card. (2024). Describes GPT-4o capabilities across text, vision, and audio. [26] Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable Code Generation from Pre-trained Language Models. In International Conference on Learning Representations. https://openreview.net/forum?id=KmtVD97J43e [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 87488763. [28] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Jian Su, Kevin Duh, and Xavier Carreras (Eds.). Association for Computational Linguistics, Austin, Texas, 23832392. doi:10.18653/v1/D16- [29] Microsoft Research. 2025. Phi-3.5-Vision: lightweight state-of-the-art open multimodal model. Describes Phi-3.5-Visions reasoning and multimodal capabilities. [30] Josh Rozner, Christopher Potts, and Kyle Mahowald. 2021. Decrypting cryptic crosswords: Semantically complex wordplay puzzles as target for nlp. Advances in Neural Information Processing Systems 34 (2021), 1140911421. [31] Abdelrahman Sadallah, Daria Kotova, and Ekaterina Kochmar. 2024. Are LLMs Good Cryptic Crossword Solvers? arXiv preprint arXiv:2403.12094 (2024). [32] Gabriele Sarti, Tommaso Caselli, Malvina Nissim, and Arianna Bisazza. 2024. Non Verbis, Sed Rebus: Large Language Models Are Weak Solvers of Italian Rebuses. In Proceedings of the 10th Italian Conference on Computational Linguistics (CLiC-it 2024), Felice DellOrletta, Alessandro Lenci, Simonetta Montemagni, and Rachele Sprugnoli (Eds.). CEUR Workshop Proceedings, Pisa, Italy, 888897. https://aclanthology.org/2024.clicit-1.96/ [33] Gemini Team. 2023. Gemini: Family of Highly Capable Multimodal Models. arXiv:2312.11805 [cs.CL] [34] Graham Todd, Tim Merino, Sam Earle, and Julian Togelius. 2024. Missed connections: Lateral thinking puzzles for large language models. In 2024 IEEE Conference on Games (CoG). IEEE, 18. [35] Eric Wallace, Nicholas Tomlin, Albert Xu, Kevin Yang, Eshaan Pathak, Matthew Ginsberg, and Dan Klein. 2022. Automated Crossword Solving. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 30733085. [36] et al. Wang. 2024. Qwen2-VL: Enhancing Vision-Language Models Perception at Any Resolution. arXiv preprint arXiv:2409.12191 (2024). Introduces dynamic resolution and M-ROPE in Qwen2-VL. [37] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. 2024. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804 (2024). [38] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 2482424837. https://proceedings.neurips.cc/paper_files/paper/2022/file/ 9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf [39] Kamyar Zeinalipour, Tommaso Iaquinta, Asya Zanollo, Giovanni Angelini, Leonardo Rigutini, Marco Maggini, and Marco Gori. 2023. Italian crossword generator: Enhancing education through interactive word puzzles. (2023). [40] Kamyar Zeinalipour, Yusuf Gökberk Keptiğ, Marco Maggini, Leonardo Rigutini, and Marco Gori. 2024. turkish educational crossword puzzle generator. In International Conference on Artificial Intelligence in Education. Springer, 226233. [41] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision. 38363847. [42] Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, and LiM3Exam: Multilingual, Multimodal, Multilevel dong Bing. 2023. Benchmark for Examining Large Language Models. In Advances in NeuInformation Processing Systems, A. Oh, T. Naumann, A. Globerson, ral K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., 54845505. https://proceedings.neurips.cc/paper_files/paper/2023/file/ 117c5c8622b0d539f74f6d1fb082a2e9-Paper-Datasets_and_Benchmarks.pdf [43] Andrea Zugarini, Thomas Röthenbacher, Kai Klede, Marco Ernandes, Bjoern Eskofier, and Dario Zanca. 2023. Die rätselrevolution: Automated german crossword solving. (2023). [44] Andrea Zugarini, Kamyar Zeinalipour, Surya Sai Kadali, Marco Maggini, Marco Gori, and Leonardo Rigutini. 2024. Clue-Instruct: Text-Based Clue Generation for Educational Crossword Puzzles. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). 33473356."
        }
    ],
    "affiliations": [
        "Tredence Inc."
    ]
}