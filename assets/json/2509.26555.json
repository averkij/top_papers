{
    "paper_title": "Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation",
    "authors": [
        "Agneet Chatterjee",
        "Rahim Entezari",
        "Maksym Zhuravinskyi",
        "Maksim Lapin",
        "Reshinth Adithyan",
        "Amit Raj",
        "Chitta Baral",
        "Yezhou Yang",
        "Varun Jampani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in video generation have enabled high-fidelity video synthesis from user provided prompts. However, existing models and benchmarks fail to capture the complexity and requirements of professional video generation. Towards that goal, we introduce Stable Cinemetrics, a structured evaluation framework that formalizes filmmaking controls into four disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera. Together, these taxonomies define 76 fine-grained control nodes grounded in industry practices. Using these taxonomies, we construct a benchmark of prompts aligned with professional use cases and develop an automated pipeline for prompt categorization and question generation, enabling independent evaluation of each control dimension. We conduct a large-scale human study spanning 10+ models and 20K videos, annotated by a pool of 80+ film professionals. Our analysis, both coarse and fine-grained reveal that even the strongest current models exhibit significant gaps, particularly in Events and Camera-related controls. To enable scalable evaluation, we train an automatic evaluator, a vision-language model aligned with expert annotations that outperforms existing zero-shot baselines. SCINE is the first approach to situate professional video generation within the landscape of video generative models, introducing taxonomies centered around cinematic controls and supporting them with structured evaluation pipelines and detailed analyses to guide future research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 5 5 5 6 2 . 9 0 5 2 : r Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation Agneet Chatterjee1,2, Max Lapin1 Chitta Baral2 Rahim Entezari1 Reshinth Adithyan1 Yezhou Yang2 Maksym Zhuravinskyi1 Amit Raj3 Varun Jampani1 1Stability AI 2Arizona State University 3Google DeepMind https://stable-cinemetrics.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in video generation have enabled high-fidelity video synthesis from user provided prompts. However, existing models and benchmarks fail to capture the complexity and requirements of professional video generation. Towards that goal, we introduce Stable Cinemetrics, structured evaluation framework that formalizes filmmaking controls into four disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera. Together, these taxonomies define 76 finegrained control nodes grounded in industry practices. Using these taxonomies, we construct benchmark of prompts aligned with professional use cases and develop an automated pipeline for prompt categorization and question generation, enabling independent evaluation of each control dimension. We conduct largescale human study spanning 10+ models and 20K videos, annotated by pool of 80+ film professionals. Our analysis, both coarse and fine-grained reveal that even the strongest current models exhibit significant gaps, particularly in Events and Camera-related controls. To enable scalable evaluation, we train an automatic evaluator, vision-language model aligned with expert annotations that outperforms existing zero-shot baselines. SCINE is the first approach to situate professional video generation within the landscape of video generative models, introducing taxonomies centered around cinematic controls and supporting them with structured evaluation pipelines and detailed analyses to guide future research."
        },
        {
            "title": "Introduction",
            "content": "The field of video generative models has made significant progress in recent years [41], drawing substantial interest from both academia and industry. This can be evidenced by the growing number of benchmarks [26, 35, 2, 38], datasets [45, 64], and both open- [3, 62, 14, 72] and closed- [6, 60, 39] source models that have collectively driven the field forward. The foundational nature of these models makes them useful for several downstream tasks, including video editing [27], 3D generation [61] and robotics [76]. This widespread adoption of video generative models underpins the growing assertion that they represent revolution for professional video generation. Generative vision offers tremendous potential for media creation, but fundamental question remains: how can we shift generative video from casual, exploratory synthesis to medium that supports professional-grade, controllable cinematic outputs? The important distinction between casual and professional generative video lies in the critical gap of cinematic control [7]: while todays models can generate videos of \"an astronaut riding horse\", professional creation necessitates granular control over cinematic elements such as the framing of the shot, position of the key light, and even whether the astronaut smiles before or after the horse gallops away - truly professional video generation system must put every one of those cinematic choices back in the creators hands. The need for this exact control over every cinematic element, from the timing of smile to the quality of Work done during an internship at Stability AI. 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Figure 1: Stable Cinemetrics introduces structured taxonomies grounded in the controls required for professional video generation. These taxonomies form the foundation of our prompt based benchmark that mirrors real-world shot creation, progressing from scriptwriting to on-screen visuals. Every control element in prompt is automatically categorized back to the taxonomy, enabling the generation of isolated evaluation questions for independent investigation into each element. This supports large scale human evaluation enabling both coarse and fine-grained insights into the capabilities of current models for professional video generation. To drive scalable annotations, we develop our own VLMs that outperform existing models in alignment with human judgements. light, is the very reason filmmakers shoot multiple takes, selecting only the frames where everything comes together to tell the story most effectively [10]. In this work, we investigate the intersection of video generative models and the nuanced control mechanisms associated with professional video production. Despite rapid advancements in video generative modeling, the field still lacks both clear definition of essential cinematic controls and standardized evaluation protocols for benchmarking progress at this crucial intersection. To bridge this gap, we present SCINE (Stable Cinemetrics) - an evaluation suite specifically designed to characterize this intersection, enabling us to directly address the question: \"Are Current Video Generative Models Ready for Professional Use?\" The core focus of SCINE is to develop taxonomy, at the intersection of generative vision and professional video generation, built on the principles of control and specificity [4, 29, 47]. These principles are important in film-making, as every decision carries cinematic meaning; low angle conveys power while low-key lighting evokes drama. SCINE captures these principles by organizing control knobs into four cinematic pillars - Setup, Events, Lighting, and Camera - enabling the evaluation of video generative models along the same axes that professional would. This disentanglement also allows evaluation around personalization: which aligns with the collaborative nature of real productions, in comparison to the monolithic nature of current video generation models. Our taxonomy is hierarchical, branching from coarse cinematic concepts to leaf-level controls that naturally map onto computer vision concepts such as object semantics and scene geometry [54]. Leveraging our taxonomy, we generate two prompt types: story-driven and visual exposition, to mirror professional workflows. Story-driven prompts act as mini-screenplays [17], specifying characters, dialogue, actions, and emotions. We enrich these with visual exposition cues, by sampling control nodes from our taxonomy, emulating the transition from script to shot in filmmaking [51]. Sampling control nodes allows automated (a) prompt categorization: mapping each control element to the taxonomy, and (b) generation of targeted evaluation questions for each element, allowing disentagled evaluation of each cinematic control. The structured nature of our taxonomy supports scalable human annotation: we evaluate 10+ models across 20K generated videos with feedback from 80+ professionals. This enables analysis across control dimensions where we observe substantial variance in performance across taxonomy pillars, even for top-performing models such as WAN-14B and Minimax. Our taxonomy facilitates both coarse insights, showing that models struggle most with 2 Events and Camera and fine-grained comparisons, such as better performance on shot size over camera framing, and on natural over artificial lighting. To support automatic evaluation of finegrained cinematic controls in generated videos, we train vision-language model (VLM) that aligns with the large-scale human annotations. Our model outperforms existing baselines, achieving an overall accuracy of 72.36% with human annotators. An overview of SCINE, outlining its contributions is shown in Figure 1."
        },
        {
            "title": "2 Related Work",
            "content": "Video Generative Models and Evaluations. Video generative models can broadly be classified into two categories based on their input conditioning: image-to-video (I2V) and text-to-video (T2V). While I2V approaches such as Stable Video Diffusion (SVD) [3] have been widely adopted by the community, the focus of our work is to evaluate T2V models for professional use. We choose text as an input modality, since it is an effective and free-form way of describing the controls defined in our taxonomy. The Sora Preview [6] served as catalyst for wave of T2V model releases across the closed [60, 11] and open-source communities. State of the art open-source models include Wan [62], HunyuanVideo [14] and StepVideo [12]. Traditional reference-based metrics such as FVD [59] are widely used in video generation, however they have been found to not align with human judgment [40] and are susceptible to content bias [18]. Several T2V evaluation benchmarks have also emerged, with VBench [26, 74] gaining broad adoption. VBench evaluates T2V models by developing text prompts that evaluate generated videos across dimensions such as temporal flickering, aesthetic quality and motion smoothness, while employing automatic metrics like CLIP-based similarity [48], LAION aesthetic predictor [33] and, frame interpolation consistency [34]. While comprehensive, VBench is sub-optimal for professional video generation, lacking fine grained controls desired by professionals. Its broad evaluation scope e.g., sampling from 8 general categories like Animal and Vehicles, does not reflect domain specific requirements in professional context. Additional T2V benchmarks include : VideoPhy [2] - which evaluates physical plausibility by measuring adherence of generated videos to real-world physics; DEVIL [35], which quantifies structural and perceptual dynamics of videos at frame and segment level, and; T2V-CompBench [53], which studies compositional consistency [24] in video generation. SCINE differs from existing benchmarks such as VBench or MovieGenBench [11] by being the first to specifically evaluate video generative models for use in professional video creation. This distinction is brought about by our taxonomy, which also introduces flexibility over the entire evaluation pipeline: managing complexity of prompts, incorporating personalization, and automating both prompt categorization and question generation. Existing benchmarks lack cinematic depth; for example, prompt such as \"A man is walking\" from VBench-2 [74] omits key details such as character appearance, setting, or camera movement; all essential to setup cinematic shot. Existing benchmarks are static, relying on fixed prompt sets that limit extensibility. In contrast, SCINEs taxonomy guided prompt generation enables future-proof evaluation, allowing prompt complexity to scale with model capabilities. Structured Video Generation and Shot-Level Control. MovieNet [25] is large-scale, annotated corpus of 1,100 movies with extensive metadata including character bounding boxes/IDs, scene boundaries, action/place tags, cinematic-style tags, and synopsis segments. Storyboard-inspired video generation has also been explored by the community: VDS [49] develops pre-visualisation system that converts story and camera textual inputs into full-length 3-D animatic storyboards; the candidate storyboards are further classified with shot ranking discriminator trained to distinguish professional film clips from randomly sampled ones. VAST [73] reformulates T2V generation as two-stage process: first generating detailed storyboards with pose and layout information from text, followed by video synthesis conditioned on both the storyboard and the text prompt. Preliminary work on multi-shot video generation includes VideoGen-of-Thought [75], which decomposes the task into four stages such as keyframe rendering and latent-space smoothing, to maintain identity and stylistic consistency across shots. MovieAgent [69] approaches the problem via hierarchical planning, where system of LLM agents employs Chain-of-Thought reasoning [66] to parse scripts into scenes, shots, camera motions, dialogue, and audio, subsequently invoking diffusion-based models for multi-shot synthesis. However, the aforementioned works do not examine the fundamental structure of shot, the nuanced controls and the interplay of cinematic elements that define it. Prior efforts often highlight isolated aspects, such as camera motion or the quality of light, while overlooking the interplay 3 (b) The Camera taxonomy defines all controls related to camera configuration during shot setup. (a) The Setup taxonomy outlines the visual components within the frame, including subjects, props, and environmental context. (c) The Lighting taxonomy specifies the illumination of shot, through light sources, their properties, and their interaction with the scene. Figure 2: Setup, Camera and Lighting Taxonomies that structure the visual elements of shot. amongst each control dimension. In contrast, SCINE formalizes structured taxonomy encompassing the full spectrum of shot-level attributes."
        },
        {
            "title": "3 Stable Cinemetrics",
            "content": "The subsequent sections detail our proposed taxonomy (Section 3.1), and its underlying design principles. Next, we develop our benchmark comprising of prompts designed for professional use (Section 3.2). Section 3.3 describes how prompt categorization and question generation are performed, enabling large-scale evaluations of video generative models for professional use. 3.1 Taxonomy Design Our taxonomies are developed in iteration with industry professionals, including organizations established by the Big Five Studios [52], independent cinematographers and screenwriters, and an Academy Award winning Visual Effects Artist. The central guiding question in our taxonomy development was: \"What controls do professionals require when setting up shot?\". Figure 3: The Events taxonomy captures the narrative dimension of shot which includes actions, emotions, and their fine grained portrayal as they evolve over time within shot. shot is the atomic unit of filmmaking; an uninterrupted sequence without cuts in which cinematic meaning emerges from the coordination of multiple cinematic choices. The average shot length (ASL) in feature films is 510 seconds [8], which closely aligns with the temporal limits of current video generation models. This duration is not limitation; it is compact canvas where rich and enough narrative and visual complexity can unfold. single shot entails numerous controls to convey intent, emotion, and story. This motivates our decision to design the taxonomy at the shot level, where finegrained control is paramount. Control is the key distinction between casual and professional video creation. In casual settings, users often accept model outputs with minimal intervention, delegating the key creative decisions to the model. In contrast, professional use demands precise, deliberate control at every stage of the generative process. In fact, pixel-level control is common in film-making [30, 68], underscoring the importance of fine-grained adjustments in achieving the desired visual effect. Professionals such as cinematographers and directors are primarily responsible for defining shots creative intent. While filmmaking is collaborative, these roles offer practical abstraction for modeling control. key insight is that, despite overlap, they are sufficiently disentangled to support distinct control dimensions: screenwriters rarely specify lighting or camera movement, and production designers are typically not associated with emotional tone or narrative pacing. These factors motivate the development of our 4 control pillars, each contributing to the composition of shot. Our taxonomies are structured as hierarchical trees, where leaf nodes correspond to the most granular control parameters, each associated with set of values. We describe each pillar below: Setup. Setup (Figure 2a) encompasses all visible elements within the frame. We organize it into three top-level groups: (1) Scene aggregates environmental controls, including Texture, Geometry, and Set Design. Texture covers aspects such as contrast and color palette, which govern the surface feel of the shot. Geometry captures dominant shapes and the spatial arrangement of elements within the scene. Set Design comprises Props and their attributes; the Backdrop, which establishes the macro context of the set; and Environment, which defines micro-level elements contributing to the \"feel\" of the shot. (2) Subjects refer to the focal characters within shot, defined by attributes such as costumes and accessories. (3) Text Generation refers to on-screen typography such as titles or lettering, designed to appear as integrated graphical elements. Each node in Setup has cinematic meaning: dawn (Time of Day) setup combined with mist (Elements) can suggest danger, while clean (Organization), symmetrical hallway (Balance) conveys order. Lighting. Lighting is the key to turning amateur footage into professional stories and presentation - Jay Holben [22]. Motivated by this principle, we define the following groups for Lighting (Figure 2b): (1) Source, the origin of illumination within the shot; (2) Color Temperature, which controls the warmth of the light; (3) Lighting Conditions, preset configurations describing scene-wide illumination; (4) Effects, visual outcomes resulting from light interacting with the scene; (5) Position, the spatial relation of the light source to the subject; and (6) Advanced Controls such as flickering modulation and the use of color gels to adjust lighting hue. Each control knob corresponds to distinct cinematic expressions: shot with only back light (Position) evokes mystery, while hard shadows are often used to amplify tension. Camera. The camera taxonomy (Figure 2c) encompasses all camera-related control dimensions involved in shot. We organize these into 4 high-level groups: (1) Intrinsics: optical and exposure parameters governing the light captured by the camera; (2) Extrinsics: position and orientation of the camera relative to the subject; (3) Trajectory: motion of the camera and the supporting gear that enables it; and (4) Creative Intent: compositional choices that shape the narrative or emotional tone of shot. Prior works have primarily focused on camera motion control [20, 23]; however, we show that much broader range of camera parameters can be independently manipulated while setting up shot. Each parameter has tangible cinematic impact; for example, shallow depth of field can isolate the subject from the background to direct emotional focus while, an insert framing spotlights narrative details with precision. Events. Events (Figure 3) encodes the narrative substance of shot, namely the depicted actions, emotions, and dialogues - which are further decomposed into dependent nodes for fine-grained control. These dependent nodes represent attributes that cannot exist independently of their parent categories; for instance, these nodes can specify the type of interaction or the delivery mode of dialogue. Emotions may appear explicitly (visible tears) or implicitly (a clenched jaw), while actions can be stand-alone or interactive. The Portrayed As category captures aspects such as: Temporal, which refers to the unfolding pattern of the event (e.g., laughter erupting simultaneously vs. sequentially), and Contextual, which indicates whether the event occurs in the foreground or background. Advanced Controls refines pacing and the story structure of the shot, such as turning point or climax. While recent works [65, 16] evaluate T2V models on sequential event generation, we show that, from professional standpoint, Events encompass much broader and richer evaluative space. The taxonomies define total of 76 leaf-level controls that can be independently adjusted when crafting shot. We structure our taxonomies as hierarchical trees to enable disentanglement and multi-level abstraction of cinematic controls. Attributes within each branch are highly correlated, while branches remain independent, ensuring, for example, adjusting Depth of Field does not affect Camera Movement. The tree structure naturally supports multi-level abstraction, aligning with 5 Table 1: Structured prompt upsampling with SCINE taxonomies. We show how control nodes from our taxonomies enable the generation of visually expressive (SCINE Visuals) prompts from narrative scripts (SCINE Scripts). The table also demonstrates how single script can yield multiple visual interpretations, enabled by our taxonomy guided prompt generation pipeline. This aligns with filmmaking principles, where script can be visually realized in diverse ways depending on the creative choices made by the filmmakers. Baseline script: man serves dinner to his family. of Field Movement Taxonomy Branch Depth (Camera) Camera (Camera) Lighting (Lighting) Backdrop / Time of Day (Setup) Props (Setup) Upsampled prompt: man serves dinner to his family with shallow depth of field on static tripod with gentle dolly-in, under warm tungsten interior lighting in the evening, in cozy earth-tone kitchen with wooden utensils. Baseline choice narrative impact Shallow isolates food/serving hand, romantic warmth Static tripod + gentle dolly-in calm focus on gesture; subtle emphasis Warm tungsten practicals cozy, inviting domestic glow Evening interior nostalgic comfort, winding-down mood Earth-tone wooden utensils homely Alternative choice narrative impact Deep every family member equally sharp, ensemble clarity Handheld tracking urgency, energetic family chaos Cool morning daylight through windows brisk freshness and emotional distance Bright morning interior optimism and upbeat tempo Silver cutlery formal, upscale Source how filmmakers conceptualize scenes, starting from high-level intent and refining toward specific implementation. For example, directive such as set tense alley at night can be decomposed into an EXT Setting, cool Color Temperature and Deep Depth of Field. This structure also allows easier scalability; adding new detail like floating ember sparks, fits cleanly under (Environment Elements) without disrupting the rest of the taxonomy. An alternate structure such as flat, linear list would not support such extensibility. Developing the taxonomy is non-trivial task. This required multiple iterations with experts since professionals interpret and prioritize controls differently. Furthermore, shot creation is multi-stage process, starting from script-writing to setup design to camera blocking, making unification under single structured framework challenging. While some taxonomies [44] focus only on filmmaking aspects, they lack structure and are not aligned with generative modeling. Our goal instead was to impose structure, and develope taxonomy that bridges professional filmmaking and generative video models. Details of each control node and their corresponding values are provided in Appendix A.1. 3.2 Designing Prompts for Professional Use The taxonomies form the foundation for constructing prompts tailored to professional use. Our core approach in creating prompts involves sampling values from the control nodes and creating prompts that reflect realistic cinematic intent. Mirroring the filmmaking process, we first generate narrative scripts and then inject visual elements into these scripts : Scripts. These prompts, referred to as SCINEScripts, contain the narrative content of individual shots. We collaborate with professional screenwriter to create seed prompts that meet strict constraints: single shot, under 10 seconds, with no reliance on off-screen elements. These seed prompts, along with sampled nodes from the Events taxonomy are provided as input to LLM, for prompt generation. We use the Events taxonomy for these prompts because it directly encodes narrative beats i.e., what happens in shot. It covers nodes such as physical dynamics (actions) and verbal interactions (dialogues), which are crucial to the story conveyed in shot. To ensure prompt diversity, we vary parameters across multiple LLM invocations, sampling emotions from Plutchiks model [46], alternating actions, dialogue structures, genres, and subject Figure 4: t-SNE visualization showing substantial overlap between ground truth screenplays and prompts in SCINE Scripts, in comparison to existing prompt based benchmarks such as VBench-2.0 6 Figure 5: 1. Prompt Generation Pipeline SCINE Scripts are created by passing seed prompts and sampled Events taxonomy nodes to an LLM, forming the narrative component of our benchmark. SCINE Visuals are then generated through structured upsampling, where nodes from the Camera, Lighting, and Setup taxonomies are sampled and injected into each SCINE Script to create prompts that capture visual exposition. 2. Automatic Categorization and Question Generation Given SCINE prompt and taxonomy, we categorize each taxonomy element present in the prompt and generate corresponding question to enable isolated evaluation of each control node. composition. We use LLMs, as prior work [43, 55] have shown their effectiveness in screenplay generation. t-SNE visualization (Figure 4) of SCINE-Scripts embeddings [63] shows substantial overlap with ground-truth screenplays [50], whereas prompts from VBench-2.0 [74] and MovieGenBench [11] exhibit minimal overlap. Visual Exposition. We refer to these set of prompts as SCINE-Visuals, which enrich SCINE-Scripts with visual elements from the Camera, Lighting, and Setup taxonomies. In contrast to Events, these taxonomies offer fine-grained control over the visual style and composition of shot. For each base prompt in SCINE-Scripts, we sample values from one or more control nodes and inject them to expand the prompt with structured visual specifications. SCINE-Visuals highlight key advantage of our taxonomy: structured prompt upsampling. Unlike existing prompt upsampling techniques that delegate all creative decisions to the LLM, our method constrains generation within the taxonomy, enabling more controlled and interpretable prompt expansion. Table 1 provides breakdown of how SCINE-Scripts is subsequently upsampled via the taxonomy to generate SCINE-Visuals. 3.3 Category and Question Generation Next, we extract categories and generate questions for each prompt. Categories link prompts back to the taxonomy, allowing fine-grained evaluations across different levels of abstraction; single prompt can map to multiple categories across different taxonomies. For each category, we generate targeted questions that are shown to human annotators during video evaluations. These questions are specific in nature and target only single control node, enabling its isolated evaluation. Unlike high-level prompt adherence questions, which lack fine-grained attribution, our framework supports per-control annotations. minimal example is shown below : Prompt: tight close-up focuses on fireplace, its embers flickering brightly. Category : Lighting Advanced Controls Motion Question : Does the scene exhibit dynamic flickering effects in its lighting that align with the description? Category : Camera Creative Intent Shot Size Question : Does the video include tight close-up shot that captures the detailed framing? The overall evaluation pipeline, depicting prompt generation, categorization and question generation, is presented in Figure 5. Additional details are presented in Appendix A.3. 7 (a) Overall results on SCINE Scripts, across genres. Minimax and Wan 14B emerge as the strongest models. The strongest genre is Biography while all models consistently struggle at Comedy. (b) Overall results on SCINE Visuals across four pillars of professional control. Events emerges as the most challenging category across models, while Setup yields the strongest performance. Figure 6: Overall results on SCINE Scripts and Visuals."
        },
        {
            "title": "4 Are current Video Generative Models Ready for Professional Use?",
            "content": "We now evaluate state of the art text-to-video (T2V) models against the professional standards defined in our taxonomy pillars. Our analysis reveals both strengths and persistent challenges of current models, offering an overview of how current models align with professional quality expectations. 4.1 Experimental Setup Prompts. The SCINE benchmark comprises two prompt categories, Scripts and Visuals, each aligned with distinct professional roles  (Table 2)  . The Visuals prompts are created by systematically upsampling Scripts using our taxonomies leading to total of 2,089 prompts. We categorize prompts by difficulty: we create basic prompts by limiting the number of sampled control nodes, while in advanced prompts, we do not impose any restriction. Models. We evaluate 13 state-of-the-art T2V models, both open-source (WAN 1B/14B [62], HunyuanVideo [14], Step Video [12], CogVideoX 5B [72], LTX-Video [19], Pyramid Flow [28], Easy Animate 5.1 [70], Mochi [57]), VChitect-2.0 [15] and closed source (Minimax [42], Luma Ray 2 [31], Pika 2.2 [32]). Our goal is to assess each models suitability for role-specific professional tasks, like evaluating narrative fidelity from screenwriters perspective. Unless otherwise noted, we use default sampling parameters and maintain consistent seed per prompt, across models for fair comparison. Events Target Role Screenwriters SCINE-Scripts # of Prompts Target Taxonomies Avg. Questions per Prompt Table 2: The SCINE benchmark includes prompts tailored to professional roles, where each prompt is paired with multiple, fine grained evaluation questions. Human Annotation Setup. To ensure high-quality evaluation, we work with pool of 84 expert annotators with an average of 6.5 years of experience in film production, across roles such as cinematographers, film editors, screenwriters, visual communication designers, and directors. Annotators were shown prompt along with two generated video samples. For each prompt, they were presented with taxonomy derived evaluation categories and corresponding questions. Each video was rated independently on 15 scale, where 1 indicated complete misalignment with the category and 5 indicated perfect match. Although the evaluation was non-comparative, our UX ablation studies showed that displaying two videos side by side improved annotator calibration, especially when selecting middle range scores. To promote consistency and reduce subjectivity, we developed comprehensive annotation guide covering each control node in the taxonomy. We collect 3 votes for every video-question pair across 13,457 unique questions, collecting total of 248,536 pairwise annotations. We observe an intra-class correlation coefficient (ICC) [56] of 80.4% for the 1-5 ratings at the model-pair level, and 95.5% when the models are considered individually. Further, we conduct Wilcoxon signed-rank tests [67] across 45 model pairs where we observe statistically significant preferences in 37 of them, which highlights that the annotators agree on model preferences. Cinematographers Production Designers Directors Camera, Lighting Setup All 5.42 4.47 4.00 3.35 10.48 5.07 SCINE-Visuals 355 298 2.57 0.98 1133 8 Figure 8: Model performance on Events across temporal portryal of Actions. Atomic actions are handled well, whereas models struggle with causal and overlapping Events. 4.2 Results and Analysis the story diSCINE Scripts We first evaluate models on narrative event generation, i.e. mension of shot. Figure 6a compares model performance across different genres, focusing on the accuracy and coherence of generated events. Minimax and WAN-14B emerge as the overall top performers, while LTX-Video consistently underperforms. We observe that models generally perform better on the Biography genre, whereas Comedy proves challengIn Figure 7, we zoom into sub-categories within Events Types. Minimax leads in ing. nearly all categories, showing the largest margins in Dialogues and Change in Environment, but falls short in Advanced Controls, where WAN-14B outperforms. Further, models are better at stand-alone actions compared to interactive actions and portray implicit emotions better than explicit ones. Within Event Types, Actions exhibit lowest variance across models. Figure 8 shows Events performance across 13 models and 6 Temporal portrayal of Actions. Models handle atomic and concurrent actions well, but struggle with causal, overlapping, and cyclic events. Performance on Causal and Sequential events is highly correlated (ρ = 0.94), as is performance on Concurrent and Overlapping events (ρ = 0.86) across models. Despite variation across models, all show limitations in multiple Event aspects, highlighting opportunities for improvement. Figure 7: Fine-grained evaluation on Events. Models handle environmental changes well but struggle with dialogues and shot pacing. Standalone actions outperform interactive, and implicit emotions are easier than explicit. SCINE Visuals Figure 6b demonstrates overall results on SCINE Visuals. Consistent with the pair-wise rankings in Figure 1, WAN-14B and Minimax emerge as top performers across all pillars. We find that current models struggle most with Events and Camera, while elements of Setup and Lighting are comparatively easier to capture. Only the top three modelsWAN-14B, WAN-1B, and Minimax - reliably depict Events, with substantial performance gap from the rest. While Camera scores are low across the board, the narrow spread suggests that all models face similar limitations. Lighting shows the most consistent performance, with most models achieving relatively high scores whereas Setup yields the highest absolute scores for the top-performing models. 9 Figure 10: Model performances across Lighting Source. Strobes and Sunlight emerge strongest, whereas HMI and Fluorescent are points of weaknesses. Figure 11: Model performances across Camera Angles. The Dutch angle poses common challenge to all current video generative models Cinematographer. We evaluate this role by creating prompts that inject control nodes from the Camera and Lighting taxonomy. Within Camera, Extrinsics and Trajectory have the lowest average performance and the narrowest intermodel spread. For Lighting, the primary bottleneck is Lighting Position. We further present results, split by prompt difficulty in Figure 9. Across all models, performance degrades on advanced prompts, indicating that under conditions resembling professional workflows where cinematographers have large amount of control, current models struggle. The biggest performance drops occur in Lighting Source, Color Temperature, and Creative Intent. Lighting Position and Advanced Controls show the smallest performance drops, but remain the weakest categories overall, Figure 9: Split results on basic vs. advanced prompts for Camera and Lighting. All models show performance drops on advanced prompts, with the largest decline in Lighting Source. 10 Figure 12: Model performances across Camera Shot Size. Models perform well on Master and Establishing shots and struggle at medium-wide and extreme-close-up shots. (a) Fine-grained results on Subjects. Models perform well on hair and accessories, but struggle with personality and makeup. (b) Fine-grained results on Set Design. Models perform better at Backdrop in comparison to Environment, and struggle most with styling the shot appropriately. Figure 13: Fine-grained results on Setup across Subjects and Set Design. highlighting persistent challenges regardless of prompt complexity. Hunyuan and WAN-1B exhibit consistent performance across complexity levels. We also probe performance at the value level. On Lighting Source (Figure 10), Sunlight, Strobes, and Firelight are handled more reliably, while HMI, Fluorescent, and Tungsten lighting show lower performance. As shown in Figure 11, Aerial and Knee level camera angles are depicted better, while the Dutch and Shoulder-level angles show lower performance. On Shot Sizes (Figure 12), Medium-Wide and Master shots have stronger performance in comparison to Full and Extreme Close-Up shots. Production Designer. Models perform strongest on the Setup pillar; within Setup, models achieve comparable performance on Subject and Scene generation, but show drop in Text Generation. In Subjects (Figure 13a), model performances largely vary, with the highest scores for Hair and Accessories, and the weakest in Personality and Make-up. In Set Design (Figure 13b), performance trends follow the order: Backdrop >Props >Environment. Within Props, models perform well in Material but struggle at generating intricate Patterns. For Environment, current limitations lie in adhering to coherent Style and Backgrounds, whereas better performance is seen in organizing Space within frame. Director. Prompts targeting this role differ from prior categories as they evaluate models across all taxonomies simultaneously. Model performance declines on average when all controls are defined jointly (Figure 14). The largest performance drop is observed in Camera, followed by Setup and Lighting. Wan 14B is the only model to show improved performance on Director prompts for Lighting and Setup, compared to its Standalone results. 11 Figure 14: Director results: Joint specification of all controls, mirroring real-world shot creation leads to performance drop on all models, compared to evaluation in standalone manner. Our evaluation identifies three-tier hierarchy among current T2V models: Minimax and WAN-14B at the top, followed by Luma Ray 2, Hunyuan, and WAN-1B, with the remaining models forming the third tier. While overall performance varies, most models struggle with the fine-grained elements critical to professional video generation. For example, atomic events are handled reasonably well, but models falter on concurrent and causal events, which demand deeper temporal reasoning. Similarly, high-level cues like lighting conditions are better captured than nuanced aspects like precise light positioning. In summary, even top performing models exhibit substantial room for improvement across all dimensions of our taxonomy. No model achieves consistently strong performance across all aspects of shot composition, underscoring the challenge of aligning generative video models with professional standards. Additional results and analysis are presented in Appendix A.2."
        },
        {
            "title": "5 Scalable Evaluation of Professional Videos",
            "content": "In the previous section, we evaluated video generative models using expert annotations across 76 control nodes defined by our taxonomy. While human evaluation remains the gold standard, it is costly and difficult to scale, and defining reliable automatic metrics for each control node is non-trivial. Recent advances in vision-language models (VLMs) [13, 36, 1] offer scalable alternative, showing strong performance in video understanding tasks. In this section, we leverage these models to perform automatic evaluation of professional video generation. Zero-shot VLM Evaluations. The rise of multimodal VLMs has enabled progress on vision-language tasks, including video understanding, making them natural candidates for evaluating professional video generation. We use expert annotations as ground truth and measure VLM alignment by prompting models with video, its associated prompt, and specific question tied to taxonomy node, asking for 15 rating similar to our user study. We explicitly instruct the VLM to ignore factors unrelated to the specified category when evaluating the video. We determine VLM preferences by independently scoring each video and selecting the higher-scoring one. This design mitigates hallucination and order-sensitivity issues commonly observed when prompting with both videos simultaneously. We use Qwen2.5-VL-Instruct models due to their strong video understanding capabilities. To study the effect of model scale, we evaluate 3 sizes: 7, 32 and 72B. Figure 15 shows that increasing model size does not yield significant improvements in alignment with human judgment. Our results reveal overall poor agreement, consistent with prior findings [21, 71] that highlight the need for finetuning on in-distribution, human-labeled data. Aligning Human and VLM ratings. We adopt Qwen-2.5-VL-7B [58] as the base model for finetuning; our training and validation dataset consist of 44,062 and 12,763 samples, respectively. We aggregate annotator scores (3/video pair) into binary preferences, excluding ties. Videos are preprocessed at 2 fps at their native resolution. Each sample consists of prompt, two videos, and binary label. The trained model acts as classifier; we Figure 15: Our trained VLM shows consistent alignment with human annotations across video generation models, outperforming baselines, most notably on WAN-14B. 12 modify the model architecture to output scalar score using linear projection over the final layers last token. The model takes as input single video, its prompt, and the evaluation question as input. For training, we use the BradleyTerry objective [5] due to its sample efficiency over regression [37]. The model is trained for 1 epoch with batch size of 8 and learning rate 6e-5. Similar to zero-shot evaluation, we compute pairwise preference accuracy against the average of the annotators as the target metric. Our fine-tuned model achieves an overall accuracy of 72.36%, outperforming all zero-shot VLM baselines. This represents an absolute improvement of 20%, over the baseline 7B model. Our model (Figure 15) shows consistent performance across videos generated by different models, highlighting its ability to generalize across different video qualities. Additional VLM results are presented in Appendix A.7."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "Stable Cinemetrics probes at the intersection of professional video generation and generative video models, grounding prompts, evaluations, and analysis in our structured taxonomies. Our findings reveal where current state-of-the-art models perform well, and where substantial improvements are needed. Our prompt suite offers strong testbed for future video generative models and can be easily extended as models improve, owing to the flexibility of our taxonomy. To support future evaluations, we introduce trained VLM aligned with expert annotations, enabling scalable annotation as new models are released. We envision several extensions of our work; while our current focus is on evaluation, the taxonomy can also support analyzing video datasets for cinematic diversity or serve as structure for video captioning. While todays text-to-video models are not yet usable in fully zero-shot capacity, our findings identify the main challenging pillars for professional filmmaking, illuminating the need for potential solutions like fine-tuning and customization that can bring these models closer to real production use. We hope SCINE encourages deeper exploration at the intersection of filmmaking and video generative models, fostering closer collaboration between artists and models."
        },
        {
            "title": "7 Acknowledgement",
            "content": "We thank Robert Legato, Hanno Basse and Heather Ferreira for their valuable input on our work. We are also grateful to the team at MovieLabs for their feedback on our taxonomies. special thanks to Cedric Wagrez for his invaluable assistance with the human annotations!"
        },
        {
            "title": "8 Limitations",
            "content": "Although our taxonomy was developed in consultation with domain experts, it is limited by the scope of our collaborator network. Filmmaking terminology and interpretive nuance vary across regions and cultures, greater expert diversity would enable broader incorporation of global cinematic controls into the taxonomy. Some taxonomy nodes (e.g., Color Temperature, ISO) were abstracted for evaluation, as we found it difficult for annotators to consistently perceive fine-grained values (such as 2000K or ISO 800). Prompt generation is based on LLMs, whose proprietary nature and potential biases can influence the language and structure of the prompts. Our zero-shot VLM evaluations were bounded by compute and data resources, limiting the scale and scope of the experiments."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. ArXiv, abs/2502.13923, 2025. URL https://api.semanticscholar.org/CorpusID:276449796. [2] Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=9D2QvO1uWj. 13 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. URL https://arxiv.org/abs/2311.15127. [4] Bruce Block. The visual story: Creating the visual structure of film, TV, and digital media. Routledge, 2020. [5] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. [7] Blain Brown. Cinematography: theory and practice: image making for cinematographers and directors. Routledge, 2016. [8] James Cutting and Ayse Candan. Shot durations, shot classes, and the increased pace of popular movies, 2015. [9] Google DeepMind. Veo 3 tech report. Technical report, DeepMind, 2025. Veo: text-to-video generation system. Available at https://storage.googleapis.com/deepmind-media/veo/ Veo-3-Tech-Report.pdf. [10] Edward Dmytryk, Andrew Lund, and Mick Hurbis-Cherrier. On film editing: an introduction to the art of film construction. Routledge, 2018. [11] Adam Polyak et al. Movie gen: cast of media foundation models, 2025. URL https: //arxiv.org/abs/2410.13720. [12] Guoqing Ma et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model, 2025. URL https://arxiv.org/abs/2502.10248. [13] OpenAI Josh Achiam et al. Gpt-4 technical report. 2023. URL https://api. semanticscholar.org/CorpusID:257532815. [14] Weijie Kong et al. Hunyuanvideo: systematic framework for large video generative models, 2025. URL https://arxiv.org/abs/2412.03603. [15] Weichen Fan, Chenyang Si, Junhao Song, Zhenyu Yang, Yinan He, Long Zhuo, Ziqi Huang, Ziyue Dong, Jingwen He, Dongwei Pan, Yi Wang, Yuming Jiang, Yaohui Wang, Peng Gao, Xinyuan Chen, Hengjie Li, Dahua Lin, Yu Qiao, and Ziwei Liu. Vchitect-2.0: Parallel transformer for scaling up video diffusion models, 2025. URL https://arxiv.org/abs/2501. 08453. [16] Weixi Feng, Jiachen Li, Michael Saxon, Tsu jui Fu, Wenhu Chen, and William Yang Wang. Tcbench: Benchmarking temporal compositionality in text-to-video and image-to-video generation, 2024. URL https://arxiv.org/abs/2406.08656. [17] Syd Field. Screenplay: The foundations of screenwriting. Delta, 2005. [18] Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, Jun-Yan Zhu, and Jia-Bin Huang. On the content bias in fréchet video distance, 2024. URL https://arxiv.org/abs/2404.12391. [19] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion, 2024. URL https://arxiv.org/abs/2501.00103. [20] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for video diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=Z4evOUYrk7. 14 [21] Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, et al. Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation. arXiv preprint arXiv:2406.15252, 2024. [22] Jay Holben. Shot in the Dark: Creative DIY Guide to Digital Video Lighting on (almost) No Budget. Course Technology Press, 2011. [23] Chen Hou and Zhibo Chen. Training-free camera control for video generation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=KI1zldOFz9. [24] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2icompbench++: An enhanced and comprehensive benchmark for compositional text-to-image generation, 2025. URL https://arxiv.org/abs/2307.06350. [25] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: holistic dataset for movie understanding. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IV 16, pages 709727. Springer, 2020. [26] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2180721818, June 2024. [27] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing, 2025. URL https://arxiv.org/abs/2503.07598. [28] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling, 2025. URL https://arxiv.org/abs/2410.05954. [29] Steven Douglas Katz. Film directing shot by shot: visualizing from concept to screen. Gulf Professional Publishing, 1991. [30] MediaBee Color Lab. ema, the-art-and-impact-of-color-grading-in-cinema. Blog post. 2024. July The art URL impact cincolor and https://www.mediabeecolorlab.com/post/ grading of in [31] Luma Labs. Ray2, 2025. URL https://lumalabs.ai/ray. Accessed: 2025-05-02. [32] Pika Labs. Pika 2.2, 2025. URL https://pikalabs.org/pika-2-2/. Accessed: 2025-05-02. [33] LAION-AI. Aesthetic predictor, 2022. URL https://github.com/LAION-AI/ aesthetic-predictor. Accessed: 2025-05-06. [34] Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, Chun-Le Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation, 2023. URL https: //arxiv.org/abs/2304.09790. [35] Mingxiang Liao, Hannan Lu, Xinyu Zhang, Fang Wan, Tianyu Wang, Yuzhong Zhao, Evaluation of text-to-video genWangmeng Zuo, Qixiang Ye, and Jingdong Wang. eration models: dynamics perspective. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 109790109816. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ c6483c8a68083af3383f91ee0dc6db95-Paper-Conference.pdf. [36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv, abs/2304.08485, 2023. URL https://api.semanticscholar.org/CorpusID:258179774. [37] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. 15 [38] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2213922149, June 2024. [39] Luma Labs. Luma ray2 video model. https://lumalabs.ai/ray, 2025. Accessed: 2025-0423. [40] Ge Ya Luo, Gian Mario Favero, ZhiHao Luo, Alexia Jolicoeur-Martineau, and Christopher Pal. Beyond FVD: An enhanced evaluation metrics for video generation distribution quality. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=cC3LxGZasH. [41] Nestor Maslej, Loredana Fattorini, Raymond Perrault, Yolanda Gil, Vanessa Parli, Njenga Kariuki, Emily Capstick, Anka Reuel, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Juan Carlos Niebles, Yoav Shoham, Russell Wald, Tobi Walsh, Armin Hamrah, Lapo Santarlasci, Julia Betts Lotufo, Alexandra Rome, Andrew Shi, and Sukrut Oak. Artificial intelligence index report 2025, 2025. URL https://arxiv.org/abs/2504. 07139. [42] MiniMax. Video generation api, 2024. URL https://www.minimax.io/news/ video-generation-api. Accessed: 2025-05-02. [43] Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, and Richard Evans. Co-writing screenplays and theatre scripts with language models: An evaluation by industry professionals, 2022. URL https://arxiv.org/abs/2209.14958. [44] Motion Picture Laboratories. Ontology for media creation: Part 3f: Images. Technical Report v2.6, MovieLabs, May 2024. URL https://mc.movielabs.com/docs/ontology/ assets-images/images/. Accessed: 2025-05-15. [45] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=j7kdXSrISM. [46] Robert Plutchik. The emotions. University Press of America, 1991. [47] Michael Rabiger. Directing: Film techniques and aesthetics. Routledge, 2013. [48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. [49] Anyi Rao, Xuekun Jiang, Yuwei Guo, Linning Xu, Lei Yang, Libiao Jin, Dahua Lin, and Bo Dai. Dynamic storyboard generation in an engine-based virtual environment for video production. In ACM SIGGRAPH 2023 Posters, pages 12. 2023. [50] Rohit Saxena and Frank Keller. Moviesum: An abstractive summarization dataset for movie screenplays, 2024. URL https://arxiv.org/abs/2408.06281. [51] Linda Seger and Edward Jay Whetmore. From Script to Screen: The collaborative art of filmmaking. Lone Eagle Publishing Company, LLC, 2004. [52] StudioBinder. What are the major film studios?, n.d. URL https://www.studiobinder.com/ blog/what-are-the-major-film-studios/. Accessed: 2025-05-14. [53] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2vcompbench: comprehensive benchmark for compositional text-to-video generation, 2025. URL https://arxiv.org/abs/2407.14505. [54] Richard Szeliski. Computer vision: algorithms and applications. Springer Nature, 2022. 16 [55] Yuying Tang, Haotian Li, Minghe Lan, Xiaojuan Ma, and Huamin Qu. Understanding screenwriters practices, attitudes, and future expectations in human-ai co-creation, 2025. URL https://arxiv.org/abs/2502.16153. [56] Judith M. Tanur, George Casella, Richard Dykstra, Mark P. Finster, Donald P. Gaver, Joel Greenhouse, Gudmund R. Iversen, guillermina Jasso, Jan Kmenta, S. James Press, Seymour Sudman, Luke Tierney, Jessica Utts, Katherine K. Wallman, Stanley Wasserman, and Mark Watson. Journal of the American Statistical Association, 84(407):830834, 1989. ISSN 01621459, 1537274X. URL http://www.jstor.org/stable/2289675. [57] Genmo Team. Mochi 1. https://github.com/genmoai/models, 2024. [58] Qwen Team. Qwen2.5-vl, January 2025. URL https://qwenlm.github.io/blog/qwen2. 5-vl/. [59] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges, 2019. URL https://arxiv.org/abs/1812.01717. [60] Veo-Team. Veo 2. 2024. URL https://deepmind.google/technologies/veo/veo-2/. [61] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion, 2024. URL https: //arxiv.org/abs/2403.12008. [62] Team Wan. Wan: Open and advanced large-scale video generative models, 2025. URL https://arxiv.org/abs/2503.20314. [63] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Multilingual e5 text embeddings: technical report, 2024. URL https://arxiv.org/abs/ 2402.05672. [64] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation, 2024. URL https://openreview.net/forum?id=dUDwK38MVC. [65] Yiping Wang, Xuehai He, Kuan Wang, Luyao Ma, Jianwei Yang, Shuohang Wang, SiIs your world simulator good story presenter? mon Shaolei Du, and Yelong Shen. consecutive events-based benchmark for future long video generation, 2024. URL https: //arxiv.org/abs/2412.16211. [66] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. [67] Frank Wilcoxon. Individual comparisons by ranking methods. In Breakthroughs in statistics: Methodology and distribution, pages 196202. Springer, 1992. [68] Aylish Wood. Pixel visions: Digital intermediates and micromanipulations of the image. Film Criticism, 32(1):7294, 2007. [69] Weijia Wu, Zeyu Zhu, and Mike Zheng Shou. Automated movie generation via multi-agent cot planning, 2025. URL https://arxiv.org/abs/2503.07314. [70] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. Easyanimate: high-performance long video generation method based on transformer architecture, 2024. URL https://arxiv.org/abs/2405.18991. [71] Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. 17 [72] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan.Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=LQzN6TRFg9. [73] Chi Zhang, Yuanzhi Liang, Xi Qiu, Fangqiu Yi, and Xuelong Li. Vast 1.0: unified framework for controllable and consistent video generation, 2024. URL https://arxiv.org/abs/2412. 16677. [74] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, and Ziwei Liu. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness, 2025. URL https://arxiv.org/abs/2503.21755. [75] Mingzhe Zheng, Yongqi Xu, Haojian Huang, Xuran Ma, Yexin Liu, Wenjie Shu, Yatian Pang, Feilong Tang, Qifeng Chen, Harry Yang, and Ser-Nam Lim. Videogen-of-thought: Step-by-step generating multi-shot video with minimal manual intervention, 2025. URL https://arxiv.org/abs/2412.02259. [76] Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, and Chuang Gan. RoboDreamer: Learning compositional world models for robot imagination. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 6188561896. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/zhou24f.html."
        },
        {
            "title": "A Appendix",
            "content": "Taxonomy Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Additional Results and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 Details on Prompt Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 Distribution of Taxonomy Categories in SCINE Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 Annotation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 Statistical Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 Additional VLM Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 Additional Results on Recent Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 A.1 Taxonomy Details We provide additional details on control nodes and their associated values in the taxonomies. Some nodes accept open-ended values, for example, range of stand-alone actions. To simplify evaluation, we abstract certain values that can be fine-grained in future works. For instance, we group Aperture into wide/medium/narrow, though exact f-stop values can also be studied in the future. Similarly, color palette is treated as discrete value in our current work, but can be decomposed into hue, brightness, and saturation. Table 3 - 6 details the control nodes and their values of the Camera, Lighting, Setup and Events Taxonomies, respectively. Description Defines the focal length and field of view of the camera lens. Table 3: Camera Taxonomy Control Nodes and Values Potential Values Standard, Fisheye, Wide, Medium, Long Lens, Telephoto Deep, Shallow, Soft, Rack, Split Diopter, Tilt Shift Wide, Medium, Narrow Depth of Field Controls the range of focus in the image, affecting subject Slow, Medium, Fast Low, Medium, High Low, High, Aerial, Overhead, Dutch, Eyelevel, Shoulder, Hip, Knee, Ground, Continuous Values - Pan left, Pan right, Tilt up, Tilt down, Zoom in, Zoom out Push In, Pull Out, Dolly Zoom, Camera Roll, Tracking, Trucking, Arc, Crane Handheld, Tripod, Pedestal, Cranes, Overhead Rigs, Dolly, Stabilizer, Snorricam, Vehicle Mount, Drones, Motion Control, Steadicam Establishing, Master, Wide, Full, MediumFull, Medium, MediumCloseup, Close-up, Extreme Close-up Single, Two Shot, Crowd, OTS, PoV, Insert Name Lens Size Aperture Shutter Speed ISO Angle Static 2D 3D Gear isolation. The camera lens opening that controls the amount of light propagated through the camera. The duration for which the camera sensor is exposed to light. Sensitivity of the camera sensor to light. Defines the cameras viewpoint in relation to the subject. fixed camera position without any movement. Camera movements restricted to horizontal or vertical axes. Camera movements that incorporate spatial depth and multi-axis motion. Specifies the support systems and stabilization equipment used to facilitate camera movement. Shot Size Determines how much of the subject and surroundings are visible in the frame. Framing Placements and composition of subjects within the frame. 19 Name Natural Light Table 4: Lighting Taxonomy Control Nodes and Values Potential Values Description Sunlight, Moonlight, Firelight Natural sources of light, such as sunlight, moonlight, or firelight. Artificial/Practicals Light Man-made light sources that illuminate the scene. Color Temperature Lighting Conditions Soft Shadows Hard Shadows Reflection Lighting Position Motion Color Gels Defines the hue of the light, typically measured in Kelvin, affecting the scenes mood. Describes various lighting scenarios or ambient conditions present in scene. Subtle and diffused shadows resulting from indirect or scattered light. Sharp, well-defined shadows generated by direct light source. The effect of light bouncing off surfaces to create reflective appearance. Specifies the placement or direction of the light source relative to the subject. Dynamic changes or movement in the lighting effect. Colored filters applied to lights to modify or enhance the color of the illumination. LED, HMI, Tungsten, Fluorescent, HID Warm, Cool, Cold Candlelight, Golden Hour, White Fluorescent, Clear Daylight, Overcast Diffused Light, High Key Lighting, Reflectors Direct Light, Low Key Lighting - Back Light, Fill Light, Top Light, Side Light, Key Light Flickering, Pulsing - Figure 16: Model performance on Events across genres. Across 13 models and 12 genres, portrayal of Events in Biography and Adventure are the strongest, while Comedy and Horror are the weakest. A.2 Additional Results and Analysis A.2.1 Events Figure 16 shows Events performance across 13 models and 12 genres. Biography and Adventure are strongest whereas Comedy and Horror are the weakest. Minimax leads in 6/12 genres, Luma Ray 2 tops Action and Drama, and WAN-14B is the most consistent, with the lowest standard deviation. Figure 17 shows performance of 10 open-source models across 19 emotion classes. Models perform best on remorse and ecstasy, but fare poorly on aggressiveness and rage. As shown in Figure 18, dialogue performance is weaker in comparison to Emotions and Actions. Models particularly struggle with multi-turn dialogues or when non-verbal reactions follow. Since T2V models do not generate audio, we evaluate whether the correct character delivers the line and/or with appropriate visual Potential Values Low, High Gaussian, Radial, Motion Gaussian, Salt and Pepper, Poisson Name Contrast Blur Noise Film Grain Color Palette Lines Regular Shapes Natural Shapes Frame Balance Table 5: Setup Taxonomy Control Nodes and Values Description Determines the difference between light and dark areas to enhance visual impact. Introduces softness to parts of the image to guide focus or create mood. Adds random variations in brightness or color, mimicking film grain or digital sensor noise. Emulates the granular texture of traditional film photography for classic look. Defines the overall range and harmony of colors in the scene, influencing its mood. Directional elements that guide the viewers gaze within the shot. Structured, geometric forms such as squares, circles, and triangles that add order to the design. Unstructured shapes that naturally emerge in the scene, without any geometric constraints. Refers to the distribution of visual weight across the composition, ensuring harmonious layout. The absolute position of an object or subject in scene. Open Set Open Set Open Set - Horizontal, Vertical, Diagonal Square, Circle, Triangle Water-like, Cloud-like Rule of Thirds, Symmetry, Right Heavy, Left Heavy Positional Accuracy Relative Positioning The relative positioning of an object in relationship to Depth Setting Time of Day Location Negative Space Positive Mood Scale Style Background Elements Prop Description Prop Class Prop Material Prop Pattern Prop Utility Subject Class Subject Accessories Subject Costume Subject Hair Subject Makeup Subject Pose Subject Silhouette Subject Proportions Text Generation other objects in the scene. Controls the perception of distance between elements, enhancing the three-dimensional feel of the scene. Defines if the scene is happening indoors or outdoors. The time of day the scene is set in. The specific place or setting of the scene. Defines if there lot of empty space. Defines how the space is occupied in the environment. The emotional atmosphere or feeling created by the environment. The relative size or extent of the environment. The artistic or visual style of the backdrop. The part of the scene that is behind the main subject and does not need to be exactly described. The natural or artificial components of the backdrop. general description of the prop. The category or type of the prop. The substance(s) the prop is made of. The design on the prop. The purpose or function of the prop, whether it just exists in the scene or will it be used by the subject. The category or type of the subjects. Items worn or carried by the subjects that enhance their appearance or functionality. The clothing worn by the subjects, especially for performance or to create specific character. The style and appearance of the subjects hair. Cosmetics applied to the subjects face or body to enhance or alter their appearance. The position or stance of the subjects, especially for photograph or portrait. The outline or shape of the subjects against light background. The relative size and scale of the subjects body parts or features. The process of creating written content to be displayed on the video. Deep, Flat, Limited, Ambiguous INT/EXT DAY, NIGHT, MORNING, EVENING, DAWN, DUSK, LATE NIGHT, MIDDAY, SUNRISE, SUNSET, AFTERNOON Open Set - Clean, Cluttered Open Set Open Set Open Set Open Set Rain, Snow, Fog, Wind, Thunder, Smoke, Dust, Ash, Fire Open Set Open Set Wood, Glass, Gold, Paper, Plastic Grid, Checker, Stripes, Zigzag, Dots, Bricks, Metal, Hexagons Decorative, Functional Open Set Open Set Open Set Open Set Open Set Open Set Open Set Open Set Open Set expression. Most models fail to localize the speaker, often attributing single dialogue to multiple characters. A.2.2 Setup For the Setup taxonomy, we also analyze performance at the value level. In Balance (Figure 19), models handle rule of thirds framing more effectively but struggle with symmetrical compositions. Name Standalone Actions Interactive Actions Temporal (Actions) Foreground (Actions) Background (Actions) Uncertainty Implicit Emotions Explicit Emotions Temporal (Emotions) Table 6: Events Taxonomy Control Nodes and Values Description If the action is stand-alone If the action involves subject-subject or object-subject interaction How actions unfold across time. Potential Values Open Set Open Set Describes if the action is taking place in the foreground Describes if the is taking place in the background The probabilistic nature of the action outcome Emotions that are suggested or implied rather than directly stated. Emotions that are clearly and directly shown or stated within the scene. How emotions evolve across time. Atomic, Concurrent, Sequential, Causal, Overlapping, Cyclic, Reverse Local, Global, Focal - Probabilistic, Deterministic, Mixed Open Set Open Set Atomic, Concurrent, Sequential, Overlapping, Causal Local, Global, Focal - Dash, Ellipsis, Monologue Local, Global, Focal - Foreground (Emotions) Background (Emotions) Type of Dialogue Delivery How the dialogue is delivered Foreground (Emotions) Background (Emotions) Describes if the emotion is taking place in the foreground Describes if the emotion is taking place in the background Change in Environment Story Structure Describes if the dialogue is being spoken in the foreground Describes if the the dialogue is being spoken the background Change of environment or occurrences within shot Key narrative elements that shape the scenes progression. Turning Point, Climax, Foreshadowing, Open Set Pace Regularity How fast the events are happening in shot How regularly the events are happening in shot Conflict Slow, Fast Regular, Irregular Figure 17: Model performance on Emotions. Among 10 models and 19 emotions, Remorse is best portrayed, while Aggressiveness is the weakest. Figure 18: Model performance on Dialogues. Compared to Actions and Emotions, models struggle at Dialogues. Within Dialogues, performance drop is seen during multi-turn conversations. 22 For Time of Day (Figure 20), among 11 categories, Sunrise and Morning are portrayed well, while Afternoon remains challenging. Figure 19: Between different frame compositions, models are better at Rule of Thirds but struggle at maintaining Symmetry. Figure 20: Across Time of Day setups, Sunrise shots are handled better, while Afternoon remains more challenging for models. A.3 Details on Prompt Generation Table 7 presents an example of prompt and the corresponding categories and generated questions. Below, we show an example of the instruction given to an LLM to upsample SCINE-Script into SCINE-Visuals by incorporating control nodes from the Camera and Lighting taxonomies. SCINE Scripts - Cinematographer System Prompt : You are world-class cinematographer known for your visionary storytelling, mastery of 23 Table 7: working example of prompt with its corresponding categories and questions. Each question targets single control node from the taxonomy, enabling human annotators to perform fine-grained, independent evaluations per node. Final Category Camera Creative Intent Shot Size Question Prompt In stark white laboratory illuminated by cool LEDs casting clinical precision, scientist carefully drops single blue chemical into beaker, the camera framing an intimate close-up as soft depth of field blurs the sterile environment behind. back light carves subtle halo around the glassware moments before the liquid erupts into bright green, intensified by strategic neon-tinted color gel that makes the reaction glow like bottled lightning. Camera Intrinsics Depth of Field Lighting Sources Artificial/Practicals Light Lighting Color Temperature Lighting Lighting Position Lighting Advanced Controls Color Gels Does the generated video clearly exhibit well-executed close-up shot that captures the subject with the intended intimacy and detail? Does the video effectively showcase soft depth of field that isolates the subject while smoothly blurring the background? Is the effect of artificial LED source clearly visible and does it emulate the clinical, cool lighting effect as described in the scene? Does the video convey cool color temperature in its lighting setup that reinforces the clinical precision suggested in the prompt? Is back lighting effect evident in the video, such that it effectively carves halo or outline around the subject as described? Does the video incorporate neon-tinted color gel effect that intensifies the lighting during the chemical reaction as detailed in the prompt? light, and camera. You have decades of experience working on award-winning films across genres, collaborating with top directors and production teams. Your insights blend technical expertise with artistic sensibility. When describing scenes or advising on visual storytelling, you use cinematic terminology with clarity and inspiration. Think like Roger Deakins, Emmanuel Lubezki, and Greig Fraseryour visual choices always elevate the emotional tone and narrative arc of project. User Prompt : GOAL You will be given prompt and 2 taxonomies that define camera and lighting Your objective is to enrich controls commonly used by cinematic professionals. the given prompt by sampling relevant nodes from both the taxonomies. As cinematographer, your role is to \"shoot\" this scene using the best possible cinematic expression, utilizing the camera and lighting control options provided in the taxonomy. PROMPT: {prompt} MOST IMPORTANT INFORMATION 1. Only Use Nodes from the Provided Taxonomies : - You must never introduce nodes that are not present in the given taxonomies. - While the values within each node can be flexibleallowing for creativity and imagination grounded in your professional experience. For example, the node \"Color Gel\" is defined, but has no values. It is upto you to define these values. - The structure must strictly adhere to the nodes defined in the taxonomy. Think expansively within the bounds of each node, but never go beyond them. 2. Preserve the Original Prompt Content : - Do NOT remove or add any of the original content from the input prompt. - Your only task is to enrich the prompt by layering in camera and lighting related information. The core semantics and narrative of the prompt must remain entirely intact. 3. Do NOT include the path through which you sample the nodes in the prompt. That is, do NOT add the paths from the taxonomy using ->. GUIDELINES 1. Input Prompt The input prompt describes single continuous event, intended to occur within one uninterrupted shot. Therefore, do not include any cuts or multiple camera setups. Assume this is one-shot sequence. 2. Each node in the taxonomies contains: - Description: definition of what the node represents. - Example: An example of how the node may appear in prompt. 24 - Values: non-exhaustive list of possible values for the node. Some notation: a. OPEN SET Indicates the node supports wide range of possible values. b. [] Indicates the node may have multiple values, which are not predefined and should be selected based on your reasoning and cinematic knowledge. 3. Enriched Prompt Your enriched version will serve as input to text-to-video model. It must be fluent, natural, and interpretable by the model, while incorporating cinematic elements effectively. CAMERA TAXONOMY The Camera Taxonomy defines elements related to the cameras intrinsics, extrinsics, and its cinematic use : {camera_taxonomy} LIGHTING TAXONOMY The Lighting taxonomy broadly defines all elements of lighting, including source, position of lighting, along with its effects such as shadows and reflections, along with color temperature, lighting motion such as flickering etc : {lighting_taxonomy} When incorporating lighting into your enriched prompt, remember that cinematographer can shape the look and feel of shot by selectively illuminating different depth planes of the scene. Lighting can be applied to the foreground, mid-ground, background, and the subject itselfeither individually or in combination. Your choices should support the emotional tone, visual focus, and narrative intent of the shot. Below, we show an example of the instruction given to an LLM to categorize and generate evaluation questions for an input prompt using the Camera taxonomy. Camera Categorization and Question Generation GOAL You are an expert prompt evaluator. Your task is to analyze video generation prompt and categorize it based on predefined taxonomy. PROMPT: {prompt} Available Categories (with Examples) The category presented to you is that of Camera. The Camera taxonomy broadly defines everything related to the camera - the intrinsics, the extrinsics and the cinematic use of camera. {camera_taxonomy} Notes about the Taxonomy Each node in the taxonomy contains : 1. Description : Definition of what that node represents. 2. Example : An example of the presence of node in the form of prompt. 3. Values : non-exhaustive list of values of these nodes. Values are list of values that this node can have. Some nomenclature : a. OPEN SET indicates that this node contains large number of values. b. [] indicates that this node may have multiple values, but are not defined explicitly and it is upto your reasoning and knowledge. Examples of Categorization 1. Static Medium-Close-Up of Davids face showing quiet devastation. Quick Push In as tears well up in his eyes. Shot with medium ISO to capture the dim apartment lighting. Static - Camera -> Trajectory -> Camera Movement -> Static Push In - Camera -> Trajectory -> Camera Movement -> 3D Medium-Close up - Camera -> Creative Intent -> Shot Size Medium ISO - Camera -> Intrinsics -> Exposure -> ISO 2. Wide shot of bustling city street at night. The neon lights of the shops and restaurants cast colorful glow on the wet pavement. People walk by, their faces illuminated by the bright signs. The camera pans up to reveal the towering skyscrapers that loom overhead, their windows reflecting the city lights. 25 Table 8: Lexical Diversity of SCINE Scripts. Compared to existing prompt-based benchmarks, SCINE-Scripts demonstrate higher lexical diversity across multiple metrics."
        },
        {
            "title": "Benchmark",
            "content": "TTR Distinct Bi-Grams Jaccard Distance VBench [26] MovieGenBench [11] EvalCrafter [38] T2V-CompBench [53] SCINE Scripts 0.1489 0.1660 0.2270 0.1435 0.1760 0.4605 0.5311 0.6038 0.4781 0.6177 0.9384 0.9285 0.9413 0.9350 0. Wide shot - Camera -> Creative Intent -> Shot Size Pans Up - Camera -> Trajectory -> Camera Movement -> 2D TASK Analyze the given prompt and return the following structured output in valid JSON format: Words: Extract important keywords or key phrases from the prompt using the following guidance: - Identify named entities related to camera in professional use as you would in NER (Named Entity Recognition). - Extract noun phrases or descriptive terms that relate to camera. - Prefer multi-word expressions where meaningful related to camera. - Avoid generic or uninformative words like a, video, the, etc. Categories: For each word or phrase, assign the most appropriate category from the taxonomy. dictionary of relevant categories from the taxonomy. - For each relevant category, assign score between 0 and 1 representing how strongly the prompt matches the category. - Provide reason for each score, referring to the words or phrases extracted and how they relate to the category. - Generate question that helps human evaluator determine whether this category is visually present in the generated video. Use your reasoning to guide the question. The evaluator will use this question to rate the video on scale from 1 (not at all) to 5 (strongly represented). - The generated question should evaluate quality, consistency and presence of the node in the video. Important Guidelines: - The camera information should be explicitly mentioned in the prompt. Do NOT imply, assume or derive anything. Only consider word or phrase match, if it is explicitly mentioned in the prompt. - Each prompt can have multiple nodes of the Camera taxonomy. You should capture all of the nodes in the prompt and map it back to the taxonomy. - You must always traverse from the root node, which is Camera in this case. That is, the category should always start as (Camera -> ..) - You will never create node that is not in the taxonomy. These nodes can have multiple values, as previously explained and you are expected to be imaginative about the values. But the nodes, should always come from the given taxonomy. - Since the taxonomy is of Camera, we do not care about objects, subjects, lighting, events, actions or emotions. Your sole focus should be about camera terms that are present in the prompt in accordance with the taxonomy. You will NOT ask any question related to objects, subjects, lighting, events, actions or emotions. Table 8 compares SCINE-Scripts with existing prompt-based video generation benchmarks. We compute token level metrics: Type-Token Ratio (TTR), Distinct Bi-Grams, and average pairwise Jaccard Distance, and find that SCINE-Scripts exhibits strong lexical diversity. 26 (a) Level 1 Activations (b) Level 2 Activations [1em] (c) Level 3 Activations (d) Level 4 Activations Figure 21: Node activations in Camera and Lighting taxonomies for the Cinematographer role in SCINE Visuals. (a) Level 2 Activations (b) Level 3 Activations [1em] (c) Level 4 Activations (d) Level 5 Activations Figure 22: Node activations in Setup taxonomy for the Production Designer role in SCINE Visuals. A.4 Distribution of Taxonomy Categories in SCINE Prompts Figures 21, 22, and 23 show the distribution of activated nodes in SCINE Visuals, aggregated at the node level, across the roles of Cinematographer, Production Designer, and Director, respectively. As shown, our prompts cover broad distribution of nodes across all the taxonomies. 27 (a) Level 1 Activations (b) Level 2 Activations [1em] (c) Level 3 Activations (d) Level 4 Activations Figure 23: Node activations in All taxonomies for the Director role in SCINE Visuals. Figure 24: User Interface used by annotators to perform evaluations. A.5 Annotation Details Figure 24 shows the annotation interface used by human annotators during evaluation. We also present the distribution of annotators years of experience in film production in Figure 25. While annotations for cinematic controls can be subjective, especially given the large number of control nodes, we try our best to mitigate this by providing clear rating guidelines to annotators for each control node. Table 9 presents minimal example of the rating guidelines shared with the annotators. 28 Figure 25: Distribution of the years of film production experience amongst human annotators in our evaluation setup. Table 9: Examples of rating guidelines provided to human annotators for different control nodes, across all taxonomies. Dimension Score What to Look For Low Angle Overlapping Actions Back Light Position Symmetrical Frame Balance Slight upward tilt, but still feels neutral. 1 Camera is at or above eye level, not low angle at all. 2 3 Below subject, mild upward view, light impact. 4 Clear low angle, subject looks larger or imposing. 5 Strong low angle, subject dominates, towering presence. 1 No action or one action is present. 2 Actions are isolated or unrelated. 3 Timing is off, they start or end awkwardly. 4 5 Some overlap, but hard to follow. Fluid overlap, actions feel natural and dynamic together. 1 Light clearly comes from front or side, no rim light or background 2 separation. Some edge lighting, but not consistent or strong, subject may still blend into background. 3 Back light is partially visible, outline is hinted but not clear on full subject. 4 Back light is clearly present, rim light separates subject from back5 ground. Strong back light effect, glowing edges around hair or shoulders. Subject clearly pops against the background. Perfect match. 1 Composition is clearly asymmetrical. Some repeating elements, but no visual mirror. 2 3 Partial symmetry or mirrored clutter thats not clean. 4 Almost perfect symmetry, small inconsistencies exist. 5 Clear and precise symmetry, mirrored subjects, reflections, or centered framing. Strong and intentional. 29 A.6 Statistical tests Pairwise t-tests show that the vast majority of model comparisons in our human evaluation, across all taxonomies are statistically significant at the 5% level (p <0.05). Figure (26 - 28) presents the t-test results of Events, Lighting and Camera, and Setup, respectively. (a) Pairwise t-tests across models on emotions (b) Pairwise t-tests across models on dialogues (c) Pairwise t-tests across models on actions Figure 26: Statistical comparison matrices for Events: Emotions, Dialogue, and Actions. 30 (a) Pairwise t-tests across models on Color Temperature (b) Pairwise t-tests across models on Lighting Effects (c) Pairwise t-tests across models on Extrinsics (d) Pairwise t-tests across models on Intrinsics (e) Pairwise t-tests across models on Trajectory Figure 27: Statistical comparison matrices for Camera and Lighting 31 (a) Pairwise t-tests across models on Subject Class (b) Pairwise t-tests across models on Subjet Costume (c) Pairwise t-tests across models on Elements (d) Pairwise t-tests across models on Time of Day (e) Pairwise t-tests across models on Location Figure 28: Statistical comparison matrices for Setup 32 Table 10: Top 10 nodes where the VLM shows the strongest performance. Score indicates the accuracy of the VLMs choice with the most common preference among human annotators. Node SetupSceneSet DesignEnvironmentMood EventsAdv.ControlsRhythmPace SetupSceneSet DesignEnvironmentStyle SetupSceneSet DesignPropsUtility EventsTypesEmotionsExp.TypesExplicit SetupSceneGeometryFrameShapesRegular LightingLighting EffectsShadowsSoft SetupSceneGeometrySpaceSpatial Loc.Rel.Pos. EventsTypesActionsInt.TypesStandalone EventsTypesEmotionsExp.TypesExplicit Score 0.88 0.82 0.82 0.80 0.79 0.78 0.76 0.75 0.75 0.75 Table 11: Bottom 10 nodes where the VLM shows the weakest performance. Score indicates the accuracy of the VLMs choice with the most common preference among human annotators. Node SetupSubjectsMakeup SetupSceneSet DesignEnvironmentBackground EventsTypesActionsPortrayed asContextualBackground SetupSubjectsAccessories LightingLighting EffectsReflection SetupSceneTextureColor Palette CameraIntrinsicsExposureShutter Speed LightingAdv.ControlsColor Gels SetupSceneTextureBlur LightingColor Temperature Score 0.33 0.33 0.46 0.53 0.55 0.55 0.57 0.57 0.57 0.58 A.7 Additional VLM results Node-specific results of VLM evaluator Table 10 and 11 list the set of nodes on which our VLM evaluator has the strongest and weakest performance, respectively. Comparison with Closed Source Models We extend our validation to closed-source, flagship SOTA models. Specifically, we evaluate two recent models from the Gemini family with distinct purposes: Gemini-2.0-Flash, optimized for fast inference, and Gemini-2.5-Pro-Preview-05-06, optimized for complex reasoning. We use the same human-aligned preference accuracy metric as with opensource models. Due to the lack of public details on model sizes, we cannot draw conclusions about scaling effects. However, Gemini-2.5-Pro consistently outperforms open-source models, including QwenVL-2.5-72B, across all categories. Notably, as shown in Figure 29, our 7B model outperforms Gemini-Flash across all categories and performs competitively with Gemini-2.5-Pro. This highlights the strength and scalability of our approach for professional video evaluation. Reliability in VLMs reliable VLM-as-a-Judge should produce consistent scores when given the same video, prompt, and focus aspect. In this analysis, we evaluate the raw scores generated by VLMs rather than preference rankings, and measure their stability under Best-of-5 sampling. Since VLMs are probabilistic, we evaluate reliability via the standard deviation of scores across runs. We use temperature=0 to sample to make ensure that the highest probability is selected at each sampling step. We exclude our model from this analysis, as its architecture includes dedicated value head, unlike zero-shot VLMs that produce rewards as text. Our results show that Qwen-2.5VL-3B exhibits high variance, making it unreliable under repeated sampling. In contrast, the flagship models and the strongest open-source model, QwenVL-2.5-72B, demonstrate high reliability, with consistently low variance  (Table 12)  . Figure 29: Preference Accuracy of open and closed-sourced VLMs in rating videos generated for Professional Use Table 12: Measuring VLM Reliability across best-of-5 sampling Model Standard-Deviation Krippendorff-alpha Qwen2.5-VL-3B Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B Gemini2.5-Flash Gemini2.5-Pro 2.34 0.37 0.47 0.23 0.20 0.14 0.36 0.84 0.65 0.95 0.90 0.95 Table 13: Average scores across taxonomy categories on recently released video generative models. Model Camera Events Lighting Setup Overall Avg. Veo 3 Fast Wan 2.2 14B Wan 2.1 14B Wan 2.2 5B Wan 2.1 1B 3.686 3.584 3.239 3.194 3.240 3.382 3.060 2.821 2.705 2.579 3.974 4.009 3.827 3.838 3. 4.078 4.114 3.975 3.913 3.742 3.780 3.692 3.466 3.412 3.315 A.8 Additional Results on Recent Models We also perform small-scale evaluations on recently released models, specifically Veo 3 (Fast) [9] and the Wan 2.2 family of models. Results are presented in Table 13."
        }
    ],
    "affiliations": [
        "Arizona State University",
        "Google DeepMind",
        "Stability AI"
    ]
}