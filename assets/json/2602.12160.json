{
    "paper_title": "DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation",
    "authors": [
        "Xu Guo",
        "Fulong Ye",
        "Qichao Sun",
        "Liyang Chen",
        "Bingchuan Li",
        "Pengze Zhang",
        "Jiawei Liu",
        "Songtao Zhao",
        "Qian He",
        "Xiangwang Hou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 0 6 1 2 1 . 2 0 6 2 : r DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation Xu Guo1,, Fulong Ye2,, Qichao Sun2,,, Liyang Chen1, Bingchuan Li2,, Pengze Zhang2, Jiawei Liu2, Songtao Zhao2,, Qian He2, Xiangwang Hou1, 1Tsinghua University, 2Intelligent Creation Lab, ByteDance Equal contribution, Project Lead, Corresponding author"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audiovideo generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within single framework remains an open challenge. In this paper, we propose DreamID-Omni, unified framework for controllable human-centric audio-video generation. Specifically, we design Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications. Date: February 13, 2026 Project Page (Demo, Codes, Models): https://guoxu1233.github.io/DreamID-Omni/"
        },
        {
            "title": "Introduction",
            "content": "Recently, joint audio-video generation has seen rapid progress, with many breakthrough works emerging. For example, commercial models such as Veo3, Sora2, Wan 2.6 [47] and Seedance 1.5 Pro [4] have achieved impressive results. In the open-source community, models like Ovi [36] and LTX-2 [15] have also demonstrated promising performance. These advances have greatly promoted the development of joint audio-video generation. However, in real-world applications, supporting more controllable generation particularly within human-centric scenarios is crucial. Controllable human-centric generation has advanced in several directions. Works such as Phantom [35] and Wan2.6 [47] utilize reference images or voice timbres for video (R2V) or audio-video (R2AV) generation, which rely solely on text prompts as weakly-constrained guidance. To achieve higher controllability, other 1 Figure 1 Showcase of DreamID-Omni. DreamID-Omni seamlessly unifies reference-based audio-video generation (R2AV), video editing (RV2AV), and audio-driven video animation (RA2V). approaches introduce stronger supervision, such as source videos or driving audio, for strongly-constrained generation. For instance, Humo [2] animates videos (RA2V) based on reference identities and driving audio, while works like HunyuanCustom [21] and VACE [25] perform video editing given reference identity and source video, which can be further extended to replace the corresponding audio (RV2AV). Despite these advancements, these capabilities are largely treated as isolated tasks. Researchers in the video-only domain have begun to shift toward unified architectures [19, 25, 30, 40, 59, 61] to enhance task flexibility and reduce the operational overhead of deploying multiple models. However, the joint audio-video domain still lacks unified perspective. Fundamentally, we observe that R2AV, RV2AV, and RA2V all share an identical objective: mapping static identity anchor (image and audio) onto dynamic spatio-temporal canvas (text, source video, or driving audio). Based on this insight, these tasks are inherently amenable to unified framework trained on consistent data source, transcending the limitations of task-specific silos. Nevertheless, developing this unified framework presents several challenges: (1) How to build unified model framework that supports generation, editing and animation; (2) How to address identity-timbre binding and speaker confusion in multi-person generation; (3) How to design effective training strategies to prevent conflicts among multiple tasks. To address these challenges, we introduce DreamID-Omni, which integrates reference-based generation, editing, and animation into single paradigm. DreamID-Omni builds upon dual-stream Diffusion Transformer [38] (DiT) architecture, where video and audio streams interact via bidirectional cross-attention for fine-grained synchronization. We propose Symmetric Conditional DiT design that unifies heterogeneous conditioning signalsreference images, voice timbres, source videos, and driving audiointo shared latent space, enabling seamless task switching without architectural changes. To resolve multi-person confusion, we propose Dual-Level Disentanglement strategy. At the signal level, Synchronized Rotary Positional Embeddings (Syn-RoPE) is introduced to bind reference identities with their corresponding voice timbres within the attention space. At the semantic level, Structured Captions utilize anchor tokens paired with fine-grained descriptions to establish explicit mappings between specific subjects and their respective attributes or speech content. Finally, we devise Multi-Task Progressive Training strategy to harmonize the three tasks. In the initial 2 two stages, we focus exclusively on the weakly-constrained R2AV task, employing in-pair reconstruction and cross-pair disentanglement to enhance identity and timbre fidelity while encouraging the model to learn robust reference representations. In the final stage, strongly-constrained tasks (RV2AV and RA2V) are introduced for joint training with R2AV. This approach prevents the model from overfitting to strongly-constrained tasks, thereby maintaining superior performance on the weakly-constrained generation task. In summary, our contributions are as follows: (1) We propose DreamID-Omni, novel human-centric controllable generation framework based on Symmetric Conditional DiT, which seamlessly integrates R2AV, RV2AV, and RA2V tasks. (2) We introduce Dual-Level Disentanglement, which addresses identity-timbre binding and speaker confusion in multi-person generation via Syn-RoPE and Structured Captions. (3) We present Multi-Task Progressive Training strategy that effectively harmonizes diverse tasks with varying constraint strengths. (4) Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even when compared to leading proprietary commercial models."
        },
        {
            "title": "2.1 Joint Audio-Video Generation",
            "content": "Recent advancements in diffusion-based foundation models in video generation [12, 26, 47] and audio generation [13, 32] have significantly expanded the frontier of joint audio-video synthesis. While pioneering works [42] use coupled U-Net backbones, current DiT-based approaches dominate the field. These methods typically employ either dual-stream architectures [15, 17, 33, 34, 36, 48] with specialized fusion layers (e.g., cross-attention) or unified DiT structures [22, 49, 50, 63] with joint self-attention to achieve synchronized multi-modal alignment. Despite their impressive generative fidelity, these models are primarily designed for vanilla text-to-audio-video or first-frame-conditioned synthesis. They lack the capability to condition the generative process on external identity or voice timbre references. This limitation restricts their utility in scenarios requiring persistent identity and timbre consistency."
        },
        {
            "title": "2.2 Controllable Video Generation Model\nReference-based Generation. To enhance controllability, reference-based video generation has emerged as a\nprominent research direction, focusing on maintaining identity consistency by integrating reference features\ninto the diffusion process. While initial efforts [18, 39, 62] were primarily tailored for single-identity scenarios,\nsubsequent research has extended these capabilities to multi-subject settings\n[1, 10, 21, 23, 29, 35, 65].\nHowever, these works are typically video-centric and do not support audio generation.",
            "content": "Video Editing and Animation. In terms of temporal control, tasks can be categorized into video editing and audio-driven video animation. Editing frameworks [5, 7, 14, 25, 37, 43, 52, 58] allow for the modification of identity attributes within the source video. Audio-driven video animation [2, 31, 51, 54, 57] aims to generate videos from reference images to produce lip movements matching input speech signals. Despite their success, these models are all task-specific, and no existing model attempts to unify reference-based generation, editing, and animation."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "We unify the landscape of controllable human-centric generation into single probabilistic framework. Given text prompt , set of reference identities = {I1, . . . , IN }, and corresponding reference voice timbres = {A1, . . . , AN }, the goal is to synthesize synchronized video-audio stream = {Yvideo, Yaudio}. To support reference-based editing and animation tasks, we introduce two optional structural conditions: source video context Vsrc and driving audio stream Adri. The framework models the conditional distribution: (Y , I, A, Vsrc, Adri) (1) 3 Figure 2 Overview of DreamID-Omni framework. We integrate reference-based generation (R2AV), editing (RV2AV), and animation (RA2V) using Symmetric Conditional DiT trained via multi-task progressive training strategy. Structured Caption and Syn-RoPE ensure robust dual-level disentanglement in multi-person scenarios. By selectively providing these conditions, our framework seamlessly transitions between three distinct tasks, as summarized in Table 1. Task Human-Reference Audio-Video Generation (R2AV) Human-Reference Video Editing (RV2AV) Human-Reference Audio-Driven Video Animation (RA2V) Input Output Goal Generate with references I, A. , I, , I, A, Vsrc Edit identity and audio in Vsrc. Animate identity using Adri. , I, Adri Table 1 Task Unification in DreamID-Omni. Our framework unifies R2AV, RV2AV and RA2V by toggling input conditions."
        },
        {
            "title": "3.2.1 Symmetric Conditional DiT",
            "content": "A core architectural contribution of DreamID-Omni is the Symmetric Conditional DiT, designed to seamlessly integrate reference-based generation, editing, and animation within unified framework. This is achieved through symmetric dual-stream conditioning strategy that composes heterogeneous control signals in the latent space with structural parity. Let zv and za represent the noisy target video and target audio latents, respectively. To guide the denoising process, we construct two comprehensive conditional sequences, Xv and 4 Xa, which integrate both identity-specific and structural guidance: Xv = [zv; Ev(I)] + [Ev(Vsrc); 0Ev(I)] Xa = [za; Ea(A)] + [Ea(Adri); 0Ea(A)] (2) (3) where [; ] denotes concatenation along the sequence dimension, 0T represents zero tensor with the same shape as , and Ev, Ea are the respective VAE encoders. In this symmetric formulation, the reference features (Ev(I), Ea(A)) are concatenated to the noisy latents, allowing the DiT blocks to extract and disentangle high-level identity and timbre priors. Simultaneously, the structural conditions (Vsrc, Adri) are injected via element-wise addition, serving as structural canvas that enforces spatial and temporal consistency. This dual-injection strategy effectively decouples the conditioning into identity-preservation and structural-guidance channels. The inherent flexibility of this design enables seamless task switching. As detailed in Table 1, providing null input for the structural conditions (Vsrc or Adri) effectively nullifies the additive term in Eqs. 2 or 3. Consequently, the model adaptively transitions between R2AV, RV2AV, and RA2V based on the available conditional modalities, maintaining unified parameter set across all functional modes."
        },
        {
            "title": "3.2.2 Dual-Level Disentanglement",
            "content": "A critical challenge in multi-person generation is the confusion between subjects, which manifests in two forms: identity-timbre mismatch (e.g., subject speaks with the voice of subject B) and attribute-content misattribution (e.g., subject erroneously inheriting the visual attributes and dialogue of subject B). We posit that these failures stem from entanglement at two distinct levels. At the signal level, standard attention mechanisms fail to bind the visual features of an identity to its corresponding voice timbre. At the semantic level, unstructured text captions provide insufficient granularity to explicitly link specific subjects to their respective visual attributes, motions, and speech content. To address this, we propose Dual-Level Disentanglement strategy. We introduce Syn-RoPE to enforce rigid binding at the signal level, and Structured Captioning scheme to resolve ambiguity at the semantic level. Syn-RoPE. Recent works [27] have explored using Rotary Position Embedding [44] for spatial localization within video frames. However, such spatially-grounded approach is incompatible with the more challenging task like R2AV, where character positions are synthesized dynamically by the model. To overcome this limitation, we propose Syn-RoPE, an identity-grounded mechanism that operates by assigning distinct, non-overlapping temporal positional segments to different semantic inputs within the models attention space. As illustrated in Figure 2, inspired by [36], we synchronize the video and audio streams by scaling the RoPE frequencies of the target audio latents by factor γ = Lv/La, where Lv and La denote the sequence lengths of the target video and audio latents, respectively. More crucially, Syn-RoPE partitions the absolute temporal positional index space into reserved RoPE Margins for the target sequence and each reference identity. Specifically, the target video and audio latents occupy the initial positional range [0, 1], where denotes the maximum temporal length. We define fixed margin such that to serve as the base interval for each identity slot. Subsequently, the latent features of the k-th reference identity (both image Ik and audio Ak) are assigned to the k-th reserved segment, [k M, (k + 1) 1]. This strategy offers two fundamental advantages: (i) Inter-Identity Decoupling: By leveraging the periodicity of RoPE, each identitys features are projected into distinct rotational subspace, naturally suppressing cross-identity attention scores and preventing feature entanglement. (ii) Intra-Identity Synchronization: By mapping the visual and acoustic features of the same identity to identical positional segments, we achieve robust, implicit cross-modal synchronization at the signal level. This design provides unified mechanism for robust identity binding across all generation, editing, and animation tasks. Structured Caption. At the semantic level, ambiguity in multi-subject scenarios typically arises when standard prompts fail to explicitly associate visual attributes, motions, and speech content with specific individuals. To resolve this, we introduce Structured Captioning scheme that establishes an unambiguous mapping between each reference identity Ik and unique anchor token, denoted as subk. The process begins by generating fine-grained attribute description for each identity to initialize the anchor tokens. Building upon this foundation, the target video content is synthesized into comprehensive script partitioned into distinct 5 semantic fields: video caption, audio caption, and joint caption. Crucially, all references to individuals across these fields consistently utilize the predefined anchor tokens subk. This format provides the model with an explicit grounding that resolves semantic-level entanglement, which is critical for the success of all three core tasks."
        },
        {
            "title": "3.3 Multi-Task Progressive Training",
            "content": "Training unified model for R2AV, RV2AV, and RA2V presents complex optimization challenge. naive joint training approach often suffers from conflicting learning objectives, where the generative objective of creating diverse content can interfere with the fidelity objective of adhering to strong conditional constraints. To circumvent this, we introduce Multi-Task Progressive Training Strategy, three-stage curriculum designed to incrementally build model capabilities, ensuring stable convergence and synergistic learning. In-pair Reconstruction. The initial stage aims to establish robust generative prior for controllable generation. We train the model exclusively on the R2AV task, using an in-pair reconstruction objective. For each training sample , we extract the reference identity and the reference timbre from the sample itself. The model is then tasked with reconstructing the full data stream conditioned on these internal references and the text prompt . To prevent the model from trivially copying the reference segments and to encourage true conditional synthesis, we introduce masked reconstruction loss. Let Mv and Ma be binary masks identifying the spatio-temporal regions of and within the ground truth latents. The loss is computed only on the unmasked regions, forcing the model to generate, rather than merely copy, the content corresponding to the references. The objective is defined as: Linpair = Ez,t,C (cid:2)λv(1 Mv) (ϵv ˆϵθ(zv,t, t, C))2 2 + λa(1 Ma) (ϵa ˆϵθ(za,t, t, C))2 (cid:3) (4) where the conditioning set for this stage is = {I, A, }, ϵ is the ground truth noise, ˆϵθ is the models prediction, and denotes element-wise multiplication. Cross-pair Disentanglement. To enhance the models generalization capabilities and force it to learn truly disentangled representation of identity and timbre, we advance to cross-pair training stage. In this phase, the reference identity and timbre are sourced from different video clip than the target video-audio stream . This more challenging objective compels the model to synthesize content based on abstract identity and timbre concepts, rather than relying on low-level correlations present in the source. The training objective for this stage, Lcross, reuses the same formulation as Linpair (Eq. 4). However, key distinction is that the masks are nullified by setting Mv = 0 and Ma = 0. This modification ensures the loss is computed over the entire data stream, pushing the model towards more robust disentanglement. Omni-Task Fine-tuning. The final stage unifies all tasks by fine-tuning the model on mixed dataset comprising R2AV, RV2AV, and RA2V samples. RV2AV samples are constructed by providing masked version of the target video as structural context (Vsrc), while RA2V samples supply the target audio as the driving signal (Adri). By training on this composite dataset, the model learns to seamlessly switch between generation, editing, and animation based on the provided conditions, as formulated in Eq. 1. This progressive, three-stage curriculum is crucial. We observe that by first mastering the weakly-constrained R2AV task, the model develops powerful and diverse generative prior. This prior then serves as robust foundation for the strongly-constrained RV2AV and RA2V tasks, allowing the model to learn high-fidelity conditional control without sacrificing generative quality, leading to truly unified and capable omni-purpose model."
        },
        {
            "title": "3.4 Inference Pipeline",
            "content": "At inference time, we employ multi-condition Classifier-Free Guidance (CFG) [20] strategy, which is applied independently to the video and audio streams, but follows the same unified formulation: ˆϵfinal = ˆϵθ(zt, , ) + wT (ˆϵθ(zt, , ) ˆϵθ(zt, , )) + wS (ˆϵθ(zt, , S) ˆϵθ(zt, , )) (5) 6 Figure 3 Qualitative comparison with state-of-the-art (SOTA) methods on R2AV. Please zoom in for more details. Support Video Audio Audio-Visual Consistency Method Video Audio AES ViCLIP Phantom VACE HunyuanCustom Qwen-Image + LTX-2 Qwen-Image + Ovi Wan2.6 Ours 0.604 0.613 0.589 0.611 0.606 0.632 0.618 13.791 11.091 12.159 8.548 8.974 13.410 13. ID-Sim. (S/M) PQ CLAP WER T-Sim. (S/M) - - - 0.657/0.572 0.664/0.395 0.659/- - - - - - - - - - Sync-C - - - Sync-D - - - Spk-Conf. - - - 0.571/0.349 0.459/0.336 0.523/0.455 0.674/0.603 6.247 5.826 6.391 6.290 0.144 0.203 0. 0.278 0.093 0.097 0.534 0.052 - - 0.391/0.217 0.493/0.402 3.706 5.857 6.026 6. 10.003 8.407 8.352 7.791 0.340 0.380 0.380 0.080 Table 2 Quantitative comparison of R2AV on our proposed benchmark. Best results are in bold, second best are underlined. The S/M notation in ID-Sim. and T-Sim. refers to results on single-person and multi-person scenarios, respectively. where ˆϵθ(zt, , S) is the models prediction under text condition and stream-specific condition S. For the video stream, = , while for the audio stream, = A. The terms wT and wS are their respective guidance scales. This chained application ensures that identity and timbre guidance operates on text-aligned basis, leading to more stable and coherent results. The MLLM system prompt for Structured Caption is shown in Fig. 8."
        },
        {
            "title": "4 Experiments",
            "content": "7 AES ViCLIP Method VACE 0.560 HunyuanCustom 0.538 14.353 14.576 ID-Sim. WER T-Sim. - - 0.565 0.590 - - Sync-C - - Ours 0.584 14.832 0.635 0. 0.513 6.241 AES ViCLIP Method Humo 0.550 HunyuanCustom 0.567 14.859 13.027 ID-Sim. 0.609 0.611 Sync-C 6.114 5. Ours 0.591 16.618 0.623 6.325 Sync-D 8.323 9.071 8.659 Table 3 Comparison with SOTA methods on RV2AV. Table 4 Comparison with SOTA methods on RA2V."
        },
        {
            "title": "4.1 Setup\nIDBench-Omni. We introduce IDBench-Omni, a new comprehensive benchmark for controllable human-centric\naudio-video generation. The benchmark comprises three specialized test sets, totaling 200 high-quality data\ninstances, designed to evaluate a model’s omni-purpose capabilities: (1) 100 identity-timbre-caption triplets\nfor evaluating generation task; (2) 50 masked videos with target identity and timbre for evaluating controlled\nvideo editing; and (3) 50 driving audios with reference identities for evaluating audio-driven animation. These\nsets cover a diverse range of challenging scenarios, including complex multi-person dialogues, significant\nvariations in identity and timbre, and in-the-wild recording conditions. IDBench-Omni provides a rigorous\nand holistic platform for evaluating the generation, editing, and animation capabilities of unified audio-video\nmodels.",
            "content": "Implementation Details. We initialize our model from Ovi [36] and train on audio-video data from [28] (construction details in Sec. A.2). During training, we set the learning rate to 1.0 105, with global batch size of 32 and Rope Margin = 150. The training curriculum begins with the In-pair Reconstruction for 10,000 steps, followed by the Cross-pair Disentanglement and Omni-Task Fine-tuning stages, which involve 20,000 iterations each. In the final Omni-Task stage, we sample R2AV, RV2AV, and RA2V data with ratio of 4:3:3. Evaluation Metrics. We evaluate our model across three key dimensions. For video, we assess fidelity and coherence through the aesthetics score (AES) from VBench [24] for video quality, the text-video similarity from ViCLIP [53] for text following, and ArcFace [9] for Identity Similarity (ID-Sim.). For audio, we evaluate its quality and fidelity from multiple aspects. We gauge audio quality via the Production Quality (PQ) score from AudioBox-Aesthetics [46] and semantic consistency using CLAP [56]. Additionally, we compute the Word Error Rate (WER) by transcribing the generated audio with Whisper-large-v3 [41] and comparing it against the ground-truth transcript, while Timbre Similarity (T-Sim.) is determined by the cosine similarity of speaker embeddings from WavLM [3]. For audio-visual consistency, we focus on synchronization and attribution. Lip-sync accuracy relies on the standard confidence (Sync-C) and distance (Sync-D) scores from SyncNet [8]. Finally, Speaker Confusion (Spk-Conf.), critical metric for multi-person dialogues, is evaluated by the Gemini-2.5-Pro model [45], detailed system prompt provided in Sec. A.3."
        },
        {
            "title": "4.2 Comparison\nComparison on R2AV. As there are no open-source methods that directly support the R2AV task, we establish\na set of strong baselines for comparison. We compare our method with the closed-source model Wan2.6 [47]\nand two cascaded pipelines constructed by first generating an initial frame with Qwen-Image [55] and then\nanimating it with LTX-2 [15] and Ovi [36]. Additionally, for video-centric metrics, we include leading R2V\nmodels: Phantom [35], VACE [25], and HunyuanCustom [21]. As demonstrated in Table 2, our method\nachieves superior or comparable results across the video, audio, and audio-visual consistency dimensions. For\nqualitative comparison in Fig. 3, in case (a), our model delivers the most realistic visual results compared to\nbaselines such as Wan2.6, while exhibiting superior identity consistency with the reference identities relative\nto Ovi and LTX-2. In case (b), only ours successfully achieves correct binding between specific identities and\ntheir corresponding timbres, whereas baselines like Wan2.6 suffer from identity-timbre mismatch. See Sec. A.4\nfor user study details.",
            "content": "Comparison on RV2AV. We compare our method with SOTA video editing methods, VACE [25] and HunyuanCustom [21] on RV2AV. The quantitative results are presented in Table 3. Since the compared methods do not support audio generation, audio-related metrics are reported exclusively for our model. The results demonstrate that our method not only achieves SOTA performance on video-centric metrics (AES, ViCLIP, 8 Figure 4 Qualitative comparison with SOTA methods on RV2AV. Please zoom in for more details. Figure 5 Qualitative comparison with SOTA methods on RA2V. Please zoom in for more details. and ID-Sim.), but also exhibits excellent audio generating capabilities, as evidenced by the strong WER, T-Sim., and Sync-C scores. Qualitative results are illustrated in Fig. 4. In case (a), our model delivers higher identity similarity and superior visual quality; in case (b), it demonstrates improved text-following capabilities compared to the baselines. Comparison on RA2V. For the RA2V task, we compare our method with Humo [2] and HunyuanCustom [21]. As shown in Table 4, our method achieves comparable lip-sync accuracy to Humo and leading performance on video-related metrics. Qualitative comparisons are provided in Fig. 5. Notably, in scenarios involving multiple subjects, both Humo and HunyuanCustom frequently exhibit speaker misattribution errors. In contrast, our model animates the correct subject by precisely following the structured captions."
        },
        {
            "title": "4.3 Ablation Studies\nAblation on Dual-level Disentanglement. To validate the effectiveness of our dual-level disentanglement design,\nwe conduct an ablation study on the challenging multi-person dialogue scenario of the R2AV task. The",
            "content": "9 Figure 6 Qualitative results of our ablation studies. (a) Ablation on our dual-level disentanglement design. (b) Ablation on multi-task progressive training. Method w/o Syn-RoPE w/o SC Ours ViCLIP T-Sim. 13.179 11. 13.613 0.211 0.378 0.402 Sync-C 4.192 5.943 Sync-D 10.411 8.064 Spk-Conf. 0.12 0. Method Only IR Only CD MT (w/o OFT) 6.074 8.027 0.08 Ours ViCLIP 11.931 14.044 9. 14.573 ID-Sim. AQ T-Sim. CLAP 0.692 0.543 0.638 0.674 5.576 6.072 4.449 6.313 0.504 0.471 0.442 0. 0.225 0.287 0.104 0.282 Table 5 Ablation study on Dual-level Disentanglement. Table 6 Ablation study on Multi-Task Progressive Training. quantitative results are presented in Table 5, with qualitative comparison in Fig. 6 (a). Our analysis highlights the distinct contributions of each component: (1)w/o SC: Following Ovi [36], we replace the Structured Caption (SC) with standard unstructured joint caption (i.e., single global description for both target video and audio), the models ability to follow textual instructions is significantly impaired, resulting in the lowest ViCLIP score. More critically, this leads to dramatic increase in speaker confusion, with the Spk-Conf. rate more than tripling from 0.08 to 0.26. This underscores the crucial role of SC in explicitly associating visual attributes and dialogue content with specific subjects in multi-person scenarios. As illustrated in the first row of Fig. 6 (a), without SC, both the visual attributes and content of sub1 and sub2 suffer from severe mismatch. (2)w/o Syn-RoPE: Removing Syn-RoPE, which is designed to bind specific speakers to their corresponding timbres, leads to severe degradation in timbre preservation, as indicated by the sharp drop in the T-Sim. score. The identity-timbre mismatch also negatively impacts lip-sync accuracy (Sync-C/D). As shown in the second row of Fig. 6 (a), without Syn-RoPE, sub1 is erroneously bound to the voice timbre of sub2. Ablation on Multi-Task Progressive Training. We conduct an ablation study on our multi-task progressive training strategy, with the results on the single-person R2AV scenario presented in Table 6. qualitative comparison is provided in Fig. 6 (b) Only IR: Training exclusively with In-pair Reconstruction (IR) leads to severe copy-paste issues, as illustrated in Fig. 6 (b) (the first row). While this results in deceptively high ID-Sim. and T-Sim. scores, the model fails to learn meaningful conditional synthesis, leading to poor text-following ability (ViCLIP) and audio quality (AQ). (2) Only CD: Conversely, training only with Cross-pair Disentanglement (CD) from the start proves too challenging. The model struggles to learn fundamental representations, resulting in very low ID-Sim. and T-Sim. scores, as shown in the second row in Fig. 6 (b). (3) MT (w/o OFT): This experiment validates our progressive training philosophy by attempting to train all tasks (R2AV, RV2AV, RA2V) jointly from scratch without Omni-Task Fine-tuning (OFT). This naive multi-task (MT) approach yields suboptimal performance on the R2AV task, particularly in text-following as indicated by ViCLIP (third row of Fig. 6 (b)). This confirms our hypothesis that when training unified model, it is crucial to first establish strong generative prior on weakly-constrained tasks (like R2AV) before introducing strongly-constrained tasks (like RV2AV/RA2V). Without this progression, the model tends to shortcut the learning process by overfitting to the easier, strongly-constrained tasks, ultimately failing to generalize on the more complex, weakly-constrained generation tasks."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we have presented DreamID-Omni, unified framework for controllable human-centric audiovideo generation. By integrating reference-based generation, editing, and animation into single paradigm, DreamID-Omni addresses the limitations of previous task-specific models. To tackle the critical challenge of multi-person confusion, we introduced Syn-RoPE for signal-level identity-timbre binding and Structured Captioning for semantic-level disentanglement. Furthermore, our proposed Multi-Task Progressive Training strategy effectively harmonizes disparate objectives. Extensive experiments on our new benchmark, IDBenchOmni, demonstrate that DreamID-Omni achieves SOTA performance across multiple tasks."
        },
        {
            "title": "References",
            "content": "[1] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. [2] Liyang Chen, Tianxiang Ma, Jiawei Liu, Bingchuan Li, Zhuowei Chen, Lijie Liu, Xu He, Gen Li, Qian He, and Zhiyong Wu. Humo: Human-centric video generation via collaborative multi-modal conditioning. arXiv preprint arXiv:2509.08519, 2025. [3] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, and Furu Wei. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. 2021. [4] Siyan Chen, Yanfei Chen, Ying Chen, Zhuo Chen, Feng Cheng, Xuyan Chi, Jian Cong, Qinpeng Cui, Qide Dong, Junliang Fan, et al. Seedance 1.5 pro: native audio-visual joint generation foundation model. arXiv preprint arXiv:2512.13507, 2025. [5] Xu Chen, Keke He, Junwei Zhu, Yanhao Ge, Wei Li, and Chengjie Wang. Hifivfs: High fidelity video face swapping. arXiv preprint arXiv:2411.18293, 2024. [6] Zhuowei Chen, Bingchuan Li, Tianxiang Ma, Lijie Liu, Mingcong Liu, Yi Zhang, Gen Li, Xinghui Li, Siyu Zhou, Qian He, and Xinglong Wu. Phantom-data: Towards general subject-consistent video generation dataset. arXiv preprint arXiv:2506.18851, 2025. [7] Gang Cheng, Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Ju Li, Dechao Meng, Jinwei Qi, Penchong Qiao, et al. Wan-animate: Unified character animation and replacement with holistic replication. arXiv preprint arXiv:2509.14055, 2025. [8] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Asian conference on computer vision, pages 251263. Springer, 2016. [9] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [10] Yufan Deng, Xun Guo, Yuanyang Yin, Jacob Zhiyuan Fang, Yiding Yang, Yizhi Wang, Shenghai Yuan, Angtian Wang, Bo Liu, Haibin Huang, et al. Magref: Masked guidance for any-reference video generation. arXiv preprint arXiv:2505.23742, 2025. [11] Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, et al. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens. arXiv preprint arXiv:2407.05407, 2024. [12] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [13] Junmin Gong, Sean Zhao, Sen Wang, Shengyuan Xu, and Joe Guo. Ace-step: step towards music generation foundation model. arXiv preprint arXiv:2506.00045, 2025. [14] Xu Guo, Fulong Ye, Xinghui Li, Pengqi Tu, Pengze Zhang, Qichao Sun, Songtao Zhao, Xiangwang Hou, and Qian He. Dreamid-v:bridging the image-to-video gap for high-fidelity face swapping via diffusion transformer, 2026. [15] Yoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko, Avishai Berkowitz, Daniel Shalem, Daphna Lifschitz, Dudu Moshe, Eitan Porat, et al. Ltx-2: Efficient joint audio-visual foundation model. arXiv preprint arXiv:2601.03233, 2026. [16] Jiangyu Han, Federico Landini, Johan Rohdin, Anna Silnova, Mireia Diez, and Lukáš Burget. Leveraging self-supervised learning for speaker diarization. In Proc. ICASSP, 2025. [17] Akio Hayakawa, Masato Ishii, Takashi Shibuya, and Yuki Mitsufuji. Mmdisco: Multi-modal discriminator-guided cooperative diffusion for joint audio and video generation. arXiv preprint arXiv:2405.17842, 2024. [18] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Zhang. Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024. 12 [19] Xuanhua He, Quande Liu, Zixuan Ye, Weicai Ye, Qiulin Wang, Xintao Wang, Qifeng Chen, Pengfei Wan, Di Zhang, and Kun Gai. Fulldit2: Efficient in-context conditioning for video diffusion transformers. arXiv preprint arXiv:2506.04213, 2025. [20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [21] Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, and Qinglin Lu. Hunyuancustom: multimodal-driven architecture for customized video generation. arXiv preprint arXiv:2505.04512, 2025. [22] Xiaohu Huang, Hao Zhou, Qiangpeng Yang, Shilei Wen, and Kai Han. Jova: Unified multimodal learning for joint video-audio generation. arXiv preprint, 2025. [23] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint arXiv:2501.04698, 2025. [24] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [25] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. [26] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [27] Zhe Kong, Feng Gao, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Xunliang Cai, Guanying Chen, and Wenhan Luo. Let them talk: Audio-driven multi-person conversational video generation. arXiv preprint arXiv:2505.22647, 2025. [28] Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, et al. Openhumanvid: large-scale high-quality dataset for enhancing human-centric video generation. arXiv preprint arXiv:2412.00115, 2024. [29] Zhaoyang Li, Dongjun Qian, Kai Su, Qishuai Diao, Xiangyang Xia, Chang Liu, Wenfei Yang, Tianzhu Zhang, and Zehuan Yuan. Bindweave: Subject-consistent video generation via cross-modal integration. arXiv preprint arXiv:2510.00438, 2025. [30] Sen Liang, Zhentao Yu, Zhengguang Zhou, Teng Hu, Hongmei Wang, Yi Chen, Qin Lin, Yuan Zhou, Xin Li, Qinglin Lu, et al. Omniv2v: Versatile video generation and editing via dynamic content manipulation. arXiv preprint arXiv:2506.01801, 2025. [31] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, Chao Liang, Yuan Zhang, and Jingtuo Liu. Omnihuman-1: Rethinking the scaling-up of one-stage conditioned human animation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1384713858, 2025. [32] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. AudioLDM: Text-to-audio generation with latent diffusion models. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 2145021474. PMLR, 2329 Jul 2023. [33] Haohe Liu, Gael Le Lan, Xinhao Mei, Zhaoheng Ni, Anurag Kumar, Varun Nagaraja, Wenwu Wang, Mark Plumbley, Yangyang Shi, and Vikas Chandra. Syncflow: Toward temporally aligned joint audio-video generation from text. arXiv preprint arXiv:2412.15220, 2024. [34] Kai Liu, Wei Li, Lai Chen, Shengqiong Wu, Yanhao Zheng, Jiayi Ji, Fan Zhou, Rongxin Jiang, Jiebo Luo, Hao Fei, and Tat-Seng Chua. Javisdit: Joint audio-video diffusion transformer with hierarchical spatio-temporal prior synchronization. In arxiv, 2025. [35] Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Gen Li, Siyu Zhou, Qian He, and Xinglong Wu. Phantom: Subject-consistent video generation via cross-modal alignment. arXiv preprint arXiv:2502.11079, 2025. [36] Chetwin Low, Weimin Wang, and Calder Katyal. Ovi: Twin backbone cross-modal fusion for audio-video generation. arXiv preprint arXiv:2510.01284, 2025. 13 [37] Xiangyang Luo, Ye Zhu, Yunfei Liu, Lijian Lin, Cong Wan, Zijian Cai, Shao-Lun Huang, and Yu Li. Canonswap: High-fidelity and consistent video face swapping via canonical space modulation. arXiv preprint arXiv:2507.02691, 2025. [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. [39] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [40] Leigang Qu, Feng Cheng, Ziyan Yang, Qi Zhao, Shanchuan Lin, Yichun Shi, Yicong Li, Wenjie Wang, Tat-Seng Chua, and Lu Jiang. Vincie: Unlocking in-context image editing from video. arXiv preprint arXiv:2506.10941, 2025. [41] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR, 2023. [42] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1021910228, 2023. [43] Hao Shao, Shulun Wang, Yang Zhou, Guanglu Song, Dailan He, Zhuofan Zong, Shuo Qin, Yu Liu, and Hongsheng Li. Vividface: robost and high-fidelity video face swapping framework. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. [44] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [45] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [46] Andros Tjandra, Yi-Chiao Wu, Baishan Guo, John Hoffman, Brian Ellis, Apoorv Vyas, Bowen Shi, Sanyuan Chen, Matt Le, Nick Zacharov, Carleigh Wood, Ann Lee, and Wei-Ning Hsu. Meta audiobox aesthetics: Unified automatic quality assessment for speech, music, and sound. 2025. [47] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [48] Duomin Wang, Wei Zuo, Aojie Li, Ling-Hao Chen, Xinyao Liao, Deyu Zhou, Zixin Yin, Xili Dai, Daxin Jiang, and Gang Yu. Universe-1: Unified audio-video generation via stitching of experts. arXiv preprint arXiv:2509.06155, 2025. [49] Jun Wang, Chunyu Qiang, Yuxin Guo, Yiran Wang, Xijuan Zeng, Chen Zhang, and Pengfei Wan. Klear: Unified multi-task audio-video joint generation. arXiv preprint arXiv:2601.04151, 2026. [50] Kai Wang, Shijian Deng, Jing Shi, Dimitrios Hatzinakos, and Yapeng Tian. Av-dit: Efficient audio-visual diffusion transformer for joint audio and video generation. arXiv preprint arXiv:2406.07686, 2024. [51] Mengchao Wang, Qiang Wang, Fan Jiang, Yaqi Fan, Yunpeng Zhang, Yonggang Qi, Kun Zhao, and Mu Xu. Fantasytalking: Realistic talking portrait generation via coherent motion synthesis. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 98919900, 2025. [52] Runqi Wang, Yang Chen, Sijie Xu, Tianyao He, Wei Zhu, Dejia Song, Nemo Chen, Xu Tang, and Yao Hu. Dynamicface: High-quality and consistent face swapping for image and video using composable 3d facial priors. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1343813447, 2025. [53] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. [54] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animations, 2024. 14 [55] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. [56] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale In IEEE contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. International Conference on Acoustics, Speech and Signal Processing, ICASSP, 2023. [57] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Yao Yao, and Siyu zhu. Hallo: Hierarchical audio-driven visual synthesis for portrait image animation, 2024. [58] Zhengbo Xu, Jie Ma, Ziheng Wang, Zhan Peng, Jun Liang, and Jing Li. End-to-end video character replacement without structural guidance. arXiv preprint arXiv:2601.08587, 2026. [59] Tao Yang, Ruibin Li, Yangming Shi, Yuqi Zhang, Qide Dong, Haoran Cheng, Weiguo Feng, Shilei Wen, Bingyue Peng, and Lei Zhang. Many-for-many: Unify the training of multiple video and image generation and manipulation tasks. arXiv preprint arXiv:2506.01758, 2025. [60] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42104220, 2023. [61] Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, and Wenhan Luo. Unic: Unified in-context video editing. arXiv preprint arXiv:2506.04216, 2025. [62] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyang Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1297812988, 2025. [63] Lei Zhao, Linfeng Feng, Dongxu Ge, Rujin Chen, Fangqiu Yi, Chi Zhang, Xiao-Lei Zhang, and Xuelong Li. Uniform: unified multi-task diffusion transformer for audio-video generation. arXiv preprint arXiv:2502.03897, 2025. [64] Shengkui Zhao, Zexu Pan, and Bin Ma. Clearervoice-studio: Bridging advanced speech processing research and practical deployment. arXiv preprint arXiv:2506.19398, 2025. [65] Yong Zhong, Zhuoyi Yang, Jiayan Teng, Xiaotao Gu, and Chongxuan Li. Concat-id: Towards universal identitypreserving video synthesis. arXiv preprint arXiv:2503.14151, 2025."
        },
        {
            "title": "A Appendix",
            "content": "In the supplementary material, the sections are organized as follows: We provide qualitative comparisons with baselines on RV2AV and RA2V in Sec. A.1. We provide the details of our data construction pipeline in Sec. A.2. We provide the MLLM-based judge prompt in Sec. A.3. We provide more details regarding the user study in Sec. A.4. We provide more qualitative results for R2AV, RV2AV, and RA2V tasks in Sec. A.5. A.1 Comparison Results on RV2AV and RA2V Figs. 4 and 5 compare DreamID-Omni with SOTA methods on RV2AV and RA2V, respectively, where the qualitative results clearly demonstrate our superior performance. A.2 Data Construction Details Our full dataset consists of approximately 1M high-quality audio-video pairs. As illustrated in Fig. 7, our data construction pipeline is categorized into two primary stages: In-pair data construction. We process each video clip to extract its internal references. The reference voice timbre set is created by applying DiariZen [16] for speaker diarization to obtain precise timestamps. Concurrently, the reference identity set is formed by using DWPose [60] to detect and crop face regions from keyframes. Cross-pair data construction. For the audio branch, we construct the reference timbre through multi-stage pipeline. First, DiariZen [16] and Gemini [45] are combined to accurately label speaker segments in multi-person dialogues. Subsequently, CosyVoice [11] is employed to clone clean voice for each speaker, which is then purified using ClearerVoice [64] for final denoising. For the video branch, the reference identity is constructed following the established Phantom-Data [6] pipeline. A.3 MLLM-Based Judge We employ Gemini-2.5-Pro as MLLM-based judge for Speaker Confusion, Fig. 9 presents the system prompt. Figure 7 Data construction pipeline. 16 Figure 8 MLLM system prompt for Structured Caption. A.4 User Study We conduct user study as part of the evaluation on IDBench-Omni. Specifically, we invited 30 professional video creators to serve as evaluators. Users rate each video on seven dimensions on 15 scale, and we average the ratings to obtain the final scores. The user study was carried out in blinded setting. Table 7 indicates that our approach performs strongly across multiple dimensions. 17 Figure 9 MLLM system prompt for speaker confusion detection. Method Phantom VACE Qwen-Image + LTX-2 Qwen-Image + Ovi Wan2.6 Ours Text-Video Alignment ID-Sim. Video Quality Text-Audio Alignment Timbre-Sim. Audio Quality Lip-sync 3.62 3.45 3.32 3.70 3.51 3. 3.55 3.47 3.09 3.05 3.18 3.95 3.35 3.28 3.14 3.64 3.77 3.68 - - 4.18 4.23 3.57 4. - - 2.41 2.41 2.95 3.50 - - 3.73 3.77 4.08 4.23 - - 2.91 3.32 3.12 4. Table 7 User Study with state-of-the-art methods on R2AV. Best results are in bold, second best are underlined. A.5 More Visual Results As shown in Fig. 10-13, we provide more qualitative results of DreamID-Omni on R2AV, RV2AV and RA2V task. 18 Figure 10 More qualitative results on R2AV 19 Figure 11 More qualitative results on R2AV II 20 Figure 12 More qualitative results on RV2AV Figure 13 More qualitative results on RA2V"
        }
    ],
    "affiliations": [
        "Intelligent Creation Lab, ByteDance",
        "Tsinghua University"
    ]
}