{
    "paper_title": "LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing",
    "authors": [
        "Daniel Fein",
        "Sebastian Russo",
        "Violet Xiang",
        "Kabir Jolly",
        "Rafael Rafailov",
        "Nick Haber"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating creative writing generated by large language models (LLMs) remains challenging because open-ended narratives lack ground truths. Without performant automated evaluation methods, off-the-shelf (OTS) language models are employed as zero-shot judges, yet their reliability is unclear in this context. In pursuit of robust evaluation for creative writing, we introduce LitBench, the first standardized benchmark and paired dataset for creative writing verification, comprising a held-out test set of 2,480 debiased, human-labeled story comparisons drawn from Reddit and a 43,827-pair training corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot LLM judges, (ii) train Bradley Terry and generative reward models, and (iii) conduct an online human study to validate reward model rankings on newly LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the strongest off-the-shelf judge, reaching 73% agreement with human preferences; among trained reward models, Bradley-Terry and Generative reward models both attain an accuracy of 78%, outperforming all off-the-shelf judges. An online human study further confirms that our trained reward models consistently align with human preferences in novel LLM-generated stories. We release LitBench and reward models at https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461, providing a vetted resource for reliable, automated evaluation and optimization of creative writing systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 9 6 7 0 0 . 7 0 5 2 : r LitBench: Benchmark and Dataset for Reliable Evaluation of Creative Writing Daniel Fein Sebastian Russo Violet Xiang Kabir Jolly Rafael Rafailov Nick Haber"
        },
        {
            "title": "Abstract",
            "content": "Evaluating creative writing generated by large language models (LLMs) remains challenging because open-ended narratives lack ground truths. Without performant automated evaluation methods, off-the-shelf (OTS) language models are employed as zero-shot judges, yet their reliability is unclear in this context. In pursuit of robust evaluation for creative writing, we introduce LitBench, the first standardized benchmark and paired dataset for creative writing verification, comprising heldout test set of 2,480 debiased, human-labeled story comparisons drawn from Reddit and 43,827-pair training corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot LLM judges, (ii) train BradleyTerry and generative reward models, and (iii) conduct an online human study to validate reward model rankings on newly LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the strongest off-the-shelf judge, reaching 73% agreement with human preferences; among trained reward models, Bradley-Terry and Generative reward models both attain an accuracy of 78%, outperforming all off-the-shelf judges. An online human study further confirms that our trained reward models consistently align with human preferences in novel LLM-generated stories. We release LitBench and reward models here, providing vetted resource for reliable, automated evaluation and optimization of creative-writing systems."
        },
        {
            "title": "Introduction",
            "content": "Automated verification with oracles or learned verifiers has catalyzed rapid progress in math and code generation [Hendrycks et al., 2021, Gao et al., 2024, Jimenez et al., 2023, Pan et al., 2024]. By contrast, creative writing is inherently divergent: given the same prompt, authors may produce different yet equally valid stories. The lack of ground truth labels hinders verification and, consequently, progress in creative writing generation. Evaluation by human experts with structured rubric is reliable, but it is expensive to collect such judgements, particularly at the scale of AIgenerated text [Chakrabarty et al., 2024]. In domains where human where ground truth are usually collected from human raters, LLM judges are often used ([Badshah and Sajjad, 2024]; [Son et al., 2024]). The agreement between LLM judgments and human preferences has been found to be reasonable in the contexts of dialog, helpfulness, and summarization tasks [Zheng et al., 2023]. But, they exhibit biases, such as favoring lengthy text [Wang et al., 2023], and lack of internal consistency [Wei et al., 2025]. Feuer et al. [2025] found that within these tasks, stylistic choices account for the judgments of language models more often than substance. This raises questions about the reliability of judges in the context of creative writing, where form and content are paramount. *Equal contribution. Correspondence: drfein@stanford.edu. Preprint. Under review. We introduce LitBench, the first standardized benchmark of high-quality, pairwise creative writing samples, derived from Reddits r/WritingPrompts. LitBench is designed to both evaluate existing zero-shot judges and enable the development of learned verifiers that better align with human preferences. Then, to study the gap between LLM-judges and trained reward models, we curate dataset of 43k further pairwise examples from r/WritingPrompts. LitBench evaluation reveals that small and open-source LLM-judges fail to evaluate creative writing accurately, but that some leading proprietary models are competitive with trained verifiers. Our investigation of various reward models reveals that generative reward models (GenRMs) are on-par with Bradley-Terry reward models in this domain. While [Mahan et al., 2024] found that training GenRMs with chain-of-thought (CoT) can lead to similar or even improved performance on some preference based benchmarks, such as RewardBench, we find it hinders performance in creative writing verification, even when CoTs are distilled from much stronger out-of-the-box verifier model. Additional human evaluation on LLM-generated stories validates that well-performing reward models on our benchmark can indeed judge creative quality. Our contributions are as follows. benchmark of 2.5k pairwise comparisons of human-written stories, coupled with filtered and labeled training dataset for verifiers consisting of 43k pairwise examples, along with generated rationales. Benchmarking of current approaches of creative writing verification, revealing that the best zero-shot LLM judge (Claude-Sonnet-3.7) underperformed small reward models (1B-7B) trained on our training set, suggesting we can get higher quality reward models at lower cost. Study of GenRMs showing that distilled chain-of-thought degrades performance for creative writing verification. Human evaluation validating that verifier performing well on LitBench can be used to select higher-quality creative writing."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Verification Recently, math and coding benchmarks with ground truth labels have facilitated progress in these domains [Gao et al., 2024, Jimenez et al., 2023]. Cobbe et al. [2021] first used inference-time verification to bootstrap language model performance on GSM8K by ranking candidate solutions from generator. More recently, Costello et al. [2025] and Zelikman et al. [2024] have shown that ground truth pruning of generations and retraining can improve the latent ability to correctly solve math problems. Without ground truth labels, verification is difficult. Reinforcement learning from human feedback (RLHF) has emerged as the dominant paradigm to align model language and behavior with human taste and steer models to follow instructions [Ouyang et al., 2022, Stiennon et al., 2020]. Bai et al. [2022] developed lower-cost method of alignment with human preferences by substituting humans for language models in the feedback process. LLM-judges have been found to agree with human preferences in some contexts Zheng et al. [2023], Liu et al. [2023], though their agreement with humans in the evaluation of creative writing or other forms of artistic expression has not been systematically evaluated. In another attempt to avoid costly human preferences, [Ethayarajh et al., 2022a] released The Stanford Human Preferences (SHP) dataset, leveraging Reddit to distill human preferences for helpfulness using post and comment annotations. Within the genre of creative writing, Chung et al. [2025] point out that having robust reward model for creative writing is difficult due to subjectivity in evaluation. Existing work towards automatic story evaluation often makes use of reference work either using language model or metrics like BLEU and ROGUE scores [Li et al., 2025, Netisopakul and Taoto, 2023]. However, Fan et al. [2018a] notes that in our open-ended generation setting, these are not useful. There have been efforts for evaluation in open-ended settings, for example, in detecting narrative incoherence using structural cues, but story coherence alone does not encapsulate creative qualities which our work is interested in [Alihosseini et al., 2019, Li et al., 2020]. 2 2.2 Creativity in Writing Common usage of creativity lacks precision, encapsulating multiplicity of ideas. The term suggests harmony of integrated elements; it often implies discovery or surprise; and paradoxically, the word can describe both technical excellence and technical incompetence that which is fresh, unshackled, in defiance of convention [Barzun, 1960]. To narrow our scope, we offer Rhodes framework for creativity: the four Ps of creativity, i.e., (1) people, (2) process, (3) press, (4) products [Rhodes, 1961]. This paper is concerned with creative products, in the form of short stories. Among the properties proposed for creative products, interdisciplinary theories have distilled two fundamental criteria: novelty and value [Callan and Foster, 2023]. Our work is, in part, inspired by observing that there exist convergent human preferences for creative products. These preferences are available for display in our cultural institutions. For example, the selections of required readings in MFA program syllabi demonstrate enormous inter-institution uniformity [Manery, 2016]. This extends to many creative domains, where there is popular notion of canon works. Psychology studies have also shown convergent creative preferences, showing that expert writers achieve high agreement when asked to rank poetry and prose. [Amabile, 1982]. Judgement of written language is rooted human emotion (e.g. finding something humorous) and, by extension, in shared human experience. These abilities are not available to language models. What is available to language models, though, is aggregated human preferences for creative products. Below, we present an aggregated collection of creative products, which we hope will further computational methods for creative process. In other words, perhaps aligning models with human creativity amounts to curating great reading list."
        },
        {
            "title": "3 LitBench",
            "content": "LitBench is benchmark for reward models that can judge creative writing quality, coupled with training set which improvements can be made on. Since reward hacking is common issue when using preference-based reward models to improve model capabilities, we carefully construct both our evaluation and training set to ensure quality. We describe the procedure in detail, and demonstrate our curation procedure is indeed helpful. Figure 1: Preprocessing methodology for dataset creation. 3.1 Data Collection We collect writing samples from the r/WritingPrompts subreddit, which has 18.9 Million subscribers. Users write stories in response to writing prompts, and freely engage with posted stories through upvotes or comments. In total, r/WritingPrompts has amassed over one million stories. Such large corpus enables highly selectivedata filtration, leaving data for which we can confidently assume human preferences signal. To collect the data for our benchmark, we use the Reddit API via the praw library. Specifically, we use the search function to collect the 100 top search results for each post collected by [Fan et al., 2018b]. This yields 5,000+ post-ids (individual prompts within the framework of the subreddit). We then construct our test set by filtering out any stories older than 2023, 3 as this data potentially overlaps with our training dataset, and is more likely to have been included in the pretraining of the models we study here. To collect our training dataset, we curate examples from the MIT-licensed euclaise/WritingPrompts_preferences dataset from Hugging Face*, which contains story posts from prior 2023. 3.2 Quality Control We begin constructing our dataset by filtering stories independently. First, to reduce the affect of noise from small up-vote counts, we guarantee that each story has reasonable amount of engagement by filtering out stories with fewer than 10 up-votes. Then, consistent with [Chung et al., 2025], we filter out stories with greater than 2048 tokens to remove excessively long stories. Lastly, we remove all entries with fewer than 50 words, because we find qualitatively that these are not sufficiently long to reflect the genre of creative fiction. To form pairs, we carry out two steps to ensure that true preferences are being captured, and then one step to address length bias. Initially, we exclude pairs with marginal differences in up-votes, filtering out those with an upvote difference less than 25%. Next, following the methodology of [Ethayarajh et al., 2022b], we only create pairs where the higher-upvote story is also published later, mitigating temporal bias from varying exposure durations. Lastly, we find that the resultant dataset has length bias, with 65.25% of chosen responses longer than rejected responses. To address this while preserving length diversity, we construct histogram of length differences (100 buckets) and prune pairs until achieving symmetry balanced proportions where chosen stories are both shorter and longer. This step is represented in Figure 2. The data preparation workflow is summarized in Figure 1.This entire process is performed independently for both our benchmark and training dataset. Figure 2: Length bias mitigation. 3.3 Final Dataset Description LitBench consists of 2,480 pairwise comparisons composed of 3,543 total stories. These stories have an average length of 550 words, and story length is right-skewed, with tail of longer stories. The data is exclusively sourced from after January 2nd, 2023. This guarantees the data have no overlap with our training dataset, as well as enabling true zero-shot evaluation of some current language models with earlier training cut-offs. We find that many of our rejected stories have an upvote-count near our prescribed minimum of 10 upvotes, with long tail of higher rated stories. Our chosen stories have minimum upvote count of 14, due to our decision to prune pairwise examples with an upvote differential of less than 25% of the chosen response. These distributions are shown in 3 The training dataset consists of 50,309 unique stories that are used in 43,827 pairwise-examples. The distribution is similar to the test set with respect to story lengths and upvote distribution, but the stories in the training set come strictly before 2023. The vast majority of stories were posted between *https://huggingface.co/datasets/euclaise/WritingPrompts_preferences 4 Figure 3: Distributions of word count, date, and upvotes for the LitBench testand train-set. 2014 and 2022. We confirm the quality of annotated training set is indeed higher than the original set by comparing reward models trained on them in Section 5. 3.4 Qualitative Analysis of Chosen Samples What is the character of winning writing samples? To determine this qualitatively, we read and annotated 50 pairwise writing samples from LitBench. We present few observations below. Why do stories win? The preferred stories often contain an unexpected twist or surprising humor; we observed many clever punchlines and wordplay. For example, we read about tyrant queen who won over her opposition not by warfare but absurdist politeness, subverting reader expectations. Another told the story of woman and her powerful captor named Decimator. The story played with dark themes, and its humor toed the line between edgy and obscene, amusing us (and Reddit users too!) Why do stories lose? Although some stories were difficult to distinguish, many felt dry and lacked emotional qualities. We found some stories were challenging to finish, due to confusing narratives or strange diction. We were perplexed by science-fiction tale with too many characters: there was an era-model soldier with chip, woman named Gabby, shape-shifting monster, and more too many characters for short story; not to mention, the reader was faced with rapidly shifting point of view. Of note, grammatical errors and narrative incoherence, while present in occasional losing samples, do not generally characterize them."
        },
        {
            "title": "4 Training and Evaluation Protocols",
            "content": "We evaluate various approaches to verification including zero-shot Bradley-Terry discriminative reward models, and generative reward models with and without chain-of-thought generation. [Wei et al., 2022]. Bradley-Terry Discriminative Reward Models We train discriminative reward model using the BradleyTerry (BT) formulation [Bradley and Terry, 1952], where each writing sample in pair is 5 scored independently, and the model is trained to assign higher reward to the preferred sample. Given reward scores rchosen and rrejected, the loss is defined as: LBT = log σ(rchosen rrejected), encouraging separation between better and worse samples. We append linear layer to the basemodels last hidden state and then fine-tune all weights of the combined regression model. Accuracy is calculated as the percentage of cases where rchosen > rrejected. Generative Reward Models Generative reward models have been shown to perform well in math and coding domains, particularly for out of distribution data.([Mahan et al., 2024];[Zhang et al., 2024]). Generative verifiers treat classification as autoregressive generation by conducting supervised finetuning with cross-entropy loss on the predictions of an instruction-tuned model. Chain-of-thought (CoT) can also be incorporated into this process by finetuning chains of thought that precede and describe the prediction that follows. Here, we train two versions of generative reward models (GenRM): (1) GenRM - to predict which single token between and is going to be selected, (2) GenRM-CoT - to reason before selecting preferred story distilled using GPT4.1 generated rationales. At test time, we randomly shuffle the chosen and rejected stories between option and to avoid position bias, GenRMs verdicts were collected at temperature=0 with one sample. Zero-shot LLM Judges Off-the-shelf, LLM judges are presented unlabeled stories and B, and asked for verdict indicating their preference (e.g. or B) between the pairwise samples. In particular, we instructed judges to form explanations prior to verdict generation. To account for the known position bias in LLM judges [Ye et al., 2024], we take the average performance of two sets of pairs, permuting the position of the stories. We selected the LLM-as-judge template prompt, specifying evaluation criteria and output format, by selecting the template with the highest precision in validation set sampled from the training set among five hand-constructed prompts. We chose not to use automatic prompt optimization tools, such as TextGrad ([Yuksekgonul et al., 2025]), as we observed that these methods resulted in poorer prompt performance. For further discussion on prompt optimization, prompt templates and response structure, see 7). We apply the judge methodology to selection of state-of-the-art proprietary and open-source LLMs, and these results are demonstrated as baselines in Figure 6 and 4."
        },
        {
            "title": "5 Results and analysis",
            "content": "This work is motivated by the premise that verification in creative, open-ended domains can be operationalized with learned reward models. We offer LitBench pursuant with this motivation, and defend its utility by: Validating the construction of the dataset by benchmarking trained reward models, and evidencing reward model generality with online studies. Characterizing LLM-based methods to verify human writing, by comparing cross-model performance and analyzing their reasoning text. BT and Generative Reward Models Outperform Zero-Shot LLMs We offer comparative reward model performance in Figure 4. The best BradleyTerry reward model (Llama-8B) fine-tuned on LitBench training set achieves 78% human agreement, marginally surpassing the best generative verifier (GenRM-Qwen). Both GenRM and BT reward models significantly outperform the strongest zero-shot judge (Claude-3.7-Sonnet, 73%). Interestingly, adding chain-of-thought reasoning to GenRMs lowers accuracy to 72%, indicating that explicit sequential reasoning, while beneficial in math and coding tasks, introduces textual noise when judging narrative quality. Zero-shot performance scales unpredictably with backbone size; SoTA OpenAI, Anthropic and Deepseek models sit in the 70% range, while smaller open-source models hover near chance-plus (5660%). These results underscore that targeted preference fine-tuning dominates parameter count for creative-writing evaluation, and that discriminative objectives remain the most reliable choice in this domain. Reasoning Degrades Verdict Accuracy Despite the general success of CoT-based performance bootstrapping (as discussed in Section 2.1), here CoTs actually degraded reward model performance. 6 Figure 4: Trained verifiers outperform zero-shot LLM-judges on LitBench. Claude3.7-Sonnet is the strongest zero-shot model. BT verifiers are competitive with GenRMs, but GenRMs with CoTs perform worse. The sizes of Qwen, Llama and Gemma backbones are 7B, 8B and 12B, respectively. Figure 5: Qualities of explanation text that impact verdict accuracy. To examine this, we computed statistics on explanation text produced by judge models, and correlated these features with verdict accuracy. We present characteristics, inspired by creative writing pedagogy [Sellers, 2021], most predictive of verdict accuracy in Figure 5. Among all models, discussions of the plot are most predictive of correctness (though particularly for Anthropic models) correlated with +14.8% higher correctness among all models. However, most of the explanation text features had minimal relation with subsequent verdict accuracy. Performance Scales Differentially by Reward Model Figure 6 shows the performance improves at different magnitudes as the model size increases across all types of reward models. GemRM-CoT starts at lower performance at lower model sizes for both Llama and Qwen backbone, but steadily improves to 74%. However, GenRM without CoT have no significant improvement across model sizes, suggesting much smaller model (1B or 1.5B) can be used to obtain similar performance. The performance of Bradley-Terry models has notable performance difference due to the varied 7 Figure 6: Human agreement scaling is inconsistent with model size for different types of RMs. backbone, particularly in smaller models (1B/1.5B and 3B). For zero-shot judges, we observe similar effect that performance improves meaningfully as the size increase. Figure 7: Naive upvote pairing and naive timestamp and upvote pairing saturate at lower accuracy than the LitBench training set. Naive timestamp and upvote pairing alone produces length biased verifier. All models are BT RMs fine-tuned on Llama-1B backbone. Validating Data Filtration Methodology We further confirm our curation process by training ablative BT reward models on datasets produced by different filtering strategies and evaluating on the debiased LitBench test-set. We create lightly filtered version of the original training set that only removes pairs containing stories that have less than 10 upvotes and pairs based on upvote difference, resulting in 395k pairs. We also create an unfiltered version paired by timestamp and upvote difference, resulting in 1.03M pairs. We train BT reward models with Llama-3.2-1B backbone on these datasets. Despite having significantly more examples, we find that performance on LitBench without pairing by timestamp saturates at much lower levels (65%). Without our length filtering, we find saturation at 70%, but we also find that the the reward model is length-biased, strongly preferring the longer of the two stories in most cases. These results of this experiment are shown in Figure 7. Human Experiments We generate 64 stories each seeded from 40 LitBench prompts using GPT 4.1 and GPT 4o, and then rank them with our Llama-8B-based Bradley-Terry reward model. In an online human studies with 46 U.S./U.K. crowd-workers (10-13 annotators per pair), we evaluate human agreement with the RM-determined best and worst stories for each prompt. Figure 8 indicates 8 Figure 8: Human preference alignment with reward model on generated writing pairs. that annotators selected the RM-preferred story 57% of the time versus 41% for the rejected story, surpassing the best LLM judge (Claude-3.7-Sonnet) which performs at chance. These results confirm that preference fine-tuning on Reddit labels generalizes to fresh creative-writing prompts, yet the 40% disagreement rate underscores substantial head-room for richer supervision signalssuch as rubric-based feedback or rationale distillationto further align automatic rewards with human literary taste."
        },
        {
            "title": "6 Discussion",
            "content": "This work shows that the strongest off-the-shelf LLM-judges begin approaching the performance of fine-tuned, domain-specific models in creative writing evaluation. Thus, in the absence of costly human preference data, proprietary LLM-judges appear to be viable substitute for trained verifiers. When training data is available, GenRMs are most consistently accurate across across different base models and sizes. Our results also call into question the use of GenRMs trained on chains of thought. Lastly, our human evaluation reveals that performance on LitBench generalizes to evaluation of newly LLM-generated stories, suggesting that future work might make use of strong verifier to improve latent creative writing generation capabilities."
        },
        {
            "title": "7 Limitations",
            "content": "A key limitation of our work stems from the assumption, inherited from [Ethayarajh et al., 2023], that upvotes on Reddit contain information about human preferences. Though we experimentally validate our dataset via human evaluation, it is unclear what other underlying factors may be encoded in upvote information. For example, [Kassaeyan, 2016] report that the decision to upvote post on social networks is at least partially driven by personal and social mechanisms, including individuation, perceived behavioral control, and altruism. Our extension of human preferences to determine writing quality is further complicated by the question of subjectivity in writing evaluation. There is body of work showing how measured features of writing can correlate with human ratings in aggregate [Zedelius et al., 2019, McNamara et al., 2010]. Moreover, authorship on fair writing evaluation in classroom setting establishes precedent for objectivity in writing [Weigle, 2002]. Elam [2023] argues that writing generated artificially renders meaning senseless via representing realities and contexts that do not actually occur within history. In similar way, our verifiers are limited by their removal from legitimate, individual human experiences that ground all creative writing. We acknowledge that no automated quality verifier can fully capture the social value of arbitrary text in real-world contexts, and we regret any implication to the contrary. Our dataset comes from Reddit, which has been reported to demographically skew male, educated, and middle-aged [Duarte, 2025, Agrawal, 2016]. Ultimately, our benchmark and accompanying dataset reflect the consensus preferences of these groups."
        },
        {
            "title": "A Appendix",
            "content": "A.1 LLM-as-judge Raw Results Model AccJan23 Avg. Expl. Len. claude-3-5-haiku claude-3-7-sonnet gpt-4.1 gpt-4.1-mini o4-mini deepseek-v3 deepseek-r gemma-3-12b-it llama-3.1-8b 0.675 0.731 0.702 0.630 0.700 0.700 0.710 0. 0.581 292.4 280.2 202.3 246.7 131.5 167.4 142.8 497.0 332. qwen-2.5-7b"
        },
        {
            "title": "0.599\nTable 1: LLM-as-a-judge evaluation results by model on LitBench.",
            "content": "174.0 A.2 LLM-as-judge Prompt Optimization Motivation. We sought to give out-of-the-box LLM evaluators the best chance of performing accurately on this benchmark. LLM-based judges have been demonstrated to be extremely sensitive to prompt content (CITE). In this experimental setting, prompts enable (a) introduction of criteria for judges to utilize in inferring verdicts, and (b) specification of an output format to ensure easily parsed results. Strategy. There are numerous prompt-optimization libraries that automatically differentiate the prompt text to improve accuracy on given evaluation metric. However, after some experimentation with these, we opted to apply methods of our own design to optimize the prompts, following the approach below. Goal: Select an optimized prompt for each family of models (e.g. Llama). Optimization Method. 1. Hand-construct six template prompts, each introducing different criteria for the judge to use when generating verdict. 2. Standardize output format: request JSON objects from large instruction-tuned models; request plaintext from smaller models. 3. Using midrange model for each family, evaluate each prompt with validation set (n = 500) drawn from the training set to avoid bias. 4. Adopt the prompt, by family, that yields the highest accuracy. 5. Append the standardized output format instruction, depending on the model size and capacity to follow instructions. A.3 Prompt Templates. 1. Writer-ly Criteria Writer-ly Criteria Prompt You're evaluating creative writing responses and B. Compare them based on these dimensions: 10 - Imagery: vivid descriptions and sensory details - Tension: dramatic interest and conflict - Pattern: structural elements and composition - Energy: engaging style and dynamic writing - Insight: meaningful ideas and depth IMPORTANT: Your answer MUST use EXACTLY this format: Reasoning: [brief comparison] Preferred: [A or B] (state which one is better) Example format: Reasoning: Response has stronger imagery and tension. Preferred: 2. Alternative Criteria Alternative Criteria Prompt Evaluate creative writing responses and B. Consider these aspects: - Originality: unique concepts, unexpected elements - Imagery: sensory language and descriptions - Emotional impact: how the writing affects the reader - Coherence: logical flow and narrative structure - Technical skill: language use and style FORMAT REQUIRED: Reasoning: [your evaluation] Preferred: [A or B] 3. Minimal Instruction Minimal Instruction Prompt Compare responses and for creative writing quality. MUST follow this format: Reasoning: [brief analysis] Preferred: [A or B] 4. Reddit-Minimal Reddit Minimal Instruction Prompt You are evaluating two creative writing responses (A and B) to the same writing prompt. Your task is to predict which response would receive more upvotes from the Reddit community. Your verdict MUST follow this exact format: Reasoning: [explain which response would likely get more Reddit upvotes and why] Preferred: [A or B] (the one you predict would get more upvotes) 5. Reddit-Verbose Reddit-Verbose Prompt You are evaluating two creative writing responses (A and B) to the same writing prompt. These responses are similar to those posted on Reddit writing subreddits like r/WritingPrompts. Your task is to predict which response would receive more upvotes from the Reddit community. Reddit users typically upvote creative writing that is engaging, original, well-written, and emotionally resonant. When making your prediction, consider what makes content popular on Reddit: - Originality and uniqueness of ideas - Engaging narrative style and pacing - Emotional impact and relatability - Clever twists or satisfying conclusions - Technical quality of writing This is an experiment to test how well language models can predict human preferences in creative writing as expressed through Reddit's voting system. Your verdict MUST follow this exact format: Reasoning: [explain which response would likely get more Reddit upvotes and why] Preferred: [A or B] (the one you predict would get more upvotes) 6. Reddit-Verbose Permuted Reddit-Verbose Permuted You are tasked with evaluating two creative writing responses (A and B) to the same prompt. Your goal is to predict which response would garner more upvotes from the Reddit community, specifically in writing subreddits like r/WritingPrompts. Consider the following key dimensions for your evaluation: - Creativity and Originality: How unique are the ideas presented? - Narrative Engagement: Is the storytelling captivating and immersive? - Emotional Resonance: Does the piece evoke feelings or relatable experiences? - Surprise and Satisfaction: Are there clever twists or fulfilling conclusions? - Writing Quality: Is the grammar, style, and structure polished? Your output must strictly follow this format: 1. Reasoning: [Explain which response is likely to receive more Reddit upvotes, citing specific strengths and weaknesses.] 2. Preferred: [A or B] Be concise and clear in your assessment, adhering to the format above."
        },
        {
            "title": "B Dataset Licenses and Access",
            "content": "All data is publicly accessible via the SAA-Lab/LitBench collection on Hugging Face. Code to use this dataset is available on GitHub. Our train set content is sourced from euclaise/WritingPrompts_preferences on Hugging Face, which has an MIT-license. In our test set, we release ids of 3.5k Reddit comments from r/WritingPrompts, along with code to rehydrate from the reddit api. We acknowledge that Reddit users retain copyright over their individual comments, and we do not claim ownership or offer any re-licensing of this content. We contacted Reddit in advance of this release to clarify acceptable use under their API terms. As of submission time, we have not received response."
        },
        {
            "title": "C Compute Usage",
            "content": "All training runs and evaluation was done on our internal cluster using node with 128 CPU cores, 8 NVIDIA A40 GPUs each with 48GB of VRAM, and total of 732GB of system RAM. Training verifiers took between 3 hours for 1B-parameter models and up to one day for 8B parameter models. The total compute used, including failed runs, data ablations, and generation for LLM-as-a-judge is estimated at 500 GPU-hours on NVIDIA A40."
        },
        {
            "title": "D Training Hyperparameters",
            "content": "For all training runs, we use an effective batch size of 128 examples, learning rate of 1e-5 with warmup ratio of 10%. We train in bfloat16 and use AdamW as our optimizer."
        },
        {
            "title": "References",
            "content": "Abhinav Agrawal. The user demographics of reddit: The official app. https://medium.com/@sm_ app_intel/the-user-demographics-of-reddit-the-official-app-7e2e18b1e0e1, December 2016. Accessed: 2025-05-06. Danial Alihosseini, Ehsan Montahaei, and Mahdieh Soleymani Baghshah. Jointly measuring diversity and quality in text generation models. In Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation, pages 9098, Minneapolis, MN, 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-2311. URL https://aclanthology. org/W19-2311. Teresa M. Amabile. Social psychology of creativity: consensual assessment technique. Journal of Personality and Social Psychology, 43(5):9971013, 1982. doi: 10.1037/0022-3514.43.5.997. Sher Badshah and Hassan Sajjad. Reference-guided verdict: Llms-as-judges in automatic evaluation of free-form text, 2024. URL https://arxiv.org/abs/2408.09235. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Jacques Barzun. The 221(1325):6974, zine, the-cults-of-research-and-creativity/. cults of 1960. research Harpers MagaURL https://harpers.org/archive/1960/10/ and creativity. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Dominic Callan and Jennifer Foster. How interesting and coherent are the stories generated by large-scale neural language model? comparing human and automatic evaluations of machinegenerated text. Expert Systems, 40(6):e13292, 2023. doi: 10.1111/exsy.13292. URL https: //doi.org/10.1111/exsy.13292. 13 Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan, and Chien-Sheng Wu. Art or artifice? large language models and the false promise of creativity. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, pages 134, 2024. John Joon Young Chung, Vishakh Padmakumar, Melissa Roemmele, Yuqian Sun, and Max Kreminski. Modifying large language model post-training for diverse creative writing, 2025. URL https: //arxiv.org/abs/2503.17126. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Caia Costello, Simon Guo, Anna Goldie, and Azalia Mirhoseini. Think, prune, train, improve: Scaling reasoning without scaling models, 2025. URL https://arxiv.org/abs/2504.18116. Fabio Duarte. Reddit user age, gender, & demographics (2025). https://explodingtopics.com/ blog/reddit-users, May 2025. Accessed: 2025-05-06. Michele Elam. Poetry will not optimize; or, what is literature to ai? American literature, 95(2): 281303, 2023. Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with V-usable information. arXiv preprint arXiv:2110.08420, 2022a. URL https://arxiv.org/ abs/2110.08420. Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with V-usable information. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 59886008. PMLR, 1723 Jul 2022b. Kawin Ethayarajh, Heidi Zhang, Yizhong Wang, and Dan Jurafsky. Stanford human preferences dataset, 2023. Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833, 2018a. URL https://arxiv.org/abs/1805.04833. Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation, 2018b. URL https://arxiv.org/abs/1805.04833. Benjamin Feuer, Micah Goldblum, Teresa Datta, Sanjana Nambiar, Raz Besaleli, Samuel Dooley, Max Cembalest, and John P. Dickerson. Style outweighs substance: Failure modes of llm judges in alignment benchmarking, 2025. URL https://arxiv.org/abs/2409.15268. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Kasra Kassaeyan. Factors Affecting Upvoting Intention on Social Bookmarking Sites. PhD thesis, Luleå tekniska universitet, 2016. Jianing Li, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. On the relation between qualitydiversity evaluation and distribution-fitting goal in text generation. In Proceedings of the 37th International Conference on Machine Learning, pages 59275937. PMLR, 2020. URL https://proceedings. mlr.press/v119/li20h.html. 14 Ruizhe Li, Chiwei Zhu, Benfeng Xu, Xiaorui Wang, and Zhendong Mao. Automated creativity evaluation for large language models: reference-based approach, 2025. URL https://arxiv. org/abs/2504.15784. Yang Liu, Jonas Schneider, Jonathan Raiman, Ian Tenney, Nitish Gupta, Diya Raghu, Douwe Kiela, and Lazaros Polymenakos. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023. URL https://arxiv.org/abs/2303.16634. Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Fränken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024. Rebecca Manery. The Education of the Creative Writing Teacher: Study of Conceptions of Creative Writing Pedagogy in Higher Education. PhD thesis, University of Michigan, 2016. URL https://deepblue.lib.umich.edu/handle/2027.42/133407. Danielle McNamara, Scott Crossley, and Philip McCarthy. Linguistic features of writing quality. Written communication, 27(1):5786, 2010. Ponrudee Netisopakul and Usanisa Taoto. Comparison of evaluation metrics for short story generation. IEEE Access, 11:140253140269, 2023. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139, 2024. Mel Rhodes. An analysis of creativity. Phi Delta Kappan, 42(7):305310, 1961. URL https: //www.jstor.org/stable/20342603. Heather Sellers. The Practice of Creative Writing: ford/St. Martins (Macmillan Learning), New York, NY, 4 edition, 2021. 9781319215958. URL The-Practice-of-Creative-Writing/p/1319215955. BedISBN https://store.macmillanlearning.com/us/product/ Guide for Students. Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, and Seunghyeok Hong. Llm-as-a-judge and reward model: What they can and cannot do, 2024. URL https://arxiv.org/abs/2409. 11239. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. arXiv preprint arXiv:2009.01325, 2020. URL https://arxiv.org/abs/2009.01325. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators, 2023. URL https://arxiv. org/abs/2305.17926. Hui Wei, Shenghua He, Tian Xia, Fei Liu, Andy Wong, Jingyang Lin, and Mei Han. Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates, 2025. URL https://arxiv.org/abs/2408.13006. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. URL https://arxiv.org/abs/2201.11903. Sara Cushing Weigle. Assessing Writing. Cambridge University Press, Cambridge, 2002. doi: 10.1017/CBO9780511732997. Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, Nitesh Chawla, and Xiangliang Zhang. Justice or prejudice? quantifying biases in llm-as-a-judge. arXiv preprint arXiv:2410.02736, 2024. Mert Yuksekgonul et al. Nature, 639:609616, 2025. Optimizing generative ai by backpropagating language model URL https://www.nature.com/articles/ feedback. s41586-025-08661-4. Claire Zedelius, Caitlin Mills, and Jonathan Schooler. Beyond subjective judgments: Predicting evaluations of creative writing from computational linguistic features. Behavior research methods, 51:879894, 2019. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Self-taught reasoner bootstrapping reasoning with reasoning. In Proc. the 36th International Conference on Neural Information Processing Systems, volume 1126, 2024. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. Lianmin Zheng et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023. URL https://arxiv.org/abs/2306.05685."
        }
    ],
    "affiliations": [
        "Stanford University"
    ]
}