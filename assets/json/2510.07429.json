{
    "paper_title": "Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs",
    "authors": [
        "Wang Wei",
        "Tiankai Yang",
        "Hongjie Chen",
        "Yue Zhao",
        "Franck Dernoncourt",
        "Ryan A. Rossi",
        "Hoda Eldardiry"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Efficient use of large language models (LLMs) is critical for deployment at scale: without adaptive routing, systems either overpay for strong models or risk poor performance from weaker ones. Selecting the right LLM for each query is fundamentally an online decision problem: models differ in strengths, prices fluctuate, and users value accuracy and cost differently. Yet most routers are trained offline with labels for all candidate models, an assumption that breaks in deployment, where only the outcome of the chosen model is observed. We bridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach that trains under the same partial-feedback restriction as deployment, while supporting preference-tunable inference: operators can dial the performance/cost trade-off at test time without retraining. Framed as a contextual bandit over prompt features and a user preference vector, our method simulates an online feedback setting during training and adapts its routing decisions to each new prompt, rather than depending on full-information offline supervision. Comprehensive experiments show that our method consistently outperforms strong offline routers by at least 12.46% and the largest LLM by at least 2.45%, and generalizes robustly for unseen tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 9 2 4 7 0 . 0 1 5 2 : r Preprint. Under Review. LEARNING TO ROUTE LLMS FROM BANDIT FEEDBACK: ONE POLICY, MANY TRADE-OFFS Wang Wei1, Tiankai Yang2, Hongjie Chen3, Yue Zhao2, Franck Dernoncourt4, Ryan A. Rossi4, Hoda Eldardiry1 1Virginia Tech, 2University of Southern California, 3Dolby Labs, 4Adobe Research {wangwei718,hdardiry}@vt.edu, {tiankaiy,yzhao010}@usc.edu hongjie.chen@dolby.com, {ryrossi,dernonco}@adobe.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Efficient use of large language models (LLMs) is critical for deployment at scale: without adaptive routing, systems either overpay for strong models or risk poor performance from weaker ones. Selecting the right LLM for each query is fundamentally an online decision problem: models differ in strengths, prices fluctuate, and users value accuracy and cost differently. Yet most routers are trained offline with labels for all candidate models, an assumption that breaks in deployment, where only the outcome of the chosen model is observed. We bridge this gap with BaRP, Bandit-feedback Routing with Preferences approach that trains under the same partial-feedback restriction as deployment, while supporting preferencetunable inference: operators can dial the performancecost trade-off at test time without retraining. Framed as contextual bandit over prompt features and user preference vector, our method simulates an online feedback setting during training and adapts its routing decisions to each new prompt, rather than depending on full-information offline supervision. Comprehensive experiments show that our method consistently outperforms strong offline routers by at least 12.46% and the largest LLM by at least 2.45%, and generalizes robustly for unseen tasks."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) vary substantially in their strengths, weaknesses, and operating costs. No single model dominates across all prompts and tasks, and both pricing and quality change over time. Users and applications also vary in how they prioritize accuracy and cost. At deployment scale, system must therefore decide per query which model to call under performancecost trade-off. common solution is to employ router, learned policy that selects an LLM for each incoming prompt. The challenge is that, once deployed, the router only receives feedback from the model it actually calls: it observes the accuracy and cost of the selected model but learns nothing about the alternatives. This setting, where supervision is restricted to the chosen action, is known as bandit feedback. In contrast, most existing routers are trained offline with labels for all candidate models on every prompt, creating mismatch between training and deployment. Figure 1: Testing score of baselines and BaRP on in-distribution and out-of-distribution tasks. Prior work illustrates two recurring limitations. The first is the reliance on full-information offline supervision, where training requires labels from all candidate LLMs on each prompt. For example, RouterDC (Chen et al., 2024) compares every prompt across multiple LLM outputs, so it cannot Corresponding author. 1 Preprint. Under Review. Table 1: Comparison of routing methods. Full-information Offline Supervision indicates that training requires labels from all candidate LLMs for each prompt. Preference-tunable Inference refers to whether the method can adjust routing at test time to accommodate user-specified performancecost trade-offs without retraining. Method Full-information Offline Supervision Preference-tunable Inference GraphRouter (Feng et al., 2025) RouterDC (Chen et al., 2024) C2MAB-V (Dai et al., 2024) MAR (Zhang et al., 2025) LLM Bandit (Li, 2025) BARP (Ours) Required Required Not required Not required Required Not required No No No No Yes Yes be trained once deployed, when only the chosen models feedback is available. GraphRouter (Feng et al., 2025) faces the same limitation, as it learns graph-structured representations that rely on fullinformation labels. The second limitation is the lack of preference-tunable inference, the ability to adjust routing at test time to reflect user-specified performancecost trade-offs without retraining. For instance, RouterDC (Chen et al., 2024) yields routing policy tied to the trade-off during training, GraphRouter (Feng et al., 2025) supports only three predefined scenarios and is therefore not fully preference-tunable, while our method can shift its choices depending on whether user prioritizes performance or cost. Bandit-style approaches such as C2MAB-V (Dai et al., 2024) and Multi-Armed Router (MAR) (Zhang et al., 2025) avoid full-information supervision but still lack this controllability, and LLM Bandit (Li, 2025) introduces preferences but relies on offline pretraining that assumes full labels. Table 1 summarizes these methods across the two dimensions of supervision and controllability. Additional related work is discussed in Section 5. We propose BARP, Bandit-feedback Routing with Preferences framework that addresses both limitations in unified manner. Our formulation treats routing as multi-objective contextual bandit problem: the router must balance two competing objectives, performance and cost, given only bandit feedback. To capture user preferences, we condition the policy on trade-off vector that specifies the relative importance of performance and cost. The router encodes each prompt together with this vector and outputs distribution over candidate LLMs. The policy is trained with policy-gradient updates regularized by entropy for exploration and stabilized by calibrated cost scaling. This design removes the need for labels from all models during training while allowing operators to adjust performancecost preference at inference without retraining. By aligning training with the partialfeedback setting of deployment and providing controllability at test time, BARP offers practical solution for real-world routing. In summary, our main contributions are as follows: We formulate multi-objective LLM routing as contextual bandit problem in which the router learns from bandit feedback while conditioning on user preference vector that specifies the trade-off between accuracy and cost. This formulation eliminates the need for full supervision across all candidate models and enables per-request controllability. We design routing policy that integrates prompt representations with the preference vector, and train it using entropy-regularized policy gradients with calibrated cost scaling, which encourages exploration and ensures stable optimization under partial feedback. We validate our framework on RouterBench and two question-answering datasets, demonstrating significant performance gains over strong baselines. On in-distribution tasks, our method surpasses the top-performing individual LLM by 3.81% and full-information offline routers by 12.46%. On out-of-distribution tasks, the gains are 2.45% and 25.99% respectively, as shown in Fig. 1."
        },
        {
            "title": "2 APPROACH",
            "content": "We present BARP, Banditfeedback Router with Preferences. The core idea is to treat routing as multi-objective contextual bandit: the router balances performance and cost while observing feed2 Preprint. Under Review. Figure 2: The training pipeline of BARP. The router takes the context (query xt and preference wt) and selects an LLM. It then receives bandit feedback (the score and cost of the chosen LLM only) to calculate reward rt. This reward drives learning algorithm to update the routers parameters, including policy gradient methods like REINFORCE (Sec. 2.3) and classic bandit algorithms such as LinUCB, Thompson Sampling, and ϵ-greedy (Sec. 4.6). back only for the selected model. This section introduces the problem setting (Sec. 2.1), then defines the policy architecture (Sec. 2.2), followed by the objective and learning procedure (Sec. 2.3). The training and inference procedures are provided in Algorithm 1 and Sec. 2.4. For intuition, Fig. 2 illustrates single request in the training process: prompt and user preference enter the router, which selects an LLM, receives bandit feedback, and updates the policy. 2.1 PROBLEM SETTING We formally define the preference-conditioned LLM routing task as contextual bandit problem. In each round t, an agent observes context and selects an arm, receiving reward based on its choice. The Context (st) is tuple st = (xt, wt), where xt is the input prompt and wt = (wq , wc t) is user preference vector on the 1-simplex. Here, wq represents the weight the user places on the performance score, while wc represents the weight on minimizing cost. The set of available LLMs constitutes the Arms (A), or the action space {1, . . . , K}. The router selects an Action (at) from this set, corresponding to choosing single LLM to process the prompt. Upon selection, the router receives scalar Reward (rt) based on bandit feedback for the chosen arm. This reward combines the two objectives according to the users preference: rt = wq qt wc ct, where ct = min (cid:17) . , 1 (cid:16) ct τ (1) where the score qt is task-appropriate metric scaled to [0, 1], τ > 0 caps cost ct so that score and (normalized) cost are on comparable scales. The overall goal is to learn policy that maximizes the expected cumulative reward. 2.2 POLICY ARCHITECTURE The routing policy πθ(a s) is neural network that maps context = (x, w) to probability distribution over the LLMs. The architecture is composed of three sequential components. First, Prompt Encoder, frozen pre-trained sentence transformer h, encodes the prompt into semantic vector h(x) Rde . Second, Preference Encoder, small multilayer perceptron (MLP) ϕ, maps the 2-dimensional preference vector into higher-dimensional embedding ϕ(w) Rdp . Finally, the prompt and preference embeddings are concatenated to form joint representation, = [h(x); ϕ(w)], which is passed to Decision Head, gθ, to produce logits RK. The final policy is obtained by applying softmax function to these logits: πθ(a x, w) = softmax(gθ(z))a = exp(oa) a=1 exp(oa) (cid:80)K . (2) 3 Preprint. Under Review. Algorithm 1 The Training and Inference Procedure for BARP. 1: Inputs: encoder h, preference MLP ϕ, head gθ; cost cap τ ; entropy coeff β. 2: Initialize parameters θ. 3: for = 1 to do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end for 14: Inference (no retraining): given and w, output a(x, w) = arg maxa πθ(a x, w). Receive prompt xt and sample preference wt (random on the 1-simplex). Compute ht h(xt) and ut ϕ(wt); form zt [ht; ut]. ot gθ(zt), Sample at Categorical(πt). Query LLM at; observe qt and ct (only for at). rt wq ct min(ct/τ, 1); Compute batch baseline bt 1 Lt (rt bt) log πt[at] β H(πt). Update θ θ ηθLt. πt softmax(ot). ct. i=1 r(i). qt wc (cid:80)B During training we sample at πθ( xt, wt) to ensure exploration. At inference, we output the deterministic choice: a(x, w) = arg max aA πθ(a x, w), (3) 2.3 OBJECTIVE AND LEARNING ALGORITHM Given the policy in equation 2 and reward in equation 1, the training objective is to find the parameters θ that maximize the expected cumulative reward: max θ J(θ) = EstD,atπθ(st) (cid:104) (cid:88) (cid:105) . rt t=1 (4) where the expectation is taken over the data distribution of contexts, D, and the actions sampled from the policy. We optimize this objective using the REINFORCE policy gradient algorithm, enhanced with baseline for variance reduction and entropy regularization for improved exploration. The per-sample loss function to be minimized is: Lt(θ) = (cid:0)rt bt (cid:1) log πθ(at st) β H(cid:0)πθ( st)(cid:1), (5) where H() is the Shannon entropy of the policy distribution, β 0 is coefficient controlling the strength of the entropy regularization, and bt is baseline used for variance reduction. We employ the mean reward over the current mini-batch as the baseline, defined as: bt = 1 (cid:88) i= r(i) , (6) where is the batch size and r(i) is the reward for the i-th example in the batch. While policy gradient methods are well-suited for training our policy, the formulation of our framework is general and can accommodate other classic learning algorithms, which we explore in our analysis in Sec. 4.6. 2.4 TRAINING AND INFERENCE Training. The policy networks parameters θ are optimized to maximize the expected reward using the REINFORCE algorithm detailed in Sec. 2.3. The training procedure has two key methodological features. First, to train single policy that can serve diverse user preferences, we randomly sample the preference vector wt for each training instance (uniformly on the 1-simplex). Second, while our training utilizes pre-existing benchmark logs with complete information, we simulate bandit environment to match deployment conditions. For each instance, after an action at is sampled from the policy, the supervision signal is restricted to only the outcome of that specific action. The policy gradient updates are performed using the Adam optimizer (Kingma & Ba, 2017). 4 Preprint. Under Review. Inference. At deployment time, the router operates deterministically to exploit the learned policy. Given prompt and user-specified preference vector w, the router selects the action with the highest probability: a(x, w) = arg max aA πθ(a x, w). (7) This allows operators to adjust the performancecost behavior on per-request basis by simply modifying the input vector w, without any need for retraining the model."
        },
        {
            "title": "3.1 DATASETS AND BENCHMARKS",
            "content": "We evaluate on RouterBench (Hu et al., 2024) and two question-answering datasets (Kwiatkowski et al., 2019; Yang et al., 2018), which provide prompt-level logs with multiple candidate LLMs per query, including task identifier, performance score per LLM, and monetary cost per LLM. While the benchmark logs contain scores/costs for all LLMs, our training strictly uses banditconsistent supervision (only the chosen arm is observed). Our experiments evaluate routing across diverse set of widely used large language models, spanning both open-source and proprietary offerings. detailed list and description of these models is provided in Appendix A.3. Tasks and Evaluation. To evaluate our framework, we curate set of eight distinct tasks(the dataset details are in A.4). Our model is trained on mixture of data from five of these tasks: GSM8K (Cobbe et al., 2021), MMLU (Hendrycks et al., 2021), ARC-C (Clark et al., 2018), Winogrande (Sakaguchi et al., 2021), and Natural Questions (NQ) (Kwiatkowski et al., 2019). We create an 80%/20% training/testing split for each of these tasks and combine the training splits to form the full training set. Our evaluation is then conducted in two settings: In-Distribution Evaluation: We test the model on the held-out 20% test sets of the five tasks it was trained on. This measures the models ability to unseen examples from familiar tasks. Out-of-Distribution Generalization: To assess generalization to entirely new tasks, we evaluate the trained model on three benchmarks it has never seen during training: MBPP (Austin et al., 2021), Hellaswag (Zellers et al., 2019), and HotpotQA (Yang et al., 2018). 3.2 BASELINE METHODS We compare our method against representative routers and common-sense baselines: Smallest LLM always routes to the smallest model. Largest LLM always routes to the largest model. RouterDC (Chen et al., 2024) learns dual-contrastive embeddings for queries and models, requires full-information labels. GraphRouter (Feng et al., 2025) learns graph-structured representations over queries, tasks, and models, also requires full labels. 3.3 METRICS Following RouterBench (Hu et al., 2024), we evaluate methods on two axes: Performance score is normalized value in [0, 1] that indicates task success, derived either from exact match accuracy or from GPT-4 ratings for more open-ended tasks. Monetary cost is the estimated API call cost per query in USD. 5 Preprint. Under Review. Table 2: Testing score (%) on in-distribution tasks. The best results are highlighted in bold, and the second-best results are underlined. Methods ARC-C GSM8K MMLU Winogrande Smallest LLM Largest LLM RouterDC GraphRouter Ours 38.78 96.19 91.99 94.18 96. 41.15 65.88 59.68 66.28 64.58 25.43 81.19 60.98 80.20 81.06 52.41 81.93 74.74 46.83 82. NQ 14.95 29.15 31.00 31.60 43.01 Avg 34.54 70.87 63.68 65.42 73. 3."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "Our policy is implemented in PyTorch. We use frozen all-MiniLM-L6-v2 (Wang et al., 2020) as the prompt encoder. The trainable components consist of two small MLPs with ReLU activations: one to encode the preference vector and decision head that produces the final logits over the candidate LLMs. All prompts are tokenized to maximum length of 512. We train our policy for 100 epochs using the Adam optimizer (Kingma & Ba, 2017) with learning rate of 1 104 and batch size of 32. For the REINFORCE algorithm, we set the entropy regularization coefficient β to 0.05. All experiments were conducted on NVIDIA A100 80GB GPUs."
        },
        {
            "title": "4 EXPERIMENTS RESULTS",
            "content": "4.1 PERFORMANCE ON IN-DISTRIBUTION TASKS We first evaluate our method (BARP) against four baselines on in-distribution tasks, with results illustrated in Fig. 1 and reported in Table 2. BARP achieves the strongest trade-off between performance and cost. It delivers the highest average score (73.57%), outperforming the strong, fullinformation routers, RouterDC and GraphRouter, by relative 15.53% and 12.44% respectively. It also establishes new best scores on ARC-C, Winogrande, and NQ. While the Largest LLM baseline is competitive on some tasks, its high monetary cost makes it impractical. In contrast, BARP achieves performance level comparable to the strongest baselines while maintaining cost significantly lower than other learned routers, establishing its superior efficiency on familiar tasks. 4.2 GENERALIZATION ABILITY TO NEW TASKS To assess robustness, we further evaluate the trained models on out-of-distribution tasks they have never seen during training. As shown in Table 3, the full-information routers (RouterDC and GraphRouter) struggle to generalize, with their performance dropping sharply on MBPP and HpQA. In contrast, BARP demonstrates robust generalization, achieving the highest average score (66.08%) among all methods. It obtains the best score on HpQA, where other learned methods fail, and maintains performance competitive with the much more expensive Largest LLM baseline on MBPP and Hellaswag. This confirms that BARP preserves its superiority not only on in-distribution tasks but also when adapting to unseen tasks, confirming its robustness and practical deployment value. Table 3: Testing score (%) on out-of-distribution tasks. The best results are highlighted in bold, and the second-best results are underlined. Methods MBPP Hellaswag HpQA Smallest LLM Largest LLM RouterDC GraphRouter Ours 34.43 68. 39.06 64.29 68.24 27.49 40.93 25.00 22.20 46.29 25.48 83.96 69.60 70.87 83.72 Avg 29.14 64.50 44.55 52.45 66.08 Preprint. Under Review. Table 4: Comparison of methods in terms of Score, Cost, and the corresponding percentage Score improvements and Cost reduction rate, relative to the state-of-the-art method(GraphRouter (Feng et al., 2025)). The score and cost are averaged over in-distribution and out-of-distribution tasks. The cost is multiplied by 103 for readability. Method Score Score Improvement (%) Monetary Cost Cost Reduction (%) Smallest LLM Largest LLM RouterDC GraphRouter BARP (Ours) 32.52 68.48 56.51 60.56 70.76 -46.30 13.08 -6.69 0 16.84 0.05 3.29 0.79 0.94 0. 94.68 -250.00 15.96 0 50."
        },
        {
            "title": "4.3 OVERALL PERFORMANCE AND COST-EFFECTIVENESS",
            "content": "Finally, to provide holistic measure of performance and cost across all evaluation settings, we summarize the results by averaging across all eight tasks in Table 4. This view confirms that BARP provides the best balance of performance and cost. Compared to GraphRouter, the strongest offline baseline, our method improves the overall average score by 16.84% while simultaneously reducing monetary cost by 50.00%. In contrast, RouterDC provides significant cost reduction but at the expense of lower score, while the Largest LLM improves accuracy by 13.08% but at the expense of more than threefold increase in cost. These results validate that our preference-conditioned, bandit-feedback approach is not only more effective but also substantially more cost-efficient than methods relying on full-information supervision. 4.4 SENSITIVITY ANALYSIS 4.4.1 ANALYSIS OF THE PREFERENCE TRADE-OFF We analyze the sensitivity of our router to the user-specified preference, which provides direct trade-off between performance and cost. Recall from Sec. 2.1 that the preference vector is = (wq, wc), where wc is the weight on cost reduction. In this analysis, we vary the cost weight wc [0, 1] (with wq = 1 wc) at inference time and observe its effect on the routers behavior. Figure 3 reports the effects of varying wc on both performance score and monetary cost across tasks. As shown in Figure 3a, smaller values of the cost weight wc (e.g., 0.2) lead the router to prioritize performance, achieving strong scores across most tasks. For example, ARC-C remains above 95% score and Winogrande above 80%. However, as wc increases, the average score gradually declines, most noticeably on NQ and MMLU, reflecting the routers increasing preference for cheaper models even when they are less performant. Conversely, Figure 3b shows that larger wc values yield significant reductions in average cost. The cost decreases steadily from $0.074 at wc = 0.2 to only $0.015 at wc = 0.8, with consistent reductions across all tasks. This demonstrates that the router effectively adapts its selections in line with the user-specified trade-off, choosing lower-cost models when cost is emphasized. Overall, these results confirm that the preference vector provides clear and interpretable control knob for operators. Lower cost weights favor high performance at higher cost, while higher cost weights sacrifice some performance to achieve substantial cost savings. This allows the behavior of BARP to be tuned to specific deployment requirements without any need for retraining. 4.4. IMPACT OF PROMPT ENCODER CHOICE We analyze how the choice of the frozen prompt encoder affects routing performance. more powerful encoder might provide better representations, but could also be less efficient. We compare three widely-used pre-trained models of increasing size: all-MiniLM-L6-v2 (Wang et al., 2020) (384-dim), BERT-base-uncased (Devlin et al., 2018) (768-dim), and E5-large-v2 (Wang et al., 2022) (1024-dim). For each, we train only the preference encoder and the routers decision head using the same bandit-feedback procedure. 7 Preprint. Under Review. (a) Effects of ωc on Performance Score (b) Effects of ωc on Cost Figure 3: Effects of ωc The results, averaged over all in-distribution tasks with balanced preference (wq = wc = 0.5), are presented in Table 5. The all-MiniLM-L6-v2 encoder achieves the highest average score (0.7432), establishing the best trade-off between performance and model size. While the much larger E5-largev2 performs comparably on score, its increased representational capacity does not translate into significant routing advantage. Conversely, BERT-base-uncased yields noticeably lower score, suggesting its representations are less effective for this task. Prompt Encoder Avg Score Avg Monetary Cost MiniLM-L6-v2 BERT-base-uncased E5-large-v2 These findings provide valuable insight: our routing framework does not require large, resource-intensive model for prompt encoding. compact, efficient sentence-level encoder like MiniLM is sufficient to capture the necessary seTable 5: Comparison of different frozen prompt enmantics for routing. We hypothesize this coders. Results are averaged across in-distribution tasks is because modern sentence transformusing balanced preference (wq = wc = 0.5) during ers, trained with contrastive objectives, inference. The Avg Cost refers to the monetary cost of produce more suitable sentence-level emthe LLMs selected by the router, not the encoders cost. beddings for this task than older models like BERT, which were trained on token-level objectives. Given its superior performance and smaller footprint, we use all-MiniLM-L6-v2 as the default encoder for all other experiments in this paper. 0.7432 0.7226 0.7418 0.0007 0.0005 0.0007 4.5 IMPACT OF DECISION HEAD ARCHITECTURE We also analyze the impact of the decision heads architecture, which sits atop the frozen encoder and maps the context representation to action logits. We evaluate three types of decision heads mentioned in Sec. 2.2: simple linear layer, parameter-efficient bilinear model, and two-layer MLP with ReLU non-linearity. As shown in Table 6, the MLP head achieves the best overall performance, reaching the highest average score (0.7432). The linear head is competitive, suggesting that direct mapping is strong baseline, while the bilinear head underperforms. These results provide key insight: while simple linear mapping is effective, the added representational capacity of the MLPs non-linearity is beneficial for learning the complex function that maps prompt and user preference to the optimal LLM choice. Head Type Avg Score Avg Monetary Cost Linear Bilinear MLP 0.7396 0.7317 0.7432 0.0007 0.0006 0.0007 Table 6: Comparison of different decision head architectures. Results are averaged across indistribution tasks, using balanced preference (wq = wc = 0.5) during inference. We hypothesize that the bilinear head, despite being designed to model interactions, may be more difficult to optimize with the sparse signal provided by bandit feedback, potentially leading to its lower score. Given that the MLP head provides the best performance without significant increase in complexity, we adopt it as the default architecture for all other experiments. 8 Preprint. Under Review."
        },
        {
            "title": "4.6 ANALYSIS OF LEARNING ALGORITHMS",
            "content": "A key feature of our framework is its flexibility to accommodate different learning algorithms. To analyze the impact of the algorithm choice, we compare our policy-gradient approach (REINFORCE) with several classic contextual bandit strategies: Linear Thompson Sampling (LinTS) (Agrawal & Goyal, 2014), LinUCB (Li et al., 2010), and ϵ-greedy. To ensure fair comparison, all algorithms operate on the identical context representation (the concatenated prompt and preference embeddings). As is standard, the classic bandit strategies are paired with linear model to map these features to rewards, while our main approach uses non-linear MLP. Table 7 presents the results evaluated with balanced preference (wq = wc = 0.5). The policygradient method (REINFORCE) achieves substantially higher average score, demonstrating superior performance on this task. Notably, bandit approaches tend to yield slightly lower costs, suggesting that their conservative exploration might favor cheaper models at the expense of performance. The primary finding from this analysis is that the routing decision function is inherently complex. While classic bandit algorithms provide strong baseline, their performance is limited by the linear assumptions they make about the relationship between context and reward. The significant performance gap suggests that an algorithm capable of learning non-linear policy, such as REINFORCE paired with an MLP, is necessary to effectively model the nuances of LLM routing."
        },
        {
            "title": "5 ADDITIONAL RELATED WORK",
            "content": "Method Avg Score Avg Monetary Cost LinTS LinUCB ϵ-greedy REINFORCE 0.6430 0.6166 0.6556 0.7432 0.00046 0.00044 0.00056 0.00070 Table 7: Comparison between REINFORCE and classical bandit algorithms. Results are averaged across in-distribution tasks, using balanced preference (wq = wc = 0.5) during inference. LLM routing. With the rapid growth of LLMs, there is increasing interest in routing strategies that decide which model to query for each input. Early approaches often rely on ensembles, such as majority voting over all outputs, or static heuristics like always choosing the largest or smallest model. Recently, learning-based routers have been proposed. GraphRouter (Feng et al., 2025) learns graph-structured representations across prompts, tasks, and models to exploit relational information. RouterDC (Chen et al., 2024) introduces dual-contrastive objectives for aligning query and model embeddings. Other efforts design mixture-of-experts systems that dynamically allocate queries across LLMs (Varangot-Reille et al., 2025). Contextual bandits. The contextual bandit framework (Langford & Zhang, 2007) formalizes decision-making under partial feedback: at each round, the learner observes context, selects an action, and only receives feedback for that action. Classical bandit algorithms include LinUCB (Li et al., 2010), which uses optimism in linear reward models; Thompson Sampling (Agrawal & Goyal, 2014), which maintains posterior over reward parameters; and ϵ-greedy strategies, which trade off exploration and exploitation through randomization. Beyond linear settings, neural contextual bandits extend these ideas with non-linear function approximators (Riquelme et al., 2018; Zhou et al., 2020). Bandit methods have been applied to recommendation (Li et al., 2010), online advertising (Chapelle & Li, 2011), and adaptive experiment design."
        },
        {
            "title": "6 CONCLUSION AND DISCUSSION",
            "content": "In this work, we address the challenge of efficiently selecting the optimal LLM from pool of candidates to balance performance and cost. We formalize this task as preference-conditioned contextual bandit problem and introduce BARP. Trained with policy gradients on bandit feedback, our method learns to map users prompt and specific performance-cost preference to the most suitable LLM. Extensive experiments demonstrate that BARP significantly outperforms both topperforming individual LLMs and strong offline routers on both in-distribution and out-of-distribution tasks. Crucially, we show that the preference vector provides an effective and interpretable control mechanism, allowing operators to tune the routers behavior at inference time without retraining. 9 Preprint. Under Review. We acknowledge several limitations for future improvement. Our method trains on static, offline logs, which is practical but differs from truly online setting where router could learn continuously from live feedback. We only consider performance and monetary cost, while real deployments may require richer, possibly task-specific preferences and constraints (e.g., latency). The current contextual bandit formulation also models routing as single-step decision, making it well-suited for many tasks but not explicitly designed for multi-turn, conversational scenarios. Furthermore, our experiments focused on pool of general-purpose LLMs, and future work could explore routing to highly specialized, domain-expert models."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "The primary goal of this research is to improve the efficiency of using large language models, direction with positive societal impact. By enabling users to select smaller, less expensive models when appropriate without significant loss in performance, our work contributes to reducing the overall energy consumption and carbon footprint associated with deploying these powerful but resource-intensive technologies. Our work relies on existing, publicly available benchmark datasets and pre-trained language models. We do not use any private or personally identifiable information, and our research does not involve human subjects. As with any system that improves the efficiency of LLM routing, there is possibility of misuse, for example, in routing to optimize spam or misinformation generation. However, we believe the risk is limited and outweighed by the benefits of more efficient LLM routing."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We are committed to ensuring the reproducibility of our work. To this end, all code required to replicate our experiments, including scripts for training, evaluation, and all analyses presented in the paper, will be made publicly available upon publication in an open-source repository. Datasets. Our primary experiments are conducted on the publicly available benchmarks. We will provide scripts to download and process all data into the format required by our codebase. Our data splits are deterministic, based on the random seed provided in our code. Models and Hyperparameters. The specific pre-trained models used for the prompt encoder and the full list of candidate LLMs are detailed in the appendix. All critical hyperparameters, including learning rates, batch sizes, and regularization coefficients, are reported in 3.4. Our code is implemented in PyTorch. Computational Resources. All experiments were conducted on single NVIDIA A100 GPU with 80GB of memory. The training for our main model completes in approximately 2-3 hours. The code for the classic bandit baselines is also provided and runs efficiently on standard CPU."
        },
        {
            "title": "REFERENCES",
            "content": "Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs, 2014. URL https://arxiv.org/abs/1209.3352. Anthropic. Model card and evaluations for claude models, 2023. URL https://www-cdn.anthropic.com/files/4zrzovbb/website/ bd2a28d2535bfb0494cc8e2a3bf135d2e7523226.pdf. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger vances in Neural Information Processing Systems, volume 24. Curran Associates, In (eds.), AdInc., Preprint. Under Review. 2011. file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf. URL https://proceedings.neurips.cc/paper_files/paper/2011/ Shuhao Chen, Weisen Jiang, Baijiong Lin, James T. Kwok, and Yu Zhang. Routerdc: Querybased router by dual contrastive learning for assembling large language models, 2024. URL https://arxiv.org/abs/2409.19886. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Xiangxiang Dai, Jin Li, Xutong Liu, Anqi Yu, and John C. S. Lui. Cost-effective online multillm selection with versatile reward models, 2024. URL https://arxiv.org/abs/2405. 16587. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805. Tao Feng, Yanzhen Shen, and Jiaxuan You. Graphrouter: graph-based router for llm selections, 2025. URL https://arxiv.org/abs/2410.03834. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2021. Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin, Gaurav Ranganath, Kurt Keutzer, and Shriyash Kaustubh Upadhyay. Routerbench: benchmark for multi-llm routing system, 2024. URL https://arxiv.org/abs/2403.12031. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. URL https: //arxiv.org/abs/2310.06825. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. URL https://arxiv.org/abs/1412.6980. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. doi: 10.1162/tacl 00276. URL https://aclanthology.org/Q19-1026/. John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits In J. Platt, D. Koller, Y. Singer, and S. Roweis (eds.), AdInc., URL https://proceedings.neurips.cc/paper_files/paper/2007/ with side information. vances in Neural Information Processing Systems, volume 20. Curran Associates, 2007. file/4b04a686b0ad13dce35fa99fa4161c65-Paper.pdf. 11 Preprint. Under Review. Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, WWW 10, pp. 661670. ACM, April 2010. doi: 10.1145/1772690.1772758. URL http://dx.doi.org/10.1145/1772690.1772758. Yang Li. Llm bandit: Cost-efficient llm generation via preference-conditioned dynamic routing, 2025. URL https://arxiv.org/abs/2502.02743. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical comparison of bayesian deep networks for thompson sampling, 2018. URL https: //arxiv.org/abs/1802.09127. Baptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024. URL https://arxiv.org/abs/2308.12950. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 2021. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Clovis Varangot-Reille, Christophe Bouvard, Antoine Gourru, Mathieu Ciancone, Marion Schaeffer, and Francois Jacquenet. Doing more with less: survey on routing strategies for resource optimisation in large language model-based systems, 2025. URL https://arxiv.org/abs/ 2502.00409. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022. Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers, 2020. URL https://arxiv.org/abs/2002.10957. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Fung, Yining Yin, and Lida Mou. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. Qwen An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yunyang Wan, Yuqi Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu, Shanghaoran Quan, and Zekun Wang. Qwen2.5 technical report. ArXiv, abs/2412.15115, 2024. URL https://api. semanticscholar.org/CorpusID:274859421. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question In Conference on Empirical Methods in Natural Language Processing (EMNLP), answering. 2018. 12 Preprint. Under Review. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng Li, Jiangcheng Zhu, Jianqun Chen, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Bowen Zhang, Gang Wang, Qi Chen, and Anton van den Hengel. How do we select right LLM for each query?, 2025. URL https://openreview.net/forum?id=AfA3qNY0Fq. Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration, 2020. URL https://arxiv.org/abs/1911.04462. 13 Preprint. Under Review."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 NOTATION Table 8: Summary of notations. Symbol Description , wc Problem Formulation xt wt wq st at qt ct ct rt Total number of candidate LLMs (actions). The set of actions {1, . . . , K}. The time step or round index. The input prompt at round t. The user preference vector (wq The weights for performance score and cost, respectively. The context (state) at round t, defined as the tuple (xt, wt). The action (chosen LLM) at round t. The performance score of the chosen LLMs output, qt [0, 1]. The monetary cost of using the chosen LLM. The normalized monetary cost, min(ct/τ, 1). The scalar reward at round t. The underlying data distribution of contexts. ) at round t. , wc Policy and Learning θ πθ(as) h() ϕ() gθ() J(θ) Lt(θ) bt H() β τ The trainable parameters of the policy network. The policy; probability of selecting action given context s. The frozen prompt encoder function. The preference encoder (MLP) function. The concatenated context representation [h(x); ϕ(w)]. The decision head of the policy network. The vector of logits produced by the decision head. The optimal action selected at inference time (via argmax). The expected cumulative reward objective function. The policy gradient loss function at round t. The reward baseline (batch-mean reward). The batch size used during training. The Shannon entropy function. The entropy regularization coefficient. The cost scaling and capping hyperparameter. A.2 ADDITIONAL RESULTS Table 9: Testing score (%) of each candidate LLM on in-distribution tasks. Candidate LLM ARC-C GSM8K MMLU Winogrande Avg WizardLM/WizardLM-13B-V1.2 claude-instant-v1 claude-v1 claude-v2 gpt-3.5-turbo-1106 gpt-4-1106-preview meta/code-llama-instruct-34b-chat meta/llama-2-70b-chat mistralai/mistral-7b-chat mistralai/mixtral-8x7b-chat zero-one-ai/Yi-34B-Chat 61.02 80.27 86.87 86.87 83.06 96.19 37.35 73.40 38.78 83.20 86.12 50.63 62.72 65.08 66.26 60.48 65.88 45.66 52.30 41.15 51.90 54.81 44.65 59.64 65.72 62.81 64.71 81.19 0.48 2.68 25.43 63.51 65. 50.75 61.96 65.98 66.06 57.93 81.93 38.44 48.22 52.41 55.25 62.90 51.76 66.15 70.91 70.50 66.55 81.30 30.48 44.15 39.44 63.47 67.42 14 Preprint. Under Review. Table 10: Testing score (%) of each candidate LLM on out-of-distribution tasks. Candidate LLM MBPP Hellaswag Avg WizardLM/WizardLM-13B-V1.2 claude-instant-v1 claude-v1 claude-v2 gpt-3.5-turbo-1106 gpt-4-1106-preview meta/code-llama-instruct-34b-chat meta/llama-2-70b-chat mistralai/mistral-7b-chat mistralai/mixtral-8x7b-chat zero-one-ai/Yi-34B-Chat 37.00 60.42 59.72 64.17 65.34 68.62 51.76 33.02 34.43 54.10 38.64 33.38 58.51 56.85 62.42 58.66 83.96 20.82 52.59 25.48 41.69 74. 35.19 59.47 58.29 63.30 62.00 76.29 36.29 42.81 29.96 47.90 56.45 A.3 CANDIDATE LLMS For tasks from RouterBench (Hu et al., 2024), we have candidate LLMs as follows: (i) WizardLM13B-V1.2 (Xu et al., 2023) is fine-tuned instruction-following model from the WizardLM series; (ii) Claude-instant-v1 is lightweight model from Anthropic optimized for speed; (iii) Claudev1 is Anthropics first-generation flagship model; (iv) Claude-v2 (Anthropic, 2023) is an improved successor with stronger reasoning ability; (v) GPT-3.5-turbo-1106 is OpenAIs production-grade model designed for efficiency and broad coverage; (vi) GPT-4-1106-preview (OpenAI et al., 2023) is OpenAIs most capable general-purpose model at the time of release; (vii) Code Llama Instruct34B-Chat (Rozi`ere et al., 2024) is code-specialized instruction-tuned model; (viii) Llama-2-70BChat (Touvron et al., 2023) is general conversational model trained with reinforcement learning from human feedback; (ix) Mistral-7B-Chat (Jiang et al., 2023) is an efficient chat-optimized model from Mistral AI; (x) Mixtral-8x7B-Chat (Jiang et al., 2024) is Mistrals mixture-of-experts model offering higher throughput; and (xi) Yi-34B-Chat (Young et al., 2024) is large-scale bilingual chat model with strong performance in both English and Chinese. For NQ and HpQA datasets, the candidate LLMs consist of Llama-3.1-8b-instruct (Grattafiori et al., 2024), Llama-3.1-70b-instruct (Grattafiori et al., 2024)2, mistral-7b-instruct-v0.3 (Jiang et al., 2023), qwen2.5-7b-instruct (Yang et al., 2024), gemma-2-27b-it (Team et al., 2024), mixtral-8x22binstruct-v0.1 (Jiang et al., 2024). A.4 DATASET DETAILS GSM8K (Cobbe et al., 2021): dataset of diverse grade school math word problems, testing models ability to perform multi-step mathematical reasoning. MMLU (Hendrycks et al., 2021): benchmark that measures the knowledge acquired by models during pretraining and evaluates models in zero-shot and few-shot settings across 57 tasks, testing both knowledge and reasoning on different fields of human knowledge. ARC-C (Clark et al., 2018): rigorous question answering dataset, ARC-Challenge includes complex, different grade-school level questions that require reasoning beyond simple retrieval, testing the true comprehension capabilities of models. Arc Challenge dataset contains those that both retrieval and co-occurrence method fail to answer correctly) Winogrande (Sakaguchi et al., 2021): large-scale and increased harness dataset inspired by the original Winograd Schema Challenge(WSC) tests models on their ability to resolve pronoun ambiguity and their ability to understand the context with commonsense knowledge. NQ (Kwiatkowski et al., 2019): comprehensive collection of real user queries submitted to Google Search, with answers sourced from Wikipedia by expert annotators. MBPP (Austin et al., 2021): The benchmark is designed to be solvable by entry-level programmers, covering programming fundamentals, standard library functionality, etc. Each problem comprises task description, code solution, and 3 automated test cases. 15 Preprint. Under Review. Hellaswag (Zellers et al., 2019): This dataset challenges models to pick the best ending choice for given sentence. It uses Adversarial Filtering(AF) to create Goldilocks zone of complexity, wherein generations are largely nonsensical to humans but always make models struggle. HpQA (Yang et al., 2018): This dataset is designed for question answering and features natural, multi-hop questions. It provides strong supervision for supporting facts, enabling the development of more explainable question answering systems. A.5 USE OF LLMS The LLMs role was strictly writing and editing assistant, used to augment and refine the work. The primary uses of the LLM included: Refining Prose and Tone: Improving the clarity, flow, and academic tone of sentences and paragraphs across all sections. Ensuring Consistency: Cross-referencing the manuscript to identify and correct inconsistencies in terminology, notation, and quantitative claims between the text and tables. All scientific contributions, including the core ideas, experimental design, analysis, and final claims, were conceived and executed by the authors. The LLM served as tool to help articulate these contributions more effectively."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Dolby Labs",
        "University of Southern California",
        "Virginia Tech"
    ]
}