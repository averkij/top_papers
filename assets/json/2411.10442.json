{
    "paper_title": "Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization",
    "authors": [
        "Weiyun Wang",
        "Zhe Chen",
        "Wenhai Wang",
        "Yue Cao",
        "Yangzhou Liu",
        "Zhangwei Gao",
        "Jinguo Zhu",
        "Xizhou Zhu",
        "Lewei Lu",
        "Yu Qiao",
        "Jifeng Dai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing open-source multimodal large language models (MLLMs) generally follow a training process involving pre-training and supervised fine-tuning. However, these models suffer from distribution shifts, which limit their multimodal reasoning, particularly in the Chain-of-Thought (CoT) performance. To address this, we introduce a preference optimization (PO) process to enhance the multimodal reasoning capabilities of MLLMs. Specifically, (1) on the data side, we design an automated preference data construction pipeline to create MMPR, a high-quality, large-scale multimodal reasoning preference dataset. and (2) on the model side, we explore integrating PO with MLLMs, developing a simple yet effective method, termed Mixed Preference Optimization (MPO), which boosts multimodal CoT performance. Our approach demonstrates improved performance across multiple benchmarks, particularly in multimodal reasoning tasks. Notably, our model, InternVL2-8B-MPO, achieves an accuracy of 67.0 on MathVista, outperforming InternVL2-8B by 8.7 points and achieving performance comparable to the 10x larger InternVL2-76B. We hope this study could inspire further advancements in MLLMs. Code, data, and model shall be publicly released."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 1 2 4 4 0 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Enhancing the Reasoning Ability of Multimodal Large Language Models\nvia Mixed Preference Optimization",
            "content": "Weiyun Wang2,1, Zhe Chen3,1, Wenhai Wang4,1, Yue Cao3,1, Yangzhou Liu3,1, Zhangwei Gao1, Jinguo Zhu1, Xizhou Zhu5,1, Lewei Lu6, Yu Qiao1, Jifeng Dai5,1(cid:66) 1OpenGVLab, Shanghai AI Laboratory, 2Fudan University, 3Nanjing University, 4The Chinese University of Hong Kong, 5Tsinghua University, 6SenseTime Research Project Page"
        },
        {
            "title": "Abstract",
            "content": "Existing open-source multimodal large language models (MLLMs) generally follow training process involving pretraining and supervised fine-tuning. However, these models suffer from distribution shifts, which limit their multimodal reasoning, particularly in the Chain-of-Thought (CoT) performance. To address this, we introduce preference optimization (PO) process to enhance the multimodal reasoning capabilities of MLLMs. Specifically, (1) on the data side, we design an automated preference data construction pipeline to create MMPR, high-quality, large-scale multimodal reasoning preference dataset; and (2) on the model side, we explore integrating PO with MLLMs, developing simple yet effective method, termed Mixed Preference Optimization (MPO), which boosts multimodal CoT performance. Our approach demonstrates improved performance across multiple benchmarks, particularly in multimodal reasoning tasks. Notably, our model, InternVL2-8B-MPO, achieves an accuracy of 67.0 on MathVista, outperforming InternVL28B by 8.7 points and achieving performance comparable to the 10 larger InternVL2-76B. We hope this study could inspire further advancements in MLLMs. Code, data, and model shall be publicly released. 1. Introduction With the remarkable success of large language models (LLMs) [1, 5, 10, 11, 26, 89, 92, 93] in the field of natural language processing, the training paradigm comprising pre-training and supervised fine-tuning (SFT) have also swept the multimodal field, becoming the primary choice for the research and development of multimodal large language models (MLLMs). Benefiting from the large-scale (cid:66) Corresponding Author: daijifeng@tsinghua.edu.cn Figure 1. Open-source model performance on MathVista. The Xand Y-axes represent the accuracy evaluated with direct-answer responses and CoT responses, respectively. The bubble size is positively correlated with the number of model parameters. The values in parentheses indicate the performance gap between CoT and direct-answer responses. Notably, most open-source models perform worse when answering with CoT. pre-training corpora [43, 48, 80, 90, 99, 114] and highquality SFT data [20, 24, 53, 55, 98], series of open-source MLLMs [6, 20, 44, 46, 52, 96, 98, 105] exhibit strong performance across various domain and tasks, some even achieving results comparable to commercial models such as GPT-4o [70] and Gemini [78, 88]. However, open-source MLLMs still exhibit limited reasoning capabilities. As shown in Figure 1, InternVL2-8B [20] achieves score of 58.3 on MathVista [61], benchmark for multimodal reasoning, when using direct answers but drops to 56.8 with Chain-of-Thought (CoT) reasoning, indicating that CoT reasoning actually reduces its performance. This decline is commonly observed across opensource MLLMs [20, 44, 96, 105]. We attribute this phenomenon primarily to distribution shift introduced by the 1 SFT loss. Specifically, SFT relies on teacher forcing, where the model is trained to predict the next token based on previous ground-truth tokens. However, during inference, models must predict each token based on their own prior outputs, leading to distribution shift between training and inference. Since the direct-answer approach requires only brief responses, while CoT reasoning involves generating long rationale, the distribution shift problem becomes more severe during CoT. This results in models performing worse with CoT reasoning compared to direct-answer responses. To address the limitations of CoT reasoning in MLLMs, we draw inspiration from recent NLP approaches [42, 74, 103] that use Preference Optimization (PO) techniques to align model outputs with desired reasoning patterns. Specifically, methods like Direct Preference Optimization (DPO) [76] allow models to learn from preference signals to generate responses that better align with user requirements, offering the foundation for Reinforcement Learning from Human Feedback (RLHF). While RLHF has been explored for MLLMs primarily to reduce hallucinations [18, 85, 106], its application for enhancing multimodal reasoning remains under-explored. Building on these insights, we conduct systematic study on using PO to strengthen the multimodal reasoning capabilities of MLLMs."
        },
        {
            "title": "Enhancing the multimodal",
            "content": "reasoning abilities of (1) MLLMs through PO presents several challenges: Limited multimodal reasoning preference data and high annotation cost. Existing multimodal preference datasets [47, 85, 106, 107, 111] primarily address hallucination issues and focus on natural images and perception data, lacking scientific images and reasoning data. Annotating these types of data requires human annotators to carefully compare the given reasoning processes, making it both time-consuming and costly. (2) Lack of open-source methods for improving multimodal reasoning via PO. Although previous works have explored fine-tuning MLLMs using feedback from various sources, these models typically exhibit performance gains on hallucination benchmarks, with little enhancement in general reasoning abilities. Thus, leveraging PO to improve multimodal reasoning capabilities remains largely under-explored. This work addresses these challenges from both the data (1) On the data side, we design an and model sides. automated preference data construction pipeline to create MMPR, high-quality, large-scale multimodal reasoning preference dataset. (2) On the model side, we explore various PO methods with MLLMs, introducing simple yet effective method, termed Mixed Preference Optimization (MPO), which boosts multimodal CoT performance without the requirement for reward model. pipeline for samples with clear ground truth. In DropoutNTP, the responses generated by InternVL2-8B are considered as positive samples. For given chosen response, we truncate it by half and then prompt InternVL2-8B to complete the remaining portion of the truncated answer without access to the image input. This generated completion serves as the rejected answer for the paired sample. Experimental results in Section 5.2 demonstrate that this straightforward method achieves comparable performance in reducing hallucinations compared to the divide-and-conquer method proposed in RLAIF-V [107]. In the correctnessbased pipeline, multiple solutions to each question are sampled from InternVL2-8B. Solutions matching the ground truth answer are used as chosen responses, while those that do not are used as rejected responses. Additionally, we propose the MPO method. The key insight behind this algorithm is that an effective PO process should enable the model to learn the relative preference between pairs of responses, the absolute quality of individual responses, and the process for generating preferred responses. Compared to previous multimodal PO methods [47, 75, 85, 106, 107, 111], our approach excels in the following aspects: (1) Efficient automated data construction pipeline: Our pipeline enables high-quality pref- (2) Effectiveerence pair generation at controlled cost. ness across diverse domains: Models fine-tuned with our data and approach show superior performance across reasoning, question-answering, and hallucination benchmarks. (3) Improvements over SoTA settings: Our results are based on InternVL2-8B, one of the leading open-source MLLMs, further highlighting the potential of our method. In summary, our main contributions are as follows: (1) We propose an efficient preference data construction pipeline. Based on this pipeline, we create MMPR, high-quality, large-scale multimodal reasoning preference dataset containing approximately 3 million samples. (2) We introduce MPO, an effective PO algorithm designed to improve the reasoning abilities of MLLMs. The resulting model, InternVL2-8B-MPO, exhibits enhanced multimodal reasoning ability and fewer hallucinations compared to its baseline model (i.e., InternVL2-8B). (3) We conduct extensive experiments to explore practical approaches for improving multimodal reasoning via PO. Results show that PO significantly improves reasoning abilities over SFT. Notably, the proposed InternVL28B-MPO achieves an accuracy of 67.0 on MathVista [61], outperforming InternVL2-8B by 8.7 points and achieving performance comparable to the 10 larger InternVL2-76B. 2. Related Work Specifically, we propose continuation-based pipeline called Dropout Next Token Prediction (DropoutNTP) for samples lacking clear ground truth and correctness-based Multimodal Large Language Models. With advancements in LLMs, significant progress has also been made in MLLMs. To leverage the abilities of pre-trained LLMs [5, 2 11, 26] and Vision Foundation Models (VFMs) [19, 77], series of works [20, 45, 46, 53, 56, 96, 99, 100] employ connector to align their latent space, achieving promising performance at controllable cost. Besides, another series of works [2, 26, 91, 97] extend pre-trained LLMs with additional fusion layers for vision features, reducing the number of visual tokens required by LLMs while introducing extra training costs. Recently, there have been explorations into vision encoder-free architectures [7, 50, 62, 87, 101], which consists of single transformer model that jointly processes visual and textual information without separate encoder. In addition to exploring model architectures, recent works [27, 48, 55, 98, 104, 109] also try to construct high-quality training data to improve multimodal reasoning abilities. Despite these advancements, MLLMs typically rely on training paradigm comprising pre-training and supervised fine-tuning, which suffers from the curve of distribution shift and exhibits limited multimodal reasoning abilities. In this work, we conduct systematic study on using preference optimization to enhance the multimodal reasoning ability of MLLMs. Preference Optimization. Preference optimization (PO) is crucial technique for advancing LLMs and MLLMs. Specifically, Reinforcement Learning from Human Feedback (RLHF) uses human preferences as reward signal to fine-tune models, aligning them with human preferences. InstructGPT [72] employs reward model as proxy for human preferences and maximizes this reward via the PPO algorithm [81], improving the models ability to follow user intent and become more helpful, honest, and harmless (3H). PPO-Max [94, 112] carefully explores the implementation details of PPO, proposing more stable version of the algorithm. Additionally, DPO [76] proposes an efficient PO algorithm based on the Bradley-Terry model [9], removing the need for an explicit reward model. Subsequent works [4, 21, 25, 28, 32, 42, 54] have further analyzed and refined this method from various perspectives. In natural language processing, series of works [42, 74] have explored how to leverage PO to enhance reasoning ability. In the multimodal field, however, most methods [47, 85, 106, 107, 111] primarily focus on reducing hallucination, leaving the potential for PO to improve multimodal reasoning ability under-explored. This work demonstrates that PO not only mitigates hallucinations but also strengthens multimodal reasoning abilities, highlighting its broader applicability in MLLM development. 3. Scalable Multimodal Preference Dataset"
        },
        {
            "title": "Generation",
            "content": "To address the scarcity of multimodal preference data, we introduce scalable data construction pipeline. Based on this pipeline, we construct million-level MultiModal PReference dataset (MMPR). 3.1. Data Engine Definition. Each data sample in our MMPR consists of an image I, an instruction , chosen response yc Yp, and rejected response yr Yn, where yc is preferable to yr. The image sets and instruction sets are collected from existing datasets. Yp and Yn represent the positive and negative response set, respectively. Given certain image and instruction x, we sample the candidate response from an initial instruction model M0 as follows: M0(y x, I), (1) where M0(y x, I) represents the response distribution of M0 conditioned on image and instruction x. For instructions with clear ground truths, the model is prompted to first provide the reasoning process and then give the final answer in the format like Final Answer: ***. Responses matching the ground truth answer constitute the positive set Yp, while those that do not match make up the negative set Yn. Additionally, responses that fail to provide clear final answer are also merged into Yn. Given these responses labeled as positive or negative, we build the preference pairs by selecting chosen response yc from Yp and negative response yr from Yn. For instructions without clear ground truths, we propose simple yet effective method: Dropout Next-Token Prediction (Dropout NTP). Specifically, we directly consider all responses generated from equation 1 as positive set Yp. To generate the negative set Yn, we sample response from Yp and drop the last half of this response. The model is required to complete the remained response as follows: yj M0(yj x, y<j), (2) where y<j and yj is the remained part and truncated part of y, respectively. yj is the completion of y<j without the image input. The original response = [y<j, yj] serves as the chosen response yc and the completed response = [y<j, yj] serves as the rejected response yr. It is worth noting that while the responses generated by M0 may not be perfect, the completions generated without the image input will introduce more hallucinations than those generated with the image input. Therefore, the partial order relationship between and holds true. Compared with previous methods, our data engine is as effective as the more complex divide-and-conquer method proposed in RLAIF-V [107] (see the experimental results in Section 5.2.2), while more efficient. Taking data generation for M3CoT as an example, our pipeline incurs token cost of 571.2 per preference pair, compared to 992.7 tokens for the divide-and-conquer approach used in RLAIF-V. Thus, the cost of our pipeline is only 57.5% of that of RLAIF-V. 3 Figure 2. Data examples in MMPR. For instructions with clear ground truths, we propose correctness-based pipeline, which samples multiple solutions and considers those with correct answers as chosen responses and those with incorrect answers as rejected responses. For instructions without clear ground truths, we propose DropoutNTP to generate rejected responses. Differences between the chosen and rejected responses are emphasized in italicized text. Red highlights incorrect responses. 3.2. Multimodal Preference Dataset Task Dataset Dataset Statistics. Using this pipeline, we build largescale multimodal preference dataset, MMPR. Data examples are presented in Figure 2. See more examples in the Appendix. This dataset comprises approximately 750K samples without clear ground truths and 2.5M samples with clear ground truths. For samples without clear ground truths, each instruction averages 25.0 tokens, while the chosen and rejected responses average 211.4 and 171.2 tokens, respectively. The longest chosen and rejected responses consist of 1,342 and 1,642 tokens, respectively, whereas the shortest chosen and rejected responses contain 20 and 17 tokens, respectively. For samples with clear ground truths, the average instruction length is 79.5 tokens, with the chosen and rejected responses averaging 300.0 and 350.5 tokens, respectively. The longest chosen and rejected responses are composed of 2,018 and 4,097 tokens, while the shortest responses contain 32 and 33 tokens, respectively. Data Source. As shown in Table 1, to ensure the diversity of instructions and images, we collect samples from diverse domains, including general visual question answering (VQA) [29, 34, 59, 63], science [16, 39, 60], chart [13, 37, 64], mathematics [12, 27, 38, 51, 58, 82], OCR [8, 33, 66, 68, 83], and document [22]. Notably, when constructing open-ended samples, we collect instructions General VQA VQAv2 [29], GQA [34], OKVQA [63], IconQA [59] Science Chart AI2D [39], ScienceQA [60], M3CoT [16] ChartQA [64], DVQA [37], MapQA [13] GeoQA+ [12], CLEVR-Math [51], Geometry3K [58], GEOS [82], GeomVerse [38], Geo170K [27] OCRVQA [68], InfoVQA [66], TextVQA [83], STVQA [8], SROIE [33] DocVQA [65] Mathematics OCR Document Table 1. Datasets used to build our preference dataset. We collect images and instructions from various tasks to ensure the diversity of our dataset. from all the data sources mentioned above and prompt the model to answer the original question without additional requirements. On the other side, when building samples through the correctness-based pipeline, we exclude questions from general VQA and document sources, as verifying the correctness of the generated answers using heuristic rules is challenging for datasets in these domains. For example, the ground truths in VQAv2 [29] consist of single word or phrase, which may lead to false-negative responses when the model outputs complete sentence or synonym as the final answer. Such false-negative responses can negatively impact training effectiveness. 4 4. Improved Multimodal Large Language"
        },
        {
            "title": "Model with Preference Optimization",
            "content": "requiring the model to differentiate the absolute quality of individual responses. The loss terms are given by: To enhance the multimodal reasoning capabilities of MLLMs, we propose mixed preference optimization (MPO), method that blends supervised fine-tuning (SFT) loss with various preference optimization losses to enhance training effectiveness. Additionally, we investigate different Chain-of-Thought (CoT) approaches with multimodal input to improve reasoning performance. 4.1. Mixed Preference Optimization We observed that when MLLMs are trained on large-scale preference datasets using direct preference optimization (DPO), they might fail to generate reasonable rationales and produce gibberish. This phenomenon aligns with the analysis presented in Smaug [73]. To address this issue, we introduce the MPO in this work, aiming to learn the relative preference between pairs of responses, the absolute quality of individual responses, and the process for generating preferred responses. Training Objective. MPO is defined as combination of preference loss Lp, quality loss Lq, and generation loss Lg, which can be formulated as follows: = wpLp + wqLq + wgLg, (3) where represents the weight assigned to each loss component. In this work, we empirically compare different variants of preference loss [4, 14, 21, 32, 36, 54, 67, 69, 76, 102]. Based on the experimental results, we use DPO [76] as our preference loss and BCO [36] as our quality loss. Preference Loss. The DPO [76] serves as the preference loss to enable the model to learn the relative preference between chosen and rejected responses. DPO eliminates the requirement of training an explicit reward model based on the assumption of the Bradley-Terry model [9] and optimizes the following loss function: (cid:18) Lp = log σ β log πθ (yc x) π0 (yc x) β log (cid:19) πθ (yr x) π0 (yr x) , (4) where β is the KL penalty coefficient, and x, yc, and yr are user query, chosen response, and rejected response, respectively. The policy model πθ is initialized from model π0. Quality Loss. The BCO loss [36] is employed as the quality loss, which helps the model to understand the absolute quality of individual responses. This algorithm trains binary classifier, where the logit serves as reward and effectively maps the chosen response to 1 and the rejected response to 0. The loss function is defined as: L+ = log σ (cid:18) β log πθ (yc x) π0 (yc x) (cid:19) δ , (6) = log σ (cid:18) (cid:18) β log πθ (yr x) π0 (yr x) (cid:19)(cid:19) δ , (7) where δ represents the reward shift, calculated as the moving average of previous rewards to stabilize training. Generation Loss. The SFT loss is used as the generation loss to help the model learn the generation process of preferred responses. The loss function is defined as: Lg = log πθ (yc x) yc . (8) 4.2. Chain-of-Thought with Multimodal Input During the data sampling process, we require the model to provide detailed CoT reasoning process instead of directly answering the final answer. For most samples, we sample the responses using the prompt shown in the bottom case of Figure 2, which requires the model to perform step-bystep analysis. Considering that multimodal models involve non-textual inputs, we further introduce the following CoT methods: (1) Background Knowledge-based CoT: The model first introduces relevant background knowledge related to the problem or image, followed by reasoning steps and the final answer. This approach is applied to samples from the science domain. (2) Visual Content-based CoT: The model begins by analyzing the visual contents in the image, then proceeds with reasoning and the final answer. This method is used for samples from chart, OCR, and document domains. (3) Grounded CoT: The model generates text response while simultaneously linking all referenced objects in the response to corresponding regions in the image. This approach is applied to general VQA domain samples. Responses generated by these above CoT methods are mixed with those sampled using the prompt shown in the bottom case of Figure 2. These approaches not only effectively integrate multimodal information into the reasoning process but also enhance data diversity. Furthermore, including the background knowledge and visual contents at the start of responses also improves the quality of the negative responses generated by DropoutNTP, preventing significant quality gap between positive and negative samples that could reduce training effectiveness. 5. Experiments Lq = L+ + , (5) 5.1. Main Results where L+ represent the loss for chosen and rejected responses, respectively. They are calculated independently, and In this section, we compare our InternVL2-8B-MPO with leading MLLMs on multimodal reasoning [16, 61, 95],"
        },
        {
            "title": "Hallucination Evaluation",
            "content": "M3CoT MathVista MathVision MMVet LLaVA-Bench POPE CRPE MMHalBench Gemini-1.5-Pro [78] GPT-4o [71] GPT-4o-Mini [71] LLaVA-1.5-13B [52] Qwen2-VL-7B [96] MiniCPM-V-2-6-8B [105] LLaVA-OneVision-7B [44] InternVL2-26B [20] InternVL2-40B [20] InternVL2-76B [20] InternVL2-Pro [20] InternVL2-8B [20] InternVL2-8B-MPO (ours) - 64.3 61.9 39.5 57.8 56.0 52. 58.2 63.6 65.4 65.6 59.3 79.2 Closed-Source Models 19.2 30.4 27.3 - 69.1 66.9 Open-Source Models 11.1 21.1 23.4 18. 36.3 60.6 57.4 51."
        },
        {
            "title": "InternVL Models",
            "content": "23.4 21.4 23.7 18.8 20.4 25.7 62.1 65.5 65.7 69.4 54.2 56.2 63.9 63.8 52.4 27.6 58.2 60.6 63.2 59.4 63.7 67.2 66.3 58.3 67.0 - 97.6 95. 70.7 67.7 83.4 79.9 92.3 100.5 99.3 99.5 73.2 76.7 - 86.9 85.1 85.9 88.1 87.3 88.4 88.0 88.4 89.0 88.2 86.9 88.1 - 76.6 73. 55.6 74.4 75.2 73.7 75.6 77.3 77.8 77.6 75.0 75.4 - 4.0 3.6 2.4 3.4 3.6 3.1 3.7 3.9 3.8 3.7 3.3 3.5 Table 2. Results on 8 multimodal benchmarks. We report the overall score of MM-Vet and LLaVA-Bench evaluated by GPT-4-Turbo. Our InternVL2-8B-MPO demonstrates superior performance compared to InternVL2-8B across multimodal reasoning, VQA, and hallucination evaluation benchmarks. Noteably, our model even achieves reasoning performance comparable to the 10 larger InternVL2-76B. complex Visual Question Answering (VQA) [53, 108], and hallucination evaluation [49, 85, 98] tasks. Benchmarks. For the multimodal reasoning task, we evaluate our model on three benchmarks: (1) M3CoT [16], comprehensive benchmark designed to evaluate the mul- (2) Mathtimodal CoT reasoning abilities of models. Vista [61], widely-used benchmark for evaluating multimodal mathematical reasoning capabilities. (3) MathVision [95], which collects evaluation data from real math competitions and presents greater challenge compared to MathVista. We report accuracy for these benchmarks. For the complex VQA task, we evaluate our model on two benchmarks: (1) MM-Vet [108], which evaluates the models ability to engage in visual conversations across diverse range of tasks. (2) LLaVA-Bench [53], commonlyused benchmark for assessing multimodal conversation, detailed description, and complex reasoning capabilities with open-ended questions. Both benchmarks use GPT-4 to evaluate the correctness and helpfulness of responses. We report the overall score for these benchmarks. For the hallucination evaluation task, we evaluate our model on three benchmarks: (1) POPE [49], which measures the hallucination level of object existence using Yes/No questions. We report the F1 score for this bench- (2) CRPE [98], which measures the hallucination mark. level of the relation between objects using multiple-choice questions. We report accuracy for this benchmark. (3) MMHal-Bench [85], which consists of open-ended questions where GPT-4 compares model outputs to human responses, assessing hallucination rate and informativeness. We report the overall score for this benchmark. Results. As shown in Table 2, our InternVL2-8B-MPO achieves superior performance across all benchmarks, particularly excelling in multimodal reasoning tasks. On the MathVista benchmark, our model achieves an accuracy of 67.0%, outperforming InternVL2-8B by 8.7 points and achieving performance comparable to the 10 larger InternVL2-76B. On the MathVision benchmark, our model achieves an accuracy of 25.7%, establishing new stateof-the-art performance among open-source models. These results demonstrate the effectiveness of our preference optimization approach in enhancing multimodal reasoning capabilities. Additionally, on the POPE benchmark, our model exhibits 1.2-point improvement over InterVL2-8B, demonstrating the effectiveness of the perception data contained in our MMPR dataset to mitigate hallucinations. Furthermore, our model also shows superior performance compared to the InternVL2-8B on complex VQA benchmarks, indicating that the general abilities of our model are also improved, benefiting from enhanced reasoning abilities and mitigated hallucinations. 5.2. Ablation Study In this section, we present ablation studies to analyze the effects of preference optimization and SFT on multimodal reasoning abilities. Additionally, we compare our proposed DropoutNTP method with the divide-and-conquer approach from RLAIF-V [107], demonstrating the effectiveness of our approach. Furthermore, we conduct extensive experiments to analyze the effects of different preference opti6 Model Name Setting M3CoT MathVista MMVet POPE InternVL2-8B InternVL2-8B-SFT InternVL2-8B-MPO Direct CoT Direct CoT Direct CoT 59.3 57.0 63.9 67.8 77.2 79. 58.3 56.8 62.7 64.2 64.5 67.0 54.2 54.7 54.7 53.8 55.1 56. 86.9 82.9 86.5 84.0 87.0 88.1 Table 3. Results of models trained with SFT and MPO. The SFT training data consists of the chosen responses from the preference pairs used in MPO training. In the Direct setting, the model is prompted to provide the answer directly, while in the CoT setting, the model is instructed to answer with detailed rationales. mization algorithms. We also present analysis of the effects on text-only performance."
        },
        {
            "title": "Method",
            "content": "Object HalBench MM HalBench Resp. () Ment. () Score Hall. () InternVL2-8B RLAIF-V [107] DropoutNTP (ours) 18.4 7.3 7.6 8.7 3.9 4.1 3.3 3.5 3.6 40.6 32.3 31. Table 4. Comparison of DropoutNTP and the divide-andconquer approach from RLAIF-V. We replace negative samples in RLAIF-V with the responses generated using DropoutNTP. method requires the model to generate only single continuation for each sample, while RLAIF-V requires the model to decompose the response into atomic claims and then verify each one individually. Therefore, our method is more efficient. quantitative analysis is provided in Section 3.1. 5.2.1. Comparison between MPO and SFT 5.2.3. Effects of optimization algorithms To compare the impact of MPO and SFT on improving multimodal reasoning ability, we use the chosen responses in MMPR as SFT data to fine-tune InternVL2-8B. As shown in Table 3, the results indicate that the model trained with MPO consistently outperforms that trained with SFT across all benchmarks. For example, the MPO-trained model achieves score of 79.2 on the multimodal reasoning benchmark M3CoT, surpassing its SFT counterpart by 11.4 points. Furthermore, the MPO-trained model also performs better on the general benchmark (MMVet) and the hallucination benchmark (POPE). Notably, the SFT-trained model performs worse with CoT responses than with directanswer responses on MMVet and POPE, demonstrating that SFT alone is insufficient to enhance multimodal CoT abilities. These results demonstrate that while SFT provides moderate improvement, preference optimization is more effective in improving the overall performance of the model. 5.2.2. Comparison with RLAIF-V Here, we compare our proposed Dropout Next-Token Prediction (Dropout NTP) method with the divide-and-conquer approach from RLAIF-V [107]. To ensure fair comparison, we use the same prompts and chosen responses as in RLAIF-V and replace the rejected responses with those generated by continuation without image input. Following RLAIF-V, we report the hallucination rates in responselevel (Resp.) and mention-level (Ment.) for Object HalBench [79] and overall score and hallucination rates (Hall.) for MMHal-Bench [85]. As shown in Table 4, the model trained with our data achieves performance comparable to that of the model trained with RLAIF-V, demonstrating the effectiveness of our method. Specifically, the response-level hallucination rate of the model trained with our data on Object HalBench is 7.6, compared to 7.3 for its counterpart. Besides, this model achieves score of 3.6 on the MMHalBench, compared to 3.5 for its counterpart. Note that our Here, we empirically compare the effectiveness of different optimization algorithms, including (1) DPO [76], which directly fine-tunes the model on an offline preference dataset without explicitly constructing reward function. (2) RSO [54], which applies hinge loss on the normalized likelihood instead of the sigmoid loss used in DPO. (3) IPO [4], which introduces modified loss function to address overfitting in DPO by averaging log-likelihoods and controlling the gap between chosen and rejected completions via beta parameter. (4) cDPO [69], which is modification of the DPO loss that accounts for potential label noise in preference data. (5) RobustDPO [21], which provides an unbiased estimate of the DPO loss designed to handle preference noise in data. Similar to cDPO, it assumes that labels are noisy with certain probability. (6) BCO [36], which introduces binary classifier trained to output logits used as reward values. (7) SPPO [102], which iteratively pushes chosen rewards toward 1/2 and rejected rewards toward -1/2 to approximate Nash equilibrium, aiming to reduce data sparsity issues. (8) AOT [67], which applies Distributional Preference Alignment via Optimal Transport. (9) TR-DPO [28], which adds synchronization between the model and reference model every few steps to mitigate overfitting during DPO training. (10) ORPO [32], reference model-free preference optimization algorithm that uses log odds ratio penalty appended to the NLL loss, allowing for preference-aligned fine-tuning without an additional preference alignment phase. For all algorithms, we set the learning rate to 5e-6 and use the hyper-parameters suggested in their corresponding paper. Additionally, we extend these algorithms with SFT loss to analyze its impact. The SFT model trained with the chosen responses of the reasoning preference data is also included as baseline. Notably, most current benchmarks lack corresponding in-distribution training samples, and the data distribution of our MMPR may differ from that of these benchmarks. This 7 Figure 3. Results of models trained with different preference optimization algorithms on M3CoT. The algorithm extended with the SFT loss is called X+ for brevity. For instance, DPO+ denotes the combination of DPO loss and SFT loss. Setting MMLU Gaokao TriviaQA NQ C3 Race-h BBH GSM8K Math TheoremQA IFEval HumanEval MBPP Average Baseline SFT MPO 73.2 71.8 71.0 75.0 74.4 74.8 62.0 63.7 64.2 28.1 94.2 28.2 94.3 29.3 94.2 90.8 90.6 90.6 72.7 72.1 71. 75.6 75.5 75.0 39.5 40.1 40.4 15.6 15.8 20.8 52.3 53.6 56.4 69.5 68.3 68.9 58.8 58.0 61. 62.1 62.0 63.0 Table 5. Results on text-only benchmarks. The model fine-tuned through MPO exhibits superior overall performance on text-only tasks compared to the baseline model and its SFT counterpart, particularly on TheoremQA and IFEval. discrepancy can introduce additional variability when analyzing the impact of different optimization algorithms on training results. Therefore, we use the training and validation sets of M3CoT [16] for ablation studies. The visualization results are illustrated in Figure 3 and the numerical results are presented in Table 6 and 7. We can observe that almost all preference optimization methods outperform their SFT counterpart in both the Direct and CoT settings. However, DPO and its variants struggle to enhance the CoT reasoning abilities of the model as the resulting models exhibit trivial or no improvement when answering with CoT reasoning responses compared to directanswer responses. On the other hand, when combining SFT Loss with these DPO variants, all algorithms are able to improve the models CoT reasoning abilities, demonstrating that the SFT loss is key component for enhancing CoT reasoning abilities. Additionally, models trained with TR-DPO, DPO variant that updates the reference model every few steps, perform much worse when using CoT reasoning compared to direct-answer responses. Similarly, the model trained with ODPO, reference-model-free method, achieves worse overall performance compared to other methods extended with SFT Loss. These results indicate that the reference model constraint on policy updates is crucial for enhancing overall reasoning abilities, and the reference model should remain frozen during training. Notably, models trained with DPO+ and BCO+ exhibit the best CoT performance among existing algorithms. Therefore, we use DPO and BCO as the preference loss and quality loss. The resulting algorithm (i.e., MPO) further improves the overall performance. 5.3. Effects on text-only performance We evaluate the text-only performance of our models on series of benchmarks [3, 15, 17, 23, 30, 31, 35, 40, 41, 84, 86, 110, 113] and report the average performance across them. As shown in Table 5, although our MMPR dataset does not include any text-only data, the MPO-trained model achieves superior average performance on these benchmarks compared to the baseline model. The most significant improvements are observed on TheoremQA and IFEval. Specifically, our model trained with MPO achieves an accuracy of 20.8 on TheoremQA, benchmark consisting of complex science problems, outperforming the baseline model by 5.2 points and the SFT counterpart by 5.0 points. Additionally, since our dataset considers responses that fail to follow instructions as negative samples when constructing data using our correctness-based pipeline, our model also exhibits enhanced instruction-following abilities on IFEval, outperforming the baseline model by 4.1 points and the SFT counterpart by 2.8 points. 6. Conclusion In this work, we introduce preference optimization (PO) process to enhance the multimodal reasoning capabilities of MLLMs. On the data side, we design an automated pipeline for preference data construction, which is applicable to instructions both with and without clear ground truths. Using this pipeline, we create MMPR, high-quality, largescale multimodal reasoning preference dataset. On the model side, we propose simple yet effective method called Mixed Preference Optimization (MPO). This algorithm aims to learn the relative preference between pairs of responses, the absolute quality of individual responses, and the process for generating preferred responses. The resulting model, InternVL2-8B-MPO, exhibits enhanced multimodal reasoning ability and fewer hallucinations compared to its baseline model (i.e., InternVL2-8B). We hope this study could inspire further advancements in MLLMs."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NIPS, 35:2371623736, 2022. 3 [3] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Program Carrie Cai, Michael Terry, Quoc Le, et al. arXiv preprint synthesis with large language models. arXiv:2108.07732, 2021. 8 [4] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pages 44474455. PMLR, 2024. 3, 5, 7, 1 [5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 1, 2 [6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [7] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tasırlar. Introducing our multimodal models, 2023. 3 [8] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marcal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In ICCV, pages 42914301, 2019. 4 [9] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. 3, 5 [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NIPS, 2020. 1 [11] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei arXiv preprint Chu, et al. arXiv:2403.17297, 2024. 1, 3 Internlm2 technical report. [12] Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In COLING, pages 15111520, 2022. [13] Shuaichen Chang, David Palzer, Jialin Li, Eric FoslerLussier, and Ningchuan Xiao. Mapqa: dataset for question answering on choropleth maps. arXiv preprint arXiv:2211.08545, 2022. 4 9 [14] Huayu Chen, Guande He, Hang Su, and Jun Zhu. Noise contrastive alignment of language models with explicit rewards. arXiv preprint arXiv:2402.05369, 2024. 5 [15] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 8 [16] Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M3cot: novel benchmark for multiarXiv domain multi-step multi-modal chain-of-thought. preprint arXiv:2405.16473, 2024. 4, 5, 6, 8, 2 [17] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 78897901, 2023. [18] Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. Dress: Instructing large vision-language models to align and interact with humans via natural language feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1423914250, 2024. 2 [19] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 3 [20] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites. arXiv preprint arXiv:2404.16821, 2024. 1, 3, 6 [21] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust dpo: Aligning language models with noisy feedback. arXiv preprint arXiv:2403.00409, 2024. 3, 5, 7, 1 [22] Christopher Clark and Matt Gardner. Simple and effective multi-paragraph reading comprehension. In ACL, pages 845855, 2018. 4 [23] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [24] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Instructblip: Towards generalFung, and Steven Hoi. purpose vision-language models with instruction tuning. NIPS, 36, 2024. 1 [25] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. 3 [26] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1, 3 [27] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. 3, 4 [28] Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita Surnachev, Yaroslav Aksenov, Ian Maksimov, Nikita Balagansky, and Daniil Gavrilov. Learn arXiv your reference model for real good alignment. preprint arXiv:2404.09656, 2024. 3, 7, 1 [29] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, pages 69046913, 2017. [30] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. 8 [31] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 8 [32] Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model. arXiv preprint arXiv:2403.07691, 2(4):5, 2024. 3, 5, 7, 1 [33] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 15161520. IEEE, 2019. 4 [34] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, pages 67006709, 2019. 4 [35] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Triviaqa: large scale distantly superZettlemoyer. vised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. [36] Seungjae Jung, Gunsoo Han, Daniel Wontae Nam, and Kyoung-Woon On. Binary classifier optimization arXiv preprint for large language model alignment. arXiv:2404.04656, 2024. 5, 7, 1 [37] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In CVPR, pages 56485656, 2018. 4 [38] Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, and Radu Soricut. Geomverse: systematic evaluation of large models for geometric reasoning. arXiv preprint arXiv:2312.12241, 2023. 4 [39] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In ECCV, pages 235251, 2016. 4 [40] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. 8 [41] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Race: Large-scale reading comarXiv preprint and Eduard Hovy. prehension dataset from examinations. arXiv:1704.04683, 2017. [42] Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. 2, 3 [43] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. NIPS, 36, 2024. 1 [44] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 6 [45] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, pages 1288812900, 2022. 3 [46] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, pages 1973019742. PMLR, 2023. 1, 3 [47] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong. Silkie: Preference distillation for large visual language models. arXiv preprint arXiv:2312.10665, 2023. 2, [48] Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, et al. Omnicorpus: An unified multimodal corpus of 10 billion-level images interleaved with text. arXiv preprint arXiv:2406.08418, 2024. 1, 3 [49] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In EMNLP, pages 292305, 2023. 6 [50] Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Gosh, Luke Zettlemoyer, and Armen Aghajanyan. Moma: Efficient early-fusion pretraining with mixture of modality-aware experts. arXiv preprint arXiv:2407.21770, 2024. 3 [51] Adam Dahlgren Lindstrom and Savitha Sam Abraham. language, visual and mathematical reasoning. arXiv preprint arXiv:2208.05358, 2022. 4 Clevr-math: dataset for compositional [52] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 1, 6 [53] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NIPS, 36, 2023. 1, 3, 6 [54] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657, 2023. 3, 5, 7, 1 [55] Yangzhou Liu, Yue Cao, Zhangwei Gao, Weiyun Wang, Zhe Chen, Wenhai Wang, Hao Tian, Lewei Lu, Xizhou Zhu, Tong Lu, et al. Mminstruct: high-quality multi-modal instruction tuning dataset with extensive diversity. arXiv preprint arXiv:2407.15838, 2024. 1, 3 [56] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Zeqiang Lai, Yang Yang, Qingyun Li, Jiashuo Yu, et al. Interngpt: Solving vision-centric tasks by interacting with chatgpt beyond language. arXiv preprint arXiv:2305.05662, 2023. 3 [57] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 1 [58] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. [59] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021. 4 [60] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. NIPS, 35:25072521, 2022. 4 [61] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 1, 2, 5, 6 [62] Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, and Xizhou Zhu. Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training. arXiv preprint arXiv:2410.08202, 2024. 3 [63] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In CVPR, pages 31953204, 2019. 4 [64] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In ACL, pages 22632279, 2022. 4 [65] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In WACV, pages 22002209, 2021. [66] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In WACV, pages 16971706, 2022. 4 [67] Igor Melnyk, Youssef Mroueh, Brian Belgodere, Mattia Rigotti, Apoorva Nitsure, Mikhail Yurochkin, Kristjan Greenewald, Jiri Navratil, and Jerret Ross. Distributional preference alignment of llms via optimal transport. arXiv preprint arXiv:2406.05882, 2024. 5, 7, 1 [68] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, pages 947952, 2019. 4 [69] Eric Mitchell. note on dpo with noisy preferences & relationship to ipo, 2023. 5, 7, 1 [70] OpenAI. Gpt-4v(ision) system card. https://cdn. openai.com/papers/GPTV_System_Card.pdf, 2023. [71] OpenAI. Gpt-4o system card. https://openai.com/ index/gpt-4o-system-card/, 2024. 6 [72] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. 3 [73] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228, 2024. 5 [74] Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, IterHe He, Sainbayar Sukhbaatar, and Jason Weston. arXiv preprint ative reasoning preference optimization. arXiv:2404.19733, 2024. 2, 3 [75] Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Runtao Liu, Rui Pan, and Tong Zhang. Strengthening multimodal large language model with bootstrapped preference optimization. arXiv preprint arXiv:2403.08730, 2024. [76] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 5, 7, 1 [77] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. NIPS, 30, 2017. 3 [78] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1, 6 [79] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156, 2018. 7 [80] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NIPS, 35:25278 25294, 2022. 11 [81] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 3 [82] Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 14661476, 2015. 4 [83] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, pages 83178326, 2019. 4 [84] Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. Investigating prior knowledge for challenging chinese machine reading comprehension. Transactions of the Association for Computational Linguistics, 8:141155, 2020. [85] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 2, 3, 6, 7 [86] Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. 8 [87] Chameleon Team. Chameleon: Mixed-modal early-fusion arXiv preprint arXiv:2405.09818, foundation models. 2024. 3 [88] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1 [89] InternLM Team. Internlm: multilingual language model https : / / with progressively enhanced capabilities. github.com/InternLM/InternLM, 2023. 1 [90] Bart Thomee, David Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):6473, 2016. [91] Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, et al. Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer. arXiv preprint arXiv:2401.10208, 2024. 3 [92] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1 [93] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1 [94] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint arXiv:2401.06080, 2024. 3 [95] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804, 2024. 5, 6 [96] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 3, [97] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. 3 [98] Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. arXiv preprint arXiv:2402.19474, 2024. 1, 3, 6 [99] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, et al. The all-seeing project: Towards panoptic visual recognition and understanding of the open world. In ICLR, 2024. 1, 3 [100] Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen Duan, Tiantong Li, Shuo Liu, Mengkang Hu, Zhe Chen, Kaipeng Zhang, Lewei Lu, et al. Needle in multimodal haystack. arXiv preprint arXiv:2406.07230, 2024. 3 [101] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 3 [102] Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment. arXiv preprint arXiv:2405.00675, 2024. 5, 7, [103] Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, et al. Training large language models for reasoning through reverse curriculum reinforcement learning. arXiv preprint arXiv:2402.05808, 2024. 2 [104] Zhen Yang, Jinhao Chen, Zhengxiao Du, Wenmeng Yu, Weihan Wang, Wenyi Hong, Zhihuan Jiang, Bin Xu, Yuxiao Dong, and Jie Tang. Mathglm-vision: Solving mathematical problems with multi-modal large language model. arXiv preprint arXiv:2409.13729, 2024. 3 [105] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 1, 6 [106] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via 12 behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13807 13816, 2024. 2, 3 [107] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, et al. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. 2, 3, 6, [108] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 6 [109] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv preprint arXiv:2407.08739, 2024. 3 [110] Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on gaokao benchmark. arXiv preprint arXiv:2305.12474, 2023. 8 [111] Yongting Zhang, Lu Chen, Guodong Zheng, Yifeng Gao, Rui Zheng, Jinlan Fu, Zhenfei Yin, Senjie Jin, Yu Qiao, Xuanjing Huang, et al. Spa-vl: comprehensive safety preference alignment dataset for vision language model. arXiv preprint arXiv:2406.12030, 2024. 2, 3 [112] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964, 2023. 3 [113] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. [114] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of images interleaved with text. NIPS, 36, 2024."
        },
        {
            "title": "Supplementary Material",
            "content": "7. Implementation Details During the construction of samples with clear ground truths, we sample at most 32 reasoning processes and construct at most 15 preference pairs for each query. When constructing data using DropoutNTP, we truncate the original response by half and ask InternVL2-8B to complete the response without the image input. Our ablation studies in Section 8.2 show that truncating the original response by 25% or 75% has negative effects on the final performance. We set the temperature to 1.0 during sampling to ensure response diversity. Besides, the maximum tiles for dynamic resolution are set to 6 for the general VQA domain and 12 for OCR-, document-, and chart-related domains. During the MPO process, the global batch size is set to 256 during training. We employ the AdamW optimizer [57] with the β1 of 0.9, the β2 of 0.999, and the weight decay of 0.05. The learning rate is initialized as 5e-6. The training phases include linear warmup that lasts until the first 5% of training steps. The warmup is followed by cosine decay strategy with minimum learning rate of 0. The KL penalty coefficient β is set to 0.1. For the Equation 3, we set wp to 0.8, wq to 0.2, and wg to 1. The model is initialized from InternVL2-8B [20], and all parameters are trainable during training. We train the model for 1 epoch. 8. More Ablation Studies 8.1. Ablation Studies about DPO variants In this section, we present the numerical experimental results of ablation studies on the effects of different preference optimization algorithms in Table 6 and Table 7. We define as the performance gap between CoT reasoning responses and direct-answer responses to quantitatively assess the effects of different preference optimisation algorithms on CoT reasoning abilities. Our results indicate that introducing an additional SFT loss can significantly improve the CoT performance compared to each algorithms vanilla counterpart. Note that, to reduce computational costs, we only extend the DPO variants, which exhibit superior performance in Table 6 compared to DPO, with SFT Loss. In addition to the ablation studies based on M3CoT, we also present the performance of models trained with DPO+ and BCO+ using our MMPR, as shown in Table 8. The experimental results show that models trained with MPO exhibits superior overall performance compared to those trained with DPO+ and BCO+."
        },
        {
            "title": "Direct CoT",
            "content": "InternVL2-8B SFT DPO [76] RSO [54] IPO [4] cDPO [69] RobustDPO [21] BCO [36] SPPO [102] AOT [67] TR-DPO [28] 59.3 65.7 75.8 74.2 72.8 76.2 75.1 78.1 66.2 76.7 75.9 57.0 68.5 72.7 74.3 73.1 76.8 74.2 78.4 67.4 76.0 66.8 -2.3 +2.8 -3.1 +0.1 +0.3 +0.6 -0.9 +0.3 +1.2 -0.7 -9.1 Table 6. Results of models trained with different preference optimization algorithms on M3CoT. represents the performance gap between CoT responses and direct-answer responses."
        },
        {
            "title": "Direct CoT",
            "content": "ORPO [32] DPO+ cDPO+ RobustDPO+ BCO+ AOT+ MPO 66.6 76.4 71.6 76.5 77.4 76.3 77.7 73.9 78.9 74.2 78.0 78.4 78.0 79.1 +7.3 +2.5 +2.7 +1.5 +1.0 +1.7 +1.4 Table 7. Results of models trained with different preference optimization algorithms extended with SFT Loss on M3CoT. The algorithm extended with the SFT Loss is called X+ for brevity. For instance, DPO+ is the combination of DPO and SFT loss. 8.2. Ablation Studies on DropoutNTP Here, we present the ablation results for the Dropout Ratio (DR) in our proposed DropoutNTP. By default, we set DR to 0.5, which means that we truncate the positive response by half. Notably, setting DR to 0.25 means using the first quarter of the positive responses for continuation. Following the experimental settings in Section 5.2.2, we replace the negative samples in RLAIF-V with the completions based on different dropout ratios. As shown in Table 9, the model trained with data generated using DR of 0.75 performs the worst. We attribute this to the fact that, with the first three-quarters of the prefix being identical, the difference in quality between the chosen and rejected responses becomes less apparent, reducing training effectiveness. Additionally, the model trained with DR of 0."
        },
        {
            "title": "Hallucination Evaluation",
            "content": "M3CoT MathVista MathVision MMVet LLaVA-Bench POPE CRPE MMHalBench InternVL2-8B InternVL2-8B-DPO+ InternVL2-8B-BCO+ InternVL2-8B-MPO 59.3 80.4 79.6 79.2 58.3 66.4 66.1 67.0 20.4 23.4 18.8 25.7 54.2 58.3 55.5 56. 73.2 74.1 78.6 76.7 86.9 87.6 88.5 88.1 75.0 75.5 75.5 75.4 3.3 3.4 3.5 3.5 Table 8. Results of models trained with DPO+, BCO+ and MPO using our MMPR."
        },
        {
            "title": "Method",
            "content": "DR=0.25 DR=0.50 DR=0.75 Object HalBench MM HalBench Resp. () Ment. () Score Hall. () 9.3 7.6 11.6 4.8 4.1 6.2 3.3 3.6 3.3 40.6 31.3 36. Table 9. Results of DropNTP with different Dropout Ratios. performs worse than that trained with dropout ratio of 0.5. We believe this is because the majority of the content in the rejected responses is generated without image input, resulting in noticeably lower quality compared to the chosen responses, which similarly hampers the training effectiveness. Therefore, we set the DR to 0.5. 8.3. Effects of data scale. To evaluate the effects of the data scale, we train the model with different amounts of preference reasoning data sampled from M3CoT [16]. The M3CoT training set contains 7,861 samples annotated with corresponding rationales. To control the data volume, we adjust the maximum number of preference pairs generated for each sample, resulting in datasets of different sizes: 10K, 40K, 70K, and 100K. As illustrated in Figure 4a, model accuracy consistently improves with the increasing data volume. As the data volume rises to 100K, the model achieves its highest accuracy of 76.4 when directly answering the final answer and 78.9 when answering with CoT. Furthermore, both the Direct and CoT performance exhibit positive correlation between data scale and accuracy, with the CoT performance achieving higher performance across all scales. These results highlight the importance of scaling up reasoning preference data to improve model performance. 8.4. Effects of hyper-parameters. We conduct ablation studies on M3CoT to study the impact of the hyper-parameters, including learning rate, PO coefficient wp, wq, and SFT coefficient wg. For the PO coefficient, we control the sum of wp and wq to equal 1.0 and adjust different proportions. Unless specifically mentioned, we set the learning rate to 5e-6, wp to 0.8, wq to 0.2, and wg to 1. As shown in Figure 4b, the learning rate significantly affects the models performance. With rel- (a) (c) (b) (d) Figure 4. Results of models trained with different data scales or hyer-parameters on M3CoT. The X-axis represents the corresponding data scale or hyper-parameter for this point, while the Y-axis indicates the accuracy on M3CoT. atively low learning rate of 5e-7, the model shows moderate improvement. As the learning rate increases to 5e-6, the models performance improves further, reaching optimal results across the tested learning rates and surpassing the baseline by 19.6 points. However, further increasing the learning rate to 5e-5 causes drastic performance drop, suggesting that higher learning rate may lead to overfitting or instability in training. Additionally, the PO coefficient w0, w1 and SFT coefficient w2 are crucial. As shown in Figure 4c and 4d, the model achieves optimal performance with wp set to 0.8, wq set to 0.2, and wg set to 1. Notably, when wg is set to 0.01, the performance of the CoT approach is inferior to that of directly answering the final answer, indicating the importance of the SFT Loss during the direct preference optimization. 9. More Data Examples in MMPR In this section, we provide data examples in MMPR for each task described in Table 1. Specifically, Figure 5a to 5f are examples from data constructed using DropoutNTP, while Figure 5g to 5j are examples from data constructed using correctness-based pipeline. 2 (a) (b) (c) (d) (e) 3 (f) (g) (h) (i) 4 Figure 5. More data examples from MMPR. Figure 5a to 5f are examples from data constructed using DropoutNTP, while Figure 5g to 5j are examples from data constructed using correctness-based pipeline. (j)"
        }
    ],
    "affiliations": [
        "Fudan University",
        "Nanjing University",
        "OpenGVLab, Shanghai AI Laboratory",
        "SenseTime Research",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}