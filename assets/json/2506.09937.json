{
    "paper_title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
    "authors": [
        "Qiao Gu",
        "Yuanliang Ju",
        "Shengxiang Sun",
        "Igor Gilitschenski",
        "Haruki Nishimura",
        "Masha Itkina",
        "Florian Shkurti"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out-of-the-box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, $\\pi_0$, and $\\pi_0$-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results can be found at https://vla-safe.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 7 3 9 9 0 . 6 0 5 2 : r SAFE: Multitask Failure Detection for Vision-Language-Action Models Qiao Gu1,2,3 Yuanliang Ju1,2,3 Shengxiang Sun1,2 Igor Gilitschenski1,2,3 Haruki Nishimura4 Masha Itkina4 Florian Shkurti1,2,3 1University of Toronto (UofT), 2UofT Robotics Institute, 3Vector Institute, 4Toyota Research Institute (TRI) q.gu@mail.utoronto.ca"
        },
        {
            "title": "Abstract",
            "content": "While vision-language-action models (VLAs) have shown promising robotic behaviors across diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out-of-the-box. To allow these policies to safely interact with their environments, we need failure detector that gives timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or few specific tasks, while VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, π0, and π0-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results can be found at https://vla-safe.github.io/."
        },
        {
            "title": "Introduction",
            "content": "Recently, scaling up robot manipulation datasets has enabled the development of large visionlanguage-action (VLA) models, which are generalist manipulation policies that can follow language instructions and accomplish wide range of tasks [16]. However, when VLAs are directly deployed on unseen tasks without collecting additional demonstrations and finetuning the model, they still suffer from limited success rates and wide range of failure modes. This has been demonstrated by evaluations in recent work [2, 7, 4]: while their success rates on seen tasks are as high as 80-90%, those on unseen ones drop to 30-60% out-of-the-box. Therefore, to safely and reliably deploy VLA policies in the real world, it is important to promptly detect their potential failures. Most existing failure detection methods train separate failure detector for each task, and evaluate the detector only on that task [816]. While these methods work well for specialist policies, they do not suit generalists like VLAs. VLAs are designed to accomplish diverse tasks and may frequently encounter novel task instructions and unseen environments during deployment. In such cases, it is impractical to exhaustively collect rollouts and train failure detector for every new task. Some recent works introduce task-generic failure detectors, but they either require sampling multiple actions [17] Preprint. Under review. Figure 1: The VLAs internal features capture high-level information about task success and failure. When the VLA is failing, even though from different tasks, the features fall in the same failure zone. This motivates SAFE, an efficient multitask failure detector that is based on VLA internal features and can generalize to unseen tasks. Plot (a) visualizes latent features of π0-FAST on LIBERO-10 [20] using t-SNE [21]. For successful rollouts, their features are colored in blue; for failed rollouts, features are colored in the blue-red gradient based on timesteps. Plot (b) visualizes the same set of t-SNE features, colored by task ID. In (c), we show two example rollouts over time and mark their corresponding projected features in (a) and (b). or need to query large VLM [18, 19], which poses significant inference overhead for large VLAs in the real world. This motivates the need for an efficient and multitask failure detector that can generalize to unseen tasks zero-shot and detect failures in timely manner during the on-policy rollout of the VLA. In this paper, we focus on the multitask failure detection problem. This setting evaluates failure detection performance of VLA policy without collecting rollouts or finetuning the failure detector on unseen tasks. To our knowledge, such multitask failure detection ability for VLAs has not been shown in the literature. To tackle this problem, we study the internal features of VLAs and find that they capture high-level knowledge about task success and failure. As shown in Fig. 1, such knowledge can effectively distinguish failed rollouts from successful ones and is generic across different tasks. Based on this insight, we introduce SAFE, ScAlable Failure Estimation method that scales across diverse tasks for generalist policies like VLAs. SAFE takes in VLAs internal features and regresses them to single scalar indicating the likelihood of failure. By training on successful and failed rollouts of multiple tasks, SAFE learns to identify task-generic representations for failure detection. To determine the threshold for failure detection, we adopt the functional conformal prediction (CP) [22, 8] framework and calibrate the prediction band on the seen tasks. We conduct failure detection experiments on OpenVLA [2], π0 [4] and π0-FAST [5], in both simulation and the real world. For evaluation, we adapt diverse baseline failure detection methods from both the LLM literature [23, 24] and the robot learning literature [8, 17] onto VLAs. SAFE and baselines are evaluated on both training tasks and set of held-out tasks. Experiments show that SAFE outperforms other existing baselines and achieves the best trade-off between accuracy and timeliness for failure detection. The contributions of our paper can be summarized as follows: We analyze the VLA feature space and show that across different task instructions and environments, VLAs internal features are separated for successful and failed rollouts. We propose SAFE, multitask failure detector designed for generalist robot policies. By operating on latent features, training on multiple tasks, and using conformal prediction methods, SAFE shows generalization ability in detecting failures on unseen tasks. 2 We evaluate SAFE and diverse baselines on several recent large VLA models in both simulation and the real world. Experiments show that SAFE outperforms baselines and achieves state-of-the-art (SOTA) performance."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Vision-Language-Action Models Recent advances in large-scale machine learning and the availability of extensive robot demonstration datasets have paved the way for VLA models [14, 7, 25, 26, 6]. These generalist robotic policies are initialized from pretrained large-scale VLMs [2729], and thus inherit the strong ability to understand diverse semantic concepts from both images and language. They are augmented with an action head that produces continuous control signals, through per-step binning [1, 7, 2, 25], diffusion networks [3, 4, 30, 31] or frequency-space tokenization [5]. These VLAs are then trained on vast robotic datasets covering wide array of tasks [3234]. As result, VLAs can successfully perform familiar tasks in new environments and even tackle previously unseen tasks when provided with novel language instructions. Nevertheless, significant variability in real-world deployments and the challenging domain gaps between training and testing environments continue to hinder VLA performance. Most state-of-the-art VLA models achieve success rates between 30% and 60% when evaluated out-of-the-box on real robots with unseen task instructions [2, 32, 4]. These limitations highlight the need for robust multitask failure detection methods tailored to generalist VLA models. 2.2 Failure Detection in Robot Manipulation Monitoring failures is critical when deploying robotic policies in real-world environments, as even minor errors can result in hazardous conditions [3537]. The literature on failure detection in robot learning can be broadly divided into unsupervised out-of-distribution (OOD) detection [811] and supervised failure detection [9, 1216]. OOD detection-based methods treat successful executions as the in-domain baseline and consider any deviation from this norm as failure. However, the assumption that any unseen scenario constitutes failure is overly restrictive for generalist VLAs, which may frequently encounter unseen tasks at test time. These unseen tasks are likely different from the in-domain training data but should not be simply treated as failures. Our proposed method, SAFE, falls within the supervised failure detection category, leveraging both successful and failed rollouts to train failure classifier. Yet SAFE trains single unified failure detector for all tasks, while other existing ones typically require training and calibration of separate classifiers for each task and have not yet demonstrated effectiveness on generalist policies like VLAs. Some recent works have explored the multitask failure detection task by designing action consistency scores [17] or instruction-finetuning VLM [18, 19], but they require either sampling multiple actions or querying large VLM, which poses significant overhead for controlling robots in real time. Recently, FAIL-Detect [8] conducted systematic evaluation of various failure detection methods, including OOD detection-based approaches [38, 39, 9], smoothness-based techniques [40], and consistency-based strategies [17]. Their experiments indicate that the best performance was achieved by LogpZO [38] and RND [39], both using learned network to model data distribution and detect OOD states. However, their evaluation is limited to only single-task policies, and our evaluation in multitask setting shows that their best performing RND and LogpZO methods suffer from overfitting to the training tasks. 2.3 Uncertainty Quantification for LLM Although LLMs and VLMs have demonstrated remarkable understanding and generative capabilities across various tasks, they are prone to producing hallucinated responses [4143]. Numerous methods have been developed for uncertainty quantification (UQ) in LLMs/VLMs. Token-level uncertainty quantification methods estimate uncertainty by analyzing the probability distribution over each generated token to assess the likelihood of an entire response [4446]. In contrast, semanticsimilarity methods generate multiple responses to the same query and evaluate their semantic alignment [24, 47, 48]; higher variance among responses typically signals low confidence. Since vision-language-action models (VLAs) share the generative nature and transformer architecture of LLMs/VLMs, we adapt these UQ methods to VLAs as promising baselines and evaluate their 3 performance on failure detection. Recent research has also explored the internal latent space of LLMs for hallucination detection [4955]. These methods train classifier on internal latent features to distinguish between truthful and hallucinated outputs, paralleling supervised failure detection techniques in robotics. This approach has proven to be simple, efficient, and effective for UQ in LLMs. In our study, we investigate its application to large VLA policies and observe promising performance in robotic tasks."
        },
        {
            "title": "3 Problem Formulation",
            "content": "This work aims to detect when robot policy fails its task execution. Specifically, we aim for multitask failure detector that performs well when generalist VLAs perform novel tasks at inference time. At timestep t, VLA is given an input observation ot, consisting of RGB images, natural language instruction, and current robot state, and outputs control signal At = [at, at+1, . . . , at+H1], which is chunk of actions for the next timesteps. The first (H H) actions in At are executed, and then the VLA replans new action sequence At+H at time + . We denote the internal embedding vector within the VLA model at time as et. Some VLAs [1, 2, 4, 5] also decode series of tokens Wt = [w1 ] before converting them into the actual action vector. To train and evaluate failure detection models, we run the VLA on different tasks in simulation or the real world, collect the rollout trajectory τi = {(ot, et, Wt, At)}t=0,H ,...,nH with time duration = nH , and annotate each rollout with failure label yi (yi = 1 if the robot fails to accomplish the task and yi = 0 if the robot succeeds). Note that for training, we only use the trajectory-level annotation yi, and dont require knowing the exact timestep when the policy starts to fail. failure detector receives the rollout information up to time and predicts failure score st, indicating the likelihood of task execution failure at time t. If st exceeds threshold δt, failure flag is raised, and then either the task execution is aborted or human monitor will step in and take over the control. In this work, we use conformal prediction [56] to calibrate the threshold δt. , . . . , wm In experiments, we split all tasks into seen and unseen subsets, where rollouts from seen tasks are used for training Dtrain and validation Deval-seen, and all rollouts from unseen tasks Deval-unseen are reserved for testing the cross-task generalization ability of failure detectors. Failure detectors are trained on Dtrain, and evaluated on Deval-seen for hyperparameter tuning and in-domain performance, and tested on Deval-unseen for out-of-distribution generalization."
        },
        {
            "title": "4 Method",
            "content": "4.1 Visual Analysis on VLA Latent Space VLAs process multi-modal inputs and extract rich semantic information in their internal feature space. We hypothesize that these features also capture the high-level and abstract knowledge about task execution success/failure, by separating features from successful/failed rollouts into different regions. We study this hypothesis by visualizing the VLA features in Fig. 1, where we plot the internal features from π0-FAST[5] when running the LIBERO-10 benchmark [20]. Fig. 1(a) demonstrates that when the VLA is failing, its internal features are grouped in the same region in the feature space (failure zone). Comparing Fig. 1(a) and Fig. 1(b), we can further see that although the features are extracted from different tasks with various instructions, objects and environments, when the VLA fails, its features fall in the same failure zone. Fig. 1(c) further illustrates how VLAs features evolve in the feature space when VLA progresses temporally. From Fig. 1(c), we can see that failure rollout initially stays out of the failure zone when it progresses normally, and when the robot mistakenly drops the pot in the middle of execution and starts to fail, it steps into the failure zone. On the contrary, for the successful rollout, its features always stay out of the failure zone. This visual analysis shows that the VLAs internal features for succeeding and failing task executions are well separated in the feature space, and this separation is general across different tasks. Furthermore, during task execution, the features reflect how well the VLA performs on the current tasks in timely manner. Inspired by this observation, we design SAFE, which uses the internal features of VLAs for failure detection. 4 Table 1: Failure detection results on simulation benchmarks, measured by area under ROC (ROCAUC). - indicates that the failure detection method does not apply. Entries with gray background indicate the failure detection methods that sample 10 actions per inference timestep, while others use only 1 action. The first and second best-performing methods are colored in red and orange, respectively. Results are averaged over 3 random seeds with different splits of seen and unseen tasks. VLA Model Benchmark Eval Task Split OpenVLA LIBERO π0-FAST LIBERO π0 LIBERO π 0 SimplerEnv Average Seen Unseen Seen Unseen Seen Unseen Seen Unseen Seen Unseen Token Unc. Embed. Distr. Sample Consist. Action Consist. SAFE (Ours) 50.25 Max prob. 44.05 Avg prob. 52.94 Max entropy 45.27 Avg entropy Mahalanobis dist. 62.03 Euclidean dist. k-NN 66.00 Cosine dist. k-NN 67.09 57.18 PCA-KMeans [9] 52.57 RND [39] 61.57 LogpZO [8] 62.76 Action total var. 55.33 Trans. total var. 47.85 Rot. total var. 61.84 Gripper total var. 50.16 Cluster entropy - STAC [17] - STAC-Single SAFE-LSTM 70.24 SAFE-MLP 72.68 53.83 51.58 53.09 50.03 58.85 55.23 69.45 55.10 46.88 52.91 65.43 58.99 55.30 64.48 51.44 - - 72.47 73.47 61.32 52.46 46.69 50.93 93.56 92.04 92.09 68.46 88.67 91.52 76.95 78.21 80.87 76.82 80.22 83.07 85.46 92.98 90.06 69.44 58.04 62.96 58.63 83.79 84.12 84.64 57.12 81.57 83.07 74.50 80.03 77.29 74.42 80.53 85.31 81.16 84.48 80. - - - - 77.12 75.64 75.76 64.92 71.92 76.80 77.20 49.38 52.94 77.19 76.19 46.55 68.46 76.98 73.50 - - - - 74.31 70.73 70.31 60.35 69.44 73.23 75.18 54.71 61.06 75.19 72.12 47.91 69.39 71.09 73.27 - - - - 88.42 89.73 90.19 66.88 85.07 88.79 68.41 63.27 58.07 69.16 68.25 60.74 68.71 88.85 89.50 - - - - 52.84 68.41 71.32 61.19 65.89 74.66 67.94 55.90 62.10 69.29 73.66 62.21 70.40 80.11 84.82 55.79 48.26 49.81 48.10 80.28 80.85 81.28 64.36 74.56 79.67 71.33 61.55 59.93 71.25 68.71 63.45 74.21 82.26 81.43 61.64 54.81 58.03 54.33 67.45 69.62 73.93 58.44 65.95 70.97 70.76 62.41 63.94 70.84 69.44 65.14 73.65 77.04 78. 4.2 Failure Detection by Feature Probing We design SAFE to learn the abstract information from the VLAs internal features and determine whether the task execution is failing. We extract the VLAs hidden state vectors from the final layer, before being decoded to token logits [2, 5] or velocity field [4]. We ablate different ways to aggregate the internal features into single embedding vector e, and select the best one based on Deval-seen performance. Please refer to Appendix for details on VLA feature extraction. The failure detector (e0:t) takes as input the VLAs features e0:t = {e1, . . . , et} up to the current timestep t, and is trained to predict st. We explore the two backbone designs for SAFE: multi-layer perceptron (fMLP) and an LSTM [57] (fLSTM). Both models are designed to be simple (only one or two layers), to avoid overfitting and improve generalization ability on unseen tasks. For fMLP, we use an MLP g() to project et into single scalar for each timestep independently and accumulate the outputs as the failure score, i.e. fMLP(e0:t) = (cid:80) τ =1,...,t σ(g(eτ )), where σ() is sigmoid function and therefore 0 < st < t. To train the MLP model, we apply an L1 loss on all timesteps to push up the scores for failed rollouts and push down those for successful ones. Specifically, LMLP = (cid:80) For fLSTM, we use an LSTM model to sequentially process the input stream of VLAs features e0:t and project the hidden state vector of LSTM into scalar score. Specifically, fLSTM(e0:t) = σ(LSTM(e0:t)), where sigmoid function σ() is applied to normalize the output score s.t. 0 st 1. To train the LSTM model, we apply binary cross entropy loss on all timesteps, i.e. LLSTM = (cid:80) st] , where index iterates over all data points in Dtrain. t(t st) + (1 yi) (cid:80) [yi log(st) + (1 yi) log(1 st)] . [yi (cid:80) (cid:80) 4.3 Threshold Selection by Conformal Prediction When the predicted failure score st exceeds the time-varying threshold δt, we raise failure flag. To determine δt in principled way, we adopt the functional conformal prediction (CP) framework [22]. Functional CP constructs time-varying prediction band by leveraging the distribution of st observed in successful rollouts within calibration set. Under the exchangeability assumption [58] and given user-specified significance level α, the CP band guarantees that, for new successful rollout, its st will lie within this band at all times with probability 1 α. Conversely, if the score of test rollout exits the band at time t, we can declare failure with nominal confidence 1 α. Formally, given time series of any scalar score st and user-specified significance level α (0, 1), functional CP gives distribution-free prediction band Cα. Following [8], we adopt the one-sided time-varying CP band formulation, where Cα is set of intervals {[lowert, uppert] : = 1, . . . , }, where lowert = and uppert = µt + ht, with time-varying mean µt and bandwidth ht. This 5 Figure 2: Illustration of real-world experiment setup (left) and example rollouts collected (right). band is calibrated on successful rollouts in Deval-seen. Under mild assumptions [59, 60], for any new successful rollout, st < µt + ht holds for all = 1, . . . , with probability 1 α. Intuitively, this gives guarantee that the false positive rate of the failure detector (a failure flag is raised at any time during successful rollout) is at most α. We use uppert as the failure flag threshold δt, and more details about functional CP can be found in Appendix."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Evaluation Benchmarks LIBERO [20]: The LIBERO benchmark has been widely adopted for evaluating VLA models in simulation [2, 46]. Among the LIBERO task suites, the LIBERO-10 suite consists of 10 long-horizon tasks with diverse objects, layouts, and instructions, and is considered the most challenging one. Therefore, we use LIBERO-10 in our experiments and test OpenVLA [2], π0 [4] and π0-FAST [5] on it. We adopt the model checkpoints that are finetuned on the LIBERO benchmark and released by their authors. In experiments, 3 out of 10 tasks are randomly picked and reserved as unseen tasks. SimplerEnv [61]: SimplerEnv provides high-fidelity simulation environment for manipulation policies, which are replicas of the demonstration data from RT-series [1, 7, 32] and BridgeData V2 [34]. On SimplerEnv, we test pretrained π0 models from reproduction [62], which we denote as π 0 in this paper. We train and evaluate the failure detection methods on the Google Robot embodiment [1] and on the WidowX embodiment [34], respectively. We exclude the pick up coke task because π 0 rarely fails on it (success rate at 98%). This leaves 4 tasks for each embodiment, among which 3 tasks are seen and 1 task is unseen. Real-robot Experiments: We deploy the π0-FAST-DROID checkpoint1 provided by [4, 5] on Franka Emika Panda Robot. This checkpoint has been finetuned on the DROID dataset [33], and we do not further collect demonstrations or finetune the VLA model. We design 13 tasks and collect 30 successful and 30 failed rollouts for each task. The real-robot setup and example rollouts are visualized in Fig. 2. In experiments, 3 tasks out of 13 are randomly selected as unseen tasks. 5.2 Uncertainty Quantification Baselines Estimating uncertainty in generated responses has been widely used to detect truthfulness or hallucination in LLMs [23, 24, 63, 43]. For VLAs, uncertainty in the generated actions may indicate lack of ability to solve the given task, and thus correlates with task failures. Therefore, we first adapt the UQ methods from the LLM literature to VLAs and use them as failure detection baselines. Token uncertainty-based methods aggregate the predictive uncertainty from each generated token. These methods are efficient, as they only require single forward inference. Given the generated tokens Wt = [w1 as pi and the entropy over the distribution of the ith token as Hi. We adopt the token-based uncertainty estimation methods proposed in [23] as follows: ], we denote the probability of sampling the token wi , . . . , wm Token max prob.: maxi( log pi); Token avg prob.: 1 Token avg entropy: 1 Token max entropy: maxi Hi; (cid:80) (cid:80) log pi; Hi. 1https://github.com/Physical-Intelligence/openpi 6 Figure 3: Performance of functional CP by varying significant level α on Deval-unseen. The plots show the change of balanced accuracy (bal-acc) w.r.t. average detection time (T-det). Good failure detection methods should detect policy failures both accurately (high bal-acc) and proactively (lower T-det), and thus place curves towards the top left in each plot. We can see in all simulation experiments, the proposed SAFE-LSTM and SAFE-MLP perform better or on par with the best baselines. Note that baselines in gray require multiple action samples. Sample consistency-based methods estimate uncertainty as the inconsistency within multiple generated sentences [23, 24, 63]. For VLA models, the output actions are continuous vectors, and we can measure inconsistency by their variance. Specifically, at time t, given sampled actions At = {Ak }k=1,...,K, we measure the uncertainty as the total variation over the set of vectors: action total var. = trace(cov(At)). Similarly, we also compute variation for the translational (trans. total var.), rotational (rot. total var.), and gripper control (gripper total var.) components of At. by inspired semantic as [24], we Furthermore, entropy(cluster({Ak }k=1,...,K)), where cluster() generates an integer set, containing cluster labels for the actions and entropy() measures the entropy of the integer set. UQ methods based on sample consistency are shown to perform better for LLM [24, 23], but it necessitates multiple inferences, which may not be practical for large VLAs that control robots in real time. entropy define entropy cluster 5.3 Failure Detection Baselines Embedding Distance: We compare to baselines that directly use the distances in the feature space as failure scores. Specifically, instead of training neural network, all VLA embeddings from Dtrain are stored in the two feature sets, Esucc and Efail, containing all VLA embeddings from successful and failed rollouts respectively. During evaluation on Deval-seen and Deval-unseen, failure scores are computed as st = d(et, Esucc) d(et, Efail), where d(, ) measures the distance between single vector and set of vectors. Intuitively, if et is far from Esucc and close to Efail, its more likely to fail. Following [17, 10], we ablate different types of distance, including Mahalanobis distance, and Euclidean and Cosine distance averaged over k-Nearest Neighbors of et. We also compare to the PCA-KMeans distance measure from [9]. Learned OOD Detector: We adopt RND [39] and LogpZO [8], which are shown to be the bestperforming OOD detection-based failure detectors by [8]. Both methods use neural network OOD() to model the embedding distribution from successful rollouts and return an OOD score for new embedding. We adapt them to learn from both successful and failed rollouts by training two models, OOD succ () and OOD fail (), on Esucc and Efail respectively. Similar to embedding distance baselines, the failure score is computed as st = OOD Action Consistency: STAC [17] detects policy failures by measuring the statistical distance on the overlapping segment of two consecutive predicted action chunks. As it requires sampling multiple actions from the policy ([17] uses 256 actions), it compromises real-time operation for real robots, because unlike relatively small diffusion policy networks, large VLAs are not optimized for parallel succ (et) OOD fail (et). 7 Figure 4: Failure scores predicted by SAFE-LSTM and the corresponding camera observations. The green areas show the functional CP band Cα. Once failure scores exceed Cα, failure flag is raised. In (a), π0-FAST policy misses the insertion, and its actions become unstable after that. In (b) and (c), OpenVLA and π 0 miss the grasp but still proceed to the placing action, causing failure detection. Note that these tasks are not seen when training SAFE-LSTM. inference2. Therefore, we only test STAC in the simulation experiments with 10 sampled actions. We also adopt STAC-Single, real-time version of STAC, which computes action inconsistency using only one sample from each inference timestep. Since OpenVLA only outputs one-step immediate action (H = 1), STAC and STAC-single do not apply to it. 5.4 Evaluation Protocol We consider two types of evaluation. The first type evaluates how well st separates the successful and failed rollouts across all possible selections of δt. Following the evaluation protocol widely adopted in the LLM UQ literature [49, 53, 23, 65], we use the area under ROC curve (ROC-AUC) metric. Furthermore, because failure flag is raised whenever st exceeds δt, successful rollout (ground truth negative) becomes false positive whenever st > δt, and remains true negative only if st δt for all time. Therefore, we consider the max-so-far score st = maxτ =1,...,t sτ and compute the ROC-AUC metric based on sT , the maximum failure score throughout the entire rollout. The second type of evaluation utilizes δt = uppert calibrated by functional CP in Section 4.3. By setting the significance level α, we get decisive positive/negative detection for each rollout. Following [17, 8], we consider the following metrics: true positive rate (TPR), false positive rate (FPR), balanced accuracy (bal-acc), and averaged detection time (T-det), where Bal-Acc = TPR+TNR . T-det is the relative timestep where st > δt for the first time (if st never exceeds δt, T-det becomes 1), averaged over all ground truth failed rollouts."
        },
        {
            "title": "6 Results",
            "content": "6.1 How well do failure detectors distinguish failures from successes? In Table 1 and Fig. 5 (a), we report the ROC-AUC metric based on sT , in simulation and realworld experiments, respectively. With higher ROC-AUC metric, failure detector achieves higher accuracy averaged over all possible thresholds. The tables show that Token Unc. methods have poor performance, which is aligned with findings in the LLM literature [24, 23]. On the other hand, the Sample Consist. and STAC [17] methods, which require multiple action samples, perform better and even achieve the best performance on unseen tasks in π0-FAST LIBERO (STAC) and π0 LIBERO (Gripper total var.). However, as these methods require multiple action samples, they cause significant overhead for VLA models and thus are not applicable to real robots. Embed. Distr. methods perform well, achieving the best performance in two simulation benchmarks (π0 and π0-FAST) and are the second best in the real world. This demonstrates that VLAs internal features are informative about task execution success/failure. The proposed SAFE methods perform better or 2π0 is 1.52x slower and π0-FAST is 2.21x slower to generate 10 action samples compared to 1 sample, tested on single NVIDIA RTX 3090 GPU, with vmap optimization and JiT compilation in Jax [64]. For comparison, SAFE methods only cost negligible overhead (<1ms, or <1% of the inference time of π0 and π0-FAST). 8 Task Split Seen Unseen 53.74 Max prob. 51.60 Avg prob. 59.23 Max entropy 50.67 Avg entropy Mahala. dist. 75.54 Euclid. k-NN 80.35 Cosine. k-NN 80.23 49.98 PCA-KMeans 62.00 RND 64.43 LogpZO STAC-Single 45.24 SAFE-LSTM 77.27 SAFE-MLP 86.76 48.59 47.30 53.50 46.08 53.93 60.27 59.51 51.03 45.83 52.24 38.01 58.70 64.16 (a) Failure Detection ROC-AUC Figure 5: Quantitative results (a) and qualitative examples (b-e) from SAFE-MLP in the real world experiments. The ROC-AUC results are averaged over 5 random seeds with different task splits. on par with the best baselines, consistently in all settings. Averaged across simulation benchmarks, SAFE-MLP and SAFE-LSTM have similar performance, both outperforming the best baseline by 4-5% on unseen tasks, while still achieving the best performance on seen tasks. On the real robot, SAFE-MLP achieves the best performance and SAFE-LSTM performs closely with the best baseline (Euclid. k-NN). Comparing SAFE with Embed. Distr. methods, we attribute the success of SAFE to its stronger ability to extract high-level abstract information from raw feature vectors through learned neural networks. 6.2 How do detection accuracy and detection time trade off using functional CP? In Fig. 3, we use Deval-seen to calibrate functional CP band Cα and evaluate on Deval-unseen. By varying the user-specified α, we can adjust the conservativeness of the failure detectors and obtain trade-off between accuracy (bal-acc) and detection time (T-det). To be good failure detector, it should detect failures both accurately (higher bal-acc) and promptly (lower T-det), and thus have the curve rise toward the top-left corner in the plots of Fig. 3. As we can see from Fig. 3, the proposed SAFE-MLP and SAFE-LSTM perform the best on OpenVLA+LIBERO and π0+SimplerEnv benchmarks, and on par with the best baseline on the other two benchmarks. We also manually annotate the ground truth (GT) failure timesteps (when human thinks that failure happens or intervention is needed) for failed rollouts, and plot them as blue vertical lines in Fig. 3. Comparing SAFEs performance with the GT fail time, we can see that SAFE can detect failures with high accuracy in the early stages of rollouts and potentially before the failure happens. This early detection allows early intervention for policy failures before they get stuck in execution or cause harm to the real-world environment. 6.3 What are detected failure modes and are they aligned with human intuition? In Fig. 4 and Fig. 5(b-d), we visualize rollouts with the failure scores detected by SAFE. Fig. 4 demonstrates common failure modes in simulation, including imprecise insertion, unstable actions and missed grasps. Two successful rollouts on the real robot are shown in Fig. 5(b-c), where failure scores stop increasing after task completion. For the failed rollouts, the failure flag is raised after the policy is frozen (Fig. 5d) or the object slips out of the gripper (Fig. 5e). This aligns well with human intuition. Please refer to Appendix for video illustrations."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduce the multitask failure detection problem for generalist VLA policies, where failure detectors are trained only on seen tasks and evaluated on unseen tasks. We analyze VLAs internal feature space and find that the internal features are separated for successful and failed rollouts. Based on this observation, we propose SAFE, simple and efficient failure detection method by operating on the VLAs internal features. SAFE is evaluated on multiple VLAs in both simulation and the real world, and compared with diverse baselines. Experiments show that SAFE achieves SOTA results in failure detection, and aligns with human intuition. 9 Limitations: Most recent VLAs have shown capabilities in handling diverse modalities, controlling diverse embodiments and learning latent actions from non-robotic action-less video data [66, 67]. This paper only considers multitask failure detection for manipulation tasks, and its not clear how well the failure detectors generalize across embodiments, sim2real or to the action-less videos. Besides, SAFE only uses features from the last layer and how to effectively aggregate information across multiple layers of VLA remains future work."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "The authors were partially supported by the Toyota Research Institute (TRI) and NSERC Discovery Grant. The authors thank Blerim Abdullai, Sebastian Aegidius, Jasper Gerigk, Ruthrash Hari, and Wei-Cheng Tseng for helpful discussions and feedback."
        },
        {
            "title": "References",
            "content": "[1] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. [2] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. In Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard, editors, Proceedings of The 8th Conference on Robot Learning, volume 270 of Proceedings of Machine Learning Research, pages 26792713. PMLR, 0609 Nov 2025. [3] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, 2024. [4] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. [5] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. [6] Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. [7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-languageaction models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. [8] Chen Xu, Tony Khuong Nguyen, Emma Dixon, Christopher Rodriguez, Patrick Miller, Robert Lee, Paarth Shah, Rares Ambrus, Haruki Nishimura, and Masha Itkina. Can we detect failures without failure data? uncertainty-aware runtime failure detection for imitation learning policies. arXiv preprint arXiv:2503.08558, 2025. [9] Huihan Liu, Yu Zhang, Vaarij Betala, Evan Zhang, James Liu, Crystal Ding, and Yuke Zhu. Multi-task interactive robot fleet learning with visual world models. In 8th Annual Conference on Robot Learning (CoRL), 2024. [10] Rohan Sinha, Amine Elhafsi, Christopher Agia, Matthew Foutter, Ed Schmerling, and Marco Pavone. Real-time anomaly detection and reactive planning with large language models. In Robotics: Science and Systems, 2024. [11] Josiah Wong, Albert Tung, Andrey Kurenkov, Ajay Mandlekar, Li Fei-Fei, Silvio Savarese, and Roberto Martín-Martín. Error-aware imitation learning from teleoperation data for mobile manipulation. In Aleksandra Faust, David Hsu, and Gerhard Neumann, editors, Proceedings of the 5th Conference on Robot Learning, volume 164 of Proceedings of Machine Learning Research, pages 13671378. PMLR, 0811 Nov 2022. [12] Huihan Liu, Shivin Dass, Roberto Martín-Martín, and Yuke Zhu. Model-based runtime monitoring with interactive imitation learning. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 41544161. IEEE, 2024. [13] Cem Gokmen, Daniel Ho, and Mohi Khansari. Asking for help: Failure prediction in behavioral cloning through value approximation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 58215828. IEEE, 2023. [14] Annie Xie, Fahim Tajwar, Archit Sharma, and Chelsea Finn. When to ask for help: Proactive interventions in autonomous reinforcement learning. Advances in Neural Information Processing Systems, 35:1691816930, 2022. [15] Rohan Sinha, Edward Schmerling, and Marco Pavone. Closing the loop on runtime monitors with fallback-safe mpc. In 2023 62nd IEEE Conference on Decision and Control (CDC), pages 65336540. IEEE, 2023. [16] Alec Farid, David Snyder, Allen Z. Ren, and Anirudha Majumdar. Failure prediction with statistical guarantees for vision-based robot control. In Robotics: Science and Systems XVIII, New York City, NY, USA, June 27 - July 1, 2022, 2022. [17] Christopher Agia, Rohan Sinha, Jingyun Yang, Ziang Cao, Rika Antonova, Marco Pavone, and Jeannette Bohg. Unpacking failure modes of generative policies: Runtime monitoring of consistency and progress. In Proceedings of The 8th Conference on Robot Learning, volume 270 of Proceedings of Machine Learning Research, pages 689723. PMLR, 2025. [18] Jiafei Duan, Wilbert Pumacay, Nishanth Kumar, Yi Ru Wang, Shulin Tian, Wentao Yuan, Ranjay Krishna, Dieter Fox, Ajay Mandlekar, and Yijie Guo. Aha: vision-language-model for detecting and reasoning over failures in robotic manipulation. arXiv preprint arXiv:2410.00371, 2024. [19] Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando de Freitas, and Serkan Cabi. Vision-language models as success detectors. In Conference on Lifelong Learning Agents, pages 120136. PMLR, 2023. [20] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:4477644791, 2023. [21] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. [22] Jacopo Diquigiovanni, Matteo Fontana, Simone Vantini, et al. The importance of being band: Finite-sample exact distribution-free prediction sets for functional data. STATISTICA SINICA, 1:141, 2024. [23] Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu, and Lei Ma. Look before you leap: An exploratory study of uncertainty measurement for large language models. arXiv preprint arXiv:2307.10236, 2023. [24] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances In The Eleventh International for uncertainty estimation in natural language generation. Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023. [25] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631, 2024. 11 [26] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, and Tao Kong. Vision-language foundation models as effective robot imitators. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024. [27] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [28] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. [29] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In International Conference on Machine Learning, pages 2312323144. PMLR, 2024. [30] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Kostas E. Bekris, Kris Hauser, Sylvia L. Herbert, and Jingjin Yu, editors, Robotics: Science and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023, 2023. [31] Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Zhibin Tang, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, et al. Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. IEEE Robotics and Automation Letters, 2025. [32] Open X-Embodiment Collaboration. Open x-embodiment: Robotic learning datasets and RT-X models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 68926903, 2024. doi: 10.1109/ICRA57147.2024.10611477. [33] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, et al. DROID: large-scale in-the-wild robot manipulation dataset. In Robotics: Science and Systems XX, Delft, The Netherlands, July 15-19, 2024, 2024. [34] Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe HansenEstruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pages 17231736. PMLR, 2023. [35] Rohan Sinha, Apoorva Sharma, Somrita Banerjee, Thomas Lew, Rachel Luo, Spencer Richards, Yixiao Sun, Edward Schmerling, and Marco Pavone. system-level view on out-ofdistribution data in robotics. arXiv preprint arXiv:2212.14020, 2022. [36] Rajesh Natarajan, Santosh Reddy, Subash Chandra Bose, HL Gururaj, Francesco Flammini, and Shanmugapriya Velmurugan. Fault detection and state estimation in robotic automatic control using machine learning. Array, 19:100298, 2023. [37] Quazi Marufur Rahman, Peter Corke, and Feras Dayoub. Run-time monitoring of machine learning for robotic perception: survey of emerging trends. IEEE Access, 9:2006720075, 2021. [38] Chen Xu, Xiuyuan Cheng, and Yao Xie. Normalizing flow neural networks by jko scheme. Advances in Neural Information Processing Systems, 36:4737947405, 2023. [39] Nantian He, Shaohui Li, Zhi Li, Yu Liu, and You He. Rediffuser: Reliable decision-making using diffuser with confidence estimation. In International Conference on Machine Learning, pages 1792117933. PMLR, 2024. [40] Sivakumar Balasubramanian, Alejandro Melendez-Calderon, Agnes Roby-Brami, and Etienne Burdet. On the analysis of movement smoothness. Journal of neuroengineering and rehabilitation, 12:111, 2015. [41] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024. 12 [42] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Comput. Surv., 55(12):248:1248:38, 2023. [43] Ola Shorinwa, Zhiting Mei, Justin Lidard, Allen Ren, and Anirudha Majumdar. survey on uncertainty quantification of large language models: Taxonomy, open research challenges, and future directions. arXiv preprint arXiv:2412.05563, 2024. [44] Chen Ling, Xujiang Zhao, Wei Cheng, Yanchi Liu, Yiyou Sun, Xuchao Zhang, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang Zhao, and Haifeng Chen. Uncertainty decomposition and quantification for in-context learning of large language models. In 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2024. URL https://openreview.net/forum?id=Oq1b1DnUOP. [45] Yijun Xiao and William Yang Wang. On hallucination and predictive uncertainty in conditional language generation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 27342744, 2021. [46] Andrey Malinin and Mark J. F. Gales. Uncertainty estimation in autoregressive structured prediction. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [47] Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Generating with confidence: Uncertainty quantification for black-box large language models. Trans. Mach. Learn. Res., 2024, 2024. URL https://openreview.net/forum?id=DWkJCSxKU5. [48] Jiuhai Chen and Jonas Mueller. Quantifying uncertainty in answers from any language model and enhancing their trustworthiness. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 51865200, 2024. [49] Amos Azaria and Tom Mitchell. The internal state of an llm knows when its lying. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 967976, 2023. [50] Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inferencetime intervention: Eliciting truthful answers from language model. Advances in Neural Information Processing Systems, 36:4145141530, 2023. [51] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [52] Xuefeng Du, Chaowei Xiao, and Sharon Li. Haloscope: Harnessing unlabeled llm generations for hallucination detection. Advances in Neural Information Processing Systems, 37:102948 102972, 2024. [53] Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, and Yarin Gal. Semantic entropy probes: Robust and cheap hallucination detection in llms. arXiv preprint arXiv:2406.15927, 2024. [54] Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. INSIDE: llms internal states retain the power of hallucination detection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [55] Seongheon Park, Xuefeng Du, Min-Hsuan Yeh, Haobo Wang, and Yixuan Li. How to steer llm latents for hallucination detection? arXiv preprint arXiv:2503.01917, 2025. [56] Anastasios Angelopoulos and Stephen Bates. gentle introduction to conformal prediction and distribution-free uncertainty quantification. arXiv preprint arXiv:2107.07511, 2021. [57] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 17351780, 1997. [58] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in random world, volume 29. Springer, 2005. [59] Chen Xu and Yao Xie. Conformal prediction for time series. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(10):1157511587, 2023. doi: 10.1109/TPAMI.2023. 3272339. [60] Chen Xu and Yao Xie. Sequential predictive conformal inference for time series. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 3870738727. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/xu23r.html. [61] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Oier Mees, Karl Pertsch, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, et al. Evaluating real-world robot manipulation policies in simulation. In 8th Annual Conference on Robot Learning, 2024. [62] Allen Z. Ren. open-pi-zero: Re-implementation of pi0 vision-language-action (vla) model from physical intelligence. https://github.com/allenzren/open-pi-zero, 2025. Version 0.1.1, released January 27, 2025. Accessed April 6, 2025. [63] Yashvir Grewal, Edwin Bonilla, and Thang Bui. Improving uncertainty quantification in large language models via semantic embeddings. arXiv preprint arXiv:2410.22685, 2024. [64] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/jax-ml/jax. [65] Mingjian Jiang, Yangjun Ruan, Prasanna Sattigeri, Salim Roukos, and Tatsunori Hashimoto. Graph-based uncertainty metrics for long-form language model outputs. arXiv preprint arXiv:2410.20783, 2024. [66] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. π0.5: visionlanguage-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. [67] Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. [68] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023. [69] Joe Ward Jr. Hierarchical grouping to optimize an objective function. Journal of the American statistical association, 58(301):236244, 1963. [70] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alexander Smola. kernel two-sample test. The Journal of Machine Learning Research, 13(1):723773, 2012. [71] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015."
        },
        {
            "title": "A Experiment Details",
            "content": "A.1 Vision-Language-Action Models We conduct experiments on 3 state-of-the-art large VLA models: OpenVLA [2], π0 [4] and π0FAST [5]. Given the internal feature vectors Rnd produced by VLA model, where dimension corresponds to different token positions, diffusion steps, etc. and is the feature dimension, we aggregate into single fixed-dimensional feature vector Rd before inputting to the proposed SAFE models. In this paper, we consider and ablate the following ways of feature aggregation: First: take the first vector along the dimension n, = E1; Last: take the last vector along the dimension n, = En; Mean: take average over the dimension n, = 1 First&Last: concatenate the first and the last vector, = concat(E0, En) R2d i=1 Ei; (cid:80)n . Both OpenVLA and π0-FAST first predict sequence of discrete tokens and then convert them into continuous actions. We take feature vectors before being decoded into the output tokens from the last transformer block as Rnd , and therefore corresponds to the number of generated tokens. We ablate the aggregation method along this dimension and denote the aggregation method as aggtoken. For π0-FAST, we additionally ablate using the feature vectors before (encoded) and after (pre-logits) the final RMS normalization layer as E. Differently, π0 (and π 0) outputs action vectors by flow matching [68], and we take the feature vectors before being projected into the velocity field. Suppose π0 predict an action chunk of horizon and performs flow matching steps, the internal features become RHkd and we perform the aggregation process along the dimension and the dimension separately to get the final embedding vector Rd . The aggregation methods are denoted as agghori and aggdiff along these two dimensions, respectively. For all VLA models, we ablate different methods of aggregating the hidden features into single feature vector and select the best method according to Deval-seen performance. The detailed ablation results are shown in Appendix A.7. OpenVLA and π 0 use MIT License; π0 and π0-FAST use Apache-2.0 license. A.2 SAFE Failure Detector SAFE-LSTM uses an LSTM model with 1 layer and hidden dimension of 256, and an additional linear layer is used to project the hidden states of LSTM into single scalar st. SAFE-MLP uses multi-layer perceptron with 2 layers and hidden dimension of 256. Since successes and failures from the generated rollouts are imbalanced, the losses on positive (failed) and negative (successful) rollouts are weighted by their inverse class frequency. We also apply an L2 regularization loss on the model weights to reduce overfitting, and this loss is weighted by λreg and optimized together with the failure score learning loss LLSTM or LMLP. λreg are determined by grid search. A.3 Failure Detection Baselines For the cluster entropy baseline, we use agglomerative clustering with the ward linkage criterion [69]. The distance threshold is denoted as δ and decided by grid search. For the STAC baseline [17], we use the Maximum Mean Discrepancy (MMD) distance measure [70] with radial basis function kernels, which was reported to have the best performance by [17]. The bandwidth of the RBF kernel is 1. For all baselines except for RND [39] and LogpZO [8], we ablate one version that only considers the failure score computed from the current timestep (cumsum=False) and another that uses the cumulative sum (cumsum) of the failure scores over time (cumsum=True). 15 For RND and LogpZO, we use the original implementation provided by the authors 3 and do not accumulate scores. In [8], RND and LogpZO are trained to model the distribution of (encoded) observations ot and predicted actions At. In this work, we adapt them to model the distribution of VLAs internal embeddings et. Note that as π0 (and π apply. And for OpenVLA, = = 1 and thus the STAC [17] and STAC-Single do not apply. 0) does not output discrete tokens, token uncertainty-based baselines do not A.4 Conformal Prediction We follow [22, 8] for CP band construction. Please refer to Section. in the Appendix of [8] for detailed formulation. Specifically, in our experiments, we use the adaptive modulation function (Equation 2 in the Appendix of [8]), which models the non-extreme behaviors of the functional data. A.5 Benchmark Details LIBERO [20]: We adopt the LIBERO-10 task suite, which contains the most diverse set of objects, environments, and instructions among the 4 LIBERO task suites, and therefore LIBERO-10 is considered the most challenging task suite. LIBERO-10 contains 10 tasks with 50 rollouts in each task. We use the initial conditions for all rollouts as specified and provided by the author4. To test VLA models on LIBERO, we adopt the trained model weights provided by the respective authors and do not further finetune them. On LIBERO-10, OpenVLA achieves success rate of 53.7%, π0-FAST achieves 60.2%, and π0 achieves 85.2%. For evaluation, 3 out of 10 tasks are unseen, and within seen tasks, 60% of rollouts are used for Dtrain and the remaining 40% for Deval-seen. Note that the LIBERO simulator stops the rollout execution when the robot finishes the task (considered success) or maximum rollout length is reached (considered failure). Therefore, in the generated rollouts, failed ones always have the maximum length, but successful ones are shorter. This could result in an unfair advantage for some of the compared failure detectors (if failure detector simply learns to count the time elapsed, i.e., st = t, it will achieve perfect failure detection since failed rollouts have fixed and longer duration). To ensure fair comparison, for evaluation in Table 1, we compute the minimum rollout length for each task and use that as for that task. The failure detection performance (in ROC-AUC) is then determined based on sT , where is the same for all successful and failed rollouts within each task. LIBERO benchmark uses the MIT license. SimplerEnv [61]: SimplerEnv carefully identifies and reduces the domain gap between the simulation and the real-world demonstration data, and provides simulated environments that highly resemble the demonstration data from RT-series [1, 7, 32] (with the Google Robot embodiment) and BridgeData V2 [34] (with the WidowX embodiment). They show that models pretrained on real-world datasets can also accomplish similar tasks in SimplerEnv without finetuning, and their performance in simulation matches that in the real world. On this benchmark, we adopt the pretrained model checkpoints of π 0 model checkpoints are trained separately on the Google Robot embodiment and the WidowX embodiment, which results in two model checkpoints that have different internal feature spaces. Therefore, all failure detectors are trained and evaluated on each embodiment separately as well. All reported evaluation metrics are computed separately for each embodiment and then averaged. In Table 2, we list the tasks used for failure detection on SimplerEnv. We generate 100 rollouts for each task with random initial configurations, and the success rates of π 0 on each task are also listed in Table 2. rollout stops after the maximum number of allowed timesteps have passed, regardless of task success or failure. Within each embodiment, 1 out of 4 tasks is unseen, and within the seen tasks, 66% of the rollouts are in Dtrain and the remaining 33% in Deval-seen. 0 [62]. Note that π SimplerEnv benchmark uses the MIT license. Real-world experiments with Franka robot: In Table 3, we list the tasks used in the real-world experiments. For each task, we set number of timesteps allowed for one rollout, and all rollouts of 3https://github.com/CXU-TRI/FAIL-Detect 4https://github.com/Lifelong-Robot-Learning/LIBERO/tree/master/libero/libero/init_files Table 2: List of tasks used in SimplerEnv benchmark. Embodiment Task ID Environment Name Google Robot Google Robot Google Robot Google Robot WidowX WidowX WidowX WidowX 1 2 3 4 1 2 3 4 google_robot_move_near_v0 google_robot_open_drawer google_robot_close_drawer google_robot_place_apple_in_closed_top_drawer widowx_carrot_on_plate widowx_put_eggplant_in_basket widowx_spoon_on_towel widowx_stack_cube π 0 Success Rate (%) 77 50 80 40 44 88 79 Table 3: List of tasks used in the real-world experiments. Task 1 2 3 4 5 6 7 8 9 10 11 12 13 Instruction close the door close the drawer pick up the ball and place it in the bowl pick up the knife and put it on the plate pick up the lid and place it on the pot pick up the lid from the pot and place it on the table pick up the marker and place it in the cup place the green block on the yellow block place the pink cup to the right of the blue cup press the button put both the carrot and the ball in the bowl put the cup to the upright position unfold the cloth Rollout Length 300 200 400 350 400 400 400 350 300 200 500 500 500 the same task are terminated after the same timesteps regardless of task success or failure. In Fig. 6, we further visualize some example successful and failed rollouts from the real-world experiments. A.6 Training Details We use Adam optimizer [71] with β1 = 0.9, β1 = 0.999, ϵ = 108, and learning rate (lr) determined by grid search. The SAFE models are trained for 1000 epochs with batch size 512. Note that each rollout is considered as one data point and thus batch size of 512 translates to training on (at most) 512 rollouts in each iteration. All training and evaluation are done on single NVIDIA A100 40GB GPU. Since SAFE uses small networks (MLP or LSTM with 1 or 2 layers), the typical training time for one model is less than one minute. A.7 Hyperparameter Tuning To determine the hyperparameters for the proposed SAFE and baselines, we perform grid search over them and select the ones with the highest failure detection performance (ROC-AUC) on the Deval-seen split. In Table 5, Table 6, and Table 7, we report the hyperparameters we have searched over and the values with the best performance. Note that for the real-world experiments, we fix the et to be the pre-logits with Mean aggregation."
        },
        {
            "title": "B Additional Results and Discussions",
            "content": "B.1 Feature Visualization and Analysis We perform the feature analysis similar to Section 4.1 and Fig. 1 on other benchmarks and show the plots in Fig. 7, Note that in this feature analysis process, the t-SNE algorithm was performed on the VLAs embeddings without any learning. Therefore, the feature dimension reduction process is unsupervised and does not know about task successes or failures. 17 Comparing the plots in Fig. 1 and Fig. 7, we can see that the embedding spaces from VLAs are different from each other, which corresponds to the different failure patterns presented by the VLAs. For π0-FAST  (Fig. 1)  and π0 on LIBERO (Fig. 7a and b), when task execution fails, the embeddings fall into the same region (failure zone). This corresponds to the major failure mode of π0-FAST trained on the LIBERO dataset, where the predicted actions At become unstable and the robot arms move to weird configurations and out of the observation frame. For OpenVLA on LIBERO (Fig. 7c and d). we observe that for most failed rollouts, the robot freezes at or shakes around certain configurations during the middle of task execution. Such failed rollouts result in features very close to each other, which corresponds to small blobs of red dots in Fig. 7c. Despite the different appearances of the embedding spaces from the above benchmarks, their successful and failed rollouts are separable in the feature space. This is aligned with the high performance of the proposed SAFE and the embedding-based baseline methods. Moreover, although the embeddings of the failed rollouts from OpenVLA are spread over the space and do not form unified failure zone, SAFE is still able to learn to separate task failures from successes (possibly by extracting the correlations that are not visualized by t-SNE) and generalize well to unseen tasks, as reported in Table 1. However, the visualized embeddings of π0-FAST on the real Franka robot (Fig. 7e and f) are different, where embeddings from successful and failed rollouts are not easily separable through the t-SNE visualization. We hypothesize that because the tasks we used for real-world experiments are more diverse, their failures do not have unified semantic meaning, and thus the embeddings are not clearly separated in the visualization. This explains the limited performance of all failure detection methods as reported by Fig. 5, where ROC-AUC is at most 64 on Deval-unseen. Nevertheless, SAFE-MLP still outperforms all baselines on both seen and unseen splits in this evaluation. B.2 Conformal Prediction Results We use functional CP [8, 22] to determine the time-varying thresholds δt for failure detection. By varying the significance level α used in functional CP, we can adjust the conservativeness of failure detection and get different performance. In Fig. 8, we plot the change of TNR (True Negative Rate), TPR (True Positive Rate) and Bal-acc (Balanced Accuracy, TNR+TPR ) w.r.t. α. 2 Note that we calibrate the CP bands on successful rollouts (negative data points), and thus if the assumptions used in CP (st are sampled i.i.d.) hold, the TNR rate is lower bounded by and close to 1 α (the gray dashed line in the TNR plots in Fig. 8) [22]. However, as the multitask failure detection problem requires detecting failures on tasks that are not in the training or the calibration sets, we need to calibrate CP bands on Deval-seen and then evaluate them on Deval-unseen. Therefore, the i.i.d assumption may not hold, and TNR may deviate from the gray dashed line. From Fig. 8, we can see that on OpenVLA+LIBERO and π ) +SimplerEnv benchmarks, the TNR curves obtained by SAFE are close to the gray dashed line 1 α, while those on the other 3 benchmarks are lower than 1 α. similar phenomenon is also observed for the baseline methods: none of the TNR curves obtained from the baselines consistently conform to the 1 α curve across all benchmarks. We attribute this to the challenging nature of the multitask failure detection problem, where the failure scores for calibration and evaluation may not come from the same distribution. Nevertheless, we still adopt the functional CP as principled method to determine the time-varying failure detection threshold δt. Moreover, from Fig. 8, we can see that SAFE can achieve higher TPR and result in fewer false negatives compared to the baselines. This is crucial for safety-critical environments, where missing failure (false negative) can be much more catastrophic than false alarm (false positive). B.3 Failure Detection Time As mentioned in Section 6.2, we manually label when the failure happens for the failed rollouts. The labeling process is based on video recordings after all rollouts are collected and no interventions were done during the task execution. While the exact times of failure are clear for some failure modes (e.g. dangerous actions, breaking objects), they can be ambiguous and hard to annotate for other failure modes. For example, policy may freeze in the middle of task execution, and after that either recovering from it or getting stuck indefinitely can be possible. In another case, policy may repeatedly try grasping the object but keep missing the grasp until timeout, and its hard to determine 18 Table 4: Mean and standard deviation of failure detection ROC-AUC on all benchmarks. This table complements the results from Table 1 and Fig. 5 left. VLA Model Benchmark Eval Task Split π 0 SimplerEnv π0-FAST Real Franka OpenVLA LIBERO π0-FAST LIBERO π0 LIBERO Unseen Unseen Unseen Unseen Unseen Seen Seen Seen Seen Seen 50.252.51 Max prob. 44.051.26 Avg prob. 52.944.36 Max entropy 45.271.78 Avg entropy Mahalanobis dist. 62.035.11 Euclidean dist. k-NN 66.002.33 Cosine dist. k-NN 67.092.74 57.182.04 PCA-KMeans [9] 52.574.56 RND [39] 61.573.62 LogpZO [8] 62.761.66 Action total var. 55.332.06 Trans. total var. 47.852.88 Rot. total var. 61.842.67 Gripper total var. 50.162.36 Cluster entropy - STAC [17] - STAC-Single SAFE-LSTM 70.241.49 SAFE-MLP 72.682.38 53.836.32 51.581.82 53.097.68 50.033.18 58.854.16 55.2310.05 69.456.14 55.101.16 46.884.92 52.915.79 65.432.50 58.995.13 55.304.38 64.483.05 51.441.01 - - 72.475.55 73.475.39 61.329.57 52.463.44 46.6913.33 50.931.22 93.562.32 92.042.39 92.091.70 68.464.92 88.673.05 91.522.39 76.957.22 78.214.09 80.875.85 76.827.10 80.227.37 83.074.61 85.466.55 92.982.62 90.062.82 69.4413.61 58.045.64 62.9619.62 58.633.47 83.797.18 84.126.47 84.644.90 57.1210.44 81.578.67 83.077.17 74.5012.19 80.039.11 77.298.71 74.4212.13 80.538.65 85.316.71 81.168.63 84.487.29 80.445.72 - - - - 77.128.57 75.646.20 75.766.16 64.928.90 71.927.02 76.809.12 77.205.65 49.389.95 52.947.56 77.195.66 76.194.31 46.558.90 68.465.10 76.985.34 73.507.43 - - - - 74.3112.64 70.7316.69 70.3116.84 60.3519.93 69.4419.39 73.2311.64 75.185.08 54.717.57 61.0610.60 75.195.08 72.121.04 47.9120.94 69.398.22 71.096.94 73.2711. - - - - 88.422.82 89.733.08 90.194.05 66.885.10 85.074.04 88.794.92 68.4110.81 63.277.17 58.0710.41 69.169.50 68.259.03 60.7413.89 68.717.06 88.856.30 89.504.49 - - - - 52.8431.97 68.419.22 71.3212.02 61.1914.76 65.896.52 74.6614.96 67.9415.97 55.9019.19 62.109.39 69.2914.77 73.6616.03 62.2116.72 70.408.76 80.1110.49 84.828.12 53.743.46 51.603.12 59.233.06 50.673.96 75.544.07 80.355.36 80.235.12 51.914.20 62.005.44 64.437.82 - - - - - - 45.243.68 77.275.82 86.762.64 48.593.00 47.304.32 53.503.15 46.084.79 53.935.06 60.274.79 59.515.76 49.866.19 45.835.10 52.243.68 - - - - - - 38.019.81 58.704.37 64.165.88 single point of failure. To handle such cases, we instruct the human annotators to pick the time where they think intervention is needed and they should take over control to prevent an execution failure. In practice, for the above ambiguous failure modes, we annotate the failures after the policy gets stuck by few seconds or re-tries the grasping action few times. For some rollouts that look very plausible but do not succeed due to the time limit, the failure time is annotated as the end of the rollout. Note that we annotate only the failed rollouts and not the successful ones, even though they may also show subtle signs of failure in the middle. In Fig. 9, we compare the times of failure detected by the proposed SAFE-MLP model and human annotator. From Fig. 9, we can see that for both π0 and π0-FAST models, SAFE-MLP can detect failures before they happen (as identified by human). When used for π0-FAST deployed on LIBERO, SAFE-MLP can forecast failures well in advance and even predict 40% of the failures after the first timestep. Furthermore, from Fig. 9a and Fig. 9c, we can see that the blue curves jump up on the right edge of the plots. This means that the human annotator does not think these rollouts are failures until the very last moment, where the VLA model is probably on the right track and fails only due to timeout. We think such failures are also hard for failure detectors to detect, and it explains the low performance of all failure detectors on these benchmarks. B.4 Result Variance In Table 4, we report the standard deviation for all results in Table 1 and Fig. 5 left. Note that for the repeated runs, not only are they using different random seeds, also the tasks are split differently into the seen and the unseen subsets. Since different tasks have different difficulties for failure detection, it is normal to see large standard deviations in Table 4. From Table 4, we can see that the proposed SAFE methods achieve high averaged performance with relatively low standard deviations compared to the baselines, across all evaluation benchmarks. This signifies the strong and also stable performance of SAFE. 19 Table 5: Grid-searched and best-performing hyperparameters (in bold text) for OpenVLA+LIBERO (left) and π0-FAST+LIBERO (right). Method Max prob. Avg prob. Max entropy Avg entropy Mahalanobis dist. Euclidean dist. k-NN Cosine dist. k-NN PCA-KMeans RND LogpZO Action total var. Trans. total var. Rot. total var. Gripper total var. Cluster entropy SAFE-LSTM SAFE-MLP HParams cumsum cumsum cumsum cumsum aggtoken cumsum aggtoken cumsum aggtoken cumsum aggtoken cumsum clusters dim aggtoken aggtoken cumsum cumsum cumsum cumsum cumsum δ aggtoken lr λreg aggtoken lr λreg Values True False True False True False True False First Last Mean True False First Last Mean True False 1 5 10 First Last Mean True False 1 5 10 First Last Mean True False 16 32 64 32 64 128 First Last Mean First Last Mean True False True False True False True False True False 0.01 0.05 First Last Mean 1e-4 3e-4 1e-3 1e-3 1e-2 1e-1 1 First Last Mean 1e-4 3e-4 1e-3 1e-3 1e-2 1e-1 1 Method Max prob. Avg prob. Max entropy Avg entropy Mahalanobis dist. Euclidean dist. k-NN Cosine dist. k-NN PCA-KMeans RND LogpZO Action total var. Trans. total var. Rot. total var. Gripper total var. Cluster entropy STAC STAC-Single SAFE-LSTM SAFE-MLP HParams cumsum cumsum cumsum cumsum aggtoken Feat cumsum aggtoken Feat cumsum aggtoken Feat cumsum aggtoken Feat cumsum clusters dim aggtoken Feat aggtoken Feat cumsum cumsum cumsum cumsum cumsum δ cumsum cumsum aggtoken Feat lr λreg aggtoken Feat lr λreg Values True False True False True False True False First Last Mean Encoded Pre-logits True False First Last Mean Encoded Pre-logits True False 1 5 10 First Last Mean Encoded Pre-logits True False 1 5 10 First Last Mean Encoded Pre-logits True False 16 32 64 32 64 128 First Last Mean Encoded Pre-logits First Last Mean Encoded Pre-logits True False True False True False True False True False 0.01 0.05 0.1 0.2 0.5 1 2 5 True False True False First Last Mean Encoded Pre-logits 3e-5 1e-4 3e-4 1e-3 1e-3 1e-2 1e-1 First Last Mean Encoded Pre-logits 3e-5 1e-4 3e-4 1e-3 1e-3 1e-2 1e-1 20 Figure 6: Example successful and failed rollouts from real-world experiments. 21 (a) π0+LIBERO, colored by task success (b) π0+LIBERO, colored by task ID (c) OpenVLA+LIBERO, colored by task success (d) OpenVLA+LIBERO, colored by task ID (e) π0-FAST+Franka, colored by task success (f) π0-FAST+Franka, colored by task ID Figure 7: t-SNE plots of VLAs internal features, from different evaluation benchmarks. Figure 8: Additional failure detection results using δt obtained by functional CP. These plots show TNR (left column), TPR (middle column), and Bal-acc (right column) w.r.t. the significance level α, for each evaluation benchmark. These plots are obtained with random seed= 0. 23 (a) π0 on LIBERO benchmark. (b) π0-FAST on LIBERO benchmark. (c) π0-FAST on the real Franka robot. Figure 9: Comparison between detected and ground truth (GT) failure w.r.t time. On the left column, we plot the cumulative number of true failures (true positives) detected by SAFE-MLP (red) and human annotator (blue), w.r.t. elapsed time in each rollout. The right column shows the time of failures detected by SAFE-MLP (y-axis) and human annotator (x-axis) for each rollout, where failures missed by the detector (false negatives) are plotted in blue crosses. Experiments are done with seed 0 and functional CP with significance level α = 0.15. 24 Table 6: Grid-searched and best-performing hyperparameters (in bold text) for π0+LIBERO (left) and π Method Mahalanobis dist. Method Mahalanobis dist. PCA-KMeans Cosine dist. k-NN Euclidean dist. k-NN 0+SimplerEnv (right). HParams agghori aggdiff cumsum agghori aggdiff cumsum agghori aggdiff cumsum agghori aggdiff cumsum clusters dim agghori aggdiff agghori aggdiff cumsum cumsum cumsum cumsum cumsum δ cumsum cumsum agghori aggdiff lr λreg agghori aggdiff lr λreg RND LogpZO Action total var. Trans. total var. Rot. total var. Gripper total var. Cluster entropy STAC STAC-Single SAFE-LSTM SAFE-MLP Values First Last First&Last First Last First&Last True False First Last First&Last First Last First&Last True False 1 5 10 First Last First&Last First Last First&Last True False 1 5 10 First Last First&Last First Last First&Last True False 16 32 64 32 64 128 First Last First&Last First Last First&Last First Last First&Last First Last First&Last True False True False True False True False True False 0.01 0.05 0.1 0.2 0.5 1 2 5 True False True False First Last First&Last First Last First&Last 1e-5 3e-5 1e-4 3e-4 1e-3 1e-3 1e-2 1e-1 First Last First&Last First Last First&Last 1e-5 3e-5 1e-4 3e-4 1e-3 1e-3 1e-2 1e-1 Euclidean dist. k-NN Cosine dist. k-NN PCA-KMeans RND LogpZO Action total var. Trans. total var. Rot. total var. Gripper total var. Cluster entropy STAC STAC-Single SAFE-LSTM SAFE-MLP HParams agghori aggdiff cumsum agghori aggdiff cumsum agghori aggdiff cumsum agghori aggdiff cumsum clusters dim agghori aggdiff agghori aggdiff cumsum cumsum cumsum cumsum cumsum δ cumsum cumsum agghori aggdiff lr λreg agghori aggdiff lr λreg Values First Last Mean First&Last First Last Mean First&Last True False First Last Mean First&Last First Last Mean First&Last True False 1 5 10 First Last Mean First&Last First Last Mean First&Last True False 1 5 10 First Last Mean First&Last First Last Mean First&Last True False 16 32 64 32 64 128 First Last Mean First&Last First Last Mean First&Last First Last Mean First&Last First Last Mean First&Last True False True False True False True False True False 0.01 0.05 0.1 0.2 0.5 1 2 5 True False True False First Last Mean First&Last First Last Mean First&Last 1e-4 3e-4 1e-3 1e-3 1e-2 1e-1 1 First Last Mean First&Last First Last Mean First&Last 1e-4 3e-4 1e-3 1e-3 1e-2 1e-1 1 Table 7: Grid-searched and best-performing hyperparameters (in bold text) for π0-FAST on real-world rollouts. Method Max prob. Avg prob. Max entropy Avg entropy Mahalanobis dist. Euclidean dist. k-NN Cosine dist. k-NN PCA-KMeans STAC-Single SAFE-LSTM SAFE-MLP HParams cumsum cumsum cumsum cumsum cumsum cumsum cumsum cumsum clusters dim cumsum lr λreg lr λreg Values True False True False True False True False True False True False 1 5 10 True False 1 5 10 True False 16 32 64 32 64 128 True False 1e-4 3e-4 1e-3 3e-3 1e-3 1e-2 1e-1 1e-4 3e-4 1e-3 3e-3 1e-3 1e-2 1e-"
        }
    ],
    "affiliations": [
        "Toyota Research Institute (TRI)",
        "University of Toronto (UofT)",
        "UofT Robotics Institute",
        "Vector Institute"
    ]
}