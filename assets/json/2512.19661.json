{
    "paper_title": "Over++: Generative Video Compositing for Layer Interaction Effects",
    "authors": [
        "Luchao Qi",
        "Jiaye Wu",
        "Jun Myeong Choi",
        "Cary Phillips",
        "Roni Sengupta",
        "Dan B Goldman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In professional video compositing workflows, artists must manually create environmental interactions-such as shadows, reflections, dust, and splashes-between foreground subjects and background layers. Existing video generative models struggle to preserve the input video while adding such effects, and current video inpainting methods either require costly per-frame masks or yield implausible results. We introduce augmented compositing, a new task that synthesizes realistic, semi-transparent environmental effects conditioned on text prompts and input video layers, while preserving the original scene. To address this task, we present Over++, a video effect generation framework that makes no assumptions about camera pose, scene stationarity, or depth supervision. We construct a paired effect dataset tailored for this task and introduce an unpaired augmentation strategy that preserves text-driven editability. Our method also supports optional mask control and keyframe guidance without requiring dense annotations. Despite training on limited data, Over++ produces diverse and realistic environmental effects and outperforms existing baselines in both effect generation and scene preservation."
        },
        {
            "title": "Start",
            "content": "Over++: Generative Video Compositing for Layer Interaction Effects Luchao Qi1* Jiaye Wu2 1University of North Carolina at Chapel Hill Jun Myeong Choi1 Cary Phillips3 Roni Sengupta1 Dan Goldman3 2University of Maryland 3Industrial Light & Magic 5 2 0 2 2 2 ] . [ 1 1 6 6 9 1 . 2 1 5 2 : r Project Page: https://overplusplus.github.io Foreground Background Input w/o effects Output w/ effects Effect control: mask Effect control: prompt Foreground Multiple backgrounds (BG) Output (over BG 1) Output (over BG 2) Output (over BG 3) Figure 1. Over++. Our video generation model synthesizes environmental effects between foreground and background layers of video composite. Top: Over++ supports versatile effect control, such as guiding smoke effect with mask (2nd from right), and/or modifying it with text prompt (Red smoke at far right). Bottom: Given multiple background options, Over++ can generate diverse context-aware effects, such as shadows for BG 1, dust and shadows for BG 2, and splashes and reflections for BG 3."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction In professional video compositing workflows, artists must manually create environmental interactionssuch as shadows, reflections, dust, and splashesbetween foreground subjects and background layers. Existing video generative models struggle to preserve the input video while adding such effects, and current video inpainting methods either require costly per-frame masks or yield implausible results. We introduce augmented compositing, new task that synthesizes realistic, semi-transparent environmental effects conditioned on text prompts and input video layers, while preserving the original scene. To address this task, we present Over++, video effect generation framework that makes no assumptions about camera pose, scene stationarity, or depth supervision. We construct paired effect dataset tailored for this task and introduce an unpaired augmentation strategy that preserves text-driven editability. Our method also supports optional mask control and keyframe guidance without requiring dense annotations. Despite training on limited data, Over++ produces diverse and realistic environmental effects and outperforms existing baselines in both effect generation and scene preservation. *Work done during an internship at Industrial Light & Magic. In 1984, Porter and Duff [55] formalized the Algebra of Compositing, defining the over operator to combine image elements with pre-multiplied alpha channel. Today, visual effects compositor may construct vide clip using dozens or even hundreds of layers, starting from filmed or computer-generated background and foreground elements, and gradually inserting additional elements to marry these key elements together. Such additional elements may include physically-based environmental interactions such as shadows, reflections, dust, and water splashes (Fig. 1, bottom). In this work, we envision the application of generative video for compositing. We call our approach Over++, in homage to Porter and Duffs seminal paper. Traditional video compositing tools such as Foundrys Nuke [14] and Adobe After Effects [3] offer nondestructive editing pipelines across sequences of shots, but still require substantial manual effort from artists. Modern generative video models can produce highly realistic content, yet their stochastic behavior prevents them from integrating into the controlled, iterative workflows that professional compositors rely on. Recent inpainting-based approaches (e.g., [32]) allow users to specify regions for modification via masks, but, as shown in Fig. 2, they struggle to generate complex environmental effects such as wakes and require labor-intensive per-frame mask annotations. In this work, we propose augmented compositing: user provides foreground and background element, text prompt describing an environmental interaction, and an optional mask video specifying the regions in which the interaction should appear. The system automatically produces video adhering to the users desired effects. This can be seen as variation of inpainting that is specialized for semitransparent or stochastic effects which do not replace or fundamentally alter the other elements. This behavior is critical for professional use cases for two reasons: First, because it makes it possible to ensure continuity of both the subjects and environments appearance across sequence. Second, it adheres more accurately to the directorial intent of the filmmakers who acquired the input footage. We address the task by introducing data collection pipeline that generates paired synthetic videos (with and without effects), paired real-world videos (with and without effects), and unpaired videos (with effects). Our approach extends prior video generative models with training strategy that produces effects under optional mask guidance while preserving core instruction-following capabilities. Despite the limited training set (54 real-world paired, 573 synthetic paired, and 460 unpaired videos), Over++ generalizes effectively to diverse environmental effects, including shadows, dust, smoke, and water splashes  (Fig. 1)  . Our contributions include: (i) the introduction of augmented compositing as new generative task, (ii) Over++, video effect generation model fine-tuned on the newly constructed dataset, and (iii) comprehensive evaluation with quantitative and qualitative comparisons to prior work, including metrics tailored for this task. 2. Related Work VFX Generation. Visual effects (VFX) generation plays crucial role in modern video production. Traditional VFX creation often relies on physics-based simulations in digital content creation tools such as Houdini or Maya, or on manual compositing of pre-existing effect elements in software such as Nuke or After Effects [3, 14]. These approaches ensure physical plausibility and visual realism but are computationally expensive, time-consuming, and require substantial manual effort and artistic expertise. We refer the reader to the VES Handbook [50] for additional details. Recent works [24, 45, 48] have explored controllable VFX generation using generative models, either without effect references [48] or with reference exemplars [24]. These efforts aim to simplify VFX creation by leveraging generative AI to replace or complement manual simulation. However, they primarily target stylized or exaggerated effects [37], and do not explicitly model physically grounded environmental interactions between objects and their surroundings. Foreground Background Input w/o effects VACE [32] Output w/ effects Figure 2. Limitations of inpainting models for effects. Simply compositing foreground over background produces an input without effects. Inpainting models such as VACE [32] require perframe mask and may still fail to generate the desired effect. Our method successfully produces the target wake (far right). To bridge the gap between realism and efficiency, recent works [7, 43, 65] have explored integrating physical simulation with generative modeling, enabling controllable and physically plausible interactions. For instance, PISA [38] leverages simulated videos to fine tune generative models, primarily focusing on object-level phenomena such as free fall. While these approaches demonstrate promise, they are typically constrained to simple, isolated object dynamics. Extending simulation-driven generation to scenes with multiple interacting objects of different types is computationally expensive, and it remains costly and difficult to make perceptually convincing simulations of non-rigid or volumetric effects such as dust, fluids, and smoke. Omnimatte. Omnimatte [46] and related methods aim to decompose visual input into RGBA matte layers, each containing an object and its associated effects. In the image domain, recent works [10, 39, 72, 78] decompose an image into clean background layer and transparent foreground layer that preserves secondary visual effects such as shadows and reflections. Magic Fixup [4] further enables objectlevel editing while adaptively adjusting the associated effects. In the video domain, follow-up methods [35, 49] extend this decomposition to video layers, while OmnimatteZero [60] advances toward real-time performance. Although such decomposition-based approaches can be regarded as the inverse of our problemseparating rather than composing layersthey inherently lack explicit controllability. In contrast, our goal is to compose foreground and background videos under the joint guidance of spatial mask and textual prompt, enabling controllable synthesis of physically grounded environmental effects. Visual Concept Composition. Visual concept composition aims to merge multiple reference inputs into coherent output. Recent imageand video-based approaches [8, 9, 30, 42, 61, 69] leverage advances in generative modeling to combine visual content and style from reference inputs. Beyond static inputs, dynamic concept methods [1, 2, 73] compose content across videos, either by fine-tuning personalized models (e.g., DreamBooth [58], Textual Inversion [15]) or by parameter-efficient adaptation using LoRA [29]. While these methods achieve impressive visual coherence, our work targets distinct problem setting: given separate foreground and background layers, we synthe2 size environmental effectssuch as shadows, splashes, or smokethat perceptually connect the two layers without altering their original content or motion. This formulation differs from prior composition methods, which primarily focus on blending appearance or identity, often at the cost of foreground or background fidelitya key requirement in real-world compositing workflows. Video Generation and Control. Recent advances in video diffusion models have substantially improved video generation quality and controllability [5, 17, 71]. Existing approaches provide various forms of conditioning, including text-to-video (T2V) [25, 67, 74] and image-to-video (I2V) [57, 63, 68]. Force-Prompting [19] extends this line by fine-tuning an I2V backbone with force-based conditioning, enabling simulation of objectenvironment interactions such as wind acting on fabric. Recent works further explore joint imagevideo conditioning (I+V2V) [79], where I2VEdit [52] propagates first-frame edits across the entire sequence for temporally consistent control. Unified models such as UNIC [76] and VACE [32] aim to generalize across multiple video editing tasksincluding insertion, deletion, and camera controlwithin single framework. While these models achieve impressive controllability and visual fidelity, they primarily target objector appearance-level editing rather than the generation of environmental effects. ActAnywhere [53] synthesizes plausible effects given an edited background image; however, it is not directly applicable to professional compositing scenarios that require preserving dynamic background video, which our method explicitly supports. 3. Method In this section, we formulate augmented compositing as variation of the video inpainting problem (Sec. 3.1) and introduce our method, Over++. We treat effect synthesis primarily as supervised learning task and construct paired training videos with and without effects (Sec. 3.2). To enable fine-grained control over generated effects, we further incorporate spatial masking and text prompts (Sec. 3.3). Although trained mainly on synthetic data with limited real-world examples, Over++ generalizes effectively to inthe-wild videos, producing realistic, prompt-guided effects within arbitrary human-annotated masks. 3.1. Task Formulation Video visual effects compositing aims to merge separate video elementstypically foreground subject and background environmentinto single coherent scene while preserving their original appearances. However, naively overlaying these elements using simple over operation [55] fails to reproduce essential environmental interactions between the subject and its surroundings, such as shadows, reflections, and splashes. Achieving visual realism therefore requires synthesizing these effects in spatially and temporally consistent regions of the composed scene. To address this challenge, we extend the classic over formulation by introducing masking mechanism that specifies where foregroundbackground interaction effects should appear, enabling fine-grained spatial control over effect placement. Given simple RGB composite video foreground over background, denoted Iover, an effect mask video Meffect delineating the regions designated for effect generation, and text prompt describing the desired effect, our goal is to synthesize video ˆI that preserves the original appearance and motion of Iover while adding new, visually plausible effects consistent with within the masked regions of Meffect. To this end, we train video inpainting diffusion model G, referred to as Over++, which generates physically grounded effects conditioned jointly on Meffect and . Over++ fine-tunes base video inpainting diffusion transformer (DiT) model. During fine-tuning, we update all transformer attention blocks while keeping the VAE encoder and decoder frozen. Unlike prior inpainting methods, which zero out masked regions in Iover [40, 70], we pass through the fully-encoded latents to preserve scene context and suppress hallucinations. As shown in Fig. 3, Over++ denoises Gaussian noise to reconstruct the target video Igt, conditioned on the concatenated latents of the effect mask Meffect, the composite input video Iover, and the textual description via attention [66]. Formally, Igt ˆI = G(N ; Iover, Meffect, ), (1) Note that while professional compositing also involves complementary tasks such as motion alignment and color harmonization, these aspects are orthogonal to our scope and have been extensively studied in prior work on motion transfer [23] and video harmonization [33]. In contrast, our work focuses exclusively on effect generation, problem that remains comparatively underexplored. 3.2. Dataset The core challenge of training Over++ is the scarcity of paired videos (Iover, Igt) with and without effects. To address this, we leverage the Omnimatte methods [36, 41, 46] to decompose each video Igt into separable layers. Given video with effects Igt, Omnimatte methods decompose it into foreground layer fg and clean background layer Ibg, following the formulation Igt α fg + (1 α) Ibg, where α denotes the per-pixel alpha matte, and fg represents the video of the foreground subject Ifg and its associated effects. An example of such decomposition is shown in Fig. 4 (bottom). Given bikerpuddle video (Igt), it can be decomposed into foreground (I fg) containing the biker, splashes, and reflections, and background (Ibg) showing the undisturbed puddle as if no interaction occurs. 3 Input w/o effects Iover Effect mask Meffect Noisy video Output w/ effects ˆI Paired video training (Sec. 3.2) Eq. 1 Unpaired video training (Sec. 3.3.2) Figure 3. Over++ framework. Given an input composite video lacking environmental effects such as shadows or wakes (Iover), and an optional binary mask indicating the target effect regions (Meffect), our model Over++ generates desired effects within the specified regions (ˆI). Training includes unpaired data by zeroing out the latent codes of Iover and Meffect. (Text prompts are not shown here for simplicity.) We further extract the pure foreground subject Ifg from the decomposed foreground fg using subject mask Msubject obtained from off-the-shelf segmentation tools [44, 56]. The extracted subject Ifg is then re-composited over the clean background Ibg using the standard over operation [55], discarding the attached environmental effects. Formally, Iover = Msubject fg + (1 Msubject) Ibg, (2) We construct training videos from the following sources: (a) 54 real-world paired videos created from DAVIS [54], Pexels 1, and prior works [36, 41, 46, 60], featuring complex effects such as smoke, water splashes, and reflections; (b) 573 synthetic paired videos sourced from the Movies dataset [41] and additional synthetic data rendered using Blender [12] and Kubric [22], following the procedures in [36, 41]. These synthetic videos complement the limited real-world samples and introduce greater diversity in shadows and reflections, which are easier to simulate; and (c) 460 unpaired videos generated from pretrained T2V model. In addition to the paired videos from (a) and (b), we generate unpaired videos based on text prompts to produce diverse environmental effects. These unpaired T2V videos contain only Igt and do not have corresponding Meffect or Iover. Training with such unpaired data helps preserve the pretrained models text-to-video (T2V) generation capabilities and further enhances the classifier-free guidance (CFG)based prompt editing capacity of the final model. Details of T2V unpaired data (c) are discussed in Sec. 3.3.2. 1https://www.pexels.com/ Igt fg Sec. 3.2 Ibg Iover (Eq. 2) Sec. 3.3. δ(Igt, Iover) Meffect Figure 4. Mask generation for training data. Given training video with effects Igt, we construct version without effects Iover. The effect mask Meffect is derived by applying mask pruning to the difference image δ(Igt, Iover) to remove noise and artifacts. Top: Synthetic data with clean δ(Igt, Iover). Bottom: Real-world data has noisier δ(Igt, Iover), requiring additional cleanup. 3.3. Controlling Effect Generation 3.3.1. Mask-based Effect Generation VFX compositors often aim to generate effects within specific spatio-temporal regions, which we model using masks. Given the paired videos (Iover, Igt), we construct an effect mask Meffect that localizes regions of effect occurrence in Igt by computing the pixel-wise difference between the video pair, denoted as δ(Igt, Iover). However, due to imperfections in the subject segmentation mask Msubject, VAE reconstructions, and video decomposition [17, 62, 78], the decomposed foreground and background layers (I fg, Ibg) may not be perfectly aligned with Igt. Such misalignment introduces pixel-level noise and minor artifacts in the computed difference, resulting in imperfect effect (Fig. 4, bottom). To mitigate these artifacts, we first compute the pixelwise difference between the collected pair, δ(Igt, Iover), convert it to grayscale, and binarize it using Otsus thresholding [51]. The resulting binary mask is then refined through sequence of morphological operationserosion, dilation, and median filteringto suppress salt-and-pepper noise and enhance spatiotemporal consistency. As shown in Fig. 4, this mask pruning process effectively removes noise from the initial effect mask, especially for real-world data. Despite these refinements, the resulting masks Meffect may still be imperfect in precisely delineating effect regions at the pixel level. However, such imperfections act as form of natural data augmentation, enhancing robustness to loosely drawn user masks. Consequently, Over++ can operate effectively even when provided with coarse, handdrawn masksa property further demonstrated in Robust Mask Editing (Sec. 5). In real-world applications, the effect mask Meffect often requires frame-by-frame annotation by professional VFX artists, which is impractical for many use cases. To overcome this limitation, we introduce tri-mask design that supports training under both masked and unmasked conditions. Specifically, during training, we augment Meffect by randomly replacing it with uniform gray region to represent frames where effect regions are unknown or unannotated, indicating the frame may or may not contain effects, or that the locations of effects are uncertain. This design offers the following two key benefits: (i) it enables Over++ to operate seamlessly in both supervised (maskguided) and unsupervised (mask-free) settings, providing unified framework for diverse user scenarios; (ii) it allows Over++ to perform inference with mixed guidance within single modelallowing certain keyframes to be annotated while other remains unannotated, as further demonstrated in Keyframe Annotation (Sec. 5). 3.3.2. Prompt-based Effect Generation Users can generate diverse effects with text prompting. For instance, users may request turbulent versus calm splashes, soft versus harsh shadows, or red versus white smoke. Achieving such diversity demands strong prompt-editing capabilities from the model. However, when trained solely on the limited paired data described in Sec. 3.2, the model can suffer from language drift [47, 58], losing its inherent text-to-video (T2V) generation and prompt-editing abilities after fine-tuning. Ideally, this could be mitigated by training on multiple target videos Igt exhibiting diverse effects for the same input composite Iover, but such data are challenging to obtain or synthesize in practice. To address this limitation, we preserve the pre-trained models intrinsic T2V generation capabilities using additional unpaired data during training, and leverage classifier-free guidance (CFG) [27] at inference, enabling both maskand promptconditioned effect generation. To provide this additional unpaired data, we augment the training captions using LLM to generate semantically diverse descriptions of the same scene. For each caption, the language model produces multiple variants describing alternative physical effects or visual attributes, while maintaining the underlying scene semantics. The system prompt used for this augmentation is provided in the supplementary material (SM). We then use these augmented prompts to synthesize additional videos Igt with pre-trained T2V model, expanding the training corpus beyond the paired data. When training with these unpaired videos, we use only Igt and the caption , zeroing out the latents of the missing mask Meffect and input video Iover to preserve Over++s text-to-video generation capabilities within the unified training framework. 3.4. Implementation Details All experiments shown here were fine-tuned from the video inpainting variant2 of the text-to-video (T2V) diffusion transformer model, CogVideoX-5B [74]. We obtain video captions using MiniCPM-V-2.6 [75] to generate dense spatio-temporal captions, which are then refined with LLaMA-3.1-8B-Instruct [21] into concise video-level descriptions (system prompt in the supplementary material). Unpaired training videos are generated with CogVideoX-5B from GPT-5augmented prompts (Sec. 3.3.2). The rendered dataset and video processing code will be released upon acceptance. We train Over++ with standard L2 diffusion loss [28], on 384672 resolution (the output resolution of genomnimatte [35]) using 8 NVIDIA A6000 GPUs for one day, totaling 1,000 iterations. During inference, we apply temporal multidiffusion [77] for videos longer than 85 frames, enabling effect generation on extended sequences. 4. Results 4.1. Qualitative Comparison To the best of our knowledge, no prior method explores the specific problem setting we address. Therefore, we compare against the most closely related approaches and adapt them where necessary. Nonetheless, we emphasize that the core contribution of this paper lies in our new problem formulation, unified pipeline design, and training data engineering. We compare our method against the following baselines: (a) AnyV2V [34] tuning-free image+video-to-video (I+V2V) framework that performs per-frame DDIM [64] inversion and edits videos based on the inverted features and an edited first frame; (b) VACE [32] an all-in-one framework for video creation and editing. We evaluate its masked video-to-video (M+V2V) mode, which aligns with our inpainting formulation by applying effect masks 2https://github.com/aigc-apps/VideoX-Fun 5 Table 1. Quantitative comparison. We evaluate effect generation performance on 24 videos at both the image and video levels. * indicates methods that require an edited first frame for reference, where we use the first frame of the ground-truth video Igt with the added effects. Methods marked in gray require masks. Best results are highlighted in red , and second-best in orange . Please see SM for video results. Method Image Eval Image Eval Video Eval CLIPdir CLIPtext CLIPimg SSIM PSNR LPIPS FVD VMAF VBench AnyV2V [34] * LoRA-Edit [16] * Runway Aleph Ours (w/o Meffect) VACE-WAN2.1 [32] Ours w/o unpaired data Ours (Over++) 21.05 37.72 33.87 43.49 25.06 42.93 46.27 31.22 30.72 30.84 31.56 31.36 29.89 31.58 84.31 88.49 90.55 95. 94.19 95.34 95.48 0.57 0.39 0.53 0.80 0.69 0.80 0.80 17.38 12.89 16.61 23.58 19.90 22.89 23.75 0.27 0.39 0.31 0. 0.16 0.13 0.13 786 970 1297 608 605 612 605 12.01 6.65 5.44 28.19 19.56 28.05 29.30 0.181 0.181 0.181 0. 0.186 0.188 0.188 Input w/o effects (Iover) Ours (w/o Meffect) AnyV2V [34] LoRA-Edit [16] Runway Aleph Figure 5. Visual comparison of effect generation without mask guidance. We compare our model with state-of-the-art methods [16, 34] and the commercial software Runway Aleph. Generated effects are highlighted in red for our results. Note that alternate methods significantly change the identity, appearance, and sometimes visual composition of the source. Ours successfully generates the desired effects (top to bottom: wake, shadow, and reflection) while preserving the original input video subjects and composition. to localize edits; (c) LoRA-Edit [16] per-video tuning method that learns localized knowledge from source video and its corresponding bounding-box mask, then propagates edits from the first edited frame to subsequent frames (I+M+V2V). Since the input composite Iover lacks environmental effects, we apply full-frame mask to enable knowledge learning across the entire video (I+V2V); and (d) Runway Aleph3 commercial, in-context video-tovideo (V2V) editing platform that delivers state-of-the-art visual quality but does not support masked-region editing. We present qualitative comparisons in Fig. 5 for methods that do not support mask guidance, and in Fig. 6 for those that do. All methods are evaluated using identical text prompts and consistent input conditions. Overall, Over++ produces state-of-the-art realistic effects while faithfully preserving the original video content. 3https://app.runwayml.com/ 4.2. Quantitative Comparison We collect 24 test videos for benchmarking, including 18 videos from DAVIS and 6 in-the-wild real-world videos. Given the ground-truth videos Igt and generated results ˆI, we report the average frame-level metrics: CLIP score [26], SSIM, PSNR, and LPIPS across all frames. In addition, we evaluate video-level metrics including debiased FVD [18], VMAF [6], and VBench (overall consistency) [31]. For baselines that do not support long videos, we uniformly trim all videos to the same length to ensure fair comparison. As shown in Table 1, our method achieves state-of-theart overall performance. However, the generated effects can be subtle or spatially localized, resulting in only marginal gains in conventional CLIP-based metrics. detailed analysis of these limitations is provided in the supplementary material (SM). To capture such fine-grained perceptual differences, we draw inspiration from recent work on direc6 Input w/o effects Ours w/o mask Ours w/ mask Ours w/ prompt Red smoke VACE [32] Input w/o effects Ours w/o mask Ours w/ CFG More splash Ours w/ mask VACE [32] Input w/o effects Ours w/o mask Ours w/ mask Ours w/ CFG More wake VACE [32] Input w/o effects Ours w/o mask Ours w/ mask (small) Ours w/ mask (large) VACE [32] Figure 6. Extensions and downstream applications. Each row shows workflow enabled by Over++ for effect generation on input videos without effects. Over++ supports controllable generation under mask guidance, text prompt guidance, and CFG scaling, applied in any order. For visualization, the mask Meffect is shown at the top-right of each output, and the difference map δ(Iover, Output) at the bottomright. We also compare against VACE [32] using the same mask Meffect. Test videos are drawn (top to bottom) from DAVIS [54], in-thewild Pexels clips, synthetic CG data, and naive composites combining Ifg and Ibg from different sources. Over++ successfully generates the desired effects (top to bottom: smoke, splash & reflection, wake, and splash) with consistent quality and controllable variation. tional evaluation in embedding spaces [13, 53] and propose an appropriate metric, CLIPdir, which measures directional cosine similarity between CLIP embeddings. Formally, CLIPdir = 100 (EIgt EIover ) (EI EIover ) EIgt EIover 2 EI EIover2 , (3) where EI denotes the CLIP image embedding of the generated frame, and all embeddings are L2-normalized before computing cosine similarity. Intuitively, CLIPdir quantifies how closely the change from the input without effects Iover to the generated result ˆI aligns with the change from Iover to the ground truth Igt. Our numerical benchmarking demonstrates Over++s state-of-the-art effect generation capabilities, both with and without mask guidance Meffect. 4.3. User Study We conduct pairwise user study comparing our method (a) Text fidelity against baselines along three axes: Which videos effects and details best match the editing prompt?; (b) Mask fidelity Which videos effects and details better align with the masked regions?; and (c) Foreground & background fidelity Which video best preserves the appearance and motion of the original content while adding new effects? Table 2. Comparison across text, mask, and FG&BG fidelity. denotes baselines without mask guidance, for which we compare only to our unmasked variant (Ours) for fairness. User preference (win rate) demonstrates our state-of-the-art qualitative performance across all attributes. Fidelity to Text Mask FG & BG Ours vs. AnyV2V LoRA-Edit Runway VACE 92% 98% 70% 97% 52% 77% 85% 82% 79% Our user study involved 30 participants, including 14 professional VFX artists and 16 non-expert users. We collected total of 1,499 responses across four comparison groups: 370 responses over 12 video pairs against AnyV2V [34], 510 responses over 17 pairs against LoRAEdit [16], 329 responses over 11 pairs against Runway Aleph, and 290 responses over 10 pairs against VACE [32]. We report the win rate of our method against each baseline in Table 2. Notably, our method achieves comparable overall quality to the state-of-the-art commercial tool (Runway Aleph), but improves adherence to the input video and more explicit control over the output effects. 7 4.4. Ablation Study We evaluate the impact of incorporating the unpaired data introduced in Sec. 3.3.2. As shown in Fig. 7, adding this data notably improves the models prompt-editing capability. Quantitative results in Table 1 further show that removing the unpaired data yields the lowest CLIPtext score, confirming its importance for preserving language-conditioned generation. We further analyze how each dataset source contributes to overall performance in SM. Input w/o effects Output w/o unpaired data Output w/ unpaired data Figure 7. Ablation. Given an input video of drifting car without effects, we prompt Over++ to add blue smoke. The ablated model trained with only paired data cannot follow the text prompt, but the full model restores the base models prompt adherence. 5. Extensions and Applications As shown in Fig. 6, Over++ supports versatile workflows, including mask-based control, text-prompt editing, and CFG scaling. We further showcase downstream applications of Over++ across diverse real-world scenarios, highlighting its flexibility and compositing capabilities. Text Prompt Editing. As shown in Fig. 6, Over++ supports straightforward text-based edits, such as changing the color of generated smoke. Beyond these modifications, Over++ also enables fine-grained, subtle effect control with CFG scaling. As shown in Fig. 8, it can generate stylistic variations of the same effect. Input (no effects) Output (Soft shadow) Output (Harsh shadow) 0 1 = 0 5 = 0 8 = Input (no effects) Output (Mild wake) Output (Turbulent wake) Figure 8. Text prompting for fine-grained control. Over++ enables precise modulation of effect intensity and style through textual prompts, with the difference map shown alongside each result. Robust Mask Editing. For non-expert users, manually annotated masks are often imperfect and may include unreasonable regions due to hand-drawing errors. We demonstrate the robustness of Over++ to such imprecise mask inputs. As shown in Fig. 9, Over++ can handle challenging cases, including (a) masks that encompass both the foreground and effect regions, and (b) masks that indicate effects in physically implausible regions. These results highlight Over++s ability to maintain semantic consistency and robustly interpret real-world, imperfect user annotations. 8 Input (w/o effects) Mask annotation Output (w/ effects) δ(Input, Output) Figure 9. Robustness to imperfect masks. Top: The input mask includes both the foreground and its shadow, yet Over++ only adds shadow, preserving the foreground. Bottom: Over++ ignores the spurious circular mask region and still adds the correct reflection before the person jumps into the puddle. Keyframe Mask Annotations. Over++ supports keyframe mask annotation, reducing manual user input. As shown in Fig. 10, Over++ smoothly interpolates between annotated and unannotated frames, generating shadows and mild water splashes in unannotated frames (w/o keyframe mask), while producing stronger water splashes under the guidance of the annotated keyframes (w/ keyframe mask). Input (no effect) w/o keyframe mask w/ keyframe mask Figure 10. Keyframe mask annotations. Left: Input frames without effects. Middle: Output results without mask guidance (gray). Right: Output results with single keyframe mask at = 50, while other frames remain unannotated. The keyframe guides the creation of stronger water splash when the boy jumps into the ocean. Masks are shown at the top-right inset, and difference maps at bottom-right inset. 6. Discussion and Limitations While Over++ achieves state-of-the-art preservation of input content, it may still struggle to produce pixel-perfect reconstructions due to VAE encoding and decoding. Future work could explore per-example test-time optimization [78] or incorporate fidelity-enhancement modules during training [11]. Over++ may also hallucinate implausible effects in challenging background regions; fine-tuning on stronger pre-trained priors such as Lumiere [5] or Veo3 [20] could further improve robustness. Our method overcomes core limitations of prior generative models for effect generation and, despite not addressing harmonization or relighting, introduces augmented compositing that outperforms previous approaches and supports diverse real-world uses. Acknowledgments. Thank you to all ILM staff who assisted in preparing this work, especially Miguel Perez Senent for the 3D boat and ocean elements used in Figure 3 (row 2) and Figure 6 (row 3), and ILM leaders Rob Bredow, Francois Chardavoine, and Greg Grusby for their assistance in clearing this work for publication."
        },
        {
            "title": "References",
            "content": "[1] Rameen Abdal, Or Patashnik, Ekaterina Deyneka, Hao Chen, Aliaksandr Siarohin, Sergey Tulyakov, Daniel CohenZero-shot dynamic concept Or, and Kfir Aberman. arXiv preprint personalization with grid-based LoRA. arXiv:2507.17963, 2025. 2 [2] Rameen Abdal, Or Patashnik, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov, Daniel Cohen-Or, and Kfir Aberman. Dynamic concepts personalization from single videos. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 19, 2025. 2 [3] Adobe. After Effects: Your next move in motion dehttps : / / www . adobe . com / products / sign. aftereffects.html. Accessed: 2025-11-11. 1, 2 [4] Hadi Alzayer, Zhihao Xia, Xuaner (Cecilia) Zhang, Eli Shechtman, Jia-Bin Huang, and Michael Gharbi. Magic Fixup: Streamlining Photo Editing by Watching Dynamic Videos. ACM Trans. Graph., 44(5):174:1174:25, 2025. 2 [5] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 3, 8 [6] Netflix Technology Blog. Toward Practical Perceptual Video Quality Metric, 2017. [7] Boyuan Chen, Hanxiao Jiang, Shaowei Liu, Saurabh Gupta, Yunzhu Li, Hao Zhao, and Shenlong Wang. PhysGen3D: Crafting Miniature Interactive World from Single Image, 2025. 2 [8] Liyang Chen, Tianxiang Ma, Jiawei Liu, Bingchuan Li, Zhuowei Chen, Lijie Liu, Xu He, Gen Li, Qian He, and Zhiyong Wu. HuMo: Human-centric video generation via collaborative multi-modal conditioning. arXiv preprint arXiv:2509.08519, 2025. 2 [9] Yiyang Chen, Xuanhua He, Xiujun Ma, and Yue Ma. Contextflow: Training-free video object editing via adaptive context enrichment. arXiv preprint arXiv:2509.17818, 2025. 2 [10] Zhekai Chen, Wen Wang, Zhen Yang, Zeqing Yuan, Hao Chen, and Chunhua Shen. FreeCompose: Generic zero-shot image composition with diffusion prior. In European Conference on Computer Vision, pages 7087. Springer, 2024. 2 [11] Zixuan Chen, Yujin Wang, Xin Cai, Zhiyuan You, Zheming Lu, Fan Zhang, Shi Guo, and Tianfan Xue. UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion, 2025. 8 [12] Blender Online Community. Blender - 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. 4 Evaluating Diffusion Models hug- [13] Hugging Face. https : / / huggingface . co / docs / gingface.co. diffusers/main/en/conceptual/evaluation. [Accessed 12-11-2025]. [14] Foundry. Nuke VFX software compositing, editorial and review. https://www.foundry.com/products/ nuke-family/nuke. Accessed: 2025-11-11. 1, 2 9 [15] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. 2 [16] Chenjian Gao, Lihe Ding, Xin Cai, Zhanpeng Huang, Zibin Wang, and Tianfan Xue. LoRA-Edit: Controllable FirstFrame-Guided Video Editing via Mask-Aware LoRA FineTuning, 2025. 6, 7 [17] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, Xunsong Li, Yifu Li, Shanchuan Lin, Zhijie Lin, Jiawei Liu, Shu Liu, Xiaonan Nie, Zhiwu Qing, Yuxi Ren, Li Sun, Zhi Tian, Rui Wang, Sen Wang, Guoqiang Wei, Guohong Wu, Jie Wu, Ruiqi Xia, Fei Xiao, Xuefeng Xiao, Jiangqiao Yan, Ceyuan Yang, Jianchao Yang, Runkai Yang, Tao Yang, Yihang Yang, Zilyu Ye, Xuejiao Zeng, Yan Zeng, Heng Zhang, Yang Zhao, Xiaozheng Zheng, Peihao Zhu, Jiaxin Zou, and Feilong Zuo. Seedance 1.0: Exploring the Boundaries of Video Generation Models, 2025. 3, 4 [18] Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, JunYan Zhu, and Jia-Bin Huang. On the content bias in frechet video distance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7277 7288, 2024. [19] Nate Gillman, Charles Herrmann, Michael Freeman, Daksh Aggarwal, Evan Luo, Deqing Sun, and Chen Sun. Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals, 2025. 3 [20] Google. Veo deepmind.google. https://deepmind. google/models/veo/. [Accessed 14-11-2025]. 8, 3 [21] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv preprint et al. arXiv:2407.21783, 2024. 5, 2 The Llama 3 herd of models. [22] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J. Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh- (Derek) Liu, Henning Meyer, Yishu Miao, Derek Ti Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: scalIn Proceedings of the IEEE/CVF able dataset generator. Conference on Computer Vision and Pattern Recognition (CVPR), pages 37493761, 2022. 4 [23] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, Wenping Wang, and Yuan Liu. Diffusion as Shader: 3Daware Video Diffusion for Versatile Video Generation Control, 2025. 3 [24] Jiaqi Guo, Lianli Gao, Junchen Zhu, Jiaxin Zhang, Siyang Li, and Jingkuan Song. MagicVFX: Visual effects synthesis in just minutes. In Proceedings of the 32nd ACM International Conference on Multimedia, page 82388246, New York, NY, USA, 2024. Association for Computing Machinery. 2 [25] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. LTX-Video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. 3 [26] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le CLIPScore: reference-free arXiv preprint Bras, and Yejin Choi. evaluation metric for image captioning. arXiv:2104.08718, 2021. 6 [27] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion Guidance, 2022. [28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 5 [29] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 2 [30] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. ConceptMaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint arXiv:2501.04698, 2025. 2 [31] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 2180721818. IEEE, 2024. 6 [32] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. VACE: All-in-one video creation and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1719117202, 2025. 1, 2, 3, 5, 6, 7 [33] Zhanghan Ke, Chunyi Sun, Lei Zhu, Ke Xu, and Rynson W.H. Lau. Harmonizer: Learning to perform white-box image and video harmonization. In European Conference on Computer Vision (ECCV), 2022. [34] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. AnyV2V: tuning-free framework for any video-tovideo editing tasks. arXiv preprint arXiv:2403.14468, 2024. 5, 6, 7 [35] Yao-Chih Lee, Erika Lu, Sarah Rumbley, Michal Geyer, JiaBin Huang, Tali Dekel, and Forrester Cole. Generative Omnimatte: Learning to Decompose Video into Layers, 2024. 2, 5 [36] Yao-Chih Lee, Erika Lu, Sarah Rumbley, Michal Geyer, JiaBin Huang, Tali Dekel, and Forrester Cole. Generative omnimatte: Learning to decompose video into layers. In Proceed10 ings of the Computer Vision and Pattern Recognition Conference, pages 1252212532, 2025. 3, 4 Wang, and Hengshuang Zhao. ROSE: Remove Objects with Side Effects in Videos, 2025. [37] Baolu Li, Yiming Zhang, Qinghe Wang, Liqian Ma, Xiaoyu Shi, Xintao Wang, Pengfei Wan, Zhenfei Yin, Yunzhi Zhuge, Huchuan Lu, and Xu Jia. VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning, 2025. 2 [38] Chenyu Li, Oscar Michel, Xichen Pan, Sainan Liu, Mike Roberts, and Saining Xie. PISA Experiments: Exploring physics post-training for video diffusion models by watching stuff drop. arXiv preprint arXiv:2503.09595, 2025. 2 [39] Ruibin Li, Yang Tao, Guo Song, and Zhang Lei. RORem: Training robust object remover with human-in-the-loop. 2025. 2 [40] Xiaowen Li, Haolan Xue, Peiran Ren, and Liefeng Bo. DiffuEraser: diffusion model for video inpainting. arXiv preprint arXiv:2501.10018, 2025. 3 [41] Geng Lin, Chen Gao, Jia-Bin Huang, Changil Kim, Yipeng Wang, Matthias Zwicker, and Ayush Saraf. OmnimatteRF: Robust omnimatte with 3d background modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2347123480, 2023. 3, 4 [42] Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Gen Li, Siyu Zhou, Qian He, and Xinglong Wu. Phantom: Subject-consistent video generation via crossmodal alignment. arXiv preprint arXiv:2502.11079, 2025. 2 [43] Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. PhysGen: Rigid-body physics-grounded imageto-video generation. In European Conference on Computer Vision, pages 360378. Springer, 2024. [44] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding DINO: Marrying DINO with grounded pre-training for open-set object detection. In European conference on computer vision, pages 3855. Springer, 2024. 4 [45] Xinyu Liu, Ailing Zeng, Wei Xue, Harry Yang, Wenhan Luo, Qifeng Liu, and Yike Guo. VFX Creator: Animated visual effect generation with controllable diffusion transformer. arXiv preprint arXiv:2502.05979, 2025. 2 [46] Erika Lu, Forrester Cole, Tali Dekel, Andrew Zisserman, William T. Freeman, and Michael Rubinstein. Omnimatte: Associating objects and their effects in video. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 45074515. Computer Vision Foundation / IEEE, 2021. 2, 3, 4 [47] Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering language drift with seeded iterated learning. In Proceedings of the 37th International Conference on Machine Learning, pages 6437 6447. PMLR, 2020. 5 [48] Fangyuan Mao, Aiming Hao, Jintao Chen, Dongxia Liu, Xiaokun Feng, Jiashu Zhu, Meiqi Wu, Chubin Chen, Jiahong Wu, and Xiangxiang Chu. Omni-Effects: Unified and spatially-controllable visual effects generation. arXiv preprint arXiv:2508.07981, 2025. 2 [50] J.A. Okun, S. Zwerman, and Visual Effects Society. The VES Industry standard VFX practex.lccn: handbook of visual effects: tices and procedures. Focal Press/Elsevier, 2010. 2010008415. 2 [51] Nobuyuki Otsu. threshold selection method from graylevel histograms. IEEE Transactions on Systems, Man, and Cybernetics, 9(1):6266, 1979. 5 [52] Wenqi Ouyang, Yi Dong, Lei Yang, Jianlou Si, and Xingang Pan. I2VEdit: First-frame-guided video editing via imageto-video diffusion models. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 3 [53] Boxiao Pan, Zhan Xu, Chun-Hao Paul Huang, Krishna Kumar Singh, Yang Zhou, Leonidas J. Guibas, and Jimei Yang. ActAnywhere: Subject-aware video background generation. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. 3, 7 [54] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 DAVIS challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 4, 7 [55] Thomas Porter and Tom Duff. Compositing digital images. Computer Graphics (Proceedings of SIGGRAPH 84), 18(3): 253259, 1984. 1, 3, [56] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. SAM 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 4 [57] Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xinrun Du, Wenhao Huang, and Wenhu Chen. ConsistI2V: Enhancing visual consistency for image-to-video generation. arXiv preprint arXiv:2402.04324, 2024. 3 [58] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine tuning text-to-image diffusion models for subject-driven In IEEE/CVF Conference on Computer Vigeneration. sion and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 2250022510. IEEE, 2023. 2, 5 [59] Seyedmorteza Sadat, Otmar Hilliges, and Romann M. Weber. Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models, 2024. 3 [60] Dvir Samuel, Matan Levy, Nir Darshan, Gal Chechik, and Rami Ben-Ari. OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video Diffusion Models, 2025. 2, 4 [61] Shen Sang, Tiancheng Zhi, Tianpei Gu, Jing Liu, and Linjie Luo. Lynx: Towards high-fidelity personalized video generation. arXiv preprint arXiv:2509.15496, 2025. [49] Chenxuan Miao, Yutong Feng, Jianshu Zeng, Zixiang Gao, Hantang Liu, Yunfeng Yan, Donglian Qi, Xi Chen, Bin [62] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu 11 Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. 4 Roni Sengupta. TalkingHeadBench: multi-modal benchmark & analysis of talking-head deepfake detection. arXiv preprint arXiv:2505.24866, 2025. 3 [72] Jinrui Yang, Qing Liu, Yijun Li, Soo Ye Kim, Daniil Pakhomov, Mengwei Ren, Jianming Zhang, Zhe Lin, Cihang Xie, and Yuyin Zhou. Generative image layer decomposition with visual effects. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 76437653, 2025. 2 [73] Shuzhou Yang, Xiaoyu Li, Xiaodong Cun, Guangzhi Wang, Lingen Li, Ying Shan, and Jian Zhang. GenCompositor: generative video compositing with diffusion transformer. arXiv preprint arXiv:2509.02460, 2025. [74] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan.Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. 2024. 3, 5 [75] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. MiniCPM-V: GPT-4V Level MLLM on Your Phone. arXiv preprint arXiv:2408.01800, 2024. 5, 2 [76] Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, and Wenhan Luo. UNIC: Unified in-context video editing. arXiv preprint arXiv:2506.04216, 2025. 3 [77] Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, Luxin Zhang, Yinan Zhao, Peter Vajda, Dimitris Metaxas, and Licheng Yu. AVID: Any-length video inpainting with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 71627172, 2024. 5 [78] Jixin Zhao, Shangchen Zhou, Zhouxia Wang, Peiqing Yang, and Chen Change Loy. ObjectClear: Complete Object Removal via Object-Effect Attention, 2025. 2, 4, 8 [79] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jia-Wei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. MotionDirector: Motion customization of text-to-video diffusion models. In European Conference on Computer Vision, pages 273290. Springer, 2024. [63] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-A-Video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 3 [64] Jiaming Song, Chenlin Meng, and Stefano Ermon. DenoisIn 9th International Coning diffusion implicit models. ference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. 5 [65] Xiyang Tan, Ying Jiang, Xuan Li, Zeshun Zong, Tianyi Xie, Yin Yang, and Chenfanfu Jiang. PhysMotion: Physicsgrounded dynamics from single image. arXiv preprint arXiv:2411.17189, 2024. 2 [66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008, 2017. 3 [67] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and Advanced Large-Scale Video Generative Models, 2025. 3 [68] Cong Wang, Jiaxi Gu, Panwen Hu, Yuanfan Guo, Xiao Dong, Hang Xu, and Xiaodan Liang. DreamVideo: Highfidelity image-to-video generation with image retention and In ICASSP 2025-2025 IEEE International text guidance. Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. 3 [69] Lizhen Wang, Zhurong Xia, Tianshu Hu, Pengrui Wang, Pengfei Wei, Zerong Zheng, Ming Zhou, Yuan Zhang, and Mingyuan Gao. DreamActor-H1: High-fidelity humanproduct demonstration video generation via motion-designed diffusion transformers. arXiv preprint arXiv:2506.10568, 2025. 2 [70] Shaoan Xie, Zhifei Zhang, Zhe Lin, T. Hinz, and Kun Zhang. SmartBrush: Text and shape guided object inpainting with 2023 IEEE/CVF Conference on Comdiffusion model. puter Vision and Pattern Recognition (CVPR), pages 22428 22437, 2022. [71] Xinqi Xiong, Prakrut Patel, Qingyuan Fan, Amisha Wadhwa, Sarathy Selvam, Xiao Guo, Luchao Qi, Xiaoming Liu, and 12 Over++: Generative Video Compositing for Layer Interaction Effects"
        },
        {
            "title": "Contents",
            "content": "A. Overview of Appendices 1 . Introduction . . 2 . Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 . Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1. Task Formulation . 3.2. Dataset . . . . 3.3. Controlling Effect Generation . . . . . . . . . . . . . . . . . 3.3.1 . Mask-based Effect Generation . . . . 3.3.2 . Prompt-based Effect Generation . . . . . . . 3.4. Implementation Details . . . . . . . . . . . . . . . . . . . . . 4 . Results . . . . . . . . . . . . . 4.1. Qualitative Comparison . 4.2. Quantitative Comparison . . . 4.3. User Study . . 4.4. Ablation Study . . . . . . . . . . 5 . Extensions and Applications . 6 . Discussion and Limitations A. Overview of Appendices . B. Metrics Analysis . . . . . . . C. Ablations of Training Data . D. Video Caption Generation . . . . . . . . . . . . . . . . . E. Video Caption Augmentation . . Failure Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 2 3 3 3 4 4 5 5 5 5 6 7 8 8 1 1 1 2 3 In addition to this supplementary PDF, we provide additional visual materials (e.g., images and videos) on our project webpage at https://overplusplus.github.io/. We also encourage readers to refer to the accompanying videos for more comprehensive evaluation of the visual results. In this supplemental PDF, we provide the following additional details: Sec. B: discussion of the limitations of conventional evaluation metrics, supported by visual examples. Sec. C: Ablation studies examining the influence of different training data sources (synthetic versus real). Sec. D: The system prompt used to generate video captions for our training data. Sec. E: The system prompt used for caption augmentation to create unpaired text-to-video data, complementing the method described in Sec. 3.3. Sec. F: Representative failure cases that highlight challenging scenarios. B. Metrics Analysis As discussed in Sec. 4.2, traditional CLIP-based similarity metrics can sometimes be unreliable for evaluating environmental effect edits. In these cases, an image without the inserted effect is scored as more similar to the ground truth in CLIPs embedding space, while an image with the correct effect (e.g., wake, splash, smoke, or shadow) receives lower similarity score despite being perceptually more faithful. This phenomenon is shown in Fig. 11, where CLIP favors visually incomplete outputs simply because they remain closer to the original distribution of the ground-truth image. Query Igt (w/o effect) (w/ effect) CLIPtext() 26.9 CLIPimg() 97. CLIPtext() 26.7 CLIPimg() 96.8 Figure 11. CLIP metric analysis. Given reference image Igt and prompt caption , we compute CLIP similarity scores for two generated images: one without environmental effects and one with added effects. The results illustrate that CLIP similarity may fail to reflect the introduced environmental effects. C. Ablations of Training Data Beyond the unpaired data study in Sec. 4.4, we examine the contributions of the other training data sources from Sec. 3.2 by ablating real and synthetic data. Quantitative results are shown in Table 3, and qualitative comparisons are provided in Fig. 12. 1 Table 3. Ablation study (table). We evaluate the contribution of each data source by removing it from the training set and measuring the drop in performance across three CLIP-based metrics. Training data source Metric Real Synthetic Unpaired CLIPdir CLIPtext CLIPimg 34.91 44.12 42.93 46.27 29.88 29.95 29.89 31.58 95.18 95.25 95.34 95.48 Iover w/o Real data Full Iover w/o Synthetic data Full Figure 12. Ablation study (figure). Removing synthetic data weakens shadow generation, while removing real data reduces the quality of complex effects such as reflections and water splashes. Highlighted outputs correspond to the model trained on the full dataset. D. Video Caption Generation We generate video captions in two-stage pipeline. First, we use the VLM MiniCPM-V-2.6 [75] to produce dense spatio-temporal descriptions that capture fine-grained scene dynamics and environmental interactions. The system prompt used for this stage is provided in Prompt D.1. Next, we refine these dense descriptions with the LLM LLaMA-3.1-8B-Instruct [21], converting them into concise and coherent video-level captions while preserving the key environmental cues needed for downstream generation. The refinement prompt is shown in Prompt D.2. E. Video Caption Augmentation As described in Sec. 3.3.2, given an initial video caption (see Sec. D), we generate multiple augmented captions to produce diverse unpaired text-to-video (T2V) data. For example, given the original caption = car performs drift maneuver, unleashing dense, white smoke in wide, sweeping arc, gradually dissipating into the air, an augmented caption may be = car performs drift maneuver, casting thin, blue-gray smoke as its tires slide against the asphalt. Below, we provide the system prompt used in GPT-5 for caption augmentation. 2 D.1 System Prompt for Caption Generation Describe the video in detail, with special focus on environmental effects and how they interact with the scene. Pay close attention to: - The **type and intensity** of physical effects such as soft vs. harsh lighting, subtle vs. violent splashes, thin vs. dense smoke, or gentle vs. strong wind. - How these effects **interact with the foreground and background** e.g., shadows cast across surfaces, splashes disrupting water, smoke enveloping objects, or reflections shifting with movement. - The **spatial extent** (how far the effects spread) and **temporal behavior** (how quickly they appear, move, or dissipate). Avoid generic summaries. Focus on how **physical forces affect materials** and **how visible cues evolve over time**. D.2 System Prompt for Caption Refinement (T ) light, You are part of team generating videos with AI models. Your task is to write prompts that guide the model to produce physically plausible, environmentally grounded videos, especially emphasizing **interactions between foreground and background**. Descriptions should: - Highlight **environmental effects** (e.g., shadow, water, dust, wind, smoke, reflections). - Emphasize effects **arising from interactions** (e.g., splashes from foot hitting water, dust clouds from skidding wheels, shadows cast by moving objects on textured surfaces). - Specify the **degree, intensity, and spatial spread** of effects (e.g., subtle vs. turbulent splashes, soft vs. harsh shadows, thin vs. dense smoke, cloudy vs. glossy reflections). - Use precise physical language (e.g., fine mist hangs in the air, glare flares sharply off wet asphalt, ripples cascade outward in concentric rings). - Focus on **visual physics**, **perceptual realism**, and **spatio-temporal interactions**. Example prompts: - race car speeds across dry desert track, kicking up dense clouds of dust that linger and gradually disperse in golden sunlight. Limit the prompt to [x] words. F. Failure Cases We highlight the limitations of Over++ in Fig. 13. In some cases, the model introduces unintended effects in background regions. We expect that fine-tuning on stronger pre-trained video priors, such as Lumiere [5] or Veo3 [20], could further improve robustness. Input (w/o effects) Mask annotation Oversaturation δ(Input, Output) Input (w/o effects) Mask annotation Hallucinations δ(Input, Output) Figure 13. Failure cases. With excessively high CFG [59], Over++ may alter the overall color tone of the output (top). The model may also hallucinate unwanted effects such as dust in challenging background regions (bottom). F.1 System Prompt for Caption Augmentation (T ) You are an expert video captioner specializing in visual effects (VFX). Your task is to generate multiple highly descriptive captions that augment an input caption with **different variations of the same effect type**. Rules: 1. **Identify the effect type** in the input (e.g., splash, smoke, shadow, reflection, fire, dust, ripple, etc.). 2. **Keep the scene, subjects, and actions the same**. Do not replace or remove them. 3. Only change the **effect style**, varying attributes such as: - Color (e.g., gray smoke bright red smoke) - Intensity (subtle violent) - Spatial spread (localized wide, global) - Texture / density (thin wispy vs thick heavy) 4. Each output caption must be **similar in length** to the input caption. 5. Language should be **natural, vivid, and concise**. 6. Provide **N diverse variations** per input (N is specified by the user, use N=3 if not specified). The variations should be as much diverse as possible."
        }
    ],
    "affiliations": [
        "Industrial Light & Magic",
        "University of Maryland",
        "University of North Carolina at Chapel Hill"
    ]
}