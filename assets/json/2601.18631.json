{
    "paper_title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
    "authors": [
        "Mingyang Song",
        "Haoyu Sun",
        "Jiawei Gu",
        "Linjie Li",
        "Luxin Xu",
        "Ranjay Krishna",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 2 ] . [ 1 1 3 6 8 1 . 1 0 6 2 : r DAREASONER: DYNAMIC TOOL ORCHESTRA-"
        },
        {
            "title": "TION FOR ITERATIVE VISUAL REASONING",
            "content": "Mingyang Song ,1, Haoyu Sun,2, Jiawei Gu,3, Linjie Li,4, Luxin Xu5, Ranjay Krishna4,, Yu Cheng5, 1Fudan University, 2Tongji University, 3National University of Singapore, 4University of Washington, 5University of Electronic Science and Technology of China, 6The Chinese University of Hong Kong Homepage: https://adareasoner.github.io Code: https://github.com/ssmisya/AdaReasoner Models and Data: https://huggingface.co/AdaReasoner"
        },
        {
            "title": "ABSTRACT",
            "content": "When humans face problems beyond their immediate capabilities, they rely on tools, providing promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce AdaReasoner, family of multimodal models that learn tool use as general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
        },
        {
            "title": "INTRODUCTION",
            "content": "Humans often rely on external tools to solve the complex reasoning problems that go beyond what they can handle through internal thinking alone (Clark & Chalmers, 1998). This offers valuable perspective for improving the visual reasoning capabilities of Multimodal Large Language Models (MLLMs). By offloading perceptual and intermediate computation to external tools, the model can more effectively handle fine-grained visual perception (Tong et al., 2024) and multi-step reasoning (Hao et al., 2025), where precise intermediate verification and long-horizon planning are crucial. In this setting, adaptive tool usage becomes particularly important. MLLMs should learn not only how to use tools, but also when to use them and how to coordinate multiple tools over several steps, including tools it has never seen before. However, unlike humans who can flexibly decide when to resort to tools and which tools to use, current models still struggle to master adaptive tool usage. Early SFTand prompt-based approaches (Ma et al., 2024; Hu et al., 2024) explored multi-tool usage, but largely relied on rigid, Equal contribution. Equal Advisory Contribution 1 Figure 1: AdaReasoner performs adaptive and generalized tool-using. pre-defined invocation patterns rather than autonomous and adaptive planning. More recent RLbased methods, such as DeepEyes (Zheng et al., 2025) and Pixel-Reasoner (Su et al., 2025b), have improved perceptual reasoning through crop-based search strategies. However, they are typically constrained to single-tool trajectories or fixed interaction loops. As result, critical gap remains: existing methods lack the ability to adaptively plan and coordinate diverse tools in flexible and task-aware manner. They do not treat the decision of what tools to use, when to use them, and how to combine them as core component of multimodal reasoning. Moreover, because these approaches are not explicitly designed for adaptability, their tool-use policies tend to be brittle, exhibiting limited generalization to unseen tools or novel task distributions beyond their training scope. We present AdaReasoner, tool-aware reasoning model designed to overcome the limitations of rigid, single-tool paradigms and poor generalization by enabling adaptive, multi-turn tool planning. Our framework is built upon three key innovations. First, to establish robust foundation, we introduce new data curation pipeline that automatically synthesizes high-quality, multi-turn trajectories. Second, we further refine this policy using tailored Tool GRPO (TG) paradigm optimized for long-horizon strategic planning. This training strategy enables the model to reason over extended interaction sequences and make more coherent tool-use decisions. Finally, integrated within both the TC and TG stages is our proposed adaptive learning method (ADL), which is specifically engineered to decouple tool-use logic from specific tasks, thereby significantly enhancing the models generalizability to unseen domains. As illustrated in Figure 1, this active reasoning cycle enables the agent to not only extract visual evidence but to dynamically transform it, yielding deeper and more robust multimodal understanding. Through adaptive tool interaction, AdaReasoner achieves substantial performance gains across diverse benchmarks. In particular, the 7B variant attains an average improvement of +24.9%, while also surpassing strong proprietary models outperforming Claude Sonnet 4 and GPT-5 on multiple tasks. Beyond accuracy, as shown in Figure 1, AdaReasoner also exhibits self-adaptive tool-use behaviors. It learns to select effective tools, discard irrelevant ones, and regulate their use according to task demands and feedback, revealing strong flexibility and generalization. Moreover, AdaReasoner is able to generalize its tool-planning capability to unseen tasks and new tool definitions, maintaining both high frequency and strong accuracy of tool usage even when encountering new tasks. These findings offer compelling answer to the long-standing question of which tools should be included and how models should learn to use them, suggesting that with proper training, MLLMs can autonomously curate tool-use strategies from broad candidate set and extend their visual reasoning capacity in goal-directed manner. In summary, our main contributions are as follows: 2 We propose comprehensive method for developing tool-augmented models, built upon three core innovations: data curation recipe for multi-turn tool planning, an RL framework for multi-turn tool interaction, and an adaptive learning method that can enhance models generalizability. Based on our method, we introduce AdaReasoner, new family of state-of-the-art models for complex tool planning. AdaReasoner exhibits self-adaptive behaviors, learning to autonomously adopt beneficial tools, discard irrelevant ones, and modulate its usage frequency, while maintaining strong generalization to unseen tool definitions and novel tasks. Our AdaReasoner achieves significant gains over their base counterparts and delivers performance that is competitive with, or superior to, leading proprietary models like GPT-5 and Claude Sonnet 4 on structured-reasoning tasks. This establishes that our methodology can elevate smaller, opensource models to the state-of-the-art."
        },
        {
            "title": "2.1 PRELIMINARY",
            "content": "Problem Formulation As shown in figure 2, we formalize tool-augmented multimodal reasoning as sequential decision-making process. An MLLM represented as policy πθ parameterized by weights θ, is tasked with solving problem by generating reasoning trajectory τ . The policy is equipped with access to predefined set of visual tools = {t1, . . . , tn}. trajectory τ is sequence of state-action-observation tuples that represent the models step-by-step reasoning process: τ = {(s0, a0, o0), (s1, a1, o1), . . . , (sT , aT , oT )} (1) Here, st denotes the problem state, at is tool-calling action encapsulated by special tokens, and ot is the resulting observation from the tools execution. Each action at induces transition from state st to st+1 based on the new information in ot: a2 . . . aT sT +1 a0 s1 a1 (2) s0 Visual Tools Our AdaReasoner framework is built upon diverse and powerful suite of visual tools, which it executes and integrates directly into the reasoning process. This toolset is intentionally designed to cover three core reasoning functions: perception (e.g., POINT, OCR), manipulation (e.g., DRAWLINE, INSERTIMAGE), and calculation (e.g., ASTAR). Furthermore, this suite seamlessly integrates both lightweight, offline tools for immediate execution and computationally intensive, expert-model-based online tools. These foundational capabilities are summarized in Table 1, with detailed specifications for each tool provided in Appendix A.1. Tool Description Arguments Tool Output Point to target object Draw path using directional commands Use A* to find the shortest obstacle-free path POINT DRAW2DPATH ASTAR DETECTBLACKAREA Detect pure black areas in an image INSERTIMAGE OCR CROP Table 1: Visual tools integrated within AdaReasoner. We illustrate their arguments, outputs, and core functions description. More detailed descriptions of our tools are presented in Appendix A.1. Insert image into base at bounding box position Image + Coordinates + Insert Combined image Extracts and localizes text from the image Crop region and augment it Point coordinates Image with line Shortest path Bounding boxes of black areas Image + Description Image + Start + Directions Start + Goal + Obstacle Image Text with their bounding box Cropped Image Image Image + Coordinates 2.2 HIGH-QUALITY TRAJECTORY DATA CURATION As illustrated in Figure 2a, our data curation follows unified, three-stage process designed to generate high-fidelity, human-like reasoning trajectories. Abstract Trajectory Design First, for each task, we manually design an abstract, optimal problemsolving blueprint. For example, the VSP trajectory follows perception-planning-verification logic, Jigsaw mimics an iterative trial-and-error process, and GUIQA involves focus-then-extract strategy. However, to ensure the model develops true robustness beyond simply following these perfect paths, we deliberately incorporate two critical types of complex scenarios: Figure 2: An overview of our AdaReasoner framework. The pipeline consists of two main stages: (a) the Tool Cold Start (TC) phase, where trajectories are carefully constructed to support multi-turn reasoning; and (b) the Tool GRPO (TG) phase, where the policy is further refined via reinforcement learning guided by our adaptive, multi-turn reward. In addition, the Adaptive Learning method (c) can be applied throughout both the TC and TG stages, enabling improved generalization across tasks and tool configurations. Reflection and Backtracking: We include trajectories designed to encourage process of trial and verification. These feature explicit self-correction steps where the model must reflect on sub-optimal outcome and backtrack, teaching it to actively validate its own hypotheses and learn from intermediate failures. Explicit Tool Failure: To prevent over-reliance on external tools, we introduce cases where tools fail or return erroneous results. In these scenarios, after recognizing that tool is not providing useful output, the trajectory prompts the model to fall back on its own intrinsic capabilities to generate best-effort answer, ensuring it develops resilient, dual-strategy approach. Tool Calling supplements Subsequently, we ground these abstract blueprints by programmatically executing the tool calls to populate them with concrete, real-world inputs and outputs. CoT Data Generation Finally, we leverage powerful LLM to generate the corresponding Chainof-Thought (CoT) reasoning that connects each step. This process yields final dataset of rich, toolaugmented trajectories that teach the model not just what tools to call, but why and how to reason between them. Details for our trajectory data curation can be found in Appendix A.2. 2.3 MULTI-TURN TOOL GRPO To train our model for complex multi-turn tool-planning scenarios, we extend the GRPO framework to effectively handle multi-turn tool-calling reasoning trajectories. Concretely, we use Multi-turn Reward Accumulation and Adaptive Tool Reward to ensure the efficacy of the RL procedure. Multi-turn Reward Accumulation Our total reward, Rtotal is formulated as Rtotal = Rformat (λtool Rtool + λacc Racc), with each component adapted for multi-turn trajectories τ = {τ0, . . . , τT }. Format Reward Rformat = (cid:81)n i=1 Rf ormat(τi) Correct formatting is mandatory at every step. Therefore, the overall format reward for trajectory is set to 1 if and only if every individual step within it is correctly formatted. single format error at any turn results in Rformat = 0, nullifying the entire reward for the trajectory. This enforces strict adherence to the reasoning structure. Tool Reward The overall tool reward is the average of the fine-grained scores from all toolcalling turns (from τ0 to τT 1). It is calculated as Rtool = 1 t=0 Rtool(τt). Each individual tool call, Rtool(τt), is evaluated using hierarchical score of 0-4 based on four criteria (Structure, Name, Parameter Name, and Parameter Content). (cid:80)T 1 Accuracy Reward This reward is granted only based on the final turn, τT . If the final answer is correct, Racc = 1; otherwise, it is 0. 4 Adaptive Reward for Encouraging Tool Use. To encourage reliable tool usage under uncertainty, we design an adaptive reward with an asymmetric structure. When the final prediction is correct, the model receives full reward regardless of tool usage, encouraging concise reasoning without unnecessary tool calls. When the prediction is incorrect, the reward depends on the quality of tool usage, granting partial credit to informative tool-based reasoning while penalizing unsupported guessing. This design treats tools as fallback mechanism under uncertainty rather than mandatory step, leading to more robust and adaptive decision-making. Details are provided in Appendix A.4."
        },
        {
            "title": "2.4 ADAPTIVE LEARNING FOR IMPROVED GENERALIZATION",
            "content": "To bolster the models generalization capabilities when encountering unseen tools and new tasks, we introduce an Adaptive Learning strategy. Concretely, we randomize tool names as well as their parameter names and descriptions, preventing the model from overfitting to fixed identifiers and encouraging it to rely on the semantic understanding of tool functionalities and parameter descriptions when selecting appropriate tools. Token-Level Randomization for Identifiers. We hypothesize that robust tool planner should not depend on semantically meaningful identifiers (e.g., relying on the word Calculator to know it performs math). To test and enforce this, we apply replacement policy to all functional identifiers, including tool names and argument names. Specifically, these identifiers are replaced with completely random alphanumeric strings (e.g., replacing GetWeather with Func X7a2). This process strips away the linguistic cues from the identifiers, compelling the model to infer functionality solely from the provided descriptions and context. Semantic-Level Paraphrasing for Descriptions. While identifiers are randomized, it remains crucial to maintain sufficient diversity in the descriptions while preserving their semantic integrity to ensure learnability. Therefore, for the descriptions of both tools and arguments, we employ semantic rephrasing strategy. We leverage Gemini 2.5 Flash to rephrase the original descriptions. The objective is to alter the syntactic structure and lexical choices while strictly maintaining the original functional meaning. This creates diverse set of equivalent tool definitions, preventing the model from overfitting to specific phrasing and enhancing its robustness to the variations in tool documentation encountered in real-world scenarios."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "In this section, we describe our experimental setup in detail, analyze the impact of tool augmentation on agent planning, investigate the adaptability and generalization of learned tool-using behaviors, and finally evaluate the resulting model across diverse multimodal reasoning tasks. Our core experiments are based on the Qwen2.5-VL-3B-Instruct and Qwen2.5-VL-7B-Instruct models (Bai et al., 2025). We evaluate our approach across four diverse tasks that probe complementary aspects of multimodal reasoning. Specifically, we consider Visual Spatial Planning, which assesses multi-step planning and perceptual grounding, evaluated on our custom out-of-distribution benchmark (VSPO) as well as the standard VSP benchmark (Wu et al., 2024). We also include Jigsaw, which focuses on visual compositionality, evaluated on both our Jigsaw-COCO dataset and the Jigsaw subset of BLINK (Fu et al., 2024). To assess fine-grained visual understanding of GUI scenarios, we further evaluate on GUIQA, using GUIChat (Chen et al., 2024) and WebQA from the WebMMU benchmark (Awal et al., 2025). To better evaluate the effectiveness of our visual tools in GUI-based scenarios, we focus on the agent acting subset of the English split of WebMMU. Detailed settings and implementation details for all tasks are provided in Appendix B.1. 3.1 VISUAL TOOLS HELP BRIDGING THE CAPABILITY GAP We first fine-tune the model on each individual task and systematically evaluate the contribution of TC and TG to the overall performance. We include Direct SFT and Direct GRPO as strong baseline settings. Specifically, Direct SFT refers to supervised fine-tuning of the base model on the training set of each individual task, while Direct GRPO applies rule-based GRPO to the base 5 model to enhance its reasoning capability following prior work (Zhou et al., 2025). The results are summarized in Table 2. Model Qwen2.5-VL 3B + Direct SFT + Direct GRPO + TC (cold-start) + TG (tool-GRPO) + TC + TG vs. base VSPO 26.53 38.15 24.55 47.01 29.10 84.73 +58.20 VSP 26.73 38.82 32.73 51.09 35.09 94.73 +68.00 Jigsaw BLINK-J GUIChat WebMMU 39.80 42.60 42.70 66.00 43.00 94.80 +55. 48.67 53.33 52.67 70.00 47.33 88.67 +40.00 45.11 55.51 52.49 45.32 89.60 85.45 +40.34 55.89 61.38 56.30 44.72 72.15 81.71 +25.82 vs. base 67.48 65.65 83.54 64.63 88.62 82.32 +14.84 59.46 62.68 67.67 61.85 92.52 88.57 +29. 28.09 46.64 30.18 64.91 73.18 97.64 +69.55 52.67 88.00 80.00 83.33 80.67 96.00 +43.33 25.39 42.18 28.38 61.69 59.70 85.09 +59.70 Qwen2.5-VL 7B + Direct SFT + Direct GRPO + TC (cold-start) + TG (tool-GRPO) + TC + TG 45.70 86.40 64.90 84.20 72.30 96.60 +50.90 Table 2: Comparison of the Effects of Tool Cold-Start (TC) and Tool-GRPO (TG) under Single-Task FineTuning Setting. WebMMU reports the Agentic Action (Act.) score. Best is bold, second-best is underlined. Detailed sub-task breakdown is provided in Appendix Table 11. Visual Tools Bring Stable Improvements As shown in Table 2, our TC + TG recipe consistently improves the performance of base models, demonstrating an average gain of +38.66% on the 7B model. This tool-augmented approach transforms tasks like VSP from an under-optimized baseline (31.64%) to near-perfect execution (97.64%). This performance significantly surpasses traditional optimization methods such as task-specific SFT (46.64%) and Direct GRPO (30.18%). Furthermore, our TC + TG recipe enables the 7B model to achieve the state-of-the-art results that are competitive with, or superior to the best proprietary models. For instance, on VSP and Jigsaw, our model outperforms GPT-5 (OpenAI, 2025) (96.60% vs. 80.10%). This confirms that our method is highly effective strategy for unlocking advanced reasoning capabilities in open-source models. Visual Tools Help Overcome Scale-Based Limitations The results also reveal that tool augmentation can redefine the performance ceiling of MLLMs by overcoming scale-based limitations. As illustrated in Table 2 and Figure 10, while the baseline performance of 3B and 7B models is disparate and low, our tool-augmented versions both achieve near-perfect accuracy (94.7% and 97.6%). This suggests that the primary performance bottleneck has shifted from the models scale to the intrinsic quality of the tools it employs. Why Visual Tools Help Our empirical analysis reveals that the effectiveness of visual tools stems from their complementary roles in enhancing perception, verification, and planning. First, expert perception tools compensate for the intrinsic visual limitations of MLLMs by providing precise, structured observations, substantially improving downstream reasoning even in zero-shot settings. Second, manipulation tools enable models to externalize intermediate hypotheses and verify them through explicit visual operations, transforming abstract reasoning into concrete, checkable decisions. Finally, high-quality tool-augmented trajectories further teach models when and how to apply these tools, aligning planning behavior with task structure. Together, these components shift the bottleneck from internal reasoning accuracy to effective tool planning, explaining the consistent and large performance gains observed across tasks. Detailed analyses are provided in Appendix C.2. 3.2 ADAREASONER CAN LEARN ADAPTIVE TOOL-USING To investigate whether MLLMs can effectively learn to select tools and adaptively regulate their usage frequency, we carried out systematic study to build an adaptive tool planning model, which is the main characteristic of our AdaReasoner. We choose VSP as the primary testbed for this adaptive study, as it comprises multiple interdependent subtasks (e.g., navigation and verification) and requires more complex tool planning decisions involving diverse tool types. AdaReasoner Can Use New Tools during Inference Time During inference, the model exhibits notable ability to adapt to previously unseen but powerful tools. To systematically examine this capability, we evaluate whether the model can effectively leverage strong planning tool, A*, which is deliberately excluded during the Tool Cold Start (TC) phase. As shown in Table 3, when the A* 6 Stage Reflection A* TC + TG TC + TG TC + TG TC + TG TC Only TC + TG TC Only RL - Inf Inf - - Inf VSP Verify 99.20 99.80 84.60 80.00 93.60 94.20 79.40 Overall Nav VSPO Verify Overall A* Statistics Succ. CPS 97.64 91.36 68.55 70.36 64.91 67.27 61.18 73.44 63.89 57.22 43.78 31.58 27.67 32.11 98.70 99.61 99.61 88.70 94.01 94.81 81.43 85.09 80.36 76.77 64.49 61.69 58.62 54.85 0.56 0.00 0.68 0.52 0.00 0.00 0.49 100.0 0.0 16.9 94.5 0.0 0.0 85. Nav 96.33 84.33 55.17 62.33 41.00 44.83 46.00 Table 3: Ablation study on adaptive tool usage. Stage compares Tool Cold Start (TC) + Tool GRPO (TG) against TC alone. Reflection indicates training with () or without () reflection data. A* specifies availability: during RL training, at Inference (Inf), or unavailable (-). A* Statistics: CPS = calls per sample, Succ. = success rate (%). Best is bold, second-best is underlined. tool is introduced solely at inference time (Inf), it provides significant performance boost to the relevant task. For our standard TC+TG model (without reflection), this elevates the VSP navigation score from 44.83 to 62.33. The A* Statistics corroborate this adaptive behavior, showing high invocation success rate of 94.53%, which indicates the model is not just guessing but is correctly learning the tools syntax and purpose in zero-shot setting. However, although the model exhibits adaptive tool invocation behavior at inference time, this adaptability remains unstable and inconsistent. For example, the ASTAR tool does not contribute to verification and instead acts as distractor in this task. When the tool is made available, the model nevertheless invokes it, leading to substantial performance drop from 94.20 to 80.00. Similar unstable tool invocation behaviors become more pronounced when reflection is enabled. Therefore, the adaptability to new tools during inference remains unstable and still requires RL to stabilize it. (a) ASTAR (b) POINT Figure 3: Trend for tool calling frequencies for ASTAR, POINT, and DRAW2DPATH during RL. The model is optimized on VSP Verification (cool-color) and VSP Navigation (warm-color) tasks. (c) DRAW2DPATH Learning to Adopt and Master Beneficial Tools through RL To improve the stability of adaptive tool invocation, we incorporate the A* tool into the tool set during the TG stage. Specifically, we initialize training from the same SFT checkpoint that has never been exposed to A*, and then introduce A* as an available tool during the TG process. As shown in Table 3, incorporating A* during RL leads to more stable tool usage patterns and yields consistently improved performance. In addition, we also observe three notable adaptive behaviors emerging during the TG stage. (1) Learning to Adopt Beneficial Tools. As illustrated in Figure 3a, for the Path Navigation task (warm-colored curves), the models invocation frequency for ASTAR progressively increases, stabilizing at high rate of over 1.0 call per sample. This upward trajectory indicates that the model, guided by task-completion rewards, correctly identifies ASTAR as highly beneficial tool for pathfinding and actively incorporates it into its problem-solving strategy. As result, this mastery translates to dramatic performance increase, with the VSP navigation score soaring to 96.33, which achieves the best performance. (2) Learning to Discard Irrelevant Tools. Critically, AdaReasoner learns to discard the tool when it is irrelevant. Figure 3a (cool-colored curves) shows the inverse trend for the Verification task. The model initially explores using the A* tool but, receiving no reward for doing so, gradually learns to suppress its usage, with the invocation frequency decaying towards zero. This adaptive pruning prevents the negative degradation observed in the zero-shot inference scenario, allowing the Verification performance to remain at near-perfect 99.20. Model Qwen2.5-VL 7B + TC + Rnd TC + TG + TC + TG + Rnd TC + TG + Rnd TC + Rnd TG (Ours) vs. base VSPO VSP Jigsaw BLINK-J GUIChat WebMMU 25.39 27.66 26.71 24.01 24.43 57.78 69.94 +44.55 28.09 29.36 30.36 30.73 28.00 47.27 78.91 +50.82 45.70 83.50 77.90 66.10 59.20 90.60 83.50 +37. 52.67 82.00 80.00 82.00 78.00 91.33 80.67 +28.00 68.09 36.59 35.55 82.54 80.35 82.74 80.87 +12.79 67.48 41.87 42.89 67.68 59.76 69.11 70.93 +3.46 Table 4: Impact of Different Experimental Settings on Model Generalization Performance. Rnd TC and Rnd TG denote the randomized Tool Cold Start and Tool GRPO settings trained using our Adaptive Learning Method. WebMMU reports the Agentic Action (Act.) performance. Best is bold, second-best is underlined. Full sub-task breakdown in Appendix Table 12. (3) Learning to Modulate Tool-Use Frequency Beyond the binary choice of adopting or discarding tool, the model exhibits more nuanced adaptive behavior: dynamically modulating the invocation frequency of continuously useful tools to find an optimal balance. For instance, as shown in Figures 3b and 3c, the model learns that the Point tool is significantly more critical for navigation, maintaining high and stable call frequency ( 3.2 calls/sample), while keeping its usage minimal for verification ( 1.0 call/sample). 3.3 ADAREASONER CAN LEARN GENERALIZED TOOL-USING While adaptability to new tools within known context is valuable, more fundamental challenge lies in cross-tool and cross-task generalization. In practice, simply applying our TC + TG training recipe is not sufficient to achieve robust generalization: models may still fail when confronted with previously unseen tools or novel task distributions. To overcome this limitation, we propose Adaptive Learning method that randomizes tool definitions at both the token level and the semantic level. This strategy can be integrated into both the TC and TG stages. We evaluate its effectiveness from two complementary perspectives: Generalize to New Tasks To validate whether the tool-planning capability can generalize to new tasks, we conduct TC only using the Jigsaw data, while withholding all training data related to the VSP and WebQA tasks. We then apply various training recipes to investigate whether tool-planning capabilities acquired from Jigsaw trajectories could be effectively transferred to these unseen domains. All data from the three tasks is used during the TG stage. Generalize to New Tools To validate whether the tool-planning capability can generalize to new tools, the toolset definition during evaluation is completely different from the one used during training, while preserving the same underlying tool functionalities. This design allows us to assess the models ability to generalize to unseen tools that provide equivalent functionality, thereby evaluating its robustness and generalizability beyond memorizing tool-specific interfaces. The results, as shown in Table 4, reveal the potential for generalized tool-using, demonstrating that tool-planning skills learned from single task can generalize to unseen domains. Specifically, the model trained with Randomized TC + Randomized TG achieves the most significant improvement over the base model. On the unseen VSP task, it boosts the overall score from 28.09 to 78.91, and also demonstrates strong generalization when averaged across all three tasks (75.81 vs. 46.50). In contrast, other training settings fail to improve performance on unseen tasks and, in some cases, even degrade it. These results indicate that our method enables the model to learn generalizable tool usage, rather than overfitting to task-specific patterns. 3.4 MAIN RESULTS Following our proposed training recipe and detailed analysis, we select the data from VSP, Jigsaw, and WebQA as our cold start data and VSP, Jigsaw, WebQA, and Visual Search as our RL data. The training details can be found in Appendix B.3.4. To enhance the models generalizable tool-use capabilities, we applied the Adaptive learning method (i.e., Randomized TC + Randomized TG) strategy on Qwen 2.5VL 7B. We evaluate our method on diverse set of benchmarks. In addition to the benchmarks introduced earlier, we further include visual search benchmarks to assess the 8 Model VSPO VSP Jigsaw BLINK-J GUIChat WebMMU HRBench V* Avg. Gemini 2.5 flash GPT 5 Claude 4 sonnet 40.12 34.25 51. 53.55 55.64 56.27 67.20 80.10 58.60 65.33 73.33 65.33 83.05 71.41 93.14 Closed-Source Models Qwen 2.5 VL 3B Qwen 2.5 VL 7B Qwen 2.5 VL 32B Qwen 2.5 VL 72B InternVL3 78B Open-Source Models 26.53 25.39 28.56 33.41 28.14 26.73 28.09 33.91 39.09 35.09 39.80 45.70 59.50 70.10 52.80 48.67 52.67 64.67 71.33 60.00 46.26 68.09 85.21 88.01 79. Tool-Planning Models 66.26 80.49 83.54 54.47 67.48 85.98 91.06 71.34 78.25 74.38 60.62 53.00 63.62 70.12 73.00 75.12 68.59 74.87 59. 65.54 68.56 66.09 43.98 63.35 72.25 80.10 81.15 42.93 51.80 62.53 68.76 60.93 28.98 30.45 45.00 39.64 45.00 61.50 52.75 71.36 84.50 10.18 12.18 44.80 21.38 24.55 52.20 88.60 78.36 71.08 +45.69 +50.27 +42.90 56.00 Qwen 2.5 VL 7B + Tools 65.33 Qwen 2.5 VL 72B + Tools 76.00 GPT 5 + Tools DeepEyes 50.67 PixelReasoner 59.33 88.00 AdaReasoner 7B vs. base +35.33 Table 5: Main results on visual reasoning, WebQA, and general VQA tasks. WebMMU reports the Agentic Action (Act.) score. Avg denotes the average score across all benchmarks. Bold and underline indicate the best and second-best results within tool-planning models. Detailed results are provided in Appendix B.4.3 and Appendix Table 13. All evaluations are conducted under unified evaluation framework. 50.93 62.82 74.83 48.88 50.87 76.49 +24. 54.97 65.97 70.16 67.54 53.40 70.68 +7.33 56.76 77.13 76.51 65.90 72.45 73.91 +5.82 69.51 76.83 88.82 72.76 69.51 72.15 +4.67 63.75 67.12 78.50 67.00 52.12 69.12 +5.50 Model Jigsaw Vstar # Turns CPS Succ(%) Acc. # Turns CPS Succ(%) Acc. Orig. Acc. DeepEyes PixelReasoner GPT5 + Tools Qwen 2.5 VL 7B + Tools Qwen 2.5 VL 72B + Tools AdaReasoner 7B 1.51 1.09 22.71 2.09 1.13 0.77 1.05 0.00 0.00 3.40 2.06 64.41 4.24 0.57 90.40 4.49 3.54 98.50 44.80 52.20 84.50 45.00 61.50 88.60 1.01 0.02 25.00 67.54 1.19 0.26 32.00 53.40 1.24 0.24 89.13 70.16 1.91 0.18 94.29 54.97 1.72 0.27 100.00 65.97 2.35 1.47 90.04 70. 90.10 84.30 Table 6: Tool usage statistics on Jigsaw and Vstar. #Turns denotes the number of interaction rounds. CPS (calls per sample) represents the average number of tool invocations per sample. Succ denotes the tool execution success rate. Acc is taken from the corresponding benchmark results in Table 5. Orig. Acc. denotes the performance under the models original tool definition. generalizability of our approach beyond structured visual reasoning tasks, specifically V*(Wu & Xie, 2024) and HRBench(Wang et al., 2025b). The comparative results are summarized in Table 5. Overall Performance Across Tasks As shown in the table, our model achieves consistent and substantial performance improvements across both visual reasoning tasks (e.g., VSP and Jigsaw) and general multimodal tasks (e.g., WebQA and Visual Search). In visual reasoning, our method yields large and stable gains ( > +40%), enabling the 7B model to outperform several competitive closed-source models and effectively narrowing the capability gap between smaller open-source models and larger counterparts. For general tasks, while tool integration cannot fully offset the performance advantages brought by model scaling due to their open-ended nature and the lack of deterministic tools, it still provides meaningful and robust performance boosts, demonstrating the broad effectiveness of visual tool augmentation. Generalizable Tool Using. defining characteristic of our approach is its robustness against domain shifts in both tool definitions and task contexts. During evaluation, the tool definitions differ from those used in training, forming strict zero-shot tool adaptation scenario for all models. As shown in Table 6, while prior tool-planning methods exhibit limited tool engagement or low execution reliability under this shift, AdaReasoner consistently demonstrates the highest level of effective tool usage. In particular, it achieves the largest number of calls per sample (CPS) while maintaining near-perfect execution success (e.g., 3.54 CPS with 98.50% success on Jigsaw), indicating that the model has learned transferable tool-planning principles rather than overfitting to specific tool APIs. Figure 4: Our AdaReasoner-7B demonstrates advanced capabilities for multi-turn, tool-assisted reasoning and reflection, enabling it to achieve performance that is on par with, or even superior to, state-of-the-art closedsource models. Crucially, this generalizable tool usage translates into clear performance gains. On Jigsaw, AdaReasoner attains the best accuracy (88.60), substantially outperforming prior methods, which exhibit limited effectiveness when confronted with previously unseen tasks and tool definitions. On VStar, which evaluates real-world general VQA scenarios and does not provide explicit tool-calling supervision during training, our model still actively invokes tools (1.47 CPS) and achieves the highest accuracy (70.68). In contrast, several existing approaches rely heavily on their original tool definitions and suffer notable performance degradation when evaluated under the new setting, as reflected by the gap between their original and adapted accuracies. Together, these results demonstrate that our method effectively decouples tool-use logic from task-specific supervision, empowering the model to autonomously formulate tool-augmented strategies for new problems. Moreover, beyond the quantitative improvements reported on benchmark metrics, as shown in Figure 4, AdaReasoner-7B exhibits robust qualitative behaviors, including multi-turn tool planning, reflective correction of imperfect tool outputs, and synergistic tool composition, enabling it to outperform strong baselines such as GPT-5."
        },
        {
            "title": "4 RELATED WORK",
            "content": "4.1 REINFORCEMENT LEARNING FOR MULTIMODAL REASONING The recent success of DeepSeek-R1 (Guo et al., 2025), which demonstrated that rule-based Group Relative Policy Optimization (GRPO) can effectively induce strong reasoning behaviors in LLMs, has spurred wave of research aimed at replicating this paradigm in the multimodal domain. Several studies have successfully extended this approach, with Zhou et al. (2025) reproducing the emergent aha moment in MLLM reasoning, R1-OneVision (Yang et al., 2025) introducing cross-modal formalization pipeline, and works like Feng et al. (2025) and Li et al. (2025) improving temporal reasoning in videos. collection of other strong works have also leveraged R1-style methods to achieve impressive results in general MLLM reasoning (Huang et al., 2025; Shen et al., 2025; Lu et al., 2025). However, key limitation of the R1-style, rule-based reward structure is that it primarily targets the reasoning process and does not directly improve the models underlying perceptual abilities. Since accurate perception is the foundation for sound reasoning, error accumulation from faulty perception can still lead to hallucinations and degrade performance. AdaReasoner directly addresses this shortcoming. By leveraging the precise perceptual capabilities of external expert models and specialized tools, our framework ensures high-fidelity understanding of the visual input, thereby improving the reliability of the entire reasoning pipeline."
        },
        {
            "title": "4.2 TOOL-AUGMENTED MULTIMODAL REASONING",
            "content": "There is growing interest in enhancing MLLMs with tool-use capabilities. Early efforts focused on foundational aspects such as infrastructure and data. LLaVA-Plus (Liu et al., 2024a), for example, introduced dedicated tool server to provide services for MLLMs. On the data front, CogCoM (Qi et al., 2024) identified six key manipulation strategies and trained models on synthetic Chainof-Manipulation (CoM) data, while TACO (Liu et al., 2024b) contributed large-scale dataset of reasoning traces derived from 15 visual tools. Subsequent research has explored different paradigms for tool interaction. One prominent line of work enhances visual reasoning by training models to generate code (Zhang et al., 2025; Zhao et al.). While powerful, these code-based environments are ill-suited for integrating computationally intensive capabilities, such as invoking large expert models. Another line of research leverages simpler, atomic visual tools like zoom-in functions to augment model perception (Wang et al., 2025a; Zheng et al., 2025; Su et al., 2025a; Zhu et al., 2025b; Su et al., 2025c). However, these approaches typically focus on single-step actions and have not explored the more complex challenges of multi-turn planning or dynamic tool composition. Our work, AdaReasoner, is designed to bridge these gaps, providing framework that enables models to perform multi-turn planning and reasoning while adaptively selecting from diverse suite of tools."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduce comprehensive framework that integrates high-quality trajectory curation, ToolGRPO, and an adaptive learning mechanism to enable effective multi-turn tool planning. Based on this framework, we develop AdaReasoner, family of tool-planning models. Unlike approaches that rely on rigid, task-specific patterns, AdaReasoner learns tool usage as generalizable reasoning capability, allowing it to coordinate complex tool sequences and adapt zero-shot to unseen tool definitions. Extensive experiments demonstrate that this paradigm achieves state-of-the-art performance, with models exhibiting autonomous adaptive behaviors, selectively adopting beneficial tools, suppressing irrelevant ones, and dynamically modulating tool usage frequency according to task demands. More fundamentally, our findings show that effective tool orchestration shifts the primary performance bottleneck from intrinsic model scale to tool utility, enabling 7B model to surpass strong proprietary systems such as GPT-5 on challenging visual reasoning tasks."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Claude 4 Sonnet Model Card. Technical report, Anthropic, May 2025. URL https:// www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47. pdf. Accessed: 2025-09-17. Rabiul Awal, Mahsa Massoud, Aarash Feizi, Zichao Li, Suyuchen Wang, Christopher Pal, Aishwarya Agrawal, David Vazquez, Siva Reddy, Juan Rodriguez, et al. Webmmu: benchmark for multimodal multilingual website understanding and code generation. arXiv preprint arXiv:2508.16763, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317, 2024. Andy Clark and David Chalmers. The extended mind. Analysis, 58(1):719, 1998. ISSN 00032638, 14678284. URL http://www.jstor.org/stable/3328150. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 11 Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379, 2024. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In European Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. conference on computer vision, pp. 740755. Springer, 2014. Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. In European conference on computer vision, pp. 126142. Springer, 2024a. Yun Liu, Haolin Yang, Xu Si, Ling Liu, Zipeng Li, Yuxiang Zhang, Yebin Liu, and Li Yi. Taco: Benchmarking generalizable bimanual tool-action-object understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2174021751, 2024b. Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing efficient action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. Zixian Ma, Jianguo Zhang, Zhiwei Liu, Jieyu Zhang, Juntao Tan, Manli Shu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Caiming Xiong, et al. Taco: Learning multi-modal action models with synthetic chains-of-thought-and-action. arXiv preprint arXiv:2412.05479, 2024. Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European conference on computer vision, pp. 6984. Springer, 2016. OpenAI. GPT-5 System Card. Technical report, OpenAI, August 2025. URL https://cdn. openai.com/gpt-5-system-card.pdf. Accessed: 2025-09-17. Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et al. Cogcom: visual language model with chain-of-manipulations reasoning. arXiv preprint arXiv:2402.04236, 2024. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model, 2025. URL https://arxiv. org/abs/2504.07615, 2025. 12 Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning, 2025a. URL https://arxiv.org/abs/2505.15966. Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025b. Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025c. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95689578, 2024. Mark Towers, Ariel Kwiatkowski, Jordan Terry, John Balis, Gianluca De Cola, Tristan Deleu, Manuel Goulao, Andreas Kallinteris, Markus Krimmel, Arjun KG, et al. Gymnasium: standard interface for reinforcement learning environments. arXiv preprint arXiv:2407.17032, 2024. Jiacong Wang, Zijian Kang, Haochen Wang, Haiyong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao Ran, Xiao Liang, Chao Feng, et al. Vgr: Visual grounded reasoning. arXiv preprint arXiv:2506.11991, 2025a. Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 79077915, 2025b. Penghao Wu and Saining Xie. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1308413094, 2024. Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William Yang Wang, Yang Zhang, and Shiyu Chang. Vsp: Assessing the dual challenges of perception and reasoning in spatial planning tasks for vlms. arXiv preprint arXiv:2407.01863, 2024. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu arXiv preprint Jiang, Changyi Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv:2508.11630, 2025. Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, and Chen Wei. Pyvision: Agentic vision with dynamic tooling, 2025. URL https://arxiv. org/abs/2507.07998. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning, 2025. URL https://arxiv.org/abs/2505.14362. 13 Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1zeros aha moment in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025a. Muzhi Zhu, Hao Zhong, Canyu Zhao, Zongze Du, Zheng Huang, Mingyu Liu, Hao Chen, Cheng Zou, Jingdong Chen, Ming Yang, et al. Active-o3: Empowering multimodal large language models with active perception via grpo. arXiv preprint arXiv:2505.21457, 2025b."
        },
        {
            "title": "A METHOD DETAILS",
            "content": "A.1 BASIC SETTINGS We first formalize multimodal reasoning with tools as an agentic planning process, enabling systematic description of how models decompose and solve complex tasks. Problem Formulation We denote an MLLM with tool-using capability as policy πθ, parameterized by model weights θ. At initialization, πθ is equipped with access to pool of tools = t1, t2, , tn, where denotes the number of available tools. Given task description and the original multimodal input = text, image, the system begins from an initial state s0. Building on this setup, the planning framework is formalized through three essential components. State st represents the current problem status: the initial state s0 corresponds to the original input, while intermediate states capture textual reasoning steps conditioned on accumulated observations, until special token triggers an action. Action at denotes one-step tool invocation, delimited by the special symbols <tool call> and </tool call>, which executes selected tool. Observation ot is the execution result returned by the invoked tool and is incorporated into the subsequent state. typical tool-integrated reasoning trajectory τ involves multiple tool invocations over several reasoning steps, which can be represented as sequence of rounds: τ = {τ0, τ1, τ2, . . . , τT } where each round is defined as τi = {si, ai, oi}, and the sequence proceeds as follows: s0 a0 s1 a1 s2 a2 . . . aT sT +1 To enable the model to autonomously generate reasoning traces and tool calls, we utilize system prompt as shown in B.2 during rollout. The tool list placeholder denotes the tool set , which contains all tools available for invocation. Tool Definition and Usage This section provides detailed description of the visual tools integrated within our AdaReasoner framework. For each tool, we outline its core functionality, input arguments, output format, and its specific role in addressing the challenges of our evaluation tasks. POINT Functionality: perceptual tool designed for precise object localization. Given an image and natural language description of target (e.g., the start point, red cars), it returns list of pixel coordinates (x, y) of the objects center. Role in VSP: This tool is fundamental for grounding the model in the spatial environment. In both Navigation and Verification, it is the first step to accurately identify the locations of the start, goal, and hazardous ice holes, converting the visual grid into structured representation that can be used for planning. DRAW2DPATH Functionality: visualization and verification tool. It takes starting coordinate and sequence of directional commands (e.g., [U, U, R]) and overlays the corresponding path onto the input image. Role in VSP: This tool externalizes the models internal plan into visual artifact. In Verification, it renders the given path for the model to judge. In Navigation, it serves as final check, allowing the model to visually confirm that its generated path is correct and safe before outputting the final answer. ASTAR Functionality: classic planning algorithm encapsulated as tool. It computes the shortest obstacle-free path between start and goal coordinate, given the locations of obstacles. Role in VSP: This tool offloads the complex pathfinding computation. After the POINT tool identifies all key locations, ASTAR can be invoked to generate an optimal, logically sound path, freeing the MLLM to focus on higher-level task management and verification. 15 DETECTBLACKAREA Functionality: specialized perception tool for the Jigsaw task. It analyzes an image and returns the bounding box coordinates of any completely black, rectangular regions, which correspond to the missing puzzle piece. Role in Jigsaw: This tool provides deterministic way to identify the problem space. It is the critical first step in the solution trajectory, telling the model precisely where the candidate patches need to be inserted. INSERTIMAGE Functionality: visual manipulation tool. It takes base image, patch image, and set of coordinates, and returns new image where the patch has been inserted at the specified location. Role in Jigsaw: This tool enables iterative hypothesis testing. The model uses it to physically place each candidate patch into the missing slot identified by DETECTBLACKAREA. The resulting composite image is then fed back to the model, allowing it to visually assess the quality of the fit. CROP Functionality: An attentional tool. It takes an image and bounding box coordinates and returns new, smaller image containing only the specified region. Role in GuiQA: This tool mimics the human ability to focus on specific part of dense interface. By cropping region of interest (e.g., button or text block), the model can reduce noise and ambiguity, creating cleaner input for subsequent analysis by other tools or by the model itself. OCR Functionality: An information extraction tool. It performs Optical Character Recognition on an input image, returning all detected text along with its corresponding bounding box coordinates. Role in GUIQA: This tool grounds the models understanding in the literal text present in the GUI. It is often used synergistically with CROP. After isolating key UI element, the model can use OCR to reliably read its label (e.g., Buy Now), preventing the visual-only hallucinations common in MLLMs. A.2 HIGH-QUALITY COLD START TRAJECTORY DATA CURATION For our structured reasoning tasks, we developed customized data generation and trajectory creation pipelines to ensure high quality and diversity. Some detailed data samples are shown in appendix A.3 VSP The VSP benchmark environments were procedurally generated using the Gymnasium (Towers et al., 2024) framework. To ensure variety, we systematically controlled the distribution of start points, end points, and ice holes. We synthesized environments of sizes 4 4, 6 6, and 8 8 for the training set, while reserving larger 5 5, 7 7, and 9 9 grids for testing. The Tool Cold Start (TC, SFT) trajectories were designed to mimic an optimal problem-solving process. For Navigation tasks, the trajectory consists of: (1) invoking the POINT tool to localize the start, end, and all ice holes; (2) performing textual reasoning based on these coordinates; and (3) calling DRAW2DPATH for final verification. Crucially, we also incorporated reflection and backtracking data derived from failure cases. For Verification tasks, the trajectory involves: (1) using POINT to locate the start, (2) employing DRAW2DPATH to render the proposed path, and (3) prompting the model to judge if this path intersects any ice holes. Jigsaw The Jigsaw dataset was constructed using images from the COCO-2017 (Lin et al., 2014) training set. Each instance was created by first dividing an image into 3 3 grid. 2 2 sub-grid was then selected as the base image, from which one patch (e.g., top-right) was removed to create the problem. The removed patch served as the correct answer, while one of the remaining five patches from the original 3 3 grid was chosen as distractor. The TC trajectory instructs the model to: (1) 16 Figure 5: An example of multi-turn cold-start trajectory for the VSP task. call DETECTBLACKAREA to identify the coordinates of the missing section, and (2) iteratively call INSERTIMAGE for each candidate patch until the puzzle is solved. To enhance robustness and diversity, we introduced several key variations: ➊ The order of patch insertion attempts was randomized to ensure uniform distribution of options. ➋ Scenarios involving tool failures (e.g., detection errors) were included, prompting the model to fall back on its intrinsic knowledge after several failed attempts. ➌ proportion of samples were designed to be solvable directly by the model without tool use, encouraging adaptive tool invocation. GUIQA The process begins with 44k single-turn instances from the Guichat dataset. (Chen et al., 2024) To identify challenging cases that necessitate tool use, we first prompted powerful visionlanguage model, Qwen-VL-2.5-72B (Bai et al., 2025), to answer the questions. We retained only the instances where the model failed, resulting in subset of 7,100 hard questions. Next, for these 7,100 instances, we rendered the ground-truth answer coordinates as bounding boxes on the 17 Figure 6: An example of multi-turn cold-start trajectory for the Jigsaw task. This trajectory showcases an iterative trial-and-error process. The agent first uses DETECTBLACKAREA to identify the missing region. It then sequentially attempts to InsertImage with each candidate patch, analyzing the visual result of each attempt before arriving at the correct solution. Figure 7: An example of multi-turn cold-start trajectory for GUI-QA task. This sample illustrates focusthen-extract strategy. The agent first uses the CROP tool to isolate specific, relevant section of the webpage. It then applies the OCR tool to this cropped, unambiguous input to perform precise information extraction. images. We then performed manual visual inspection to ensure these boxes contained meaningful and relevant information, which filtered the set down to 1,800 valid data points. To generate highfidelity tool-use trajectories for these cases, we provided the ground-truth answer and coordinates to gemini-2.5-flash Comanici et al. (2025), prompting it to produce the chain-of-thought reasoning and tool invocation sequence required to solve the problem. Finally, all generated trajectories were validated against our predefined format, and only those that strictly conformed were retained. This final curation step yielded high-quality dataset of 1,139 instances for our TC stage. After defining the abstract trajectory structure for all tasks, we followed unified, two-stage process to create the final training data. First, we executed these trajectories programmatically to populate them with real tool inputs and outputs. Subsequently, we leveraged Gemini 2.5 Flash (Comanici et al., 2025) to generate the corresponding chain-of-thought (CoT) reasoning for each step. This process resulted in final dataset of high-fidelity, tool-augmented trajectories complete with explicit reasoning chains, ready for our cold-start training. A.3 DATA SAMPLES To provide more concrete understanding of our cold-start data, we present representative multi-turn trajectory samples for each of our core tasks in Figures 5, 6, and 7. These examples are designed to showcase the sophisticated, human-like reasoning patterns we aim to instill in the model during the supervised fine-tuning phase. The VSP sample (Figure 5) illustrates methodical, multi-stage problem-solving process that includes perception, verification, and analysis. The Jigsaw sample (Figure 6) demonstrates an iterative trial-and-error strategy, where the agent actively evaluates the outcome of each tool call. Finally, the GUIQA sample (Figure 7) highlights synergistic tool-use pattern, where one tool (Crop) is used to create optimal conditions for another (OCR). Across all examples, the interplay between the models internal thoughts (<think>), tool calls, and observations from the environment is clearly demonstrated. A.4 TOOL GRPO Group Relative Policy Optimization (GRPO) is reinforcement learning algorithm that evaluates policy performance by directly comparing group of candidate reasoning trajectories. The process of Tool GRPO in AdaReasonerbegins with the initial state s0 = g, text, image, for which the policy πθ samples set of complete trajectories as candidate responses, {τ 1, τ 2, . . . , τ }. Each trajectory is evaluated by reward function, yielding rewards ri = R(τ i). GRPO then calculates group-relative advantage Ai for each trajectory by normalizing its reward against the statistics of the entire group: Ai = ri mean{r1, r2, . . . , rN } std{r1, r2, . . . , rN } . (3) The policy is then updated to favor trajectories with higher relative advantages by maximizing clipped surrogate objective function. This objective is designed to ensure stable training by preventing excessively large policy updates. The full objective is: JGRPO(θ) = qP (Q),{τ i}G i=1πθold (q) (cid:34) (cid:88) τ (cid:88) i= j=1 1 Gτ (cid:16) min (cid:16) jAi, clip mi si j, 1 ε, 1 + ε (cid:17) (cid:17) Ai β DKL (cid:0)πθ (cid:13) (cid:13) πref (cid:35) (cid:1) . (4) πθ(τ πθold (τ = sisi) Here, mi sisi) is the importance sampling ratio that measures the change between the new policy πθ and the old policy πθold used to generate the samples. The Kullback-Leibler (KL) divergence penalty, DKL(πθπref) regularizes the policy update by penalizing large deviations from reference policy πref. Reward Design Our reward function is designed to evaluate both the structural syntax and the semantic correctness of the models output. The total reward, Rtotal, is composite score defined as: Rtotal = Rformat (λtool Rtool + λacc Racc) (5) Here, Rformat acts as binary gate, ensuring that rewards for tool usage (Rtool) and final answer accuracy (Raccuracy) are granted only if the output adheres to the required structure. This design incentivizes the model to first master the correct syntax before optimizing for functional correctness. λacc is hyperparameter and the effect of it is discussed in Appendix B.5. Format Reward (Rformat) The format reward is binary signal that assesses the structural integrity of the models output. It verifies that the generated response contains all required special tokens in the correct order and follows predefined rules. (cid:40) Rformat = 1 if the output format is valid 0 otherwise (6) 19 If Rformat is 0, the total reward for the trajectory is nullified, creating strong imperative for the model to learn the required output structure. Tool Reward (Rtool) The tool reward provides fine-grained evaluation of the tool-calling process, with score ranging from 0 to 4. We employ hierarchical scoring system where each level must be passed to proceed to the next. I. Invocation Structure (Score 1): score of 1 is awarded if the tool call is correctly encapsulated within the <tool call> and </tool call> tokens. If not, the score is 0, and no further tool evaluation occurs. II. Tool Name Validity (Score 2): If the structure is correct, we verify that the invoked tool name exists in the set of available tools, . If the name is valid, the score becomes 2. III. Parameter Name Correctness (Score [2, 3]): If the tool name is valid, we assess the parameter names. partial score is awarded based on the proportion of correctly named parameters. perfect match yields score of 3. The score is calculated as: Rtool = 2 + paramscorrect name paramstotal (7) IV. Parameter Content Validity (Score [3, 4]): Finally, if all parameter names are correct (base score of 3), we evaluate the parameter values for semantic and contextual validity. The final score is proportional to the number of correct values, reaching maximum of 4. Rtool = 3 + paramscorrect content paramstotal (8) Accuracy Reward (Raccuracy) The accuracy reward evaluates the final outcome of the reasoning process, providing clear signal based on the correctness of the models final answer. Raccuracy = (cid:40) 4 if the final answer is correct 0 otherwise (9) This multi-faceted reward structure guides the model toward not only achieving the correct final answer but also mastering the intermediate steps of correct formatting and precise tool invocation."
        },
        {
            "title": "B EXPERIMENT DETAILS",
            "content": "B.1 TASK DEFINITION We evaluate our approach across diverse suite of three challenging tasks to validate whether our approach can help enhance the reasoning ability. Visual Spatial Planning We adopt the FrozenLake scenario (Towers et al., 2024) to evaluate models spatial planning and verification abilities. The navigation task requires the model to generate safe path from the start to the goal while avoiding holes, which demands accurate perception of the grid map and robust sequential reasoning to plan multi-step trajectories. The verification task instead focuses on state checking, determining whether given location or proposed path is safe, which isolates the perception and reasoning components of the planning pipeline. Together, these tasks expose two fundamental challenges for VLMs: (i) precise visual perception of spatial layouts under varying map sizes, and (ii) reliable reasoning over action sequences to ensure safety and goal completion. Concretely, we evaluate models on two benchmarks. The first is VSPO, dataset we construct to assess out-of-distribution generalization. During training, we use maps of sizes 4 4, 6 6, and 8 8, while reserving maps of sizes 3 3, 5 5, 7 7, and 9 9 for testing. This setup not only probes the models ability to generalize to unseen spatial configurations but also examines whether it truly learns to leverage tool usage for problem solving. The second is the original VSP benchmark (Wu et al., 2024), where we adopt the navigation and verification tasks to further test visual-spatial reasoning and state-checking capabilities under standardized settings. 20 Jigsaw The Jigsaw task (Noroozi & Favaro, 2016) evaluates models ability to reconstruct holistic understanding from fragmented visual inputs. Specifically, the model must infer the correct spatial arrangement of shuffled image patches and reason about their partwhole relationships. This requires fine-grained perception to capture local details, as well as global reasoning to integrate them into coherent whole. The key challenges lie in bridging localglobal consistency and maintaining semantic alignment across patches, making it strong test of visual compositionality and structural reasoning. Concretely, we evaluate models on two benchmarks. The first is Jigsaw-COCO, where we construct training and test splits based on the COCO train 2017 dataset (Lin et al., 2014). We extract the topleft, top-right, and bottom-left patches of each image to form the training set, while reserving the bottom-right patches for testing. This design allows us to probe the models out-of-distribution generalization and examine whether it truly learns to invoke tool usage for solving the puzzle. The second is the Jigsaw benchmark from BLINK (BLINK-J) (Fu et al., 2024), which provides standardized evaluation of fine-grained visual reasoning and compositional understanding under more challenging and diverse settings. GUIQA The GUIQA task is designed to evaluate models sophisticated capabilities in finegrained visual understanding and critical information extraction from GUIs. In this task, model is provided with GUI screenshot and an associated question, where the main difficulty lies in precisely grounding UI elements on dense layout, comprehending their functional roles, and performing multi-step reasoning by integrating scattered information. The evaluation is conducted on two distinct datasets. The first is the GUIChat (Chen et al., 2024), which specifically probes the models capacity for interactive, dialogue-based comprehension of webpage screenshots. Models are required to process complex queries related to visual information, human-centric needs, world knowledge, and reasoning. The second is the WebQA from the WebMMU (Awal et al., 2025). It offers structured evaluation across three distinct categories. Agentic Action tests the ability to understand UI elements like buttons and menus in order to formulate the necessary user actions, complete with precise spatial grounding. General Visual Comprehension assesses how well the model can extract and synthesize information from varied page components, including text, images, and graphics. And Multi-step Reasoning demands complex inference, numerical calculations, and comparisons across different parts of the UI. To better evaluate the effectiveness of our visual tools in GUI-based scenarios, we focus on the agent acting subset of the English split of WebMMU and report results on this subset in the main table to highlight the impact of our approach. Some of the complete WebMMU results are provided in Appendix B.4.3. General VQA General VQA aims to evaluate the general visual ability of models beyond structured visual reasoning tasks, focusing on open-ended visual understanding and reasoning in unconstrained real-world scenarios. Unlike structured benchmarks that provide well-defined goals and tool usage patterns, general VQA tasks often require flexible perception, commonsense reasoning, and robust interpretation of diverse visual content without explicit procedural guidance. To assess the general visual capabilities of our approach in such settings, we consider two representative benchmarks: V*(Wu & Xie, 2024) and HRBench(Wang et al., 2025b). V* evaluates models ability to perform fine-grained visual search and object attribute recognition in highresolution, complex visual scenes, focusing on precise localization and relationship reasoning that goes beyond simple image understanding. HRBench assesses the perception and reasoning capabilities of Multimodal Large Language Models on ultra-high-resolution images (up to 8K), testing fine-grained single-instance and cross-instance visual understanding, including detailed attribute extraction and spatial relationships in larger scenes. B.2 PROMPTS The system prompt used for guiding the tool-planning model is provided in Figure 8. B.3 IMPLEMENTATION DETAILS We developed AdaReasoner Framework, an end-to-end framework that orchestrates the entire lifecycle of our tool-planning models, from data curation to evaluation. At the heart of this framework 21 System Tool Prompt You are visual assistant capable of solving visual reasoning problems. You can rely on your own capabilities or use external tools to assist in solving. Available Tools In your response, you can use the following tools: {Tool List} Steps for Each Turn 1. Think: First, silently analyze the users request to understand the goal. This thinking process should be enclosed in <think> and </think> tags. 2. Decide Action: Based on your thinking, decide on one of the following two actions: - If you need to use tool: Generate your tool call, enclosed between <tool call> and </tool call> tags. Do not generate <response> in this turn. - If you have enough information to answer: Generate your final, user-facing answer, enclosed between <response> and </response> tags. Do not generate <tool call> in this turn. Output Format: Your output must always begin with your thought process. After the <think> block, you must provide either <tool call> or <response>, but never both in the same turn. Case 1: Tool Use is Required <think> Your thoughts and reasoning </think> <tool call> {name: Tool name, parameters: {Parameter name: Parameter content, ...: ...}} </tool call> Case 2: Ready to Respond to the User <think> Your thoughts and reasoning </think> <response> Your final response </response> Important Notes 1. You must always include the <think> field to outline your reasoning. Provide one of <tool call> or <response>. You must not include both <tool call> and <response> in the same turn because they are mutually exclusive. If tool usage is required, you must instead include both <think> and <tool call>, and omit <response> for that turn. If no further tool usage is required and ready to answer the users question, you can then use <think> to summarize your reasoning and include <response> with your final answer, and this indicates the ends of the conversation. 2. You can only invoke single tool call at time in the <tool call> fields. The tool call should be JSON object with name field and parameters field containing dictionary of parameters. If no parameters are needed, leave the parameters field an empty dictionary. All images have their coordinate origin at the top-left corner. 3. Some tools require image input. You do not need to generate or upload the actual image data simply refer to an image using placeholder in the form of img n. There may be multiple images present in the dialogue. Besides the original image, additional images may appear as result of prior tool calls (e.g., edited images returned by visual editing tools). You are free to select which image to use as input for the next tool. The index in img refers to the images position in the dialogue history: - The original image is always referred to as img 1. - Each subsequent image, including those returned from tools, is assigned img 2, img 3, and so on, in the order they appear in the dialogue. For example:{parameters: {image: img 1, other params: other values}} 4. All image coordinates used must be in absolute pixel values, not relative or normalized coordinates. 5. At the end, provide your final answer by placing it inside boxed{}, and wrap the entire final output inside <response></response> tags. Figure 8: Our system employs tool prompts to guide models in learning how to use tools effectively. 22 Category Hyperparameter Value / Setting Model Dataset Training Base Model Vision Tower Frozen MM Projector Frozen Finetuning Type DeepSpeed Stage Max Samples Cutoff Length Preprocessing Workers Batch Size per Device Gradient Accumulation Steps Effective Batch Size Learning Rate Epochs LR Scheduler Warmup Ratio Mixed Precision Logging / IO Logging Steps Checkpoint Save Steps Qwen2.5-VL-7B-Instruct True True Full ZeRO-3 332,649 35,536 64 1 2 2 1e-5 3 cosine 0.1 bfloat16 10 Evaluation Train/Validation Split Eval Batch Size per Device Eval Steps 90% / 10% 1 100 Table 7: Tool Cold Start (SFT) Training Configuration and Hyperparameters. is the Tool Server, unified, MCP-like service that manages all available tools, from simple offline utilities to compute-heavy online expert models. B.3.1 DATA CURATION During the data curation stage, we employ our AdaDataCuration module, which leverages the Tool Server to automatically generate high-quality cold-start trajectories. Specifically, we first design abstract, optimal problem-solving blueprints for each task, consisting of tool-call chain and chainof-thought (CoT) placeholders. We then prompt Gemini-2.5-Flash to fill these placeholders with detailed CoT reasoning. Finally, the Tool Server executes the corresponding tool calls and integrates the results into the dialogue, yielding complete and coherent training instance. B.3.2 TOOL COLD START STAGE During the cold-start stage, these trajectories are used for full-parameter supervised fine-tuning, for which we adopt the LLaMA Factory framework (Zheng et al., 2024). The key configurations and hyperparameters are summarized in Table 7. B.3.3 TOOL GRPO STAGE Following SFT, the model is further refined in the Tool-GRPO stage using AdaTG, our custom reinforcement learning framework inspired by Sheng et al. (2024); Zheng et al. (2025), which also relies on the Tool Server for live tool interactions. Specifically, AdaTG enables online reinforcement learning by allowing the model to interact with tools in real time through the Tool Server, receiving execution feedback at each planning step. Combined with the Tool-GRPO optimization objective, this interactive training process explicitly aligns the policy with long-horizon tool planning behaviors, resulting in robust and adaptive tool planning model. The key configurations and hyperparameters of Tool GRPO stage are summarized in Table 8. B.3.4 DETAILS OF OUR FINAL MODEL During the training of our final model, AdaReasoner-7B, we adopt two-stage training pipeline consisting of cold-start stage followed by Tool-GRPO stage. In the Tool Cold Start (TC) stage, we fine-tune the model using data from VSP, Jigsaw, and WebQA. During this phase, we enable Adaptive Learning to facilitate efficient adaptation across diverse task structures and reasoning 23 Category Hyperparameter Value / Setting Max Prompt Length Max Response Length Train Batch Size Shuffle Filter Overlong Prompts 8192 tokens 20480 tokens 32 True True FSDP Strategy True Gradient Checkpointing 8 PPO Mini-batch Size PPO Micro-batch Size / GPU 1 Max Token Len / GPU (PPO) Grad Clip Clip Ratio (PPO) PPO Epochs Entropy Coeff Use KL Loss Actor LR Weight Decay FSDP Param Offload FSDP Optimizer Offload # Nodes / GPUs 16384 1.0 0.2 1 0.0 False 1e-6 0.01 True True 1 node, 8 GPUs Data Policy Rollout Engine Temperature Top-p Top-k # Samples per Prompt (n) Dtype Tensor Model Parallel Size Max # Batched Tokens GPU Memory Utilization Enforce Eager Chunked Prefill Tool-Agent Max Turns per Episode Critic Algorithm Strategy LR Weight Decay PPO Epochs Grad Clip Advantage Estimator Gamma Lambda Use KL in Reward KL Coef Norm Adv by Std in GRPO vLLM 1.0 1.0 -1 4 bfloat16 2 32768 0.65 False False 10 FSDP 1e-5 0.01 1 1.0 GRPO 1.0 1.0 False 0.0 True Table 8: Key configurations and hyperparameters used in the Tool GRPO stage. patterns. In the subsequent Tool GRPO (TG) stage, we further train the model using broader set of tasks, including VSP, Jigsaw, WebQA, and additional visual search data. Adaptive Learning remains enabled throughout this stage to continuously adjust the models behavior as it interacts with different tools and task distributions. This two-stage training strategy allows AdaReasoner-7B to progressively acquire strong reasoning capabilities while maintaining robustness and generalization across diverse multimodal tasks. In Table 6, we report the performance differences of several prior methods under their original evaluation settings and our unified new setting. We observe that the performance gaps mainly stem from changes in tool types and definitions, as well as differences in the input order of images and questions. In contrast, our model exhibits strong robustness on the V* benchmark: its performance remains stable even when the imagequestion order is swapped or when the tool definition is changed, demonstrating superior adaptability to variations in tool definitions and evaluation protocols. 24 B.4 EVALUATION DETAILS B.4.1 BASELINES We evaluate our method against comprehensive set of strong baselines. First, we include proprietary models with state-of-the-art multimodal capabilities, including GPT-5-20250807 (OpenAI, 2025), Claude-sonnet-4-20250514 (Anthropic, 2025), and Gemini-2.5-flash (Comanici et al., 2025). Second, we consider open-source MLLMs, namely Qwen-2.5-VL-32B/72B-Instruct (Bai et al., 2025) and InternVL-3-78B (Zhu et al., 2025a), which are selected as primary testbeds due to their strong performance in visual understanding and reasoning, enabling us to assess the scalability of our approach across different model sizes. B.4.2 EVALUATION METHOD We design AdaEval, unified evaluation framework for tool planning that supports the evaluation of both tool-planning models and nontool-planning models. To ensure fair and consistent comparison, all models in our experiments are evaluated exclusively within the AdaEval framework under the same evaluation protocol. For benchmarks that require open-ended or subjective judgment, including VStar, WebMMU, and GUIQA, we adopt an LM-as-a-Judge evaluation strategy. Specifically, we use Qwen-2.5-VL-72B as the judge model. The prompt used for judgment is illustrated in Figure 9. System Tool Prompt You are an expert evaluator. Your goal is to determine if [Model Answer] correctly and factually answers [Question] when compared against [Standard Answer]. Core Evaluation Principle: The [Model Answer] is considered consistent if it contains the essential key information present in the [Standard Answer]. The [Model Answer] is allowed to be much more verbose, conversational, and include additional correct context or explanations. Your primary task is to verify the presence of the core facts, not to penalize extra information. If question asks for specific formatting like coordinates or tables, but the model identifies the correct core element textually, it should still be considered consistent. - Consistent (Judgement: 1): The [Model Answer] successfully identifies the main point or action from the [Standard Answer]. For example, if the standard answer is to click button A, the model answer is consistent if it mentions clicking or interacting with button A, even if its surrounded by other text. - Inconsistent (Judgement: 0): The [Model Answer] fails to mention the key information, provides contradictory information, or hallucinates different solution. Output Format: Just output Judgement: 1 or Judgement: 0. Do not output anything else. Figure 9: System Prompt Used for the LM-as-a-Judge Evaluation In addition, we conduct human evaluation on the V* benchmark by manually assessing the extracted model predictions of Qwen 2.5 VL 7B and AdaReasoner-7B. The human judgment scores are consistent with the scores produced by our evaluation framework, providing strong evidence for the reliability and correctness of our automated evaluation. B.4.3 DETAILED RESULTS Due to space constraints and for clarity of presentation, we report abridged results in the main paper and provide the full results here. Specifically, in Section 3.1, we evaluate the contributions of TC and TG under single-task fine-tuning. The summarized results are reported in Table 2, with the corresponding detailed results provided in Table 13. In Section 3.3, we present the generalization performance in Table 4, and report the full benchmark breakdown in Table 12. Finally, in Section 3.4, we report the main results in Table 5, with the complete version shown in Table 13. 25 B.5 ABLATION STUDY We systematically adjust λtool and λacc to evaluate their influence on learning dynamics and final performance. Specifically, we train the model on the same VSP task data for 100 RL steps under different reward-weight settings, monitor the training curves to ensure convergence, and then evaluate each checkpoints performance. The results are summarized in the table 9. As shown in table 9, the models performance consistently improves as the ratio λtool : λacc increases. This indicates that larger tool rewards not only accelerate convergence during RL training but also lead to significantly better final performance. These results validate that our tool-reward design is effective and plays crucial role in helping the model learn tool calling more efficiently and robustly."
        },
        {
            "title": "C FURTHER ANALYSIS",
            "content": "λtool : λacc VSP (%) Verify Overall Nav VSPO (%) Nav Verify Overall 0:1 1:2 1:1 2:1 51.83 49.50 64.00 90.33 71.45 70.55 78.73 93.27 Table 9: Ablation on reward-weight configurations for VSP and VSPO. 95.00 95.80 96.40 96.80 57.37 63.11 70.54 82.34 75.58 94.29 96.23 96. 41.78 36.44 48.56 70.33 C.1 VISUAL TOOLS HELP OVERCOME SCALE-BASED LIMITATIONS The results shown in section 3.1 reveal that tool augmentation can redefine the performance ceiling of MLLMs by overcoming scale-based limitations. As illustrated in Figure 10, while the baseline performance of 3B and 7B models is disparate and low, our tool-augmented versions both achieve near-perfect accuracy (94.7% and 97.6%). This indicates that the primary performance bottleneck has shifted from the models intrinsic scale to the extrinsic quality of the tools it wields. Consequently, this establishes powerful paradigm where even smaller, more efficient models can achieve state-of-the-art results, contingent not on their size, but on the instruments they are equipped with. C.2 WHY VISUAL TOOLS HELP Figure 10: Overcoming scale-based limitations with tool augmentation. On the VSP task, our tools boost the performance of both 3B and 7B models, elevating them from disparate baselines to near-uniform high performance. Our framework decomposes complex reasoning tasks into manageable steps, each resolved either by the model itself or by high-precision external tool. This design fundamentally shifts the problem-solving burden: instead of requiring flawless internal reasoning, the models primary task becomes effective tool planning. By delegating precise sub-tasks to reliable tools, the model is freed to focus on its core competencies of judgment, synthesis, and integrating the resulting outputs. Model Qwen 2.5 VL 3B Qwen 2.5 VL 7B Qwen 2.5 VL 32B Qwen 2.5 VL 72B Our POINT Tool Perception Point Loc. Acc. 2.47 47.01 6.54 50.00 100. VSP-Verify Reasoning Accuracy w/ Line w/ Point 57.92 (+7.01) 57.68 (+6.83) 61.31 (+8.19) 61.57 (+9.23) 49.09 (-1.82) 57.87 (+7.02) 87.87 (+34.75) 87.53 (+35.19) Base 50.91 50.85 53.12 52.34 Table 10: From Perception to Reasoning: Impact of our POINT tool. (Left) Comparison of start-point localization accuracy between Qwen 2.5 VL models and our specialized tool. (Right) Zero-shot reasoning performance on VSP-Verify task when augmented with tool-generated visual context (Line/Point). The significant gains in 32B/72B models demonstrate that high-precision perception is the key to unlocking agentic reasoning. Perception Tools Help MLLMs to See Our framework leverages expert perception tools to overcome the intrinsic perceptual limitations of MLLMs. As shown in Table 10, in VSP-verification, our 26 expert POINT tool achieves perfect localization accuracy (100.0% vs. 50.0% for baselines), and providing its coordinate output as context boosts the downstream zero-shot reasoning performance by an average of +18.79 points. This principle holds even with imperfect tools: for the Jigsaw task, our DETECTBLACKAREA tool achieves only 72.6% accuracy, yet it still provides significant perceptual advantage, underscoring the value of delegating such challenges to specialized tools. Manipulating Tools help MLLMs to verify Our manipulating tools empower the model to formulate and subsequently verify its own hypotheses. For example, in the VSP-Verify task, we teach the model to call DRAW2DPATH to explicitly draw red line on the frozen lake question picture. The problem is thus converted to verifying whether the red line crosses the blue ice holes. As shown in Table 10, even under zero-shot context-appending setting, the DRAWLINE does help improve the judge accuracy of the model, yielding an average performance improvement of +7.82 points. Similarly, in the Jigsaw task, the model can actively invoke the INSERTIMAGE tool to compose and evaluate different candidate pieces by inserting them into the original image, enabling more informed decision-making. High-Quality Trajectory data help MLLMs to plan While augmenting context with tool outputs is effective for zero-shot reasoning, this strategy alone is insufficient for achieving optimal performance. Tool-Cold-Start addresses this gap by explicitly teaching the model two foundational capabilities: how to use tools correctly and how to recognize the patterns where they should be applied. As shown in Table 2, for the 7B models, adding the Tool-Cold-Start phase before ToolGRPO yields massive performance improvement of +24.93 points on VSP and +19.82 points on Jigsaw compared to using Tool-GRPO alone. Besides this, the inclusion of reflection data during the Cold-Start phase provides further benefits to the models reasoning. As shown in Table 3, when ASTAR search is disabled, training with reflection data yields substantial improvement over the no-reflection checkpoints (91.36 vs. 67.27). C.3 THE DUAL ROLE OF COLD-START SUPERVISION central finding of our work concerns the dual role of the Tool Cold Start (TC, SFT) phase, which highlights critical trade-off between imparting expert knowledge and preserving models exploratory freedom. Our results suggest that the decision to include supervised pre-training stage is not universally beneficial, but rather highly contingent on the nature of the task. For complex, structured tasks with discernible optimal solutions, such as VSP and Jigsaw, the SFT phase provides decisive advantage. In these scenarios, discovering an effective tool-use trajectory from scratch is non-trivial exploration problem for the model due to its inherent reasoning or knowledge deficits. By exposing the model to high-quality, deterministic solution paths, the Tool Cold Start phase effectively bootstraps the learning process, instilling strong inductive bias towards correct strategy. The empirical results in Table 2 validate this unequivocally: for our 7B models, adding this SFT phase before Tool-GRPO yields massive performance improvement of +24.93 points on VSP and +19.82 points on Jigsaw compared to using Tool-GRPO alone. Conversely, for open-ended and highly generalized domains like GUIQA, the limitations of this predefined guidance become apparent. In such settings, the optimal tool-use strategy is often unknown even to human designers, making any human-designed trajectory likely sub-optimal. We find that rigid SFT phase can inadvertently restrict the models exploratory freedom during subsequent RL by creating strong policy bias, which hinders the discovery of more effective, emergent strategies. This effect is clearly observed in our results for the 7B model on the WebMMU benchmark, where the standalone Tool-GRPO approach actually outperforms the combined pipeline (72.97 vs. 68.16). This dichotomy suggests key principle for training tool-augmented agents: while injecting expert knowledge via SFT is powerful method for tasks with well-defined solution spaces, pure reinforcement learning approach like Tool-GRPO may be superior for more dynamic and general tasks that benefit from unconstrained exploration. 27 Model Qwen2.5-VL 3B + Direct SFT + Direct GRPO + TC (cold-start) + TG (tool-GRPO) + TC + TG vs. base Qwen2.5-VL 7B + Direct SFT + Direct GRPO + TC (cold-start) + TG (tool-GRPO) + TC + TG vs. base VSPO VSP WebMMU Nav Verify Overall Nav Verify Overall Avg. Act. Comp. Reason. 5.67 27.42 2.78 14.67 11.22 73.00 67.33 5.22 33.68 10.33 31.58 65.89 73.44 68.22 50.91 49.66 50.00 84.81 50.00 98.44 47.53 48.96 51.30 49.48 94.01 52.47 98.70 49.74 26.53 38.15 24.55 47.01 29.10 84.73 58.20 25.39 42.18 28.38 61.69 59.70 85.09 59. 7.50 34.50 18.33 23.33 22.67 92.17 84.67 12.33 42.67 12.50 41.00 88.17 96.33 84.00 49.80 44.00 50.00 84.40 50.00 97.80 48.00 47.00 51.40 51.40 93.60 55.20 99.20 52.20 26.73 38.82 32.73 51.09 35.09 94.73 68.00 28.09 46.64 30.18 64.91 73.18 97.64 69. 45.39 46.54 48.44 35.03 58.88 63.48 18.09 59.08 55.62 70.19 51.63 72.97 68.16 9.08 55.89 61.38 56.30 44.72 72.15 81.71 25.82 51.82 54.46 51.49 42.24 62.05 57.43 5.61 67.48 69.31 63.70 65.65 69.31 83.54 54.13 64.63 66.34 88.62 82.32 67.33 14.84 1.98 34.95 32.31 41.41 24.82 47.87 53.01 18. 48.46 44.79 60.94 41.12 64.61 58.30 9.84 Table 11: Sub-task Breakdown for Single Task Study. Detailed results for VSPO (Navigation and Verification), VSP (Navigation and Verification), and WebMMU (Avg. = overall average, Act. = Agentic Action, Comp. = Visual Comprehension, Reason. = Multi-step Reasoning). Best is bold, second-best is underlined. See Table 2 for complete results including Jigsaw, BLINK-J, and GUIChat. Model VSPO Verify Overall Nav VSP Verify Overall Avg. Nav WebMMU Act. Comp. Reason. Qwen2.5-VL 7B + TC + Rnd TC + TG + TC + TG + Rnd TC + TG 5.22 9.11 7.33 1.78 2.56 35.00 Rnd TC + Rnd TG 47.78 vs. base 48.96 49.35 49.35 50.00 50.00 84.42 95.84 +42.56 +46.88 25.39 27.66 26.71 24.01 24.43 57.78 69.94 +44.55 47.00 12.33 49.20 12.83 49.80 14.17 50.00 14.67 49.80 9.83 71.00 27.50 65.33 95.20 +53.00 +48.20 28.09 29.36 30.36 30.73 28.00 47.27 78.91 +50.82 59.08 43.16 40.51 56.71 55.15 59.62 60.98 +1. 67.48 41.87 42.89 67.68 59.76 69.11 70.93 +3.46 69.31 58.75 52.15 55.78 59.08 64.36 66.34 -2.97 48.46 37.15 33.63 49.19 50.07 50.66 51.40 +2.94 Table 12: Sub-task Breakdown for Generalization Study. Detailed results for VSPO (Navigation and Verification), VSP (Navigation and Verification), and WebMMU (Avg. = overall average, Act. = Agentic Action, Comp. = Visual Comprehension, Reason. = Multi-step Reasoning) from Table 4. Rnd TC and Rnd TG indicate randomized training on trajectories from 1-shot to 3-shot prompts. Best is bold, second-best is underlined. Model VSPO Verify Overall Nav VSP Verify Overall Attr. V* Spatial. Nav Closed-Source Models Gemini 2.5 flash GPT 5 Claude 4 sonnet 15.44 26.89 37.56 68.96 42.86 67.92 40.12 34.25 51.56 34.50 48.17 48.17 76.40 64.60 66. 53.55 55.64 56.27 74.78 72.17 59.13 59.21 78.95 60.53 Open-Source Models Qwen 2.5 VL 3B Qwen 2.5 VL 7B Qwen 2.5 VL 32B Qwen 2.5 VL 72B InternVL3 78B 5.67 5.22 7.56 17.22 7. 50.91 48.96 53.12 52.34 52.60 26.53 25.39 28.56 33.41 28.14 7.50 12.33 24.33 28.00 21.67 49.80 47.00 45.40 52.40 51.20 26.73 28.09 33.91 39.09 35.09 Tool-Planning Models Qwen 2.5 VL 7B + Tools Qwen 2.5 VL 72B + Tools GPT 5 + Tools DeepEyes PixelReasoner AdaReasoner 7B vs. base 48.96 11.89 51.43 29.56 69.87 38.11 17.27 4.11 37.79 7.33 47.78 98.31 +42.56 +49.35 28.98 39.64 52.75 10.18 21.38 71.08 +45.69 45.60 17.83 50.00 40.83 83.00 61.67 16.00 9.00 38.80 12.67 63.00 96.80 +50.67 +49.80 30.45 45.00 71.36 12.18 24.55 78.36 +50.27 38.26 64.35 72.17 79.13 80. 49.57 62.61 66.96 69.57 48.70 64.35 0.00 52.63 61.84 72.37 81.58 81.58 63.16 71.05 75.00 64.47 60.53 80.26 +18.42 Table 13: Sub-task breakdown for Main Results. Detailed results for VSPO (Navigation and Verification), VSP (Navigation and Verification), and V* (Attr. = Attribute Recognition, Spatial. = Spatial Relationship Reasoning) from Table 5. Best is bold, second-best is underlined."
        }
    ],
    "affiliations": [
        "Fudan University",
        "National University of Singapore",
        "The Chinese University of Hong Kong",
        "Tongji University",
        "University of Electronic Science and Technology of China",
        "University of Washington"
    ]
}