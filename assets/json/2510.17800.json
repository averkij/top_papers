{
    "paper_title": "Glyph: Scaling Context Windows via Visual-Text Compression",
    "authors": [
        "Jiale Cheng",
        "Yusen Liu",
        "Xinyu Zhang",
        "Yulin Fei",
        "Wenyi Hong",
        "Ruiliang Lyu",
        "Weihan Wang",
        "Zhe Su",
        "Xiaotao Gu",
        "Xiao Liu",
        "Yushi Bai",
        "Jie Tang",
        "Hongning Wang",
        "Minlie Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at https://github.com/thu-coai/Glyph."
        },
        {
            "title": "Start",
            "content": "Glyph: Scaling Context Windows via Visual-Text Compression Jiale Cheng1,2* , Yusen Liu2* , Xinyu Zhang2* , Yulin Fei2* , Wenyi Hong2,3 Ruiliang Lyu2 , Weihan Wang2 , Zhe Su2 , Xiaotao Gu2 , Xiao Liu2,3 , Yushi Bai2,3 Jie Tang3, Hongning Wang1 , Minlie Huang1 1The Conversational Artificial Intelligence (CoAI) Group, Tsinghua University 2Zhipu AI 3The Knowledge Engineering Group (KEG), Tsinghua University chengjl23@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn 5 2 0 2 0 2 ] . [ 1 0 0 8 7 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take different perspectivevisual context scalingto tackle this challenge. Instead of extending token-based sequences, we propose Glyph, framework that renders long texts into images and processes them with visionlanguage models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 34 token compression while maintaining accuracy comparable to leading LLMs such as Qwen38B on various long-context benchmarks. This compression also leads to around 4 faster prefilling and decoding, and approximately 2 faster SFT training. Furthermore, under extreme compression, 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits realworld multimodal tasks, such as document understanding. Our code and model are released at https://github.com/thu-coai/Glyph."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in large language models (LLMs) have enabled remarkable progress across wide spectrum of real-world tasks (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023; GLM et al., 2024; Yang et al., 2025). As LLMs become increasingly capable, the demand for long-context * Core contributors. Corresponding author. 1 Figure 1: (Upper) Comparison of two paradigms for long-context tasks: conventional approaches directly feeding plain text into LLMs, and the proposed VLMbased paradigm, Glyph, which renders text as compact images to achieve substantial input-token compression. (Lower) Glyph attains competitive performance on LongBench and MRCR, while offering significant compression and inference speedup over its text backbone model on 128K-token inputs. modeling has grown critical, especially for applications such as document understanding, code analysis, and multi-hop reasoning (Bai et al., 2024; Comanici et al., 2025). However, scaling context windows to hundreds of thousands or even millions of tokens poses prohibitive training and inference costs in both computation and memory, severely limiting the practicality of such models in realworld applications. Recent work has explored two main directions to alleviate these costs. One line of work extends positional encodings, such as YaRN (Peng et al., 2023), allowing well-trained models to accept longer inputs without additional training. However, such methods neither accelerate inference nor maintain accuracy when extrapolated to much longer sequences (Wu et al., 2024). Another line focuses on modifying the attention mechanism, e.g., sparse or linear attention (Huang et al., 2023; Yang et al., 2024; Peng et al., 2025; Chen et al., 2025a), which reduces the quadratic complexity of self-attention and improves per-token efficiency. Yet, as context length grows to hundreds of thousands of tokens, the overall overhead remains substantial, since the number of tokens is unchanged. Retrievalaugmented approaches (Laban et al., 2024; Yu et al., 2025a) instead shorten the input length through external retrieval, but they risk missing important information and could introduce additional latency. Distinct from the aforementioned approaches, we propose Glyph, new paradigm that scales context length by rendering plain text into compact images and leveraging vision-language modIn els (VLMs) to process the rendered inputs. this way, the VLM operates directly on the glyphs of the texttreating each visual token as compact carrier of multiple textual tokensthereby increasing the information density without sacrificing semantic fidelity. This glyph-based visual representation allows fixed-context VLM to process substantially longer texts than text-only LLM with the same context length, thereby enabling long-context understanding without expanding the context window or relying on external retrieval mechanisms. For example, consider the novel Jane Eyre (240K text tokens). conventional 128Kcontext LLM cannot accommodate the entire book, and truncation easily leads to wrong answers for questions requiring global coverage, such as Who supports Jane when she is in distress after leaving Thornfield? In contrast, Glyph renders the book into compact images (e.g. 80K visual tokens), enabling 128Kcontext VLM to process the full novel and answer such questions reliably. Specially, Glyph consists of three main stages, namely, continual pre-training, LLM-driven rendering search and post-training. In the continual pretraining stage, we render large-scale long-context text into diverse visual forms, enabling the VLM to transfer its long-context capability from text tokens to visual tokens. Since the text-to-image conversion directly determines the trade-off between context compression and model performance, devising an optimal configuration of the conversion is crucial for downstream performance. To this end, we design an LLM-driven genetic search to automatically explore rendering parameters (e.g. font size, layout, resolution) to maximize compression while preserving long-context ability. The resulting configuration is then applied in the post-training stage, where we perform supervised fine-tuning and reinforcement learning to further improve the models performance on visualized input. An auxiliary OCR task is applied to enhance the models ability to recognize textual content within images, thereby better aligning its visual and textual representations, yielding the final Glyph model. We conduct extensive experiments to evaluate the performance of Glyph. Results demonstrate that Glyph achieves 34 token compression of long sequences while preserving accuracy comparable to state-of-the-art LLMs such as Qwen3-8B. This compression not only extends the effective context length but also improves both training and inference efficiency, yielding up to 4.8 faster prefilling, 4.4 faster decoding, as well as about 2 faster SFT training. Moreover, we find that incorporating rendered text data effectively enhances performance on real-world multimodal long-context tasks, such as document understanding. Our contributions can be summarized as follows: We introduce novel framework, Glyph, which enables long-context modeling through visual-text compression using VLMs, providing an alternative route to scaling context windows without incurring prohibitive computational and memory costs. We propose an LLM-driven genetic search that automatically identifies the optimal configurations of text-to-image rendering, ensuring both task performance and effective compression. We demonstrate that Glyph can achieve 3-4 token compression for long text sequences while preserving performance, enabling substantial improvements in memory efficiency, training and inference speed."
        },
        {
            "title": "2.1 Long-Context Modeling",
            "content": "Research on extending LLMs to long contexts mainly focuses on architectural and training methods. Architecturally, studies have proposed sparse and hierarchical attention (Yang et al., 2016; Beltagy et al., 2020; Huang et al., 2023; Yang et al., 2 Figure 2: Glyph consists of three main stages: continual pre-training on rendered long-text data, LLM-driven genetic search for optimal rendering configurations, and post-training with SFT, RL. Together, these stages enable efficient long-context modeling with visual-text compression. 2024; Peng et al., 2025; Chen et al., 2025a), positional interpolation and extrapolation (Su et al., 2021; Press et al., 2021; Sun et al., 2022; Peng et al., 2023), and content-aware encodings (Chen et al., 2025b; Zhu et al., 2024). On the training side, LongAlign (Zhang et al., 2024) had built instruction datasets and loss-weighting strategies for sequences up to 100k tokens, while LongLoRA (Chen et al., 2024) had combined shifted sparse attention with parameter-efficient fine-tuning. LongRecipe (Wang et al., 2024b) had improved efficiency by integrating token analysis, index transformation, and optimization, scaling open-source models from 8k to 128k. ProLong (Liu et al., 2024b) had taken data-centric view, selecting samples with long-range dependencies. In contrast, our method compresses text into visual tokens, which can be combined with existing techniques to reduce cost and extend context length."
        },
        {
            "title": "2.2 Multimodal Large Language Model",
            "content": "Multimodal large language models (MLLMs) extend traditional LLMs to process and reason over text and visual inputs jointly. Early studies primarily focus on architectural design and effectively leveraging powerful language backbones, as exemplified by PALI (Chen et al., 2022), LLaVA (Liu et al., 2023), and CogVLM (Wang et al., 2024a). Subsequent work further enhances these models through improvements in both LLM backbones and large-scale vision-language pretraining (Hong et al., 2024a; Bai et al., 2025), while also expanding to additional modalities such as video and audio (Hurst et al., 2024). Notably, MLLMs demonstrate strong capabilities in image perception and optical character recognition (OCR) (Hong et al., 2024b; Liu et al., 2024a), where multiple characters or words can be represented by single visual token, highlighting the potential for effective context compression."
        },
        {
            "title": "3 Method",
            "content": "We present Glyph, novel paradigm for scaling long-context text understanding through visual compression. Unlike conventional long-context LLMs that extend token-based context windows, Glyph transforms ultra-long textual inputs into compact visual images and processes them with visionlanguage model. This fundamentally different modeling method bypasses the prohibitive memory and computation costs of million-token sequences while preserving textual semantics. Furthermore, we introduce an LLM-driven genetic search to automatically discover optimal rendering configurations, ensuring the best trade-off between compression ratio and performance."
        },
        {
            "title": "3.1 Overall Framework",
            "content": "As illustrated in Figure 2, Glyph consists of three tightly-coupled stages: (1) Continual Pre-Training, which teaches the VLM to understand and reason over rendered long texts with diverse visual styles; (2) LLM-Driven Rendering Search, automatically discovers the optimal rendering configuration for downstream tasks; and (3) Post-Training, including SFT and RL under the discovered configuration to further improve the models long-context capabilities. Together, these stages enable Glyph to 3 achieve both high accuracy and significant gains in token compression, computational efficiency, and memory usage."
        },
        {
            "title": "3.2 Task Definition",
            "content": "Task Formulation. We formalize the standard long-context instruction following task as triple (I, C, R), where is concise user instruction specifying the core goal, = {c1, . . . , cT } is an ultra-long textual context, and is the target response. The conventional learning objective is to maximize (R I, C), i.e., to generate an accurate response conditioned on both the instruction and the long textual context. Scaling this token-based formulation to milliontoken contexts, however, imposes prohibitive memory and computation costs. To overcome these limitations, we reformulate the input representation through visual compression. Instead of directly feeding as text tokens, we render it into sequence of visual pages = {v1, . . . , vn} each containing the glyphs of multiple text segments. This allows the model to reason over compressed but semantically equivalent input: (R I, V). Each training instance is thus represented as (I, V, R). Rendering Pipeline. The rendering pipeline parameterizes how text is visualized before being fed into the model. Each rendering is specified by configuration vector: θ = (cid:0)dpi, page_size, font_family, font_size, line_height, alignment, indent, spacing, h_scale, colors, borders, . . . (cid:1), which controls typography, layout, and visual style of the rendered pages. Given the context and configuration θ, the pipeline produces sequence of images that serve as the VLMs long-context input. To quantify the degree of compression, we define the compression ratio: ρ(θ) = i=1 τ (vi) (cid:80)n , where τ (vi) denotes the number of visual tokens consumed by page vi. higher ρ indicates that each visual token encodes more textual information, thus achieving stronger compression. In practice, θ determines both information density (through font size, dpi) and visual clarity (through layout and spacing). By varying θ, we can continuously adjust the balance between compression and readability for the VLM."
        },
        {
            "title": "3.3 Continual Pre-Training",
            "content": "The purpose of continual pre-training is to transfer long-context comprehension from the textual to the visual modality. This stage exposes the VLM to wide range of rendering styles and tasks so that it can align the semantics between rendered images and their corresponding texts. Data Construction. To enhance model robustness, better aligning long-text capability, we adapt diverse rendering configurations over large amount of long-context text data. We also develop series of rules to exclude the improper combination of rendering parameters, e.g., smaller line height than font size. Moreover, with human prior, we define several style themes, including document_style, web_style, dark_mode, code_style and artistic_pixel. These themes are designed to capture wide range of document layouts and text styles, which can better exploit the knowledge that VLM has obtained in its pre-training stage."
        },
        {
            "title": "We further introduce three families of continual",
            "content": "pre-training tasks, including: OCR Tasks: the model reconstructs all text on one or multiple rendered pages. Interleaved Language Modeling: certain text spans are rendered as images, while the rest remain in text, training the model to switch seamlessly between modalities. Generation Tasks: given partial rendered pages (e.g., the beginning or end), the model completes the missing parts. These tasks jointly teach the model to read, reason, and generate under visually compressed contexts. Loss Function. We minimize the cross-entropy loss LCPT = E(I,V,R) (cid:88) log Pϕ(rt I, V, r<t), (1) where denotes an optional instruction (e.g. absent in interleaved language modeling tasks) and ϕ is initialized from the base VLM. This stage 4 produces model capable of understanding rendered text and handling long contexts, referred to as Glyph-Base."
        },
        {
            "title": "3.4 LLM-Driven Rendering Search",
            "content": "Although diverse rendering improves generalization, downstream tasks often require specific trade-off between compression and visual clarity for the VLM. We therefore perform an LLM-driven genetic search after continual pre-training to automatically identify the optimal rendering configuration θ used in the post-training stage. Genetic Algorithm. Starting from an initial population of candidate configurations {θk} sampled from pre-training configurations, we iteratively perform the following steps: 1. Rendering Data: render the validation set using each configuration θk to obtain visual inputs. 2. Evaluation on Validation Set: perform model inference on the rendered data, measure task accuracy and compression ratio, and update the results. 3. LLM Analysis & Critique: use an LLM to suggest promising mutations and crossovers based on the current population and validation results. 4. Search History: record all configurations and their performance; rank and sample promising candidates for the next iteration. This process continues until the population converges, i.e., when no further improvement is observed in validation accuracy or compression over pre-defined number of generations. The resulting configuration θ is then adopted for post-training."
        },
        {
            "title": "3.5 Post Training\nWith the optimal rendering configuration θ∗ fixed,\nwe further improve Glyph-Base through two com-\nplementary optimization stages—supervised fine-\ntuning and reinforcement learning—supplemented\nby an auxiliary OCR alignment task. Together,\nthese components jointly enhance the model’s abil-\nity to reason over visually compressed inputs and\nto recognize textual details.",
            "content": "in which each example contains explicit reasoning traces (e.g., <think>...</think>). This encourages the model to perform step-by-step reasoning when reading massive token contexts. Formally, the loss function can be written as LSFT = E(I,V,R) (cid:88) log Pϕ(rt I, V, r<t), (2) where ϕ is initialized from the continual pretraining checkpoint. This stage establishes strong initialization for reinforcement learning. Reinforcement Learning. After SFT, we further refine the policy using Group Relative Policy Optimization (GRPO). For each input (I, V), we sample group of candidate responses {r1, . . . , rG} from the old policy πϕold. We first define the importance sampling weight: wi = πϕ(ri I, V) πϕold(ri I, V) . (3) Each sampled response ri receives reward score u(ri) {0, 1}, which integrates: Verifiable rewards from an external LLM judge, scoring based on the accuracy of answer, which is reference-based llm-as-ajudge with the reference being the ground truth. Format rewards that ensure the response correctly follows the defined thinking style. The group-normalized advantage is computed as: Ai = u(ri) mean({u(rj)}G std({u(rj)}G j=1) j=1) , (4) and the GRPO objective is JGRPO(ϕ) = (I,V)P, {ri}G i=1πϕold (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) (cid:16) i=1 min (cid:0)wiAi, clip(wi, 1 ϵl, 1 + ϵh) Ai (cid:1)(cid:17) (cid:35) (cid:1) β DKL (cid:0)πϕ πSFT , where ϵ and β are hyperparameters. (5) Supervised Fine-Tuning. To endow the model with robust comprehension under visual inputs, we curate high-quality text SFT corpus and render its long-context inputs using the optimal configuration. Each response adopts thinking-style format, Auxiliary OCR Alignment. persistent challenge of visual compression is the faithful recovery of fine-grained text from rendered images. Throughout both SFT and RL, we therefore incorporate an auxiliary OCR alignment task that"
        },
        {
            "title": "Model",
            "content": "GPT-4.1 Model GPT-4.1 Single-Doc QA Multi-Doc QA"
        },
        {
            "title": "Summarization",
            "content": "Few-shot"
        },
        {
            "title": "Avg",
            "content": "QP"
        },
        {
            "title": "NQA HQA",
            "content": "2QA QSUM GovRep TREC TriQA PR Zh PR En RB"
        },
        {
            "title": "LCC",
            "content": "51.60 35.73 69.10 74.15 23.50 LLaMA-3.1-8B-Instruct 44.56 Qwen3-8B 44.67 43.75 GLM-4-9B-Chat-1M Qwen2.5-7B-Instruct-1M 45. 26.34 26.13 26.72 25.61 56.88 65.83 58.98 60.70 46.67 73.92 50.89 40."
        },
        {
            "title": "Glyph",
            "content": "40.64 28.45 66.42 72.98 23.28 19.60 22.84 22.95 19. 33.36 32.36 26.85 27.60 29.97 25.53 77.00 93.36 100. 100.00 67.94 68.43 56.03 19.25 70.50 61.50 59.37 82. 89.12 87.98 90.07 86.93 62.20 100.00 100.00 98.5 99.50 97.26 99.50 100.00 42.81 40.89 55.64 29.80 46.35 44.87 59.54 21.72 41.34 47.46 49.27 42. 88.54 89.03 99.50 60.80 48.85 50. Table 1: Performance comparison of Glyph with leading LLMs on LongBench (%). Our model achieves competitive results in the overall average score. Best results are bolded, and second-best are underlined. Refer to Table 10 for the reset of results. 4 Needle 8 Needle 0k-8k 8k-16k 16k-32k 32k-64k 64k-128k Avg 0k-8k 8k-16k 16k-32k 32k-64k 64k-128k Avg 38 33.42 LLaMA-3.1-8B-Instruct 29.34 Qwen3-8B GLM-4-9B-Chat-1M 15.17 Qwen2.5-7B-Instruct-1M 25.96 25.97 22.67 13.78 20.13 Ours 35.44 26. 29 22.73 20.34 9.18 19.93 24.15 42 26.97 23.63 20.27 24.25 25. 38 39.4 33 26 12.68 19.11 15.05 17.29 16. 24.35 23.02 14.69 21.51 23.80 18.75 14.55 17.64 17.69 19.69 9.65 19.48 25.81 25.12 21. 17 19.85 16.81 9.34 12.41 16.43 22 17.72 17.86 9.47 14.80 13. 19 11.79 15.00 8.97 14.24 13.51 23.4 18.17 17.62 10.40 15.71 18. Table 2: Performance comparison of our model against leading LLMs on the 4-needle and 8-needle sub-tasks of the MRCR benchmark (%). Our method consistently ranks first or second across most settings while preserving about 3 compression ratio. Performance on the 2-needle task is deferred to the Appendix. Figure 3: Performance comparison of Glyph and the baseline across different context windows, demonstrating that Glyph achieves performance equivalent to longer contexts with substantially shorter context window. encourages the model to correctly read and reproduce low-level textual details. The form of OCR task is the same as in the continual pre-trianing stage. In the RL stage, the reward for the OCR task is given by the Levenshtein distance. By integrating structured SFT supervision, RL optimization, and continuous OCR-aware alignment, Glyph acquires both powerful long-context reasoning ability and stable low-level text recognition, achieving strong downstream performance under highly compressed visual contexts."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "To comprehensively evaluate the effectiveness of our method, we have conducted extensive experiments covering long-context understanding, efficiency, cross-modal generalization and several ablations and analysis. Implementation details, descriptions of baselines and benchmarks are provided in Appendix B."
        },
        {
            "title": "4.2.1 Results on LongBench & MRCR",
            "content": "Tables 1 and 2 summarize overall results. Glyph achieves performance on par with or surpassing state-of-the-art text-only LLMs of similar size, including Qwen3-8B and GLM-4-9B-Chat-1M, demonstrating that Glyph remains effective on long-context tasks with large reduction in input tokens. Figure 3 further illustrates the context scaling 6 Model GPT-4.1 Niah-S1 Niah-S2 Niah-M1 Niah-M2 Niah-V Niah-Q VT CWE FWE QA-1 QA-2 Avg 100.0 98.85 LLaMA-3.1-8B-Instruct 99.33 Qwen2.5-7B-Instruct-1M 100.00 100.00 Qwen3-8B 100.00 GLM-4-9B-Chat-1M 99.33 99.67 100.00 100.00 100. 99.33 99.67 95.33 92.67 100.0 99.00 99.00 84.67 99.00 99.67 98.17 93.83 97.42 95.00 100. 100.0 97.87 98.66 86.82 77.47 96. 99.67 98.75 99.33 100.00 87.07 85.40 98.47 98.20 57.30 72.10 74.67 49.50 81.85 85.67 86.67 83.22 84.00 80.00 70.33 72.67 58.00 60.67 53.33 56. 87.55 88.61 87.29 86.08 Glyph Glyph Glyph DPI: 72 / Compression rate: average 4.0, up to 7.7 73. 64.67 67.33 56.00 73.42 71.42 77. 94.40 92.67 59.33 63.33 72.17 DPI: 96 / Compression rate: average 2.2, up to 4. 98.00 95.33 95.67 85.00 96.33 95. 94.93 94.80 98.00 79.00 70.67 91. DPI: 120 / Compression rate: average 1.2, up to 2.8 99.67 99.00 100.00 93.67 99. 99.58 99.33 98.97 99.11 79.00 74. 94.67 Table 3: Performance on the Ruler benchmark (%). We demonstrate the impact of different DPI settings on our models performance and the resulting compression ratios. For each configuration, the table includes both the average compression ratio across all sub-tasks and the maximum compression achieved for specific sub-task types. Figure 4: Speedup ratios of Glyph over the text backbone model for prefill, decoding, and training across different sequence lengths. of around 3effectively gains about 96k tokens worth of original text. This advantage translates into faster improvement as the context length increases."
        },
        {
            "title": "4.2.2 Results on Ruler",
            "content": "On the Ruler benchmark, Glyph also achieves performance comparable to leading LLMs across most categories  (Table 3)  . We exclude the UUID task from this benchmark due to its huge difficulty for VLMs, which is further discussed in the limitations section. Beyond raw scores, we demonstrate the advantage of test-time scaling. When we increase the rendering resolution (DPI) at inference time, our model shows substantial gains: at higher DPI settings, it even surpasses strong text-only baselines. This demonstrates that the performance of VLMs on text-only long-context tasks has high ceiling, and that Glyph still holds considerable potential. Furthermore, we analyze performance under different sequence lengths (Figure 5). At short contexts, text-only models such as LLaMA-3.1-8BInstruct maintain slight edge. However, as the input length grows, Glyph exhibits obviously slower degradation. This aligns with the earlier observaFigure 5: Model performance degradation across different sequence lengths on the Ruler benchmark. behavior of Glyph. For LongBench, we report the results with truncated contexts; for MRCR, we utilize the original dataset split. On LongBench, our model achieves an average effective compression ratio of 3.3, with certain tasks reaching up to around 5. On MRCR, the average compression ratio is 3.0. This means that within the same token budget, Glyph can effectively utilize several times more original context than text-only models. More importantly, as the input length grows, this advantage scales up. When text-only model extends its window from 32k to 64k tokens, it gains 32k additional tokens of usable context. Under the same expansion, Glyph with compression ratio"
        },
        {
            "title": "Model",
            "content": "SP CP UA"
        },
        {
            "title": "Acc",
            "content": "F1 GLM-4.1V-9B-Base 36.76 23.41 21.52 29. 28.78 Glyph-Base 47.91 22.24 14.80 32. 34."
        },
        {
            "title": "Glyph",
            "content": "57.73 39.75 27.80 45.57 46.32 Table 4: Results on MMLongBench-Doc (%). SP, CP, UA, and Acc denote Single-page, Cross-page, Unanswerable, and Overall Accuracy, respectively."
        },
        {
            "title": "Configuration",
            "content": "LongBench MRCR Ruler Avg."
        },
        {
            "title": "Random Config\nManual Config",
            "content": "Search-based Config 41.78 43.45 43.45 15.82 19.33 65.13 68.09 40.91 43. 22.10 71.24 45.60 Table 5: Ablation study comparing randomly combined, manually designed, and search-based configurations on three benchmarks under SFT setting. The search-based configuration achieves the best overall performance. tions on LongBench and MRCR: thanks to compression, an increase in the nominal text context window translates to much smaller increase in the effective length the Glyph model actually needs to handle. Consequently, our model maintains accuracy more stably as the context grows."
        },
        {
            "title": "4.3 Efficiency Evaluation",
            "content": "We further evaluate the efficiency of our method in both training and inference, comparing Glyph with the text backbone model. The evaluation setting is detailed in Appendix B. As shown in Figure 4, Glyph provides clear speedups in both metrics, demonstrating significant gains at the inference stage and SFT training stage. As the sequence length grows from 8k to 128k, our model demonstrates markedly better scalability, achieving stable SFT training throughput speedup and growing inference speedup."
        },
        {
            "title": "4.4 Cross-Modal Generalization",
            "content": "Although our training data mainly consists of rendered text images rather than natural multimodal inputs, we are interested in whether such training can generalize to real-world multimodal tasks, like long document understanding. To this end, we evaluate Glyph on the MMLongBench-Doc benchmark, which contains 130 long PDF documents with diverse layouts and embedded images. As shown in Table 4, Glyph achieves clear improvements over our backbone model GLM-4.1V-9B-Base, confirming its ability to generalize across modalities."
        },
        {
            "title": "Glyph",
            "content": "w/o OCR (in RL) w/o RL w/o OCR (in SFT)"
        },
        {
            "title": "Ruler",
            "content": "50.56 -1.40 -7.11 -8.12 26.27 72.17 -2.00 -4.17 -8.42 -0.35 -0.93 -1. Table 6: Ablation study showing the performance drop (%) relative to the final Glyph model when components are progressively removed."
        },
        {
            "title": "8 Needle",
            "content": "GLM-4-9B-Chat-1M Qwen2.5-7B-Instruct-1M"
        },
        {
            "title": "Glyph",
            "content": "10.08 11.36 9.36 6.19 7.34 7.62 2.26 7.77 7. Table 7: Average MRCR performance (%) across 128K1M context lengths under different needle counts."
        },
        {
            "title": "4.5 Ablation Study & Analysis",
            "content": "We conduct series of ablations and analyses to better understand our method. Configuration Search. We compare three types of rendering configurations for SFT: (i) random sampled configuration from the pre-training sets, (ii) manually designed settings based on prior knowledge, and (iii) the configuration obtained from our search procedure. While all settings achieve comparable compression ratios, Table 5 shows that the searched configuration consistently outperforms the other two, both on average and across most individual tasks. This demonstrates the importance of systematic exploration for finding appropriate rendering strategies. OCR Auxiliary Tasks. We also test the impact of adding OCR auxiliary tasks during both SFT and RL training. As shown in Table 6, including OCR objectives yields consistent performance gains across benchmarks. This suggests that explicitly reinforcing low-level text recognition helps the model build stronger representations, which in turn improves long-context understanding ability. Extreme Compression Exploration To further examine the potential of our approach, we explore more aggressive compression settings. We apply configuration with an effective 8 compression ratio during post-training, and evaluate the resulting model on MRCR with sequence lengths extended from 128k to 1024k. The results  (Table 7)  show that Glyph successfully demonstrates the potential for 8 effective context expansion, achieving 8 performance on par with GLM-4-9B-Chat-1M and Qwen2.5-1M. This experiment highlights that our method can indeed be pushed to more extreme compression regimes while retaining performance, suggesting substantial headroom for extending usable context far beyond current limits, like model that can deal with 4M even 8M context tokens."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present Glyph, an efficient longcontext modeling framework that renders long texts into compact images and processes them with vision-language models. With continual pretraining, an LLM-driven genetic rendering search and targeted post-training, Glyph achieves 34 context compression while maintaining competitive performance with similar size leading LLMs such as Qwen3-8B. Extensive experiments further demonstrate substantial gains in inference speed and memory efficiency, and show that our method demonstrates cross-modal benefits, enhancing multimodal long-context tasks like document understanding. Our findings demonstrate that enhancing token information density constitutes promising new paradigm for scaling long-context LLMs, orthogonal to existing attention-based approaches, and there remains great room for further exploration in depth."
        },
        {
            "title": "Limitations and Future Work",
            "content": "Despite the effectiveness of Glyph and its strong potential for broader applications, we want to discuss several limitations of the current work that worth further exploration. Sensitivity to rendering parameters. Our method relies on rendering textual inputs into images before processing. We find that performance can be noticeably affected by rendering configurations such as resolution, font, and spacing. Although our search procedure allows us to identify configuration that performs well on downstream tasks, how to make the model more robust across various rendering settings remains an open problem. OCR-related challenges. As discussed in the Ruler benchmark, UUID recognition remains particularly challenging for current VLMs, and even the strongest models (e.g., Gemini-2.5-Pro) often fail to reproduce them correctly. Such rare alphanumeric sequences frequently result in misordered or misclassified characters, which may stem from their distributional sparsity in training data or from architectural limitations of visual encoders. While these cases have little impact on most tasks, improving OCR fidelity could push the upper bound of our approach. Task diversity. The benchmarks in this work mainly focus on long-context understanding. While these tasks provide strong proof of concept, they do not fully capture the diversity of real-world applications, such as agentic or reasoning-heavy tasks. We also observe that, compared with textual models, the visual-text model tends to generalize less effectively across tasks. Extending the scope of evaluation and training to wider range of tasks will help better assess and improve the robustness and generality of our approach. Future directions. Building upon the current study, several directions could further advance the proposed visualtext compression paradigm. First, rather than using fixed rendering strategy, one promising avenue is to train adaptive rendering models that condition on the task type or user query, producing tailored visualizations that balance compression and performance. Second, enhancing the visual encoders capability for fine-grained text recognition and alignment with language representations could improve robustness and transferability across tasks. Third, improving the alignment between visualtext and purely textual models, for instance through knowledge distillation or crossmodal supervision, could narrow the performance gap in generalization. Fourth, our approach could be extended to broader applications, such as agent memory systems capable of managing long-term conversations or agentic contexts, and tasks that can leverage structured visual layouts for reasoning and retrieval. From the perspective of context engineering, this method offers new way to optimize how contextual information is represented and managed. With further advances along this line, future models could go beyond the current limits of context length, effectively scaling from 1M to 10M input tokens."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. 9 Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, and 1 others. 2024. Longbench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3119 3137. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. In Longformer: The long-document transformer. Proceedings of ACL. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, and 12 others. 2020. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS20, Red Hook, NY, USA. Curran Associates Inc. Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, and 38 others. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. Preprint, arXiv:2406.12793. Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, and 1 others. 2024a. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and 1 others. 2024b. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290. Yunpeng Huang, Jingwei Xu, Junyu Lai, Zixu Jiang, Taolue Chen, Zenan Li, Yuan Yao, Xiaoxing Ma, Lijuan Yang, Hao Chen, and 1 others. 2023. Advancing transformer architecture in long-context large language models: comprehensive survey. arXiv preprint arXiv:2311.12351. Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, and 1 others. 2025a. Minimaxm1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Xi Chen, Xiao Wang, Soravit Changpinyo, Anthony Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, and 1 others. 2022. Pali: jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794. Philippe Laban, Alexander Richard Fabbri, Caiming Xiong, and Chien-Sheng Wu. 2024. Summary of haystack: challenge to long-context llms and rag systems. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 98859903. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2024. Longlora: Efficient fine-tuning of long-context large language models. In The International Conference on Learning Representations (ICLR). Yutao Chen, Yiren Wang, and 1 others. 2025b. Cope: Complex positional encoding for long context extrapolation. arXiv preprint arXiv:2508.18308. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, and 1 others. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and 1 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Team GLM, :, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024a. Llavanext: Improved reasoning, ocr, and world knowledge. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Advances in neural information processing systems, 36:34892 34916. Yang Liu, Wei Gao, and 1 others. 2024b. Long context is not long at all: prospector of long-dependency arXiv preprint data for large language models. arXiv:2407.11234. Baolin Peng, Zhuohan Li, and 1 others. 2023. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071. Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Xingjian Du, Haowen Hou, Jiaju Lin, Jiaxing Liu, Janna Lu, William Merrill, and 1 others. 2025. Rwkv-7\" goose\" with expressive dynamic state evolution. arXiv preprint arXiv:2503.14456. Ofir Press, Noah A. Smith, and Mike Levy. 2021. Train short, test long: Attention with linear biases enarXiv preprint ables input length extrapolation. arXiv:2108.12409. 10 Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer arXiv preprint with rotary position embedding. arXiv:2104.09864. Zhen Sun, Peng Cheng, Wei He, and 1 others. 2022. Xpos: Improving position interpolation with extrapolation. arXiv preprint arXiv:2212.10554. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and 1 others. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, and 1 others. 2024a. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing Systems, 37:121475121499. Yizhi Wang, Fan Yang, and 1 others. 2024b. Longrecipe: Recipe for efficient long context generalization in large language models. arXiv preprint arXiv:2406.12345. Yingsheng Wu, Yuxuan Gu, Xiaocheng Feng, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, and Bing Qin. 2024. Extending context window of large language models from distributional perspective. arXiv preprint arXiv:2410.01490. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. 2024. Gated linear attention transformers with hardware-efficient training. In International Conference on Machine Learning, pages 5650156523. PMLR. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document classification. In Proceedings of NAACL. Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, WeiYing Ma, Jingjing Liu, Mingxuan Wang, and 1 others. 2025a. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, and 1 others. 2025b. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Tianle Zhang, Zhuohan Li, and 1 others. 2024. Longalign: Instruction-tuning long-context llms. arXiv preprint arXiv:2401.10968. Wei Zhu, Ziheng Wang, and 1 others. 2024. Dataadaptive positional encoding for length generalization. NeurIPS."
        },
        {
            "title": "A Rendering Parameters",
            "content": "Our rendering parameters are detailed in Table 8. The best result, obtained through our LLM-driven genetic search, is presented in Fig. 6, which shows the detailed configuration and its corresponding rendering."
        },
        {
            "title": "Factor",
            "content": "dpi page_size font_family font_size alignment margins indent spacing h_scale colors borders Specification / Sampling Strategy Mixture of sets: lowest (4559), low (6071), medium (72119), normal ({72,80,96,100,110,120,144,150,300}), high (over 300); favor normal/medium with small prob. spikes to extremes. (i) Fixed paper sizes (A4, Letter, Legal, A5, B5, A3, B4, Tabloid) with priors; (ii) common aspect ratios (e.g., 1.414, 1.333, 1.5, 1.778); (iii) fully random aspect via piecewise distribution (narrow tall). Pooled and deduplicated families across serif/sans/mono/pixel; italics sampled by filename heuristics (suffixes, italic/oblique). {7, 7.5, 8, 9, 9.5, 10, 11, 12, 14}; line_height tied as font_size + {0, . . . , 3}. LEFT/JUSTIFY (dominant) with smallprob. RIGHT/CENTER. Three patterns: all-equal; verticallarger; horizontal-larger; values in 10 40pt ranges. Modes: none; first-line indent (1 2.5 em); block/hanging with left/right indents. space-before/space-after use multi-mode prior (none, small, large). Horizontal glyph scaling (0.751.0) with decaying probabilities. palettes Page/background/font for doculight/dark ment/web/code styles inherit coherent triplets (page, paragraph, font). Optional paragraph borders with width/padding; disabled by default. themes; newline_markup With small probability, explicit markers (e.g., n, tags, or tokens) inserted to preserve structure. Optional white-margin cropping and last-page trimming. auto_crop Table 8: Controllable factors in the rendering pipeline and their sampling strategies. The mixture design yields broad yet realistic typography/layout coverage and tunable compression ρ(θ)."
        },
        {
            "title": "B Implementation Details",
            "content": "Training Details For continual pre-training of the 9B long-context backbone, the model is initial11 Backbone Model. Our method rely on strong VLM to process long-context task. Considering the impressive performance of GLM-4.1v-9B, especially in OCR and long document tasks, we choose GLM-4.1V-9B-Base as our backbone model. Evaluation Benchmarks. To conduct comprehensive analysis of the long-context performance, we have adopt three popular benchmarks, including LongBench, MRCR and Ruler. LongBench consists of totally 21 datasets in 6 categories, covering deiverse long-context tasks. MRCR is task proposed by , we use the OpenAI version, which consists of multi-turn conversations about writing, asking models to recall one of the context in dialogue history. Ruler is widely-used synthetic benchmark with 11 NIAH tasks. To validate crossmodel benefits, we choose the MMLongBenchDoc, which involves 130 lengthy PDF with diverse layout and images, and 1062 questions. Efficiency Evaluation Setting. For training, we focus on SFT since RL involves rollouts time, making it difficult to compare fairly. Moreover, running RL at very long context lengths (e.g., 64k or beyond) requires excessive memory resources. This again highlights the advantage of our approach: through compression, we can conduct RL training at 32k context length while effectively covering over 100k tokens of raw text input, where RL is prohibitively difficult for LLMs due to memory and computation demands. For SFT, we measure per-sample training time under the same number of training data using 880G H100 GPUs. For inference, we deploy both models on single 80G H100 and measure efficiency along two axes: (i) prefill latency at batch size 1, and (ii) per-sample inference time at the maximum feasible batch size with output length set to 256 tokens. We omit the KV cache testing, because KV cache scales linearly with the sequence length, compression translates almost directly into savings of about 67% memory usage. Model GPT-4.1 2 Needle 0k-8k 8k-16k 16k-32k 32k-64k 64k-128k Avg 83 72 54.27 LLaMA-3.1-8B-Instruct 58.95 Qwen3-8B GLM-4-9B-Chat-1M 39.77 Qwen2.5-7B-Instruct-1M 45.92 53.21 41.18 15.87 51.07 Glyph 41.51 40.78 67 51.05 36.18 18.42 46.97 39.58 29.81 24.99 18.63 34.67 29.67 59 24.98 20.89 18.42 37.57 22.41 68. 42.66 36.44 22.22 43.24 34.85 Table 9: Performance of various models on the MRCR task (%) with the 2 Needle setting across different context length intervals (0k8k, 8k16k, 16k32k, 32k64k, 64k128k) and the average score. ized from the released GLM-4.1V-9B-Base checkpoint and trained on diverse mixture of rendered long-context data and vision-language corpora (e.g., OCR task) within 128k context length. The training uses global batch size of 170 and learning rate of 2e-6 with cosine decay for around 4000 steps. For the rendering search, we run for 5 times with 200 steps in each round, to find the optimal configuration that maximizes compression ratio while maintaining good performance. After this, we conduct further SFT and RL training. For SFT, we train for 1.5k steps with batch size of 32. The Adam optimizer (β1 = 0.9, β2 = 0.95) is used with cosine decay and 160 warm-up steps, where the learning rate decays from 5e-6 to 2e-6. For reinforcement learning, we adopt the GRPO algorithm. Each training group samples 16 candidate responses, and degenerate samples with all-zero or all-one rewards are discarded. We apply the clip-higher trick from DAPO (Yu et al., 2025b) with ϵl being 0.2 and ϵh being 0.28. Training runs for 500 iterations with batch size of 32. We also use the Adam optimizer with constant learning rate of 1e-6. Baselines. We compare Glyph with leading opensourced LLMs of similar size: Qwen3-8B achieves state-of-the-art performance across reasoning and wide range of tasks. Qwen2.5-7B-Instruct-1M excels at longcontext understanding and achieves strong performance across diverse benchmarks. LLaMA-3.1-8B-Instruct is widely used model with strong instruction-following and multilingual capabilities. GLM-4-9B-Chat-1M delivers powerful longcontext tasks and overall high performance."
        },
        {
            "title": "Model",
            "content": "GPT-4.1 Single-Doc QA Multi-Doc QA Summarization Few-shot"
        },
        {
            "title": "News VcSum Sam Lsht",
            "content": "63.90 51.27 55.63 24.58 23.70 14. 41.25 50."
        },
        {
            "title": "62.20\nLLaMA-3.1-8B-Instruct\n60.98\nQwen3-8B\n63.17\nGLM-4-9B-Chat-1M\nQwen2.5-7B-Instruct-1M 62.98",
            "content": "54.98 49.78 52.88 53.62 31.61 45.54 39.14 34.72 33.75 16.69 28.27 21.85 24.21 18.55 23.90 21.02 16.23 12.08 16.21 12.20 7.61 36.47 36.15 39. 0.00 42.00 47.38 28."
        },
        {
            "title": "Glyph",
            "content": "37.23 45.89 56.18 26.87 21.52 12. 32.49 44."
        },
        {
            "title": "Pa C",
            "content": "26.5 7.13 12.81 2.39 3.50 30.50 Table 10: The reset of results on LongBench benchmark (%), which encompasses Single-Document QA, MultiDocument QA, Summarization, Few-shot Learning, and Synthetic task. Figure 6: The optimal parameter setting. The left column lists the values for page layout, font, and spacing, while the right column provides an example of the rendered text."
        }
    ],
    "affiliations": [
        "The Conversational Artificial Intelligence (CoAI) Group, Tsinghua University",
        "The Knowledge Engineering Group (KEG), Tsinghua University",
        "Zhipu AI"
    ]
}