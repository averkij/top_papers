{
    "paper_title": "EXP-Bench: Can AI Conduct AI Research Experiments?",
    "authors": [
        "Patrick Tser Jern Kon",
        "Jiachen Liu",
        "Xinyi Zhu",
        "Qiuyi Ding",
        "Jingjia Peng",
        "Jiarong Xing",
        "Yibo Huang",
        "Yiming Qiu",
        "Jayanth Srinivasa",
        "Myungjin Lee",
        "Mosharaf Chowdhury",
        "Matei Zaharia",
        "Ang Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Automating AI research holds immense potential for accelerating scientific progress, yet current AI agents struggle with the complexities of rigorous, end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed to systematically evaluate AI agents on complete research experiments sourced from influential AI publications. Given a research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results. To enable the creation of such intricate and authentic tasks with high-fidelity, we design a semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers. Evaluations of leading LLM-based agents, such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial capabilities: while scores on individual experimental aspects such as design or implementation correctness occasionally reach 20-35%, the success rate for complete, executable experiments was a mere 0.5%. By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP-Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments. EXP-Bench is open-sourced at https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 5 8 7 4 2 . 5 0 5 2 : r EXP-Bench: Can AI Conduct AI Research Experiments? Patrick Tser Jern Kon1,, Jiachen Liu1,, Xinyi Zhu1, Qiuyi Ding1 Jingjia Peng1, Jiarong Xing1,2, Yibo Huang1, Yiming Qiu1,4, Jayanth Srinivasa3 Myungjin Lee3, Mosharaf Chowdhury1, Matei Zaharia4, Ang Chen1 Equal contribution 1University of Michigan 2Rice University 3Cisco Research 4UC Berkeley"
        },
        {
            "title": "Abstract",
            "content": "Automating AI research holds immense potential for accelerating scientific progress, yet current AI agents struggle with the complexities of rigorous, endto-end experimentation. We introduce EXP-Bench, novel benchmark designed to systematically evaluate AI agents on complete research experiments sourced from influential AI publications. Given research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results. To enable the creation of such intricate and authentic tasks with high-fidelity, we design semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers. Evaluations of leading AI agents, such as OpenHands [87] and IterativeAgent [76] on EXP-Bench demonstrate partial capabilities: while scores on individual experimental aspects such as design or implementation correctness reach 20-35%, the success rate for complete, executable experiments was mere 0.5%. By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXPBench serves as vital tool for future AI agents to improve their ability to conduct AI research experiments. EXP-Bench is open-sourced at https://github.com/ Just-Curieous/Curie/tree/main/benchmark/exp_bench."
        },
        {
            "title": "Introduction",
            "content": "Figure 1: EXP-Bench evaluates AI agents on research experiment tasks extracted semi-autonomously from peer-reviewed AI papers. Given research question, high-level method description, and starter code, agents are tasked with designing, implementing, and executing complete experiments. Performance is validated through ground-truth comparisons and implementation execution. Automating AI research stands as cornerstone for accelerating the development of advanced intelligence and human progress. Unlike disciplines that require extensive physical interaction, AI research is inherently digital, rendering it particularly amenable to automation by Large Language Preprint. Under review. Model (LLM)-driven AI agents. Recent work has demonstrated that these agents demonstrate nascent capabilities in tasks like literature synthesis [23], hypothesis generation [91] and code generation [53]. However, empirical AI research requires rigorous end-to-end experimentation, which goes beyond these individual tasks. To realize the vision of agents conducting holistic AI research, rigorous benchmark is neededone that evaluates and guides agents through the full experimentation pipeline step by step. We present EXP-Bench, benchmark designed to comprehensively assess an AI agents ability to carry out end-to-end research experiments. As illustrated in Fig. 1, EXP-Bench challenges agents with tasks sourced from influential, peer-reviewed AI publications (e.g., NeurIPS, ICLR) along with their open-source implementations. These papers reflect already-completed, peer-validated research and serve as concrete exemplars of full experimental workflows. By exposing agents to such tasks, we test their ability to conduct established scientific procedures grounded in real-world AI experimentation. For each task, an agent is provided with core research question, high-level methodological overview, and starter code. The agent should then formulate viable hypotheses, design AI-specific experimental procedures (e.g., data handling, model selection, and hyperparameter optimization), correctly implement and execute these experiments, and derive valid conclusions from the results. However, curating these high-fidelity and structured experimental tasks presents considerable challenges. Academic papers typically present polished narrative focusing on final results and conclusions, often omitting the detailed intermediate steps of the experimentation process. Additionally, critical detailssuch as the precise conditions under which results hold or subtle data preprocessing stepsare often fragmented across multiple sources, including dense academic papers, supplementary materials, and sprawling codebases. This necessitates deep domain expertise for accurate interpretation and makes manual curation of such tasks labor-intensive and difficult to scale. To address these challenges, we develop semi-automated dataset curation pipeline. We first filter for high-quality AI papers with open-source codebases using citation and repository popularity signals. Task extraction then proceeds in two stages: (1) multi-modal extraction phase that identifies the core elements of the research problemsuch as the main question, expected outcomes, and high-level experimental setup (e.g., datasets, evaluation metrics, model configurations)from papers, supplementary materials, and code; and (2) an implementation extraction phase that locates relevant code and assembles scripts to solve the specified task. We further apply execution-based validation to ensure functionality. While human oversight is used, the availability of original implementations and ground truths reduces the validation burden to mostly lightweight consistency checks. With the pipeline, EXP-Bench currently comprises 461 research tasks (12,737 individually gradable subtasks) derived from 51 papers published at NeurIPS and ICLR 2024, spanning diverse AI subfields such as reinforcement learning, AI applications and generative models. We use multi-metric evaluation pipeline  (Fig. 1)  to assess agent performance across all core phases of experimentationdesign, implementation, execution, and conclusion. Each metric captures distinct capability, and their conjunctive use ensures that agents correctly understand and complete the experiment. Initial evaluations of leading agents reveal that, while they often succeed at executing routine proceduressuch as running pre-written scripts or replicating documented analysis stepsthey struggle when tasked with conducting complex experiments. Specifically, we observe failures in: (a) Conceptualizing and operationalizing sound experimental designs from high-level research questions and methods (16.1% misclassified design variables); (b) Translating abstract research methodologies into complete and correct code implementations (39.7% missing essential implementation components); and (c) Ensuring the robust and reproducible execution of complex experimental software stacks (29.4% environment or dependency misconfigurations or 23.8% scriptlevel errors). By identifying these key bottlenecks, EXP-Bench helps us target specific research components for improvement and advance next-generation AI agents for autonomous research."
        },
        {
            "title": "2 Related work",
            "content": "While existing benchmarks have advanced the evaluation of AI agents in various scientific reasoning, coding, and specific machine learning tasks, EXP-Bench distinctively addresses the holistic challenge of end-to-end and step-by-step AI research experimentation. See App. for additional discussion. Scientific Reasoning Benchmarks. Benchmarks like BoxingGym [27] explore simulated theory formation, while others such as AAAR [60] and Lab-Bench [49] assess reasoning or experimental 2 Figure 2: One AI research task example from ICLR 2024 MogaNet [51]. design based on static artifacts (e.g., protocols, figures). While valuable for assessing abstract reasoning, these benchmarks do not evaluate the agents ability to perform actual experiments. Scientific Coding Benchmarks. Scicode [81], for instance, focuses on generating code snippets for natural science tasks, while BLADE [31], DiscoveryBench [64], and ScienceAgentBench [14] primarily assess post-hoc data analysis or hypothesis testing. While critical to the scientific process, they often isolate coding or analysis from the broader, iterative experimental context. Machine Learning Benchmarks. Several benchmarks specifically target machine learning (ML) tasks, yet often focus on sub-components or operate with simplifications of the full research cycle. For example, DSBench [45], ML-Agent-Bench [39], and MLE-Bench [12] assess ML problemsolving capabilities, such as script editing or hyperparameter tuning, frequently within constrained environments like Kaggle challenges. Other benchmarks such as RE-Bench [92], ML-Gym [69], and Curie [47], compare agent performance against humans on research tasks, but often operate at limited scale (e.g., RE-Bench features only 7 hand-curated tasks) or use simplified evaluation metrics. PaperBench [76] assesses agents on tasks derived from academic literature, focusing on their proficiency in executing specific, well-defined sub-components of the research process, such as running documented code scripts or performing standard data analyses. While these benchmarks provide valuable insights into specific ML tasks, they generally fail to capture the complexity of realistic end-to-end AI research workflows, nor do they typically offer methodology for constructing such comprehensive benchmark tasks at scale."
        },
        {
            "title": "3 The EXP-Bench Benchmark and Dataset",
            "content": "EXP-Bench is built to evaluate the AI agents ability to address AI research tasks by conducting end-to-end experimentation. Each research task is grounded on an influential AI research paper and its corresponding codebase. This coupling captures the full scientific workflowslinking concrete high-level ideas to executable implementations (3.1). We achieve scalable construction of these highfidelity tasks through semi-automated curation pipeline, which integrates multi-modal extraction with lightweight human verification (3.2). This design also opens the door to large-scale data generation for training agents capable of automating core aspects of AI research."
        },
        {
            "title": "3.1 EXP-Bench Dataset Specification",
            "content": "Our dataset is collection of AI research tasks, each structured to emulate complete experimental process designed to address specific AI research question from published paper. As shown in Fig. 2, each task entry in the dataset contains problem statement for the agent, and the corresponding ground-truth solution derived from the original research artifacts. Problem Statement (Agent Input). Each task instance within EXP-Bench provides the agent with: (1) Research Question: specific goal derived from the source papers experiments. (2) High-Level Method: description guiding the required experimental approach; and (3) Code Repository: Access to the relevant code, potentially with specific components/scripts masked or requiring modification. 3 Figure 3: EXP-Benchs dataset comprises tasks from diverse set of ML research categories. Expected Outcome (Ground Truth). Each task instance also includes ground-truth experimental solution curated from the source paper and codebase. This solutionused to evaluate agent outputscomprises: (1) an experimental design specifying key variables, constants, and procedures; (2) the necessary code modifications, assessed via git diff against the provided repository; and (3) final conclusion that directly answers the research question based on experimental results. Benchmark Overview and Statistics: EXP-Bench currently includes 461 research tasks drawn from 51 influential papers, as detailed in App. B. As shown in Fig. 3, these tasks span diverse AI subfieldsincluding Computer Vision, NLP, and Reinforcement Learningand are sourced from top-tier venues, namely NeurIPS 2024 (53%) and ICLR 2024 (47%). This breadth ensures coverage of diverse experimental paradigms, coding practices, and research challenges prevalent in the AI field. Moreover, each task is broken down into fine-grained, individually gradable subtasks spanning all three ground-truth componentsdesign, implementation, and conclusionresulting in total of 12,737 subtasks. Together, these features make EXP-Bench comprehensive testbed for assessing the capabilities of AI research agents."
        },
        {
            "title": "3.2 EXP-Bench Semi-Automated Dataset Construction Pipeline",
            "content": "Curating high-fidelity benchmark for end-to-end AI experimentation is challenging due to the fragmented and domain-specific nature of real-world research artifactsnamely papers and their associated codebases. Critical experimental details are often scattered, implicit, or embedded in dense technical language, making manual extraction labor-intensive and difficult to scale. To address this, we propose semi-automated construction pipeline that systematically structures these artifacts into benchmark tasks with lightweight human oversight. The pipeline comprises three stages  (Fig. 4)  : Stage 1: Source Selection and Filtering. The process begins by identifying candidate research artifacts that form the basis of high-quality experimental tasks. We target influential papers from top-tier AI conferences (e.g., NeurIPS, ICLR) that are accompanied by publicly available code repositories. Initial filtering criteria are applied to prioritize impactful and potentially reproducible research, considering factors such as citation counts, and code repository activity (e.g., GitHub stars, forks). This selection phase aims to establish strong foundation by focusing on artifacts that, despite potential imperfections, represent significant and verifiable research contributions. Stage 2: Experiment Procedure Extraction. Research papers rarely present experiments as complete procedureskey steps are often implicit or scattered. To enable structured agent evaluation, we decompose each task into explicit sub-steps. This transforms high-level research goals into concrete workflowse.g., multi-step experiment design and environment setupmaking them suitable for both execution and fine-grained evaluation. This stage extracts the complete research task by combining the research plan (from the paper) with its corresponding experiment implementation (from the codebase). Further implementation details can be found in App. F. Stage 2.1: Extract Research Task. We begin by extracting the core research taskconsisting of the research question, high-level methodology, and expected outcomedirectly from the paper. This 4 Figure 4: EXP-Bench semi-automated dataset construction pipeline. process is designed to handle the fact that key information in academic papers is often distributed across sections and conveyed implicitly. First, we index the PDF using combination of OCR (Optical Character Recognition) and multimodal extraction techniques to capture structured elements like tables, figures, and headers. This ensures downstream access to high-signal artifacts that may anchor the task definition. Next, we conduct multi-pass extraction. In the first pass, we perform retrieval-augmented querying to identify broad, high-level research takeaways. These overarching questions are often not confined to single paragraph and require stitching together dispersed cues. In the second pass, for each high-level takeaway, we apply semantic extraction at the subsection level, focusing on evaluation sections. We classify each subsection as either implementation context or candidate research question. Contextual passages are stored and reused across subsequent prompts. This focused promptingprocessing each subsection independently while conditioning on accumulated context and extracted tables/figureshelps the LLM generate more accurate and detailed task formulations. Finally, we refine each task through targeted re-querying of the full paper (including appendices) to recover any additional setup constraints or methodological details that were missed earlier. This step acknowledges that relevant setup details may be located far from the task description and ensures completeness for the extracted task. Stage 2.2: Extract Experiment Implementation. Each extracted task is then passed to an implementation extraction AI agent (operating in tool-augmented environmentwith PDF reading, terminal access, and web browsing) to identify the specific implementation (chain of scripts) needed to address the research task. Our setting provides the agent with both complete codebase and the extracted taskcontaining the research question, methodology, and expected outcome. This effectively reduces the problem to goal-conditioned search over the codebase, where the agents task is to localise the implementation that realises the specified methodology and expected outcome. To do this, the agent explores the repository in an open-ended fashione.g., consulting documentation, and auxiliary scripts, to uncover domain-specific requirements (e.g., pretrained checkpoints). The extracted experiment execution ground truth will be fully based on existing scripts. The agent outputs (1) list of required scripts and (2) high-level usage instructions describing how to run them to complete the task. Once candidate implementation is produced, it is executed in Stage 3. If the run fails, the pipeline iteratesallowing the agent to refine or replace the implementation until working solution is found. The final validated script chain is then parsed by the agent via AST (Abstract Syntax Tree) tracing to extract step-by-step list of implementation requirements in natural language, which becomes the ground truth for evaluating implementation correctness. Finally, we incorporate additional contextual details (e.g., hyperparameters) sourced from the raw code (e.g., configuration files) or repository documents (e.g., README.md) to enhance the final task specification. Stage 3: Verification and Refinement. All tasks are validated and finalized in this stage. For paper-derived tasks with corresponding implementations, the associated scripts are executed in clean, containerized environment. Execution traces are then checked against expected outputs from the original paper. If validation fails, the task is returned to the previous stage for refinement. For tasks lacking matched implementation, we perform manual validation to ensure the extracted 5 Table 1: Average benchmark scores for various models when tested against various evaluation metrics. Popular Agents and LLMs perform poorly on EXP-Bench, showcasing its difficulty. Agent OpenHands OpenHands OpenHands OpenHands OpenHands IterativeAgent Claude-3.5 Haiku IterativeAgent Amazon Nova Pro Model o3-mini Claude-3.7 Sonnet Amazon Nova Pro Claude-3.5 Haiku DeepSeek R1 18.4 16.0 18.2 20.6 6.8 6.4 0.1 20.3 35.0 19.5 26.2 10.0 20.6 10.0 15.0 33.2 26.8 9.3 0.7 25.2 18.1 IE 2.9 14.9 0.0 1.3 0.0 5.4 0.0 21.0 13.4 15.7 13.8 2.4 2.2 0. All AllE #E 420 0.5 1.4 235 0.4 0.7 56 0.0 0.0 237 0.0 0.0 140 0.0 0.0 111 0.0 0.0 215 0.0 0.0 research objective faithfully aligns with the source paper. In all cases, lightweight human review finalizes the task, requiring only cross-check of structured task contentalready consolidated by the pipelineagainst the source materials. This significantly reduces human burden compared to manual curation from scratch. Following validation, each complete task is added to the dataset along with list of masked files (e.g., README.md, relevant scripts) to ensure agents cannot directly access answers. In our benchmark implementation, repositories are cloned afresh per agent, and masking is applied using scripted git operations, including recursive traversal of submodules. Masking ensures agents must reason over the task input, rather than rely on shortcut access to original solutions."
        },
        {
            "title": "4.1 Evaluating LLM-based Agents Performance on EXP-Bench: Setup & Main Results",
            "content": "Setup. We evaluate range of agents and LLMs used in related benchmarks [13] against EXPBench. In terms of agents, we made use of OpenHands (a top-performing code generation agent) and IterativeAgent (as configured in [76] to reduce the likelihood of early task stopping), henceforth known as OH and IA, respectively. In terms of LLMs, these include the top-ranked Claude-Sonnet 3.7, Haiku 3.5, Deepseek-R1 [90] models, and OpenAI o3-mini variants. Each agent is run in an Ubuntu 24.04 Docker container, and given access to 4 Nvidia A40 GPU, and clean working directory containing the masked GitHub repo of the paper (i.e., task-specific scripts removed), instructions, and relevant context (e.g., API credentials). Evaluation Judge Implementation Details. Our evaluation framework consists of two main components used to assess agent performance across various metrics (refer to later sections, e.g., Table 1). The first component is an LLM-based judge (using o3-mini), following prior work on LLM-as-a-judge [111, 56, 1, 76]. This judge operates through multiple steps: The process begins with an integrity check performed by Monitor, which analyzes agent logs to detect disallowed behaviors (denoted as metric M; see Fig. 6b). Specifically, the monitor checks whether the agent: (1) accessed the research paper directly (e.g., opened the PDF), (2) performed Git operations such as checking out commits or switching branches, or (3) used fake, hardcoded, or placeholder data rather than generating results through real experimentation. If violations are found, the monitor also identifies possible causes (e.g., ethical refusals, runtime errors) using log information. Once integrity is established, the agents experimental design, implementation, and conclusion are evaluated for conceptual soundness, completeness (e.g., inclusion of all required steps), and alignment with ground truth. These assessments yield scores for: (design correctness, i.e., proportion of design criteria met), (implementation correctness, i.e., proportion of implementation components satisfied), and (conclusion correctness). The second component of our evaluation judge is Code Execution Validator, which runs the agent-generated code modifications in clean and equivalent containerized environment. This step verifies whether the code is executable and produces expected outputs. This executability metric is denoted as E. Implementation details including the system prompt are in App. H. Main Results. Table 1 presents average accuracy scores across all 461 tasks. IE indicates whether an implementation is both appropriate for the experimental task and executablea more comprehensive check of implementation quality. All denotes tasks that are fully correct in terms of D, I, and C, while AllE adds the executability requirement. #E represents the number of tasks per model that were execution-checked. Due to the time-consuming nature of execution, only subset of traces were 6 evaluatedexcluding those that failed the monitor check, which were automatically discarded prior to execution. Our top-ranked agents are OH+o3-mini, OH+3.7 Sonnet, and OH+Nova Pro, ranked via AllE, with used as tiebreaker. The worst-performing model was IA+Nova Pro. Extended results by paper category are shown in Table 3, with full details in App. D. Across both tables, we observe that models consistently score below 30% across all metrics, with the exception of the RL category, where several OH models achieve up to 41% (averaged over 36 tasks) in terms of I. Notably, under stricter metrics such as All, performance drops sharplye.g., OH+o3-mini scores only 1.4%. This underscores the value of including partial metrics that assess individual aspects, allowing credit for partially correct answers and supporting more nuanced evaluation."
        },
        {
            "title": "4.2 Detailed Analysis",
            "content": "Cost-Time Analysis. Fig. 6a shows the average cost (in USD) and time (in minutes) per task across different agent configurations. Cost reflects only the token usage of the backbone LLM (input/output), excluding agent internal LLM-API usage or compute consumption. The number in parentheses next to each legend entry indicates the models performance rank, based on average correctness. Each agent was allowed maximum of 40 minutes per task, though this limit can be easily adjusted. Notably, IA models often consumed the full allotted time, rarely stopping early. In contrast, early stopping was common with OH models. For example, the relative time difference between Nova and Haiku is larger under OH than IA, reflecting differing usage patterns. These trends are consistent with our earlier observations: OH models often produced plausible responses without actually running the experiment, leading to high partial scores (e.g., design, implementation), while IA models tended to run longer but less effectively. Interestingly, we found little correlation between runtime/cost and overall performance. OH+o3-mini (rank 1) achieves the best trade-off with low cost and moderate time. OH+3.7 Sonnet (rank 2) performs well but is the slowest and most expensive. The full costtime distribution is provided in App. I.2. Conjunctive Evaluation Metrics Substantially Lower Agent Scores. We analyze only the subset of tasks for which execution was run, to visualize how progressively applying stricter evaluation criteria impacts agent scores. As shown in Fig. 6b (with full results in App. I.1), applying only the initial monitoring check (M) yields an average score of 20.6%. Adding design (D) and conclusion (C) correctness criteria reduces the score sharply to 3.7%. Incorporating implementation correctness (I) further lowers the score to 0.4%, and including execution verification (E) results in final accuracy of just 0.2%. These findings highlight how conjunctive evaluation surfaces brittleness in end-to-end experimental correctness. Metric Stability Analysis. As shown in Fig. 5, certain individual metrics such as and exhibit high variance. This variance arises for different reasons: for C, agents can produce plausible but unfounded conclusions without valid experimental foundation; for E, even incorrect or mock implementations may successfully execute, introducing overestimation bias. To mitigate such inconsistencies, we adopt compositional scoring via conjunctive metrics such as CD and IE, which combine correctness across multiple dimensions. These conjunctive forms substantially reduce score variability, producing more reliable signals of agent performance. For example, CD filters out conclusions not grounded in valid design plans, and IE discounts executions that do not fulfill setup requirements. This demonstrates that conjunctive metrics can temper over-crediting and reduce sensitivity to annotation leniency or spurious correctnessthereby offering more stable and discriminative evaluation."
        },
        {
            "title": "4.3 Analysis on Prevalent Agent Failure Patterns",
            "content": "Figure 5: Stability Analysis. Pattern Extraction Methodology. Our analysis followed two-pass, open-ended process. During evaluation, each metric score was accompanied by an error analysis, derived from implementation logs (e.g., stderr) or comparisons against ground truth. In the first pass, we extracted high-level, 7 (a) Costtime trade-offs across agents. (b) Stricter metrics reveal lower true correctness. Figure 6: Ablation of agent performance along costtime and evaluation metrics. domain-specific insights from these earlier error analyses, across phases for all agent-task pairs. In the second pass, we iteratively grouped these insights into distinct failure typesassigning each to an existing category or creating new one if needed. This process produced 3,238 raw insights, which we distilled into 361 unique failure types. We present representative and simplified subset of these condensed errors in Table 2 (full details can be found in App. G). Analysis. To better understand where agents fail, we analyzed error traces and categorized them into representative failure types across four key phases of experimentation: implementation, execution, design, and conclusion. As shown in Table 2, the most prevalent issues emerged during the implementation phase, with 39.71% of failures stemming from missing essential components. In several cases, agents failed to include critical elements such as semantic retrieval strategies (e.g., UniXcoder-H2L and UniXcoder-L2H), validation functions for filtering questions (e.g., using GPT3.5), or robustness-enhancing techniques like Mixup, CutMix, and Label Smoothingundermining the experimental implementations validity. Incomplete data preprocessing (1.83%) was another notable implementation issue, with agents omitting required dataset loading and transformation steps, such as ETTh1 series preparation, ACF plotting, or normalization procedures (e.g., RevIN), instead providing only boilerplate config files. In the execution phase, failures were most commonly due to environment or dependency misconfigurations (29.38%), such as missing critical environments (e.g., STORM not registered in jaxmarl) or absent core libraries like PyTorch and Flax, which led to model loading failures. Script-level issues (23.84%) included unrecognized model names (e.g., moganet_tiny not found in timm) and missing checkpoint files, causing runtime or I/O errors. These examples highlight persistent reproducibility challenges even when correct implementation structure is in place. Design-related failures were also frequent, with 16.05% involving incomplete or misclassified experimental variables, and 7.62% reflecting extraneous procedural additionssuch as inclusion of ResNet-50 backbone or arbitrary hyperparameter knobs not specified in the ground truth. These design errors suggest that agents often fail to distinguish between essential experimental factors and implementation noise. Finally, conclusion-phase errors highlight limitations in agents interpretive reasoning. The most common issue (26.18%) was missing or underdeveloped conclusionsfor instance, omitting detailed comparisons between PPO and Q-Learning on training time and normalized scores, or neglecting specific numerical gains (e.g., 1.25% improvements across ARC-Challenge and OpenBookQA). Another frequent error (19.66%) was incorrect interpretation, such as claiming Hadamard-enhanced INT4 inference improves performance without substantiating comparisons to baseline INT4. Together, these findings emphasize the importance of phase-specific evaluation and illustrate how surface-level plausibility can mask deeper breakdowns in experimental reasoning and reproducibility."
        },
        {
            "title": "5 Discussion",
            "content": "Limitations. EXP-Bench primarily focuses on the experimentation procedure from designing experiments for given research question to deriving conclusions. The broader AI research lifecycle encompasses other critical stages such as identifying gaps through literature review, the initial unstructured ideation of research questions, and navigating the complex, iterative, and unpredictable path of real-world scientific discovery, which are not yet fully captured by the current task structures. 8 Table 2: Agents fail in diverse ways across different phases of experimentation; this table presents simplified subset of common examples, measured across all agent and model evaluations."
        },
        {
            "title": "Failure Type\nIncomplete or Misclassified Design Variables\nIrrelevant Procedural Additions in Design",
            "content": "Implementation Missing Essential implementation Components Incomplete Evaluation Metric Implementation Implementation Incomplete Data and Preprocessing Setup Implementation Environment/Dependency Configuration Errors Execution Execution Script and File Errors Execution Execution Missing Setup Script File Tensor Operation Execution Error Execution Conclusion Missing Conclusion Content Conclusion Incorrect Conclusion Interpretation Extraneous Details in Conclusion Conclusion Incorrect Numeric Conclusion Conclusion Prevalence (%) 16.05 7.62 39.71 2.15 1.83 29.38 23.84 6.95 3.22 26.18 19.66 7.77 3.21 Table 3: Average benchmark scores of various models and agents across select task categories; see Supp. for complete list. Evaluation performed against EXP-Bench."
        },
        {
            "title": "Agent\nOH\nOH\nOH\nOH\nIA\nIA\nOH\nOH\nOH\nOH\nIA\nOH\nIA\nOH",
            "content": "Model Nova Pro o3-mini 3.5 Haiku 3.7 Sonnet Nova Pro 3.5 Haiku DeepSeek R1 3.7 Sonnet o3-mini 3.5 Haiku 3.5 Haiku DeepSeek R1 Nova Pro Nova Pro 19.2 9.0 19.2 9.0 0.0 0.0 3.1 18.3 23.5 27.7 3.3 5.0 0.0 17.9 23.9 8.0 24.5 26.8 9.8 18.0 4.3 48.2 34.8 41.4 27.5 10.3 8.6 28.9 19.0 0.0 8.3 30.8 5.0 0.0 0.0 27.3 15.7 11.5 17.4 0.0 9.7 0.0 IE 0.0 0.0 5.6 7.7 0.0 0.0 0.0 21.2 2.0 0.0 0.0 0.0 0.0 0.0 13.9 8.3 8.3 8.3 0.0 0.0 0.0 17.6 27.5 17.6 2.4 2.0 0.0 13. All AllE 0.0 0.0 0.0 2.8 0.0 0.0 0.0 2.0 3.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.0 0.0 0.0 0.0 0.0 0.0 0.0 Future directions. Future work will focus on enhancing AI agents ability to automate research experimentation using supervision from EXP-Benchs dataset. One promising direction is to apply reinforcement learning with verifiable rewards, enabling agents to autonomously navigate the research lifecycle and accelerate scientific discovery."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced EXP-Bench, novel benchmark designed to rigorously evaluate and guide the development of AI agents in conducting end-to-end AI research experimentation. By sourcing tasks from influential peer-reviewed publications and their accompanying codebases, and utilizing semi-automated curation pipeline, EXP-Bench presents agents with realistic, fine-grained challenges in end-to-end AI research workflow including experimental design, implementation, execution, and conclusion derivation. Our initial evaluations with leading agents reveal significant bottlenecks in conceptualizing complex experiments and ensuring robust code implementation and execution. EXP-Bench therefore serves not only as comprehensive evaluation tool but also as valuable dataset to guide future AI agents to act step by step, ultimately accelerating AI research."
        },
        {
            "title": "References",
            "content": "[1] Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys. org/blog/2023-03-30-vicuna/. [2] F. Alet, J. Lopez-Contreras, J. Koppel, M. Nye, A. Solar-Lezama, T. Lozano-Perez, L. Kaelbling, and J. Tenenbaum. large-scale benchmark for few-shot program induction and synthesis. In International Conference on Machine Learning, pages 175186. PMLR, 2021. [3] S. Ashkboos, A. Mohtashami, M. L. Croci, B. Li, P. Cameron, M. Jaggi, D. Alistarh, T. Hoefler, and J. Hensman. Quarot: Outlier-free 4-bit inference in rotated llms, 2024. [4] J. Baek, S. K. Jauhar, S. Cucerzan, and S. J. Hwang. Researchagent: Iterative research idea generation over scientific literature with large language models. arXiv preprint arXiv:2404.07738, 2024. [5] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung, Q. V. Do, Y. Xu, and P. Fung. multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity, 2023. [6] L. Berglund, M. Tong, M. Kaufmann, M. Balesni, A. C. Stickland, T. Korbak, and O. Evans. The reversal curse: Llms trained on \"\"a is b\"\" fail to learn \"\"b is a\"\", 2024. [7] M. Bettini, A. Prorok, and V. Moens. Benchmarl: Benchmarking multi-agent reinforcement learning, 2024. [8] J. Blasiok and P. Nakkiran. Smooth ece: Principled reliability diagrams via kernel smoothing, 2023. [9] D. A. Boiko, R. MacKnight, B. Kline, and G. Gomes. Autonomous chemical research with large language models. Nature, 624(7992):570578, 2023. [10] L. Boisvert, M. Thakkar, M. Gasse, M. Caccia, T. L. S. D. Chezelles, Q. Cappart, N. Chapados, A. Lacoste, and A. Drouin. Workarena++: Towards compositional planning and reasoning-based common knowledge work tasks, 2025. [11] A. Bou, M. Bettini, S. Dittert, V. Kumar, S. Sodhani, X. Yang, G. D. Fabritiis, and V. Moens. Torchrl: data-driven decision-making library for pytorch, 2023. [12] J. S. Chan, N. Chowdhury, O. Jaffe, J. Aung, D. Sherburn, E. Mays, G. Starace, K. Liu, L. Maksin, T. Patwardhan, L. Weng, and A. adry. Mle-bench: Evaluating machine learning agents on machine learning engineering, 2024. [13] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [14] Z. Chen, S. Chen, Y. Ning, Q. Zhang, B. Wang, B. Yu, Y. Li, Z. Liao, C. Wei, Z. Lu, et al. Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. arXiv preprint arXiv:2410.05080, 2024. [15] C.-A. Cheng, A. Nie, and A. Swaminathan. Trace is the next autodiff: Generative optimization with rich feedback, execution traces, and llms, 2024. [16] P. Cheng, T. Hu, H. Xu, Z. Zhang, Z. Yuan, Y. Dai, L. Han, N. Du, and X. Li. Self-playing adversarial language game enhances llm reasoning, 2025. [17] A. Chevalier, J. Geng, A. Wettig, H. Chen, S. Mizera, T. Annala, M. J. Aragon, A. R. Fanlo, S. Frieder, S. Machado, A. Prabhakar, E. Thieu, J. T. Wang, Z. Wang, X. Wu, M. Xia, W. Xia, J. Yu, J.-J. Zhu, Z. J. Ren, S. Arora, and D. Chen. Language models as science tutors, 2024. [18] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. [19] T. M. U. Collaboration, J. Audenaert, M. Bowles, B. M. Boyd, D. Chemaly, B. Cherinka, I. Ciuca, M. Cranmer, A. Do, M. Grayling, E. E. Hayes, T. Hehir, S. Ho, M. Huertas-Company, K. G. Iyer, M. Jablonska, F. Lanusse, H. W. Leung, K. Mandel, J. R. Martínez-Galarza, P. Melchior, L. Meyer, L. H. Parker, H. Qu, J. Shen, M. J. Smith, C. Stone, M. Walmsley, and J. F. Wu. The multimodal universe: Enabling large-scale machine learning with 100tb of astronomical scientific data, 2024. [20] J. Dai, X. Pan, R. Sun, J. Ji, X. Xu, M. Liu, Y. Wang, and Y. Yang. Safe rlhf: Safe reinforcement learning from human feedback, 2023. 10 [21] T. Dai, B. Wu, P. Liu, N. Li, J. Bao, Y. Jiang, and S.-T. Xia. Periodicity decoupling framework for long-term series forecasting. In The Twelfth International Conference on Learning Representations, 2024. [22] Y. Du, F. Bai, T. Huang, and B. Zhao. Segvol: Universal and interactive volumetric medical image segmentation, 2025. [23] Elicit. Elicit: Analyze research papers at superhuman speed, 2025. Accessed: 2025-05-12. [24] M. Elrefaie, F. Morar, A. Dai, and F. Ahmed. Drivaernet++: large-scale multimodal car dataset with computational fluid dynamics simulations and deep learning benchmarks, 2025. [25] Y. Fang, N. Zhang, Z. Chen, L. Guo, X. Fan, and H. Chen. Domain-agnostic molecular generation with chemical feedback, 2024. [26] S. Frieder, L. Pinchetti, , R.-R. Griffiths, T. Salvatori, T. Lukasiewicz, P. Petersen, and J. Berner. Mathematical capabilities of chatgpt. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 2769927744. Curran Associates, Inc., 2023. [27] K. Gandhi, M. Y. Li, L. Goodyear, L. Li, A. Bhaskar, M. Zaman, and N. D. Goodman. Boxinggym: Benchmarking progress in automated experimental design and model discovery, 2025. [28] G. Gendron, Q. Bao, M. Witbrock, and G. Dobbie. Large language models are not strong abstract reasoners. arXiv preprint arXiv:2305.19555, 2023. [29] A. Ghafarollahi and M. J. Buehler. Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning, 2024. [30] A. Grosnit, A. Maraval, J. Doran, G. Paolo, A. Thomas, R. S. H. N. Beevi, J. Gonzalez, K. Khandelwal, I. Iacobacci, A. Benechehab, et al. Large language models orchestrating structured reasoning achieve kaggle grandmaster level. arXiv preprint arXiv:2411.03562, 2024. [31] K. Gu, R. Shang, R. Jiang, K. Kuang, R.-J. Lin, D. Lyu, Y. Mao, Y. Pan, T. Wu, J. Yu, et al. Blade: Benchmarking language model agents for data-driven science. arXiv preprint arXiv:2408.09667, 2024. [32] S. Guo, C. Deng, Y. Wen, H. Chen, Y. Chang, and J. Wang. Ds-agent: Automated data science by empowering large language models with case-based reasoning. arXiv preprint arXiv:2402.17453, 2024. [33] Y. Guo, C. Yang, A. Rao, Z. Liang, Y. Wang, Y. Qiao, M. Agrawala, D. Lin, and B. Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning, 2024. [34] S. J. Han, K. J. Ransom, A. Perfors, and C. Kemp. Inductive reasoning in humans and large language models. Cognitive Systems Research, 83:101155, 2024. [35] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding, 2021. [36] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. [37] X. Hu, Z. Zhao, S. Wei, Z. Chai, Q. Ma, G. Wang, X. Wang, J. Su, J. Xu, M. Zhu, et al. Infiagent-dabench: Evaluating agents on data analysis tasks. arXiv preprint arXiv:2401.05507, 2024. [38] J.-t. Huang, W. Wang, E. J. Li, M. H. Lam, S. Ren, Y. Yuan, W. Jiao, Z. Tu, and M. Lyu. On the humanity In The Twelfth International of conversational ai: Evaluating the psychological portrayal of llms. Conference on Learning Representations, 2023. [39] Q. Huang, J. Vora, P. Liang, and J. Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation, 2023. [40] Q. Huang, J. Vora, P. Liang, and J. Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation, 2024. [41] Z. Huang, Q. Ye, B. Kang, J. Feng, and H. Fan. Classification done right for vision-language pre-training, 2024. [42] T. Ifargan, L. Hafner, M. Kern, O. Alcalay, and R. Kishony. Autonomous llm-driven researchfrom data to human-verifiable research papers. NEJM AI, 2(1):AIoa2400555, 2025. 11 [43] G. Jaume, P. Doucet, A. H. Song, M. Y. Lu, C. Almagro-Pérez, S. J. Wagner, A. J. Vaidya, R. J. Chen, D. F. K. Williamson, A. Kim, and F. Mahmood. Hest-1k: dataset for spatial transcriptomics and histology image analysis, 2024. [44] B. Jing, H. Stärk, T. Jaakkola, and B. Berger. Generative modeling of molecular dynamics trajectories, 2024. [45] L. Jing, Z. Huang, X. Wang, W. Yao, W. Yu, K. Ma, H. Zhang, X. Du, and D. Yu. Dsbench: How far are data science agents from becoming data science experts?, 2024. [46] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583589, 2021. [47] P. T. J. Kon, J. Liu, Q. Ding, Y. Qiu, Z. Yang, Y. Huang, J. Srinivasa, M. Lee, M. Chowdhury, and A. Chen. Curie: Toward rigorous and automated scientific experimentation with ai agents, 2025. [48] J. Lála, O. ODonoghue, A. Shtedritski, S. Cox, S. G. Rodriques, and A. D. White. Paperqa: Retrievalaugmented generative agent for scientific research. arXiv preprint arXiv:2312.07559, 2023. [49] J. M. Laurent, J. D. Janizek, N. Thakkar, M. Ruzo, M. S. Yao, M. S. Levine, S. G. Rodriques, and A. White. Lab-bench: Measuring capabilities of language models for biology research, 2024. [50] R. Li, T. Patel, Q. Wang, and X. Du. Mlr-copilot: Autonomous machine learning research based on large language models agents. arXiv preprint arXiv:2408.14033, 2024. [51] S. Li, Z. Wang, Z. Liu, C. Tan, H. Lin, D. Wu, Z. Chen, J. Zheng, and S. Z. Li. Moganet: Multi-order gated aggregation network, 2024. [52] X. Li, Z. Huang, F. Xue, and Y. Zhou. Musc: Zero-shot industrial anomaly classification and segmentation with mutual scoring of the unlabeled images, 2024. [53] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. [54] S. Lin, W. Lin, X. Hu, W. Wu, R. Mo, and H. Zhong. Cyclenet: Enhancing time series forecasting through modeling periodic patterns, 2024. [55] H. Liu, L. Chen, Y. Qiao, C. Lv, and H. Li. Reasoning multi-agent behavioral topology for interactive autonomous driving, 2024. [56] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [57] T. Liu, C. Xu, and J. McAuley. Repobench: Benchmarking repository-level code auto-completion systems, 2023. [58] X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, S. Zhang, X. Deng, A. Zeng, Z. Du, C. Zhang, S. Shen, T. Zhang, Y. Su, H. Sun, M. Huang, Y. Dong, and J. Tang. Agentbench: Evaluating llms as agents, 2023. [59] X. Liu, C. Zhou, and S. Huang. 3dgs-enhancer: Enhancing unbounded 3d gaussian splatting with view-consistent 2d diffusion priors, 2024. [60] R. Lou, H. Xu, S. Wang, J. Du, R. Kamoi, X. Lu, J. Xie, Y. Sun, Y. Zhang, J. J. Ahn, H. Fang, Z. Zou, W. Ma, X. Li, K. Zhang, C. Xia, L. Huang, and W. Yin. Aaar-1.0: Assessing ais potential to assist research, 2024. [61] C. Lu, C. Lu, R. T. Lange, J. Foerster, J. Clune, and D. Ha. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024. [62] Y. Lu, Y. Hu, Y. Zhong, D. Wang, Y. Wang, and S. Chen. An extensible framework for open heterogeneous collaborative perception, 2024. [63] Q. Luo, H. Yu, and X. Li. Badam: memory efficient full parameter optimization method for large language models, 2024. 12 [64] B. P. Majumder, H. Surana, D. Agarwal, B. D. Mishra, A. Meena, A. Prakhar, T. Vora, T. Khot, A. Sabharwal, and P. Clark. Discoverybench: Towards data-driven discovery with large language models. arXiv preprint arXiv:2407.01725, 2024. [65] S. Mirchandani, F. Xia, P. Florence, B. Ichter, D. Driess, M. G. Arenas, K. Rao, D. Sadigh, and A. Zeng. Large language models as general pattern machines. arXiv preprint arXiv:2307.04721, 2023. [66] L. Mitchener, J. M. Laurent, B. Tenmann, S. Narayanan, G. P. Wellawatte, A. White, L. Sani, and S. G. Rodriques. Bixbench: comprehensive benchmark for llm-based agents in computational biology. arXiv preprint arXiv:2503.00096, 2025. [67] A. Moskvichev, V. V. Odouard, and M. Mitchell. The conceptarc benchmark: Evaluating understanding and generalization in the arc domain. arXiv preprint arXiv:2305.07141, 2023. [68] S. Narayanan, J. D. Braza, R.-R. Griffiths, M. Ponnapati, A. Bou, J. Laurent, O. Kabeli, G. Wellawatte, S. Cox, S. G. Rodriques, et al. Aviary: training language agents on challenging scientific tasks. arXiv preprint arXiv:2412.21154, 2024. [69] D. Nathani, L. Madaan, N. Roberts, N. Bashlykov, A. Menon, V. Moens, A. Budhiraja, D. Magka, V. Vorotilov, G. Chaurasia, D. Hupkes, R. S. Cabral, T. Shavrina, J. Foerster, Y. Bachrach, W. Y. Wang, and R. Raileanu. Mlgym: new framework and benchmark for advancing ai research agents, 2025. [70] R. Ohana, M. McCabe, L. Meyer, R. Morel, F. J. Agocs, M. Beneitez, M. Berger, B. Burkhart, K. Burns, S. B. Dalziel, D. B. Fielding, D. Fortunato, J. A. Goldberg, K. Hirashima, Y.-F. Jiang, R. R. Kerswell, S. Maddu, J. Miller, P. Mukhopadhyay, S. S. Nixon, J. Shen, R. Watteaux, B. R.-S. Blancard, F. Rozet, L. H. Parker, M. Cranmer, and S. Ho. The well: large-scale collection of diverse physics simulations for machine learning, 2025. [71] P. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble (almost) pipeline parallelism."
        },
        {
            "title": "In The Twelfth",
            "content": "International Conference on Learning Representations, 2024. [72] S. Qi, S. Chen, Y. Li, X. Kong, J. Wang, B. Yang, P. Wong, Y. Zhong, X. Zhang, Z. Zhang, N. Liu, W. Wang, Y. Yang, and S.-C. Zhu. Civrealm: learning and reasoning odyssey in civilization for decision-making agents, 2024. [73] J. Ren, X. Feng, B. Liu, X. Pan, Y. Fu, L. Mai, and Y. Yang. Torchopt: An efficient library for differentiable optimization, 2022. [74] A. Rutherford, B. Ellis, M. Gallici, J. Cook, A. Lupu, G. Ingvarsson, T. Willi, R. Hammond, A. Khan, C. S. de Witt, A. Souly, S. Bandyopadhyay, M. Samvelyan, M. Jiang, R. T. Lange, S. Whiteson, B. Lacerda, N. Hawes, T. Rocktaschel, C. Lu, and J. N. Foerster. Jaxmarl: Multi-agent rl environments and algorithms in jax, 2024. [75] S. Schmidgall, Y. Su, Z. Wang, X. Sun, J. Wu, X. Yu, J. Liu, Z. Liu, and E. Barsoum. Agent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227, 2025. [76] G. Starace, O. Jaffe, D. Sherburn, J. Aung, J. S. Chan, L. Maksin, R. Dias, E. Mays, B. Kinsella, W. Thompson, et al. Paperbench: Evaluating ais ability to replicate ai research. arXiv preprint arXiv:2504.01848, 2025. [77] L. Sun, Y. Han, Z. Zhao, D. Ma, Z. Shen, B. Chen, L. Chen, and K. Yu. Scieval: multi-level large language model evaluation benchmark for scientific research. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):1905319061, Mar. 2024. [78] W. Sun, L. Yan, X. Ma, S. Wang, P. Ren, Z. Chen, D. Yin, and Z. Ren. Is chatgpt good at search? investigating large language models as re-ranking agents, 2024. [79] K. Swanson, W. Wu, N. L. Bulaong, J. E. Pak, and J. Zou. The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation. bioRxiv, pages 202411, 2024. [80] J. Tang, H. Lu, R. Wu, X. Xu, K. Ma, C. Fang, B. Guo, J. Lu, Q. Chen, and Y.-C. Chen. Hawk: Learning to understand open-world video anomalies, 2024. [81] M. Tian, L. Gao, S. D. Zhang, X. Chen, C. Fan, X. Guo, R. Haas, P. Ji, K. Krongchon, Y. Li, S. Liu, D. Luo, Y. Ma, H. Tong, K. Trinh, C. Tian, Z. Wang, B. Wu, Y. Xiong, S. Yin, M. Zhu, K. Lieret, Y. Lu, G. Liu, Y. Du, T. Tao, O. Press, J. Callan, E. Huerta, and H. Peng. Scicode: research coding benchmark curated by scientists, 2024. 13 [82] P. Trirat, W. Jeong, and S. J. Hwang. Automl-agent: multi-agent llm framework for full-pipeline automl. arXiv preprint arXiv:2410.02958, 2024. [83] F. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi. Knowledge fusion of large language models, 2024. [84] Q. Wang, D. Downey, H. Ji, and T. Hope. Scimon: Scientific inspiration machines optimized for novelty. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 279299, 2024. [85] W. Wang, S. Zhang, Y. Ren, Y. Duan, T. Li, S. Liu, M. Hu, Z. Chen, K. Zhang, L. Lu, X. Zhu, P. Luo, Y. Qiao, J. Dai, W. Shao, and W. Wang. Needle in multimodal haystack, 2024. [86] X. Wang, Z. Hu, P. Lu, Y. Zhu, J. Zhang, S. Subramaniam, A. R. Loomba, S. Zhang, Y. Sun, and W. Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models, 2024. [87] X. Wang, B. Li, Y. Song, F. F. Xu, X. Tang, M. Zhuge, J. Pan, Y. Song, B. Li, J. Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. [88] Y. Wang, D. Huang, W. Ye, G. Zhang, W. Ouyang, and T. He. Neurodin: two-stage framework for high-fidelity neural surface reconstruction, 2024. [89] T. Webb, K. J. Holyoak, and H. Lu. Emergent analogical reasoning in large language models. Nature Human Behaviour, 7(9):15261541, 2023. [90] Y. Wei, Z. Wang, J. Liu, Y. Ding, and L. Zhang. Magicoder: Source code is all you need. arXiv preprint arXiv:2312.02120, 2023. [91] W.-H. Weng et al. Towards an ai co-scientist, 2025. [92] H. Wijk, T. Lin, J. Becker, S. Jawhar, N. Parikh, T. Broadley, L. Chan, M. Chen, J. Clymer, J. Dhyani, E. Ericheva, K. Garcia, B. Goodrich, N. Jurkovic, M. Kinniment, A. Lajko, S. Nix, L. Sato, W. Saunders, M. Taran, B. West, and E. Barnes. Re-bench: Evaluating frontier ai rd capabilities of language model agents against human experts, 2024. [93] D. Wu, J. Chang, F. Jia, Y. Liu, T. Wang, and J. Shen. Topomlp: simple yet strong pipeline for driving topology reasoning, 2023. [94] S. Wu, W. Zhang, L. Xu, S. Jin, X. Li, W. Liu, and C. C. Loy. Clipself: Vision transformer distills itself for open-vocabulary dense prediction, 2024. [95] C. Xiao, P. Zhang, X. Han, G. Xiao, Y. Lin, Z. Zhang, Z. Liu, and M. Sun. long-context extrapolation for llms with an efficient context memory, 2024. Infllm: Training-free [96] T. Xie, X. Qi, P. He, Y. Li, J. T. Wang, and P. Mittal. Badexpert: Extracting backdoor functionality for accurate backdoor input detection, 2023. [97] F. Xu, Q. Lin, J. Han, T. Zhao, J. Liu, and E. Cambria. Are large language models really good logical IEEE Transactions on Knowledge and Data reasoners? comprehensive evaluation and beyond. Engineering, 2025. [98] Y. Xu, W. Li, P. Vaezipoor, S. Sanner, and E. B. Khalil. Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations. arXiv preprint arXiv:2305.18354, 2023. [99] Z. Xu, F. Liu, and H. Liu. Bag of tricks: Benchmarking of jailbreak attacks on llms, 2024. [100] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen. Large language models as optimizers, 2024. [101] L. Yang, Z. Yu, T. Zhang, S. Cao, M. Xu, W. Zhang, J. E. Gonzalez, and B. Cui. Buffer of thoughts: Thought-augmented reasoning with large language models, 2024. [102] Z. Yang, L. Dong, X. Du, H. Cheng, E. Cambria, X. Liu, J. Gao, and F. Wei. Language models as inductive reasoners. arXiv preprint arXiv:2212.10923, 2022. [103] Z. Yang, X. Du, J. Li, J. Zheng, S. Poria, and E. Cambria. Large language models for automated open-domain scientific hypotheses discovery. arXiv preprint arXiv:2309.02726, 2023. [104] Z. Yao, L. Guo, X. Yang, W. Kang, F. Kuang, Y. Yang, Z. Jin, L. Lin, and D. Povey. Zipformer: faster and better encoder for automatic speech recognition, 2024. 14 [105] J. Yuan, X. Yan, B. Shi, T. Chen, W. Ouyang, B. Zhang, L. Bai, Y. Qiao, and B. Zhou. Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback, 2025. [106] V. Zambaldi, D. La, A. E. Chu, H. Patani, A. E. Danson, T. O. Kwan, T. Frerix, R. G. Schneider, D. Saxton, A. Thillaisundaram, et al. De novo design of high-affinity protein binders with alphaproteo. arXiv preprint arXiv:2409.08022, 2024. [107] G. Zhang, L. Fan, C. He, Z. Lei, Z. Zhang, and L. Zhang. Voxel mamba: Group-free state space models for point cloud based 3d object detection, 2024. [108] L. Zhang, Y. Zhang, K. Ren, D. Li, and Y. Yang. Mlcopilot: Unleashing the power of large language models in solving machine learning tasks, 2024. [109] S. Zhang, C. Gong, L. Wu, X. Liu, and M. Zhou. Automl-gpt: Automatic machine learning with gpt, 2023. [110] X. Zhang, J. Helwig, Y. Lin, Y. Xie, C. Fu, S. Wojtowytsch, and S. Ji. Sinenet: Learning temporal dynamics in time-dependent partial differential equations, 2024. [111] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. [112] Y. Zheng, B. Huang, W. Chen, J. Ramsey, M. Gong, R. Cai, S. Shimizu, P. Spirtes, and K. Zhang. Causal-learn: Causal discovery in python, 2023. [113] Q. Zhou, G. Pang, Y. Tian, S. He, and J. Chen. Anomalyclip: Object-agnostic prompt learning for zero-shot anomaly detection, 2025. [114] X. Zhu, Y. Guan, D. Liang, Y. Chen, Y. Liu, and X. Bai. Moe jetpack: From dense checkpoints to adaptive mixture of experts for vision tasks, 2024. [115] Z. Zhu, J. Wang, H. Cheng, and Y. Liu. Unmasking and improving data credibility: study with datasets for training harmless language models, 2024."
        },
        {
            "title": "A Extended Related Works",
            "content": "LLMs for scientific discovery. Many methods have adopted LLMs to generate novel hypotheses for common scientific discovery. For example, Baek et al. [4], Wang et al. [84], and Yang et al. [103] developed approaches for generating innovative domain-specific research ideas. Going beyond domain-specific ideas, line of work also focuses on generate hypothesis with LLMs in the commonsense domains [28, 67, 102, 91, 65, 89, 2, 98, 34, 97]. Moreover, prior research on automated scientific discovery proposes to combine hypothesis with LLM-assisted code generation for end-to-end workflows [50, 42, 64]. While these efforts works on various stages of the scientific lifecycle, experimentationa critical, rigor-sensitive aspectremains underexplored. Some existing research explores building an automated scientific discovery workflow with rigorous validation using AI agents [61, 91, 46, 106, 75, 9, 79, 105, 29], they often either have limited automated evaluation or rely on domain-specific ad-hoc prompting optimizations to guide predefined workflows, struggling with the complexities of rigorous end-to-end experimentation to automate AI research. Particularly, Lu et al. [61] introduced fully automated system called \"The AI Scientist\" to conduct research by collaborating with multiple LLM agents. These agents handle the full research process, from defining research problems and reviewing related literature to synthesizing and executing experiments. However, their solution has limited automated evaluation with focus on commonsense domains. Gottweis et al. [91] proposed an AI Co-scientist built on Gemini 2.0, aiming at building helpful AI collaborator for scientists. They focus on the scaling of the testtime compute paradigm to generate high-quality hypotheses and research proposals. While general purpose, the AI co-scientist is mainly validated in biomedical areas. Overall, these efforts often require experimental validation to follow constrained, framework-specific formats, resulting in extra overhead and hindering their usability. Benchmarks for domain-specific AI agent tasks. wide range of benchmarks have been developed to evaluate the capabilities of AI agents across diverse domains. Existing benchmarks predominantly target problem-solving [36, 26, 86, 77, 17], logical reasoning [18, 35, 5, 48], machine learning training [40, 109, 108, 30, 82, 66, 68, 31, 32], and knowledge retrieval and analysis [78, 37]. These benchmarks typically involve well-defined tasks with clear, deterministic solutions, allowing for consistent and objective assessment of AI agent performance. By contrast, our proposed EXP-Bench focuses on experimentation for automating AI research, which requires more rigorous and systematic approach beyond problem-solving. Experimental tasks demand iterative hypothesis refinement, complex experiment design/implementation and execution, and rigorous result interpretation. Our benchmark captures these challenges by semi-automatically evaluating AI agents on real-world experimentation tasks arising from influential AI research papers with high-impact open-source artifacts. Extended Details of the EXP-Bench Dataset In this section, we provide full list of the papers in the EXP-Bench dataset, including source paper and, AI sub-domain. The complete dataset can be found in our HuggingFace repository https://huggingface.co/datasets/Just-Curieous/EXP-Bench. Table 4: ICLR 2024 Papers Title Stars Cit.# Domain Key Dist. Resource T1 T2 T3 ID 19292 Zipformer: faster and better encoder for automatic speech recognition[104] 1023 Deep Learning Attention Mechanisms propose an architecture 1 3 3 0 0 memory needed: 32GB or more recommended, GPU type: NVIDIA V100 or A100, GPU amount: 2-8 OpenAI API key required; GPU: 1; memory: 16GB RAM 19033 The Reversal Curse: LLMs trained on is fail to learn is [6] 179 Deep Learning Large Language Models propose an architecture 16 ID Title Stars Cit.# Domain Key Dist. Resource T1 T2 19044 AnimateDiff: Animate 11093 845 Deep Learning Generative Models propose an architecture GPU type: NVIDIA; GPU amount: 1; GPU memory: 13GB 1 4 4 Your Personalized Text-to-Image Diffusion Models without Specific Tuning [33] BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection[96] MuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with Mutual Scoring of the Unlabeled Images [52] Domain-Agnostic Molecular Generation with Chemical Feedback[25] Periodicity Decoupling Framework for Long-term Series Forecasting[21] AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection[113] Unmasking and Improving Data Credibility: Study with Datasets for Training Harmless Language Models[115] RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems[57] Knowledge Fusion of Large Language Models[83] 19269 19281 18244 18318 19388 18013 18865 19610 17388 18889 Gated Aggregation Network[51] TopoMLP: Simple yet Strong Pipeline for Driving Topology Reasoning[93] AgentBench: Evaluating LLMs as Agents[58] An Extensible Framework for Open Heterogeneous Collaborative Perception[62] CLIPSelf: Vision Transformer for Dense Prediction[94] 165 12 Deep Learning Robustness Other 26 Applications Computer Vision 149 16 Applications Chemistry 38 Deep Learning Time Series propose training algorithm propose an architecture propose an architecture 0 0 8 0 0 GPUs with CUDA support (exact types not specified), 4GB+ recommended memory memory needed: 8GB; GPU type: NVIDIA; GPU amount: memory: 16GB RAM; GPU type: NVIDIA; GPU amount: 1 GPU: at least one (unspecified) 4 3 3 5 2 339 150 Deep Learning LLMs propose an architecture memory: 24GB; GPU: RTX 3090; amount: 5 3 1 2706 20 Social Aspects Accountability Other standard resources + 1 GPU recommended 3 3 6 GPU: 6 3 1 GPUs: 14 A100; RAM: 32GB+ 0 6 1 GPU (NVIDIA); RAM: 16GB RAM: 16GB; GPUs: 14 V100/A100 GPUs: 12 RTX 2080+; RAM: 16GB RAM: 15GB; GPU: 1; OpenAI API RAM: 16GB; GPUs: 2; CUDA compatible 5 9 3 5 0 3 3 0 0 6 2 0 RAM: 8GB; GPU: 1 3 3 propose dataset/library propose new ML application propose an architecture propose an architecture propose an architecture propose dataset/library propose new ML application propose new ML application 145 Deep Learning LLMs 535 101 Deep Learning LLMs 179 Applications Robotics 2406 174 Deep Learning LLMs 181 RL Multi-agent 183 70 Applications Computer Vision 17 SineNet: Learning Temporal Dynamics in PDEs[110] 572 14 Applications Physics 18447 MogaNet: Multi-order 220 Deep Learning GNNs ID 18660 19209 18439 18531 18540 19008 ID 93022 98316 95333 98326 98318 Title Stars Cit.# Domain Key Dist. Resource T1 T2 T3 TorchRL: data-driven decision-making library for PyTorch[11] Large Language Models as Optimizers[100] Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing[8] Zero Bubble (Almost) Pipeline Parallelism [71] CivRealm: Learning and Reasoning Odyssey for Agents[72] Safe RLHF: Safe Reinforcement Learning from Human Feedback[20] On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs[38] 2570 46 RL Deep RL 506 139 21 Optimization Learning for Optimization Probabilistic Methods Calibration 344 Optimization Parallel 106 21 RL Multi-agent 1421 RL Safe RLHF 110 58 Social Aspects Trustworthy ML propose dataset/library propose new ML application propose new ML application Other RAM: 8GB; GPUs recommended; API access may be needed likely RAM-heavy; API keys required; GPUs recommended standard memory for Python scripts memory: 16GB; GPUs: 8 propose dataset/library RAM: 816GB; GPU: 1; Freeciv-web access propose training algorithm Other OpenAI API; GPUs: A800-80GB 8 RAM: 8GB; OpenAI API; GPU recommended 5 0 0 0 5 3 0 0 1 0 4 0 3 3 2 2 3 1 Table 5: NeurIPS 2024 Papers"
        },
        {
            "title": "Stars",
            "content": "Cit.#"
        },
        {
            "title": "Domain",
            "content": "Key Dist."
        },
        {
            "title": "Resource",
            "content": "T1 T2 T3 Generative Modeling of Molecular Dynamics Trajectories[44] 144 13 Generative Models New Approaches Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs[15] Causal-learn: Causal Discovery in Python[112] 3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors[59] TorchOpt: An Efficient Library for Differentiable Optimization[73] BenchMARL: Benchmarking Multi-Agent Reinforcement Learning[7] 492 9 Optimization Generative Models"
        },
        {
            "title": "Causality",
            "content": "170 16 Computer Vision Video Generation 570 17 30 Optimization Zero-order and Black-box Optimization Reinforcement Learning Multi-agent 8 0 6 3 3 3 5 3 2 3 2 4 4 4 1 propose an architecture propose an architecture propose an architecture propose an architecture propose an architecture propose dataset GPUs not specified, but PyTorch and related libraries suggest need for CUDA-compatible GPU. Memory requirements unspecified. memory needed: 8 GB RAM minimum, OpenAI API key required, GPU: 1 NVIDIA GPU recommended, memory needed: Standard (depends on the dataset), GPU: Not required, memory needed: 16GB, Yes, GPU type: NVIDIA, GPU amount: 1, memory needed: At least 8GB RAM, GPU type: NVIDIA, GPU amount: 1, memory needed: At least 8GB RAM recommended, GPU: 1x NVIDIA GPU (e.g., GTX 1080 or better), Title Stars Cit.# Domain Key Dist. Resource T1 T2 T3 ID 95818 95974 97514 97713 93219 94065 96264 96897 93638 94328 Classification Done Right for Vision-Language Pre-Training[41] Reasoning Multi-Agent Behavioral Topology for Interactive Autonomous Driving[55] HEST-1k: Dataset For Spatial Transcriptomics and Histology Image Analysis[43] JaxMARL: Multi-Agent RL Environments and Algorithms in JAX[74] WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks[10] HAWK: Learning to Understand Open-World Video Anomalies[80] NeuRodin: Two-stage Framework for High-Fidelity Neural Surface Reconstruction[88] Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models[101] BAdam: Memory Efficient Full Parameter Optimization Method for Large Language Models[63] InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory[95] Self-playing Adversarial Language Game Enhances LLM Reasoning[16] QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs[3]"
        },
        {
            "title": "95262 MoE Jetpack: From\nDense Checkpoints to\nAdaptive Mixture of\nExperts for Vision\nTasks[114]",
            "content": "200 107 2 2 Multimodal Models Reinforcement Learning Multi-agent propose dataset propose dataset 236 36 Computational Biology propose dataset 520 20 Reinforcement Learning Multi-agent propose dataset 164 Theory Reinforcement Learning and Planning propose dataset 177 13 Computer VisionVideo Understanding propose dataset 117 6 Computer Vision 3D Reconstruction propose dataset 608 Generative Models Reasoning propose an architecture 244 4 Optimization Large Scale, Parallel and Distributed propose dataset 335 38 Language Knowledge propose an architecture 120 Generative Models Reasoning propose dataset 351 143 Deep Learning Attention Mechanisms propose an architecture 105 3 Deep Learning Algorithms propose dataset 5 4 3 4 2 3 2 2 4 1 0 5 3 0 0 4 2 1 4 11 6 3 4 3 8 0 5 17 5 1 1 8 memory needed: 16 GB, GPU type: NVIDIA V100, GPU amount: 1, memory needed: 8 GB RAM minimum, GPU: 1 NVIDIA GPU recommended, memory needed: 2GB, GPU type: NVIDIA, GPU amount: 1, memory needed: 8GB RAM minimum, GPU: 1 NVIDIA GPU (recommended), memory needed: 8GB, GPU: 1, memory needed: Not specified, but requires high-performance GPUs for training., GPU: 4 RTX A6000 48G, memory needed: At least 8GB, GPU type: NVIDIA GPU, GPU amount: 1 or more, memory needed: 16GB, True, GPU type: NVIDIA, GPU amount: 1, memory needed: 23.5 GB for Llama 3-8B, 21.8 GB for Llama 2-7B, GPU type: RTX3090, GPU amount: 1, memory needed: Minimum 16 GB GPU memory, GPU type: NVIDIA, GPU amount: 1, memory needed: 40G per GPU, GPU type: A100, GPU amount: 32, memory needed: 16 GB, GPU type: NVIDIA A100, GPU amount: 1, memory needed: Not specified, but likely requires significant RAM for training., GPU type: NVIDIA, GPU amount: 4, 19 Title Stars Cit.# Domain Key Dist. Resource T1 T2 T3 ID 94391 94155 CycleNet: Enhancing Time Series Forecasting through Modeling Periodic Patterns[54] SegVol: Universal and Interactive Volumetric Medical Image Segmentation[22] Voxel Mamba: Group-Free State Space Models for Point Cloud based 3D Object Detection[107] 133 15 Time Series 295 48 Computer Vision Segmentation propose an architecture propose an architecture 26 Computer Vision 3D Object Detection propose dataset 122 15 Trustworthy Machine Learning propose dataset 379 2 Multimodal Models propose dataset 20 Datasets and Benchmarks propose dataset 112 20 Multimodal Models propose dataset 761 8 Physical Models Physics propose dataset Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs[99] 97791 97609 97674 97882 The Multimodal Universe: Enabling Large-Scale Machine Learning with 100 TB of Astronomical Scientific Data[19] DrivAerNet++: Large-Scale Multimodal Car Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks[24] Needle In Multimodal Haystack[85] The Well: Large-Scale Collection of Diverse Physics Simulations for Machine Learning[70]"
        },
        {
            "title": "C Societal Impact",
            "content": "7 3 4 6 3 2 4 5 12 0 4 3 1 2 0 3 0 0 3 0 memory needed: Minimum of 16 GB RAM, GPU: 1 NVIDIA GPU (e.g., Tesla V100 or equivalent), memory needed: 16GB, GPU type: NVIDIA, GPU amount: 1, memory needed: Not specified, but high memory usage expected due to multi-GPU training, GPU type: NVIDIA A100, GPU amount: 8, Requires access to multiple GPUs (50 A800 GPUs recommended); approximately 55,000 GPU hours for experiments. memory needed: 64 GB RAM, GPU: 2 NVIDIA A100, memory needed: 1000 GB, GPU: 2 NVIDIA GPUs (e.g., RTX 3090 or equivalent), memory needed: not specified, GPU type: NVIDIA, GPU amount: 8, memory needed: 16GB, GPU type: NVIDIA CUDA, GPU amount: 1, The advancement of AI agents capable of conducting AI research, as facilitated by benchmarks like EXP-Bench, offers positive societal impacts. It might significantly shorten innovation cycles within AI itself and lead to more rapid advancements in machine learning capabilities. While faster pace of AI development can also democratize research tools and improve overall scientific efficiency, it concurrently amplifies the importance of addressing potential negative societal consequences. On the other hand, the rapid evolution of AI capabilities heightens risks, where we need to be careful about potential misuse, algorithmic bias, and the evolving role of human researchers, alongside the development of robust governance."
        },
        {
            "title": "D Average Scores across All Paper Categories",
            "content": "Defintions. Comp. Biology refers to Computational Biology. CV refers to Computer Vision. & refers to Datasets and Benchmarks. Gen. Models refers to Generative Models. Proba. Methods refers to Probabilistic Methods. RL refers to Reinforcement Learning. Addendum. Table. 6 contains updated values for IA+3.5 Haiku for the Applications and Reinforcement Learning categories. Table 6: Average benchmark scores of various models and agents across select task categories. Evaluation performed against EXP-Bench. Category Applications Applications Applications Applications Applications Applications Applications Causality Causality Causality Causality Causality Causality Causality Comp. Biology Comp. Biology Comp. Biology Comp. Biology Comp. Biology Comp. Biology Comp. Biology CV CV CV CV CV CV CV & & & & & & Deep Learning Deep Learning Deep Learning Deep Learning Deep Learning Deep Learning Deep Learning Gen. Models Gen. Models Gen. Models Gen. Models Gen. Models Gen. Models"
        },
        {
            "title": "Agent\nOH\nOH\nOH\nOH\nIA\nIA\nOH\nOH\nOH\nIA\nIA\nOH\nOH\nOH\nOH\nIA\nIA\nOH\nOH\nOH\nOH\nOH\nOH\nOH\nOH\nOH\nIA\nIA\nIA\nOH\nOH\nOH\nOH\nOH\nOH\nOH\nOH\nOH\nIA\nIA\nOH\nOH\nOH\nIA\nIA\nOH\nOH",
            "content": "Model Nova Pro o3-mini 3.5 Haiku 3.7 Sonnet Nova Pro 3.5 Haiku DeepSeek R1 o3-mini 3.7 Sonnet 3.5 Haiku Nova Pro 3.5 Haiku Nova Pro DeepSeek R1 o3-mini Nova Pro 3.5 Haiku Nova Pro 3.5 Haiku 3.7 Sonnet DeepSeek R1 Nova Pro 3.7 Sonnet o3-mini 3.5 Haiku DeepSeek R1 Nova Pro 3.5 Haiku Nova Pro o3-mini DeepSeek R1 3.5 Haiku 3.7 Sonnet Nova Pro o3-mini 3.5 Haiku 3.7 Sonnet DeepSeek R1 Nova Pro 3.5 Haiku Nova Pro o3-mini 3.7 Sonnet 3.5 Haiku Nova Pro DeepSeek R1 Nova Pro 19.0 0.0 8.3 30.8 5.0 33.3 0.0 22.2 88.9 40.0 20.0 0.0 0.0 0.0 11.1 20.0 0.0 0.0 0.0 0.0 0.0 42.9 18.2 5.1 4.5 0.0 28.6 0.0 0.0 0.0 0.0 0.0 50.0 0.0 13.4 12.8 37.1 0.0 21.7 14.9 0.0 30.4 26.1 45.5 10.0 0.0 0.0 23.9 8.0 24.5 26.8 9.8 32.3 4.3 44.6 83.1 40.6 11.7 36.6 14.9 18.7 30.3 16.7 7.8 29.3 9.7 11.1 22.1 20.9 28.9 9.3 28.3 11.2 5.3 17.4 0.0 46.5 0.0 0.0 89.5 0.0 14.1 25.3 37.4 6.3 9.2 16.9 16.5 27.9 25.8 24.8 13.9 19.2 16.7 19.2 9.0 19.2 9.0 0.0 6.8 3.1 37.8 36.6 23.1 0.0 48.7 17.7 10.0 37.0 0.0 3.2 31.2 4.8 4.8 12.2 24.8 18.5 11.5 15.9 5.8 0.0 6.4 0.0 0.0 0.0 0.0 19.0 0.0 17.6 20.2 17.2 7.4 0.0 9.4 16.4 23.6 17.6 5.4 0.0 9.2 14.7 21 IE 0.0 0.0 5.6 7.7 0.0 16.7 0.0 11.1 66.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 9.1 2.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 50.0 0.0 0.9 1.3 5.7 0.0 0.0 0.0 0.0 17.4 13.0 27.3 0.0 0.0 0. 13.9 8.3 8.3 8.3 0.0 0.0 0.0 44.4 44.4 20.0 0.0 33.3 11.1 0.0 44.4 0.0 0.0 33.3 11.1 11.1 0.0 28.2 21.2 12.8 12.8 2.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 20.5 15.2 14.3 2.2 0.0 0.0 16.1 29.0 7.4 3.0 0.0 0.0 9.7 All AllE 0.0 0.0 0.0 2.8 0.0 0.0 0.0 11.1 0.0 0.0 0.0 0.0 0.0 0.0 11.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.9 0.0 0.0 0.0 0.0 6.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 11.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.3 0.0 0.0 0.0 0.0 0.0 Gen. Models Language Language Language Language Language Language Language Multimodal Multimodal Multimodal Multimodal Multimodal Multimodal Multimodal Optimization Optimization Optimization Optimization Optimization Optimization Optimization Physical Models Physical Models Physical Models Physical Models Physical Models Physical Models Proba. Methods Proba. Methods Proba. Methods Proba. Methods Proba. Methods Proba. Methods Proba. Methods RL RL RL RL RL RL RL Social Aspects Social Aspects Social Aspects Social Aspects Social Aspects Social Aspects Social Aspects Theory Theory Theory Theory Theory Theory Theory Time Series Time Series Time Series"
        },
        {
            "title": "OH\nOH\nIA\nOH\nIA\nOH\nOH\nOH\nOH\nOH\nOH\nOH\nOH\nIA\nIA\nOH\nOH\nOH\nOH\nOH\nIA\nIA\nOH\nOH\nOH\nOH\nOH\nIA\nOH\nIA\nOH\nOH\nOH\nOH\nIA\nOH\nOH\nOH\nIA\nOH\nIA\nOH\nOH\nOH\nOH\nIA\nOH\nIA\nOH\nIA\nIA\nOH\nOH\nOH\nOH\nOH\nOH\nOH\nIA",
            "content": "31.0 8.5 11.7 22.9 2.1 0.0 11.5 4.3 23.7 32.8 17.7 27.0 9.1 17.1 25.3 25.2 35.8 28.1 21.0 18.6 9.8 31.4 23.0 0.0 0.0 0.0 16.7 0.0 32.0 0.0 49.3 23.7 86.0 11.0 57.0 48.2 34.8 41.4 29.0 10.3 8.6 28.9 20.2 22.8 23.4 18.7 23.5 0.0 9.1 11.7 16.7 11.2 0.0 3.3 17.0 0.0 37.9 65.4 23.6 0.0 13.3 80.0 20.0 33.3 0.0 0.0 0.0 13.6 16.7 0.0 18.2 0.0 50.0 25.0 25.5 52.0 11.5 50.0 3.8 19.2 60.0 66.7 50.0 0.0 0.0 33.3 0.0 33.3 0.0 100.0 0.0 100.0 0.0 0.0 27.3 15.7 11.5 24.0 0.0 9.7 0.0 0.0 16.7 5.6 16.7 100.0 0.0 0.0 33.3 0.0 16.7 0.0 0.0 0.0 0.0 7.7 61.5 12.5 0.0 0.0 0.0 6.7 0.0 0.0 0.0 0.0 4.5 0.0 0.0 9.1 0.0 0.0 0.0 6.4 20.0 0.0 0.0 0.0 0.0 12.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 21.2 2.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 30.8 0.0 9.7 20.0 13.3 6.7 0.0 0.0 13.3 6.7 13.6 13.6 13.6 9.5 9.1 0.0 0.0 25.5 18.6 12.8 10.6 6.4 3.2 2.5 66.7 33.3 0.0 0.0 0.0 0.0 66.7 0.0 0.0 0.0 0.0 66.7 0.0 17.6 27.5 17.6 2.2 2.0 0.0 13.7 27.8 22.2 11.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 30.8 15.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 3.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0."
        },
        {
            "title": "3.5 Haiku\no3-mini\n3.5 Haiku\n3.7 Sonnet\nNova Pro\nDeepSeek R1\n3.5 Haiku\nNova Pro\no3-mini\nNova Pro\n3.5 Haiku\n3.7 Sonnet\nDeepSeek R1\nNova Pro\n3.5 Haiku\no3-mini\n3.7 Sonnet\n3.5 Haiku\nNova Pro\nDeepSeek R1\nNova Pro\n3.5 Haiku\no3-mini\nNova Pro\nDeepSeek R1\n3.5 Haiku\n3.7 Sonnet\nNova Pro\n3.5 Haiku\nNova Pro\no3-mini\nDeepSeek R1\n3.7 Sonnet\nNova Pro\n3.5 Haiku\n3.7 Sonnet\no3-mini\n3.5 Haiku\n3.5 Haiku\nDeepSeek R1\nNova Pro\nNova Pro\nNova Pro\no3-mini\n3.5 Haiku\nNova Pro\n3.7 Sonnet\n3.5 Haiku\nDeepSeek R1\nNova Pro\n3.5 Haiku\no3-mini\n3.7 Sonnet\nDeepSeek R1\nNova Pro\n3.5 Haiku\no3-mini\n3.7 Sonnet\nNova Pro",
            "content": "23.6 16.3 4.4 4.6 0.0 6.4 8.5 8.4 10.9 18.9 15.5 19.7 4.6 0.0 3.0 21.5 18.2 25.6 17.1 9.5 1.4 8.9 60.0 51.7 0.0 0.0 0.0 0.0 49.0 0.0 28.7 0.0 86.0 45.0 0.0 18.3 23.5 27.7 3.0 5.0 0.0 17.9 15.5 21.7 18.9 0.0 11.1 0.0 9.7 0.0 0.0 0.0 0.0 13.8 16.7 0.0 24.0 16.5 0.0 22 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0."
        },
        {
            "title": "3.5 Haiku\nNova Pro\nDeepSeek R1\n3.5 Haiku\n3.5 Haiku\nNova Pro\no3-mini\nNova Pro\n3.7 Sonnet",
            "content": "19.5 16.7 6.3 19.2 15.5 0.0 6.7 20.3 12.2 34.5 10.0 8.7 15.2 21.3 4.0 3.8 1.5 20.8 37.5 0.0 0.0 0.0 0.0 16.7 8.3 12.5 16.7 37.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 15.4 0.0 0.0 25.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 23 Extended EXP-Bench Examples This section presents two extended examples from EXP-Bench: question concerning robust detection in collaborative perception under imperfect localization, and question focused on implementing Time Delay Neural Network (TDNN) for automatic speech recognition. Each example details the experiments objective, methodology, relevant source code, and expected outcomes. The examples also include an analysis of agent performance on completing the task. E.1 Example 1: Robust Detection in Collaborative Perception This example question was extended from the paper An Extensible Framework for Open Heterogeneous Collaborative Perception [62]. The objective of this experiment is to assess whether HEAL (HEterogeneous ALliance) can maintain robust detection performance under realistic conditions of imperfect localization, when Gaussian noise is added to the agents poses. The experiment maintains constant variables such as the dataset (OPV2V-H) and the model architecture (HEAL). The independent variables are the position noise and rotation noise, while the dependent variable is the models detection performance matrices (AP30, AP50, and AP70). Experimental groups test the addition of Gaussian rotation and position noise at levels of 0, 0.2, 0.4, and 0.6 meters/degrees to accurate poses. The results will contribute to evaluating the robustness of cooperative perception model under conditions of imperfect localization. EXP-Bench extends this task from the original paper section 5.3 QUANTITATIVE RESULTS and utilizes the source code: /workspace/opencood/tools/inference_w_noise.py from the GitHub repository https://github.com/yifanlu0227/HEAL. Note that /workspace/ refers to the working directory from the agents initialization context. The general formulation of the task includes the question posted to the agent, the overall method of the experiment, the source of this question (specifically the section in the paper and the source code), and the expected outcome. This is illustrated in Fig. 7a. The agents task is to use the provided GitHub repository, with the source code masked, to conduct this experiment. To aid the agent in reconstructing the masked file, detailed instructions are provided, as shown in Fig. 7b. Evaluation of the task includes design, conclusion, and setup evaluation. The conclusion appears in the expected outcome in Fig. 7a. Design and setup evaluation are based on design complexity and requirements respectively, shown in Fig 8. An example agent output using the bedrock-us-anthropic-claude-3-7-sonnet-20250219-v1-0 LLM as backbone is showcased here. We perform diff operation between the code generated by the agent and the original source code. As this agent reconstructs several files to fulfil the task requirement, we focus on diff operation between the core reconstructed file (evaluate_robustness.py) and the source file (inference_w_noise.py), shown in Fig. 9. The two files share the same functional goal and have similar overall structure; however, the agent performs invalid operations in another file (reproduce_exp_bench.sh), leading to failure in completing the task. The detailed reasoning provided by the judge is illustrated in Fig. 10. (a) The formulation of the task question. (b) Instructions provided to the agent. Figure 7: Task Fields for Example 1. 25 Figure 8: Evaluation of the design and setup for the Extended Task in Example 1. Figure 9: Example 1s Git diff comparing the masked source file and the agent-reconstructed source code. Red highlights indicate deletions, while green highlights represent additions. 27 Figure 10: Error Analysis and Comprehensive Explanation of the agents failure to complete the task in Example 1. 28 E.2 Example 2: Time Delay Neural Network for ASR This example is extended from the paper Zipformer: Faster and Better Encoder for Automatic Speech Recognition [104]. The objective of this experiment is to implement Time Delay Neural Network (TDNN) that achieves Word Error Rate (WER) of less than 1% on the test set. This setup focuses on constructing TDNN model with three Conv1d layerseach followed by ReLU activation and Batch Normalizationand final linear layer to produce log probabilities for phoneme classes. The dataset (yesno), model type (TDNN), loss function (Connectionist temporal classification), and feature extraction method (23-dimensional fbank features) are held constant. Independent variables include the model architecture, training hyperparameters (e.g., learning rate, weight decay), and number of epochs, while the dependent variable is the WER obtained during evaluation. This task emphasizes practical training and decoding using k2s one_best_decoding method and evaluates performance using the WER metric, targeting values below 1%. EXP-Bench extends this task beyond the baseline speech recognition example by formalizing an end-to-end pipeline using code modules: /workspace/egs/yesno/ASR/tdnn/model.py, train.py, decode.py, and asr_datamodule.py from the Github repository https://github.com/k2-fsa/icefall. The general formulation of the task includes the question posted to the agent, the overall method of the experiment, the source of this question (specifically the section in the paper and the source code), and the expected outcome. This is illustrated in Fig. 11a. Again, the agents task is to use the provided GitHub repository, with the source code masked, to conduct this experiment. To aid the agent in reconstructing the masked file, detailed instructions are provided, as shown in Fig. 11b. Similarly, evaluation of the task includes design, conclusion, and setup evaluation. The conclusion appears in the expected outcome in Fig. 11a. Design and setup evaluation are based on design complexity and requirement respectively, shown in Fig 12. For the agent performance in this example, we make use of an agents output using the bedrock-usamazon-nova-pro-v1-0 LLM backbone. We perform diff operation between the code generated by the agent and the original implementation files provided in the baseline. Since the agent restructures multiple modules to accomplish the speech recognition task, our analysis focuses on diff between the core model implementation file (model.py) and the original reference. The agent correctly builds the TDNN model and integrates it with the training and decoding pipeline. The two versions of model files share similar architectural skeleton, but differ in details such as layer configuration and parameter initialisation. The differences are shown in Fig. 13. (a) The formulation of the task question. (b) Instructions provided to the agent. Figure 11: Task fields for Example 2. 30 Figure 12: The design and setup evaluation of the extended task in Example 2. Figure 13: Example 2s Git diff of the masked source file and the agent reconstructed source code. In the diff, red highlights are deletions. Green highlights are additions."
        },
        {
            "title": "F Dataset Curation and Benchmark Construction Details",
            "content": "To ensure the quality and integrity of EXP-Bench, we developed the curation pipeline through careful, iterative process. Each component was prototyped, tested on real papers, and refined based on manual inspection by collaborators. This allowed us to isolate and address specific failure modes incrementally, steadily increasing curation throughput without compromising accuracy. Several representative issues that were patched in our final pipeline are documented in Table. 7. Manual validation was also aided by the availability of ground truth from the papers and open-source code repositories themselves, making the verification process straightforward. The EXP-Bench team is committed to the long-term maintenance and growth of the dataset. We actively monitor feedback and bug reports via GitHub and HuggingFace issue trackers and will address any concerns raised by the community post-release. All data is hosted on both platforms to ensure accessibility and stability, with potential plans to replicate the dataset on archival storage for long-term preservation. To foster transparency, reuse, and critical engagement, the dataset is released under the permissive Creative Commons Attribution 4.0 license, and all code under the MIT license. We encourage the community to explore, build upon, and challenge EXP-Bench as an open and evolving resource. Table 7: Examples of extraction issues identified that were subsequently patched in the final pipeline."
        },
        {
            "title": "Expected\noutcome",
            "content": "Method / Usage Instruction / Agent Instruction"
        },
        {
            "title": "Conclusion data mentioned in\nthe hypothesis",
            "content": "Masked source doesnt exist"
        },
        {
            "title": "Invalid operation",
            "content": "Conclusion not aligned with the papers findings Mentioned specific parts of the paper (tables or figures)"
        },
        {
            "title": "Invalid operations",
            "content": "Actual Example BaDExpert outperforms baseline defenses in backdoor detection on CIFAR10, achieving significantly higher AUROC (near 99%). Specifically, can PDF achieve around 34.64% lower MACs compared to PatchTST and 74.38% lower MACs ...? \"source\": [\"/workspace/- topomlp_setA_r50_w_otransform.py\"...] MuSc has musc.py under workspace/model/ but the source file indicates it under workspace/example/ Run the evaluation script for the baseline EVA-CLIP ViT-B/16 model using distributed processing with 8 GPUs... Merge the trained models using the heal_tools.py script (/workspace/opencood/tools/heal_tools.py:115130) Analyze execution outcomes from Table 4, comparing... N/A The scripts will log metrics including mean rewards and standard deviations, which can be compared with the reported results in Table 2 of the paper. Set appropriate model architecture parameters (encoder layers, attention heads, dimensions) Collect and analyze performance results from Table 3, ... Time and Cost Expenditure. During the initial phasesbefore our curation pipeline was finalizedeach paper required roughly two hours of manual effort. This involved full read-through (with emphasis on evaluation sections), task-by-task verification, and iterative pipeline corrections to ensure compatibility. The process included checking GitHub repositories, assessing setup validity and complexity, and verifying alignment with the papers descriptions. Once the pipeline was fully 33 constructed and refined based on feedback, manual validation time dropped to around 20 minutes per paper, primarily to confirm alignment. Only minor adjustments were rarely needed, and we expect this time to decrease further in future deployments. LLM-related extraction costs varied by task type and count, averaging approximately $60 USD per paper. For extraction, we used o3-mini-2025-0101-preview for the main task extraction and claude-3-7-sonnet-20250219-v1:0 for implementation extraction. Costs were primarily driven by input tokens, as the models required full paper texts and codebases to perform accurate extraction."
        },
        {
            "title": "G Extended Analysis on Prevalent Agent Failure Patterns",
            "content": "Some overlap between categories may exist, as the classification was performed by an LLM. Table 8: Agents fail in diverse ways across different phases of experimentation, measured across all agent and model evaluations."
        },
        {
            "title": "Phase\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\nconclusion\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign\ndesign",
            "content": "Failure Type Missing Conclusion Content Incorrect Conclusion Interpretation Incomplete Conclusion Outcome Statement Extraneous Details Missing Conclusion Analysis Missing Comparative Conclusion Analysis Minor Omission of Specific Details Incorrect Numeric Conclusion Mismatched Conclusion Format Error Message Output Incomplete Conclusion with Missing Exp. Findings Conclusion Diverges from Expected Emphasis Missing Comparative Analysis Missing Quantitative Performance Metrics Missing Visualization Details Incomplete Performance Evaluation Missing Numerical Equivalence Verification Missing Trend Analysis Naming Inconsistency Output Conclusion Partially Matching with Numerical Deviations Deviation in Saturation Point Conclusion Inconsistent ASR Reporting Missing Conclusion Analysis on Attack Budget Effects Missing Diminishing Returns Analysis Missing Methodological Innovation Discussion Missing Performance Evaluation Metrics Missing Submission Format Specification Incomplete or Misclassified Design Variables Omission of Required Design Variables Complete Omission of Exp. Design Variables Incorrect Design Specification Details Incomplete Exp. Design Details Irrelevant Procedural Additions Missing Design Variable Information Inclusion of Extraneous Factors Incorrect Parameter Details Partial Omission of Constant Variables Incomplete Constant Variable Specification Partial Fulfillment of DV Error Message Returned Instead of Design Information Incomplete Differentiation of Constant and Ind. Variables Missing Dependent Variable Tracking Incomplete Exp. Design Specification Incomplete Specification of Design Variables Missing Hyperparameter Design Details Partially Complete Design Variable Specification Missing Design Formatting Details Missing Design Variables Details Missing Explicit Variable Labeling Missing Configuration File Variable Missing Input Format Details 35 Prevalence (%) 26.18 19.66 14.43 7.77 4.35 4.03 3.47 3.21 2.7 2.67 2.14 1.6 0.8 0.8 0.56 0.53 0.53 0.53 0.53 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 16.05 19.84 13.1 8.32 7.67 7.62 3.83 3.64 3.18 2.75 3.61 1.93 1.27 1.27 1.06 0.64 0.64 0.64 0.64 0.42 0.42 0.42 0.21 0.21 design design design exec exec exec exec exec exec exec exec exec exec exec exec exec exec exec exec exec exec exec exec exec exec exec setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup Omission of Exp. Configuration Details Omission of Fixed Block Partition Partial Design Variable Extraction with Misclassification Environment/Dependency Configuration Errors Execution Script and File Errors Missing Dependency Error Missing Setup Script File Tensor Operation Execution Error Syntax Error in Execution Environment Missing Input Data File Missing Required Attribute in Execution Missing Evaluation Output Files Missing Requirements File Runtime Indexing Error During Generation Insufficient Shared Memory in DataLoader Execution Execution Environment Warning: Root Privilege Usage Incorrect Dependency Import in Execution Dependency Version Conflict Docker Execution Failure Incomplete Results Saving Impl. Incorrect Dataset Loading Incorrect Function Argument Handling Missing Hugging Face API Token Authentication Missing Performance Metrics and Argument Parsing Missing Trust Remote Code Flag in Execution Environment Missing Setup Script File Missing Essential Impl. Components Incomplete Evaluation Metric Impl. Missing Critical Exp. Setup Details Incomplete Data and Preprocessing Setup Missing Command Line Argument Parsing Incomplete Exp. Setup Impl. Incomplete Training Regimen Impl. Incomplete Comparative Setup Features Missing Modular Helper Functions Incomplete Dataset Splitting Setup Missing Comparative Evaluation Methods Naming Inconsistencies Components Missing Detailed Architectural Parameters Incorrect Model Initialization Missing Optimizer Configuration Incomplete Evaluation Procedure Missing Critical Import Statements Missing Essential Library Imports Incomplete Results Saving Impl. Incomplete or Misplaced Setup Impls. Incorrect Dependency Import Misconfigured Exp. Infrastructure Missing C++ Acceleration Integration Missing Distributed Training Parameters Missing Hardware/Device Configuration Missing Training Pipeline Configuration Incorrect Evaluation Metric Impl. Incorrect Forward Method Impl. Hard-Coded Configuration Instead of YAML Loading Incomplete Benchmark Configuration Incomplete Rendering Pipeline Impl. Incorrect Model Architecture Incorrect Testing Dataset Usage 0.21 0.21 0.21 29.38 23.84 11.9 6.95 3.22 2.86 2.27 2.27 1.82 1.82 1.82 1.41 1.36 1.36 0.91 0.91 0.91 0.91 0.91 0.91 0.91 0.91 0.45 39.71 2.15 1.88 1.83 1.58 1.49 1.47 1.25 1.13 1.04 1.02 1.02 0.91 0.9 0.9 0.79 0.79 0.56 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.37 0.35 0.34 0.34 0.34 0.34 0.34 setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup Misconfigured Exp. Setup Parameters Missing Configuration File Loading Missing Critical Function Impls. Missing Critical Quantization Procedures Missing Essential Parameter Initializations Missing Intermediate Data Reuse Mechanism Missing Loss Function and Evaluation Metric Setup Missing Model Architectures Missing Model Evaluation Mode Invocation Missing Model and Dataset Integration Missing Reproducibility Measures Missing Feedback Mechanism Environment/Dependency Configuration Errors Faulty Dataset Integration Faulty Training Script Logic Impl. Mismatch with Setup Specification Incomplete Benchmarking Function Impl. Inconsistent Configuration Incorrect File Naming/Structure Insufficient Exp. Setup Impl. Missing Chain-of-Thought Module Missing Conditional Model Initialization Missing Dimensionality Reduction Impl. Missing Evaluation on Validation Data Missing Exp. Entry Point Impl. Missing Exp. Resumption Mechanism Missing Explicit Data Loader Missing Explicit Model Arch. for Inner Optimization Missing Final Agent Return Impl. Missing Final Test Validation Missing Finalization Message Missing GPT-based Evaluation Component Missing GPU Batch Size Adjustment Missing Initial Policy Evaluation Missing Logging Mechanism Missing Loop Termination Mechanism Missing Model and Transform Initialization Missing Multi-GPU Result Merge Missing Multiple Runs for Statistical Significance Missing Periodic Evaluation Missing Pretrained Model Loading Missing Real-World Benchmark Dataset Missing Regularization Term Missing Scalability Testing Configuration Missing Test Cases Missing Visualization Impl. Missing Visualization Impl. Synthetic Dataset Used Instead of Specified Dataset Incomplete Defense Testing Setup Missing Critical Evaluation Computation Steps Missing Custom CI Test Impl. Missing Hydra-based Exp. Runner and Plotting Script Missing Key Exp. Pipeline Steps Missing Multiple Forecast Horizon Configurations Missing Performance Comparison Missing Required Evaluation Script Modifications Missing Sequence Length Computation Ambiguous Evaluation Metric Reporting Deviation from Required Library Usage 37 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.24 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.14 0.14 0.12 0.12 0.12 0.12 0.12 0.12 0.12 0.11 0.11 setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup Faulty Early Stopping Impl. Faulty Quantization Branch Flawed Visualization Utility Impl. Incomplete AstroCLIP Integration Incomplete Benchmark Directory Setup Incomplete Exp. Setup Impl. Incomplete Explanation of Comp. Graph Visualization Incomplete Extraction of Runtime Configuration Incomplete Inner Objective Impl. Incomplete MemoryUnit Impl. Incomplete Model Architecture Impl. Incomplete Model Weights Download Handling Incomplete Optimizer and Scheduler Setup Incomplete Output Processing Incomplete Parallel Processing Impl. Incomplete Prediction and Data Loading Impl. Incomplete Quantization Benchmark Param. Config Incomplete Result Logging Impl. Incomplete Results Saving Impl. Incomplete Training Loop and Reproducibility Measures Incomplete Visualization Impl. Incomplete Visualization Pipeline Impl. Incomplete or Misplaced Plotting Impl. Inconsistent Dataset Collection Specification Inconsistent Feature Extraction Impl. Incorrect Benchmark Command Structure Incorrect BlockOptimizer Impl. Incorrect Class Structure Incorrect Dataset Loading Incorrect Exp. Split Configuration Incorrect Function Signature Incorrect Hard-coded Parameter Block Impl. Incorrect Independence Test Impl. Incorrect Inner Objective Impl. Incorrect Optimizer Comparison Impl. Incorrect Spatial Matching Impl. Ineffective Caching Setup Insufficient Benchmark Dataset Insufficient Positional Encoding Impl. Misconfigured Benchmark Parameters Misconfigured Warmup Parameters Mismatch in Benchmark Parameter Settings Missing Ablation Study Configuration Missing Alternative OOP API Impl. Missing Benchmark Data Processing Components Missing Benchmark Runner Function Missing Block Parameter Switching Logic Missing Block-specific Finetuning Parameters Missing Block-wise Parameter Grouping Impl. Missing BlockOptimizer Impl. Details Missing BoT Pipeline Components Missing Checkpoint Loading Impl. Missing Checkpoint Saving Impl. Missing Command-Line Toggle for Final Execution Missing Comparative Optimization Strategy Impl. Missing Comparison Visualization Component Missing Comprehensive Plotting Components Missing Computational Performance Metrics Missing Conditional Checks 38 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup Missing Conversation and Interactive Setup Components Missing Critical Analysis Components Missing Critical Benchmark File Missing Critical CycleNet Components Missing Critical Exp. Tracking Components Missing Critical Information Extraction Missing Critical Model Module Impls. Missing Custom Optimizer and Trainer Integration Missing Data Reshaping to Remove Padding Missing Data Structure Impl. Missing Dataset Download Script Missing Dedicated Custom Function for Mesh Extraction Missing Dedicated Inference Script Missing Dedicated Quantized KV Cache Decode Impl. Missing Dedicated Trainable Bundle Methods Impl. Missing Detailed Exp. Protocol Missing Dynamic Dataset Configuration Missing Edge Case Scenario Missing Embedding Extraction Impl. Missing Entry Point Script for Exp. Setup Missing Essential Feature Extraction Missing Essential Modules and Evaluation Components Missing Essential Post-Processing Functions Missing Expected Configuration Output Missing Expected Conversation Templates Missing Exp. File Location Missing Exp. Tracking and Run Naming Mechanisms Missing Exp. Replication Configuration Missing Exp. Script Modifications Missing Explicit Meta-Parameter Definition Missing Explicit Model Components Missing Explicit Parameter Configuration Missing Explicit Return of Submission File Path Missing Explicit Task List Definition Missing External Data and Model Weight Downloading Missing Final Evaluation Routine Missing Final Output Return Missing Fine-tuning Orchestration Function Missing Finetuning Type Loader Impl. Missing Framework Integration Components Missing GPU Optimization Missing GPU Synchronization Missing GPU Transfer and Synchronization Setup Missing Gene Expression and Embedding Matching Impl. Missing Hugging Face API Token Authentication Missing Implicit Differentiation Step Missing InfLLM Context Memory Impl. Missing Inference Prompt Configuration Missing Input Pre-processing Missing Input and Key-Cache Quantization Configurations Missing Integration Components for Advanced Training Missing Integration Components for Block-wise Optimization Missing Intermediate Data Reuse Mechanism Missing Learning Rate Scheduler Impl. Missing Lightning CLI Training Configuration Missing Logging Configuration Missing MLP Layer Size Configuration Missing Main Execution Function Missing Mem. Management and Precision Conversion Impl. 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup Missing Memory Precision Conversion Impl. Missing Memory Saving Metric Calculation Missing Meta-Buffer Template Retrieval Missing Model and Transform Initialization Missing Network State Extraction in Visualization Missing Normalization Configuration Missing Normalization Disabling Parameter Missing Optimality Condition Impl. Missing Optional Analysis Component Missing Optional Parameter Impl. Missing Output Directory and Logging Setup Missing Performance Metrics and Argument Parsing Missing Platform-Specific Parameter Handling Missing Platform-Specific Parameter Impl.s Missing Parameter Switching and Gradient Checkpointing Missing Post-Processing Component Missing Post-Processing and Result Saving Impl. Missing QMIX Algorithm Configuration Missing Random Sampling Impl. Missing Replication Procedures Missing Required Torch-based Impl.s Missing Result Storage Configuration Missing Result Visualization Missing RevIN Normalization Impl. Missing Reward Shaping Impl. Missing Scalability Testing Configuration Missing Single Scan Visualization Function Missing Specialized Trainer Integration Missing Standardized Testing Components Missing Statistical Evaluation Metrics Missing Supervised Fine-Tuning Data Integration Missing Surrogate Model (AutoGluon) Impl. Missing Symbolic Mathematics Library Impl. Missing Synchronization of Initial Conditions Missing Task-Specific Configuration Missing Testing Dataset Configuration Missing Testing Procedure Missing Training Loss Visualization Impl. Missing Tree Mapping Benchmark Impl. Missing Unique Run Name and Configuration Adjustments Missing Video Processing Impl. Missing Video Processing and Loss Calculation Impl. Missing Working Directory Change Command Missing Zero-Shot Classifier Impl. Missing Zoom-In Inference Impl. Missing train.sh File Non-standard Evaluation Script Usage Omission of Required Benchmark Parameter Oversimplified Instrumentation Oversimplified Pipeline Simulation Missing Separation of Optimization Pipelines Partial Impl. of Data Splitting and Hyperparameters Response Parsing Failure Security and Credential Management Issue Successful Setup for Performance Monitoring Missing Forecast Generation Step 40 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0."
        },
        {
            "title": "H Evaluation Judge",
            "content": "As detailed in the main text (4.1), our evaluation framework leverages an LLM-based judge (using o3-mini-2025-01-01-preview) to assess various aspects of agent performance, including adherence to integrity guidelines, experimental design, implementation correctness, and conclusion validity. We provide the full system prompts used by our LLM judge in this section. When we encounter long inputs such as Git diffs and agent logs, the content can sometimes exceed the context window of the o3-mini model. To address this, we segment the input into individual chunks that fit within the models context length. These chunks are then fed iteratively to the judge, with the evaluation results and relevant context from one chunk carried over to the subsequent chunk. This iterative processing ensures that the entire input is evaluated comprehensively without loss of information due to context length limitations. Monitor Prompt. The initial integrity check, performed by the Monitor, uses the following prompt to analyze agent logs for disallowed behaviors (e.g., accessing the research paper directly, performing Git operations, or using hardcoded data). Inspect the log to verify the following criteria: 1. The agent did NOT access (i.e., read) the research paper itself (e.g., the PDF). 2. The agent did NOT perform any Git operations, such as checking out commits, switching branches, or accessing other versions of the repository. 3. The agent did NOT use fake, hardcoded, or mock data in place of real experimental results. In particular, check for any signs of: - Hardcoded or fabricated data values used as results. - Use of mock or placeholder data instead of running actual experiments. - Directly copying or using result data already present in the repository, rather than generating it through experimentation. - Any indication that crucial data for the experiment was not produced by creating and running the intended experimental task. - If you find evidence that the agent faked data or failed to run the experiment, provide the reason why this happened if possible. For example, did the agent or LLM (such as nova pro) refuse to perform the task due to ethical concerns, security restrictions, or because the task involved potentially malicious actions? If the log contains an explicit refusal, error message, or explanation for why the agent faked or could not generate the data, include this in your reasoning. Output format exactly as this JSON: { } \"paper_access\": <boolean>, \"git_operations\": <boolean>, \"faked_or_nonexperimental_data\": <boolean>, \"setup_monitor_comprehensive_reason\": \"<comprehensive explanation for your decisions on the above keys, including specific evidence from the log and your reasoning process. Clearly justify each decision and reference relevant log entries or the absence thereof. If you found evidence for any key being True, explain exactly what that evidence was and why it led to your conclusion. If you found no evidence, explain how you searched and why you are confident in your answer. If you found evidence that the agent faked data or failed to run the experiment, also explain the reason for this only if possible (e.g., LLM refusal, security/ethical block, or other failure), citing the relevant log entries.>\" 41 Explanation of output keys: - \"paper_access\": True if the agent accessed the research paper, False otherwise. - \"git_operations\": True if the agent performed any Git operations, False otherwise. - \"faked_or_nonexperimental_data\": True if there is any evidence of fake, hardcoded, mock, or reused (non-experimental) data being used for crucial results, False otherwise. - \"setup_monitor_comprehensive_reason\": comprehensive explanation describing the evidence and reasoning for the above decisions, referencing specific log entries or the absence of evidence as appropriate. Here is the log: {logs} Design and Conclusion Evaluation Prompt. Once integrity is established, the agents experimental design and conclusion are evaluated for conceptual soundness, completeness, and alignment with ground truth. This assessment contributes to the (design correctness) and (conclusion correctness) metrics. You are judge tasked to evaluate systems output against ground truth answers for an experimental design task. Input fields: - design_ground_truth: the correct list of variables (constants, independent, dependent variables). - conclusion_ground_truth: the correct conclusion as string. - design_output: the predicted design. It may not be formatted as list; extract and match relevant variable information from its content. - conclusion_output: the predicted conclusion string. Evaluation Instructions: - Design Evaluation: Compare design_output to design_ground_truth. Count how many items in design_output match items in design_ground_truth. Return the percentage of correct items as an integer (e.g., use 75 to represent 75%), along with short explanation. If applicable, include failure analysis on what the system got wrong. - Conclusion Evaluation: Compare conclusion_output to conclusion_ground_truth. Return \"correct\" or \"incorrect\" based on semantic match, along with short explanation. If applicable, include failure analysis on what the system got wrong. Here is the input: {{ design_ground_truth: {design_gt}, conclusion_ground_truth: {conclusion_gt}, design_output: {design_output}, conclusion_output: {conclusion_output} }} 42 Output format exactly as this JSON: {{ \"design_evaluation_explanation\": \"<short explanation string>\", \"design_score\": <integer from 0 to 100>, \"design_error_analysis\": \"<short explanation of what was wrong with the output, i.e., what the system failed at, if applicable>\", \"conclusion_evaluation_explanation\": \"<short explanation string>\", \"conclusion_score\": \"<correct/incorrect>\", \"conclusion_error_analysis\": \"<short explanation of what was wrong with the output, i.e., what the system failed at, if applicable>\" }} Implementation Evaluation Prompt. The agents implementation is assessed by comparing the ground truth requirements against the Git diff generated by the agent. This evaluation contributes to the (implementation correctness) metric. You are judge tasked to evaluate systems experiment setup against ground truth requirements. Input fields: - setup_ground_truth: the correct experiment setup requirements, given as either list of step-by-step required actions/configs or natural language description. - setup_ground_truth_scripts: Source scripts that implement the ground truth setup. These may not match the setup_output exactly, but serve as code-level references for what correct setups may look like. - setup_output: the systems actual changes, given as Git diff patch (e.g., modifications to config files, scripts, etc.). Evaluation Instructions: - Setup Evaluation: - Compare setup_output against setup_ground_truth. Go step-by-step through each ground-truth requirement (explicit or implied) one-by-one to see if they are fulfilled in the diff. - Use the setup_ground_truth_scripts as code-level guidance: While the output doesnt need to match these scripts exactly, use them to ground your judgment of whether the implementation is reasonable and sufficiently close to what correct implementation should look like. - Focus on intent over exact matching: Variations in filenames or function names are fine if the requirement is fulfilled. - At the end, calculate score based on the number of requirements that are correctly implemented. - Return: - score as an integer percentage (e.g., 80 for 80%) representing how many ground truth setup requirements were correctly implemented. - detailed explanation of the evaluation result. - If applicable, include failure analysis of what requirements were missed or incorrectly implemented. 43 Here is the input: { \"setup_ground_truth\": {setup_gt}, \"setup_ground_truth_scripts\": {setup_scripts} \"setup_output\": {setup_output}, } Output format exactly as this JSON: { } \"setup_evaluation_explanation\": \"<detailed explanation string>\", \"setup_score\": <integer from 0 to 100>, \"setup_error_analysis\": \"<Explanation of what was wrong with the setup, i.e., what requirements were missed or done incorrectly, if applicable>\" Figure 14: Stricter metrics reveal lower true correctness. Table 9: Cost-time summary statistics for all evaluated agents and models. OH = OpenHands, IA = IterativeAgent. Med = median, Std = standard deviation, = time (minutes), = cost (USD)."
        },
        {
            "title": "Agent\nOH\nOH\nOH\nOH\nOH\nIA\nIA\nAgent\nOH\nOH\nOH\nOH\nOH\nIA\nIA",
            "content": "Model o3-mini 3.7 Sonnet Nova Pro 3.5 Haiku DeepSeek R1 3.5 Haiku Nova Pro Model o3-mini 3.7 Sonnet Nova Pro 3.5 Haiku DeepSeek R1 3.5 Haiku Nova Pro 23.24 29.64 15.03 24.23 31.40 26.13 26.31 13.93 16.67 11.85 13.26 19.09 19.63 19.51 Std Min Max Avg Med Q1 Q3 47.72 1.70 16.74 33.60 24.89 74.04 2.55 10.03 37.72 33.53 74.33 0.64 9.37 24.06 17.82 37.85 1.21 17.38 32.72 25.17 60.77 0.97 11.82 38.69 32.24 402.84 0.30 54.62 38.24 30.24 30.09 360.52 0.17 27.61 38.09 Avg Med Q1 Q3 Std Min Max 1.34 1.11 0.55 19.83 14.20 10.15 2.99 2.18 1.09 3.24 2.68 0.68 4.08 2.49 1.55 5.09 4.23 2.82 6.96 5.31 3.93 0.01 0.03 0.00 0.01 0.00 0.02 0.02 0.17 3.04 0.33 0.15 0.83 0.52 0. 0.35 7.53 0.77 0.42 1.28 1.86 3.26 0.56 6.30 0.93 1.47 1.70 2.90 3."
        },
        {
            "title": "I Additional Analysis",
            "content": "We include detailed breakdowns of the analysis performed in 4.2. I.1 Conjunctive Evaluation Metrics Analysis In Fig. 14, we include details for all agents and models evaluated, as opposed to the subset in Fig. 6b. I.2 Cost-Time Distribution We showcase the full costtime distribution in Table. 9 in the form of summary statistics. For timerelated statistics, although soft timeout of 40 minutes was enforced during trials, agents occasionally exceeded this limit due to non-compliance with the timeout mechanism. Additionally, both time and cost values can appear unusually low in cases where the agent failed to complete the experiment."
        }
    ],
    "affiliations": [
        "Cisco Research",
        "Rice University",
        "UC Berkeley",
        "University of Michigan"
    ]
}