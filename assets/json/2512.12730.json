{
    "paper_title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents",
    "authors": [
        "Jingzhe Ding",
        "Shengda Long",
        "Changxin Pu",
        "Huan Zhou",
        "Hongwan Gao",
        "Xiang Gao",
        "Chao He",
        "Yue Hou",
        "Fei Hu",
        "Zhaojian Li",
        "Weiran Shi",
        "Zaiyuan Wang",
        "Daoguang Zan",
        "Chenchen Zhang",
        "Xiaoxu Zhang",
        "Qizhi Chen",
        "Xianfu Cheng",
        "Bo Deng",
        "Qingshui Gu",
        "Kai Hua",
        "Juntao Lin",
        "Pai Liu",
        "Mingchen Li",
        "Xuanguang Pan",
        "Zifan Peng",
        "Yujia Qin",
        "Yong Shan",
        "Zhewen Tan",
        "Weihao Xie",
        "Zihan Wang",
        "Yishuo Yuan",
        "Jiayu Zhang",
        "Enduo Zhao",
        "Yunfei Zhao",
        "He Zhu",
        "Chenyang Zou",
        "Ming Ding",
        "Jianpeng Jiao",
        "Jiaheng Liu",
        "Minghao Liu",
        "Qian Liu",
        "Chongyao Tao",
        "Jian Yang",
        "Tong Yang",
        "Zhaoxiang Zhang",
        "Xinjie Chen",
        "Wenhao Huang",
        "Ge Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 0 3 7 2 1 . 2 1 5 2 : r NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents 1ByteDance Seed China, 2M-A-P, 32077AI, 4Humanlaya Data, 5Nanjing University, 6Peking University, 7Beijing University of Posts and Telecommunications, 8Beihang University Full author list in Contributions"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce fully installable Python library. Our experiments across state-of-the-art openand closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as central bottleneck for the next generation of autonomous coding agents. Date: December 16, 2025 Project Page: https://github.com/multimodal-art-projection/NL2RepoBench"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have rapidly evolved from passive code completion tools into increasingly autonomous coding agents capable of planning, editing, executing, and validating software [4, 11, 15, 20, 28, 31, 32]. This progress has shifted the frontier of automated programming from short-horizon, localized tasks toward more ambitious goal: end-to-end software construction driven solely by natural-language intent. Achieving this goal requires not only strong code synthesis capabilities, but also sustained long-horizon reasoning, global planning, and cross-file consistencycapabilities that are central to the vision of autonomous software engineering and, more broadly, Artificial General Intelligence (AGI). Despite this progress, the evaluation landscape has not kept pace with the capabilities being claimed. Most existing benchmarks for coding agents emphasize short-horizon behaviors, such as generating individual functions [5, 6], completing partially specified modules [17, 18], or repairing bugs within pre-existing reposito1 Figure 1 An example NL2Repo task document, illustrating the structured specification (project description, supports, and API usage guide) that agents receive before repository generation. ries [8, 13]. While valuable, these settings significantly reduce the demands placed on long-term planning and system-level coherence by providing strong structural priors, limited temporal scope, or frequent human intervention. As result, they do not adequately measure whether an agent can sustain coherent decision-making over the hundreds of steps required to design, implement, debug, and finalize complete software repository. Recent repository-level benchmarks partially address this gap, but important limitations remain. Some rely on scaffolded project structures or predefined function signatures [37], converting the task into constrained code infilling rather than autonomous construction. Others depend on LLM-based evaluators or qualitative judgments [7, 21, 24, 34], which introduce bias and obscure true functional correctness. Although recent works have expanded evaluation to visual artifacts [35] and web agent interactions [14], rigorous evaluation of full-repository construction from natural language remains underexplored. Even benchmarks that use unit tests often assume the presence of an existing codebase [9, 13], shifting the challenge toward repair or regression rather than creation. Consequently, fundamental question remains unanswered: can current coding agents reliably generate complete, installable software repository from scratch while maintaining long-horizon coherence? To address the above limitations, we introduce NL2Repo-Bench, benchmark designed to evaluate the longhorizon repository generation capabilities of coding agents. In the NL2Repo-Bench, an agent is provided with single natural-language requirements document and an empty workspace. From this minimal starting point, the agent must autonomously perform architectural design, dependency management, multi-file implementation, and packaging to produce fully functional Python library. Crucially, no project scaffolding, source code, or test cases are revealed during development, forcing the agent to reason globally and persistently across the entire construction process. Besides, evaluation in NL2Repo-Bench is strictly execution-based. Each generated repository is verified against the original upstream pytest suite of real-world open-source project, executed within controlled environment. This design ensures an objective and binary notion of correctness grounded in real software behavior, rather than subjective judgments or proxy metrics. Moreover, the benchmark comprises 104 tasks drawn from diverse application domains and varying complexity levels, with input documents 2 averaging nearly 19k tokens, reflecting the scale and ambiguity of realistic software specifications. Through extensive experiments on state-of-the-art (SOTA) openand closed-source models [3, 4, 10, 15, 20, 25, 26, 30] within multiple agent frameworks [1, 2], we find that long-horizon repository generation remains major unsolved challenge. Even the strongest agents achieve average test pass rates below 40% and rarely succeed in fully reproducing repository. Beyond aggregate performance, our analysis reveals systematic long-horizon failure modes, including premature termination due to overconfidence, loss of global architectural consistency, brittle dependency handling, and an inability to persistently execute and verify plans over extended interaction sequences. By explicitly targeting long-horizon reasoning and execution, NL2Repo-Bench provides missing evaluation axis for coding agents and complements existing function-level [5, 6] and repair-focused benchmarks [13]. We argue that progress on NL2Repo-Bench will require advances beyond larger context windows, including improved agentic planning, robust self-verification loops, and mechanisms for maintaining global coherence over long development trajectories. As such, NL2Repo-Bench serves both as diagnostic tool for current systems and as guiding benchmark for future research on autonomous, long-horizon software engineering. To summarize, our contributions are as follows: We formalize the NL2Repo task as constructing software repository from an empty workspace given only single requirements document, and introduce NL2Repo-Bench, strictly verifiable and long-horizon agentic coding benchmark, which requires the generation of complete, installable Python library that passes the upstream pytest suite. We release reverse-engineered, quality-assured corpus of tasks and standardized evaluation image that isolates environment effects, enabling apples-to-apples comparisons across agents. We provide baseline results with state-of-the-art coding agents, revealing substantial gaps versus repair/completion settings and highlighting open challenges in architecture, dependency management, and cross-file consistency."
        },
        {
            "title": "2.1 Coding Benchmarks for LLMs",
            "content": "Function-level benchmarks such as HumanEval and MBPP primarily assess localized reasoning for isolated programming tasks, abstracting away repository-level constraints [5, 6]. At the repository scale, prior work clusters into three paradigms. (i) Repair/Regression: the SWE-bench series [8, 12, 19, 29, 33] evaluate resolving real issues within existing projects and validating fixes against project test suites. (ii) Completion/Autocompletion: RepoBench [18] measures the capacity to complete incomplete projects by generating missing components within real repositories [17]. (iii) Paper-to-repository Reproduction: PaperBench [24] tests whether agents can replicate AI research by constructing repository and executing experiments based on papers, with performance often judged by LLMs on code and results rather than authoritative upstream tests. In different direction, Commit0 [37] targets from-scratch library generation but provides project structure and function signatures as strong priors. input priors (Existing Repository vs. Scaffolding/Signatures vs. These settings differ along critical axes: Single Document), the evaluation judge (LLM Judgment or Regression Tests vs. Authoritative Upstream Tests), and the required output form (edits within codebase, completion of components, or fully installable packages). Distinct from all of the above, NL2Repo-Bench focuses on repository-level generation from single natural-language document, with no scaffolding or signatures, and uses the upstream repositorys official pytest suite as the sole evaluator."
        },
        {
            "title": "2.2 LLM-Driven Coding Agents",
            "content": "The rise of LLMs has given birth to coding agents that assist or autonomously perform software development. Beyond IDE-embedded assistance, more autonomous approaches have emerged. SWE-agent introduces an AgentComputer Interface (ACI) that abstracts file search, navigation, editing, and testing, enabling LLMs to 3 operate on repository-level tasks and achieve strong results on SWE-bench [32]. OpenHands further generalizes this paradigm as framework for end-to-end autonomous development, allowing agents to plan, code, and validate with minimal human intervention [27]. These systems generally operate within existing codebases. By contrast, NL2Repo-Bench evaluates agents on constructing complete, installable repository from single document, highlighting challenges in architecture design and packaging that are orthogonal to edit/repair settings."
        },
        {
            "title": "2.3 Coding-Oriented LLMs",
            "content": "Large-scale code foundation models form the backbone of advances in automated programming and repositorylevel code generation. Code Llama [22] extends Llama 2 with code-oriented pretraining and long-context support (up to 100k tokens), making it suitable for cross-file reasoning. Open-source model families (e.g., StarCoder-style and DeepSeek-style code models) and recent open cookbooks like OpenCoder [11] further push instruction-following and repository-aware capabilities. Advanced training strategies, including domain knowledge distillation [16] and self-curated data synthesis (e.g., Seed-Coder [36]), along with reasoningenhanced models like Seed-Thinking [23], continue to improve code understanding and generation. While these models improve code understanding and generation, most evaluations still emphasize function-level tasks, underscoring the need for systematic assessment tailored to repository-level generation."
        },
        {
            "title": "3 NL2Repo-Bench",
            "content": "To evaluate repository-level coding abilities with verifiable ground truth, NL2Repo-Bench derives tasks from real-world Python libraries characterized by modular architectures and authoritative pytest suites, as shown in Figure 2. Agents receive only single natural-language specification and must reconstruct the complete repository from scratch, including file structures and functional logic. Correctness is strictly measured by executing the generated code against the original upstream tests. In the following sections, we detail the pipeline for task selection, document generation, and quality assurance (Section 3.1), followed by the statistical characteristics of the resulting dataset (Section 3.2)."
        },
        {
            "title": "3.1 Benchmark Construction",
            "content": "3.1.1 Repository Selection To ensure that each task reflects realistic and sufficiently challenging repository-level generation scenario, we curate set of Python open-source libraries from GitHub as the targets to be reproduced. Our selection procedure follows four principled criteria designed to guarantee task complexity, stability, and testability: Complexity. Each repository must contain 300 to 120,000 lines of code. This range excludes trivial projects while avoiding extremely large systems that exceed the context windows of current coding agents, ensuring tasks remain both meaningful and tractable. Maturity. We require each repository to have at least 10 GitHub stars, which serves as minimal proxy for community adoption, maintenance quality, and functional reliability. Completeness and Testability. repository must include pytest-based test cases, and its official version must successfully pass all of them. This requirement ensures that the behavioral ground truth is both well-defined and verifiable. Recency. Only repositories that have been created or updated within the past three years are considered, allowing NL2Repo-Bench to reflect contemporary coding practices and avoid outdated dependency ecosystems. After initial selection, human annotators clone each candidate project and perform an preliminary review of its structure, dependency, and overall organization. Once they develop sufficient understanding of the repositorys layout and expected behavior, the annotators run the projects built-in test suite. Only repositories that successfully pass all of their native tests at this stage are deemed qualified and retained as target repositories in our benchmark. 4 Figure 2 Construction of NL2Repo-Bench. 3.1.2 Project Document Writing Upon selecting the target repositories, our annotator construct the input specification through systematic reverse-engineering process. The goal is to translate the entire repository, including implementation files, core functionality, module relationship, and test logic, into coherent natural-language (NL) document. This document serves as high-level functional specification that enables developer (or agent) to fully reproduce the repositorys behavior without accessing the source code. The details of tutorials of our reverse engineering process can be found at Appendix C.3. To standardize the task formulation, every specification document in NL2Repo-Bench is structured into four specific sections: Project Description: high-level overview of the repositorys goals, scope, and primary functionality. Supports: Supplementary materials required for development, such as required third-party packages and the expected directory structure. API Usage Guide: Detailed descriptions of the core features to be implemented, including specific requirements for classes, functions, and their expected behaviors. Implementation Nodes: Concrete examples or references for critical APIs (provided where applicable) to ground the agents implementation planning. 5 Among these sections, the API Usage Guide is particularly crucial. Since NL2Repo-Bench evaluates generated repositories using the original test suite from the target repository, this section must provide complete and accurate description of all functional nodes exercised by the tests. Any omission or misaligned API definition would make the task unresolvable for the agent. To guarantee this completeness, we incorporate an AST-assisted annotation workflow. Annotators begin by running an automated AST scanner over the target repository, which extracts structured inventory of all functional elements, including class and function names, signatures, and code locations,etc.. Using this inventory as blueprint, annotators then reconstruct each API entry case-by-case, producing detailed descriptions of expected behaviors, input/output specifications, and semantic constraints. This pipeline ensures that the API Usage Guide is both exhaustive and faithfully aligned with the original repository, enabling reliable long-horizon repository generation tasks."
        },
        {
            "title": "3.1.3 Environment Building",
            "content": "To guarantee deterministic evaluation and isolate code generation quality from environmental variance, we construct dedicated Docker-based execution environment for each task. The provisioning process follows strict ground-truth verification protocol. Initially, the environment is configured according to the official dependency documentation of the upstream repository. We then execute the official test suite within this container to establish baseline. Any execution failures trigger an iterative refinement of the system configuration, such as dependency version pinning or system library adjustments, strictly without modifying the functional source code, until the upstream repository passes all tests. Furthermore, we implement strategy of non-functional constraint relaxation within build configurations. We sanitize build manifests (e.g., setup.py) to eliminate rigid constraints that are irrelevant to functional logic. For instance, mandatory existence checks for auxiliary documentation (such as README.md or license files) are either removed or satisfied via the pre-creation of synthetic artifacts. This ensures that agents are evaluated solely on their ability to generate functional software, rather than their adherence to rigid, non-code build prerequisites. 3.1.4 Verification and Refinement To ensure the quality and reliability of the constructed tasks, we employ multi-stage validation pipeline that integrates both human and automated verification. Our quality assurance framework consists of the following steps: Human Expert Review: Professional Python experts manually verify the fidelity of the source content, the correctness of function signatures(including names, input/output parameters, etc..), and the elimination of hallucinated information, guaranteeing that the document faithfully corresponds to the original codebase. Static Coverage Verification: To ensure that the specification document is sufficiently comprehensive, we perform static coverage analysis over the target repository. We parse the repository using an AST-based tool to extract all core API definitions, including function names, class structures, and method signatures, and verify that each of these APIs is explicitly documented in the specification with accurate signatures and semantics. This ensures that the specification contains all information necessary for models to implement the full repository from scratch. Preliminary Experiment Refinement: We validate the feasibility of each task by running SOTA coding agents using only the provided specification document. The resulting workspaces are packaged and evaluated within the containerized environments constructed in the previous stage. To further ensure the reliability of the benchmark design, senior Python engineer conducts systematic analysis of all failing test cases and exceptions reported during the test stage. By cross-examining these failures against the task documentation and the corresponding execution environment, the engineer identifies which errors stem from latent issues or ambiguities in the specification or environment configuration, rather than from the models capabilities. Based on this diagnosis, we refine the task 6 Table 1 Task categories and statistics of the NL2Repo-Bench. Category Web Development Testing Utility Libraries Machine Learning Data Analysis & Processing Database Interaction Networking Tools Batch File Processing System Tools Overall Count 10 13 11 7 18 7 9 5 24 104 documents and environments accordingly, ensuring that all benchmark failures are attributable to the models reasoning and implementation abilities rather than artifacts of task or environment design. This helps uncover deep semantic ambiguities in the documentation or edge cases in the evaluation environment. task is recognized as valid only upon passing all verification stages. Any task that fails check undergoes iterative refinement and re-evaluation until full compliance is achieved."
        },
        {
            "title": "3.2 Benchmark Statistics",
            "content": "Following the rigorous selection, construction, and validation pipeline described in Section 3.1, we assemble the final NL2Repo-Bench dataset. The benchmark comprises 104 tasks spanning nine distinct categories of Python libraries, as detailed in Table 1. NL2Repo-Bench represents the first evaluation framework designed to assess coding agents on their ability to generate fully-functional Python repositories solely from natural-language descriptions. The average input length of NL2Repo-Bench task is approximately 18,800 tokens, scale that substantially exceeds the input complexity of existing repository-level benchmarks. Beyond domain diversity, NL2Repo-Bench covers wide spectrum of task complexities. We categorize tasks into three difficulty levels, including easy, medium, and hard, based on the original project size and total lines of code. Detailed criteria and statistics for this categorization are provided in Table 2. Table 2 Difficulty statistics of the NL2Repo benchmark. Difficulty Level Easy Medium Hard LOC Range 1500 LOC 15004000 LOC 4000 LOC #Tasks 26"
        },
        {
            "title": "4.1 Experimental Settings\nIn our study, we mainly apply NL2Repo-Bench to evaluate the performance of various models within the\nOpenHands-CodeAct Agent framework1. To ensure the comprehensiveness of our experimental results, the\nevaluation encompassed the following models: open-source models â€” DeepSeek-V3.1, DeepSeek-V3.2[15],",
            "content": "1For model Gemini-3-pro, we apply Cursor-CLI framework since it suffers from agent-in-a-loop error frequently in Openhands framework. 7 Figure 3 Leaderboard of model performance on NL2Repo-Bench benchmark. Pass rates are evaluated on the official test suite of the target repository. While Claude-Sonnet-4.5 based agents (OpenHands, Claude Code) achieve the highest performance, the best pass rate remains little above 40%, highlighting the significant challenge of end-to-end repository generation. Qwen3-235B-Instruct2,Qwen3-235B-Thinking 3 [30], Kimi-k2[26], and GLM-4.6[25]; and closed-source models Claude-Sonnet-4, Claude-Sonnet-4.5 [3, 4], Gemini-3-pro [10] and GPT-5 [20]. Considering the impact of different end-to-end agent frameworks on model performance, we additionally employed two commercial coding agents: Cursor-CLI and Claude Code. For both agents, we selected ClaudeSonnet-4.5 as the underlying language model, ensuring consistent comparison basis across frameworks while isolating the effect of agent design and tool integration on overall task performance. The experimental setup is defined as follows: for each model or agent, the working environment consists of an empty workspace containing only the task specification document. After receiving single initialization instruction from the user, the agent completes the entire task autonomously without any further human input. No additional constraints are imposed on the environment, meaning that the agent is free to invoke any tools available within its system. Considering the complexity of the tasks, we do not set limit on the number of iteration rounds for the agents. Once the agent finishes all development processes, the resulting workspace is packaged and passed into testing image, where the original repositorys pytest suite is executed. The average test pass rate is used as the task score for evaluating each model or agent. To improve the accuracy of the evaluation process, we modified the execution of the original repository tests such that all collected test cases were executed even if errors occurred during the pytest collection stage. This design prevents the entire generated repository from being assigned zero score due to small number of collection errors."
        },
        {
            "title": "4.2 Main Results",
            "content": "As summarized in Figure 3 and Table 3, several noteworthy observations are as follows: Coding agents remain far from being able to synthesize full repositories. 2abbr. Qwen3-Instruct 3abbr. Qwen3-thinking or Qwen3-T 8 Table 3 Main Results: Model performance (Pass Rate %) across difficulty levels. We report the overall pass rate, Pass@1 count, and breakdown by difficulty. Best results are bolded. Unless otherwise specified in parentheses, all models use the OpenHands agent framework. Model Overall Score (%) Pass@1 (Count) Easy (1.5k LOC) Medium (1.5k-4k LOC) Hard (4k LOC) Claude-Sonnet-4.5 (Claude Code) Claude-Sonnet-4.5 Claude-Sonnet-4.5 (Cursor) Claude-Sonnet-4 Gemini-3-pro (Cursor) DeepSeek-V3.2 Kimi-k2 DeepSeek-V3.1 GPT-5 Qwen3-Instruct GLM-4.6 Qwen3-thinking 40.2 39.9 39.2 37.0 34.2 27.6 22.7 22.2 21.7 17.9 17.5 13.8 3 3 4 5 3 1 3 1 1 1 2 1 51.8 55.3 52.9 53.1 44.9 43.1 40.8 35.7 38.4 34.7 34.4 25.0 44.5 43.0 41.4 41.3 40.9 29.1 20.2 21.6 20.7 15.2 15.5 11.3 25.1 21.4 24.8 16.1 16.8 12.9 11.6 12.1 9.6 8.9 6.5 6.5 Table 4 Model performance (Pass Rate %) across different task categories. Model Claude-Sonnet-4.5 (Claude Code) Claude-Sonnet-4.5 Claude-Sonnet-4.5 (Cursor) Claude-Sonnet-4 Gemini-3-pro (Cursor) DeepSeek-V3.2 Kimi-k2 DeepSeek-V3.1 GPT-5 Qwen3-Instruct GLM-4.6 Qwen3-thinking Model Claude-Sonnet-4.5 (Claude Code) Claude-Sonnet-4.5 Claude-Sonnet-4.5 (Cursor) Claude-Sonnet-4 Gemini-3-pro (Cursor) DeepSeek-V3.2 Kimi-k2 DeepSeek-V3.1 GPT-5 Qwen3-Instruct GLM-4.6 Qwen3-thinking Web Dev 56.9% 37.3% 44.0% 48.2% 33.8% 33.6% 29.5% 21.2% 26.0% 18.2% 17.7% 15.5% Testing 30.4% 31.9% 33.0% 27.5% 28.2% 21.7% 18.7% 16.0% 18.6% 15.2% 14.4% 9.1% DB Interact. Network 41.3% 44.4% 46.9% 33.1% 40.8% 22.5% 23.8% 25.0% 23.4% 24.4% 28.9% 16.6% 23.4% 30.5% 25.4% 23.4% 28.3% 16.9% 15.6% 14.8% 10.4% 14.2% 0.6% 6.4% Utility Libs 59.8% 52.4% 53.8% 51.2% 43.9% 44.4% 27.3% 30.8% 28.0% 18.7% 29.5% 19.2% Batch Ops 43.3% 43.7% 35.0% 32.0% 44.0% 36.4% 37.0% 33.7% 18.9% 32.6% 20.5% 16.3% ML 19.7% 16.2% 9.4% 8.5% 10.0% 11.1% 7.5% 10.5% 7.0% 8.0% 8.4% 7.4% Sys Tools 43.6% 50.3% 49.7% 43.2% 45.2% 34.1% 22.3% 28.0% 20.2% 17.6% 21.0% 15.8% Data Proc. 36.7% 36.8% 34.6% 41.3% 26.0% 20.8% 24.8% 18.2% 31.6% 18.9% 14.1% 11.6% - - - - - - - - - - - - - As shown in Table 3, all models achieve an average test pass rate below 40.5%, and nearly half of them fall below the 20% threshold. Across the entire set of 104 tasks, the strongest model fully passes the official pytest suite for only five repositories in single run (Pass@1). These results indicate that current LLMs and coding agents still lack the robustness, long-horizon planning ability, and cross-file consistency required to generate complete repository from scratch. Even the best-performing systems struggle to construct end-to-end runnable software purely from natural-language specifications, underscoring the substantial gap that remains in achieving reliable repository-level synthesis. Claude outperforms other models. Across all evaluated models, the Claude series (including Claude-Sonnet-4 and 4.5) demonstrates clear performance advantage over the others. This superiority is closely tied to its extremely large context window (up to 1M tokens, compared with the 256K-token limits of most other 9 models) and its ability to sustain long interaction traces without losing track of prior decisions. Given the substantial size of both the input text and the code that must be generated, models with larger context capacities naturally possess significant advantage, pattern we analyze in more detail in Section 4.3.5. GPT-5 underperforms expectations. Despite GPT-5s strong performance on other coding-related tasks, its results on NL2Repo-Bench are comparatively weaker. Our case studies suggest potential explanation: GPT-5 tends to halt prematurely before completing the repository, and waits for additional user input. This observation aligns with our quantitative analysis as well: GPT-5 exhibits the lowest average number of interaction turns, being the only model with fewer than 100 turns, and its turn count is substantially lower than that of other models, as shown in Table 6. These behaviors suggest that GPT-5 is better aligned with human-in-the-loop assistance than with fully autonomous repository generation. Performance degrades with repository complexity. To further examine this trend, we compute model scores across tasks of different difficulty levels. clear pattern emerges: as task difficulty increases, the performance of nearly all models declines substantially. This monotonic degradation confirms that the NL2Repo-Bench difficulty hierarchy effectively captures the intrinsic complexity of real-world, repository-level software development. Moreover, the sharp performance drop on harder tasks highlights fundamental limitation of current LLM-based coding agents, as they remain far from mastering long-horizon reasoning, multi-module coordination, and dependency-rich engineering workflows that are essential for constructing coherent, large-scale codebases. Performance varies substantially across task categories. Table 4 further breaks down results by library category, revealing that models like Claude-Sonnet-4.5 are particularly strong on system tools and database interaction tasks, while all models struggle more on machine learning and networking tasks. This pattern suggests that current agents handle infrastructure-style repositories with clearer packaging and dependency structure better than domains that require reproducing complex algorithmic pipelines or protocol-heavy logic. Same baseline model behaves similarly across different frameworks. From Table 3, we observe that ClaudeSonnet-4.5 exhibits less than 1% performance variation across the three agent frameworks, difference that is negligible given the complexity of NL2Repo-Bench, where each task requires constructing full repository and passing multiple test cases. In contrast, inter-model performance gaps are substantially larger, indicating that the dominant factor driving NL2Repo-Bench outcomes is the intrinsic capability of the underlying LLM, rather than the specific framework used to orchestrate tool interactions. This result suggests that NL2Repo-Bench behaves primarily as model-centric benchmark, where improvements to base-model reasoning and generation ability yield far greater gains than modifications to agent-level strategies."
        },
        {
            "title": "4.3 Analysis and Discussions",
            "content": "4.3.1 Tools Used During Development We analyze the distribution of tool calls across all models within the OpenHands framework, as illustrated in Figure 4. The most frequently used tools during the development process are: execute_bash, str_replace_editor, and task_tracker, which are primarily used for file management & testing, code editing, and planning, respectively. We computed the correlation coefficients between the total number of tool invocations and model scores for all models, as shown in Table 5. Among the three most frequently used tools, task_tracker exhibits the strongest correlation with model performance (0.711), underscoring the central role of effective task planning in constructing large-scale code repositories. In contrast, although the think tool shows high raw correlation, its low average invocation frequency suggests that this metric may not robustly reflect causal relationship with overall performance in this specific setting. 4.3.2 Interaction Turns and Model Performance critical factor distinguishing model performance on NL2Repo-Bench is the number of interaction turns required to complete repository generation. As shown in Table 6, different models exhibit markedly different interaction patterns. Figure 4 The total times using different tools for models in NL2Repo-Bench Tasks. The Dominance of Premature and Incomplete Task Completion in GPT-5. striking finding is GPT-5s significantly lower turn countaveraging only 78.4 turns, which is merely 42% of Claude-4.5s 181.6 turns and the lowest among all evaluated models. Despite this brevity, GPT-5 achieves moderate score of 0.217, suggesting high per-turn quality but insufficient task completion. Our case analysis reveals that GPT-5 frequently halts prematurely and awaits user confirmation, with 13.4% of tasks exhibiting early termination behavior. This pattern indicates that GPT-5s design prioritizes human-in-the-loop collaboration over fully autonomous task completion, misaligning with NL2Repos requirement for end-to-end repository generation without human intervention. 11 Table 5 Correlation between tool invocation frequency and model performance. Tool Correlation #Models Not Using This Tool think task_tracker finish execute_bash browser execute_ipython_cell fetch str_replace_editor 0.816 0.711 0.512 0.371 -0.264 0.402 -0.1917 0.161 4 1 0 0 3 3 7 0 Table 6 Interaction turns statistics and model performance. Model Claude-Sonnet-4.5 Claude-Sonnet-4 Kimi-k2 DeepSeek-V3.1 DeepSeek-V3.2 GPT-5 Qwen3-Instruct GLM-4.6 Qwen3-Thinking Avg. Turns 181.6 180.7 275.1 202.3 254.3 78.4 212.9 138.6 70.2 Std. Dev. 64.1 80.3 164.6 170.5 119.9 29.1 166.0 110.7 40.9 Max Turns 394 416 878 990 822 165 940 533 246 Score (%) 39.9 37.0 22.7 22.2 22.2 21.7 17.9 17.5 13.8 Turns/ Score 455.1 505.4 1211.9 919.6 1145.5 361.2 1225.4 825.0 508. Efficiency vs. Persistence Trade-off. Apart from GPT-5, Claude4.5 achieves the best balance between efficiency and performance (turns/score ratio of 455.1), completing tasks with moderate turn counts while maintaining the highest test pass rate. In contrast, DeepSeek-V3.2 employs more turns (254.3) within openhands framework but with mixed resultsDeepSeek-V3.2 achieves the third-best score (27.6%), but with high cost. This suggests that simply increasing interaction attempts does not guarantee success; the quality of planning and execution strategy matters more. 4.3.3 The Early Termination and Non-Finish Problem Failure to complete the repository structure is primary cause of low pass rates. We categorize incomplete tasks into two distinct behaviors: Early Termination and Non-Finish. Early Termination (Overconfidence): The agent explicitly invokes the finish action, claiming the task is done, but does so prematurely (defined here as fewer than 100 interaction turns). This typically indicates false positive estimation of progress. Non-Finish (Passive Failure): The agent never invokes the finish tool. Instead, the session ends because the agent halts to await user input (e.g., asking for clarification or confirmation) or reaches system timeout. This behavior reflects lack of agency or an inability to proceed autonomously. Figure 5 quantifies these phenomena. The Non-Finish Rate is calculated as the percentage of tasks where the agent never invoked the finish tool out of the total 104 tasks, while the Early-Stop-Rate refers to the percentage of tasks ending within 100 iteration rounds by invoking finish tool. The Thinking Model Paradox. Qwen3-Thinking terminates early in 49.0% of tasks. This high rate suggests that the models internal reasoning mechanism leads to premature confidence. Rather than verifying implementation details against the requirements, the model thinks it has completed the work, resulting in incomplete repositories. We hypothesize that this behavior stems from hallucination of verification within the models internal monologue. The thinking process acts as self-reinforcing echo chamber where the model convinces itself of the correctness of its code through reasoning chains, effectively bypassing the need for actual execution and testing. This internal success masks the external failure, leading to the observed premature termination. 12 Figure 5 Early termination rates across models. Thinking models (e.g., Qwen3-T) exhibit an alarmingly high early stop rate of 49.0% and non-finish rate of 46.2%, leading to unsatisfactory scores. The Human-in-the-Loop Dependency of GPT-5. striking finding is the dichotomy between GPT-5s low Early Stop rate (13.4%) and its massive Non-Finish rate (84.5%). Unlike thinking models that confidently fail, GPT-5 tends to halt and wait for user guidance (e.g., \"I have set up the structure, should proceed?\"). In our fully autonomous benchmark, where no human feedback is provided, this conservative strategy is fatal. It indicates that GPT-5 is aligned more towards collaborative assistance than autonomous execution. Persistent Models. Claude-Sonnet-4 demonstrates the most robust agentic behavior, with 1.9% Non-Finish rate and 0 Early Stop rate. It consistently drives the development process to conclusion, which correlates with its superior performance. 4.3.4 Tool Usage Patterns and Efficiency We analyze tool invocation patterns to understand how different models approach repository generation. Table 7 and Figure 6 present tool usage statistics. Table 7 Tool usage statistics across models (104 tasks). Tool Claude -Sonnet-4 str_replace_editor execute_bash task_tracker Other Total Avg/Task 4623 2766 1185 8880 85.38 DeepSeek GPT-5 GLM Deepseek -v3.1 5304 3528 819 217 9868 94.88 -4. 3955 1781 735 79 6530 62.79 1861 1148 516 42 3567 34.30 -V3.2 6333 4205 1447 12541 120.59 Qwen3 -Instruct Qwen3 Kimi -T -k2 Claude -Sonnet-4.5 6379 3382 554 10429 100.28 2143 934 0 57 3134 30.13 7134 4642 994 378 4284 3293 1045 320 13148 126. 8942 85.98 Code Editing vs. Execution Ratio. All models allocate the majority of tool calls to code editing (str_replace_editor), ranging from 50-62% of total calls. The edit-to-execution ratios reveal different development strategies: Claude4.5 (1.67:1), GPT-5 (1.62:1), and Qwen3-Thinking (2.29:1) maintain similar ratios, while DeepSeek-V3.2 uses lower ratio (1.51:1), suggesting more frequent testing and validation cycles. Task Planning and Organization. notable differentiator is task_tracker usage, which reflects systematic planning capabilities. GPT-5 devotes 14.5% of calls to task trackingthe highest among all modelsfollowed by Claude-Sonnet-4 (14.2%). In stark contrast, Qwen3-Thinking allocates 0 to planning, relying instead on 13 Figure 6 Tool usage distribution across models. All models allocate 50-60% of calls to code editing, 27-35% to command execution, but differ significantly in task planning usage. its internal reasoning mechanism. This disparity correlates with performance: models with higher planning tool usage (Claude, GPT-5) achieve better scores, while those neglecting explicit planning (Qwen3-Thinking) suffer from premature termination and incomplete implementations. Quantity vs. Quality Trade-off. Figure 7 plots total tool calls against final scores, revealing non-monotonic relationship. GPT-5 achieves moderate score (0.217) with the fewest calls (3567), demonstrating high per-call quality. Kimi-k2 makes the most calls (13148), but with divergent outcomes, indicating that the sheer quantity of attempts cannot compensate for poor strategy. Claude-4.5-Sonnets 8942 calls yield the best score, establishing an efficiency baseline. 4.3.5 Impact of Context Window Size Context window capacity emerges as crucial factor in NL2Repo-Bench performance. Models can be broadly categorized by their context capabilities: those with ultra-long context ( 1M tokens, e.g., Claude series, Gemini-3-pro) and those with standard context (e.g., DeepSeek, GPT-5, Qwen). Figure 8 visualizes performance differences. The Long-Context Advantage. The top tier of the leaderboard is exclusively occupied by long-context models. Claude-Sonnet-4.5 (40.2%) and Gemini-3-pro (34.2%) substantially outperform the standard-context cohort. This advantage stems from NL2Repos demanding context requirements: average task documents contain 18,800 tokens, generated code can span 10,000-50,000 tokens, and 180 average interaction turns accumulate approximately 90,000 tokens of conversation history. While 128K capacity theoretically suffices for snapshots, the 1M+ window provides critical headroom for maintaining full context throughout extended development 14 Figure 7 Tool call efficiency: quantity vs. quality. The dashed line represents Claudes efficiency baseline. GPT-5 operates above this line (high efficiency but low volume), while most other models fall below it. Figure 8 Impact of context window size on performance. The 1M context model (Claude) substantially outperforms 256K models. GPT-5 (256K+) underperforms despite larger context, suggesting context size is necessary but not sufficient. sessions, enabling better cross-file consistency and architectural coherence. Context Size Is Necessary But Not Sufficient. However, large context window does not guarantee superior performance. For instance, despite supporting long contexts, Kimi-k2 achieves pass rate of only 22.7%, lagging behind DeepSeek-V3.2 (27.6%) which operates within 128K limit. Similarly, GPT-5 (21.7%) underperforms compared to the top-tier models and even open-source baselines. This suggests that while 15 context capacity provides the potential for maintaining global coherence, the models underlying reasoning capability and agentic behavior (e.g., persistence vs. early termination) are equally critical determinants of success. Context Amplifies Planning Effectiveness. While explicit planning (via task_tracker) is strategy employed by several models regardless of their context sizeDeepSeek-V3.2 (11.5%), GPT-5 (14.5%), and ClaudeSonnet-4.5 (11.7%) all exhibit high usagethe outcomes differ significantly. Standard-context models like DeepSeek utilize planning tools frequently but achieve lower success rates (27.6%), suggesting that without massive context window to retain the full history of the plan and its execution states, the effectiveness of planning diminishes over long horizons. In contrast, Claudes 1M+ window allows it to maintain persistent and coherent view of the task lifecycle, maximizing the utility of its planning actions."
        },
        {
            "title": "4.3.6 Agentic Workflow Patterns",
            "content": "Beyond aggregate tool usage, the sequence of actions reveals the agents underlying reasoning strategy. We analyzed the transition probabilities between consecutive tool calls to identify distinct workflow patterns. The Edit-Test Loop. High-performing models like Claude-Sonnet-4.5 exhibit strong cyclic pattern between str_replace_editor and execute_bash (specifically pytest). This Edit-Test loop indicates TestDriven Development (TDD) or rapid feedback strategy, where the agent verifies changes immediately after implementation. In contrast, lower-performing models show high transition probabilities between The Navigation Trap. execute_bash (ls/cd) and read_file, often without intervening edit actions. This Navigation Loop suggests the agent is struggling to locate relevant files or build mental map of the repository, leading to wasted context and interaction turns. Blind Editing. We also observed Blind Editing pattern in some models, characterized by consecutive str_replace_editor calls without intermediate testing. This often leads to accumulated errors that are difficult to debug later in the session. 4.3.7 Failure Taxonomy To understand why agents fail, we categorized the error types encountered during the evaluation of generated repositories. Unlike function-level benchmarks where AssertionError (logic errors) dominates, NL2RepoBench reveals more complex failure landscape. Environment and Dependency Issues. significant portion of failures stem from ImportError or ModuleNotFound exceptions. This highlights key challenge in repository-level generation: agents often struggle to correctly structure the package (e.g., missing __init__.py) or manage internal dependencies between modules, resulting in code that is logically correct but structurally broken. Test Suite Alignment. Another common failure mode is the mismatch between the agents implementation and the official test suites expectations (e.g., function signatures or class attributes). This suggests that while agents can follow the natural language instructions, they may miss subtle constraints required by the pre-existing tests."
        },
        {
            "title": "4.4 Ablation Studies",
            "content": "We conduct two ablation studies to disentangle how specific evaluation conditions influence model performance on NL2Repo. These analyses clarify whether observed failures arise from benchmark constraints or from fundamental limitations in current coding agents. 4.4.1 Impact of interaction round limits The main experiments permit unlimited interaction rounds, allowing agents to iteratively refine the repository without budget constraints. To assess whether high scores depend on extensive trial-and-error rather than genuine reasoning, we re-evaluate each model under varying maximum round limits. This ablation quantifies 16 the degree to which agents rely on long iterative loops and examines their effectiveness under more realistic, bounded interaction settings. As shown in Fig. 9, model performance increases steadily across all difficulty levels as the interaction limit is raised from 50 to 200 rounds. Once the limit reaches 200 roundswhich is slightly above Claude-Sonnet-4.5s average number of interactions under the unrestricted settingfurther expanding the maximum round budget yields only marginal improvements. These findings indicate that while interaction budget matters in the low-round regime, model performance quickly saturates once the budget exceeds its natural working range, suggesting that the primary limitations lie in semantic reasoning, architectural planning, and cross-file consistency rather than in the sheer number of allowed interaction steps. Figure 9 The effect of limitations on iteration rounds on the performance of Claude-Sonnet-4.5 on different difficulty level tasks. 4.4.2 Impact of revealing all test cases. By default, agents only receive the natural-language specification, while the pytest suite remains hidden to mimic real development settings. To estimate an upper bound on performance and determine how much models struggle with implicit requirement inference, we run an additional condition where all test cases are made visible during generation. Comparing the two settings reveals whether performance bottlenecks stem primarily from semantic understanding of the NL document or from deeper challenges in implementing coherent, multi-file system even with full supervision. Results in Table 8 show that exposing the full test suite yields substantial performance gains. For example, the pass rate of Claude-Sonnet-4.5 (in Claude Code framework) increases markedly from 40.2% to 59.4%, and the number of fully-passed tasks jumps from 3 to 18. This trend is consistent with the expected benefit of providing test cases, which can guide and constrain both the development process and the resulting implementation. However, it is equally noteworthy thateven under this cheating scenario where evaluation-phase tests are made available during developmentthe models overall score still does not exceed 60%. This indicates that, despite the advantages of test-driven development, generating fully functional, end-to-end runnable repository from scratch remains substantial challenge for current coding agents, pointing to fundamental limitations in long-horizon coordination and large-scale code synthesis rather than merely missing supervision signals."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce NL2Repo-Bench, the first benchmark to rigorously evaluate the capability of LLMs and agents to generate complete, installable Python repositories from scratch, starting from single natural language 17 Table 8 Comparison of the performance of Claude-Sonnet-4.5(Claude Code) on whether revealing all test cases. Inputs Easy Medium Hard Overall Avg Score (%) Pass@1 Avg Score (%) Pass@1 Avg Score (%) Pass@1 Avg Score (%) Pass@1 Document only Document + unittest 51.8 73.2 1 9 44.5 67.5 1 25.1 35.6 1 2 40.2 59.4 3 18 document. By separating development from evaluation and using authoritative upstream test suites, we provide strict measure of autonomous software engineering capability. Our extensive experiments reveal substantial gap between current SOTA models and the requirements of repository-level generation. While models like Claude-Sonnet-4.5 show promise with 39.6% pass rate, all agents struggle with the complexity of long-horizon planning and cross-file consistency. We identified two critical failure modes: the overconfidence of thinking models leading to early termination, and the collaborative bias of models like GPT-5 that fail to proceed autonomously. NL2Repo-Bench serves as testbed for the next generation of coding agents. We believe that progress in this domain will require not just larger context windows, but architectural innovations in agentic planning, self-correction loops, and reliable environment management. We release the NL2Repo benchmark, including the dataset, docker environments, and evaluation toolkit, to the research community."
        },
        {
            "title": "6 Contributions",
            "content": "Leading Authors(Î±-Î² order) Jingzhe Ding, Shengda Long, Changxin Pu, Ge Zhang, Huan Zhou Core Contributors (Î±-Î² order) Hongwan Gao, Xiang Gao, Chao He, Yue Hou ,Fei Hu, Zhaojian Li, Weiran Shi, Zaiyuan Wang, Daoguang Zan, Chenchen Zhang, Xiaoxu Zhang Contributors (Î±-Î² order) Qizhi Chen, Xianfu Cheng, Bo Deng, Qingshui Gu, Kai Hua, Juntao Lin, Pai Liu, Mingchen Li, Xuanguang Pan, Zifan Peng, Yujia Qin, Yong Shan, Zhewen Tan, Weihao Xie, Zihan Wang, Yishuo Yuan, Jiayu Zhang, Enduo Zhao, Yunfei Zhao, He Zhu, Chenyang Zou Sponsor Commitee(Î±-Î² order) Ming Ding, Jianpeng Jiao, Jiaheng Liu, Minghao Liu, Qian Liu, Chongyao Tao, Jian Yang, Tong Yang, Zhaoxiang Zhang Corresponding XinJie Chen (chenxinjie.bj@bytedance.com) Wenhao Huang (huang.wenhao@bytedance.com) Ge Zhang (zhangge.eli@bytedance.com) Corresponding author"
        },
        {
            "title": "References",
            "content": "[1] Cursor AI. Cursor: The ai code editor. https://www.cursor.com, 2024. [2] Anthropic. Claude code: Ai-powered coding assistant. https://www.claude.com/product/claude-code, 2024. [3] Anthropic. System card: Claude opus 4 & claude sonnet 4. https://www-cdn.anthropic.com/ 4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf, 2025. Accessed: 2025-12-09. [4] Anthropic. System card: Claude sonnet 4.5. https://assets.anthropic.com/m/12f214efcc2f457a/original/ Claude-Sonnet-4-5-System-Card.pdf, 2025. Accessed: 2025-12-09. [5] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. ArXiv preprint, abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.07732. [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [7] Jason Chou, Ao Liu, Yuchi Deng, Zhiying Zeng, Tao Zhang, Haotian Zhu, Jianwei Cai, Yue Mao, Chenchen Zhang, Lingyun Tan, et al. Autocodebench: Large language models are automatic code benchmark generators. arXiv preprint arXiv:2508.09101, 2025. [8] Xiang Deng, Jeff Da, Edwin Pan, Yannis Yiming He, Charles Ide, Kanak Garg, Niklas Lauffer, Andrew Park, Nitin Pasari, Chetan Rane, et al. Swe-bench pro: Can ai agents solve long-horizon software engineering tasks? arXiv preprint arXiv:2509.16941, 2025. [9] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. Classeval: manually-crafted benchmark for evaluating llms on class-level code generation. arXiv preprint arXiv:2308.01861, 2023. [10] Google Cloud. Gemini 3 Pro Model Card, 12 2025. URL https://cloud.google.com/vertex-ai/generative-ai/ docs/models/gemini/2-5-pro. [11] Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, Yang, JH Liu, Chenchen Zhang, Linzheng Chai, et al. Opencoder: The open cookbook for top-tier code large language models. arXiv preprint arXiv:2411.04905, 2024. [12] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In ICLR, 2024. [13] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VTF8yNQM66. [14] Yuhang Li, Chenchen Zhang, Ruilin Lv, Ao Liu, Ken Deng, Yuanxing Zhang, Jiaheng Liu, Wiggin Zhou, and Bo Zhou. Relook: Vision-grounded rl with multimodal llm critic for agentic web coding. arXiv preprint arXiv:2510.11498, 2025. [15] Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, et al. Deepseek-v3.2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025. [16] Jiaheng Liu, Chenchen Zhang, Jinyang Guo, Yuanxing Zhang, Haoran Que, Ken Deng, Jie Liu, Ge Zhang, Yanan Wu, Congnan Liu, et al. Ddk: Distilling domain knowledge for efficient large language models. Advances in Neural Information Processing Systems, 37:9829798319, 2024. [17] Jiaheng Liu, Ken Deng, Congnan Liu, Jian Yang, Shukai Liu, He Zhu, Peng Zhao, Linzheng Chai, Yanan Wu, JinKe JinKe, et al. M2rc-eval: Massively multilingual repository-level code completion evaluation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1566115684, 2025. [18] Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto-completion systems. arXiv preprint arXiv:2306.03091, 2023. 20 [19] Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can frontier llms earn $1 million from real-world freelance software engineering? In Forty-second International Conference on Machine Learning, 2025. [20] OpenAI. Gpt-5 system card. Technical report, OpenAI, August 2025. URL https://cdn.openai.com/ gpt-5-system-card.pdf. Accessed: 2025-09-13. [21] Tejal Patwardhan, Rachel Dias, Elizabeth Proehl, Grace Kim, Michele Wang, Olivia Watkins, SimÃ³n Posada Fishman, Marwan Aljubeh, Phoebe Thacker, Laurance Fauconnet, et al. Gdpval: Evaluating ai model performance on real-world economically valuable tasks. arXiv preprint arXiv:2510.04374, 2025. [22] Baptiste Roziere, Loubna Ben Allal, Jie Li, Lewis Tunstall, Leandro von Werra, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. [23] ByteDance Seed, Yufeng Yuan, Yu Yue, Mingxuan Wang, Xiaochen Zuo, Jiaze Chen, Lin Yan, Wenyuan Xu, Chi Zhang, Xin Liu, et al. Seed-thinking-v1. 5: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. [24] Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, et al. Paperbench: Evaluating ais ability to replicate ai research. In Forty-second International Conference on Machine Learning, 2025. [25] 5 Team, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li, Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, and Jie Tang. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models, 2025. URL https://arxiv.org/abs/2508.06471. [26] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming 21 Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. [27] Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, 2024. [28] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for AI software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=OJd3ayDDoF. [29] Jingxuan Xu, Ken Deng, Weihao Li, Songwei Yu, Huaixi Tang, Haoyang Huang, Zhiyi Lai, Zizheng Zhan, Yanan Wu, Chenchen Zhang, et al. Swe-compass: Towards unified evaluation of agentic coding abilities for large language models. arXiv preprint arXiv:2511.05459, 2025. [30] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [31] Jian Yang, Wei Zhang, Shark Liu, Jiajun Wu, Shawn Guo, and Yizhi Li. From code foundation models to agents and applications: practical guide to code intelligence. arXiv preprint arXiv:2511.18538, 2025. [32] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. [33] Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, et al. Multi-swe-bench: multilingual benchmark for issue resolving. arXiv preprint arXiv:2504.02605, 2025. [34] Alexander Zhang, Marcus Dong, Jiaheng Liu, Wei Zhang, Yejie Wang, Jian Yang, Ge Zhang, Tianyu Liu, Zhongyuan Peng, Yingshui Tan, et al. Codecriticbench: holistic code critique benchmark for large language models. arXiv preprint arXiv:2502.16614, 2025. [35] Chenchen Zhang, Yuhang Li, Can Xu, Jiaheng Liu, Ao Liu, Shihui Hu, Dengpeng Wu, Guanhua Huang, Kejiao Li, Qi Yi, et al. Artifactsbench: Bridging the visual-interactive gap in llm code generation evaluation. arXiv preprint arXiv:2507.04952, 2025. [36] Yuyu Zhang, Jing Su, Yifan Sun, Chenguang Xi, Xia Xiao, Shen Zheng, Anxiang Zhang, Kaibo Liu, Daoguang Zan, Tao Sun, et al. Seed-coder: Let the code model curate data for itself. arXiv preprint arXiv:2506.03524, 2025. [37] Wenting Zhao, Nan Jiang, Celine Lee, Justin Chiu, Claire Cardie, Matthias GallÃ©, and Alexander Rush. Commit0: Library generation from scratch. In The Thirteenth International Conference on Learning Representations, 2024."
        },
        {
            "title": "Appendix",
            "content": "A Difficulty Level in NL2Repo Tasks We classify the difficulty of tasks in NL2Repo based on the number of lines of code (LOC) in the original repository. As shown in Table 2, repositories with fewer than 1,500 LOC are categorized as Easy, while those exceeding 4,000 LOC are categorized as Hard. Repositories falling between these thresholds are considered Medium."
        },
        {
            "title": "B Available Tools in OpenHands CodeAct Framework",
            "content": "In our experiments, agents interact with the environment using the following standardized tools provided by the OpenHands CodeAct framework: execute_bash: Executes bash command in persistent shell session. This tool is essential for file navigation, package installation, and running test suites. think: Enables the agent to articulate internal reasoning traces and plan next steps without executing any changes in the environment. finish: Signals the completion of the current task. browser: Allows the agent to interact with web browser using Python code (e.g., for documentation lookup). execute_ipython_cell: Runs cell of Python code in an interactive IPython environment. task_tracker: Provides structured task management capabilities, allowing the agent to view, add, and update the status of development tasks. str_replace_editor: custom file editing tool designed for viewing, creating, and editing files. It uses strict string matching to ensure precise code modifications. fetch: Retrieves content from specified URL and optionally extracts it as markdown. create_pr / create_mr: Tools to simulate the submission of Pull Request or Merge Request on platforms like GitHub, GitLab, or Bitbucket. Tutorial for NL2Repo-Bench Annotators This section provides the guidelines and step-by-step workflow used by annotators to construct task specifications for NL2Repo. The goal is to ensure that all specifications are (1) semantically faithful to the original repository, (2) sufficiently comprehensive for end-to-end repository development, and (3) consistent across annotators. The process consists of three major phases: project selection, repository comprehension and environment validation, and structured specification writing. The procedures summarized below are derived from the operational annotation guidelines used in practice.4 C.1 Phase 1: Project Selection Following the criteria in Section 3.1.1 of the main paper, annotators first determine whether candidate GitHub repository is eligible for inclusion. This includes checking project maturity, testing completeness, license compatibility, and the feasibility of isolating the core functionality. 4Summary based on internal annotation draft. 23 C.2 Phase 2: Repository Understanding & Test Validation Annotators must obtain precise and executable understanding of the target repository before writing its specification. (1) Local Setup and Preliminary Analysis. Download the repository locally from GitHub. Conduct an initial review of the project, including its purpose, core functionality, directory structure, and external dependencies. (2) Environment Construction. Annotators create an isolated environment based on the repositorys documentation (e.g., README, requirements.txt, setup.py, or pyproject.toml). All relevant dependencies, including possible undocumented ones, must be installed. (3) Full Test Execution. Annotators must run all existing test cases in the repository. repository is considered valid only if all tests pass. If failures occur, annotators must diagnose and resolve environment-related issues (e.g., Python version mismatches, incompatible dependency versions, missing system packages). Repositories with unresolvable failures are excluded. C.3 Phase 3: Task Specification Construction Each task requires comprehensive specification consisting of three parts: project-level description, support information, and an API-level usage guide. C.3.1 Project Description Annotators provide high-level functional summary of the repository, describing: the overall purpose and design of the project; its major components and interaction patterns. Annotators may reference official documentation or use LLM-based tools to draft an initial summary, but the final text must be manually verified to ensure factual correctness. C.3.2 Support Information (1) Third-Party Dependencies. Annotators list all external libraries needed to run the project, including explicit dependencies and any additional packages required for the tests to pass. Version numbers must be preserved where applicable. (2) Repository File Structure. complete directory layout of the target codebase is included (excluding the testing directory). All files relevant to the implementation must be listed to ensure that the model can reconstruct the same structure during development. C.3.3 API Usage Guide This component provides natural-language documentation for all functional units in the repository. (1) Static Extraction of Functional Nodes. Annotators run static analysis tool to extract: all classes, functions, and constants; their names and signatures; their file locations. Test directories are excluded from the scan. (2) Node-Level Documentation. For each functional node, annotators write standalone description containing: 24 the name and purpose of the node; input arguments and return values (with additional explanation if complex); functional description aligned strictly with the implementation. Text may be drafted with AI tools for assistance, but annotators must manually verify that every detail matches the underlying source code exactly. (3) Module Import Instructions. Annotators provide examples of how modules and APIs should be imported. These import paths are derived from the projects internal import patterns as observed in the test files (excluding imports from external libraries). C.4 Implemention Nodes Annotators provide concrete examples or references for critical APIs (provided where applicable) within the target repository to help understand the specific usage of APIs. When completing this part, annotators could refer to the examples in the repository. C.5 Quality Requirements All descriptions must be: fully consistent with the repositorys implementation; complete with respect to all functional nodes; free of speculative or missing behaviors not grounded in the source code. This ensures that the resulting specification is both comprehensive and faithful, enabling an agent to implement the entire repository solely from the provided instructions."
        }
    ],
    "affiliations": [
        "2077AI",
        "Beihang University",
        "Beijing University of Posts and Telecommunications",
        "ByteDance",
        "Humanlaya Data",
        "M-A-P",
        "Nanjing University",
        "Peking University"
    ]
}