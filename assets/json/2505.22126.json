{
    "paper_title": "SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model",
    "authors": [
        "Yifan Chang",
        "Yukang Feng",
        "Jianwen Sun",
        "Jiaxin Ai",
        "Chuanhao Li",
        "S. Kevin Zhou",
        "Kaipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent years have seen rapid advances in AI-driven image generation. Early diffusion models emphasized perceptual quality, while newer multimodal models like GPT-4o-image integrate high-level reasoning, improving semantic understanding and structural composition. Scientific illustration generation exemplifies this evolution: unlike general image synthesis, it demands accurate interpretation of technical content and transformation of abstract ideas into clear, standardized visuals. This task is significantly more knowledge-intensive and laborious, often requiring hours of manual work and specialized tools. Automating it in a controllable, intelligent manner would provide substantial practical value. Yet, no benchmark currently exists to evaluate AI on this front. To fill this gap, we introduce SridBench, the first benchmark for scientific figure generation. It comprises 1,120 instances curated from leading scientific papers across 13 natural and computer science disciplines, collected via human experts and MLLMs. Each sample is evaluated along six dimensions, including semantic fidelity and structural accuracy. Experimental results reveal that even top-tier models like GPT-4o-image lag behind human performance, with common issues in text/visual clarity and scientific correctness. These findings highlight the need for more advanced reasoning-driven visual generation capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 6 2 1 2 2 . 5 0 5 2 : r SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model Yifan Chang1,2 Yukang Feng 2,3 Jianwen Sun2,3 Jiaxin Ai2,4 Chuanhao Li5 S. Kevin Zhou1 Kaipeng Zhang2,5 1 University of Science and Technology of China 4 Wuhan University 3 Nankai University 2 Shanghai Innovation Institute 5 Shanghai AI Laboratory"
        },
        {
            "title": "Abstract",
            "content": "Recent years have witnessed rapid progress in AI-driven image generation. Early diffusion-based methods focused on perceptual quality, while more recent multimodal modelssuch as GPT-4o-imagehave begun integrating high-level reasoning into the generation process, demonstrating stronger capabilities in semantic understanding and structural composition. Scientific research illustration generation stands at the forefront of this evolution. Unlike general-purpose image synthesis, this task requires models to accurately interpret complex technical descriptions and transform abstract structures into clear and standardized visual representations. It is significantly more knowledge-intensive than ordinary image generation. According to recent surveys, producing single research figure typically demands several hours of manual work, often accompanied by expensive software tools and repeated revisions. Automating this process in controllable and intelligent manner would therefore yield substantial practical benefits. However, no benchmark currently exists to systematically evaluate AI performance on this task. To address this gap, we present SridBench, the first benchmark designed to assess multimodal models on scientific figure generation. It consists of 1,120 instances via human experts and multimodal large language models(MLLM) on the authoritative scientific paper website which spanning 13 disciplines under natural science and computer science, with each sample evaluated along six well-designed dimensions including semantic fidelity and structural accuracy. Our experiments show that even state-of-the-art models like GPT-4o-image fall far short of human-level performance. At present, the lack of text and visual information and scientific errors are the main bottlenecks of GPT-4o-image for scientific illustration drawing, underscoring the need for further advances in reasoning-driven visual generation."
        },
        {
            "title": "Introduction",
            "content": "Scientific illustration are essential tools for communicating research findings. They translate complex frameworks, data, and experimental procedures into intuitive visuals, playing central role in both scholarly publications and scientific discourse. However, creating high-quality illustrations is timeconsuming, labor-intensive, and often requires proficiency in both domain-specific knowledge and design tools. This bottleneck limits productivity and slows down the rapid iteration cycles demanded by modern research workflows. Recent advances in generative AI have made automatic image generation promising direction in graphic design. Diffusion models [13](e.g., Stable Diffusion [4], DALLE [5, 6], Flux [7]) have demonstrated impressive capabilities in visual fidelity and stylistic diversity. Autoregressive visionEqual contributions. Corresponding author Submitted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025). language models (e.g., Emu3 [8], NAR [9], VAR [10], Janus-Pro [11]) further extend the boundaries by improving semantic alignment between textual inputs and visual outputs, especially in open-ended generation tasks. However, their limited reasoning ability in complex application scenarios remains major constraint on generative performance. With the advancement of reasoning capabilities in language models, models like GPT-4o-image [12], which incorporate chain-of-thought reasoning and stronger multimodal foundations, mark shift towards more controllable and content-aware generation. In principle, such models can be used to generate scientific illustrations directly from textual descriptions. Currently, research on AI-assisted scientific illustration remains in its early stages and is mainly focused on benchmarking the understanding capabilities of multimodal models (e.g., SciFIBench [13], ScImage [14]). There is noticeable lack of evaluation frameworks for assessing the ability of models to generate scientific diagrams. key open question is how to objectively and systematically evaluate the quality of scientific illustrations produced by generative models. To fill this gap, we introduce SridBench, the first benchmark specifically designed to evaluate the capability of multimodal models to generate scientific graphics from textual descriptions. This dataset includes 1,120 generation instances collected from peer-reviewed publications across 13 academic disciplines as shown in Fig.1. For systematic evaluation, each instance is annotated and assessed along six dimensions, supporting both human and automated evaluation protocols. Additionally, we conduct extensive benchmarking of wide range of generative models. Results reveal significant performance gap between current models and expert-created graphics. Even the best-performing model in our study, GPT-4o-image, achieves an average score of fair level. Semantic understanding emerges as the primary bottleneck. Open-source models perform worse, with average scores close to 1, and proprietary models like Gemini-2.0-Flash only reach score of 1.0, highlighting the considerable room for improvement in this domain. In summary, this work makes the following key contributions: 1. SridBench, new benchmark dataset featuring 1,120 high-quality generation instances from real-world scientific literature and spanning 13 disciplines under natural science and computer science; 2. multi-dimensional evaluation protocol assessing figure quality along six dimensions, supporting both human and automated scoring; 3. comprehensive empirical study providing the first systematic comparison of representative generative models in the context of scientific illustration, revealing actionable research challenges. Figure 1: General description of SridBench. We collected triple data from 13 directions in natural science and computer science, and designed 6 evaluation metrics"
        },
        {
            "title": "2 Related work",
            "content": "In the task of image generation, the current mainstream AI generation models mainly include two categories: diffusion models and autoregressive models. They have continuously made breakthroughs in generation quality, control ability, and multimodal understanding, providing an important foundation for application scenarios such as scientific research illustrations. Diffusion models: By simulating the process of gradually adding noise to data and then reverserestoring it, diffusion models [4] have made significant progress in generation accuracy and controllability in recent years. Represented by the Stable Diffusion series, its latest version, Stable Diffusion XL [15], has been included in the MLPerf [16] Inference v4.0 benchmark test, demonstrating its powerful performance in high-quality image generation. Stable Diffusion series have demonstrated its powerful performance in high-quality image generation. DALLE3 [6] performs well in design-related tasks such as DEsignBench [17], indicating its strong text-to-image alignment ability. The FLUX series of models strikes balance among image resolution, generation speed, and cost-efficiency, being particularly suitable for high-resolution image generation tasks. Although diffusion models have achieved remarkable results in visual generation, their performance in complex scenarios still faces challenges. Especially in scientific research drawing tasks that require strict semantic control and structural constraints, there is still room for improvement in their context understanding ability and structural controllability. Autoregressive models: In contrast, autoregressive models [18, 19, 8] predict the pixels or feature positions in an image step by step, enabling more accurate alignment with the semantics of the input text while ensuring consistency. Emu3 has achieved leading results in text-to-image tasks such as T2I-CompBench [20], showing its high-fit ability in multimodal understanding and image generation tasks. Janus-Pro performs superiorly in the multimodal consistency of text-to-image, especially demonstrating the unique advantages of the autoregressive structure in detail restoration and instruction response. Currently, autoregressive models are widely regarded as generation paradigm more suitable for handling high-semantic-density inputs and have important potential in scientific research-related image generation tasks. Reasoning ability: Recent research trends [2124] indicate that reasoning ability is key factor affecting whether generation model can be adapted to complex scientific research scenarios. Starting from the o1 model released by OpenAI, large models that introduce the Chain-of-Thought (CoT) [25] mechanism (such as DeepSeek-R1 [26], QwQ [27], Doubao, etc.) have demonstrated powerful capabilities in multimodal reasoning tasks. These models can not only analyze the deep-level logical relationships in the input text but also generate responses that conform to the context semantics in complex scenarios. Furthermore, new-generation models such as GPT-4o-image have deeply integrated advanced reasoning ability with image generation ability for the first time, possessing the ability to \"understand scientific research texts\" and generate scientific research illustrations with reasonable structures and accurate content accordingly. The evolution of this paradigm indicates that generation models are moving from \"art-level\" to \"scientific-research-level\", providing new path to solve the understanding and control problems in scientific research illustration generation. Research Gaps: Although the development of generative models and reasoning models has laid good foundation for scientific research diagram tasks, there is currently lack of systematic evaluation framework to measure their actual performance in this specific scenario. Relying on human evaluation lacks unified standard, making it difficult to objectively quantify the performance of models in terms of semantic accuracy, structural rationality, and aesthetic quality. At the same time, most of the current research efforts are focused on the understanding of scientific images and the generation of image captions (such as SciFIBench [13], FigCaps-HF [28], etc. [29]). The field of evaluating the generation of scientific research drawings is almost blank. Therefore, this paper focuses on constructing multi-dimensional scientific research drawing evaluation system and systematically comparing the performance of different types of generative models, providing theoretical support and practical basis for the implementation of multimodal generation technology in the scientific research visualization scenario."
        },
        {
            "title": "3 Method",
            "content": "In order to test the scientific research drawing ability of image generation models, we collect and carefully select the scientific research illustration drawing data, and set the process and standard for scientific research drawing evaluation. This process can be seen in Fig.2. We collect data in two disciplines: Computer Science and Natural Science. Prompts mentioned in this section are shown in Appendix.A. Figure 2: The framework of our Benchmark of Scientific Research Illustration Drawing of Image Generation Model. As can be seen from the framework, human experts set the standards for batch downloading and filtering paper data from the Internet. MLLM and human experts work together to screen triplet data to ensure the authority and scientific nature of the data. At the same time, we use the MLLM which is consistent with the human preference and evaluation for automatic scoring. 3.1 Collection and structuring of data We collect papers and filter data on professional and authoritative paper websites in these two disciplines. The filtering process is to use Multimodal Large Language Models(MLLMs) to determine whether the illustration in the paper is schematic diagram or illustration (rather than real photo, experimental results diagram, and statistical data analysis diagram) , find and extract the caption and section. In this way, we can get lot of structured triple data: image, caption, section . Human experts will sift through the resulting triplet data. Specifically, the goal illustration in the triple should be clear, scientific, rigorous, and has certain degree of expressiveness. The text should also be able to support and cover the elements that generate the illustration. We try to find papers in the top journals and conferences in all directions that have recently been published. ArXiv can help us get some of the computer science papers TeX source files. This is necessary for us to construct triples because we can obtain the LaTeX expression of the formula in the original paper. ArXivs API makes it easy to get the TEX files for large number of papers in given direction. However, not all papers from top conferences and journals are submitted to arXiv. Therefore, we screened arXiv preprint papers that were not published in formal journals and conferences. For articles with fewer than 25 citations, we remove them directly. We then invited human experts to assess the content, quality, and quality of the illustrations. Only papers and illustrations considered to be of high quality and scientific quality by human experts will be used to construct triples. For natural science papers, we crawl from the Nature website, which ensures the quality and authority of the data. 3.2 Automatic generation and scoring Once we have the triples, we fill the text of the caption and section into well-designed prompt template, and then use the image generation model to draw the research illustration. Using MLLMs API, we implement batch and automatic image generation process. After obtaining the generated illustrations, we compare them to the images in the triple and score them using MLLM. We designed 6 scoring dimensions for scientific illustration. First, in order to measure the scientificalness and completeness of visual elements (such as organelles, molecular structure, neural network modules) , we set up two indicators: Diagrammatic structural integrityand Diagrammatic logic. Secondly, in order to measure the quality of the text in the illustrations, we set two indicators of Completeness of textual information and Accuracy of textual information. Finally, we design Cognitive readability and Aesthetic feeling to evaluate the quality of the generated results as whole. These six metrics are written into the prompt using MLLM scoring on scale of 1 to 5 (1: fail, 2: poor, 3: fair, 4: good, 5: excellent)"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Model Due to the limitation of input length, only three image generation models are chosen by us to evaluate the capability of scientific research drawing, which are GPT-4o-image, Gemini-2.0Flash [30] and Emu-3. We quantitatively analyzed GPT-4o-image and Gemini-2.0-Flash because Emu-3 takes too long time to generate all images. GPT-4o is used to judge the generation results of those models. Data Computer Science data are collected on arXiv and in top journals and conferences of computer science, while Natural Science data are collected on Reviews & Analysis section on the website of Nature (https://www.nature.com/nature/reviews-and-analysis). Since there is no section in the above paper, we choose the paragraph where the illustration is located as the section. Inspired by the classification on arXiv, we set nine specific directions in the collection of Computer Science data: Software Engineering, Robotics, Networking and Internet Architecture, HumanComputer Interaction, Distributed, Parallel, and Cluster Computing, Computer Vision and Pattern Recognition, Cryptography and Security, Computation and Language, and Hardware Architecture. 100 triples are selected in each direction. Meanwhile, we carefully select 220 triples from the latest reviews and analysis section of Nature. The selection process is done by experts in various fields. This careful selection process ensures the quality, timeliness, and diversity of the data. In addition, we use GPT-4o to further categorize images by scene and function to facilitate the analysis of more results. Eight categories are chosen to categorize computer science images: software design, noun classification, mathematical structure, hardware design, engineering system design, algorithm flow, AI model, and other types. Natural science images are categorized into four types: Physics diagram, Organic Chemistry diagram, Geographical Environment diagram, and Biological Structure diagram. 4.2 Overall Evaluation As we can see in Fig.3(a), Gemini-2.0-Flash scored less than 2 on each of these measures, meaning that the model had little or no ability to draw professional illustrations for scientific research papers. In the category diagrammatic structural integrity, Gemini-2.0-Flash earned the highest score of all. This shows that Gemini-2.0-Flash has basic understanding of the basic style and frame structure of scientific drawing. However, in terms of concrete content, as well as scientific logic and deduction, Gemini-2.0-Flash has no such ability at all. GPT-4o-image fully demonstrates its superiority over Gemini-2.0-Flash in this task. Every subject, computer science or natural science, is rated at around 3, with most scoring above that mark. This means that GPT-4o-images scientific mapping capabilities are at level of basic eligibility that humans would consider acceptable. 5 Figure 3: (a). On the computer science and natural science data, the average score of GPT-4o-image and Gemini-2.0-Flash scores in the six major indicators judged by GPT-4o. (b). For images generated by GPT-4o-image and Gemini-2.0-Flash, the comparison of score judged by Gemini-2.0-pro, GPT-4o and human expert. At the same time, we selected 50 natural science and 50 computer science triplets from the dataset, allowing Gemini-2.0-pro, GPT-4o and human experts to independently rate them simultaneously. The results, shown in Fig.3(b), show that the GPT-4o scores are broadly in line with those of human experts, while Gemini-2.0-pro scores are significantly biased relative to human scores. Therefore, we use GPT-4o for automated scoring. However, GPT-4o is still slightly overrated in terms of completeness and accuracy compared to human expert ratings. 4.3 Evaluation on Natural Science Data Figure 4: On different subjects of natural science data, the average score of GPT-4o-image and Gemini-2.0-Flash scores in the six major indicators. The performance of the two models on natural science data is demonstrated in Fig.4. More specifically, the integrity of GPT-4o-image generated image elements (e.g., cell structure, sensing instrument structure) is significantly higher than the integrity of the text elements (whether or not it completely covers all the information in the reference image). GPT-4o-image can guarantee the accuracy of expressing text, although it cannot express text information completely. As can be seen, the accuracy of the text information on this side of the score is higher than the integrity. However, in terms of logic, simplicity and aesthetics, GPT-4o-image scored below average. This means that there is still much room for improvement in the overall look and feel of natural science image rendering. Overall, 6 GPT-4o-image does not show significant gap in competence between different disciplines of the natural sciences . 4.4 Evaluation on Computer Science Data Figure 5: On different subjects of computer science data, the average score of GPT-4o-image and Gemini-2.0-Flash scores in the six major indicators. As can be seen from Fig.5, the Gemini-2.0-Flash is still considered to have no preliminary ability to generate scientific maps in computer science, although there is some improvement in the ratings compared to the natural science data. For GPT-4o-image, there was significant decrease in the scores on the measures of completeness and accuracy of the text information. Compared with the schematics of natural science, the schematics of computer science often have more words and more complex flow structure. This makes the generation of image elements and text elements, GPT-4oimage does not meet the performance of natural science, the same ability to generate images. But at the same time, another noticeable improvement is the ability of GPT-4o-image to be readable and aesthetically pleasing. This is still relevant to the schematic nature of computer science, because most of them are flowcharts, consisting of elements such as text, borders, and arrows. For such diagrams, GPT-4o-image is easier to draw. In contrast to natural science images, generative models need to accurately depict complex and specialized graphical elements such as cellular structures, electron spins, animal organs, and ecosystems. As result, there is worse performance in natural science diagrams. In different subjects, GPT-4o-image alternative shows absolutely no difference. However, Computer Vision and Pattern Recognition and Computation and Language doesnt score well in terms of brevity and aesthetics. In fact, these two disciplines represent one of the hottest areas of artificial intelligence right now which are computer vision, pattern recognition and natural language processing. The average level of human drawing in this field is also gradually increasing. Therefore, for the corresponding image generation model, requirement of the quality of the generated image will also be higher. 4.5 Analysis of Generation Results Fig.7 and 8 show the comparison between the illustrations generated by the three image generation models (Emu-3, Gemini-2.0-Flash, and GPT-4o-image) and the authors original image in the paper. As can be seen, Emu-3 does not have any understanding of scientific writing, and the content it generates is irrelevant to our requirements. Gemini-2.0-Flash simply draws text in an image in Fig.7. There are no graphic elements, and the text is problematic because they are more like symbols than words. In Fig.8, despite the appearance of the plant-like structure, the resulting image is still difficult to interpret. At the same time, there are large number of text symbols in the generated illustrations 7 Figure 6: On different types of computer science data, the average score of GPT-4o-image and Gemini-2.0-Flash scores in the six major indicators. Figure 7: Computer science paper illustrations generated by different image generation models under the same prompt are compared with the original paper illustration. Figure 8: Natural science paper illustrations generated by different image generation models under the same prompt are compared with the original paper illustration. 8 which are similar to the original text of the paper. This situation is also common in other generated illustrations. However, GPT-4o-image has significant advantage over other models in terms of the quality of content generated. It produces illustrations with well-defined and well-expressed text. The structure of the illustration is clear. The basic elements of the reference image are reflected in the generated results. It can be said that GPT-4o-image has had preliminary, relatively qualified scientific text understanding and image generation capabilities. It can simply and clearly generate scientific, inferential and logical images. However, this is only preliminary capability. It should be noted that there are still significant problems in generating scientific illustrations from GPT. Examples include missing elements, omissions and errors in textual representation. Compared with the reference images drawn by human experts, there is still big gap in their correctness and scientific accuracy. Figure 9: Illustrations generated by GPT-4o-image (left) and their reference from original paper (right). Fig.9 shows generation results for GPT-4o-image which reflect the common problem of GPT-4oimage. As can be seen from Fig.9(a) , the illustrations generated by GPT contain common errors, such as the sun orbiting the Earth. At the same time, although it can draw multi-sub-graphs according to the requirements, the detailed information in the graph is still missing. In Fig.9(b) , it is seen that GPT-4o-image has the rudimentary ability to draw structural formulas for organic compounds. But there are still obvious scientific errors in the results, such as the reaction conditions in the diagram. At the same time, the compounds involved in the reactions and the results are not plotted correctly. This shows that the GPT understanding and expression of organic chemistry also has lot of room for improvement. As you can see from Fig.9(c) , GPT-4o-image has rudimentary location understanding and map generation capabilities. However, for more precise positioning and interpretation of geographical processes, GPT-image still has errors and omissions."
        },
        {
            "title": "5 Conclusion and Discussion",
            "content": "In this paper, we propose SridBench, which is the first benchmark of scientific research illustration drawing for image generation model. We propose strongly inferential drawing scenario called 9 scientific research illustration drawing. Using human experts and MLLM, we meticulously collected and screened triplet data on the scientific paper website for the evaluation of the scientific mapping ability of the image generation model. 1120 triples across 13 disciplines in the natural and computer science were used to test the ability of current scientific research to map multiple graph models. At the same time, we design six indicators to evaluate the generated illustrations. We found that, with the exception of GPT-4o-image, other image generation models, such as Gemini2.0-Flash, do not have any scientific mapping capabilities. GPT-4o-image can preliminarily complete scientific research drawing tasks, generate clear text and complete structure, and have certain degree of professional results. However, there are problems with the GPT-4o-image as well. There is general lack of text information, visual elements are also missing. At the same time, some hallucinations and common sense errors were also found in the GPT-4o-image generation results. This means that, at present, there is still much room for improvement in the ability of scientific research illustration drawing of image generation models. How to improve the generation ability of the image generation model in the task of strong inference should be the focus of the next researchers."
        },
        {
            "title": "References",
            "content": "[1] N. Metzger, Dsm refinement with deep encoder-decoder networks, 2020. [Online]. Available: https://arxiv.org/abs/2012.07427 [2] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, 2020. [Online]. Available: https://arxiv.org/abs/2006.11239 [3] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, High-resolution image synthesis with latent diffusion models, in CVPR, 2022, pp. 10 68410 695. [4] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel et al., Scaling rectified flow transformers for high-resolution image synthesis, in Forty-first International Conference on Machine Learning, 2024. [5] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, Hierarchical text-conditional image generation with clip latents, arXiv preprint arXiv:2204.06125, vol. 1, no. 2, p. 3, 2022. [6] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo et al., Improving image generation with better captions, Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, vol. 2, no. 3, p. 8, 2023. [7] Black Forest Labs, Flux, https://github.com/black-forest-labs/flux, 2024, accessed: 2024-1105. [8] X. Wang, X. Zhang, Z. Luo, Q. Sun, Y. Cui, J. Wang, F. Zhang, Y. Wang, Z. Li, Q. Yu et al., Emu3: Next-token prediction is all you need, arXiv preprint arXiv:2409.18869, 2024. [9] Y. He, Y. He, S. He, F. Chen, H. Zhou, K. Zhang, and B. Zhuang, Neighboring [Online]. Available: autoregressive modeling for efficient visual generation, 2025. https://arxiv.org/abs/2503.10696 [10] K. Tian, Y. Jiang, Z. Yuan, B. Peng, and L. Wang, Visual autoregressive modeling: Scalable image generation via next-scale prediction, Advances in neural information processing systems, vol. 37, pp. 84 83984 865, 2024. [11] X. Chen, Z. Wu, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, and C. Ruan, Janus-pro: Unified multimodal understanding and generation with data and model scaling, arXiv preprint arXiv:2501.17811, 2025. [12] OpenAI, 2025, gpt-4o-image-generation-system-card-addendum/ Addendum to 2025-04-02. accessed: system card: [Online]. Available: gpt-4o 4o image generation, https://openai.com/index/ [13] J. Roberts, K. Han, N. Houlsby, Benchmarking large multimodal models for scientific figure interpretation, 2024. [Online]. Available: https://arxiv.org/abs/2405.08807 and S. Albanie, Scifibench: [14] L. Zhang, S. Eger, Y. Cheng, W. Zhai, J. Belouadi, C. Leiter, S. P. Ponzetto, F. Moafian, and Z. Zhao, Scimage: How good are multimodal large language models at scientific text-to-image generation? 2024. [Online]. Available: https://arxiv.org/abs/2412.02368 [15] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller, J. Penna, and R. Rombach, Sdxl: Improving latent diffusion models for high-resolution image synthesis, arXiv preprint arXiv:2307.01952, 2023. [16] V. J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmuelling, C.-J. Wu, B. Anderson, M. Breughe, M. Charlebois, W. Chou, R. Chukka, C. Coleman, S. Davis, P. Deng, G. Diamos, J. Duke, D. Fick, J. S. Gardner, I. Hubara, S. Idgunji, T. B. Jablin, J. Jiao, T. S. John, P. Kanwar, D. Lee, J. Liao, A. Lokhmotov, F. Massa, P. Meng, P. Micikevicius, C. Osborne, G. Pekhimenko, A. T. R. Rajan, D. Sequeira, A. Sirasao, F. Sun, H. Tang, M. Thomson, F. Wei, E. Wu, L. Xu, K. Yamada, B. Yu, G. Yuan, A. Zhong, P. Zhang, and Y. Zhou, Mlperf inference benchmark, 2020. [Online]. Available: https://arxiv.org/abs/1911.02549 11 [17] K. Lin, Z. Yang, L. Li, J. Wang, and L. Wang, Designbench: Exploring and benchmarking dalle 3 for imagining visual design, 2023. [Online]. Available: https://arxiv.org/abs/2310. [18] Y. Wu, Z. Zhang, J. Chen, H. Tang, D. Li, Y. Fang, L. Zhu, E. Xie, H. Yin, L. Yi et al., Vila-u: unified foundation model integrating visual understanding and generation, arXiv preprint arXiv:2409.04429, 2024. [19] Q. Sun, Y. Cui, X. Zhang, F. Zhang, Q. Yu, Z. Luo, Y. Wang, Y. Rao, J. Liu, T. Huang, and X. Wang, Generative multimodal models are in-context learners, 2023. [20] K. Huang, K. Sun, E. Xie, Z. Li, and X. Liu, T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation, Advances in Neural Information Processing Systems, vol. 36, pp. 78 72378 747, 2023. [21] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, Y. Fan, K. Dang, M. Du, X. Ren, R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin, Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, arXiv preprint arXiv:2409.12191, 2024. [22] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin, Qwen2.5-vl technical report, arXiv preprint arXiv:2502.13923, 2025. [23] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu et al., Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 24 18524 198. [24] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu et al., Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, arXiv preprint arXiv:2412.05271, 2024. [25] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., Chain-ofthought prompting elicits reasoning in large language models, NeurIPS, 2022. [26] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [27] Team, QwQ: Reflect Deeply on the Boundaries of the Unknown, Nov. 2024, accessed: 2025-01-01. [Online]. Available: https://qwenlm.github.io/blog/qwq-32b-preview/ [28] A. Singh, P. Agarwal, Z. Huang, A. Singh, T. Yu, S. Kim, V. Bursztyn, N. Vlassis, and R. A. Rossi, Figcaps-hf: figure-to-caption generative framework and benchmark with human feedback, 2023. [Online]. Available: https://arxiv.org/abs/2307.10867 [29] Z. Xu, S. Du, Y. Qi, C. Xu, C. Yuan, and J. Guo, Chartbench: benchmark for complex visual reasoning in charts, 2024. [Online]. Available: https://arxiv.org/abs/2312.15915 [30] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican et al., Gemini: family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023."
        },
        {
            "title": "A Prompts used in data process and judgement",
            "content": "Image Filter Please strictly judge whether this picture belongs to the concept diagram, model frame diagram, process flow diagram or structure diagram in the academic paper. Return 1 if it is of the following type: concept diagram, model frame diagram, process flow diagram or structure diagram. Return 0 if it is of the following type: experimental result graph, statistical graph, photo, table, mathematical formula, pseudocode. Returns only single number, 1 or 0, with no explanation. Note that some of the subplots in the images contain schematics and graphs of statistical analysis and experimental results. In each case, a0 is returned as long as it contains statistical analysis of the data and diagram of the experimental results (not just diagram). Text Generation(for computer science) You will get the text of TEX file. In this file, find the text associated with the image {imgname}. Note that you can first find the latex figure class where {imgname} is located and identify its latex label in label, and then locate the text according to the label. The process cannot be included in the output. The output is the original content associated with {imgname}. Here is the paper: {papertex}. Output the whole text content of the section (not just the name and label of the section) in which the image is located directly, without anything else. Image Generation You are scientist and now you are going to draw diagram for the computer (natural) science research paper. You will be given the paper section where the diagram is located and the caption of the diagram in the paper. The section is: section. The caption is: caption. Please draw professional, rigorous and scientific diagram. You can use different colors and some graphic legends or logos appropriately. Note that the captions we provide do not need to be drawn in the diagram. Image Judgement You are researcher who evaluates illustrations in research papers. Next youll receive two diagrams, the first one by an anthropologist and the second one by an AI model based on the same cue. Please rate the graph generated by the AI model on: completeness of textual information (whether it contains all the textual information in the reference graph), accuracy of textual information (whether the textual information is scientifically rigorous), diagrammatic structural integrity (does it draw all the elements of the diagram) , diagrammatic logic (does it arrange the elements scientifically and logically) , cognitive readability (does it allow the reader to understand the content concisely) , aesthetic feeling, ie whether drawing is aesthetically pleasing or has sense of design. On scale of 1 to 5(1: fail, 2: poor, 3: fair, 4: good, 5: excellent). Please return your comments in the following format: completeness of textual information: 4, accuracy of textual information: 4, diagrammatic structural integrity: 5 , diagrammatic logic:4 , cognitive readability: 2, aesthetic feeling: 3 Just return and its contents, as in the example above, without returning anything else. Here are two pictures: the first is drawn by human, and the second is drawn by an AI. \""
        }
    ],
    "affiliations": [
        "Nankai University",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "University of Science and Technology of China",
        "Wuhan University"
    ]
}