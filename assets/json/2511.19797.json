{
    "paper_title": "Terminal Velocity Matching",
    "authors": [
        "Linqi Zhou",
        "Mathias Parger",
        "Ayaan Haque",
        "Jiaming Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the $2$-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 2 7 9 7 9 1 . 1 1 5 2 : r a"
        },
        {
            "title": "TERMINAL VELOCITY MATCHING",
            "content": "Linqi Zhou Luma AI Mathias Parger Luma AI Ayaan Haque Luma AI Jiaming Song Luma AI Figure 1: (Left) conceptual comparison of our method to prior methods. TVM guides the one-step model via terminal velocity rather than initial velocity. (Right) 1-NFE samples on ImageNet at 256 and 512 resolution."
        },
        {
            "title": "ABSTRACT",
            "content": "We propose Terminal Velocity Matching (TVM), generalization of flow matching that enables high-fidelity oneand few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the 2-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256256, TVM achieves 3.29 FID with single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512512, representing state-of-the-art performance for one/few-step models from scratch."
        },
        {
            "title": "INTRODUCTION",
            "content": "Can we build generative models that simultaneously deliver high-quality samples, fast inference, and scalability to high-dimensional data, all from single training stage? This is the central challenge that continues to drive research in generative models. While Diffusion Models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) and Flow Matching (Liu et al., 2022; Lipman et al., 2022) have become the dominant paradigms for generating images (Rombach et al., 2022; Podell et al., 2023; Esser et al., 2024) and videos (OpenAI, 2024; Wan et al., 2025), they typically require many sampling steps (e.g., 50) to produce high-quality outputs. This multi-step nature makes generation computationally expensive, especially for high-dimensional data like videos. In pursuing single-stage training for few-step inference, recent methods have focused on directly learning integrated trajectories rather than relying on ODE solvers. Consistency-based approaches (CT (Song et al., 2023), CTM (Kim et al., 2023), sCT (Lu & Song, 2024)) and trajectory matching methods like MeanFlow (Geng et al., 2025) learn to predict or match trajectory derivatives. However, these methods lack explicit connections to distribution matching, fundamental measure of generative model quality. While Inductive Moment Matching (IMM) (Zhou et al., 2025) addresses 1For Text-to-Image results at 10B+ scale, visit https://lumalabs.ai/blog/engineering/tvm 1 this gap by providing distribution-level guarantees through Maximum Mean Discrepancy, it requires multiple particles per training step, limiting scalability. We propose Terminal Velocity Matching (TVM), new framework for learning ground-truth trajectories of flow-based models in single training stage. Instead of matching time derivatives at the initial time, TVM matches them at the terminal time of trajectories. This conceptually simple shift yields powerful theoretical guarantees. We prove that our training objective upper bounds the 2-Wasserstein distance between data and model distributions. Unlike IMM, our method provides distribution-level guarantees without requiring multiple particles. Our analysis also reveals critical architectural limitation: current diffusion transformers (Peebles & Xie, 2023) lack Lipschitz continuity, which destabilizes TVM training. We address this with minimal architectural modifications, including RMSNorm-based QK-normalization and time embedding normalization. To make TVM practical at scale, we develop an efficient Flash Attention kernel that supports backward passes on Jacobian-Vector Products (JVP), crucial for our terminal velocity computation. Our implementation achieves up to 65% speedup and significant memory reduction compared to standard PyTorch operations. We introduce scaled parameterization where the network output naturally scales with the CFG weight w, allowing the model to handle varying guidance strengths more effectively. During training, we randomly sample CFG weights and directly incorporate them into our objective function with appropriate weighting (1/w2) to prevent gradient explosion. This approach enables stable training across diverse guidance scales without requiring curriculum learning or specialized loss modifications, making TVM straightforward to implement and scale. TVM achieves state-of-the-art results on ImageNet-256256, with 3.29 FID in single-step generation (outperforming MeanFlows (Geng et al., 2025) with 3.43 FID) and matches/exceeds diffusion baselines with just 4 function evaluation steps (i.e., 1.99 FID for TVM vs. 2.27 FID for DiT). Similarly, our method surpasses diffusion baselines with 4-NFE on ImageNet-512512 (i.e. 2.94 FID for TVM vs. 3.04 FID for DiT) while outperforming prior from-scratch methods such as sCT (Lu & Song, 2024) and MeanFlow on single-step generation. Our method naturally interpolates between one-step and multi-step sampling without retraining, requires no training curriculum or loss modifications, and maintains stability with simple architectures. Our construction provides new insights into building scalable one/few-step generative models with distributional guarantees, demonstrating that principled theoretical design can lead to practical improvements in both training stability and generation quality."
        },
        {
            "title": "2 PRELIMINARIES: FLOW MATCHING",
            "content": "For given data distribution p0(x0) and prior distribution p1(x1), Flow Matching (FM) (Lipman et al., 2022; Liu et al., 2022) constructs time-augmented linear interpolation xt between data x0 RD and prior x1 RD such that xt = (1 t)x0 + tx1 2. For each path xt conditioned on (x0, x1) pair, there exists conditional velocity vt = x1 x0 for each xt. Under this definition, ground-truth velocity field : RD [0, 1] RD marginal over all data and prior exists but is not known in analytical form. Therefore, neural network uθ(xt, t) is used as approximation via loss LFM(θ) = Ext,vt,t[uθ(xt, t) vt2 2] for all [0, 1] and xt pt(xt) where pt(xt) denotes the marginal distribution over all data and prior. It can be shown that the minimizer θmin implies uθmin(xt, t) = u(xt, t) which can be used during inference to transport prior to data distribution by solving an ODE (1) dt xt = u(xt, t). For each ground-truth u(xt, t), there exists corresponding displacement map ψ : RD [0, 1] [0, 1] RD (i.e. flow map (Boffi et al., 2024)) from any start time [0, 1] to an end time [0, 1]. It is defined as the ODE integral following u(xr, r) for all [s, t], i.e. ψ(xt, t, s) = xt + (cid:90) t u(xr, r)dr. (2) Empirically, uθ(xt, t) is used with classical ODE integration techniques such as the Euler method to produce samples. 2See Lipman et al. (2022); Albergo et al. (2023) for general path constructions. Figure 2: An illustration of Terminal Velocity Matching. Left shows the ground-truth displacement map by integrating the true velocity. Right shows our model path directly jumping between points on the ground-truth path in one step. In our method, the one-step generation x0 from xt coincides with ground-truth x0 if the terminal velocity of model ds (xt, t, s) coincides with ground-truth velocity u(xs, s) for all [0, t] along the true flow path (see Eq. (7)). The terminal velocity condition is jointly satisfied with the boundary case when model displacement is 0, where matching ds (xt, t, s)s=t with u(xt, t) reduces to Flow Matching."
        },
        {
            "title": "3 TERMINAL VELOCITY MATCHING",
            "content": "We propose Terminal Velocity Matching (TVM), single-stage objective that directly learns the ODE integral in Eq. 2. By learning the transition between any two timesteps, TVM can generate high quality solutions in one step or few steps, while enjoying inference-time scaling. Let (xt, t, s) := ψ(xt, t, s) xt denote the net displacement of the velocity field. We observe that it must satisfy the following two conditions: 1 (xt, t, s) = (cid:90) u(xr, r)dr , 2 ds (xt, t, s) (cid:12) (cid:12) (cid:12)s=t = u(xt, t). (3) The first condition is the definition of net displacement and the second condition is true by differentiating both sides of the first condition w.r.t. evaluated at = t. It explicitly relates the displacement map (with large time jump) to the marginal velocity field (with infinitesimal time jump), allowing us to interpolate between one-step sampling and ODE-like infinite-step sampling. One of our key insights is that we can use single two-time conditioned neural network Fθ(xt, t, s) to learn both the one-step displacement sampler from to and the instantaneous velocity field. For simplicity, we let our model with learnable parameters θ be fθ(xt, t, s) = (s t)Fθ(xt, t, s), uθ(xt, t) := ds fθ(xt, t, s) (cid:12) (cid:12) (cid:12)s=t = Fθ(xt, t, t) (4) where the scaling (s t) is chosen to satisfy integral boundary condition when = s3. Condition 2 can be easily enforced by FM loss (in Eq. (1)) and condition 1 can be naıvely enforced via the displacement error displ(θ) := Ext Lt (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) fθ(xt, t, 0) (cid:90) 0 u(xr, r)dr (cid:35) . 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 (5) Once the above error is minimized to zero, one can obtain one-step samples by calling xt + fθ(xt, t, 0) for any xt pt(xt) at [0, 1]. However, this objective is infeasible because it requires ODE integration for each starting point xt. We address this challenge by proposing simple sufficient condition to the network that bypasses explicit training-time ODE simulation. Terminal Velocity Condition. Explicit integration can be bypassed via differentiating w.r.t. integral boundaries. For the ground-truth net displacement (xt, t, s) in condition 1 , differentiating w.r.t. gives rise to the following condition on terminal velocity, i.e. ds (xt, t, s) = u(ψ(xt, t, s), s). (6) This condition is true for any ground-truth net displacement , and we show in Appendix A.2 that given [0, 1] and our parameterized map fθ(xt, t, s), Lt displ(θ) (cid:90) 0 Ext (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) ds fθ(xt, t, s) u(ψ(xt, t, s), s) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:35) ds. (7) 3This is similar to CTM (Kim et al., 2023). See Appendix A.1 for conditions on general scaling factors. 3 This result shows that the terminal velocity error on the right hand side upper bounds the displacement error, and so zero terminal velocity error implies that displacement from to 0 matches exactly. Moreover, it is easy to see that the terminal velocity error reduces to the marginal FM loss as (see Appendix A.3). FM can thus be understood as matching trajectorys terminal velocity when the net displacement is 0. An illustration of our framework is shown in Figure 2. Despite the simplicity and generality, in practice, fulfilling this condition is still difficult due to the requirement of ψ and u. Fortunately, this issue can be effectively addressed using learned network as proxies. Learned networks as proxies. Specifically, we propose the following approximation u(ψ(xt, t, s), s) uθ(xt + fθ(xt, t, s), s) (8) as proxies for the ground-truths. To properly guide the terminal velocity, uθ(xs, s) needs to first approximate the ground-truth u(xs, s) for any xs and s. Therefore, the proxy terminal velocity error can be jointly optimized with Flow Matching, which, as noted above, is special boundary case of the terminal velocity error when displacement is 0. We use the term Terminal Velocity Matching for this joint minimization of general and boundary-case velocity error, where the objective is (cid:35) Lt,s TVM(θ) = Ext,xs,vs fθ(xt, t, s) uθ(xt + fθ(xt, t, s), s) + (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (cid:125) (cid:13) (cid:13) (cid:13)uθ(xs, s) vs (cid:124) (cid:123)(cid:122) satisfies 2 (cid:13) 2 (cid:13) (cid:13) 2 (cid:125) (9) ds (cid:34) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124) (cid:123)(cid:122) satisfies 1 for each time [0, 1] and [0, t]. Intuitively, this objective leverages single network to parameterize both the instantaneous velocity field and the displacement map, the former of which is learned from data to guide the learning of the latter. To provide further theoretical justification, in the following theorem, we formally establish weighted integral of our objective as proper upper bound on the 2-Wasserstein distance between the data distribution p0(x0) and our model distribution θ t0#pt(xt) pushforward from pt(xt) via our parameterized flow map. Theorem 1 (Connection to the 2-Wasserstein distance). Given [0, 1], let θ t0#pt(xt) be the distribution pushforward from pt(xt) via fθ(xt, t, 0), and assume uθ(, s) is Lipschitz-continuous for all [0, t] with Lipschitz constants L(s), with additional mild regularity conditions, 2 2 (f θ t0#pt, p0) (cid:90) 0 λ[L](s)Lt,s TVM(θ)ds + C, (10) where W2(, ) is 2-Wasserstein distance, λ[] is functional of L(), and is non-optimizable constant. Training objective. The theorem relates our per-time objective to distribution divergence. However, for practicality, we avoid computation of the above weighting function and instead choose to randomly sample both and via distribution p(s, t) such that TVM(θ)(cid:3) LTVM(θ) = Et,s (cid:2)Lt,s (11) where notably LTVM(θ) reduces to Flow Matching objective when = (see Appendix A.5). In practice, we employ biased estimate of the above objective by using exponentially averaged (EMA) weights and stop-gradient for our proxy networks (Li et al., 2023). The biased per-time objective ˆLt,s TVM(θ) is Ext,xs,vs (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) ds fθ(xt, t, s) uθ sg (cid:13) 2 (cid:13) (xt + fθsg(xt, t, s), s) (cid:13) (cid:13) 1t=s + (cid:13) (cid:13) (cid:13)uθ(xs, s) vs (cid:13) 2 (cid:13) (cid:13) 2 (cid:35) (12) where θsg and θ and 1 otherwise to ensure the constraint to reduce to FM loss when = s. sg are the stop-grad weight and stop-grad EMA weight of θ, and 1t=s is 0 when = Classifier-free guidance (CFG). In the case of class-conditional generation. The ground-truth velocity field is replaced by linear combination of class-conditional velocity u(xr, r, c) and unconditional velocity u(xr, r) (Ho & Salimans, 2022), such that the new displacement map is ψw(xt, t, s, c) = xt + (cid:90) [wu(xr, r, c) + (1 w)u(xr, r)] dr, (13) 4 where is the CFG weight, is class and denotes empty label. To train with CFG, we additionally condition the network on and c, and our class-conditional map is fθ(xt, t, s, c, w) = (s t)Fθ(xt, t, s, c, w) where the additional scale is chosen due to linear scaling in magnitude for marginal velocity w.r.t. w. The instantaneous velocity uθ(xs, s, c, w) is regressed against conditional velocity wvt + (1 w)u(xr, r) where we can approximate u(xr, r) with our own network (Chen et al., 2025). The per-time and per-class Flow Matching term can be modified as ˆLs,c,w FM (θ) = Exs,vs (cid:20)(cid:13) (cid:13) (cid:13)uθ(xs, s, c, w) (cid:104) wvs + (1 w)uθ sg (xs, s, , 1)) (cid:105)(cid:13) 2 (cid:13) (cid:13) 2 (cid:21) , (14) where θ coincides with the ground-truth CFG velocity in Eq. (13). Our class-conditional objective ˆLt,s,w can be modified as sg denotes EMA weights. We show in Appendix A.6 that the minimizer of this objective TVM (θ) 1 w2 Ext,c (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) ds fθ(xt, t, s, c, w) uθ sg (xt + fθsg (xt, t, s, c, w), s, c, w) 1t=s + ˆLs,c,w (cid:35) FM (θ) . (15) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 The weighting 1/w2 is to prevent exploding gradients because the magnitude of ground-truth velocity scales linearly with w. Final objective simply samples each of t, s, under some distribution p(t, s)p(w) and computes the above loss in expectation. We randomly set = with some probability (e.g. 10%) and for each = we set = 1. Our training algorithm is shown in Algorithm 1. Sampling. Our construction can naturally interpolate between one-step and n-step sampling. See Figure 3 for PyTorch-style sampling code."
        },
        {
            "title": "4 PRACTICAL CHALLENGES",
            "content": "We note and address several challenges to practically implement our objective. def sampling (net , x, n, c, w): ts = torch. linspace (1, 0, n+1) for t,s in zip(ts [:1] , ts [1:]) : = + (st) * net(x, t, s, c, w) return Figure 3: PyTorch-style sampling code. Semi-Lipschitz control. Theorem 1 makes the crucial assumption that uθ(xs, s) is Lipschitz continuous. However, modern transformers with scaled dot-product attention (SDPA) and LayerNorm (LN, Ba et al. (2016)) are not Lipschitz continuous (Kim et al., 2021; Qi et al., 2023; Castin et al., 2023). This issue similarly applies to diffusion transformers (DiT) (Peebles & Xie, 2023). Our insight is to make minimal and non-restrictive changes to the architecture for Lipschitz control. As shown in Figure 4, the original DiT experiences training instability leading to steep jump in network activations. As solution, we adopt RMSNorm as QK-Norm, which coinsides with the proposed L2 QK-Norm (Qi et al., 2023) with learnable scaling and is provably Lipschitz continuous. We also substitute all LN with RMSNorm (without learnable parameters, denoted as RMSNorm()), whose Lipschitzness we show in Appendix B.1. In addition, DiT introduces Adaptive LayerNorm (AdaLN) where the output of RMSNorm is modulated by MLP outputs of time embeddings denoted as RMSNorm(x) a(t) + b(t) where is the input feature and a(t), b(t) are scale and shift respectively. However, the Lipschitz constant of this layer depends on the magnitude of a(t) which can grow unbounded and is subject to instability. We therefore employ RMSNorm() again on all modulation parameters for Figure 4: Activation norm of last time embedding layer. Same trends follow for all other layers. AdaLN(x, t) = RMSNorm(x) RMSNorm(a(t)) + RMSNorm(b(t)). (16) Figure 4 also shows the activation with our proposed changes. Activations stay smooth after our fixes. Finally, we follow Qi et al. (2023) and use Lipschitz initialization for all linear layers except for time embedding layers. Note that these modifications do not explicitly constrain the Lipschitz constants of all but the key layers where instability can arise. We find such partial control of the Lipschitzness is sufficient for empirical success. 5 Flash Attention JVP with backward pass. The training objective involves the time derivative of our map fθ(xt, t, s), which can be derived as ds fθ(xt, t, s) = Fθ(xt, t, s) + (s t)sFθ(xt, t, s) (17) where the last term involves differentiating through the network with Jacobian-Vector Product (JVP). This poses significant challenge for transformers because automatic differentiation packages, e.g. PyTorch, often do not efficiently handle JVP of SDPA. Open-source Flash Attention (Dao et al., 2022) also has limited support for JVP. Crucially, different from prior works (Lu & Song, 2024; Geng et al., 2025; Sabour et al., 2025), gradient is also propagated through the JVP term sFθ(xt, t, s). To tackle these challenges, we propose an efficient Flash Attention kernel that (i) fuses JVP with forward pass, (ii) uses significantly less memory than naıve PyTorch attention, and (iii) supports backward pass on JVP results. We detail the implementation in Appendix C. Optimizer parameter change. Due to higher-order gradient through JVP, our loss can be subject to fluctuation with the default AdamW β2 = 0.999. We take inspiration from language models (Touvron et al., 2023) for mitigation and use β2 = 0.95 to speed up update of the gradient second moment. As show in Figure 5, the terminal velocity error fluctuates significantly less after β2 change. Scaled parameterization. The ground-truth CFG velocity scales linearly in magnitude with w, so using neural networks to directly predict the velocity may be suboptimal. We therefore additionally investigate simple scaled alternative as fθ(xt, t, s, c, w) = (s t)wFθ(xt, t, s, c, w) so that uθ(xs, s, c, w) = wFθ(xs, s, s, c, w) which scales with by design. We study the effect of this parameterization in experiments. Figure 5: Smoother terminal velocity error with β2 = 0.95. Different time distribution for FM loss. We find it empirically helpful to use separate distribution to sample different specifically for the FM loss (see Eq. (15)), even though this may deviate from Wasserstein interpretation. This is because we can directly transfer the proven successful time distribution from FM training for TVM. How this is used can be found in Algorithm 1 and we ablate this decision in Section 7.3."
        },
        {
            "title": "5 CONNECTION TO PRIOR WORKS",
            "content": "MeanFlow. MeanFlow (Geng et al., 2025) minimizes loss Ext,t,s (cid:104) Fθ(xt, t, s) Ftgt2 2 (cid:105) where (cid:104) (cid:105) (18) Ftgt = u(xt, t) + (s t) u(xt, t) xtFθsg (xt, t, s) + tFθsg (xt, t, s) (cid:104)(cid:13) dt fθ(xt, t, s) + u(xt, t)(cid:13) 2 (cid:13) where fθ(xt, t, s) = This loss can be equivalently rewritten as Ext,t,s (cid:13) 2 (st)Fθ(xt, t, s) and loss is minimized if and only if dt fθ(xt, t, s) = u(xt, t) (see Appendix E.1). This exhibits duality with our proposed method in that we enforce differential condition w.r.t. while MeanFlow differentiates w.r.t. which requires u(xt, t) to be propagated through JVP. In practice, u(xt, t) is replaced with vt, which introduces additional variance during training and can cause fluctuation in gradient, especially under random CFG during training (see Section 7.2). Additionally, the relationship between the loss and distribution divergence remains elusive with the introduction of vt. In contrast, we show our loss upper bounds 2-Wasserstein distance up to some constant, and our theory provides the unique insight of enforcing the Lipschitzness of our network, which stablizes training. (cid:105) Physics Informed Distillation (PID). PID (Tee et al., 2024) as inspired by Physics Informed Neural Networks (Raissi et al., 2019; Cuomo et al., 2022) distills pretrained diffusion models uϕ(xt, t) into one-step samplers. It parameterizes the one-step net displacement as fθ(x1, s) = (s 1)uθ(x1, s) where x1 p1(x1) and trains via distillation loss Ex1,s (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) ds fθ(x1, s) uϕ(x1 + fθsg (x1, s), s) (cid:35) (cid:13) 2 (cid:13) (cid:13) (cid:13) (19) 6 Figure 6: One-step samples from TVM on both ImageNet-512512 and ImageNet-256256. Our method generalizes the setting by introducing the starting time in addition to the terminal time s. Under this view, PID sets = 1 and can only generate one-step samples. We additionally show in Section 7.3 that naıve combination of PID and FM loss suffers from optimization instability and continuous distribution on is necessary for empirical success."
        },
        {
            "title": "6 RELATED WORKS",
            "content": "Diffusion and Flow Matching. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) learn generative models by reversing stochastic processes, while Flow Matching (Liu et al., 2022; Lipman et al., 2022) generalizes this to arbitrary priors with simplified training. Both approaches ultimately solve ODEs with neural networks during sampling. One-Step and Few-Step Models from Scratch. To address slow inference from ODE simulation, recent methods aim for few-step generation in single training stage. Consistency models (Song et al., 2023; Lu & Song, 2024) parameterize networks to represent ODE integrals but cannot jump between arbitrary timesteps without injecting additional noise, which can limit multi-step performance. Two-time conditioned approaches enable arbitrary timestep transitions: IMM (Zhou et al., 2025) provides distribution consistency via Maximum Mean Discrepancy but requires multiple particles; MeanFlow (Geng et al., 2025) and Flow Map Matching (Boffi et al., 2024) match trajectory derivatives but lack distributional guarantees. Other variants bypass differentiation via Monte Carlo (Liu & Yue, 2025) or combine distillation with FM (Frans et al., 2024; Boffi et al., 2025). Unlike these methods, TVM regularizes path behavior at the terminal time rather than initial time and provides explicit 2-Wasserstein bounds. While sCT and MeanFlow only compute forward JVP, TVM uniquely supports backward passes through the JVP computation, enabling full gradient flow for the terminal velocity objective. These innovations drive both our theoretical insights and architectural improvements."
        },
        {
            "title": "7 EXPERIMENTS",
            "content": "We investigate how well TVM can generate natural images (Section 7.1), discuss its advantages compared to previous methods (Section 7.2), ablate various practical choices (Section 7.3) and discuss its computation cost (Section 7.4). 7.1 IMAGE GENERATION ImageNet-256256. We present quantitative results in Table 1 under FID (Heusel et al., 2017). We adopt the default DiT-XL/2 architecture (Peebles & Xie, 2023) and inject as the second timestep, following IMM (Zhou et al., 2025) and MeanFlow (Geng et al., 2025). We additionally employ our semi-Lipschitz control techniques for training stability or we notice activation explosion as described in Figure 4, and we train with constantly sampled CFG, i.e. models with = 2 7 NFE () FID () # Params. NFE () FID () # Params. Diffusion/Flow ADM (Dhariwal & Nichol, 2021) LDM-4-G (Rombach et al., 2022) DiT-XL/2 (Peebles & Xie, 2023) (w = 1.25) DiT-XL/2 (Peebles & Xie, 2023) (w = 1.5) SiT-XL/2 (Ma et al., 2024) (w = 1.5) 2502 2502 2502 2502 2502 One/Few-Step from Scratch iCT-XL/2 (Song & Dhariwal, 2023) Shortcut-XL/2 (Frans et al., 2024) IMM-XL/2 (Zhou et al., 2025) MeanFlow-XL/2 (Geng et al., 2025) TVM-XL/2 (Ours) (w = 2) TVM-XL/2 (Ours) (w = 1.75) 1 2 1 1 2 2 2 2 4 1 2 1 2 1 2 4 10.96 3.60 3.22 2.27 2.15 34.24 20.3 10.60 8.05 3.99 2.51 3.43 2.93 3.29 2.80 4.58 2.61 1.99 554M 400M 675M 675M 675M 675M 675M 675M 675M 675M 675M 676M 676M 678M 678M 678M 678M 678M Diffusion/Flow ADM-G (Dhariwal & Nichol, 2021) SimDiff (Hoogeboom et al., 2023) VDM++ (Kingma & Gao, 2024) U-ViT-H/4 (Bao et al., 2023) EDM2-L (Karras et al., 2024) EDM2-XL (Karras et al., 2024) DiT-XL/2 (Peebles & Xie, 2023) (w = 1.25) DiT-XL/2 (Peebles & Xie, 2023) (w = 1.5) SiT-XL/2 (Ma et al., 2024) (w = 1.5) 2502 5122 5122 2502 632 632 2502 2502 2502 One/Few-Step from Scratch sCT-L (Lu & Song, 2024) sCT-XL (Lu & Song, 2024) MeanFlow-XL/2 (Geng et al., 2025) TVM-XL/2 (Ours) (w = 2.50) TVM-XL/2 (Ours) (w = 2.25) 1 2 1 2 1 2 1 2 1 2 4 7.72 3.02 2.65 4.05 1.88 1.85 4.64 3.04 2. 5.15 4.65 4.33 3.73 5.24 3.17 4.32 3.50 5.37 3.89 2.94 559M 2B 2B 501M 778M 1.1B 675M 675M 675M 778M 778M 1.1B 1.1B 676M 676M 678M 678M 678M 678M 678M Table 1: FID results on ImageNet-256256. Table 2: FID results on ImageNet-512512. Figure 7: (Left) MeanFlow is subject to wide variation in gradient norm if CFG scales (i.e., κ and ω) are randomly sampled under naıve settings (see Appendix F.2 for details). TVM shows much smoother gradient norm. (Middle) MeanFlows gradient norm is strongly correlated with the fluctuation of u(xt, t). TVMs u(xt, t) is much more stable under the same CFG setting. (Right) Our method converges with random CFG at training time, although tradeoff exists between different CFG in FID. Constantly sampled CFG works best. and = 1.75 are two different models trained from scratch. We describe additional training details in Appendix F. Our method achieves state-of-the-art 1-NFE FID among methods trained from scratch, outperforming MeanFlow and IMM. With CFG = 2, TVM can achieve noticeable improvements over MeanFlow, e,g, 3.29 FID vs. 3.43 FID for 1-NFE and similarly for 2-NFE. With 4 NFEs, = 1.75 also exceeds 500-NFE diffusion baselines. We additionally show qualitative 1-NFE samples on the right of Figure 6. ImageNet-512512. We train with the same settings as in 256256-resolution and we show the FID scores in Table 2. We rerun MeanFlow using the same settings as in ImageNet-256256 as our baseline in addition to sCT (Lu & Song, 2024) under similar model sizes. TVM again outperforms sCT and MeanFlow in 1-NFE and 2-NFE regime. Notably, TVM-XL/2 outperforms sCT-XL with 1.1B parameters, highlighting TVMs more optimal use of model capacity in fitting the image distribution. Moreover, with = 2.25, TVM with 4-NFE can match 500-NFE DiT-XL/2 baseline in performance, further demonstrating the scalability of our algorithm to higher resolution. Intriguingly, for both datasets, TVM trained with higher CFG performs better on 1 NFE while worse on 2 NFEs. We believe this implies fundamental trade-off between different NFE quality and that the network is limited in capacity in fitting all NFEs well. We leave more detailed studies on this trade-off and any design improvements to future work. 7.2 DISCUSSION ON TRAINING ADVANTAGES Single sample objective. Unlike IMM (Zhou et al., 2025) which uses more than 4 samples to calculate its loss, we use single sample to for loss calculation without losing distribution-matching interpretation. This also allows the objective to be scaled to large models and high-dimensional datasets where batch size on each GPU is constrained to be 1. Training with random CFG. Our construction allows us to randomly sample CFG scale during training without collapse. We attribute this stability to our JVP being only calculated w.r.t. which is invariant to starting position xt and time t. In contrast, CT (Song et al., 2023; Lu & Song, 2024) and MeanFlow (Geng et al., 2025) require velocity u(xt, t) to be used in the JVP calculation. In 8 trunc (µt, σt), (µs, σs) (0.4, 1.0), (0.4, 1.0) (2.0, 1.0), (0.4, 1.0) (2.0, 2.0), (0.4, 1.0) (2.0, 2.0), (0.6, 1.0) (1.0, 1.0), (0.4, 1.0) FID 4.59 4.00 4.01 7.88 3. clamp (µt, σt), (µs, σs) (2.0, 2.0), (0.4, 1.0) (2.0, 1.0), (0.4, 1.0) (2.0, 1.0), (0.6, 1.0) (1.0, 1.0), (0.4, 1.0) (1.0, 2.0), (0.4, 1.0) FID 3.88 4.11 4.00 3.66 3.83 gap (µg, σg), (µs, σs) (0.4, 1.0), (0.4, 1.0) (0.8, 1.0), (0.4, 1.0) (0.8, 1.4), (0.4, 1.0) (1.0, 1.2), (0.4, 1.0) (1.0, 1.4), (0.4, 1.0) FID 5. 3.72 3.95 3.82 3.94 Table 3: Ablation studies on different time sampling schemes, evaluated by 1-NFE FID. Figure 8: FID trend on the sampling schemes. p(w) 1-NFE EMA rate γ 1-NFE Scaled Param. 1-NFE 2-NFE % t=s 1-NFE 2-NFE rand., = 1.5 rand., = 2 const., = 1.5 const., = 9.37 5.14 6.66 4.81 γ = 0 γ = 0. γ = 0.99 γ = 0.999 10.24 5.08 4.90 6. yes, = 2 no, = 2 yes, = 1.5 no, = 1.5 3.72 3. 6.04 9.32 3.35 3.27 4.60 7. 0 10% 20% 30% 3.72 3. 3.88 3.97 3.35 3.18 2.97 3. (a) Random vs. constant CFG sampling evaluated at example ws. (b) EMA of pseudotarget θ sg. (c) With vs. without scaled parameterization. (d) Prob. for = during training. Table 4: FID ablation on various sampling/parameterization decisions. the case of random CFG, this velocity can vary widely in magnitude which, if propagated through JVP, can cause wide fluctuation in gradient norm (see left two in Figure 7) and causes training instability. Our method, in comparison, enjoys much smoother gradient norm and u(xt, t) norm, and successfully converges even in the presence of random CFG. We note that random sampling of CFG is not optimal as some CFG scales experience degradation in FID during training, and constant CFG performs better in comparison. We postulate that the under-performance of random CFG is due to limited capacity of the network and the 1/w2 factor that downweights high CFG. This phenoemenon is similarly observed in CFG-conditioned FM training (see Appendix F.3) and we leave any improved design to future work. No schedules and loss modification. We do not rely on training curriculum such as warmup schedules in sCT. For each CFG scale, we use the default CFG velocity for all t, s, while MeanFlow relies on additional hyperparameters to turn on CFG only when is within predetermined range. We also strictly adhere to the simple L2 loss without any adaptive weighting as proposed by MeanFlow. We believe the simplicity in our design allows for more scalability. 7.3 ABLATION STUDIES We ablate various implementation decisions and discuss insights from different parameter choices. Results are presented with XL/2 architecture trained for 200K steps with batch size 1024. Time sampling. Similar to Flow Matching, different time sampling schemes can greatly affect performance. We explore 3 different kinds of sampling schemes. Truncated sampling (trunc). Let (µt, σt), (µs, σs) denote being sampled from logitnormal distribution with mean and standard deviation (µt, σt) and beinsg sampled from truncated logit-normal distribution with parameters (µs, σs) such that t. Clamped independent sampling (clamp). Let (µt, σt), (µs, σs) denote and being independently sampled from logit-normal distributions with mean and standard deviation (µt, σt) and (µs, σs), and set = if > t. Truncated gap sampling (gap). Let (µg, σg), (µs, σs) denote the gap = ts being sampled from logit-normal distribution with mean and standard deviation (µg, σg), and sampled from logit-normal with parameters (µs, σs) truncated at 1 g. Then set = + g. In Table 3 we show comparison within each sampling scheme and conclude that better results are obtained when is biased towards 1 and biased towards 0 for the model to learn taking longer strides. However, biasing too much, e.g. µt = 2.0, σt = 2.0, leads to worse results. For gap, sampling with lower mean is preferrable to higher mean. In Figure 8, we also observe truncs performance degrades and clamp plateaus faster than gap. Therefore gap wins over longer training horizons. 9 FID 3.36 3. gap (0.8, 1.0), (0.4, 1.0) gap* (0.8, 1.0), (0.4, 1.0) Figure 9: Sampler comparisons. All above sampling schemes follow the naıve joint sampling of (s, t). We lastly explore separate time distribution for the FM loss term (see Section 4). We follow gap-sampler and denote the sampler gap* with parameters (µg, σg), (µs, σs) where (s, t) is jointly sampled for the first loss term and (µs, σs) is used to construct new logit-normal distribution to independently sample for the FM loss. We find in Figure 9 that gap* generally performs better in 1-NFE FID. CFG sampling. As described in the previous section, due to limited capacity of the model, we observe tradeoff in performance when CFG is randomly sampled during training. This is reflected in Table 4a. We note that constant CFG always outperforms random CFG, and for constant CFG sampling we find = 2 converging faster than the default = 1.5 for Flow Matching. EMA target rate γ. The target EMA weight θ plays significant role in accelerating convergence of the model. Shown in Table 4b, non-EMA target, i.e. γ = 0, noticeably lags behind γ > 0 alternatives. However, too large of γ, e.g. 0.9999, also causes instability because of the overly slow target update. sweet spot exists around γ = 0.99 which we use as default. Besides attribute its success to variance reduction because EMAs slower weight update implies much lower optimization noise. In addition, EMA is commonly used to evaluate diffusion models for its quality boost (Song et al., 2020), so being the optimization target also provides better learning signal to the model. Scaled parameterization. In Table 4c, we find scaled parameterization is generally beneficial, but its benefit may vary depending on training/data settings. We therefore suggest testing different choices for different settings for best performance. Probability for = s. Inspired by MeanFlow (Geng et al., 2025), we investigate whether setting = (when it reduces to pure FM training) is helpful for overall performance. We find that > 0% actually degrades 1-NFE performance while it marginally improves 2-NFE performance. This tradoff persists throughout the training but we observe diminishing return as training goes on. Therefore, we do not find this practice helpful in general and leave it out of our design space in general. 7.4 MEMORY AND RUNTIME ANALYSIS - 0.81 OOM TVM (w/ naıve DiT) TVM (w/ improved DiT) MeanFlow (w/ our kernel) MeanFlow (w/ naıve SDPA) Runtime (s) Memory (GB) We analyze per-step runtime and per-GPU memory consumption (averaged over 10 training steps without counting EMA update cost) without any performance optimization (e.g. torch.compile). Shown on the right is comparison with MeanFlow using 256 batch size on 8-GPU H100 cluster on ImageNet-256256. Since JVP with Flash Attention is not officially supported by PyTorch, the simplest way to implement MeanFlow is to use naıve SDPA, which runs out of memory. MeanFlow with our kernel does not run OOM. TVM with Lipschitz control (Section 4) experiences higher runtime and memory mostly due to architectural change, since TVM with naıve DiT is only marginally more expensive than MeanFlow with naıve DiT. We note that much of the additional compute can be compiled away via PyTorch. Additionally, if step time is concern, we can simply detach the JVP which biases learning gradient but dramatically reduces runtime. We leave further efficiency optimization to future work. Figure 10: Per-step time and per-GPU memory study. TVM (w/ improved DiT, detach JVP) 71.44 59.53 46.73 55.71 0.95 0. 0."
        },
        {
            "title": "8 CONCLUSION",
            "content": "We present Terminal Velocity Matching, framework for training one/few-step generative model from scratch. Different from prior works, we match the terminal velocity of flow trajectory instead of the initial velocity, and we show our objective can explicitly upper bound 2-Wasserstein distance up to constant. Our proposed objective is conceptually simple and easy to implement, and our theory sheds light on flaws of current diffusion transformers for their lack of Lipschitz continuity. TVM achieves state-of-the-art one-step result for model trained from scratch and surpasses baseline diffusion models with only 4 NFEs. We hope it can provide new insights into making scalable and performant one/few-step generative paradigms to come."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth In Proceedings of the IEEE/CVF conference on words: vit backbone for diffusion models. computer vision and pattern recognition, pp. 2266922679, 2023. Nicholas Boffi, Michael Albergo, and Eric Vanden-Eijnden. Flow map matching. arXiv preprint arXiv:2406.07507, 2, 2024. Nicholas Boffi, Michael Albergo, and Eric Vanden-Eijnden. How to build consistency model: Learning flow maps via self-distillation. arXiv preprint arXiv:2505.18825, 2025. Valerie Castin, Pierre Ablin, and Gabriel Peyre. How smooth is attention? arXiv preprint arXiv:2312.14820, 2023. Huayu Chen, Kai Jiang, Kaiwen Zheng, Jianfei Chen, Hang Su, and Jun Zhu. Visual generation without guidance. arXiv preprint arXiv:2501.15420, 2025. Salvatore Cuomo, Vincenzo Schiano Di Cola, Fabio Giampaolo, Gianluigi Rozza, Maziar Raissi, and Francesco Piccialli. Scientific machine learning through physicsinformed neural networks: Where we are and whats next. Journal of Scientific Computing, 92(3):88, 2022. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. arXiv preprint arXiv:2410.12557, 2024. Zhengyang Geng, Mingyang Deng, Xingjian Bai, Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for In International Conference on Machine Learning, pp. 1321313232. high resolution images. PMLR, 2023. Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2417424184, 2024. 11 Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. Hyunjik Kim, George Papamakarios, and Andriy Mnih. The lipschitz constant of self-attention. In International Conference on Machine Learning, pp. 55625571. PMLR, 2021. Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36, 2024. Lingxiao Li, Samuel Hurault, and Justin Solomon. Self-consistent velocity matching of probability flows. Advances in Neural Information Processing Systems, 36:5703857057, 2023. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Wenze Liu and Xiangyu Yue. Learning to integrate diffusion odes by averaging the derivatives. arXiv preprint arXiv:2505.14502, 2025. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. CoRR, abs/1805.02867, 2018. URL http://arxiv.org/abs/1805.02867. OpenAI. Video generation models as world simulators. https://openai.com/sora/, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Xianbiao Qi, Jianan Wang, Yihao Chen, Yukai Shi, and Lei Zhang. Lipsformer: Introducing lipschitz continuity to vision transformers. arXiv preprint arXiv:2304.09856, 2023. Maziar Raissi, Paris Perdikaris, and George Karniadakis. Physics-informed neural networks: deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378:686707, 2019. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Amirmojtaba Sabour, Sanja Fidler, and Karsten Kreis. Align your flow: Scaling continuous-time flow map distillation. arXiv preprint arXiv:2506.14603, 2025. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. Yang Song and Prafulla Dhariwal. preprint arXiv:2310.14189, 2023. Improved techniques for training consistency models. arXiv Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 12 Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Joshua Tian Jin Tee, Kang Zhang, Hee Suk Yoon, Dhananjaya Nagaraja Gowda, Chanwoo Kim, and Chang Yoo. Physics informed distillation for diffusion models. arXiv preprint arXiv:2411.08378, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Pascal Vincent. connection between score matching and denoising autoencoders. Neural computation, 23(7):16611674, 2011. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive moment matching. In Forty-second International Conference on Machine Learning, 2025."
        },
        {
            "title": "A THEOREMS AND DERIVATIONS",
            "content": "A.1 GENERAL NETWORK PARAMETERIZATION In general, we can parameterize our net displacement as fθ(xt, t, s) = γ(t, s)Fθ(xt, t, s) (20) for some γ(t, s) that satisfies γ(t, t) = 0 for boundary condition. And for the velocity condition, we let uθ(xt, t) := ds fθ(xt, t, s) (cid:12) (cid:12) (cid:12)s=t = γ(t)Fθ(xt, t, t) (21) where γ(t) = sγ(t, s)s=t. We derive ds fθ(xt, t, s)s=t below for clarity. ds fθ(xt, t, s) (cid:12) (cid:12) (cid:12)s=t = sγ(t, s)Fθ(xt, t, s) + γ(t, s)sFθ(xt, t, s) (cid:12) (cid:12) (cid:12)s=t (cid:104) sFθ(xt, t, s) (cid:105) (cid:12) (cid:12) (cid:12)s=t = sγ(t, s)s=tFθ(xt, t, t) + γ(t, t) = sγ(t, s)s=tFθ(xt, t, t) = γ(t)Fθ(xt, t, t) A.2 TERMINAL VELOCITY ERROR UPPER BOUNDS DISPLACEMENT ERROR Lemma 1. Under mild regularity assumptions, the following inequality holds, (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) fθ(xt, t, s) u(ψ(xt, t, s), s) displ(θ) ds Ext (cid:90) Lt (cid:35) 0 (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 ds where pt(xt) is marginal distributions for initial points xt. Proof. We assume both displacement maps are Riemann-integrable, then (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) displ(θ) = Extpt(xt) Lt = Extpt(xt) fθ(xt, t, s)ds fθ(xt, t, 0) u(xs, s)ds 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 ds (cid:90) 0 (cid:90) (cid:90) (cid:35) 0 u(ψ(xt, t, s), s)ds (cid:35) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:90) () 0 Extpt(xt) ds fθ(xt, t, s) u(ψ(xt, t, s), s) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:35) ds 0 (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) where () uses triangle inequality and regularity assumption. A.3 TERMINAL VELOCITY ERROR REDUCES TO FM Consider the terminal velocity error for each time as Expand the inner term Ext (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) ds fθ(xt, t, s) u(ψ(xt, t, s), s) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:35) ds fθ(xt, t, s) = Fθ(xt, t, s) + (s t)sFθ(xt, t, s) 14 (22) (23) (24) (25) (26) (27) (28) (29) (30) (31) and for the inner norm term its limit exists as s: (cid:20) ds [Fθ(xt, t, s) + (s t)sFθ(xt, t, s) u(ψ(xt, t, s), s)] (cid:21) fθ(xt, t, s) u(ψ(xt, t, s), s) lim ts = lim ts (32) (33) (34) Thus, the limit of its expected L2-norm exists (assuming this norm is bounded) and is equal to L2-norm of its limit, which is = Fθ(xs, s, s) u(xs, s) (cid:104) Exs Fθ(xs, s, s) u(xs, s)2 (cid:105) (35) and this is the original FM loss, which is equivalent (up to constant) to conditional Flow Matching loss used in practice in Eq. (1). A.4 MAIN THEOREM Theorem 1 (Connection to the 2-Wasserstein distance). Given [0, 1], let θ t0#pt(xt) be the distribution pushforward from pt(xt) via fθ(xt, t, 0), and assume uθ(, s) is Lipschitz-continuous for all [0, t] with Lipschitz constants L(s), with additional mild regularity conditions, 2 (f θ t0#pt, p0) (cid:90) 0 λ[L](s)Lt,s TVM(θ)ds + C, (10) where W2(, ) is 2-Wasserstein distance, λ[] is functional of L(), and is non-optimizable constant. Proof. Note that the ground-truth flow map ψ is invertible and that ψ(ψ(xt, t, 0), 0, t) = xt and ψ(ψ(x0, 0, t), t, 0) = x0. (i) p0(x0)fθ(ψ(x0, 0, t), t, 0) x02 t0#pt, p0) 2 (f θ 2 (36) (cid:90) 2dx0 (cid:90) (cid:90) (cid:90) = = = p0(x0)xt + fθ(ψ(x0, 0, t), t, 0) ψ(ψ(x0, 0, t), t, 0)2 2dx pt(xt)xt + fθ(xt, t, 0) ψ(xt, t, 0)2 2dxt pt(xt) (cid:90) (ii) pt(xt) (cid:13) (cid:13) (cid:13) (cid:13) (cid:90) 0 (cid:90) 0 ds (cid:13) (cid:13) (cid:13) (cid:13) (cid:124) ds fθ(xt, t, s)ds (cid:90) 0 u(xs, s)ds fθ(xt, t, s) u(ψ(xt, t, s), s) dxt dsdxt 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (cid:125) (cid:123)(cid:122) ε(xt,t,s) where (i) is due to Wasserstein distance being the infimum of all couplings, and we choose particular coupling of the two distribution by inverting x0 with ψ and remapping with respective flow maps. And (ii) is due to Lemma 1. Now, we inspect ε(xt, t, s) specifically by noticing that ε(xt, t, s) ds = fθ(xt, t, s) u(ψ(xt, t, s), s) + uθ(ψ(xt, t, s), s) uθ(ψ(xt, t, s), s) + uθ(ψ(xt, t, s), s) u(ψ(xt, t, s), s)2 2 (i) ds + uθ(fθ(xt, t, s), s) uθ(fθ(xt, t, s), s)2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:124) fθ(xt, t, s) uθ(fθ(xt, t, s), s) (cid:123)(cid:122) δ(xt,t,s) + uθ(fθ(xt, t, s), s) uθ(ψ(xt, t, s), s)2 2 (cid:90) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (ii) δ(xt, t, s) + L(s) ds (cid:13) (cid:13) (cid:13) (cid:13) (cid:124) fθ(xt, t, u) u(ψ(xt, t, u), u) (cid:125) du (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (cid:125) (cid:123)(cid:122) ε(xt,t,u) (37) (38) (39) (40) (41) (42) (43) where (i) is due to triangle inequality and (ii) is due to Lipschitz-continuous assumption. We further notice that right-hand-side contains term that is the integral of the left-hand-side. For simplicity, we hold xt and constant and let y(s) = (cid:90) ε(xt, t, u)du , y(s) = ε(xt, t, s) and we arrive at the following inequality, y(s) δ(xt, t, s) + L(s)y(s) (cid:82) L(u)duδ(xt, t, r) δ(xt, t, s) y(s) + L(s)y(s) dr (cid:104) (cid:82) L(u)duy(r) (cid:105)t (cid:16) (cid:82) L(u)duy(r) (cid:17) (cid:82) L(u)duδ(xt, t, r)dr 0 (cid:26)(cid:26)(cid:62) y(t) (cid:82) L(u)duy(s) (cid:82) L(u)duδ(xt, t, r)dr (cid:26) (cid:90) (cid:82) L(u)duy(s) (cid:90) s (cid:90) e (cid:82) L(u)duδ(xt, t, r)dr t L(u)du(cid:82) (cid:82) L(u)duδ(xt, t, r)dr (cid:82) L(u)duδ(xt, t, r)dr y(s) y(s) (cid:90) (cid:90) Therefore, setting = 0 we have (cid:90) 0 ε(xt, t, u)du (cid:90) 0 (cid:124) (cid:82) 0 L(u)du (cid:123)(cid:122) (cid:125) λ[L](r) δ(xt, t, u)du where the left-hand side is the inner term of Eq. (40). Then, Eq. (40) (cid:90) pt(xt) (cid:90) 0 λ[L](s) δ(xt, t, s)dsdxt = (cid:90) 0 = (cid:90) 0 λ[L](s) Ext (cid:104) fθ(xt, t, s) uθ(fθ(xt, t, s), s)2 2 + uθ(ψ(xt, t, s), s) u(ψ(xt, t, s), s)2 (cid:105) ds (54) (cid:34) λ[L](s) Ext (cid:104) fθ(xt, t, s) uθ(fθ(xt, t, s), s)2 + Ext (cid:104) uθ(ψ(xt, t, s), s) u(ψ(xt, t, s), s)2 2 (cid:105) (cid:35) ds (cid:34) λ[L](s) = (cid:90) 0 Ext (cid:104) fθ(xt, t, s) uθ(fθ(xt, t, s), s)2 (cid:104) + Exs (cid:124) uθ(xs, s) u(xs, s)2 2 (cid:123)(cid:122) (a) (cid:35) ds (cid:105) (cid:125) where (a) can be rewritten as (a) = Exs,vs (cid:104) uθ(xs, s) vs2 2 (cid:105) + where is some non-optimizable constant (Lipman et al., 2022). This is also classical result connecting score matching and denoising score matching (Vincent, 2011). 16 (44) (45) (46) (47) (48) (49) (50) (51) (52) (53) (55) (56) (57) Now, after substitution, we notice that our bound in Eq. (56) becomes (cid:90) λ[L](s)Lt,s TVM(θ)ds + where is some other constant, which completes the proof. A.5 REDUCTION TO FLOW MATCHING When = s, we show that LTVM(θ) reduces to Flow Matching loss. Lt,t TVM(θ) = Ext,xs,vs (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) ds fθ(xt, t, s) uθ(fθ(xt, t, s), s) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 + = Ext,vt (cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40) (cid:20) uθ(xt, t) uθ(xt, t)2 2 + (cid:13) (cid:13) (cid:13)uθ(xt, t) vt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)uθ(xs, s) vs 2(cid:21) (58) (59) (60) 2(cid:35) (cid:12) (cid:12) (cid:13) (cid:12) (cid:13) (cid:12) (cid:13) (cid:12)s=t A.6 DERIVATION FOR CLASS-CONDITIONAL TRAINING TARGET In Eq. (14), we introduced the CFG training target as wvt + (1 w)u1 θ sg (xs, s, ) We derive below that the minimizer of Eq. (14) is the CFG velocity wu(xs, s, c) + (1 w)u(xs, s). Proof. Consider the training objective (without weighting for simplicity) Exs,vs,s,c,w (cid:20)(cid:13) (cid:13)uw (cid:13) θ (xs, s, c) (cid:104) wvt + (1 w)u1 θsg (xs, s, )) (cid:21) (cid:105)(cid:13) 2 (cid:13) (cid:13) 2 when = , = 1, then it reduces to Exs,vs,s (cid:104)(cid:13) (cid:13)u θ(xs, s, ) vt (xs, s, ) = u(xs, s). (cid:105) (cid:13) 2 (cid:13) 2 θmin with the minimizer θmin satisfying u1 At minimum of the loss for other and c, it must satisfy (cid:2)wvs + (1 w)u1 (xs, s, c) = Evs = wEvs [vs xs, s, c] + (1 w)u1 = wu(xs, s, c) + (1 w)u(xs, s) uw θmin θmin (xs, s, ) xs, s, c, w(cid:3) (xs, s, ) θmin (61) (62) (63) (64) (65)"
        },
        {
            "title": "B ADDITIONAL DETAILS ON PRACTICAL CHALLENGES",
            "content": "B.1 LIPSCHITZNESS OF RMSNORM Recall the definition of RMSNorm, for input Rd and small constant ϵ > 0 RMSNorm(x) = RMS(x) , where RMS(x) = (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) i=1 xi + ϵ (66) And its Jacobian can be calculated as dxj RMSNorm(xi) = = = (cid:19) (cid:18) xi RMS(x) dxj δijRMS(x) xixj/RMS(x)/d RMS(x)2 xixj RMS(x)3 δij RMS(x) (67) (68) (69) 17 def f d (net , xt , t, s, c, w): def model wrapper ( , , ): # we use ts for second time condition return net( , , ( ), c, w) F, dFds = torch.func.jvp( model wrapper , (xt , t, s), (0, 0, 1)) ts = xt + (s t) * dfds = (F + (s t) * dFds) return ts , dfds Figure 11: PyTorch-style JVP code. Since matrix norm (largest singular value) σ(A) of matrix is upper bounded by its Frobenius norm, and RMS(x) ϵ, we have each element RMSNorm(xi) in the Jacobian matrix bounded dxj via (cid:12) (cid:12) (cid:12) (cid:12) dxj RMSNorm(xi) 2 (cid:12) (cid:12) (cid:12) (cid:12) = = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) δij RMS(x) δij RMS(x) 2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) + + (cid:12) xixj (cid:12) (cid:12) RMS(x)3 (cid:12) (cid:32) (cid:33)2 xi/ RMS(x) 2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:33)2 (cid:32) xj/ RMS(x)"
        },
        {
            "title": "1\nRMS(x)",
            "content": "+ 1 ϵ 1 ϵ 2 ϵ (70) (71) (72) (73) Therefore, the Frobenius norm is bounded and hence the matrix norm. B.2 FULL DESCRIPTION OF NORMALIZATION OF MODULATION Note that there are 6 modulation parameters in total for each DiT layer, denoted as a1(t), b1(t), c1(t), a2(t), b2(t), c2(t) = split(AdaLN Modulation(t), 6) (74) and we pass each of the above parameters through RMSNorm() to obtain 1 (t), 2 (t), 1 (t), a 2 (t) (which can be done in parallel) and the new normalized DiT layer is 1 (t) ATTN(RMSNorm(x) 2 (t) MLP(RMSNorm(x) = + = + 1 (t), 2 (t), 1 (t) + 2 (t) + 1 (t)) 2 (t))"
        },
        {
            "title": "C FLASH ATTENTION JVP WITH BACKWARD PASS",
            "content": "In transformer models, scaled dot-product attention (SDPA) is often among the most, if not the most, computationally expensive operations. The cost stems not only from its high FLOP requirements O(M ) in general, and O(N 2) in the case of self-attention but also from the quadratic memory footprint of the querykey matrix multiplication. Computing the Jacobian-Vector Product (JVP) of SDPA is even more demanding, typically requiring about three times the cost of the standard forward pass. Flash attention (Dao et al., 2022) fuses the matrix multiplication with an online softmax operation (Milakov & Gimelshein, 2018), thereby eliminating the need to store the intermediate QK matrix in GPU memory. Subsequent work has shown that JVP SDPA can also be implemented in FlashAttention-style manner, where both, primal SDPA and JVP SDPA are computed jointly to avoid redundant computation (Lu & Song, 2024). Building on these ideas, we implement efficient JVP SDPA forward and backward kernels in Triton. We first take inspiration from open-source implementations without backward support4. And the 4https://github.com/Ryu1845/min-sCM/blob/main/standalone_multihead_jvp_ test.py additional backward pass through the standard (primal) SDPA is handled independently using the open-source implementation from (Dao et al., 2022). To obtain full gradients with respect to Q, K, and , we combine the input gradients from both backward passes. Similar to standard SDPA, the JVP backward pass can leverage online softmax to avoid storing large intermediate matrices in GPU memory. However, the increased complexity of JVP SDPA requires additional optimizations to run efficiently on GPUs. Most notably, we found it crucial to split the backward computation into multiple smaller kernels to reduce register spills caused by the large number of intermediate tensors. Background. Recall the attention operation as ATTN(Q, K, ) = softmax (cid:19) (cid:18) QK dk (75) and let the query, key, and value blocks be denoted by RM d, RN and RN d. The tangent inputs are denoted as Q, K, . We use α = 1 as the softmax scaling factor, and ℓi dk denotes the log-sum-exponential normalization for the i-th row of the attention scores, short form for combining the softmax stabilization factor and the normalization. C.1 MULTI-STEP BACKWARD PASS For best performance, we decided to split up the backward pass into multiple smaller operations with shared paths through the graph. Furthermore, the gradients dQ and are computed in row-parallel order, while dK, K, dV and are processed in column-parallel order. In our tests, redundant, but coalesced computation of the large parts of the backward pass greatly outperformed single, fused kernel relying on atomic operations. We split the operation into 6 steps: 1) preprocess shared intermediates, 2) process and first part of dK, 3) process and first part of dQ, 4) process second part of dK, 5) process second part of dQ, 6) process and dV . Step 1: Preprocess shared intermediates row-parallel. In the first step, we preprocess two intermediate sums Σ1 RM and Σ2 RM used in steps 2-5. Σ1,i = (cid:88) (cid:16) OV (cid:17) Pij ij Σ2,i = (cid:88) Pij (cid:18)(cid:16) V (cid:17) (cid:16) + ij OV (cid:17) ij (cid:19) Nij where amd Pij = exp (αSij ℓi) , Sij = dk(cid:88) r=1 QirKjr, Nij = α Sij µi li Sij = dk(cid:88) r= (cid:16) QirKjr + Qir Kjr (cid:17) , µi = (cid:88) (cid:16) α Sij (cid:17) Pij Step 2: process and dK1 column-parallel. (dK1)j,: = α (cid:20)(cid:18)(cid:16) (cid:88) OV (cid:17) (cid:19) (cid:21) Σ1,i Pij Qi,: ij (cid:19) (cid:21) Σ1,i Pij Qi,: ij (d K)j,: = α (cid:20)(cid:18)(cid:16) (cid:88) OV (cid:17) 19 (76) (77) (78) (79) (80) (81) Step 3: Process and dQ1 row-parallel. (dQ1)i,: = α (cid:20)(cid:18)(cid:16) (cid:88) OV (cid:17) (cid:19) (cid:21) Σ1,i"
        },
        {
            "title": "Pij",
            "content": "Kj,: ij (d Q)i,: = α (cid:20)(cid:18)(cid:16) (cid:88) OV (cid:17) (cid:19) (cid:21) Σ1,i"
        },
        {
            "title": "Pij",
            "content": "Kj,: ij (82) (83) Step 4: Process dK column-parallel. (dK)j,: = (dK1)j,: + α (cid:26)(cid:20) (cid:88) α (Σ1,i) Sij + Σ1,i (cid:20)(cid:16) V (cid:17) + (cid:16) + ij Step 5: Process dQ row-parallel. (cid:21) µi li OV (cid:17)"
        },
        {
            "title": "Pij",
            "content": "(cid:18) ij α Sij (cid:19) µi li (cid:21) (cid:27) Σ2,i"
        },
        {
            "title": "Pij",
            "content": "Qi, (84) (dQ)i,: = (dQ1)i,: + α (cid:26)(cid:20) (cid:88) α (Σ1,i) Sij + Σ1,i (cid:20)(cid:16) V (cid:17) + (cid:16) + ij Step 6: Process dV and column-parallel. (cid:21) µi li OV (cid:17) Pij (cid:18) ij α Sij (cid:19) µi li (cid:21) (cid:27) Σ2,i Pij Kj,: (d )j,: = Pij(d O)i,: (cid:88) (dV )j,: = (cid:88) (cid:18) (cid:20) Pij α Sij (cid:19)(cid:21) µi li (d O)i,: (85) (86) (87) Caching softmax statistics. Like previous flash-attention implementations, we cache softmax statistics from the forward pass to speed up the backward pass, namely the log-sum-exp ℓ, the sums and µ for each row of the output O. Thus, the total overhead of the cache is only three values per row of Q. C.2 EVALUATION We built test bench to evaluate latency and peak memory consumption of our flash JVP SDPA kernels on different input shapes using an NVIDIA H100 SXM 80GB. Due to the lack of existing alternatives, we compare against vanilla SDPA, i.e. SDPA written as explicit math operations, which currently is the only way to train transformers in PyTorch with JVP enabled. As our contribution focuses on the backward pass, we limit the latency and peak memory evaluation to the backward pass of single SDPA operation, combining both paths through the primal (normal) and the tangent (JVP) gradients. Results. Shown in Table 5, our implementation achieves significant reduction in peak memory consumption. Compared to the reference, we save memory not only by reducing the cached variables between forward and backward pass, but more importantly by avoiding to store 2 intermediate attention scores. At the same time, our implementation achieves speedup of up to 65% compared to the reference. 20 1 1 1 1 1 1 24 24 24 24 24 24 128 1,024 4,096 8,192 16,384 32,768 128 1,024 4,096 8,192 16,384 32,768 Latency [ms] Peak Memory [MB] ours vanilla ours 1.31 1.38 1.96 3.98 10.06 40.24 1.40 1.42 15.13 58.70 238.4 958.6 1.51 1.54 1.53 4.33 16.11 63.85 1.55 2.03 24.52 96.93 - - 64.69 69.52 86.06 108.1 152.3 240.5 80.55 196.4 593.5 1,123 2,182 4,300 vanilla 64.80 94.02 508.1 1,816 7,024 27,808 83.17 784.4 10,721 42,115 - - Table 5: Performance comparison of our flash JVP kernels against vanilla SDPA kernels in PyTorch. and stand for number of heads in multi head attention and sequence length. Vanilla SDPA ran out of memory on NVIDIA H100 in the last two tests. Algorithm 1 TVM Training Input: initialized model θ, data p0(x0, c) and prior p1(x1), time distribution p(t, s), guidance distribution p(w) Initialize θ θ, θ θ while model not converged do // θ, θ are EMA with rate λ, λ. Sample (x0, c, x1) p0(x0, c)p1(x1) Randomly drop with prob. 10% Sample (t, s, w) p(t, s)p(w) xt (1 t)x0 + tx1 xs (1 s)x0 + sx1 vs x1 x0 θ optimizer step by minimizing ˆLTVM(θ) = Et,s,w // optionally set xs (1 s)x0 + sx1. // optionally set vs x1 x0. (cid:104) ˆLt,s,w (cid:105) TVM (θ) // see Eq. (15) // optionally sample p(s) for the second loss term. // optionally use and xs for the second loss term θ EMA update with rate λ θ EMA update with rate λ end while Output: learned model θ"
        },
        {
            "title": "D TRAINING ALGORITHM",
            "content": "We present the training algorithm in Algorithm 1. We additionally show PyTorch-style pseudocode in Figure 11 for calculating fθ(xt, t, s) and ds fθ(xt, t, s) together with one JVP pass."
        },
        {
            "title": "E RELATION TO PRIOR WORKS",
            "content": "E.1 MEANFLOW Let fθ(xt, t, s) = (s t)Fθ(xt, t, s), we inspect dt fθ(xt, t, s) + u(xt, t) = Fθ(xt, t, s) + (s t) = Fθ(xt, t, s) + (s t) Fθ(xt, t, s) + u(xt, t) dt (cid:104) u(xt, t) xtFθ(xt, t, s) + tFθ(xt, t, s) (cid:105) + u(xt, t) 21 (88) (89) (90) Parameterization Setting ImageNet-256256 ImageNet-512512 Architecture Params (M) 2nd time conditioning Hidden dim Number of heads Main normalization QK-Norm type Linear layer init5 Time Embed init6 Training iter Training Setting Optimizer Optimizer ϵ β1 β2 Learning rate Weight decay Batch size p(s, t) Scaled param. % = s7 Target EMA rate Eval EMA rate8 Label dropout DiT-XL/2 678 1152 18 DiT-XL/2 678 1152 18 DiT-XL/2 678 1152 18 DiT-XL/2 678 1152 RMS Norm RMS Norm RMS Norm RMS Norm RMS Norm RMS Norm RMS Norm RMS Norm Spectral Spectral 300K Spectral Spectral (0, 0.02) (0, 0.02) 300K 300K Spectral Spectral 300K AdamW 108 0.9 0.95 0.0001 0 2048 yes 0% 2 0.99 0.9999 0.1 AdamW 108 0.9 0.95 0.0001 0 2048 AdamW 108 0.9 0.95 0.0001 0 gap* (0.8, 1.0), (0.4, 1.0) yes 0% 1.75 0.99 0.9999 0.1 no 0% 2.5 0.99 0.9999 0.1 AdamW 108 0.9 0.95 0.0001 0 2048 yes 0% 2.25 0.99 0.9999 0.1 Table 6: Experimental settings for different architectures and datasets. Therefore, fθ(xt, t, s) + u(xt, t) (cid:13) (cid:13) (cid:13) dt (cid:13) (cid:13) (cid:13) (cid:13) Fθ(xt, t, s) + (s t) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (cid:104) = u(xt, t) xtFθ(xt, t, s) + tFθ(xt, t, s) (cid:124) (cid:123)(cid:122) Ftgt which is the MeanFlow loss."
        },
        {
            "title": "F ADDITIONAL EXPERIMENT DETAILS",
            "content": "We present the overall training details in Table 6 F.1 ARCHITECTURE AND OPTIMIZATION (91) (92) (cid:105) (cid:13) 2 (cid:13) (cid:13) + u(xt, t) (cid:125) VAE. We follow Zhou et al. (2025) for the VAE setting, which uses the standard Stable Diffusion VAE (Rombach et al., 2022) but with different scale and shift. Please refer to the paper for details. Architecture. All architecture decisions follow DiT (Peebles & Xie, 2023) except for the changes described in the main text. For our XL-sized model, we follow DiT-XL and use 1152 hidden size 5Except for zero-init layers in AdaLN-Zero. 6All time embedding MLP layers before input into DiT blocks. 7This means the percentage of setting = s, e.g. 0% means both loss terms exist at all times. 8EMA used for final evaluation, separate from the EMA used for training target. 22 but use 18 heads instead of 16 heads. This is purely for efficiency reasons because 18 heads under 1152 total hidden size implies head dimension is 64, while the original 16 heads result in head dimension 72. Flash attention JVPs runtime is sensitive to redundancy in memory allocations. As 64 is power of 2 our kernel can fully allocate appropriately sized CUDA blocks, while 72 leaves significant chunks unused. We observe that the original 16-head decision is 1.25 slower than the 18-head variant. In comparing FID of the two versions, we observe they perform similarly throughout training. Following Zhou et al. (2025), we use as our second time condition into the architecture rather than directly injecting s. For injecting w, we follow Chen et al. (2025) and use β = 1/w as our condition, and if random CFG is used training, we sample β U( 1 ) and set = 1/β. Note wmax that Chen et al. (2025) uses β U(0, 1) which amounts to wmin = 1 and wmax = , but arbitrarily large is never used in practice so wmax can be set to realistic finite value. , 1 wmin Optimization. Besides setting β2 = 0.95, we follow the default optimizer used by DiT and optimize with BF16 precision. We de not use any learning rate scheduler. F.2 DETAILS ON RANDOM CFG WITH MEANFLOW In MeanFlow (Geng et al., 2025), the authors introduce mixing scale κ such that the field with guidance scale is given by v(xt, t, c, w) = wvt + κuθ(xt, t, c, w) + (1 κ)uθ(xt, t, w) (93) It specifies that the effective guidance scale is = v(xt, t, c, w), rearranging it to LHS and dividing both sides by (1 κ) gives (1κ) . This is because since uθ(xt, t, c, w) (1 κ)v(xt, t, c, w) = wvt + (1 κ)uθ(xt, t, w) (1 κ) (1 κ) v(xt, t, c, w) = vt + (1 )uθ(xt, t, w) (94) (95) This constrains κ [0, 1). However, in the case of random CFG, to make use of uθ(xt, t, c, w), we try the simple linear mixing (the default CFG reweighting) v(xt, t, c, + κ) = wvt + κuθ(xt, t, c, 1) + (1 κ)uθ(xt, t, 1) (96) where and κ are both randomly sampled with finite boundaries. In this case uθ(xt, t, c, 1) v(xt, t, c, +κ) and thus κ is not constrained to be smaller than 1. When = 0, it becomes regular CFG with network approximation of the CFG velocity, and when κ = 0 it becomes MeanFlow CFG with vt approximation of the CFG velocity. This construction subsumes both implementation cases. In our experiments, we use κ U(0, cmax), U(1, cmax) for some constant cmax. However, we acknowledge that this observed training fluctuation may depend on exact training settings and environments, and may be fixable via empirical tricks such as adjusting AdamW parameters or gradient clipping, etc. We present the training in the simplest settings without such tricks to best illustrate our point. F.3 CFG-CONDITIONED FLOW MATCHING As in our method, we similarly observe tradeoff in FID if FM is trained to condition on CFG scale with randomly sampled during training (Chen et al., 2025). During inference time, is injected into the network so that the CFG velocity field can be approximated by single forward call. We inject using positional embedding just like the diffusion time, and during training we sample β U(0, 1) and set = 1/β, following Chen et al. (2025). We show in Figure 12 that as the model trains, the FID of = 1.5 decreases but = 2 increases for later training steps. This tradeoff is similarly observed in our method as presented in the main text. 23 Figure 12: w-conditioned FM training experiences tradeoff. F.4 ADDITIONAL VISUAL SAMPLES Figure 13: Additional ImageNet-256256 samples from 1-NFE TVM model."
        }
    ],
    "affiliations": [
        "Luma AI"
    ]
}