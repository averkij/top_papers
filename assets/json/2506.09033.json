{
    "paper_title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning",
    "authors": [
        "Haozhen Zhang",
        "Tao Feng",
        "Jiaxuan You"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \\textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave \"think\" actions (internal deliberation) with \"route\" actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management.Code is available at https://github.com/ulab-uiuc/Router-R1."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 3 3 0 9 0 . 6 0 5 2 : r Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning Haozhen Zhang1 Tao Feng Jiaxuan You University of Illinois at Urbana-Champaign {haozhenz,taofeng2,jiaxuan}@illinois.edu, 1wazhz14@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform single-round, one-to-one mapping (i.e., assigning each query to single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present Router-R1, reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as sequential decision process. Router-R1 instantiates the router itself as capable LLM, leveraging its reasoning ability to interleave think actions (internal deliberation) with route actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ lightweight rule-based reward comprising format rewards, final outcome rewards, and novel cost reward for performance and cost trade-off optimization, opening pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management. Code is available at https://github.com/ulab-uiuc/Router-R"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have proliferated at an unprecedented pace, with new architectures and fine-tuned variants released on monthly or even weekly basis [21, 1]. To harness the complementary strengths of multiple LLMs (e.g., one models fluency versus anothers factual accuracy), LLM routers have emerged as critical infrastructure component, dynamically dispatching user queries to single selected model in order to maximize answer quality and efficiency [3, 6, 2, 5, 26, 30, 4]. While this one-shot routing strategy can improve average performance, it overlooks the fact that truly complex reasoning tasks often require coordinated interactions among multiple models, orchestrating not just single choice but sequence of model calls to leverage their complementary strengths. This observation raises key challenge: how can we coordinate multiple LLMs in multi-round routing and aggregation process to jointly solves complex tasks? Answering this challenge is non-trivial. First, the discrete decision process of selecting which LLM to call at each round is inherently non-differentiable, precluding straightforward end-to-end training via backpropagation. Although prior work has applied gradient-based methods to single-shot routing [3, 6, 2, 5], extending them to multi-round selection and aggregation rapidly becomes intractable. Second, existing routers operate in single-step regime: given query, pick one model, collect its output, and stop [3, 6]. However, complex tasks (e.g., multi-hop question answering) demand Work done as an intern at University of Illinois at Urbana-Champaign sequence of interleaved reasoning and model-selection decisions, and one-off choice rarely suffices. Therefore, we must design interplay mechanism that alternates between long thought reasoning (internal deliberation) and targeted LLM selection to iteratively refine an answer. To address these challenges, we introduce Router-R1, reinforcement learningbased framework for multi-round LLM routing and aggregation. Rather than making single dispatch decision, we formulate LLM coordination as sequential decision-making problem: at each step, Router-R1 chooses whether to perform internal reasoning (think) or to invoke specific model from pool of available LLMs (route), gradually constructing an answer through iterative interaction. To support this interplay between reasoning and routing, we instantiate the router itself as capable LLM, leveraging its inherent reasoning capability to perform long-form deliberation and targeted model selection. This flexible interleaving of reasoning and model selection enables Router-R1 to adaptively compose the strengths of multiple LLMs in task-aware manner. To optimize this decision policy, we adopt reinforcement learning and design simple yet effective rule-based reward function composed of three parts: format reward for producing well-structured outputs, final outcome reward based on task correctness, and cost reward that penalizes excessive use of expensive routed models, providing Router-R1 the capability to navigate performancecost trade-offs during training. Additionally, Router-R1 also exhibits strong generalization capability to newly added LLM candidates without the need for retraining by conditioning its routing decisions on simple descriptors such as pricing, latency, and example performance. Together, these components make Router-R1 robust and flexible solution for coordinating multiple LLMs to solve complex reasoning tasks. Through comprehensive experiments on seven diverse QA benchmarks, covering both general and multi-hop question answering, we demonstrate that Router-R1 consistently outperforms over several strong baselines and achieves state-of-the-art performance. Further analyses on cost-aware routing and generalization to unseen LLMs also highlight the flexibility and robustness of our approach. Our contributions are summarized as follows: We propose Router-R1, reinforcement learningbased framework for multi-round LLM routing and aggregation. By instantiating the router itself as capable LLM, Router-R1 naturally interleaves internal reasoning and external model selection, enabling adaptive coordination across multiple LLMs for complex task solving. We design simple and effective rule-based reward function consisting of format rewards, final outcome rewards, and cost rewards, enabling Router-R1 to navigate performancecost trade-offs. Additionally, by conditioning on simple model descriptors, Router-R1 can generalize to unseen LLMs without retraining. Through extensive experiments on seven question-answering benchmarks, we show that Router-R1 outperforms several competitive baselines, achieving superior performance and robust generalization."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Query-based Routers for LLM Selection The rapid rise of various LLMs has spurred the development of query-based LLM routers, which aim to direct queries to the most appropriate model to improve response quality and efficiency. HybridLLM [5] proposes dynamic router that selects between small and large LLMs based on predicted query difficulty and user-defined quality budget. GraphRouter [6] frames LLM selection as inductive edge prediction over taskquerymodel graph, enabling cost-performance estimation and effortless integration of new LLMs. To explicitly balance performance and cost, FrugalGPT [2] adopts an LLM cascade approach, while FORC [26] routes queries to appropriately sized models for cost-effective inference. TO-Router [30] unifies multiple domain-specific expert LLMs under one interface and dispatches queries based on task needs. C2MAB-V [4] leverages cost-aware combinatorial multi-armed bandit to dynamically select optimal LLM subsets. RouterDC [3] enhances routing via dual contrastive learning between queries and LLM embeddings. Finally, RouteLLM [22] utilizes human preference data to dynamically choose between strong and weak LLMs, effectively reducing costs while maintaining quality. In contrast to previous methods, Router-R1 treats routing as sequential decision process, interleaving internal think steps with multi-round model routing to refine its answer by instantiating the router 2 itself as an capable LLM. Moreover, its RL-based training harnesses cost reward to balance performance and cost, enabling flexible performancecost trade-offs and resource-aware routing. 2.2 Optimizing LLM Behaviors via Reinforcement Learning In recent years, reinforcement learning (RL) has emerged as powerful paradigm for fine-tuning large language models (LLMs) to better align with human preferences. Early work like RLHF (Reinforcement Learning from Human Feedback) [23], trains reward model on human judgments and then applies policy-optimization algorithms such as PPO [27] to steer the base language model toward more desirable outputs. Building on this foundation, RLAIF [17] shows comparable or better performance on tasks like summarization and dialogue, and its direct variant (d-RLAIF) removes the need for an explicit reward model, improving efficiency. RRHF [37] ranks model-generated and external responses to train preference without reward predictor, achieving results on par with RLHF. More recently, Direct Preference Optimization (DPO) series methods [25, 20] take further step by training directly on human preference data, avoiding RL sampling and complex tuning while matching or outperforming RLHF. Alongside these advances in reward-driven fine-tuning, other RL-based techniques such as Search-R1 [13] have empowered LLMs to adaptively interact with external tools like search engines, allowing them to dynamically retrieve and incorporate external information during inference. These approaches highlight the potential of RL in optimizing LLM behaviors beyond static prompt-based generation, particularly in tasks requiring real-time information access and decision-making."
        },
        {
            "title": "3 Router-R1",
            "content": "In this section, we detail our proposed Router-R1 from three parts. Section 3.1 introduces the reinforcement learning formulation with LLM routing pool. Section 3.2 presents the reward curation strategy, which includes format rewards, final outcome rewards, and cost rewards. Section 3.3 describes the multi-round interaction training process, including training prompt template and multiround interaction with LLM routing pool. We show the architecture of Router-R1 in Figure 1. 3.1 Reinforcement Learning via Coordination with LLM Routing Pool In Router-R1, we adopt general policy optimization objective with LLM routing pool formulated as: max π ExD, yπ(x;P) (cid:20) rϕ(x, y) β log (cid:21) π(y x; P) πref(y x; P) , (1) where π denotes the policy LLM to be optimized, and πref is reference LLM that may be fixed or updated iteratively for stable training. represents input samples from the dataset D, and denotes the generated outputs sampled from the policy πref(y x; P), which are interleaved with results obtained from accessing the LLM routing pool P. rϕ(x, y) is the reward function, and is the LLM routing pool, which provides set of candidate LLMs available for selection. The KL regularization term ensures that the updated policy remains close to the reference, with the regularization coefficient β controlling this trade-off. This formulation is general and encompassed various regularized reinforcement learning algorithms such as PPO [27], GRPO [28], and KL-constrained methods, allowing for flexible policy updates over pool of LLM candidates. Moreover, under this optimization objective, the policy LLM can dynamically select candidate LLM from the LLM routing pool multiple rounds, and obtain auxiliary information about the input sample by providing the candidate LLM with relevant context, thereby enhancing the reasoning process of the policy LLM. In this case, the policy LLM can be regarded as coordinator, selecting and coordinating multiple candidate LLMs to jointly solve complex tasks. 3.2 Reward Curation To provide Router-R1 with reasonable and effective supervision signals, we carefully design the reward function, including format rewards, final outcome rewards, and cost rewards, which we will describe in detail below. 3 Figure 1: Router-R1 architecture. (a) Single-round Routing: conventional router assigns each query to single LLM in isolation via one-shot decision, without internal reasoning or multi-model coordination. (b) Multi-round Routing (ours): Router-R1 casts multi-LLM routing as sequential decision process, which leverages an LLM-based router to interleave internal reasoning with external LLM routing and integrates retrieved information into its evolving context. This enables adaptive multi-model coordination for complex tasks, surpassing single-round routing with better performance. 3.2.1 Format Rewards Inspired by [8], to stabilize the training and ensure that the LLM response rollouts conform to the predefined format (detailed below in Section 3.3), we impose strict format validation on Router-R1. Specifically, format reward is assigned according to the following rule: if the response rollouts do not satisfy the required format, the format reward is set to -1; otherwise, it is set to 0: Rformat = (cid:26)1, 0, if the format is incorrect if the format is correct (2) 3.2.2 Final Outcome Rewards In Router-R1, we adopt Exact Match (EM) to measure the correctness between the answer predicted by LLM and the ground truth, and utilize it as the only final outcome reward to guide the optimization of Router-R1: Routcome = EM(ya, gt), (3) where ya is the predicted answer extracted from the generated output and gt denotes the ground truth. EM emphasizes the complete and strict matching of the predicted answer with the golden answer, which has been proven by many works to be concise and effective rule-based reward [8, 13]. 3.2.3 Cost Rewards To balance the additional cost of invoking candidate LLM from the routing pool, we introduce the computational cost incurred by querying candidate LLM as cost reward. This design enables Router-R1 to potentially optimize not only for model performance but also for the trade-off between performance and computational efficiency. Formally, the cost reward is inversely proportional to both the number of output tokens produced by the candidate LLM and the model-dependent cost-per-token function, which reflects the computational price of using different models: Rcost m(PLLM) Tout, (4) where PLLM denotes the number of parameters of the selected candidate LLM and Tout is the number of output tokens generated by it. m() is predefined cost function that maps model size to its per-token computational cost (e.g., based on pricing tiers of the LLM API service). Note that the cost reward will be normalized to between 0 and 1 during training. In this way, the larger the model size, the more output tokens, and the smaller the cost reward, which provides Router-R1 with the capability to achieve performance-cost balance. 4 3.2.4 Overall Rewards To sum up, the overall reward function of Router-R1 can be defined as: rϕ(x, y) = Rformat + (1 α)Routcome + αRcost (5) where α serves as hyper-parameter controlling the balance between model performance and cost. In particular, to mitigate reward hacking [29] and improve optimization stability, we introduce hierarchical reward as refinement to the overall reward function (not shown in Formula 5 for brevity). In Router-R1, the three reward components are assigned different priorities, decreasing from left to right in Formula 5. Concretely, if the format reward is -1, the remaining two rewards are nullified (set to zero), regardless of their original values. This hierarchical design enforces critical constraints before optimizing for performance or computational efficiency, and contributes to the stable and reliable training of Router-R1. In our experiments, such clear rule-based reward combination is sufficient to optimize Router-R1 well, which also demonstrates the rationality and effectiveness of our reward function design. 3.3 Multi-Round Interaction Training Paradigm In this section, we describe the training prompt template of Router-R1 and the multi-round interaction with LLM routing pool. 3.3.1 Training Prompt Template Inspired by [8, 13], we construct the training prompt template for Router-R1 as shown in Figure 2. To ensure accurate and well-justified responses, we adopt structured prompt combining internal reasoning and selective external querying. Upon receiving question, Router-R1 first performs internal analysis within <think> and </think> block to assess whether additional information is required. If so, it queries suitable specialized LLM via <search> Candidate LLM: Query </search> based on predefined LLM routing pool and model descriptions that include details such as parameter size and task specialization (we detail it in Appendix C. ). Retrieved information is returned within <info> and </info> tags, and the process may iterate to gather complementary insights. The final answer is output within an <answer> and </answer> block. It is worth noting that the model descriptions here serve only as an initial prior for each candidate LLM. During the policy optimization process, the policy LLM adaptively learns the strengths and weaknesses of each candidate LLM through interaction and feedback. To further enhance adaptability, the prompt design also supports the seamless integration of new candidate LLMs without requiring retraining. Specifically, Router-R1 can achieve this generalization capability by incorporating the descriptions of newly added LLMs directly into the prompt. This flexibility allows Router-R1 to dynamically expand its routing pool and effectively accommodate the rapid and ongoing emergence of new LLMs. Answer the given question. Every time you receive new information, you must first conduct reasoning inside <think> and </think>. After reasoning, if you find you lack some knowledge, you can call specialized LLM by writing query inside <search> Candidate LLM: Query </search>. Before each LLM call, you must explicitly reason inside <think> and </think> about \"why external information is needed\" and \"which LLM from the list is most suitable for answering your query,\" based on the brief model descriptions provided below. When you call an LLM, the response will be returned between <info> and </info>. You are encouraged to explore and utilize different LLMs multiple times to better understand their respective strengths and weaknesses, as well as gather more comprehensive information. Description of LLM Candidates: {candidates_intro} If you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> and </answer>, without additional explanation or illustration. Question: {question} Figure 2: Training prompt template for Router-R1 (some texts are omitted for page space). 3.3.2 Multi-Round Interaction with LLM Routing Pool According to the prompt in Figure 2, Router-R1 first analyzes the input question to identify the necessary information and selects the most appropriate candidate LLM from routing pool to query via sub-question. During training, it learns to decompose complex queries and route them adaptively based on the strengths of different LLMs. The routing process is triggered whenever special <search> tag appears in the generated sequence, specifying the target LLM and intended sub-query. Upon detection, Router-R1 queries the designated LLM and inserts its response back into the sequence to continue reasoning (to ensure stable training, external responses marked by <info> tags are excluded from the loss computation). For complex tasks, Router-R1 can perform multi-round routing, iteratively integrating information from multiple sources to arrive at final answer. For simple questions, Router-R1 can rely solely on the policy LLMs internal knowledge to produce answers, demonstrating its ability to judge whether external information is needed."
        },
        {
            "title": "4 Experimental Setup",
            "content": "4.1 Datasets and Metrics We evaluate Router-R1 on seven question-answering (QA) datasets, i.e., (1) General QA: Natural Question (NQ) [16], TriviaQA [14], PopQA [19]; (2) Multi-Hop QA: HotpotQA (HpQA) [36], 2WikiMultiHopQA (2wiki) [9], Musique [32], and Bamboogle (Bamb) [24]. These datasets cover both single-hop and multi-hop QA benchmarks, providing comprehensive testbed for evaluating the performance of Router-R1. For metrics, the correctness of the predictions generated by Router-R1 is evaluated using the Exact Match (EM) and F1-Score (F1) against the ground truth. 4.2 Baselines To have comprehensive evaluation, we compare our proposed Router-R1 with dozens of baselines: Basic Baselines. (1) Direct Inference (Direct), (2) Chain-of-Thought (CoT) Prompting [34], (3) supervised fine-tuning (SFT), (4) Retrieval-Augmented Generation, which utilizes Wikipedia-18 [15] as the external knowledge and E5 [33] as the retriever (RAG), (5) SearchR1 [13]. Query-based LLM Routers. These baselines follow similar setup: one or more candidate LLMs are selected from the routing pool to answer the input question (or its sub-questions, unless specified), and their responses are integrated into the base models input to generate the final answer, simulating Router-R1s response generation process for fair comparison. (6) Prompt the base LLM model to select candidate LLM (Prompt LLM), (7) always select the largest LLM (Largest LLM), (8) KNN Router [10], (9) MLP Router [10], (10) BERT Router [22], (11) RouterDC [3], (12) GraphRouter [6], (13) prompt the base LLM to decompose the original question into sub-queries and assign each to candidate LLM (Prompt LLM*), (14) prompt the base LLM to decompose the original question into sub-queries and utilize KNN Router to assign each to candidate LLM (KNN Router*). To train query-based LLM routers, we additionally construct dedicated router training dataset. Specifically, each training question is independently fed to every model in the LLM routing pool multiple times with temperature sampling. The responses are then evaluated using the EM metric to assess answer quality. This process yielded, for each question, set of EM scores corresponding to all candidate LLMs in the routing pool, effectively labeling each LLM with its performance on that question. These questionLLM score pairs form the supervision signal for training the router. 4.3 Implementation Details We conduct our experiments using Qwen2.5-3B-Instruct [35] and LLaMA-3.2-3B-Instruct [7] as base models for training. To empower LLMs with multi-round routing capabilities, we design Router-R1 with maximum of 4 routing steps per input query. The model is built with Search6 R1 framework [13] and trained using veRL 2 for reinforcement learning in LLMs, employing the Proximal Policy Optimization (PPO) as the default algorithm. The batch size is set to 64, with maximum of 225 training steps. The cost coefficient α is set to 0.0 in our main experiment unless otherwise specified. To incentivize both single-round and multi-round routing capabilities during training, we construct joint dataset consisting of 7K samples each from NQ and HotpotQA datasets, respectively. This results in 14K sample training set, which we find sufficient to induce effective routing strategies without requiring extensive data filtering or complex sampling procedures. As demonstrated in our experimental analysis in Section 5, this modestly sized dataset enables robust learning of routing and aggregation behavior. After training, we evaluate in-domain performance on NQ and HotpotQA datasets, where Router-R1 has seen similar data during training, and assess out-of-domain generalization performance across five other QA datasets mentioned above. For each evaluation dataset, we randomly sample 500 test instances (except for Bamboogle, which contains only around 120 test examples in total). All baseline models are trained (if needed) and evaluated under the consistent dataset and settings to ensure fair comparison. To support routing strategies, we adopt diverse LLM routing pool comprising six representative models of varying sizes and families: Qwen2.5-7B-Instruct [35], LLaMA-3.1-8B-Instruct [7], LLaMA-3.1-70B-Instruct [7], Mistral-7B-Instruct [12], Mixtral8x22B-Instruct [11], and Gemma-2-27B-Instruct [31]. The base model training is conducted on NVIDIA A6000 GPUs, while routing LLMs are accessed via NVIDIA NIM APIs3. More details about experimental details and prompts are described in Appendix and C. We also provide case study of Router-R1 in Appendix B."
        },
        {
            "title": "5 Experimental Analysis",
            "content": "In this section, we present comprehensive empirical studies to evaluate the effectiveness of Router-R1. We analyze its performance across diverse QA benchmarks (Section 5.1), explore its adaptability to cost constraints (Section 5.2), test its generalization capability when encountering unseen LLMs (Section 5.3), and provide insights into its routing behavior and training dynamics (Section 5.4). 5.1 Main Results We conduct comprehensive experiments across seven QA benchmarks, spanning both general QA and multi-hop QA settings, to evaluate the effectiveness of Router-R1 against wide range of baselines. The results are presented in Table 1, from which we can derive some key observations. Router-R1 consistently outperforms all basic baselines across all seven datasets, achieving SOTA performance. Compared to Direct, CoT, and SFT, which rely solely on the base LLMs internal knowledge and reasoning capabilities, Router-R1 delivers substantially better results, particularly on these knowledge-intensive tasks where those baselines often struggle. It also outperforms RAG by significant margin, as Router-R1 can dynamically query specialized LLMs during multi-round reasoning, offering more flexibility and relevance than methods relying on static external retrieval. Among these baselines, Search-R1 stands out as stronger method that supports multi-turn search engine calling, resembling Router-R1s multi-round routing capabilities. However, Router-R1 still shows clear advantages, achieving better results across both base LLM models and most QA datasets. Notably, Router-R1-Llama reaches the highest average exact match score of 0.409, while RouterR1-Qwen further improves this to 0.416. These results demonstrate the superior effectiveness and adaptability of Router-R1 in routing decisions for both general and multi-hop QA. Router-R1 significantly surpasses all LLM router baselines in overall performance, benefiting from its multi-round routing and interleaved reasoning. Compared to single-round routers like Prompt LLM and KNN Router, Router-R1 shows clear and consistent improvements. Even when enhanced baselines like Prompt LLM* and KNN Router* are introduced, where inputs are decomposed into sub-queries before routing, Router-R1 maintains significant advantage. This superiority stems from the core design of Router-R1: using an LLM as the router itself, enabling flexible interleaving of reasoning and routing steps. Such design allows Router-R1 to adaptively 2 https://github.com/volcengine/verl 3 https://build.nvidia.com/ 7 Table 1: Experimental results on seven QA datasets w.r.t. Exact Match. Methods General QA Multi-Hop QA NQ TriviaQA PopQA HpQA 2wiki Musique Bamb Avg. Qwen2.5-3B-Instruct Direct CoT SFT RAG Search-R1 Prompt LLM Largest LLM KNN Router MLP Router BERT Router RouterDC GraphRouter Prompt LLM* KNN Router* 0.092 0.126 0.212 0.298 0.328 0.300 0.296 0.262 0.252 0.230 0.278 0.276 0.258 0.236 Router-R1-Qwen 0.388 Llama-3.2-3B-Instruct Direct CoT SFT RAG Search-R1 Prompt LLM Largest LLM KNN Router MLP Router BERT Router RouterDC GraphRouter Prompt LLM* KNN Router* 0.202 0.256 0.076 0.308 0.372 0.304 0.344 0.292 0.282 0.256 0.310 0.316 0.236 0.202 0.260 0.358 0.400 0.540 0.510 0.580 0.578 0.528 0.460 0.516 0.592 0.586 0.500 0.478 0.706 0.328 0.468 0.098 0.478 0.578 0.638 0.616 0.572 0.506 0.560 0.614 0.602 0.446 0. 0.122 0.160 0.160 0.366 0.324 0.340 0.354 0.222 0.222 0.192 0.282 0.280 0.256 0.232 0.384 0.176 0.182 0.084 0.356 0.360 0.374 0.394 0.254 0.248 0.222 0.298 0.290 0.164 0.166 0.140 0.168 0.198 0.216 0.236 0.268 0.278 0.224 0.198 0.216 0.244 0.234 0.206 0.154 0.352 0.144 0.172 0.100 0.162 0.282 0.248 0.258 0.210 0.178 0.210 0.250 0.222 0.118 0. 0.266 0.208 0.256 0.146 0.278 0.262 0.274 0.196 0.210 0.206 0.218 0.180 0.248 0.234 0.434 0.134 0.168 0.224 0.084 0.226 0.198 0.242 0.182 0.188 0.188 0.204 0.170 0.080 0.060 Router-R1-Llama Bold indicates the highest score in each column for each base model. indicates in-domain evaluation; all others are out-of-domain. 0.416 0. 0.432 0.680 0.368 0.026 0.046 0.052 0.078 0.090 0.108 0.104 0.066 0.072 0.058 0.080 0.076 0.078 0.072 0.138 0.018 0.040 0.026 0.038 0.084 0.132 0.122 0.078 0.064 0.066 0.088 0.084 0.036 0. 0.128 0.040 0.224 0.112 0.224 0.272 0.448 0.480 0.360 0.360 0.312 0.504 0.448 0.472 0.384 0.135 0.184 0.199 0.267 0.291 0.329 0.338 0.265 0.253 0.247 0.314 0.297 0.288 0.256 0.512 0.416 0.048 0.272 0.016 0.176 0.272 0.528 0.472 0.376 0.360 0.296 0.504 0.416 0.208 0. 0.150 0.223 0.089 0.229 0.311 0.346 0.350 0.281 0.261 0.257 0.324 0.300 0.184 0.161 0.520 0.409 coordinate across models and aggregate their strengths more effectively. Furthermore, Router-R1 consistently outperforms advanced LLM router baselines such as GraphRouter and RouterDC across both base models and evaluation benchmarks. These improvements highlight the flexibility of Router-R1s routing mechanism, which interleaves internal reasoning with multi-round routing to progressively refine its answers. By adapting routing decisions to each query through interaction and feedback, Router-R1 demonstrates strong performance in general and multi-hop QA tasks. Router-R1 demonstrates strong generalization to unseen data. Despite being trained with indomain samples only on NQ and HotpotQA, it achieves robust performance on the remaining five out-of-domain datasets. This indicates that Router-R1 learns transferable routing and reasoning strategies from limited training data, underscoring its ability to generalize across diverse QA tasks. 5.2 Analysis of Cost Rewards In this section, we investigate how varying the cost coefficient α influences Router-R1s learned performancecost balance. We initiate the mapping function m() according to the pricing tiers of Together API4 and evaluate four cost coefficients (i.e., 0.6, 0.7, 0.8, and 0.9) on Router-R1-Qwen. The results, shown in Figure 3, reveal two clear trends. First, as the cost coefficient increases, overall 4 https://www.together.ai/pricing 8 performance gradually declines. Second, the cost reward rises simultaneously, indicating that the policy places greater emphasis on minimizing expensive LLM calls. Notably, we observe that the integration of cost rewards encourages an emergent routing strategy where Router-R1 prefers to query smaller models first and only escalates to larger models when necessary, thereby forming an adaptive routing strategy that dynamically balances efficiency and accuracy based on the cost constraints. We provide extended case studies and analysis of cost rewards in Appendix A.2 and B. These findings confirm that, by optimizing under composite reward, Router-R1 dynamically adjusts its routing behavior to adhere to resource constraints and achieves controllable trade-off between accuracy and computational expense. An extensive analysis of cost rewards is provided in Appendix A.2. Figure 3: Analysis of cost rewards on the NQ, PopQA, HotpotQA (HpQA), and 2WikiMultiHopQA (2wiki) datasets. 5.3 Generalization Capability to Unseen Candidate LLMs To evaluate Router-R1s generalization ability, we extend the routing pool of Router-R1-Qwen with two previously unseen models, i.e., Palmyra-Creative-122B5 and LLaMA3-ChatQA-1.5-8B [18], and include their corresponding model descriptors in the inference prompt. Inference is conducted directly using the original Router-R1-Qwen checkpoint without any additional fine-tuning. The results, presented in Table 2, show that Router-R1-Qwen maintains and even slightly improves performance across all four benchmarks when augmented with the new candidate LLMs. In particular, it achieves new best scores on several datasets, including TriviaQA, PopQA, and overall averages. This improvement demonstrates Router-R1s capacity to generalize to unseen candidate LLMs by interpreting their textual descriptors at inference time. Rather than relying on prior exposure or retraining, Router-R1 leverages descriptor-based reasoning to infer the strengths of new models and dynamically assign queries accordingly. Notably, this is achieved without sacrificing in-domain performance on tasks like NQ and HotpotQA, further validating the robustness of Router-R1s routing strategy. Compared to baselines, Router-R1 demonstrates stronger generalization, achieving consistent improvements with newly added LLMs, while baselines like GraphRouter and Prompt LLM* show limited or inconsistent gains, highlighting their weaker adaptability to unseen models. Overall, the results highlight Router-R1s ability to flexibly adapt to unseen candidates and selectively utilize newly introduced LLMs, indicating promising potential for real-world deployment in evolving model ecosystems. 5.4 Discussion LLM API Call Count Analysis. To assess the adaptability of Router-R1 to tasks of varying difficulty, we analyze the average number of LLM API calls (i.e., the number of times candidate LLMs within the routing pool are invoked) by Route-R1-Qwen across seven QA benchmarks. As shown in Figure 4a, Router-R1-Qwen makes significantly more average LLM API calls on multi-hop QA 5 https://build.nvidia.com/writer/palmyra-creative-122b 9 Table 2: Experimental results of generalization capability of Router-R1 on NQ, TriviaQA, PopQA, and HotpotQA (HpQA) datasets w.r.t. Exact Match and F1-Score ( indicates routing pool extension). Methods NQ TriviaQA PopQA HpQA Avg. Prompt LLM Prompt LLM BERT Router BERT Router GraphRouter GraphRouter Prompt LLM* Prompt LLM* EM 0.300 0.296 0.230 0.238 0.276 0.282 0.258 0. F1 0.437 0.433 0.362 0.370 0.412 0.420 0.373 0.311 EM 0.580 0.584 0.516 0.550 0.586 0.594 0.500 0.440 F1 0.694 0. 0.635 0.662 0.690 0.696 0.600 0.541 EM 0.340 0.342 0.192 0. 0.280 0.276 0.256 0.196 F1 0.409 0.412 0.251 0.323 0.324 0. 0.315 0.247 0.484 Router-R1-Qwen Router-R1-Qwen 0.493 Bold indicates the highest score in each column for each base model. indicates in-domain evaluation; all others are out-of-domain. 0.384 0.402 0.706 0.722 0.772 0.778 0.388 0. 0.447 0.464 EM 0.268 0.266 0.216 0.198 0.234 0.228 0.206 0. 0.352 0.346 F1 EM F1 0.407 0.409 0.335 0. 0.366 0.360 0.313 0.252 0.449 0.459 0.372 0.372 0.289 0.306 0.344 0. 0.305 0.254 0.458 0.463 0.487 0.488 0.396 0.419 0.448 0.450 0.400 0. 0.538 0.549 benchmarks (i.e., HotpotQA, 2WikiMultiHopQA, Musique, and Bamboogle) compared to general QA benchmarks (i.e., NQ, TriviaQA, and PopQA). This indicates that Router-R1 can adaptively assess task difficulty and decide whether external LLM routing is necessary, demonstrating its ability to selectively utilize external resources when tasks are more complex. Convergence Analysis of Router-R1 Training. To evaluate the convergence behavior of Router-R1, we show two crucial curves during its policy training: the reward and the policy LLMs action entropy. Figure 4b and 4c illustrates that Router-R1 converges within 100 training steps, as evidenced by rising rewards and decreasing policy entropy, indicating rapid and robust convergence. Its worth noting that occasional formatting errors may cause brief drops in reward, but our hierarchical reward design swiftly corrects them, ensuring stable and accelerated learning. In particular, we observe that without format rewards, Router-R1 exhibits greater training instability, frequently generating meaningless or nonsensical text that leads to severe formatting breakdowns in the output. (a) LLM API Call Count (b) Reward Curve (c) Entropy Curve Figure 4: Analysis of LLM API call count and Router-R1 training convergence."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce Router-R1, reinforcement learningbased framework that formulates multi-LLM routing and aggregation as sequential decision process. By instantiating the router as capable LLM, Router-R1 interleaves internal reasoning with targeted model selection and incrementally builds responses through multi-round interaction. Our lightweight rule-based rewards, which combines format, outcome, and cost rewards, enable Router-R1 to achieve superior performance while learning flexible performancecost trade-offs. On seven diverse QA benchmarks, it outperforms more than ten strong baselines, maintains robustness and generalization in the presence of distractor models. These results demonstrate the promise of RL-driven routing for orchestrating multiple LLMs."
        },
        {
            "title": "Acknowledgments",
            "content": "We sincerely appreciate the support from Amazon grant funding project #120359, \"GRAG: Enhance RAG Applications with Graph-structured Knowledge\", and Meta gift funding project \"PERM: Toward Parameter Efficient Foundation Models for Recommenders\"."
        },
        {
            "title": "References",
            "content": "[1] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. survey on evaluation of large language models. ACM transactions on intelligent systems and technology, 15(3):145, 2024. [2] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance. arXiv preprint arXiv:2305.05176, 2023. [3] Shuhao Chen, Weisen Jiang, Baijiong Lin, James Kwok, and Yu Zhang. Routerdc: Query-based router by dual contrastive learning for assembling large language models. Advances in Neural Information Processing Systems, 37:6630566328, 2024. [4] Xiangxiang Dai, Jin Li, Xutong Liu, Anqi Yu, and John Lui. Cost-effective online multi-llm selection with versatile reward models. arXiv preprint arXiv:2405.16587, 2024. [5] Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Ruhle, Laks VS Lakshmanan, and Ahmed Hassan Awadallah. Hybrid llm: Cost-efficient and qualityaware query routing. arXiv preprint arXiv:2404.14618, 2024. [6] Tao Feng, Yanzhen Shen, and Jiaxuan You. Graphrouter: graph-based router for llm selections. arXiv preprint arXiv:2410.03834, 2024. [7] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [9] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. [10] Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin, Gaurav Ranganath, Kurt Keutzer, and Shriyash Kaustubh Upadhyay. Routerbench: benchmark for multi-llm routing system. arXiv preprint arXiv:2403.12031, 2024. [11] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [12] Fengqing Jiang. Identifying and mitigating vulnerabilities in llm-integrated applications. Masters thesis, University of Washington, 2024. [13] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [14] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, Vancouver, Canada, July 2017. Association for Computational Linguistics. [15] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online, November 2020. Association for Computational Linguistics. [16] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. [17] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023. [18] Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catanzaro. Chatqa: Surpassing gpt-4 on conversational qa and rag. arXiv preprint arXiv:2401.10225, 2024. [19] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 98029822, Toronto, Canada, July 2023. Association for Computational Linguistics. [20] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 37:124198124235, 2024. [21] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. comprehensive overview of large language models. arXiv preprint arXiv:2307.06435, 2023. [22] Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms from preference data. In The Thirteenth International Conference on Learning Representations, 2024. [23] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [24] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 56875711, Singapore, December 2023. Association for Computational Linguistics. [25] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [26] Marija Šakota, Maxime Peyrard, and Robert West. Fly-swat or cannon? cost-effective language model choice via meta-modeling. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, pages 606615, 2024. [27] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [28] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 12 [29] Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:9460 9471, 2022. [30] Dimitris Stripelis, Zijian Hu, Jipeng Zhang, Zhaozhuo Xu, Alay Dilipbhai Shah, Han Jin, Yuhang Yao, Salman Avestimehr, and Chaoyang He. Tensoropera router: multi-model router for efficient llm inference. arXiv preprint arXiv:2408.12320, 2024. [31] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. [32] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. [33] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022. [34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [35] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [36] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. [37] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023."
        },
        {
            "title": "A Experimental Details",
            "content": "A.1 Implementation Details A.1.1 Model Descriptors To enrich model descriptors and better inform routing decisions, we draw on the publicly available model cards6. Specifically, we prompt GPT-4o to generate brief, standardized summaries of each candidate LLM based on the information in these model cards. These concise descriptions are then injected into the training prompt of Router-R1. This enables the model to develop an initial understanding of each LLMs capabilities. It is worth noting that these model descriptors serve solely as cold-start priors to guide early-stage decisions. During policy optimization, Router-R1 gradually refines its understanding of each LLMs strengths and weaknesses through repeated interaction and reward feedback. Furthermore, this design allows Router-R1 to seamlessly integrate new LLMs into the routing pool without retraining by simply appending their descriptions to the prompt. This flexibility supports fast adaptation to newly released models and enhances Router-R1s generalization in dynamic multi-LLM environments. A.1.2 Format Rewards To ensure the structural quality and consistency of generated completions, we design rule-based format reward that penalizes malformed outputs. The function inspects whether the completion correctly follows the expected structure using specific action tags. The format reward function imposes the following requirements: All tags must be properly opened and closed, with no nesting allowed. The response must begin with <think> ... </think> block and end with single <answer> ... </answer> block. At least one <think> ... </think> block must be present, and exactly one <answer> ... </answer> block is required. Each <search> ... </search> must be paired with corresponding <info> ... </info> block. Routed queries must follow the format \"llm_name: query\", where the LLM name is valid and both parts are non-empty. This format reward is lightweight but effective in enforcing disciplined structure in the models output, guiding the policy toward well-formed and interpretable reasoning trajectories. A.1.3 Cost Rewards To encourage cost-efficient routing decisions, we design cost reward that penalizes the use of high-cost LLMs. This reward is computed as the inverse of dynamically normalized cost value using sliding window approach. Sliding Window Normalization. We maintain fixed-size buffer of recent cost values (window size = 1000), and apply smoothing transformation to each cost before normalization. Specifically, we adopt square root transformation: = where is the raw cost associated with the selected LLM (i.e., = m(PLLM) Tout). To ensure robustness against outliers, we normalize using the 5th and 95th percentiles of the buffer. Let rmin and rmax be the 5th and 95th percentile values respectively. The normalized cost is then computed as: rnorm = rmin rmax rmin If the range rmax rmin is too small (less than small threshold ε), we return neutral reward value of 0.5 to maintain stability. 6 https://build.nvidia.com/models 14 Reward Inversion. Finally, to encourage the selection of low-cost LLMs, we invert the normalized value: Rcost = 1.0 clip(rnorm, 0.0, 1.0) This ensures that higher-cost models receive lower rewards, guiding the policy to prefer efficient candidates without relying on hard constraints. This cost reward design allows Router-R1 to learn cost-sensitive routing policies that adapt dynamically to the resource profiles of available LLMs. A.2 More Experimental Results A.2.1 Main Results w.r.t. F1-Score We present extensive experimental results on seven QA datasets w.r.t. F1-Score in Table 3. Router-R1 consistently outperforms all baselines across both general and multi-hop QA tasks, achieving the highest average F1-scores for both Qwen and LLaMA backbones. Notably, it surpasses strong baselines like Prompt LLM and GraphRouter by clear margin, demonstrating its effective coordination of model selection for diverse QA challenges. Table 3: Experimental results on seven QA datasets w.r.t. F1-Score. Methods General QA Multi-Hop QA NQ TriviaQA PopQA HpQA 2wiki Musique Bamb Avg. Qwen2.5-3B-Instruct Direct CoT SFT RAG Search-R1 Prompt LLM Largest LLM KNN Router MLP Router BERT Router RouterDC GraphRouter Prompt LLM* KNN Router* 0.162 0.218 0.289 0.414 0.407 0.437 0.431 0.388 0.368 0.362 0.410 0.412 0.373 0.360 Router-R1-Qwen 0.484 Llama-3.2-3B-Instruct Direct CoT SFT RAG Search-R1 Prompt LLM Largest LLM KNN Router MLP Router BERT Router RouterDC GraphRouter Prompt LLM* KNN Router* 0.281 0.364 0.125 0.410 0.478 0.450 0.469 0.410 0.395 0.383 0.435 0.441 0.342 0. 0.341 0.431 0.460 0.622 0.575 0.694 0.695 0.627 0.557 0.635 0.694 0.690 0.600 0.572 0.772 0.377 0.534 0.159 0.557 0.650 0.733 0.711 0.651 0.585 0.656 0.708 0.704 0.524 0.476 0.154 0.185 0.207 0.452 0.383 0.409 0.423 0.281 0.272 0.251 0.328 0.324 0.315 0.272 0.447 0.209 0.230 0.117 0.413 0.412 0.441 0.448 0.292 0.290 0.272 0.338 0.327 0.209 0. 0.215 0.260 0.281 0.307 0.328 0.407 0.416 0.341 0.295 0.335 0.381 0.366 0.313 0.248 0.449 0.211 0.241 0.154 0.247 0.375 0.369 0.377 0.312 0.271 0.310 0.371 0.343 0.188 0.149 0.304 0.251 0.291 0.187 0.317 0.393 0.397 0.289 0.277 0.284 0.293 0.258 0.355 0.299 0.487 0.165 0.213 0.267 0.119 0.271 0.301 0.341 0.248 0.236 0.244 0.257 0.227 0.136 0. Router-R1-Llama Bold indicates the highest score in each column for each base model. indicates in-domain evaluation; all others are out-of-domain. 0.520 0.740 0.461 0.429 0. 0.081 0.106 0.121 0.134 0.145 0.205 0.199 0.141 0.142 0.135 0.165 0.154 0.165 0.138 0.212 0.070 0.104 0.075 0.070 0.138 0.222 0.204 0.140 0.134 0.126 0.161 0.146 0.088 0.074 0.188 0.112 0.332 0.173 0.303 0.387 0.579 0.608 0.496 0.460 0.423 0.623 0.546 0.580 0.493 0.196 0.255 0.260 0.346 0.363 0.446 0.453 0.366 0.339 0.346 0.413 0.393 0.386 0. 0.635 0.498 0.097 0.385 0.042 0.240 0.337 0.628 0.563 0.504 0.455 0.388 0.601 0.545 0.302 0.259 0.201 0.296 0.134 0.294 0.380 0.449 0.445 0.365 0.338 0.340 0.410 0.390 0.256 0.226 0.625 0. 15 A.2.2 Extensive Analysis of Cost Rewards We conduct an extensive study on the effect of different cost coefficients α in Router-R1 and compare with various baselines in terms of EM and average raw cost rewards. The results shown in Table 4 reveal clear trade-offs: with α = 0.0, Router-R1 achieves the highest EM across almost all datasets, demonstrating its performance-oriented routing strategy. As α increases, we observe substantial reduction in cost, accompanied by decrease in EM, which highlights the inherent trade-off between answer accuracy and computational efficiency. Notably, α = 0.6 provides favorable balance, consistently achieving strong EM while substantially lowering costs compared to α = 0.0. Compared to baselines, Router-R1 with moderate α values outperforms others in both accuracy and efficiency, validating the flexibility and effectiveness of our cost-aware reward design. Table 4: Extensive analysis of cost rewards on NQ, PopQA, HpQA, and 2wiki datasets w.r.t. Exact Match and raw cost rewards (unnormalized). Methods NQ PopQA HpQA 2wiki EM Cost EM Cost EM Cost EM Cost Qwen2.5-3B-Instruct Prompt LLM Largest LLM KNN Router MLP Router BERT Router RouterDC GraphRouter Prompt LLM* KNN Router* 0.300 0.296 0.262 0.252 0.230 0.278 0.276 0.258 0.236 20.0 20.2 41.0 35.8 26.0 57.5 29.6 286.4 102.2 0.340 0.354 0.222 0.222 0.192 0.282 0.280 0.256 0.232 17.6 17.6 30.3 21.0 16.3 21.8 19.2 111.7 49.8 Router-R1-Qwen (α = 0.0) 150.6 Router-R1-Qwen (α = 0.6) 150.0 Router-R1-Qwen (α = 0.7) 32.3 Router-R1-Qwen (α = 0.8) 28.9 Router-R1-Qwen (α = 0.9) 5.5 Bold indicates the best score in each column. indicates in-domain evaluation; all others are out-of-domain. 0.388 0.388 0.318 0.320 0. 0.384 0.414 0.280 0.270 0.234 98.3 75.9 17.2 14.9 5.3 0.268 0.278 0.224 0.198 0.216 0.244 0.234 0.206 0.154 0.352 0.332 0.254 0.238 0.186 20.1 20.1 45.4 29.5 26.0 62.3 24.7 313.4 133.0 138.6 124.3 27.2 28.2 5. 0.262 0.274 0.196 0.210 0.206 0.218 0.180 0.248 0.234 0.434 0.422 0.236 0.202 0.252 20.2 20.1 51.8 41.3 26.3 40.7 28.6 222.4 99.4 150.8 113.8 31.4 31.4 6."
        },
        {
            "title": "B Case Study",
            "content": "In this section, we present case study of Router-R1 under both zero and non-zero cost coefficients α. This case study illustrates the output behavior (e.g., routing) of Router-R1 under both the presence and absence of cost reward constraints. B.1 Case Study without Cost Reward Constraints **Question**: Which film was released more recently, Sacred Silence or Ek Haseena Thi Ek Deewana Tha? **Ground Truth**: Ek Haseena Thi Ek Deewana Tha Response of Router-R1: <think> To answer the question \"Which film was released more recently, Sacred Silence or Ek Haseena Thi Ek Deewana Tha?\", need specific information about the release dates of these films. dont have this knowledge stored within me, so will need to consult an external specialized language model to find the answer. </think> <search> LLaMA-3.1-70B-Instruct: Which film was released more recently, Sacred Silence or Ek Haseena Thi Ek Deewana Tha? </search> <info> can help with that! After checking, found that: * \"Sacred Silence\" (also known as \"Il silenzio sacro\") is 2016 Italian drama film. * \"Ek Haseena Thi Ek Deewana Tha\" is 2017 Indian romantic thriller film. Based on the release years, \"Ek Haseena Thi Ek Deewana Tha\" (2017) was released more recently than \"Sacred Silence\" (2016). </info> <answer> Ek Haseena Thi Ek Deewana Tha </answer> **Question**: Which film has the director who died earlier between To See Or Not To See and Prince (1969 Film)? **Ground Truth**: [To See or Not to See, To See Or Not To See] Response of Router-R1: <think> To answer the question about which film has the director who died earlier between To See Or Not To See and Prince (1969 Film), need to determine the directors of these films and their death dates. dont have immediate knowledge of these details, so will need to consult an external specialized language model to find the required information. </think> <search> LLaMA-3.1-70B-Instruct: Which film has the director who died earlier, To See Or Not To See or Prince (1969 Film)? </search> <info> very specific question! After conducting research, found that the 1969 film \"Prince\" is directed by Lekh Tandon, an Indian film director. Unfortunately, couldnt find any information on film called \"To See Or Not To See\" that would allow me to determine the directors name or their date of passing. However, did find that Lekh Tandon passed away on October 15, 2017. If youre looking for more information on \"To See Or Not To See\" or its director, recommend consulting other LLMs or film databases for further assistance. </info> <think> Since dont have enough information to directly answer the question, will need to consult another specialized language model to determine the director of \"To See Or Not To See\" and compare their death dates with Lekh Tandon. </think> <search> LLaMA-3.1-70B-Instruct: Which film is called \"To See Or Not To See\" and who is its director? Additionally, please compare the directors death date with Lekh Tandons death date (October 15, 2017). </search> <info> Im happy to help with the sub-question! After conducting thorough search, found that the film \"To See Or Not To See\" is also known as \"Andha Kanoon\" (1983), but couldnt confirm this title exactly matching \"To See Or Not To See\". However, did find film titled \"To See or Not to See\" (1969), which is an Indian film directed by Biren Nag. Regarding the comparison of the directors death date with Lekh Tandons death date, found that Biren Nag passed away on August 5, 2018. Therefore, Biren Nags death date is approximately 10 months after Lekh Tandons death date (October 15, 2017). Please note that couldnt confirm the exact title match, but provided relevant information related to the question. If the base model requires further assistance or confirmation, recommend consulting other LLMs for additional information. </info> <think> With the information that Biren Nag directed \"To See or Not to See\" (1969) and passed away on August 5, 2018, can now compare his death date with Lekh Tandons death date (October 15, 2017). Since Biren Nag died later than Lekh Tandon, the director of \"To See or Not to See\" (1969) died earlier than the director of \"Prince\" (1969 Film). </think> <answer> To See or Not to See </answer> 18 **Question**: Which film has the director who was born first, Women HeS Undressed or The KingS Stamp? **Ground Truth**: [\"The Kings Stamp\", \"Kings Stamp\", \"The KingS Stamp\"] Response of Router-R1: <think> To answer the question \"Which film has the director who was born first, Women HeS Undressed or The KingS Stamp?\", need to determine the birth years of the directors of these films. However, dont have immediate knowledge of these directors birth years. will need to consult an external specialized language model to find this information. </think> <search> LLaMA-3.1-70B-Instruct: Which film has the director who was born first, Women HeS Undressed or The KingS Stamp? </search> <info> To answer this question, need to identify the directors of the two films and their birth dates. After researching, found that: * \"Women Hes Undressed\" (2015) is documentary film directed by Gillian Armstrong. * \"The Kings Stamp\" doesnt seem to be well-known film. However, found film called \"The Kings Speech\" (2010) directed by Tom Hooper, but also found another film called \"The Kings Stamp\" (1935) directed by Michael Powell. Assuming the question refers to the 1935 film \"The Kings Stamp\", here are the birth dates of the directors: * Gillian Armstrong was born on December 18, 1950. * Michael Powell was born on September 30, 1905. Based on this information, Michael Powell, the director of \"The Kings Stamp\" (1935), was born first. Please note that if the question refers to different film with the title \"The Kings Stamp\", my answer may not be accurate. </info> <think> With the information obtained from the external LLaMA-3.1-70B-Instruct model, now know that Michael Powell, the director of \"The Kings Stamp\" (1935), was born first. However, need to determine if \"Women HeS Undressed\" (2015) has director who was born earlier than Michael Powell. Since dont have the birth year of the director of \"Women HeS Undressed\", will need to consult another external specialized language model. </think> <search> LLaMA-3.1-70B-Instruct: If the question is \"Which film has the director who was born first, Women HeS Undressed or The KingS Stamp?\", and found that \"The Kings Stamp\" (1935) was directed by Michael Powell who was born on September 30, 1905, but dont have the birth year of the director of \"Women HeS Undressed\" (2015). What is the answer? </search> <info> nice question about film directors! Although you dont have the birth year of the director of \"Women HeS Undressed\" (2015), can provide some helpful context. After quick search, found that \"Women HeS Undressed\" (2015) is documentary film directed by Gillian Armstrong. Now, lets compare the birth years: Michael Powell, the director of \"The Kings Stamp\" (1935), was born on September 30, 1905. Gillian Armstrong, the director of \"Women HeS Undressed\" (2015), was born on December 18, 1950. Based on this information, its clear that Michael Powell was born first, so the answer is: \"The Kings Stamp\" (1935) has the director who was born first. </info> <answer> The Kings Stamp </answer> 19 B.2 Case Study with Cost Reward Constraints **Question**: The radiographic term used to describe the dense bone of the socket and septal crest is? **Ground Truth**: [lamina dura, alveolar process, the lamina dura] Response of Router-R1: <think> need to determine which LLM would be most suitable for this query. The LLMs have different strengths, but typically, LLaMA-3.1-8B-Instruct and LLaMA-3.1-70B-Instruct are strong in multilingual and complex reasoning tasks, which might be helpful here. will search for the most appropriate LLM. </think> <search> LLaMA-3.1-8B-Instruct: the radiographic term used to describe the dense bone of the socket and septal crest is? </search> <info> The radiographic term used to describe the dense bone of the socket and septal crest is **lamina dura**. The lamina dura is radiodense layer of bone that lines the socket of tooth and the septal crest of the maxilla. It is normal anatomical structure that can be visible on radiographs (x-rays) and is an important landmark for dental and maxillofacial radiology. In dental radiography, the lamina dura is often used as reference point to assess the health of the surrounding bone and to detect any abnormalities, such as periapical lesions or bone loss. </info> <answer> lamina dura </answer> **Question**: Which film whose director was born first, Maker Of Men or Kentucky Kernels? **Ground Truth**: [Maker Of Men, Maker of Men] Response of Router-R1: <think> need to determine which film, Maker Of Men or Kentucky Kernels, had director born first. will search externally to find this information. </think> <search> LLaMA-3.1-8B-Instruct: Which film, Maker Of Men or Kentucky Kernels, had director born first? </search> <info> Im happy to help with this question. However, need to clarify that couldnt find any information on films called \"Maker Of Men\" or \"Kentucky Kernels\". Its possible that these films are obscure, unknown, or non-existent. If you could provide more context or details about these films, Id be happy to try and assist you further. Alternatively, can suggest some possible sources where you may be able to find the information youre looking for. If youre unable to provide more context, Im afraid Im unable to assist with this question. In this case, would instruct the base model to consult other LLMs for further assistance, such as those specializing in film databases or obscure movie information. </info> <think> Based on the inability to find direct information, will now consult another LLM to gather the necessary details. </think> <search> LLaMA-3.1-70B-Instruct: Which film, Maker Of Men or Kentucky Kernels, had director born first? </search> <info> To answer this question, need to provide some background information on the directors of the two films mentioned. \"Maker of Men\" (1931) was directed by Edward Laemmle, an American film director. Edward Laemmle was born on October 25, 1887. \"Kentucky Kernels\" (1934) was directed by George Stevens, an American film director, producer, screenwriter, and cinematographer. George Stevens was born on December 18, 1904. Based on the birth dates, Edward Laemmle, the director of \"Maker of Men\", was born first. </info> <answer> Maker Of Men </answer> 20 **Question**: Where was the place of death of Topa Inca Yupanquis father? **Ground Truth**: [Cusco, Cuzco, Cusco, Peru, Cuzco, Peru] Response of Router-R1: <think> need to determine the place of death of Topa Inca Yupanquis father. will search externally to find this information. </think> <search> LLaMA-3.1-8B-Instruct: Where was the place of death of Topa Inca Yupanquis father? </search> <info> Im unable to assist with this question as dont have specific information on the place of death of Topa Inca Yupanquis father. The historical records of the Inca Empire are extensive, but detailed information on individual family members lives, especially their places of death, can be scarce or not well-documented. To further assist with this question, recommend that the base model consult other LLMs or historical databases that specialize in Inca history or Peruvian archaeology. These sources may provide more detailed or accurate information on the life and death of Topa Inca Yupanquis father. </info> <think> Based on the need to consult external knowledge, will now search again to find the answer. </think> <search> LLaMA-3.1-70B-Instruct: Where was the place of death of Topa Inca Yupanquis father? </search> <info> question about Inca history! Topa Inca Yupanquis father was Pachacuti, the ninth Sapa Inca (emperor) of the Inca Empire. According to historical records, Pachacuti died in Cusco, Peru, which was the capital of the Inca Empire at that time. To provide more context, Pachacuti was renowned leader who expanded the Inca Empire significantly during his reign (1438-1472). He was succeeded by his son, Topa Inca Yupanqui, who continued to expand the empire. If you need more information about Pachacuti, Topa Inca Yupanqui, or the Inca Empire, feel free to ask! </info> <answer> Cusco </answer> The case study illustrates the behavioral differences of Router-R1 under different reward settings. In the first case (without cost reward constraints), the routing policy converges toward always selecting the most capable LLMLLaMA-3.1-70B-Instructwhich leads to the best answer accuracy but incurs higher inference cost. In contrast, the second case (with cost reward constraints) demonstrates that Router-R1 learns to initially route queries to smaller, more cost-efficient modelLLaMA-3.18B-Instruct. Only when the smaller model fails to provide sufficient information does the policy escalate to querying the larger LLM. This strategy reflects cost-aware routing behavior that balances performance with efficiency."
        },
        {
            "title": "C Prompts",
            "content": "In this section, we detail the prompts used in our experiments."
        },
        {
            "title": "Model Descriptors",
            "content": "Qwen2.5-7B-Instruct: Qwen2.5-7B-Instruct is powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages. LLaMA-3.1-8B-Instruct: LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks. LLaMA-3.1-70B-Instruct: LLaMA-3.1-70B-Instruct is 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting new standard against both open and closed models in benchmark evaluations. Mistral-7B-Instruct: Mistral-7B-Instruct is fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively. Mixtral-8x22B-Instruct: Mixtral-8x22B-Instruct is cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall. Gemma-2-27B-Instruct: Gemma-2-27B-Instruct is cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible. 22 Model Descriptors (Unseen LLMs) LLaMA3-ChatQA-1.5-8B: LLaMA3-ChatQA-1.5-8B is an 8-billion-parameter instruction-tuned language model built on top of the LLaMA-3 base, specifically optimized for conversational question answering (QA) and retrieval-augmented generation (RAG). Developed with an improved training recipe from the ChatQA paper, it incorporates rich conversational QA data to enhance its performance on tasks involving tabular reasoning and arithmetic calculations. Palmyra-Creative-122B: Palmyra-Creative-122B is 122B-parameter model by Writer, built for high-quality creative writing and content generation. It excels at tasks like storytelling, poetry, scriptwriting, and marketing copy, adapting to various styles and tones while maintaining consistent voice. Ideal for writers and content creators, it supports diverse creative workflows."
        },
        {
            "title": "Prompts for Querying LLM Candidates",
            "content": "You are helpful assistant. You are participating in multi-round reasoning process, where base model delegates sub-questions to specialized models like you. Your task is to do your **absolute best** to either: + Answer the question directly, if possible, and provide brief explanation; or + Offer helpful and relevant context, background knowledge, or insights related to the question, even if you cannot fully answer it. If you are completely unable to answer the question or provide any relevant or helpful information, you must: + Clearly state that you are unable to assist with this question, and + Explicitly instruct the base model to consult other LLMs for further assistance. **Important Constraints**: + Keep your response clear, concise, and informative (preferably under 512 tokens). Your response will help guide the base models reasoning and next steps. + Stay strictly on-topic. Do not include irrelevant or generic content. Here is the sub-question for you to assist with: {sub_query}"
        },
        {
            "title": "D Hyperparameter Settings",
            "content": "Table 5: Hyperparameter Settings (shared across all datasets) Hyperparameter Value Hyperparameter Learning Rate (Actor) Total Batch Size Micro-batch Size Max Routing Steps Max Response Length Tensor Parallel Size Rollout Sampling Temperature (Train) 1e-6 64 8 4 1024 1 1. Learning Rate (Critic) Mini-batch Size Max Training Steps Max Sequence Length Max Length for LLM API Response GPU Utilization Ratio Rollout Sampling Temperature (Eval) Value 1e-5 32 225 4096 600 0.6 1."
        },
        {
            "title": "E Limitations",
            "content": "While Router-R1 shows strong empirical performance, it has several limitations: Task Scope: Our evaluation focuses primarily on QA tasks. It remains to be seen how well Router-R1 generalizes to other domains such as dialogue, summarization, or code generation, which may have different routing dynamics. Reward Simplicity: The rule-based reward function, while effective, may be insufficient for capturing more nuanced objectives like factual consistency or long-term dialogue coherence. Incorporating learned or human-in-the-loop reward functions could further enhance the framework. Inference Latency: Although Router-R1 aims to optimize cost, its multi-round nature introduces inference latency, especially when reasoning steps are interleaved with multiple model calls. This may limit its suitability for time-sensitive applications. Dependence on Model Descriptors: The generalization to unseen LLMs relies on simple descriptors like pricing and latency. These may not capture deeper model behaviors or capabilities, especially in settings with limited performance history. Future work may explore improving Router-R1s reward modeling, reducing inference latency through model pruning or routing heuristics, and broadening its application scope to wider range of language tasks."
        },
        {
            "title": "F Broader Impacts",
            "content": "Router-R1 provides principled and flexible framework for coordinating multiple large language models, offering potential benefits for improving the efficiency, scalability, and quality of language model deployments. By learning to dynamically balance performance and cost, Router-R1 can help reduce reliance on expensive models where unnecessary, which may lower the environmental footprint and financial cost of large-scale LLM applications. Additionally, Router-R1s architecture encourages the reuse and composition of existing LLMs, promoting modularity and potentially accelerating progress in collaborative AI systems. Its generalization ability to unseen LLMs may support faster integration of new models into production systems without extensive retraining."
        }
    ],
    "affiliations": [
        "University of Illinois at Urbana-Champaign"
    ]
}