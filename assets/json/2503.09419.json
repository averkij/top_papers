{
    "paper_title": "Alias-Free Latent Diffusion Models:Improving Fractional Shift Equivariance of Diffusion Latent Space",
    "authors": [
        "Yifan Zhou",
        "Zeqi Xiao",
        "Shuai Yang",
        "Xingang Pan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation. Code is available at: https://github.com/SingleZombie/AFLDM"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 9 1 4 9 0 . 3 0 5 2 : r Alias-Free Latent Diffusion Models: Improving Fractional Shift Equivariance of Diffusion Latent Space Yifan Zhou1 Zeqi Xiao1 Shuai Yang 2 Xingang Pan1 1S-Lab, Nanyang Technological University 2 Wangxuan Institute of Computer Technology, Peking University {yifan006, zeqi001, xingang.pan}@ntu.edu.sg williamyang@pku.edu.cn Figure 1. Visualization of AE and LDM latent shift experiments. Top: we shift the input of an 8 upsampling VAE decoder by 1/8 pixel in each step. The intermediate output images from Stable Diffusion (SD) VAE appear increasingly blurry. Bottom: we shift the noisy latent of LDM by 1/8 pixel in each step. Without cross-frame attention, SD struggles to generate consistent results; even with it, we can observe bouncing effect, where textures initially stick but suddenly shift in subsequent steps. In contrast, our alias-free SD (AF-SD) produces shift-equivariant results in both experiments, maintaining consistency and clarity across steps."
        },
        {
            "title": "Abstract",
            "content": "Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) selfattention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation. Code is available at: https://github.com/ SingleZombie/AFLDM 1. Introduction Latent Diffusion Models (LDMs) [35] achieve highresolution image synthesis by performing the denoising dif1 fusion process in compressed latent space obtained via VAE [22]. Thanks to the rich image prior learned in LDMs, they have been broadly adopted in variety of downstream applications. For example, LDMs are widely used for video editing by processing each frame [5, 10, 21, 48, 50, 51, 57]. They can also be finetuned for image-to-image translation tasks such as super-resolution [46, 47, 58] and normal estimation [9, 27, 49, 53], etc. These tasks often require the results to be consistent, e.g., the texture details of consecutive video frames should be coherent without flickering, and the normal estimation results of the same region should remain stable regardless of random shifts of the input image. Despite the significant demand for ensuring such consistency, LDMs unfortunately fall short in this aspect due to the inherently unstable generation process from the Gaussian noise to the clean image. Even small perturbations, such as pixel shifts or flow warping applied to the initial noisy latent, can result in drastically different images [4, 50], as shown in Fig. 1 (SD and SD*). While some attempts introduce additional sophisticated modules to alleviate the inconsistency [50], how to fundamentally address this issue from the design of LDM remains underexplored. This motivates us to investigate improving the shift-equivariance of LDMs, i.e., shift to the latent noise (e.g., 1/8 pixel) should lead to rescaled shift in the final generated image (e.g., 1 pixel), where the rescale is caused by the upsampling in the decoder. This useful property can facilitate various vision synthesis tasks that require high stability and consistency. Previous works have studied shift-equivariance for vanilla convolutional neural networks (CNNs) from the signal processing perspective [3, 18, 30, 56]. common cause of the failure to preserve equivariance is aliasing effects, which means that when we consider the discrete features in the continuous domain, they contain high frequencies beyond what the discrete sampling rate can represent according to the NyquistShannon sampling theorem [39]. This happens in CNNs because some operations, such as downsampling, upsampling, and non-linear layers, cannot correctly band-limit the features in the continuous domain. To address this issue, several anti-aliasing operations have been proposed, which constrain the output signal to be bandlimited [18, 30, 56]. For example, StyleGAN3 [18] has successfully adopted this principle to build an alias-free and shift-equivariant generator. natural question then arises: Can we simply adopt those anti-aliasing modules in LDMs to achieve shiftequivariance? Our preliminary study reveals that existing anti-aliasing modules alone are insufficient to achieve highly shift-equivariant LDM. We identify several reasons: 1) Although randomly initialized VAE with aliasfree modules initially exhibits reduced aliasing, these effects intensify as training progresses  (Fig. 2)  . This is likely Figure 2. Frequency map visualization of upsampled VAE latent. The upsampled latent serves as an approximation of continuous signal. Left: Illustration of latent upsampling algorithm. For an encoder that downsamples the image by 8, we can leverage its shift-equivariance to achieve 2 upsampling of the latent. By shifting the input image by 4 pixels, we create 0.5-pixel shift in the latent. Combining these fractional shifted latents yields higher-resolution latent. Right: Visualization of 8 upsampled latent in both spatial and frequency domains. (a) AF-VAE with random weights, (b) AF-VAE without equivariance loss, (c) AFVAE with equivariance loss. Ideally, correctly band-limited latent would only use 1/8 of the frequencies in the upsampled frequency domain. During training, the aliasing effects in randomly initialized VAE with alias-free modules become more prominent; however, our proposed equivariance loss helps suppress the aliasing during training. because the VAEs learning process benefits from amplifying the remaining high-frequencies leaked from imperfect alias-free designs, such as the processing of boundary pixels [20] and nonlinearities [30]. 2) The iterative denoising process in LDMs requires multiple U-Net inferences, causing aliasing to accumulate across denoising steps  (Fig. 5)  . 3) The self-attention operations in the U-Net, which are sensitive to global translations, are not shift-equivariant. To overcome these unique challenges in LDMs, we redesign alias-free LDMs in two distinct aspects. First, given the aliasing amplification in both VAE and U-Net inferences, it is extremely challenging to address aliasing purely from model design. Instead, we introduce an effective but much less explored way, which is an equivariance loss that includes fractional shift-equivariance of LDMs VAE and U-Net as part of the training objective. This significantly reduces aliasing without sacrificing performance. Fig. 2 verifies that this loss effectively suppresses the bandwidth of the VAE latent space. Second, we demonstrate that in order to make self2 attention shift-invariant, the pool of keys and values must be fixed, while queries should shift along with the input. To achieve this, we select reference frame, and always use the keys and values of that frame in attention calculation to ensure shift-equivariance with respect to that frame. While this solution happens to have the same form as the crossframe attention (CFA) [21], we are the first to disclose its significance in equivariance. In addition, while previous works often use CFA in test time [5, 21, 50, 51], we find it critical to use CFA in both training and inference to suppress aliasing. With these improvements, we present Alias-Free Latent Diffusion Model (AF-LDM). As shown in Fig. 1, our AF-LDM demonstrates high fractional shift-equivariance throughout the generation pipeline. Further experiments verify that AF-LDM is robust even for irregular pixel shifts, such as flow warping. This property motivates us to propose simple warping-equivariant video editing method that models the deformation among frames without using additional mechanisms for temporal coherence. Beyond generation, AF-LDM generalizes effectively to other image-toimage tasks that benefit from consistency and stability, such as super-resolution and normal estimation. In summary, our contributions are as follows: We take the first attempt to investigate the shiftequivariance in LDMs and introduce novel alias-free LDM (AF-LDM), which effectively mitigates aliasing effects and ensures shift-equivariance. To address aliasing amplification in LDMs beyond the capabilities of existing techniques, we introduce an equivariance loss to constrain the bandwidth of the underlying continuous feature. Additionally, we ensure shiftequivariance in attention layers by employing consistent reference keys and values during shifting or warping, applying this strategy in both training and inference stages. We validate the effectiveness of AF-LDM in several downstream tasks that require consistent results, including video editing and image-to-image tasks, demonstrating its broad applicability and significant potential. 2. Related Work 2.1. Diffusion Models The diffusion model is family of generative models that transform Gaussian noise into clean image through progressive denoising process. Early diffusion models [15, 42] operate the denoising process directly in image space, requiring up to thousand denoising steps. While effective, the image diffusion models are criticized for their extremely long training and inference time. Inspired by previous twostage generative models [7, 44], Latent Diffusion Models (LDMs) [35] accelerate the image diffusion models by decomposing the image generation process into latent generation followed by latent decoding. Although advancements in LDMs have introduced improvements from various perspectives, such as network backbone [8, 33] and training objectives [24, 26, 28], the two-stage generation structure remains integral. Our experiments reveal that all existing latent diffusion models suffer from instability caused by aliasing effects. 2.2. Stability of Latent Diffusion Models Compared to GANs, LDMs exhibit notable instability, especially concerning smooth latent space and shiftequivariance. smooth latent space, concept welldeveloped in GAN literature, enables continuous changes in generated images when interpolating latents [16, 17]. This smoothness enhances image editing applications for diffusion models, such as inversion [31, 43] and interpolation [13, 40, 54]. The other property, shift-equivariance, ensures consistency in generated outputs as input objects shift within scene. This property is crucial for achieving temporal coherence in video editing applications [4, 5, 10, 21, 50, 51]. Common practices that mitigate the instability of diffusion models include image overfitting [19, 41, 54], crossframe attention [21], and regularization [11, 52]. However, the shift-equivariance of LDM is still an underexplored topic. 2.3. Alias-Free Neural Networks Aliasing, the overlap of signals due to improper sampling, significantly affects neural networks by reducing their shiftequivariance and shift-invariance [1, 56]. Initial efforts to create alias-free neural networks focus on improving the shift-invariance of classification CNNs through optimized downsampling techniques [3, 37, 56]. Later, StyleGAN3 [18] reveals that the upsampling, downsampling, and nonlinear operations of neural networks are not alias-free and introduce Kaiser filtering as an approximation to ideal signal processing. Following this, Alias-Free Convnets (AFC) [30] expanded upon StyleGAN3s approach, replacing the Kaiser filters with FFT-based operations and implementing polynomial nonlinearities. While most previous works discuss the aliasing effects in classification CNNs and GANs, limited research has explored the aliasing effects in LDM. 3. Method 3.1. Preliminaries Stable Diffusion. Stable Diffusion (SD) [35] is latent diffusion model that consists of Variational autoencoder (VAE) [22] and text-conditioned denoising U-Net [36]. First, given an input image RHW 3, the encoder and decoder of the VAE are trained to minimize the distance between and the reconstructed image D(z), where = E(x) RH/kW/k4 is the latent downsampled by 3 k. Next, latent denoising U-Net ϵθ(zt, t) is trained to denoise the noisy latent zt at timestep t. During sampling, clean latent is sampled via reverse diffusion process and then decoded to obtain the final image D(z). Both VAE and U-Net are built on the encoder-decoder architecture that consists of similar modules. Specifically, the networks incorporate ResNet blocks [12] and self-attention blocks [45]. To rescale the feature, the networks employ the nearest downsample and bilinear upsample. Although the networks heavily rely on shift-equivariant convolution layers, the other modules, including nonlinearities, sampling, and self-attention, are not shift-equivariant. StyleGAN3 StyleGANs [16, 17] generate high-resolution images by gradually upsampling constant low-resolution feature with CNN-based synthesis network. Despite high generation quality, texture sticking effect occurs in the original StyleGAN, where the details in the output image stick to the same position when shifting the input feature. StyleGAN3 [18] argues that the aliasing in the synthesis network causes this effect. The issue is analyzed by continuous signal interpretation: each network feature is equivalent to continuous signal. discrete feature can be converted into continuous signal via interpolation filter and be converted back via Dirac comb. Similarly, each network layer has its counterpart in the continuous domain. If network layer is shift-equivariant, then its effect in the discrete and continuous domain should be equivalent, i.e., the conversion between (Z) and (z) should still be invertible. Based on the continuous signal interpretation, StyleGAN3 makes several primary improvements: 1. Fourier Latent: Transform the input of the synthesis network from the image feature to the Fourier feature. 2. Cropping Boundary Pixels: Crop the border pixels after each layer to stop leaking absolute position. 3. Ideal Resampling: Replace bilinear upsampling with Kaiser filters that better approximate an ideal low-pass filter. 4. Filtered Nonlinearity: Wrap nonlinearities between 2 upsampling and 2 downsampling to suppress the high frequencies introduced by this operation. Fractional Shift Equivariance. Let be the shift operator that shifts an image RHW by N2 pixels, an operator : RHW RHW is shift equivariant if it satisfies (T(x)) = T(f (x)). The shift equivariance can be extended to fractional shift equivariance, i.e., R2. In this work, we implement it as the Fourier shift, which shifts the phase of an image in the Fourier domain according to . With fractional shift, we can extend to an operator that rescales the image. Assume : RHW RkHkW is an operator that rescale the image by factor of k, then it is fractional shift equivariant if (T(x)) = Tk(f (x)). Since the resolution of an image is limited, some pixels are moved out or missing after shifting. There are two comFigure 3. The architecture of AF-LDM. Both VAE and U-Net of SD can be represented by an encoder-decoder structure. We implement ideal upsample, ideal downsample and filtered nonlinearity following [18, 30]. mon ways to process the edge pixels: circular shift cir fills the missing pixels with pixels that just moved out. In constant, cropped shift cro fills the missing pixels with constant, and the shift-equivariance is only considered in valid regions. This paper uses cropped shift by default since it is closer to the default padding mode of convolutional layers. 3.2. Alias-free Latent Diffusion Models In this work, we focus on improving the fractional shiftequivariance of SD [35] for its widespread applications in the community. While StyleGAN3 provides an effective methodology for designing alias-free CNNs, these aliasfree mechanisms cannot be directly applied to SD. This is because the generation process of StyleGAN3 solely depends on the latent-to-image synthesis network, enabling extremely flexible latent-related design for equivariance, such as using Fourier latents and cropping border pixels as reviewed in Sec. 3.1. In contrast, the equivariance of LDMs depends jointly on AE encoder, decoder, and U-Net. Specifically, the latent is produced by AE encoder, leaving less room to redefine the latent, not to mention the self-attention operators that are sensitive to small changes. To this end, we propose Alias-Free LDM (AF-LDM)  (Fig. 3)  , which has two significant improvements. 1) We propose continuous latent representation (Sec. 3.3), which has limited frequency bandwidth and naturally supports fractional shift. To ensure this representation, we modify network modules with anti-aliasing designs and regularize the learning process with an equivariance loss. 2) To enhance the equivariance of self-attention, we employ equivariant attention (Sec. 3.4) that fixes the key and value features of self-attention when processing the shifted latents. Some anti-aliasing techniques are inspired by StyleGAN3. The comparison between StyleGAN3 and our AF-LDM is 4 Table 1. Comparison between StyleGAN3 and AF-LDM. Our main improvements are highlighted in bold. StyleGAN3 AF-LDM Latent Representation Resampling Filtered Nonlinearity Cropping Boundary Pixels Equivariance Loss Equivariant Attention Fourier feature Continuous latent Kaiser filtering - FFT summarized in Table 1. 3.3. Continuous Latent Representation via Equivariance Loss Based on the definition, LDM pipeline is fractional shiftequivariant if ϵθ(T(zt), t) = T(ϵθ(zt, t)), D(T(z)) = Tk(D(z)), (1) (2) where is the downsampling factor of VAE. Since we implement fractional shift as Fourier shift, each should be band-limited continuous latent that can be sampled everywhere by applying DFT, phase shift, and IDFT. For example, if the resolution of input image is s, then the maximum frequency of should be less than s/(2k) based on sampling theorem [39]. By incorporating anti-aliasing layers shown in Fig. 3, the shift-equivariance of randomly initialized AE is significantly enhanced1, as verified in Table 2. However, as training progresses, shift-equivariance gradually deteriorates. We hypothesize that this is because of the imperfect anti-aliasing designs, such as the processing of boundary pixels and nonlinearities, which enable the network to introduce aliasing that undermines equivariance. These aliasing effects may inadvertently aid the networks learning process, leading to their amplification over time. Recognizing that anti-aliasing modules alone are insufficient to fully resolve the equivariance issue, we shift our focus to regularizing network learning by directly optimizing the error between the outputs of shifted inputs and the corresponding shifted outputs. Specifically, we propose an equivariance loss for any network that rescales the input by k: = (T(x)) Tk(f (x))2 2. (3) The loss is directly added to the original training loss of . Since is Fourier shift, this loss forces the network to only use the low-frequency information that the discrete latent can represent. With all these modifications, the continuous latent of Alias-free VAE is nearly band-limit signal as illustrated in Fig. 2. In practice, the equivariance loss is implemented as follows. VAE. We compute the equivariance loss for the VAE encoder and decoder separately. Formally, given input RHW C, the equivariance loss is defined as (4) Lenc = (cid:2)E(T(x)) T/k(E(x))(cid:3) M/k2 2, Ldec = (cid:2)D(T/k(z)) T(D(z))(cid:3) M2 2, where = sg(E(x)), sg is stop gradient operator, and denotes the valid mask for cropped shift T. Since the fractional shift of an image is not well defined, the offsets are integers, i.e., = (x, y) N2. U-Net. For latent zt RH/kW/kC equivariance loss of U-Net is defined as at timestep t, the (5) Lunet = (cid:2)ϵ θ(T cir (zt), t) T(ϵθ(zt, t))(cid:3) M2 2, (6) where = (x/k, y/k), and are sampled in the same way as VAE. ϵ θ is the U-Net with equivariant attention (Sec. 3.4). We first compute ϵθ(zt, t) and cache attention features and then compute ϵ (zt), t). Considering U-Net is more easily affected by the padded pixels due to cropped shift in the iterative denoising process, we apply circular shift cir when shifting the input latent to pad meaningful pixels. Note that we still use cropped valid mask to filter out the padded pixels when computing loss. θ(T cir 3.4. Equivariant Attention Given an input image token sequence RHW dm, the self-attention operation [45] is defined as SA(x) = softmax(xW Q(xW K))xW , (7) where Q, Rdmdk and Rdmdv are projection matrices. For simplicity, the scaling factor is omitted. While self-attention is inherently shift-equivariant under circular shifts, it fails to maintain equivariance under cropped shifts or non-rigid deformations (e.g., warping). That is because to achieve equivarince, we need softmax(T (x)W Q(T (x)W K))T (x)W = (softmax(xW Q(xW K))xW ), (8) of the side and () denotes shifting (reindexing rows). The to right-hand softmax(T (x)W Q(xW K))xW softmax and matrix right multiplication are row-wise independent. The equation breaks since (x)W and (x)W are modified by cropping. 8 since equation reduces 1We found the effect of self-attention layers in VAE is neglectable. We do not apply Equivariant Attention to VAE. To address this, we redefine self-attention as point-wise operation, ensuring equivariance under any relative shifts 5 Table 2. AF-VAE reconstruction quality and shift-equivariance on 256256 ImageNet validation set. Equiv. Loss refers to Equivariance Loss. : Original SD VAE was trained on OpenImages [23]. This model is trained on ImageNet using our code. Model Configuration Rec. PSNR rFID Enc. SPSNR Dec. SPSNR SD VAE SD VAE + Ideal sampling + Filtered nonlinearity + Equiv. loss (AF-VAE) SD VAE + Equiv. loss AF-VAE random weights 26.94 26.45 25.76 25.42 25.53 24.59 - 0.69 0.63 1.01 0.86 1.00 4.18 - 25.07 19.71 21.51 29.49 45.10 41.46 29.43 22.31 21.36 21.75 24.61 33.88 30.74 29. Table 3. Unconditional AF-LDM sampling quality and shiftequivariance on 256 256 FFHQ. Model Configuration FID Latent SPSNR Image SPSNR LDM w/ SD VAE + CFA in Inference LDM w/ AF-VAE + CFA in Inference + Ideal Sampling + Filtered Nonlinearity + Equiv. Loss w/o CFA + CFA in Equiv. Loss (AF-LDM) LDM w/ AF-VAE + Equiv. loss AF-LDM random weights 14.91 14.91 15.27 15.27 17.03 14.28 16.90 19.05 39.14 - 27.47 38.22 22.87 23.58 24.44 24.53 28.57 40. 37.63 39.84 12.38 21.69 12.18 13.09 14.03 14.15 17.65 28.06 25.99 29.68 to reference frame. Formally, given reference frame xr RHW dm and its shifted version xs, we propose Equivariant Attention (EA), defined as: EA(xr, xs) = softmax(xsW Q(xrW K))xrW . (9) Since matrix right multiplication is row-wise independent and each token is row vector of xs, the above equation is point-wise operation applied individually to each token in xs. As result, the operation is inherently equivariant for any relative deformation. Interestingly, we observe that this equivariant attention mechanism has the same form as Cross-Frame Attention (CFA) in video editing literature, which is primarily used as an inference-time technique. In our method, however, it is important to incorporate EA together with equivariance loss during training to effectively mitigate aliasing effects. This is because, without EA, the equivariance loss will focus on fixing the attention module rather than the aliasing. For ease of understanding, we refer to EA as CFA in the experiment section. 4. Experiment 4.1. Ablation Study In the ablation study, we first train the Alias-Free VAE (AFVAE) on 256 256 ImageNet [6], initializing weights from the Stable Diffusion VAE (kl-f8 model in [35]). We then train an unconditional AF-LDM in the latent space of AFVAE on 256 256 FFHQ [16] from scratch. Figure 4. Visualization of LDM latent fractional shifts. The latent is obtained by DDIM inversion. The difference map between outputs and shifted outputs is given in the bottom right corner. To assess the fractional shift-equivariance of models, we compute shift PSNR (SPSNR) [18, 56] that positively correlates with shift-equivariance. Formally, for model that rescales the inputs by k, SPSNR for an input is given by: SPSNRf (x) = PSNR(f (T(x)), Tk(f (x))). (10) In the following experiments, we evaluate both the quality and shift-equivariance of VAE and LDM. Our alias-free models are trained with an objective that balances enhancing equivariance with maintaining the original task quality. VAE. To evaluate VAE quality, we use reconstruction PSNR and FID [14]. Shift-equivariance is measured by encoder SPSNR and decoder SPSNR. The encoder SPSNR checks if the VAE latents are properly band-limited, while the decoder SPSNR evaluates output consistency under input shift. All metrics are computed on 50,000 256 256 ImageNet validation images. Table 2 shows that ideal sampling and filtered nonlinearity improve VAE shift-equivariance. However, both encoder and decoder SPSNR decrease compared to randomly initialized model (last row), possibly due to imperfect anti-aliasing modules. Adding an equivariance loss as regularization term balances task fidelity with high shiftequivariance. Note that applying equivariance loss without 6 Figure 5. Shift-equivariance during denoising process. We perform two denoising processes: one with Gaussian noise and the other with 1/2 pixel-shifted version of the same noise, computing the SPSNR after each denoising step. The experiment is conducted on 10,000 samples over 20 denoising steps. anti-aliasing modules significantly lowers the reconstruction quality (second-to-last row), highlighting that alias-free architecture is necessary for adding equivariance loss. LDM. For LDM quality evaluation, we use FID computed on 50,000 samples generated with 50 DDIM steps. Shiftequivariance of LDM is measured by latent SPSNR and image SPSNR, with the former only considering the denoising process and the latter encompassing both denoising and VAE encoding. The SPSNR is computed on 10,000 randomly sampled noisy latents. Unless otherwise specified, CFA is enabled by default during inference, as disabling it would result in significant changes to the images identity. The quantitative results are shown in Table 3, and partial visualization results are shown in Fig. 4. Although LDM with SD VAE has good SPSNR, we can observe severe quality degradation in fractional shift results (Fig. 4(a)). Using AF-VAE can improve the quality of shifted frames, but texture sticking still occurs (Fig. 4(b)). This issue can be addressed by adding equivariance loss (Fig. 4(c)). By applying CFA in equivariance loss, the final model, AF-LDM, has the best overall shift-equivariance. To further demonstrate the improvements of our AFLDM over standard LDM, we evaluate SPSNR at each step of the denoising process, as shown in Fig. 5. In standard LDM, shift-equivariance degrades progressively due to error accumulation throughout the denoising process. In contrast, our AF-LDM mitigates this degradation, maintaining higher shift-equivariance across denoising steps. Fractional Shift Consistency. To verify consistent shiftequivariance under fractional shifts, we horizontally shift set of images and compute the averaged SPSNR for each step  (Fig. 6)  . Baseline models exhibit high shiftequivariance only for integer shifts, causing the blur outputs of the VAE decoder and the bouncing effect of LDM. In contrast, alias-free models achieve higher average SPSNR and lower variance, resulting in more stable AF-LDM outputs. Figure 6. Quantitative comparison of fractional shift consistency. We horizontally shift latents by 1/8 pixels per step and compute the average SPSNR for the VAE decoder and LDM pipeline for each step. The VAE is tested on 500 ImageNet validation images, and LDM is tested on 500 FFHQ images. Figure 7. Warping consistency experiments. Given neighboring frames and their ground truth optical flow, we can warp the inverted latents and compute the inversion warping error. We then reconstruct an inverted latent and its warped version to test the generation warping error. Cross-Frame attention (CFA) is enabled in both inversion and generation. The input warping error is computed as reference. 4.2. Warping-Equivariant Video Editing With more stable generation process, our AF-LDM can enhance video editing by improving temporal consistency. To showcase this application, we implement an aliasfree Stable Diffusion (AF-SD) by integrating AF-VAE, anti-alias U-Net modifications, and retraining the textconditional LDM on LAION Aesthetic 6.5+ dataset [38]. We then propose simple and elegant Warping-equivariant video editing algorithm. Although our AF-SD is designed for pixel shiftequivariance, it performs well under irregular pixel shifts, such as flow warping. We verify this through the experiment shown in Fig. 7. Quantitative results in Table 4 demonstrate superior inversion and generation warping-equivariance in AF-SD over SD, indicating that AF-SD can better preserve deformation information. Higher inversion warpingequivariance is especially advantageous in inversion-based video editing because even if the reconstruction process is changed (for example, using another prompt), the diffusion model can still generate consistent results by utilizing the deformation information from latents. 7 Figure 8. Qualitative comparison of video editing consistency between SD and AF-SD using warping-equivariant video editing. Texture flickering can be observed in SDs results. More results are available in the supplementary. Table 4. Warping error (defined in Fig. 7) computed on Sintel [2]. Model Input Warping PSNR Inversion Warping PSNR Generation Warping PSNR SD AF-SD 22. 17.53 19.68 21.95 26.10 Based on these observations, we propose novel warping-equivariant video editing method. First, we apply DDIM inversion to input video to an intermediate denoising timestep (similar to SDEdit [29]) with an empty prompt. Then, we regenerate video from the inverted latents with new prompt. CFA is enabled in both processes. This method implicitly deforms noisy latents without the need for explicit latent warping [4, 5, 50, 51]. As illustrated in Fig. 8, AF-SDs robust inversion and generation ensure more consistent edited videos than SD. 4.3. Image-to-image Translation I2SB. Super-resolution with Latent Image-to-Image Schrodinger Bridge (I2SB) [25] is conditional diffusion model that directly maps one image distribution to another without going through Gaussian noise. Originally, I2SB operates in image space, with model weights initialized from an unconditional diffusion model. In our experiments, we implement two variants of I2SB for 4 super-resolution in latent space: one based on LDM and the other on AF-LDM. As shown in Fig. 9, the standard latent I2SB exhibits severe flickering and quality degradation when input shifts. In contrast, the alias-free variant demonstrates strong shiftequivariance, providing consistent outputs. Normal Estimation with YOSO. Normal Estimation is another image-to-image task that requires the model to produce consistent outputs across shifts. We implement oneFigure 9. Qualitative comparison of shift-equivairance in latent I2SB 4 super-resolution results. Figure 10. Qualitative comparison of shift-equivariance in YOSO normal estimation. The contrast of local regions is enhanced. Please refer to the supplementary for clearer video comparison. step conditional diffusion model [49], You-Only-SampleOnce (YOSO), following StableNormal [53]. YOSO leverages ControlNet [55] to adapt pre-trained SD into normal map prediction model conditioned on input RGB images. For our implementation, we use x0 parameterization instead of xt parameterization used in [53]. We first train baseline YOSO initialized from SD. We then enhance the ControlNet of YOSO with our anti-alias designs and train an alias-free YOSO (AF-YOSO) based on AF-SD. We compare the shift-equivariance of vanilla YOSO and AF-YOSO in Fig. 10. Although the baseline model produces largely consistent outputs, some flickering remains, particularly around high-frequent textures. In contrast, AFYOSO delivers fully consistent results across the entire image, effectively mitigating flickering. 8 5. Conclusion We present Alias-Free Latent Diffusion Models, novel framework designed to eliminate the instability of LDMs when input shifts occur. By incorporating equivariance loss and equivariant attention along with anti-aliasing modules, our model achieves significantly improved shiftequivariance. Experimental results demonstrate that our model can facilitate various editing tasks, such as inversionbased video editing, providing consistent and stable outputs. Additionally, the approach can be extended to other tasks that leverage pre-trained latent diffusion model, such as super-resolution and normal estimation, substantially enhancing consistency over spatial shifts. Acknowledgments. This research is supported by NTU SUGNAP. This study is also supported under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s)."
        },
        {
            "title": "References",
            "content": "[1] Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to small image transformations? Journal of Machine Learning Research, 20(184):1 25, 2019. 3 [2] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. naturalistic open source movie for optical flow evaluation. In European Conf. on Computer Vision (ECCV), pages 611 625. Springer-Verlag, 2012. 8 [3] Anadi Chaman and Ivan Dokmanic. Truly shift-invariant the convolutional neural networks. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 37733783, 2021. 2, 3 In Proceedings of [4] Pascal Chang, Jingwei Tang, Markus Gross, and Vinicius Azevedo. How warped your noise: temporally-correlated noise prior for diffusion models. In The Twelfth International Conference on Learning Representations, 2024. 2, 3, 8, 14 [5] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flowguided attention for consistent text-to-video editing. arXiv preprint arXiv:2310.05922, 2023. 2, 3, [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 6 [7] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 3, 12 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 3 [9] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. In ECCV, 2024. 2 [10] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arXiv:2307.10373, 2023. 2, 3 [11] Jiayi Guo, Xingqian Xu, Yifan Pu, Zanlin Ni, Chaofei Wang, Manushree Vasu, Shiji Song, Gao Huang, and Humphrey Shi. Smooth diffusion: Crafting smooth latent spaces in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7548 7558, 2024. 3 [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 4 [13] Qiyuan He, Jinghao Wang, Ziwei Liu, and Angela Yao. Aid: arXiv Attention interpolation of text-to-image diffusion. preprint arXiv:2403.17924, 2024. 3, 13 [14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [16] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. 3, 4, 6 [17] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improvIn Proceedings of ing the image quality of stylegan. the IEEE/CVF conference on computer vision and pattern recognition, pages 81108119, 2020. 3, 4 [18] Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. Advances in neural information processing systems, 34:852863, 2021. 2, 3, 4, 6, [19] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60076017, 2023. 3 [20] Osman Semih Kayhan and Jan van Gemert. On translation invariance in cnns: Convolutional layers can exploit absolute spatial location. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1427414285, 2020. 2 [21] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. In 9 Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1595415964, 2023. 2, 3 [22] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 3 [23] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. 6 [24] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [25] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos Theodorou, Weili Nie, and Anima Anandkumar. I2sb: Image-to-image schrodinger bridge. arXiv preprint arXiv:2302.05872, 2023. 8 [26] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [27] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008, 2023. [28] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. 3 [29] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 8 [30] Hagay Michaeli, Tomer Michaeli, and Daniel Soudry. Aliasfree convnets: Fractional shift invariance via polynomial In Proceedings of the IEEE/CVF Conference activations. on Computer Vision and Pattern Recognition, pages 16333 16342, 2023. 2, 3, 4, 12 [31] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. 3 [32] Simon Niklaus and Feng Liu. Softmax splatting for video frame interpolation. In IEEE Conference on Computer Vision and Pattern Recognition, 2020. 13 [33] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 12 [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 3, 4, 6, 12 [36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 3 [37] Sourajit Saha and Tejas Gokhale. Improving shift invariance in convolutional neural networks with translation invariant polyphase sampling. arXiv preprint arXiv:2404.07410, 2024. 3 [38] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 7 [39] Claude Elwood Shannon. Communication in the presence of noise. Proceedings of the IRE, 37(1):1021, 1949. 2, 5, 12 [40] Liao Shen, Tianqi Liu, Huiqiang Sun, Xinyi Ye, Baopu Li, Jianming Zhang, and Zhiguo Cao. Dreammover: Leveraging the prior of diffusion models for image interpolation with large motion. arXiv preprint arXiv:2409.09605, 2024. 3 [41] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88398849, 2024. [42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 3 [43] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3 and Stefano Ermon. arXiv preprint [44] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 3 [45] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 4, 5 [46] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C.K. Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. 2024. 2 [47] Yufei Wang, Wenhan Yang, Xinyuan Chen, Yaohui Wang, Lanqing Guo, Lap-Pui Chau, Ziwei Liu, Yu Qiao, Alex Kot, and Bihan Wen. Sinsr: diffusion-based image superresolution in single step. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2579625805, 2024. 2 [48] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu 10 Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. [49] Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. Diffusion models trained with large data are transferable visual models. arXiv preprint arXiv:2403.06090, 2024. 2, 8 [50] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender video: Zero-shot text-guided video-to-video In SIGGRAPH Asia 2023 Conference Papers, translation. pages 111, 2023. 2, 3, 8, 14 [51] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Fresco: Spatial-temporal correspondence for zero-shot video In Proceedings of the IEEE/CVF Conference translation. on Computer Vision and Pattern Recognition, pages 8703 8712, 2024. 2, 3, 8, 14 [52] Zhantao Yang, Ruili Feng, Han Zhang, Yujun Shen, Kai Zhu, Lianghua Huang, Yifei Zhang, Yu Liu, Deli Zhao, Jingren Zhou, et al. Eliminating lipschitz singularities in diffusion models. arXiv preprint arXiv:2306.11251, 2023. 3 [53] Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, Stablenormal: Reducing diffusion and Xiaoguang Han. arXiv preprint variance for stable and sharp normal. arXiv:2406.16864, 2024. 2, 8 [54] Kaiwen Zhang, Yifan Zhou, Xudong Xu, Bo Dai, and Xingang Pan. Diffmorpher: Unleashing the capability of difIn Proceedings of fusion models for image morphing. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79127921, 2024. 3, [55] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 8 [56] Richard Zhang. Making convolutional networks shiftIn International conference on machine invariant again. learning, pages 73247334. PMLR, 2019. 2, 3, 6 [57] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023. 2 [58] Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, and Chen Change Loy. Upscale-a-video: Temporalconsistent diffusion model real-world video superIn Proceedings of the IEEE/CVF Conference resolution. on Computer Vision and Pattern Recognition, pages 2535 2545, 2024. 2 for 11 Table 5. Hyperparameters of AF-VAE, AF-LDM, and AF-SD. All models are trained on 8 A100 GPUs. Model Task Dataset (a) AF-VAE (b) AF-LDM (c) AF-SD Reconstruction ImageNet Unconditional Generation FFHQ Text-to-Image LAION Aesthetic 6.5+ Resolution Latent channels U-Net channels Attention Layers Mid Block Attention Head Channels Depth Batch Size Learning Rate 256 256 4 128, 256, 512, 512 - True 512 2 32 5 10 256 256 4 192, 384, 384, 768, 768 1 2 3 4 True 24 2 96 1 104 512 512 4 320, 640, 1280, 1280 1 2 3 True 8 2 32 1 106 A. Implementation Details A.1. Alias-free Modules To obtain the fractional shift of function applied to discretely sampled input Z, common approach is to convert the into ca ontinuous signal z, apply the fractional shift and corresponding function in the continuous domain, and convert the result (T (z)) back to the discrete domain. Formally, we define (T (Z)) = to discrete(f (T (z))), (11) where = to continuous(Z). To ensure is shift-equivarint, we require (T (Z)) = to discrete(f (T (z))) = to discrete(T (f (z))) (12) = (F (Z)). As noted in StyleGAN3 [18], the above equation holds when the conversions between and are invertible, which in turn requires the signal to satisfy the sampling theorem [39]: the bandwidth of the signal must be smaller than half the sampling rate of Z. However, StyleGAN3 identifies two operations in common CNNs that violate this requirement: 1. Nonlinearities, which introduce new high-frequency components into z. 2. Upsampling and downsampling, which may fail to correctly adjust the frequency of z. To address these issues, we build upon prior work on alias-free neural networks [18, 30] and apply the following modifications to the network layers in both the VAE and U-Net architectures: Downsampling. Strided downsampling convolutions are replaced with standard convolution followed by low-pass filter and nearest downsampling. The low-pass filter is implemented as an ideal low-pass filter, removing high frequencies in the Fourier domain, as described in [30]. Upsampling. Upsampling is performed by zero-padding between existing pixels followed by convolution with sinc interpolation kernel. Similar to the ideal low-pass filter in downsampling, this convolution is executed in the Fourier domain via multiplication. Nonlinearity. All nonlinearities, except those in the last layer, are wrapped between 2 ideal upsampling and 2 ideal downsampling. A.2. VAE AF-VAE is initialized from the Stable Diffusion (SD) VAE (kl-f8 AE in [35]). Alias-free modules replace the corresponding components in both the VAE and GAN discriminator [7]. Hyperparameters are detailed in Table 5(a). The model is retrained on ImageNet using combination of reconstruction, KL, GAN [7], and equivariance loss. LVAE = Lrec + λ1LKL + λ2LGAN + λ3LVAE (13) We set λ1 = 106, λ2 = 0.25, λ3 = 1 by default. The equivariance loss of VAE is formulated as follows: eq . eq = Ex( (cid:2)E(T(x)) T/k(E(x))(cid:3) M/k2 LVAE 2+ (cid:2)D(T/k(z)) T(D(z))(cid:3) M2 2), (14) where RHW 3 is the input image, = sg(E(x)), RH/kW/k4 is the latent downsampled by k, sg is stop gradient operator, and denotes the valid mask for cropped shift T. Offsets = (x, y) N2. x, are uniformly sampled from [ 3 8 W, 3 8 ], respectively. 8 H] and [ 3 8 H, 3 A.3. Unconditional LDM After we obtain the AF-VAE, we train unconditional Latent Diffusion Models (LDMs) from scratch in its latent space. Similar to AF-VAE, alias-free modules are incorporated into the U-Net. The hyperparameters of U-Net are detailed in Table 5(b). The training loss combines diffusion loss and equivariance loss. LLDM = EE(x),ϵN (0,1),t, [ϵ ϵθ(zt, t)2 2] + λLLDM eq , (15) where zt is the noisy latent at timestep t. λ is set to 1 by default. The equivariance loss of LDM is defined as follows: (cid:104) LLDM ϵ θ(T cir (cid:105) (zt), t) T(ϵθ(zt, t)) eq = EE(x),t M2 2, (16) where = (x/k, y/k), and are sampled in the same way as VAE. ϵ θ is the U-Net with equivariant attention. A.4. Text-conditional LDM We also train an alias-free text-conditional LDM (AF-SD) in the latent space of AF-VAE. Specifically, we initialize the U-Net from Stable Diffusion V1.5 [35], modify it with alias-free modules, and retrain it on the LAION Aesthetic 6.5+ dataset (Table 5(c)). The loss function is similar to that of AF-LDM: LSD = EE(x),ϵN(0,1),t,c, [ϵ ϵθ(zt, t, c)2 2] + λLLDM eq , (17) where is the text embedding of obtained from CLIP [34]. B. Details of Video Editing B.1. Warping-Equivariant Video Editing The algorithm of warping-equivariant video editing mentioned in the main text is illustrated in Alg. 1. Unlike standard inversionbased methods that invert each frame independently, we utilize 12 i=1, encoder E, decoder D, U-Net i=1. Algorithm 1 Warping-equivariant Video Editing Input: Video = {xi}N ϵθ, U-Net with cross-frame attention ϵ θ. Output: Edited video ˆX = {ˆxi}N 1: for in [1, ..., N] do 0 E(xi) zi 2: 3: z1 4: for in [2, ..., ] do 5: 6: ˆz1 7: ˆx1 D(ˆz1 0) 8: for in [2, ..., ] do 9: 10: DDIMInversion(zi zi , ϵθ) 0 DDIMSampling(zi ˆzi ˆxi D(ˆzi 0) DDIMInversion(z1 0 DDIMSampling(z 0, ϵ θ) 0, ϵθ) Cache K, in ϵ θ 0, ϵ θ) Cache K, in ϵ θ i=1. θ , ϵ2 0 E(x1) 0 E(x2) DDIMInversion(z1 DDIMInversion(z Algorithm 2 Image Splatting and Interpolation Input: Input images x1, x2, encoder E, decoder D, U-Net ϵθ, U-Net with cross-frame attention to the first and second image ϵ1 θ , interpolation frame numbers . Output: Interpolation video ˆX = {ˆxi}N 1: ff wd, fbwd FlowEstimation(x1, x2) 2: z1 3: z2 4: z1 5: z2 6: DDIMSampling(z1 7: DDIMSampling(z2 8: for in [1, ..., ] do α 1/(N + 1) 9: θ KVInterpolation(ϵ1 ϵα 10: Splatting(z1 z1 , α ff wd) 11: Splatting(z2 z2 , (1 α) fbwd) 12: , z2 Slerp(z1 zα 13: ˆzi 0 DDIMSampling(zα ˆxi D(ˆzi 0) Cache K, in ϵ1 θ Cache K, in ϵ2 θ 0, ϵθ) 0, ϵθ) 0, ϵθ) 0, ϵθ) , ϵα θ ) θ , ϵ , α) θ , α) 14: 15: cross-frame attention during inversion to naturally preserve deformation information in noisy latent. Additional results can be found in Fig. 11. B.2. Image Splatting and Interpolation Leveraging AF-SDs high warping-equivariance, we also implement simple image interpolation method using latent splatting (Alg. 2). Given two input images, we invert their latents, splat them using forward and backward flows with interpolation ratio α, and interpolate the latents and attention features to generate intermediate frames [13, 54]. Compared to standard SD, AF-SD produces smoother results. Although the method sometimes produces flickering artifacts in the occlusion region caused by latent splatting, this issue can be mitigated by integrating depth information and softmax splatting [32]. To keep the algorithm as simple as possible, we do not apply this more advanced technique. The results are shown in Fig. 12. 13 Figure 11. Visualization of warping-equivariant video editing. Figure 12. Visualization of image interpolation. mended to view it on the project page. It is recomFigure 13. Comparison on video editing. Prompt: wolf Husty. C. Comparison to State-of-the-art Zero-shot"
        },
        {
            "title": "Video Editing Methods",
            "content": "Figure 13 and Table 6 present both qualitative and quantitative comparisons with SOTA zero-shot editing methods. For fair assessment, we use SD 1.5 as the backbone without enabling ControlNet. We conduct experiments on 13 video clips that exhibit small motions. Notably, FRESCO fails to produce satisfactory outputs without ControlNet. In contrast, our warping-equivariant editing pipeline (inversion and editing with cross-frame attention) with AF-SD achieves lower neighboring-frame warping error (measured as the MSE between the warped first frame and the second frame) than both TokenFlow and our SD version, highlighting the effectiveness of our alias-free design for consistent video editing. Although FLATTEN obtains an even lower warping error, it relies on an external flow estimation model, whereas our approach does not require any additional modules. Table 6. Quantitative comparison of video editing. FRESCO TokenFlow FLATTEN Ours (SD) Ours (AF-SD) Warping MSE 0.325 0. 0.036 0.094 0.056 D. Limitation Our AF-LDM has limitation: since we implement equivariant attention as cross-frame attention, all editing methods depend on reference frame. This assumption means that objects or textures not present in the reference frame can lead to incoherent content in occlusion areas due to overlapping or new objects entering subsequent frames. Similar to flow-based video editing methods [4, 50], our video editing method may produce incoherent results in static background regions where flow guidance is insufficient [51]. E. More Qualitative Results We present additional qualitative comparisons in this section, including AF-VAE vs. VAE (Fig 14) and AF-LDM vs. LDM (Fig 15). It is recommended to watch the videos on the project page. 14 Figure 14. Quantitative comparison of SD VAE and AF-VAE in latent shifting experiments. Shift offsets are 1, 19, 37, 55, 74, and 92 pixels. 15 Figure 15. Quantitative comparison of FFHQ unconditional LDM and AF-LDM in noisy latent shifting experiments. Latents are obtained from DDIM inversion. Shift offsets are 1, 19, 37, 55, 74, and 92 pixels."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University",
        "Wangxuan Institute of Computer Technology, Peking University"
    ]
}