{
    "paper_title": "UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction",
    "authors": [
        "Siyuan Yao",
        "Dongxiu Liu",
        "Taotao Li",
        "Shengjie Li",
        "Wenqi Ren",
        "Xiaochun Cao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet"
        },
        {
            "title": "Start",
            "content": "UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction Siyuan Yao, Dongxiu Liu, Taotao Li, Shengjie Li, Wenqi Ren and Xiaochun Cao, Senior Member, IEEE. 1 5 2 0 2 5 1 ] . [ 1 1 4 9 2 1 . 2 1 5 2 : r AbstractBuilding extraction from remote sensing images is challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated GlobalLocal Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet. Index TermsBuilding extraction, Cooperative encoder, Global-local fusion, Uncertainty-aggregated decoder. I. INTRODUCTION"
        },
        {
            "title": "B UILDING extraction is a fundamental research topic",
            "content": "aiming to distinguish the buildings from high-resolution remote sensing (RS) images [1], which has been deployed into wide range of applications including urban planning, population forecasting and Geographic Information System (GIS) [2], [3], [4], etc. As satellite and aerial remote sensing imaging technologies advance rapidly, how to precisely and Manuscript received January 5, 2025; revised March 12, 2025 and July 16, 2025; accepted December 10, 2025. This work was supported by the National Natural Science Foundation of China (No. 62402055, No. 62302053, No. U24B20175, No. 62322216 and No. 62025604), the Shenzhen Science and Technology Program (No. KQTD20221101093559018), the Open Fund of Key Laboratory of the Ministry of Education on Artificial Intelligence in Equipment (No. 2024-AAIE-KF04-01) and the Guangdong Basic and Applied Basic Research Foundation under Grant 2024A1515012360. (Corresponding author: Xiaochun Cao.) S. Yao, D. Liu and S. Li are with School of Computer Science (National Pilot Software Engineering School), Beijing University Of Posts and Telecommunications, Beijing 100876, China. (email: yaosiyuan04@gmail.com; liudongxiu0504@gmail.com; lishengjie@bupt.edu.cn). T. Li, W. Ren and X. Cao are with School of Cyber Science and Technology, Shenzhen Campus, Sun Yat-sen University, Shenzhen 518107, litt93@mail.sysu.edu.cn; rwq.renwenqi@gmail.com; caoxiChina. (email: aochun@mail.sysu.edu.cn). Fig. 1. general introduction to UAGLNet. We construct complementary local and global representations of the buildings under the guidance of uncertainty modeling in dual branches, which enforces the model to focus on the ambiguous regions for high-quality building extraction. efficiently extract the buildings from aerial images has gained significant attention in the research community. Over the past few decades, numerous approaches have been proposed to extract the buildings from high-resolution RS images. Early works apply hand-crafted descriptors, e.g. texture [5], geometrical shape [6] and neighboring connection [7] to identify the pixels. Despite the intuitive simplicity, the manually extracted features are not effective in handling complex structure variations of the buildings. resulting in unsatisfying accuracy and limited generalization capability. Inspired by the significant successes of convolutional neural networks (CNNs), series of CNN based building extraction methods have emerged [1], [8], [9], [10]. These methods employ local convolution kernels to capture the discriminative features via forward-backward propagation, allowing the models to construct hierarchical semantic representation of the foreground buildings by large-scale model training. Unfortunately, as the CNN based architecture relies on stacking multiple convolutional layers/blocks with limited reception fields, it fails to capture the long-range relationship in different regions. In most recent years, the vision transformers (ViTs) have been applied in building extraction task [11], [12], [13]. The ViTs based approaches utilize non-local self-attention to capture the long-range dependencies, thus being able to excavate the informative global contexts for dense prediction. The most advanced approach GraphGST [14] aggregates the local-toglobal correlations by integrating graph structural information into the Transformer architecture. The positional encoder in 2 Kernel Feature Modulator (MKFM) is designed to extract the features with different reception fields, which is combined with the standard Multi-Head Self-Attention (MHSA) in an intermediate cooperative interaction block (CIB). The features obtained by the CIB module in the third stage exhibits stronger discriminability while preserving local details. Second, to overcome the insufficient global-local feature integration, we design Global-Local Fusion (GLF) module. This module complementarily leverages the CIB outputs to compensate the local features and global features via efficient interaction. Finally, to enforce the models discriminative capability in challenging regions, these interacted global-local features are passed into Uncertainty Aggregated Decoder (UAD), which suppresses the low-confidence uncertain region and alleviate the segmentation uncertainties eventually. In conclusion, our contributions can be summarized as follows: We design Cooperative Encoder, which captures local/global information using intermediate cooperative interaction block (CIB) with hybrid convolutional and selfattention operators, allowing the model to narrow the inherent gap of the feature pyramids. We propose Global-Local Fusion (GLF) module to complementarily fuse the global and local features, making the model perform more efficient feature integration among the hierarchical pyramids. We propose an Uncertainty-Aggregated Decoder (UAD), which enforces the model to focus on the challenging pixels to mitigate the segmentation uncertainty. II. RELATED WORK A. Building Extraction Existing building extraction methods typically employ deep learning techniques. The representative approaches utilize CNNs [8], [9], [19], [20] to extract the discriminative features. For example, SRINet [19] aggregates multi-scale contexts for semantic understanding by successively fusing multi-level features. HRNet [21] connects feature maps of different resolutions and performs interaction between them for semantic feature enhancement. MTPA-Net [20] proposes multi-task parallel attention convolutional structure to improve the accuracy using the scene prior. CBR-Net [1] progressively refines the building boundary by perceiving the direction of pixels to the nearest candidate object. LFEMAP-Net [9] enhances the representation of spatial details under the guidance of edge prior and utilizes multi-scale attention module at the decoding stage for feature pyramid aggregation. key limitation of the CNN based approaches is that they fail to capture the long-range relationship of pixels in different regions. Recently, the transformer based methods have been proposed for building extraction. MSST-Net [22] utilizes the Swin transformer as backbone and develops scale adaptive decoder for multi-scale feature representation. STT [23] represents the buildings as set of sparse feature vectors to reduce the computational complexity in transformers. BuildFormer [15] captures the global context while preserving spatialdetailed features by parallelizing convolutions and transformers. DSAT-Net [16] combines CNNs and Transformers using Fig. 2. Comparison of different hybrid CNN-Transformer structures for building extraction. (a) Parallel CNN-Transformer structure [15]; (b) Sequentially Mixed CNN-Transformer structure by stacking CNNs in shallow layers and Transformers in deep layers [18]; (c) Alternative CNN-Transformers structure [16]; (d) Our Cooperative architecture. GraphGST learns visual semantics between adjacent pixels and uses transformer to facilitate the feature fusion. However, these methods still require to employ the hand-crafted window partition mechanisms in the transformer architecture, which are data-agnostic and ignore the input content, so the query tokens may attend to irrelevant keys/values, leading to performance degradation. To address these issues, some works explore to utilize hybrid CNN-Transformer architecture for building extraction [15], [16], [17]. For example, BuildFormer [15] designs dual-path structure to simultaneously exploit global context and local details. DSAT-Net [16] designs plug-and-play transformer block appended after CNN modules to supplement information. BCTNet [17] adopts two parallel the global i.e. convolutional encoder branch (CB) and branches, transformer encoder branch (TB) to extract multi-scale feature maps. Despite the progress, the performance of existing hybrid CNN-Transformer methods are still limited by the inherent gap of the feature pyramids and insufficient integration of the global-local visual semantics. As demonstrated in Fig. 1, the features in shallow layers tend to preserve more local information but are insensitive to the global contexts of the buildings. Conversely, the features in the deep layers are more favorable at exploiting the overall global connectivity of the building instance, while prone to lose the local details. Therefore, an ideal building extraction approach should not only construct high-quality global-local representation to narrow the gap of the feature pyramids, but also make full use of the complementary global-local representation for feature integration. In this paper, we propose novel Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet) to alleviate the inherent gap of the feature pyramids and insufficient globallocal feature integration. As shown in Fig. 2, instead of stacking CNN and transformer blocks in parallel or sequential fashion like conventional hybrid CNN-Transformer models [15], [18], [16], our UAGLNet employs convolutional blocks in the early stages to capture local information, and transformer blocks in the terminate stages to capture global information. To narrow the gap of the feature pyramids, Multian efficient dual spatial attention module to maximize the advantages of them. BCT-Net [17] parallelizes the convolutional branch and the transformer branch to extract multiscale feature maps. However, the aforementioned methods ignore the gap of the feature pyramids, yielding inaccurate, ambiguous extraction results due to the insufficient globallocal feature integration. In this work, we make full use of the complementary global-local representations for feature integration, which significantly boosts the building extraction performance in complex scenarios. B. Hybrid CNN-Transformer Networks Besides, To leverage the advantage of both CNNs and transformers, the hybrid architecture has proven to be effective in computer vision tasks [24], [25], [26], [27], [28], [29], [30], [31]. CvT [28] first introduces convolutional neural networks before self-attention operation to construct hierarchical representation for local and spatial context modeling. CMT [29] combines depth-wise convolution and lightweight MHSA to balance the performance and computational cost between these two operators. Some works [18], [32], [33], [34] further incorporate the transformer with classical CNN models, e.g. UNet [35] to enhance the global dependencies learning. These methods sequentially stacked CNN and transformer to capture multiscale feature pyramids, and progressively fuse the feature pyramids using UNet decoder to recover the pixel-wise details. the most advanced researches [15], [36], [37], [38], [39], [40], [41], [42], [43] point out that the parallel or asymmetric structures can provide more valuable global-local semantics in different scales. For example, ACAHNet [38] combines CNN and transformer in series-parallel manner and enhances the interaction between features by introducing an adaptively updated semantic map. SMT [36] proposes scale-aware modulation mechanism to fuse the multi-scale information and expand the receptive field. TCNet [44] introduces Interactive Self-attention (ISa) and Windowed Selfattention Gating (WSaG) mechanism to fuse the multi-level features. Zhang et al. [16] propose dual spatial attention, where global attention path (GAP) and local attention path (LAP) are designed for global dependencies and local details prediction. Despite the promising successes, these methods neglect the inherent gaps of the feature pyramids and lack efficient interactions between the global and local features, the segmentation accuracy in the ambiguous which limit region. C. Uncertainty Estimation Uncertainty property [45], [46], [47], [48], [49], [50] has attracted increasing attention in recent safety-critical deep models, which provides powerful evaluator to measure the confidence and assists to identify the ambiguous predictions. There are mainly two types of uncertainty in machine learning, i.e. aleatoric uncertainty and epistemic uncertainty. Aleatoric uncertainty is relative to the inherent variability or randomness of the data, while epistemic uncertainty reflects the model inadequacy [45]. For aleatoric uncertainty modeling, Gaussian to YOLOv3 [47] incorporates Gaussian mixture model 3 alleviate localization ambiguity for target objects in challenging scenarios. In [46], Lakshminarayanan et al. employ an adversarial perturbation technique to generate additional data to train probabilistic neural network to model the aleatoric uncertainty. Wang et al. [48] propose domain adaptive segmentation network under the guidance of uncertain pseudo labels for model transfer training. UMNet [49] introduces multi-source uncertainty mining method to generate reliable pixel-wise labels from multiple pseudo labels, which is capable to improve the robustness in an unsupervised learning manner. For epistemic uncertainty, recent advances utilize probabilistic representation models that learn the distribution over network weights or features [51]. Kendall et al. [52] present Bayesian convolutional neural networks to produce probabilistic pixel-wise semantic segmentation. Huang et al. [53] use the temporal information to simulate the sampling procedure. Postels et al. [54] present sampling-free approach, which approximates the epistemic uncertainty estimates using variance propagation technique. UGTR [55] utilizes probabilistic information to locate difficult-to-detect regions and reason over these uncertain regions with additional attention module. In this work, we focus on identifying the challenging pixels, aiming to mitigate the segmentation ambiguity in uncertain regions. III. METHODOLOGY A. Overview We present the overall structure of the proposed UAGLNet in Fig. 3. UAGLNet follows an encoder-decoder pipeline, which consists of Cooperative Encoder (CE), GlobalLocal Fusion module (GLF) and an Uncertainty-Aggregated Decoder (UAD). The CE employs convolutional blocks in the early stages and transformer blocks in the terminate stage to capture the local and global visual semantics. Among them, an intermediate cooperative interaction block (CIB) is introduced to narrow the gap between the local and global features. Afterwards, the output features are then fed into GLF to complementarily fuse the hierarchical visual representations. Subsequently, we utilize UAD to quantify the pixel-wise to focus on the uncertainty, aiming to enforce the model unconfident regions to mitigate the segmentation uncertainty. B. Cooperative Encoder We propose cooperative encoder to capture both local and global information using hybrid convolutional and selfattention operators. In the early stages, we employ hierarchical CNN blocks to construct local representation of the buildings. Then we introduce cooperative interaction block using hybrid CNN/transformer layers at the intermediate stage to enforce the global-local information interaction. For the terminate stage, we utilize pure transformer to enhance the global contexts. Formally, given an input image R3HW , we pass it into the cooperative encoder to generate hierarchical feature maps = {F1, F2, F3, F4}, where Fi RCiHiWi denotes the output feature map in the i-th stage. At the first two stages, we utilize CNNs with different convolutional kernels to 4 Fig. 3. The overall framework of UAGLNet. UAGLNet consists of Cooperative Encoder (CE), Global-Local Fusion module (GLF) and an UncertaintyAggregated Decoder (UAD). In CE, the intermediate cooperative interaction block (CIB) is introduced to narrow the gap between the local and global features. The hierarchical features are fed into GLF and UAD for feature fusion and uncertainty aggregation. capture the local information. The image is firstly flattened and linearly projected to be 1-dim vector, which can be reshaped as an embedding feature map RC1H1W1. Then we split the feature map across the channel dimension to construct versatile groups, i.e. = [Z1, Z2, Z3, ..., Zn]. For each feature group, we apply depth-wise separable convolution with different kernel sizes to capture the local information, and perform point-wise convolution to combine these grouped features, which can be given by: = Cat(DW33(Z1), ..., DWkk(Zn)) = Wp Z, (1) where Wp denotes the point-wise convolution, Cat() denotes the concatenation operations, DWkk() means the depthwise separable convolutions with kernel size of k. The Depthwise Convolutions is used to enhance the computationally efficient and exploit the visual semantic without introducing the channel-wise redundancy. The output feature map RCiHiWi can be regarded as multi-kernel feature modulator (MKFM), which enhances the local representation with different reception fields. The modulation process on feature Fi can be formulated as: MKFM(Fi) = ϕ(Fi), (2) where is the Hadamard product, ϕ() is linear embedding function with reshaped dimension. By stacking multiple MKFMs in the first two stages, UAGLNet can capture diverse multi-scale local features with only small amount of computation costs. On the contrary, the features learned in the deep stages tend to focus on the discriminative global contexts. To narrow the gap between the local and global features, we develop cooperative interaction block (CIB) using hybrid CNNtransformer layers in the following stage. As illustrated in Fig. 4 (b), at the third stage, we stack multiple CIBs to conduct the global-local information interaction. For the l-th CIB in the third stage, the input feature Xl1 RCiHiWi is firstly normalized and fed into MKFM with residual connection as follows: Xl = l = Xl1 + MKFM(Norm(Xl1)), + FFN(Norm(X )), (3) RCiHiWi denotes the intermediate feature where in the l-th block. FFN is the feed-forward network. Afterwards, we send Xl into standard Multi-Head Self-Attention (MHSA) [56] to model the global dependencies: Xl+1 = l+1 = Xl + MHSA(Norm(Xl)), l+1 + FFN(Norm(X l+1)). (4) With alternative communications between MKFM and MHSA using multiple CIBs, the cooperative encoder allows to perform effective interaction among the global and local visual semantics, thus the semantic gaps can be narrowed. Finally, in the last stage, we utilize multiple transformer blocks with MHSA to enhance the global context learning, which can be formulated by: Attention(Q, K, ) = Softmax( QK Ci )V, (5) where Q, K, RNiCi. Ni = Hi Wi represents the number of tokens and Ci is the feature dimension. The MHSA enhances vanilla self-attention by projecting the input vector into multiple heads to perform self-attention independently to capture various aspects of the input data. The MHSA can be formulated as follows: MHSA(X) = Concat[head1, head2, ..., headh]W O, headj = Attention(Qj, Kj, Vj), (6) where RNiCi is the input feature, is the number of heads, RCiCi denotes weights metrics of the linear projection layer. Qj, Kj, Vj represent the query, key, and value for the j-th head, respectively. 5 Similarly, for the global representation, we fuse features in the last two stages to establish global representation. The output FG can be given by: FG = UpConv(Cat( ˆF3, UpConv( ˆF4))). (9) Based on such fusion strategy, the intermediate feature ˆF3 generated by the CIBs can be complementarily integrated into the local and global representations, respectively. D. Uncertainty-Aggregated Decoder To further enhance the segmentation accuracy in the ambiguous regions, we propose an Uncertainty-Aggregated Decoder (UAD) to identify the challenging pixels. Take the local representation branch as an example, we treat the category xp of each pixel as random variable to predict the pixel-wise uncertainty. Formally, suppose the size of FL is w, we pass them through two embedding functions Φµ() and Φσ() to generate the mean map µ R1hw and the variance map σ R1hw. Afterwards, we randomly draw sample ϵ from the Gaussian distribution ϵ (0, I), and generate the sample by computing µ + ϵσ. By doing this, we can obtain sampled feature maps x(1), x(2), ..., x(T ) via the learned distribution. We measure the uncertainty map by computing the normalized variance, which can be formulated as: µ = Φµ(FL) , σ = Φσ(FL) xp (µp, σ2 p), (10) where FL denotes the local feature, (µp, σ2 p) denotes the corresponding Gaussian distribution for pixel p. Note that the variance map σ also measures the pixel-wise uncertainty. If the response of arbitrary pixel in σ is high, it means the model is unconfident about the prediction output at this position. Afterwards, we randomly draw samples from the probabilistic representation to generate series of uncertaintyaware segmentation maps. We consider the turbulence of the segmentation outputs in Eq. 10 and calculate their variance as the uncertainty map U, which can be formulated as: = Norm(Var(x(1), x(2), ..., x(T ))), (11) where x(1), x(2), ..., x(T ) means the samples, Var() denotes the variance function and Norm() denotes the min-max normalization. Although intuitively feasible for uncertainty modeling, the sampling operation in Eq. 11 requires non-differentiable sampling operation to estimate the uncertainty map, making it challenging to train the UAD using forward-backward propagation. To address this issue, we adopt the reparameterization the direct sampling operation to be trick [57] to convert trainable components. Specifically, instead of directly drawing samples from the probabilistic representation, we introduce the turbulence ϵ R1hw following the standard Gaussian distribution (0, I) and generate = σϵ+µ. By doing this, the non-differentiable sampling process can be avoided and the gradient information can be effectively backward propagated for uncertainty learning. Through the aforementioned approach, we can obtain the local uncertainty UL for FL and the global uncertainty UG Fig. 4. (a) Details of the multi-kernel feature modulator in our cooperative encoder (CE). (b) Illustration of the CIB, which stacks hybrid CNN and transformer layers for global-local information interaction. C. Global-Local Fusion After obtaining the cooperative representations of the RS image, we propose global-local fusion (GLF) module to complementarily fuse the hierarchical visual semantics. The hierarchical feature maps = {F1, F2, F3, F4} are firstly passed into depth convolutional block with residual connection to establish informative propagation pathways, which can be formulated as: ˆFi = Fi + DWConv(Fi), (7) where DWConv() denotes the depth convolutional block consists of two depth-wise separable convolution layers and point-wise convolution layer. Afterwards, we conduct feature fusion on the feature maps ˆF = { ˆF1, ˆF2, ˆF3, ˆF4}. Note that the cooperative interaction block (CIB) using hybrid CNN-transformer structure is only available in the third stage, which excavates both local and global visual semantics by cooperatively merging the multiscale local features using MHSA. Therefore, we selectively fuse {F1, F2, F3} and {F3, F4} to construct local and global representations. For the local representation, we utilize the CNN and cooperative features, i.e. {F1, F2, F3} to enhance the local information as follows: FL = Conv(Cat( ˆF1, UpConv(Cat( ˆF2, UpConv( ˆF3))))), (8) where UpConv() denotes series of convolutions with spatial up-sampling, Cat() denotes the concatenation operations. TABLE DETAILED ARCHITECTURE SPECIFICATIONS AT FOUR STAGES OF THE COOPERATIVE ENCODER (CE). Downsp rate (Output size) Layers Conv 33, stride = 2, dim = stage 1 4 (128 128) stage 2 8 (64 64) stage 3 16 (32 32) stage 4 32 (16 16) Conv 22, stride = 2, dim = 64 (cid:34) MKFM, = 9, = 4 (cid:35) FFN, mlp ratio = 4 Conv 33, stride = 2, dim = 128 (cid:34) MKFM, = 9, = (cid:35) 2 2 FFN, mlp ratio = 4 Conv 33, stride = 2, dim = 256 MKFM, = 9, = FFN, mlp ratio = 4 MHSA, num head = 8 FFN, mlp ratio = 4 Conv 33, stride = 2, dim = 512 (cid:34) MHSA, num head = 16 (cid:35) 1 FFN, mlp ratio = for FG. To mitigate the impact of low-confidence regions, the uncertainty maps in both local and global branches can be regarded as attenuation weights, which aggregate FL and FG for feature integration: Fout = (1 UG) FG + (1 UL) FL, (12) where Fout is the aggregated feature. With such integration, the low-confidence uncertain region of the global-local features can be effectively suppressed, which boosts the segmentation outputs in complex scenarios. E. Loss Function"
        },
        {
            "title": "We present",
            "content": "the detailed online tracking framework of UAGLNet in Algorithm 1. To train the UAGLNet, we employ joint loss function to minimize the difference between the ground truth mask and the predicted segmentation output, which can be formulated as: Lseg(S) = Ldice(S, Y)+Lbce(S, Y)+γLbce(S, Y), (13) where and denote the predicted output and ground truth, Ldice is the dice loss and Lbce is the binary cross-entropy loss. and represent the building boundaries extracted by the Laplacian convolution [58] with kernel of (cid:105) . (cid:104) 1 1 1 1 8 1 1 1 1 Besides, we also introduce an uncertainty loss Lunc to supervise the segmentation uncertainty, which can be given by: 6 uncertainty in both global and local branches for joint optimization. The complete loss function Ltotal can be formulated by: Ltotal = Lseg(S) + λ1Lunc(UG) + λ2Lunc(UL), (15) where λ1 and λ2 are the combination weights used to balance the influence of objectives. Algorithm 1 Pseudocode of the Proposed UAGLNet Input: Input image I; Ground truth mask Y; Output: Segmentation output S; Local uncertainty UL; Global uncertainty UG; 1: for Epochs do 2: 3: for Batches do CIB F2, GLF F3, F4. MKFM F1, F3 MKFM I, F2 MHSA F3. F1 F4 //* Global-Local Fusion via GLF *// GLF F1, F2, F3, and FG FL Predict the pixel-wise uncertainty µ, σ by Eq. 10. Randomly draw samples x(1), x(2), ..., x(T ) from (µ, σ2). Calculate local and global uncertainty maps UL and UG via Eq. 11. Fout (1 UG) FG + (1 UL) FL. Squeeze the incorporated feature Fout to obtain binary prediction. Lseg(S) use and to compute segmentation loss. Lunc(UL), Lunc(UG) Compute local and global uncertainty losses using Eq. 14. Ltotal Lseg(S) + λ1Lunc(UG) + λ2Lunc(UL). Minimize Ltotal and update the model parameters accordingly. 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for 15: 16: end for IV. EXPERIMENTS To evaluate the performance of the proposed UAGLNet, we conduct experiments on three publicly available building extraction datasets, i.e. Inria Aerial dataset [59], Massachusetts dataset [60] and WHU building dataset [8]. We compare UAGLNet with multiple state-of-the-art CNN-based and Transformer-based methods, including UNet [35], SIUNet [8], HRNet [21], DSNet [61], SwinUNet [62], TransFuse [63], STT [23], DMBCNet [64], BOMSNet [65], LCS [66], CBRNet [1], BuildFormer [15], DSATNet [16], SDSCUNet [12], UANet [67], MAFCN [10], BRRNet [68], MSNet [69], BCTNet [17], FDNet [70], MAPNet [71] and LFEMAPNet [9]. Lunc(U) = Lbce(x, Y) + ηD(N (µ, σ2) (0, I)), (14) where is sample randomly drawn from the probabilistic distribution, is the ground truth mask, is the KullbackLeibler (KL) divergence loss, η is hyperparameters used to balance the weights of the loss terms. Here we consider the A. Implementation and Evaluation Details Implementation Setting. We run the proposed UAGLNet using PyTorch framework on single NVIDIA RTX A6000 GPU with 48GB memory. The detailed architecture of the cooperative encoder is presented in Tab. I. The remote sensing images in all datasets are cropped to 512 512 for training Fig. 5. Visual comparison with the representative state-of-the-art methods on Massachusetts building dataset. (a) Image; (b) Ground Truth; (c) UAGLNet; (d) UNet; (e) HRNet; (f) BuildFormer; (g) DSATNet. and testing phase. The data augmentation operations including random crop, random horizontal flipping, photometric distortion and mixup are used to improve the models robustness. During training, the AdamW optimizer is employed to train the models. The learning rate is set to 5 104 with the weight decay 0.01. To mitigate the overfitting problem. we utilize residual connections in CIB and MKFM modules to facilitate the gradient propagation, enabling the network to learn robust features. We also employ Cosine Annealing Warm Restarts as our learning rate scheduler to prevent the model from getting stuck in local minima. The hyper-parameter λ1 and λ2 are both 0.5 and the scaling parameter η is set to 0.2. The model is trained with 105 epochs. The batch size is set to 16 for Inria dataset and 8 for Massachusetts and WHU datasets. On the Inria Aerial dataset and the WHU dataset, the random drop path technique rate is set to 0.2, while on the Massachusetts Dataset with only 151 aerial images, it is increased to 0.4. Evaluation Metrics. To evaluate the performance of different methods, we adopt the commonly used evaluation metrics in remote sensing datasets for fair comparison. The Precision (P), Recall (R), F1 score and Intersection over Union (IoU) are employed to evaluate the performance. These metrics are calculated by: TABLE II COMPARISON OF THE STATE-OF-THE-ART METHODS AND OURS ON MASSACHUSETTS BUILDING DATASET Method UNet [35] HRNet [21] MAFCN [10] BRRNet [68] DSNet [61] STT [23] BOMSNet [65] MSNet [69] CBRNet [1] BuildFormer [15] DSATNet [16] SDSCUNet [12] BCTNet [17] FDNet [70] UANet [67] UAGLNet (ours) Year 2015 2019 2020 2020 2020 2021 2022 2022 2022 2022 2023 2023 2023 2023 2024 IoU(%) 72.59 72.89 73.80 73.25 75.04 74.51 74.71 70.21 74.55 75.74 76.54 76.71 75.04 74.54 76.41 76. F1(%) 84.12 84.32 84.93 84.56 85.74 85.40 85.13 79.33 85.42 86.19 86.71 86.82 85.74 85.42 86.63 86.99 P(%) 84.46 85.83 87.07 - 87.56 86.55 86.64 78.54 86.50 87.52 87.94 88.05 87.95 87.95 87.94 88.28 R(%) 83.77 82.86 82.89 - 83.99 84.27 83.68 80.14 84.36 84.90 85.52 85.62 83.02 83.02 85.35 85.73 where TP, FP, TN and FN represent the pixel-wise predictions of true positive, false positive, true negative, and false negative, respectively. = = F1 = IoU = TP TP + FP TP TP + FN 2 + TP TP + FN + FP , B. Massachusetts Dataset Massachusetts Dataset. The Massachusetts dataset 1 contains 151 aerial images of the Boston area with the size of 1500 1500 pixels and spatial resolution of 1 m/pixel. The dataset is divided into training set, validation set, and test set with 137, 4, and 10 images, respectively. Each image with (16) 1https://www.cs.toronto.edu/vmnih/data/ 8 Fig. 6. Visual comparison with the representative state-of-the-art methods on Inria Aerial Image Labeling dataset. (a) Image; (b) Ground Truth; (c) UAGLNet; (d) UNet; (e) HRNet; (f) BuildFormer; (g) DSATNet. the size of 1500 1500 is annotated by pixel-wise labels. The spatial resolution of the whole dataset is 1.2m per-pixel, covering around 340 km2. Evaluation Results. Table II shows the quantitative comparison of various methods on the Massachusetts building dataset. UAGLNet achieves the best performance with the precision 88.28%, the recall 85.73% and the IoU 76.97%. Compared to BuildFormer, our UAGLNet obtains performance gains of 1.23% in IoU and 0.76% in precision. For the most recent state-of-the-art method UANet [67], UAGLNet surpasses it by 0.56% in the IoU metric, 0.36% in the F1 metric, 0.34% in the precision metric, and 0.38% in the recall metric, respectively. The superior performance on this challenging dataset confirms the effectiveness of the proposed method. We further demonstrate the segmentation results in Fig. 5, note that the Massachusetts dataset contains large quantities of small buildings with low resolutions, The global-local fusion (GLF) and uncertainty-aggregated decoder (UAD) in UAGLNet can capture high-quality local details and mitigate the ambiguity in the neighboring regions, thus the extraction results are better than HRNet, BuildFormer and DSATNet. C. Inria Aerial Dataset Inria Aerial Dataset. Inria aerial dataset 2 is widely used urban buildings extraction dataset for semantic segmentation. There are 360 images in this dataset with the size of 5000 5000 and spatial resolution of 0.3m, wherein 180 are presented with the ground truth labels. These images are collected in five cities: Austin (U.S.), Chicago (U.S.), Kitsap (U.S.), Tyrol (Austria), and Vienna (Austria). Similar to [67], the original 2https://project.inria.fr/aerialimagelabeling/ images are padded into 5120 5120 pixels and then cropped into tiles of 512 512 pixels. As result, 9737 and 1942 image tiles are used for training and validation, respectively. Evaluation Results. Table III shows the quantitative comparison of various methods on the Inria aerial dataset. UAGLNet achieves the best performance with the precision and recall scores of (92.09%, 90.22%), the IoU and F1 scores of (83.74%, 91.15%). our UAGLNet outperforms BuildFormer by 2.30% in IoU, 1.38% in F1 score, 1.34% in the precision, and 1.41% in the recall metric. Compared to the most recent state-of-the-art method UANet [67], UAGLNet obtains the performance gains of 0.66% and 0.39% in the IoU and F1 metrics, respectively. Fig. 6 demonstrates the comparative visualization results on Inria aerial dataset. We can see that our UAGLNet also outperforms other building extraction methods in recognizing the hard building pixels and maintaining the integrity of buildings, which benefits from the cooperative global-local contexts integration and segmentation uncertainty mitigation. Moreover, we also observe that the proposed UAGLNet achieves superior performance using the fewest parameters and floating-point operations among all methods. UGALNet only requires 28.90G flops and 15.34M parameters, which significantly reduce the computation and memory costs. Compared with BuildFormer, it saves 75.32% computational complexity and 62.14% parameters. For the efficient ViT-based Ushaped model SDSC-UNet, UGALNet also reduces 28.05% parameters. This is because our UGALNet utilizes depth-wise separable convolutions and point-wise convolution in MKFMs and CIBs at the first three stages, thus the computation and memory costs are greatly decreased, allowing the model to capture the local and global information with efficiency and 9 TABLE III COMPARISON OF THE STATE-OF-THE-ART METHODS AND OURS ON INRIA AERIAL IMAGE LABELING DATASET Method UNet [35] SIUNet [8] HRNet [21] DSNet [61] SwinUNet [62] TransFuse [63] STT [23] DMBCNet [64] BOMSNet [65] LCS [66] CBRNet [1] BuildFormer [15] DSATNet [16] SDSCUNet [12] UANet [67] UAGLNet (ours) Year 2015 2019 2019 2020 2021 2021 2021 2021 2022 2022 2022 2022 2023 2023 2024 IoU(%) 75.66 71.40 77.14 81.02 76.27 80.71 79.42 80.74 78.18 78.82 81.10 81.44 82.68 83.01 83.08 83.74 F1(%) 86.14 83.33 87.10 89.52 86.54 90.29 87.99 89.35 87.75 88.15 89.56 89.77 90.52 90.71 90.76 91.15 P(%) 85.88 84.60 89.04 90.32 86.87 88.38 - 89.94 87.93 89.58 89.93 90.75 91.62 91.52 92.04 92.09 R(%) 86.40 82.10 85.24 88.73 86.21 89.32 - 88.77 87.58 86.77 89.20 88.81 89.44 89.92 89.52 90.22 FLOPs 202.95G 215.85G 90.39G - 97.69G 50.53G 106.21G 60.01G - 135.98G 186.56G 117.12G 57.75G 29.82G 111.11G 28.90G Params 32.09M 31.18M 67.17M - 84.02M 31.87M 18.87M 30.01M 129.32M 20.16M 22.69M 40.52M 48.50M 21.32M 15.59M 15.34M Fig. 7. Visual comparison with the representative state-of-the-art methods on WHU building dataset. (a) Image; (b) Ground Truth; (c) UAGLNet; (d) UNet; (e) HRNet; (f) BuildFormer; (g) DSATNet. effectiveness. D. WHU Dataset WHU Dataset. The WHU building dataset 3 contains 8189 image tiles with spatial resolution of 0.3 m/pixel, and the size of each image is 512 512. There are 4736 tiles selected for training, 1036 tiles for validation, and 2416 tiles for testing. The whole dataset covers huge area of over 450 km2, including about 187000 buildings with different scales and shapes. 3https://gpcv.whu.edu.cn/data/ Evaluation Results. Table IV shows the quantitative comparison of various methods on the WHU building dataset. UAGLNet obtains the IoU score of 92.07% and the F1 score of 95.87%, which outperforms recent state-of-the-art method LFEMAPNet [9] by 0.59% and 0.56% in the IoU and precision metric, respectively. We also present the comparative visualization results in Fig. 7. The UGALNet is more powerful in extracting buildings with occluded trees, and it can identify the challenging cases that closely resemble their surroundings. E. Ablation Study Ablation Study of Each Component. To verify the effectiveness of each component in our UAGLNet, we selectively TABLE IV COMPARISON OF THE STATE-OF-THE-ART METHODS AND OURS ON WHU BUILDING DATASET. Method UNet [35] SIUNet [8] HRNet [21] MAFCN [10] DSNet [61] STT [23] MAPNet [71] DMBC-Net [64] BOMSNet [65] LCS [66] MSNet [69] BuildFormer [15] BCT-Net [17] FDNet [70] LFEMAPNet [9] UAGLNet (ours) Year 2015 2019 2019 2020 2020 2021 2021 2021 2022 2022 2022 2022 2023 2023 2024 IoU(%) 88.08 88.40 88.21 90.70 89.54 90.48 90.86 91.66 90.15 90.71 89.07 91.44 91.15 91.14 91.48 92. F1(%) 93.66 93.85 93.73 95.04 94.48 94.97 95.21 95.65 94.80 95.12 93.96 95.53 95.37 95.36 95.55 95.87 P(%) 93.17 93.80 93.63 95.20 94.05 - 95.62 96.15 95.14 95.38 94.83 95.65 95.47 95.27 95.65 96.21 R(%) 94.16 93.90 93.84 95.10 94.91 - 94.81 95.16 94.50 94.86 93.12 95.40 95.27 95.46 95.45 95.54 TABLE ABLATION EXPERIMENTAL RESULTS OF VARIANTS WITH DIFFERENT COMPONENTS ON INRIA AERIAL IMAGE LABELING DATASET CE (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) GLF UL UG IoU(%) F1(%) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) 82.46 83. 83.58 83.62 83.74 90.39 90.90 91. 91.08 91.15 choose the cooperative encoder (CE), the global-local fusion module (GLF) and the uncertainty-aggregated decoder (UAD) for comparison. Here we classify our UAD into the local uncertainty UL and the global uncertainty UG to strengthen our experiments. The ablation results are shown in Table V. The variant approach only with CE achieves 82.46% and 90.39% in the IoU and F1 metrics. By introducing GLF to generate refined global and local features, the variant approach obtains the performance improvement of 0.87% and 0.51%, respectively. If the local uncertainty UL or the global uncertainty UG is further introduced, the variants can gain the IoU scores of 0.25% or 0.29%, respectively. If the complete UAD is introduced, the variant can achieve obvious improvement of 0.41%. The ablation results demonstrate the effectiveness of each module in our UAGLNet. TABLE VI COMPARISON OF HYBRID CNN-TRANSFORMER ARCHITECTURES ON INRIA AERIAL IMAGE LABELING DATASET Architecture IoU(%) F1(%) Parallel [15] Sequential [18] Alternative [16] Ours 81. 82.84 82.15 83.74 89.77 90.62 90. 91.15 P(%) 90.75 91.60 91.50 92. R(%) 88.81 89.66 88.94 90.22 Ablation Study of Encoder Network. We conduct an ablation study to verify the effectiveness of the proposed cooperative encoder. Table VI shows the quantitative comparison of differ10 ent hybrid architectures on Inria aerial dataset. We refer to the encoder networks in Buildformer [15], TransUNet [18] and DSATNet [16] as parallel, sequential and alternative variants of the hybrid CNN and transformer blocks, respectively. Compared to the three existing hybrid architectures for building extraction, our cooperative encoder achieves the improvements of 2.30%, 0.90% and 1.59% in the IoU metric. The impressive performance gains indicate that our cooperative encoder can effectively narrow the inherent gap of the feature pyramids. TABLE VII ABLATION EXPERIMENTAL RESULTS OF OUR DECODER ON INRIA AERIAL IMAGE LABELING DATASET Decoder FPN [72] UperHead [73] ASPPHead [74] Ours IoU(%) F1(%) 82.78 82. 83.26 83.74 90.58 90.61 90.86 91. P(%) 91.32 91.30 91.67 92.09 R(%) 89.85 89.92 90.07 90.22 Ablation Study of Decoder Network. Table VII shows the quantitative comparison of different decoders on the Inria aerial dataset. We take FPN [72], UperHead [73] and ASPPHead [74] as comparative variants to validate the effectiveness of our decoder. The variant method using FPN obtains the IoU and F1 scores of 82.78% and 90.58%. The ASPPHead obtains higher performance due to the larger reception field perceived by the atrous spatial pyramid pooling operation. Our UAGLNet using UAD achieves more competitive performance with the corresponding scores of 83.74% and 91.15%, which obtains improvement of 0.96% and 0.57% compared to FPN. TABLE VIII ABLATION STUDY OF ENCODER GLOBAL-LOCAL BALANCE AND DECODER GLOBAL-LOCAL FUSION Global-Local Interaction (cid:37) Global-Local Fusion (cid:37) CIB (cid:37) CIB (cid:37) GLF GLF IoU(%) F1(%) P(%) R(%) 82.02 82. 83.08 83.33 90.22 90.37 90.76 90. 91.42 91.18 91.59 91.85 88.86 89. 89.94 89.97 Ablation Study of CIB and GLF. In UAGLNet, we introduce the Cooperative Interaction Block (CIB) to balance the global and local representations. To verify the effectiveness of the global-local balance in the encoder, we conduct an ablation study by selectively incorporating CIB and GLF for comparison. As shown in Table VIII, the results demonstrate that CIB and GLF can effectively improve the performance in all of the metrics. Hyperparameter Setting in MKFM. In MKFM, we utilize multi-kernel structure to enhance the models perceptional capacity while maintaining controllable computational budget. Its important to note that must be divisible by the feature dimension (64), and is defined as = 2n+1. To evaluate the effectiveness of these hyperparameter settings, we conducted an ablation study, and the results are presented in Table IX. The results show that setting to 4 and to 9 yields the best performance for our model. 11 TABLE IX ABLATION STUDY OF HYPERPARAMETERS SETTING IN MKFM ON INRIA AERIAL IMAGE LABELING DATASET TABLE XII COMPARATIVE ANALYSES OF HIGH-RESOLUTION PREDICTION RESULTS ON MASSACHUSETTS BUILDING DATASET. Hyperparameters IoU(%) F1(%) = 3, = 1 = 5, = 2 = 9, = = 17, = 8 83.12 83.44 83.74 83.31 90. 90.97 91.15 90.89 P(%) 91.86 91. 92.09 91.86 R(%) 89.73 90.16 90. 89.95 TABLE XI COMPARISON OF STATE-OF-THE-ART METHODS AND OURS FOR REAL-TIME APPLICATIONS Methods IoU(%) F1(%) HRNet [21] SDSCUNet [12] UANet PVT [67] UAGLNet (ours) 77.14 83.01 83. 83.74 87.10 90.71 90.91 91.15 FPS 19.59 19.38 23.89 27.53 Params 67.17M 21.32M 25.64M 15.34M TABLE ABLATION EXPERIMENTAL RESULTS OF DIFFERENT GLOBAL-LOCAL FUSION STRATEGY ON THE INRIA AERIAL IMAGE LABELING DATASET FL {F1, F2} {F1, F2, F3} {F1, F2} {F1, F2, F3} FG {F4} {F4} {F3, F4} {F3, F4} IoU(%) F1(%) P(%) 83.13 83.36 83. 83.74 90.79 90.92 91.02 91.15 91. 91.52 91.85 92.09 R(%) 89.94 90. 90.21 90.22 Ablation Study of Feature Fusion Strategy. To verify the effectiveness of our global-local fusion strategy within GLF, we conduct an ablation study to test the variants with different fusion strategies. The ablation results are shown in Table X. If we select {F1, F2} to construct local representation FL and {F4} for global representation FG, the variant yields 83.13% in the IoU metric. By incorporating F3 into the generation of FL, the variant obtains the IoU performance gain of 0.23%. Since local context is enhanced by F3, this variant is capable to discover more foreground pixels but also exhibits more misclassifications, resulting in higher recall and lower precision. Meanwhile, by incorporating F3 into the global representation FG, the variant obtains the IoU score improvement of 0.39%. If F3 is utilized for both FL and FG, the variant can achieve the best performance with the IoU score of 83.74%. F. Discuss on the Computation Efficiency We discuss the computational efficiency of our UAGLNet by measuring the frames per second (FPS) and parameters (Params). We take the CNN-based model HRNet [21], the recent state-of-the-art transformer-based models such as SDSCUNet [12] and UANet PVT [67] for comparison, the results presented in Table XI. The experiment shows that UAGLNet achieves the highest IoU and F1 scores with 27.53 fps, which is 42.05% faster than SDSCUNet. Besides, it only requires 15.34M, which reduce 28.05% parameters compared to SDSCUNet. Method UANet [67] UAGLNet Year 2024 IoU(%) F1(%) 70. 72.32 82.87 83.94 P(%) 86.21 87. R(%) 79.78 81.01 FLOPs 1004.75G 419.58G G. Discuss on High-Resolution Images We test UAGLNet and UANet on the high-resolution Massachusetts Building dataset without image cropping, the size of each image is 1500 1500. The results are presented in Table XII, we can see that for the high-resolution input setting, UAGLNet obtains obvious performance gains, it only requires less than half computational flop compared to UANet. This is because UAGLNet integrates multi-kernel feature modulator(MKFM) with lightweight depth-wise separable convolution and transformer blocks, which significantly reduce the computational complexity. Discuss on UADs Robustness. To evaluate the effectiveness of UAGLNet in more challenging scenarios, we synthesize the low-resolution and noisy images using the Inria Aerial Image Labeling dataset. Specifically, the low resolution version degrades the resolution by factor of 16, the noisy version adds Gaussian noise to the images with the standard deviation 5. The comparative results are demonstrated in Table XIII. We can see that compared to the variant approach without Uncertainty-Aggregated Decoder (UAD), our UAGLNet obtains higher performance in all metrics, indicating that UAD is more robust if the dataset is contaminated by the lowresolution or noisy images. TABLE XIII ABLATION EXPERIMENTAL RESULTS OF OUR UAD ON THE SYNTHESIS LOW-RESOLUTION AND NOISE VARIANTS OF INRIA AERIAL IMAGE LABELING DATASET. Low Resolution IoU(%) F1(%) w/o UAD + UAD (ours) Add Noise w/o UAD + UAD (ours) 80. 81.19 89.09 89.62 IoU(%) F1(%) 82. 83.41 90.54 90.96 P(%) 89.74 90. P(%) 91.35 91.91 R(%) 88.45 88. R(%) 89.75 90.02 H. Feature Visualizations We add more examples to demonstrate the effectiveness of the uncertainty prediction in UAGLNet. As shown Fig. 8, the local feature FL within the red box region contains more details, while the global feature FG is more discriminative. To obtain accurate prediction, the local uncertainty UL pays more attention to filter out the irrelevant details and the global uncertainty UG focuses on maintaining reliable boundaries, leading the fused output Fout to be more stable against the uncertain noises. I. Generalization Analyse of UAGLNet We add cross-dataset experiment to assess UAGLNets generalization capability. Here we train all of the methods 12 Fig. 8. The visualized features in the Uncertainty-Aggregated Decoder (UAD). The irrelevant noises in FL are suppressed and the low-confidence uncertain regions in FG are aggregated within the red rectangles. TABLE XIV COMPARISON OF THE STATE-OF-THE-ART METHODS AND OURS IN CROSS-DOMAIN TESTING ON THE WHU BUILDING DATASET . Methods Trained on WHU F1(%) IoU(%) UNet [35] 88.08 93.66 HRNet [21] 88.21 93. BuildFormer [15] 91.44 95.53 UAGLNet (ours) 92.07 95. Trained on Inria F1(%) 80.28 (13.38) 84.08 (9.65) 86.07 (9.46) 91.42 (4.45) IoU(%) 67.06 (21.02) 72.52 (15.69) 75.56 (15.88) 84.20 (7.87) on Inria dataset and test them on WHU dataset for fair comparison. As shown in Table XIV, the performance of the CNN based method like HRNet drops 15.69% and 9.65% in terms of IoU and F1, respectively. The transformer based approach BuildFormer drops 15.88% and 9.46%. Compared to recent state-of-the-art methods, UAGLNet exhibits superior generalization capability with 7.87% and 4.45% performance degradation. J. Future Work We have noticed that the proposed UAGLNet can be future applied to other remote sensing task satellite imagery. For example, it can be rapidly deployed to other satellite imagery based semantic segmentation datasets by simply replacing the decoder head for task-specific model training. While UAGLNet can already run at real-time, the running speed can be further improved by introducing model lightweight tricks such as distillation, pruning. Besides, we would explore to integrate UAGLNet and super-resolution module into unified framework for the challenging low-resolution images. V. CONCLUSIONS In this paper, we propose an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet) to exploit highquality global-local visual semantics under the guidance of uncertainty modeling. We first design cooperative encoder, which adopts convolutional and non-local self-attention operators at different stages to capture the local and global visual semantics. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global semantics when the network becomes deeper. Afterwards, we propose Global-Local Fusion (GLF) module to complementarily fuse the hierarchical visual representations. Moreover, we propose an Uncertainty-Aggregated Decoder (UAD) to mitigate the segmentation uncertainty in ambiguous regions. Extensive experiments demonstrate that our method achieves superior performance on the public benchmarks."
        },
        {
            "title": "REFERENCES",
            "content": "[1] H. Guo, B. Du, L. Zhang, and X. Su, coarse-to-fine boundary refinement network for building footprint extraction from remote sensing imagery, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 183, pp. 240252, 2022. [2] L. Dong and J. Shan, comprehensive review of earthquake-induced building damage detection with remote sensing techniques, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 84, pp. 8599, 2013. [3] M. Belgiu and L. Drˇagut, Comparing supervised and unsupervised multiresolution segmentation approaches for extracting buildings from very high resolution imagery, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 96, pp. 6775, 2014. [4] J. Kang, M. Korner, Y. Wang, H. Taubenbock, and X. X. Zhu, Building instance classification using street view images, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 145, pp. 4459, 2018. [5] Y. Zhang, Optimisation of building detection in satellite images by combining multispectral classification and texture filtering, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 54, no. 1, pp. 5060, 1999. [6] Z. Li, W. Shi, Q. Wang, and Z. Miao, Extracting man-made objects from high spatial resolution remote sensing images via fast level set evolutions, IEEE Transactions on Geoscience and Remote Sensing, vol. 53, no. 2, pp. 883899, 2014. [7] E. Li, J. Femiani, S. Xu, X. Zhang, and P. Wonka, Robust rooftop extraction from visible band images using higher order CRF, IEEE Transactions on Geoscience and Remote Sensing, vol. 53, no. 8, pp. 44834495, 2015. [8] S. Ji, S. Wei, and M. Lu, Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set, IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no. 1, pp. 574586, 2019. [9] Y. Liu, E. Li, W. Liu, X. Li, and Y. Zhu, Lfemap-net: Low-level feature enhancement and multiscale attention pyramid aggregation network for building extraction from high-resolution remote sensing images, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 17, pp. 27182730, 2024. [10] S. Wei, S. Ji, and M. Lu, Toward automatic building footprint deimages using CNN and regularization, IEEE lineation from aerial Transactions on Geoscience and Remote Sensing, vol. 58, no. 3, pp. 21782189, 2020. [11] Z. Xu, W. Zhang, T. Zhang, Z. Yang, and J. Li, Efficient transformer for remote sensing image segmentation, Remote Sensing, vol. 13, no. 18, p. 3585, 2021. [12] R. Zhang, Q. Zhang, and G. Zhang, Sdsc-unet: Dual skip connection vit-based u-shaped model for building extraction, IEEE Geoscience and Remote Sensing Letters, vol. 20, pp. 15, 2023. [13] L. Ding, D. Lin, S. Lin, J. Zhang, X. Cui, Y. Wang, H. Tang, and L. Bruzzone, Looking outside the window: Wide-context transformer for the semantic segmentation of high-resolution remote sensing images, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 113, 2022. [14] M. Jiang, Y. Su, L. Gao, A. Plaza, X. Zhao, X. Sun, and G. Liu, Graphgst: Graph generative structure-aware transformer for hyperspectral image classification, IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 116, 2024. [15] L. Wang, S. Fang, X. Meng, and R. Li, Building extraction with vision transformer, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 111, 2022. [16] R. Zhang, Z. Wan, Q. Zhang, and G. Zhang, Dsat-net: Dual spatial attention transformer for building extraction from aerial images, IEEE Geoscience and Remote Sensing Letters, vol. 20, pp. 15, 2023. [17] L. Xu, Y. Li, J. Xu, Y. Zhang, and L. Guo, Bctnet: Bi-branch crossfusion transformer for building footprint extraction, IEEE Transactions on Geoscience and Remote Sensing, vol. 61, pp. 114, 2023. [18] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, and Y. Zhou, Transunet: Transformers make strong encoders for medical image segmentation, CoRR, vol. abs/2102.04306, 2021. [19] P. Liu, X. Liu, M. Liu, Q. Shi, J. Yang, X. Xu, and Y. Zhang, Building footprint extraction from high-resolution images via spatial residual inception convolutional neural network, Remote Sensing, vol. 11, no. 7, p. 830, 2019. [20] H. Guo, Q. Shi, B. Du, L. Zhang, D. Wang, and H. Ding, Scene-driven multitask parallel attention network for building extraction in highresolution remote sensing images, IEEE Transactions on Geoscience and Remote Sensing, vol. 59, no. 5, pp. 42874306, 2021. [21] K. Sun, B. Xiao, D. Liu, and J. Wang, Deep high-resolution representation learning for human pose estimation, in IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 56935703. [22] W. Yuan and W. Xu, Msst-net: multi-scale adaptive network for building extraction from remote sensing images based on swin transformer, Remote Sensing, vol. 13, no. 23, p. 4743, 2021. [23] K. Chen, Z. Zou, and Z. Shi, Building extraction from remote sensing images with sparse token transformers, Remote Sensing, vol. 13, no. 21, p. 4441, 2021. [24] D. Zhou, Z. Yu, E. Xie, C. Xiao, A. Anandkumar, J. Feng, and J. M. Alvarez, Understanding the robustness in vision transformers, in International Conference on Machine Learning, vol. 162, 2022, pp. 27 37827 394. [25] C. Li, R. Cong, J. Hou, S. Zhang, Y. Qian, and S. Kwong, Nested network with two-stream pyramid for salient object detection in optical remote sensing images, IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no. 11, pp. 91569166, 2019. [26] S. Yao, H. Zhang, W. Ren, C. Ma, X. Han, and X. Cao, Robust online tracking via contrastive spatio-temporal aware network, IEEE Transactions on Image Processing, vol. 30, pp. 19892002, 2021. [27] A. Srinivas, T. Lin, N. Parmar, J. Shlens, P. Abbeel, and A. Vaswani, Bottleneck transformers for visual recognition, in IEEE Conference on Computer Vision and Pattern Recognition, 2021, pp. 16 51916 529. [28] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang, Cvt: Introducing convolutions to vision transformers, in IEEE International Conference on Computer Vision, 2021, pp. 2231. [29] J. Guo, K. Han, H. Wu, Y. Tang, X. Chen, Y. Wang, and C. Xu, CMT: convolutional neural networks meet vision transformers, in IEEE Conference on Computer Vision and Pattern Recognition, 2022, pp. 12 16512 175. [30] L. Wang, R. Li, C. Duan, C. Zhang, X. Meng, and S. Fang, novel transformer based semantic segmentation scheme for fine-resolution remote sensing images, IEEE Geoscience And Remote Sensing Letter, vol. 19, pp. 15, 2022. [31] S. Yao, R. Zhu, Z. Wang, W. Ren, Y. Yan, and X. Cao, Umdatrack: Unified multi-domain adaptive tracking under adverse weather conditions, in IEEE International Conference on Computer Vision, 2025. [32] X. He, Y. Zhou, J. Zhao, D. Zhang, R. Yao, and Y. Xue, Swin transformer embedding unet for remote sensing image semantic segmentation, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 115, 2022. [33] L. Wang, R. Li, C. Zhang, S. Fang, C. Duan, X. Meng, and P. M. Atkinson, Unetformer: unet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 190, pp. 196214, 2022. [34] Q. Li, R. Zhong, X. Du, and Y. Du, Transunetcd: hybrid transformer network for change detection in optical remote-sensing images, IEEE 13 Transactions on Geoscience and Remote Sensing, vol. 60, pp. 119, 2022. [35] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional networks for biomedical image segmentation, in Medical Image Computing and Computer-Assisted Intervention, vol. 9351, 2015, pp. 234241. [36] W. Lin, Z. Wu, J. Chen, J. Huang, and L. Jin, Scale-aware modulation transformer, in IEEE International Conference on Computer meet Vision, 2023, pp. 59926003. [37] Q. Zhang, R. Cong, C. Li, M. Cheng, Y. Fang, X. Cao, Y. Zhao, and S. Kwong, Dense attention fluid network for salient object detection in optical remote sensing images, IEEE Transactions on Image Processing, vol. 30, pp. 13051317, 2021. [38] X. Zhang, S. Cheng, L. Wang, and H. Li, Asymmetric cross-attention hierarchical network based on CNN and transformer for bitemporal remote sensing images change detection, IEEE Transactions on Geoscience and Remote Sensing, vol. 61, pp. 115, 2023. [39] H. Liu, M. Yao, X. Xiao, and Y. Xiong, Rockformer: u-shaped transformer network for martian rock segmentation, IEEE Transactions on Geoscience and Remote Sensing, vol. 61, pp. 116, 2023. [40] H. Liu, M. Yao, X. Xiao, B. Zheng, and H. Cui, Marsscapes and udaformer: panorama dataset and transformer-based unsupervised domain adaptation framework for martian terrain segmentation, IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 117, 2024. [41] S. Yao, H. Sun, T. Xiang, X. Wang, and X. Cao, Hierarchical graph interaction transformer with dynamic token clustering for camouflaged object detection, IEEE Transactions on Image Processing, vol. 33, pp. 59365948, 2024. [42] Y. Ni, J. Liu, W. Chi, X. Wang, and D. Li, Cgglnet: Semantic segmentation network for remote sensing images based on category-guided global-local feature interaction, IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 117, 2024. [43] Y. Tian, H. Cao, Y. Liu, C. Tian, and R. Wang, Wb-former: hybrid model of cnn & transformer for water body extraction in complex scenes, IEEE Transactions on Geoscience and Remote Sensing, vol. 63, pp. 118, 2025. [44] X. Xiang, W. Gong, S. Li, J. Chen, and T. Ren, Tcnet: Multiscale fusion of transformer and cnn for semantic segmentation of remote sensing images, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 17, pp. 31233136, 2024. [45] A. Kendall and Y. Gal, What uncertainties do we need in bayesian deep learning for computer vision? in Advances in Neural Information Processing Systems, 2017, pp. 55745584. [46] B. Lakshminarayanan, A. Pritzel, and C. Blundell, Simple and scalable predictive uncertainty estimation using deep ensembles, in Advances in Neural Information Processing Systems, 2017, pp. 64026413. [47] J. Choi, D. Chun, H. Kim, and H. Lee, Gaussian yolov3: An accurate and fast object detector using localization uncertainty for autonomous driving, in IEEE International Conference on Computer Vision, 2019, pp. 502511. [48] Z. Wang, Y. Li, Y. Guo, L. Fang, and S. Wang, Data-uncertainty guided multi-phase learning for semi-supervised object detection, in IEEE Conference on Computer Vision and Pattern Recognition, 2021, pp. 45684577. [49] Y. Wang, W. Zhang, L. Wang, T. Liu, and H. Lu, Multi-source uncertainty mining for deep unsupervised saliency detection, in IEEE Conference on Computer Vision and Pattern Recognition, 2022, pp. 11 71711 726. [50] S. Yao, Y. Guo, Y. Yan, W. Ren, and X. Cao, Unctrack: Reliable visual object tracking with uncertainty-aware prototype memory network, IEEE Transactions on Image Processing, vol. 34, pp. 35333546, 2025. [51] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, Weight uncertainty in neural network, in International Conference on Machine Learning, vol. 37, 2015, pp. 16131622. [52] A. Kendall, V. Badrinarayanan, and R. Cipolla, Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding, in British Machine Vision Conference, 2017. [53] P. Huang, W. T. Hsu, C. Chiu, T. Wu, and M. Sun, Efficient uncertainty estimation for semantic segmentation in videos, in European Conference on Computer Vision, 2018, pp. 536552. [54] J. Postels, F. Ferroni, H. Coskun, N. Navab, and F. Tombari, Samplingfree epistemic uncertainty estimation using approximated variance propagation, in IEEE International Conference on Computer Vision, 2019, pp. 29312940. [55] F. Yang, Q. Zhai, X. Li, R. Huang, A. Luo, H. Cheng, and D. Fan, Uncertainty-guided transformer reasoning for camouflaged object detection, in IEEE International Conference on Computer Vision, 2021, pp. 41264135. [56] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, in Advances in Neural Information Processing Systems, 2017, pp. 59986008. [57] D. P. Kingma and M. Welling, Auto-encoding variational bayes, in International Conference on Learning Representations, 2014. [58] M. Fan, S. Lai, J. Huang, X. Wei, Z. Chai, J. Luo, and X. Wei, Rethinking bisenet for real-time semantic segmentation, in IEEE Conference on Computer Vision and Pattern Recognition, 2021, pp. 97169725. [59] E. Maggiori, Y. Tarabalka, G. Charpiat, and P. Alliez, Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark, in IEEE International Geoscience and Remote Sensing Symposium, 2017, pp. 32263229. [60] V. Mnih, Machine learning for aerial image labeling, Ph.D. dissertation, University of Toronto, Canada, 2013. [61] H. Zhang, Y. Liao, H. Yang, G. Yang, and L. Zhang, local-global dual-stream network for building extraction from very-high-resolution remote sensing images, IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 3, pp. 12691283, 2022. [62] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang, Swin-unet: Unet-like pure transformer for medical image segmentation, in European Conference on Computer Vision, 2022, pp. 205218. [63] Y. Zhang, H. Liu, and Q. Hu, Transfuse: Fusing transformers and cnns for medical image segmentation, in Medical Image Computing and Computer-Assisted Intervention, vol. 12901, 2021, pp. 1424. [64] F. Shi and T. Zhang, multi-task network with distance-mask-boundary images, consistency constraints for building extraction from aerial Remote Sensing, vol. 13, no. 14, p. 2656, 2021. [65] Y. Zhou, Z. Chen, B. Wang, S. Li, H. Liu, D. Xu, and C. Ma, Bomscnet: Boundary optimization and multi-scale context awareness based building extraction from high-resolution remote sensing imagery, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 117, 2022. [66] Z. Liu, Q. Shi, and J. Ou, LCS: collaborative optimization framework of vector extraction and semantic segmentation for building extraction, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 1 15, 2022. [67] J. Li, W. He, W. Cao, L. Zhang, and H. Zhang, Uanet: An uncertaintyaware network for building extraction from remote sensing images, IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 1 13, 2024. [68] Z. Shao, P. Tang, Z. Wang, N. Saleem, S. Yam, and C. Sommai, Brrnet: fully convolutional neural network for automatic building extraction from high-resolution remote sensing images, Remote Sensing, vol. 12, no. 6, p. 1050, 2020. [69] Y. Liu, Z. Zhao, S. Zhang, and L. Huang, Multiregion scale-aware network for building extraction from high-resolution remote sensing images, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 110, 2022. [70] H. Guo, X. Su, C. Wu, B. Du, and L. Zhang, Decoupling semantic and edge representations for building footprint extraction from remote sensing images, IEEE Transactions on Geoscience and Remote Sensing, vol. 61, pp. 116, 2023. [71] Q. Zhu, C. Liao, H. Hu, X. Mei, and H. Li, Map-net: Multiple attending path neural network for building footprint extraction from remote sensed imagery, IEEE Transactions on Geoscience and Remote Sensing, vol. 59, no. 7, pp. 61696181, 2021. [72] T. Lin, P. Dollar, R. B. Girshick, K. He, B. Hariharan, and S. J. Belongie, Feature pyramid networks for object detection, in IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 936944. [73] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, Unified perceptual parsing for scene understanding, in European Conference on Computer Vision, 2018, pp. 432448. [74] L. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, Encoderdecoder with atrous separable convolution for semantic image segmentation, in European Conference on Computer Vision, 2018, pp. 833851. 14 Siyuan Yao received the Ph.D. degree from Institute of Information Engineering, Chinese Academy of in 2022. He is currently an Assistant Sciences, Professor with the School of Computer Science, Beijing University of Posts and Telecommunications (BUPT), China. He was supported by the Tencent Rhino-Bird Elite Talent Training Program in 2021. His research interests include visual object tracking, video/image analysis and machine learning. Dongxiu Liu received the B.E. degree from Beijing University of Posts and Telecommunications, China, where he is currently pursuing M.S. degree in computer science. His research interests include image segmentation, scene understanding, and spatial reasoning. Taotao Li received the Ph.D. degree in cyber security from Institute of Information Engineering, Chinese Academy of Sciences and University of Chinese Academy of Sciences, China, in 2022. He has worked as Research Fellow with Sun YatSen University, China. He is currently an assistant professor with the School of Software Engineering, Sun Yat-Sen University, China. His main research interests include blockchain, Data security, and applied cryptography. Shengjie Li received the Ph.D. degree in information and communication engineering from the Beijing University of Posts and Telecommunications (supervisor: Prof. Junliang Chen) in 2020. He is currently Lecturer with the State Key Laboratory of Networking and Switching Technology at Beijing University of Posts and Telecommunications. His current research interests include Internet of Things technology and visual object tracking. Wenqi Ren received the Ph.D. degree from Tianjin University, Tianjin, China, in 2017. From 2015 to 2016, he was supported by China Scholarship Council and working with Prof. Ming-Husan Yang as Joint-Training Ph.D. Student with the Electrical Engineering and Computer Science Department, University of California at Merced. He is currently Professor with the School of Cyber Science and Technology, Sun Yatsen University, Shenzhen Campus, Shenzhen, China. His research interests include image processing and related high-level vision problems. He received the Tencent Rhino Bird Elite Graduate Program Scholarship in 2017 and the MSRA Star Track Program in 2018. Xiaochun Cao received the BE and ME degrees in computer science from Beihang University (BUAA), China, and the PhD degree in computer science from the University of Central Florida, USA. He is professor and dean with the School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University. His dissertation nominated for the university level Outstanding Dissertation Award. After graduation, he spent about three years with ObjectVideo Inc. as research scientist. From 2008 to 2012, he was professor with Tianjin University. Before joining SYSU, he was professor with the Institute of Information Engineering, Chinese Academy of Sciences. He has authored and coauthored more than 200 journal and conference papers. In 2004 and 2010, he was the recipients of the Piero Zamperoni best student paper award at the International Conference on Pattern Recognition. He is on the editorial boards of IEEE Transactions on Pattern Analysis and Machine Intelligence and IEEE Transactions on Image Processing, and was on the editorial boards of IEEE Transactions on Circuits and Systems for Video Technology and IEEE Transactions on Multimedia."
        }
    ],
    "affiliations": [
        "School of Computer Science (National Pilot Software Engineering School), Beijing University Of Posts and Telecommunications",
        "School of Cyber Science and Technology, Shenzhen Campus, Sun Yat-sen University"
    ]
}