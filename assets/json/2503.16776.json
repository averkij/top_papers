{
    "paper_title": "OpenCity3D: What do Vision-Language Models know about Urban Environments?",
    "authors": [
        "Valentin Bieri",
        "Marco Zamboni",
        "Nicolas S. Blumer",
        "Qingxuan Chen",
        "Francis Engelmann"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) show great promise for 3D scene understanding but are mainly applied to indoor spaces or autonomous driving, focusing on low-level tasks like segmentation. This work expands their use to urban-scale environments by leveraging 3D reconstructions from multi-view aerial imagery. We propose OpenCity3D, an approach that addresses high-level tasks, such as population density estimation, building age classification, property price prediction, crime rate assessment, and noise pollution evaluation. Our findings highlight OpenCity3D's impressive zero-shot and few-shot capabilities, showcasing adaptability to new contexts. This research establishes a new paradigm for language-driven urban analytics, enabling applications in planning, policy, and environmental monitoring. See our project page: opencity3d.github.io"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 6 7 7 6 1 . 3 0 5 2 : r OpenCity3D: What do Vision-Language Models know about Urban Environments? Valentin Bieri1, Marco Zamboni1, Nicolas S. Blumer1,2, Qingxuan Chen1,2, Francis Engelmann1,3 1ETH Zurich 2University of Zurich 3Stanford University Figure 1. OpenCity3D is method for zero-shot urban 3D scene understanding, enabling insights into higher-level attributes such as crime rates, population density, housing prices, and local landmarks. For each text prompt, we visualize response heatmap, where areas of higher relevance are highlighted in yellow, transitioning to blue for lower relevance."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Vision-language models (VLMs) show great promise for 3D scene understanding but are mainly applied to indoor spaces or autonomous driving, focusing on low-level tasks like segmentation. This work expands their use to urbanscale environments by leveraging 3D reconstructions from multi-view aerial imagery. We propose OpenCity3D, an approach that addresses high-level tasks, such as population density estimation, building age classification, property price prediction, crime rate assessment, and noise pollution evaluation. Our findings highlight OpenCity3Ds impressive zero-shot and few-shot capabilities, showcasing adaptability to new contexts. This research establishes new paradigm for language-driven urban analytics, enabling applications in planning, policy, and environmental monitoring. See our project page: opencity3d.github.io Recent advancements vision-language models in (VLMs) and neural rendering techniques, such as Neural Radiance Fields (NeRFs) [21] and Gaussian Splatting (GS) [14], have unlocked wide range of applications in open-vocabulary 3D scene understanding. These approaches go beyond traditional 3D scene understanding capabilities by recognizing arbitrary object classes and concepts without requiring task-specific annotated training Instead, they leverage the generalization power of data. VLMs pre-trained on internet-scale image-text pairs, which encapsulate vast array of human knowledge and concepts. key application of these methods is open-vocabulary 3D segmentation, where user-provided queries expressed as natural language or images guide the recognition of regions within 3D scene that correspond to the query. Meth1 ods such as OpenScene [25] and LeRF [15] have demonstrated this paradigm for 3D semantic segmentation on polygon meshes and neural fields, respectively. Similarly, OpenMask3D [36] and Open3DIS [22] address 3D instance segmentation. However, these methods have been predominantly evaluated in constrained domains, such as indoor environments and autonomous driving scenarios. This work explores, for the first time, the application of VLMs to city-scale urban 3D environments. Understanding urban-scale properties ranging from building age to population density and crime rates is vital for urban planning and sustainable development. Urban settings, however, introduce unique challenges, including large-scale spatial complexity and heterogeneous scene composition, which pose significant limitations for existing methods. Despite these challenges, leveraging VLMs in urban environments has the potential to provide actionable insights, offering pathway to improved urban analysis and sustainability. To address these challenges, we propose OpenCity3D, framework for open-vocabulary city-scale 3D scene understanding. OpenCity3D generates language-enriched point cloud by processing RGB-D images rendered from aerial mesh reconstructions. Through the integration of language encoders, OpenCity3D enables querying of this enriched point cloud to analyze features of urban objects (e.g., buildings) and properties (e.g., population density, crime rates). Our findings demonstrate significant promise for urban scene understanding, particularly in tasks such as identifying building ages, estimating housing prices, and analyzing population density, while preliminary results for higherorder tasks like crime rate prediction and noise emission analysis remain less conclusive. In summary, our contributions are as follows: We introduce OpenCity3D, novel framework for open-vocabulary city-scale 3D scene understanding. We present the first systematic analysis of VLM applicability to urban-scale question answering and scene analysis. We establish an initial benchmark for open-vocabulary urban scene understanding, providing foundation for future research in this domain. 2. Related Work Open-Vocabulary 3D Segmentation Recent work has introduced advanced methods for 3D scene understanding, including large variety of tasks including 3D segmentation [12, 25, 36, 39, 40], human segmentation [37], affordances [8, 42], localization [20] and robot applications [18, 44]. Peng et al. [25] proposed OpenScene, which computes per-point features and fuses them with multi-view CLIP [29] embeddings to support open-vocabulary queries. 2 However, OpenScene struggles with producing sharp segmentation masks and lacks instance-level discrimination. OpenMask3D [36] addresses open-vocabulary 3D instance segmentation by building on Mask3D [31], using CLIP embeddings derived from SAM [16] masks on posed RGB-D images. These embeddings are matched to Mask3D instance masks, allowing comparison with text queries. key advantage of OpenMask3D is its mask-level reasoning, which improves scalability in terms of computation and storagecrucial for city-scale 3D environments. However, as Mask3D is trained on indoor datasets, it does not generalize well to urban-scale scenes. Similarly, class-agnostic models like Segment3D [7] also underperform on city-scale data due to their indoor training regime. This concept is extended in Search3D [] to towards hierarchical 3D scenes. LERF [15] and OpenNeRF [9] extend NeRF [21] to language-driven queries by learning language fields from 2D CLIP features, enabling similarity-based rendering. LangSplat [28] instead combines 3D Gaussian Splatting with CLIP and SAM features, compressing VLM embeddings via an autoencoder and optimizing them through rendered-view comparisons with CLIP. While LangSplat shows promise, scaling Gaussian Splatting to large urban environments remains challenging. Moreover, its reliance on feature compression limits open-vocabulary performance. Our approach removes this bottleneck by adapting LangSplats hierarchical feature extraction to sparse point cloud representation inspired by OpenScene. By forgoing Gaussian Splatting in favor of point clouds, we retain full, uncompressed VLM featuressacrificing some geometric fidelity, which is less critical for the high-level urban analytics tasks we target. Large-Scale 3D Scene Representations Existing methods for 3D scene understanding use diverse scene representations, including polygon meshes [25, 36], point clouds [11, 35], scene graphs [17], NeRFs [9], and Gaussian splats [28]. Urban-scale environments pose unique challenges, requiring representations that scale efficiently in both memory and computation. Implicit models such as ImpliCity [34] and Block-NeRF [38] offer strong denoising and compression but incur high optimization cost and require rendering for querying. In contrast, our method adopts an explicit point cloud representation to enable direct and efficient access to scenes. 3. Method 2 illustrates 3D City Representation. Fig. our OpenCity3D model. The goal is to create 3D city representation augmented with VLM features for answering high-level socio-economical questions, such as population density, housing costs, and noise pollution. This 3D city representation is computed in three stages: Figure 2. The OpenCity3D model. Multi-view RGB-D images are rendered from aerial 3D reconstructions, followed by extracting pixelwise hierarchical visual-language features. These features are mapped back to the 3D mesh, enabling language-based queries. highlight the segmented area, as shown in Fig. 4. The highlighting is form of visual prompt tuning [33] which consists of two steps: outlining the segment with solid red line and reducing the opacity of the background. In our experiments (Tab. 2), we found that this approach outperforms existing methods such as [19, 28], which completely mask the background, potentially disregarding relevant scene context. We then process each highlighted segment with SigLIP [41] to extract VLM features and assign them to all pixels within that segment. We also compute the VLM feature for the entire rendered image and consider it as an additional hierarchy level: l=0. This process results in 4 (3 hierarchies from SAM and 1 global) sets of VLM features per pixel and rendered viewpoint. Next, we want to assign multi-level VLM features to the vertices of the mesh. To do so we project each vertex to each of the rendered RGB-D images and, if the point is visible, assign the hierarchical VLM features of the corresponding pixel. To verify the visibility we use the rendered depth maps to perform an occlusion test similar to [25, 36]. In each hierarchy level the features of the vertex are averaged over all the images where the vertex is visible. The point cloud defined by the vertices with the associated multi-level VLM features is the final 3D city representation that we will use for downstream tasks. Processing scene with 104 rendered RGB images across all L=4 hierarchy levels takes 48 hours on single NVIDIA 4090 GPU. Similarity-based Prediction. Given natural language text query and set of negative queries , we use the SigLIP [41] text-encoder to compute their embedding ϕq, ϕn, . For each point in the pointcloud we then infer the similarity scores sq,p and sn,p by comparing the encoded queries to the multi-level per-point features ϕl stored at point and level in the 3D scene representation. Specifically, we follow [15, 28] and compute the maximal cosine similarity score across every level l, between the point embedding ϕl and the query embedding ϕq: sq,p = max l{1, ,L} exp(ϕ ϕl p) (1) (a) Large (l=1) (b) Medium (l=2) Figure 3. Visualization of SAMs multi-scale segments across three hierarchy levels: small, medium, and large. (c) Small (l=3) rendering RGB-D images from textured 3D mesh, extracting multi-scale pixel-aligned VLM features, and backprojecting these features onto the mesh vertices. The input consists of 3D polygon mesh derived from an aerial 3D reconstruction, such as Google Maps 3D Tiles [10]. We first render RGB color and depth images of the 3D city mesh from multiple randomly selected viewpoints. Specifically, the camera position is randomly placed within the horizontal bounds of the 3D mesh and at random height between 15 and 100 above sea level. The camera azimuth orientation is sampled uniformly from [0, 360], and the elevation from [0, 90], avoiding skyfacing views. From the collection of rendered RGB-D images we discard images where the depth is closer than 50 and images where more than 20% of the pixels are at infinite depth. After rendering the RGB-D images, we apply SAM [16] to the RGB images in order to obtain segments across three hierarchy levels (see Fig. 3). For each segment across all levels, we tightly crop the rendered image around the segment, similar to [15, 36], and Figure 4. Example of highlighted segments. Not removing the background provides context 3 The final score for point and query is the cosine similarity described above normalized against the negative queries: sq,p = sq,p sq,p + (cid:80) nN sn,p (2) We can use the point scores for classification (e.g., land use), where they serve as probabilities; or for regression (e.g., property cost or building age), in which case the scores are mapped to bins: specifically, we split the distributions of both the predicted scores and ground-truth distributions into quantiles (i.e. bins), then map each predicted bin to the mean of the corresponding ground-truth bin. We use k=5 for property price and building age prediction. Supervised Prediction. Notably, the prediction via similarity scores sq,p does not depend on or benefit from taskspecific training data. To overcome this we propose to perform supervised learning using the VLM features as inputs. To do so, we discretize the ground truth values into quantilebased bins and have the classifier predict the probability of each bin. During inference, we multiply the predicted bin probabilities with the bin centers and sum over them to obtain continuous values. We experiment with two types of classifiers: K-Nearest Neighbors (KNN) and Light Gradient Boosting Machines (LGBMs [13]). Unless otherwise stated, we set the number of quantiles to = 5 with 30% of the data as training data and report the average over five random draws of train-test splits. GPT-4o-based Prediction. As an alternative to SigLIP and in in order to explore the capabilities of state of the art commercial model we also propose method based on GPT-4o [23]. For each experiment we design specific textual prompt where we ask the model to return value between 0 and 10 in relation to the query. In order to better guide the model we also include the string return the result without explanation and if possible we provide indicative numbers associated to the values (See Tab. 8 for examples). We then use the RGB images rendered from the 3D mesh and feed them together with the textual prompt to the OpenAI API; in this way for each image we obtain value between 0 and 10. In the same way as what we do with VLM features we then associate the values to the points that are visible in the image. Those values are then equivalent to sq,p scores and can be used for similarity-based prediction. We decided to use only full images instead of SAM segments (so only layer = 0) to avoid exploding costs and processing time. 4. Experiments In this experiment, we assess the ability of VLMs to address broad socio-economic questions in urban settings. Using publicly available data, we evaluate six tasks across cities in the Netherlands, the US, and Buenos Airescovering building footprint and construction year estimation (Sec.4.1), property valuation (Sec.4.2), and socio-economic indicators such as population density, crime rates, and noise pollution (Sec.4.3). All experiments are conducted using VLM-augmented 3D city representations derived from texturized meshes in Google Earth [10], as detailed in Sec. 3. 4.1. The Netherlands: Building Footprint and Age Dataset. Our first set of experiments uses the Basisregistraties Adressen en Gebouwen (BAG), the official building and address registry of the Netherlands [26]. This dataset provides regularly updated information on all buildings in the country; from it, we construct benchmark of 19,349 buildings to evaluate the ability of VLMs to predict 2D building footprints and construction years. Building Footprints. This task assesses whether VLMs can accurately predict building footprints. We formulate it as binary classification problem, where the model labels each 3D point as either building or background. As evaluation metric, we compute the Receiver Operating Characteristic Area Under the Curve [2] (ROC-AUC) score. The ROC-AUC score indicates how clearly classifier distinguishes positive from negative classes. We query the 3D scene representation with the positive query building and set of negative queries representing common urban background objects such as tree road, or car (full list in Appendix Sec. B.3). Using Eq. 2, we compute the similarity score sq,p [0, 1] which we interpret as probability score. The predicted scores are projected onto 2D grid by averaging scores of multiple 3D points per cell; empty cells are filled via linear interpolation. The resulting 2D map is then compared to ground truth footprint labels for evaluation. We find that this classifier attains ROC-AUC score between 86.0% and 94.6%, accompanied by accuracies in the range of 83.2% and 89.8 %. This is significant improvement compared to LangSplat-style features projected to the same point cloud, which achieves only 79.8 % accuracy with ROC-AUC of 86.2 % on the Rotterdam scene. Furthermore, our method strongly benefits from projecting the features to 3D point cloud, instead of directly on 2D point grid (see Tab. 5, in the Appendix). Construction Year. In this task, the goal is to predict the year of construction of building. In first experiment, we predict age scores by comparing the positive prompt modern building to the negative old building. The ratio (Eq. 2) between the similarity of the two is our score for the building age. Then we again 4 The result is interpreted as score for expensiveness, projected to 2D, and linearly interpolated to the known coordinates of the sold properties in the Zillow dataset. The resulting score has correlations between 0.28 and 0.67 with the ground-truth sales prices. Training LGBM Classifier across scenes improves upon this (Tab. 3) and reaches MAE of 0.25M$, which is significantly better than chance (0.52M $ MAE). These results indicate that VLMs understand some of the mechanics that determine urban property value. Their features may be valuable addition to larger parametric models such as Zillows Zestimate [5]. 4.3. Population Density, Crime Rate and Pollution We collect official statistics from the Autonomous City of Buenos Aires (CABA) of population count [1], crime records [3], and urban noise emissions [4]. Along with them, we process one larger mesh sourced from Google Earth [10] following Sec. 3 to obtain point cloud with point-wise VLM features, using the coarsest feature level. Population Density. Given the point cloud and features, we use prompts to estimate population density as given by the CABA data. The population density is given at the granularity of neighborhoods and computed by dividing the number of residents between 2015 and 2018 by the area (see Fig. 7 a). We build an indicator using the positive prompts densely populated area, and strongly populated district. As negatives, we choose loosely populated area, and unpopulated area. Once again, we project the points to two dimensions, resample them to regular grid, and assign them the ground truth value taken from the CABA records. We find that the indicator yields Spearman correlation of 0.63. The model correctly identifies the population cluster in the north-western section (see Fig. 7 a). However, it erroneously assigns high scores to the city center south of the train station. With the two additional negatives nature and industrial area, the correlation is boosted to 0.75. We also evaluate the features in few-shot setting, using 28 training and 94 validation neighborhoods to train KNN regressor. This results in similar correlation of 0.61 (see Tab. 4). These comparably strong results do not come unexpectedly. The population density is in direct relationship with the number and size of visible residential buildings. Crime Rate. Given the same features we predict Buenos Aires crime rates and validate the result against the CABA records. CABA provides locations and descriptions of all recorded crimes between 2016 and 2022 [3]. We remove any crimes that do not involve weapon to exclude incidents that are not necessarily tied to location, such as tax evasion or fraud. This leaves us with dataset of Figure 5. Illustration of the challenge to estimate building ages in the Rotterdam mesh: the left building dates from 1907, while the right one (directly adjacent) was built in 1997. project the points to two dimensions and re-sampling them on regular grid. Each point within building is assigned ground truth construction year, all other points are ignored. The results are displayed in Tab. 1. With Spearman correlations above 50% for both similarity-based approaches in four out of seven cities, our model provides solid first baseline for vision-based building age prediction. In the classifier-based setting, we train an LGBM Classifier [13], which predicts an actual construction year instead of similarity score. When trained within cities (Tab. 1), this consistently results in higher correlations, combined with robust F1 scores between 0.42 and 0.64. Yet, outliers such as medieval churches lead to varying Mean Absolute Error (MAE) scores - particularly in the historic Amsterdam scene. Similar results of experiments across scenes are displayed in the Appendix (Tab. 6). Fig. 6 shows qualitative results and illustrates how the method is able to distinguish entire districts consisting of more modern architecture from more traditional areas. Modern houses that are built back-to-back to older houses as seen in Fig. 5 are harder to differentiate, as these are often built to match the style of the existing neighborhood. We furthermore find that OpenCity3D outperforms LangSplatstyle features as shown in Tab. 2. 4.2. United States: Housing Prices Next, we use Zillow [43] data to evaluate the prediction of housing prices. Zillow provides commercial analytics tool for the US real estate market that combines property data from public records with property listings from various sources. We evaluate on 1260 homes sold between 2020 and 2024 across seven US cities. Housing Prices. Taking the point cloud with per-point features as input, we estimate the sales price of the listed homes. To this end, we construct an indicator analogous to the previous section, using expensive property as positive and cheap property as negative prompt. 5 Figure 6. Similarity-based predicted age (left) vs. ground truth construction years (right) in Rotterdam. City Method Building Age Building Segmentation Correlation F1 Score MAE [y] Max Accuracy ROC-AUC [2] F1 Score Rotterdam OpenCity3D (prompt) OpenCity3D (LGBM) OpenCity3D (GPT-4o) Amsterdam OpenCity3D (prompt) OpenCity3D (LGBM) OpenCity3D (GPT-4o) The Hague Utrecht Eindhoven Groningen Maastricht OpenCity3D (prompt) OpenCity3D (LGBM) OpenCity3D (GPT-4o) OpenCity3D (prompt) OpenCity3D (LGBM) OpenCity3D (GPT-4o) OpenCity3D (prompt) OpenCity3D (LGBM) OpenCity3D (GPT-4o) OpenCity3D (prompt) OpenCity3D (LGBM) OpenCity3D (GPT-4o) OpenCity3D (prompt) OpenCity3D (LGBM) OpenCity3D (GPT-4o) 0.556 0.769 0. 0.507 0.577 0.293 0.533 0.689 0.498 0.364 0.516 0.502 0.430 0.753 0.636 0.636 0.744 0.716 0.473 0.717 0. 0.317* 0.639 0.566* 0.343* 0.419 0.300* 0.321* 0.457 0.313 0.280* 0.482 0.355* 0.270* 0.501 0.380* 0.329* 0.503 0.392* 0.332* 0.542 0.341* 35.7* 20.9 34.5* 240.22* 97.7 251.22* 151.34* 50.0 146.80 170.76* 58.3 156.76* 27.63* 12.82 21.60* 81.52* 19.28 74.9* 223.81* 57.3 210.50* 87.7% 84.6% 85.3% 76.5% 83.8% 86.4% 83.2% 74.1% 87.2% 87.7% 89.8% 84.4% 84.5% 81.8% 0.927 0.906 0.860 0.853 0.925 0.928 0.866 0.813 0.931 0.899 0.946 0.901 0.901 0.889 0.796 0.702 0.722 0.642 0.791 0.761 0.752 0.703 0.798 0.626 0.813 0.665 0.760 0.722 Table 1. Result overview for building age prediction within various cities in the Netherlands. The asterisk (*) indicates scores estimated by matching the score with the ground truth distribution based on quantiles, which is described in the supplementary material. 2146 crimes within the scene. To avoid artifacts at region boundaries and attenuate sparsity effects, we consider each crime 2D Gaussian distribution (σ = 50m), from which we sample to compute the ground truth expected number of annual armed crimes per km2 and neighborhood. As an estimator we invoke Eq. 2, using positive 6 (a) Population Density. (b) Crimes per km2 and year. Figure 7. Zero-shot prediction (top) and ground truth (bottom) on Buenos Aires scenes. (c) Measured noise emissions. Feature Type Age Correlation Building seg. max accuracy LangSplat + CLIP + prompt + CLIP + KNN + SigLIP + prompt + SigLIP + KNN Ours + CLIP + prompt + CLIP + KNN + SigLIP + prompt + SigLIP + KNN 0.394 0.544 0.186 0.577 0.520 0.681 0.556 0. 79.8 80.7 81.3 80.9 76.1 79.1 87.7 83.0 Table 2. Evaluating feature extraction methods on the Rotterdam scene. Mask highlighting, done by OpenCity3D, outperforms the white-background LangSplat approach. For LangSplat, the uncompressed, point-projected features are evaluated. alized in Fig. 7 b, the task mainly consists of identifying the port-facing side of the north-western district as dangerous area. The model however assigns high danger scores to the port as well as the park to the southeast. We can once again include prior knowledge to increase the correlation to 0.42. In this case, however, this prior is less easily justified, as large city parks do not universally induce lower crime rates - though the mere absence of people may indicate such tendency. When evaluated in the aforementioned few-shot setting, KNN classification on the averaged neighborhood embeddings results in an improved correlation of 0.67. In summary, predicting crime rates presents itself as complex task where many influential factors may not be immediately visible. Having reference values, like in the KNN version, greatly increased the quality of the results. This finding indicates the need for more nuanced approach, potentially incorporating broader range of data types. query dangerous neighborhood and the negative safe neighborhood. The resulting indicator obtains relatively low Spearman correlation of 0.30. As visuNoise Pollution. We follow the same procedure to estimate urban noise levels, comparing the results to official CABA measurements. The relevant CABA noise emis7 Detroit Miami San Juan Boston San Fran. Seattle Los Angeles Overall Spearman F1 Score MAE [M$] OpenCity3D (prompt) OpenCity3D (LGBM) OpenCity3D (GPT-4o) OpenCity3D (prompt) OpenCity3D (LGBM) OpenCity3D (GPT-4o) OpenCity3D (prompt) OpenCity3D (LGBM) OpenCity3D (GPT-4o) 0.528 0.506 0. 0.298 0.489 0.362 0.201 0.174 0.195 0.492 0.338 0.487 0.278 0.398 0.318 0.354 0.698 0.364 0.348 0.432 0. 0.337 0.340 0.337 0.477 0.389 0.498 0.278 0.433 0.194 0.254 0.397 0.223 0.173 0.160 0.189 0.674 0.568 0. 0.381 0.594 0.413 0.179 0.163 0.171 0.419 0.414 0.366 0.300 0.485 0.309 0.365 0.349 0.419 0.504 0.728 0. 0.294 0.541 0.212 0.276 0.174 0.350 0.402 0.739 0.339 0.308 0.491 0.309 0.360 0.251 0.373 Table 3. Result overview for housing price prediction across scenes in North America for zeroand few-shot setting. Zero-shot estimates of F1 and MAE are again computed by quantile-based distribution matching as described in the supplementary material. sion dataset [4] provides map of estimated average daytime noise in decibels along major city roads (see Fig. 7 c). To build an estimator, we again prompt the features using Eq. 2 with noisy urban area as positive prompt, contrasted with quiet area as negative. This gives us weak Spearman correlation of 0.19. In the few-shot setting, we train KNN regressor and obtain moderate correlation of 0.71. Similar to the prediction of crime rates, noise level estimations remain difficult for VLMs, in particular in zero-shot setting. Model Population Density Crime Rate Noise Level Prompt KNN GPT-4o 62.5 60.9 45.1 42.2 67.3 54. 19.8 71.6 28.6 Table 4. Spearman correlations in % for predictions: population density, crime rate, and noise levels on the Buenos Aires dataset. 5. Limitations key challenge in large-scale urban 3D scene understanding is the absence of standardized datasets and benchmarks. This work takes an initial step by establishing baselines using two substantial datasets: the BAG building dataset [26] and the Zillow housing dataset [43]. However, our dataset selection is constrained to regions where public data is available, potentially introducing bias toward more developed areas that collect such information. Additionally, the scale of large cities remains technical limitation. Unlike methods such as LangSplat [28], our approach does not compress the VLM feature space into three dimensions, preserving open-vocabulary capabilities at the expense of higher memory consumption. Large cities are processed in rectangular chunks, which may introduce artifacts at chunk boundaries if overlap is insufficient. Another limitation is the reliance on relatively low-quality 3D meshes for rendering, which may affect how well VLMs interpret the imagery. To minimize bias, we recommend comparing predictions only between meshes of the same quality and source. Moreover, discrepancies between the imagery and ground-truth data may arise due to differences in capture dates (see supplementary material). Beyond technical constraints, OpenCity3D may also reflect social and cultural biases present in visual language models. Such biases originate from the underor overrepresentation of certain demographic groups in training datasets. tasks like crime rate prediction (Sec.4.3) risk perpetuating stereotypes and systemic discrimination, especially given the limited availability of diverse test data, as highlighted by Pouget et al. [27]. Approaches like those proposed by Seth et al. [32] may help mitigate these biases. In particular, 6. Conclusion This work explores whether visual-language models (VLMs) can address city-scale socio-economic questions in urban environments, including population density, building age, property value, crime rates, and noise levels. Our experiments indicate that VLMs show strong potential for estimating population density, building age, and property value from visual data alone, while tasks like crime rate and noise level prediction remain more challenging. However, future research must account for potential biases inherent in VLMs. We view this study as first step in this domain and hope to inspire further exploration in this direction. Acknowledgments. This project originated from student coursework conducted at the CVG group at ETH Zurich and is partially supported by an ETH AI Center Postdoctoral Fellowship and an SNF Postdoc.Mobility Fellowship."
        },
        {
            "title": "References",
            "content": "[1] Bits and Bricks. Buenos aires population density. https: //bitsandbricks.github.io/data/CABA_rc. geojson, 2024. Accessed: 2024-06-15. 5 [2] Andrew P. Bradley. The Use of the Area Under the ROC Curve in the Evaluation of Machine Learning Algorithms. Pattern Recognition, 1997. 4, 6, 11 [3] Buenos Aires City Government. Buenos aires government open data portal, 2021. Accessed: 2024-05-18. 5 [4] Buenos Aires City Government. Buenos aires government open data portal, 2021. Accessed: 2024-05-18. 5, [5] Angela Burden. Imputing Data for the Zestimate. https: / / www . zillow . com / tech / imputing - data - for-the-zestimate/. Accessed: 2024-06-15. 5 [6] CARTO. Carto basemap styles. Accessed: 2024-07-16. 17, 18, 19 [7] Meida Chen, Qingyong Hu, Thomas Hugues, Andrew Feng, Yu Hou, Kyle McCullough, and Lucio Soibelman. STPLS3D: Large-Scale Synthetic and Real Aerial PhoIn British Machine togrammetry 3D Point Cloud Dataset. Vision Conference (BMVC), 2022. 2, 13 [8] Alexandros Delitzas, Ayca Takmaz, Federico Tombari, Robert Sumner, Marc Pollefeys, and Francis Engelmann. SceneFun3D: Fine-Grained Functionality and Affordance In International Conference Understanding in 3D Scenes. on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [9] Francis Engelmann, Fabian Manhardt, Michael Niemeyer, Keisuke Tateno, Marc Pollefeys, and Federico Tombari. OpenNerf: Open Set 3D Neural Scene Segmentation with In InterPixel-Wise Features and Rendered Novel Views. national Conference on Learning Representations (ICLR), 2024. 2 [10] Google. Google 3d tiles. https://www.google.com/ 3dtiles/. Accessed: 2024-06-15. 3, 4, 5, 16 [11] Rui Huang, Songyou Peng, Ayca Takmaz, Federico Tombari, Marc Pollefeys, Shiji Song, Gao Huang, and Francis Engelmann. Segment3D: Learning Fine-Grained Class-Agnostic 3D Segmentation without Manual Labels. In European Conference on Computer Vision (ECCV), 2023. 2, 13 [12] Guangda Ji, Silvan Weder, Francis Engelmann, Marc Pollefeys, and Hermann Blum. ARKit LabelMaker: New Scale for Indoor 3D Scene Understanding. arXiv preprint arXiv:2410.13924, 2024. 2 [13] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. LightGBM: Highly Efficient Gradient Boosting Decision Tree. International Conference on Neural Information Processing Systems (NeurIPS), 2017. 4, 5, 11, 12 [14] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Transactions On Graphics (TOG), 2023. 1 [15] Justin* Kerr, Chung Min* Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded In International Conference on Computer radiance fields. Vision (ICCV), 2023. 2, [16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment Anything. In International Conference on Computer Vision (ICCV), 2023. 2, 3 [17] Sebastian Koch, Narunas Vaskevicius, Mirco Colosi, Pedro Hermosilla, and Timo Ropinski. Open3DSG: Openvocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-set Relationships. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [18] Oliver Lemke, Zuria Bauer, Rene Zurbrugg, Marc Pollefeys, Francis Engelmann, and Hermann Blum. Spot-Compose: Framework for Open-Vocabulary Object Retrieval and Drawer Manipulation in Point Clouds. In ICRAW, 2024. 2 [19] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Open-vocabulary Semantic Segmentation with Mask-Adapted Alip. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3 [20] Yang Miao, Francis Engelmann, Olga Vysotska, Federico Tombari, Marc Pollefeys, and Daniel Bela Barath. SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs. In European Conference on Computer Vision (ECCV), 2025. 2 [21] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing Scenes as Neural Radiance Fields for View In European Conference on Computer Vision Synthesis. (ECCV), 2020. 1, 2 [22] Phuc Nguyen, Tuan Duc Ngo, Evangelos Kalogerakis, Chuang Gan, Anh Tran, Cuong Pham, and Khoi Nguyen. Open3DIS: Open-Vocabulary 3D Instance Segmentation In International Conference on with 2D Mask Guidance. Computer Vision and Pattern Recognition (CVPR), 2024. 2 gpt-4o. https://openai.com/index/hello-gpt-4o. Accessed: 2024-099. [23] OpenAI. Open hello ai. [24] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 2011. 12 [25] Songyou Peng, Kyle Genova, Chiyu Max Jiang, Andrea Tagliasacchi, Marc Pollefeys, and Thomas Funkhouser. OpenScene: 3D Scene Understanding with Open VocabularIn International Conference on Computer Vision and ies. Pattern Recognition (CVPR), 2023. 2, [26] Ravi Peters, Balazs Dukai, Stelios Vitalis, Jordi van Liempt, and Jantien Stoter. Automated 3D Reconstruction of LoD2 and LoD1 Models for all 10 million Buildings of the Netherlands, 2022. 4, 8 [27] Angeline Pouget, Lucas Beyer, Emanuele Bugliarello, Xiao Wang, Andreas Peter Steiner, Xiaohua Zhai, and Ibrahim Alabdulmohsin. No filter: Cultural and socioeconomic diversityin contrastive vision-language models. arXiv preprint arXiv:2405.13777, 2024. 8 9 [40] Yuanwen Yue, Sabarinath Mahadevan, Jonas Schult, Francis Engelmann, Bastian Leibe, Konrad Schindler, and Theodora Kontogianni. AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation. In International Conference on Learning Representations (ICLR), 2024. 2 [41] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Sigmoid Loss for Language Image PreIn International Conference on Computer Vision Lucas Beyer. training. (ICCV), 2023. [42] Chenyangguang Zhang, Alexandros Delitzas, Fangjinhua Wang, Ruida Zhang, Xiangyang Ji, Marc Pollefeys, and Francis Engelmann. Open-Vocabulary Functional 3D Scene In International Graphs for Real-World Indoor Spaces. Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2 [43] Zillow. Zillow group, inc. https://www.zillow. com/homes/. Accessed: 2024-07-13. 5, 8 [44] Rene Zurbrugg, Yifan Liu, Francis Engelmann, Suryansh Kumar, Marco Hutter, Vaishakh Patil, and Fisher Yu. ICGNet: Unified Approach for Instance-centric Grasping. In International Conference on Robotics and Automation (ICRA), 2024. 2 [28] Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. LangSplat: 3D Language Gaussian Splatting. International Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 3, 8 [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models from Natural Language Supervision. In International Conference on Machine Learning (ICML), 2021. [30] David Rozenberszki, Or Litany, and Angela Dai. LanguageGrounded Indoor 3D Semantic Segmentation in the Wild. In Proceedings of the European Conference on Computer Vision (ECCV), 2022. 13 [31] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3D: Mask TransIn Interformer for 3D Semantic Instance Segmentation. national Conference on Robotics and Automation (ICRA), 2023. 2, 13 [32] Ashish Seth, Mayur Hemani, and Chirag Agarwal. DeAR: Debiasing Vision-Language Models with Additive ResiduIn International Conference on Computer Vision and als. Pattern Recognition (CVPR), 2023. 8 [33] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What Does CLIP Know About Red Circle? Visual Prompt Engineering for VLMs. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3 [34] Corinne Stucker, Bingxin Ke, Yuanwen Yue, Shengyu Huang, Iro Armeni, and Konrad Schindler. ImpliCity: City Modeling from Satellite Images with Deep Implicit Occupancy Fields. Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences (ISPRS), 2022. 2 [35] Ayca Takmaz, Alexandros Delitzas, Robert Sumner, Francis Engelmann, Johanna Wald, and Federico Tombari. Search3D: Hierarchical Open-Vocabulary 3D Segmentation. IEEE Robotics and Automation Letters (RA-L), 2025. 2 [36] Ayca Takmaz, Elisabetta Fedele, Robert W. Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. OpenMask3D: Open-Vocabulary 3D Instance Segmentation. In International Conference on Neural Information Processing Systems (NeurIPS), 2023. 2, 3, 11, 13 [37] Ayca Takmaz, Jonas Schult, Irem Kaftan, Mertcan Akcay, Bastian Leibe, Robert Sumner, Francis Engelmann, and Siyu Tang. 3D Segmentation of Humans in Point Clouds with In International Conference on Computer Synthetic Data. Vision (ICCV), 2023. [38] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul Srinivasan, Jonathan Barron, and Henrik Kretzschmar. Block-NeRF: Scalable Large Scene Neural View Synthesis. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2 [39] Silvan Weder, Hermann Blum, Francis Engelmann, and Marc Pollefeys. LabelMaker: Automatic Semantic Label Generation from RGB-D Trajectories. In International Conference on 3d Vision (3dV), 2024. 2 10 A. Additional Results A.1. Evaluation Across and Within Scenes In the main paper, we presented results for building age prediction within cities and property price estimation across scenes. The complementary results for building age across cities and property price within scenes are presented in Tables 1 and 7, featuring additional metrics. Furthermore, confusion matrices are visualized in Fig. 8 and Fig. 9. Geometry Type ROC-AUC [2] F1 Score 3D Point Cloud + prompt + KNN Flat Geometry + prompt + KNN 0.946 0.828 0.904 0.789 0.813 0. 0.724 0.591 3D mesh. Tab. 5 shows that performance degrades significantly in that case. We believe that this is caused by the imprecise assignment of points to masks. An example for such imprecision are pixels near the horizon, which in this case are assumed to correspond to faraway points - although they may be occluded by the foreground. B. Implementation Details B.1. Rendering RGB-D Views from 3D Mesh We sample positions based on 2D grid, adding random offsets on all three axes. The angle to the up-axis is sampled between 0 and 90 degrees to avoid sky-facing cameras. The other angle is sampled uniformly at random. RGB-D images with depth closer than 50 and images with infinite depth in more than 20% of the pixels are discarded. See Tab. 9 for details on the scenes. Table 5. Comparison of building segmentation performance in Groningen with 3D point cloud vs. using flat point grid. B.2. Projection to Point Cloud A.2. Evaluation with more Training Data We find that the results across scenes can be significantly boosted when training with more than 30% of the dataset. Fig. 10 and Fig. 11 visualize this effect. A.3. Ablation: 3D Point Cloud vs. Flat Grid Although only evaluating on 2D grid we find that the usage of 3D point cloud is beneficial for feature fusing. We demonstrate that by performing an experiment without 3D geometry or depth information. That is, we project the same features onto 2D point grid instead of the original The point cloud is first downsampled to 1M points (0.5M if only the coarsest level was processed) to reduce memory consumption. Following OpenMask3D [36], point visibility is determined based on depth. However, we filter the masks before projection. As most segments only cover handful of pixels, we retain only those that cover at least 0.25% of the image.[]. This leads to the removal of roughly 60% of all segments and speeds up the overall processing time by 40%. Figure 8. Confusion matrix of property price classification with LGBM [13] across scenes. Figure 9. Confusion matrix of building age classification with LGBM [13] across scenes. 11 Figure 10. Property price estimation results against dataset size for experiment across scenes. Zero-shot MAE baselines were obtained from scores by matching quantiles. Figure 11. Building age estimation results against dataset size for experiment across scenes. Note how quantile matching fails to produce meaningful zero-shot baselines, producing MAE significantly worse than chance. B.3. Prompting the Point Embeddings As mentioned in the main paper, we prompt the model with positive and negative queries. We find that the choice of negatives can have strong impact on performance. For building segmentation, the full set of negatives was: tree, road, park, river, car, sea / lake / canal, urban scene, and city. parking lot, B.4. Estimation We use scikit-learn [24] to build unweighted KNN regressors and classifiers (k = 5). Each point and feature level provides data point. As for LightGBM [13], we use the official package with default settings. We find that classifiers on building age, crime rate, noise levels, and population density benefit significantly from reducing noise by averaging the per-point embeddings of the relevant area (district) before training and inference. B.5. Projection of Scores to Ground Truth Scale For property price and building age prediction, we experiment with methods to convert the scores into estimates matching the scale of the ground truth distribution. To that end, we compute the quantiles of the predicted and the ground truth distribution. Then we assign prediction in the i-th quantile of the score distribution the mean of the values in the i-th quantile of the true distribution. We implement this strategy with = 5 B.6. GPT-4o Integration We use GPT-4o to produce one score per prompt and image. The obtained score is then fused into the point cloud analogously to the embeddings. Due to cost and time constraints, we only process full images (coarsest level) and no individual segment masks from SAM. Table 8 shows the used prompts for the GPT experiments (GPT4o). For estimating property price and building age experiments, the rating has been grounded by providing reference values for ratings 3, 6, and 9. These reference values are obtained by binning the ground truth data into 10 bins. Despite this grounding, the resulting scores only match the ground truth distribution to limited extent. We therefore evaluate them analogously to the similarity scores. The induced prompting cost scales with the number and quality of images as well as the length of the response. Our experiments with 12 Figure 12. Example segmentation of city area using Segment3D 7k to 10k images per scene cost 10-20$ per query. At the time of creation (September 2024), the inference time was roughly at 4-8h per scene. B.7. Evaluation We evaluate our method in 2D against ground truth map data. To obtain Unless stated otherwise, the 3D point cloud is projected to 2D and then interpolated linearly on regular grid. Correlation is computed on the points (not the districts/buildings). The validation set of the KNN estimators is uniformly randomly downsampled to 20k points per scene to reduce inference time. Preliminary experiments showed that this has no significant effect on the results. C. OpenMask3D for Urban Point Clouds key characteristic of OpenMask3D [36] is that it segments the input point cloud and then stores one feature per 3D segment. This greatly boosts storage and memory efficiency, making it well-suited for city-scale input. Unfortunately, OpenMask3D relies on the 3D segmentation method Mask3D [31] which is trained on indoor data and therefore fails to generate meaningful segments for our 3D city scenes. Neither OpenMask3Ds Scannet200 [30] and STPLS3D [7] checkpoints, nor the more recent Segment3D [11] model claimed to have superior generalization performances compared to Mask3D remedied the situation (see Fig. 12). In particular, we find that the models display high sensitivity to the density and scale of the point clouds. D. Additional Visualizations We provide qualitative results for open-set segmentation in Fig. 13. Fig. 14 and Fig. 15 visualize the complete results for property price prediction, whereas Fig. 16 and Fig. 17 display the ones for building age prediction. 13 (a) Rendered mesh (b) Prompt building (c) Prompt road (d) Prompt water (e) Prompt tree (f) Prompt train tracks Figure 13. Qualitative results for open-set segmentation in Amsterdam. We can see that buildings 13b, trees 13e and train tracks 13f are recognized with high precision, but the model has difficulties for water 13d and roads 13c"
        },
        {
            "title": "Overall Amsterdam The Hague Eindhoven Groningen Maastricht Rotterdam Utrecht",
            "content": "F1 Score lgbm linear knn dummy 0.67 0.61 0.61 0.20 0.54 0.52 0.51 0."
        },
        {
            "title": "Spearman Correlation",
            "content": "lgbm linear knn dummy MAE [y] lgbm linear knn dummy 0.73 0.67 0.67 0.00 0.32 0.29 0.25 -0.01 50.85 62.84 55.62 102. 122.23 137.46 125.30 166.55 MAPE [%] lgbm linear knn dummy 3.03 3.63 3.30 5.85 8.28 8.94 8.53 11.10 0.47 0.38 0.43 0. 0.56 0.46 0.46 0.01 57.99 88.79 62.59 93.28 3.11 4.71 3.36 5.03 0.81 0.76 0.78 0.28 0.40 0.32 0.33 -0.02 12.64 13.09 14.46 77. 0.64 0.66 0.73 3.94 0.75 0.66 0.70 0.24 0.84 0.70 0.77 0.03 18.26 25.09 24.12 88.49 0.94 1.29 1.23 4.51 0.60 0.56 0.54 0. 0.65 0.61 0.56 -0.00 63.50 82.48 67.76 106.14 3.43 4.40 3.67 5.82 0.76 0.55 0.70 0.23 0.76 0.57 0.67 -0.01 15.65 22.31 18.67 75. 0.81 1.15 0.96 3.93 0.59 0.53 0.49 0.21 0.68 0.60 0.52 0.01 60.62 68.57 72.12 109.80 3.72 4.10 4.31 6.33 Table 6. OpenCity3D results for construction year prediction trained across various cities in the Netherlands. Mean Detroit Miami San Juan Boston San Fran. Seattle Los Angeles F1 Score lgbm linear knn dummy 0.34 0.28 0.32 0.20 0.33 0.30 0.34 0.20 Spearman Correlation lgbm linear knn dummy 0.49 0.51 0.51 0.00 MAE [M$] lgbm linear knn dummy 0.34 0.37 0.32 0.52 RMSE [M$] lgbm linear knn dummy 0.58 0.60 0.55 0.80 0.55 0.55 0.59 0. 0.19 0.21 0.17 0.28 0.28 0.31 0.26 0.38 0.25 0.19 0.19 0.20 0.24 0.30 0.29 0.00 1.03 1.10 0.97 1.29 2.20 2.23 2.15 2. 0.38 0.33 0.36 0.19 0.45 0.38 0.39 0.00 0.39 0.45 0.37 0.55 0.56 0.64 0.54 0.74 0.34 0.29 0.31 0.22 0.49 0.44 0.41 0. 0.14 0.16 0.14 0.20 0.17 0.21 0.17 0.25 0.34 0.24 0.35 0.21 0.57 0.68 0.63 0.00 0.17 0.16 0.14 0.39 0.24 0.22 0.19 0. 0.33 0.22 0.26 0.17 0.39 0.43 0.46 0.00 0.32 0.35 0.30 0.51 0.42 0.44 0.39 0.64 0.40 0.38 0.45 0.18 0.75 0.79 0.77 0. 0.14 0.13 0.11 0.39 0.19 0.17 0.15 0.50 Table 7. OpenCity3D few-shot results for property price prediction trained within various cities in the US. This experiment was conducted using 50% of the samples as training data. The small training set size (down 30 samples) can otherwise lead to overfitting. 15 Experiment Prompt Noise Levels, Population Density and Dangerous Neighborhoods"
        },
        {
            "title": "Building Age",
            "content": "Estimate the noise level, population density and how dangerous the neighborhood might be of the area shown in this image from 0 to 10. return the result without explanation Estimate the average property value of the area in the US from scale from 0 to 10: 3 meaning around 250k$ 6 meaning around 600k$ 9 meaning around 1.5m$ return the result without explanation Estimate the average building age of the area on scale from 0 to 10: 3 meaning around 1739 6 meaning around 1883 9 meaning around 1987 return the result without explanation Table 8. GPT4-o experiments and their corresponding prompts. Scene Area (km2) Latitude Bounds Longitude Bounds Sampling Year Rendered Images Buenos Aires (Argentina) Rotterdam (Netherlands) Amsterdam (Netherlands) The Hague (Netherlands) Utrecht (Netherlands) Eindhoven (Netherlands) Groningen (Netherlands) Maastricht (Netherlands) San Juan (Puerto Rico) Detroit (USA) Miami Beach (USA) Seattle (USA) Boston (USA) San Francisco (USA) Los Angeles 5.20 1.68 1.99 1.70 1.78 1.35 1.10 2.20 3.45 4.12 3.18 2.10 3.83 1.98 2.67 [-58.3801, -58.3593] [51.9088, 51.9194] [52.3698, 52.3809] [52.0782, 52.0887] [52.0818, 52.0929] [5.42727, 5.44250] [6.57495, 6.59036] [5.68648, 5.70744] [-66.0883, -66.0707] [-83.0038, -82.9789] [-80.1444, -80.1272] [-122.39508, -122.36096] [-70.99674, -70.96593] [-122.16672, -122.15059] [-117.71718, -117.69846] [-34.6041, -34.5803] [4.4542, 4.4741] [4.8937, 4.9174] [4.3073, 4.3285] [5.0987, 5.1197] [51.43233, 51.44241] [53.21107, 53.21964] [50.8425, 50.8525] [18.4475, 18.4642] [42.3467, 42.3648] [25.7664, 25.7831] [47.49694, 47.51248] [42.36831, 42.39076] [37.67978, 37.69241] [33.61083, 33.62591] 2021 - 2023 2019 - 2023 2021 - 2023 2020 - 2023 2017 - 2019 2015 - 2023 2024 2011 - 2023 2016 2019 - 2023 2018 - 2022 2018 - 2023 2018 - 2021 2022 - 2023 2017 - 2024 14261 5704 6597 6520 6527 8946 7310 12390 9369 9649 9377 12834 14800 9822 7610 Table 9. Scene information. Sampling year indicates the time underlying footage for the reconstruction was taken according to Google Earth [10]. 16 Figure 14. Visualization of zero-shot property price predictions (left) vs ground truth (right) by OpenCity. Basemaps are from CartoDB [6]. 17 Figure 15. Visualization of zero-shot property price predictions (left) vs ground truth (right) by OpenCity. Basemaps are from CartoDB [6]. Figure 16. Visualization of zero-shot building age predictions (left) vs ground truth (right) by OpenCity. Basemaps are from CartoDB [6]. 18 Figure 17. Visualization of zero-shot building age predictions (left) vs ground truth (right) by OpenCity. Basemaps are from CartoDB [6]."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Stanford University",
        "University of Zurich"
    ]
}