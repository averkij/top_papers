{
    "paper_title": "MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models",
    "authors": [
        "Huanqia Cai",
        "Yijun Yang",
        "Winston Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "IQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence research currently lacks systematic benchmarks to quantify these critical cognitive dimensions in multimodal systems. To address this critical gap, we propose MM-IQ, a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms. Through systematic evaluation of leading open-source and proprietary multimodal models, our benchmark reveals striking limitations: even state-of-the-art architectures achieve only marginally superior performance to random chance (27.49% vs. 25% baseline accuracy). This substantial performance chasm highlights the inadequacy of current multimodal systems in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 8 9 6 0 0 . 2 0 5 2 : r MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models Huanqia Cai Yijun Yang Winston Hu"
        },
        {
            "title": "Tencent Hunyuan Team",
            "content": "Figure 1: Left: Performance (accuracy) of top-performing multimodal models and humans across eight reasoning paradigms of MM-IQ. Right: Visual examples of eight reasoning paradigms of MM-IQ (Detailed information can be found in Section 3.2)."
        },
        {
            "title": "Abstract",
            "content": "IQ testing has served as foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence research currently lacks systematic benchmarks to quantify these critical cognitive dimensions in multimodal systems. To address this critical gap, we propose MM-IQ, comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms. Through systematic evaluation of leading open-source and proprietary multimodal models, our benchmark reveals striking limitations: even state-of-the-art architectures achieve only marginally superior performance to random chance (27.49% vs. 25% baseline accuracy). This substantial performance chasm highlights the inadequacy of current multimodal systems in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide. Homepage: acechq.github.io/MMIQ-benchmark/"
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of large multimodal models (LMMs) has intensified debates about their capacity for human-like abstraction and reasoning. While existing benchmarks evaluate specialized capabilities such as OCR, object localization, and medical image analysis [11, 26, 10], these taskspecific metrics fail to quantify the critical cognitive dimensions in multimodal systems. This caihuanqia19@mails.ucas.ac.cn Preprint. Technical Report. limitation mirrors long-standing challenge in human cognitive assessment: early methods conflated domain knowledge with innate reasoning ability until IQ testing emerged to isolate core cognitive competencies through languageand knowledge-agnostic evaluations [18]. Inspired by this paradigm, we argue that multimodal intelligence evaluation should also similarly decouple linguistic proficiency and task-specific knowledge from the measurement of abstract reasoning capacities. Abstract Visual Reasoning (AVR) offers plausible solution to the above challenge. As shown in Figure 4, AVR problems usually contain visual puzzles with simple 2D/3D shapes. Solving these problems requires identifying and understanding the underlying abstract rules and generalizing them to novel configurations. Although there exists wide range of AVR benchmarks, e.g., RAVEN [27], Bongard-LOGO [16], and SVRT [5], most of them have limited input modalities, reasoning paradigms, and restricted problem configurations, which can lead to biased evaluation results [22]. To this end, we propose MM-IQ, comprehensive AVR benchmark comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms. Like human IQ tests, MM-IQ fully eliminates domain-specific and linguistic biases while systematically diversifying problem configurations to prevent pattern memorization, presenting striking challenges for LMMs: even state-of-the-art models achieve only 27.49% accuracy, marginally exceeding random chance (25%) but far below human-level performance (51.27%). This substantial performance chasm highlights the inadequacy of current LMMs in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide. By applying IQ-testing principles to multimodal models, MM-IQ fills critical gap in existing multimodal benchmarks, e.g., MMBench [10] and MMMU [26] that focus on broad task coverage rather than core reasoning abilities. Our results demonstrate that current architectures lack the intrinsic abstraction abilities necessary for human-like intelligence, shedding light on potential directions toward developing systems capable of genuine cognitive adaptation."
        },
        {
            "title": "2 Related Work",
            "content": "Following [14, 7, 13], all existing AVR benchmarks, including our MM-IQ, can be cataloged along three dimensions: input shape, problem configuration, and reasoning paradigm, as shown in Table 1. Input shape refers to the input forms of the objects in the given image, which contributes to evaluating models cognition abilities of different shapes. Diverse problem configurations assess models abstract reasoning capabilities across multi-dimensional aspects, including pattern recognition (Ravens Progressive Matrices [17]), analogical transfer ability (Visual Analogy [6]), discrimination ability (Odd-one-out [15]), extrapolation and generalization ability (Visual Extrapolation [24]), and numerical reasoning ability (Arithmetic Reasoning [29]), etc. MM-IQs inclusion of diverse problem configurations ensures thorough evaluation of multimodal models abstract reasoning capabilities across various AVR problems. Reasoning paradigm is more fine-grained category that evaluates LMMs abstract reasoning capabilities, like logical deduction, temporal and spatial cognition, geometric, etc. It includes various reasoning paradigms such as temporal movement, spatial relationships, logical operations, and both 2D and 3D geometry, which are based on the internal forms, relationships, and numbers of objects in the given image. Existing benchmarks have only three paradigms on average except for MARVEL, which has five ones, but its quantity is relatively small. Although RAVEN [27], G-set [15], VAP [6], and DOPT [24] have more than 1,000 instances, all of their data are generated by computer programs, which lack diversity and complexity [4]. MM-IQ comprises total of 2,710 meticulously selected problems, 3x larger than MARVEL, and covers diverse spectrum of 8 fine-grained reasoning paradigms."
        },
        {
            "title": "3 Construction of MM-IQ",
            "content": "Two features distinguish MM-IQ from other existing benchmarks for LMMs: (1) MM-IQ adopts data from professional and authoritative examinations and performs rigorous quality control, which ensures its correctness and validity; (2) MM-IQ is comprehensive AVR benchmark for evaluating the intelligence of LMMs, comprising total of 2,710 problems and covering diverse spectrum of 8 fine-grained reasoning paradigms. 2 Input Shape Geometric Abstract Concrete Object Problem Configuration Reasoning Paradigm Ravens Progressive Matrices [17] Visual Analogy [6] Odd-one-out [15] Visual Extrapolation [24] Arithmetic Reasoning [29] Visual Grouping Temporal Movement Spatial Relationship 2D-Geometry 3D-Geometry Logical Operation Concrete Object Visual Instruction mathematics RAVEN G-set VAP SVRT DOPT ARC MNS IQTest MARVEL MM-IQ 2,710 Dataset Size 14, 1,500 100,000 23 95, 600 - 770 Table 1: Comparison between our MM-IQ and related benchmarks: RAVEN [27], G-set [15], VAP [6], SVRT [5], DOPT [24], ARC [4], MNS [29], IQTest [12], MARVEL [7]. denotes that the dataset is automatically produced through procedural content generation. 3.1 Data Collection The collection of MM-IQ involves three stages. Initially, we examined existing AVR datasets [27, 15, 4, 16] and discovered that most of them are generated by hand-coded procedures. Although programmatic synthesis can produce substantial amounts of data, it often lacks the necessary diversity. Hence, we chose to collect AVR problems from existing resources. Following [9, 7, 28], we collected problems from publicly available questions of the National Civil Servants Examination of China. These problems are specifically designed to evaluate civil servant candidates critical thinking and problem-solving skills, and they meet our criteria for both quantity and diversity. The collected data underwent rigorous filtering process conducted by two human annotators to eliminate any low-quality entries. The filtering principle is that the problems can be solved only by the extraction and utilization of high-level abstract reasoning information based on visual inputs. To create systematic and comprehensive benchmark, we proceeded to the second stage, which involved classifying the data into different paradigms and further adding more problems to those with fewer instances. Based on the descriptions of collected problems, we classified them into the corresponding reasoning paradigms. Additionally, we identified the common attributes of each paradigms problems, such as attributes and entity types, and supplemented those with fewer instances to ensure that each fine-grained attribute or entity type had sufficient problems. The final stage involved more thorough cleaning of the collected data through deduplication and extraction of the final answers. We performed deduplication in two ways. The first way was to employ the MD5 hashing algorithm to find the same images and removed them if their input text was the same. Secondly, we utilized the problems corresponding information, where similar ones were considered suspected duplicates, and then reviewed by human annotators based on the input image and corresponding information to identify and eliminate duplications. Additionally, the final answers were extracted by human annotators to facilitate efficient evaluation later. To further support the development of the open-source community, we also translated all content of questions and answers from Chinese to English based on GPT-4, resulting in bilingual version of the dataset. All translations were verified by humans to ensure their correctness. Specifically, the data distribution of the reasoning paradigms is shown in the Fig. 13, where concrete object and visual instruction are less than 2% since they are rare in the existing data. 3.2 Reasoning Paradigms of MM-IQ For simplicity and consistency, we follow MARVEL, dataset evaluating LMMs AVR ability but 3x smaller than ours, and extend its taxonomy to 8 categories, including logical operation, mathematics, 2D-geometry, 3D-geometry, visual instruction, temporal movement, spatial relationship, and concrete object. Notably, we merge mathematical and quantity categories from MARVELs taxonomy into mathematics to align more closely with our taxonomy. Logical Operation refers to the application of logical operators, such as AND (conjunction), OR (disjunction), XOR (exclusive disjunction), etc. This reasoning process involves observing and summarizing the abstract logical operations represented in the given graphics to derive general logical rules, which can then be applied to identify the required graphics. An example of reasoning involving the AND operation is shown in Fig. 2. 2D-Geometry encompasses two distinct categories. The first category involves understanding the attribute patterns of the provided 2D geometric graphics, such as symmetry, straightness, openness, and closure, and making analogies or extrapolations based on these attributes. The second category focuses on graphic splicing, which entails identifying complete pattern that can be formed by assembling existing 2D geometric fragments. Together, these two types assess the capability of LMMs to perceive geometric shapes from both local and global perspectives. visualized example of 2D-geometry reasoning concerning the symmetry property is shown in Fig. 4. 3D-Geometry can be categorized into three categories. The first category assesses the capability of LMMs to perceive 3D geometry comprehensively by observing polyhedron and identifying the required view from specific direction. The second category is analogous to 2D graphic splicing, but it involves basic fragments and target objects that are three-dimensional in nature. The third category evaluates LMMs comprehension of the interior structure of 3D solid shape with the goal of identifying cross-sectional view of the solid. An example of 3D-geometry reasoning for the specific directional view is shown in Fig. 8. Visual Instruction employs visual cues such as points, lines, and arrows to highlight key areas necessary for solving visual puzzles. Unlike other reasoning paradigms, this approach allows test-takers to concentrate solely on these visual indicators rather than requiring comprehensive observation of the entire panel. visualized example of visual instruction reasoning with arrows is shown in Fig. 5. Temporal Movement focuses on changes in position or movement, including translation, rotation, and flipping. This paradigm encompasses several problem configurations discussed in Section 2, including Ravens Progressive Matrices, Visual Analogy, and Visual Extrapolation. visualized example of temporal movement reasoning involving rotation is shown in Fig. 9. Spatial Relationship examines the static relative positional relationships among objects. This paradigm also encompasses various problem configurations, including Ravens Progressive Matrices, Visual Analogy, Visual Extrapolation and Visual Grouping. An example of spatial relationship reasoning is shown in Fig. 6. Concrete Object involves real-world objects, such as vases, leaves, or animals, and requires LMMs to categorize these objects based on their characteristics, which may require external knowledge to solve. visualized example of concrete object reasoning is shown in Fig. 7. Mathematics evaluates LMMs ability to reason about quantity, numbers, and arithmetic operations through visual inputs. This paradigm contains two types of tasks. The first type involves perceiving basic graphical elements, such as points, angles, lines, and planes, and applying arithmetic operations to these elements. The second type involves identifying an arithmetic expression that is satisfied by the numbers in the given images and determining the missing number based on the four fundamental operators: addition, subtraction, multiplication, and division. Examples of the two types of mathematics reasoning are shown in Fig. 10 and Fig. 3. The intersection point is the basic element that used in Fig. 10. 4 Table 2: Model and Human Performance on MM-IQ (%). Abbreviations adopted: LO for Logical Operation; 2D-G for 2D-Geometry; 3D-G for 3D-Geometry; VI for Visual Instruction; TM for Temporal Movement; SR for Spatial Relationship; CO for Concrete Object. Model Mean LO Math 2D-G 3D-G VI TM SR CO LLaVA-1.6-7B [8] Deepseek-vl-7b-chat [3] Qwen2-VL-72B-Instruct [23] QVQ-72B-Preview [21] GPT-4o [1] Gemini-1.5-Pro-002 [20] Claude-3.5-Sonnet [2] 19.45 22.17 26.38 26.94 26.87 26.86 27.49 Open-Source LMMs 17.92 20.34 22.25 20.30 28.60 24.40 29.23 25.59 24.22 19.53 24.74 28. Proprietary LMMs 28.32 25.70 28.03 27.43 26.60 29.48 25.52 19.53 23.41 15.83 27.39 27.39 26.38 27.64 25.88 24.37 20.00 35.56 24.44 26.67 18.23 23.72 26.93 25. 17.82 24.75 32.67 22.77 18.42 15.79 23.68 34.21 26.67 24.44 35.56 25.69 31.17 25.69 27.72 25.74 27.72 50.00 39.47 42. Human Performance 51.27 61.36 45.03 60.11 47. 46.67 55.61 36.63 65.79 Prompt: Choose the most appropriate option from the given four choices to fill in the question mark, so that it presents certain regularity: LMMs Response: The first two panels in each row have similar parts to the third panel. In the first and second rows, the third panel is obtained by removing the differences and finding the similarities between the first two panels; the same rule is applied to the third row, and the small circle is obtained by removing the differences and finding the similarities between the first two panels. Only option meets the requirements. Therefore, option is selected. Ground Truth: Figure 2: visualized example of logical operation paradigm. Prompt: Choose the most appropriate option from the given choice: Option A: 4; Option B: 5; Option C: 6; Option D: 7. LMMs Response: By observing the numbers in the figure, we notice the following patterns: 6 - 4 = 8 - 6, 5 - 7 = 1 - 3, and 3 - 8 = -3 - 2. The pattern is that the number in the bottom right corner minus the number in the top left corner equals the number in the bottom left corner minus the number in the top right corner. Using this pattern, we can set up the equation: ? - 9 = 6 - 8. Solving for ?, we find that ? = 7. Therefore, we choose option D. Ground Truth: Figure 3: visualized example of mathematics paradigm. 5 Prompt: The option that best fits the given pattern of figures is ( ). LMMs Response: To determine the option that best fits the given pattern of figures, lets analyze the symmetry of each figure in the sequence: The first panel is cylinder, which has rotational symmetry around its vertical axis. The second panel is circle with two vertical lines inside, which has vertical symmetry. The third panel is rectangle inside square, which have both vertical and horizontal symmetry. The fourth panel is circle with two small circles inside, which has rotational symmetry around its center. Now, lets consider the options: A. trapezoid with an inside, which has vertical symmetry. B. recycling symbol, which has no symmetry. C. cube, which has multiple planes of symmetry. D. spiral, which has no symmetry. Based on the symmetry analysis, the option that best fits the pattern of figures is A, as it has rotational symmetry similar to the other figures in the sequence. Ground Truth: Figure 4: visualized example of 2D-geometry paradigm. Prompt: Choose the most appropriate option from the given four options to present certain regularity: LMMs Response: By looking at the images, we notice that each picture has arrows, so we focus on their direction. When we examine the figure in the question from left to right, we see that all the arrows point to acute angles. Among the four options, only option meets the requirement that the arrows point to acute angles. Therefore, option is the correct choice. Ground Truth: Figure 5: visualized example of visual instruction paradigm. 6 Prompt: Choose the most appropriate option from the given four options to present certain regularity: LMMs Response: As can be seen from the figure, the number of elements is the same, but the styles are different, mainly indicating that the positions and methods of placing multiple elements vary. We can identify the pattern in the three figures from the left box and then apply this pattern to the right box. All the figures in the square box on the left are composed of solid circles, triangles, and quadrilaterals. The positions of the solid circles are upper inner, lower, and lower inner, respectively; the positions of the triangles are upper outer, upper inner, and upper, respectively; and the positions of the quadrilaterals are lower, upper outer, and lower outer, respectively. By applying this pattern to the square box on the right, the figure at the question mark should be solid trapezoid in the lower inner position, rhombus in the upper position, and circle in the lower outer position, which corresponds to option D. Therefore, the correct option is option D. Ground Truth: Figure 6: visualized example of spatial relationship paradigm. Prompt: Choose the most appropriate option from the given four choices to fill in the question mark, so that it presents certain regularity: LMMs Response: To solve this puzzle, we need to identify the pattern or regularity in the given grid. Looking at the grid: 1. The first row contains hat, roller skates, and gloves, which are all wearable items. 2. The second row contains radio, gramophone, and television, which are all related to audio-visual media. 3. The third row contains tools (screwdrivers), stool, and the missing item. The pattern seems to be categorization based on function or use. The third row appears to be related to household or utility items. Given the options: - A: Fire extinguisher - B: Pills - C: Soccer ball - D: Padlock The most appropriate choice to complete the pattern of household or utility items is the padlock (D). Ground Truth: Figure 7: visualized example of concrete object paradigm. 7 Prompt: The one that matches the top view is: LMMs Response: The top view is the view obtained by orthographic projection from the top of the object, also called the top view. Analysis options: A: There is hollow cylinder in the three-dimensional figure that is not reflected, so it is excluded; B: The hollow cylinder in the three-dimensional figure is only partially reflected, which is incomplete, so it is excluded; C: There is hollow cylinder in the middle of the bottom of the three-dimensional figure. The visible lines are represented by solid lines, and the invisible but real lines are represented by dotted lines. This is correct; D: When observing the top row of the three-dimensional figure from top to bottom, there should be only 3 solid line surfaces, so it is excluded. Therefore, the final choice is option C. Ground Truth: Figure 8: visualized example of 3D-geometry reasoning. Prompt: Choose the most appropriate option from the given four choices to fill in the question mark, so that it presents certain regularity: LMMs Response: By observing the images, we can see that in the first row, image 1 is rotated 90 degrees clockwise to produce image 2, and then image 2 is flipped to the right to create image 3. The second row follows the same pattern. Applying this same logic to the third row, we flip image 2 to the right, which allows us to obtain option C. Therefore, the final choice is option C. Ground Truth: Figure 9: visualized example of temporal movement paradigm. Prompt: Choose the most appropriate option from the given four choices to fill in the question mark, so that it presents certain regularity: LMMs Response: Based on the pattern observed, each figure in the sequence has 7 intersections. After analyzing the options, only option meets this requirement. Therefore, the correct answer is option A. Ground Truth: Figure 10: visualized example of mathematics paradigm. Prompt: The option that best fits the given pattern of figures is ( ). Response: To solve this problem, we need to analyze the pattern in the given sequence of figures and then determine which of the options (A, B, C, or D) best continues that pattern. Looking at the sequence: 1. We start with simple line connecting two points. 2. Then we have triangle with 3 points. 3. Next is shape with 4 points, forming square with crossed lines inside. 4. The fourth shape has 5 points, forming an irregular pentagon. The pattern seems to be increasing the number of points by one each time, while also increasing the complexity of the connections between these points. Following this pattern, the next shape should have 6 points. Looking at the options: A. star shape with 5 points - doesnt fit the pattern as it doesnt increase to 6 points. B. house-like shape with 6 points - fits the number of points, but the connections are simpler than the previous shapes. C. hexagon with interconnected points - this fits both the number of points (6) and the complexity of connections. D. An irregular shape with 6 points - fits the number of points, but the connections dont follow the increasing complexity. Therefore, the option that best fits the given pattern is C. It has 6 points, continuing the increasing point count, and its interconnected structure follows the pattern of increasing complexity in how the points are connected. Final Answer: Figure 11: An example of Claude-3.5-Sonnets prediction."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup We evaluate open-source and closed-source LMMs on the MM-IQ dataset with zero-shot prompting and employ the same question prompt for all models. The few-shot prompting results will be included in the future version of MM-IQ since how to design appropriate multimodal prompts is still an open problem [25, 19]. For open-source LMMs, we select widely used and state-of-the-art models, including QVQ-72B-Preview [21], Qwen2-VL-72B-Instruct [23], Deepseek-VL-7B-Chat [3], and LLaVA-1.6-7B [8]. For closed-source LMMs, we adopt GPT-4o-2024-08-06 [1], Gemini-1.5-Pro002 [20], and Claude-3.5-Sonnet-2024-06-20 [2]. For fair comparison, we employ the same settings and default hyper-parameters for all LMMs (please refer to Table 3 for more details). Each model generates single response to each problem in the dataset. The evaluation process of LMMs consists of three steps: (1) response generation, (2) answer extraction, and (3) accuracy calculation. We extract the final answer using regular expression (regex) matching. For example, the final answer will be extracted from the response The correct answer is A. as A. If there is no valid answer in the models response, it will be considered incorrect. Table 3: Generating parameters for various LMMs. Model Generation Setup Claude-3.5-Sonnet-2024-06temperature = 1.0, output_token_limit = 8,192, top_p = 1.0 GPT-4o-2024-08-06 Gemini-1.5-Pro-002 DeepSeek-vl-7b-chat temperature = 1.0, output_token_limit = 16,384, top_p = 1.0 temperature = 1.0, output_token_limit = 8, temperature = 1.0, output_token_limit = 2,048, do_sample = False, top_p = 1.0 LLaVA-1.6-7B temperature = 0, output_token_limit = 2,048 Qwen2-VL-72B-Instruct QVQ-72B-Preview temperature = 1.0, output_token_limit = 8,192, top_p = 0.001, top_k = 1, do_sample = True, repetition_penalty = 1. temperature = 0.01, output_token_limit = 8,192, top_p = 0.001, top_k = 1, do_sample = True, repetition_penalty = 1.0 4.2 Overall Performance According to the results from Table 2, we have the following conclusions. Firstly, human performance significantly outperforms all LMMs, achieving an average accuracy of 51.27%, while the best LMM Claude-3.5-Sonnet only achieves 27.49%. This substantial gap highlights LMMs limitations in AVR tasks and underscores the necessity of our MM-IQ dataset. By comparing small LMMs (7B) with larger ones (72B), we find that increased model size improves performance, from an average accuracy of 20.81% to 26.66%. We further compare the performance between open-source and proprietary models and find that the 72B ones (averaging 26.66%) can achieve comparable performance with proprietary models (averaging 27.07%), highlighting the potential of the open-source community. Secondly, several noteworthy phenomena are revealed by the more comprehensive analysis of the results across different reasoning paradigms. Among these paradigms, humans and closed-source LMMs perform better in object-concrete reasoning. Humans achieve an accuracy of 65.79%, while GPT-4o achieves 50%. Their scores are significantly higher than other models, especially the opensource ones. The object-concrete reasoning may require additional knowledge since the objects of the images are concrete. This observation may align with MMbench, which argues that proprietary models significantly outperform the open-source ones on tasks requiring additional knowledge, like celebrity recognition, physical property reasoning, natural relation reasoning, etc. The hardest paradigm for LMMs is the logical operation, which only scores at 23.69% average, because the solving of logical operation needs to identify more fine-grained relationships between multiple objects 10 and extract high-level abstract rules, like AND, OR, and XOR, raising significant challenge to LMMs. 4.3 Failure Analysis of LMMs on MM-IQ Figure 12: Distribution over three representative LMMs human-annotated errors. Figure 13: Data distribution of reasoning paradigms of MM-IQ. Table 2 demonstrates that the highest accuracy of LMMs (27.49%) is almost equivalent to randomly guessing correct answer among four options, which motivates us to ask: Does the strongest LMM, e.g., Claude, actually possess the reasoning abilities required by AVR tasks? To investigate this, we selected three representative models: Claude-3.5-Sonnet, Qwen2-VL-72B-Instruct, and LLaVA1.6-7B, and examined their generated wrong responses through human-in-the-loop evaluation. We sampled total of 90 predictions from each model for analysis. The 90 problems include 10 instances drawn from each reasoning paradigm and 20 instances from the mathematics paradigm, as the mathematics paradigm is significantly larger than the other paradigms, constituting 34.5% of the entire MM-IQ dataset. First of all, we take an in-depth look at the average length of predictions and their response styles. Compared to LLaVA-1.6-7B and Qwen2-VL-72B-Instruct, the best-performing LMM, Claude3.5-Sonnet, tends to generate longer responses. Moreover, Claude-3.5-Sonnets responses share consistent structure: they first offer detailed caption of the given image and the possible abstract reasoning paradigms, and then discuss each option to identify the correct answer. visual example of Claude-3.5-Sonnets response is illustrated in Fig. 11. In contrast, LLaVA-1.6-7B and Qwen2VL-72B-Instruct fail to generate responses in structured manner. These observations suggest that structured outputs may enhance reasoning performance. Furthermore, we examined each wrong response and categorized them into three types: incorrect reasoning, incorrect visual understanding, and incorrect final answers, examples of which can be found in Fig. 15, Fig. 14 and Fig. 17. As shown in Fig. 12, incorrect paradigm reasoning constitutes major part of failures (32.3% on average). In these responses, we observe that LMMs tend to solve problems by considering simpler wrong rules or focusing on more superficial changes rather than extracting higher-level abstract rules. Examples of simpler rules include objects in the image becoming progressively more compact or dense, and increasingly complex or detailed. corresponding visualized example is provided in Fig. 15, where the red parts indicate the incorrect reasoning due to wrongly recognizing simpler rules. Enhancing LMMs ability to perceive more fine-grained image details and accurately identify abstract paradigms is critical to address these shortcomings. Additionally, as shown in Fig. 12, Qwen2-VL-72B-Instruct and LLaVA-1.6-7B exhibit an additional error type compared to Claude-3.5-Sonnet: incorrect final answers, which accounts for nearly onethird of the errors. To further investigate whether the absence of explanations is critical factor, we calculated the accuracy of all responses without explanations. Notably, for the top-performing model, Qwen2-VL-72B-Instruct, directly providing the final answer results in performance drop of 4.7% (from 26.9% to 22.5%) on average. Conversely, for LLaVA-1.6-7B, it leads to an improvement of 2.8% (from 19.4% to 22.2%) on average. This underscores that generating detailed reasoning chains helps improve the performance of larger models. 11 Figure 14: Proportions of incorrect visual understanding across eight reasoning paradigms. Prompt: Which of the following options conforms to the given pattern of changes in the figures? LMMs Response: To solve this problem, we need to analyze the pattern in the top row and then find which option in the bottom row follows that pattern. In the top row, we see: 1. circle inside triangle inside square 2. The same shapes, but the triangle is cut off on one side 3. The same shapes, but now the circle is cut off 4. new pattern of intersecting squares 5. The same intersecting squares, but with an additional square added The pattern seems to be adding complexity or elements as we move from left to right. Now, looking at the options A, B, C, and D: A: Shows two intersecting squares B: Shows three intersecting squares C: Shows four intersecting squares D: Shows five intersecting squares Following the pattern of increasing complexity and number of elements, the correct answer should be D. It shows the most complex arrangement with five intersecting squares, which follows the progression we see in the top row. Therefore, the answer that best conforms to the given pattern is option D. Final Answer: Ground Truth: Figure 15: An example of incorrect paradigm reasoning. The red parts are the incorrect reasoning due to wrongly recognizing simpler rules. The correct reasoning solution is that the intersection in the third panel is the sum of the intersections in the first two panels in the same box. 12 Prompt: Choose the most appropriate one from the given four options to present certain regularity: LMMs Response: To solve this problem, we need to analyze the pattern in the given sequence of shapes and then determine which of the options A, B, C, or best continues that pattern. Lets examine the sequence: 1. Star (5-pointed) 2. Shape with two curved lines 3. Shape with two straight lines forming an acute angle 4. Shape with one curved line (quarter circle) 5. Shape with one straight line The pattern seems to be progression from more complex shapes to simpler ones, reducing the number of lines or points: - From 5 points (star) to 2 lines, then to 1 line - From curved lines to straight lines Following this pattern, the next shape should be even simpler, possibly just single straight line or single point. Looking at the options: A: Curved line B: Straight line C: Two intersecting straight lines D: Single point Option (straight line) or (single point) would be the most logical continuations of the pattern. However, since weve already seen straight line in the sequence, the single point (Option D) represents further simplification and reduction, making it the most appropriate choice to continue the observed pattern. Therefore, the best answer is D. Final Answer: Ground Truth: Figure 16: An example of incorrect visual understanding. The red parts are the incorrect visual understanding. Prompt: Choose the only option from the four figures to fill in the question mark, so that it presents certain regularity. LMMs Response: The only option that presents certain regularity is B. Final Answer: Ground Truth: Figure 17: An example of incorrect final answers. 13 Finally, we conducted deeper analysis of incorrect visual understanding, which subsequently leads to reasoning errors. As shown in Figure 14, we found that all three models perform consistently poorly on certain paradigms, such as logical operation, temporal movement, and spatial relationship, due to the graphics in the image being more complex. Moreover, we found that the proportion of incorrect visual understanding is inversely proportional to the performance of the model. For instance, Claude3.5-Sonnet performs poorly on temporal movement and spatial relationship reasoning paradigms, and also performs worse on visual understanding of both paradigms. This underscores the necessity of enhancing the models perceptual capacity to accurately interpret complex visual paradigms, thereby improving LMMs reasoning capabilities. In summary, our failure analysis of LMMs on the MM-IQ dataset highlights several critical points for further research and improvement in multimodal abstract reasoning: 1) Structured response generation: Models like Claude-3.5-Sonnet, which produce longer and more structured responses, tend to perform better, suggesting that enhancing the ability to generate structured and detailed reasoning chains can improve accuracy. 2) Abstract pattern recognition: significant portion of errors stems from incorrect reasoning due to reliance on simpler rules. Improving models ability to identify and apply high-level abstract paradigms is essential. 3) Visual understanding: All models exhibit poor performance on complex visual paradigms, such as logical operations and spatial relationships, indicating need for enhanced perceptual capabilities to accurately interpret intricate visual details. 4) Explanatory vs. concise answers: The presence of detailed explanations can improve performance in stronger models but may not benefit weaker ones, highlighting the nuanced role of explanatory reasoning in model accuracy. Addressing these challenges is crucial for advancing the reasoning capabilities of LMMs."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose MM-IQ, comprehensive benchmark for evaluating the abstract visual reasoning of LMMs. It covers diverse range of 2,710 AVR problems across 8 distinct reasoning paradigms, enabling rigorous assessment of LMMs abstraction and reasoning capabilities. Experimental results reveal striking limitations in current state-of-the-art LMMs, with the leading models achieving only slightly above the accuracy of random guessing, far behind human performance. We conduct thorough failure analysis that identifies several key points for improvement, including structured reasoning, abstract pattern recognition, visual understanding, and inference-time scaling. MM-IQ is expected to complement existing multimodal benchmarks and provide valuable resource for steering progress in multimodal research and promoting the advancements of AGI."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Anthropic. The Claude 3 Model Family: Opus, Sonnet, Haiku. https://www-cdn.anthropic. 2024. com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, Accessed: 2025-01-11. [3] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. [4] François Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019. [5] François Fleuret, Ting Li, Charles Dubout, Emma Wampler, Steven Yantis, and Donald Geman. Comparing machines and humans on visual categorization test. Proceedings of the National Academy of Sciences, 108(43):1762117625, 2011. [6] Felix Hill, Adam Santoro, David GT Barrett, Ari Morcos, and Timothy Lillicrap. Learning to make analogies by contrasting abstract relational structure. arXiv preprint arXiv:1902.00120, 2019. 14 [7] Yifan Jiang, Jiarui Zhang, Kexuan Sun, Zhivar Sourati, Kian Ahrabian, Kaixin Ma, Filip Ilievski, and Jay Pujara. Marvel: Multidimensional abstraction and reasoning through visual evaluation and learning. arXiv preprint arXiv:2404.13591, 2024. [8] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/. [9] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124, 2020. [10] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2025. [11] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. [12] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [13] Mikołaj Małkinski and Jacek Mandziuk. Deep learning methods for abstract visual reasoning: survey on ravens progressive matrices. arXiv preprint arXiv:2201.12382, 2022. [14] Mikołaj Małkinski and Jacek Mandziuk. review of emerging research directions in abstract visual reasoning. Information Fusion, 91:713736, 2023. [15] Jacek Mandziuk and Adam Zychowski. Deepiq: human-inspired ai system for solving iq test problems. In 2019 International Joint Conference on Neural Networks (IJCNN), pages 18. IEEE, 2019. [16] Weili Nie, Zhiding Yu, Lei Mao, Ankit Patel, Yuke Zhu, and Anima Anandkumar. Bongardlogo: new benchmark for human-level concept learning and reasoning. Advances in Neural Information Processing Systems, 33:1646816480, 2020. [17] Jean Raven. Raven progressive matrices. In Handbook of nonverbal assessment, pages 223237. Springer, 2003. [18] RE Snow. The topography of ability and learning correlations. Advances in the psychology of human intelligence/Erlbaum, 1984. [19] Yan Tai, Weichen Fan, Zhao Zhang, and Ziwei Liu. Link-context learning for multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2717627185, 2024. [20] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [21] Qwen Team. Qvq: To see the world with wisdom, December 2024. URL https://qwenlm. github.io/blog/qvq-72b-preview/. [22] Han LJ Van der Maas, Lukas Snoek, and Claire Stevenson. How much intelligence is there in artificial intelligence? 2020 update. Intelligence, 87:101548, 2021. [23] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 15 [24] Taylor Webb, Zachary Dulberg, Steven Frankland, Alexander Petrov, Randall OReilly, and Jonathan Cohen. Learning representations that support extrapolation. In International conference on machine learning, pages 1013610146. PMLR, 2020. [25] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. [26] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [27] Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: dataset for relational and analogical visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53175327, 2019. [28] Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, et al. Cmmmu: chinese massive multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2401.11944, 2024. [29] Wenhe Zhang, Chi Zhang, Yixin Zhu, and Song-Chun Zhu. Machine number sense: dataset of visual arithmetic problems for abstract and relational reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13321340, 2020."
        }
    ],
    "affiliations": []
}