{
    "paper_title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling",
    "authors": [
        "Hayeon Kim",
        "Ji Ha Jang",
        "Se Young Chun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing. Code is available at https://janeyeon.github.io/romap."
        },
        {
            "title": "Start",
            "content": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling Hayeon Kim1, Ji Ha Jang1, Se Young Chun1,2, 1 Dept. of Electrical and Computer Engineering, 2 INMC & IPAI Seoul National University, Republic of Korea {khy5630, jeeit17, sychun}@snu.ac.kr 5 2 0 J 1 2 ] . [ 2 1 6 0 1 1 . 7 0 5 2 : r Figure 1. Enhanced controllability in 3D Gaussian part-level editing achieved with RoMaP, surpassing prior arts. RoMaP enables highly controllable and localized part-level edits, allowing even for unconventional modifications such as emerald nose or modifications requiring high-level controllability such as blue left eye, right green eye while maintaining global consistency. In contrast, existing baselines perform well for instance-level editing, but struggle with part-level editing, especially with drastic changes."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in 3D neural representations and instancelevel editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3DGALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations *Authors contributed equally. Corresponding author. across viewpoints. Second, we propose regularized SDS loss that combines the standard SDS loss with additional In particular, an L1 anchor loss is introregularizers. duced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing. Code is available at https://janeyeon.github.io/romap. 1. Introduction Recent advances in 3D neural representations [25, 39, 50, 61, 66] and generative models [19, 53] have enabled efficient, high-quality 3D content creation, increasingly vital for industries such as mixed reality and robotics. Unlike traditional, labor-intensive methods, text-to-image diffusion models [10, 38, 47, 48] generate contents from text prompts, potentially reducing production costs and effort significantly. Enhancing controllability in 3D content generation is crucial for customizing these assets. Text-guided editing methods [5, 6, 9, 40, 62, 64] enhance this by enabling flexible expression of abstract and specific concepts while enabling edits at various levels of detail. Local 3D editing involves modifying part-level attributes like texture and color, or replacing parts. While previous works [5, 6, 9, 12, 17, 60, 62] have achieved excellent performance in instance-level 3D editing, local 3D editing remains challenging (See Fig. 1). Prior methods [5, 6, 60, 62] often use 2D segmentation [28] to localize changes and apply 2D multi-view editing [2] for 3D modifications. However, these approaches face two major challenges for partlevel modifications, often leading to inaccurate or no edit. First, achieving consistent 3D editing across multiple views requires precise masking to preserve unchanged regions, typically relying on 2D multi-view image segmentation. However, compared to instance segmentation, part segmentation is challenging due to occlusions and variExisting apations in appearance across viewpoints. proaches [5, 6, 60, 62] leverage language-based SAM [28] to segment target parts in multi-view images and re-project them onto 3D for editing. While 2D instance segmentation remains consistent across views, part-level segmentation is much less reliable (e.g., some views may capture only one eye, merge both, or miss them entirely), resulting in unstable and incomplete masks, as shown in Fig. 2. Additionally, assigning hard segmentation label to each Gaussian from 2D map may be inappropriate, as Gaussians at part boundaries could represent different parts depending on the view, thus resulting in mixed soft-labels. Second, part-level 3D editing remains challenging as existing models struggle to isolate and modify specified parts [2] or handle semantically low-probability edits [58]. Learned part-instance correlations often cause unintended changes or failures when the target attribute deviates from the original context. As shown in Fig. 2, InstructPix2Pix [2], widely used for 2D editing in prior works [5, 6, 12, 17], excels in instance edits but struggles with part edits. Instead of applying precise direct changes to the eyes, the model alters the background to green and turns the eyes blue, as odd-eye coloration is rare in human faces, making the edit statistically more likely. Moreover, achieving such fine-grained control remains highly challenging. To address this challenge, we introduce RoMaP, novel part-level 3D editing framework that enables precise and substantial local modifications for Gaussian. RoMaP comprises two core components: (1) robust 3D mask generation module with 3D-Geometry Aware Label Prediction (3D-GALP): 3D-GALP leverages spherical harmonics (SH) coefficients to explicitly model view-dependent label variations, effectively capturing the mixed-label property of Gaussians. This results in accurate and consistent part segmentations across viewpoints, enabling reliable local ed- (2) regularized Score Distillation Sampling (SDS) its. loss: Our regularized SDS combines the standard SDS loss with additional regularizers, including an L1 anchor loss from Scheduled Latent Mixing and Part (SLaMP) edited images. SLaMP generates 2D multi-view images with drastic changes strictly confined to the target region, guiding SDS optimization toward the intended modification. Additionally, robust 3D masking prevents unintended changes. Gaussian prior removal allows flexible adjustments, and together they enable precise local 3D editing, even along rare or unconventional directions. Our RoMaP enables local 3D Gaussian editing, allowing diverse changes in specific areas. As seen in Fig. 1, our RoMaP achieved even drastic local edits, enabling unlikely or unconventional modifications while preserving the original identity, thereby enhancing controllability in 3D content editing. Our contributions are summarized as: Proposing RoMaP for precise and consistent local 3D Gaussian editing, enabled through our robust full 3D mask using our 3D-geometry aware label prediction, exploiting the uncertainty in soft-label Gaussians. Proposing regularized SDS loss, enabling drastic part edits with scheduled latent mixing part editing and robust masks, along with Gaussians prior removal. Experiments show that RoMaP enhances 3D Gaussian editing quality both qualitatively and quantitatively across reconstructed and generated Gaussian scenes and objects, improving controllability in 3D content generation. 2. Related Works 2.1. Diffusion and Rectified Flow based generation Recent advances in Diffusion Models (DMs) [13, 48] have greatly enhanced image generation, excelling in tasks like image editing, stylization [18, 24, 44, 63]. Rectified Flows (RFs) [36], flow-based approach [11], streamline diffusion by linearizing the its path, enabling more efficient training, faster sampling, and more accurate latent space inversion. Recent combinations of RF and Diffusion Transformer (DiT) [42] models, like FLUX and Stable Diffusion 3 (SD3) [13], have advanced high-quality image genFigure 2. Limitations of prior local 3D editing methods leveraging 2D part level segmentation and edits. Although existing 3D editing methods excel in instance level editing, they struggle with part level editing as part segmentation [28] (for eye) lacks view consistency, and 2D editing [2] often misplaces changes, turning wall green instead of the left eye. In contrast, our method achieves accurate 3D eye segmentation with geometric awareness and clearly defines modification direction, enabling successful 3D Gaussian editing. eration, benefiting applications like text-to-3D and image editing. These models enhance prompt-faithful editing and inversion by refining noise [65] and utilizing RFs linearity [49] but still lack part-level controllability. Similarly, prior works [31, 33, 65] employ these models in text-to-3D, achieving high-fidelity and faster convergence. Notably, SD3 has been applied to part-level controllable text-to-3D generation [33] but is limited to animals, leaving broader applications unexplored. Our approach enables previously unattainable drastic local 3D edits by leveraging the SD3s part-awareness and RFs linearity, allowing flexible edits across various reconstructed and generated Gaussians. 2.2. Editing of 3D Gaussian Splatting Editing 3D neural fields has advanced 3D generation by enhancing controllability, attracting research interest [17, 34]. Early works focused on Neural Radiance Field (NeRF) [12, 17, 29], but recent works have shifted toward 3D Gaussians for better local control and efficient rendering. Editing Gaussians requires both an editing and masking strategy to target specific parts. In editing strategy, some methods [5, 6, 62] edit 2D-rendered Gaussian images from multiple views using image editing models [2, 5, 56] and project them back onto Gaussians. However, this approach is limited by the constraints of the 2D editing model and causes inconsistencies in 3D projection. Others [40] directly update Gaussians using Score Distillation Sampling (SDS) loss, but struggle to make significant modifications due to its implicit characteristic [4, 6, 15, 23]. We first remove priors and set the modification direction with the SLaMPedited image, then refine for greater control beyond the original context. For masking strategies, most works utilize 2D masks for localized edits, projecting them onto Gaussians [6, 60, 62]. However, noisy multi-view 2D masks introduce inconsistencies, affecting unintended regions or preventing proper transfer of 2D changes to 3D. Also, Gaussians at part boundaries can represent different parts depending on the view. However, assigning 3D Gaussian labels based on 2D map overlooks this, resulting in inaccurate segmentation at part boundaries. To address this, our 3D-GALP selects anchors based on view-dependent label prediction consistency and enforces neighbor consistency in 3D, refining 3D masks to correct 2D imperfections. 2.3. Local editing of 3D representations Most 3D editing methods discussed in Sec. 2.2 focus on instance level modifications or scene wide style changes. Some extend this to local edits, enabling precise adjustments to specific parts for finer control and prompt adaptability. key challenge in local editing is effectively selecting specific areas. Some methods [9, 64] use bounding boxes from users or Large Language Models (LLMs) to make local changes, but these restrict selection to simple shapes, and their fixed nature prevents deformable edits. Figure 3. Overall pipeline of RoMaP. RoMaP first segments 3D Gaussian using 3D-GALP, leveraging the soft-label properties of Gaussians to address the intricacies of part-level segmentation. With anchors consisting of both label-consistent and inconsistent Gaussians, we refine 3D segmentation considering locality with neighboring Gaussians. Then, in local 3D editing, we first remove Gaussian priors and introduce new modification direction using SLaMP-edited images, followed by refinement via regularized SDS loss. Another work [22] utilizes pre-trained 3D GAN [3] with CLIP [46] to select local areas and generate changes in human and cat faces. While effective for some edits, it remains limited to specific targets and struggles with more drastic edits. Our model is the first to achieve part-level 3D editing for general objects in both reconstructed and generated Gaussians. By fully utilizing SD3 and Gaussian properties, RoMaP enables faithful and drastic 3D local edits. 3. Method We propose RoMaP, novel method for locally editing 3D Gaussians with text prompts, enabling targeted regional modifications. Existing approaches struggle with part edits because (1) projecting 2D segmentations to 3D is unreliable due to inconsistent part-aware models and ambiguous part boundaries, and (2) isolating specific parts is difficult due to entanglements in 2D diffusion models. To address these challenges, RoMaP first performs explicit local 3D segmentation by adopting view-dependent segmentation labels and resolving 2D segmentation inconsistencies using 3D Geometry-Aware Label Prediction (3DGALP), as discussed in Sec. 3.2. To enable drastic part edits beyond pre-existing contexts, we introduce new modification direction using regularized score distillation sampling, guided by regularizers: anchored L1 with Scheduled Latent Mixing and Part (SLaMP) editing, Gaussian prior removal and masking. This process is detailed in Sec. 3.3. The full pipeline of RoMaP is shown in Fig. 3. 3.1. Preliminary: 3D Gaussian Splatting Gaussian Splatting [25] is point-based method that represents 3D scene using Gaussian properties. Let Ω be set of Gaussians composing the scene, where each Gaussian Ωi is defined as Ωi = {pi, si, qi, αi, ci}, where pi, si, qi, αi, and ci represent the centroid, standard deviations, rotational quaternion, opacity, and spherical harmonics (SH) coefficients, respectively. The projected RGB color of Gaussians varies by viewpoint ϕ and is computed as cϕ = SH(c, ϕ), where SH(c, ϕ) evaluates the SH coefficients at ϕ. The rendered image Cϕ for view ϕ is obtained by projecting Ω onto 2D plane using the differentiable rasterization D: Ω = {p, s, q, α, c} ϕ cϕ = SH(c, ϕ) Cϕ. 3.2. Local 3D segmentation: 3D-GALP This section describes the Local 3D Segmentation on the left side of the Fig. 3. To localize changes in the target region, we create 3D segmentation M3D given Ω. The goal is to predict which regions of M3D correspond to each predefined part label lj. This involves two steps: attention map extraction and 3D geometry-aware label prediction (3D-GALP). Given segmentation prompt, we extract the attention map A(Cϕ) from randomly rendered view Cϕ and treat it as pseudo 2D segmentation map to guide 3D-GALP. More details on attention map extraction are in the supplementary material. Attention-based pseudo segmentation for 3D Gaussians In this step, we obtain the explicit 3D segmentation M3D using 3D-GALP, guided by A(Cϕ). Once constructed, M3D provides segmentation information for all Gaussians. To represent these labels, we introduce new parameter ri and incorporate it into the Gaussian representation: Ωi = {pi, si, qi, αi, ci, ri}. Since single Gaussian may correspond to different labels depending on the viewpoint, it exhibits mixed-label property. To model this view-dependent labeling, we represent each Gaussians label as SH coefficients. We interpret Rϕ, the 2D projection of Gaussians obtained via at view ϕ, as segmentation map: ϕ Ω = {p, s, q, α, c, r} rϕ = SH(r, ϕ) Rϕ. The learnable parameter is then optimized via L1(A(Cϕ), Rϕ) loss, encouraging the rendered map to align appropriately with the pseudo 2D attention map in the given view. While this process aligns Gaussians with the attention map across multiple views, the alignment may remain imperfect. To further refine segmentation, we apply an anchor-based neighbor consistency loss, with anchors sampled by considering label softness. Label-softness based anchor sampling Occlusions and view-dependent shape complexity can lead A(Cϕ) to produce incomplete segmentation maps (See Fig. 3). To achieve complete and view-consistent 3D segmentation, we refine the segmentation by leveraging the view-dependent label softness of Gaussians. Here, ri is treated as an SH color, and Gaussian is considered to exhibit label softness if rϕ varies with the viewpoint ϕ. To quantify the label softness, we measure vi, the variance of rϕ across ϕ. Then, we calculate the cosine similarity between ri, the mean color observed from all directions and lj, the label assigned to each part. We then compute the entropy as follows: where pij denotes the probability obtained from the cosine similarity between predicted label ri and ground truth label lj, while Hi denotes the entropy of pij. We define the softness of the label of each Gaussian as the product of Hi and vi, given by Si = Hi vi. As visualized in Fig. 4, Si is high at part boundaries, where Gaussians inherently exhibit softlabel properties. This is due to the 2D part segmentation map classifying Gaussians noisly around these boundaries. All Gaussians are sorted based by Si, then anchors are selected: the top K/2 from those with the highest softness values and the bottom K/2 from those with the lowest. This sampling method selects anchors from both Gaussians Figure 4. Effectiveness of label softness-based anchor sampling. By applying 3D loss with anchors sampled based on label softness, we observe that differentiation of boundaries between parts is much more precise compared to random sampling. with high soft-label properties and those with consistent labels, enabling refinement of 3D segmentation while preserving locality and effectively handling part boundaries. Fig. 4 shows that part boundaries can be segmented precisely with label-softness based sampling compared to random sampling. Anchor-based neighboring loss Given the selected anchors A, we enforce neighbor consistency by incorporating segmentation information from nearby Gaussians. For each anchor Ωi A, we find its nearest neighbors, where NK(i) denotes the top-k nearest neighbors of the i-th anchor. We then compute the L1 between the segmentation label rj of neighboring points and the ri of the anchor point: LGALP ="
        },
        {
            "title": "1\nK",
            "content": "(cid:88) iA (cid:88) kNK (i) ri rk . (2) As shown in Fig. 5, 3D-GALP effectively can segment various parts of diverse objects. Additional 3D segmentation results in various scenes are provided in the supplementary. 3.3. Local 3D Editing: Regularized score distillaRegularized score distillation sampling We can now explicitly select Gaussian regions for editing using M3D. Since the SDS loss primarily serves as an implicit objective but has limited direct impact on 3D Gaussians [4, 6, 15, 23], we enable more effective modifications by introducing regularized SDS loss. This loss combines two regularizers: Gaussian prior removal and masking, and an anchoredbased L1 loss using SLaMP edited image. The regularized SDS loss is defined as: LR-SDS = λ1 ˆLSDS(cϕ pr)). (3) Here, λ1 and λ2 are hyperparameters that balance the conˆL denotes tribution of ˆLSDS and ˆL1 during training. pr, pedit) + λ2 ˆL1(cϕ pr, SLaMP(cϕ pij = rilj rilj rilj rilj (cid:80) , Hi = (cid:88) pij log(pij) (1) tion sampling Figure 5. 3D Gaussian segmentation results of 3D-GALP. With our 3D-GALP, 3D Gaussian segmentation accurately captures diverse object parts, addressing the limitations of 2D part segmentation and the inherent mixed nature of 3D Gaussian segmentation labels. key aspect of SLaMP is the scheduled blending of latents over time, enabling fine-grained control over the influence of the original image. Effective part-level editing requires isolating the target region while guiding it toward the desired change without compromising global identity. SLaMP achieves this by scheduling sharp transition in the blending ratio between the target latent zt and the original latent zt,orig. The resulting latent zt+1 is expressed as follows: zt+1 = zt(1Ft(1M2D))+zt,origFt(1M2D). (4) Here, Ft is time-dependent blending coefficient. We begin with low Ft to generate new context without strong influence from the original image. At timestep ts, we increase Ft sharply to preserve the alignment with original. As shown in Fig. 6, setting ts too low disrupts the original image context, while setting it too high hinders new content generation. To balance preservation and editing, we set ts to where SSIM is stable while CLIPdir remains high. More details are in the supplementary. 4. Experiments 4.1. Experimental setting Dataset and evaluation metrics To evaluate editing performance on reconstructed Gaussians, we use scenes from IN2N [17] and NeRF-Art [17], testing 75 editing prompts targeting different parts and changes in each scene. For evaluation metrics, we used two CLIP-based metrics, CLIP Similarity [46] and CLIPdir Similarity [14], to measure the overall fidelity between the input text and the edited scene. Furthermore, we used BLIP-VQA [21] and TIFA [20] to assess how well edits align with specific text prompt components via visual question answering. Baselines We compared RoMaP with three state-of-theart 3D Gaussian editing methods (DGE [5], GaussianEditor [6], and GaussCtrl [62]) and three NeRF editing methods (Instruct-Nerf2Nerf (IN2N) [17], ViCA-NeRF (ViCA) [12], and Posterior Distillation Sampling (PDS) [29]). All baselines perform 2D edits before lifting them to 3D. [5, 6, 12] Figure 6. Experiments on the effect of ts. ts controls the extent of deviation from the original. We set ts to induce drastic changes in the target region while preserving the surrounding identity. masked loss leveraging M3D and M2D to restrict changes only to intended regions. cϕ pr refers to the 2D projection of prior-removed Gaussians in view ϕ and SLaMP refers to our 2D part editing method that enables clear directional change that SDS loss cannot achieve. These two components will be discussed in following section. Regularizer 1: Gaussian prior removal and masking Due to the inherent ambiguity of SDS loss and the localized nature of Gaussians, applying SDS alone limits modification extent [4, 15]. To address this, we introduce an L1 loss on explicitly edited images to provide more targeted and controllable guidance. However, directly combining L1 with LSDS often induces overly broad changes, since LSDS operates in all directions and biases the optimization toward preserving strong appearance priors. To mitigate this, we perform Gaussian prior removal by replacing dominant color priors with neutral colors (e.g., white or gray), producing cϕ pr to discourage fixation on original appearances. Additionally, we explicitly prevent gradient updates to Gaussians on M3D, avoiding unintended changes and ensuring that edits are confined to the target regions. Regularizer 2: Anchored L1 with SLaMP edited image To generate an anchor image for the L1 loss, we propose SLaMP editing, part level editing strategy that balances localized modification with global identity preservation. Figure 7. Enhanced controllability in 3D asset generation with RoMaP. Our approach enables precise manipulation of specific 3D parts. As shown above, RoMaP provides diverse control over multiple narrow regions within single 3D object, allowing deformations in targeted areas like ducks beak or jellyfish hair and facilitating various modifications in targeted area such as lamps lampshade. Editing Methods Metrics CLIP CLIPdir B-VQA TIFA NeRF baselines IN2N [17] ViCA [12] PDS [29] 0.248 0.223 0.167 Gaussian Splatting baselines GaussCtrl [62] GaussianEditor [6] DGE [5] RoMaP (Ours) 0.182 0.179 0.201 0.277 0.072 0.048 -0.005 0.044 0.087 0.095 0.205 0.142 0.241 0. 0.190 0.370 0.497 0.723 0.634 0.427 0.212 0.432 0.571 0.565 0.674 Table 1. Quantitative comparison with 3D editing methods. Our method outperforms various baselines in multiple metrics. employ InstructPix2Pix [2], while [62] utilizes ControlNet [68], and [29] applies posterior distillation sampling. In user study, we compared RoMaP against generation models [7, 65, 67], assessing how editing improves controllability in generating previously difficult samples. 4.2. Experimental results Quantitative comparisons Tab. 1 presents quantitative comparison of RoMaP against 3DGS and NeRF editing models, where it outperforms all baselines across metrics. As shown in Tab. 2, user study further validates RoMaPs superior performance. Statistical significance of user study is confirmed by Friedman and pairwise Wilcoxon tests. Editing Method User Study Generation Method User Study GaussCtrl [62] GaussianEditor [6] DGE [5] RoMaP (Ours) 0.201 0.201 0.224 0. GSGEN [7] GaussianDreamer [67] RFDS [65] RoMaP (Ours) 0.203 0.198 0.234 0.365 Table 2. User study results. Quantitative comparison of user study results for editing and generation methods. Qualitative comparisons Fig. 8 shows qualitative results comparing RoMaP with 3DGS generation and editing methods. Ours enables drastic local changes, such as butterfly lips and goats head, while others fail. Its enhanced controllability also enables text-aligned generation that other models struggle with. As shown in Fig. 7, RoMaP enables diverse 3D creations, such as lamp with different bulbs and lampshades, simplifying 3D asset customization. Metrics Baseline + Mask CLIP CLIPdir 0.218 0.060 0.228 0.162 + Mask & ˆL1 0.267 0.205 Full (Ours) 0.277 0.205 Table 3. Ablation study results The ablation study shows results from sequentially adding key methods. 4.3. Ablation study Tab. 3 presents an ablation study validating each step of RoMaP. In Tab. 3, Mask refers to results using masks (M2D & M3D) generated from 3D-GALP. ˆL1 is the result of regularized SDS loss, by only employing the second term. The Full represent our full regularized SDS loss, enabling modification in the desired direction. This confirms the necessity of all steps in RoMaP. 5. Conclusion In this work, we introduce RoMaP, novel approach for local 3D Gaussian editing that enables precise and consistent part-level edits. To localize part accurately, we employ robust segmentation with geometry-aware label prediction, utilizing the soft-label properties of Gaussians. We also propose the regularized SDS loss using scheduled latent mixing and Gaussian prior removal, enabling drastic part-level edits while preserving remaining areas. Experimental results demonstrate RoMaPs significant improvements in 3D Gaussian editing quality across various scenes even in challenging scenarios. Figure 8. Comparison results The results of comparing our methodology with various reconstruction-based 3D editing methods and text-to-3D generation approaches are presented. In the reconstructed scene, our method enables drastic changes in very narrow regions, breaking the existing priors that other approaches have been unable to overcome. This allows for diverse transformations, such as replacing human face with goats face or substituting hair with butterflies. In the text-to-3D generation scenario, our approach achieves success in examples where naive text prompts alone fail, demonstrating its ability to generate wider range of 3D assets."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported in part by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) [NO.RS-2021-II211343, Artificial Intelligence Graduate School Program (Seoul National University)], the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. RS-2025-02263628) and AI-Bio Research Grant through Seoul National University. Also, the authors acknowledged the financial support from the BK21 FOUR program of the Education and Research Program for Future ICT Pioneers, Seoul National University."
        },
        {
            "title": "Supplementary Material for",
            "content": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling S.1. Additional details on quantitative results S.1.1. Experimental setting S.1.1.1. Comparison with 3D Gaussian editing models We collected human face scenes from the IN2N [17] and Nerf-Art [58] datasets. For each facial part: eyes, nose, mouth and hair, we applied five editing prompts: silvertextured, gold-textured, diamond, green, pink to evaluate editing success. Additionally, we designed five prompts requiring drastic changes: delicious croissant nose, hair made of metallic gears, steampunk style, hair on fire, red and blue flame, hair covered with beautiful butterfly, left blue and right green eye, and categorized them as hard to assess extreme editing performance. For models incorporating InstructPix2Pix [2] in their pipelines, we adapted the prompts to the format: Turn ... into .... S.1.1.2. Comparison with 3D Gaussian generation models To prove that our local 3D editing method enhances controllability in 3D content generation, we designed prompts for samples that were challenging for previous 3D generation methods to create. The prompts included: beautiful woman with cheeks beak, woman with cloudy hair, beautiful woman with butterfly hair, snail with skyscapes inside its shell, and vase with yellow tulip and stained glass-textured rose. We tasked 3D generation models with directly generating 3D content from these prompts. In our approach, we first generated the base objects, such as snail, then applied these prompts as editing instructions to assess whether our method could successfully produce the desired samples. User Study We conducted user study across three categories: (1) Alignment - Is the 3D Gaussian edited to match the text? (2) Fidelity - Does the image look visually appleaing? (3) Accuracy - Were only the specified parts edited correctly?. Users were asked to score 4-point scale, and we averaged it for mean opinion score (MOS). For reconstructed scene, participants evaluated all three criteria, collecting 4,680 responses from 260 respondents. For generated 3D, evaluations were based on alignment and fidelity, yielding 2,600 responses from 260 respondents. S.1.1.3. Metrics CLIP and CLIP directional score The CLIP-based metrics calculate the cosine similarity between text and image features extracted using CLIP [46]. CLIP scores are commonly utilized in evaluating text-to-3D [34, 43, 55]. CLIP directional scores are specifically employed to evaluate whether the changes occurred in the desired direction, first introduced by [14] and adopted mostly by editing models [5, 6, 9, 62]. We used the ViT-L/14 version of the model, with images cropped to 512 pixels and resized to 336 pixels before being input into the model. TIFA and BLIP score While CLIP-based metrics effectively evaluate coarse similarity between image and text, they have limitations in assessing fine-grained correspondences [1, 9, 20, 21, 52]. To address this, we adopted two additional evaluation metrics focused on fine-grained visual-textual alignment, based on visual question answering (VQA). The TIFA score, introduced in [20], measures the faithfulness of generated image to text input by generating questions with LLaMA2 [54], answering with UnifiedQA-v2 [27]. BLIP-VQA, proposed in [21] breaks down prompt into multiple questions, assigning score based on the probability of answering yes to each question, leveraging the vision-language understanding and generation capabilities of BLIP [32]. S.1.1.4. Implementation details Our method is implemented in PyTorch [41], based on Threestudio [16]. We employ Stable Diffusion 3 [13]. All experiments are conducted on single A100. S.1.2. Experimental Results Quantitative results Detailed quantitative results are shown in Tab. S.1, Tab. S.2, Tab. S.3 and Tab. S.4. The tables present quantiative results for each part editing. Our approach outperformed all other baselines in NeRF and Gaussian Splatting editing across all parts and metrics [5, 6, 12, 17, 29, 62]. Notably, considering that our models achieve strong performance on both CLIP-based and VQA-based scores, we can conclude that our models perform well in editing at both coarse and fine levels. Detailed results of user study for each evaluation criterion are provided in Table. S.5 and Table. S.6. Validity of the user method eye nose mouth hair hard avg CLIP CLIPdir CLIP CLIPdir CLIP CLIPdir CLIP CLIPdir CLIP CLIPdir part GaussCtrl [62] 0.191 0. 0.183 0.035 0.173 0.056 0.195 0. 0.168 0.026 0.182 0.044 GaussianEditor [6] 0. 0.068 0.130 0.057 0.140 0.086 0. 0.144 0.202 0.083 0.179 0.087 DGE [5] 0.193 0.076 0.190 0.058 0.182 0. 0.232 0.161 0.211 0.110 0.201 0. RoMaP(ours) 0.246 0.150 0.263 0.210 0. 0.265 0.277 0.211 0.291 0.188 0. 0.205 Table S.1. Comparison with GS editing methods. CLIP score and CLIP directional score value for each method and part. method eye nose mouth hair hard avg B-VQA TIFA B-VQA TIFA B-VQA TIFA B-VQA TIFA B-VQA TIFA B-VQA TIFA part GaussCtrl [62] GaussianEditor [6] DGE [5] RoMaP(ours) 0.194 0.361 0. 0.700 0.422 0.561 0.539 0.667 0. 0.301 0.427 0.797 0.561 0.633 0. 0.733 0.223 0.448 0.512 0.935 0. 0.572 0.5 0.711 0.239 0.593 0. 0.796 0.494 0.722 0.683 0.717 0. 0.148 0.255 0.399 0.292 0.368 0. 0.543 0.190 0.370 0.497 0.723 0. 0.571 0.565 0.674 Table S.2. Comparison with GS editing methods. BLIP-VQA score and TIFA score value for each method and part. method eye nose mouth hair hard avg CLIP CLIPdir CLIP CLIPdir CLIP CLIPdir CLIP CLIPdir CLIP CLIPdir CLIP CLIPdir part iN2N [17] 0.247 0.067 0.257 0.071 0. 0.084 0.253 0.079 0.227 0.060 0. 0.072 VICA [12] 0.224 0.050 0.225 0. 0.219 0.052 0.229 0.049 0.217 0. 0.223 0.048 PDS [29] 0.162 -0.033 0. 0.014 0.177 0.007 0.176 0.008 0. -0.020 0.167 -0.005 RoMaP(ours) 0.246 0. 0.263 0.210 0.311 0.265 0.277 0. 0.291 0.188 0.277 0.205 Table S.3. Comparison with NeRF editing methods. CLIP score and CLIP directional score value for each method and part. part method eye nose mouth hair hard avg B-VQA TIFA B-VQA TIFA B-VQA TIFA B-VQA TIFA B-VQA TIFA B-VQA TIFA iN2N [17] VICA [12] PDS [29] RoMaP(ours) 0.168 0.277 0.267 0.700 0.589 0. 0.2 0.667 0.168 0.204 0.287 0. 0.489 0.507 0.173 0.733 0.163 0. 0.264 0.935 0.471 0.387 0.147 0. 0.139 0.228 0.333 0.796 0.671 0. 0.160 0.717 0.072 0.205 0.034 0. 0.623 0.41 0.380 0.543 0.142 0. 0.237 0.723 0.565 0.427 0.212 0. Table S.4. Comparison with GS editing methods. BLIP-VQA score and TIFA score value for each method and part. study result is evaluated using pairwise Wilcoxon tests and the Friedman test, as shown in Fig.S.3. The test results confirm that our method significantly outperforms other editing and generation methods with strong statistical significance and validating the effectiveness of our method. Qualitative results We included more qualitative results of our approach in Fig. S.1, Fig. S.2, Fig. S.8, Fig. S.9, and Figure S.1. 3DGS part editing results in complex 3DGS scenes. We performed RoMaP editing on complex 3DGS scenes from the LERF dataset. As shown above, our RoMaP achieved precise open-vocabulary part segmentation for parts of varying sizes, such as the collar, eyes, body, and rubber duck. Additionally, we achieved accurate part editing based on prompts like sheep with purple ears and rubber duck with white hat. Figure S.2. 3DGS part editing results in complex scenes. We demonstrate RoMaP editing results on complex 3D Gaussian Splatting (3DGS) scenes from both the 3D-OVS and LERF datasets. As shown above, RoMaP achieves high-quality normal editing, effectively handling diverse and practical edits such as with blue hair or with Hi name tag. These results highlight RoMaPs ability to generalize across various scene complexities. Fig. S.10. As shown in Fig. S.8, Fig. S.9 and Fig. S.10, our RoMaP can generate diverse 3D assets by editing the original 3D Gaussian Splatting (3DGS). Also, Fig. S.1 and Fig. S.2 show part-editing of our RoMaP in complex scenes. The results demonstrate that our 3D-GALP and editing strategies achieve high precision in 3D segmentation and enable precise modifications to the targeted regions, highlighting the scalability of our method to more complex and cluttered 3D scenes."
        },
        {
            "title": "Fidelity Accuracy",
            "content": "S.2. Additional results in complex scene GaussCtrl [62] GaussianEditor [6] DGE [5] RoMaP (Ours) 20.6% 19.98% 19.70% 20.72% 19.98% 19.61% 23.18% 20.24% 23.62% 36.73% 36.31% 38.43% Table S.5. User study results on comparison with 3D Gaussian editing models."
        },
        {
            "title": "Fidelity",
            "content": "GSGEN [7] GaussianDreamer [67] RFDS [65] RoMaP (Ours) 20.09% 20.48% 19.98% 19.61% 23.18% 23.62% 36.73% 36.31% Table S.6. User study results on comparison with 3D Gaussian generation models. Figure S.3. Statistical results from user study. (a) Pairwise (b) Wilcoxon test results for editing and generation methods. Friedman test p-values for fidelity, accuracy, and alignment. Our approach (Ours) achieves significantly better performance in both reconstruction and generation compared to existing methods. Qualitative results of baselines We visualized qualitative results of Gaussian and NeRF-editing baselines in Fig. S.15 and Fig. S.16. For the NeRF baseline model, we present result from IN2N [17]. Due to the implicit nature of NeRF, precisely selecting the target region is challenging, often resulting in unintended global changes. For example, when applying the prompt Turn his hair into silvertextured hair, the entire scene shifts to silver hue S.15. Similarly, prompts such as hair on fire or left eye blue and right eye green lead to incorrect region selection, causing widespread color alterations across the scene. For the Gaussian Splatting baseline, we show results from GaussianEditor [6]. Inconsistencies in 2D part segmentation lead to unreliable 3D part segmentation, as shown in Fig. S.16. Additionally, 2D editing results demonstrate difficulties in precisely modifying the desired regions. For instance, croissant appears in the background instead of the intended edit, or the entire scene turns pink rather than just his eyes. To further validate the robustness and generalizability of RoMaP, we present additional editing results on complex 3DGS scenes from both the 3D-OVS [35] and LERF [26] datasets. These scenes contain multiple objects with intricate part-level structures and diverse contextual settings. As illustrated in Fig. S.1, RoMaP demonstrates precise open-vocabulary part segmentation and editing across wide range of object types and part granularity. Examples include edits guided by prompts such as white cup with pink handle, rubber duck with white hat, and dog figurine with yellow eyes. RoMaP effectively identifies and modifies fine-grained parts such as handles, beaks, collars, and ears, even under cluttered backgrounds and occlusions. In addition, Fig. S.2 further showcases our models ability to perform practical part editing tasks involving realistic human and animal figures. Prompts such as with blue hair, with purple dress, and with Hi name tag illustrate RoMaPs capability to generalize beyond common categories and execute attribute-level modifications across highly complex scenes. These results collectively highlight RoMaPs strength in both semantic understanding and finegrained spatial localization, making it versatile tool for open-vocabulary 3D scene editing. S.3. Additional validation and details of pipeline S.3.1. Attention map extraction Unlike the naive reverse flow-matching process used in textto-3D generation, we adopted controlled forward ODE to extract more accurate attention maps for real images, thereby enhancing robustness. Controlled forward ODE, proposed in [49], helps maintain consistency with the given image while aligning with the distribution of typical images. This balancing mechanism allows for effective inversion and editing across various inputs, especially real images, even when the given image is corrupted or atypical. Additionally, we adopted the approach proposed in [59] for dense prediction. This method allows for faster and more accurate extraction of attention maps. Post-processing We post-processed extracted attention maps by normalizing them with softmax temperature and utilizing refiner [8]. Adjusting softmax temperature allowed us to segment regions with varying granularity, while the refiner, by incorporating the original image features, enabled segmentation of parts with more precise edges, as shown in Fig. S.4. Figure S.4. Ablation study of attention map post-processing procedure By adjusting the softmax temperature, we achieved segmentation with varying levels of granularity, while the refiner, leveraging the original image features, facilitated the segmentation of parts with sharper and more defined edges. S.3.2. 3D-geometry aware label prediction S.3.2.1. Details of 3D-geometry aware label prediction The detailed algorithm for 3D-Geometry Aware Label Prediction (3D-GALP) is provided in Algo. 1. 3D-GALP produces high-quality 3D segmentation maps even when part segmentation maps from multiple views are noisy, by applying neighbor consistency loss that considers the softlabel property of Gaussian segmentation. Label softness is typically higher at part boundaries due to abrupt shape changes, which can lead to substantial variation in segmentation results across different views. Moreover, in practice, the Gaussians at these part boundaries may simultaneously represent pixels belonging to multiple parts depending on the viewpoint, further complicating consistent segmentation. To address this, Gaussians with both high and low softness are sampled, enabling continuous refinement of ambiguous as well as more view-invariant regions while taking surrounding information into account. S.3.2.2. Part segmentation performance of 3D-GALP language-embedded compared with other 3DGS model in complex scenes Experimental setting To evaluate how effectively 3DGALP performs part segmentation in complex scenes, we annotated part segmentation for every object in all scenes of the 3D-OVS dataset [35]. We compared 3D-GALP with two text-aligned segmentation models for 3D Gaussians, LangSplat [45] and LeGaussian [51]. We kept hyperparameter, the softmax value for our 2D attention map extraction, to 0.2 during segmentation. We then evaluated part-segmentation results for each object from three different views, comparing them against ground truth using the mean Intersection over Union (mIoU). Examples of partsegmentation annotation are presented in Fig. S.5. Figure S.5. Examples of part segmentation annotation in 3DOVS dataset. outperforming other 3DGS segmentation baselines across all scenes. Furthermore, 3D-GALP successfully performs open-vocabulary 3DGS segmentation for parts of varying sizes in complex scenes, as illustrated in Fig. S.11. Scene LangSplat [45] LeGaussian [51] 3D-GALP (Ours) Bench Blue sofa Cov.desk Room Average 0.093 0.005 0.264 0.320 0.546 0. 0.129 0.257 0.502 0.076 0.288 0.559 0.076 0.312 0.580 Table S.7. Comparison of 3D-GALP with part segmentation on complicated 3D scenes. S.3.2.3. Ablation study on SH degree Experimental setting We ablated the SH order to analyze its effect on part-level segmentation. While low-order SH is typically sufficient for modeling lighting in color representation, part-level segmentation requires sharper spatial transitions, particularly around object boundaries. To evaluate this, we conducted experiments using the same experimental settings as in S.3.2.2 with different SH degree settings. Experimental results As shown in Tab. S.7, our 3D segmentation method, 3D-GALP, achieves the highest mIoU, Experimental results As shown in Tab. S.8 and Fig. S.6, SH=3 consistently provides the best average mIoU across Figure S.6. Part-level segmentation visusalizations with different SH orders. scenes and captures fine-grained parts more clearly than lower orders. Although SH=4 performs best in some scenes, it introduces more noise and higher memory usage, leading to slightly worse overall performance. Based on these observations, we fix SH=3 for all segmentation experiments, as it provides the best trade-off between detail preservation and stability. Order of SH 1 3 4 mIoU 0.4777 0.5306 0. 0.5506 Table S.8. mIoU average scores across the scenes per SH degree. Best per scene is in bold. S.3.3. Scheduled latent mixing and part editing S.3.3.1. Scheduled latent mixing and part editing The detailed algorithm is provided in Algo. 2. This method leverages the property of rectified flow that is more faithful to the original image. During the editing process, αbase is multiplied by the mask to ensure that regions outside the target editing area retain their original information. This introduces weak conditioning at intermediate steps of image generation, guiding the generated regions to align with the original context. At the timestep ts, αlast is applied to ensure that most of the Minv regions are replaced with ztarget, preserving the majority of the reference images information in the final output. Further results on the selection of ts are shown in Fig. S.14. low ts induces dramatic changes based on the prompt, while high ts ensures faithful adherence to the mask, taking into account the origiIn the ts selection described nal content and its context. in the main paper, we randomly selected 100 person images from the CelebAMaskHQ [30] dataset, performed part-level editing using 25 prompts, and evaluated the results using CLIPdir [14] and SSIM to assess the direction of change while preserving the original content. The full experimental results with 25 prompts are shown in Fig. S.7. S.3.3.2. Comparison of SLaMP with other image editing models Experimental setting To evaluate the effectiveness of our SLaMP in preserving non-target regions while accurately modifying only the specified parts compared to other Figure S.7. Statistical result for finding sweet spot using CLIP and SSIM results. models, we randomly selected 15 male and female images from the CelebAMaskHQ [30] dataset. For each image, we performed image editing using 25 prompts as described in Sec. S.1.1.1. For comparison, we selected SD3based models (SD3-inpainting [65], Plug&Play [13], RFinversion [49]), as well as an editing model based on naive latent mixing (RePaint [37]), in contrast to our scheduled latent mixing approach. Additionally, we include trainingbased model, InstructPix2Pix (IP2P [2]), which is commonly adopted in 3DGS and NeRF editing approaches. For RePaint, we used Stable Diffusion-integrated variant from HuggingFace Diffusers [57] library since RePaint is not originally designed for text-based image editing. We evaluated how well the changes aligned with the prompts using the CLIPdir [14] and B-VQA [21] metrics. Metrics RePaint [37] CLIPdir B-VQA 0.111 0.439 iP2P [2] 0.117 0.668 SD3-inp. [13] 0.147 0. Plug&Play RF-inv. [65] 0.044 0.564 [49] 0.089 0.740 SLaMP (Ours) 0.165 0.758 Table S.9. Quantitative comparison of SLaMP with other 2D part editing baselines. Experimental results The quantitative experimental results are presented in Tab. S.9, and the qualitative results in Fig. S.12. SLaMP outperforms all other 2D image editing baselines across all metrics, including CLIPdir [14] and BLIP-VQA [21]. Unlike baselines that either fail to reflect the prompt or fail to preserve the original context, SLaMP produces significant changes in the target part while accurately maintaining the untouched regions, achieving strong alignment with the text prompt. As shown in Fig. S.12, the widely used 2D image editing baseline for 3D editing research, iP2P [2], struggles to perform meaningful part edits and often deviates from the original image context. This helps explain why existing 3D editing models often produce no visible changes in part editing tasks. RePaint [37] employs fixed blending ratio for harmonized inpainting, making it unsuitable for strong, prompt-driven part-level edits. In contrast, SLaMP adopts scheduled blending strategy that enables bold edits early on and gradually preserves global context, achieving both precise modifications and faithful preservation. Additional results of SLaMP editing can be found in Fig. S.13. S.4. Social Impact and Limitations In our methodology, we utilized existing datasets from prior works [2, 58]. These datasets include information about real individuals, and if the results of our editing approach are misused, it could lead to concerns regarding negative societal impacts. Therefore, we strongly advocate for the responsible use of our methodology in adherence to ethical guidelines and relevant laws. In perspective on limitation, our approach relies on 3D segmentation based on attention maps observed from 360-degree viewpoints. Consequently, it may not perform well when dealing with objects with highly complex geometries (e.g., Klein bottle), leading to unintended editing results. Additionally, if the Gaussian Splatting scene is inherently blurry or poorly reconstructed, it becomes difficult to distinguish individual components. This can cause SD3 to fail in accurately interpreting the scene, resulting in incorrect 3D segmentation or undesired editing outcomes. Algorithm 1: Algorithm of 3D-geometry aware label prediction (3D-GALP). Input: Gaussian Representation Ω, Camera Parameters C, Number of Anchors K, Nearest Neighbors k, Segmentation Labels slabels Output: Segmentation Loss L3D // Initialize multi-view camera dataset 1 Dtest LoadMultiviewDataset(C) // Compute SH consistency 2 Ω.get sh objects() 3 // Store SH values for different views 4 foreach in Dtest do ComputeViewDirection(b, C) sb EvalSH(Ω, S, d) sb // Compute variance and entropy for each Gaussian 8 foreach Gaussian in Ω do 9 (cid:80) rT r2, (cid:80) rT Compute variance: vi 1 where = 1 Compute entropy: sim rRlabels pi esim (cid:80) esim Hi (cid:80) pi log(pi + ϵ) entropy Compute label softness: Ui Hi vi rRlabels // Compute // Anchor Selection Based on label softness 14 Sort all Gaussians by Ui in descending order 15 Select K/2 anchors with highest Ui 16 Select K/2 anchors with lowest Ui 17 Define set of selected anchors: // Compute Anchor-Based Neighbor"
        },
        {
            "title": "Consistency Loss",
            "content": "18 foreach anchor do 19 Find nearest neighbors Nk(i) = {j1, . . . , jk} using Euclidean distance Compute L1 loss: (cid:104) 1 L3D (cid:80) jNk(i) ri rj1 (cid:80) iS (cid:105) 6 7 10 11 12 20 21 return L3D"
        },
        {
            "title": "References",
            "content": "[1] Saba Ahmadi and Aishwarya Agrawal. An examination of the robustness of reference-free image captioning evaluation metrics. ACL Anthology, 2023. 10 [2] Tim Brooks, Aleksander Holynski, and Alexei Efros. InAlgorithm 2: Scheduled latent mixing and part editing Algorithm Input: Latents z, Text Embeddings E, Camera Condition C, Timestep T, Noise ntarget, Cfg scale c, γ, ηvalues, αbase, αlast, Mask M, , Mix timestep ts Output: Model Prediction mpred // Latent Initialization and Noise"
        },
        {
            "title": "Target",
            "content": "1 for tcurr, tprev in timesteps[: 1], timesteps[1 :] do 2 tcurr 1000 vpred transformer(znoisy, t, Euncond) vtarget (ntarget znoisy)/(1 tcurr) vinterp γ vtarget + (1 γ) vpred znoisy znoisy + (tprev tcurr) vinterp 3 4 5 7 ztarget z.clone 8 for in timesteps do t/1000 9 vpred transformer(znoisy, t, Emix) vtarget (ztarget znoisy)/t η ηvalues[i] vinterp vpred + η (vtarget vpred) znoisy scheduler.step(vinterp, t, znoisy) αlast if > timesteps ts else αbase 10 11 12 13 15 16 Minv (1 M) 14 znoisy znoisy (1 Minv) + ztarget Minv 17 mpred znoisy 18 return mpred structpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 2, 3, 7, 10, 15, 16 [3] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In CVPR, 2022. 4 [4] Honghua Chen, Yushi Lan, Yongwei Chen, Yifan Zhou, and Xingang Pan. Mvdrag3d: Drag-based creative 3d editing via multi-view generation-reconstruction priors. arXiv preprint arXiv:2410.16272, 2024. 3, 5, [5] Minghao Chen, Iro Laina, and Andrea Vedaldi. Dge: Direct gaussian 3d editing by consistent multi-view editing. ECCV, 2024. 2, 3, 6, 7, 10, 11, 13 [6] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. Gaussianeditor: Swift and controllable 3d editing with gaussian splatting. In CVPR, 2024. 2, 3, 5, 6, 7, 10, 11, 13, 26 [7] Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu. Text-to-3d using gaussian splatting. In CVPR, 2024. 7, 13 [8] Ho Kei Cheng, Jihoon Chung, Yu-Wing Tai, and Chi-Keung Tang. Cascadepsp: Toward class-agnostic and very highFigure S.8. Additional qualitative results of RoMaP. Our approach, RoMaP, enables editing across wide range of parts, objects, and prompts in generated 3D Gaussians, further providing users with enhanced controllability over 3D content generation. Figure S.9. Additional qualitative results of RoMaP. Our approach, RoMaP, enables editing across wide range of parts, objects, and prompts in generated 3D Gaussians, further providing users with enhanced controllability over 3D content generation. resolution segmentation via global and local refinement. In CVPR, 2020. [9] Xinhua Cheng, Tianyu Yang, Jianan Wang, Yu Li, Lei Zhang, Jian Zhang, and Li Yuan. Progressive3d: Progressively local editing for text-to-3d content creation with complex semantic prompts. ICLR, 2023. 2, 3, 10 [10] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. NeurIPS, 2021. 2 [11] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. ICLR, 2016. 2 [12] Jiahua Dong and Yu-Xiong Wang. Vica-nerf: Viewconsistency-aware 3d editing of neural radiance fields. NeurIPS, 2023. 2, 3, 6, 7, 10, [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 2, 10, 15 [14] Rinon Gal, Or Patashnik, Haggai Maron, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: ClipFigure S.10. Additional comparison results of RoMaP. Our approach, RoMaP, enables editing across wide range of parts, objects, compare to other methods in 3D scene reconstruction settings. guided domain adaptation of image generators. 2022. 6, 10, 15 open-world compositional text-to-image generation. 2023. 6, 10, 15 [15] Pengsheng Guo, Hans Hao, Adam Caccavale, Zhongzheng Ren, Edward Zhang, Qi Shan, Aditya Sankar, Alexander Schwing, Alex Colburn, and Fangchang Ma. Stabledreamer: Taming noisy score distillation sampling for textto-3d. arXiv preprint arXiv:2312.02189, 2023. 3, 5, [16] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, ZiXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: unified framework for 3d content generation. https://github.com/threestudioproject/ threestudio, 2023. 10 [17] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. In CVPR, 2023. 2, 3, 6, 7, 10, 11, 13, 25 [18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. ICLR, 2022. 2 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 2 [20] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In CVPR, 2023. 6, [21] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for [22] Junha Hyung, Sungwon Hwang, Daejin Kim, Hyunji Lee, and Jaegul Choo. Local 3d editing via 3d distillation of clip knowledge. In CVPR, 2023. 4 [23] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani ICLR, 2023. 3, Lischinski. Noise-free score distillation. 5 [24] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In CVPR, 2023. 2 [25] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 2023. 2, 4 [26] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded In Proceedings of the IEEE/CVF Internaradiance fields. tional Conference on Computer Vision, pages 1972919739, 2023. [27] Daniel Khashabi, Yeganeh Kordi, and Hannaneh Hajishirzi. Unifiedqa-v2: Stronger generalization via broader crossformat training. 2022. 10 [28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. 2, 3 [29] Juil Koo, Chanho Park, and Minhyuk Sung. Posterior distillation sampling. In CVPR, 2024. 3, 6, 7, 10, 11 Figure S.11. Open-voca part segmentation results comparison in complicated 3DGS scenes of 3D-OVS dataset. Figure S.12. Local editing results between SLaMP and 2D image editing methods. SLaMP editing employs rectified flow inversion to achieve effective modifications while maintaining the original context in unedited regions. This contrasts with 2D image editing baselines, which struggle to edit the specified part in alignment with the text prompt. Figure S.13. More 2D part editing results with SLaMP. [30] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive facial image manipulation. In CVPR, 2020. 15 [31] Hangyu Li, Xiangxiang Chu, and Dingyuan Shi. Dreamcouple: Exploring high quality text-to-3d generation via rectified flow. arXiv preprint arXiv:2408.05008, 2024. 3 [32] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified In ICML, vision-language understanding and generation. 2022. 10 [33] Runjia Li, Junlin Han, Luke Melas-Kyriazi, Chunyi Sun, Zhaochong An, Zhongrui Gui, Shuyang Sun, Philip Torr, and Tomas Jakab. Dreambeast: Distilling 3d fantastical animals with part-aware knowledge transfer. 3DV, 2025. 3 [34] Yuhan Li, Yishun Dou, Yue Shi, Yu Lei, Xuanhong Chen, Yi Zhang, Peng Zhou, and Bingbing Ni. Focaldreamer: Textdriven 3d editing via focal-fusion assembly. In AAAI, 2024. 3, [35] Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, Abdulmotaleb El Saddik, Christian Theobalt, Eric Xing, and Shijian Lu. Weakly supervised 3d openvocabulary segmentation. NeurIPS, 2023. 13, 14 [36] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. ICLR, 2023. 2 [37] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1146111471, 2022. 15 [38] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Latent consistency models: Synthesizing highZhao. resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 2 [39] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. ECCV, 2020. [40] Francesco Palandra, Andrea Sanchietti, Daniele Baieri, and text-guided editarXiv preprint Emanuele Rodol`a. ing of 3d objects via gaussian splatting. arXiv:2403.05154, 2024. 2, 3 Gsedit: Efficient [41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 2019. 10 [42] William Peebles and Saining Xie. Scalable diffusion models with transformers. In CVPR, 2023. [43] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. 2023. 10 [44] Tianhao Qi, Shancheng Fang, Yanze Wu, Hongtao Xie, Jiawei Liu, Lang Chen, Qian He, and Yongdong Zhang. Deadiff: An efficient stylization diffusion model with disentangled representations. In CVPR, 2024. 2 [45] Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Figure S.14. Effect of different ts in SLaMP editing. Hanspeter Pfister. Langsplat: 3d language gaussian splatting. In CVPR, 2024. 14 [46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 4, 6, 10 [47] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. [48] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2 [49] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations. arXiv preprint arXiv:2410.10792, 2024. 3, 13, 15 [50] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis. NeurIPS, 2021. 2 [51] Jin-Chuan Shi, Miao Wang, Hao-Bin Duan, and ShaoHua Guan. Language embedded 3d gaussians for openvocabulary scene understanding. In CVPR, 2024. 14 [52] Yaya Shi, Xu Yang, Haiyang Xu, Chunfeng Yuan, Bing Li, Weiming Hu, and Zheng-Jun Zha. Emscore: Evaluating video captioning via coarse-grained and fine-grained embedding matching. In CVPR, 2022. 10 [53] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. 2 [54] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. 2023. [55] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tombari. Textmesh: Generation of realistic 3d meshes from text prompts. In 3DVS, 2024. 10 [56] Cyrus Vachha and Ayaan Haque. Instruct-gs2gs: Editing 3d gaussian splats with instructions, 2024. 3 [57] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Figure S.15. Qualitative results of nerf baseines [17] in 3D part editing. Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models, 2022. multi-view consistent text-driven 3d gaussian splatting editing. ECCV, 2024. 2, 3, 6, 7, 10, 11, 13 [58] Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Nerf-art: Text-driven neural radiance fields stylization. TVCG, 2023. 2, 10, 16 [59] Feng Wang, Jieru Mei, and Alan Yuille. Sclip: Rethinking self-attention for dense vision-language inference. In ECCV, 2025. 13 [60] Junjie Wang, Jiemin Fang, Xiaopeng Zhang, Lingxi Xie, and Qi Tian. Gaussianeditor: Editing 3d gaussians delicately with text instructions. In CVPR, 2024. 2, 3 [61] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021. 2 [62] Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, and Victor Adrian Prisacariu. Gaussctrl: [63] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning In of image diffusion models for text-to-video generation. ICCV, 2023. 2 [64] Yunqiu Xu, Linchao Zhu, and Yi Yang. Gg-editor: Locally editing 3d avatars with multimodal large language model guidance. In ACM International Conference on Multimedia, 2024. 2, 3 [65] Xiaofeng Yang, Cheng Chen, Xulei Yang, Fayao Liu, and Guosheng Lin. Text-to-image rectified flow as plug-and-play priors. arXiv preprint arXiv:2406.03293, 2024. 3, 7, 13, 15 [66] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. NeruIPS, 2021. 2 [67] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Figure S.16. Qualitative results of 3DGS baseine [6] in 3D part editing. Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In CVPR, 2024. 7, [68] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. CVPR, 2023."
        }
    ],
    "affiliations": [
        "Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea",
        "INMC & IPAI Seoul National University, Republic of Korea"
    ]
}