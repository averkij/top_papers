{
    "paper_title": "ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch",
    "authors": [
        "Zheng Liu",
        "Honglin Lin",
        "Chonghan Qin",
        "Xiaoyang Wang",
        "Xin Gao",
        "Yu Li",
        "Mengzhang Cai",
        "Yun Zhu",
        "Zhanping Zhong",
        "Qizhi Pei",
        "Zhuoshi Pan",
        "Xiaoran Shang",
        "Bin Cui",
        "Conghui He",
        "Wentao Zhang",
        "Lijun Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chart reasoning is a critical capability for Vision Language Models (VLMs). However, the development of open-source models is severely hindered by the lack of high-quality training data. Existing datasets suffer from a dual challenge: synthetic charts are often simplistic and repetitive, while the associated QA pairs are prone to hallucinations and lack the reasoning depth required for complex tasks. To bridge this gap, we propose ChartVerse, a scalable framework designed to synthesize complex charts and reliable reasoning data from scratch. (1) To address the bottleneck of simple patterns, we first introduce Rollout Posterior Entropy (RPE), a novel metric that quantifies chart complexity. Guided by RPE, we develop complexity-aware chart coder to autonomously synthesize diverse, high-complexity charts via executable programs. (2) To guarantee reasoning rigor, we develop truth-anchored inverse QA synthesis. Diverging from standard generation, we adopt an answer-first paradigm: we extract deterministic answers directly from the source code, generate questions conditional on these anchors, and enforce strict consistency verification. To further elevate difficulty and reasoning depth, we filter samples based on model fail-rate and distill high-quality Chain-of-Thought (CoT) reasoning. We curate ChartVerse-SFT-600K and ChartVerse-RL-40K using Qwen3-VL-30B-A3B-Thinking as the teacher. Experimental results demonstrate that ChartVerse-8B achieves state-of-the-art performance, notably surpassing its teacher and rivaling the stronger Qwen3-VL-32B-Thinking."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 2 ] . [ 1 6 0 6 3 1 . 1 0 6 2 : r ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Zheng Liu1,4, Honglin Lin2,4, Chonghan Qin3,4, Xiaoyang Wang4, Xin Gao4, Yu Li4 Mengzhang Cai4, Yun Zhu4, Zhanping Zhong4, Qizhi Pei4, Zhuoshi Pan4, Xiaoran Shang4 Bin Cui1, Conghui He4, Wentao Zhang1 (cid:0) 1Peking University, 2Shanghai Jiao Tong University, 3The University of Hong Kong, 4Shanghai Artificial Intelligence Laboratory , Lijun Wu4 (cid:0) Chart reasoning is critical capability for Vision Language Models (VLMs). However, the development of open-source models is severely hindered by the lack of high-quality training data. Existing datasets suffer from dual challenge: synthetic charts are often simplistic and repetitive, while the associated QA pairs are prone to hallucinations and lack the reasoning depth required for complex tasks. To bridge this gap, we propose ChartVerse, scalable framework designed to synthesize complex charts and reliable reasoning data from scratch. (1) To address the bottleneck of simple patterns, we first introduce Rollout Posterior Entropy (RPE), novel metric that quantifies chart complexity. Guided by RPE, we develop complexity-aware chart coder to autonomously synthesize diverse, high-complexity charts via executable programs. (2) To guarantee reasoning rigor, we develop truth-anchored inverse QA synthesis. Diverging from standard generation, we adopt an answer-first paradigm: we extract deterministic answers directly from the source code, generate questions conditional on these anchors, and enforce strict consistency verification. To further elevate difficulty and reasoning depth, we filter samples based on model fail-rate and distill high-quality Chain-of-Thought (CoT) reasoning. We curate ChartVerse-SFT-600K and ChartVerse-RL-40K using Qwen3-VL-30B-A3B-Thinking as the teacher. Experimental results demonstrate that ChartVerse-8B achieves state-of-the-art performance, notably surpassing its teacher and rivaling the stronger Qwen3-VL-32B-Thinking. * Equal contribution (cid:0) Corresponding author HomePage https://chartverse.github.io Date January 21,"
        },
        {
            "title": "1 Introduction",
            "content": "Recent Vision Language Models (VLMs) [2, 43, 30, 6] exhibit strong reasoning capabilities, yet their performance on chart reasoning remains far from satisfactory. central limitation lies in the lack of high-quality data that simultaneously achieves scale, diversity, and complexity. For example, manually curated datasets [23, 28] provide realistic chart distributions but are severely limited by annotation cost. Synthetic approachesranging from template-based rendering [13, 7] to recent code-driven pipelines [8, 46, 15]offer scalability but typically rely on fixed templates or seed charts, resulting in limited visual diversity and poor coverage of real-world, long-tail chart patterns. Although proprietary systems [41] can produce visually rich charts, their dependence on closed commercial APIs prevents large-scale, open-source adoption. Beyond visual diversity, the quality of reasoning supervision presents second major challenge. Existing chart Question-Answering (QA) datasets are constrained in both difficulty and reliability. Visually simplistic charts inherently limit question complexity, making it difficult to construct multi-step reasoning problems. Moreover, answer correctness is also hard to guarantee at scale: manual verification [23] 1 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Table 1: Comparison of ChartVerse-SFT-600K with existing chart datasets. Chart Properties Q&A Properties Complexity & Diversity Datasets Chart Count Data Source Textual Data QA Count Total Tokens ChartQA [23] PlotQA [28] FigureQA [14] ReachQA [8] ECD [42] CoSyn [41] START [21] ChartGen [15] ChartCoder [46] ChartVerse-SFT-600K(Ours) 18K Real-world, Synthetic 156K Real-world, Synthetic 100K 3K 10K 116K 400K 216K 163K 412K Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Table Table - Code Code Code Code Code Code Code 28K 100K 20M 117M 1.3M 2.6M 1.2M 20K 321K 18.6M 1.1M 5.4M 143M 400K - - - - 3.9B 603K Answer Accuracy Reasoning Data Rollout Posterior Entropy Color Entropy Semantic Emb Spread 0.26 0.21 0.25 0.29 0.31 0.35 0.33 0.30 0.33 0.44 2.03 0.83 1.04 1.91 2.13 1.73 1.63 1.31 1.48 3.17 0.37 0.30 0.24 0.47 0.40 0.54 0.27 0.39 0.38 0.51 is infeasible, while LLM-generated QA pairs [38, 41, 4] frequently suffer from hallucinations and numerical errors. As result, current pipelines struggle to provide large-scale, high-fidelity chart reasoning data with rigorous and error-free supervision for model training. To address the above challenges, we propose ChartVerse, scalable, code-driven framework for chart reasoning data synthesis. ChartVerse is designed to jointly improve visual complexity, distributional diversity, and reasoning reliability, enabling the construction of large-scale, challenging chart QA data without relying on fixed templates or manual verification. We first introduce Rollout Posterior Entropy (RPE), metric for estimating intrinsic chart difficulty. Given chart image, we repeatedly prompt an existing VLM to parse the image into executable chart code and measure the consistency across multiple rollouts. Charts that admit diverse, inconsistent code reconstructions exhibit higher posterior entropy, indicating greater visual and semantic ambiguity. RPE thus provides an objective, model-agnostic signal for identifying challenging charts and guiding data selection. Using RPE, we first filter existing chart corpora to construct high-difficulty seed dataset, which is then used to train complexity-aware chart coder. Unlike prior template-based or seed-conditioned approaches, our chart coder generates chart code from scratch via high-temperature sampling, enabling broad exploration of the long-tail chart distribution. Through an iterative loop of generation, difficulty filtering, and retraining, the chart coder progressively improves both code quality and visual complexity, producing diverse, realistic charts. Further, to ensure both correctness and reasoning difficulty for QA, ChartVerse adopts truth-anchored inverse QA synthesis pipeline. Ground-truth answers are first computed directly from chart code, guaranteeing numerical and semantic accuracy. Questions are then reverse-synthesized conditioned on these answers. To control difficulty, we evaluate the resulting QA pairs using strong VLM and retain samples with non-trivial fail-rate, ensuring that the final dataset emphasizes genuinely challenging reasoning cases. Leveraging Qwen3-VL-30B-A3B-Thinking as the teacher model, we finally distill high-quality chainof-thought (CoT) supervision and curate two datasets: ChartVerse-SFT-600K and ChartVerse-RL-40K. Initializing from the Qwen3-VL Instruct series [2], we train ChartVerse models at 2B, 4B, and 8B scales. Despite their smaller sizes, these models achieve state-of-the-art (SOTA) performance on chart reasoning benchmarks. Notably, ChartVerse-4B surpasses Qwen3-VL-8B-Thinking, while ChartVerse8B even outperforms its 30B teacher, approaching the performance of Qwen3-VL-32B-Thinking."
        },
        {
            "title": "2 Related Work",
            "content": "Chart VLMs. Leading proprietary models [30, 6, 1, 2] have demonstrated exceptional reasoning capabilities in complex chart understanding. In the open-source domain, early methods like ChartLlama [7], 2 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Figure 1: Illustration of Rollout Posterior Entropy. We quantify complexity via generative stability: simple charts yield consistent reconstructions (low RPE), while complex charts result in divergent outcomes (high RPE). ChartGemma [25], and EvoChart [9] primarily focused on visual instruction tuning to handle fundamental structural extraction tasks like OCR and element counting. Recent works have transitioned towards reasoning-centric designs. TinyChart [44] employs template-based Program-of-Thought synthesis. ChartReasoner [11] and Chart-R1 [4] integrate reinforcement learning with supervised finetuning, directly incentivizing the model to generate robust CoT paths for complex problem-solving. Chart Image Synthesis. Conventional approaches [13, 7] rely on rule-based rendering engines to visualize data tables. These methods often yield charts with rigid layouts and limited diversity. Recent works [46, 8, 38, 12, 15, 42] leverage LLMs to generate executable code or augment fixed visual seeds. Despite offering better flexibility, these methods remain constrained by finite and simple templates. The charts exhibit trivial topologies that are insufficient to elicit the complex reasoning capabilities of VLMs. Methods like Cosyn [41] employ proprietary model Claude-3.5-Sonnet for end-to-end synthesis to avoid template restrictions. However, the dependence on commercial APIs introduces prohibitive costs and scalability bottlenecks. Chart QA Construction. High-quality QA pairs is pivotal for chart understanding. While human annotation [23, 28] guarantees accuracy, its unscalability has necessitated shift towards automated generation. Recent approaches [41, 4, 24, 17, 27, 20, 19, 29] typically prompt leading VLMs to synthesize QA pairs from images, with some works [8] leveraging auxiliary code. Despite their popularity, these methods are constrained by the stochastic nature of LLMs, which leads to unavoidable hallucinations and inaccuracies. Lacking ground truth for verification, these methods predominantly yield uncalibrated, simplistic questions and lack multi-step CoT, accurate answers."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we introduce our ChartVerse framework for scalable chart reasoning data synthesis. We first introduce difficulty metric to quantify intrinsic chart complexity, which guides data selection and generation. Based on this metric, we develop complexity-aware chart coder and truth-anchored inverse QA synthesis pipeline to produce challenging and reliable training data. Figure 2 shows the complete process. 3 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Figure 2: Overview of ChartVerse. Stage 1 trains complexity-aware chart coder via RPE-guided iterative self-enhancement to synthesize diverse, high-complexity chart codes. Stage 2 generates verifiable QA pairs using truth-anchored inverse synthesis and CoT distillation, followed by failurerate filtering to guarantee difficulty."
        },
        {
            "title": "3.1 Quantifying Intrinsic Chart Complexity",
            "content": "A prerequisite for training robust chart generator is the identification and filtration of high-complexity samples. However, distinguishing complexity via direct pixel analysis is intractable, as visual density does not correlate with structural difficulty. When humans interpret complex chart, they often form diverse understandings and struggle to capture every detail. This implies that the decoding space for complex visual data is variable, making consistent reproduction difficult. Guided by this insight, we propose Rollout Posterior Entropy (RPE) to measure the variability. Specifically, we employ VLM to generate executable code for charts as proxy for structural interpretation. As illustrated in Figure 1, simple charts yield consistent code that renders into identical images, whereas complex charts induce high instability, resulting in divergent reconstructions. Modeling Structural Variability. Given chart, we utilize Qwen3-VL-2B-Thinking to generate plotting code 8 times with the temperature set to 1.0. We then execute these codes to render reconstructed images. Let denote the number of successfully executed samples. We utilize CLIP [31] to extract 1 dimensional features for each image and stack them into an image matrix RKd. To quantify the variability, we first center to remove translational shifts and compute the Gram matrix to model pairwise similarities: (cid:18) (cid:19) Vc = 11T V, = VcVT . 1 (1) Next, we calculate the singular values σi of G. We normalize these values and compute the spectral entropy to measure the degree of variation. pi = σi/ σj, = i=1 pi log pi. (2) ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Since execution failures inherently signal difficulty, we normalize by the valid count to obtain final RPE: RPE = . (3)"
        },
        {
            "title": "S\nK",
            "content": "Higher RPE denotes greater reproduction difficulty, reflecting intrinsic chart complexity. Diagnosing Existing Datasets. Leveraging RPE, we evaluate the complexity of mainstream datasets. As shown in Table 1, nearly all existing datasets exhibit low RPE scores, indicating that such charts are trivial. To drive further progress, it is critical to synthesize data that transcends simple visual variation, offering the complexity required to challenge the consistency of modern VLMs."
        },
        {
            "title": "3.2 Complexity-Aware Chart Coder",
            "content": "RPE suggests that executable code is high-fidelity proxy of chart structure, we therefore synthesize complex charts via code and propose complexity-aware chart coder that can autonomously generate non-standard, high-complexity charts from scratch. Difficulty-Filtered Cold Start. We first construct high-quality cold-start seed code Ccold. We aggregate chart images from existing datasets in Table 1 to form an image pool Ipool and apply RPE to quantify visual complexity. Based on the statistics in Table 1, we set an RPE threshold of 0.4 to retain only high RPE samples: Ihard = {x Ipool RPE(x) 0.4}, (4) Since real-world charts in Ihard lack ground-truth source code, we employ Claude-4-Sonnet to infer candidate code C(x) for each image Ihard. We discard samples that trigger execution errors, obtaining the cold-start training set: Ccold = {C(x) Ihard}. (5) Ultimately, we curate cold-start set Ccold comprising 60k high-quality samples, which serves as the foundational corpus for our chart coder. Unlike existing methods that rely on code as rigid templates, we train strong coder LLM, Qwen2.5Coder-7B [10], to function as flexible generative model. We structure each training instance by pairing concise system instruction (see Prompt 9) as the model input with the chart code sequence = (c1, c2, . . . , cL) Ccold as the target answer. The model is optimized using the standard crossentropy loss to internalize the structural and logic patterns of executable chart codes. During inference, by applying high sampling temperature, the coder leverages its aligned internal knowledge and learned patterns to generate diverse chart codes. This approach enables the model to move beyond simple replication and synthesize novel, complex chart code. Iterative Self-Enhancement Loop. Despite its quality, Ccold is typically scarce, and the coder trained on it exhibited limited structural complexity, making large-scale high-difficulty synthesis non-trivial. To address this, we introduce self-enhancement mechanism that iteratively expands data and strengthens the coder capability. Specifically, we first employ the cold-start coder to perform large-scale sampling at temperature of 1.0, generating raw candidate pool Craw of 2M code samples. To ensure quality and diversity, We discard codes that trigger execution errors, then render the charts and filter out low-complexity samples using RPE. To ensure diversity, we exclude charts that exhibit high visual similarity to Ihard ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch based on image embeddings from CLIP. We apply an RPE threshold 0.4 and similarity limit 0.65 to retain only complex, non-redundant samples: Cboost = (cid:8)C Craw RPE(R(C)) 0.4 Sim(R(C), Ihard) 0.65(cid:9), (6) where R(C) denotes the rendered image, and Sim(x, Ihard) = maxxIhard cos(e(x), e(x)) calculates the maximum cosine similarity between the CLIP embedding e(x) of candidate and Ihard. Ultimately, we curate boosted set Cboost of 200k samples and retrain the coder on Ccold Cboost to obtain stronger model. We repeat this procedure for two iterations to ensure the models generative stability and structural diversity. Synthetic Chart Dataset. We utilize the final complexity-aware coder to perform large-scale sampling at temperature of 1.0, generating 1.2M raw chart code candidates Csyn. We again apply RPE filtering to retain only high-complexity samples. This process yields final dataset Dsyn comprising 900k pairs of rendered images and their corresponding executable codes: Dsyn = (cid:8)(R(C),C) Csyn This extensive corpus of complex charts serves as the foundation for our QA generation. RPE(R(C)) 0.4(cid:9). (7)"
        },
        {
            "title": "3.3 Truth-Anchored Inverse QA Synthesis",
            "content": "While the complexity of Dsyn provides foundation for constructing difficult questions, it introduces significant challenges in accurate answer extraction. To generate QA pairs that preserve both accuracy and difficulty, we propose to invert the generation flow to enforce Answer as the Source of Truth. Inverse Logic Construction. Existing QA generation approaches, which first synthesize question and then employ VLM to derive the answer (Q A), suffer from stochastic hallucinations and cannot guarantee correctness. To address this, we propose an Inverse Synthesis paradigm (A Q). An advantage of Dsyn is that the images are rendered from code, ensuring that codes encapsulate the absolute ground truth of all visual data. Leveraging this, we synthesize the answer first. As shown in Figure 2, to ensure absolute precision, we employ Qwen3-30B-A3B-Thinking πLLM to analyze the chart code and generate Python script that performs meaningful operations on the data in chart code, yielding precise numerical or categorical result. By executing this script in deterministic environment , we obtain the ground-truth answer Apy, completely bypassing numerical errors from direct VLM answering: πLLM(C), Apy = (S). (8) We then instruct the LLM again to reverse-engineer question that strictly aligns with chart code and Script S: πLLM(C, S). (9) To guarantee that the synthesized question is unambiguously solvable, we perform consistency check. We feed the question and chart code back into the LLM to produce solution ˆA, ˆA πLLM(C, Q). We strictly retain samples where the inferred solution matches the programmatic ground truth: Dpair = {(R(C), Q, Apy) ˆA = Apy}. (10) (11) 6 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Table 2: Comparison of different models. The shaded rows denote our ChartVerse models. Model ECD-7B [42] START-7B [21] Chart-R1-7B [4] ChartVerse-2B InternVL3.5-38B [34] InternVL3.5-241B-A28B Qwen3-VL-8B-Thinking [2] ChartVerse-4B Qwen3-VL-30B-A3B-Thinking ChartVerse-8B Qwen3-VL-32B-Thinking Qwen3-VL-235B-A30B-Thinking ChartQA CharXiv CharXiv (RQ) (DQ) Pro Chart Museum ChartX Evo Chart ChartBench Avg (GPT-acc) 45.1 43.5 45.6 48.2 47.8 50.7 53.9 55.2 55.8 56.2 58.8 60.0 41.7 46.3 47. 46.9 45.5 47.5 53.0 56.2 56.6 60.8 65.2 66.1 74.9 76.8 70.0 71.2 86.5 88.1 85.9 84.1 86.9 88.0 90.2 90.5 24.5 29.7 33.4 37.5 34.2 36.1 44.3 45.9 49.2 49.2 55.9 60.0 59.3 57.3 60. 60.5 55.5 59.1 59.6 63.7 62.3 63.9 64.1 64.5 56.0 63.8 67.1 66.8 65.0 67.4 74.1 75.0 77.2 76.2 80.8 79.9 48.6 50.0 50.5 49.1 49.0 48.6 49.1 52.9 52.4 54.2 54.3 54.5 50.0 52.5 53. 54.3 54.8 56.8 60.0 61.9 62.9 64.1 67.0 67.9 Through this rigorous inverse pipeline, we ensure that every sample is logically sound and factually accurate. It is worth noting that this logical construction phase operates exclusively within the textual domain. By relying solely on the code-text modality, we achieve high-efficiency synthesis while eliminating potential visual encoding errors. CoT Distillation & Difficulty Filtration. Correctness alone does not imply training value. We further estimate visual reasoning difficulty via repeated VLM rollouts and keep only samples that are neither trivial nor impossible. We employ Qwen3-VL-30B-A3B-Thinking πVLM to generate 3 CoT reasoning traces {τ1, τ2, τ3}. As shown in Figure 2, we extract the predicted answer ˆaj from each trace τj and calculate the failure rate r(Q) against the ground truth: r(Q) = 1 1 3 j=1 Match( ˆaj, Apy). (12) We construct Dhard by retaining samples where r(Q) is neither 0 nor 1. This ensures that samples are neither impossible to solve nor trivially simple. Dhard = {(R(C), Q, Apy, τ) 0 < r(Q) < 1}. (13) We prioritize the most challenging samples with the highest r(Q) in Dhard for the Reinforcement Learning dataset Drl, while utilizing the remaining samples enriched with correct CoT traces τ for the Supervised Finetuning dataset Dsft. After the processing, we finally obtain the ChartVerse-SFT-600K and ChartVerse-RL-40K datasets. Details and prompts on synthesis pipelines and coder training are available in Appendix C."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Training Details. All ChartVerse models are initialized from the Qwen3-VL-Instruct [2] series. We first perform Supervised Fine-Tuning (SFT) on the ChartVerse-SFT-600K dataset using LLaMA-Factory [48]. Subsequently, Reinforcement Learning (RL) is conducted on the ChartVerse-RL-40K dataset with 7 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch (a) Performance on chart-related benchmarks (b) Performance on STEM-related benchmarks Figure 3: Performance evolution of ChartVerse across different training phases on chart and STEM benchmarks. (a) Comparison of different datasets (b) Ablation study on RPE strategy. Figure 4: Ablation study results on different datasets and the proposed RPE strategy. VeRL [32], adopting the GSPO [47] algorithm. Comprehensive training configurations and hyperparameters are provided in Appendix A. Baselines. We compare our tuned ChartVerse models against two categories of models: (1) General open-source VLMs: leading models including Qwen3-VL-Thinking series (8B, 30B, 32B, 235B) [2], InternVL3.5 series (38B, 241B) [34]. (2) Specialized chart-domain VLMs: ECD [42],START [21], and Chart-R1 [4]. Benchmarks. We evaluate performance across 6 benchmarks necessitating complex chart understanding and reasoning: ChartQA-Pro [26], CharXiv [35], ChartMuseum [33], ChartX [36], ChartBench [40], and EvoChart [9]."
        },
        {
            "title": "4.2 Main Results",
            "content": "Overall Results. Table 2 compares ChartVerse models with above baselines. Overall, ChartVerse consistently delivers strong performance across all model scales, demonstrating the effectiveness of our difficulty-aware data synthesis framework. We highlight three key observations: (1) Competitive performance at small scale. ChartVerse-2B achieves an average score of 54.3, exceeding all chart-specific baselines, including ECD-7B, START-7B, and Chart-R1-7B. This shows that complexity-controlled chart data can substantially offset model size limitations. 8 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch (a) Visualization of chart image embeddings (b) Visualization of chart code embeddings Figure 5: t-SNE visualizations of feature distributions for ChartVerse-SFT and existing datasets. (2) Data quality over model scale. ChartVerse-4B attains an average score of 61.9, outperforming Qwen3-VL-8B-Thinking (60.0) despite using only half the parameters. This gap highlights the dominant role of data quality in improving chart reasoning performance. (3) Beyond the teacher model. ChartVerse-8B further improves the average score to 64.1, surpassing its teacher Qwen3-VL-30B-A3B-Thinking (62.9) and approaching Qwen3-VL-32B-Thinking (67.0). This result indicates that ChartVerse enables student models to exceed the distillation ceiling. Collectively, these observations validate the advantages of ChartVerses rigorously controll chart and QA synthesis pipeline, showing that scalable generation of training data translates into consistent gains across model scales."
        },
        {
            "title": "4.3 Training Stages and Generalization",
            "content": "SFT and RL are Both Effective. Figure 3a reports results on chart-related benchmarks under different training stages. Across all model sizes, ChartVerse-SFT yields substantial improvements over the Qwen3-VL-Instruct baseline, and ChartVerse-RL further provides consistent gains. For instance, ChartVerse-2B improves from 42.5 to 49.8 after SFT and reaches 54.3 after RL, while ChartVerse-8B increases from 56.9 to 62.5 and 64.1, respectively. These results indicate that SFT establishes strong chart reasoning foundations, and RL further enhances performance by focusing on more challenging samples. Strong Generalization to STEM tasks. We further evaluate ChartVerse on STEM-related benchmarks, including MathVista [22], DynaMath [49], MathVerse [45], LogicVista [37], and VisuLogic [39]. As shown in Figure 3b, ChartVerse-trained models outperform the Instruct baseline across all scales. Notably, ChartVerse-8B improves from 56.7 to 61.2 after SFT and reaches 62.2 with RL, demonstrating that the reasoning skills learned from ChartVerse data transfer effectively to out-of-domain STEM reasoning tasks."
        },
        {
            "title": "5.1 Fair Comparison with Existing Datasets",
            "content": "To control for data scale, we compare different training datasets under fixed budget of 100K samples. As summarized by the Avg scores in Table 4a, most existing datasets provide little benefit to Qwen3VL-4B-Instruct: CoSyn even degrades performance (53.9 51.0), while START and ECD yield only 9 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Table 3: Performance comparison of different QA synthesis strategies. All models are trained on Qwen3-VL-4B-Instruct with 100k synthesized samples. Training Data (100k) Qwen3-VL-4B-Instruct + Image-Space Gen. + Code-Space Gen. + Truth-Anchored Gen. w/o Failrate + Truth-Anchored Gen. w/ Failrate (Ours) ChartQA CharXiv CharXiv (RQ) (DQ) Pro Chart Museum ChartX Evo Chart ChartBench Avg (GPT-acc) 53.7 53.1 53.9 54.0 53.4 39.7 46.6 47.0 47.4 48. 76.2 81.3 81.2 81.7 82.6 37.2 38.4 38.6 39.8 40.3 57.2 58.4 58.9 59.7 60. 68.2 70.4 70.3 70.7 70.9 45.1 47.3 47.8 48.2 48.7 53.9 56.5 56.8 57.4 57. marginal gains (53.8 and 54.2, respectively), barely surpassing the baseline. In contrast, ChartVerse-SFT leads to substantial improvement, boosting the average score to 57.8 (+3.9). This clear gap under identical data budgets indicates that the performance gains stem from data quality and task structure rather than scale, highlighting ChartVerse-SFT as significantly more effective for enhancing strong modern VLMs."
        },
        {
            "title": "5.2 Effectiveness of Rollout Posterior Entropy",
            "content": "To verify the effectiveness of RPE, we compare RPE with two representative data selection strategies: VLM-as-Judge, which selects samples with high model-assessed visual complexity, and Python Code Complexity, which favors charts with dense Matplotlib structures. All strategies select 100K samples from the same data pool to ensure fair comparison. As shown in Figure 4b, RPE identifies substantially harder samples, as reflected by the highest failure rate of Qwen3-VL-4B-Instruct (27.6%), compared to 21.1% for VLM-as-Judge and 23.5% for Python Code Complexity. This suggests that RPE goes beyond surface-level visual or structural heuristics and captures samples that are intrinsically more challenging for the model. Crucially, this increased difficulty translates into better downstream performance. Fine-tuning on RPE-selected data achieves the highest average score (55.4), outperforming both competing strategies. In contrast, methods based on perceived complexity yield smaller gains despite selecting easier samples. These results indicate that RPE is more effective at selecting learning-relevant hard examples, leading to stronger performance improvements under fixed data budget."
        },
        {
            "title": "5.3 Diversity and Distribution Analysis",
            "content": "To analyze the quality and diversity of ChartVerse-SFT, we compare it with representative chart datasets from multiple perspectives, including data scale, supervision signals, and distributional diversity. For feature-level analysis, we extract embeddings from chart code and images using Qwen38B-Embedding and CLIP, and visualize their distributions with t-SNE. As illustrated in Figure 5, ChartVerse-SFT covers substantially broader feature space than prior datasets, largely subsuming the distributions of existing chart corpora. This indicates that our dataset is not concentrated around narrow set of visual or structural patterns, but instead spans diverse chart styles and semantics. We further quantify diversity using complementary metrics. Color Distribution Entropy measures the richness of visual appearance, while Semantic Embedding Spread captures structural and semantic variation across samples. As reported in Table 1, ChartVerse-SFT consistently attains the highest values on both metrics, exceeding all existing datasets by clear margin. Together with its higher rollout posterior entropy, these results suggest that ChartVerse-SFT is not only larger in scale, but also more diverse and information-rich, offering broader coverage of chart layouts, visual styles, and semantic structures. 10 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch"
        },
        {
            "title": "5.4 Breaking the Distillation Ceiling",
            "content": "A central goal of ChartVerse is to enable the student model to outperform its teacher, which we achieve through truth-anchored inverse QA synthesis. We compare this method with two common QA construction strategiesdirect image-based generation and direct code-based generationand include an ablation that removes fail-rate based selection. As shown in Table 3, all QA synthesis strategies improve upon the baseline, indicating that synthetic supervision is generally beneficial. However, the magnitude of improvement varies substantially across methods. Direct image-space generation yields only moderate gains (Avg 56.5), while codespace generation performs slightly better (56.8), suggesting that structural cues from code are helpful but still limited. In contrast, truth-anchored generation leads to clear performance jump (57.4), demonstrating the advantage of anchoring QA synthesis to verifiable ground truth rather than surface-level patterns. Importantly, incorporating failure-rate-based selection further boosts the average score to 57.8, achieving the best performance. This additional gain highlights the role of hard-sample mining in maximizing the effectiveness of synthesized data, and confirms that our strategy produces not only more accurate but also more effective supervision."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce ChartVerse, scalable framework addressing the scarcity of complex data for chart reasoning. We introduce Rollout Posterior Entropy to quantify chart complexity, guiding ComplexityAware Chart Coder to synthesize diverse, non-trivial charts. We propose Truth-Anchored Inverse QA Synthesis, ensuring accuracy via code-derived ground truths, reverse-synthesized questions, and consistency checks. Trained on our ChartVerse-SFT-600k and ChartVerse-RL-40k, ChartVerse-8B surpasses its teacher model Qwen3-VL-30B-A3B-Thinking and approaches Qwen3-VL-32B-Thinking, demonstrating the superior quality of our data. 11 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch References [1] Anthropic. Claude 3.5 technical report. Technical report, Anthropic, 2025. URL https://www.anthropic. com/news/claude-3-family. [2] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report, 2025. URL https://arxiv.org/abs/2511.21631. [3] Mengzhang Cai, Xin Gao, Yu Li, Honglin Lin, Zheng Liu, Zhuoshi Pan, Qizhi Pei, Xiaoran Shang, Mengyuan Sun, Zinan Tang, Xiaoyang Wang, Zhanping Zhong, Yun Zhu, Dahua Lin, Conghui He, and Lijun Wu. Opendataarena: fair and open arena for benchmarking post-training dataset value, 2025. URL https: //arxiv.org/abs/2512.14051. [4] Lei Chen, Xuanle Zhao, Zhixiong Zeng, Jing Huang, Yufeng Zhong, and Lin Ma. Chart-r1: Chain-of-thought supervision and reinforcement for advanced chart reasoner, 2025. URL https://arxiv.org/abs/2507.15509. [5] Haodong Duan, Xinyu Fang, Junming Yang, Xiangyu Zhao, Yuxuan Qiao, Mo Li, Amit Agarwal, Zhe Chen, Lin Chen, Yuan Liu, Yubo Ma, Hailong Sun, Yifan Zhang, Shiyin Lu, Tack Hwa Wong, Weiyun Wang, Peiheng Zhou, Xiaozhe Li, Chaoyou Fu, Junbo Cui, Jixuan Chen, Enxin Song, Song Mao, Shengyuan Ding, Tianhao Liang, Zicheng Zhang, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models, 2025. URL https://arxiv.org/abs/2407.11691. [6] Google DeepMind. Gemini 3 technical report. Technical report, Google, 2025. URL https://storage. googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf. [7] Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. Chartllama: multimodal llm for chart understanding and generation, 2023. URL https://arxiv.org/abs/ 2311.16483. [8] Wei He, Zhiheng Xi, Wanxu Zhao, Xiaoran Fan, Yiwen Ding, Zifei Shan, Tao Gui, Qi Zhang, and Xuanjing Huang. Distill visual chart reasoning ability from llms to mllms, 2025. URL https://arxiv.org/abs/2410. 18798. [9] Muye Huang, Han Lai, Xinyu Zhang, Wenjun Wu, Jie Ma, Lingling Zhang, and Jun Liu. Evochart: benchmark and self-training approach towards real-world chart understanding, 2025. URL https://arxiv. org/abs/2409.01577. [10] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report, 2024. URL https://arxiv.org/abs/2409.12186. [11] Caijun Jia, Nan Xu, Jingxuan Wei, Qingli Wang, Lei Wang, Bihui Yu, and Junnan Zhu. Chartreasoner: Code-driven modality bridging for long-chain reasoning in chart question answering, 2025. URL https: //arxiv.org/abs/2506.10116. [12] Gongyao Jiang and Qiong Luo. Chart-coca: Self-improving chart understanding of vision lms via code-driven synthesis and candidate-conditioned answering, 2025. URL https://arxiv.org/abs/2508.11975. [13] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering, 2018. URL https://arxiv.org/abs/1801.08163. [14] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning, 2018. URL https://arxiv.org/abs/ 1710.07300. 12 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch [15] Jovana Kondic, Pengyuan Li, Dhiraj Joshi, Zexue He, Shafiq Abedin, Jennifer Sun, Ben Wiesel, Eli Schwartz, Ahmed Nassar, Bo Wu, Assaf Arbelle, Aude Oliva, Dan Gutfreund, Leonid Karlinsky, and Rogerio Feris. Chartgen: Scaling chart understanding via code-guided synthetic chart generation, 2025. URL https: //arxiv.org/abs/2507.19492. [16] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention, 2023. URL https://arxiv.org/abs/2309.06180. [17] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning, 2024. URL https://arxiv.org/abs/2311.10774. [18] Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek F. Wong, Songyang Zhang, and Kai Chen. Compassverifier: unified and robust verifier for llms evaluation and outcome reward, 2025. URL https://arxiv.org/abs/2508.03686. [19] Zheng Liu, Hao Liang, Bozhou Li, Wentao Xiong, Chong Chen, Conghui He, Wentao Zhang, and Bin Cui. Synthvlm: Towards high-quality and efficient synthesis of image-caption datasets for vision-language models, 2025. URL https://arxiv.org/abs/2407.20756. [20] Zheng Liu, Mengjie Liu, Jingzhou Chen, Jingwei Xu, Bin Cui, Conghui He, and Wentao Zhang. Fusion: Fully integration of vision-language representations for deep cross-modal understanding, 2025. URL https://arxiv.org/abs/2504.09925. [21] Zhuoming Liu, Xiaofeng Gao, Feiyang Niu, Qiaozi Gao, Liu Liu, and Robinson Piramuthu. Start: Spatial and textual learning for chart understanding, 2025. URL https://arxiv.org/abs/2512.07186. [22] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [23] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning, 2022. URL https://arxiv.org/abs/2203. 10244. [24] Ahmed Masry, Mehrad Shahmohammadi, Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty. Chartinstruct: Instruction tuning for chart comprehension and reasoning, 2024. URL https://arxiv.org/abs/2403.09028. [25] Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, and Shafiq Joty. Chartgemma: Visual instruction-tuning for chart reasoning in the wild, 2024. URL https://arxiv.org/abs/2407. 04172. [26] Ahmed Masry, Mohammed Saidul Islam, Mahir Ahmed, Aayush Bajaj, Firoz Kabir, Aaryaman Kartha, Md Tahmid Rahman Laskar, Mizanur Rahman, Shadikur Rahman, Mehrad Shahmohammadi, Megh Thakkar, Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty. Chartqapro: more diverse and challenging benchmark for chart question answering, 2025. URL https://arxiv.org/abs/2504.05506. [27] Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao, Kaipeng Zhang, Yu Qiao, and Ping Luo. Chartassisstant: universal chart multimodal language model via chart-to-table pre-training and multitask instruction tuning, 2024. URL https://arxiv.org/abs/2401.02384. [28] Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots, 2020. URL https://arxiv.org/abs/1909.00997. [29] Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang, Linke Ouyang, Zhiyuan Zhao, Tao Chu, Tianyao He, Fan Wu, Qintong Zhang, Zhenjiang Jin, Guang Liang, Rui Zhang, Wenzheng Zhang, Yuan Qu, Zhifei Ren, Yuefeng Sun, Yuanhong Zheng, Dongsheng Ma, Zirui Tang, Boyu Niu, Ziyang Miao, Hejun Dong, Siyi Qian, Junyuan Zhang, Jingzhou Chen, Fangdong Wang, Xiaomeng Zhao, Liqun Wei, Wei Li, Shasha Wang, Ruiliang Xu, Yuanyuan Cao, Lu Chen, Qianqian Wu, Huaiyu Gu, Lindong Lu, Keming Wang, Dechen Lin, Guanlin Shen, Xuanhe Zhou, Linfeng Zhang, Yuhang Zang, Xiaoyi Dong, Jiaqi Wang, Bo Zhang, Lei Bai, Pei Chu, Weijia Li, Jiang Wu, Lijun Wu, Zhenxiang Li, Guangyu Wang, Zhongying Tu, Chao Xu, Kai Chen, Yu Qiao, 13 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Bowen Zhou, Dahua Lin, Wentao Zhang, and Conghui He. Mineru2.5: decoupled vision-language model for efficient high-resolution document parsing, 2025. URL https://arxiv.org/abs/2509.22186. [30] OpenAI. Gpt-5 system card. Technical report, OpenAI, 2025. URL https://cdn.openai.com/ gpt-5-system-card.pdf. [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. [32] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, page 12791297. ACM, March 2025. doi: 10.1145/ 3689031.3696075. URL http://dx.doi.org/10.1145/3689031.3696075. [33] Liyan Tang, Grace Kim, Xinyu Zhao, Thom Lake, Wenxuan Ding, Fangcong Yin, Prasann Singhal, Manya Wadhwa, Zeyu Leo Liu, Zayne Sprague, Ramya Namuduri, Bodun Hu, Juan Diego Rodriguez, Puyuan Peng, and Greg Durrett. Chartmuseum: Testing visual reasoning capabilities of large vision-language models, 2025. URL https://arxiv.org/abs/2505.13444. [34] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, Zhaokai Wang, Zhe Chen, Hongjie Zhang, Ganlin Yang, Haomin Wang, Qi Wei, Jinhui Yin, Wenhao Li, Erfei Cui, Guanzhou Chen, Zichen Ding, Changyao Tian, Zhenyu Wu, Jingjing Xie, Zehao Li, Bowen Yang, Yuchen Duan, Xuehui Wang, Zhi Hou, Haoran Hao, Tianyi Zhang, Songze Li, Xiangyu Zhao, Haodong Duan, Nianchen Deng, Bin Fu, Yinan He, Yi Wang, Conghui He, Botian Shi, Junjun He, Yingtong Xiong, Han Lv, Lijun Wu, Wenqi Shao, Kaipeng Zhang, Huipeng Deng, Biqing Qi, Jiaye Ge, Qipeng Guo, Wenwei Zhang, Songyang Zhang, Maosong Cao, Junyao Lin, Kexian Tang, Jianfei Gao, Haian Huang, Yuzhe Gu, Chengqi Lyu, Huanze Tang, Rui Wang, Haijun Lv, Wanli Ouyang, Limin Wang, Min Dou, Xizhou Zhu, Tong Lu, Dahua Lin, Jifeng Dai, Weijie Su, Bowen Zhou, Kai Chen, Yu Qiao, Wenhai Wang, and Gen Luo. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency, 2025. URL https://arxiv.org/abs/2508.18265. [35] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, and Danqi Chen. Charxiv: Charting gaps in realistic chart understanding in multimodal llms, 2024. URL https://arxiv.org/abs/2406.18521. [36] Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Peng Ye, Min Dou, Botian Shi, Junchi Yan, and Yu Qiao. Chartx & chartvlm: versatile benchmark and foundation model for complicated chart reasoning, 2025. URL https://arxiv.org/abs/2402.12185. [37] Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. [38] Duo Xu, Hao Cheng, Xin Lin, Zhen Xie, and Hao Wang. Chartm3: multi-stage code-driven pipeline for constructing multi-dimensional and multi-step visual reasoning data in chart comprehension, 2025. URL https://arxiv.org/abs/2511.02415. [39] Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, et al. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025. [40] Zhengzhuo Xu, Sinan Du, Yiyan Qi, Chengjin Xu, Chun Yuan, and Jian Guo. Chartbench: benchmark for complex visual reasoning in charts, 2024. URL https://arxiv.org/abs/2312.15915. [41] Yue Yang, Ajay Patel, Matt Deitke, Tanmay Gupta, Luca Weihs, Andrew Head, Mark Yatskar, Chris CallisonBurch, Ranjay Krishna, Aniruddha Kembhavi, and Christopher Clark. Scaling text-rich image understanding via code-guided synthetic multimodal data generation, 2025. URL https://arxiv.org/abs/2502.14846. [42] Yuwei Yang, Zeyu Zhang, Yunzhong Hou, Zhuowan Li, Gaowen Liu, Ali Payani, Yuan-Sen Ting, and Liang Zheng. Effective training data synthesis for improving mllm chart understanding, 2025. URL https://arxiv.org/abs/2508.06492. 14 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch [43] Kaichen Zhang, Keming Wu, Zuhao Yang, Kairui Hu, Bin Wang, Ziwei Liu, Xingxuan Li, and Lidong Bing. Openmmreasoner: Pushing the frontiers for multimodal reasoning with an open and general recipe. arXiv preprint arXiv:2511.16334, 2025. [44] Liang Zhang, Anwen Hu, Haiyang Xu, Ming Yan, Yichen Xu, Qin Jin, Ji Zhang, and Fei Huang. Tinychart: Efficient chart understanding with visual token merging and program-of-thoughts learning, 2024. URL https://arxiv.org/abs/2404.16635. [45] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [46] Xuanle Zhao, Xianzhen Luo, Qi Shi, Chi Chen, Shuo Wang, Zhiyuan Liu, and Maosong Sun. Chartcoder: Advancing multimodal large language model for chart-to-code generation, 2025. URL https://arxiv.org/ abs/2501.06598. [47] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization, 2025. URL https://arxiv.org/abs/2507.18071. [48] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models, 2024. URL https://arxiv.org/abs/ 2403.13372. [49] Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024. 15 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch"
        },
        {
            "title": "A More Training Details",
            "content": "We implement our training pipeline using the LLaMA-Factory and Verl frameworks, conducted on cluster of 4 nodes equipped with 8 A100 GPUs each (32 GPUs in total). Supervised Fine-Tuning. We perform full-parameter fine-tuning on the Qwen3-VL-Instruct models. The visual encoder is configured to handle dynamic resolutions ranging from 802, 816 to 3, 211, 264 pixels. To support extensive chart reasoning, we set large context cutoff length of 22,000 tokens. To enhance training efficiency, we employ sequence packing and configure the training with global batch size of 128. The model is trained for 1 epoch using the cosine learning rate scheduler with peak learning rate of 1.0 105 and warmup ratio of 0.1. We utilize DeepSpeed ZeRO-3 optimization and BF16 to maximize memory efficiency during the full-parameter update. The entire fine-tuning process for the 8B model takes approximately 1.5 days. Reinforcement Learning. Following SFT, we apply GSPO to further align the models reasoning capabilities. We employ vLLM as the rollout engine, sampling = 16 distinct responses for each prompt with temperature of 1.0 and no top-k restriction. The context window is expanded to support long-chain reasoning, with maximum prompt length of 16,384 tokens and maximum response length of 16,384 tokens. The actor model is optimized with learning rate of 1.0 106, utilizing global batch size of 128. To ensure stability with such long contexts, we disable the standard KL-divergence penalty (βKL = 0) and instead rely on conservative clipping strategy (ratio ϵ [3 104, 4 104]). The RL process runs for approximately 300 steps. Training the 8B model in this stage requires approximately 4 days."
        },
        {
            "title": "B Evaluation Details",
            "content": "We conduct comprehensive evaluation using the VLMEvalKit [5]. Since specific domain-specialized datasets such as ChartX, EvoChart, and ChartBench are not natively integrated, we preprocessed and converted these benchmarks into the standard VLMEvalKit interface format. For evaluation metrics, we replace traditional exact string matching with the compass-verifier [18], which employs an LLM-as-a-Judge to accurately assess response correctness. Regarding rollout settings, following the guidelines from OpenDataArena [3] and the official Qwen documentation, we set the temperature to 0.6, top-p to 0.95, and top-k to 20. Additionally, we apply repetition penalty of 1.05 and limit the maximum response length to 32768 tokens. Furthermore, we exclude all system prompts during inference."
        },
        {
            "title": "C Data Synthesis Details",
            "content": "C.1 Complexity-Aware Chart Coder We first illustrate the distribution of our cold start dataset Ccold, in Table 4. This diverse collection serves as the foundational corpus for initializing our coders. Furthermore, to generate high-quality code representations for complex charts, we utilize Claude-4-Sonnet. The specific prompt designed for this generation process is presented in Table 8. Training Configuration. We train the Complexity-Aware Chart Coder using Qwen2.5-Coder-7B-Instruct as the backbone model. During both training and inference, we employ the identical system prompt 16 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Table 4: Data distribution of the cold start dataset Ccold. CoSyn ChartGen ChartCoder ChartQA FigureQA PlotQA ECD ReachQA Total Count Percentage(%) 22088 35.67 15337 24.77 10214 16. 5602 9.05 4214 6.81 2150 3.47 1771 2.86 543 0.88 61919 100. , as presented in Table 9. The training pipeline is implemented on the LLaMA-Factory framework. We apply the same training configuration across every iterative training stage to ensure consistency. We perform full-parameter fine-tuning with global batch size of 16 and context cutoff length of 4,096 tokens, utilizing sequence packing to maximize computational efficiency. The optimization process runs for 5 epochs using cosine learning rate scheduler, configured with peak learning rate of 2.0 105 and warmup ratio of 0.05. We employ DeepSpeed ZeRO-3 optimization with BF16 precision. Sampling Configuration. During the large-scale sampling phase, we set the sampling temperature to 1.0, top-p to 0.95, and top-k to 20, ensuring diverse and robust generation. RPE Data Generation. Regarding the RPE, we processed approximately 5 million samples. This process involved approximately 40 million inference calls to the Qwen3-VL-2B-Thinking model. Leveraging vLLM [16], our RPE computation is highly efficient, completed in 4 days on 64 A100 GPUs. C.2 Truth-Anchored Inverse QA Synthesis Inverse Logic Construction. Our synthesis pipeline relies on sequence of carefully designed prompts to ensure logical consistency and factual accuracy. We present the specific prompts utilized for Python script generation, reverse question synthesis, and consistency checking in Table 10, Table 11, and Table 12, respectively. Specifically, in the Python script generation phase  (Table 10)  , we enforce strict constraint: the generated code must execute to yield precise numerical value or definitive categorical label, ensuring the answer is deterministic. Following this, during the reverse question synthesis  (Table 11)  , we mandate that the synthesized text must be valid interrogative sentence, and the answer to this question must correspond exactly to the execution result of the preceding Python script. In our specific implementation, we synthesize two distinct Python scripts for each chart code, followed by separate reverse question synthesis and consistency checks. This process involved total of 5 million calls to Qwen-30B-A3B-Thinking, taking approximately 5 days on 128 A100 GPUs. CoT Distillation & Difficulty Filtration. We display the prompt used for distilling Chain-of-Thought (CoT) reasoning traces in Table 13. To ensure high-quality and non-redundant reasoning traces, we apply multi-stage filtering procedure to the distilled outputs: Template and Length Validation: We first impose strict structural validation to ensure the usability of the distilled output. Specifically, we filter out any reasoning trace that fails to adhere to the mandated <think>...</think> and <answer>...</answer> output template. Furthermore, to prevent the retention of superficial or trivial rationales, we enforce minimum length constraint, discarding traces that are shorter than 100 words. N-gram De-duplication: We detect and remove templated or overly repetitive CoTs using an n-gram overlap criterion. Concretely, we flag CoTs that contain any 50-gram that repeats at least 3 times. Flagged traces are discarded. ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Table 5: Performance evolution of ChartVerse across different training phases on chart and STEM benchmarks. Model Qwen3-VL-Instruct-2B + SFT + RL Qwen3-VL-Instruct-4B + SFT + RL Qwen3-VL-Instruct-8B + SFT + RL Chart-Related STEM-Related ChartQA-Pro CharXiv(DQ) CharXiv(RQ) ChartMuseum ChartX EvoChart ChartBench Avg MathVista DynaMath MathVerse LogicVista VisuLogic Avg 42.1 44.4 48.2 53.7 52.9 55.2 54.4 55.5 56. 26.8 40.8 46.9 39.7 52.8 56.2 46.4 56.2 60.8 62.3 69.1 71.2 76.2 84.5 84.1 83.0 88.3 88. 23.9 30.0 37.5 37.2 42.5 45.9 39.6 47.5 49.2 49.8 56.9 60.5 57.2 61.9 63.7 58.2 61.0 63. 53.6 61.0 66.8 68.2 73.3 75.0 70.2 76.7 76.2 38.8 46.4 49.1 45.1 50.3 52.9 46.6 52.2 54. 42.5 49.8 54.3 53.9 59.7 61.9 56.9 62.5 64.1 61.3 58.7 60.4 73.7 70.9 72.8 77.2 75.0 75. 52.1 47.6 49.2 46.8 60.7 62.0 62.1 67.4 69.0 54.2 55.8 56.8 65.3 71.1 71.8 67.7 75.6 76. 35.8 43.2 48.5 53.2 57.3 59.3 55.3 61.3 62.6 11.5 17.5 23.1 19.0 25.0 27.0 22.5 26.5 27. 43.0 44.6 47.6 51.6 57.0 58.6 56.7 61.2 62.2 Table 6: Comparison of Different Datasets. All models are trained on Qwen3-VL-4B-Instruct using 100K samples. Training Data (100K) ChartQA CharXiv CharXiv (RQ) (DQ) Pro Chart Museum ChartX Evo Chart ChartBench Avg (GPT-acc) Qwen3-VL-4B-Instruct + CoSyn + START + ECD + ChartVerse-SFT (Ours) 53.7 50.3 53.5 54.4 53.4 39.7 35.1 39.2 38.3 48. 76.2 74.4 77.2 77.5 82.6 37.2 33.5 37.9 37.7 40.3 57.2 55.8 56.4 58.3 60. 68.2 62.4 68.0 68.8 70.9 45.1 45.8 44.5 44.7 48.7 53.9 51.0 53.8 54.2 57. Table 7: Comparison of data selection strategies. The Fail Rate (left) indicates sample difficulty, while the right section shows the results of Qwen3-VL-4B-Instruct trained on 100k selected data. Strategy Difficulty Fail Rate (N=500) Downstream Performance (%) ChartQA CharXiv CharXiv (RQ) (DQ) Pro Chart Museum ChartX Evo Chart ChartBench Avg (GPT-acc) Qwen3-VL-4B-Instruct + VLM-as-Judge + Python Code Complexity + RPE (Ours) - 21.1% 23.5% 27.6% 53.7 53.4 53.7 53. 39.7 40.3 40.5 41.0 76.2 75.7 77.7 78.6 37.2 38.8 39.0 39. 57.2 58.6 58.5 58.5 68.2 69.2 70.3 70.8 45.1 45.4 44.7 46. 53.9 54.5 54.9 55.4 In this step, we processed approximately 1 million samples. With each sample requiring three distinct inference calls, this amounted to total of 3 million calls to Qwen3-VL-30B-A3B-Thinking, consuming approximately 7 days on 128 A100 GPUs. Throughout all processes in this stage, we set the inference parameters to temperature of 0.6, top-p of 0.95, top-k of 20, and maximum output length of 32768."
        },
        {
            "title": "D Additional Results",
            "content": "D.1 Detailed Performance Analysis of Training Phases We provide the specific performance corresponding to Figure 3 in Table 5. The results across the 2B, 4B, and 8B scales demonstrate consistent upward trajectory. ChartVerse-8B achieves significant leap on CharXiv (RQ), rising from baseline of 46.4 to 60.8. Similarly, on ChartMuseum, it reaches peak performance of 49.2. The reasoning skills acquired from chart-specific data also generalize effectively to out-of-domain STEM tasks. ChartVerse-8B improves its performance on MathVerse from 67.5 to 76.5 and on DynaMath from 62.1 to 69.0. These gains confirm that the logical reasoning capabilities learned through our framework transfer robustly to complex scientific contexts. 18 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch D.2 Comparison with Existing Datasets Complementing the visual comparison in Figure 4a, Table 6 evaluates ChartVerse-SFT against competing datasets under fixed 100K sample budget. While datasets like CoSyn and START degrade performance on specific benchmarks, ChartVerse-SFT consistently drives improvements. Specifically, our dataset boosts ChartMuseum to 40.3 and significantly lifts CharXiv (RQ) performance from 39.7 to 48.8, confirming that our gains stem from superior data quality rather than scale. D.3 Ablation on RPE Strategy The effectiveness of the Rollout Posterior Entropy strategy, as shown in Figure 4b, is further quantified in Table 7. RPE successfully identifies intrinsically harder samples, evidenced by model failure rate of 27.6, compared to only 21.1 for VLM-as-Judge. This focus on difficulty translates directly to downstream accuracy: on the CharXiv (DQ) benchmark, the RPE-trained model achieves 78.6, outperforming both VLM-as-Judge 75.7 and Python Code Complexity 77.7. ChartVerse-SFT Analysis We first demonstrate the visual complexity of our synthesized training corpus, ChartVerse-SFT. As illustrated in Figure 6, the dataset exhibits exceptional diversity by covering wide spectrum of visualization types. These range from standard statistical graphs such as violin plots and radar charts to high-dimensional 3D representations and hierarchical structures like treemaps. Crucially, significant portion of the data features intricate multi-subplot layouts and mixed-type dashboards. This structural complexity compels the model to handle diverse visual encoding schemas and perform finegrained reasoning across spatially distinct panels, significantly surpassing the difficulty of traditional chart datasets. Furthermore, we showcase the intricate QA pairs generated within our dataset through specific case studies shown in Figures 7, 8, and 9. Leveraging the high information density of the synthesized charts, our pipeline is capable of formulating highly challenging queries. As evident in these examples, the questions transcend simple data retrieval by necessitating rigorous multi-step reasoning and cross-subplot integration. For instance, the example in Figure 7 presents multi-condition verification task where the model must sequentially validate three separate metrics including average scores and total efficiency across distinct experiment sub-panels. Similarly, Figure 8 demonstrates demand for complex derived calculations. Here the model must visually extract statistical markers such as quartiles and medians to determine the region with the greatest proportional variation. This design ensures that the model must develop holistic understanding of the visual context rather than relying on local pattern matching. Finally, Figure 9 requires the model to aggregate information from multiple spatially distinct plots. To identify the element with the optimal trade-off, the model must synthesize data regarding power levels, temperature stability, and complexity scores from three separate charts. 19 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Figure 6: Overview of the ChartVerse-SFT dataset. The samples demonstrate high diversity in chart types including 3D, hierarchical, and statistical plots, as well as structural complexity featuring various multi-subplot layouts. 20 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Figure 7: ChartVerse-SFT QA Example21 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Figure 8: ChartVerse-SFT QA Example-2 22 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Figure 9: ChartVerse-SFT QA Example23 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Table 8: Prompt for Claude-4-Sonnet to generate code Prompt for Claude-4-Sonnet to generate code ## Role You are matplotlib expert. Given chart image, generate Python code that accurately reproduces this visualization. ## Inputs Chart Image: The visualization to reproduce ## Requirements 1. Code Structure: Import necessary libraries Include clear comments explaining the code only (no words like reproduce, match, original, etc.) Ensure code is fully executable DO NOT use read csv or any file reading operations Define all data directly in the code as variables/lists/dictionaries DO NOT use plt.show(), save the figure as image.png instead 2. Visual Elements to Match: Chart type and all data points Colors, styling, and proportions Title, axis labels, legend, and grid 3. Text Positioning: Ensure all text labels are clearly visible and DO NOT overlap Include loc=best parameter when setting legend to avoid text conflicts Position each label carefully in its appropriate location Adjust text placement as needed to maintain readability Use plt.tight layout() to optimize overall spacing 4. Output Format: First briefly analyze the chart type and data structure. Then provide complete matplotlib code: ˋˋˋpython [Code] ˋˋˋ ## Key Points IMPORTANT: ensure data is reasonable(avoid nan) Text labels must not overlap - adjust positions to keep them readable Match visual style as closely as possible 24 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Table 9: System Prompt for Complexity-Aware Chart Coder System Prompt for Complexity-Aware Chart Coder ## Role You are Python visualization expert. ## Goal Generate random Python visualization code focusing on charts, tables, or diagrams. ## Requirements 1. Scope & Tools: Choose any visualization type (chart, table, flowchart, diagram, etc.). Create comprehensive sample data. Use standard Python visualization libraries (e.g., matplotlib, graphviz). 2. Visual Design: Ensure the visualization is visually appealing with proper labels, titles, and color schemes. Include sufficient visual elements to demonstrate complexity. 3. Layout & Clarity (Critical): Carefully design the layout to avoid any overlapping text or elements. Adjust figure size, margins, and spacing parameters for optimal clarity. 4. Output Format: Only output the Python visualization code wrapped in: ˋˋˋpython [Code] ˋˋˋ 25 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Table 10: Prompt for Inverse Logic Construction - Step 1: Logic Architecture & Code Generation Step 1: Logic Architecture & Code Generation ## Role You are an expert Data Scientist specializing in Algorithmic Synthesis. Your task is to look at raw data arrays from visualization code and engineer sophisticated, multi-step quantitative operations without needing instructions. ## Objective You will be provided with Visualization Code Snippet. Your workflow is: 1. Decode: Extract the raw data signals from the visualization code. 2. Engineer: Design multi-stage analysis logic. 3. Implement: Write standalone Python script that executes this logic to produce single significant result (a specific number or text label). ## Design Constraints (Difficulty Control) Target Level: The logic must be accessible to High School student. Linearity: Focus on clear, linear logic, but avoid trivial operations. ## Logic Architecture (The Triangle of Complexity) You must invent calculation path that adheres to these rules: 1. Cross-Variable Correlation: You must use interactions between at least three different data lists. 2. Multi-Subplot Integration (CRITICAL): If the input code contains multiple subplots, your calculation logic MUST synthesize data across at least 2 subplots. 3. Structural Dependency: Do not simply process the dataset uniformly. 4. Derived Aggregation: The final output must be single synthesized value (numerical scalar OR categorical label). ## Output Requirements 1. Data Context: brief professional summary of what the data represents. 2. Logic Blueprint: step-by-step text description of the algorithm. 3. The Python Script (The Solution): Self-Contained: Define all data lists explicitly inside the script. Narrative Variable Naming: Variable names MUST serve as documentation. Atomic Output: The script must end with exactly ONE print() statement showing ONLY the final result. Wrap the code in: <answer>[Your Code] </answer> ## Input Data Input Code for Analysis: {chart code} 26 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Table 11: Prompt for Inverse Logic Construction - Step 2: Semantic Reverse-Engineering Step 2: Semantic Reverse-Engineering (Question Synthesis) ## Role You are an expert in Reverse-Engineering complex code logic into natural, high-level business or scientific inquiries. ## Objective Your task is to formulate the Analytical Question that would prompt human to write exactly the provided Solution Script based on the Original Visualization Code. ## Instructions Phase 1: Logic Analysis Analyze the Solution Script to understand what is being calculated. Look at the variable names to determine the INTENT. Phase 2: Question Formulation Write the question that requires this exact logic to solve. Constraint A: Explicit Target (Crucial) The question must ask for specific number OR specific entity. Avoid Vague Phrasing: Ask for the specific metric calculated. Constraint B: Strict Logic Mapping The Code IS The Answer: Ensure the provided Python script is the exact, step-by-step solution to your question. Constraint C: Semantic Abstraction (Most Important) PROHIBITION: Do NOT describe mechanical steps (e.g., Add list and B). REQUIREMENT: Describe the INTENT and DOMAIN MEANING (e.g., Calculate the profit margin). ## Output Format Structure your response strictly as follows: 1. Logic interpretation: Briefly explain the semantic meaning of the Python script. 2. The Analytical Question: The abstract, scenario-based question wrapped in <question>... </question>. ## Input Data 1. Original Visualization Code: {chart code} 2. The Solution Script (The Answer Logic): {generated python code} 27 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Table 12: Prompt for Inverse Logic Construction - Step 3: LLM Inference Step 3: LLM Inference Prompt ## Role You are an expert in Chart Code Comprehension and Data Reasoning. Your task is to answer specific question by interpreting the raw data structures defined in Python code snippet. ## Core Principles 1. Code as Data Source: Treat the Python code as structured document containing the ground truth data. You do not need to run the code, but you must read and understand the variables (lists, dictionaries, values). 2. Semantic Mapping: Map the terms in the question to the corresponding variables and indices in the code. 3. Logical Derivation: Based on the extracted data, perform the necessary logical reasoning to answer the question. ## Solution Framework Phase 1: Code Structure Analysis Scan the code to identify key variables and data lists. Understand the relationship between different lists (e.g., axis usually corresponds to axis by index). Phase 2: Information Extraction & Reasoning Locate Data: Pinpoint the specific data points (values, labels, axis limits) in the code required by the question. Step-by-Step Derivation: Execute clear, logical reasoning process. First, explicitly quote the values found in the code. Then, perform the necessary logical comparisons or arithmetic calculations step-by-step to derive the answer. Note: Rely ONLY on the information explicitly present in the code. Phase 3: Answer Formulation Formulate clear, concise answer based on your findings. Ensure the answer directly addresses the inquiry. ## Strict Formatting Requirements 1. The final result goes ONLY inside <answer>...</answer> tags. 2. The last line of your response must be EXACTLY: Therefore, the final answer is <answer>ANSWER</answer>. ## Input Data 1. Chart Code: {chart code} 2. Question: {generated question} 28 ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch Table 13: Prompt for Vision-Language Model (VLM) CoT Distillation CoT Distillation Prompt ## Role You are an expert in science and visual reasoning with advanced capabilities in multimodal analysis. Your response will be used as high-quality example to train new AI model. Solve the problem efficiently and clearly by integrating ALL information from multimodal inputs. ## Core Principles 1. Equal Weight to All Inputs: Information from images is AS IMPORTANT as text. Never ignore visual elements. 2. Systematic Analysis: Follow rigorous, reproducible approach. 3. Precision and Accuracy: Double-check all calculations. 4. Adaptive Reasoning: Choose the most appropriate method based on context. ## Solution Framework Phase 1: Comprehensive Information Extraction Carefully analyze ALL text content for requirements and constraints. Thoroughly examine ALL visual elements, extracting every piece of relevant information. Explicitly connect visual and textual information. Phase 2: Strategic Problem Setup Compile information and clearly state what needs to be found. Identify relevant principles and state assumptions. Phase 3: Rigorous Solution Execution Present solution with complete logical flow and proper notation. Reference specific parts of visual inputs to support reasoning. Maintain unit consistency and precision. Phase 4: Solution Validation Verify the answer makes scientific/logical sense. Ensure dimensional analysis is correct. ## Strict Formatting Requirements 1. The final result goes ONLY inside <answer>...</answer> tags. 2. Include units inside the tags when required (e.g., <answer>5.2 m/s</answer>). 3. The last line must be EXACTLY: Therefore, the final answer is <answer>ANSWER</answer>. ## Input Data Problem: {question} (Visual Inputs are provided via the vLLM interface)"
        }
    ],
    "affiliations": [
        "Peking University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "The University of Hong Kong"
    ]
}