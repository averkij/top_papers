{
    "paper_title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
    "authors": [
        "Zhiwei Zhang",
        "Fei Zhao",
        "Rui Wang",
        "Zezhong Wang",
        "Bin Liang",
        "Jiakang Wang",
        "Yao Hu",
        "Shaosheng Cao",
        "Kam-Fai Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents."
        },
        {
            "title": "Start",
            "content": "Robust Tool Use via FISSION-GRPO: Learning to Recover from Execution Errors Zhiwei Zhang1, Fei Zhao2, Rui Wang1, Zezhong Wang1, Bin Liang1, Jiakang Wang2, Yao Hu2, Shaosheng Cao 2, Kam-Fai Wong 1 1The Chinese University of Hong Kong, 2Xiaohongshu Inc. zhangzhiwei1019@link.cuhk.edu.hk, caoshaosheng@xiaohongshu.com 6 2 0 2 2 ] . [ 1 5 2 6 5 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the models on-policy error modes. To bridge this gap, we propose FISSION-GRPO, framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into new training instance by augmenting it with diagnostic feedback from finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, FISSION-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding 4% overall accuracy gain (42.75% 46.75%) over GRPO and outperforming specialized tool-use agents."
        },
        {
            "title": "Introduction",
            "content": "Agentic AI is moving from prototypes to production, driving demand for tool-using agents that are not only capable but also efficient enough for lowlatency and on-device deployment (Belcak et al., 2025). Smaller language models (SLMs) are increasingly recognized as the practical foundation for such systems (Sharma and Mehta, 2025). However, for SLMs to fulfill this role, they must exhibit robustness, the ability to handle the inevitable 1 (a) Typical failure: an API error triggers hallucinated retry loop. (b) Error recovery rates on BFCL v4 Multi-Turn across model scales and evaluation subsets. Figure 1: Error recovery is key bottleneck for smaller toolusing models in multi-turn execution. (a) shows representative hallucinated retry loop after an API error, while (b) reports recovery rates on BFCL v4 across model scales. execution errors that arise in dynamic, multi-turn tool-use environments (Patil et al., 2025). This robustness requirement exposes critical gap. In practice, APIs return errors, parameters become invalid, and system states change unexpectedly; robust agent must interpret such feedback, diagnose the fault, and self-correct (Yao et al., 2022; Shinn et al., 2023). Yet as shown in Figure 1b, smaller models exhibit pronounced deficiency in this error recovery capability (defined as success rate conditioned on at least one prior execution error). On the BFCL v4 Multi-Turn benchmark (Patil et al., 2025), Claude Sonnet 4 achieves recovery rates exceeding 50%, while Qwen3-8B averages only around 20% across the four evaluation subsets. Figure 1a illustrates representative failure mode: upon receiving an error (e.g., StateConflict), the model fails to diagnose the root cause and instead hallucinates invalid parameters (e.g., non-existent force argument), entering repetitive loop until the conversation collapses. Bridging this robustness gap is essential for enabling smaller models to serve as reliable foundations for agentic systems. Current approaches fall short of addressing this challenge. Methods based on static synthetic datasets (Liu et al., 2024; Zhang et al., 2025a,b) construct error-correction pairs offline, but the error distribution shifts as the policy improves, making offline error corpora quickly stale and leading to distribution mismatch. Meanwhile, reinforcement learning (RL) approaches such as GRPO (Shao et al., 2024) treat errors merely as sparse negative rewards. This signals that something went wrong, but offers no guidance on how to recover: the gradient discourages the failed action without teaching corrective alternative. When all sampled rollouts fail, the advantage variance collapses, yielding vanishing gradients that stall learning entirely (Yu et al., 2025; Nan et al., 2025). In essence, existing methods treat errors as outcomes to be avoided rather than opportunities to be learned from. To bridge this gap, we propose FISSION-GRPO, framework that transforms execution errors into dense, on-policy-aligned corrective supervision (Figure 2). The framework operates in three stages. In Stage 1, we perform standard GRPO exploration, sampling multiple rollouts per query and updating the policy with group-relative advantages under the GRPO objective. In Stage 2, failed rollouts are intercepted and augmented with diagnostic feedback from learned Error Simulator, constructing corrective contexts of the form [dialogue; failed call; feedback]. In Stage 3, these contexts trigger fission update: each error is expanded into parallel recovery attempts by resampling new rollouts conditioned on the augmented context, analogous to nuclear fission where one event induces multiplicative chain of subsequent reactions and thus generates many new training signals. The Error Simulator is trained via supervised fine-tuning to produce realistic, context-aware diagnostics that resemble runtime error traces. To avoid trivial target leakage, its outputs are restricted to non-revealing error descriptions (e.g., parameter status expects value OPEN) rather than the full target call. This closed-loop process continuously focuses learning on the models current error modes, mitigating the distribution mismatch of static error-correction datasets. We evaluate FISSION-GRPO on the BFCL v4 Multi-Turn benchmark and demonstrate substantial improvements. Our main contributions are: Fission-GRPO Framework. We propose RL framework that dynamically converts execution errors into corrective training instances. By resampling from augmented error contexts onpolicy, our approach maintains alignment with the models evolving error distribution. Learned Error Simulator. We develop supervised fine-tuned error simulator to generate realistic diagnostic feedback resembling runtime error traces, enabling effective recovery training without live API interactions or target leakage. Empirical Validation. On the BFCL v4 MultiTurn benchmark, FISSION-GRPO achieves stateof-the-art performance across the Qwen3 model family (1.7B, 4B, and 8B), consistently outperforming GRPO, DAPO, and Dr.GRPO baselines. For Qwen3-8B, our method improves the error recovery rate by 5.7% absolute, yielding 4% overall accuracy gain (42.75% 46.75%) and surpassing specialized 8B-scale tool agents."
        },
        {
            "title": "2.1 RL for Tool Use",
            "content": "RL has become the standard for aligning LLMs (Schulman et al., 2017; Ouyang et al., 2022). Among recent algorithms, GRPO (Shao et al., 2024) reduces memory overhead by estimating baselines from group averages, making it particularly suitable for tool-calling tasks characterized by binary or scalar rewards (Guo et al., 2025). Despite its efficiency, GRPO relies on intragroup variance, creating vulnerabilities when In sampled group is homogeneously incorrect. such cases, the reward variance drops to zero, yielding null gradients and wasting training signalsa limitation targeted by DAPO (Yu et al., 2025) and 2 Figure 2: Overview of the FISSION-GRPO Framework. The framework operates in three stages: (1) Standard Exploration, utilizing GRPO to optimize policy πθ on the query distribution D; (2) Error Identification & Synthesis, where simulator Sϕ generates diagnostic feedback for filtered error trajectories; and (3) Fission-based Update, where corrective samples trigger multiplicative resampling process (factor G) to align the policy with recovery paths. NGRPO (Nan et al., 2025). Furthermore, even when gradients exist, blindly applying negative feedback can trigger Lazy Likelihood Displacement (LLD) (Deng et al., 2025), where valid reasoning steps are suppressed simply because they appear in failed trajectories."
        },
        {
            "title": "While strategies exist",
            "content": "to mitigate these issues, such as filtering homogeneous batches (DAPO), calibrating advantages (NGRPO), or down-weighting negative gradients (NTHR (Deng et al., 2025)), they primarily optimize the loss landscape of negative signals. They do not fundamentally address the scarcity of positive guidance during exploration. Our approach bridges this gap by actively constructing recovery trajectories via fission, transforming zero-reward errors into dense, supervised learning signals."
        },
        {
            "title": "Synthesis",
            "content": "Research in tool utilization has evolved from ensuring syntactic correctness in single-turn interactions (Schick et al., 2023; Patil et al., 2024) to maintaining reliability across complex, multi-turn workflows (Qin et al., 2023; Yao et al., 2022). As tasks grow in complexity, the capacity to recover from inevitable environment errors (e.g., timeouts, invalid parameters) becomes defining metric of robustness. This requirement is codified in benchmarks like BFCL (Patil et al., 2025) and StableToolBench (Guo et al., 2024), which specifically evaluate persistence under error conditions. To address these challenges, recent approaches have formalized diagnosis-and-repair mechanisms (Su et al., 2025; Huang et al., 2025) or trained models on diverse error scenarios (Vuddanti et al., 2025). In parallel, synthetic correction methods, originally proven in reasoning domains (Pan et al., 2025; Xu et al., 2025), have been adapted to tool-use by frameworks like ToolACE (Liu et al., 2024) and LoopTool (Zhang et al., 2025b) to expand training coverage through model-based synthesis. However, critical limitation persists: these methods predominantly rely on offline data construction. This creates temporal mismatch where static training data fails to reflect the models evolving on-policy error distribution (Kumar et al., 2024; Zhang et al., 2025b). Unlike prior offline synthesis approaches (Pan et al., 2025; Su et al., 2025), our work integrates error simulation directly into the training loop, ensuring alignment with current policy limitations."
        },
        {
            "title": "3 Method",
            "content": "We propose FISSION-GRPO, framework designed to imbue small language models with robust error recovery capabilities. As illustrated in Figure 2, our approach operates in dual-stream manner: standard exploration to maintain general tool3 use competence, and conditional fission stream that intercepts errors to enable active remedial learning. 3.1 Preliminaries We formulate tool use as language generation task. Given query and tool library, policy πθ generates trajectory τ consisting of reasoning thoughts and tool calls. We adopt GRPO (Shao et al., 2024) as our optimization backbone. Unlike PPO, GRPO eliminates the need for value network by estimating the baseline from the group average. For each query x, we sample group of outputs {τi}G i=1 and optimize: (θ) = ExD (cid:34) 1 (cid:88) ˆR(τi) πratio(τi) βDKL (cid:35) i=1 (1) where ˆR(τi) = R(τi)µR is the normalized reward, with µR and σR being the mean and standard deviation of rewards within the group, and πratio is the clipped probability ratio. σR+ϵ"
        },
        {
            "title": "3.2 Reward Design",
            "content": "To guide the policy from syntactic compliance to semantic precision, we design time-dependent composite reward function R(τ, t), where denotes the training step. The total reward is weighted sum of three components: Format Compliance (Rfmt). This binary term Rfmt(τ ) {0, 1} enforces structural constraints, ensuring outputs adhere to the required XML/JSON schema. We apply decaying weight wfmt(t) that reduces its maximum contribution from 2 to 1, shifting focus from syntax to semantics as training progresses. Functional Correctness (Rcorr). This term evaluates alignment between invoked tools and user intent. To accommodate partial matching in complex parameters, we define Rcorr [0, 2] as: Rcorr(τ, y) =α I(N = )+ (1 α) 1 (cid:88) F1(a, a) (a,a)M from 2 to 3 to prioritize parameter precision in later stages. Efficiency Regularization (Rlen). To prevent verbose or degenerate reasoning, we impose length penalty Rlen [0, 1] via piecewise Gaussian function with time-annealing tolerance. 3.3 The FISSION-GRPO Framework As illustrated in Figure 2, FISSION-GRPO operates in three-stage closed loop. Stage 1 focuses on optimizing fundamental tool-use capabilities, while Stages 2 and 3 are dedicated to developing error recovery skills through targeted error correction. 3.3.1 Stage 1: Standard Exploration and Update This stage aims to establish and maintain the models base performance on tool-calling tasks. Sampling and Evaluation. Given query x, we sample group of trajectories {τi}G i=1 from the current policy πθ. We evaluate these rollouts using the composite reward function defined in 3.2, computing format compliance Rfmt, functional correctness Rcorr, and efficiency regularization Rlen, which are then aggregated into the total reward. Optimization. We apply the standard GRPO update (Eq. 1) using these trajectories to improve the models fundamental tool-use capabilities. This step ensures continuous optimization on the core skill of accurate tool invocation and parameter grounding. Subsequently, all sampled trajectories from this stage are forwarded to Stage 2 for diagnostic error analysis and corrective training."
        },
        {
            "title": "3.3.2 Stage 2: Error Identification and\nCorrective Sample Construction",
            "content": "Stage 2 converts error traces produced in Stage 1 into actionable corrective instances. Concretely, we apply two-level filter to isolate erroneous trajectories and then synthesize feedback that can be appended to the original context for subsequent corrective updates. (2) where I(N = ) indicates correct function selection, denotes matched argument pairs between prediction τ and ground truth y, and F1 measures token-level overlap. The weight wcorr(t) increases monotonically, scaling its maximum contribution Error Identification. We decompose error detection into format validity and functional correctness. Let Rfmt denote whether the tool-call format is valid. If Rfmt(τ ) = 0, the trajectory is immediately treated as an error without consulting correctness. Otherwise, we further evaluate correctness with 4 scalar score Rcorr and flag the trajectory when it falls below tunable threshold δcorr: the on-policy approximation in multi-turn tool-use training. = {τi Rcorr(τi) < δcorr Rfmt(τi) = 0} (3) In Fig. 2, we use simplified illustration (e.g., < δ) to emphasize the gating effect; this does not contradict Eq. (3). Hybrid Feedback Synthesis. For effective correction, scalar penalty is insufficient; we require an explicit diagnostic message that resembles the runtime system feedback. We adopt hybrid strategy: (i) for format errors (Rfmt = 0), we use deterministic error messages (e.g., parser/compiler-style feedback) to explicitly state the violated schema/serialization constraints; (ii) for semantic errors (Rcorr < δcorr), we query learned Error Simulator Sϕ to produce concise, actionable runtime error string. The simulator is implemented as Qwen332B model fine-tuned via SFT to emulate runtime environment responses. We construct training set of approximately 2K instances from error logs, where each instance comprises: (i) the original system prompt and tool specification along with the dialogue state, (ii) the models failed tool call (τerr), (iii) the ground-truth tool call (τgt), and (iv) teacher-written diagnostic error message (e.g., generated via ClaudeSonnet-4), followed by quality filtering. During both training and inference, the simulator consumes (system + tools, dialogue history, τgt, τerr) and produces concise feedback string Sϕ(x, τerr, τgt), where is constrained to be realistic runtime response. We provide the exact prompting template used to query Sϕ in Appendix A. Corrective Sample Construction and LIFO Buffering. Given flagged trajectory τerr with feedback , we construct corrective context by appending the failed attempt and the diagnostic message to the original multi-turn input: xcorr = (cid:2) x; τerr; (cid:3). (4) We optionally deduplicate corrective instances by hashing (x, τerr) to avoid repeatedly training on near-identical errors. All corrective samples are stored in LIFO buffer Bcorr, so that the most recent errors are consumed first during corrective updates. This design keeps the corrective batch distribution closer to the current policy πθ, improving 3.3.3 Stage 3: Corrective Batch Training Once the LIFO buffer accumulates sufficient recent errors (Batch Trigger), we activate Fission to perform targeted remedial updates for recovery. Multiplicative Resampling. We pop the freshest corrective contexts xcorr and, for each of them, sample fission group of trajectories conditioned on the same context: {τ j}G j=1 πθ( xcorr). (5) This turns single error case into multiple parallel recovery attempts, densifying training signals around the observed error. More Informative Advantages. Hard queries can yield near-homogeneous outcomes in standard exploration, weakening within-group relative advantages. Conditioning on explicit feedback typically increases outcome diversity within the fission group, improving the usefulness of advantage estimates for recovery updates. We optimize the same GRPO-style objective as Eq. (1), but over the corrective distribution: Jcorr(θ) = Excorr"
        },
        {
            "title": "1\nG′",
            "content": "G (cid:88) j=1 ˆA(τ j) log πθ(τ xcorr) . (6) Summary. These three stages form continuous loop; detailed pseudocode and hyperparameters are provided in Algorithm 1 (Appendix B)."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Data Construction Diverging from prevalent tool-learning paradigms that emphasize extensive scaling of synthetic corpora (e.g., ToolACE (Liu et al., 2024), XLAM (Zhang et al., 2025a)), we prioritize data quality and trajectory correctness. We implement three-stage pipeline to construct compact yet rigorous training set: (1) Domain Schema Curation: We curated diverse schema library spanning 11 domains (e.g., Healthcare, Smart Home, Vehicle Control), prompting Claude-4-Sonnet to generate realistic API definitions grounded in BFCL characteristics. (2) Trajectory Synthesis: Utilizing Claude-4Sonnet, we first synthesized multi-turn user queries 5 Model Method Overall Acc Base Miss Func Miss Param Long Context Qwen3-1.7B Qwen3-4B Base GRPO DAPO Dr.GRPO FISSION-GRPO (Ours) Base GRPO DAPO Dr.GRPO FISSION-GRPO (Ours) External 8B Agent Models ToolACE-2-8B BitAgent-8B Qwen3-8B Base GRPO DAPO Dr.GRPO FISSION-GRPO (Ours) 7.80% 10.00% 17.12% 22.00% 16.00% 22.00% 16.12% 19.50% 20.38% 29.00% 19.37% 24.50% 36.38% 46.50% 38.25% 48.50% 34.75% 43.00% 40.87% 51.50% 37.00% 47.00% 37.75% 46.50% 28.75% 35.50% 42.75% 50.50% 43.12% 54.50% 44.88% 55.00% 46.75% 57.50% 11.00% 18.50% 17.00% 17.50% 18.50% 19.00% 34.50% 36.50% 34.50% 42.50% 31.00% 37.50% 37.00% 41.00% 44.50% 45.00% 43.50% 8.00% 15.50% 14.00% 14.50% 16.00% 14.50% 27.50% 28.00% 27.50% 30.50% 28.00% 24.00% 22.50% 36.00% 29.00% 32.50% 38.00% 2.50% 12.50% 11.00% 13.00% 18.00% 19.50% 37.00% 40.00% 34.00% 39.00% 42.00% 43.00% 20.00% 43.50% 44.50% 47.00% 48.00% Table 1: Performance comparison on BFCL V4 Multi-Turn benchmark across model scales and training methods. based on these schemas, followed by generating full interaction trajectories that fulfill the requests. (3) Hierarchical Filtering and Factorization: To ensure rigorous quality control, we applied hierarchical protocol. First, raw trajectories underwent global coherence check via Claude-4Sonnet. Validated trajectories of length were then factorized into discrete decision instances {(ht, at)}K t=1, where ht denotes the cumulative context history. Finally, these decomposed instances underwent double-blind verification via Qwen3-235B-A22B-Instruct-2507 (Team, 2025) and Kimi K2 (Team et al., 2025). Only samples achieving unanimous consensus were retained, distilling an initial pool of 2,000 trajectories down to 630 high-quality training instances. Training Details All models are trained using the Verl framework (Sheng et al., 2024) on single node with 8H800 80GB GPUs. For GRPO training, we use learning rate of 1e-6 with cosine warmup, batch size of 8, and sample 8 rollouts per query (G = 8). The maximum prompt length is set to 12,800 tokens and the maximum response length to 4,096 tokens. We use temperature 0.95 and top-k 50 for sampling. For FISSION-GRPO, we set the correctness threshold δcorr = 1 for error identification (Eq. 3), determined empirically as stable threshold across multiple runs. cally chosen for its stress-testing of state tracking and robustness. distinctive feature of this benchmark, crucial to our study, is its interactive error feedback mechanism: upon execution error, the environment provides explicit error traces and permits the agent up to 20 retries to correct its action. This setup directly aligns with our research objective, allowing us to measure how improved error recovery dynamics translate to overall success rates in tool use. Baselines. We compare FISSION-GRPO against advanced RL baselines implemented on the Qwen3 series (1.7B/4B/8B), including: (1) GRPO (Shao et al., 2024), utilizing group-normalized advan- (2) DAPO (Yu et al., 2025), tages; incorporating dynamic sampling constraints; and (3) Dr.GRPO (Liu et al., 2025), employing meancentered estimators to mitigate length bias. For broader context, we also report performance of specialized 8B-scale tool agents such as ToolACE (Liu et al., 2024) and BitAgent."
        },
        {
            "title": "4.2 Main Results",
            "content": "Table 1 presents the performance on the BFCL v4 Multi-Turn benchmark. As shown, FISSIONGRPO achieves consistent state-of-the-art (SOTA) performance across all Qwen3 model scales (1.7B, 4B, and 8B) when compared to other GRPO-based post-training methods. Benchmarks. We evaluate on the BFCL V4 Multi-Turn benchmark (Patil et al., 2025), specifiScalability and Subset Performance. Our method demonstrates superior scalability and ro6 bustness. On Qwen3-1.7B, FISSION-GRPO yields substantial improvement, elevating overall accuracy to 20.38%, relative gain of over 160% compared to the Base model (7.80%) and surpassing standard GRPO (17.12%). As the model scale increases, the performance gap remains distinct: on Qwen3-4B and Qwen3-8B, our method achieves 40.87% and 46.75% accuracy respectively, outperforming strong baselines like DAPO and Dr.GRPO. Notably, FISSION-GRPO excels in the Base and Miss Param categories, achieving the highest scores across most settings (e.g., 57.50% on 8B Base and 30.50% on 4B Miss Param), indicating precise understanding of function calls and parameters. Comparison with Specialized Agents. Furthermore, we compare our generalist approach with specialized 8B-scale tool agents. FISSIONGRPO (Qwen3-8B) significantly outperforms both ToolACE-2-8B (37.00%) and BitAgent-8B (37.75%) by margins of 9.75 and 9.00 percentage points, respectively. This underscores the efficacy of our method in enhancing tool-use capabilities beyond varying baselines."
        },
        {
            "title": "4.3 Error Recovery Analysis",
            "content": "To identify the source of performance gains, we decouple the overall success rate into two components: One-Shot Success (success achieved without triggering any errors) and Error Recovery Rate (conditional probability of success after an error occurs). Figure 3 illustrates this breakdown for the Qwen3-8B model. The results clearly indicate that the performance improvement is primarily driven by enhanced error recovery capabilities. FISSION-GRPO yields an average improvement of 5.7% in Error Recovery Rate across all categories, with particularly substantial gains in Long Context (+11.8%) and Base (+5.5%) scenarios. This enhanced recovery capability serves as the primary contributor to the overall accuracy improvement from 42.75% to 46.75%. Crucially, this gain does not come at the expense of fundamental capabilities. The One-Shot Success Rate is preserved and even modestly improved by an average of 1.75%, with notable gains in Long Context (+3.5%) and Base (+2.0%). This confirms that our framework effectively equips the model with robust recovery skills while simultaneously enhancing its standard tool-use competence, demonstrating that the fission mechanism provides comFigure 3: Performance decomposition on BFCL v4 MultiTurn (Qwen3-8B). Method Avg. Base M.Func M.Param Long 17.12 22.00 GRPO Static 17.75 23.50 Dynamic 20.38 29.00 Qwen3-1.7B 18.50 21.00 18.50 36.38 46.50 GRPO Static 37.25 50.50 Dynamic 40.87 51.50 Qwen3-4B 34.50 34.00 42.50 42.75 50.50 GRPO Static 44.00 53.50 Dynamic 46.75 57.50 Qwen3-8B 41.00 43.00 43. 15.50 15.50 16.00 27.50 28.50 30.50 36.00 35.50 38.00 12.50 11.00 18.00 37.00 36.00 39.00 43.50 44.00 48. Table 2: Ablation on Feedback Quality. Static denotes Fission training with generic error prompts; Dynamic uses our simulated feedback. Avg. denotes Overall Accuracy; M.Func and M.Param denote Missing Function and Missing Parameter errors, respectively. plementary benefits to both error prevention and error correction. 4."
        },
        {
            "title": "Impact of Feedback Quality",
            "content": "To disentangle the contribution of the Fission mechanism from the informational gain of the Error Simulator, we conduct an ablation study across three settings: (1) GRPO: The standard baseline (2) Fissionwithout explicit recovery training. Static: Applies the fission update but uses fixed, generic error message for all errors.1 (3) FissionDynamic: Our full method using the Error Simulator for context-aware feedback. Results in Table 2 reveal hierarchical improvement. First, Fission-Static consistently outperforms GRPO. Even with uninformative feedback, the explicit process of re-sampling and penalizing failed trajectories forces the model to refine its internal state tracking (e.g., +1.25% on Qwen3-8B Overall Acc). This validates that the structural intervention 1The static prompt is: ERROR: Function call failed. Please verify your output format, function name, required parameters, and parameter values are correct. 7 level errors and long-context interactions particularly benefit from timely correction signals. These results suggest that standard policy optimization alone does not reliably prevent error patterns from accumulating and reappearing over long horizons, whereas correction updates serve as stabilizing mechanism for multi-turn reliability. At the same time, the stability observed over small-tomoderate range of indicates that correction does not need to be inserted extremely often to remain effective, highlighting practical trade-off between correction timeliness and scheduling cost. 4.6 Case Study: Error Recovery Behaviors To qualitatively illustrate the robustness improvements, we compare three Qwen3-8B variants (Base, GRPO, FISSION-GRPO) on representative multiturn file manipulation task (from BFCL V4 MultiTurn Base) requiring state tracking across directory changes and file moves (full logs in Appendix C). We observe three distinct error-recovery patterns. The Base model exhibits collapse: it fails to update internal state after partial command success, entering repetitive invalid retries until conversation breakdown. GRPO shows hallucination: it recognizes errors but lacks groundingwhen file path becomes invalid, it invents non-existent parameters (e.g., path argument for ls) rather than verifying the actual state. In contrast, FISSIONGRPO demonstrates active diagnosis: it employs diagnose-then-correct strategy, deploying verification tools (e.g., find) to resolve state uncertainty before reattempting the task. This comparison shows that FISSION-GRPO transforms error signals into active diagnostic capabilities rather than brittle heuristics."
        },
        {
            "title": "5 Conclusion",
            "content": "We presented FISSION-GRPO, framework that transforms execution errors into on-policy corrective supervision for multi-turn tool use. By intercepting errors, augmenting them with simulated feedback, and resampling recovery attempts, our approach enables smaller models to learn robust self-correction rather than collapsing into repetitive loops. On BFCL v4 Multi-Turn, Qwen3-8B achieves 5.7% gain in error recovery and 4% overall accuracy improvement, outperforming specialized tool agents. The fission paradigm may generalize to other iterative refinement domains such as code debugging and mathematical reasoning. Figure 4: Multi-turn performance across different correction trigger intervals (N ) on BFCL v4 Multi-Turn. of the fission mechanism is inherently valuable. Second, Fission-Dynamic yields significant marginal gains. The gap between Static and Dynamic (e.g., +3.62% on Qwen3-4B) underscores the necessity of precise supervision. Generic signals fail to guide the model through complex errors, whereas simulated feedback effectively directs the gradient towards correcting specific semantic errors, particularly in the Miss Param and Long Context subsets."
        },
        {
            "title": "Frequency",
            "content": "To investigate how frequently correction updates should be triggered in training, and to balance timely suppression of recurring error patterns with scheduling overhead and potential training instability, we conduct sensitivity study on the minimum trigger interval, denoted by (in global steps). Here controls how often correction can be inserted by limiting correction updates to occur no more than once every global steps. We fix the total training budget to 234 global steps and vary only , enforcing the constraint that at most one correction update occurs every global steps. We further adopt LIFO sampling strategy to prioritize the most recent correction samples, which keeps updates better aligned with the current policy distribution and therefore closer to the on-policy assumption. Results. Figure 4 shows Multi-turn Overall Accuracy and category-wise metrics as function of . Performance remains relatively stable when correction is triggered frequently, corresponding to small , but declines noticeably as updates become sparse, corresponding to large , with consistent degradation across subcategories. The drop is most pronounced on Miss Param and is also evident on Long Context, indicating that parameter-"
        },
        {
            "title": "Limitations",
            "content": "Our work has several limitations that suggest directions for future research. Evaluation Scope. We evaluate FISSION-GRPO exclusively on the BFCL v4 Multi-Turn benchmark. We chose this benchmark because it is one of the few that features an interactive error feedback mechanism permitting retry attempts, which directly aligns with our focus on error recovery. Most existing tool-use benchmarks emphasize singleturn correctness without explicit retry mechanisms. Extending evaluation to other domains with errorretry dynamics (e.g., interactive code debugging or web navigation with fallback) is promising direction for future work. Computational Overhead. The fission mechanism introduces additional computational cost by resampling rollouts for each intercepted error. We partially mitigate this through configurable trigger interval (Section 4.5), which allows trading off correction frequency against training efficiency. Our analysis shows that moderate intervals maintain effectiveness while reducing overhead, though further optimization of this trade-off remains an open direction."
        },
        {
            "title": "References",
            "content": "Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, and Pavlo Molchanov. 2025. Small language models are the future of agentic ai. arXiv preprint arXiv:2506.02153. Wenlong Deng, Yi Ren, Muchen Li, Danica Sutherland, Xiaoxiao Li, and Christos Thrampoulidis. 2025. On the effect of negative gradient in group relative deep reinforcement optimization. arXiv preprint arXiv:2505.18830. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. 2024. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. In ACL (Findings). Shiting Huang, Zhen Fang, Zehui Chen, Siyu Yuan, Junjie Ye, Yu Zeng, Lin Chen, Qi Mao, and Feng Zhao. 2025. Critictool: Evaluating self-critique capabilities of large language models in tool-calling error scenarios. arXiv preprint arXiv:2506.13977. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, and 1 others. 2024. Training language models to selfcorrect via reinforcement learning. arXiv preprint arXiv:2409.12917. Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, and 1 others. 2024. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783. Gongrui Nan, Siye Chen, Jing Huang, Mengyu Lu, Dexun Wang, Chunmei Xie, Weiqi Xiong, Xianzhou Zeng, Qixuan Zhou, Yadong Li, and 1 others. 2025. Ngrpo: Negative-enhanced group relative policy optimization. arXiv preprint arXiv:2509.18851. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Zhuoshi Pan, Yu Li, Honglin Lin, Qizhi Pei, Zinan Tang, Wei Wu, Chenlin Ming, Vicky Zhao, Conghui He, and Lijun Wu. 2025. Lemma: Learning from errors for mathematical advancement in llms. arXiv preprint arXiv:2503.17439. Shishir Patil, Huanzhi Mao, Fanjia Yan, Charlie Cheng-Jie Ji, Vishnu Suresh, Ion Stoica, and Joseph Gonzalez. 2025. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. 2024. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 37:126544126565. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, and 1 others. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551. 9 John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Raghav Sharma and Manan Mehta. 2025. Small language models for agentic systems: survey of architectures, capabilities, and deployment trade offs. arXiv preprint arXiv:2510.03847. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652. Junhao Su, Yuanliang Wan, Junwei Yang, Hengyu Shi, Tianyang Han, Junfeng Luo, and Yurui Qiu. 2025. Failure makes the agent stronger: Enhancing accuracy through structured reflection for reliable tool interactions. arXiv preprint arXiv:2509.18847. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, and 1 others. 2025. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534. Qwen Team. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Sri Vatsa Vuddanti, Aarav Shah, Satwik Kumar Chittiprolu, Tony Song, Sunishchal Dev, Kevin Zhu, and Maheep Chaudhary. 2025. Paladin: Self-correcting language model agents to cure tool-failure cases. arXiv preprint arXiv:2509.25238. Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Chak Tou Leong, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, and Wenjie Li. 2025. Subtle errors in reasoning: Preference learning via error-injected self-editing. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3118431203. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, and 1 others. 2025. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Quoc Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, and 1 others. 2025a. xlam: family of large action models In Proceedings of to empower ai agent systems. the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1158311597. Kangning Zhang, Wenxiang Jiao, Kounianhua Du, Yuan Lu, Weiwen Liu, Weinan Zhang, and Yong Yu. 2025b. Looptool: Closing the data-training loop for robust llm tool calls. arXiv preprint arXiv:2511.09148."
        },
        {
            "title": "Simulator",
            "content": "To improve reproducibility, we provide the prompting template used to query the error simulator Sϕ. We use two-message chat format: system prompt that specifies the simulator role and output constraints, followed by user prompt that injects the original context, ground-truth tool calls, and the models failed attempt."
        },
        {
            "title": "B Training Algorithm Details",
            "content": "Algorithm 1 outlines the detailed execution flow of the FISSION-GRPO framework. The process alternates between standard exploration (to maintain general capability and mine errors) and fissionbased updates (to learn specific recovery strategies)."
        },
        {
            "title": "C Extended Case Study Analysis",
            "content": "In this section, we provide detailed breakdown of the case study referenced in Section 4.6. Figure 6 visualizes the trajectories of Qwen3-8B under three training conditions on multi-turn file manipulation task (Sample ID: multi_turn_base_1). Scenario Overview. The user requests to verify the current directory, move log.txt file into new archive folder, and then search for keyword within that file. The key challenge arises in Turn 2: mkdir archive fails (directory already exists), but cd workspace and mv log.txt succeed. This partial-success state requires careful trackingin Turn 3, since the file was moved to archive, direct grep will fail, requiring the agent to locate the file first. 10 Detailed Behavioral Comparison. 1. Qwen3-8B (Base): State Awareness Collapse. The Base model correctly issues the initial batch command [cd, mkdir, mv]. However, it fails to update its internal state to reflect that it is already inside workspace after the successful cd. When attempting to handle the mkdir error, it redundantly retries cd workspace, which fails (No such directory within the current directory). Confused by this feedback, it spirals into loop of invalid operations, ultimately failing to realize the file was already moved. 2. Qwen3-8B + GRPO: Latent State Mismatch & Hallucination. The GRPO model succeeds in Turn 2 (the file is moved), but fails to track the consequencespecifically, that log.txt is no longer in the current directory but in the archive subdirectory. This latent it first tries state mismatch surfaces in Turn 3: grep(\"log.txt\") (fails), then attempts heuristic guess grep(\"archive/log.txt\") (also fails). Lacking grounded fallback strategy, it resorts to hallucination, inventing non-existent path parameter for ls. 3. Qwen3-8B + FISSION-GRPO: Active Diagnosis. Our model handles the Turn 2 state transition correctly. More importantly, in Turn 3, when faced with the same No such file error, it demonstrates superior recovery mechanism: instead of guessing, it deploys find(name=\"log.txt\", path=\"workspace\") to empirically verify the files location. Using the confirmed path, it performs precise state update via cd(folder=\"archive\"), then executes grep successfully. This confirms that FISSION-GRPO learns to bridge state gaps through active diagnosis rather than relying on fragile internal memory or hallucinated corrections. 11 Algorithm 1 Detailed Training Procedure of FISSION-GRPO Require: Policy πθ, Reference Policy πref, Error Simulator Sϕ Require: Training dataset Require: Hyperparameters: Learning rate η, KL coefficient β, Clip ratio ϵ Require: Group sizes: (Exploration), (Fission/Correction) Require: Thresholds: Buffer trigger Btrig, Success score Rthresh = 1.0 1: Initialize Corrective Sample Pool Implemented as LIFO Stack 2: for iteration = 1, . . . , do 3: 4: 5: 6: // STAGE 1: STANDARD EXPLORATION & MINING Sample batch of user queries Generate exploration group {τi}G Compute rewards for each trajectory: i=1 πθ(x) ri Rcorr(τi) + Rfmt(τi) 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: Compute GRPO Advantages: (cid:80) ri, σR Std(ri) µR 1 ˆAi riµR σR+ϵ Update Policy (Standard): LGRPO (cid:104) min(ρi ˆAi, clip(ρi, 1 ϵ) ˆAi) βDKL(πθπref) (cid:105) (cid:80)G 1 i=1 θ θ + ηθLGRPO // STAGE 2: SYNTHESIS & ACCUMULATION Identify error set = {τi Rcorr(τi) < Rthresh Rfmt(τi) = 0} for each error trajectory τerr do if Rfmt(τerr) == 0 then GetFormatError(τerr) else Sϕ(x, τerr) Generate diagnostic feedback end if Construct corrective context xcorr [x; τerr; ] Compute Deduplication Key Hash(x, τerr, ) if / Keys(B) then Push(xcorr) end if LIFO Push end for // STAGE 3: FISSION-BASED REMEDIAL UPDATE if Btrig then Xbatch Pop(Btrig) items from top of LIFO: Fetch freshest errors Initialize batch loss Ltotal 0 for each corrective context xcorr Xbatch do πθ(xcorr) tempts Fission Fission Resampling: Generate recovery group {τ j}G j=1 Compute rewards {r j} for recovery atCompute Corrective Advantages: (cid:80) j, σ µ 1 µ ˆA σ R+ϵ Std(r j) Variance restored via Accumulate Gradients: Lcorr min(ρ ˆA j, . . . ) βDKL Ltotal Ltotal + Lcorr (cid:105) end for θ θ + ηθLtotal Apply corrective update 1 (cid:80)G j=1 (cid:104) 40: 41: 42: 43: 44: end for end if"
        },
        {
            "title": "Prompt Templates",
            "content": "Prompt 1: System prompt for querying the error simulator. You are Runtime Environment Simulator for an AI Agent. Your role is to act as the API Server or Operating System that executes tool calls. IMPORTANT CONTEXT: You are receiving tool call from an Agent that has ALREADY FAILED validation or logic checks against the Ground Truth. Your task is NOT to judge correctness. Your task is to generate the specific ERROR MESSAGE that the system would return to the Agent. GOAL: Generate short, realistic, and actionable error message (starting with \"ERROR: \") that will help the Agent understand why its call failed compared to the expected Ground Truth. PRIORITY ERROR CATEGORIES: - Dependency & Sequence Violations - Parameter Hallucination - Schema & Parameter Errors - Business Logic Errors EVALUATION LOGIC: - Reference the Ground Truth: ground_truth is usually correct and serves as the primary standard. - Verify Context: cross-check the Agent output against the User Request in the Original Context. - Ambiguity Rule: if the Agent output differs from ground_truth but is still plausible, note missing validation. OUTPUT RULES: - Start with \"ERROR: \" (case-sensitive) - Be specific: mention actual parameter names/values from the failed attempt - Sound like system/API response - Keep it concise (1--2 sentences) - Return ONLY the error string (no JSON, no markdown, no extra explanation) ERROR MESSAGE EXAMPLES (Real error style): <<ERROR_EXAMPLES_SNIPPET>> CRITICAL REMINDERS: - Do NOT output JSON like {\"error\": \"...\"}; output plain text only - Do NOT add any preamble; only the \"ERROR: ...\" line - Do NOT fabricate placeholders unless they appear in the failed attempt - The error should be what the runtime system returns, not an analysis Prompt 2: User prompt template (simulation input). ## Simulation Task The Agent attempted to execute tool call, but it was INCORRECT compared to the Ground Truth. Generate the system error message triggered by the Agent's specific mistake. 1) Original Context (what the Agent saw) [System instructions & tools] <<SYSTEM_AND_TOOLS>> [Dialogue history before this attempt (non-system turns)] <<DIALOGUE_HISTORY>> 2) Execution Comparison [Ground-truth tool call(s)] <<GROUND_TRUTH_TOOL_CALLS>> [Failed tool call(s) extracted from the model output] <<FAILED_TOOL_CALLS>> 3) Instruction Compare the failed attempt against the ground truth under the given context. Identify the first critical failure and generate the runtime error. Output: Return ONLY one error string starting with \"ERROR:\". Figure 5: Two-message prompting format used to query the error simulator Sϕ. 12 Figure 6: Detailed visualization of Multi-turn Error Recovery. Comparisons of trajectories generated by Qwen3-8B under different training regimes. The Base model collapses due to immediate state loss; the GRPO model suffers from latent state mismatch leading to hallucination in later turns; FISSION-GRPO overcomes this by employing diagnostic tools (find) to actively resolve state uncertainties."
        }
    ],
    "affiliations": [
        "The Chinese University of Hong Kong",
        "Xiaohongshu Inc."
    ]
}