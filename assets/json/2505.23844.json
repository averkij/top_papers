{
    "paper_title": "Enabling Flexible Multi-LLM Integration for Scalable Knowledge Aggregation",
    "authors": [
        "Zhenglun Kong",
        "Zheng Zhan",
        "Shiyue Hou",
        "Yifan Gong",
        "Xin Meng",
        "Pengwei Sui",
        "Peiyan Dong",
        "Xuan Shen",
        "Zifeng Wang",
        "Pu Zhao",
        "Hao Tang",
        "Stratis Ioannidis",
        "Yanzhi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown remarkable promise but remain challenging to continually improve through traditional finetuning, particularly when integrating capabilities from other specialized LLMs. Popular methods like ensemble and weight merging require substantial memory and struggle to adapt to changing data environments. Recent efforts have transferred knowledge from multiple LLMs into a single target model; however, they suffer from interference and degraded performance among tasks, largely due to limited flexibility in candidate selection and training pipelines. To address these issues, we propose a framework that adaptively selects and aggregates knowledge from diverse LLMs to build a single, stronger model, avoiding the high memory overhead of ensemble and inflexible weight merging. Specifically, we design an adaptive selection network that identifies the most relevant source LLMs based on their scores, thereby reducing knowledge interference. We further propose a dynamic weighted fusion strategy that accounts for the inherent strengths of candidate LLMs, along with a feedback-driven loss function that prevents the selector from converging on a single subset of sources. Experimental results demonstrate that our method can enable a more stable and scalable knowledge aggregation process while reducing knowledge interference by up to 50% compared to existing approaches. Code is avaliable at https://github.com/ZLKong/LLM_Integration"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 4 4 8 3 2 . 5 0 5 2 : r Enabling Flexible Multi-LLM Integration for Scalable Knowledge Aggregation Zhenglun Kong1,3, Zheng Zhan1, Shiyue Hou1, Yifan Gong1, Xin Meng2, Pengwei Sui3, Peiyan Dong1, Xuan Shen1, Zifeng Wang4, Pu Zhao1, Hao Tang2, Stratis Ioannidis1, Yanzhi Wang1 1Northeastern University, 2Peking University, 3Harvard University, 4Google"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown remarkable promise but remain challenging to continually improve through traditional finetuning, particularly when integrating capabilities from other specialized LLMs. Popular methods like ensemble and weight merging require substantial memory and struggle to adapt to changing data environments. Recent efforts have transferred knowledge from multiple LLMs into single target model; however, they suffer from interference and degraded performance among tasks, largely due to limited flexibility in candidate selection and training pipelines. To address these issues, we propose framework that adaptively selects and aggregates knowledge from diverse LLMs to build single, stronger model, avoiding the high memory overhead of ensemble and inflexible weight merging. Specifically, we design an adaptive selection network that identifies the most relevant source LLMs based on their scores, thereby reducing knowledge interference. We further propose dynamic weighted fusion strategy that accounts for the inherent strengths of candidate LLMs, along with feedback-driven loss function that prevents the selector from converging on single subset of sources. Experimental results demonstrate that our method can enable more stable and scalable knowledge aggregation process while reducing knowledge interference by up to 50% compared to existing approaches."
        },
        {
            "title": "Introduction",
            "content": "The emergence of large language models (LLMs) has revolutionized various domains, catalyzing the development of many specialized models [63, 9, 14, 8, 30, 31, 54, 27]. Training new LLM from scratch is prohibitively expensive in terms of computational resources and data requirements. As result, the prevalent strategy is to finetune existing models using localized or task-specific data. While traditional finetuning can incrementally enhance models performance within specific domain, it often fails to leverage the rich, domain-specific expertise embedded in other LLMs, especially when relevant datasets are inaccessible or require extensive pre-processing [28]. Furthermore, for organizations that have invested heavily in developing and tailoring their own LLMs, replacing the current model with new, pre-trained one introduces challenges such as re-adaptation, retraining, and potentially significant costs to maintain alignment with their unique requirements. Therefore, we focus on building stronger LLM by incorporating knowledge from multiple specialized models. Instead of finetuning single model in isolation, we aggregate knowledge from various LLMs to enhance performance in stable, efficient, and scalable way. This approach preserves the strengths of the existing model while infusing it with complementary knowledge, ensuring that the final model is both highly capable and aligned with specific needs. Equal contribution. 2Code is avaliable at Github. Preprint. Existing solutions, such as ensemble methods [22, 35, 67], enhance prediction performance by aggregating outputs from multiple models but require additional memory and increased inference time due to operating several models simultaneously. Another method involves merging several neural networks into single network within the parameter space [23]. This generally requires ensuring uniform architecture and depends on manually configured weight merging or adding additional layers. Additionally, Mixture of Expert (MoE) structures such as Mistral-7bx8 [21], address some inference and weight-sharing issues, but still face long inference times, homogeneous architectures, and larger FuseLLM [49] and model sizes. FuseChat [50] attempted to integrate the knowledge of multiple source LLMs using generated probability distribution matrices. However, these approaches suffer from interference and performance degradation in various tasks compared to the original target model due to suboptimal model selection and uncontrolled fusion processes. Figure 1: Scaling number of fusion candidates. We show accuracy (histogram, higher the better) and the percentage of tasks degrading the baseline (Line chart, lower the better) when integrating three, four, and five LLMs on the BBH and MMLU benchmarks. Dotted lines represent the baseline. To overcome the limitations of existing LLM integration approaches, we propose dynamic framework that adaptively selects LLMs for integration. Given diverse set of source LLMs with heterogeneous structures, we introduce an adaptive selection network, learnable mechanism that explicitly evaluates and selects the best-performing source LLMs based on their important scores, thereby alleviating interference issues typically associated with model fusion. The scores are computed based on the performance of each model across predefined set of tasks. Our framework provides flexibility in the number of LLMs selected during this process. To improve the knowledge aggregation process, we propose dynamic weighted fusion strategy that considers the intrinsic characteristics of candidate LLMs during fusion. The assigned weights are derived from the score evaluations, allowing the fusion process to prioritize models that are more likely to enhance the overall performance of the composite LLM. The selector often converges to state in which it consistently assigns large weights to small subset of candidates. To mitigate this, we introduce feedback-driven loss function that optimizes the training of our adaptive selection network and guides the selection of candidates. Our method ensures stable and scalable integration of LLMs while maintaining both efficiency and effectiveness despite model diversity, as shown in Fig. 1 (Detailed analysis in Sec. 6: Model Scaling Results). It achieves this without increasing the parameter size or computation of the target model, making it more efficient compared to traditional methods. Our contributions are as follows: We find that merely increasing the number of fusion candidates and expanding the source model pool does not necessarily enhance the fusion process, selective strategy is more effective in minimizing knowledge interference. We propose novel dynamic integration framework that adaptively selects LLMs for integration, leveraging an adaptive selection network, dynamic weighted fusion strategy, and feedback-driven loss function to alleviate interference issues and enhance performance. Our model shows improvement in accuracy across multiple benchmarks as more models are integrated, while reducing knowledge interference by up to 50% compared to previous methods."
        },
        {
            "title": "2 Related Work",
            "content": "Model Integration. Research on model integration has evolved into distinct categories, each addressing different aspects of combining models [52, 53]: 1) Ensemble: LLM-Blender [22] uses ensemble techniques to enhance performance by combining outputs from multiple models. This 2 Figure 2: Overall framework: Multiple LLMs are evaluated and selected based on performance by an adaptive selection network. Top candidates then proceed through dynamic weighted fusion process guided by feedback loss to enhance the ability of the target LLM. The lower right shows results on CommonSense, MMLU, and Big-Bench Hard benchmark. process includes inferring all candidate models and then ranking them, which can be resourceintensive and slow. 2) Weight Merging: Zipit [44] merges partial layers of two models without additional training, creating multi-head model for various tasks. [39, 2, 55] employ weighted averaging methods. TIES-Merging [56] eliminates parameter interference among multiple models by removing delta parameters with low magnitudes and merging parameters with consistent signs. [62] compose models through linear arithmetic operations in the weight space. These techniques are typically limited to models with identical architectures. 3) Knowledge Fusion: FuseLLM [49] and FuseChat [50] focuses on fusing the probability distributions from various LLM candidates, integrating them into single base LLM, blending knowledge across models. Knowledge distillation [19] is also used to integrate information into model. However, student models are typically smaller and have lower performance than their teacher models. In our scenario, there is no limitation on the size or performance of the source models."
        },
        {
            "title": "3 Preliminaries and Motivation",
            "content": "t }M Preliminaries. As general integration approach parallel to ensembling and weight merging, knowledge fusion [49] combines the probabilistic distribution matrices from set of LLMs, denoted as {P θi i=1, where θi represents the parameters of the i-th LLM. These reflect each models inherent knowledge for text understanding. Let be text sequence of length , and t<n denote the sequence preceding the n-th token. The probabilistic distribution matrix Pθi (tnt<n)(cid:3) , for the i-th LLM is: (t2t<2), . . . , pθi = (cid:2)pθi Pθi (t1t<1), pθi (1) where pθi tokens t<n, according to the i-th LLM parameterized by θi. Each element pθi probabilities corresponding to each token in the vocabulary, summing to 1. (tnt<n) is the predicted probability distribution for the n-th token given the preceding (tkt<k) is vector of Their fusion process is achieved by minimizing the divergence between the probabilistic distributions of target LLM (pre-defined among the source LLMs) and source LLMs: Pf = F(Pθ1 , Pθ , . . . , PθM ), (2) where is the function that combines multiple matrices. The overall objective for continual training consists of weighted combination of the causal language modeling and the fusion objective: = λLlm + (1 λ)Lfuse, (3) 3 where Llm is the causal language modeling objective, and Lfuse is the cross-entropy loss between the target LLMs predictions (output) and the fused representation matrix Pf . Motivation. We conduct an evaluation of FuseLLM [49] on 27 tasks of the Big-Bench Hard [45] benchmark and 57 tasks from the Multi-task Language Understanding (MMLU), as shown in Fig. 1. There are two key observations: high percentage of tasks exhibit performance degradation (red trend lines) compared to the original unmerged model (detailed numbers are shown in Tab. 3). Integrating more models leads to progressively greater performance degradation (red bars), emphasizing the impact of knowledge interference. This phenomenon can arise due to: 1) Dilution of Valuable Knowledge: Introducing less relevant or lower-quality information can dilute the original models knowledge [43]. 2) Overfitting to Irrelevant Patterns: The fused model may overfit to noise or less useful patterns from new models, reducing its ability to perform the tasks it was originally trained for."
        },
        {
            "title": "4 Methodology",
            "content": "Motivated by the above observations, we propose fusion framework to advance existing knowledge fusion methods by introducing dynamic framework that consists of an Adaptive Selection Network and Dynamic Weighted Fusion mechanism, as illustrated in Fig. 2. Specifically, at each training step, the Adaptive Selector evaluates performance metrics to dynamically select subset of candidate models based on their probabilistic distribution matrices rather than all candidates. More importantly, both the selection of candidates and the number of candidates selected are adaptive, preventing knowledge interference and enhancing overall model performance. The selected candidates are then fused using weighted sum based on normalized selection probabilities. This process is guided by specially designed loss function that refines model selection through feedback. Our framework provides flexibility for future scalability and allows the integration process to accommodate varying computational constraints and application needs. 4.1 Adaptive Selection Network We propose an Adaptive Selection Network (ASN) to evaluate the source models based on continuous learning process. It integrates feedback from ongoing interaction, which will be introduced in Sec. 4.3. The network takes the normalized matrices {Pθi i=1 (simplified as Pi in later equations) as input. These matrices are flattened and normalized using layer normalization to stabilize training. The network then computes the logits for each candidate model. It consists of three linear layers with specified dimensions and uses the GELU activation function to introduce non-linearity, thereby enhancing its ability to capture complex patterns in the input data. We concatenate all Pi as Pcat and send into the module to obtain logits: }M zϕ = (f 3 GELU 2 GELU 1)Pcat, (4) where 1, 2, and 3 represent linear layers. The adaptive selection mechanism utilizes the scores from the network, converting these into probability via the softmax function pi = ezϕ/(cid:80)N i=1 ezϕ, where pi is the softmax probability associated with the i-th candidate. We also compared the effects of adding Gumbel softmax [20] or noise [40] before the softmax (see Tab. 1 Selection metric). The better performance of softmax shows that the selection process benefits more from the smooth and differentiable mapping of logits, as well as improved convergence, rather than from adding randomness and increasing variance. Dynamic Candidate Selection. To determine which candidate models to select for fusion, we apply dynamic thresholding mechanism. Candidates with selection probabilities exceeding the threshold τ are selected: Xselected = pj > τ, = 1, . . . , , (5) (cid:110) θj (cid:111) where the output set Xselected = {Pθj simplify the notation Pθj }K as Pj in the later equations. j=1 represents subset of the original set of LLMs. We 4 Table 1: Different Design Choices of our framework under Fusion-X -T scale with four source models. We show ablation results on Commonsense (CS) and BBH, along with perplexity (PPL). Category Setting PPL CS BBH Category Setting PPL CS BBH Selection count Selection metric Layer choice Top-2 Adaptive All Softmax Gumbel Noise Conv. 1Linear 3Linear 11.67 11.04 11.91 11.04 13.41 13. 12.21 11.42 11.04 40.55 41.32 40.52 41.32 39.15 38.97 39.73 40.78 41.32 6.75 7.31 6.64 7.31 5.00 4. 6.11 6.86 7.31 Fusion method Avg Max w/o Weight. Weight. Threshold setting 0.2 0.15 0.12 Adding loss w/o Loss Feed. loss 11.32 11.77 11.96 11.04 13.78 11.04 11.67 11.48 11.04 40.85 40.11 39.82 41.32 39.24 41.32 40. 40.91 41.32 6.80 6.68 6.38 7.31 5.07 7.31 6.75 6.92 7.31 To ensure that at least one candidate is selected per sample 1 , we check if no candidates meet the threshold, then select the candidate with the highest probability: Xselected = {arg max pj}, if Xselected = 0, (6) We set τ = 0.15 in our implementation. This approach allows the model to adaptively choose the most relevant candidates based on input data and current learning context. 4.2 Dynamic Weighted Fusion We proceed with the fusion process after selecting the candidate models. First, we normalize the weights of the selected probabilities pi: (cid:32) ˆp = π i=1 pimi + ϵ (cid:80)M (cid:33) , (7) where mi is binary mask indicating the selected candidates (mi = 1 if pi Xselected, else mi = 0), and ϵ is small constant to prevent division by zero. π() is function to resize the vector by removing 0-valued elements, so that ˆp has elements corresponding to selected LLMs despite of elements in p. To perform the weighted sum, we reshape the normalized probabilities and masks to match the dimensions of the candidate outputs, enabling element-wise multiplication. The outputs of the selected candidates pj are accumulated based on their respective weights to produce unified model output Pf . This is calculated as follows: Pf = sum (cid:16) concat (cid:16)(cid:8)Pj ˆpj (cid:9)K j=1 (cid:17) , dim=-1 (cid:17) , dim=-1 , (8) We assign the proportion of the candidates probabilistic distributions based on the weights in Eq. (7). concat denotes the concatenation of all K-selected candidates. sum function is for weighted sum of these aligned metrics Pf . This dynamic fusion process can constantly let the more influential candidates have greater effect on the final model. Next, the fused representation Pf goes through the cross-entropy loss Lfuse. We highlight that we explore various configurations for the fusion method, threshold, and so on, as shown in Tab. 1, and choose the configuration with the best performance. Our method fundamentally transforms the approach to integration by utilizing data-driven, adaptive mechanism to dynamically evaluate contributions of candidate LLMs and select accordingly. 4.3 Loss and Training Pipeline In practice, we find that the selection network often converges to state where it consistently assigns large weights to the same few candidates. To mitigate this issue, we implement feedback approach to guide the selection of candidates. Consequently, we adopt soft constraint approach. The importance of model relative to batch of training examples is defined as the batch-wise sum of the values ˆpj for each LLM. We define feedback loss Lfeed, which is added to the existing loss function for the model as described in Sec. 3. This loss is calculated as the square of the coefficient of variation CV 5 of the importance values. The importance values are derived from the weights of different candidates in the model, summed over the index set K. This formulation is given by: Lfeed = CV 2 (cid:16) { ˆpj}K j=1 (cid:17) = σ2 (cid:0){ ˆpj}K j=1 µ2 (cid:0){ ˆpj}K j=1 (cid:1) (cid:1) + ϵ , (9) Here, σ2 is the variance, µ is the mean, and ϵ is small constant added to ensure numerical stability. This refined definition emphasizes the goal of making the distribution of source LLMs importance more uniform across the model. Minimizing the variance of the importance values ˆpj reduces the spread or difference between these values, making the distribution of importance more uniform. Simultaneously maximizing the mean ensures that the feedback loss does not become excessively sensitive to small variances. Squaring the mean in the denominator helps to normalize the loss and maintain consistent scale, emphasizing relative changes in the variance. The full objective is: L(θT , ϕASN ) = EtC [D(Tt, Ot)] (cid:125) (cid:124) (cid:123)(cid:122) Llm + λfuse (EtC [D(Tt, Pf )]) (cid:123)(cid:122) (cid:125) Lfuse (cid:124) + λfeedCV 2 (cid:16)(cid:88) (cid:123)(cid:122) Lfeed (cid:124) jK (cid:17) , ˆpj (cid:125) (10) where θT , ϕASN are parameters of the target LLM and the selection network. Llm reduces the discrepancy between target model output Tt and the one-hot label matrix Ot {0, 1}N , where is the vocabulary size. Lfuse enforces assignment between the output of the target LLM Tt and the fused representation matrix Pf . We set λfuse = 0.1 and λfeed = 0.5 in our experiments. Grid search results are shown in Appx. B. Our training algorithm is described in the Appx. A."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Implementation Details Models and Datasets. Following existing methods [22, 49, 16, 51], we use llama-2-7B as the target model and evaluate on various benchmarks for fair comparison. To demonstrate scaling performance of both parameter size and number of models, we evaluate on mutiple scales, including Llama-160M [37], GPT-Neo-125M [5], Pythia-160M [4], Tiny-starcoder [30], LiteLlama-460M-1T, OpenLLaMAV2-3B [15], MiniMA-3B [61], Amber[34], Starcoder2-3B [30], Llama-2-7B [48], OpenLLaMA-7B [15], MPT-7B [47], Pythia-6.9B [4], Starcoder2-7B [30], Llama 3-8B [17], Yi-6B [58]. These models have different parameter sizes, architectures, tokenizers, and vocabularies. We follow [49] to use MiniPile [24] for continual training. Training details. Our model is optimized using the AdamW optimizer with beta1 = 0.9 and beta2 = 0.95, with gradient clipping set to 1.0 and weight decay to 0.1. cosine learning rate schedule is employed, with maximum learning rate of 3e-5 for models under 1B and 1e-5 for models larger than 1B and warmup ratio of 0.008. We train with 8 A100 GPUs, each with 80GB of memory. Evaluation benchmarks. We evaluate Fusion-X on three benchmarks that represent different core capabilities of LLMs: Common Sense (CS) [46], Big-Bench Hard (BBH) [45], Multi-task Language Understanding (MMLU) [18], and MultiPL-E (ME) [6], representing the ability of commonsense, reasoning, and code generation. 5.2 Main Results Common Sense (CS) Evaluation. Tab. 2 shows the zero-shot performance of Fusion-X and the baseline methods when fusing 4 LLMs. We present our model in three scales: 1) Fusion-X -T: Integrated with Llama-160M, GPT-Neo-125M, Pythia-160M, Tiny-starcoder. 2) Fusion-X -S: Integrated with OpenLLaMA-V2-3B, MiniMA-3B, Amber, Starcoder2-3B. 3) Fusion-X -B: Integrated with Llama-2-7B, OpenLLaMA-7B, MPT-7B, Starcoder2-7B. The rows with -CT stand for continue training the target LLM with only extra training steps (e.g., Llama-2-7B-CT). The results demonstrate that our model consistently surpasses the target models across all six tasks, with standard deviation of 0.02 +0.02. We compare our model with the continued training of the target model using the causal language modeling objective (denoted as \"-CT\" in the Tab. 2), as well as FuseLLM, demonstrating consistent improvement across all scales. More importantly, our approach effectively prevents model performance degradation caused by integrating models with less relevant or lower-quality information on tasks such as ARC-Challenge, HellaSwag, and 6 Table 2: Overall results of Fusion-X and baselines in commonsense evaluations on CommonSense (CS), where percentages indicate the rate of improvement/decrease compared to our target model, denoted with \"*\". \"-CT\" denotes the target model with extra continue training steps. Model / Task OpenBookQA Winogrande ARC-challenge Avg. 6 Tasks HellaSwag ARC-easy BoolQ Llama-160M* GPT-Neo-125M Pythia-160M Tiny-starcoder Llama-160M-CT FuseLLM Fusion-X -T OpenLLaMA-V2-3B* MiniMA-3B Amber Starcoder2-3B OpenLLaMA-V2-3B-CT FuseLLM Fusion-X -S Llama-2-7B* OpenLLaMA-7B MPT-7B Starcoder2-7B FuseLLM Llama-2-7B-CT FuseLLM Fusion-X -B 43.35 43.60 43.90 30.72 43.43 (+0.18%) 43.54 (+0.44%) 44.23 (+2.03%) 63.30 25.88 65.87 55.47 63.64 (+0.54%) 63.72 (+0.66%) 65.03 (+2.73%) 74.58 69.70 70.12 60.61 75.04 75.10 (+0.70%) 75.23 (+0.87%) 75.46 (+1.18%) 23.04 22.95 23.55 20.31 23.00 (-0.17%) 21.93 (-4.82%) 22.95 (-0.39%) 36.35 28.41 36.60 30.80 36.25 (-0.28%) 35.75 (-1.65%) 36.43 (+0.22%) 46.33 41.38 42.15 34.90 47.44 46.85 (+1.12%) 47.14 (+1.75%) 47.50 (+2.53%) 61.44 61.68 54.59 61.68 61.56 (+0.19%) 61.48 (+0.7%) 61.59 (+0.24%) 65.44 62.17 68.72 64.40 66.40 (+1.47%) 66.51 (+1.64%) 67.31 (+2.86%) 77.71 72.29 74.74 69.08 78.13 78.22 (+0.66%) 78.22 (+0.66%) 78.86 (+1.48%) 35.23 30.44 30.24 29.24 34.84 (-1.11%) 34.74 (-1.39%) 35.47 (+0.68%) 69.93 25.19 72.41 46.43 70.05 (+0.17%) 70.23 (+0.43%) 70.75 (+1.17%) 76.00 74.50 76.25 51.00 76.78 76.28 (+0.37%) 76.40 (+0.53%) 76.97 (+1.28%) 30.00 26.20 27.00 25.20 30.05 (+0.17%) 30.20 (+0.67%) 31.60 (+5.33%) 37.80 28.20 41.40 30.00 37.43 (-0.98%) 37.20 (-1.69%) 38.25 (+1.19%) 44.20 40.80 42.40 32.00 45.40 44.06 (-0.32%) 44.34 (+0.32%) 46.02 (+4.12%) 50.20 50.67 51.38 51.78 50.42 (+0.44%) 51.23 (+2.05%) 52.09 (+3.76%) 63.22 49.33 64.33 54.70 63.20 (-0.03%) 63.59 (+0.59%) 64.69 (+2.33%) 69.30 65.82 68.15 55.17 69.03 69.41 (+0.16%) 69.22 (-0.12%) 70.33 (+1.49%) 40.54 39.26 38.44 36.49 40.55 (+0.02%) 40.52 (-0.05%) 41.32 (+1.92%) 56.01 36.53 58.22 46.97 56.16 (+0.27%) 56.17 (+0.29%) 57.08 (+1.91%) 64.69 60.75 62.30 50.46 65.30 64.97 (+0.43%) 65.09 (+0.62%) 65.85 (+1.80%) OpenBookQA. Given that the source models have large differences in performance, our method ensures the preservation of the original models knowledge. Code Generation Evaluation. 3 shows the zero-shot performance of Llama-2-7B, FuseLLM, and our Fusion-X on the ME benchmark when integrating three (left figure) and four (right figure) models. We observe that Fusion-X consistently outperforms FuseLLM across all coding tasks. Notably, our method more effectively aggregates the coding knowledge from Starcoder2-7B (our 4th LLM), showing larger performance increase than FuseLLM. Fig. Big-Bench Hard Evaluation. The results of the Fusion-X model compared to baseline methods on the BBH benchmark few-shot CoT prompting with exact match (EM) are presented in Tab. 3. We report results across all 27 tasks and compare the performance against continued training of the target model and FuseLLM, evaluating the integration of four LLMs (Llama-2-7B, OpenLLaMA-7B, MPT7B, Starcoder2-7B). Our Fusion-X model achieves an average improvement of 5.3% across all tasks, demonstrating the effectiveness of our approach. Compared to FuseLLM, our method nearly doubles the performance gain for Llama-2 (2.7% vs. 5.3%) . Knowledge interference is observed in some tasks, potentially because certain source LLMs, apart from Llama-2, perform poorly on specific tasks, thereby negatively affecting the fusion results. Figure 3: Results on ME benchmark (3&4 LLM). Therefore, despite FuseLLM showing an average performance gain compared to Llama-2-7B, it performs worse on 10 tasks, indicating significant knowledge interference. For instance, in the Snarks task, Llama-2 achieves 50.56%, while FuseLLM scores 46.21%. In contrast, Fusion-X only has five tasks that perform lower than Llama-2-7B, showing 50% reduction in knowledge interference compared to FuseLLM. We are also able to reduce the performance drop of the tasks that are affected by knowledge interference. These results indicate that our method effectively limits knowledge interference, resulting in more consistent performance improvements. Evaluation on More Models. We ran additional experiments using Llama 3-8B as the target model, and fused it with OpenLLaMA-7B, Yi-6B, and StarCoder2-7 B. Results are shown in Tab. 4 Table 3: Detailed results of Fusion-X and baselines in reasoning evaluations on BBH, where percentages indicate the rate of improvement/decrease compared to Llama-2-7B. Llama-2-7B Llama-2-7B-CT FuseLLM Fusion-X Task Boolean Expressions Causal Judgement Date Understanding Disambiguation QA Dyck Languages Formal Fallacies Geometric Shapes Hyperbaton Logical Deduction (3 objects) Logical Deduction (5 objects) Logical Deduction (7 objects) Movie Recommendation Multistep Arithmetic Two Navigate Object Counting Penguins in Table Reasoning about Colored Objects Ruin Names Salient Translation Error Detection Snarks Sports Understanding Temporal Sequences Tracking Shuffled Object (3 objects) Tracking Shuffled Object (5 objects) Tracking Shuffled Object (7 objects) Web of Lies Word Sorting Avg. 27 Tasks 69.60 52.94 62.80 46.40 6.00 49.60 32.80 51.60 56.00 32.00 24.00 70.40 0.40 53.20 49.20 31.51 48.00 33.20 24.80 50.56 88.40 12.40 32.40 17.60 10.80 51.60 10.80 39.59 70.12 (+0.7%) 67.50 (+27.5%) 51.50 (-18.0%) 47.60 (+2.6%) 6.00 (+0.0%) 47.15 (-4.9%) 27.20 (-17.1%) 50.60 (-1.9%) 62.50 (+11.6%) 37.80 (+18.1%) 11.25 (-53.1%) 61.50 (-12.6%) 1.40 (+250%) 65.00 (+22.2%) 48.00 (-2.4%) 34.50 (+9.5%) 48.00 (+0.0%) 36.20 (+9.0%) 27.40 (+10.5%) 57.50 (+13.7%) 87.50 (-1.0%) 16.55 (+33.5%) 33.46 (+3.3%) 14.80 (-15.9%) 9.45 (-12.5%) 60.40 (+17.1%) 7.50 (-30.6%) 65.00 (-6.6%) 46.67 (-11.9%) 61.40 (-2.2%) 46.30 (-0.2%) 10.20 (+70%) 50.80 (+2.4%) 20.20 (-38.4%) 61.20 (+18.6%) 58.00 (+3.57%) 33.20 (+3.75%) 27.60 (+15.0%) 74.40 (+5.7%) 4.80 (+1100.0%) 64.00 (+20.3%) 54.40 (+10.6%) 27.27 (-13.5%) 48.20 (+0.4%) 30.40 (-8.4%) 31.00 (+25%) 46.21 (-8.6%) 88.50 (+0.1%) 15.80 (+27.4%) 33.20 (+2.5%) 15.40 (-12.5%) 14.80 (+37.0%) 61.80 (+19.8%) 6.80 (-38.9%) 72.60 (+4.3%) 51.20 (-3.3%) 57.60 (-8.3%) 50.40 (+8.6%) 7.60 (+26.7%) 50.20 (+1.2%) 22.00 (-32.9%) 58.00 (+12.4%) 56.40 (+0.7%) 32.40 (+1.3%) 24.40 (+1.7%) 72.80 (+3.4%) 3.20 (+700.0%) 63.60 (+19.5%) 54.80 (+11.4%) 31.51 (+0.0%) 52.00 (+8.3%) 34.00 (+2.4%) 30.00 (+21.0%) 54.44 (+7.7%) 90.40 (+2.3%) 18.00 (+45.2%) 33.60 (+3.7%) 14.80 (-15.9%) 22.90 (+112%) 60.00 (+16.3%) 7.10 (-34.2%) 40.31 (+1.8%) 40.64 (+2.7%) 41.70 (+5.3%) Table 4: Overall results of Fusion-X and baselines in reasoning evaluations on three benchmarks. Task Llama-3-8B Llama-3-8B-CT FuseLLM Fusion-X BBH MMLU CS Avg. 63.1 67.5 73.6 68.1 63.5 (+0.6%) 68.1 (+0.9%) 73.8 (+0.3%) 66.1 (+4.8%) 68.5 (+1.5%) 74.2 (+0.8%) 68.2 (+8.0%) 69.2 (+2.5%) 76.1 (+3.4%) 68.5 (+0.6%) 69.6 (+2.2%) 71.2 (+4.6%) Training Efficiency Evaluation. Our method also demonstrates superior training efficiency. Fig. 4 depicts the learning trend against the number of training steps. We can achieve the same perplexity with 50% training steps compared to other approaches."
        },
        {
            "title": "6 Ablation & Analysis",
            "content": "Model Scaling Results. Model scaling is critical for LLMs. In this study, we explore two scaling directions: increasing model size and expanding the number of source models. We present the results in Fig. 1. The histograms represent the average accuracy (left y-axis) of FuseLLM and our model when fusing three, four, and five LLMs. The dotted line in each subfigure represents the baseline performance of Llama-160M (100M scale) and Llama-2-7B (7B scale). In the BBH 100M scale, the performance of FuseLLM is even lower than the baseline when fusing four and five LLMs. In contrast, our model consistently increases performance when integrating more LLMs. The performance degradation in FuseLLM is due to knowledge interference, as illustrated in the line charts (right y-axis), which show the percentage of tasks that perform lower than the baseline for BBH (total 27 tasks) and MMLU (total 57 tasks). FuseLLM exhibits significantly higher performance decline ratio compared to our model, with degradation affecting up to 44% of tasks. Moreover, it shows an increasing degradation trend as more LLMs are merged (3 out of the 4 scales). In contrast, our model maintains more stable decline ratio, showing 50% less degradation than FuseLLM as the number of models and scale increase. Therefore, we believe that selective strategy for LLM integration is crucial, as simply scaling the LLM integration does not always improve performance. More importantly, well-designed selection strategy can mitigate knowledge interference and maximize overall performance. 8 Figure 4: Left: Training perplexity. During training, our method exhibits greater consistency than existing methods and requires fewer training steps to achieve comparable perplexity. Right: Scaling number of tokens. Comparison between varying scales of training data on BBH. Number of Training Tokens. Our approach achieves higher training efficiency than competing methods, as shown in Fig. 4, which shows the learning trend relative to the number of training tokens. By effectively fusing LLMs during training, our model requires fewer tokens to achieve competitive or superior performance. For example, we can match FuseLLMs performance while using almost three times less training tokens. When trained with the same number of tokens, our method achieves stable performance boost of up to 2.6%. Different Integration Methods. We compare Fusion-X with various works in Tab. 5 when integrating four LLMs on BBH and MMLU. With minimal training of the target model, our method outperforms ensemble methods that have larger parameter sizes and high inference costs, making them hard to scale up the number of LLMs due to memory overhead. For example, PackLLM uses greedy algorithm that ensembles LLMs sequentially during inference. Table 5: Comparison with different integration methods Model Approach MMLU BBH Llama-2-7B Llama-2-7B-CT LLM-Blender [22] Top1-PPL PackLLM [36] FoE [51] FuseLLM [49] Fusion-X -B SLERP [16] TIES-Merging [56] AdaMerging [57] EVO-Merge [1] Fusion-X -B* - CT 39.59 40.31 (+1.8%) 45.4 46.0 (+1.3%) Ensemble Ensemble Ensemble MoE Knowledge Ours Weight Weight Optimization Evolution Ours 37.65 (-4.9%) 39.75 (+0.4%) 41.36 (+4.5%) 41.02 (+3.6%) 40.64 (+2.7%) 41.70 (+5.3%) 40.93 (+3.3%) 41.40 (+4.6%) 41.13 (+3.9%) 41.71 (+5.4%) 42.32 (+6.9%) 45.1 (-0.2%) 45.6 (+0.4%) 47.8 (+5.3%) 47.3 (+4.2%) 46.5 (+2.4%) 48.3 (+6.4%) 47.2 (+4.0%) 48.1 (+5.9%) 47.4 (+4.4%) 48.4 (+6.6%) 49.6 (+9.3%) For weight merging methods, fundamental limitation is their requirement for identical architectures, making them not directly comparable to our model. Therefore, we merge several LLaMA-based models (Meditron-7B [7], Vicuna-7B-v1.5 [66], and OpenLLaMA-7B) for fair comparison with weight merging methods, denoted as Fusion-X -B*. Comparing with weight merging techniques, we have advantage of supporting heterogeneous models. This shows the effectiveness of our approach in creating more stable, efficient, and scalable method for enhancing the capabilities of LLMs."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we propose novel framework for integrating multiple LLMs. Our adaptive selection network selectively integrates the best-performing source LLMs, overcoming the limitations of existing methods and minimizing knowledge interference. We also introduce dynamic weighted fusion strategy and feedback-driven loss function to enhance the fusion process. Our method significantly improves adaptability and performance, offering an efficient solution for LLM integration while maintaining parameter size and computational efficiency. Limitations remain due to the additional token alignment required prior to training, and future work should explore training on diverse datasets."
        },
        {
            "title": "References",
            "content": "[1] Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. Evolutionary optimization of model merging recipes. Nature Machine Intelligence, pages 110, 2025. [2] Devansh Arpit, Huan Wang, Yingbo Zhou, and Caiming Xiong. Ensemble of averages: Improving model selection and boosting performance in domain generalization. Advances in Neural Information Processing Systems, 35:82658277, 2022. [3] BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. [4] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 23972430. PMLR, 2023. [5] Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale If you use this Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. software, please cite it using these metadata. [6] Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Feldman, et al. Multiple: scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering, 2023. [7] Zeming Chen, Alejandro Hernández-Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, and Antoine Bosselut. Meditron70b: Scaling medical pretraining for large language models, 2023. [8] Zhenyi Lu Chenghao Fan and Jie Tian. Chinese-vicuna: chinese instruction-following llama-based model. 2023. [9] Clément Christophe, Praveen Kanithi, Prateek Munjal, Tathagata Raha, Nasir Hayat, Ronnie Rajan, Ahmed Al-Mahrooqi, Avani Gupta, Muhammad Umar Salman, Gurpreet Gosal, Bhargav Kanakiya, Charles Chen, Natalia Vassilieva, Boulbaba Ben Amor, Marco AF Pimentel, and Shadab Khan. Med42 evaluating fine-tuning strategies for medical llms: Full-parameter vs. parameter-efficient approaches. 2024. [10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [11] Nan Du, Yanping Huang, Andrew Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 55475569. PMLR, 2022. [12] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. [13] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 12 2023. [14] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: dialogue model for academic research. Blog post, April 2023. [15] Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. 10 [16] Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. Arcees mergekit: toolkit for merging large language models. arXiv preprint arXiv:2403.13257, 2024. [17] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [18] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. [19] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. stat, 1050:9, 2015. [20] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. [21] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [22] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1416514178, 2023. [23] Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. Dataless knowledge fusion by merging weights of language models. In The Eleventh International Conference on Learning Representations, 2022. [24] Jean Kaddour. The minipile challenge for data-efficient language models. arXiv preprint arXiv:2304.08442, 2023. [25] Zhenglun Kong, Yize Li, Fanhu Zeng, Lei Xin, Shvat Messica, Xue Lin, Pu Zhao, Manolis Kellis, Hao Tang, and Marinka Zitnik. Token reduction should go beyond efficiency in generative models from vision, language to multimodality, 2025. [26] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. [27] Bin Li, Bin Sun, Shutao Li, Encheng Chen, Hongru Liu, Yixuan Weng, Yongping Bai, and Meiling Hu. Distinct but correct: generating diversified and entity-revised medical response. Science China Information Sciences, 67(3):132106, 2024. [28] Bin Li, Yixuan Weng, Fei Xia, and Hanjun Deng. Towards better chinese-centric neural machine translation for low-resource languages. Computer Speech & Language, 84:101566, 2024. [29] Bingbing Li, Zhenglun Kong, Tianyun Zhang, Ji Li, Zhengang Li, Hang Liu, and Caiwen Ding. Efficient transformer-based large scale language representations using hardware-friendly block structured pruning. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 31873199, Online, November 2020. Association for Computational Linguistics. [30] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. [31] Zhengyang Li, Qijin Ji, Xinghong Ling, and Quan Liu. comprehensive review of multi-agent reinforcement learning in video games. Authorea Preprints, 2025. [32] Jun Liu, Zhenglun Kong, Peiyan Dong, Xuan Shen, Pu Zhao, Hao Tang, Geng Yuan, Wei Niu, Wenbin Zhang, Xue Lin, et al. Rora: Efficient fine-tuning of llm with reliability optimization for rank adaptation. arXiv preprint arXiv:2501.04315, 2025. 11 [33] Jun Liu, Zhenglun Kong, Pu Zhao, Changdi Yang, Hao Tang, Xuan Shen, Geng Yuan, Wei Niu, Wenbin Zhang, Xue Lin, et al. Toward adaptive large language models structured pruning via hybrid-grained weight importance assessment. arXiv preprint arXiv:2403.10799, 2024. [34] Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, and Eric P. Xing. Llm360: Towards fully transparent open-source llms, 2023. [35] Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, and Jingren Zhou. Routing to the expert: Efficient reward-guided ensemble of large language models. arXiv preprint arXiv:2311.08692, 2023. [36] Costas Mavromatis, Petros Karypis, and George Karypis. Pack of llms: Model fusion at test-time via perplexity optimization. arXiv preprint arXiv:2404.11531, 2024. [37] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification, 2023. [38] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. [39] Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Gallinari, and Matthieu Cord. Diverse weight averaging for out-of-distribution generalization. Advances in Neural Information Processing Systems, 35:1082110836, 2022. [40] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [41] Xuan Shen, Peiyan Dong, Lei Lu, Zhenglun Kong, Zhengang Li, Ming Lin, Chao Wu, and Yanzhi Wang. Agile-quant: Activation-guided quantization for faster inference of llms on the edge. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1894418951, 2024. [42] Xuan Shen, Zhenglun Kong, Changdi Yang, Zhaoyang Han, Lei Lu, Peiyan Dong, Cheng Lyu, Chih-hsiang Li, Xuehang Guo, Zhihao Shu, et al. Edgeqat: Entropy and distribution guided quantization-aware training for the acceleration of lightweight llms on the edge. arXiv preprint arXiv:2402.10787, 2024. [43] Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, and Weiqiang Zhang. Knowledge unlearning for llms: Tasks, methods, and challenges. arXiv preprint arXiv:2311.15766, 2023. [44] George Stoica, Daniel Bolya, Jakob Brandt Bjorner, Pratik Ramesh, Taylor Hearn, and Judy In The Twelfth Hoffman. Zipit! merging models from different tasks without training. International Conference on Learning Representations, 2023. [45] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. [46] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. [47] MosaicML NLP Team. Introducing mpt-7b: new standard for open-source, commercially usable llms, 2023. Accessed: 2023-05-05. 12 [48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [49] Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi. Knowledge fusion of large language models. In The Twelfth International Conference on Learning Representations, 2024. [50] Fanqi Wan, Ziyi Yang, Longguang Zhong, Xiaojun Quan, Xinting Huang, and Wei Bi. Fusechat: Knowledge fusion of chat models. arXiv preprint arXiv:2402.16107, 2024. [51] Hongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu, Eric Xing, and Mikhail Yurochkin. Fusing models with complementary expertise. arXiv preprint arXiv:2310.01542, 2023. [52] Mingyang Wang, Heike Adel, Lukas Lange, Jannik Strötgen, and Hinrich Schütze. Learn it or leave it: Module composition and pruning for continual learning. In Proceedings of the 9th Workshop on Representation Learning for NLP (RepL4NLP-2024), pages 163176, 2024. [53] Mingyang Wang, Heike Adel, Lukas Lange, Jannik Strötgen, and Hinrich Schütze. Rehearsalfree modular and compositional continual learning for language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 469480, 2024. [54] Yiting Wang, Jiachen Zhong, and Rohan Kumar. systematic review of machine learning applications in infectious disease prediction, diagnosis, and outbreak forecasting. 2025. [55] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International conference on machine learning, pages 2396523998. PMLR, 2022. [56] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36, 2024. [57] Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, and Dacheng Tao. Adamerging: Adaptive model merging for multi-task learning. arXiv preprint arXiv:2310.02575, 2023. [58] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng Li, Jiangcheng Zhu, Jianqun Chen, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. [59] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [60] Zheng Zhan, Yushu Wu, Zhenglun Kong, Changdi Yang, Yifan Gong, Xuan Shen, Xue Lin, Pu Zhao, and Yanzhi Wang. Rethinking token reduction for state space models. arXiv preprint arXiv:2410.14725, 2024. [61] Chen Zhang, Dawei Song, Zheyu Ye, and Yan Gao. Towards the law of capacity gap in distilling language models. 2023. [62] Jinghan Zhang, Junteng Liu, Junxian He, et al. Composing parameter-efficient modules with arithmetic operation. Advances in Neural Information Processing Systems, 36:1258912610, 2023. [63] Xinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang Chen, Zekun Li, and Linda Ruth Petzold. Alpacare:instruction-tuned large language models for medical application, 2023. [64] Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, and Yanzhi Wang. 7b fully open source moxin-llm from pretraining to grpo-based reinforcement learning enhancement, 2025. 13 [65] Pu Zhao, Fei Sun, Xuan Shen, Pinrui Yu, Zhenglun Kong, Yanzhi Wang, and Xue Lin. Pruning foundation models for high accuracy without retraining. arXiv preprint arXiv:2410.15567, 2024. [66] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [67] Jiachen Zhong and Yiting Wang. Enhancing thyroid disease prediction using machine learning: comparative study of ensemble models and class balancing techniques. 2025. [68] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022."
        },
        {
            "title": "A Design Details",
            "content": "Adaptive Selection Networks Decision-making Process: Our Adaptive Selection Network parameterized by ϕ acts as learned function that maps the probabilistic output distribution matrix Pi RN of each source LLM for given input sequence to corresponding logit score zϕ(Pi) R. As defined in Equation 4, this mapping is realized through series of linear transformations and non-linear activations: zϕ(Pi) = (f 3 GELU 2 GELU 1)(Pi) where k() = Wk () + bk represents the k-th linear layer (with appropriate flattening/reshaping of Pi implicit in 1). The parameters ϕ = {W1, b1, W2, b2, W3, b3} are learned end-to-end by minimizing the overall objective function (Equation 10). The layers are defined as follows: Layer 1: Linear mapping from input_features to 2 input_features, followed by GELU activation. Layer 2: Linear mapping from 2 input_features back to input_features, followed by GELU activation. Layer 3: Linear mapping from input_features to (number of candidates), without activation. We initialize the weights of the linear layers using Xavier uniform initialization to facilitate better convergence during training. The learning process enables the ASN to extract relevant information from the high-dimensional input Pi. The sequence of linear and non-linear operations allows the network to capture complex patterns within each LLMs conditional probability predictions across the input sequence. These patterns may include identifying instances where specific LLM exhibits high confidence (e.g., sharp probability peak for the ground truth token) or displays distinctive distribution shape that signals unique knowledge. The resulting logit zϕ(Pi) thus becomes learned estimate of the i-th source LLMs expected utility in reducing the total loss for the given input context. By maximizing the logits (and thus the softmax probabilities pi) for source LLMs whose outputs are conducive to minimizing Llm and Lf use, the ASN implicitly learns to discern which source distributions Pi contain knowledge most relevant and beneficial for the target model in the current scenario, effectively acting as data-driven relevance predictor. Ensuring Candidate Diversity Our dynamic selection mechanism allows for varying the number of selected candidates from one up to . By adjusting the threshold τ , we can control the strictness of candidate selection, promoting diversity when beneficial or focusing on top performers when necessary. Our algorithm is shown in Alg. 1. Algorithm 1 Fusion-X for LLMs Integration Require: Source LLMs probabilistic distribution matrices {P θi C. Ensure: Fused representation matrix Pf , Target LLM 1: Initialize the adaptive selection network zϕ(Pi). 2: for each text in do }M i=1 (simplify as Pi), training corpus 3: 4: 5: 6: 7: 8: // Step1: Select fusion candidates with adaptive selection network. for each input Pi do # Tensor shape:(L, D, N) Obtain logits zϕ(Pi) using using Eq. (4). Calculate softmax probability pi. # Tensor shape:(L, D, N) end for // Step2: Fuse selected candidates using dynamic weighted fusion. Obtain Xselected using Eq. (5). # Selecting based on adaptive threshold τ Compute Pf using Eq. (8). # shape: (L,D,K) // Step3: Training schedule. Calculate feedback loss Lfeed using Eq. (9). Compute final loss using q. 10 # Combination of Llm, Lfuse, and Lfeed Update model parameters based on it. 9: 10: 11: 12: end for 13: return Trained ."
        },
        {
            "title": "B Training Details",
            "content": "Training Dataset. We use MiniPile [24] for continue training the target model. The dataset comprises approximately 1.8 billion tokens originated from 1 million documents across 22 domains. Hyperparameter Search for Loss. To determine the optimal weight for our feedback loss and fusion loss, we conducted comprehensive grid search, exploring different weight combinations. Our goal was to identify weights that would bring all loss components to similar order of magnitude, ensuring no single component dominates the overall loss function. This step is crucial to ensure that no single component dominates the overall loss function. We performed this grid search using 10% of the validation set. We show the grid search results in Fig. 5. The best combination is λfuse = 0.1, λfeed = 0.5. Figure 5: Loss grid search. Smaller and darker circle means lower perplexity. Training Procedure. During training, the model processes batches of candidate outputs and rewards. The rewards are first flattened and normalized. The Adaptive Selection Network computes selection probabilities, which are then used to dynamically select candidates based on the threshold τ . The selected probabilities are normalized, and the candidates outputs and rewards are fused using weighted sum."
        },
        {
            "title": "C Distribution of Activation Frequencies",
            "content": "Fig. 6 presents the LLM fusion candidate selection distribution during the training of Fusion-X -T, using Llama-160M, GPT-Neo-125M, Pythia-160M, and Tiny-Starcoder, respectively as source models. The left panel displays the selection trends over 120K training steps, revealing consistent patterns in the selection distribution. This indicates that our adaptive selection network can dynamically adjust LLM selection based on the ongoing learning process. The right panel illustrates the proportion of each selection throughout the training. These results indicate that our method finds LLM 4 (Tiny-starcoder) to be more valuable than the others, and LLM 3 (Pythia-160M) to be less valuable for the current integration process. Our statistical results show that we can accurately identify effective LLM candidates for the current task from the source model pool at each training step."
        },
        {
            "title": "D More Evaluation Results",
            "content": "Knowledge Interference Comparison: Tab. 6 shows the results of fusion four and five LLMs on BBH benchmark. Under the 100M scale, the performance of FuseLLM is even lower than the baseline when fusing four and five LLMs. In contrast, our model consistently increases performance when integrating more LLMs. 16 Figure 6: Candidate selection distribution. The Left shows the selection for each training step, and the right shows the proportion of each selection for the training. Table 6: More results of Fusion-X and baselines on Big-Bench Hard (BBH) benchmark. Numbers in red represent the tasks that have performance decrease compared to the target model. Task Boolean Expressions Causal Judgement Date Understanding Disambiguation QA Dyck Languages Formal Fallacies Geometric Shapes Hyperbaton Logical Deduction (3 objects) Logical Deduction (5 objects) Logical Deduction (7 objects) Movie Recommendation Multistep Arithmetic Two Navigate Object Counting Penguins in Table Reasoning about Colored Objects Ruin Names Salient Translation Error Detection Snarks Sports Understanding Temporal Sequences Tracking Shuffled Obj. (3 objects) Tracking Shuffled Obj. (5 objects) Tracking Shuffled Obj. (7 objects) Web of Lies Word Sorting Avg. 27 Tasks Target Model Integrate 5 LLMs Integrate 4 LLMs llama-160 llama-160-CT FuseLLM Fusion-X FuseLLM Fusion-X 9.60 4.81 17.20 0.00 2.40 0.00 0.00 0.00 12.00 5.60 6.40 0.00 0.00 0.00 8.40 11.64 13.20 0.00 0.00 19.66 52.40 0.00 7.60 3.20 0.40 0.00 0.00 6. 10.50 8.26 19.20 0.00 2.00 0.00 0.00 0.00 11.50 8.20 7.00 0.00 0.00 5.00 7.40 14.70 12.50 0.00 0.00 17.33 43.26 0.00 9.20 2.00 0.40 0.00 0.00 34.00 21.93 20.00 1.20 0.00 0.00 0.00 0.00 6.00 5.20 3.20 0.40 0.00 0.00 5.60 8.90 2.40 0.00 0.80 3.93 51.60 0.00 0.00 1.60 0.00 0.00 0.00 25.60 29.95 20.00 4.40 2.40 0.00 0.00 0.00 12.00 6.40 6.80 0.00 0.00 28.40 0.40 15.07 4.00 0.00 0.00 4.49 50.00 1.20 2.00 1.20 0.40 0.00 0.00 22.00 26.20 19.60 0.00 0.00 0.00 0.00 0.00 2.40 1.60 3.60 0.00 0.00 0.00 0.40 8.37 2.80 0.00 0.40 3.37 51.20 0.00 0.00 0.00 0.40 0.00 0.00 12.50 22.50 20.00 2.50 2.40 0.00 0.00 0.00 0.00 10.00 6.40 0.00 0.00 47.50 2.50 11.71 2.50 0.00 0.00 2.50 60.00 0.00 0.00 0.00 0.40 0.00 0.00 6.61 (+2.3%) 6.18 (-4.3%) 7.86 (+21.7%) 5.27 (-18.4%) 7.44 (+15.2%)"
        },
        {
            "title": "E Source Model selection",
            "content": "When performing model fusion, its crucial to understand the performance differences between source and target models. Unlike knowledge distillationwhich enhances less performant model using more advanced teacher modelour model fusion approach doesnt rely solely on the largest or most complex models. Instead, we can merge smaller models that excel in specific tasks to create more capable target model. We also do not need careful target and source LLM selection, due to our adaptive selection approach. Thereby reducing the time and cost prior training, as well as the risk of integrating models that can make the models perform worse. Our fusion selection for each scale are as follows: By not restricting ourselves to specific architectures or \"good\" candidate models, we allow the adaptive selection mechanism to determine the most effective contributions from each model. This approach minimizes the need for manual selection and demonstrates that even models with lower standalone performance (e.g., MiniMA-3B) do not negatively impact the fused models overall performance. Our rationale is that model-agnostic design enhances flexibility and broad applicability, allowing the fusion process to capitalize on the unique strengths of each model without being hindered by their individual weaknesses. Table 7: Experimental configurations: models fused in each run. Configuration Target Models Other Source Models Fuse 3 ( 100M) Fuse 4 ( 100M) Fuse 5 ( 100M) Llama-160M [37] Llama-160M [37] Llama-160M [37] GPT-Neo-125M [5], Pythia-160M [4] GPT-Neo-125M [5], Pythia-160M [4], Tiny-starcoder [30] GPT-Neo-125M [5], Pythia-160M [4], Tiny-starcoder [30], LiteLlama-460M-1T Fuse 3 ( 3B) Fuse 4 ( 3B) Fuse 3 ( 7B) Fuse 4 ( 7B) Fuse 5 ( 7B) Fuse 3 ( 8B) Fuse 4 ( 8B) OpenLLaMA-V2-3B [15] MiniMA-3B [61], Amber [34] OpenLLaMA-V2-3B [15] MiniMA-3B [61], Amber [34], Starcoder2-3B [30] Llama-2-7B [48] Llama-2-7B [48] Llama-2-7B [48] Llama-3-8B Llama-3-8B OpenLLaMA-7B [15], MPT-7B [47] OpenLLaMA-7B [15], MPT-7B [47], Starcoder2-7B [30] OpenLLaMA-7B [15], MPT-7B [47], Starcoder2-7B [30], Pythia-6.9B [4] Yi-1.5-9B, Gemma-2-7b Yi-1.5-9B, Gemma-2-7b, Qwen2.5-7b As shown in Tab. 2 For instance, in the case of Fusion-X -T, we observe that the Llama-160M model demonstrates the best performance with an average score of 40.54 across the six tasks. Consequently, Llama-160M serves as the target model for Fusion-X -T. Similarly, for Fusion-X -S, the Amber model shows superior performance with an average score of 58.22, while our target model is OpenLLaMAV2-3B. Lastly, for Fusion-X -B, the Llama-2-7B model leads with an impressive average score of 64.69."
        },
        {
            "title": "F Token Alignment",
            "content": "We follow the Token alignment process in [49] in the context of input text involves aligning two distribution matrices from two different LLMs (Large Language Models). This alignment is carried out along two dimensions: token-wise alignment relative to the text and distribution-wise alignment with respect to the vocabulary. Token-wise Alignment: For token-wise alignment, dynamic programming is used to minimize the total cost of editing one sequence of tokens to match another. The proposed MinED (Minimal Edit Distance) method in [49] aligns tokens by minimizing the edit distance between them, effectively capturing the nuances between the two LLMs vocabularies. Distribution-wise Alignment: For distribution-wise alignment, the process is between two vocabularies from different tokenizers of the two LLMs. Tokens with similar distribution values are aligned effectively. However, for distribution values involving different tokens, the EM method fails to align these due to minor differences in values. The MinED method maps based on their minimal edit distance, ensuring successful alignment of these distribution values. This systematic mapping minimizes misalignment and ensures that the integrated knowledge is coherent and meaningful rather than just introducing beneficial noise from extra training steps."
        },
        {
            "title": "G Evaluation benchmarks",
            "content": "We evaluate Fusion-X on three benchmarks that represent different core capabilities of LLMs, spanning reasoning, commonsense, science, and code generation. Common Sense (CS) [46] is benchmark to evaluate the commonsense capability of LLMs. We consider 5 standard multiple-choice tasks: ARC easy and challenge [10], BoolQ [10], HellaSwag [59], and OpenBookQA [38]. We employ lm-eval-harness [13] to conduct likelihood-based zero-shot evaluation. Specifically, we select the option with the highest likelihood given the context and report the accuracy. Big-Bench Hard (BBH) [45] is benchmark to evaluate the general reasoning ability of LLMs. It contains 23 multiple-choice tasks and 4 free-form generation tasks from the Big-Bench [3], which can be classified into four categories: algorithmic and arithmetic reasoning, natural language understanding, world knowledge, and multilingual knowledge. We follow previous work to generate the predictions based on few-shot chain-of-thought (CoT) prompts and then calculate the exact match (EM) accuracy. Multi-task Language Understanding (MMLU) [18] is benchmark designed to measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. The benchmark covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics. MultiPL-E (ME) [6] is multilingual programming benchmark to assess the coding ability of LLMs. It is translated from the Python benchmark into parallel datasets in 18 programming languages. We use the bigcode-evaluation-harness to perform zero-shot code generation in 10 popular programming languages in the HumanEval category and report the pass@1 based on 20 generated samples for each question. Q&A example comparison We present case studies to demonstrate how Our Fusion-X method combines the strengths of multiple source LLMs to produce accurate results in different tasks in Fig. 7. We compare the Q&A results with both Llama-2-7B and FuseLLM. We can provide more accurate and relevant answer given question compared to the others."
        },
        {
            "title": "I Extended Related Work",
            "content": "Mixture of experts in LLMs As the usage of LLMs grows, finding ways to boost their efficiency without massively increasing computational demands becomes crucial. In response to this challenge, [40] introduced the concept of Sparsely-gated Mixture-of-Experts (SMoE). Building on this foundation, GShard [26] and Switch Transformers [12] presented some of the first large-scale models leveraging SMoE. This technique reduces computational overhead by dynamically routing inputs to selected subset of available experts, thereby utilizing only the most relevant resources for given tasks. To further improve the performance of SMoE-based LLMs, optimizing the routing policy has been identified as essential. Various attempts have been made, such as Mixtral [21], GLaM [11], and ST-MoE [68], which refine the routing mechanisms and expand the models capacity to handle diverse tasks efficiently. However, these works often face challenges as introducing more experts increases the memory footprinta significant issue for LLMs, given their already substantial resource requirements. Efficient LLMs wide range of approaches have been developed to improve the efficiency of LLMs, which include pruning [29, 33, 65], quantization-aware training [41, 42], token reduction [25, 60], and efficient training [32, 64]. Structured pruning removes redundant weight blocks or individual parameters to accelerate inference, while quantization reduces weight and activation precision for edge deployment. Token reduction techniques compress or prune input representations to maintain semantic fidelity across modalities, and adapter-style or rank-adaptive fine-tuning enables taskspecific updates with minimal overhead. Together, these complementary strategies enable scaling LLMs under strict resource constraints. 19 Figure 7: Comparison of Q&A examples between Llama-2, FuseLLM, and Fusion-X ."
        }
    ],
    "affiliations": [
        "Google",
        "Harvard University",
        "Northeastern University",
        "Peking University"
    ]
}