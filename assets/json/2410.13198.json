{
    "paper_title": "Failing Forward: Improving Generative Error Correction for ASR with Synthetic Data and Retrieval Augmentation",
    "authors": [
        "Sreyan Ghosh",
        "Mohammad Sadegh Rasooli",
        "Michael Levit",
        "Peidong Wang",
        "Jian Xue",
        "Dinesh Manocha",
        "Jinyu Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative Error Correction (GEC) has emerged as a powerful post-processing method to enhance the performance of Automatic Speech Recognition (ASR) systems. However, we show that GEC models struggle to generalize beyond the specific types of errors encountered during training, limiting their ability to correct new, unseen errors at test time, particularly in out-of-domain (OOD) scenarios. This phenomenon amplifies with named entities (NEs), where, in addition to insufficient contextual information or knowledge about the NEs, novel NEs keep emerging. To address these issues, we propose DARAG (Data- and Retrieval-Augmented Generative Error Correction), a novel approach designed to improve GEC for ASR in in-domain (ID) and OOD scenarios. We augment the GEC training dataset with synthetic data generated by prompting LLMs and text-to-speech models, thereby simulating additional errors from which the model can learn. For OOD scenarios, we simulate test-time errors from new domains similarly and in an unsupervised fashion. Additionally, to better handle named entities, we introduce retrieval-augmented correction by augmenting the input with entities retrieved from a database. Our approach is simple, scalable, and both domain- and language-agnostic. We experiment on multiple datasets and settings, showing that DARAG outperforms all our baselines, achieving 8\\% -- 30\\% relative WER improvements in ID and 10\\% -- 33\\% improvements in OOD settings."
        },
        {
            "title": "Start",
            "content": "Failing Forward: Improving Generative Error Correction for ASR with Synthetic Data and Retrieval Augmentation Sreyan Ghosh1*, Mohammad Sadegh Rasooli2, Michael Levit2, Peidong Wang2, Jian Xue2, Dinesh Manocha1, Jinyu Li2, 1University of Maryland, College Park, USA 2Microsoft, USA Correspondence: sreyang@umd.edu 4 2 0 2 7 1 ] . e [ 1 8 9 1 3 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Generative Error Correction (GEC) has emerged as powerful post-processing method to enhance the performance of Automatic Speech Recognition (ASR) systems. However, we show that GEC models struggle to generalize beyond the specific types of errors encountered during training, limiting their ability to correct new, unseen errors at test time, particularly in out-of-domain (OOD) scenarios. This phenomenon amplifies with named entities (NEs), where, in addition to insufficient contextual information or knowledge about the NEs, novel NEs keep emerging. To address these issues, we propose DARAG (Dataand Retrieval-Augmented Generative Error Correction), novel approach designed to improve GEC for ASR in in-domain (ID) and OOD scenarios. We augment the GEC training dataset with synthetic data generated by prompting LLMs and text-to-speech models, thereby simulating additional errors from which the model can learn from. For OOD scenarios, we simulate test-time errors from new domains similarly and in an unsupervised fashion. Additionally, to better handle named entities, we introduce retrieval-augmented correction by augmenting the input with entities retrieved from database. Our approach is simple, scalable, and both domainand language-agnostic. We experiment on multiple datasets and settings, showing that DARAG outperforms all our baselines, achieving 8% 30% relative WER improvements in ID and 10% 33% improvements in OOD settings."
        },
        {
            "title": "Introduction",
            "content": "Automatic Speech Recognition (ASR) is the foundational task of converting spoken language into text. As fundamental goal in computational language processing (Jurafsky, 2000), ASR has facilitated communication across diverse fields, including education (Caballero et al., 2017), health- *Work done during internship at Microsoft Figure 1: Comparison of traditional GEC and DARAG. We augment the training dataset with synthetic data generated using our algorithm and named entities retrieved from datastore to improve in-domain and out-of-domain ASR. care (Latif et al., 2020), and others (den Bogaert et al., 2022). Advances in deep learning have driven significant progress in ASR, with end-to-end models achieving impressive results on various tasks (Li et al., 2022). However, one of the key challenges in real-world ASR applications (Li et al., 2015) is handling variations in speech due to factors like background noise (Chen et al., 2022), speaker accents (Turan et al., 2022), and different speaking styles (Syed et al., 2021). These factors lead to significant reduction in the accuracy of ASR. Humans demonstrate exceptional resilience to challenging speech conditions due to our inherent linguistic knowledge. Traditional ASR systems mimic this by incorporating separate language model (LM) to rescore hypotheses during decoding (Toshniwal et al., 2018; Kannan et al., 2018). The LM evaluates the fluency of the N-best hypotheses generated by the ASR model, and the scores are combined with the ASRs own scores in weighted fashion. The hypothesis with the highest combined score is then selected as the final transcript. However, the rise of large language models (LLMs) with advanced reasoning capabilities has opened possibilities beyond simple rescoring. This has led to the development of Generative Error Correction (GEC) (Chen et al., 2024), where models are trained to correct errors in the best hypothesis by leveraging information from other hypotheses, ultimately improving transcription accuracy. Conventional Generative Error Correction (GEC) models are typically trained by pooling hypothesis-transcription pairs from various ASR systems and datasets, with the expectation that they will generalize well across diverse data at test time (Chen et al., 2024; Hu et al., 2024a; Ghosh et al., 2024b). However, we identify key limitations in this approach. Previous work has primarily focused on foundational or semi-open-source models (e.g., Whisper (Radford et al., 2023)). To explore these limitations, we conducted several single-domain, single-dataset experiments (see Table 1), training GEC models on the same datasets used to train the ASR models. We observed only minor improvements in Word Error Rate (WER) on in-domain tests and no improvements on out-ofdomain (OOD) tests. Upon closer examination, we attribute these shortcomings to three main factors: 1. ASR models generate too few errors on their training data for GEC models to effectively learn error correction. 2. The GEC models are unable to generalize to the novel types of errors it sees at test time. This problem is exacerbated in OOD scenarios, where there is significant shift in the nature of errors encountered during training versus those at test time. 3. GEC models continue to struggle with accurately correcting novel named entities (NEs) in transcriptions. While large language models possess extensive linguistic knowledge, named entities often do not follow general language patterns. We attribute this challenge to insufficient context or lack of knowledge about specific named entities. This problem aplifies in OOD settings. These observations lead us to central hypothesis: The generalization ability of GEC models is limited by the diversity and nature of error types encountered during training. Improving performance requires training GEC models on broader and diverse set of errors (for richer training signals) that are consistent in its characteristics with the types the ASR model generates on the test set. To better generalize to OOD, GEC models need to be trained to correct errors that the in-domain ASR model might plausibly make on the OOD test set. Our Contributions. To this end, we propose DARAG (Dataand Retrieval-Augmented Generative Error Correction), simple, scalable, and domain-agnostic approach designed to enhance GEC performance in ID and OOD scenarios. Our approach is driven by the hypothesis that GEC models perform better when trained to correct errors they are likely to encounter at test time. To achieve this, DARAG generates synthetic training data using generative models. We start by prompting an LLM with few-shot examples of domain-specific transcripts to produce synthetic transcripts. For OOD settings, DARAG uses small set of unsupervised audio samples from the target domain, which are transcribed using the in-domain ASR model to create in-context exemplars. The synthetically generated transcripts are then used to generate synthetic speech via text-to-speech model and voice cloning. Finally, this synthetic speech is used to generate hypothesis-transcription pairs. This process simulates errors that are specific to the targetdomain vocabulary and also imitates the phonetic confusions that the ID ASR model would make in the target domain. Additionally, to improve named entity correction, we introduce retrieval augmentation method (Lewis et al., 2020). Specifically, we extract and store all named entities from the training dataset in datastore and retrieve the top-k most similar entities during GEC. Our proposed method is scalable, with the datastore being easily extendable at test time to incorporate new entities as they are encountered. To summarize, our main contributions are as follows: 1. We conduct thorough investigation into the generalization limitations of LLM-based GEC, demonstrating that its performance can be improved by exposing it to diverse but consistent errors that ASR models are likely to produce at test time. 2. To address these challenges, we propose DARAG, novel method for enhancing GEC in both ID and OOD scenarios. DARAG augments GEC training datasets with synthetic data and decouples named entity correction from the error correction learning process through RAG. DARAG significantly outperforms traditional GEC methods, improving ASR performance by 8%-33%. 3. We provide extensive ablation studies and result analysis to validate the effectiveness of DARAG across various scenarios."
        },
        {
            "title": "2 Related Work\nGenerative Error Correction. Post-ASR error\ncorrection using language models (LMs) has been\nwidely studied (Ma et al., 2023b,a; Zhang et al.,\n2023). Recently, large language models (LLMs)\nhave been applied to this task, rebranded as genera-\ntive error correction (Hu et al., 2024a; Ghosh et al.,\n2024b). While LLMs excel due to their advanced\nlanguage comprehension, it remains unclear which\nerrors they effectively correct, which they miss, and\nhow well they handle novel or unknown named en-\ntities (NEs) that they lack prior knowledge of.",
            "content": "Domain Generalization and Named Entity in ASR. Transcribing NEs is persistent challenge for ASR models (Das et al., 2022). Techniques such as memorization (Bekal et al., 2021) and biasing (Jayanthi et al., 2023) have been developed to improve NE transcription. However, these methods typically focus on known NEs seen during training and struggle with unseen entities, as autoregressive models tend to memorize NEs but generalize poorly to new ones (Heinzerling and Inui, 2020). Improving NE transcription using post-ASR processing or GEC has not been well explored. ASR models often fail under distribution shifts, such as domain, accent, or dialect changes (Singhal et al., 2023). However, the robustness of GEC to domain shifts remains underexplored."
        },
        {
            "title": "ASR Train",
            "content": "Mismat. WER () Mat. WER () LS (960) (No GEC) LS (Clean) LS (960) Vox SPGI Vox (No GEC)"
        },
        {
            "title": "Vox",
            "content": "Vox LS (960) SPGI SPGI (No GEC)"
        },
        {
            "title": "SPGI",
            "content": "SPGI LS (960) Vox 4.6 4.4 7.4 8.8 10.1 9.4 14.5 11.8 7. 7.3 14.2 10.5 4.6 4.4 3.9 4.0 10.1 9.4 6.9 7.7 7. 7.3 4.8 4.9 Table 1: Performance comparison of GEC across three different ASR benchmarks from three different domains. We evaluate and compare across two scenarios: (i) Matched Scenario: In this case, the hypotheses-transcription pairs for training our GEC model are derived from the Train split of the Test dataset (and not from the dataset the ASR model is trained on) (ii) Mismatched Scenario: In this case, the hypothesestranscription pairs are derived from the same dataset the ASR model is trained on. We show that (a) For domain shifts, i.e., in cases where both the hypotheses and the ASR training dataset are from domain different from the test, GEC leads to little to no improvement, and (b) For in-domain scenarios where only the hypotheses are derived from the same domain as the test, employing an ASR model trained on different domain to derive the hypothesis boosts performance. Our objective is to create synthetic dataset, syn = {(ˆai, ˆti), 1 nsyn}, generate NDid best hypotheses for each instance in it ( ˆHid train = {(ˆhi, ˆti), 1 nsyn}), and augment the original set with ˆH to improve error correction on the test set Did test. Alternatively, for an out-of-domain test set Dood test , we assume the availability of small train set from the same domain Dood train = {(ai, ti), 1 nsmall} where nsmall and the accompanying transcripts ti may be human-annotated or generated from Aθ."
        },
        {
            "title": "3.2 What do Error Correction Models Learn",
            "content": "to Correct? Most prior work on Generative Error Correction (GEC) relies on foundational open-access ASR models, like Whisper, to generate hypotheses from various datasets and then trains GEC models on these hypotheses-transcription pairs, denoted as Hid train. However, because the training data used for such ASR models is often undisclosed, there is limited insight into the nature of errors present in the hypotheses and, consequently, the types of errors that the GEC models learn to correct. In this work, we aim to study error correction from more transparent perspective. Table 1 presents experiments where we train an ASR model on single dataset (LibriSpeech (LS) (Panayotov et al., 2015), VoxPopuli (Wang et al., 2021) (Vox), SPGIspeech (ONeill et al., 2021)), then derive hypotheses from either the same or different dataset, and use these pairs to train GEC model. Our key findings are as follows: (i) When GEC models are trained on dataset in different domain (i.e., both Did train and Hid train come from domain that is different from Did test), no performance improvements are observed. We hypothesize this is due to the GEC model encountering errors at test time that differ significantly from those it saw during training. For instance, hypotheses (HP)-transcription (GT) pair generated from the LibriSpeech train set using an ASR model trained on LibriSpeech is as follows: GT: biscuits with sugar on the top preserved ginger hams brawn under glass everything in fact that makes life worth living HP 1: biscuits with sugar on the top preserved ginger hams brawn under glass everything in fact that makes life worth living HP 2: biscuits with sugar on the top preserved ginger hams bran under glass everything in fact that makes life worth living An error by the same ASR model on the VoxPopuli test set, is as follows: GT: spyware allows third party to access the same data as the user. HP 1: spygware allows third party to possess the same data as the user HP 2: spygware allows third party to occupy the same data as the user As we can see, it introduces semantic and lexical errors that are out of the domain knowledge learned during training. (ii) When GEC models are trained on dataset in similar domain (i.e., both Did train and Hid train come from domain identical to Did test), improvements are minimal. We attribute this to the ASR model making fewer errors during inference, providing limited opportunities for the GEC model to learn effective corrections. For example, an ASR model trained on LibriSpeech and VoxPopuli have WERs of 2.2 and 5.1 on their respective train sets. (iii) To examine whether higher error rate in hypotheses enhances GEC training, we use an ASR model trained on different domain to generate hypotheses on our in-domain dataset Did train for GEC model training (the same ASR model is also used for test inference). Surprisingly, this setup consistently yields the most significant improve-"
        },
        {
            "title": "Test ASR Train",
            "content": "Mismat. F1 () Mat. F1 () Vox (No GEC)"
        },
        {
            "title": "Vox",
            "content": "Vox LS (960) SPGI 87.8 87.8 80.9 81.4 87.8 87.8 83.2 84.0 Table 2: Performance comparison of GEC on VoxPopuli, an entity-rich dataset. The Matched Scenario and Mismatched Scenarios are defined as in Table 1. We show that (a) For domain shifts, model performance degrades significantly on NEs. (b) For in-domain scenarios, GEC does not prove to be effective in correcting NEs. ments, likely because the GEC model learns from broader range of errors, enhancing its corrective abilities. These findings highlight (i) the need for large and diverse set of errors and (ii) the need for consistency in error characteristics with those that GEC models will encounter at test time."
        },
        {
            "title": "3.3 How Well do they fair on Named Entities?",
            "content": "To assess the ability of GEC models to correct named entities (NEs), we analyze their performance in various settings. As mentioned earlier, transcribing NEs is major challenge in ASR, particularly in knowledge-rich domains. Table 2 compares GEC performance on VoxPopuli using models trained under different conditions. For this experiment, we leverage annotated NEs from the MSNER dataset (Meeus et al., 2024) for VoxPopuli. Our key findings are: (i) GEC models struggle to correct NEs, likely due to insufficient prior knowl- (ii) In domain-shift scenarios, edge or context. where ASR or GEC models have not encountered the target NEs during training, NE transcription accuracy declines sharply. These results emphasize the importance of incorporating explicit knowledge of NEs to improve correction performance."
        },
        {
            "title": "4 Methodology",
            "content": "Fig. 2 illustrates our proposed method. We propose two simple extensions to improve conventional GEC. First, we propose training the GEC model on additional synthetic data generated using generative models. Additionally, instead of memorizing the named entities, we propose decoupling them from the learning process with RAG. To achieve this, we first extract named entities and store them in datastore. During training and inference, we retrieve them from the datastore and augment them to the instruction with the best hypothesis and other hypotheses. In the following subsections, we explain our methodology in detail. Figure 2: Illustration of DARAG. 1 We generate synthetic data with LLMs and TTS models that are then used to generate hypotheses with diverse errors consistent with the types the ASR model generates on the test set. 2 We extract the NEs and store them in datastore. During training, for every instance, we retrieve the top-k most similar NEs to the best hypothesis and use it to construct an instruction-response pair. Note that in OOD settings we only assume the availability of only few unsupervised speech samples in the original train set and pseudo-transcripts for prompting are generated using the in-domain ASR model."
        },
        {
            "title": "4.1 Synthetic Training Data Augmentation",
            "content": "For In-Domain Scenarios. As discussed in Section 3.2, GEC models fail to learn effective error correction due to the low number of errors in ASR training data. We hypothesize that generating novel spoken utterances not seen during ASR training will introduce more errors that can provide rich training signals for learning error correction. Our goal is to generate spoken utterances that closely mimic the speech characteristics of speakers in the same domain, replicating the style as if spoken by similar speakers in similar contexts. These utterances can then be used to generate new hypotheses, ˆHid train, which we augment into the original dataset Hid train. We achieve this through 3-step process: Step 1. We prompt an LLM (LLaMa-2.0Instruct (Touvron et al., 2023)) with in-context examples sampled from Did train to generate in-domain transcripts (prompt in Appendix B). Step 2. Using voice cloning via TTS, we generate spoken utterances from the transcripts. The TTS model (Parler-TTS Mini (Lacombe et al., 2024)) is conditioned on randomly selected utterances from Did train to replicate the domains speech style. Steps 1 and 2 ensure the generated utterances align with the domain and produce error patterns similar to those expected at test time. Step 3. We generate hypotheses for these utterances using the ASR model Aθ. The resulting hypotheses, ˆHid train, are then added to Hid train to improve GEC model training. For Out-of-Domain Scenarios. In OOD settings, we follow the same steps using Dood train. If annotated transcripts are unavailable, we first transcribe the utterances with the ASR model Aθ. Recall that in our setting Dood train only has few utterances (nsmall 50) and is unsuitable for adaptation of Aθ."
        },
        {
            "title": "4.2 Retrieval Augmented Correction",
            "content": "To enhance the correction of named entities (NEs), we decouple NE correction from the main GEC process and introduce Retrieval-Augmented Correction (RAC) approach. Inspired by RAG, we retrieve the most relevant NEs during both training and inference. Our method follows three steps: Step 1. We apply named entity recognition (NER) on all transcriptions, including those generated synthetically during the previous data augmentation step. The extracted NEs are stored in datastore, DS = {(st), 1 d} where is the total number of extracted NEs. Step 2. During GEC training and inference, we use SentenceBERT (Reimers, 2019) to retrieve the top-k NEs, s, from DS based on their similarity to the best hypothesis. This is formally defined as: (cid:18) (cid:19)(cid:19) = top-k1td sim (1) (cid:18) ei et eiet where ei is the SentenceBERT embedding for the best hypothesis, et is the embedding for an NE in DS, and sim(.) is the cosine similarity between embeddings. We calculate similarity for each NE in DS and select the top-k most similar NEs. This simple method proves to be extremely effective in our case as most errors in named entities belong to misspelled characters due to phonemes misrecognized by the ASR model. However, real-world datasets may contain multiple similarly spelled NEs, and retrieving all such NEs might make it difficult for error correction. We further discuss this in the limitations section. Step 3. The retrieved NEs are then added to the input prompt during training and inference as simple comma-separated list. We found that different prompt formats yielded similar results."
        },
        {
            "title": "4.3 Fine-tuning",
            "content": "To train the LLM for error correction, we follow previous approaches by creating instructionresponse pairs and fine-tuning our LLM on this data. The template for our instruction is as follows: Below is the best hypothesis transcribed from speech recognition system. Please try to revise it using the words that are only included in the other hypotheses and list of named entities from database, both of which will be provided to you. Best-hypothesis: Other-hypothesis: Named-Entities: The ground-truth transcription serves as the target for fine-tuning. Following prior work, we finetune only LoRA adapters (Hu et al., 2021)."
        },
        {
            "title": "5 Experimental Setup\nModels and Hyper-Parameters. For our ASR\nmodel, we employ an encoder-decoder model with\na 12-layer transformer-based encoder and a 6-layer\nconformer-based decoder. We train all datasets for\n100 epochs with Adam optimizer, a learning rate\nof 1e-3, and an effective batch size of 128. For\nlearning GEC, we train the LLM for 10 epochs\nwith Adam optimizer, a learning rate of 5e-5, and\nan effective batch size of 32. We used a standard\nrank of 8, and we did not find a substantial change\nin performance by decreasing or increasing it. We\ngenerate nsyn = n or as many synthetic augmenta-\ntions as the size of the original training set. For\ntop-k NE retrieval, we set k=5. For N-best hypothe-\nses, we set N=5. For OOD, we set nsmall=100. All\nresults are averaged over 3 runs for 3 random seeds.\nDatasets. We evaluate DARAG on 5 bench-\nmark ASR datasets, including LibriSpeech-960\n(LS), SPGISpeech (SPGI), VoxPopulien(Vox), Gi-\ngaspeech (Chen et al., 2021) (Giga) and TED-\nLIUM (Rousseau et al., 2012) (TED). We acknowl-\nedge that for OOD evaluation, prior works use\ndifferent and varied settings. However, we want to\nemphasize that OOD adaptation or evaluation is\nnot our main focus; rather, only to show DARAG\nimproves performance in typical OOD settings.\nComparison Methods and Ablations. For com-\nparison with DARAG, we employ (i) Baseline –\nOnly ASR, and we perform no post-processing. (ii)\nSynth. Adap. – For ID, we add the synthetic data\nto the original ASR training data. For OOD, we\ndo adapter-based continual fine-tuning of the ASR\nmodel (full-fine-tuning gave us worse performance)\n(iii) GER (Chada et al., 2021) – This can be con-\nsidered as DARAG without data aur retrieval aug-\nmentation (iv) RobustGER (Radford et al., 2023)\n(v) LMrank – We use the same LLM (continually\nfine-tuned on the text from training and synthetic\ndataset) as GER for re-scoring the N -best hypothe-\nses and finally take the hypothesis with the best\nscore averaged across the LLM and ASR model\nscores. (vi) Enhance – we also employ a speech en-\nhancement front-end, a HiFi-GAN (Su et al., 2020),\nto denoise the noisy speech before passing it to the\nASR model. For ablations, we employ (i) w/o\nRAC: DARAG without retrieval augmented cor-\nrection. (ii) w/o Aug.: DARAG without synthetic\ndata augmentation but only retrieval augmentation\nbased error correction. (iii) only Synth.: The GEC\nmodel is only trained on hypotheses-transcription\npairs from only the synthetically generated data.",
            "content": "Table 3: Performance comparison (WER) of DARAG with other methods on various in-domain and out-of-domain settings (the Test is OOD w.r.t. the Train). We assume all 5 datasets are from different domains. We also report the absolute improvements w.r.t. to the ASR-only Baseline. DARAG outperforms other methods by 8%30% in in-domain and 10%33% in OOD settings. Test Train Baseline Synth. Adapt. +LMrank +Enhance +GER +RobustGER +DARAG (ours) w/o RAC (ours) w/o Aug. (ours) only Synth. (ours) In-Domain LS (Clean) Out-of-Domain In-Domain LS (Other) Out-of-Domain In-Domain Vox Out-of-Domain In-Domain TED Out-of-Domain In-Domain Giga Out-of-Domain LS Vox SPGI TED LS Vox SPGI TED Vox LS SPGI TED TED SPGI LS Vox Giga TED LS Vox In-Domain SPGI SPGI Out-of-Domain TED LS Vox 4. 8.2 8.9 11.6 8.4 13.7 14.2 17.9 10.1 14.9 11.8 17.0 6. 10.4 9.1 9.9 11.5 22.7 18.0 16.3 7.5 17.7 14.4 11.3 4.6+0% 4.44.3% 4.52.2% 4.44.3% 4.44.3% 4.013.0% 4.28.7% 4.110.9% 4.6+0.0% 8.8+7.3% 9.0+1.1% 11.50.9% 7.49.8% 8.31.2% 8.11.2% 8.81.1% 8.81.1% 8.81.1% 11.14.3% 11.44.3% 11.32.6% 7.49.8% 8.63.4% 11.32.6% 6.125.6% 8.010.1% 10.212.1% 5.928.0% 7.812.4% 9.914.7% 6.817.1% 8.010.1% 10.96.0% 16.8+44.8% 8.31.2% 7.78.3% 7.214.3% 7.214.3% 6.917.9% 6.423.8% 7.016.7% 6.621.4% 8.04.8% 14.0+2.2% 15.5+9.2% 18.6+3.9% 13.51.5% 13.21.5% 13.51.5% 14.01.4% 13.51.4% 13.82.8% 17.9+0.0% 17.5+0.0% 17.42.8% 13.51.5% 13.82.8% 17.42.8% 11.913.1% 13.05.1% 13.05.1% 12.611.3% 13.45.6% 13.45.6% 15.314.5% 15.811.7% 16.010.6% 19.2+7.2% 9.92.0% 9.55.9% 9.92.0% 9.46.9% 9.46.9% 8.614.9% 9.46.9% 8.911.9% 9.55.9% 15.2+2.0% 13.4+13.6% 18.6+9.4% 14.9+0.0% 14.9+0.0% 14.52.7% 11.43.4% 11.83.4% 11.8+0.0% 17.0+0.0% 17.2+0.0% 17.3+1.8% 14.52.7% 11.61.7% 17.3+1.8% 9.834.2% 10.032.9% 8.131.4% 8.428.8% 14.415.3% 14.713.5% 12.118.8% 10.312.7% 19.8+16.5% 15.96.5% 6.53.0% 6.61.5% 6.7+0.0% 6.61.5% 6.8+1.5% 6.27.5% 6.36.0% 6.61.5% 7.0+4.5% 10.03.8% 9.01.1% 10.8+9.1% 10.21.9% 10.41.9% 10.8+3.8% 8.56.6% 9.13.3% 8.83.3% 10.2+3.0% 9.9+0.0% 9.9+0.0% 10.8+3.8% 8.56.6% 10.2+3.0% 8.815.4% 8.29.9% 9.09.1% 8.122.1% 8.74.4% 8.910.1% 10.12.9% 8.29.9% 10.1+2.0% 15.8+51.9% 14.8+28.7% 10.86.1% 10.67.8% 11.04.3% 10.67.8% 9.120.9% 10.211.3% 9.517.4% 11.04.3% 24.3+7.0% 23.4+30.0% 20.2+23.9% 21.55.3% 21.85.3% 22.31.8% 17.71.7% 17.51.7% 17.81.1% 16.20.6% 16.20.6% 16.6+1.8% 22.31.8% 17.81.1% 16.6+1.8% 18.518.5% 18.518.5% 14.718.3% 14.420.0% 14.511.0% 15.08.0% 21.36.2% 16.96.1% 16.4+0.6% 26.2+15.4% 11.0+46.7% 7.15.3% 7.41.3% 7.32.7% 7.41.3% 5.230.7% 6.020.0% 6.414.7% 7.6+1.3% 24.6+39.0% 18.1+25.7% 14.7+30.1% 17.41.7% 17.61.7% 17.7+0.0% 14.4+0.0% 14.4+0.0% 14.21.4% 10.93.5% 11.03.5% 10.57.1% 17.7+0.0% 14.21.4% 10.47.9% 13.921.5% 14.418.6% 12.016.7% 11.619.4% 8.029.2% 8.227.4% 17.04.0% 13.46.9% 10.110.6% 24.9+40.7%"
        },
        {
            "title": "6 Results and Analysis",
            "content": "Main Results. Table 3 presents our main results, comparing performance across five datasets in both In the ID setting, the ID and OOD scenarios. training and test sets come from the same dataset, whereas in the OOD setting, the training set is sourced from different dataset, making the test set OOD for both the ASR and GEC models. For the OOD experiments, we randomly selected three datasets for training without any particular preference. Additionally, we did not assume the availability of ground-truth transcripts in Dood train and instead used our ASR model to generate transcripts. Unlike previous experiments, we did not assume separate dataset for ASR training; both the ASR model and hypotheses were generated from the same training data. Our key findings can be summarized as follows: (i) DARAG substantially improves ASR performance for both ID (8%-30%) and OOD (10%-33%) settings. (ii) In ID settings, both RAC and synthetic augmentation prove essential, as ablating either component leads to decline in performance. (iii) In OOD settings, augmentation is more beneficial than RAC, likely because most NEs in the datastore do not match the NEs encountered during testing. (iv) DARAG proves to be better way to utilize synthetic data for improving ASR as adaptation with synthetic data leads to performance decrease over baseline.(v) In some OOD cases, removing RAC actually improves performance, which we attribute to mismatched OOD NEs causing the GEC model to incorrectly adjust certain NEs. (vi) Relying solely on synthetic data is not effective for OOD scenarios, consistent with prior research indicating that human-annotated data remains crucial for optimal performance (Ghosh et al., 2024a). In Appendix we demonstrate that DARAG does not simply replicate the original training data as result of LLM memorization."
        },
        {
            "title": "6.1 Does Retrieval Augmentation Improve\nTranscription of Named Entities?",
            "content": "Table 4 presents comparison of F1-micro scores for DARAG and various baselines in both ID and OOD settings. The results reveal several key insights: (i) DARAG consistently outperforms the baseline and conventional GEC approaches, with particularly large gains in OOD scenarios, demonstrating its robustness to domain shifts. (ii) Incorporating datastore containing NEs from the in-domain dataset significantly improves OOD performance, in some cases matching the results of GEC models trained on ID datasets. This highlights the effectiveness of retrieval-augmented correction in enhancing ASR performance, including practical applications like meeting applications, where datastore can be constructed with list of relevant NEs and not necessarily included during training."
        },
        {
            "title": "Method",
            "content": "OOD F1 () ID F1 ()"
        },
        {
            "title": "ASR Train",
            "content": "GEC Train WER ()"
        },
        {
            "title": "Baseline",
            "content": "+GEC +DARAG"
        },
        {
            "title": "Vox",
            "content": "+synth. NE +DARAG w/ ID NE +synth. NE"
        },
        {
            "title": "Baseline",
            "content": "+GEC +DARAG +synth. NE +DARAG w/ ID NE +synth. NE LS (Other) 79.5 80.9 82.3 82.8 89.9 90.7 82.5 82.0 83.1 84.9 93.1 93.4 87.8 87.8 90.0 92.3 - - 93.2 93.5 96.0 96.4 - - Table 4: Performance comparison of DARAG with other methods on the NE transcription. For ID, we employ the train set of the dataset as the test. For OOD, we employ LS for Vox and Vox for LS. w/ ID NE refers to DARAG, where the NE datastore is from the ID train set. w/ synth NE refers to additional synthetic NEs we add to the NE datastore. (iii) Augmenting the datastore with synthetically generated NEs also shows promise in boosting DARAGs performance, indicating the potential to dynamically add emerging NEs to the datastore. This approach reduces the reliance on continual fine-tuning for ASR adaptation, which is typically required in other methods (Das et al., 2022)."
        },
        {
            "title": "6.2 DARAG for Source-Free UDA",
            "content": "Most Unsupervised Domain Adaptation (UDA) methods for ASR assume the presence of the entire unlabeled dataset from the target domain (Hu et al., 2024b). On the other hand, DARAG assumes the presence of only few unlabeled instances. Fig. 3 shows DARAG proves to be effective for extreme low-resource UDA and outperforms STAR and continual fine-tuning with pseudo-labeling."
        },
        {
            "title": "6.3 Real Data Outperforms Synthetic",
            "content": "Table 5 shows comparison between DARAG and various baseline configurations where the synthetic dataset is replaced with the original training set of the target domain. The results clearly demonstrate that using real training data for generating GEC hypotheses significantly boosts performance, often surpassing complete ID settings. We attribute this improvement to two main factors: (i) the ASR model produces more errors on the GEC training dataset due to domain mismatch, providing richer training signals, and (ii) the datastore is enriched with real NEs from the original training set, offering more accurate context for corrections. Extra Results. We present extra results in the Appendix, including key hyper-parameter tuning results, importance of the voice cloning module and"
        },
        {
            "title": "Baseline",
            "content": "+DARAG"
        },
        {
            "title": "Baseline\nBaseline",
            "content": "+DARAG +DARAG"
        },
        {
            "title": "Baseline\nBaseline",
            "content": "+DARAG +DARAG"
        },
        {
            "title": "Baseline",
            "content": "+DARAG"
        },
        {
            "title": "Baseline\nBaseline",
            "content": "+DARAG +DARAG"
        },
        {
            "title": "Baseline\nBaseline",
            "content": "+DARAG +DARAG"
        },
        {
            "title": "Vox\nVox",
            "content": "LS LS + Vox LS LS TED TED + Vox TED TED"
        },
        {
            "title": "SPGI\nSPGI",
            "content": "LS LS + SPGI LS LS TED TED + SPGI TED TED - Vox - - LS Vox - - TED Vox - SPGI - - LS SPGI - - TED SPGI 10.1 8.6 14.9 10.3 10.0 6.9 17.0 10.0 14.4 7.5 7.5 5. 13.3 7.7 12.0 4.8 17.7 7.9 13.9 5.0 Table 5: Performance comparison of DARAG in OOD settings with the baseline. We replace the generated augmentations with the original target domain training dataset (and do not generate extra augmentations). Training on hypotheses from the target domain train set leads to superior performance. 50 40 20 10 m r e o A"
        },
        {
            "title": "DARAG\nPseudo\nSTAR",
            "content": "0 100"
        },
        {
            "title": "Target Domain Instances",
            "content": "Figure 3: Comparison of DARAG with other methods on lowresource source-free UDA (LS Vox). DARAG outperforms other methods with significant improvements. other results. Additionally, we provide examples of generated augmentations in Table 12 and DARAG corrections in Table 13."
        },
        {
            "title": "7 Conclusion",
            "content": "We present DARAG, novel approach for enhancing GEC in ASR systems. Our study reveals that GEC models struggle to generalize due to the limited diversity of error types encountered during training. To address this, we introduce two key improvements: (i) synthetic data augmentation strategy that generates hypotheses with diverse errors resembling those the ASR model might realistically produce on test set, and (ii) retrieval-augmented NE correction mechanism. DARAG achieves performance superior to our compared methods."
        },
        {
            "title": "Limitations",
            "content": "As part of future work, we would like to work on the following limitations of our proposed DARAG approach: 1. when the NE database is large, semantic similarity may result in the retrieval of multiple phonetically similar named entities, potentially causing confusion for the GEC model in choosing the correct entity. To address this, we plan to develop phoneme-aware NE retrieval methods to enhance retrieval accuracy. 2. The use of synthetic data generated by LLMs could introduce biases inherent to the language models, potentially affecting the GEC models performance. In future work, we aim to explore strategies for mitigating such biases to ensure more robust error correction. 3. Although DARAG involves additional computational overhead for generating synthetic data, we anticipate that as model efficiency improves and lighter architectures become available, the overhead will be reduced, leading to even greater gains in performance. 4. We only study ASR datasets in the English language. Future work includes evaluating DARAGs performance in low-resource languages beyond English."
        },
        {
            "title": "References",
            "content": "Dhanush Bekal, Ashish Shenoy, Monica Sunkara, Sravan Bodapati, and Katrin Kirchhoff. 2021. Remember the context! asr slot error correction through In 2021 IEEE Automatic Speech memorization. Recognition and Understanding Workshop (ASRU), pages 236243. IEEE. Daniela Caballero, Roberto Araya, Hanna Kronholm, Jouni Viiri, André Mansikkaniemi, Sami Lehesvuori, Tuomas Virtanen, and Mikko Kurimo. 2017. Asr in classroom today: Automatic visualization of conceptual network in science classrooms. In Data Driven Approaches in Digital Education: 12th European Conference on Technology Enhanced Learning, ECTEL 2017, Tallinn, Estonia, September 1215, 2017, Proceedings 12, pages 541544. Springer. Rakesh Chada, Pradeep Natarajan, Darshan Fofadiya, and Prathap Ramachandra. 2021. Error detection in large-scale natural language understanding systems using transformer models. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 498503, Online. Association for Computational Linguistics. Chen Chen, Nana Hou, Yuchen Hu, Shashank Shirol, and Eng Siong Chng. 2022. Noise-robust speech recognition with 10 minutes unparalleled in-domain data. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 42984302. IEEE. Chen Chen, Yuchen Hu, Chao-Han Huck Yang, Sabato Marco Siniscalchi, Pin-Yu Chen, and EngSiong Chng. 2024. Hyporadise: An open baseline for generative speech recognition with large language models. Advances in Neural Information Processing Systems, 36. Guoguo Chen et al. 2021. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. In Proc. Interspeech 2021. Nilaksh Das, Monica Sunkara, Dhanush Bekal, Duen Horng Chau, Sravan Bodapati, and Katrin Kirchhoff. 2022. Listen, know and spell: Knowledgeinfused subword modeling for improving asr performance of oov named entities. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 78877891. IEEE. Joachim Van den Bogaert, Laurens Meeus, Alina Kramchaninova, Arne Defauw, Sara Szoc, Frederic Everaert, Koen Van Winckel, Anna Bardadym, and Tom Vanallemeersch. 2022. Automatically extracting the semantic network out of public services to support cities becoming smart cities. In Proceedings of the 23rd Annual Conference of the European Association for Machine Translation, pages 343344, Ghent, Belgium. European Association for Machine Translation. Sreyan Ghosh, Sonal Kumar, Zhifeng Kong, Rafael Valle, Bryan Catanzaro, and Dinesh Manocha. 2024a. Synthio: Augmenting small-scale audio classification datasets with synthetic data. arXiv preprint arXiv:2410.02056. Sreyan Ghosh, Sonal Kumar, Ashish Seth, Purva Chiniya, Utkarsh Tyagi, Ramani Duraiswami, and Dinesh Manocha. 2024b. LipGER: VisuallyConditioned Generative Error Correction for Robust In Proc. INTERAutomatic Speech Recognition. SPEECH 2024. Benjamin Heinzerling and Kentaro Inui. 2020. Language models as knowledge bases: On entity representations, storage capacity, and paraphrased queries. arXiv preprint arXiv:2008.09036. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Chao Zhang, Pin-Yu Chen, and Eng Siong Chng. 2024a. Large language models are efficient learners of noise-robust speech recognition. In International Conference on Learning Representations. Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Chengwei Qin, Pin-Yu Chen, Eng Siong Chng, and Chao Zhang. 2024b. Self-taught recognizer: Toward unsupervised adaptation for speech foundation models. arXiv preprint arXiv:2405.14161. Sai Muralidhar Jayanthi, Devang Kulshreshtha, Saket Dingliwal, Srikanth Ronanki, and Sravan Bodapati. 2023. Retrieve and copy: Scaling asr personalization to large catalogs. arXiv preprint arXiv:2311.08402. Daniel Jurafsky. 2000. Speech and language processing. Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara Sainath, Zhijeng Chen, and Rohit Prabhavalkar. 2018. An analysis of incorporating an external language model into sequence-to-sequence model. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15828. IEEE. Yoach Lacombe, Vaibhav Srivastav, and Sanchit Gandhi. 2024. Parler-tts. https://github.com/ huggingface/parler-tts. Siddique Latif, Junaid Qadir, Adnan Qayyum, Muhammad Usama, and Shahzad Younis. 2020. Speech technology for healthcare: Opportunities, challenges, and state of the art. IEEE Reviews in Biomedical Engineering, 14:342356. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474. Jinyu Li, Li Deng, Reinhold Haeb-Umbach, and Yifan Gong. 2015. Robust automatic speech recognition: bridge to practical applications. Academic Press. Jinyu Li et al. 2022. Recent advances in end-to-end automatic speech recognition. APSIPA Transactions on Signal and Information Processing, 11(1). Danni Liu and Jan Niehues. 2024. Recent highlights in multilingual and multimodal speech translation. In Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024), pages 235253. Rao Ma, Mark JF Gales, Kate Knill, and Mengjie Qian. 2023a. N-best T5: Robust ASR error correction using multiple input hypotheses and constrained decoding space. arXiv preprint arXiv:2303.00456. Rao Ma, Mengjie Qian, Potsawee Manakul, Mark Gales, and Kate Knill. 2023b. Can generative large language models perform asr error correction? arXiv preprint arXiv:2307.04172. Patrick ONeill, Vitaly Lavrukhin, Somshubra Majumdar, Vahid Noroozi, Yuekai Zhang, Oleksii Kuchaiev, Jagadeesh Balam, Yuliya Dovzhenko, Keenan Freyberg, Michael Shulman, et al. 2021. Spgispeech: 5,000 hours of transcribed financial audio for fully formatted end-to-end speech recognition. arXiv preprint arXiv:2104.02014. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 52065210. IEEE. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR. Reimers. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. Anthony Rousseau, Paul Deléglise, and Yannick Esteve. 2012. Ted-lium: an automatic speech recognition dedicated corpus. In LREC, pages 125129. Peeyush Singhal, Rahee Walambe, Sheela Ramanna, and Ketan Kotecha. 2023. Domain adaptation: challenges, methods, datasets, and applications. IEEE access, 11:69737020. Jiaqi Su, Zeyu Jin, and Adam Finkelstein. 2020. Hifigan: High-fidelity denoising and dereverberation based on speech deep features in adversarial networks. arXiv preprint arXiv:2006.05694. Shahbaz Syed, Khalid Al Khatib, Milad Alshomary, Henning Wachsmuth, and Martin Potthast. 2021. Generating informative conclusions for argumentative texts. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 34823493, Online. Association for Computational Linguistics. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Shubham Toshniwal, Anjuli Kannan, Chung-Cheng Chiu, Yonghui Wu, Tara Sainath, and Karen Livescu. 2018. comparison of techniques for language model integration in encoder-decoder speech recognition. In 2018 IEEE spoken language technology workshop (SLT), pages 369375. IEEE. Quentin Meeus, Marie-Francine Moens, and Hugo Van hamme. 2024. MSNER: multilingual speech dataset for named entity recognition. In Proceedings of the 20th Joint ACL - ISO Workshop on Interoperable Semantic Annotation @ LREC-COLING 2024, pages 816, Torino, Italia. ELRA and ICCL. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Tugtekin Turan, Dietrich Klakow, Emmanuel Vincent, and Denis Jouvet. 2022. Adapting language models when training on privacy-transformed data. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 43674373, Marseille, France. European Language Resources Association. Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. 2021. Voxpopuli: large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. arXiv preprint arXiv:2101.00390. Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al. 2018. Espnet: End-to-end speech processing toolkit. arXiv preprint arXiv:1804.00015. Ziji Zhang, Zhehui Wang, Rajesh Kamma, Sharanya Eswaran, and Narayanan Sadagopan. 2023. Patcorrect: Non-autoregressive phoneme-augmented transformer for asr error correction. arXiv preprint arXiv:2302.05040."
        },
        {
            "title": "A Appendix",
            "content": "In the Appendix, we provide: 1. Section B: Prompts 2. Section C: Analysis of LLMs copying the training data 3. Section D: Performance of DARAG w/o Voice"
        },
        {
            "title": "Cloning",
            "content": "4. Section E: In-Domain Performance After Outof-Domain Adaptation 5. Section F: Key Hyper-Parameter Tuning Results 6. Section G: Examples of DARAG Generations 7. Section H: Examples of DARAG Corrections 8. Section I: Additional Details"
        },
        {
            "title": "B Prompts",
            "content": "We prompt LLaMa-2-Instruct in batched-mode with temperature of 0.7 and top-p of 1. We use this setting throughout all our experiments for generation and correction. We use the below prompt to generate synthetic transcripts using LLaMa-2Instruct: You need to act as synthetic data generator. will provide you with some example transcripts from speech recognition dataset that have transcribed using an ASR model. The transcripts are not related to each other. You need to first understand the nature of the spoken utterances from the transcripts and analyze their distinct features, like domain, style, length, etc. Next, with what you understood, you need to generate 2 short and diverse utterances with the same properties but diverse content. Each utterance should be single sentence. Please include named entities as and when possible, but it is not necessary. Keep the utterances short and in line with the examples. Your generated transcripts should be coherent. Here are the example transcripts, one in each line:{}. Return JSON with 2 keys named \"First Transcript\" and \"Second Transcript\" with the values as the generated transcripts."
        },
        {
            "title": "C Are LLMs Just Replicating the",
            "content": "Original Training Data? Previous research has suggested that LLMs may memorize open-domain ASR training transcripts (Liu and Niehues, 2024; Team et al., 2023), raising the risk of replicating training data while genrating synthetic data. To evaluate whether this occurs with DARAG, we perform two checks: (i) We use SentenceBERT to calculate the cosine similarity between each generated transcript and all transcripts in the original training set, reporting the average semantic similarity across instances in Table 6 ii) We compute the BLEU score for each generated transcript, using the transcript with the highest cosine similarity from the previous step as reference. Table 6 shows the average BLEU scores across BLEU1, BLEU2, and BLEU3. The low BLEU scores indicate that DARAG does not simply replicate the training data. The semantic similarity indicates that with DARAG generates transcripts that are consistent with the domain. DARAG w/o Voice Cloning Table 7 compares the performance of DARAG in both ID and OOD scenarios, with and without voice cloning. As discussed in Section 4.1, voice cloning via TTS allows the model to generate synthetic speech that, when transcribed, produces hypothe-"
        },
        {
            "title": "Method",
            "content": "Train OOD Adapt. WER ()"
        },
        {
            "title": "LS\nVox\nSPGI\nGiga\nTED",
            "content": "0.32 0.29 0.25 0.22 0.26 0.12 0.10 0.06 0.13 0.14 Table 6: Semantic similarity and BLEU scores between original and generated transcripts across all datasets. ses containing errors similar to those encountered during testing in that domain. As shown in the table, DARAG experiences performance drop without voice cloning, with more significant decline in OOD scenarios."
        },
        {
            "title": "Method",
            "content": "Train WER ()"
        },
        {
            "title": "Vox",
            "content": "LS (Other)"
        },
        {
            "title": "Baseline",
            "content": "Vox +DARAG Vox +DARAG w/o Voice Cloning Vox LS LS LS +DARAG +DARAG w/o Voice Cloning"
        },
        {
            "title": "Baseline",
            "content": "+DARAG +DARAG w/o Voice Cloning LS LS LS Vox +DARAG Vox +DARAG w/o Voice Cloning Vox"
        },
        {
            "title": "Baseline",
            "content": "10.1 8.6 8.8 14.9 10.0 12.2 8.4 6.4 7.3 13.7 11.9 14.5 Table 7: Performance comparison of DARAG with and without voice cloning. Performance drops sharply without voice cloning, especially in OOD scenrios, thereby confirming the importance of the voice cloning for generating augmentations. In-Domain Performance in Out-of-Domain Settings Table 8 presents the performance of DARAG on in-domain tests after augmenting the hypotheses dataset with OOD hypotheses-transcription pairs. The results demonstrate that DARAG maintains its performance on the in-domain test with only negligible drop. Hyper-parameter Tuning F.1 Effect of for NE retrieval"
        },
        {
            "title": "Vox",
            "content": "LS (Other)"
        },
        {
            "title": "Baseline",
            "content": "+DARAG +DARAG +DARAG +DARAG"
        },
        {
            "title": "Baseline",
            "content": "+DARAG +DARAG +DARAG +DARAG - Vox Vox Vox Vox - LS LS LS LS - - LS SPGI TED - - Vox SPGI TED 10.1 8.6 8.9 9.0 9. 8.4 6.4 7.5 7.8 6.9 Table 8: Performance comparison of DARAG across different settings. OOD Adapt. refers to the dataset for which synthetic data was generated and augmented to the original hypotheses for GEC training. Our results show that, even with the addition of synthetically generated training data, DARAG maintains its in-domain performance. Furthermore, improvements in specific domain occur only when the augmentations are consistent with that domain. This approach ensures that the errors used for training match the characteristics of those the ASR model will encounter during testing. model. Lower values of can lead to cases where the GT NE is not retrieved."
        },
        {
            "title": "Test",
            "content": "Vox LS (Other) k=1 87.8 94.5 k=2 88.7 94.5 k= 90.0 96.4 k=7 87.9 93.9 k=9 87.8 93.3 Table 9: Performance comparison of DARAG on two indomain settings with various values of for NE retrieval. F.2 Effect of nsmall in OOD settings Table 10 compares the performance of DARAG across various values of nsmall. Larger nsmall can lead to more diverse and consistent augmentations, improving performance. For our primary experiments, we stick to 100 to keep our setting ultra-lowresource."
        },
        {
            "title": "Test",
            "content": ""
        },
        {
            "title": "Vox\nSPGI",
            "content": "15.2 17.9 50 11.3 14.1 100 10.0 12.0 9.5 11.7 Table 10: Performance comparison of DARAG on two OOD settings (with LS as training set) with various values of nsmall. Larger values can lead to improved performance. Table 9 compares the performance of DARAG across various values of for NE retrieval. We choose two in-domain settings as our main experiments show NE retrieval is most effective in indomain scenarios. We show both higher and lower values of can lead to drop in performance and find 5 as the most optimal value. Higher values of can retrieve irrelevant NEs and confuse the GEC F.3 Effect of nsyn Table 11 compares the performance of DARAG using different values of nsyn, represented as factor of (the size of the original training set for the target dataset in an OOD setting). Increasing the number of synthetic samples (higher nsyn) can provide more diverse and consistent augmentations in OOD settings, resulting in better performance. However, the improvements plateau beyond certain point. For our main experiments, we use nsyn = 1 due to resource limitations."
        },
        {
            "title": "Test",
            "content": "0.5"
        },
        {
            "title": "Vox\nSPGI",
            "content": "13.1 14.2 10.0 12.0 2 9.6 11.3 5 9.7 11. Table 11: Performance comparison of DARAG on two OOD settings (with LS as training set) across different scaling factors of nsyn relative to n. More synthetic samples can lead to improved performance, but plateaus beyond certain point."
        },
        {
            "title": "G Examples of Generated Transcripts",
            "content": "Table 12 provides examples of synthetically generated transcripts for each dataset from our evaluation setup. The transcripts are coherent and consistent with the characteristics of the domain."
        },
        {
            "title": "H Examples of DARAG Corrections",
            "content": "Table 13 qualitatively compares DARAG with traditional GEC on various instances from benchmark datasets. We show that DARAG is able to accurately correct NEs which traditional GEC cannot. Additionally, DARAG shows superior performance in OOD scenarios."
        },
        {
            "title": "I Additional Details",
            "content": "Compute details. For all our pre-training and finetuning experiments, we used four NVIDIA A600048GB GPUs. Each training requires 4-24 hours. Potential Risk. As mentioned in the limitations section of the paper, DARAG might encode biases inherent to the LLM. This might lead to unsafe generations and corrections. Additionally, voice cloning systems used as part of our method can be employed to create deep fake voices. Software and Packages details. We implement all our models in PyTorch 1 and use Parler-TTS 2 and LLaMa-2 3. We employ ESPnet (Watanabe et al., 2018) for training our ASR models. Use of AI models. We used GPT-4 for rephrasing certain parts of the writing. Datasets. Dataset details, together with statistics are provided below: 1https://pytorch.org/ 2https://github.com/huggingface/parler-tts 3https://huggingface.co/meta-llama LibriSpeech 4 The LibriSpeech dataset is largescale corpus of approximately 1,000 hours of 16kHz English speech derived from audiobooks in the LibriVox project, with text sourced primarily from Project Gutenberg. It is split into training sets (100hr, 360hr, and 500hr) and dev/test sets categorized as dev clean(5hr), dev other(5hr), test clean(5hr), and test other(5hr) based on transcription difficulty. The dataset also includes n-gram language models and texts with 803 million tokens and 977,000 unique words, making it valuable for Automatic Speech Recognition (ASR) research. SPGISpeech 5 SPGISpeech is large-scale speech transcription dataset containing 5,000 hours of professionally transcribed financial audio, including company earnings calls with variety of L1 and L2 English accents. It features approximately 50,000 speakers and offers high-quality transcripts that have been thoroughly edited for accuracy, including proper punctuation, capitalization, and denormalization of non-standard words. The audio is split into 5 to 15-second slices, formatted as singlechannel, 16kHz, 16-bit WAV files, making it ideal for training advanced speech recognition models. VoxPopuli 6 VoxPopuli is large-scale multilingual speech corpus designed for tasks like representation learning, semi-supervised learning, and interpretation. It offers 400,000 hours of unlabeled speech in 23 languages, resulting in 8K-24K hours of data for each language, 1,800 hours of transcribed speech in 16 languages, and 17,300 hours of speech-to-speech interpretation across 15 language pairs. In transcribed speech, the filtered utterances are split into train, development and test sets with disjoint speakers and target duration ratio (18:1:1). Additionally, it includes 29 hours of transcribed non-native English speech for research on accented speech in ASR. GigaSpeech 7 GigaSpeech is large-scale English speech recognition corpus with 10,000 hours of training set of high-quality human-transcribed audio for supervised learning, 12 hours of dev set, and 40 hours of test set. It is designed for both supervised and unsupervised/semi-supervised learning tasks, covering wide range of domains. It is particularly suited for large-scale speech recognition model training and adaptation. 4https://www.openslr.org/12 5https://datasets.kensho.com/datasets/spgispeech 6https://github.com/facebookresearch/voxpopuli 7https://github.com/SpeechColab/GigaSpeech"
        },
        {
            "title": "Synthetic Transcripts",
            "content": "the duke entered the grand hall as the musicians began playing lively gavotte her highness attended the gala wearing the renowned emerald necklace from the royal collection Sarah, can we reassess the projected growth for the third quarter and adjust our targets accordingly? Our current expectation is to maintain minimum margin of 40%, though market conditions may lead to some adjustments. please navigate to the settings page to update your api key and configure the callback url. she served as the vice chair of the european data protection board for three years before joining the united nations privacy task force. as the smoke cleared the battered zeppelin drifted slowly back towards the enemys encampment yet shall not yield to their demands but will defend my honor just as young frederick once did in times of great peril we are often overwhelmed by too many options and that can make even simple decisions difficult to navigate must admit that my journey has had its ups and downs but in the end found exactly what was looking for Table 12: Examples of generated transcripts by the DARAG methodology. TED-LIUM (v1) 8 The TED-LIUM corpus is dataset of English-language TED talks, featuring transcriptions of talks sampled at 16kHz. It contains approximately 118 to 452 hours of transcribed speech data, with 56,803 examples in the training set, 1,469 in the test set, and 591 in the validation set. This dataset is widely used for Automatic Speech Recognition (ASR) research and model training. All datasets used in our paper are openly available for download and free to use to academic research. 8https://www.openslr.org/7/"
        },
        {
            "title": "DARAG",
            "content": "how eye wish you could get me coffee of that pitcher phillip laura said in treating lee how wish you could get me coffee of that pitcher phillip laura said in treatingly how wish you could get me copy of that picture philip laura said treatingly LibriSpeech Other (OOD on Vox) but she fixed up on pitcher which she said she preferred too anything she had scene in the galley but she fixed up on pitcher which she said she preferred too anything she had scene in the galley but she fixed upon picture which she said she preferred to anything she had seen in the gallery"
        },
        {
            "title": "SPGI",
            "content": "and we expect once the Sharon Nation Credit gets taken care of, were in arrange where we will be managing in flows and out flows on normal and we expect once the Sharon Nation Credit gets taken care of, were in arrange where we will be managing in flows and out flows on normal and we expect once the Shared National Credit gets taken care of, were in range where we will be managing inflows and outflows on normal SPGI (OOD on Vox) obviously, the confidence level on future and growing exploration in the Golf of Mexico, in South East Asia. So obviously, the confidence level on future and growing exploration in the Golf of Mexico, in South East Asia. So obviously, the confidence level on future and growing exploration in the Gulf of Mexico, in Southeast Asia. So"
        },
        {
            "title": "GigaSpeech",
            "content": "TRULY THE EIGHT WONDER OF THE WORLD SEAN ELLIOT. THANK YOU SO MUCH. TRULY THE EIGHT WONDER OF THE WORLD SEAN ELLIOT. THANK YOU SO MUCH. TRULY THE EIGHTH WONDER OF THE WORLD SHAWN ELLIOTT . THANK YOU SO MUCH . GigaSpeech (OOD on Vox) MICROSOFT FIRED BACK WITH ITS OWN SEARCH INJUN MICROSOFT FIRED BACK WITH ITS OWN SEARCH INJUN MICROSOFT FIRED BACK WITH ITS OWN SEARCH ENGINE"
        },
        {
            "title": "VoxPopuli",
            "content": "we need mores sources we need mores pipes than one from rush ya we need mores sources we need mores pipes than one from rush ya we need more sources we need more pipes than one from russia VoxPopuli (OOD on LibriSpeech) may in decay however that the protection of arbitration agreements should not limited the free circulation of judgments in the union may indicate however that the protection of arbitration agreements should not limited the free circulation of judgments in the union may indicate however that the protection of arbitration agreements should not limit the free circulation of judgements in the union Table 13: Examples of incorrect ASR transcriptions and their corresponding corrections by DARAG."
        }
    ],
    "affiliations": [
        "Microsoft, USA",
        "University of Maryland, College Park, USA"
    ]
}