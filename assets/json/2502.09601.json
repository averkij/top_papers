{
    "paper_title": "CoT-Valve: Length-Compressible Chain-of-Thought Tuning",
    "authors": [
        "Xinyin Ma",
        "Guangnian Wan",
        "Runpeng Yu",
        "Gongfan Fang",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chain-of-Thought significantly enhances a model's reasoning capability, but it also comes with a considerable increase in inference costs due to long chains. With the observation that the reasoning path can be easily compressed under easy tasks but struggle on hard tasks, we explore the feasibility of elastically controlling the length of reasoning paths with only one model, thereby reducing the inference overhead of reasoning models dynamically based on task difficulty. We introduce a new tuning and inference strategy named CoT-Valve, designed to allow models to generate reasoning chains of varying lengths. To achieve this, we propose to identify a direction in the parameter space that, when manipulated, can effectively control the length of generated CoT. Moreover, we show that this property is valuable for compressing the reasoning chain. We construct datasets with chains from long to short for the same questions and explore two enhanced strategies for CoT-Valve: (1) a precise length-compressible CoT tuning method, and (2) a progressive chain length compression approach. Our experiments show that CoT-Valve successfully enables controllability and compressibility of the chain and shows better performance than the prompt-based control. We applied this method to QwQ-32B-Preview, reducing reasoning chains on GSM8K from 741 to 225 tokens with a minor performance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with only one additional incorrect answer."
        },
        {
            "title": "Start",
            "content": "CoT-Valve: Length-Compressible Chain-of-Thought Tuning Xinyin Ma*, Guangnian Wan*, Runpeng Yu, Gongfan Fang, Xinchao Wang National University of Singapore {maxinyin, guangnian}@u.nus.edu, xinchao@nus.edu.sg 5 2 0 2 3 1 ] . [ 1 1 0 6 9 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Chain-of-Thought significantly enhances models reasoning capability, but it also comes with considerable increase in inference costs due to long chains. With the observation that the reasoning path can be easily compressed under easy tasks but struggle on hard tasks, we explore the feasibility of elastically controlling the length of reasoning paths with only one model, thereby reducing the inference overhead of reasoning models dynamically based on task difficulty. We introduce new tuning and inference strategy named CoT-Valve, designed to allow models to generate reasoning chains of varying lengths. To achieve this, we propose to identify direction in the parameter space that, when manipulated, can effectively control the length of generated CoT. Moreover, we show that this property is valuable for compressing the reasoning chain. We construct datasets with chains from long to short for the same questions and explore two enhanced strategies for CoT-Valve: (1) precise length-compressible CoT tuning method, and (2) progressive chain length compression approach. Our experiments show that CoT-Valve successfully enables controllability and compressibility of the chain and shows better performance than the prompt-based control. We applied this method to QwQ-32B-Preview, reducing reasoning chains on GSM8K from 741 to 225 tokens with minor performance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with only one additional incorrect answer."
        },
        {
            "title": "Introduction",
            "content": "Chain-of-Thought (CoT) reasoning (Wei et al., 2022) has emerged as powerful technique for enhancing the reasoning capabilities of large language models (Jaech et al., 2024; Dubey et al., 2024; Abdin et al., 2024), particularly in complex *Equal contribution Corresponding Author 1 tasks such as mathematics and coding (Sprague et al., 2024) that require multi-step inference. By simulating the process of human-like thought progression, CoT enables models to break down complex problems into sub-questions, improving accuracy and interpretability (Joshi et al., 2023). Those reasoning abilities have also been tested in different domains, such as image generation (Ma et al., 2025) and visual understanding (Shao et al., 2024). Training reasoning models often involves generating extensive reasoning paths through methods such as sampling (Wang et al., 2023), tree search (Yao et al., 2023; Guan et al., 2025a; Zhang et al., 2024) or reinforcement learning (DeepSeekAI, 2025) to ultimately reach the correct answer. However, these long chains often incorporate redundant intermediate steps that can be unnecessary or too complex (Lightman et al., 2024), and the redundancy in the reasoning paths for training leads to inefficiencies in token usage and increased inference costs. However, crafting an optimal reasoning chain that omits extraneous details is challenging due to the limited availability of intermediate rewards to guide the process and human annotations (Zhang et al., 2025). Removing some or all of the intermediate steps and then training or distilling the model (Liu et al., 2024b; Yu et al., 2024) will degrade the performance. Alternative approaches employ information-theoretic measures (Ton et al., 2024) or identify an \"overthinking\" solution in QwQ (Team, 2024b) to evaluate the contribution of each sentence to the final answer. We observe that current reasoning models, such as QwQ (Team, 2024b) and DeepSeekR1 (DeepSeek-AI, 2025) allocate an excessive number of tokens to simple tasks, while potentially providing insufficient tokens for complex tasks. Thus, long reasoning path is still essential, while maintaining the ability to compress reasoning paths for simpler questions is equally important. To solve this, our goal is to fine-tune model caFigure 1: The reasoning model, after the length-compressible CoT tuning, can generate reasoning paths from long to short, leveraging LoRA as Valve. We show one example from our constructed dataset MixChain. pable of generating both long and short reasoning paths, rather than being restricted to compressed form. We offer new way to control the length of CoT, which we refer to as Length-Compressible Chain-of-Thought Tuning. central component of the proposed method is to identify an update direction in the parameter space, which, by manipulating it, acts as increasing or decreasing the length of CoT. Taking large step in this direction leads the model to generate short sequence, while small step still produces long and complex reasoning trajectory. We choose to incorporate this update direction by LoRA (Hu et al., 2022), enabling it to function as an additional branch that facilitates easy modulation of intensity while imposing minimal extra parameters on the model. We explore methods to identify this direction and demonstrate that it offers superior controllability compared to promptbased approaches, which enables the generation of short CoT that prompt-based methods are unable to achieve. Besides, we observe that the direction can be extrapolated, allowing the reasoning chains to be extended beyond or shortened to lengths unseen in the training set. Leveraging this compressibility, we construct dataset that pairs long and short reasoning chains for each question. This dataset is then utilized in two ways: (1) to refine the direction for more precise tuning, and (2) to progressively compress the reasoning path. We evaluate our method across different types of models, ranging from pre-trained LLM with little reasoning ability, LLaMA-3.1-8B and LLaMA-3.2-1.5B-Instruct (Dubey et al., 2024), to post-trained reasoning models, QwQ-32BPreview (Team, 2024b), and distilled reasoning models, DeepSeek-R1 (DeepSeek-AI, 2025). Our results demonstrate that, with training for one time, our approach enables model to generate reasoning paths of varying lengths, and we can achieve better results than previous chain compression baselines. Besides, our study highlights several interesting findings: (1) Short reasoning paths can sometimes outperform longer ones, underscoring the significance of CoT-Valve in enhancing model efficiency. (2) Not every reasoning chain, despite all leading to the correct final answer, is conducive to model optimization. Excessively long or short chains complicate the distillation of CoT, posing challenges to the model training. In summary, our contributions are: (1) CoTValve: Enables elastic control of length for CoT within the parameter space, allowing single model to generate CoT from short to long. (2) MixChain Dataset: dataset with reasoning paths of varying lengths for each question. (3) Improved Tuning & Progressive Compression: Refines the directiontuning process based on MixChain and introduces progressive compression for inference efficiency. (4) Performance & Controllability: Achieves controllable reasoning generation and state-of-theart results for compressed CoT."
        },
        {
            "title": "2 Related Work",
            "content": "Chain-of-Thought. Chain-of-thought (Wei et al., 2022) reasoning has shown promising progress in recent years, especially the success of OpenAiO1 (Jaech et al., 2024) and Deepseek-R1 models (DeepSeek-AI, 2025). This introduces the testtime scaling law, apart from the traditional scaling law for training (Hoffmann et al., 2022). Several approaches have been proposed to boost the language model to have better problem-solving abilities, including the model has its self-reasoning abilities (Team, 2024b) or use Best-of-N (Nakano et al., 2021), beam search and Monte Carlo Tree Search (Kocsis and Szepesvari, 2006; Guan et al., 2025b) to search and refine the solution without further finetune the large language models. The outcome reward model and process reward models are 2 also introduced to evaluate the score for the entire solution, especially the final answer (Cobbe et al., 2021a) and the quality of the reasoning path (Wang et al., 2024; Luo et al., 2025b) Chain Compression in reasoning model. Due to the high computational cost associated with inference in reasoning models, particularly for longchain reasoning, chain compression has become critical area of research. (Yu et al., 2024) attempts to distill the chain-of-thought into System 1 but fails to observe improvements when intermediate steps are omitted. (Deng et al., 2024b) proposes internalizing reasoning steps within the hidden states of models, while several implicitbased approaches(Deng et al., 2024a; Hao et al., 2024; Cheng and Durme, 2024) aim to compress token-wise generation by transitioning from language space to hidden space. Other studies focus on skipping intermediate reasoning steps (Liu et al., 2024b) or using summarization techniques to generate shorter reasoning chains (Kang et al., 2024). Additionally, (Chen et al., 2024) addresses the overthinking issue in QwQ (Team, 2024b) and employs SimPO (Meng et al., 2024) for optimization. Kimi K1.5 (Team et al., 2025) proposes merging longCoT models with short-CoT models in trainingfree manner. O1-Pruner (Luo et al., 2025a) adopts reinforcement learning to shorten responses."
        },
        {
            "title": "3 Method",
            "content": "In this section, we provide an in-depth discussion of our method. Section 3.1 introduces simple yet effective approach that enables single tuning process to generate models with CoT with different lengths. This stage also serves as an initial step for subsequent refinements. Next, in Section 3.2, we explore multiple scenarios in which we can apply CoT-Valve to construct the dataset MixChain. In Section 3.3, we propose several advanced methods that take advantage of long-to-short datasets to improve precision and control over the generated reasoning paths in compressible fine-tuning. 3.1 Length-Compressible CoT Tuning Our primary objective is to achieve new way to control the length of reasoning paths after training reasoning model. Existing approaches, such as prompt-based control, explicitly define sequence length in the prompt (Han et al., 2024) or utilize summary tokens (Ding et al., 2024) for guidance. However, these methods offer only limited control over the length of CoT generated. For instance, requesting sequence of less than 20 tokens may result in the model generating over 350 tokens (see Table 12 in the Appendix), and these methods struggle to produce answers with very short lengths. To address these limitations, we introduce CoT-Valve for training one model but can adjust the length of reasoning paths. Consider reasoning model defined by the parameter θ. For given question in the dataset D, the probability of generating an answer and its reasoning thoughts {ti}n i=1 given the question can be described by: (a t1, . . . , tn, q; θ) (cid:89) i=1 (ti t<i, q; θ) (1) where {ti}n i=1 might include errors or unnecessary details. With short synthesized or human-annotated explanations {ti}m i=1 with < n, the training objective is to adjust the parameter in such way that the chain is shortened while still yielding the correct answer: max θ E(q,a)Dp (a t1, . . . , tm, q; θ + θ) (cid:89) i=1 (ti t<i, q; θ + θ) (2) and θ denotes the change in the parameter space that steers the model towards generating more concise chain. Since the model, with and without θ, outputs the same final answer, θ can be interpreted as task vector (Ilharco et al., 2022). The task here is to control the length of the CoT, provided that the only difference in the training set lies in intermediate reasoning steps {ti}n i=1. Those reasoning paths are different in length but ultimately lead to the same final answer. Thus, we can control the task vector to achieve the goal of adjusting the length of CoT. θ is designed within parameter-efficient space, functioning as an external branch for inference that incurs minimal overhead. Controlling this external branch enables the manipulation of the length of the reasoning path. Task Arithmetic: Interpolation and Extrapolation of θ. To manipulate this update within the parameter space, we can control the magnitude of θ as an arithmetic operation. We use two primary operations on θ here: interpolation and extrapolation. Let α denote the magnitude of θ for LoRA. 3 Figure 2: Illustration of CoT-Valve. In Stage 1, we first determine θ from distilling or post-training. Then, the trained θ is utilized to construct the MixChain dataset. Using this dataset, we can then apply two enhanced training methods to achieve more precise control over reasoning paths, or to shorten the reasoning paths as needed. When α falls within the range of (0,1), the model smoothly transitions between longer and shorter reasoning paths, similar to weight interpolation between two models (Frankle et al., 2020; Team et al., 2025). When α > 1, extrapolation is introduced, further shortening the reasoning path beyond what was observed during training. This enables an exploration of the minimal reasoning length required to arrive at given answer. Thus, by adjusting α at inference, we can modulate the models behavior, with each value of α corresponding to different CoT lengths. Application Unlike prompt-based approaches that can only regulate the overall length of the reasoning process using prompt words, θ provides finer granularity control. θ is served in the external parameter space. This allows for greater flexibility in adjusting the reasoning trajectory. Specifically, it facilitates the selective retention of longchain reasoning in certain thoughts while applying stronger compression to simpler reasoning segments. As result, reductions in chain length can be localized to specific portions of the inference process rather than being uniformly applied across the entire reasoning path. We remain the design of this segment selection in future work. 3.2 Construct the MixChain Dataset crucial thing for the above process is the construction of the training dataset, especially the reasoning chain {ti}n i=1. To have reasoning chains with different lengths, previous approaches rely on multiple rounds of sampling, selecting reasoning paths under different random seeds, or using some handcrafted way to remove parts of the answer (Chen et al., 2024). We introduce MixChain, dataset inherently generated by our method that contains reasoning paths of varying lengths. This dataset is structured such that each question is associated with multiple reasoning paths, with lengths progressively decreasing from long to short. By simply adjusting the parameter α, our approach avoids the need for repeated sampling and achieves this diverse set of reasoning paths. In contrast to multi-sampling techniques, MixChain enables more reliable and consistent generation of shorter reasoning paths while simultaneously capturing spectrum of reasoning lengths. To construct MixChain, we consider two possible scenarios: solutions available, If well-annotated dataset with humansuch is labeled as GSM8K (Cobbe et al., 2021b) or PRM800k (Lightman et al., 2024), it can be leveraged to fine-tune the model for generating shorter reasoning chains as cold start (θ1 θ1 and θ2 θ2 in Figure 2). In the absence of dataset containing explicit reasoning paths, or when only final answers are available without full explanations, training solely on final answers is unlikely to enable the model to generate reasoning steps. To address this limitation, we propose an alternative method for constructing MixChain. Specifically, we leverage an existing base LLM (e.g., LLaMA-3.1-8B or Qwen32B-Instruct) as θ1 and use its corresponding reasoning model (e.g., DeepSeek-R1-DistillLlama-8B or QwQ-Preview) to derive θ. 4 The parameter update between these models serves as form of linear interpolation, enabling the transition from θ1 to θ2. This transition is then used to construct the dataset, as illustrated in Figure 2, where the parameter shift is represented by θ1 θ2. 3.3 Improved Tuning for CoT-Valve In this section, we present two enhanced variants of CoT-Valve: one aimed at achieving improved controllability and the other focused on optimizing the compression ratio of the reasoning paths. More Precise CoT-Valve Paradigm: CoTValve++. In the previously proposed CoT-Valve framework, the training process only constrained θ to satisfy the final objective with α = 1. However, during inference, we expect all positions along this direction to exhibit reasoning trajectories of varying lengths. This leads to the inconsistency between training and inference. With MixChain, we can explicitly incorporate this requirement during training by introducing an additional constraint, ensuring that the model can adapt to reasoning chains of different lengths across all positions in this direction. For each training sample, in addition to the question, answer, and solution, we have introduced normalized term β, which represents the factor for the length of the reasoning path. Under this dataset, our training objective is modified to find parameter update θ such that it satisfies: E(q,a)Dp (cid:16) max θ t<m, q; θ + βθ (cid:17) (cid:89) i=1 p(tit<i, q; θ + βθ ) (3) Where is the Mixchain dataset. Each sample consists of the question q, the answer a, the solution {ti}m i=1 and β, where β is calculated as: β = 1 mmin mmax mmin (4) Here, mmin and mmax is the length of the shortest solution and longest solution for this question. Based on synthetic samples, we introduce additional constraints that enable us to better identify the updated parameter θ , facilitating more precise compressibility and controllability. facilitates progressive chain-length compression strategy. This approach is similar to iterative pruning in model compression (Molchanov et al., 2016). In this process, the model is trained with shorter reasoning path from the dataset at each iteration, rather than training directly with the shortest reasoning CoT. This gradual compression method allows the model to progressively reduce the length of its reasoning paths."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Models. We evaluate our method under several models: QwQ-32B-Preview (Team, 2024b), DeepSeek-R1-Distill-Llama-8B (DeepSeek-AI, 2025), LLaMA-3.1-8B (Dubey et al., 2024), LLaMA-3.2-1B (Dubey et al., 2024) and Qwen32B-Instruct (Team, 2024a) with LIMO (Ye et al., 2025). We tested different scenarios for CoT-Valve: (Long to Short CoT) For QwQ-32B-Preview (QwQ for abbreviation) and DeepSeek-R1Distill-Llama-8B (R1-Distill), we used our method to control and compress the length of the reasoning chain. (Short to Long CoT) For LLaMA-3.1-8B and LLaMA-3.2-1B-Instruct, we applied our method to distill reasoning abilities from QwQ-32B-Preview and incorporated CoTValve in the distillation process. (Short-Long-Short CoT) We tested another setting to first post-train short-CoT LLM, Qwen-2.5-32B-Instruct (Team, 2024a), to generate Long CoT and then compress it to Short CoT. CoT-Valve can be applied in both two stages. Metrics. We report both accuracy and the number of tokens in the answer for each experiment. Given the trade-off between reasoning path length, model size, and performance, we use new metric, Accuracy per Computation Unit(ACU), to better capture this balance and evaluate model efficiency. It is defined as: ACU = Accuracy #Params #Tokens (5) Progressive Chain Compression: CoT-Valve+P. The structure of MixChain, which features progressively shorter reasoning paths for each question, Since the ACU value typically falls within the range of 105 to 102, we report it in units of 102 for improved readability. (a) GSM8K, QwQ-32B-Preview (b) GSM8K, Llama-3.2-1B-Instruct (c) AIME, Qwen2.5-32B-I w/ LIMO Figure 3: Token length and accuracy for different methods, datasets and reasoning models. Points connected by curves in (a) and (b) represent results from one model. Training and Evaluation. For training the model, we use LoRA (Hu et al., 2022) in most of our experiments, except in the experiment for LIMO on Qwen-2.5-32B-Instruct we use full parameter fine-tuning. We also show the results using DoRA (Liu et al., 2024a) in the Appendix. The hyper-parameters for each experiment are shown in Appendix A. We select two math datasets to evaluate the performance, for one easy math dataset, GSM8K (Cobbe et al., 2021b) and one hard math dataset, AIME24. 4.2 Datasets We find in our experiments that the quality of the solution is important to the performance, even if all the human-annotated solutions or synthesized solutions reach the final answer. In our experiments, we use the question from the train set of GSM8K, the math split of PRM800K or the question from LIMO, and we employ three types of datasets with those questions in our experiments: Ground-truth Dataset: The dataset provides human-annotated or model-synthesized solution. We use this as the cold start. MixChain from cold-start (MixChain-C): After taking the ground-truth dataset to train the model, we can get the first model to generate solutions from short to long. Then we use it to generate the dataset. MixChain from zero-shot (MixChain-Z): We employ CoT-Valve between reasoning model (θ2) and base LLM (θ1) to generate the solutions. Method Accuracy #Token ACU Llama-3.3-70B-Instruct Llama-3.1-405B-Instruct Qwen2.5-32B-Instruct Qwen2.5-Math-72B-Instruct QwQ-32B-Preview Prompt (Han et al., 2024) Prompt (Ding et al., 2024) 92.6 95.6 93.1 95.8 95.1 93.6 95.5 In-domain Train Set: GSM8K CoT-Valve - Ground-Truth CoT-Valve++ - MixChain-C CoT-Valve+P - MixChain-Z CoT-Valve+P - MixChain-Z 94.0 94.4 96.1 94.9 Out-of-Domain Train Set: PRM12K Overthink(Chen et al., 2024) - SFT Overthink(Chen et al., 2024) - SimPO O1-Pruner(Luo et al., 2025a) - SFT O1-Pruner(Luo et al., 2025a) CoT-Valve+P - MixChain-Z 94.8 94.8 95.7 96.5 95.4 235.4 186.7 269.3 312.1 741. 355.5 617.7 352.8 276.3 317.1 225.5 749.5 326.2 717 534 288.5 0.56 0.13 1.09 0.43 0.40 0.82 0.48 0.83 1.07 0.95 1. 0.40 0.91 0.42 0.56 1.03 Table 1: Results of QwQ-32B-Preview on GSM8K. Values of ACU are scaled by 102 for readability. We list the dataset we use after the method name. 4.3 From Long-CoT to Short-CoT. Controllable Results. We illustrate the result in Figure 3a. First, using ground-truth samples as cold start, we develop model capable of generating reasoning paths of various lengths, as demonstrated in CoT-Valve in Figure 3a. CoT-Valve already matches the performance of prompt-based control but can generate shorter reasoning chains. We then extrapolate θ to produce even shorter reasoning paths. Then, building on MixChain-C from this first model, we conduct further training by CoTValve++. CoT-Valve++ substantially surpasses the baseline and shows greater generalization capabilities in cases of extrapolation. For each dataset, we filter out all the solutions with incorrect answers. We show the statistics of the dataset in Table 9 in the Appendix. Compression Results. We evaluated our method against previous chain compression approaches, with the results detailed in Table 1, Table 2, and 6 Method AIME24 #Token ACU Method Accuracy #Tokens ACU Qwen2.5-32B-Instruct Qwen2.5-Math-72B-Instruct Gemini-Flash-Thinking (Team et al., 2023) 4/30 7/30 15/30 1794.2 1204.5 10810.5 0.023 0.061 - QwQ-32B-Preview.Train set: GSM8K QwQ-32B-Preview Prompt (Han et al., 2024) Prompt (Ding et al., 2024) Overthink (Chen et al., 2024) CoT-Valve - GSM8K CoT-Valve++ - MixChain-C CoT-Valve+P - MixChain-Z 14/30 13/30 13/30 13/30 14/30 13/30 13/30 6827.3 6102.5 5562.3 5154.5 5975.0 5360.5 4629. Qwen-32B-Instruct. Train set: LIMO Qwen-32B-LIMO CoT-Valve SFT - MixChain - Solution 1 CoT-Valve - MixChain - Solution 1 15/30 11/30 13/30 15/30 10498.2 6365.2 5368.0 8174.8 0.021 0.022 0.024 0.026 0.024 0.025 0. 0.015 0.018 0.025 0.019 LLaMA-3.2-1B-Instruct(8-shot) LLaMA-3.2-1B-Instruct(0-shot) SFT-Full Finetune - GSM8k SFT - GSM8k Prompt SFT - QwQ Distill CoT-Valve - QwQ Distill CoT-Valve+P - MixChain-Z SFT - MixChain-Z - Solution 1 CoT-Valve - MixChain-Z - Solution 1 45.9 45.9 46.1 43.8 46. 52.7 55.5 55.8 57.0 58.9 104.3 199.8 139.4 137.7 209.9 759.3 267.0 291.0 288.4 275.4 44.008 22.973 33.070 31.808 22. 6.941 20.786 19.175 19.764 21.387 Table 4: Results on LLaMA-3-2-1B-Instruct. We report the result of Flexible Match here. QwQ Distill means we use QwQ to synthesize the solution and distill it. Table 2: Results of QwQ-32B-Preview and Qwen-32BInstruct w/ LIMO on AIME 24. Model GSM8k AIME Acc #Token Acc # Token Llama-3.1-8B (0-shot) R1-Distill-Llama-8B 15.7 87. CoT-Valve 87.3 CoT-Valve+P - MixChain-Z 84.0 915.0 1636.6 1315.2 755.2 0/30 14/30 6/30 11/30 1517.6 12359. 7410.5 9039.0 Table 3: Result of DeepSeek-R1-Distill-Llama-8B. Method Accuracy #Tokens ACU LLaMA-3.1-8B (8-shot) LLaMA-3.1-8B (0-shot) SFT-LoRA - GSM8k SFT-LoRA - QwQ Distill CoT-Valve - QwQ Distill CoT-Valve+P - MixChain-Z CoT-Valve + MixChain-Z - Solution 1 56.9 15.7 59.0 76.3 77.5 77.1 75.7 282.1 915. 2.521 0.214 191.9 3.843 644.8 569.8 371.2 264.1 1.479 1.700 2.596 3.583 Table 5: Result on LLaMA-3.1-8B. We report the result of Strict Match here. Table 3. For GSM8K, we adhered to the baseline setup to train with PRM12K. Utilizing progressive compression, our method surpassed the baseline by producing shorter reasoning paths and improved performance. We also report experimental results on AIME, where the model was trained using MixChain-Z derived from GSM8K. To minimize the impact of randomness on performance, we employed greedy decoding in our AIME experiments. Compared to the baseline (Chen et al., 2024), our method reduced the token count from 5155 to 4630 while maintaining the same accuracy, despite being trained on an easier dataset. 4.4 From Short-CoT to Long-CoT & Short-Long-Short CoT Our method can also be applied if short-CoT model is distilled or post-trained to be Long-CoT model. The results are shown in Figure 3b, Table 4 and Table 5. We found that CoT-Valve can also effectively control the length of the chains in this setting. Notably, we observed that shorter chains could achieve higher accuracy on GSM8K. Moreover, if the model is trained using the MixChain-Z dataset, the results are significantly better, whether using CoT-Valve (55.5 to 58.9) or just simply SFT (52.7 to 57.0). Additionally, after training longchain model, we can employ the MixChain dataset to reduce the length of its reasoning chains further. As illustrated in Figure 3c, the results suggest that initially training the chains to be long and subsequently compressing them to be shorter (Results with Long-to-Short) can yield better performance than directly using CoT-Valve in the short-to-long stage (Results with Short-to-Long). This demonstrates significant potential for compressing the reasoning chains. We can also surpass the result of Gemini-Flash-Thinking, with the same accuracy but fewer tokens (10810.5 v.s. 8174.8) Training dynamics does not have the same effect as CoT-Valve. We also explore whether intermediate training steps can achieve similar effects. As depicted in Figure 3c, during the early training phases, the length of the CoT increases but does not correspond with the same rapid improvement in performance. As training progresses, the token length begins to decrease while performance improves. CoT-Valve exhibits distinct pattern, smoothly bridging the gap between the length of CoT and performance. 4.5 Observations Based on the results from LLaMA-3.1-8B, LLaMA-3.2-1.5B, QwQ, DeepSeek-R1-Distill7 Solution Solution Length Accuracy #Token Solution Used #Epoch #Samples Accuracy #Tokens ACU Ground-Truth (Solution 0) Solution 1 Solution 2 Solution 3 Solution 116.0 279.6 310.7 386.7 497.2 43.8 57.0 55.1 56.5 52.5 139.4 288.4 330.0 414.6 558.3 Table 6: Train LLaMA-3.2-1B-Instruct with solutions in MixChain-Z of different lengths on GSM8K. - 4 4+3 4+3+2 4+3+2+1 4+3+2+1+ 0 - 1 1 1 1 1 5 - 6.8k 13.7k 20.5k 27.4k 34.2k 37.4k 95.07 95.68 94.84 94.84 96.13 94.92 92.19 741.1 597.3 458.4 339.9 317.1 225. 250.5 0.40 0.50 0.65 0.87 0.95 1.32 1.15 Llama-8B and Qwen2.5-32B-Instruct with LIMO, we summarize the following observations: Table 7: Ablation of Progressive Compression on QwQ. Here, solution 0 is the human-annotated solution from the original dataset. Longer reasoning chains are not always the best on simple datasets. Across nearly all models, we find that those directly trained on long CoT data typically do not show the best performance. These models often underperform compared to those generated through CoT-Valve, which results in shorter but more accurate reasoning chains. This trend is particularly pronounced in smaller models. For instance, in the LLaMA-3.2-1B model, training on QwQ synthesized data yields an accuracy of 52.69 with 759.3 tokens. However, using CoT-Valve, we can achieve an accuracy of 55.50 with only 267.0 tokens. However, we do not observe this phenomenon in more complex datasets, indicating that while the reasoning model may be redundant for simple datasets, it still requires test-time scaling to effectively handle complex datasets. Some reasoning chains are difficult for the model to learn, especially for small LLMs. We fine-tuned LLaMA-3.2-1B-Instruct using only one solution from MixChain, where all solutions lead to the same final answer but involve different intermediate reasoning steps. The results, presented in Table 6, indicate that neither the shortest nor the longest chains are optimal for learning. Instead, the model most effectively learns from moderately short chains, achieving the highest accuracy while maintaining relatively low token count. This phenomenon is particularly evident in smaller models, but it is not observed in larger models. We believe this could be beneficial for the distillation of CoT in small LLMs. 4.6 Analysis Ablation on Progressive Compression. Table 7 demonstrates the effect of progressive compression. 8 Method QwQ-32B-Preview Llama-3.2-1B-I #Token Acc #Token Acc Prompt (Shortest) Ours (Best) Ours (Shortest) 93.6 94.4 87.5 355.5 276.3 133. 52.5 55.5 50.4 621.0 267.0 247.0 Table 8: CoT-Valve can achieve shorter chains than prompts with better performance. We compare two settings: training directly with the ground-truth solution for five epochs and applying progressive compression for five epochs in total, with the final epoch using the ground-truth data. Our results show that progressive compression significantly improves the performance of short CoT (from 92.19 to 94.92). For each turn, progressive compression gradually reduces the token number while maintaining accuracy. CoT-Valve achieves shorter chains compared to prompt control We also present in Table 8 the shortest chain achieved by our method and compare these with those obtained using prompt control. Our method outperforms prompt control methods at shorter chain lengths. Additionally, we explored the limits of chain length for both methods and found that our approach can generate substantially shorter chains than what can be achieved through prompt control."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose method that enables model to generate reasoning chains of varying lengths instead of the prompt control. Based on this approach, we construct dataset containing both long and short reasoning chains to further enhance controllability and compression efficiency. Experimental results demonstrate the effectiveness of our method in dynamic reasoning chain control and the compression of CoT. Future research can further explore finer-grained control strategies to improve reasoning efficiency and model controllability."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. 2024. Phi-4 technical report. arXiv preprint arXiv:2412.08905. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. 2024. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187. Jeffrey Cheng and Benjamin Van Durme. 2024. Compressed chain of thought: Efficient reasoning through dense representations. Preprint, arXiv:2412.13171. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021a. Training verifiers to solve math word problems. ArXiv, abs/2110.14168. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021b. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Yuntian Deng, Yejin Choi, and Stuart Shieber. 2024a. From explicit cot to implicit cot: Learning to internalize cot step by step. Preprint, arXiv:2405.14838. Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. 2024b. Implicit chain of thought reasoning via knowledge distillation. Mengru Ding, Hanmeng Liu, Zhizhang Fu, Jian Song, Wenbo Xie, and Yue Zhang. 2024. Break the chain: Large language models can be shortcut reasoners. arXiv preprint arXiv:2406.06580. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. 2020. Linear mode connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning, pages 32593269. PMLR. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025b. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519. Tingxu Han, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen, and Zhenting Wang. 2024. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024. Training large language models to reason in continuous latent space. Preprint, arXiv:2412.06769. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models. Preprint, arXiv:2203.15556. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. arXiv preprint 2024. Openai o1 system card. arXiv:2412.16720. Brihi Joshi, Ziyi Liu, Sahana Ramnath, Aaron Chan, Zhewei Tong, Shaoliang Nie, Qifan Wang, Yejin Choi, and Xiang Ren. 2023. Are machine rationales (not) useful to humans? measuring and improving human utility of free-text rationales. arXiv preprint arXiv:2305.07095. Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou. 2024. C3ot: Generating shorter chain-of-thought Preprint, without compromising effectiveness. arXiv:2412.11664. Levente Kocsis and Csaba Szepesvari. 2006. Bandit based monte-carlo planning. In European Conference on Machine Learning. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025a. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. Preprint, arXiv:2501.04519. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. 2024a. Dora: Weightdecomposed low-rank adaptation. In ICML. Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang. 2024b. Can language models learn to skip steps? In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. 2025a. O1-pruner: Lengthharmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Lei Meng, Jiao Sun, and Abhinav Rastogi. 2025b. Improve mathematical reasoning in language models with automated process supervision. Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, and Saining Xie. 2025. Inference-time scaling for diffusion models beyond scaling denoising steps. Preprint, arXiv:2501.09732. Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. Simpo: Simple preference optimization with In Advances in Neural Inreference-free reward. formation Processing Systems (NeurIPS). Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. 2016. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Ouyang Long, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. Webgpt: Browserassisted question-answering with human feedback. ArXiv, abs/2112.09332. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. 2024. Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models. Preprint, arXiv:2403.16999. Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. 2024. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie family of Millican, et al. 2023. Gemini: highly capable multimodal models. arXiv preprint arXiv:2312.11805. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Qwen Team. 2024a. Qwen2.5: party of foundation models. Qwen Team. 2024b. Qwq: Reflect deeply on the boundaries of the unknown. Jean-Francois Ton, Muhammad Faaiz Taufiq, and Yang Liu. 2024. Understanding chain-of-thought Preprint, in llms through information theory. arXiv:2411.11984. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2024. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, Bangkok, Thailand. Association for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. In Thirty-seventh Conference on Neural Information Processing Systems. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387. Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. 2024. Distilling system 2 into system 1. ArXiv, abs/2407.06023. Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. 2024. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. Preprint, arXiv:2406.07394. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2025. The lessons of developing process reward models in mathematical reasoning. Preprint, arXiv:2501.07301."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Evaluation Metric. For experiments on LLaMA, we use lm-evalharness1 to evaluate the model performance. For LLaMA-3.1-8B, we report the strict matching metric due to observed repetition in the models responses, which causes the flexible match to extract incorrect numerical values. For LLaMA-3.21B-Instruct, we report results using the flexible match metric. For QwQ-32B-Preview, DeepSeekR1-Distill-Llama-8B and Qwen-2.5B-LIMO, we first extract the result enclosed within boxed{}. If no such boxed answer is found, we default to using the last digit in the response as the final answer. A.2 Training Setting. LLaMA-3.1-8B The model is trained using eight A5000 24GB GPUs. We set the batch size to 64 and the peak learning rate to 4e-5, following cosine decay schedule. weight decay of 0.01 is applied. For the progressive chain compression experiment, we train the model for two epochs with each type of solution. For all other experiments, we train for maximum of eight epochs. For LoRA, the rank is set to 32, and the lora_alpha for training is set to 64. During inference, the maximum number of tokens is set to 2048. LLaMA-3.2-1B-Instruct The model is trained using 8 A5000 24GB GPUs. We set the batch size to 8 for the CoT-Valve experiment and 64 for all other experiments. The peak learning rate is 4e-5, following cosine decay schedule, except for the SFT - GSM8K experiment, where the peak learning rate is 1e-5. weight decay of 0.01 is applied. For the CoT-Valve and SFT-Full Finetune - GSM8k experiment, we train for maximum of four and six epochs, respectively. For the progressive chain compression experiment, we train the model for two epochs with each type of solution. For all other experiments, training is conducted for up to 8 epochs. For LoRA, the rank is set to 32, and the lora_alpha for training is set to 64. During inference, the maximum number of tokens is set to 2048. QwQ-32B-Preview. The model is trained on two H100-80G GPUs. We set the batch size to 64 and trained for maximum of five epochs. The learning rate is 1e-5, with weight decay of 0.01 applied 1https://github.com/EleutherAI/lm-evaluation-harness during training. For LoRA, the rank is set to 2, and the lora_alpha for training is set to 8. During inference, we set the maximum token to be 4192 for GSM8K and the maximum token as 8192 for AIME correspondingly. DeepSeek-R1-Distill-Llama-8B. Our experiment on DeepSeek-R1-Distill-Llama-8B2 is conducted using the MixChain-zero-shot-GSM8K dataset. The batch size is set to 128, and training is performed for maximum of five epochs. To ensure that the inference process successfully generates the final answer, we set the maximum token limit to 30K. Qwen2.5-32B-LIMO. We fine-tuned Qwen32B-Instruct using LIMO, training on four H100 GPUs for 10 epochs with batch size of 4 and maximum sequence length of 16K. The learning rate was set to 5e-6. We define Qwen-32B-Instruct as θ0 and the trained model as θ1, treating the update direction between them as θ. By adjusting α, we generated the MixChain-C-LIMO dataset, which includes two solutions: solution 1 (α=0.8) and solution 0 (α=0.6). Based on this, we further trained θ2 for 5 epochs with batch size of 32, learning rate of 5e-6, and weight decay of 0.01, obtaining the results of MixChain-Solution 0 in Table 2. This model can be further refined through CoT-Valve (Results: CoTValve + MixChain - Solution 0). Unlike previous experiments, we applied full fine-tuning instead of LoRA. The maximum generated sequence length in this experiment was 15K. A.3 Dataset Explanation As detailed in Section 4.2, we constructed two types of datasets: MixChain-C and MixChain-Z. The statistics for the datasets are shown in 9. For these datasets, we select α values ranging from [0.6, 0.8] for LIMO and [0.2, 0.4, 0.6, 0.8] for other datasets, ensuring all incorrect responses are excluded. For MixChain-Z, while the training transition from θ1 to θ2 remains black box, we can still identify numerous model pairs such as Qwen-32BInstruct QwQ-32B-Preview, and LLaMA-3.18B R1-Distill-Llama-8B, as documented in the technical report. We find that the performance of the base model significantly influences the quality of the dataset. 2https://huggingface.co/deepseek-ai/DeepSeek-R1Distill-Llama-8B 11 Dataset Solution Index #Samples #Avg Token Modules GSM8K #Tokens #Params ACU Ground-Truth MixChain-C MixChain-Z Ground-Truth MixChain-Z GSM8K 1 1 0 (Ground-Truth) 1 2 3 PRM12K 1 0 (Ground-Truth) 1 2 3 4 LIMO Ground-Truth MixChain-C 1 1 7473 22419 6863 12000 8841 817 474 564 121.8 294.8 116.0 279.6 310.7 386.7 497. 223.1 172.3 583.2 613.7 739.3 1003.2 6984.1 2994.7 4890.6 Table 9: Dataset Statistic. Here we use the tokenizer from QwQ-32B-Preview to count the number of tokens. α 0 0. 0.25 0.5 0.75 1.0 # Tokens Accuracy 199.8 45. 219.4 47.5 233.4 50.2 257.7 57.1 466.3 55.0 772.7 54.5 Table 10: Results of LLaMA-3.2-1B-Instruct trained with DoRA using different α values for interpolation."
        },
        {
            "title": "B More Analysis",
            "content": "Experiments on DoRA. In addition to LoRA, we also train LLaMA-3.2-1B using DoRA (Liu et al., 2024a) and control the magnitude of θ by adjusting the α for DoRA. The model is trained on QwQ synthesized data for maximum of five epochs. We set the batch size to 8 and the peak learning rate to 4e-5, following cosine decay schedule. weight decay of 0.01 is applied. For DoRA, the rank is set to 32, and the lora_alpha for training is set to 64. As shown in Table 10, the chain length increases with the α value, demonstrating the effectiveness of interpolating θ for DoRA. Furthermore, similar to our observations with LoRA, the best result is not obtained by directly training the model on long CoT data. Specifically, training on QwQ synthesized data (α = 1.0) achieves an accuracy of 54.5 with 772.7 tokens, whereas the best model obtained via CoT-Valve (α=0.5) achieves an accuracy of 55.72 with only 257.7 tokens. Attention has less effect on the length of the reasoning path than MLP. We experimented - K+V Attention MLP All Linear 95.1 95.0 95.2 95.2 94.2 93.5 92.4 741.1 687.7 621.4 484.2 284.2 221.8 227. - 0.005% 0.004% 0.004% 0.013% 0.038% 0.051% 0.40 0.43 0.48 0.61 1.04 1.32 1. Table 11: LoRA on Different Modules. with applying LoRA fine-tuning to only specific model components in QwQ. Table 11 shows that fine-tuning only the query, key, or value projection had less effect than other linear projections on shortening the reasoning chain. This suggests that attention computation may have less impact on controlling chain length, whereas the MLP layers and the final projection in attention play more significant role. We find this to be an intriguing phenomenon, and further investigation into its underlying causes could provide deeper insights into optimizing reasoning chain compression. Prompt Control. We evaluate the length of CoT under constraint in prompts. Table 12 presents the number of tokens generated when using various prompts across two models. For these two models, the prompts used here are: Prompt Template for QwQ-32B-Preview System: You are helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step. User: Generate the solution in less than <token_count> tokens. <Question> Prompt Template for LLaMA-3.2-1B-Instruct System: Cutting Knowledge Date: December 2023 Today Date: 01 Jan 2025 User: Given the following problem, reason and give final answer to the problem using less than <token_count> tokens. <Question> From the results, we observe that while these prompts provide control over the length, there remains significant discrepancy between the generated token count and the intended target. 12 Generated Answer for the Same Question Question: Amanda had 10 notebooks. This week, she ordered 6 more and then lost 2. How many notebooks does Amanda have now? Ground Truth Solution - 49 Tokens: This week, Amanda added 6 notebooks - 2 notebooks = 6-2=44 notebooks. Now, Amanda has 10 notebooks + 4 notebooks = 10+4=1414 notebooks. Extrapolation - 300 Tokens: So Amanda started with 10 notebooks. Then, she ordered 6 more. So, think need to add those 6 to the original 10. That should be 10 plus 6, which is 16. Okay, now she lost 2 notebooks. So, need to subtract those 2 from the 16. That should be 16 minus 2, which equals 14. So, Amanda has 14 notebooks now. Wait minute, let me double-check that. She had 10, ordered 6 more, so thats 10 + 6 = 16. Then lost 2, so 16 - 2 = 14. Yeah, that seems right.Alternatively, maybe can think of it differently. Maybe consider the net gain of notebooks. She ordered 6 and lost 2, so net gain is 6 - 2 = 4. Then, add that net gain to the original 10: 10 + 4 = 14. Same answer. Or, maybe another way: starting with 10, losing 2 would bring her to 8, and then ordering 6 more would bring her to 14. Again, the same result. think no matter how look at it, the answer is 14 notebooks. **Final Answer** boxed{14} QwQ-32B-Preview - 257 tokens: So Amanda started with 10 notebooks. Then, she ordered 6 more. So, need to add those 6 to the original 10. That should be 10 plus 6, which is 16. Okay, now she lost 2 notebooks. So, need to subtract those 2 from the 16 she had after ordering more. That means 16 minus 2, which equals 14. So, Amanda has 14 notebooks now. Wait minute, let me double-check that. She had 10, ordered 6 more, so thats 10 + 6 = 16. Then lost 2, so 16 - 2 = 14. Yeah, that seems right. Alternatively, maybe can think of it differently. Maybe combine the ordered and lost notebooks first. She ordered 6 and lost 2, so overall, she gained 6 - 2 = 4 notebooks. Then, add that to the original 10: 10 + 4 = 14. Same answer. think thats correct. Amanda has 14 notebooks now. **Final Answer** boxed{14} Interpolation - 207 Tokens: Lets break this down step by step. Amanda started with 10 notebooks. Then, she ordered 6 more. So, need to add those 6 to the original 10. That would be 10 plus 6, which equals 16 notebooks. Okay, so now she has 16 notebooks. But then, she lost 2. So, need to subtract those 2 from the 16. That would be 16 minus 2, which equals 14 notebooks. Therefore, Amanda now has 14 notebooks. Wait minute, let me double-check that. She started with 10, ordered 6 more, so thats 10 + 6 = 16. Then lost 2, so 16 - 2 = 14. Yeah, that seems right. think thats the answer: Amanda has 14 notebooks now. Interpolation - 133 Tokens: Lets solve this step-by-step: 1. **Initial number of notebooks**: Amanda started with 10 notebooks. 2. **Notebooks ordered**: This week, she ordered 6 more notebooks. - So, we add these 6 notebooks to her initial 10 notebooks. - 10 + 6 = 16 notebooks. 3. **Notebooks lost**: Then, Amanda lost 2 notebooks. - We need to subtract these 2 lost notebooks from the total she had after ordering more. - 16 - 2 = 14 notebooks. Therefore, Amanda now has 14 notebooks. Extrapolation - 103 Tokens: To find out how many notebooks Amanda has now, we can break the problem into steps: 1. Amanda initially had 10 notebooks. 2. This week, she ordered 6 more notebooks, so we add 6 to the initial amount: 10 + 6 = 16 notebooks. 3. Amanda then lost 2 notebooks, so we subtract 2 from the total: 16 - 2 = 14 notebooks. So, Amanda has 14 notebooks now. Figure 4: An example of the generated solution for the same question."
        },
        {
            "title": "C Examples",
            "content": "Here we show in Fig.4 an example of the generated CoT from short to long, and we also show two extrapolation cases to show the generalization ability of our method. Our method notably generates longer reasoning process compared to the original QwQ model, incorporating an extra reflection phase. During the chain shortening process, it reduces multiple rounds of reasoning and streamlines the language, ultimately enabling us to produce an answer with only 103 tokens through extrapolation. QwQ-32B-Preview Llama-3.2-1B Instruct Token in Prompt #Token Generated Token in Prompt #Token Generated 20 50 100 200 300 400 355 422 511 569 623 666 50 100 200 300 400 500 118 132 141 160 183 186 Table 12: Significant discrepancies exist between the conditions specified in the prompt and the number of generated tokens on GSM8k."
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}