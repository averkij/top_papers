{
    "paper_title": "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once",
    "authors": [
        "Zhuoshi Pan",
        "Qizhi Pei",
        "Yu Li",
        "Qiyao Sun",
        "Zinan Tang",
        "H. Vicky Zhao",
        "Conghui He",
        "Lijun Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on task-specific benchmarks, yet their evaluation methods remain constrained by isolated problem-solving paradigms. Existing benchmarks predominantly assess single-question reasoning through sequential testing, resulting critical limitations: (1) vulnerability to data contamination and less challenging (e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual creation of new questions with large human efforts, (2) failure to evaluate models under multi-context pressure, a key requirement for real-world deployment. To bridge this gap, we present REST (Reasoning Evaluation through Simultaneous Testing), a stress-testing framework that concurrently exposes LRMs to multiple problems simultaneously. Beyond basic reasoning, REST specifically evaluates several under-tested capabilities: contextual priority allocation, cross-problem interference resistance, and dynamic cognitive load management. Our evaluation reveals several striking findings: Even state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance degradation under stress testing. Crucially, REST demonstrates stronger discriminative power than existing benchmarks, revealing pronounced performance differences among models that exhibit similar, near-ceiling performance under single-question evaluations. Some key mechanistic insights emerge from our analysis: (1) the \"overthinking trap\" is a critical factor contributing to the performance degradation; (2) the models trained with \"long2short\" technique preserve more accuracy of their single-problem performance under REST, outperforming standard-trained counterparts. These results establish REST as a cost-efficient, future-proof evaluation paradigm that better reflects real-world reasoning demands while reducing reliance on continuous human annotation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 1 4 5 0 1 . 7 0 5 2 : r REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Zhuoshi Pan1,2,, Qizhi Pei2,3,, Yu Li2 Qiyao Sun2 Zinan Tang2 H. Vicky Zhao1, Conghui He2, Lijun Wu2, 1Tsinghua University, 2OpenDataLab, Shanghai Artificial Intelligence Laboratory, 3Renmin University of China Recent Large Reasoning Models (LRMs) have achieved remarkable progress on task-specific benchmarks, yet their evaluation methods remain constrained by isolated problem-solving paradigms. Existing benchmarks predominantly assess single-question reasoning through sequential testing, resulting critical limitations: (1) vulnerability to data contamination and less challenging (e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual creation of new questions with large human efforts, (2) failure to evaluate models under multicontext pressure, key requirement for real-world deployment. To bridge this gap, we present REST (Reasoning Evaluation through Simultaneous Testing), stress-testing framework that concurrently exposes LRMs to multiple problems simultaneously. Beyond basic reasoning, REST specifically evaluates several under-tested capabilities: contextual priority allocation, crossproblem interference resistance, and dynamic cognitive load management. Our evaluation across 34 advanced reasoning models on 7 reasoning benchmarks reveals several striking findings: Even state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance degradation under stress testing, challenging the prevailing assumption that LLMs are multi-problem solvers. Crucially, REST demonstrates stronger discriminative power than existing benchmarks, revealing pronounced performance differences among models that exhibit similar, near-ceiling performance under single-question evaluations. Some key mechanistic insights emerge from our analysis: (1) the overthinking trap is critical factor contributing to the performance degradation; (2) the models trained with long2short technique preserve more accuracy of their single-problem performance under REST, outperforming standard-trained counterparts. These results establish REST as cost-efficient, future-proof evaluation paradigm that better reflects real-world reasoning demands while reducing reliance on continuous human annotation. Date: July 15, 2025 Correspondence: H. Vicky Zhao, vzhao@tsinghua.edu.cn; Conghui He, heconghui@pjlab.org.cn; Lijun Wu, wulijun@pjlab.org.cn Code: https://github.com/opendatalab/REST"
        },
        {
            "title": "1 Introduction",
            "content": "Recent years have witnessed significant advancements in the reasoning capabilities of Large Reasoning Models (LRMs), which have demonstrated impressive performance across variety of reasoning tasks, including mathematical problem solving [57, 35, 42, 36], code generation [21, 39], and complex concept understanding [38]. Researchers have proposed various benchmarks [7, 17, 38, 22, 25] to evaluate the reasoning capabilities of LRMs. However, the rapid advancement of LRMs has exposed critical limitations in current evaluation paradigms. Most benchmarks rely on single-question testing, where *Corresponding Authors. Equal Contribution. Work during internship at Shanghai Artificial Intelligence Laboratory. 1 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Figure 1: Illustration of REST evaluation compared with single-question evaluation. REST concatenates multiple questions into single prompt, stress testing LRMs ability in handling increasing reasoning loads within single reasoning process. models process and answer questions in isolation. While this approach is effective for early model development, it now faces two fundamental challenges. First, many benchmarks, such as GSM8K [7] and MATH [17], are becoming less effective at distinguishing model performance, as current LRMs have achieved nearly saturated performance on them (e.g., DeepSeek-R1 achieves 97.0% on MATH500), the community is forced into costly cycle of benchmark obsolescence - continuously developing new, more difficult datasets (e.g., AIME24 [59]) while discarding still-valuable existing ones. This raises our first key question: Can we enhance the utility of current benchmarks by making them more challenging without complete replacement? Second, single-question evaluation fails to assess how models perform in real-world, multi-context scenarios where reasoning must occur across multiple, potentially interfering questions. For instance, in educational tutoring systems, an AI might need to simultaneously address students follow-up questions while correcting previous misconceptions; also, in technical support, it may process multiple user-reported issues within single context window. Though few prior studies [6, 54, 44] have investigated multi-question prompting, they only focus on simple tasks such as text classification [40] or commonsense QA [46], which are insufficient to assess the reasoning capabilities of current LRMs. This leads to our second critical question: How well can current LRMs handle multiple questions simultaneously, and what factors affect their performance in such settings? To answer the aforementioned research questions, we propose REST (Reasoning Evaluation through Simultaneous Testing), simple yet powerful method that repurposes existing benchmarks into more challenging variants. Specifically, REST transforms existing benchmarks by concatenating multiple questions into single instruction to evaluate these questions at once. Based on 7 representative reasoning benchmarks (e.g., GSM8K, MATH500, and AMC23), we reconstruct them into multiquestion format (see Fig. 1 for illustration). Through comprehensive evaluation under our REST of more than 30 state-of-the-art (SOTA) LRMS, we have obtained some valuable findings. Even SOTA LRMs like DeepSeek-R1 [13] exhibit significant performance degradation under REST, such as 29.1% accuracy drop on AIME24, revealing critical limitation in their reasoning robustness, challenging that LLMs are inherently multi-problem solvers [6, 54, 44]. Despite excelling in the single-question evaluation, several LRMs struggle to maintain their advantage under REST compared to their non-reasoning counterparts. REST provides enhanced discriminative power. While many LRMs exhibit similar performance on single-question evaluations, they display notable differences in accuracy under REST. Effectively revitalizes existing benchmarks, making them challenging again for top-tier models. 2 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Through further analysis, we identify key factor behind this performance drop: the overthinking phenomenon [45, 5, 53], where LRMs tend to generate unnecessarily redundant reasoning even for relatively simple problems. Besides, LRMs trained with Long2Short training [45, 1, 3], which encourages concise reasoning, perform better in REST, suggesting promising direction for developing multi-question-capable LRMs. These findings have immediate implications for both evaluation practices and model development. REST offers cost-effective, scalable alternative to constant benchmark replacement, while providing insights for building more robust reasoning systems capable of handling real-world, multi-context scenarios."
        },
        {
            "title": "2.1 Compositional Instruction Training and Evaluation",
            "content": "Some recent works use multi-instruction or problem combinations for training data augmentation [26, 36, 24]. MathFusion [36] fuses semantically similar problem pairs to construct more difficult one for enhanced mathematical problem solving. Mosaic-IT [24] concatenates randomly sampled instruction data with meta-instructions to reduce training cost and improve performance. To evaluate the ability of LLMs to follow compositional instructions simultaneously, several benchmarks have been proposed. Batch prompting [6], multi-problem prompting [54], and multi-task inference benchmark [44] investigate concatenating multiple independent tasks, primarily for inference efficiency on simple tasks like text classification. Compound-QA [19] focuses on increasing complexity within single instruction, requiring models to follow multiple constraints, compositional directives, or sequential steps. These methods, however, do not focus on the evaluation of complex reasoning tasks."
        },
        {
            "title": "2.2 Overthinking and Controlled Reasoning Length in LRMs",
            "content": "Excessive verbosity in the reasoning processes of LRMs can hinder their inference efficiency. The phenomenon of overthinking, where models generate excessively long and complex reasoning steps without proportional gains in accuracy, has been identified [45, 5]. In response, several long2short or controlled reasoning length methods emerge. prominent approach involves fine-tuning LRMs using reinforcement learning (RL) with penalties for excessive token generation [1, 3, 29], which effectively reduces the reasoning length while preserving or improving accuracy. Other strategies to promote conciseness include constrained prompting [33] and dynamic token budget allocation [14]. Importantly, the validation of these length-control techniques occur in single-problem evaluation settings. Our REST provides new dimension for assessing LRMs under multi-problem scenarios."
        },
        {
            "title": "2.3 Reasoning Benchmarks",
            "content": "The rigorous assessment of the reasoning ability of LLMs relies heavily on specialized benchmarks. In mathematical reasoning, GSM8K [8] and MATH [16] are most commonly used. As model capabilities surge, harder benchmarks like OlympiadBench [15], AIME [2, 27], and Omni-Math [12] have emerged, further pushing the boundaries of mathematical evaluation. Beyond mathematics, reasoning in other domains like code generation [22, 37, 23], cryptographic decryption [25], and scientific problem-solving [38], is also critical. Despite the increasing difficulty and diversity of these benchmarks, the rapid improvement of models means that their differentiating power can diminish when evaluated solely through standard single-instance problem solving. Our REST builds on these existing benchmarks and repurposes them into more challenge variants. 3 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once"
        },
        {
            "title": "3 REST: Reasoning Evaluation through Simultaneous Testing",
            "content": "Despite the significant progress in large reasoning models (LRMs), many existing benchmarks, such as MATH500 [17], have reached near-saturation performance levels, limiting their utility in distinguishing between the capabilities of increasingly powerful models. Our introduced REST aims to address this limitation, which is new evaluation protocol designed to systematically increase the cognitive load on LRMs by aggregating multiple questions into single prompt. This design enables finer-grained assessment of models ability to handle multi-step, continuous reasoning under stress. Below are the details of the benchmark reconstruction and evaluation under REST."
        },
        {
            "title": "3.1 REST Benchmark Reconstruction",
            "content": "Let the original benchmark be denoted as = {q1, q2, . . . qN}, where qi represents an individual question and is the total number of questions. In REST, we transform this benchmark into new prompt set by concatenating consecutive questions into each prompt. We refer to the parameter Z+ as the stress level, since larger imposes greater reasoning burden on the model. Formally, for each {1, 2, . . . , N}, we define the stress-level-s prompt ps as: ps = Compose(qi, qi+1, . . . , q[(i+s1) mod N]), {1, 2, . . . , N}. To ensure continuity and full coverage, we apply cyclic indexing when the end of the benchmark is reached. The function Compose() formats multiple questions into single prompt: Compose(q1, . . . , qs) = Q1 : {q1}, . . . , Qs : {qs}. Answer the above questions one by one. This transformation yields new prompt set = {ps N}, where each prompt contains questions. Importantly, Ps = N, matching the size of the original benchmark. However, unlike the original benchmark, every prompt ps in comprises consecutive questions, spanning from qi to q[(i+s1) mod N] rather than just single question qi. Moreover, each original question qi appears exactly times across all the prompts and exactly once in each of the possible positions within the concatenated prompts. This design mitigates positional biases and ensures comprehensive coverage across stress levels. 2, ..., ps 1, ps"
        },
        {
            "title": "3.2 REST Evaluation",
            "content": "Under REST, given LRM model , we evaluate it on each prompt ps (ps using function Extract(): ). From this response, we extract the individual predicted answers { ˆas to obtain an output response os i+1, . . . , ˆas = [(i+s1) mod N]} , ˆas { ˆas , ˆas i+1, . . . , ˆas [(i+s1) mod N]} = Extract(os ). To facilitate accurate extraction, we instruct LRMs to format their answers in task-specific ways. For Mathematical and GPQA problems, we instruct the model to put each answer within boxed{}, while for code generation tasks, answers must be wrapped in Python code blocks ```python ```. Notably, although more structured format method like JSON output could offer convenience for output parsing, we deliberately avoid imposing such strict formatting constraints, as prior studies [47, 43] indicate that such requirements will degrade performance. We employ both rule-based and LLMbased extraction methods: (a) The rule-based method uses regular expressions to extract answers from predefined markers (e.g., boxed{}); (b) The LLM-based method prompts model to retrieve the predicted answer from the response for each question. Further implementation details can be found 4 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once in Appendix C. Finally, we define the models accuracy at stress level by comparing the predicted answer ˆa and the ground truth answer a: Acc(P s) ="
        },
        {
            "title": "1\nN",
            "content": "N i=1 Acc(ps ) ="
        },
        {
            "title": "1\nN",
            "content": "N i=1 1 j=1 δ( ˆas [(i+j1) mod N], as [(i+j1) mod N]), where δ is the Kronecker delta function such that δ(i, j) = 1 if and only if = and 0 otherwise."
        },
        {
            "title": "4.1 Evaluation Setup",
            "content": "We evaluate total of 34 LRMs, spanning parameter size range from 1.5B to 671B. The temperature and top_p parameters are configured according to the corresponding official guidelines for each model. We set maximum output token length of 32K for reasoning models and 8K for non-reasoning models. In addition, we conduct experiments with an extended 128K token limit, but observe negligible performance differences, as shown in Tab. 5 of Appendix D. Our evaluation is based on OpenCompass* toolkit. To ensure consistency, we adopt the official prompt for each task, with minor format adjustment for multi-question responses: Answer the above questions one by one. Remember to put each answer within boxed{} (or ```python ``` for code generation). We select 7 representative benchmarks for evaluation, and in REST, we set different stress levels for each benchmark. Specifically, for relatively simple benchmarks like GSM8K, the stress levels are {1, 3, 6, 9, 12}; for medium-difficulty benchmarks, including MATH500 and AMC23, they are set as {1, 3, 5, 7, 9}; and for more challenging benchmarks, such as AIME24, AIME25, GPQA, and LiveCodeBench (v5), the stress levels are {1, 2, 3, 4, 5}. To ensure clarity and consistency in evaluation, we report the performance of REST as the average accuracy across stress levels greater than 1. Detailed accuracy statistics for each stress level are provided in Appendix A. For relatively small benchmarks like AIME24, AIME25, and AMC23, we conduct 8 sampling runs and report the average results to reduce variance."
        },
        {
            "title": "4.2 Evaluation Result",
            "content": "Tab. 1 and Tab. 2 present the performance of various LRMs on the original reasoning benchmarks (Single) and their transformed counterparts by REST (Stress). Our key findings from the experimental results can be summarized as follows: LRMs can handle multiple simple questions at once, but struggle with challenging ones. LRMs can address several relatively simple problems within single reasoning process. As shown in Tab. 1, on GSM8K, the accuracy drops under REST for DeepSeek-R1-Distill-Qwen-7B (R1-7B) and DeepSeek-R1Distill-Qwen-32B (R1-32B) are just 0.43% and 0.04%, respectively. However, this robustness diminishes significantly when the models face more challenging problems. In Tab. 1 and Tab. 2, even SOTA LRMs (e.g., DeepSeek-R1) encounter substantial difficulties when tasked with multiple Olympiad-level competition problems within single reasoning process. For instance, under REST, the accuracy of DeepSeek-R1 on AIME24 and AIME25 decreases by 29.17% and 31.58%, respectively, compared to the single-question setting. This performance gap is even more pronounced for smaller models, such as R17B, whose accuracy on AIME24 drops sharply from 54.17% to 16.53% under REST. Moreover, despite strong performance in the single-question setting, several LRMs fail to maintain their advantage under REST compared to their non-reasoning counterparts. For example, DeepSeek-R1-Distill-Qwen-1.5B achieves an accuracy of 83.40% on MATH500 in the single-question evaluation, surpassing the 73.00% *https://github.com/open-compass/opencompass 5 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Table 1: Evaluation results on various math benchmarks. The Stress column reports the average performance across four stress levels, as described in Sec. 4.1. Accuracy on other tasks are in Tab. 2. Detailed accuracy statistics for each stress level can be found in Tab. 6 of the Appendix. Models that perform best under Stress are highlighted in gray. Bench GSM8K MATH500 AMC23 AIME24 AIME Avg. Model Single Stress Single Stress Single Stress Single Stress Single Stress Single Stress DS-R1-Distill-Qwen-1.5B [13] DeepScaleR-1.5B [30] L1-Qwen-1.5B-Exact [1] L1-Qwen-1.5B-Max [1] Qwen2.5-Math-1.5B-Inst [58] DS-R1-Distill-Qwen-7B [13] DS-R1-Distill-LLaMA-8B [13] Efficient-R1-7B (α = 0.1) [3] Efficient-R1-7B (α = 0.2) [3] Nemotron-Nano-8B [4] AReaL-boba-RL-7B [31] Light-R1-7B-DS [55] OpenR1-Qwen-7B [10] OpenThinker2-7B [51] SimpleRL-Zoo-Qwen-7B [62] Open-Reasoner-Zero-7B [20] Marco-O1-7B [63] MathFusion-Qwen-7B [36] Eurus-2-7B-PRIME [9] Qwen2.5-Math-7B-Inst [57] Qwen2.5-7B-Inst [56] DS-R1-Distill-Qwen-32B [13] Qwen-QwQ-32B [52] AReaL-boba-SFT-32B [31] Light-R1-32B-DS [55] S1.1-32B [32] OpenThinker2-32B [51] SimpleRL-Zoo-Qwen-32B [62] Open-Reasoner-Zero-32B [20] Qwen2.5-32B-Inst [56] DeepSeek-R1 [13] O3-mini [34] O4-mini [34] Gemini-2.5-Flash-Thinking [48] 84.62 84.84 84.87 84.17 85.37 89.49 90.45 88.63 87.95 91.36 91.66 88.05 95.60 94.39 90.52 92.87 89.08 89.46 92.72 95.53 92.27 95.54 95.83 95.01 95.83 89.84 96.44 96.06 95.83 95.53 96.20 95.83 93.71 89.23 70.21 66.58 79.01 78.29 67.49 89.06 85.18 84.76 80.38 70.52 77.80 82.69 90.22 91.99 84.01 65.14 79.56 83.78 88.01 78.53 85. 95.50 95.78 94.75 94.79 61.10 95.17 93.49 91.80 93.77 96.16 93.85 93.07 91.28 83.40 87.60 84.00 83.40 73.00 93.00 89.80 90.00 88.20 94.40 95.00 93.20 92.20 93.80 77.80 83.00 72.40 74.00 81.40 83.60 77.60 94.60 96.20 95.00 95.60 90.40 96.20 83.20 92.00 82.20 97.00 95.00 90.00 97. 1.5B LRMs 62.50 42.47 76.25 59.77 71.25 72.07 77.50 73.23 53.94 57.50 7 8B LRMs 87.50 66.75 87.50 81.34 87.50 74.99 85.00 76.41 86.04 90.00 91.25 60.77 90.00 61.73 83.75 81.64 85.00 83.30 68.50 62.41 60.00 32.51 47.50 48.19 52.50 68.15 62.50 64.69 60.00 56.59 42.50 65.78 32B LRMs 94.75 95.00 97.50 96.25 90.00 95.00 67.50 83.75 60.00 88.97 92.49 88.92 83.66 53.85 90.10 78.90 82.90 73.39 API-based LRMs 92.09 86.62 82.40 69.92 93.75 90.00 96.25 97. 13.98 32.05 47.37 48.37 22.22 36.06 70.75 44.25 48.05 76.24 32.94 34.91 54.11 63.23 16.16 31.23 17.23 36.24 38.58 28.46 34.46 86.24 82.89 78.96 68.80 32.26 81.00 57.02 70.04 49.72 81.80 59.17 82.79 47.63 29.17 38.75 21.25 20.00 10.83 54.17 50.42 54.58 50.42 63.33 61.25 55.83 47.50 54.58 26.67 17.92 10.00 9.58 20.83 14.17 10. 72.92 78.75 77.50 77.50 55.83 68.33 27.20 46.67 20.00 81.66 79.16 73.33 76.67 4.97 12.82 12.62 15.13 6.17 16.53 31.23 21.45 22.12 43.55 21.43 16.63 26.77 34.50 7.55 6.13 4.35 7.89 10.84 6.40 7.02 52.51 54.79 45.79 41.26 24.42 53.01 16.80 31.65 9.61 52.49 34.07 49.69 26. 25.00 31.25 18.33 22.92 10.83 35.42 28.33 35.42 33.75 50.00 45.83 45.83 32.92 41.67 10.00 16.25 10.83 5.83 14.58 11.67 3.75 51.67 69.58 60.00 60.00 45.42 52.50 16.67 36.67 16.67 68.75 71.66 80.00 71.67 5.91 14.23 12.96 14.95 2.83 11.37 22.66 15.75 17.25 32.28 12.33 12.96 21.19 23.66 6.47 3.89 3.64 2.35 4.49 5.33 3. 33.83 41.53 33.55 33.80 19.13 38.20 8.87 23.63 6.73 37.17 20.63 41.42 16.54 56.94 63.74 55.94 57.60 47.51 71.92 69.30 71.23 69.06 77.82 77.00 74.58 70.39 73.89 54.70 54.01 45.96 46.27 54.41 52.99 45.22 81.90 87.07 85.00 85.04 74.30 81.69 58.13 70.98 54.88 87.47 86.25 86.66 86. 27.51 37.09 44.81 45.99 30.53 43.95 58.23 48.24 48.84 61.71 41.05 41.78 54.79 59.33 35.32 27.78 30.59 39.68 41.32 35.06 39.14 71.41 73.49 68.39 64.46 38.15 71.49 51.01 60.00 46.64 71.94 55.51 69.87 50.39 of the Qwen2.5-Math-1.5B-Instruct model. However, under REST, its accuracy drops sharply to 42.47%, falling significantly behind Qwen2.5-Math-1.5B-Instruct, which achieves 53.94%. REST enhances the discriminative power of existing benchmarks. By concatenating multiple problems into single prompt, REST significantly amplifies performance differences among models that exhibit comparable, near-ceiling accuracy in traditional single-question evaluations. For example, on MATH500, R1-7B and R1-32B achieve single-question accuracies of 93.0% and 94.6%, respectively, with R1-7B trailing by just 1.6%. However, when evaluated under REST, the accuracy of R1-7B plummets to 66.75%, while R1-32B maintains substantially higher 88.97%, revealing pronounced 22.22% performance gap. This contrast is similarly evident among models of the same size. For instance, 6 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once (a) GSM8K (b) MATH500 (c) AMC23 Figure 2: Performance comparison of LRMs of different sizes under various stress levels. REST reveals significant performance disparities among models of different sizes, although they demonstrate comparable, near-ceiling performance in traditional single-question evaluations. (a) 1.5B Model on GSM8K (b) 7B Model on MATH500 (c) 32B Model on AMC23 Figure 3: Limitation of post training on REST. Post training exacerbates performance degradation under higher stress levels compared to the original DeepSeek distilled models. AReaL-boba-RL-7B and OpenThinker2-7B achieve similar, near-ceiling single-question accuracies of 95.0% and 93.8% on MATH500, respectively. Yet, under REST, the accuracy of AReaL-boba-RL-7B drops sharply to 60.77%, while OpenThinker2-7B preserves significantly higher 83.30% accuracy. Similar trends are observed on AMC23, validating that REST can enhance the discriminative power of existing medium-difficulty benchmarks through problem concatenation. Limitations of post training. The recent success of DeepSeek-R1 [13] has inspired numerous followup studies [30, 31, 55, 10, 20, 62], some of which have adopted post RL training with verifiable rewards (RLVR) [30, 31] or supervised fune-tuning (SFT) [55] based on DeepSeek-distilled models. Although such post training often yields improved single-question accuracy, our findings suggest that they can not maintain their performance advantage on more challenging, multi-question scenarios as in REST. For instance, models like AReaL-boba-RL-7B and Light-R1-7B-DS, both post-trained on DeepSeek-R1Distill-Qwen-7B, achieve single-question accuracies of 95.00% and 93.20% on MATH500, slightly higher than the 93.00% accuracy of R1-7B. However, under REST, their accuracies drop significantly to 60.77% and 61.73%, respectively, both falling behind the 66.75% achieved by R1-7B. This discrepancy becomes even more pronounced at higher stress levels. As detailed in Fig. 3b and Tab. 7 (in Appendix), at stress level = 9, the accuracies of AReaL-boba-RL-7B and Light-R1-7B-DS decline sharply to 38.07% and 39.11%, respectively, more than 10.00% below the 49.42% achieved by R1-7B. This performance degradation is not limited to smaller models but also extends to larger ones, such as Light-R1-32BDS and R1-32B, and is consistently observed across other benchmarks like AMC23. These findings reinforce recent calls [11, 61, 41, 18] for more cautious assessment of the benefits of post RL training in enhancing reasoning capabilities. Potential of Long2Short Training. Despite their impressive performance on reasoning tasks, LRMs often struggle with excessively verbose reasoning processes [45, 5, 53]. To address this issue, recent studies [1, 3, 50] propose the Long2Short technique, which introduces length reward in the training objective to promote more concise and efficient reasoning. Our experiments demonstrate that 7 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once (a) 1.5B Model on MATH500 (b) 7B Model on MATH500 (c) 7B Model on AMC Figure 4: The effect of Long2Short training on REST. Long2Short training mitigates the performance degradation under higher stress levels. Long2Short training significantly enhances reasoning robustness under REST. As illustrated in Fig. 4, models trained with Long2Short, such as L1Qwen-1.5B-Exact and L1-Qwen-1.5B-Max [1], exhibit notable performance advantages at high stress levels. For instance, as shown in Tab. 6 of appendix, L1-Qwen-1.5B-Max surpasses R1-1.5B by substantial 44.71% accuracy margin at stress level = 9 on MATH500. similar trend is observed among 7B models. For example, the Efficient-R1-7B (α = 0.2) model [3], despite having lower singlequestion accuracy (88.20%) compared to R1-7B (93.00%), achieves 66.02% accuracy at stress level = 9, outperforming the R1-7Bs 49.42% by significant 16.60% accuracy margin. Table 2: Evaluation results on GPQA Diamond and LiveCodeBench. The Stress column reports the average performance across four distinct stress levels, as described in Sec. 4.1. Detailed accuracies for each stress level are in Tab. 6 of the Appendix. Models perform best under Stress are highlighted in gray. GQPA Diamond LiveCodeBench Stress Single 1.5B LRMs 37.37 31.82 33.84 36.87 26.26 DS-R1-Distill-Qwen-1.5B [13] DeepScaleR-1.5B [30] L1-Qwen-1.5B-Exact [1] L1-Qwen-1.5B-Max [1] Qwen2.5-1.5B-Inst [56] 15.05 21.15 18.28 19.35 1. 22.11 27.90 31.01 32.03 21.52 0.48 1.83 2.70 2.45 0.37 Model Single Bench Stress 7 8B LRMs"
        },
        {
            "title": "5.1 Error Analysis",
            "content": "Maximum output length overflow is not the only cause of performance degradation. To uncover the underlying reasons for LRMs failures in REST, we analyze the primary factors contributing to performance degradation under stress conditions. straightforward hypothesis is that performance drops primarily due to models exceeding their maximum output length when handling multiple problems. However, our findings indicate that this is not the sole causemodels frequently fail even when their responses remain well within the maximum output token limit. DS-R1-Distill-Qwen-7B [13] DS-R1-Distill-LLaMA-8B [13] Efficient-R1-7B (α = 0.2) [3] Nemotron-Nano-8B [4] AReaL-boba-RL-7B [31] Light-R1-7B-DS [55] OpenR1-Qwen-7B [10] OpenThinker2-7B [51] SimpleRL-Zoo-Qwen-7B [62] Open-Reasoner-Zero-7B [20] Marco-O1-7B [63] Qwen2.5-7B-Inst [56] DS-R1-Distill-Qwen-32B [13] Qwen-QwQ-32B [52] AReaL-boba-SFT-32B [31] Light-R1-32B-DS [55] S1.1-32B [32] OpenThinker2-32B [51] SimpleRL-Zoo-Qwen-32B [62] Open-Reasoner-Zero-32B [20] Qwen2.5-32B-Inst [56] 51.01 50.00 47.97 51.01 48.98 41.91 38.38 49.49 33.84 37.37 30.81 35.86 32B LRMs 60.10 63.64 63.13 65.66 61.62 62.12 46.46 60.10 42.93 31.67 33.99 34.37 34.06 29.13 30.32 36.04 40.60 35.49 34.25 28.32 35.15 37.63 39.43 38.71 50.90 37.99 39.07 4.66 39.43 5.73 16.13 9.32 13. 2.89 11.25 2.50 8.56 4.73 2.85 1.27 14.51 0.16 0.64 7.24 10.19 53.73 60.03 50.59 50.11 54.54 57.79 46.21 49.57 40.04 55.56 62.37 60.93 60.93 25.45 56.27 26.52 35.13 26.88 26.71 32.16 31.74 28.22 24.46 37.53 23.95 13.01 24.00 To investigate the cause of errors, we categorize the observed mistakes into six distinct types, as summarized in Tab. 3. detailed distribution of these error types for four representative LRMs on the AIME24 benchmark is illustrated in Fig. 5. The analysis reveals that Question Omission (QO) and Reasoning Error (RE) are the dominant DeepSeek-R1 [13] O3-mini [34] O4-mini [34] Gemini-2.5-Flash-Thinking [48] 63.44 60.21 70.61 61.65 64.63 67.39 73.11 68.00 40.83 48.36 63.07 48.34 70.20 71.21 76.26 78.79 API-Based LRMs REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Table 3: Six common cause of the performance degradation in REST. Error Cause Definition Output Truncation (OT) Endless Repetition (ER) Format Violation (FV) Question Omission (QO) Summary Error (SE) Reasoning Error (RE) The output is trucated due to reaching the maximum output length (32K tokens for LRMs). The model repeatedly generates the same phrase or sentence in its output. The model fails to comply with the required output format (e.g., not putting the answer whtin boxed{}). The model fails to respond to all the given questions in the thinking process. (e.g., only answers the first question) The model fails to summarize all the answers generated in the thinking process. Error occurs in the reasoning process, such as calculation mistakes or concept misunderstandings. (a) R1-7B [13] (b) Light-R1-7B [55] (c) Nemotron-7B [4] (d) DeepSeek-R1 [13] Figure 5: The error type distribution for various LRMs on AIME24 under REST. error types for DeepSeek-R1 (in subfigure 5d), collectively accounting for substantial 42.00% of the performance degradation under stress test. The high frequency of Question Omission (QO) suggests that DeepSeek-R1 suffers from pronounced question position bias, often focusing disproportionately on the first question in multi-problem prompts, despite explicit instructions to address each question sequentially. In contrast, error causes like Summary Error (SE), Output Truncation (OT), and Endless Repetition (ER) are rare in such high-capacity models. However, for smaller models like Light-R1-7B (in subfigure 5b), the error profile shifts significantly, with Endless Repetition (ER) and Output Truncation (OT) becoming more prominent, collectively accounting for 48.90%. This shift underscores smaller models unique challenges, whose limited capacity significantly exacerbates performance degradation under the stress imposed by REST. Finally, it is important to note that no Format Violation (FV) errors were found for any of the four models, which is why they are not included in Fig. 5. This observation supports the robustness of our prompt design and answer extraction strategy, confirming that the observed performance degradation arises primarily from the intrinsic limitations of the LRMs themselves, rather than artifacts introduced by the evaluation process. LRMs that perform better on REST exhibit concise reasoning for earlier questions. In this paragraph, we investigate why certain models exhibit substantial performance differences under REST, despite achieving similar results on traditional single-question benchmarks. To analyze this, on AIME24, we manually split the entire response into chunks corresponding to each question and (a) DS-R1-Distill-Qwen-7B [13] (b) Nemotron-nano-7B [4] (c) DeepSeek-R1 [13] Figure 6: The reasoning token count for questions at different positions on AIME24. 9 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once calculate the reasoning token count for each chunk. The results are presented in Fig. 6. It is evident that high-performing models under REST (e.g., Nemotron-nano-7B and DeepSeek-R1) tend to use fewer reasoning tokens for the first question when the stress level exceeds 1. In contrast, models with lower REST performance, such as R1-7B, often overthink the first question, consuming substantial portion of their reasoning tokens and leaving insufficient space for subsequent questions. This observation reveals that LRMs with superior performance in REST tend to employ more concise reasoning for earlier questions under stress, leaving sufficient space to address subsequent questions. We refer to this ability as adaptive reasoning effort allocation, which we believe is critical factor for achieving robust performance under REST."
        },
        {
            "title": "5.2 The Impact of Question Position and Question Order",
            "content": "Question position affects performance under stress test. Recent studies [60, 28] have identified positional biases of LLMs over long inputs. In this paragraph, we investigate whether the position of questions within the input can influence response accuracy. Fig. 7 presents heatmap illustrating the accuracy for questions placed at different positions. The results indicate that questions appearing earlier consistently achieve higher accuracy, even under the highest stress levels, while the accuracy for later questions tends to decline as the stress level increases. This observation contrasts with earlier findings [54], which suggest that question position has negligible effect on performance because previous analyses primarily focused on simpler text classification tasks. In contrast, complex reasoning tasks exhibit stronger positional dependence. We attribute this position-dependent performance degradation to three primary factors. First, our error analysis reveals that Question Omission (QO) is prevalent failure mode, even for SOTA LRMs like DeepSeek-R1, leading to high accuracy for the first question but significantly lower accuracy for the later ones. Second, the extensive reasoning required for earlier questions can introduce substantial noise, potentially disrupting the logical processing needed for subsequent responses. Finally, as shown in Fig. 6, answering earlier questions may consume considerable portion of the output tokens, reducing the models available reasoning tokens for later questions. (a) 7B on AIME24 (b) 7B on AIME25 (c) 32B on AIME24 (d) 32B on AIME25 Figure 7: The effect of question position under stress tests. The performance at each position is calculated by averaging the accuracy of questions appearing at the same position across the full benchmark. LRMs generally achieve higher accuracy for earlier questions, while their performance declines for subsequent ones. Question order affects performance under stress test. Building on the observation that question position impacts performance, we further investigate the effect of question order within each prompt on overall benchmark accuracy. Specifically, we assess this by arranging the questions within each instruction in either descending (hard-first) or ascending (easy-last) order of difficulty, using the fail rate of R1-7B as proxy for difficulty for problems in AIME24 and AIME25. As shown in Fig. 8, presenting questions from easy to hard consistently yields better overall accuracy compared to the reverse order. This discrepancy can be attributed to the tendency of LRMs to engage in lengthy and redundant reasoning processes when confronted with difficult questions early in the input. In contrast, placing simpler 10 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once (a) MATH500 (b) AIME24 (c) AIME Figure 8: The effect of question order on overall performance under stress tests. LRMs consistently perform worse when problems are arranged from hard to easy. questions first encourages more concise and straightforward reasoning chains, thereby minimizing the cognitive load and reducing the interference with subsequent questions."
        },
        {
            "title": "6 Conclusion",
            "content": "In conclusion, we present REST, stress testing framework designed to evaluate Large Reasoning Models (LRMs) in simultaneous multi-problem solving scenarios. REST significantly enhances the discriminative power of existing benchmarks that often suffer from performance saturation, revealing substantial performance differences among models that appear comparable in traditional singlequestion evaluations. Notably, even SOTA models like DeepSeek-R1 exhibit marked performance degradation under stress testing, challenging the common assumption that LLMs are multi-problem solvers. Additionally, REST uncovers few reasoning misbehaviors, such as question omission and summary error, which remain obscure in single-question assessments. Critical findings, such as the identification of question position bias and the demonstrated effectiveness of long2short training, offer valuable insights for the future development of more robust and capable LRMs. 11 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once References [1] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. [2] AI-MO. AIMO Validation AIME Dataset. [3] Daman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025. [4] Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, Eric Chung, and Chris Alexiuk. Llama-nemotron: Efficient reasoning models, 2025. URL https://arxiv.org/abs/2505.00949. [5] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. [6] Zhoujun Cheng, Jungo Kasai, and Tao Yu. Batch prompting: Efficient inference with large language model apis. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 792810, 2023. [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168. [8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [9] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [10] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github.com/ huggingface/open-r1. [11] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. [12] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. 12 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once [13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [14] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budgetaware llm reasoning. arXiv preprint arXiv:2412.18547, 2024. [15] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, 2024. [16] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/ file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper-round2.pdf. [17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https: //arxiv.org/abs/2103.03874. [18] Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias Bethge. sober look at progress in language model reasoning: Pitfalls and paths to reproducibility. arXiv preprint arXiv:2504.07086, 2025. [19] Yutao Hou, Yajing Luo, Zhiwen Ruan, Hongru Wang, Weifeng Ge, Yun Chen, and Guanhua Chen. Compound-qa: benchmark for evaluating llms on compound questions. arXiv preprint arXiv:2411.10163, 2024. [20] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Openreasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. [21] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report, 2024. URL https://arxiv.org/abs/2409.12186. [22] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [23] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations. [24] Ming Li, Pei Chen, Chenguang Wang, Hongyu Zhao, Yijun Liang, Yupeng Hou, Fuxiao Liu, and Tianyi Zhou. Mosaic-it: Free compositional data augmentation improves instruction tuning. arXiv preprint arXiv:2405.13326, 2024. [25] Yu Li, Qizhi Pei, Mengyuan Sun, Honglin Lin, Chenlin Ming, Xin Gao, Jiang Wu, Conghui He, and Lijun Wu. Cipherbank: Exploring the boundary of llm reasoning capabilities through cryptography challenges. arXiv preprint arXiv:2504.19093, 2025. [26] Honglin Lin, Zhuoshi Pan, Yu Li, Qizhi Pei, Xin Gao, Mengzhang Cai, Conghui He, and Lijun Wu. Metaladder: Ascending mathematical solution quality via analogical-problem reasoning transfer. arXiv preprint arXiv:2503.14891, 2025. [27] Yen-Ting Lin. Aime 2025 dataset, 2025. URL https://huggingface.co/datasets/yentinglin/aime_2025. Accessed: 2025-03-29. 13 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once [28] Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12, 2024. [29] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570, 2025. [30] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. [31] Zhiyu Mei, Wei Fu, Kaiwei Li, Guangju Wang, Huanchen Zhang, and Yi Wu. Real: Efficient rlhf training of large language models with parameter reallocation. In Proceedings of the Eighth Conference on Machine Learning and Systems, MLSys 2025, Santa Clara, CA, USA, May 12-15, 2025. mlsys.org, 2025. [32] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. [33] Sania Nayab, Giulio Rossolini, Marco Simoni, Andrea Saracino, Giorgio Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli. Concise thoughts: Impact of output length on llm reasoning and cost. arXiv preprint arXiv:2407.19825, 2024. [34] OpenAI. Openai o3 and o4-mini system card, Apr 2025. URL https://openai.com/index/ o3-o4-mini-system-card/. [35] Zhuoshi Pan, Yu Li, Honglin Lin, Qizhi Pei, Zinan Tang, Wei Wu, Chenlin Ming, Vicky Zhao, Conghui He, and Lijun Wu. Lemma: Learning from errors for mathematical advancement in llms. arXiv preprint arXiv:2503.17439, 2025. [36] Qizhi Pei, Lijun Wu, Zhuoshi Pan, Yu Li, Honglin Lin, Chenlin Ming, Xin Gao, Conghui He, and Rui Yan. Mathfusion: Enhancing mathematic problem-solving of llm through instruction fusion. arXiv preprint arXiv:2503.16212, 2025. [37] Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, et al. Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings. arXiv preprint arXiv:2501.01257, 2025. [38] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [39] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024. URL https://arxiv.org/abs/2308.12950. [40] Tushar Semwal, Promod Yenigalla, Gaurav Mathur, and Shivashankar Nair. practitioners guide to transfer learning for text classification using convolutional neural networks. In Proceedings of the 2018 SIAM international conference on data mining, pages 513521. SIAM, 2018. [41] Darsh Shah, Peter Rushton, Somanshu Singla, Mohit Parmar, Kurt Smith, Yash Vanjani, Ashish Vaswani, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, et al. Rethinking reflection in pre-training. arXiv preprint arXiv:2504.04022, 2025. [42] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. 14 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once [43] Connor Shorten, Charles Pierse, Thomas Benjamin Smith, Erika Cardenas, Akanksha Sharma, John Trengrove, and Bob van Luijt. Structuredrag: Json response formatting with large language models. arXiv preprint arXiv:2408.11061, 2024. [44] Guijin Son, SangWon Baek, Sangdae Nam, Ilgyun Jeong, and Seungone Kim. Multi-task inference: Can large language models follow multiple instructions at once? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 56065627, 2024. [45] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. [46] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North, page 4149. Association for Computational Linguistics, 2019. [47] Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-yi Lee, and Yun-Nung Chen. Let me speak freely? study on the impact of format restrictions on performance of large language models. arXiv preprint arXiv:2408.02442, 2024. [48] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [49] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [50] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [51] Open Thoughts Team. Open Thoughts, January 2025. URL https://www.open-thoughts.ai/. [52] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm. github.io/blog/qwq-32b/. [53] Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, et al. Thoughts are all over the place: On the underthinking of o1-like llms. arXiv preprint arXiv:2501.18585, 2025. [54] Zhengxiang Wang, Jordan Kodner, and Owen Rambow. Evaluating llms with multiple problems at once: new paradigm for probing llm capabilities. arXiv e-prints, pages arXiv2406, 2024. [55] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. [56] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [57] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. URL https://arxiv.org/abs/2409.12122. [58] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [59] Yixin Ye, Yang Xiao, Tiantian Mi, and Pengfei Liu. Aime-preview: rigorous and immediate evaluation framework for advanced mathematical reasoning. https://github.com/GAIR-NLP/AIME-Preview, 2025. GitHub repository. 15 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once [60] Yijiong Yu, Huiqiang Jiang, Xufang Luo, Qianhui Wu, Chin-Yew Lin, Dongsheng Li, Yuqing Yang, Yongfeng Huang, and Lili Qiu. Mitigate position bias in large language models via scaling single dimension. In First Workshop on Long-Context Foundation Models@ ICML 2024. [61] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. [62] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https: //arxiv.org/abs/2503.18892. [63] Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions, 2024. URL https://arxiv.org/abs/2411.14405. 16 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once"
        },
        {
            "title": "A Detailed Evaluation Result",
            "content": "We present the detailed accuracy statistics for each stress level of various LRMs in Table 6, Table 7 and Table 8. The stress degree {I, I, I, IV, V} corresponds to different stress levels in each dataset. For GSM8K: {1, 3, 6, 9, 12}; for MATH500 and AMC23: {1, 3, 5, 7, 9}; for AIME24, AIME25, GPQA and LiveCodeBench: {1, 2, 3, 4, 5}."
        },
        {
            "title": "B Prompt",
            "content": "We present the prompt for mathematical tasks, GPQA Diamond and LiveCodeBench in Figure 9, Figure 10 and Figure 11, respectively. Prompt 1: Prompt for Math Tasks {Questions}. Answer the above questions one by one. Remember to put your final answer within boxed{}. Figure 9: Evaluation prompt for math tasks. Prompt 2: Prompt for GQPA Diamond {Questions}. Answer the above multiple-choice question one by one. Remember to give each answer in the following format: ANSWER: boxed{LETTER} (without quotes) where LETTER is one of ABCD. Figure 10: Evaluation prompt for GPQA Diamond. Prompt 3: Prompt for Code Generation {Questions}. Answer the above questions one by one. Enclose the code for each question within delimiters as follows. ```python #YOUR CODE HERE ```. ### Answer: (use the provided format with backticks) Figure 11: Evaluation prompt for code generation tasks. Comparison between Rule-based and LLM-based Answer Extraction In this section, we compare the performance of several representative LRMs under REST when using rule-based versus LLM-based answer extraction. For rule-based extraction, we search the specific pattern sequentially using code modified from Qwen-Math evaluation tool . For the LLM-based approach, we employ gemma-3-27b-it [49] with prompt shown in Figure 12. The results in Table 4 https://github.com/QwenLM/Qwen2.5-Math REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Prompt 4: Answer Extraction Promopt Extract the final answers from the given predictions. Here are some extraction criteria: 1. Dont try to answer the original question. Your task is to extract the final answer from the prediction as it is, even if it is incorrect. 2. Prediction sometimes involves lengthy thinking processes, you dont need to consider these, just extract the final answer. 3. If there is question that has not been answered, dont answer it yourself. You should set the final answer to None (e.g., boxed{None}). 4. Ensure the number of final answers you extract is exactly the same as the number of the given questions. 5. Extract the final answer for each question one by one and enclose each final answer within an boxed. For example, if there are three questions, the output should be Answer to Q1: boxed{answer 1} Answer to Q2: boxed{answer 2} Answer to Q3: boxed{answer 3}. Here is your task. Simply extract the final answers from the given predictions. Dont apologize or correct yourself if there was mistake in the predictions; we are just trying to extract the final answer. <Questions Begin>: {question} <Questions End> <Predictions Begin>: {prediction} <Predictions End> Extract the final answers from the given predictions. Figure 12: Prompt for extracting answers from the response. Rule-Based LLM-Based Extraction Method GSM8K MATH500 AMC23 AIME24 AIME25 DS-R1-Distill-Qwen-7B 66.75 70.59 DS-R1-Distill-Qwen-32B 88.97 92.18 Rule-Based LLM-Based 52.51 53.63 16.53 18. 89.06 90.00 33.83 34.39 95.50 95.83 11.37 14.16 86.24 87.48 36.06 41. Table 4: Comparison of rule-based answer extraction and LLM-based answer extraction. indicate that LLM-based answer extraction can lead to modest performance improvement. However, this approach significantly increases evaluation costs and may introduce additional instability, making it less practical for large-scale assessments. Therefore, we adopt rule-based answer extraction as the default approach in our experiments. 18 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once"
        },
        {
            "title": "D The Impact of the maximum output length",
            "content": "Extending the maximum output length can not provide substantial improvement. As our error analysis identified output truncation as significant error type, it is questionable whether extending the maximum output length could improve performance under stress tests. To evaluate this, we extended the maximum output length from 32K to 128K tokens for top-tier 7B32B reasoning models. As shown in Tab. 5, this extension has negligible impact on stress test accuracy. 32K 128K Max Output Token GSM8K MATH500 AMC23 AIME24 AIME25 DS-R1-Distill-Qwen-7B 66.75 65.83 DS-R1-Distill-Qwen-32B 88.97 89.71 89.06 89.37 95.50 95.62 52.51 52. 36.06 36.66 16.53 16.17 86.24 86.77 11.37 11.73 33.83 33.87 32K 128K Table 5: The effect of extending the maximum output length. Extending the maximum output length has almost negligible impact on both 7B and 32B models."
        },
        {
            "title": "E Limitation",
            "content": "While REST provides powerful and discriminative evaluation framework, several limitations remain. First, it increases evaluation costs, as multi-problem prompts generally require longer reasoning sequences, leading to increased computational overhead. Additionally, given the rapid pace of advancement in reasoning models, there is always risk that our experiments may not capture all the latest state-of-the-art models released after our initial evaluations. To address this, we plan to continuously monitor newly released models and maintain an up-to-date leaderboard to reflect the latest progress in the field."
        },
        {
            "title": "F The Impact of Question Type",
            "content": "To further investigate the factors influencing model performance under stress testing, we analyze the effect of question types. Fig. 13 shows model performance across different stress levels for various types of math problems. We observe that both R1-7B and R1-32B exhibit similar trends. Specifically, performance degradation under stress is minimal for Algebra and Number Theory problems, indicating that models can handle multiple such questions in single prompt without significant loss. In contrast, Pre-Calculus, Pre-Algebra, and Geometry questions show substantial drops in accuracy under stress, even for the more capable R1-32B model. Notably, this performance degradation is not always correlated with single-question accuracy. For example, under stress level 1 (i.e., single-question setting), R1-32B achieves 98.21% accuracy on Pre-Calculus, higher than 94.31% on Pre-Algebra. However, at stress level 9, its Pre-Calculus accuracy drops to 90.71%, falling below the 92.51% accuracy on Pre-Algebra. This indicates that performance degradation under stress is not solely determined by problem difficulty, but also influenced by question type."
        },
        {
            "title": "G Error Type Illustrations",
            "content": "In this section, we provide detailed case study of each error type under stress tests in figures 14 to 18. Our findings indicate that Output Truncation is not the only reason for the performance degradation 19 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Bench Stress Degree Model GSM8K MATH500 AMC23 AIME24 AIME25 GPQA Diamond LiveCode Bench 1.5B LRMs DS-R1-Distill-Qwen-1.5B DeepScaleR-1.5B L1-Qwen-1.5B-Exact L1-Qwen-1.5B-Max Qwen2.5-Math-1.5B-Inst Qwen2.5-1.5B-Inst II III IV II III IV II III IV II III IV II III IV II III IV 84.62 82.21 76.21 66.30 56.13 84.84 83.37 73.81 61.10 48.04 84.87 84.25 83.19 76.62 69.99 84.17 83.92 81.98 77.37 67.89 85.37 81.32 75.74 64.67 48.21 65.13 25.52 12.94 7.65 6. 83.40 65.53 47.52 34.43 22.40 87.60 79.67 66.88 50.31 42.20 84.00 78.07 72.96 70.46 66.78 83.40 78.93 75.28 71.60 67.11 73.00 65.00 58.36 52.09 40.29 53.40 37.00 28.60 23.29 19.31 62.50 28.75 13.75 6.61 6.81 76.25 54.58 37.25 21.79 14.58 71.25 57.92 50.00 41.96 39.58 77.50 61.67 52.00 41.61 38.19 57.50 37.50 24.50 15.36 11.53 30.00 14.17 11.00 10.89 6.53 29.17 9.17 6.11 2.08 2.50 38.75 21.88 13.19 9.79 6.42 21.25 13.33 14.72 12.19 10.25 20.00 16.25 16.11 15.00 13.17 10.83 9.38 5.97 6.15 3.17 2.50 0 4.58 2.92 0.67 25.00 13.54 5.83 2.60 1.67 31.25 21.67 15.97 11.77 7.50 18.33 18.33 12.08 11.25 10.17 22.92 18.33 16.53 13.44 11.50 10.83 8.75 0.14 1.77 0.67 0.00 1.67 0 0 0 37.37 23.74 23.06 21.34 20.30 31.82 31.31 27.27 26.77 26.26 33.84 31.57 32.83 30.43 29.19 36.87 32.32 32.49 30.68 32.63 26.77 - - - - 26.26 22.80 21.96 21.03 20.27 15.05 1.08 0.60 0.18 0.07 21.15 3.76 1.67 1.61 0.29 18.28 5.73 1.79 2.33 0.93 19.35 4.48 2.27 1.61 1.43 1.79 - - - - 1.79 0.90 0.36 0.09 0. Table 6: Detailed accuracy statistics for each stress level of various LRMs. Some models are only evaluated on math-related benchmarks, as their official guidelines do not recommend their use for other task domains. under stress tests, revealing some unexpected misbehaviors like Question Omission and Summary Error that remain hidden in standard evaluation settings. 20 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Bench Stress Degree Model GSM8K MATH500 AMC23 AIME24 AIME25 GPQA Diamond LiveCode Bench DS-R1-Distill-Qwen-7B Efficient-R1-7B (α = 0.1) Efficient-R1-7B (α = 0.2) DS-R1-Distill-LLaMA-8B Nemotron-Nano-8B AReaL-boba-RL-7B Light-R1-7B-DS II III IV II III IV II III IV II III IV II III IV II III IV II III IV 89.49 89.01 90.03 89.37 87.83 88.63 85.49 84.52 84.85 84.16 87.95 84.10 81.49 78.92 77.02 90.45 89.13 86.59 84.33 80.67 91.36 90.37 69.27 34.93 18.71 91.66 91.08 85.72 72.48 61.91 88.05 83.90 84.96 83.56 78.34 7 8B LRMs 93.00 83.67 72.16 61.74 49.42 90.00 86.80 79.12 72.86 61.18 88.20 85.20 81.28 73.14 66.02 89.80 85.20 82.92 80.69 76.53 94.40 89.67 87.48 84.83 82.18 95.00 83.00 68.56 53.46 38.07 93.20 83.47 69.16 55.17 39.11 87.50 68.33 41.00 22.14 12.78 87.50 70.83 53.00 29.29 23.89 85.00 76.25 52.75 35.00 28.19 87.50 81.25 75.25 64.29 62.22 90.00 88.75 80.00 70.36 65.83 91.25 60.42 37.00 20.71 13.61 90.00 67.50 38.00 23.57 10.56 54.17 35.00 16.94 8.44 5.75 54.58 40.62 21.11 14.58 9.50 50.42 38.54 24.86 16.67 8.42 55.00 38.33 36.67 27.92 22.00 63.33 52.50 47.78 41.25 32.67 61.25 37.08 22.22 15.00 11.42 55.83 31.87 18.19 10.21 6.25 35.42 21.67 13.06 7.92 2.83 35.42 26.46 16.94 12.19 7.42 33.75 27.29 19.31 14.06 8.33 28.33 25.00 22.78 24.17 18.67 50.00 40.83 36.11 27.50 24.67 45.83 22.08 12.64 8.33 6.25 45.83 24.17 14.17 7.81 5.67 51.01 38.38 31.31 29.80 27.17 48.98 39.65 38.38 33.84 31.72 47.97 38.13 33.67 33.46 32.22 50.00 39.14 34.85 32.07 29.90 51.01 31.57 34.18 38.76 31.72 48.99 38.64 27.61 25.00 25.25 51.01 36.87 32.15 28.41 23. 37.63 7.35 2.15 0.90 1.15 39.07 7.35 2.75 0.72 0.65 38.71 6.63 2.15 0.63 0.57 39.43 12.54 14.34 10.75 7.38 50.90 15.41 9.08 6.09 3.66 37.99 8.60 3.46 3.41 3.44 39.07 8.06 1.43 0.81 1.08 Table 7: Detailed accuracy statistics for each stress level of various LRMs. Some models are only evaluated on math-related benchmarks, as their official guidelines do not recommend their use for other task domains. 21 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Bench Stress Degree Model GSM8K MATH500 AMC23 AIME24 AIME25 GPQA Diamond LiveCode Bench OpenR1-Qwen-7B OpenThinker2-7B SimpleRL-Zoo-Qwen-7B Open-Reasoner-Zero-7B Marco-O1-7B MathFusion-Qwen-7B Eurus-2-7B-PRIME Qwen2.5-Math-7B-Inst Qwen2.5-7B-Instruct II III IV II III IV II III IV II III IV II III IV II III IV II III IV II III IV II III IV 95.60 91.43 92.67 90.38 86.38 94.39 93.78 92.85 91.31 90.01 90.52 81.20 86.57 86.31 81.97 92.87 87.19 68.46 55.10 49.80 89.08 83.62 76.06 77.95 80.61 89.46 87.47 84.76 82.92 79.97 92.72 90.12 88.38 86.83 86.70 95.53 92.14 86.62 76.03 59.32 92.27 88.07 85.37 83.89 83.16 7 8B LRMs 92.20 86.40 83.64 80.11 76.42 93.80 89.20 83.48 82.23 78.29 77.80 70.53 66.76 59.54 52.82 83.00 52.93 34.84 26.46 15.82 72.40 57.80 51.08 46.66 37.20 74.00 72.27 69.32 67.17 63.84 81.40 70.60 64.72 63.80 59.64 83.60 73.27 64.28 49.89 38.91 77.60 68.80 67.60 64.89 61.84 83.75 73.75 61.00 44.46 37.22 85.00 78.75 63.00 58.39 52.78 62.50 41.67 29.50 31.79 21.94 60.00 30.83 15.75 10.00 8.06 47.50 26.67 17.00 13.57 11.67 52.50 50.00 36.00 32.86 26.11 62.50 49.17 42.75 35.18 27.22 60.00 45.00 29.00 21.79 18.06 42.50 41.67 34.00 30.36 31.81 47.50 37.08 26.11 26.15 17.75 54.58 45.83 36.67 31.15 24.33 26.67 6.67 12.08 7.29 4.17 17.92 9.79 7.92 3.54 3.25 10.00 5.42 5.69 4.27 2.00 9.58 10.42 7.78 6.67 6.67 20.83 14.17 13.75 7.60 7.83 14.17 8.12 7.78 6.35 3.33 10.00 7.71 6.53 5.42 8. 32.92 27.50 25.56 18.12 13.58 41.67 30.21 27.36 20.31 16.75 10.00 10.00 5.42 5.21 5.25 16.25 7.50 4.31 1.56 2.17 10.83 5.62 3.47 4.79 0.67 5.83 3.12 4.03 1.98 0.25 14.58 6.46 4.31 4.17 3.00 11.67 9.17 3.75 4.58 3.83 3.75 0 3.89 4.38 5.00 38.38 35.86 36.70 38.76 32.83 44.44 41.67 40.91 42.05 37.78 33.84 36.62 33.16 36.11 36.06 37.37 33.33 35.69 35.86 32.12 30.81 28.54 30.64 29.67 24.44 - - - - - - - - - - 35.35 33.58 34.34 35.86 33.08 35.69 36.99 34.85 4.66 2.33 0.72 0.72 1.29 39.43 17.38 16.37 12.81 11.47 5.73 0.18 0.36 0.09 0 16.13 0.72 0.48 0.63 0.72 9.32 8.24 6.69 7.08 6.95 - - - - - - - - - - 4.30 2.87 1.55 0.81 0.57 13.98 11.83 11.47 9.05 8.39 Table 8: Detailed accuracy statistics for each stress level of various LRMs. Some models are only evaluated on math-related benchmarks, as their official guidelines do not recommend their use for other task domains. 22 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Bench Model Stress Degree GSM8K MATH500 AMC23 AIME24 AIME GPQA Diamond LiveCode Bench 32B LRMs DS-R1-Distill-Qwen-32B Qwen-QwQ-32B AReaL-boba-SFT-32B Light-R1-32B-DS S1.1-32B OpenThinker2-32B SimpleRL-Zoo-Qwen-32B Open-Reasoner-Zero-32B Qwen2.5-32B-Inst II III IV II III IV II III IV II III IV II III IV II III IV II III IV II III IV II III IV 95.54 95.73 95.44 95.43 95.39 95.83 96.21 95.99 95.71 95.20 93.33 93.56 95.01 95.09 95.32 95.83 95.68 95.40 94.01 94.07 89.84 70.71 61.04 56.81 55.82 96.44 95.48 95.27 95.19 94.74 96.06 95.40 94.35 92.79 91.43 95.83 93.86 92.86 91.21 89.27 95.53 94.36 94.17 93.37 93.19 94.60 91.47 90.72 88.69 85.00 96.2 93.80 92.48 92.49 91.20 95.00 93.00 89.64 87.26 85.78 95.60 91.53 85.52 79.69 77.91 90.40 65.13 54.92 47.91 47.44 96.20 92.80 90.48 88.63 88.49 83.20 79.80 79.52 79.37 76.89 92.00 87.13 83.28 80.06 81.13 82.20 77.07 74.76 72.20 69.51 94.75 91.25 89.00 82.50 82.22 95.00 94.17 87.25 79.29 70.83 97.50 95.00 83.50 75.54 61.81 96.25 91.67 75.25 56.07 52.22 90.00 48.75 35.00 26.96 18.33 95.00 94.58 80.25 78.04 71.11 67.50 61.67 57.00 56.07 53.33 83.75 80.00 68.50 68.04 63.61 60.00 60.00 50.00 48.04 40.83 72.92 63.33 56.25 48.54 41.92 78.75 66.88 59.86 49.58 42.83 77.50 63.12 55.42 37.60 27.00 77.50 57.50 49.17 32.60 25.75 55.83 36.87 25.97 20.10 14.75 68.33 61.46 61.94 50.21 38.42 27.20 16.67 16.67 19.17 14.67 46.67 38.33 35.00 27.92 25.33 20.00 10.00 11.11 8.33 9.00 51.67 46.04 36.39 29.79 23.08 69.58 56.04 45.83 37.50 26.75 60.00 50.83 37.36 28.02 18.00 66.67 52.71 36.67 26.56 19.25 45.42 29.79 19.86 14.69 12.17 52.50 50.62 41.53 33.96 26.67 16.67 8.12 7.92 8.75 10.67 36.67 26.67 26.11 22.08 19.67 16.67 11.67 6.39 4.17 4. 60.10 57.83 55.22 51.14 50.71 63.64 60.35 59.43 61.36 58.99 63.13 54.80 54.21 47.98 45.35 65.66 57.58 52.19 48.23 42.42 61.62 55.30 58.08 52.15 52.63 62.12 59.09 56.73 58.46 56.87 46.46 44.19 48.48 45.58 46.57 60.10 51.77 50.51 47.60 48.38 42.93 39.39 39.06 41.79 39.90 55.56 37.63 26.64 23.30 19.28 62.37 45.34 32.97 26.52 23.80 60.93 45.34 36.32 27.24 18.06 60.93 39.96 32.97 22.31 17.63 25.45 25.09 25.09 24.37 23.30 56.27 45.34 39.19 35.39 30.18 26.52 25.81 25.33 22.49 22.15 35.13 14.16 13.50 11.92 12.47 26.88 25.45 24.25 23.30 23.01 Table 9: Detailed accuracy statistics for each stress level of various LRMs. 23 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Bench Model Stress Degree GSM8K MATH500 AMC23 AIME24 AIME25 GPQA Diamond LiveCode Bench API-based LRMs DeepSeek-R1 O3-Mini O4-Mini Gemini-2.5-Flash-Thinking II III IV II III IV II III IV II III IV 96.20 96.10 96.20 96.18 95.75 95.83 95.17 94.35 93.69 92.17 93.71 93.25 92.96 92.97 93.11 89.23 89.87 91.76 92.12 91. 97.00 94.20 93.40 91.94 88.80 95.00 93.87 80.60 88.94 90.31 90.00 85.67 83.40 82.83 80.96 97.20 87.67 71.96 69.60 68.20 93.75 88.75 84.75 84.11 69.58 90.00 70.75 59.58 58.75 56.25 96.25 90.42 80.75 84.82 75.83 97.50 75.00 54.00 41.25 41.39 81.66 66.67 57.78 54.17 31.33 79.16 49.00 40.56 30.83 30.83 73.33 62.50 53.89 52.50 42.67 76.67 65.83 38.89 22.92 18.00 68.75 55.00 40.00 31.67 22.00 71.66 58.33 38.33 27.50 13.75 80.00 52.50 53.33 49.17 33.67 71.67 47.50 27.22 18.75 14.33 70.20 70.45 64.48 62.88 60.71 71.21 68.43 67.51 66.54 67.07 76.26 74.24 73.40 71.97 72.83 78.79 72.73 67.34 68.18 63.74 63.44 50.54 44.44 34.59 33.76 60.21 52.87 51.73 47.67 41.15 70.61 67.20 62.96 61.56 60.57 61.65 53.05 50.42 47.94 41. Table 10: Detailed accuracy statistics for each stress level of various LRMs. (a) DS-R1-Distill-Qwen-7B (b) DS-R1-Distill-Qwen-32B Figure 13: Performance degradation across different question types on MATH500. 24 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Example G.1: Example of Output Truncation 2 , 0), and B(0, Query: 3 Q1: Let O(0, 0), A( 1 2 ) be points in the coordinate plane. Let be the family of segments PQ of unit length lying in the first quadrant with on the x-axis and on the y-axis. There is unique point on AB, distinct from and B, that does not belong to any segment from other than AB. Then OC2 = , where and are relatively prime positive integers. Find + q. Q2: Find the number of triples of nonnegative integers (a, b, c) satisfying + + = 300 and a2b + a2c + b2a + b2c + c2a + c2b = 6, 000, 000. Answer the above questions one by one. Remember to put your final answer within boxed{}. Response: Okay, so have these two problems to solve. Let me start with the first one. Problem Q1: We have points O(0,0), A(1/2, 0), and B(0, sqrt(3)/2). The family consists of segments PQ of unit length in the first quadrant, with on the x-axis and on the y-axis. We need to find unique point on AB (other than and B) that doesnt lie on any other segment from except AB. Then compute OC² and express it as reduced fraction p/q, and find p+q. Alright, lets break this down. First, lets visualize the points. is the origin, is (1/2, 0), which is halfway between (0,0) and (1,0). is (0, sqrt(3)/2), which is halfway between (0,0) and (0,1) in some sense, but sqrt(3)/2 is approximately 0.866. So, AB is line segment connecting (1/2, 0) to (0, sqrt(3)/2). Let me compute the equation of line AB. . . . (Reasoning content for Q1) . . . So, to find such point C, we need to find point on AB that is not on any other PQ, which are all lines from (p, 0) to (0, q) with + = 1. Problem Q2: Find the number of triples of nonnegative integers (a, b, c) satisfying + + = 300 and ab + ac + ba + bc + ca + cb = 6, 000, 000. Okay, so we have two equations: . . . (Reasoning content for Q2) . . . Thus, each non-symmetric set is unique, so 6 such sets, each contributing 6 ordered triples 36. Plus the symmetric solution 1. Total=37. But, in reality, the equation may have more solutions. Wait, let me think. Wait, the equation is:(a(b + c) + b(c + a) + c(a + b))/2006 = abc. So, with (Truncated) Figure 14: Example of Output Truncation error on DS-R1-Distill-Qwen-7B. REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Example G.2: Example of Endless Repetiion Query: Q1: Let ABCD be tetrahedron such that AB = CD = 89. There exists point inside the tetrahedron such that the distances from to each of the faces of the tetrahedron are all equal. This distance can be written in the form relatively prime, and is not divisible by the square of any prime. Find + + p. Q2: Let be the least prime number for which there exists positive integer such that n4 + 1 is divisible by p2. Find the least positive integer such that m4 + 1 is divisible by p2. Answer the above questions one by one. Remember to put your final answer within boxed{}. , where m, n, and are positive integers, and are 80, and BC = AD = 41, AC = BD = 41, AC and BD are both 80, and BC and AD are both Response: Problem Q1: We have tetrahedron ABCD with some specific edge lengths. The edges AB and CD are both 89. Theres point inside this tetrahedron such that the distances from to each face are equal. We need to find this distance, express it in specific form, and then compute + + where the distance is n/p. Hmm, okay. So, first, remember that in tetrahedron, the point equidistant from all four faces is called the incenter, similar to the incenter of triangle. The distance from this point to each face is the inradius. . . . (Reasoning content for Q1) . . . Wait, 236? Hmm, that seems bit high, but maybe its correct. Wait, let me check the formula for the inradius again. Yes, for tetrahedron, = 3V/A, where is the total surface area. So, yes, thats correct. Alternatively, another way to compute the inradius is to note that in disphenoid, the inradius can be computed using the formula = (3V)/Atotal, which is what did. So, think 236 is the answer for Q1. Problem Q2: We need to find the least prime number such that there exists positive integer where p2 divides n4 + 1. Once we find that prime p, we need to find the least positive integer such that m4 + 1 is divisible by p2. So, first, we need to find the smallest prime where p2 n4 + 1 for some n. To do this, think we can consider primes where 1 is quartic residue modulo p2. Because n4 1 mod p2, which implies that n4 + 1 0 mod p2. Therefore, we need to find the smallest prime such that the equation x4 1 mod p2 has solution. Alternatively, since if x4 1 mod p2, then x4 1 mod p, so first, we can check for primes where 1 is quartic residue modulo p. . . . (Reasoning content for Q2) . . . But wait, 110 is quite large, maybe can find smaller one. Wait, let me try = 55. 552 = 3, 025. 3, 025 mod 289: 289 10 = 2, 890. 3, 025 2, 890 = 135. 1352 = 18, 225. 18, 225 mod 289: 289 62 = 17, 918. 18, 225 17, 918 = 307. 307 289 = 18. So, 554 18 mod 289. 18 + 1 = 19 = 0. = 55 doesnt work. Wait, = 68. 682 = 4, 624. 4, 624 mod 289: 289 15 = 4, 335. 4, 624 4, 335 = 289. 289 0 mod 289. So, 682 0 mod 289. So, 684 0 mod 289. Doesnt work. = 69. 692 = 4, 761. 4, 761 mod 289: 289 16 = 4, 624. 4, 761 4, 624 = 137. 1372 = 18, 769. 18, 769 mod 289: 289 64 = 18, 496. 18, 769 18, 496 = 273. 273 289 = 16 273 mod 289. So, 694 273 mod 289. 273 + 1 = 274 274 = 0. = 70. 702 = 4, 900. 4, 900 mod 289: 289 16 = 4, 624. 4, 900 4, 624 = 276. 2762 = 76, 176. 76, 176 mod 289: 289 263 = 76, 107. 76, 176 76, 107 = 69. So, 704 69 mod 289. 69 + 1 = 70 70 = 0. = 71. 712 = 5, 041. 5, 041 mod 289: 289 17 = 4, 913. 5, 041 4, 913 = 128. 1282 = 16, 384. 16, 384 mod 289: 289 56 = 16, 184. 16, 384 16, 184 = 200. 200 + 1 = 201 = 0. = 72. 722 = 5, 184. 5, 184 mod 289: 289 17 = 4, 913. 5, 184 4, 913 = 271. 2712 = 73, 441. 73, 441 mod 289: 289 253 = 73, 197. 73, 441 73, 197 = 244. 244 289 = 45 244 mod 289. 244 + 1 = 245 = 0. = 73. 732 = 5, 329. 5, 329 mod 289: 289 18 = 5, 202. 5, 329 5, 202 = 127. 1272 = 16, 129. 16, 129 mod 289: 289 55 = 15, 895 55 = 15, 895 55 = 15, 895 55 = 15, 895 55 = 15, 895 55 = 15, 895 55 = 15, (Endless Repeating 289 55 = 15, 895). Figure 15: Example of Endless Repetition error on DS-R1-Distill-Qwen-7B. REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Example G.3: Example of Question Omission Query: Q1: Let be the greatest four-digit positive integer with the property that whenever one of its digits is changed to 1, the resulting number is divisible by 7. Let and be the quotient and remainder, respectively, when is divided by 1000. Find + R. Q2: Alice chooses set of positive integers. Then Bob lists all finite nonempty sets of positive integers with the property that the maximum element of belongs to A. Bobs list has 2024 sets. Find the sum of the elements of A. Answer the above questions one by one. Remember to put your final answer within boxed{}. Response: Alright, so have these two math problems to solve. Let me start with Q1. It says: Let be the greatest four-digit positive integer with the property that whenever one of its digits is changed to 1, the resulting number is divisible by 7. Let and be the quotient and remainder, respectively, when is divided by 1000. Find + R. Okay, so is four-digit number, and if you change any one of its digits to 1, the new number is divisible by 7. We need to find N, then divide it by 1000 to get and R, and then add them together. First, let me think about how to approach this. Since changing any digit to 1 results in number divisible by 7, that gives us several conditions. Lets denote as four-digit number: = ABCD, where A, B, C, are its digits. Then, changing each digit one by one to 1 should result in number divisible by 7. So, changing to 1: 1BCD should be divisible by 7. Changing to 1: A1CD should be divisible by 7. . . . (Reasoning content for Q1) . . . After checking various possibilities, we find that the valid number is 5694. When = 5694 is divided by 100, the quotient is 56 and the remainder is 94. Therefore, we compute + 10R: Thus, the final answer is 996 . (Missing answer for Q2) + 10R = 56 + 10 94 = 56 + 940 = Figure 16: Example of Question Omission error on DS-R1-Distill-Qwen-7B. 27 REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Example G.4: Example of Summary Error Query: Q1: Consider the paths of length 16 that follow the lines from the lower left corner to the upper right corner on an 8 8 grid. Find the number of such paths that change direction exactly four times, like in the examples shown below. [asy] size(7.5cm); usepackage(\"tikz\"); label(\"begin{tikzpicture}[scale=.4] draw(0,0)grid(8,8); draw[line width=2,red](0,0)--(2,0)--(2,3)--(5,3)--(5,8)--(8,8); end{tikzpicture}\",origin); label(\"begin{tikzpicture}[scale=.4]draw(0,0)grid(8,8); draw[line width=2,red](0,0)--(0,3)--(3,3)--(3,5)--(8,5)--(8,8); end{tikzpicture}\",E); [/asy] Q2: Rectangles ABCD and EFGH are drawn such that D, E, C, are collinear. Also, A, D, H, all lie on circle. If BC = 16, AB = 107, FG = 17, and EF = 184, what is the length of CE? [asy] import graph; unitsize(0.1cm); pair = (0,0);pair = (70,0); pair = (70,16);pair = (0,16);pair = (3,16);pair = (90,16);pair = (90,33); pair = (3,33); dot(A^^B^^C^^D^^E^^F^^G^^H); label(\"$A$\", A, S); label(\"$B$\", B, S);label(\"$C$\", C, N);label(\"$D$\", D, N); label(\"$E$\", E, S);label(\"$F$\", F, S);label(\"$G$\", G, N); label(\"$H$\", H, N); draw(E--D--A--B--C--E--H--G--F--C); [/asy] Answer the above questions one by one. Remember to put your final answer within boxed{}. Response: Okay, so have these two questions to solve. Let me start with the first one. Q1 Paths on an 8x8 Grid Alright, so the problem is about finding the number of paths of length 16 on an 8x8 grid that ... . . . (Reasoning content for Q1) . . . Similarly, the second example alternates more, but think the reasoning still holds. So, yeah, 294 seems correct. Therefore, the answer is 294. Q2: Rectangles and Circle Problem Alright, moving on to the second question. Its geometry problem involving rectangles and circle. . . . (Reasoning content for Q2) . . . Therefore, regardless of the exact value of e, CE is 201. Wait, thats interesting. Therefore, CE = 201. So, the exact value is 201. Therefore, the length of CE is 201. </think> Final Answer To solve the first problem, we need to find the number of ways to arrange the letters of the rectangle ROOT such that the letters and are adjacent. 1. The word ROOT has 4 letters with two Os. The distinct letters are R, O, O, T. 2. We treat the pair DE as single letter, resulting in three units: DE, R, T. 3. These three units can be arranged in 3! = 6 ways. 4. However, DE can also be treated as ED, so we multiply by 2, giving 12 ways. 5. Considering the two Os, the total number of distinct arrangements is 12. Thus, the number of ways is 12 . (The answer to Q1 in the final answer is inconsistent with the answer given during the thinking process.) For the second problem, we need to find the length of CE in rectangle ABCD with specific conditions. . . . (Answer content for Q2) . . . Figure 17: Example of Summary Error on DS-R1-Distill-Qwen-7B. REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once Example G.5: Example of Reasoning Error Query: Q1: Eight circles of radius 34 are sequentially tangent, and two of the circles are tangent to AB and BC of triangle ABC, respectively. 2024 circles of radius 1 can be arranged in the same manner. The inradius of triangle ABC can be expressed as [asy] pair = (2,1); pair = (0,0); pair = (3,0); dot(A^^B^^C); , where and are relatively prime positive integers. Find + n. label(\"$A$\", A, N); label(\"$B$\", B, S); label(\"$C$\", C, S); draw(A--B--C--cycle); for(real i=0.62; i<2.7; i+=0.29){ draw(circle((i,0.145), 0.145)); } [/asy] Q2: Find the largest possible real part of (75 + 117i)z + 96+144i Answer the above questions one by one. Remember to put your final answer within boxed{}. where is complex number with = 4. Response: Okay, so have these two problems to solve, Q1 and Q2. Let me try to tackle them one by one. Ill start with Q1 because it seems bit more straightforward, even though might need to think carefully. Problem Q1: Alright, so we have triangle ABC with two circles tangent to sides AB and BC respectively, each of radius 34. Then, there are 2024 circles of radius 1 arranged in the same manner. We need to find the inradius of triangle ABC expressed as fraction m/n, where and are coprime, and then find + n. Hmm, okay. So, recall that when circles are tangent to sides of triangle and each other, it often relates to similar triangles or some kind of geometric progression. Maybe its something to do with the inradius being built up by smaller circles? . . . (Reasoning content for Q1) . . . (cid:112)(34 1) = Alternatively, maybe its the geometric mean, 34, but thats irrational. But since the problem states that the inradius can be expressed as m/n, where and are coprime integers, 35 is reasonable answer. So, perhaps the answer is 35, so + = 36. Therefore, after all this thinking, think the inradius is 35, so the answer is 36. Final Answer 36 Now, moving on to problem 2. We have the function (x) = 75+14xx2 , and we need to find the maximum where and are complex numbers with = = 4. First, need to value of the expression understand what is being asked. We have complex expression, and we need to find its maximum value as and range over complex numbers with magnitude 4. So, let me denote = i, = j, with = = 4. Then, the expression becomes: 75+117z144w (z)4 . . . (Reasoning content for Q2) . . . Wait, maybe the maximum is 15, so the answer is 15, sum is 16. But to be honest, given that my calculations are not consistent, maybe the answer is 15. But to end, think the maximum value is 15, so 15/1, so 15 + 1 = 16. Final Answer Problem 2 16 (Errors occur in the reasoning process of Q1 and Q2.) (75+117i144j) (i)4 1+2x Figure 18: Example of Reasoning Error on DS-R1-Distill-Qwen-7B."
        }
    ],
    "affiliations": [
        "OpenDataLab",
        "Renmin University of China",
        "Shanghai Artificial Intelligence Laboratory",
        "Tsinghua University"
    ]
}